{"title": "QUOTE: Question-Oriented Text Embeddings", "authors": ["Andrew Neeser", "Kaylen Latimer", "Aadyant Khatri", "Chris Latimer", "Naren Ramakrishnan"], "abstract": "We present QUOTE (Question-Oriented Text Embeddings), a novel\nenhancement to retrieval-augmented generation (RAG) systems,\naimed at improving document representation for accurate and nu-\nanced retrieval. Unlike traditional RAG pipelines, which rely on\nembedding raw text chunks, QuOTE augments chunks with hypo-\nthetical questions that the chunk can potentially answer, enriching\nthe representation space. This better aligns document embeddings\nwith user query semantics, and helps address issues such as ambi-\nguity and context-dependent relevance. Through extensive exper-\niments across diverse benchmarks, we demonstrate that QUOTE\nsignificantly enhances retrieval accuracy, including in multi-hop\nquestion-answering tasks. Our findings highlight the versatility of\nquestion generation as a fundamental indexing strategy, opening\nnew avenues for integrating question generation into retrieval-\nbased Al pipelines.", "sections": [{"title": "1 INTRODUCTION", "content": "Retrieval-augmented generation (RAG [35, 36, 40]) serves as a sig-\nnificant contribution to the deployment and acceptance of LLMs in\npractice. Given a user's prompt, RAG retrieves relevant information\nfrom a document collection, augments (prefixes) it to the user's\nprompt, thus helping ensure that any generated content can be\naccurate, pertinent, and grounded in up-to-date information. In\na typical RAG implementation, at pre-query time, the corpus is\nbroken down into chunks, which are stored as vector embeddings.\nAt query time, these chunks are searched and used to augment the\nuser's prompt. Several variants of RAG have been proposed over\nthe years [2, 4, 13] to address specific use cases and challenges.\nRAG has helped reinforce the criticality of information retrieval\n(IR) as a vital component of modern NLP and AI pipelines. De-\nspite this resurgence, much of the focus has been on enhancing\nthe G (generation) component, often leaving advancements in the\nR (retrieval) aspect comparatively underexplored. Recently, some\nnotable efforts have emerged to address this imbalance.\nFor example, Anthropic introduced contextual retrieval [1] where\neach chunk is augmented with additional context before embed-\nding; this approach is claimed to reduce incorrect chunk retrieval\nrates by up to 67%. Similarly, recent works have explored prompt\ncaching [8], a strategy to reuse previously retrieved or generated\nresults to optimize latency and computation costs in iterative or\nrepetitive query scenarios.\nOur work aligns with this vein of 'advancing R for G', partic-\nularly focusing on improving the modeling of document chunks\nas they are embedded. One of our key insights is that documents\ncan often be more effectively represented by the questions they\ncan answer, rather than solely by their direct content. To this end,\nfor each chunk, we propose generating a set of questions that the\nchunk is likely to answer, embedding these alongside the original\ncontent. We refer to such embeddings as Question-Oriented Text\nEmbeddings (QUOTE)."}, {"title": "2 RELATED WORK", "content": "Many studies have highlighted the impact of key design choices\nfor the success of a RAG implementation [25, 27, 31]."}, {"title": "2.1 Dense vs Sparse Retrievers", "content": "The debate between dense and sparse retrievers continues into\nRAG research [3, 30]. Dense retrievers, such as those based on\nvector embeddings, excel at capturing semantic similarity, making\nthem particularly effective for nuanced queries. However, sparse\nretrievers like BM25 and TF-IDF continue to dominate in scenarios\nwhere explicit token matches, such as named entities, acronyms, or\nabbreviations, are critical to relevance. This distinction has led to\nhybrid approaches in many RAG systems, which combine dense and\nsparse retrievers. For example, a typical implementation involves\nfirst running a keyword-based sparse retrieval to gather an initial\npool of relevant chunks, followed by a dense retrieval to refine the\nresults."}, {"title": "2.2 Retrievers vs Rerankers", "content": "Many RAG systems employ a two-step pipeline: a fast retriever\nselects the top-k candidate chunks, and a reranker, typically a com-\nputationally intensive cross-encoder, reorders these candidates for\nfinal use. While rerankers generally improve the quality of retrieved\nresults, recent research [12] cautions against extending reranking to\nlarger candidate sets. Beyond a certain threshold, performance tends\nto plateau and may even degrade, likely due to noise introduced in\nlarger retrieval pools. These findings underscore the importance\nof balancing efficiency and effectiveness in the retrieval-reranking\npipeline."}, {"title": "2.3 Exact search vs Approximate Nearest\nNeighbors (ANN)", "content": "Approximate nearest neighbor (ANN) techniques [11] have become\nthe de facto standard for scalable dense retrieval due to their ability\nto handle large corpora efficiently. However, exact search methods,\nwhile computationally more demanding, offer greater precision in\ncertain use cases, such as high-stakes QA tasks. Several studies [21,\n37] compare these approaches, highlighting trade-offs in latency,\naccuracy, and robustness to query variations. For instance, ANN\nmethods may struggle with long-tail queries or datasets containing\nsubtle semantic distinctions."}, {"title": "2.4 Distractions vs Noise in RAG", "content": "Cuconasu et al. [5] study the performance of RAG for QA tasks in\nthe presence of so-called distracting and noise documents. Distract-\ning documents are those with high retrieval scores, but that do not\ncontain the answer; noise documents are picked at random from the\ncorpus. The interesting finding from this study was that while dis-\ntracting documents lead to performance deterioration as expected,\nnoise documents lead to improved performance, presumably due\nto better reliance on pretrained reasoning. However, these findings\nare somewhat questioned by recent work [18], which suggests that\nnoise documents can degrade system reliability in certain settings,\ncalling for further investigation."}, {"title": "2.5 Real vs Hypothetical Embeddings", "content": "Contextual retrieval techniques, such as Anthropic's approach to\naugmenting chunks with additional information before embedding,\nhave emerged as promising ways to reduce retrieval errors. Sim-\nilarly, Hypothetical Document Embeddings (HyDE) [7] involve\ngenerating synthetic text based on the query and embedding it\nalongside real documents. These methods aim to capture query-\nspecific nuances, resulting in more robust retrieval in open-domain\nand QA contexts. Our work builds on these approaches by leverag-\ning question-based chunk representations for improved relevance."}, {"title": "2.6 Supporting Asymmetric QA Tasks", "content": "In many QA scenarios, particularly in customer support and enter-\nprise search, there exists a fundamental asymmetry: user queries are\noften brief, while answers require detailed, structured information.\nRAG systems addressing this imbalance have incorporated tech-\nniques such as hierarchical retrieval [20], multi-hop reasoning [22],\nand weighted retrieval pipelines [15] to bridge this gap. Recent\nefforts in this domain include query-expansion strategies [34] and\nretrieval conditioning [39] to better align user intent with document\ngranularity."}, {"title": "2.7 Neural Information Retrieval", "content": "Neural information retrieval methods aim to model complex seman-\ntic relationships and contextual relevance more effectively than\ntraditional approaches. While approaches like ColBERT [16] and\nDPR [26] have made significant strides in dense retrieval, they\ncontinue to struggle with nuanced information seeking behav-\niors, involving hierarchical relationships, managing distributed\ninformation across multiple documents, and dealing with context-\ndependent relevance ranking."}, {"title": "2.8 End-to-End RAG Systems", "content": "Fully integrated, end-to-end RAG systems (e.g., from companies\nlike Vectorize.io) are becoming increasingly popular for tasks re-\nquiring seamless interaction between retrieval and generation. Re-\ncent work [28] has focused on optimizing these systems for effi-\nciency, scalability, and robustness. End-to-end designs often inte-\ngrate prompt caching, hybrid retrieval, and adaptive reranking to\nachieve state-of-the-art performance across diverse NLP tasks."}, {"title": "3 QUOTE", "content": "QUOTE can be viewed in the lineage of query reformulation [32]\nand multi-hop reasoning [19], but takes a unique perspective by\nfocusing on question generation as a fundamental indexing strategy.\nAs discussed earlier, the naive RAG approach can fail to capture\nthe intent behind user queries, especially when queries are succinct\n(e.g., entity lookups) or require extracting specific details from a\nchunk. In QuOTE we transform each chunk of text into multiple\n(question + chunk) representations capturing a range of opportuni-\nties for retrieval. Note that each generated question (plus chunk) is\nthen stored as a separate \"document\" or embedding in the vector\ndatabase.\nSee Algorithm 1 for pseudocode to illustrate how QuOTE builds\nan index, and Algorithm 2 for how it is queried. We next describe\nkey stages of the pipeline (see Fig. 1):"}, {"title": "3.1 Question Generation at Pre-Query Time", "content": "We split the corpus into smaller passages (or chunks). For each\nchunk, we prompt an LLM to generate a set of questions that the\nchunk can answer. While question generation is a well studied topic\nin NLP [9, 10, 41]. The quality and diversity of generated questions\nplay a significant role in QuOTE's effectiveness. We use an LLM\nwith prompt engineering (see Section 5.1) to create a representative\nset of questions with specificity and coverage. By creating mul-\ntiple question-based embeddings for each chunk, QuOTE better\ncaptures diverse user queries that reference the same text in dif-\nferent ways. If a user's query is similar (semantically) to one of\nthe chunk-generated questions, that chunk becomes more likely to\nrank highly, leading to more accurate retrieval."}, {"title": "3.2 Embedding", "content": "Instead of storing just the original chunk embedding, we store each\ngenerated question (along with the original chunk) in the vector\ndatabase. In Section 5.2 we demonstrate that the performance of\nQUOTE is agnostic to the choice of embedding model."}, {"title": "3.3 Retrieval and Deduplication at Query Time", "content": "During query time, multiple retrieved \"documents\" often reference\nthe same underlying chunk. Hence, a deduplication step is necessary\nto ensure we select the top-k distinct chunks, avoiding wasted slots.\nTo this purpose we 'over-retrieve' top-k \u00d7 M results (for some\nvalue of M) from the question-based embeddings. (Note that this\nde-duplication step is unique to the QuOTE pipeline and is not a\nfeature in classical RAG pipelines.)"}, {"title": "4 DATASETS AND METRICS", "content": "While there exist a variety of datasets for RAG evaluation (see Ta-\nble 1) not all are geared toward evaluating retrieval performance as\ndistinct from generation, which is our focus here. For instance, QA\ndatasets where we are evaluated against the quality of the generated\nanswer, or where the original ground truth chunks are not available,\ndo not support assessing the performance of QuOTE in helping im-\nprove retrieval of relevant chunks. Accordingly, we focus on three\nbenchmark datasets commonly used for question answering: Natu-\nral Questions (NQ) [17], SQUAD [23, 24], and MultiHop-RAG [33].\nThese datasets vary in complexity, domain coverage, and the style\nof questions, providing a broad platform to test the retrieval capa-\nbilities of our approach."}, {"title": "4.1 Natural Questions (NQ)", "content": "The Natural Questions (NQ) dataset [17] is a large-scale benchmark,\nwith questions directly sourced from real user queries and answers\nkeyed to Wikipedia articles. The dataset is split into approximately\n307k training examples and roughly 7.8k each in the development\nand test sets. For each query, the dataset provides the relevant\npassages (long answer) and the precise phrases or entities (short\nanswer) where the answer resides.\nOne non-trivial issue pertains to multiple, highly similar passages\nin the same article. For example, consider passages about the song\n\"\"Heroes\"\" by David Bowie from the Wikipedia article titled \"Heroes\n(David Bowie song)\". This article has two passages that are nearly\nidentical, differing only in minor wording (e.g., \"in the UK\u201d vs. \u201cin\nthe United Kingdom\u201d). These slight variations do not change the\nfactual content but result in multiple, nearly duplicate contexts.\nSuch minor differences unnecessarily fragment the dataset into\nmultiple contexts, each labeled as distinct. This discrepancy com-\nplicates retrieval-based evaluations because systems are penalized\nif they return an almost-correct chunk that differs by only a few\nwords from the one labeled as ground-truth.\nTo address this issue, we merge highly similar chunks based on\na text-similarity threshold, combining their respective questions\ninto a single context group. This merging strategy reduces noise\nand ensures that semantically equivalent passages (or chunks) are\ntreated as one, allowing retrieval mechanisms to focus on true\ndistinctions in content rather than trivial rephrasings."}, {"title": "4.2 SQUAD", "content": "The Stanford Question Answering Dataset (SQUAD) [23] is widely\nrecognized as a benchmark for reading comprehension and extrac-\ntive QA. Each question is associated with an exact answer span\nin the corresponding article, ideal for our extractive evaluation\npurposes."}, {"title": "4.3 MultiHop-RAG", "content": "MultiHop-RAG [33] is specifically designed to test multi-hop ques-\ntion answering. Unlike SQUAD and NQ, which pair each question\nwith a single relevant paragraph or article, MultiHop-RAG asso-\nciates multiple ground-truth documents with each query. For in-\nstance, a query such as: \"Which company is being scrutinized by\nmultiple news outlets for anticompetitive practices and is also sus-\npected of foul play by individuals in other reports?\" will require\ncross-referencing two or more articles to gather the necessary evi-\ndence."}, {"title": "4.4\nEvaluation Metrics", "content": "For Natural Questions (NQ) and SQuAD, each query typically has\na single correct Wikipedia article and a specific paragraph in that\narticle as ground truth. We use the following metrics aimed at\ncapturing whether QuOTE can precisely isolate the article along\nwith the correct answer span.\n\u2022 Context Accuracy (C@k): The fraction of queries for\nwhich the correct paragraph-level context is retrieved within\nthe top-k results. If a system retrieves the exact paragraph\ncontaining the short answer at any rank \u2264 k, we consider\nit a successful retrieval.\n\u2022 Title Accuracy (T@k): The fraction of queries for which\nthe correct article-level title is found among the top-k re-\nsults. This is a coarser (i.e., easier) measure compared to\nparagraph-level context accuracy but still offers insight into\nwhether the system can identify the right document (for\ninstance, the correct Wikipedia page).\nMultiHop-RAG queries can reference multiple relevant documents.\nConsequently, we employ:\n\u2022 Full Match Accuracy (Full@k): All evidence pieces re-\nquired by the query must be found within the top-k re-\ntrieved results. If even one piece of critical evidence is miss-\ning, the query is marked as a failure under this measure.\n\u2022 Partial Match Accuracy (Part@k): Because missing one\nor more documents can still lead to a partially correct an-\nswer, we measure the percentage of required evidence found\nin the top-k results. This measure highlights how retrieval\nerrors degrade performance. For instance, a system might\nretrieve 2 of the 3 needed documents (66.7% partial match),\nwhich can be useful for partial downstream reasoning but\nmight not yield the fully correct answer.\nThis two-level evaluation (full vs. partial) captures the difficulty\nof multi-hop retrieval where multiple documents must be combined\nto arrive at a final answer."}, {"title": "5 EVALUATION", "content": "We conduct a comprehensive evaluation to answer the below ques-\ntions:\n(1) (Section 5.1) Is QuOTE able to automatically generate ques-\ntions that improve the performance of retrieval-augmented\ngeneration?\n(2) (Section 5.2) How sensitive is QUOTE performance to the\nchoice of embedding model?\n(3) (Section 5.3) How many questions must be generated for\nQUOTE to be effective?\n(4) (Section 5.4) How does QUOTE compare to HyDE, the state-\nof-the-art approach to query enrichment?\n(5) (Section 5.5) How negligible or significant is QuOTE's dedu-\nplication overhead?\n(6) (Section 5.6) Because QuoTE uses an LLM for question gen-\neration as well as for answer generation, can we employ a\ncheaper model for question generation and does this signif-\nicantly affect performance?\n(7) (Section 5.7) Can we characterize the properties of contexts\nfor which QuOTE has selective superiority?"}, {"title": "5.1 Effect of Different Prompts to Generate\nQuestions", "content": "A central consideration for QuOTE-style indexing is how the prompt\nitself influences the quality of generated questions. We compare\ntwo main prompt templates:\n\u2022 Basic Prompt: Instructs the model to \"Generate enough\nquestions to properly capture all the important parts of the\ntext\". The questions are short, direct, and do not include\nadvanced reasoning cues.\n\u2022 Complex Prompt: Adds instructions for more detailed or\nmulti-hop reasoning. In MultiHop-RAG, for example, the\ncomplex prompt explicitly requests multi-hop questions\nreferencing multiple pieces of information. In SQUAD or\nNQ, it encourages short factual queries without referencing\nthe text directly, thereby aiming for more robust coverage\nof the chunk's content.\nWe compare the performance of both these prompts with a naive\nRAG implementation. As Table 3 shows, the Complex Prompt\nachieves the highest Top-1 Context Accuracy overall. Title Accu-\nracy metrics remain near-perfect across all methods beyond Top-1,\nindicating that differences among prompts are most pronounced at\nthe paragraph selection level. These observations suggest that more\nadvanced prompting yields modest but meaningful improvements\nin precisely identifying relevant questions for specific passages."}, {"title": "5.2 QUOTE Performance vis-a-vis Embedding\nModel", "content": "Table 4 compares Naive vs. QuOTE modes on three datasets-\nSQUAD (single-hop), NQ (single-hop), and MultiHop-RAG (multi-\nhop) across a range of datasets. We report Top-k context/title\naccuracy for SQUAD and NQ, and full/partial match for Multi-\nHop. Despite large differences in baseline quality (e.g., jinaai vs.\nWhereIsAI vs. Alibaba), note that QuOTE generally improves re-\ntrieval metrics (especially Top-1 Context Accuracy or Full@20) re-\ngardless of the underlying embedding model. QuOTE often raises Top-\n1 Context Accuracy by 5-17 points on SQUAD and 1-3 points on\nNQ, and can improve Full@20 by up to several points in MultiHop-\nRAG. MultiHop-RAG remains challenging, as even large gains may\nyield relatively modest absolute numbers (e.g., 9% or 10% full match\nat k = 5). However, QuOTE still outperforms or closely matches a\nnaive approach across all embedding models."}, {"title": "5.3 Effect of Number of Questions", "content": "One key factor in question-oriented retrieval is deciding how many\nquestions an LLM should generate for each chunk of text. Gener-\nating too few may overlook critical details, while generating too\nmany can introduce redundancy or noise. We therefore tested mul-\ntiple settings across our three datasets (Natural Questions, SQUAD,\nand MultiHop-RAG), varying the number of questions (1, 5, 10, 15,\n20, 30) and also including an 'LLM decides' setting. In each case, we\nmeasure how Context Accuracy and Title Accuracy changes, or\nin the case of MultiHop-RAG, how Full Match and Partial Match\nscores are affected.\nTo systematically investigate the effect of varying the number\nof generated questions, we parameterize our LLM prompt to either\ngenerate:\n\u2022 Fixed # Questions: If a desired quantity num_questions is\nprovided, the prompt includes a directive such as:\n\"Generate exactly {num_questions} questions\nto properly capture all the important parts\nof the text.\"\n\u2022 An LLM Decides # Questions: Here, the LLM is simply\ninstructed to:\n\"Generate enough questions to properly capture\nall the important parts of the text.\"\nSQUAD. Table 5 shows that as the number of generated questions\nper chunk increases from 5 to around 10 or 20, Top-1 Context Ac-\ncuracy rises from about 73% to as high as 76%, and Top-5 surpasses\n97% in most settings. Title Accuracy also remains consistently high,\ncrossing 99% even at Top-1 for 10+ questions. Interestingly, letting\nthe LLM decide how many questions to generate (\"LLM Decides\")\nyields a strong Top-1 Context Accuracy of 76.17% and Top-1 Title\nAccuracy of 99.30%.\n\u2022 Naive vs. 10 questions. A naive approach (66.60% Top-1\nContext) significantly lags behind generating 10 questions\n(74.91% Top-1), showing that question augmentation dra-\nmatically helps correct chunk retrieval."}, {"title": "5.4 Comparison with HyDE", "content": "A popular technique for query enrichment is HyDE, which gen-\nerates a hypothetical document at query time before embedding\nit and retrieving relevant chunks. Although HyDE can improve\ncoverage, it requires an LLM call for each incoming query, intro-\nducing significant latency. In contrast, QuOTE moves question\ngeneration to index time, incurring a one-time cost but speeding up\nthe overall querying process. We compare Naive RAG (no query\ntransformations), HyDE, and QuOTE on all three benchmarks."}, {"title": "5.5 Effect of Deduplication", "content": "Deduplication is essential in QuOTE because each chunk can be\nindexed multiple times-once per generated question-leading to\nredundant matches at query time. Again, We compare Naive RAG,\nHyDE and QuOTE. When k = 1, deduplication is unnecessary,\nas only one chunk is retrieved. However, when k \u2208 {5, 10, 20},\nQUOTE systems fetch more than k results from the vector index\n(e.g., k x 5) and then deduplicate by original chunk text. This extra\nstep introduces a small overhead, but we find that QuOTE remains\nmuch faster than HyDE (which invokes an LLM at each query) and\nsubstantially outperforms Naive in Top-1 Context Accuracy.\nTable 7 shows a head-to-head comparison of the three approaches\non a SQUAD subset. Each approach processes 923 queries for all\nbenchmarks, we see that Naive is the fastest and QuOTE the most\naccurate. The added overhead incurred by QuOTE over Naive\nsmall relative to the cost of per-query generation in HyDE. Hence,\nQuOTE obtains both superior accuracy and faster query times than\nHyDE, while incurring a one-time cost for indexing. In settings\nwhere repeated queries are common, paying a higher index-time\ncost can significantly improve responsiveness and end-user experi-\n      ence."}, {"title": "5.6 Can we use a Cheaper LLM for Question\nGeneration?", "content": "An important practical consideration in RAG-based pipelines is\nwhether cheaper, smaller models can generate effective questions\nfor indexing, or if premium, large-scale LLMs (e.g., GPT-4) are\nnecessary. To investigate, we experimented with a variety of local\nlanguage models (e.g., gemma2-9b, 11ama3-8b, and qwen2.5-7b),\nas well as gpt-40-mini, gpt-40, and a baseline Naive approach\nthat relies solely on the chunk text without question generation. All\nruns were conducted on a SQUAD-based subset. Table 8 summarizes\nthe results in terms of Top-k Context Accuracy and Top-k Title\nAccuracy.\nWe observe that even smaller models such as llama3.2-3b\nachieve over 70% Top-1 Context Accuracy-only a few percent-\nage points behind the more capable gpt-40-mini or gpt-40 mod-\nels. For nearly all models, Top-1 Title Accuracy remains around or\nabove 98%, indicating that the question generation step-regardless"}, {"title": "5.7 Effect of the Number of Contexts on\nRetrieval Accuracy", "content": "Analysis of the impact of the number of contexts on retrieval perfor-\nmance reveals distinct patterns between SQUAD and NQ, reflecting\ntheir fundamentally different dataset characteristics.\nSQUAD exhibits a rich context structure, with 442 titles having\na mean of 42.74 contexts per title (median=36). This substantial\ndensity, ranging from 5 to 149 contexts per title, creates significant\npotential for confusion with naive retrieval approaches, particularly\nwhen similar passages exist within the same document.\nIn stark contrast, NQ presents a much sparser context landscape.\nAcross its 48,525 titles, NQ maintains a mean of just 1.52 contexts"}]}