{"title": "Features that Make a Difference:\nLeveraging Gradients for Improved Dictionary Learning", "authors": ["Jeffrey Olmo", "Jared Wilson", "Max Forsey", "Bryce Hepner", "Thomas Vin Howe", "David Wingate"], "abstract": "Sparse Autoencoders (SAEs) are a promising\napproach for extracting neural network repre-\nsentations by learning a sparse and overcom-\nplete decomposition of the network's internal\nactivations. However, SAEs are traditionally\ntrained considering only activation values and\nnot the effect those activations have on down-\nstream computations. This limits the infor-\nmation available to learn features, and biases\nthe autoencoder towards neglecting features\nwhich are represented with small activation val-\nues but strongly influence model outputs. \u03a4\u03bf\naddress this, we introduce Gradient SAEs (g-\nSAEs), which modify the k-sparse autoencoder\narchitecture by augmenting the TopK activation\nfunction to rely on the gradients of the input\nactivation when selecting the k elements. For\na given sparsity level, g-SAEs produce recon-\nstructions that are more faithful to original net-\nwork performance when propagated through\nthe network. Additionally, we find evidence\nthat g-SAEs learn latents that are on average\nmore effective at steering models in arbitrary\ncontexts. By considering the downstream ef-\nfects of activations, our approach leverages the\ndual nature of neural network features as both\nrepresentations, retrospectively, and actions,\nprospectively. While previous methods have\napproached the problem of feature discovery\nprimarily focused on the former aspect, g-SAEs\nrepresent a step towards accounting for the lat-\nter as well.", "sections": [{"title": "1 Introduction", "content": "Sparse Autoencoders (SAEs) have emerged as a\npromising method for interpreting neural networks,\naiming to recover a model's features via dictio-\nnary learning (Bricken et al., 2023; Cunningham\net al., 2023; Templeton et al., 2024). While there is\nno universally accepted definition of features, they\nare generally understood to be the atomic units\nof language models' computation, possessing the\nquality of monosemanticity as both representations\nand causal intermediates. The success of SAEs in\nidentifying directions in language model activation\nspace that are causally relevant and interpretable\nprovides evidence that a significant fraction of the\ninternal activations of language models are sparse,\nlinear combinations of vectors which are each ori-\nented in a direction corresponding to a feature of\nthe model. (Park et al., 2024b,a). The elements of\nthe SAE-learned dictionary corresponding to these\ndirections we call latents.\nHowever, SAEs are trained to encode input acti-\nvations such that the reconstruction loss over many\ntokens from a training corpus is minimized. This\nhas raised some concerns that SAEs may not learn\nlatents corresponding to the features of a model,\nbut rather the features of a dataset (Braun et al.,\n2024; Dooms and Wilhelm, 2024). That is, SAEs\nmight give the illusion of interpretability by learn-\ning latents based on frequent and distinct concepts\nin the training corpus, rather than learning latents\nbecause they correspond to features that play an\nimportant and distinct role in the model's decision-\nmaking process. We would like a better guarantee\nthat our interpretability tools discover the features\nof a model, rather than primarily reflecting the train-\ning data.\nAdditionally, if one is interested in interpretabil-\nity as a means to exert more fine-grained control\nover the behavior of models, it would be desirable\nto have interpretability tools that are biased towards\nuncovering the features that are most responsible\nfor a model's output. Templeton et al. (2024) spec-\nulate that current Large Language Models (LLMs)\nmay represent orders of magnitude more features\nthan the size of the dictionaries of the largest SAEs\ntrained to date. The limited capacity of SAEs in-\ncentivizes them to neglect features that strongly\naffect model outputs if overlooking them usually\nresults in small reconstruction errors (Braun et al.,\n2024).\nThese desiderata motivate our approach, which"}, {"title": "2 Motivation and Background", "content": "aims to address the challenges above by connect-\ning dictionary learning more closely with model\noutputs. In this paper we show:\n1.  Distance in LLM activation space corre-\nlates poorly with degree of downstream ef-\nfect, but gradients can yield accurate first\norder predictions. We show that there is\nonly a weak correlation between the norm of\na perturbation vector and it's effect on LLM\ndownstream outputs, as measured by change\nin cross-entropy loss. In contrast to the norm\nalone, the first order approximation given by\ngradients of activations with respect to loss\nstrongly predict downstream effects for per-\nturbations of moderate size. This suggests\nthat reconstruction loss minimization doesn't\ndirectly incentivize the SAE to learn recon-\nstructions that minimize difference in effect.\n2.  In SAE training, incorporating gradients\nproduces improvements on various Pareto\nfrontiers defined by popular SAE architec-\ntures, and results in more complete use of\nthe SAE's capacity. For a given expansion\nsize, using a gradient-aware TopK activation\nfunction leads to improvements in the min-\nimization of difference in model loss when\nreconstructed outputs are propagated down-\nstream. It also leads to fewer permanently\ninactive units of the SAE hidden layer.\n3.  g-SAE latents provide superior steering\nwith no cost to interpretability. g-SAEs re-\ncover latents that appear to be as interpretable\nas existing architectures, and on average are\nmore influential to the specific logits they are\nassociated with, when used as steering vectors\nin arbitrary contexts."}, {"title": "2.1 SAE Preliminaries", "content": "The core of an SAE is the following: an encoder\nmatrix which projects the input activation into a\nhigher dimensional latent space, where the ele-\nments represent a dictionary of learned features,\nand a decoder which reconstructs the input from\nthis sparse vector. Formally, for a model's activa-\ntion vector $x \\in \\mathbb{R}^d$,\n$y = \\text{ReLU}(W_{enc} (x \u2013 b_{dec}) + b_{enc})$\n$x = W_{dec}y + b_{dec}$\nwhere $W_{enc}x \\in \\mathbb{R}^h$, $h$ is the size of the dictionary,\nand $W_{enc} \\in \\mathbb{R}^{h \\times d}$. We will denote the sparse latent\nvector $W_{enc}x$ as $y$ and the reconstructed activation\nas $x$. The elements of $y$ are the SAE's latents.\nGenerally, SAEs are trained with a loss function\ndefined by the minimization of the reconstruction\nerror $||x - \\hat{x}||_3$ and some mechanism for promoting\nsparsity. To this end, Bricken et al. (2023) use a\nterm in the loss function to minimize the $L_1$ norm\nof $y$. This approach comes with some downsides,\nincluding the phenomenon of shrinkage, that is, the\nunintended optimization of the elements of $y$ to\nbe minimized in addition to promoting the sparsity\nof $y$. Various alternative architectures have been\nproposed, which we detail briefly below."}, {"title": "2.2 Relevant Work", "content": "Gao et al. (2024) introduced the TopK autoen-\ncoders architecture which use a TopK activation\nfunction to serve the purpose of direct sparsity en-\nforcement. This activation function zeroes out all\nbut the $k$ most active elements of the pre-activation\n$z$. Various other architectures have been proposed\nin addition to the TopK architecture to improve"}, {"title": "2.3 LLM outputs are especially sensitive in\nthe direction of their features", "content": "The linear representation hypothesis (Park et al.,\n2024b,a; Elhage et al., 2022) posits that deep learn-\ning models represent features as linear directions in\nactivation space. If this is the case, and here we will\nassume it is, then it also follows that the difference\nbetween two real activation vectors corresponds to\na linear combination of feature directions. Previ-\nous work has analyzed the sensitivity of GPT-2's"}, {"title": "2.4 First order approximations are a strong\npredictor of effect", "content": "Standard SAEs use activation values to learn their\ndecompositions of model activations. Are the mag-\nnitudes of activation values a reliable guide to in-\nfluence on outputs?\nWe find that in all MLP output layers of the\nLLMs we tested, there is generally a weak corre-\nlation between the norm of an isotropic random\npermutation to an MLP output, and the resulting\nchange to prediction cross-entropy loss (Fig. 3). In\nthe context of SAEs, this low correlation means that"}, {"title": "3 g-SAEs", "content": "A g-SAE is a standard SAE but with the following\nactivation function:\n$\\sigma(z) = \\begin{cases} z_i & \\text{if } i \\in K \\\\ 0 & \\text{otherwise} \\end{cases}$\n$K = \\text{TopK} \\big(z + \\beta z_0 |W_{dec}^T \\nabla_x L(x)| \\big)$\nwhere $L(x)$ is the function mapping a residual\nstream activation to its associated predictive cross\nentropy loss, $W_d$ is the decoder matrix, and $\u03b2$ is a\ntunable hyperparameter. This activation function\nselects $k$ latents $z_i$ based on two criteria: 1) the\nvalue of $z_i$, and 2) $z_i$ times the $i^{th}$ component of\n$|W^T \\nabla_x L(x)|$, which is the linear transformation\nof the gradient at $x$ into the SAE latent space. The\nlatter term can be thought of as the attribution of\n$z$ with respect to the model loss; it resembles the\nattribution vector derived by attribution patching\n(Nanda, 2024). The term $z |W^T \\nabla_x L(x)|$ rep-\nresents the local linear approximation of the effect\nthat turning latent $z_i$ off has on the loss yielded by\n$x$.\nThese dual criteria place productive constraints\non both the encoder and decoder: when $\u03b2$ is high,\nthe decoder is forced to work with only latents\ncorresponding to directions in the input space with\nlarge effects as estimated by the local linear approx-\nimation, and therefore learn to reconstruct $x$ by\nleveraging these directions. Likewise, with high $\u03b2$,\nthe encoder is only updated along rows correspond-\ning to high-influence latents, (because in backprop-\nagation, no gradients flow through the zeroed-out\nlatents) and consequently, it must learn a decompo-\nsition of $x$ along directions with high attributions\nwith respect to the model's loss. Thus, while the\nSAE loss remains reconstruction error, the SAE is\nbiased towards learning latents oriented towards\nhigh-influence directions. A major advantage of"}, {"title": "4 Results", "content": "4.1 Capacity\ng-SAEs perform comparably on the NMSE-\nsparsity frontier, and add slightly less loss for\na given sparsity, particularly at denser sparsities.\nWhen sparsity is held constant and expansion size\nvaries, g-SAEs display a steeper curve than TopK,\nindicating increased efficiency in utilizing addi-\ntional capacity. This suggests that g-SAEs lever-\nage additional latents more productively than pure\nTopK SAEs.\nRelated to this is the fact that g-SAEs have fewer\ninactive, or \"dead\u201d latents than other architectures,\nfor a given sparsity. Because of the vacillatory na-\nture of the gradients of hidden layer activations,\nthe TopK activation function introduces a natural\nkind of stochasticity which allows for previously\nunused latents to receive updates; this may be a\nmechanism behind the fewer dead latents associ-\nated with g-SAEs.\nWe find that when $\u03b2$ is too large, performance\ndegrades in terms of NMSE and loss added, sug-\ngesting that both terms in the activation function $\u03c3$\nare necessary."}, {"title": "4.2 Steering", "content": "A core motivation behind g-SAEs is the desire to\nlearn latents which have an interpretable and strong\ncausal connection with the model's actions. Such\na connection would be useful in providing more\ncontrol over model behavior through the use of la-\ntent directions as steering vectors (Li et al., 2024;\nTurner et al., 2024; Marks and Tegmark, 2024; Pan-\nickssery et al., 2024; Conmy and Nanda, 2024;\nTempleton et al., 2024). Additionally, evidence of\na causal relationship between latent activations and\nmodel outputs would suggest that latents are more\nclosely tracking model features, if we assume that\nfeatures have a dual nature as both representations\nand actions.\nTo investigate this, for a given alive latent $y_i$, we\ncollected the set $L$ of the $n$ logits that $y_i$ is most\nassociated with: $L$ is computed by projecting $y_i$'s\ndirection into the vocabulary space via the unem-\nbedding matrix $W_u$, yielding a logit vector $W_j y_i$,\nand then selecting the $n$ logits corresponding to the\nlargest values in $W_j^T W_u y_i$, which represent the vocab-"}, {"title": "4.3 Activity Density Analysis", "content": "Intuitively, latents shouldn't be too specific, nor\ntoo general. A sufficiently large SAE that learns\na feature for every activation could achieve a very\nlow reconstruction loss, however, the latents would\nbe uninformative and likely not reflect the model's\nrepresentational structure. On the other hand, la-\ntents that activate for many (possibly unrelated)\npatterns of tokens are likewise uninformative. We\nanalyze the activation density of a sample of latents\nfrom each type of SAE, which is a measure of how\ngeneral or specific the latents of an SAE are. Hold-\ning the sparsity of the SAEs constant at $L_o$ = 32,\nthe cumulative distribution functions of latent ac-\ntivity (Fig.7) show that g-SAEs tend to have fewer\nmoderately high frequency latents which activate\non more than 1% percent of tokens, but more very\nhigh frequency latents which activate on more than\n10% of tokens. We also see that the median latent\nof g-SAEs is more specific than than the others,\nwith a median activation frequency of 0.24%, com-\npared to 0.43% and 1.10% for topK and e2e SAEs,\nrespectively."}, {"title": "4.4 Feature Splitting", "content": "Bricken et al. (2023) and Cunningham et al. (2023)\nidentified the phenomenon of feature splitting, in\nwhich an SAE learns several latents which are as-\nsociated with inputs that have little to no apparent\nsemantic difference between them. Braun et al.\n(2024) suggest that the cosine similarities between\neach SAE dictionary latent and next-closest latent\nin the same dictionary is a reasonable proxy for\nfeature splitting, since latents which split features\ntend to be oriented in highly similar directions.\nIn Fig. 7 we see that the decoder column simi-\nlarity histogram of g-SAEs has a higher mean than\nother SAE types. To investigate this phenomenon\nfurther, we take a sample of latents from the left,\nmiddle, and right tails of the top decoder similar-\nity histograms and calculate the average directional\nderivative in the direction of these decoder columns\nwith respect to $L(x)$ over many activations $x$. The\nresults in Table 2 suggest that in g-SAEs (as well\nas e2e SAEs), feature directions are more tightly\nclustered in high-influence directions. This might\nindicate that the SAE is allocating more latents\ntowards these directions, such that the density of\nlatents in these high influence directions is higher.\nWhether this phenomenon leads to more problem-\natic instances of feature splitting in g-SAEs is un-\nclear, and could be the subject of further research."}, {"title": "4.5 Manual Interpretability", "content": "To ensure that the increased functional influence\nof g-SAE latents don't come at the cost of their\nmonosemanticity as representations, we follow a\ndouble blind experimental setup similar to that of\nRajamanoharan et al. (2024a) and Rajamanoharan\net al. (2024b) to measure the interpretability of\ng-SAE latents according to human raters. We se-\nlected three SAE architectures (TopK, E2E TopK,"}, {"title": "5 Conclusion", "content": "Our experiments show that introducing gradient\ninformation into the activation function of SAEs\nproduces improvements in output explained at a\ngiven expansion size and sparsity. Additionally, the\nresulting SAE latents appear to have an increased\nability to steer model outputs in a concentrated\nmanner in arbitrary contexts by consistently in-\ncreasing the probability of the associated set of\ntokens. We believe this improvement is significant\nboth for practical applications aiming to exert more\nfine-grained control over model outputs, and for\ninterpretability applications that aim to uncover\nthe primary units of models\u2019 representations and\nactions. We find that this improvement comes at\nno measurable cost to semantic interpretability to\nhuman reviewers.\nWe believe that these developments will lead\nto dictionary learning that more fully accounts for\nboth the representational and causal aspects of mod-\nels\u2019 features, ultimately contributing to a better un-\nderstanding of the fundamental workings of LLMs."}, {"title": "6 Limitations", "content": "Here we discuss limitations of g-SAEs, along with\na discussion of the ethical considerations.\n\u2022 Activation Plateaus: There is evidence that\nmodels may be robust to small perturbations\naround activations, as a consequence of emer-\ngent robustness against small noise (Giglemi-\nani et al., 2024). Despite our evidence that\ngradients in feature directions are higher than\nthose of baseline directions, it may be that\nsuch regions of noise-robustness are locally\nflat enough that the local gradient doesn\u2019t pro-\nvide information about the sensitivity of the\nmodel to larger perturbations in the vicinity.\nIf this is the case, we would plausibly want\nto add a term to the activation function that\nprivileges directions with a large higher or-\nder derivatives, but it isn\u2019t clear how to do so\nefficiently.\n\u2022 Computational Complexity: One of the po-\ntential drawbacks of g-SAEs is their theoret-\nical computational cost, as both training and\nrunning inference on them requires a partial\nforward pass and backpropagation.\nIn practice, we found that this was not a\ncomputational bottleneck and there was lit-\ntle change in cost compared to TopK SAEs\nwhen training on GPT-2 small.\n\u2022 Increasing Scale: While our results are\npromising for GPT-2 small, further work\nneeds to be done to confirm that the trends\nwe observe hold for larger LLMs and larger\nSAEs.\n\u2022 Evaluations: There is a significant need\nwithin the community for more thorough eval-\nuations that truly test the quality of SAEs and"}, {"title": "6.1 Ethics", "content": "The ethical considerations of this paper reflect the\nbroader ethical considerations around LLMs. We\nuse publicly available datasets and models which\ncan represent potentially problematic biases. How-\never, our work does not contribute in any specific\nway to the propagation of potentially negative ideas\npresent in the data.\nA potential negative use case of g-SAEs, as with\nother sparse autoencoders, is to steer advanced\nmodels towards producing harmful outputs. How-\never, we believe that while progress in interpretabil-\nity likely inherently comes with the possibility of\nmore fine grained control over AI systems for both\ngood and ill, the upside to a deeper understanding\nof deep learning systems is currently net positive."}, {"title": "7 Acknowledgements", "content": "We thank Thomas Dooms and Daniel Wilhelm for\ntheir smol-sae library which we modified and used\nfor the training of our SAEs. We also thank Callum\nMcDougal for the sae_vis library which was used\nin our manual interpretability experiments."}]}