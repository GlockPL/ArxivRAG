{"title": "UNIFIED DEEP LEARNING MODEL FOR GLOBAL PREDICTION OF ABOVEGROUND BIOMASS, CANOPY HEIGHT AND COVER FROM HIGH-RESOLUTION, MULTI-SENSOR SATELLITE IMAGERY", "authors": ["Manuel Weber", "Carly Beneke", "Clyde Wheeler"], "abstract": "Regular measurement of carbon stock in the world's forests is critical for carbon accounting and reporting under national and international climate initiatives, and for scientific research, but has been largely limited in scalability and temporal resolution due to a lack of ground based assessments. Increasing efforts have been made to address these challenges by incorporating remotely sensed data. We present a new methodology which uses multi-sensor, multi-spectral imagery at a resolution of 10 meters and a deep learning based model which unifies the prediction of above ground biomass density (AGBD), canopy height (CH), canopy cover (CC) as well as uncertainty estimations for all three quantities. The model is trained on millions of globally sampled GEDI-L2/L4 measurements. We validate the capability of our model by deploying it over the entire globe for the year 2023 as well as annually from 2016 to 2023 over selected areas. The model achieves a mean absolute error for AGBD (CH, CC) of 26.1 Mg/ha (3.7 m, 9.9 %) and a root mean squared error of 50.6 Mg/ha (5.4 m, 15.8%) on a globally sampled test dataset, demonstrating a significant improvement over previously published results. We also report the model performance against independently collected ground measurements published in the literature, which show a high degree of correlation across varying conditions. We further show that our pre-trained model facilitates seamless transferability to other GEDI variables due to its multi-head architecture.", "sections": [{"title": "1 Introduction", "content": "It is estimated that the worlds forests store up to 500 petagram (Pg) of carbon in their above ground biomass (FAO [2020]) and act as a major carbon sink which is vital for maintaining stable ecosystems. According to the IPCC Sixth Assessment Report (IPCC [2023]), deforestation and forest degradation contribute roughly 8-10 % of global greenhouse gas emissions, accelerating climate change. Regular assessment of global above ground carbon stock is necessary to understand and mitigate climate impacts as well as for accounting for corporate emissions disclosures, regulatory compliance, and is part of international climate initiatives, including the UN Paris Agreement (UNFCCC [2015]). This poses an immense challenge since the only validated and accurate method for both measuring the tree profiles (including canopy height) and calibration of ecosystem specific allometric equations (allowing the conversion of tree profile into biomass) requires labor intense field measurements. More efficient and scalable methods involve air- or space-borne LiDAR instruments which are capable of scanning the tree profiles across large areas (Balestra et al. [2024]). However, there is a trade-off between the density and resolution of such LiDAR maps and the spatial extent. For example, airborne LiDAR surveys can generate high resolution maps, due to the dense flight paths, but are"}, {"title": "2 Previous work", "content": "Over the past two decades increasing focus and effort has been directed to environmental monitoring based on space-borne earth observation missions. Such missions go as far back as the 1970s with the Landsat (Wulder et al. [2012]) constellation which offers an immense archive of medium resolution satellite imagery. In recent years, specialized missions have paved the way for more accurate insights into the global dynamics of environments with higher revisit rates and resolution, including multi-spectral passive sensors (Drusch et al. [2012], Donlon et al. [2012], Veefkind et al. [2012], Irons et al. [2012]), active sensors such as synthetic aperture radar (SAR) (Torres et al. [2012], Markus et al. [2017]) light detection and ranging (LiDAR) (Dubayah et al. [2020]) and missions dedicated to understanding the carbon cycle (Le Toan et al. [2011], Das et al. [2021]). The increasing volume of data collected by all these missions has motivated the development of modern and novel algorithms based on machine- and deep learning (Paheding et al. [2024]). Previous work has focused on the model development for estimating either aboveground biomass density, canopy height or canopy cover. We could not find any references which combine the prediction of multiple variables into a single model. In most previous approaches the limitation in spatial resolution arises from the choice of input data source and ranges from 250 m-1 km (e.g. MODIS) to 30 m (e.g. Landsat), 10 m (e.g. Sentinel-1/2) and ~1 m (e.g. MAXAR).\nAboveground biomass maps are available today, covering up to decades of history, but are often produced at local scale based on ground measurements and forest inventories (Zhang et al. [2019]). Scaling these maps to larger regions requires the collection of plot data which cover various ecosystems. Capturing ground truth data across the broad range of ecosystems and land cover that would be required to scale this methodology would be prohibitively expensive. Early efforts which incorporate simple machine learning techniques have focused on pan-tropical regions and use medium to low resolution satellite imagery (Saatchi et al. [2011], Baccini et al. [2012], Baccini et al. [2016]). At global scale, a number of aboveground biomass maps have been generated at low resolution (~1 km) (Su et al. [2016], Yang et al. [2020]) including the gridded version of the GEDI level-4 product (Dubayah et al. [2022]). Only recently, with the incorporation of modern deep learning techniques, higher resolution maps at scale have emerged (Sialelli et al. [2024], Bereczky et al. [2024]) which combine the use of satellite imagery with global scale LiDAR surveys.\nIn contrast to aboveground biomass, canopy height estimations from satellite imagery is less reliant on regional calibrations as ground measurements can be gathered directly from forest inventories or LiDAR measurements which provide insights into the canopy structure. Early approaches utilize simple pixel-to-pixel machine learning algorithms such as random forest at medium resolution (Li et al. [2020], Potapov et al. [2021]) while more recent methodologies were developed based on deep learning models and higher resolution, single-sensor imagery (Lang et al. [2023]) as well as combining multiple sensors as input data (Pauls et al. [2024]). The advancements in deep learning based computer vision models, which exhibit great skills at depth estimation (Oquab et al. [2024]), have allowed the development of very high resolution canopy height maps (Tolan et al. [2024]) and models which characterize single trees (Li et al. [2023], Mugabowindekwe et al. [2023], Cambrin et al. [2024]). The main drawback of very high resolution maps at global scale is the immense computational effort and cost for a single deployment. In order to cover the entire globe, high resolution imagery is often gathered within a large time window (multiple years) which creates an inconsistency in the temporal resolution and complicates the change monitoring. In order to create consistent and high quality global maps, it is therefore advisable to revert to lower resolution imagery (~10 m) with higher frequency of observations and to merge multiple sources which may compliment each other. Large scale LiDAR surveys provide a more direct way of generating canopy height maps since they do not rely on models which estimate canopy height based on imagery which has limitations in information content. However, high-resolution aerial surveys (National Ecological Observatory Network (NEON) [2021]) are limited in scalability while global scale surveys have low resolution (Dubayah et al. [2021]).\nEstimating canopy cover from satellite imagery is the least complex of the three tasks as it does not rely on detailed three dimensional canopy structure or local calibrations. However, it may still pose many"}, {"title": "3 Data", "content": "In this work, we utilize multi-spectral, multi-source satellite imagery, digital elevation model as well as geographic coordinates as input to the model. The model is trained in a weakly supervised manner (see section 4) on point data from the Global Ecosystem Dynamics Investigation (GEDI) instrument. In this section, we describe the processing steps for generating a global dataset of input and target samples. We will also briefly explain the relevant concepts of the GEDI mission as well as the different data processing levels as it will be an important aspect of understanding the inherent model uncertainties and its limitations."}, {"title": "3.1 Ground truth data", "content": "The GEDI instrument is a space-borne LiDAR experiment mounted on the International Space Station (ISS) and has been operational since 2019. It comprises 3 Nd:YAG lasers, optics and receiver technology allowing to measure the elevation profile along the orbital track of the ISS. Within the lifetime of the experiment, it is expected to collect 10 billion waveforms at a footprint resolution of 25 m. The setup of 3 lasers, one of which is split into two beams, as well as dithering every second shot leads to a pattern of point measurements with 8 tracks per pass where the tracks are separated by 600 m and each point by 60 m along the flight path. Each GEDI measurement consists of the waveform resulting from the returned signal of the laser pulse sent out at the given location. The collection of all these waveforms is referred to as level-1 data. Each waveform is further processed to extract metrics which characterize the vertical profile of the trees within a given beam footprint.\nThe signal with the longest time of flight corresponds to the ground return and is used as the reference for the relative height (RH) metrics. The RH[X] metrics correspond to the relative height at which [X] percent of the total accumulated energy is returned. These metrics characterize the vertical profile of the GEDI footprint where RH100 corresponds to the largest trees in the footprint. These metrics, together with other parameters related to the measurement conditions, are stored in a dataset referred to as level-2a. In additional steps these metrics are used to calculate the percent canopy coverage (level-2b) as well as a gridded version (level-3). Further processing involving regional calibration of allometric equations, leveraging level-2 data, results in estimations of aboveground biomass density (AGBD) as well as uncertainties referred to level-4a. The estimations are based on models which were fit to on-the-ground biomass measurements in a number of field plots located in various regions around the globe. Since most of these measurements do not intersect with a GEDI footprint, airborne LiDAR was used to measure the return signal which was then translated into a simulated GEDI waveform. In this work we utilize both the level-2a/b and level-4a data as ground truth. The on-the-ground measurements for biomass were mostly done without any tree clearing but by measuring canopy height and diameter and using allometric equations, specific to the tree type and world region, to determine biomass. The simulated waveforms undergo the same processing to extract RH metrics which provide the predictors for linear models to predict AGBD. Various models were developed for all combinations of plant functional type (PFT) and world region defined as prediction stratum. For details about the selected predictors for each model and their performances see section 2 and 3 in (Duncanson et al. [2022]).\nThe models are linear functions of the predictors with a general form of\n$f_j = X_j b_j$ (1)\nwhere $X$ is a $n \\times m$ matrix of n measurements with m predictors and $b_j$ a $m \\times 1$ vector of parameters for prediction stratum j. The best parameters are determined by linear regression where the predicted variable may be in transformed units using a function h which is either unity, square root or log. For new measurements $X'$ the model of the corresponding prediction stratum is chosen and the predictions are given by\n$AGBD = h^{-1}(X'b + \\epsilon)$ (2)"}, {"title": "3.2 Input data", "content": "In order to leverage the respective benefits of different data sources, we fuse optical bands (red, green, blue) from the Sentinel-2 (Copernicus Sentinel-2 (processed by ESA) [2021]) satellite with thermal bands (nir, swirl, swir2) from the same satellite (processed with the SEN2COR algorithm (Main-Knorn et al. [2017]) to provide surface reflectance) and synthetic aperture radar (SAR) signal (VV and VH backscatter) from Sentinel-1 (Potin et al. [2022]). In addition, we use altitude, aspect and slope from the Shuttle Radar Topography Mission (SRTM) (NASA JPL [2013]) to further enrich the predictive capability of our model. The predictors from the digital elevation model (DEM) carry important information about the local topography which affects the distribution of plant functional types and their growth pattern (Wang et al. [2022]). We further provide the global coordinates of each data sample by encoding longitude and latitude in the interval [-1, 1]. The optical bands of Sentinel-2 as well as the Sentinel-1 bands have a native resolution of 10 m while the thermal bands of Sentinel-2 have a resolution of 20 m and the DEM of 30 m. In order for the CNN to process the input layers at different resolutions, we resample all layers to 10 m resolution using the bilinear interpolation method and stack them to form a 13 channel input tensor.\nTo generate a global training and test dataset, we uniformly sample locations within the latitude (longitude) range of [-51.6, 51.6] degrees ([-180, 180] degrees) which intersect with landmass. We use the Descartes Labs (DL) proprietary tiling system to create image tiles of size 512 pixels\u00d7512 pixels at 10 m/pixel resolution with the sampled coordinate being at the center of the tile. Each tile is required to contain >20 GEDI footprints. During training, we dynamically split each tile into 4 non-overlapping sub-tiles of size 256 pixels\u00d7256 pixels, which increases the total number of samples in the dataset four-fold. We introduce a naming convention for the tiles based on their location being either in the southern hemisphere (lat < -23.5\u00b0), the tropics (-23.5\u00b0 < lat > 23.5\u00b0) or the northern hemisphere (lat > 23.5\u00b0)."}, {"title": "3.2.1 Cloud mask and image composite", "content": "In order to reduce obstruction from clouds and cloud shadows in Sentinel-2 imagery, we generate cloud free composites from a stack of images which are masked with the binary output of our proprietary cloud and cloud shadow detection"}, {"title": "3.2.2 Global dataset", "content": "For each sampled tile, we generate Sentinel-1 and Sentinel-2 composites according to the definition in section 3.2.1 by gathering imagery using the Descartes Labs platform for the year 2021 (2021/2022 for the southern hemisphere). The DEM has a fixed acquire year of 2000. For a given tile, we gather all GEDI level-2a/b and level-4a point data which lay within the tile geometry and have a collection date no more than \u00b11 month from the composite time range (this buffer is set to 0 months for the 6 months composites). In order to match the data from GEDI level-2a/b with level-4a, they are required to have the same footprint coordinates to 5 decimal point precision and the same acquire date. Further, we only accept data with quality flag 14_quality_flag equal to 1 and require at least 20 data points to be within the tile geometry. Due to the sparsity of the GEDI data, it is saved as vector data along with each footprints pixel coordinate and rasterized on the fly during training. Despite the footprint being 25 m in diameter, we assign the corresponding target value to only one pixel of size 10 m\u00d710 m at the location of the footprint center.\nIn total 276'766 data samples were created of which 14'745 (5%) are randomly selected and stored as test dataset. Each sample is composed of an average of 50 scenes across Sentinel-1 and Sentinel-2 amounting to a total of 13.8 M scenes processed. The total number of ground truth data points is 67 M (3.8 M) for the training (test) dataset."}, {"title": "4 Methodology", "content": "In this section we describe our approach to building a computer vision (CV) model which fuses selected bands of multiple sensors as well as encoded geographic location to form a 13 channel input tensor and simultaneously predicts a continuous map of above ground biomass density, canopy height, canopy cover and their respective uncertainties at the same resolution as the input imagery."}, {"title": "4.1 Model development", "content": "The GEDI dataset offers measurements of AGBD, CH and CC for recent years but is largely incomplete at higher resolution due to its sparsity. Here we describe the model we developed which uses multiple source as input to predict AGBD, CH and CC as a continuous map at the resolution of the input source while using the GEDI level-2 and level-4 dataset for training. In this work, we use the surface level processed Sentinel-2 with 6 bands (red, green, blue, nir, swirl,"}, {"title": "4.2 Model training", "content": "Training of the model is divided into three parts: First we train the entire network including prediction heads 1-3 on the full dataset of ~1 M samples. The network is optimized to simultaneously predict AGBD, CH and CC using their"}, {"title": "4.2.1 Loss function", "content": "We consider the prediction of each pixel (i, j) in the output map $Y_{i,j}$ as an independent measurement of a normal distributed variable with a standard deviation of $\\sigma_{i,j}$. The probability for a given ground truth value $\\hat{Y}_{i,j}$ is then given by\n$P(Y_{i,j} | Y_{i,j}(\\theta), \\sigma_{i,j}(\\theta)) = \\frac{1}{\\sqrt{2 \\pi \\sigma_{i,j}^{2}(\\theta)}} e^{-\\frac{(Y_{i,j} - \\hat{Y}_{i,j}(\\theta))^{2}}{2 \\sigma_{i,j}^{2}(\\theta)}}$ (8)\nwhere $\\theta$ are the parameters of the network. The likelihood function can be written as\n$L(\\theta) = \\Pi_{i,j} P(\\hat{Y}_{i,j} | Y_{i,j}(\\theta), \\sigma_{i,j}(\\theta)) .$ (9)\nWe use gradient descent to optimize the parameters $\\theta$ which minimize the negative log likelihood (NLL) which defines our loss function\n$\\mathcal{L} = -log(L) = \\sum_{i,j} ( \\frac{(Y_{i,j} - \\hat{Y}_{i,j}(\\theta))^{2}}{2 \\sigma^{2}(\\theta)} + log(\\sigma^{2}(\\theta)) )$ (10)\nwhere we omitted the factor $\\frac{1}{2 \\pi}$ from equation 8. $\\sigma_{i,j}(\\theta)$ is the uncertainty predicted by the model for each variable separately. By definition, we expect 68 % of all samples to have an absolute error between predicted and true value within the range of 1$\\,\\sigma$. During training we verify this by calculating the fraction of z-scores, defined as z = |$\\hat{y}$ \u2013 y|/$\\sigma$, to be <1. Even though the $log(\\sigma^{2})$ term in equation 10 acts as regularization to make sure the model does not learn a trivial solution by predicting a very large $\\sigma$, we noticed that the coverage may still be >0.68. We therefore introduce an addition regularization term in the definition of the loss as\n$\\mathcal{L} = \u2013 log(L) + \\lambda \\sigma^{2}$ (11)\nwhere $\\lambda$ is a hyper parameter determined for each variable separately. For a given sample, the number of hard labels is much smaller than the number of soft labels (on average the ratio of soft to hard labels is ~1/1000) and varies between samples. We therefore introduce a pixel weighting scheme to balance the contribution of hard and soft labels to the loss. In addition to the imbalance between hard and soft labels, we also want to assign relative weighting of hard to soft labels. This is an essential requirement in order to make the student-teacher approach work well. Let us consider the relative weight of a hard label to be $\\lambda_h$ and that of a soft label to be $\\lambda_s$. Then the balanced loss function, taking both the relative weights as well as the number of hard and soft label pixels into account, becomes\n$\\mathcal{L}_b =  \\frac{\\lambda_h n_h + (1-m) \\mathcal{L}}{\\lambda_s n_s m}$ (12)\nwhere $n_h (n_s)$ are the number of hard (soft) label pixels in a given sample and m has the same definition as in equation 6. By default, we choose $\\lambda_h = 1$ and vary $\\lambda_s$ during training according to an exponential decay from 1 to 1e-3 during the initial epochs, then exponentially increase it to le-2 during the remaining epochs. We have considered other schedules such as linear, constant and zero (corresponding to no soft label) which all resulted in worse model performance.\nSo far we have only formulated the loss function in equation 12 considering one variable. However, we train the model for all variables simultaneously for which we construct the final loss as the weighted sum over the variable specific components\n$\\mathcal{L}_{total} = \\alpha_{0} \\mathcal{L}_{AGBD} + \\alpha_{1} \\mathcal{L}_{CH} + \\alpha_{2} \\mathcal{L}_{CC}$ (13)\nwhere the weights $\\alpha_{i}$ allow for balancing the different contributions due to the different target scales of the variables. In this work we set $\\alpha_{0} = \\alpha_{1} = a_{2} = 1$ since we did not observe any improvements using variable specific weighting."}, {"title": "4.2.2 Training details", "content": "For the first stage of pre-training on the full global dataset, we train the model for 40 epochs with a batch size of 72 on a multi-GPU node with 4 A10G GPUs. We reserve one of the GPUs for data pre-processing, such as calculation of the soft labels, while the remaining GPUs perform the model training. We use the Adam optimizer (Kingma and Ba [2017]) with a linearly increased learning rate from le-7 to 1e-4 over a warm up period of 1 epoch after which it is continuously decreased according to a cosine function over the remaining training period. The second stage consists of fine tuning each variable separately on the balanced datasets and applying the sample weighting according to the inverse frequency distribution. In this stage only the prediction heads 1-3 are trained while keeping all other model weights frozen. We use three single GPU nodes to train each head in parallel with a batch size of 32. In the third and last stage, we fine tune the prediction heads 4-6 which are responsible for predicting the variable uncertainty. In all stages, the loss function as defined in equation 13 is minimized. However, in stages one and two the uncertainty estimation is ignored which is equivalent to setting $\\sigma$ = 1 for all pixels."}, {"title": "5 Results", "content": "In this section we discuss the results of evaluating our model both qualitatively and quantitatively on the test dataset as well as independent datasets which focus on one of the three variables which our model predicts. We first show some qualitative results by deploying the model over specific regions covering a diverse range of biomes. We then evaluate the model using the held out global test dataset and determine a number of metrics such as mean error (ME), mean absolute error (MAE), mean absolute percentage error (MAPE), root mean squared error (RMSE) and correlation for each of the predicted variables separately. Finally, we determine our models accuracy against publicly available datasets."}, {"title": "5.1 Qualitative assessment", "content": "The deployment of the model at a global scale for the year 2023 allows for a qualitative assessment of its performance, in particular in challenging areas such as those with high cloud coverage which may affect the quality of the input imagery."}, {"title": "5.2 Quantitative assessment", "content": "We have evaluated the performance of our model based on the test dataset consisting of 14'745 samples and 13.8 M individual GEDI footprints. Model inference is performed on each test sample of size 256 pixels\u00d7256 pixels which generates predictions for every pixel and all output variables including their uncertainties. We measure the correlation, mean error (ME), mean absolute error (MAE), mean absolute percentage error (MAPE) and root mean squared error (RMSE) between the GEDI ground truth data and the model predictions gathered at the pixel coordinates of the GEDI footprints. Due to the non-uniform distribution of ground truth values, we assess the model performance on both the test set with its original as well as a uniform sample distribution. For this purpose, we sample data points according to the inverse PDF of each variables respective distribution. This ensures a more fair performance assessment across the value ranges of each variable. Due to the low frequency of occurrence of high values of AGBD and CH, we set a reference value of 300 Mg/ha for AGBD and 30 m for CH to define the lowest value of probability. Values with lower probabilities are always included during sampling. After the sampling procedure, a total number of 200 k - 450 k data points are remaining, depending on the variable."}, {"title": "6 Applications", "content": "In this section we demonstrate some use cases of our model including the monitoring of changes over time as well as fine-tuning the model on regional ground truth data to better align the global model to local conditions. Traditionally, many use cases require the deployment of multiple models in order to perform a down-stream task where each model output provides the solution for sub-tasks. One of the main motivations of this work is to create a single model which is versatile enough to provide results for all sub-tasks in one forward pass. One such application is the detection of deforestation and association of loss of biomass which we discuss in detail in section 6.2."}, {"title": "6.1 Global model deployment", "content": "Our model is deployable at scale due to its global training dataset. This is an important factor when it comes to monitoring changes in the ecosystems. Our model is scalable both spatially as well as temporally which is important for change detection. While global deployments may not be required very frequently, deployments over local regions of interest can rapidly be performed at temporal resolutions of <1 year.\nWe generated global maps for all three prediction variables AGBD, CH and CC as well as their uncertainties at 10 m resolution for the recent years. They cover the latitude range of 57\u00b0 S to 67\u00b0 N according to the availability of Sentinel-1 data. The deployment year is 2023 for all regions except for the areas where Sentinel-1 experienced an"}, {"title": "6.2 Monitoring biomass change and deforestation detection", "content": "The monitoring of global deforestation and carbon accounting has become an important part of climate change mitigation where remotely sensed data plays an integral part for many models developed in the past. Most these models utilize optical or SAR based imagery and are based on a variety of approaches, from pixel based anomaly detection in time series data (Decuyper et al. [2022]) to modern deep learning and computer vision algorithms (Sol\u00f3rzano et al. [2023]). In this section we demonstrate the usability of our model for the task of deforestation detection and accounting of corresponding loss in biomass. We selected an area in Brazil at location 59\u00b0 26\u2032 46\u2033 W, 7\u00b0 1' 4\" S with a total area of 507 kha. We deploy our model over the area for each year from 2017 to 2023. In this study, we measure changes year over year and compare the results to Global Forest Watch (Hansen et al. [2013]) which provides annual tree cover loss data based on observations from Landsat at 30 m resolution. We follow their approach for defining forested land by requiring CH > 5 m and determine the regions with significant canopy cover loss (ACC > 20 %).  Note that not all regions correspond to complete deforestation but rather significant tree cover loss in accordance to the definition by Global Forest Watch. We aggregate the areas with tree cover loss for each year, with respect to the previous year, and compare our numbers to those reported by Global Forest Watch. Our numbers agree with those by Global Forest Watch to within 10% on average. We determine the total area of tree cover loss over the 6 year time span to be 103.3 kha while Global Forest Watch reports a number of 101.2 kha, a difference of 2.1%. In addition to the total area of tree cover loss, our model also provides the amount of biomass lost in these areas by taking the difference of AGBD maps in 2017 and 2023 and multiplying by the total area. We measure a total loss in biomass of 14.9 Mt, equivalent to 25.66 Mt of CO2. This study"}, {"title": "6.3 Model fine-tuning for local conditions", "content": "One of the main contributors to the uncertainty of AGBD estimations are limited availability of ground measurements for calibrating allometric equations at global scale. On the other hand, estimations of canopy height metrics are not depending on such calibrations as they are derived from the LiDAR waveforms directly. Our model therefore offers a unique capability for fine-tuning on local conditions in order to achieve more accurate AGBD estimations. The majority of allomtric equations use relative height (RH) metrics as predictors for AGBD which our model is able to estimate. The base model predicts AGBD, RH98 and CC. However, due to its multi-head architecture, it can easily be fine-tuned on other variables. We demonstrate such a use case based on the data and coefficients for allometric equations provided by (Rodda et al. [2024]). The authors determine the coefficients of the allometric equation in 14 for each of the 13 sites. They find that the average of all listed RH metrics is the best predictor for AGBD. We expand our model to 7 heads, each representing one of the RH metrics and fine-tune the weights of these heads for one epoch while keeping the weights of the encoder and decoder frozen. We apply sample weighting according to the inverse PDF of each variables distribution (see appendix A for details)."}, {"title": "7 Discussion", "content": "In this work we propose a novel deep learning based model which unifies the prediction of several biophysical indicators which describe the structure and function of vegetation across multiple ecosystems. We utilize both Sentinel-1 backscatter as well as multi-spectral Sentinel-2 data at 10 m resolution. We further enrich the input data by adding topographic information from the SRTM dataset as well as geographic coordinates. We developed an approach which allows the end-to-end training of the model through a weakly supervised learning method on point data representing individual measurements of the respective variables. For this purpose, we construct a loss function which balances the varying number of point data per sample as well as the distribution non-uniformity"}, {"title": "A Uniform data sampling", "content": "All target variables used to train the model have a highly non-uniform distribution of sample values. Without sample weighting, this can lead to over- (under-)fitting in regions with higher (lower) sample frequency. In order to account for this imbalance, we determine variable specific weight functions according to the inverse probability distribution function (PDF) of the sample distributions. The PDFs are determined by kernel density estimations (KDE) of the binned data distributions. The weight function is incorporated into the loss function 12. During training, the weight function needs to be evaluated for each sample in the batch which can be computationally intensive when using a KDE. We therefore save the weight functions as pre-computed lookup tables which can be evaluated with minimal time delay. The same procedure is applied for fine-tuning the model on RH variables (see section 6.3)"}, {"title": "B Model performance details", "content": "This section contains further details on the various studies conducted for assessing the model performance."}, {"title": "B.1 Coverage of uncertainty estimations", "content": "Our model estimates the uncertainty of each variable with additional prediction heads and incorporating the variances in the loss function 11. Whether the model correctly predicts the standard error can be verified by measuring the fraction of samples with a z-score <1 which is expected to be 68 %. During training, we choose the regularization weight in equation 11 such that this fraction reaches 0.68 across the full variable range. However, the fraction may vary across different values of the predicted variable.  It shows that the standard error is slightly over-estimated for lower values, leading to coverage >68%, and tends to be under-estimated at higher values, causing the coverage to be <68%. We will address this discrepancy in future works. However, these plots can be used in order to interpret the models uncertainty estimations at a given variable prediction."}, {"title": "B.2 Effect of target uncertainties on predictions", "content": "In general, when fitting a model to observed data, the inherent uncertainty on each data point can be accounted for by proper construction of the objective function such as the reduced x2 which then allows to determine the uncertainties imposed on each parameter of the model. However, when the model is represented by a deep neural network, the incorporation of ground truth uncertainties is much more complicated as the fitting procedure is done iteratively with mini batches and the number of parameters is far bigger than usual models characterizing a physical process. During training, each target sample is essentially considered to be the true value despite its uncertainty. This can lead to situations where the same input X is associated to two different outputs $Y_1$ and $Y_2$. This can not be modeled by a continuous function which is why the model likely learns the prediction of the average between $Y_1$ and $Y_2$. Therefore, the predictions will also have an uncertainty but it is not trivial to quantify them. The ground truth values for AGBD have an uncertainty attached due to the uncertainties of the predictors in the allometric equations 4. These uncertainty quantifications are provided in the GEDI level-4 dataset as the standard error and 95 % confidence intervals. In order to investigate the effect of these uncertainties on the models prediction of AGBD, we plot the true and predicted values for all samples within a given batch ordered by the true value from small to large. The predictions follow the trend of the true values but fluctuate around the"}, {"title": "B.3 Performance assessment for different PFTS", "content": "The model assessment in section 5.2 was performed across all world regions and plant functional types (PFT). We repeated the analysis where we grouped the GEDI data point according to their PFT classification. This classification is provided within the GEDI dataset. We use the following classes: Deciduous Broadleaf Trees (DBT), Evergreen Broadleaf Trees (EBT), Evergreen Needleleaf Trees (ENT) and Grasslands/Shrublands/Woodlands (GSW)."}, {"title": "B.4 Model fine-tuning for local RH metrics", "content": "Figure 25 shows a comparison of ME, MAE and RMSE metrics between the base- and fine-tuned model. The base model predicts AGBD directly while the fine"}]}