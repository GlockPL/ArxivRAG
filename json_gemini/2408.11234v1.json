{"title": "UNIFIED DEEP LEARNING MODEL FOR GLOBAL PREDICTION OF ABOVEGROUND BIOMASS, CANOPY HEIGHT AND COVER FROM HIGH-RESOLUTION, MULTI-SENSOR SATELLITE IMAGERY", "authors": ["Manuel Weber", "Carly Beneke", "Clyde Wheeler"], "abstract": "Regular measurement of carbon stock in the world's forests is critical for carbon accounting and reporting under national and international climate initiatives, and for scientific research, but has been largely limited in scalability and temporal resolution due to a lack of ground based assessments. Increasing efforts have been made to address these challenges by incorporating remotely sensed data. We present a new methodology which uses multi-sensor, multi-spectral imagery at a resolution of 10 meters and a deep learning based model which unifies the prediction of above ground biomass density (AGBD), canopy height (CH), canopy cover (CC) as well as uncertainty estimations for all three quantities. The model is trained on millions of globally sampled GEDI-L2/L4 measurements. We validate the capability of our model by deploying it over the entire globe for the year 2023 as well as annually from 2016 to 2023 over selected areas. The model achieves a mean absolute error for AGBD (CH, CC) of 26.1 Mg/ha (3.7 m, 9.9 %) and a root mean squared error of 50.6 Mg/ha (5.4 m, 15.8%) on a globally sampled test dataset, demonstrating a significant improvement over previously published results. We also report the model performance against independently collected ground measurements published in the literature, which show a high degree of correlation across varying conditions. We further show that our pre-trained model facilitates seamless transferability to other GEDI variables due to its multi-head architecture.", "sections": [{"title": "1 Introduction", "content": "It is estimated that the worlds forests store up to 500 petagram (Pg) of carbon in their above ground biomass (FAO [2020]) and act as a major carbon sink which is vital for maintaining stable ecosystems. According to the IPCC Sixth Assessment Report (IPCC [2023]), deforestation and forest degradation contribute roughly 8-10 % of global greenhouse gas emissions, accelerating climate change. Regular assessment of global above ground carbon stock is necessary to understand and mitigate climate impacts as well as for accounting for corporate emissions disclosures, regulatory compliance, and is part of international climate initiatives, including the UN Paris Agreement (UNFCCC [2015]). This poses an immense challenge since the only validated and accurate method for both measuring the tree profiles (including canopy height) and calibration of ecosystem specific allometric equations (allowing the conversion of tree profile into biomass) requires labor intense field measurements. More efficient and scalable methods involve air- or space-borne LiDAR instruments which are capable of scanning the tree profiles across large areas (Balestra et al. [2024]). However, there is a trade-off between the density and resolution of such LiDAR maps and the spatial extent. For example, airborne LiDAR surveys can generate high resolution maps, due to the dense flight paths, but are"}, {"title": "2 Previous work", "content": "Over the past two decades increasing focus and effort has been directed to environmental monitoring based on space-borne earth observation missions. Such missions go as far back as the 1970s with the Landsat (Wulder et al. [2012]) constellation which offers an immense archive of medium resolution satellite imagery. In recent years, specialized missions have paved the way for more accurate insights into the global dynamics of environments with higher revisit rates and resolution, including multi-spectral passive sensors (Drusch et al. [2012], Donlon et al. [2012], Veefkind et al. [2012], Irons et al. [2012]), active sensors such as synthetic aperture radar (SAR) (Torres et al. [2012], Markus et al. [2017]) light detection and ranging (LiDAR) (Dubayah et al. [2020]) and missions dedicated to understanding the carbon cycle (Le Toan et al. [2011], Das et al. [2021]). The increasing volume of data collected by all these missions has motivated the development of modern and novel algorithms based on machine- and deep learning (Paheding et al. [2024]). Previous work has focused on the model development for estimating either aboveground biomass density, canopy height or canopy cover. We could not find any references which combine the prediction of multiple variables into a single model. In most previous approaches the limitation in spatial resolution arises from the choice of input data source and ranges from 250 m-1 km (e.g. MODIS) to 30 m (e.g. Landsat), 10 m (e.g. Sentinel-1/2) and ~1 m (e.g. MAXAR).\nAboveground biomass maps are available today, covering up to decades of history, but are often produced at local scale based on ground measurements and forest inventories (Zhang et al. [2019]). Scaling these maps to larger regions requires the collection of plot data which cover various ecosystems. Capturing ground truth data across the broad range of ecosystems and land cover that would be required to scale this methodology would be prohibitively expensive. Early efforts which incorporate simple machine learning techniques have focused on pan-tropical regions and use medium to low resolution satellite imagery (Saatchi et al. [2011], Baccini et al. [2012], Baccini et al. [2016]). At global scale, a number of aboveground biomass maps have been generated at low resolution (~1 km) (Su et al. [2016], Yang et al. [2020]) including the gridded version of the GEDI level-4 product (Dubayah et al. [2022]). Only recently, with the incorporation of modern deep learning techniques, higher resolution maps at scale have emerged (Sialelli et al. [2024], Bereczky et al. [2024]) which combine the use of satellite imagery with global scale LiDAR surveys.\nIn contrast to aboveground biomass, canopy height estimations from satellite imagery is less reliant on regional calibrations as ground measurements can be gathered directly from forest inventories or LiDAR measurements which provide insights into the canopy structure. Early approaches utilize simple pixel-to-pixel machine learning algorithms such as random forest at medium resolution (Li et al. [2020], Potapov et al. [2021]) while more recent methodologies were developed based on deep learning models and higher resolution, single-sensor imagery (Lang et al. [2023]) as well as combining multiple sensors as input data (Pauls et al. [2024]). The advancements in deep learning based computer vision models, which exhibit great skills at depth estimation (Oquab et al. [2024]), have allowed the development of very high resolution canopy height maps (Tolan et al. [2024]) and models which characterize single trees (Li et al. [2023], Mugabowindekwe et al. [2023], Cambrin et al. [2024]). The main drawback of very high resolution maps at global scale is the immense computational effort and cost for a single deployment. In order to cover the entire globe, high resolution imagery is often gathered within a large time window (multiple years) which creates an inconsistency in the temporal resolution and complicates the change monitoring. In order to create consistent and high quality global maps, it is therefore advisable to revert to lower resolution imagery (~10 m) with higher frequency of observations and to merge multiple sources which may compliment each other. Large scale LiDAR surveys provide a more direct way of generating canopy height maps since they do not rely on models which estimate canopy height based on imagery which has limitations in information content. However, high-resolution aerial surveys (National Ecological Observatory Network (NEON) [2021]) are limited in scalability while global scale surveys have low resolution (Dubayah et al. [2021]).\nEstimating canopy cover from satellite imagery is the least complex of the three tasks as it does not rely on detailed three dimensional canopy structure or local calibrations. However, it may still pose many"}, {"title": "3 Data", "content": "In this work, we utilize multi-spectral, multi-source satellite imagery, digital elevation model as well as geographic coordinates as input to the model. The model is trained in a weakly supervised manner (see section 4) on point data from the Global Ecosystem Dynamics Investigation (GEDI) instrument. In this section, we describe the processing steps for generating a global dataset of input and target samples. We will also briefly explain the relevant concepts of the GEDI mission as well as the different data processing levels as it will be an important aspect of understanding the inherent model uncertainties and its limitations."}, {"title": "3.1 Ground truth data", "content": "The GEDI instrument is a space-borne LiDAR experiment mounted on the International Space Station (ISS) and has been operational since 2019. It comprises 3 Nd:YAG lasers, optics and receiver technology allowing to measure the elevation profile along the orbital track of the ISS. Within the lifetime of the experiment, it is expected to collect 10 billion waveforms at a footprint resolution of 25 m. The setup of 3 lasers, one of which is split into two beams, as well as dithering every second shot leads to a pattern of point measurements with 8 tracks per pass where the tracks are separated by 600 m and each point by 60 m along the flight path. Each GEDI measurement consists of the waveform resulting from the returned signal of the laser pulse sent out at the given location. The collection of all these waveforms is referred to as level-1 data. Each waveform is further processed to extract metrics which characterize the vertical profile of the trees within a given beam footprint.\nThe signal with the longest time of flight corresponds to the ground return and is used as the reference for the relative height (RH) metrics. The RH[X] metrics correspond to the relative height at which [X] percent of the total accumulated energy is returned. These metrics characterize the vertical profile of the GEDI footprint where RH100 corresponds to the largest trees in the footprint. These metrics, together with other parameters related to the measurement conditions, are stored in a dataset referred to as level-2a. In additional steps these metrics are used to calculate the percent canopy coverage (level-2b) as well as a gridded version (level-3). Further processing involving regional calibration of allometric equations, leveraging level-2 data, results in estimations of aboveground biomass density (AGBD) as well as uncertainties referred to level-4a. The estimations are based on models which were fit to on-the-ground biomass measurements in a number of field plots located in various regions around the globe. Since most of these measurements do not intersect with a GEDI footprint, airborne LiDAR was used to measure the return signal which was then translated into a simulated GEDI waveform. In this work we utilize both the level-2a/b and level-4a data as ground truth. The on-the-ground measurements for biomass were mostly done without any tree clearing but by measuring canopy height and diameter and using allometric equations, specific to the tree type and world region, to determine biomass. The simulated waveforms undergo the same processing to extract RH metrics which provide the predictors for linear models to predict AGBD. Various models were developed for all combinations of plant functional type (PFT) and world region defined as prediction stratum. For details about the selected predictors for each model and their performances see section 2 and 3 in (Duncanson et al. [2022]).\nThe models are linear functions of the predictors with a general form of \n\\(f_j = X_jb_j\\) \nwhere X is a n \u00d7 m matrix of n measurements with m predictors and \\(b_j\\) a m \u00d7 1 vector of parameters for prediction stratum j. The best parameters are determined by linear regression where the predicted variable may be in transformed units using a function h which is either unity, square root or log. For new measurements X' the model of the corresponding prediction stratum is chosen and the predictions are given by \n\\(AGBD = h^{-1}(X'b + \\epsilon)\\)"}, {"title": "3.2 Input data", "content": "In order to leverage the respective benefits of different data sources, we fuse optical bands (red, green, blue) from the Sentinel-2 (Copernicus Sentinel-2 (processed by ESA) [2021]) satellite with thermal bands (nir, swirl, swir2) from the same satellite (processed with the SEN2COR algorithm (Main-Knorn et al. [2017]) to provide surface reflectance) and synthetic aperture radar (SAR) signal (VV and VH backscatter) from Sentinel-1 (Potin et al. [2022]). In addition, we use altitude, aspect and slope from the Shuttle Radar Topography Mission (SRTM) (NASA JPL [2013]) to further enrich the predictive capability of our model. The predictors from the digital elevation model (DEM) carry important information about the local topography which affects the distribution of plant functional types and their growth pattern (Wang et al. [2022]). We further provide the global coordinates of each data sample by encoding longitude and latitude in the interval [-1, 1]. The optical bands of Sentinel-2 as well as the Sentinel-1 bands have a native resolution of 10 m while the thermal bands of Sentinel-2 have a resolution of 20 m and the DEM of 30 m. In order for the CNN to process the input layers at different resolutions, we resample all layers to 10 m resolution using the bilinear interpolation method and stack them to form a 13 channel input tensor.\nTo generate a global training and test dataset, we uniformly sample locations within the latitude (longitude) range of [-51.6, 51.6] degrees ([-180, 180] degrees) which intersect with landmass. We use the Descartes Labs (DL) proprietary tiling system to create image tiles of size 512 pixels\u00d7512 pixels at 10 m/pixel resolution with the sampled coordinate being at the center of the tile. Each tile is required to contain >20 GEDI footprints. During training, we dynamically split each tile into 4 non-overlapping sub-tiles of size 256 pixels\u00d7256 pixels, which increases the total number of samples in the dataset four-fold. We introduce a naming convention for the tiles based on their location being either in the southern hemisphere (lat < -23.5\u00b0), the tropics (-23.5\u00b0 < lat > 23.5\u00b0) or the northern hemisphere (lat > 23.5\u00b0)."}, {"title": "3.2.1 Cloud mask and image composite", "content": "In order to reduce obstruction from clouds and cloud shadows in Sentinel-2 imagery, we generate cloud free composites from a stack of images which are masked with the binary output of our proprietary cloud and cloud shadow detection"}, {"title": "3.2.2 Global dataset", "content": "For each sampled tile, we generate Sentinel-1 and Sentinel-2 composites according to the definition in section 3.2.1 by gathering imagery using the Descartes Labs platform for the year 2021 (2021/2022 for the southern hemisphere). The DEM has a fixed acquire year of 2000. For a given tile, we gather all GEDI level-2a/b and level-4a point data which lay within the tile geometry and have a collection date no more than \u00b11 month from the composite time range (this buffer is set to 0 months for the 6 months composites). In order to match the data from GEDI level-2a/b with level-4a, they are required to have the same footprint coordinates to 5 decimal point precision and the same acquire date. Further, we only accept data with quality flag 14_quality_flag equal to 1 and require at least 20 data points to be within the tile geometry. Due to the sparsity of the GEDI data, it is saved as vector data along with each footprints pixel coordinate and rasterized on the fly during training. Despite the footprint being 25 m in diameter, we assign the corresponding target value to only one pixel of size 10 m\u00d710 m at the location of the footprint center.\nIn total 276'766 data samples were created of which 14'745 (5%) are randomly selected and stored as test dataset. Each sample is composed of an average of 50 scenes across Sentinel-1 and Sentinel-2 amounting to a total of 13.8 M scenes processed. The total number of ground truth data points is 67 M (3.8 M) for the training (test) dataset."}, {"title": "4 Methodology", "content": "In this section we describe our approach to building a computer vision (CV) model which fuses selected bands of multiple sensors as well as encoded geographic location to form a 13 channel input tensor and simultaneously predicts a continuous map of above ground biomass density, canopy height, canopy cover and their respective uncertainties at the same resolution as the input imagery."}, {"title": "4.1 Model development", "content": "The GEDI dataset offers measurements of AGBD, CH and CC for recent years but is largely incomplete at higher resolution due to its sparsity. Here we describe the model we developed which uses multiple source as input to predict AGBD, CH and CC as a continuous map at the resolution of the input source while using the GEDI level-2 and level-4 dataset for training. In this work, we use the surface level processed Sentinel-2 with 6 bands (red, green, blue, nir, swirl,"}, {"title": "4.2 Model training", "content": "Training of the model is divided into three parts: First we train the entire network including prediction heads 1-3 on the full dataset of ~1 M samples. The network is optimized to simultaneously predict AGBD, CH and CC using their"}, {"title": "4.2.1 Loss function", "content": "We consider the prediction of each pixel (i, j) in the output map \\(Y_{i,j}\\) as an independent measurement of a normal distributed variable with a standard deviation of \\(\\sigma_{i,j}\\). The probability for a given ground truth value \\(\\hat{y}_{i,j}\\) is then given by \n\\(P(Y_{i,j}|Y_{i,j}(\\theta), \\sigma_{i,j} (\\theta)) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2_{i,j} (\\theta)}} e^{- \\frac{(Y_{i,j} - Y_{i,j}(\\theta))^2}{2 \\sigma^2_{i,j} (\\theta)}}\\) \nwhere \u03b8 are the parameters of the network. The likelihood function can be written as \n\\(L(\\theta) = \\prod_{i,j} P (Y_{i,j}|Y_{i,j}(\\theta), \\sigma_{i,j} (\\theta)) .\\) \nWe use gradient descent to optimize the parameters \u03b8 which minimize the negative log likelihood (NLL) which defines our loss function\n\\(L = -log(L) = \\sum_{i,j} ( \\frac{( \\hat{y}_{i,j} - y_{i,j} (\\theta))^2}{2 \\sigma^2 (\\theta)} + log( \\sigma^2 (\\theta)))\\) \nwhere we omitted the factor 2\u03c0 from equation 8. \\(\\sigma_{i,j}(\\theta)\\) is the uncertainty predicted by the model for each variable separately. By definition, we expect 68 % of all samples to have an absolute error between predicted and true value within the range of 1\u03c3. During training we verify this by calculating the fraction of z-scores, defined as z = |\u0177 \u2013 y|/\u03c3, to be <1. Even though the log(\u03c32) term in equation 10 acts as regularization to make sure the model does not learn a trivial solution by predicting a very large \u03c3, we noticed that the coverage may still be >0.68. We therefore introduce an addition regularization term in the definition of the loss as \n\\(L = \u2013 log(L) + \\lambda \\sigma^2\\) \nwhere \u03bb is a hyper parameter determined for each variable separately. For a given sample, the number of hard labels is much smaller than the number of soft labels (on average the ratio of soft to hard labels is ~1/1000) and varies between samples. We therefore introduce a pixel weighting scheme to balance the contribution of hard and soft labels to the loss. In addition to the imbalance between hard and soft labels, we also want to assign relative weighting of hard to soft labels. This is an essential requirement in order to make the student-teacher approach work well. Let us consider the relative weight of a hard label to be \u03bbh and that of a soft label to be \u03bbs. Then the balanced loss function, taking both the relative weights as well as the number of hard and soft label pixels into account, becomes \n\\(L_b = \\frac{ \\lambda_h n_h + (1-m) L}{\\lambda_s n_s}\\) \nwhere \\(n_h (n_s)\\) are the number of hard (soft) label pixels in a given sample and m has the same definition as in equation 6. By default, we choose \u03bbh = 1 and vary \u03bbs during training according to an exponential decay from 1 to 1e-3 during the initial epochs, then exponentially increase it to le-2 during the remaining epochs. We have considered other schedules such as linear, constant and zero (corresponding to no soft label) which all resulted in worse model performance.\nSo far we have only formulated the loss function in equation 12 considering one variable. However, we train the model for all variables simultaneously for which we construct the final loss as the weighted sum over the variable specific components \n\\(L_{total} = \\alpha_0 L^{AGBD} + \\alpha_1 L^{CH} + \\alpha_2 L^{CC}\\) \nwhere the weights \\(\u03b1_i\\) allow for balancing the different contributions due to the different target scales of the variables. In this work we set \u03b10 = \u03b1\u2081 = a2 = 1 since we did not observe any improvements using variable specific weighting."}, {"title": "4.2.2 Training details", "content": "For the first stage of pre-training on the full global dataset, we train the model for 40 epochs with a batch size of 72 on a multi-GPU node with 4 A10G GPUs. We reserve one of the GPUs for data pre-processing, such as calculation of the soft labels, while the remaining GPUs perform the model training. We use the Adam optimizer (Kingma and Ba [2017]) with a linearly increased learning rate from le-7 to 1e-4 over a warm up period of 1 epoch after which it is continuously decreased according to a cosine function over the remaining training period. The second stage consists of fine tuning each variable separately on the balanced datasets and applying the sample weighting according to the inverse frequency distribution. In this stage only the prediction heads 1-3 are trained while keeping all other model weights frozen. We use three single GPU nodes to train each head in parallel with a batch size of 32. In the third and last stage, we fine tune the prediction heads 4-6 which are responsible for predicting the variable uncertainty. In all stages, the loss function as defined in equation 13 is minimized. However, in stages one and two the uncertainty estimation is ignored which is equivalent to setting \u03c3 = 1 for all pixels."}, {"title": "5 Results", "content": "In this section we discuss the results of evaluating our model both qualitatively and quantitatively on the test dataset as well as independent datasets which focus on one of the three variables which our model predicts. We first show some qualitative results by deploying the model over specific regions covering a diverse range of biomes. We then evaluate the model using the held out global test dataset and determine a number of metrics such as mean error (ME), mean absolute error (MAE), mean absolute percentage error (MAPE), root mean squared error (RMSE) and correlation for each of the predicted variables separately. Finally, we determine our models accuracy against publicly available datasets."}, {"title": "5.1 Qualitative assessment", "content": "The deployment of the model at a global scale for the year 2023 allows for a qualitative assessment of its performance, in particular in challenging areas such as those with high cloud coverage which may affect the quality of the input imagery. In figure 5 we present four sample locations which all cover an area of 1 km\u00d71 km at 10 m resolution. The left"}, {"title": "5.2 Quantitative assessment", "content": "We have evaluated the performance of our model based on the test dataset consisting of 14'745 samples and 13.8 M individual GEDI footprints. Model inference is performed on each test sample of size 256 pixels\u00d7256 pixels which generates predictions for every pixel and all output variables including their uncertainties. We measure the correlation, mean error (ME), mean absolute error (MAE), mean absolute percentage error (MAPE) and root mean squared error (RMSE) between the GEDI ground truth data and the model predictions gathered at the pixel coordinates of the GEDI footprints. Due to the non-uniform distribution of ground truth values, we assess the model performance on both the test set with its original as well as a uniform sample distribution. For this purpose, we sample data points according to the inverse PDF of each variables respective distribution. This ensures a more fair performance assessment across the value ranges of each variable. Due to the low frequency of occurrence of high values of AGBD and CH, we set a reference value of 300 Mg/ha for AGBD and 30 m for CH to define the lowest value of probability. Values with lower probabilities are always included during sampling. After the sampling procedure, a total number of 200 k - 450 k data points are remaining, depending on the variable. In table 1 we summarize the evaluation metrics. These"}, {"title": "5.3 Assessment against third party datasets", "content": "In the following section we present the results from studies where we compared our models output with third party datasets that are independent of the GEDI measurements. All these datasets were either generated from high resolution, airborne LiDAR or on-the-ground measurements. We deployed our model over the respective regions for the years the ground truth data was collected. We divide our studies into estimation of above ground biomass and canopy height since the dataset are focused on one of these variables."}, {"title": "5.3.1 Above ground biomass", "content": "For the verification of biomass estimates, we utilize the dataset created by (Rodda et al. [2024]) which consists of 13 plots of variable size at a maximum resolution of 40 m. 8 of the plots are located in the Central Africa region and 5 of the plots in the South Asia region. This dataset was compiled from forest inventory collected within the respective sites during different time ranges. Site-level measurements followed a strict protocol where the diameter at breast height (DBH) was determined for each individual tree within the plots as well as the tree height (H) for a subset of trees. The tree level taxonomic identification and relative coordinates within the plots were recorded along with geographic coordinates of the plot borders at intervals of 20 m. The forest inventories were split into 1 ha and 0.16 ha plots. The collected data of DBH-H on a subset of trees within these plots was used to fit allometric equations which relate H to DBH and allow the extrapolation of tree height measurements to all trees in the plots. Then the wood density based on the tree taxonomy was used to calculate the reference aboveground biomass (AGBref). Aerial LiDAR at a resolution of 1 m was obtained over the sites within an absolute temporal difference of 2.2\u00b11.9 years from the forest inventory"}, {"title": "5.3.2 Canopy height", "content": "To further assess our models CH estimation, we use data from the National Ecological Observatory Network (NEON) (National Ecological Observatory Network (NEON) [2021]). NEON is a high-resolution LiDAR dataset which provides detailed three-dimensional information about the Earth's surface. This dataset includes measurements of vegetation structure, topography, and land cover across diverse ecological regions in the United States. The data is collected using airborne LiDAR sensors, capturing fine-scale details at a resolution of 1 m. We selected all measurement sites in the states AL, CA, FL, GA, OR, UT, VA and WA from the year 2021. We subdivide each site into areas of size 2560 m\u00d72560 m and rasterize our models CH estimation as well as NEONS CH measurement at their respective resolution (10 m for our model and 1 m for NEON). This results in tiles of size 256 pixels\u00d7256 pixels (our model) and 2560 pixels\u00d72560 pixels (NEON). In order to compare the two maps, we resample the NEON map by determining the 98th percentile in each 10 pixels\u00d710 pixel area resulting in a map of the same pixel size as our models. We choose this approach because our model effectively estimates the RH98 metric for each pixel corresponding to 10 pixels\u00d710 pixels in the NEON map. Figure 12 illustrates the RGB imagery as well as the CH maps of our model and NEON for two samples (top row corresponds to a sample in site ABBY, WA and bottom row to a sample in site TEAK, CA). The visual comparison between our models CH estimation and NEONs CH measurement shows very good agreement despite the fact that our models CH map is at 10x lower resolution. We quantify the agreement between our and NEONS CH map by calculating RMSE, ME and MAE considering all pixel values from all tiles and sites as data points. Overall we achieve an RMSE of 7.46 m, an ME of 0.17 m and a MAE of 5.57 m. Figure 13 shows a 2D histogram of all data points as well as ME and MAE aggregated in bins of size 1 m with respect to the NEON CH. The R2 value of the model predictions compared to NEONs measurements is 0.51. We also illustrate the error distribution within each bin by indicating the interquartile as well as 90 % range. The red, dashed line corresponds to the mean in each bin. The agreement between our and NEONs CH map is very good across the entire value range. We note that there is some larger disagreement at low values (<5 m) which is also visible in the sample maps in figure 12 where our model generally estimates slightly higher values at small CH. This can be attributed to the fact that the GEDI dataset contains a very small amount of data points <3 m so the model generally over-estimates in this region. However, it is remarkable that the absolute error mostly stays below 5 m across the entire value range up to 50 m. The NEON dataset offers the capability to re-calibrate the model estimations where it exhibits a bias. This is true for any high-accuracy, regional dataset. We reserve the demonstration of re-calibration of the model for future work."}, {"title": "6 Applications", "content": "In this section we demonstrate some use cases of our model including the monitoring of changes over time as well as fine-tuning the model on regional ground truth data to better align the global model to local conditions. Traditionally, many use cases require the deployment of multiple models in order to perform a down-stream task where each model output provides the solution for sub-tasks. One of the main motivations of this work is to create a single model which is versatile enough to provide results for all sub-tasks in one forward pass. One such application is the detection of deforestation and association of loss of biomass which we discuss in detail in section 6.2."}, {"title": "6.1 Global model deployment", "content": "Our model is deployable at scale due to its global training dataset. This is an important factor when it comes to monitoring changes in the ecosystems. Our model is scalable both spatially as well as temporally which is important for change detection. While global deployments may not be required very frequently, deployments over local regions of interest can rapidly be performed at temporal resolutions of <1 year.\nWe generated global maps for all three prediction variables AGBD, CH and CC as well as their uncertainties at 10 m resolution for the recent years. They cover the latitude range of 57\u00b0 S to 67\u00b0 N according to the availability of Sentinel-1 data. The deployment year is 2023 for all regions except for the areas where Sentinel-1 experienced an"}, {"title": "6.2 Monitoring biomass change and deforestation detection", "content": "The monitoring of global deforestation and carbon accounting has become an important part of climate change mitigation where remotely sensed data plays an integral part for many models developed in the past. Most these models utilize optical or SAR based imagery and are based on a variety of approaches, from pixel based anomaly detection in time series data (Decuyper et al. [2022]) to modern deep learning and computer vision algorithms (Sol\u00f3rzano et al. [2023]). In this section we demonstrate the usability of our model for the task of deforestation detection and accounting of corresponding loss in biomass. We selected an area in Brazil at location 59\u00b0 26\u2032 46\u2033 W, 7\u00b0 1' 4\" S with a total area of 507 kha. We deploy our model over the area for each year from 2017 to 2023. Figure 15 shows the map of AGBD for 2017 (left) and 2023 (middle) respectively. In this study, we measure changes year over year and compare the results to Global Forest Watch (Hansen et al. [2013]) which provides annual tree cover loss data based on observations from Landsat at 30 m resolution. We follow their approach for defining forested land by requiring CH > 5 m and determine the regions with significant canopy cover loss (ACC > 20 %). Figure 15 (right) shows the regions which meet these requirements accumulated from 2017 to 2023. Note that not all regions correspond to complete deforestation but rather significant tree cover loss in accordance to the definition by Global Forest Watch. We aggregate the areas with tree cover loss for each year, with respect to the previous year, and compare our numbers to those reported by Global Forest Watch. Figure 16 summarizes the results. Our numbers agree with those by Global Forest Watch to within 10% on\""}, {"title": "6.3 Model fine-tuning for local conditions", "content": "One of the main contributors to the uncertainty of AGBD estimations are limited availability of ground measurements for calibrating allometric equations at global scale. On the other hand, estimations of canopy height metrics are not depending on such calibrations as they are derived from the LiDAR waveforms directly. Our model therefore offers a unique capability for fine-tuning on local conditions in order to achieve more accurate AGBD estimations. The majority of allomtric equations use relative height (RH) metrics as predictors for AGBD which our model is able to estimate. The base model predicts AGBD, RH98 and CC. However, due to its multi-head architecture, it can easily be fine-tuned on other variables. We demonstrate such a use case based on the data and coefficients for allometric equations provided by (Rodda et al. [2024]). The authors determine the coefficients of the allometric equation in 14 for each of the 13 sites. They find that the average of all listed RH metrics is the best predictor for AGBD. We expand our model to 7 heads, each representing one of the RH metrics and fine-tune the weights of these heads for one epoch while keeping the weights of the encoder and decoder frozen. We apply sample weighting according to the inverse PDF of each variables distribution (see appendix A for details). Figure 17 shows 2D histograms of the models estimate for each variable versus the true value, evaluated on a uniformly sampled test dataset. Generally there is very good agreement"}, {"title": "7 Discussion", "content": "In this work we propose a novel deep learning based model which unifies the prediction of several biophysical indicators which describe the structure and"}]}