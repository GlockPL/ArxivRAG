{"title": "Event Stream based Human Action Recognition: A High-Definition Benchmark Dataset and Algorithms", "authors": ["Xiao Wang", "Shiao Wang", "Pengpeng Shao", "Lin Zhu", "Bo Jiang", "Yonghong Tian"], "abstract": "Human Action Recognition (HAR) stands as a pivotal research domain in both computer vision and artificial intelligence, with RGB cameras dominating as the preferred tool for investigation and innovation in this field. However, in real-world applications, RGB cameras encounter numerous challenges, including light conditions, fast motion, and privacy concerns. Consequently, bio-inspired event cameras have garnered increasing attention due to their advantages of low energy consumption, high dynamic range, etc. Nevertheless, most existing event-based HAR datasets are low resolution (346 \u00d7 260). In this paper, we propose a large-scale, high-definition (1280 \u00d7 800) human action recognition dataset based on the CeleX-V event camera, termed CeleX-HAR. It encompasses 150 commonly occurring action categories, comprising a total of 124,625 video sequences. Various factors such as multi-view, illumination, action speed, and occlusion are considered when recording these data. To build a more comprehensive benchmark dataset, we report over 20 mainstream HAR models for future works to compare. In addition, we also propose a novel Mamba vision backbone network for event stream based HAR, termed EVMamba, which equips the spatial plane multi-directional scanning and novel voxel temporal scanning mechanism. By encoding and mining the spatio-temporal information of event streams, our EVMamba has achieved favorable results across multiple datasets. Both the dataset and source code will be released.", "sections": [{"title": "1. Introduction", "content": "Human Activity Recognition (HAR) represents a critical domain within computer vision that has experienced substantial advancement in recent times, thanks to the integration of deep learning techniques [1, 35]. Typically, these models are tailored for analyzing video frames recorded by RGB cameras and have been extensively employed across a broad spectrum of practical scenarios. For example, we can achieve pre-prevention, in-process monitoring, and post-inspection in the security monitoring field, and intelligent referee in sports through the analysis of human behavior. Although the RGB cameras based HAR works well in regular scenarios, however, the issues caused by its imaging quality may limit the applications of HAR severely, such as low illumination and fast motion. On the other hand, the privacy protection is also widely discussed in the human-centered research. Awkwardly, the ethical problems caused by high-quality data and the data quality problems caused by low-quality video both require new behavior recognition paradigms.\nRecently, the event camera (also termed Dynamic Vision Sensors, DVS) which is a bio-inspired sensor draws more and more attention from researchers. Different from the RGB camera which records the scene into video frames in a synchronous way, each pixel in the event camera is triggered asynchronously by saving an event point if and only if the variation of intensity exceeds the given threshold. Due to the aforementioned unique imaging principle, the event camera shows the following advantages or features: high dynamic range, low energy-consumption, dense temporal resolution but sparse spatial resolution [22]. Therefore, it performs well even in low-illumination, overexposure, and fast-motion scenarios. Also, the spatial resolution is getting"}, {"title": "2. Related Work", "content": "In this section, we review the most related research topics to our paper, including Event-based Recognition and the State Space Model. More related works can be found in surveys [36, 69, 88, 92]."}, {"title": "2.1. Event-based Recognition", "content": "Current works can be divided into three streams for the event-based recognition, including the CNN based [81], spiking neural network (SNN) based [16, 19], graph neural network (GNN) based models [5, 6], due to the flexible representation of event stream. For the CNN based models, Wang et al. [81] propose to identify human gaits using an event camera and design a CNN model for recognition. As the third generation of neural networks, the SNN is also adopted to encode the event stream for energy-efficient recognition. To be specific, Peter et al. [12] propose the weight and threshold balancing method to achieve efficient ANN-to-SNN conversion. Nicolas et al. [59] propose a sparse backpropagation method for SNNs and achieve faster and more memory efficiency. Zhou et al. [90] propose Ex-ACT, a novel approach to event based action recognition through cross-modal conceptualization, along with an adaptive fine-grained event representation and uncertainty estimation module based on conceptual reasoning. Gao et al. [23] proposes an event camera based behavior recognition framework EV-ACT, which integrates multiple event information through Learning Multi Fused Representation (LMFR). Gao et al. [24] propose a multi-view event camera action recognition framework HyperMV based on hypergraphs, which captures the relationship between cross-view and temporal features by constructing a multi-view hypergraph neural network."}, {"title": "2.2. Human Action Recognition", "content": "Human action recognition(HAR) has been a popular task in the past decade due to its practicality in the real world. It is a fundamental task that recognizes a human action from a video containing complete action execution. In deep learning, CNN is widely used in computer vision, including action recognition. Wang et al. [75] proposes an effective method to encode the spatiotemporal information into color distributions in three two-dimensional images, learning discriminative features for human action recognition through CNN. Soo et al. [65] propose a new model for 3D human motion recognition based on a Time Convolutional Neural Network (TCN). This model can interpret 3D bones and easily understand spatiotemporal representations. Li et al. [38] utilize CNN to process the spatiotemporal information of skeleton sequences, significantly improving the effectiveness of human action and interaction recognition in single and cross-view scenarios. Sudhakaran et al.[67] propose a novel spatiotemporal feature extraction module called Gate Shift Use (GSF) to improve the performance of 3D CNN in video action recognition.\nWith the emergence of Transformer networks in the past few years, many Transformer based HAR methods have emerged. Wang et al. [80] introduce a new framework for human activity recognition based on dynamic visual sensors. The event stream is mapped to spatiotemporal embedding through StemNet, and a Transformer network is used to encode and decode dual-view representations. Li et al. [39] propose a new pattern recognition framework that integrates RGB/Event features and semantic features using a multimodal Transformer network, thereby solving the semantic gap and small-scale backbone network problems in existing methods. Wang et al. [78] propose an action recognition method that integrates RGB frames and event stream, called SSTFormer. By using a memory support Transformer network for RGB frame encoding, and a spiking neural network for raw event stream encoding, the current problems in event camera pattern recognition are solved. Wang et al. [74] improves the extraction and compression model of spatiotemporal feature semantics by training a generative model based attention module in the feature representation stage.\nIn addition to the work mentioned above, a great deal of excellent works have also been proposed in recent years. Chen et al. [11] propose a method for optimizing spatiotemporal descriptors to improve text knowledge and promote general video recognition. It's innovation lies in enhancing action category names through a large language model. Zheng et al. [89] propose a human action recognition method that achieves spatiotemporal fusion through joint trajectory maps. This method can better capture rich spatiotemporal dependencies and achieve excellent performance. Kahatapitiya et al. [32] propose a new video text modeling method, VicTR, which can achieve better performance in video text models by combining video and text information to generate \"video conditional text\u201d embeddings. Different from these works, in this paper, we design a novel SSM based human action recognition method to learn the visual representations effectively."}, {"title": "2.3. State Space Model", "content": "State Space Model (SSM) [33] is the recently popular model, which is used to achieve state space transformation. To improve the long-range modeling capability of the models, Gu et al. [28] introduce a new deep learning model that combines the advantages of recurrent neural networks (RNNs), time convolution network, and neural differential equations (NDEs) in the form of linear state space layers (LSSL). LSSL maps sequences by simulating linear continuous time state space representations, theoretically proving its close relationship with the above three models and inheriting their advantages. Subsequently, Gu et al. [27] proposes structured state space sequence models (S4) for deep learning, as a novel alternative to CNNs or Transformers. Based on the new State Space Model(SSM) parameterization, S4 can efficiently model long-range dependencies in long sequences, and it solves the computational and memory requirements encountered by existing methods when processing long sequences.\nBuilding on these works, many researchers are beginning to explore the potential of SSM. Islam et al. [30] uses S4 as a decoder to model long-range temporal interactions in movie clips. Nguyen et al. [55] further extends 1-dimensional sequence modeling to 2-dimensional and 3-dimensional models, such as images and videos. Subse-"}, {"title": "3. Methodology", "content": "In this section, we will introduce how to effectively explore the task of event-based human action recognition using a Mamba-based framework. Firstly, we provide an overview to summarize the framework proposed in this work. Next, we introduce the effective representations of event data in this work. Finally, we analyze the detailed network architectures in our framework for event-based human action recognition."}, {"title": "3.1. Overview", "content": "To explore the effectiveness of the Mamba network in event-based human action recognition tasks, in this work, we propose a novel action recognition framework based on a spatial-temporal scanning Mamba architecture. As shown in Fig. 2, we input various forms of event data representations (event image and event voxel) into the network to fully utilize the effective information of the event data. Specifically, we propose a voxel temporal scanning mechanism to process the voxel grids by rearranging them in chronological order, which effectively integrates the dense temporal information of each event trajectory. Subsequently, we used VMamba [49] as our backbone network to extract robust feature representations for both the spatial and the temporal. Finally, we add the features of the two perspectives and send them to the classification head for efficient action classification. More details will be introduced in the subsequent paragraphs."}, {"title": "3.2. Input Representation", "content": "Owing to the rich temporal information inherent in event data, our approach involves integrating data from multi-view to harness the full potential of event data's informative representation. We consider an event stream denoted as $\\mathcal{E} \\in \\mathbb{R}^{W \\times H \\times T} = \\{e_1, e_2, ..., e_n\\}$, each $e_i = [x,y,t,p]$ represents an individual event point triggered asynchronously, $(x, y)$ denotes the spatial coordinates, $t$ is the timestamp, $p$ is the polarity, with $i$ ranging from 1 to $N$, and $N$ signifying the total count of event points within the current sample. $H, W, T$ denotes the height, width, and overall time step. Utilizing the event stream $\\mathcal{E}$, one can effectively compile them into event frames $E_F \\in \\mathbb{R}^{T' \\times C \\times H \\times W}$, $T'$ denotes the number of event frames, $C$ is the channel of each frame, thereby leveraging the full potential of pre-existing deep network architectures (e.g., ViT [14], Mamba [26]). Specifically, event images are generated by segmenting the event stream into a series of clips, each encapsulating a fixed time interval. This approach is effective for event-based recognition [79, 80], however, the rich potential of the dense temporal dimension is seldom fully exploited.\nIn this work, we also investigate the use of the event voxel to further enhance event representation which can effectively preserve the temporal information of the event stream. Specifically, we dissect the event stream $\\mathcal{E} \\in \\mathbb{R}^{W \\times H \\times T}$ into multiple cubic voxels $E_v \\in \\mathbb{R}^{a \\times b \\times c}$, each of which may contain several event points. Thus, we have $\\frac{W \\times H}{C}$ voxel grids for event stream $\\mathcal{E}$. In the following sub-sections, we will further explore how to obtain the voxel tokens using the temporal voxel scanning mechanism for our framework."}, {"title": "3.3. Network Architecture", "content": "As shown in Fig. 2 (the top sub-figure), our event-based HAR framework is built based on the Mamba network which takes the event frames and voxels as the input. We will first introduce the regular Mamba network for event frame-based action recognition. Then, we will introduce the temporal voxel scan mechanism to augment the HAR framework further.\n\u2022 Mamba based Visual Backbone. Given the stacked event frames $E_F \\in \\mathbb{R}^{T' \\times C \\times H \\times W}$, we first partition each image into patches and re-formulate them using spatial cross-scan employed in VMamba network. Then, a patch embedding layer is adopted to transform them into token representations before feeding into the VMamba backbone network. A four-stage VSS block is adopted to construct the VMamba backbone. Specifically, Mamba [26] maps a 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\rightarrow y(t) \\in \\mathbb{R}$ through a hidden state $h(t) \\in \\mathbb{R}^N$. Just like the recurrent neural network (RNN), Mamba usually inputs the previous state and the current input into the network to obtain the current output. This continuous system can be defined as,\n$\\begin{aligned} &h^{\\prime}(t)=A h(t)+B x(t), \\\\ &y(t)=C h(t)+D x(t). \\end{aligned}$  (1)\nwhere $A \\in \\mathbb{R}^{N \\times N}$ is the state matrix, $B \\in \\mathbb{R}^{N \\times L}$ is the input matrix, $C \\in \\mathbb{R}^{L \\times N}$ is the output matrix, and $D \\in \\mathbb{R}^{L \\times L}$ is the feed-through matrix. $x(t) \\in \\mathbb{R}^L$ and $h^{\\prime}(t) \\in \\mathbb{R}^N$ are the current input sequence and the derivative of the hidden state. Finally, the $y(t)$ can be obtained as the current output. For more detailed information, including details of discretization, please refer to the supplementary materials or Mamba [26].\nFor each Mamba layer, we first input image data for feature learning from a spatial view. Fig. 2 (c) shows the detailed structure of the VMamba backbone blocks. Firstly, we take $X_i \\in \\mathbb{R}^{B \\times H \\times W \\times C}, i \\in \\{\\text {image}, \\text {voxel}\\}$ as input (voxel will be introduced in the next sub-section), where $B$ is the batch size, $H$ and $W$ denote height and width, $C$ is the dimension of feature embedding. After passing through a linear normalization (LN) layer, the input $X_i$ is divided into two parts, $x_i$ and $z_i$, according to the last dimension. Subsequently, we take $x_i$ as the input of the main branch, and after the linear layer, DW convolution, and SiLU activation function, it is sent to the SS2D module. The formulas can be described as,\n$x_i^{\\prime} = \\operatorname{SS} 2 \\mathrm{D}(\\operatorname{SiLU}(\\mathrm{DW}(\\operatorname{Linear}(x_i)))).$ (2)\nThe SS2D module uses a cross-selective scanning mechanism to scan image patches with four-way repeatedly. For detailed information on four-way scanning, please refer to VMamba [49]. Next, we merged the four different orders of patches to the original 2D feature map through the cross-merge operation. Then, we multiply the $x_i^{\\prime}$ obtained from the SS2D module and the linear layer by $z_i$, which also through a linear layer and SiLU activation function. Finally, we add the result of this multiplication to the initial input $X_i$ to produce the final output result of the current block. These processes can be expressed as,\n$X_i^{\\prime} = \\operatorname{Linear}\\left(\\operatorname{LN}\\left(x_i^{\\prime}\\right) \\times \\operatorname{SiLU}\\left(\\operatorname{Linear}\\left(z_i\\right)\\right)\\right)+X_i.$ (3)\nAfter that, we input $X_i^{\\prime}$ into the block of the next layer. Through the above steps, we stacked $L$ layers of S6 blocks for effective feature extraction.\n\u2022 Voxel Temporal Scanning Mechanism. The aforementioned spatial cross-scan used in Mamba captures the spatial information well, but we believe the temporal information can also be further enhanced. In this work, we propose a temporal voxel scan mechanism to augment the frame-based Mamba network for event-based human action recognition. As shown in Fig. 2 (b), given the event stream $\\mathcal{E}$, we first divide it into multiple clips (i.e., blue, orange, green blocks) and extract the voxel representation for each clip."}, {"title": "4. CeleX-HAR Benchmark Dataset", "content": "In this section, we first introduce the protocols we followed when collecting the event videos. Then, we will focus on statistical analysis and baselines we build for future works to compare."}, {"title": "4.1. Protocols", "content": "When collecting our CeleX-HAR dataset, we obey the following standards: 1). Multi-view: We record each action from different views, including front, left, right, down, and up views. 2). Various illumination: Our dataset also contains videos with different illumination conditions, including low-, middle-, and full-illumination. Note that, the low-illumination videos account for about 40% for each class. 3). Camera motions: The movement of the event camera affects the imaging quality greatly. As the camera moves, a great deal of background information is recorded; when the camera is still, only the moving subject is recorded. Note that half of the videos in each class are recorded using a moving event camera. 4). Video length: Usually,\nan action lasts about 2-3 seconds in our dataset; but some videos are larger than 5 if it takes longer to complete. 5). Speed of action: The video of the three kinds of speeds (i.e., low-, medium-, and high-speed) occupies the proportion of 30%, 40%, 30%, respectively. 6). Occlusion: In a certain movement, there can be partial occlusion of the interference. 7). Glitter: As the event camera is sensitive to glitter, in our dataset collection, we also record videos with conspicuous flash. 8). Capture distance: These videos are approximately 1-2 meters, 2-3 meters, and larger than 3 meters away from the target object. 9). Large-scale & High-definition: 124,625 high-definition (1280\u00d7800) event video sequences (the largest real-event HAR dataset) are recorded which cover 150 classes of human activities."}, {"title": "4.2. Statistical Analysis and Baselines", "content": "Our dataset comprises 150 categories of common human behavioral actions, as illustrated in Fig. 3 (a), a total of 124,625 videos are collected using a CeleX-V event camera and the training and testing subset contains 99,642 and 24,983 videos respectively. As visualized in Fig. 3 (b), we compare the number of categories (Y-axis values) and the number of videos (bubble size) between the CeleX-HAR dataset and other datasets.\nAs shown in Table 7, we also establish benchmarks for a variety of recognition models on our dataset, which provides a wide baseline for future comparisons on the CeleX-HAR dataset, including: 1). CNN based models (ResNet50 [29], ConvLSTM [63], C3D [70], R3Plus1D [72], TSM [44], ACTION-Net [83], TAM [50], GSF [67]), 2). Transformer based models (Video-SwinTrans [51], TimeSformer [4], SlowFast [21], SV-Former [86], EFV++ [8], ESTF [80]), 3). RWKV based models (VRWKV [15]) and 4). Mamba based models (Vision Mamba [93], VMamba [49], Video Mamba [41])."}, {"title": "5. Experiments", "content": "In this section, our experiments are conducted on the ASL-DVS [6], N-Caltech101 [56], DVS128-Gait-Day [82], Bully10K [13], Dailydvs-200 [77], and our newly proposed CeleX-HAR dataset. More detailed introductions to these datasets can be found in our Supplementary Materials. The top-1 accuracy is adopted for the evaluation of our proposed model and other SOTA action recognition algorithms."}, {"title": "5.1. Dataset and Evaluation Metric", "content": "In this paper, our experiments are conducted on the ASL-DVS [6], N-Caltech101 [56], DVS128-Gait-Day [82], Bully10K [13], Dailydvs-200 [77], and our newly proposed CeleX-HAR dataset. More detailed introductions to these datasets can be found in our Supplementary Materials. The top-1 accuracy is adopted for the evaluation of our proposed model and other SOTA action recognition algorithms."}, {"title": "5.2. Implementation Details", "content": "Our proposed framework can be optimized in an end-to-end manner. The learning rate and weight decay are set as 0.001 and 0.0001, respectively. The SGD is selected as the optimizer and trained for a total of 30 epochs. In our implementations, a total of 33 S6 blocks are stacked as our"}, {"title": "5.3. Comparison with Other SOTA Algorithms", "content": "\u2022 Results on ASL-DVS [6] Dataset. As shown in Table 2, our proposed method achieves 99.9% (top-1 accuracy) on the ASL-DVS dataset. The compared method M-LSTM which adopts learnable event representation is still inferior to our method. Some graph-based event recognition models are also worse than ours, including EV-VGCNN, VMV-GCN, and Ev-Gait-3DGraph. Therefore, we can conclude that our proposed model is more effective for event-based HAR.\n\u2022 Results on N-Caltech101 [56] Dataset. As shown in Table 3, our model achieves 93.9% (top-1 accuracy) on this benchmark dataset which is significantly better than the compared methods. To be specific, our model outperforms"}, {"title": "5.4. Component Analysis", "content": "To verify the effectiveness of our proposed method, as shown in Table 7, we compared our algorithm EVMamba with the version without incorporating voxel temporal scanning. To further enhance the robustness of the Mamba model, we study how to bring dense temporal information to the input based on the image spatial scanning. Ultimately, our results were further improved with the voxel temporal scanning mechanism, achieving a top-1 accuracy of 72.3%. Thus, we believe that the Mamba model is a good backbone replacement solution for event-based human action recognition. Meanwhile, our voxel temporal scanning strategy can further improve the accuracy of the model."}, {"title": "5.5. Ablation Study", "content": "\u2022 Analysis of Different Event Representations for HAR. In our experiments, we investigate different event representations for the human action recognition task. As shown in Table 8, we can see that considering only the event voxel (acc/top-1, 0.451) as input would result in a significant discrepancy in the outcome compared to the event frames (acc/top-1, 0.720). Despite event voxel's commendable spatio-temporal information integration, their omission of the intrinsic polarity of events hampers a comprehensive feature portrayal. In contrast, we opted to incorporate the event voxels' temporal information into the event frames and achieve outstanding performance.\n\u2022 Analysis of Other Exploited Fusion Methods. Since our model's input is in the form of dual branch data (event image and voxel), we need to design a suitable scheme to integrate dense voxel temporal information into the images. Specifically, through careful study of the Mamba model, we propose a matrix interaction method as shown in Fig. 5. We interact with the input matrix of the image branch and the voxel branch, namely the $B$ matrix, in a gate-controlled manner to learn complementary information between the two branches. However, as shown in Table 8, it is disappointing that this fusion method will result in a decrease in accuracy. The potential reason for this may be that the matrix interaction method breaks the original information and brings noise to the output state. Surprisingly, the simplest addition can bring the best results.\n\u2022 Analysis of the Threshold & of Minimal Length of Event Temporal Voxels. When conducting the temporal event voxel scanning, we set a threshold & to check the influence of trajectory length for the final recognition performance. In other words, we filter out the trajectories whose length is less than the threshold 8. Specifically, the threshold & is set as 1, 3, and 9, as shown in Table 8, we can find that the best results can be achieved when threshold \u03b4 is 1. That is to say, it will be better when all our temporal"}, {"title": "5.6. Visualization", "content": "\u2022 Confusion Marix on CeleX-HAR. As shown in Fig. 6 (a) and (b), we present the confusion matrices for both training and testing on the CeleX-HAR dataset. The diagonal elements, which are closer to yellow, indicate more accurate predictions. It is evident that the model performs better on the training set compared to the testing set, highlighting the challenges posed by our dataset and suggesting that further improvements are needed for our model's generalization.\n\u2022 Feature Distribution. As depicted in Fig. 6 (c) and (d), we also visualize the feature distributions of TSM [44] and the method proposed in this paper. A total of 10 classes are randomly selected to illustrate the feature distribution of the CeleX-HAR dataset. It can be seen that our method outperforms TSM in cases of performance errors, successfully aggregating the features of each class.\n\u2022 Feature Map. As presented in Fig. 7, we concurrently display the feature maps derived from TSM, ESTF, and our model, and add them to the raw images. Specifically, we process the output features by channelizing them, resizing, and normalizing them to align with the dimensions of the raw images, and subsequently superimpose them onto the raw images. As can be observed from the figures, the feature maps generated by our model exhibit a heightened focus on the target.\n\u2022 Top-5 Recognition Results. As shown in Fig. 8, the top-5 prediction results on our proposed CeleX-HAR dataset are presented. We present six action categories including Open an Umbrella, Eat a banana, Making telephone calls, Give a Like, Take somebody's Pulse and Blow a kiss. It is clear that the highest scores predicted by our model for these actions accurately correspond to their respective labels."}, {"title": "5.7. Limitation Analysis", "content": "Although our model can achieve better accuracy compared to dense Transformer models and has advantages in efficiency, processing high-resolution event data directly still poses computational cost issues. The training and inference of the model still require high-end GPUs to complete. Therefore, in future work, we will consider techniques such as model distillation and quantization to further reduce the inference cost. Due to the limitation of GPU memory, we are unable to complete the model training and testing on the original high-definition (1280 \u00d7 800) event stream data at this stage. In future work, we will design new deep neural network models for efficient high-definition data perception."}, {"title": "6. Conclusion", "content": "This paper has made significant contributions to the field of Human Action Recognition (HAR) using event cameras. We have introduced CeleX-HAR, a large-scale, high-definition event-based HAR dataset that addresses the limitations of existing low-resolution datasets. With 150 action categories and 124,625 video sequences, CeleX-HAR considers various challenging factors such as multi-view, illumination, action speed, and occlusion, providing a comprehensive benchmark for future research. Additionally, we have proposed EVMamba, a novel event stream-based HAR model that leverages a Mamba vision backbone with innovative spatio-temporal scanning mechanisms. EVMamba has demonstrated promising performance across multiple datasets, further advancing the state-of-the-art in event-based HAR. With the release of both the dataset and source code upon acceptance, we anticipate that this work will serve as a valuable resource and foundation for future developments in the field of event-based HAR and computer vision. In our future works, we will focus on further reducing the model complexity to handle higher-resolution event data."}]}