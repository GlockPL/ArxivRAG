{"title": "Event Stream based Human Action Recognition: A High-Definition Benchmark Dataset and Algorithms", "authors": ["Xiao Wang", "Shiao Wang", "Pengpeng Shao", "Lin Zhu", "Bo Jiang", "Yonghong Tian"], "abstract": "Human Action Recognition (HAR) stands as a pivotal research domain in both computer vision and artificial intelligence, with RGB cameras dominating as the preferred tool for investigation and innovation in this field. However, in real-world applications, RGB cameras encounter numerous challenges, including light conditions, fast motion, and privacy concerns. Consequently, bio-inspired event cameras have garnered increasing attention due to their advantages of low energy consumption, high dynamic range, etc. Nevertheless, most existing event-based HAR datasets are low resolution (346 \u00d7 260). In this paper, we propose a large-scale, high-definition (1280 \u00d7 800) human action recognition dataset based on the CeleX-V event camera, termed CeleX-HAR. It encompasses 150 commonly occurring action categories, comprising a total of 124,625 video sequences. Various factors such as multi-view, illumination, action speed, and occlusion are considered when recording these data. To build a more comprehensive benchmark dataset, we report over 20 mainstream HAR models for future works to compare. In addition, we also propose a novel Mamba vision backbone network for event stream based HAR, termed EVMamba, which equips the spatial plane multi-directional scanning and novel voxel temporal scanning mechanism. By encoding and mining the spatio-temporal information of event streams, our EVMamba has achieved favorable results across multiple datasets. Both the dataset and source code will be released.", "sections": [{"title": "1. Introduction", "content": "Human Activity Recognition (HAR) represents a critical domain within computer vision that has experienced substantial advancement in recent times, thanks to the integration of deep learning techniques [1, 35]. Typically, these models are tailored for analyzing video frames recorded by RGB cameras and have been extensively employed across a broad spectrum of practical scenarios. For example, we can achieve pre-prevention, in-process monitoring, and post-inspection in the security monitoring field, and intelligent referee in sports through the analysis of human behavior. Although the RGB cameras based HAR works well in regular scenarios, however, the issues caused by its imaging quality may limit the applications of HAR severely, such as low illumination and fast motion. On the other hand, the privacy protection is also widely discussed in the human-centered research. Awkwardly, the ethical problems caused by high-quality data and the data quality problems caused by low-quality video both require new behavior recognition paradigms.\nRecently, the event camera (also termed Dynamic Vision Sensors, DVS) which is a bio-inspired sensor draws more and more attention from researchers. Different from the RGB camera which records the scene into video frames in a synchronous way, each pixel in the event camera is triggered asynchronously by saving an event point if and only if the variation of intensity exceeds the given threshold. Due to the aforementioned unique imaging principle, the event camera shows the following advantages or features: high dynamic range, low energy-consumption, dense temporal resolution but sparse spatial resolution [22]. Therefore, it performs well even in low-illumination, overexposure, and fast-motion scenarios. Also, the spatial resolution is getting higher, for example, 1280 \u00d7 800 and 1280 \u00d7 720 can be achieved by the CeleX-V [10] and PROPHESEE, respectively. These features all inspired us to address the pain points of HAR using a high-resolution event camera.\nIndeed, several datasets are proposed for the event-based classification [2, 34, 40, 45, 54, 56, 62] before, but they are either simulation data or obtained by recording the screen, thus it's intractable to reflect the true features of event camera fully. For the realistic ones [6, 47], their resolution, length or scale is limited (ASL-DVS [6]: 240 \u00d7 180, DailyAction [47]: 346 \u00d7 260), and the overall performance is almost saturated (TORE [3] achieves 0.9995 on ASL-DVS [6], 0.945 on N-Cars [64]). Existing works [49] and our experimental results demonstrate that current network architectures perform well on low-resolution data (e.g., 224 \u00d7 224), but they achieve inferior results on high-resolution ones. This raises an interesting and contemplative research problem. We hypothesize that early in the deep learning boom, given the computational constraints, successful models were optimized for low-resolution data. As computing resources and sensor technology have evolved, high-definition data has become increasingly widespread. Continuing to rely on low-resolution data, and disregarding the valuable information in high-definition data, would be a misuse of resources. Therefore, a high-quality, high-resolution event-based action recognition dataset is urgently needed in the current academic community, as it can effectively support and promote the design of network structures for high-resolution input signals.\nTo bridge the data gap, we propose a large-scale benchmark dataset for event-based HAR in this work, termed CeleX-HAR. As shown in Fig. 1, CeleX-HAR contains 124,625 high-resolution event streams (1280 \u00d7 800), collected using a CeleX-V event camera and covers 150 categories of human daily activities, such as moving a chair, putting on shoes, opening an umbrella, etc. Furthermore, CeleX-HAR considers different attributes, such as multi-view, various illumination conditions, camera motions, speed of action, occlusion, glitter, and capture distance. We split the CeleX-HAR dataset into a training and testing subset which contains 99,642 and 24,983 videos, respectively. Due to the scarcity of baseline methods for future comparison, we have trained and reported on over 20 recognition models on our dataset to facilitate better the comparison and development of event data-based action recognition models. We anticipate that our newly introduced high-definition CeleX-HAR dataset will stimulate and foster research in this field.\nBased on the newly proposed CeleX-HAR dataset, we further propose an event spatial-temporal scanning mechanism-guided Mamba framework for human action recognition, termed EVMamba. The key insight of this framework is that existing action recognition algorithms primarily focus on learning local features only using convolutional neural network (CNN) [29], but ignore the long-range relations. Visual Transformers [14] excels at capturing long-range dependencies, however, they are computationally expensive (O(N2)) and memory-intensive, which presents significant challenges for practical deployment. The newly introduced Mamba model, which maintains a good balance between precision and computational cost with a complexity of O(N), and has been adapted for video classification tasks [41, 52]. Nonetheless, there has been no research on adapting it for human action recognition using event cameras. The output event streams, similar to 3D point clouds, necessitate efficient tokenization. This is a crucial factor that allows sequential models to function optimally in the field of HAR. Specifically, as shown in Fig. 2, given the event streams, we propose to extract the tokens from both spatial and temporal views using the newly proposed event spatial-temporal scanning mechanism. It conducts a spatial cross-scan to get the event frame tokens and a temporal voxel scanning for temporal token mining. These tokens are transformed using patch and voxel embedding layers and fed into the VMamba blocks for spatial-temporal feature learning. Finally, we add these two features and feed them into the classification head for action recognition.\nTo sum up, the contributions of this paper can be summarized as the following three aspects:\n\u2022 We propose a large-scale benchmark dataset for event-based human activity recognition, termed CeleX-HAR. To the best of our knowledge, it is the largest high-resolution event dataset for human action recognition.\n\u2022 We further present an event recognition framework based on the Visual Mamba architecture and develop an innovative voxel temporal scanning mechanism, it strikes a good balance between the model complexity and recognition performance.\n\u2022 We train and report more than 20 classification models on the proposed CeleX-HAR dataset, which provides a good platform for subsequent works to compare. Extensive experiments conducted on CeleX-HAR and multiple other widely used benchmark datasets fully demonstrate the effectiveness of the proposed model."}, {"title": "2. Related Work", "content": "In this section, we review the most related research topics to our paper, including Event-based Recognition and the State Space Model. More related works can be found in surveys [36, 69, 88, 92].\n2.1. Event-based Recognition\nCurrent works can be divided into three streams for the event-based recognition, including the CNN based [81], spiking neural network (SNN) based [16, 19], graph neural network (GNN) based models [5, 6], due to the flexible representation of event stream. For the CNN based models, Wang et al. [81] propose to identify human gaits using an event camera and design a CNN model for recognition. As the third generation of neural networks, the SNN is also adopted to encode the event stream for energy-efficient recognition. To be specific, Peter et al. [12] propose the weight and threshold balancing method to achieve efficient ANN-to-SNN conversion. Nicolas et al. [59] propose a sparse backpropagation method for SNNs and achieve faster and more memory efficiency. Zhou et al. [90] propose Ex-ACT, a novel approach to event based action recognition through cross-modal conceptualization, along with an adaptive fine-grained event representation and uncertainty estimation module based on conceptual reasoning. Gao et al. [23] proposes an event camera based behavior recognition framework EV-ACT, which integrates multiple event information through Learning Multi Fused Representation (LMFR). Gao et al. [24] propose a multi-view event camera action recognition framework HyperMV based on hypergraphs, which captures the relationship between cross-view and temporal features by constructing a multi-view hypergraph neural network.\nFor the point cloud based representation, Wang et al. [76] treat the event stream as space-time event clouds and adopt PointNet [61] as their backbone for gesture recognition. Sai et al. [73] propose the event variational auto-encoder (eVAE) to achieve compact representation learning from the asynchronous event points directly. Fang et al. [18] propose SEW (spike-element-wise) residual learning for deep SNNs which addresses the vanishing/exploding gradient problems effectively. Meng et al. [53] propose an accurate and low latency SNN based on the Differentiation on Spike Representation (DSR) method. TORE [3] is short for Time-Ordered Recent Event (TORE) volumes, which compactly stores raw spike timing information. VMV-GCN [85] is proposed by Xie et al. which is a voxel-wise graph learning model to fuse multi-view volumetric. Li et al. [42] introduce the Transformer network to learn event-based representation in a native vectorized tensor way. Different from these works, in this paper, we design a novel event spatial-temporal scanning mechanism to adapt the VMamba network for the action recognition task effectively and efficiently.\n2.2. Human Action Recognition\nHuman action recognition(HAR) has been a popular task in the past decade due to its practicality in the real world. It is a fundamental task that recognizes a human action from a video containing complete action execution. In deep learning, CNN is widely used in computer vision, including action recognition. Wang et al. [75] proposes an effective method to encode the spatiotemporal information into color distributions in three two-dimensional images, learning discriminative features for human action recognition through CNN. Soo et al. [65] propose a new model for 3D human motion recognition based on a Time Convolutional Neural Network (TCN). This model can interpret 3D bones and easily understand spatiotemporal representations. Li et al. [38] utilize CNN to process the spatiotemporal information of skeleton sequences, significantly improving the effectiveness of human action and interaction recognition in single and cross-view scenarios. Sudhakaran et al.[67] propose a novel spatiotemporal feature extraction module called Gate Shift Use (GSF) to improve the performance of 3D CNN in video action recognition.\nWith the emergence of Transformer networks in the past few years, many Transformer based HAR methods have emerged. Wang et al. [80] introduce a new framework for human activity recognition based on dynamic visual sensors. The event stream is mapped to spatiotemporal embedding through StemNet, and a Transformer network is used to encode and decode dual-view representations. Li et al. [39] propose a new pattern recognition framework that integrates RGB/Event features and semantic features using a multimodal Transformer network, thereby solving the semantic gap and small-scale backbone network problems in existing methods. Wang et al. [78] propose an action recognition method that integrates RGB frames and event stream, called SSTFormer. By using a memory support Transformer network for RGB frame encoding, and a spiking neural network for raw event stream encoding, the current problems in event camera pattern recognition are solved. Wang et al. [74] improves the extraction and compression model of spatiotemporal feature semantics by training a generative model based attention module in the feature representation stage.\nIn addition to the work mentioned above, a great deal of excellent works have also been proposed in recent years. Chen et al. [11] propose a method for optimizing spatiotemporal descriptors to improve text knowledge and promote general video recognition. It's innovation lies in enhancing action category names through a large language model. Zheng et al. [89] propose a human action recognition method that achieves spatiotemporal fusion through joint trajectory maps. This method can better capture rich spatiotemporal dependencies and achieve excellent performance. Kahatapitiya et al. [32] propose a new video text modeling method, VicTR, which can achieve better performance in video text models by combining video and text information to generate \u201cvideo conditional text\u201d embeddings. Different from these works, in this paper, we design a novel SSM based human action recognition method to learn the visual representations effectively.\n2.3. State Space Model\nState Space Model (SSM) [33] is the recently popular model, which is used to achieve state space transformation. To improve the long-range modeling capability of the models, Gu et al. [28] introduce a new deep learning model that combines the advantages of recurrent neural networks (RNNs), time convolution network, and neural differential equations (NDEs) in the form of linear state space layers (LSSL). LSSL maps sequences by simulating linear continuous time state space representations, theoretically proving its close relationship with the above three models and inheriting their advantages. Subsequently, Gu et al. [27] proposes structured state space sequence models (S4) for deep learning, as a novel alternative to CNNs or Transformers. Based on the new State Space Model(SSM) parameterization, S4 can efficiently model long-range dependencies in long sequences, and it solves the computational and memory requirements encountered by existing methods when processing long sequences.\nBuilding on these works, many researchers are beginning to explore the potential of SSM. Islam et al. [30] uses S4 as a decoder to model long-range temporal interactions in movie clips. Nguyen et al. [55] further extends 1-dimensional sequence modeling to 2-dimensional and 3-dimensional models, such as images and videos. Subse-"}, {"title": "3. Methodology", "content": "In this section, we will introduce how to effectively explore the task of event-based human action recognition using a Mamba-based framework. Firstly, we provide an overview to summarize the framework proposed in this work. Next, we introduce the effective representations of event data in this work. Finally, we analyze the detailed network architectures in our framework for event-based human action recognition.\n3.1. Overview\nTo explore the effectiveness of the Mamba network in event-based human action recognition tasks, in this work, we propose a novel action recognition framework based on a spatial-temporal scanning Mamba architecture. As shown in Fig. 2, we input various forms of event data representations (event image and event voxel) into the network to fully utilize the effective information of the event data. Specifically, we propose a voxel temporal scanning mechanism to process the voxel grids by rearranging them in chronological order, which effectively integrates the dense temporal information of each event trajectory. Subsequently, we used VMamba [49] as our backbone network to extract robust feature representations for both the spatial and the temporal. Finally, we add the features of the two perspectives and send them to the classification head for efficient action classification. More details will be introduced in the subsequent paragraphs."}, {"title": "3.2. Input Representation", "content": "Owing to the rich temporal information inherent in event data, our approach involves integrating data from multi-view to harness the full potential of event data's informative representation. We consider an event stream denoted as E \u2208 RW\u00d7H\u00d7T = {e1, e2, ..., eN}, each ei = [x, y, t, p] represents an individual event point triggered asynchronously, (x, y) denotes the spatial coordinates, t is the timestamp, p is the polarity, with i ranging from 1 to N, and N signifying the total count of event points within the current sample. H, W, T denotes the height, width, and overall time step. Utilizing the event stream E, one can effectively compile them into event frames EF \u2208 RT'\u00d7C\u00d7H\u00d7W, T' denotes the number of event frames, C is the channel of each frame, thereby leveraging the full potential of pre-existing deep network architectures (e.g., ViT [14], Mamba [26]). Specifically, event images are generated by segmenting the event stream into a series of clips, each encapsulating a fixed time interval. This approach is effective for event-based recognition [79, 80], however, the rich potential of the dense temporal dimension is seldom fully exploited.\nIn this work, we also investigate the use of the event voxel to further enhance event representation which can effectively preserve the temporal information of the event stream. Specifically, we dissect the event stream E \u2208 RW\u00d7H\u00d7T into multiple cubic voxels Ev \u2208 Ra\u00d7b\u00d7c, each of which may contain several event points. Thus, we have W\u00d7\u00d7 voxel grids for event stream E. In the following sub-sections, we will further explore how to obtain the voxel tokens using the temporal voxel scanning mechanism for our framework."}, {"title": "3.3. Network Architecture", "content": "As shown in Fig. 2 (the top sub-figure), our event-based HAR framework is built based on the Mamba network which takes the event frames and voxels as the input. We will first introduce the regular Mamba network for event frame-based action recognition. Then, we will introduce the temporal voxel scan mechanism to augment the HAR framework further.\n\u2022 Mamba based Visual Backbone. Given the stacked event frames EF \u2208 RT'\u00d7C\u00d7H\u00d7W, we first partition each image into patches and re-formulate them using spatial cross-scan employed in VMamba network. Then, a patch embedding layer is adopted to transform them into token representations before feeding into the VMamba backbone network. A four-stage VSS block is adopted to construct the VMamba backbone. Specifically, Mamba [26] maps a 1-dimensional function or sequence x(t) \u2208 R \u2192 y(t) \u2208 R through a hidden state h(t) \u2208 RN. Just like the recurrent neural network (RNN), Mamba usually inputs the previous state and the current input into the network to obtain the current output. This continuous system can be defined as,\nh'(t) = Ah(t) + Bx(t), (1)\ny(t) = Ch(t) + Dx(t).\nwhere A \u2208 RN\u00d7N is the state matrix, B \u2208 RN\u00d7L is the input matrix, C\u2208 RL\u00d7N is the output matrix, and De IRLXL is the feed-through matrix. x(t) \u2208 R\u0139 and h'(t) \u2208 RN are the current input sequence and the derivative of the hidden state. Finally, the y(t) can be obtained as the current output. For more detailed information, including details of discretization, please refer to the supplementary materials or Mamba [26].\nFor each Mamba layer, we first input image data for feature learning from a spatial view. Fig. 2 (c) shows the detailed structure of the VMamba backbone blocks. Firstly, we take X\u00bf \u2208 RB\u00d7H\u00d7W\u00d7C, i \u2208 {image, voxel} as input (voxel will be introduced in the next sub-section), where B is the batch size, H and W denote height and width, C is the dimension of feature embedding. After passing through a linear normalization (LN) layer, the input X\u2081 is divided into two parts, xi and zi, according to the last dimension. Subsequently, we take xi as the input of the main branch, and after the linear layer, DW convolution, and SiLU activation function, it is sent to the SS2D module. The formulas can be described as,\nx'; = SS2D(SiLU(DW (Linear(xi)))). (2)\nThe SS2D module uses a cross-selective scanning mechanism to scan image patches with four-way repeatedly. For detailed information on four-way scanning, please refer to VMamba [49]. Next, we merged the four different orders of patches to the original 2D feature map through the cross-merge operation. Then, we multiply the x obtained from the SS2D module and the linear layer by zi, which also through a linear layer and SiLU activation function. Finally, we add the result of this multiplication to the initial input Xi to produce the final output result of the current block. These processes can be expressed as,\nX = Linear(LN(x) \u00d7 SiLU(Linear(zi))) + X\u2081. (3)\nAfter that, we input X into the block of the next layer. Through the above steps, we stacked L layers of S6 blocks for effective feature extraction.\n\u2022 Voxel Temporal Scanning Mechanism. The aforementioned spatial cross-scan used in Mamba captures the spatial information well, but we believe the temporal information can also be further enhanced. In this work, we propose a temporal voxel scan mechanism to augment the frame-based Mamba network for event-based human action recognition. As shown in Fig. 2 (b), given the event stream E, we first divide it into multiple clips (i.e., blue, orange, green blocks) and extract the voxel representation for each clip. Note that, we removed voxels that hardly contain any events to save on computational costs. Let's take the i-th blue event voxel Ev as an example, we search its trajectory in the next clip (i.e., the orange one) by measuring its cosine similarity (CoSim):\nargmax(CoSim(Ev, E)), k \u2208 {1, 2, ..., M}, (4)\nwhere M is the maximum number of informative event voxels in the orange clip. The orange voxel with the highest similarity will be selected, and similar operations will be conducted for other event voxels to construct the event voxel sequence [EV, E, ..., EV]. We propose a voxel embedding layer to transform them into event tokens before feeding into the VMamba network.\n\u2022 Classification Head. After passing through the S6 blocks, we can obtain the output features from both the spatial and temporal views. Then, we repeat the features of the voxel into the same shape as the image features and fuse the features of the two branches through simple addition. Subsequently, we input the fusion features to the classification head after passing through an average pooling layer. Our classification head utilizes a simple linear layer to map feature dimensions and obtain predicted scores for each action category. Finally, we calculate the cross entropy loss between the predicted results and the ground truth labels, which can be formulated as:\nL =  \u2211 Yi log \u0177i + (1 - yi) log(1 \u2013 \u0177i). (5)\nwhere y is the predicted scores and yi is the true label."}, {"title": "4. CeleX-HAR Benchmark Dataset", "content": "In this section, we first introduce the protocols we followed when collecting the event videos. Then, we will focus on statistical analysis and baselines we build for future works to compare.\n4.1. Protocols\nWhen collecting our CeleX-HAR dataset, we obey the following standards: 1). Multi-view: We record each action from different views, including front, left, right, down, and up views. 2). Various illumination: Our dataset also contains videos with different illumination conditions, including low-, middle-, and full-illumination. Note that, the low-illumination videos account for about 40% for each class. 3). Camera motions: The movement of the event camera affects the imaging quality greatly. As the camera moves, a great deal of background information is recorded; when the camera is still, only the moving subject is recorded. Note that half of the videos in each class are recorded using a moving event camera. 4). Video length: Usually, an action lasts about 2-3 seconds in our dataset; but some videos are larger than 5 if it takes longer to complete. 5). Speed of action: The video of the three kinds of speeds (i.e., low-, medium-, and high-speed) occupies the proportion of 30%, 40%, 30%, respectively. 6). Occlusion: In a certain movement, there can be partial occlusion of the interference. 7). Glitter: As the event camera is sensitive to glitter, in our dataset collection, we also record videos with conspicuous flash. 8). Capture distance: These videos are approximately 1-2 meters, 2-3 meters, and larger than 3 meters away from the target object. 9). Large-scale & High-definition: 124,625 high-definition (1280\u00d7800) event video sequences (the largest real-event HAR dataset) are recorded which cover 150 classes of human activities.\n4.2. Statistical Analysis and Baselines\nOur dataset comprises 150 categories of common human behavioral actions, as illustrated in Fig. 3 (a), a total of 124,625 videos are collected using a CeleX-V event camera and the training and testing subset contains 99,642 and 24,983 videos respectively. As visualized in Fig. 3 (b), we compare the number of categories (Y-axis values) and the number of videos (bubble size) between the CeleX-HAR dataset and other datasets.\nAs shown in Table 7, we also establish benchmarks for a variety of recognition models on our dataset, which provides a wide baseline for future comparisons on the CeleX-HAR dataset, including: 1). CNN based models (ResNet50 [29], ConvLSTM [63], C3D [70], R3Plus1D [72], TSM [44], ACTION-Net [83], TAM [50], GSF [67]), 2). Transformer based models (Video-SwinTrans [51], TimeSformer [4], SlowFast [21], SV-Former [86], EFV++ [8], ESTF [80]), 3). RWKV based models (VRWKV [15]) and 4). Mamba based models (Vision Mamba [93], VMamba [49], Video Mamba [41])."}, {"title": "5. Experiments", "content": "5.1. Dataset and Evaluation Metric\nIn this paper, our experiments are conducted on the ASL-DVS [6], N-Caltech101 [56], DVS128-Gait-Day [82], Bully10K [13], Dailydvs-200 [77], and our newly proposed CeleX-HAR dataset. More detailed introductions to these datasets can be found in our Supplementary Materials. The top-1 accuracy is adopted for the evaluation of our proposed model and other SOTA action recognition algorithms.\n5.2. Implementation Details\nOur proposed framework can be optimized in an end-to-end manner. The learning rate and weight decay are set as 0.001 and 0.0001, respectively. The SGD is selected as the optimizer and trained for a total of 30 epochs. In our implementations, a total of 33 S6 blocks are stacked as our backbone network like VMamba-B [49]. We rich the temporal information of the input through the implementation of a novel temporal scanning strategy. Besides, we select 8 event frames as the input of the event images, as other benchmarked baselines. Our code is implemented using Python based on PyTorch [57] framework and the experiments are conducted on a server with CPU Intel(R) Xeon(R) Gold 5318Y CPU @2.10GHz and GPU RTX3090s. More details can be found in our source code.\n5.3. Comparison with Other SOTA Algorithms\n\u2022 Results on ASL-DVS [6] Dataset. As shown in Table 2, our proposed method achieves 99.9% (top-1 accuracy) on the ASL-DVS dataset. The compared method M-LSTM which adopts learnable event representation is still inferior to our method. Some graph-based event recognition models are also worse than ours, including EV-VGCNN, VMV-GCN, and Ev-Gait-3DGraph. Therefore, we can conclude that our proposed model is more effective for event-based HAR.\n\u2022 Results on N-Caltech101 [56] Dataset. As shown in Table 3, our model achieves 93.9% (top-1 accuracy) on this benchmark dataset which is significantly better than the compared methods. To be specific, our model outperforms\n5.4. Component Analysis\nTo verify the effectiveness of our proposed method, as shown in Table 7, we compared our algorithm EVMamba with the version without incorporating voxel temporal scanning. To further enhance the robustness of the Mamba model, we study how to bring dense temporal information to the input based on the image spatial scanning. Ultimately, our results were further improved with the voxel temporal scanning mechanism, achieving a top-1 accuracy of 72.3%. Thus, we believe that the Mamba model is a good backbone replacement solution for event-based human action recognition. Meanwhile, our voxel temporal scanning strategy can further improve the accuracy of the model.\n5.5. Ablation Study\n\u2022 Analysis of Different Event Representations for HAR. In our experiments, we investigate different event representations for the human action recognition task. As shown in Table 8, we can see that considering only the event voxel (acc/top-1, 0.451) as input would result in a significant discrepancy in the outcome compared to the event frames (acc/top-1, 0.720). Despite event voxel's commendable spatio-temporal information integration, their omission of the intrinsic polarity of events hampers a comprehensive feature portrayal. In contrast, we opted to incorporate the event voxels' temporal information into the event frames and achieve outstanding performance.\n\u2022 Analysis of Other Exploited Fusion Methods. Since our model's input is in the form of dual branch data (event image and voxel), we need to design a suitable scheme to integrate dense voxel temporal information into the images. Specifically, through careful study of the Mamba model, we propose a matrix interaction method as shown in Fig. 5. We interact with the input matrix of the image branch and the voxel branch, namely the B matrix, in a gate-controlled manner to learn complementary information between the two branches. However, as shown in Table 8, it is disappointing that this fusion method will result in a decrease in accuracy. The potential reason for this may be that the matrix interaction method breaks the original information and brings noise to the output state. Surprisingly, the simplest addition can bring the best results.\n\u2022 Analysis of the Threshold & of Minimal Length of Event Temporal Voxels. When conducting the temporal event voxel scanning, we set a threshold & to check the influence of trajectory length for the final recognition performance. In other words, we filter out the trajectories whose length is less than the threshold 8. Specifically, the threshold & is set as 1, 3, and 9, as shown in Table 8, we can find that the best results can be achieved when threshold \u03b4 is 1. That is to say, it will be better when all our temporal event voxels are used. This experiment fully demonstrate the effectiveness of event voxels obtained using our temporal scan mechanism.\n\u2022 Analysis of Number of Input Frames. To check the influence of the input event frames, in this experiment, we input 4, 8, and 12 event frames into the VMamba backbone network, and the corresponding results are 69.8%, 72.3%, and 70.8% respectively. A better result can be obtained when eight frames are used. Fewer video frames can result in the model being unable to learn more effective video information, leading to underfitting. An excessive number of frames might lead to model complexity, making training more difficult and potentially increasing the risk of overfitting. This phenomenon is also referred to as information overload.\n\u2022 Analysis of Input Resolution of Event Stream. As this"}, {"title": "5.7. Limitation Analysis", "content": "Although our model can achieve better accuracy compared to dense Transformer models and has advantages in efficiency, processing high-resolution event data directly still poses computational cost issues. The training and inference of the model still require high-end GPUs to complete. Therefore, in future work, we will consider techniques such as model distillation and quantization to further reduce the inference cost. Due to the limitation of GPU memory, we are unable to complete the model training and testing on the original high-definition (1280 \u00d7 800) event stream data at this stage. In future work, we will design new deep neural network models for efficient high-definition data perception."}, {"title": "6. Conclusion", "content": "This paper has made significant contributions to the field of Human Action Recognition (HAR) using event cameras. We have introduced CeleX-HAR, a large-scale, high-definition event-based HAR dataset that addresses the limitations of existing low-resolution datasets. With 150 action categories and 124,625 video sequences, CeleX-HAR considers various challenging factors such as multi-view, illumination, action speed, and occlusion, providing a comprehensive benchmark for future research. Additionally, we have proposed EVMamba, a novel event stream-based HAR model that leverages a Mamba vision backbone with innovative spatio-temporal scanning mechanisms. EVMamba has demonstrated promising performance across multiple datasets, further advancing the state-of-the-art in event-based HAR. With the release of both the dataset and source code upon acceptance, we anticipate that this work will serve as a valuable resource and foundation for future developments in the field of event-based HAR and computer vision. In our future works, we will focus on further reducing the model complexity to handle higher-resolution event data."}]}