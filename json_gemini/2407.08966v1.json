{"title": "LAPT: Label-driven Automated Prompt Tuning\nfor OOD Detection with Vision-Language Models", "authors": ["Yabin Zhang", "Wenjie Zhu", "Chenhang He", "Lei Zhang"], "abstract": "Out-of-distribution (OOD) detection is crucial for model reliability, as it identifies samples from unknown classes and reduces errors due to unexpected inputs. Vision-Language Models (VLMs) such as CLIP are emerging as powerful tools for OOD detection by integrating multi-modal information. However, the practical application of such systems is challenged by manual prompt engineering, which demands domain expertise and is sensitive to linguistic nuances. In this paper, we introduce Label-driven Automated Prompt Tuning (LAPT), a novel approach to OOD detection that reduces the need for manual prompt engineering. We develop distribution-aware prompts with in-distribution (ID) class names and negative labels mined automatically. Training samples linked to these class labels are collected autonomously via image synthesis and retrieval methods, allowing for prompt learning without manual effort. We utilize a simple cross-entropy loss for prompt optimization, with cross-modal and cross-distribution mixing strategies to reduce image noise and explore the intermediate space between distributions, respectively. The LAPT framework operates autonomously, requiring only ID class names as input and eliminating the need for manual intervention. With extensive experiments, LAPT consistently outperforms manually crafted prompts, setting a new standard for OOD detection. Moreover, LAPT not only enhances the distinction between ID and OOD samples, but also improves the ID classification accuracy and strengthens the generalization robustness to covariate shifts, resulting in outstanding performance in challenging full-spectrum OOD detection tasks.", "sections": [{"title": "1 Introduction", "content": "In real-world applications, artificial intelligence (AI) systems often encounter\ndata of unknown classes, known as out-of-distribution (OOD) data. When tack-\nling such OOD data, AI models may erroneously remain overconfident in their"}, {"title": "2 Related Work", "content": "Traditional OOD detection aims to address the severe issue of deep learn-\ning models in producing overconfident predictions on OOD data [33,59], which\nposes significant challenges to the deployment of deep models in real-world ap-\nplications. To tackle this problem, researchers have developed various OOD de-\ntection methods, including score-based [15, 19, 24\u201326, 45, 52, 54, 55], distance-\nbased [6, 31, 44, 46-48] and generative-based [22, 40] approaches. Score-based\nmethods, which arguably garner the most attentions, distinguish ID from OOD\nsamples using carefully designed scoring functions, such as confidence-based\n[15, 25, 45, 52, 55], discriminator-based [22], energy-based [26,54], and gradient-\nbased [18] scores. In contrast, distance-based methods detect OOD samples by\nmeasuring the distance in feature space between the test data and their nearest"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Problem Setup", "content": "Let X and Y = {y1,...,yc} be the image space and ID label space, where y is\na set of class names, e.g., Y = {cat, dog,..., bird} and C is the number of ID\nclasses. Given the ID random variable xin \u2208 X and the OOD random variable\nxood \u2208 X, we use Prin and Prood to denote the ID marginal distribution and\nOOD marginal distribution, respectively.\nOOD detection. In traditional classification tasks, we assume that the test\nimage x is sampled from the ID distribution and belongs to a specific ID class,\ni.e., x \u2208 Pain and y \u2208 y, where y is the label of x. However, in real-world\napplications, AI systems often encounter samples from unknown classes, i.e.,\nx \u2208 Prood and y & Y. AI models may misclassify these data into known ID classes\nwith high confidence [33, 41], posing critical errors and security risks. To tackle"}, {"title": "3.2 Reviews on MCM and NegLabel", "content": "CLIP-like models. Given the ID test image x within the label space Y, we\nextract the image representation v = fimg(x) \u2208 RD and the textual represen-\ntation C = ftxt(p(V)) \u2208 RC\u00d7D with the pre-trained CLIP-like encoders, where\nD denotes the feature dimension. fimg(\u00b7) and ftxt(\u00b7) denote the image and text\nencoders, respectively. p(.) denotes the text prompt function, which is typically\nmanually defined as 'a photo of a <label>.', where the 'label' token is replaced\nby the specific class name, e.g., 'cat' or 'dog'. Both v and Care L2 normal-\nized along the D dimension. Then the zero-shot classification probability can be\nachieved using Cas the classifier:\np = Softmax(vCT) \u2208 RC,\nwhere the fixed scaling parameter is omitted for simplicity.\nMCM. Besides the remarkable classification capabilities, MCM [29] reveals\nthat pre-trained CLIP models also exhibit strong zero-shot OOD detection abil-\nities. Specifically, MCM treats textual representations C as ID prototypes and\nassesses OOD uncertainty based on the scaled cosine distance between the visual\nfeatures and the nearest ID prototype. Furthermore, it has been validated that\napplying a softmax function as a post hoc mechanism enlarges the distinction\nbetween ID and OOD samples, leading to the following score function:\nSmcm(v) = maxi=1Cecos(v,ci)/\u03c4\u03a3ecos(v,ci)/\u03c4,\nwhere cos(,) measures the cosine similarity, c\u2081 is the i-th entry of C, and \u315c > 0\nis the scaling temperature.\nNegLabel. While it is validated in [29] that leveraging the textual informa-\ntion of ID classes enhances the OOD detection capability, the potential contribu-\ntion of negative labels is overlooked. To fill this gap, NegLabel [21] explores neg-\native class names Y\u00af = {yc+1,...,YC+M} from extensive text corpora, where\n\u05d7-\u05e2 Y = 0. Under the assumption that ID samples are more similar to ID\nlabels and less similar to negative labels than OOD samples, NegLabel employs\na scoring function to improve OOD detection:\nSNegLabel(v) =\u03a3i=1Cecos(v,ci)/\u03c4\u03a3i=1Cecos(v,ci)/\u03c4 + \u03a3j=C+1C+Mecos(v,cj)/\u03c4,"}, {"title": "3.3 LAPT: Label-driven Automated Prompt Tuning", "content": "Though NegLabel has achieved a notable success, its performance is highly sen-\nsitive to the configuration of text prompts. As illustrated in Fig. 1, given an ID\ndataset like ImageNet, varying text prompts leads to considerable differences in\nOOD detection outcomes. For instance, prompts such as 'The nice <label>' gen-\nerally improve performance on far-OOD datasets, while 'a photo of a <label>'\nshows enhanced effectiveness for near-OOD scenarios. The choice of prompt can\nlead to over 10% fluctuation in the FPR95 metric. Hence, manually setting text\nprompts for optimal results is a labor-intensive and time-consuming process.\nWe aim to streamline OOD detection by automating prompt generation with\nminimal manual intervention. Our method learns continuous prompts using auto-\ncollected samples, needing only ID class names. The automated process is very\nuseful in practical applications. Details of LAPT are provided as follows.\nDistribution-aware prompts. Given the ID class names, we first follow NegLabel\n[21] to mine the negative labels Y\u00af from extensive corpus databases. Then, in-\nstead of hand-crafting the optimal text prompts, we design a learnable text\nprompt function in the following form:\n\u03c1\u03b9 (LABEL) = [V]1[V]2\uff65\uff65\uff65 [V]N[LABEL],\nwhere [V]n \u2208 RD(n \u2208 {1,2\u2026\u2026, N}) represents the learnable context tokens,\nand N is the total number of context tokens."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Setup", "content": "Datasets and benchmarks. Our validation primarily uses the ImageNet-1k\ndataset [4] as ID data, and more evaluations on smaller-scaled ID datasets are\nprovided in the Supplementary Materials. Aligning with standards from prior\nresearch [19,21,29], we use four diverse datasets of iNaturalist [49], SUN [56],\nPlaces [71], and Textures [3] as OOD test sets. Additionally, we assess our method\non the OpenOOD benchmark [58, 65], which differentiates OOD datasets into\nnear-OOD (e.g., SSB-hard [50], NINCO [2]) and far-OOD (e.g., iNaturalist [49],\nTextures [3], OpenImage-O [52]) based on semantic similarity or difficulty. This\ngrouping allows for a comprehensive evaluation of OOD detectors against various\nOOD sample types.\nIn addition to conventional OOD detection that solely considers semantic\nshifts, we consider a full-spectrum OOD detection problem [60], which addition-\nally accounts for non-semantic covariate shift generalization. An optimal system\nshould not only be capable of identifying semantically shifted OOD samples but\nshould also exhibit robustness to non-semantic covariate shifted OOD samples."}, {"title": "4.2 Main Results", "content": "Results on four OOD datasets. As illustrated in Tab. 1, our approach con-\nsistently surpasses existing methods. Specifically, we re-implemented traditional\nmethods [7, 15, 18, 25, 26, 46, 48, 52] by fine-tuning CLIP-encoders with labeled\ntraining samples from ImageNet in accordance with [48]. Without utilizing any\nmanually annotated data, our method significantly outperforms these methods,\ncorroborating the effectiveness of our proposed VLMs adaptation technique. No-\ntably, our approach exceeds the performance of NegLabel [21], confirming the\nsuperiority of learnable prompts over hand-crafted prompts. The OOD samples\nin these datasets have a relatively large semantic difference from the ID data"}, {"title": "4.3 Analyses and Discussions", "content": "The analyses are conducted on the OpenOOD dataset with a VITB/16 encoder.\nPrompt construction. We conduct comprehensive analyses on prompt\nconstruction as demonstrated in Fig. 4. As shown in Fig. 4a, our proposed\ndistribution-aware prompts are well-matched for OOD detection, outperform-\ning both class-specific and unified prompts. Excessively long prompts slightly\nimpair experimental outcomes, as evidenced in Fig. 4b, possibly due to the in-\ncreased capacity overfitting noise presented in the training images. Consequently,"}, {"title": "5 Conclusion and Limitations", "content": "We developed a novel label-driven automated prompt tuning (LAPT) approach\nthat autonomously configures effective prompts for OOD detection. Requiring\nonly ID class names, our method can automatically mine OOD class names, col-\nlect training images per class, and learn effective prompts, thereby outperforming\nmanually crafted prompts with minimal manual intervention. The automatically\nlearned prompts not only enhanced the distinction between ID and OOD samples\nbut also improved the ID classification accuracy and the strengthened general-\nization robustness to covariate shifts.\nThe efficacy of our approach was highly dependent on the quality of collected\ntraining samples. Employing more advanced generation techniques to create a\ndiverse set of synthetic images [8] or expanding the retrieval space [43] to obtain\nreal images that match better the class names could yield improved results.\nThese possibilities present exciting avenues for future research."}]}