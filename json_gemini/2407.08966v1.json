{"title": "LAPT: Label-driven Automated Prompt Tuning\nfor OOD Detection with Vision-Language Models", "authors": ["Yabin Zhang", "Wenjie Zhu", "Chenhang He", "Lei Zhang"], "abstract": "Out-of-distribution (OOD) detection is crucial for model re-\nliability, as it identifies samples from unknown classes and reduces er-\nrors due to unexpected inputs. Vision-Language Models (VLMs) such\nas CLIP are emerging as powerful tools for OOD detection by integrat-\ning multi-modal information. However, the practical application of such\nsystems is challenged by manual prompt engineering, which demands\ndomain expertise and is sensitive to linguistic nuances. In this paper,\nwe introduce Label-driven Automated Prompt Tuning (LAPT), a novel\napproach to OOD detection that reduces the need for manual prompt\nengineering. We develop distribution-aware prompts with in-distribution\n(ID) class names and negative labels mined automatically. Training sam-\nples linked to these class labels are collected autonomously via image\nsynthesis and retrieval methods, allowing for prompt learning without\nmanual effort. We utilize a simple cross-entropy loss for prompt optimiza-\ntion, with cross-modal and cross-distribution mixing strategies to reduce\nimage noise and explore the intermediate space between distributions, re-\nspectively. The LAPT framework operates autonomously, requiring only\nID class names as input and eliminating the need for manual intervention.\nWith extensive experiments, LAPT consistently outperforms manually\ncrafted prompts, setting a new standard for OOD detection. Moreover,\nLAPT not only enhances the distinction between ID and OOD samples,\nbut also improves the ID classification accuracy and strengthens the gen-\neralization robustness to covariate shifts, resulting in outstanding per-\nformance in challenging full-spectrum OOD detection tasks. Codes are\navailable at https://github.com/YBZh/LAPT.", "sections": [{"title": "1 Introduction", "content": "In real-world applications, artificial intelligence (AI) systems often encounter\ndata of unknown classes, known as out-of-distribution (OOD) data. When tack-\nling such OOD data, AI models may erroneously remain overconfident in their"}, {"title": "2 Related Work", "content": "Traditional OOD detection aims to address the severe issue of deep learn-\ning models in producing overconfident predictions on OOD data [33,59], which\nposes significant challenges to the deployment of deep models in real-world ap-\nplications. To tackle this problem, researchers have developed various OOD de-\ntection methods, including score-based [15, 19, 24\u201326, 45, 52, 54, 55], distance-\nbased [6, 31, 44, 46-48] and generative-based [22, 40] approaches. Score-based\nmethods, which arguably garner the most attentions, distinguish ID from OOD\nsamples using carefully designed scoring functions, such as confidence-based\n[15, 25, 45, 52, 55], discriminator-based [22], energy-based [26,54], and gradient-\nbased [18] scores. In contrast, distance-based methods detect OOD samples by\nmeasuring the distance in feature space between the test data and their nearest"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Problem Setup", "content": "Let \\( \\mathcal{X} \\) and \\( \\mathcal{Y} = \\{y_1,...,y_C\\} \\) be the image space and ID label space, where \\( y \\) is\na set of class names, e.g., \\( \\mathcal{Y} = \\{\\text{cat, dog,..., bird}\\}\\) and C is the number of ID\nclasses. Given the ID random variable \\( x_{\\text{in}} \\in \\mathcal{X} \\) and the OOD random variable\n\\( x_{\\text{ood}} \\in \\mathcal{X} \\), we use \\( P_{\\text{in}} \\) and \\( P_{\\text{ood}} \\) to denote the ID marginal distribution and\nOOD marginal distribution, respectively.\nOOD detection. In traditional classification tasks, we assume that the test\nimage x is sampled from the ID distribution and belongs to a specific ID class,\ni.e., \\( x \\in P_{\\text{in}} \\) and \\( y \\in \\mathcal{Y} \\), where y is the label of x. However, in real-world\napplications, AI systems often encounter samples from unknown classes, i.e.,\n\\( x \\in P_{\\text{ood}} \\) and \\( y \\notin \\mathcal{Y} \\). AI models may misclassify these data into known ID classes\nwith high confidence [33, 41], posing critical errors and security risks. To tackle"}, {"title": "3.2 Reviews on MCM and NegLabel", "content": "CLIP-like models. Given the ID test image x within the label space \\( \\mathcal{Y} \\), we\nextract the image representation \\( v = f_{\\text{img}}(x) \\in \\mathbb{R}^D \\) and the textual represen-\ntation \\( C = f_{\\text{txt}}(p(\\mathcal{Y})) \\in \\mathbb{R}^{C \\times D} \\) with the pre-trained CLIP-like encoders, where\nD denotes the feature dimension. \\( f_{\\text{img}}(\\cdot) \\) and \\( f_{\\text{txt}}(\\cdot) \\) denote the image and text\nencoders, respectively. p(.) denotes the text prompt function, which is typically\nmanually defined as 'a photo of a <label>.', where the 'label' token is replaced\nby the specific class name, e.g., 'cat' or 'dog'. Both v and C are \\( L_2 \\) normal-\nized along the D dimension. Then the zero-shot classification probability can be\nachieved using C as the classifier:\n\\[\np = \\text{Softmax}(vC^T) \\in \\mathbb{R}^C,\n\\]\nwhere the fixed scaling parameter is omitted for simplicity.\nMCM. Besides the remarkable classification capabilities, MCM [29] reveals\nthat pre-trained CLIP models also exhibit strong zero-shot OOD detection abil-\nities. Specifically, MCM treats textual representations C as ID prototypes and\nassesses OOD uncertainty based on the scaled cosine distance between the visual\nfeatures and the nearest ID prototype. Furthermore, it has been validated that\napplying a softmax function as a post hoc mechanism enlarges the distinction\nbetween ID and OOD samples, leading to the following score function:\n\\[\nS_{\\text{mcm}}(v) = \\max_{i=1}^{C} \\frac{\\text{e}^{\\text{cos}(v, c_i)/\\tau}}{\\sum_{i=1}^{C} \\text{e}^{\\text{cos}(v, c_i)/\\tau}},\n\\]\nwhere cos(,) measures the cosine similarity, \\( c_i \\) is the i-th entry of C, and \\( \\tau > 0 \\)\nis the scaling temperature.\nNegLabel. While it is validated in [29] that leveraging the textual informa-\ntion of ID classes enhances the OOD detection capability, the potential contribu-\ntion of negative labels is overlooked. To fill this gap, NegLabel [21] explores neg-\native class names \\( \\overline{\\mathcal{Y}} = \\{y_{C+1},...,y_{C+M}\\} \\) from extensive text corpora, where\n\\( \\mathcal{Y} \\cap \\overline{\\mathcal{Y}} = \\emptyset \\). Under the assumption that ID samples are more similar to ID\nlabels and less similar to negative labels than OOD samples, NegLabel employs\na scoring function to improve OOD detection:\n\\[\nS_{\\text{NegLabel}}(v) = \\frac{\\sum_{i=1}^{C} \\text{e}^{\\text{cos}(v, c_i)/\\tau}}{\\sum_{i=1}^{C} \\text{e}^{\\text{cos}(v, c_i)/\\tau} + \\sum_{j=C+1}^{C+M} \\text{e}^{\\text{cos}(v, c_j)/\\tau}}.\n\\]"}, {"title": "3.3 LAPT: Label-driven Automated Prompt Tuning", "content": "Though NegLabel has achieved a notable success, its performance is highly sen-\nsitive to the configuration of text prompts. As illustrated in Fig. 1, given an ID\ndataset like ImageNet, varying text prompts leads to considerable differences in\nOOD detection outcomes. For instance, prompts such as 'The nice <label>' gen-\nerally improve performance on far-OOD datasets, while 'a photo of a <label>'\nshows enhanced effectiveness for near-OOD scenarios. The choice of prompt can\nlead to over 10% fluctuation in the FPR95 metric. Hence, manually setting text\nprompts for optimal results is a labor-intensive and time-consuming process.\nWe aim to streamline OOD detection by automating prompt generation with\nminimal manual intervention. Our method learns continuous prompts using auto-\ncollected samples, needing only ID class names. The automated process is very\nuseful in practical applications. Details of LAPT are provided as follows.\nDistribution-aware prompts. Given the ID class names, we first follow NegLabel\n[21] to mine the negative labels \\( \\overline{\\mathcal{Y}} \\) from extensive corpus databases. Then, in-\nstead of hand-crafting the optimal text prompts, we design a learnable text\nprompt function in the following form:\n\\[\n\\rho_\\text{l}(\\text{LABEL}) = [V]_1 [V]_2 \\cdots [V]_N \\text{LABEL},\n\\]\nwhere \\( [V]_n \\in \\mathbb{R}^{D} (n \\in \\{1,2\\ldots, N\\}) \\) represents the learnable context tokens,\nand N is the total number of context tokens."}, {"title": "Automated sample collection with class labels.", "content": "Traditional prompt learn-\ning methods [32,72] often optimize the prompts with manually collected image\nsamples, which are typically selected from well-constructed benchmark datasets.\nSuch a manual sampling approach is inefficient for negative label-based OOD\ndetection, primarily because mined negative classes are often not included in\nexisting datasets. Moreover, manually collecting and annotating these data is\ntime-consuming and labor-intensive. To mitigate the manual effort and facili-\ntate practical application, we introduce automated sample collection strategies\nthat require only class names, as detailed subsequently.\nWe introduce two automated sample collection methods for enhancing OOD\ndetection: text-to-image generation and text-based image retrieval. In the gener-\nation approach, class names are fed into pre-trained text-to-image models [35,39]\nto generate synthetic images with varied appearances using different seeds:\n\\[\n\\mathcal{X}^{\\text{col}} = \\text{T2I}(\\mathcal{Y} \\cup \\overline{\\mathcal{Y}}),\\)\n\\]\nwhere \\( \\mathcal{X}^{\\text{col}} \\) is the collected set of synthetic images and T2I(\u00b7) is the pre-trained\ngeneration model. The performance of OOD detection is closely tied to the fi-\ndelity of these generated images, depending on the generative power of the un-\nderlying model. By harnessing advanced generative models [35,39], we are able\nto produce images that are not only visually convincing but also exhibit strong\nconsistency with the input text. Our empirical findings indicate that these syn-\nthetic images can significantly enhance the OOD detection capabilities."}, {"title": "Prompt tuning with cross-modal and cross-distribution mixing.", "content": "Given\nthe training images collected on a per-class basis, the most straightforward learn-\ning approach is to use a classification objective, which is commonly achieved with\nthe following cross-entropy loss function:\n\\[\n\\mathcal{L} = \\frac{1}{\\lvert \\mathcal{X}^{\\text{col}} \\rvert} \\sum_{x \\in \\mathcal{X}^{\\text{col}}} \\sum_{i=1}^{C+M} l_i \\log \\frac{\\text{e}^{\\text{cos}(v, (f_{\\text{txt}}(\\rho_\\text{l}(y)))/\\tau}}{\\sum_{i=1}^{C+M} \\text{e}^{\\text{cos}(v,cos(f_{\\text{txt}}(\\rho_\\text{l}(y)))/\\tau}} - f_{\\text{img}}(x),\n\\]\nwhere y is the class name of x. \\( l_i \\) is the i-th entry of \\( l \\in \\mathbb{R}^{C+M} \\), the softla-\nbel of sample x. In Sec. 4.3, we will demonstrate that this training objective\naligns effectively with our proposed distribution-aware prompt construction and\nautomated sample collection strategies.\nCross-modal mixing. While the simple classification loss has achieved com-\nmendable results, we could further enhance it by addressing potential sample\nnoise and exploring broader data space through carefully designed data mixing\nstrategies. Despite the efforts to collect clean images that match their respective\nclass names, noise is inevitable in the collected images. To mitigate the impact"}, {"title": "Cross-distribution mixing.", "content": "In addition to using cross-modal data mixing\nto mitigate image noise, we employ the data mixing strategy to explore a broader\nfeature space. In the vanilla NegLabel approach [21], the selected negative classes\nare typically far from the ID distribution, leaving a substantial intermediate area\nbetween ID and negative regions underutilized. To enhance the utilization of\nthese spaces, we introduce a cross-distribution mixing strategy, which combines\nfeatures and corresponding labels of ID and negative samples to create new\ntraining samples. This mixing strategy effectively bridges the distribution gap,\ngenerating a continuous spectrum of features that extend from ID to negative\nareas. The mixing process can be formalized as follows:\n\\[\nv_{\\text{cd}} = L_2 (\\lambda_{\\beta}f_{\\text{img}}(x_{\\text{id}}) + (1 - \\lambda_{\\beta})f_{\\text{img}}(x_{\\text{ood}})),\n\\]\n\\[\nl_{\\text{cd}} = \\lambda_{\\beta} l_{\\text{id}} + (1 - \\lambda_{\\beta})l_{\\text{ood}},\n\\]\nwhere \\( x_{\\text{id}} \\) and \\( x_{\\text{ood}} \\) are collected positive and negative images, and \\( l_{\\text{id}} \\) and\n\\( l_{\\text{ood}} \\) are soft labels of \\( x_{\\text{id}} \\) and \\( x_{\\text{ood}} \\), respectively. \\( \\lambda_{\\beta} \\sim \\text{Beta}(\\beta, \\beta) \\in [0,1] \\) with\n\\( \\beta \\in (0,\\infty) \\). This method not only allows the model to become more aware of the\nintermediate space between ID and negative regions, but also encourages it to\nlearn more discriminative features that can better generalize to new and unseen\nOOD samples. We denote the cross-entropy loss of Eq. (9) with \\( v_{\\text{cd}} \\) and \\( l_{\\text{cd}} \\) as\n\\( \\mathcal{L}_{\\text{cd}} \\). The final objective is constructed as:\n\\[\n\\mathcal{L}_{\\text{all}} = \\mathcal{L} + \\mathcal{L}_{\\text{cm}} + \\mathcal{L}_{\\text{cd}}.\n\\]\nRemarks. Our proposed data mixing strategies are distinct from existing\nones [51,61,62,64] in both implementation and objectives. Traditional data mix-\ning strategies typically aim to enhance ID classification performance by mixing\nrandomly selected ID images [51,62]. In contrast, our approach is carefully de-\nsigned to include multi-modal and multi-distribution data in the mixing pro-\ncess to enhance the OOD detection capability. While there are some methods"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Setup", "content": "Datasets and benchmarks. Our validation primarily uses the ImageNet-1k\ndataset [4] as ID data, and more evaluations on smaller-scaled ID datasets are\nprovided in the Supplementary Materials. Aligning with standards from prior\nresearch [19,21,29], we use four diverse datasets of iNaturalist [49], SUN [56],\nPlaces [71], and Textures [3] as OOD test sets. Additionally, we assess our method\non the OpenOOD benchmark [58, 65], which differentiates OOD datasets into\nnear-OOD (e.g., SSB-hard [50], NINCO [2]) and far-OOD (e.g., iNaturalist [49],\nTextures [3], OpenImage-O [52]) based on semantic similarity or difficulty. This\ngrouping allows for a comprehensive evaluation of OOD detectors against various\nOOD sample types.\nIn addition to conventional OOD detection that solely considers semantic\nshifts, we consider a full-spectrum OOD detection problem [60], which addition-\nally accounts for non-semantic covariate shift generalization. An optimal system\nshould not only be capable of identifying semantically shifted OOD samples but\nshould also exhibit robustness to non-semantic covariate shifted OOD samples."}, {"title": "4.2 Main Results", "content": "Results on four OOD datasets. As illustrated in Tab. 1, our approach con-\nsistently surpasses existing methods. Specifically, we re-implemented traditional\nmethods [7, 15, 18, 25, 26, 46, 48, 52] by fine-tuning CLIP-encoders with labeled\ntraining samples from ImageNet in accordance with [48]. Without utilizing any\nmanually annotated data, our method significantly outperforms these methods,\ncorroborating the effectiveness of our proposed VLMs adaptation technique. No-\ntably, our approach exceeds the performance of NegLabel [21], confirming the\nsuperiority of learnable prompts over hand-crafted prompts. The OOD samples\nin these datasets have a relatively large semantic difference from the ID data"}, {"title": "4.3 Analyses and Discussions", "content": "The analyses are conducted on the OpenOOD dataset with a VITB/16 encoder.\nPrompt construction. We conduct comprehensive analyses on prompt\nconstruction as demonstrated in Fig. 4. As shown in Fig. 4a, our proposed\ndistribution-aware prompts are well-matched for OOD detection, outperform-\ning both class-specific and unified prompts. Excessively long prompts slightly\nimpair experimental outcomes, as evidenced in Fig. 4b, possibly due to the in-\ncreased capacity overfitting noise presented in the training images. Consequently,"}, {"title": "Automated sample collection.", "content": "As shown in Fig. 5a, the synthetic image\ngeneration strategy outperforms realistic image retrieval, possibly because the\ngenerator has been exposed to a vast amount of training data (e.g., 5 billion\nimages), which greatly surpasses the scale of our utilized WebData (e.g., 60\nmillion images). The best results are obtained by synergistically leveraging both\nsynthetic and real data, confirming their complementary nature. In terms of\nimage generation, employing a stronger image generator can result in better\nperformance, as shown in Fig. 5b. Regarding image retrieval, utilizing a larger-"}, {"title": "5 Conclusion and Limitations", "content": "We developed a novel label-driven automated prompt tuning (LAPT) approach\nthat autonomously configures effective prompts for OOD detection. Requiring\nonly ID class names, our method can automatically mine OOD class names, col-\nlect training images per class, and learn effective prompts, thereby outperforming\nmanually crafted prompts with minimal manual intervention. The automatically\nlearned prompts not only enhanced the distinction between ID and OOD samples\nbut also improved the ID classification accuracy and the strengthened general-\nization robustness to covariate shifts.\nThe efficacy of our approach was highly dependent on the quality of collected\ntraining samples. Employing more advanced generation techniques to create a\ndiverse set of synthetic images [8] or expanding the retrieval space [43] to obtain\nreal images that match better the class names could yield improved results.\nThese possibilities present exciting avenues for future research."}]}