{"title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models", "authors": ["Mahtab Bigverdi", "Zelun Luo", "Cheng-Yu Hsieh", "Ethan Shen", "Dongping Chen", "Linda G. Shapiro", "Ranjay Krishna"], "abstract": "Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel. Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object instances benefits from object detection. Yet, MLMs can not produce intermediate depth or boxes to reason over. Finetuning MLMs on relevant data doesn't generalize well and outsourcing computation to specialized vision tools is too compute-intensive and memory-inefficient. To address this, we introduce Perception Tokens, intrinsic image representations designed to assist reasoning tasks where language is insufficient. Perception tokens act as auxiliary reasoning tokens, akin to chain-of-thought prompts in language models. For example, in a depth-related task, an MLM augmented with perception tokens can reason by generating a depth map as tokens, enabling it to solve the problem effectively. We propose AURORA, a training method that augments MLMs with perception tokens for improved reasoning over visual inputs. AURORA leverages a VQVAE to transform intermediate image representations, such as depth maps into a tokenized format and bounding box tokens, which is then used in a multi-task training framework. AURORA achieves notable improvements across counting benchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench, outperforming finetuning approaches in generalization across datasets. It also improves on relative depth: over +6% on BLINK. With perception tokens, AURORA expands the scope of MLMs beyond language-based reasoning, paving the way for more effective visual reasoning capabilities. Code will be released at the project page.", "sections": [{"title": "1. Introduction", "content": "In contrast to the growing emphasis on building multimodal language models (MLMs), computer vision was originally attempting to interpret images as projections of indescribable 3D intrinsics, not just processing 2D arrays of language patterns [9, 36, 37]. Towards this endeavor, early vision research developed a series of intermediate image representations-enabling geometric reasoning through depth estimation [44] and instance reasoning through bounding box grounding [14]. As pointed out by recent work, we have focused less on such perceptual representations and instead tackled reasoning problems that require limited visual involvement [11, 42, 43]. This is likely because many traditional vision tasks remain ambiguous through natural language. Consider the task of identifying which of a set of N points is furthest away from the camera. While language doesn't lend itself to reason over this problem, a depth estimation would provide the appropriate abstraction to reason over.\nNumerous attempts have been made to enable MLMs to reason over intrinsic image representations. The default approach is to finetune MLMs on data tailored to the specific perception task of interest, hoping that the model implicitly learns the required intrinsic representations [4]. Another option is to outsource the computation to external tools: the MLM can invoke a depth estimator or object detector to produce the appropriate intrinsic [17]. Unfortunately, relying on external models makes the task more computationally expensive and requires loading additional models with more memory. Similarly, vanilla fine-tuning (even with advancements like LoRA [16]) has shown marginal improvements.\nConceptually, we introduce Perception Tokens, intrinsic image representations that aid in reasoning where language is insufficient. To solve the aforementioned task, an MLM augmented with perception tokens can solve the task similar to how language models use chain-of-thought. They will produce a response like the following: \u201cThe depth map is <perception_tokens>. Therefore, point D is closest to the camera.\u201d Here, <perception_tokens> is a set of tokens that implicitly estimate the depth of the image. Similarly, for a counting task, the model can first generate perception tokens that represents the location of the relevant bounding boxes of the desired object, and count the number of boxes to support its final answer.\nTo demonstrate the utility of perception tokens, we introduce AURORA, a training algorithm to augment MLMs with the ability to use perception tokens as intermediate reasoning steps. For certain intermediate representations (e.g. depth maps), we train a VQVAE to transform them into a set of tokens, treating the learned VQVAE codebook indices as a collection of perception tokens. while for others, such as bounding boxes, we use directly encoded structured tokens. Next, we follow a multi-task training approach [15, 18] to train MLMs to use perception tokens as chain-of-thought tokens (see Fig. 1). Additionally, we adopt a curriculum learning approach to avoid catastrophic forgetting.\nWe apply the AURORA training algorithm to the LLaVA [28] model, resulting in our LLaVA-AURORA variant. Our LLaVA-AURORA model significantly outperforms standard fine-tuning approaches across multiple perception-demanding tasks, demonstrating the generality and effectiveness of our method. LLaVA-AURORA achieves state-of-the-art results on both relative depth estimation and object counting tasks. For instance, on BLINK relative depth estimation, LLaVA-AURORA delivers a performance boost of 6.4% points compared to the fine-tuning baseline. Similarly, on counting tasks, LLaVA-AURORA drives improvements of 10.8% points on BLINK, 11.3% points on CVBench, and 8.3% on SEED-Bench. Fig. 2 illustrates examples from these tasks. Perception tokens open up a whole new modality through which MLMs can begin to reason, tackling tasks beyond just language reasoning"}, {"title": "2. Related work", "content": "Multimodal language models (MLMs). MLMs aim to solve a variety of tasks (e.g., Visual Question Answering (VQA) and captioning) based on vision and language inputs. Most modern architectures accomplish this by relying on either cross-attention [1, 2] or visual instruction tuning [26, 28, 30, 53] to interleave multimodal information. Cross-attention architectures operate by independently encoding images and then cross-attending to a language model backbone. On the other hand, visual instruction tuning produces token embeddings from image representations that can be interleaved with language tokens to ground generation. While these techniques work well for high level tasks, MLMs still struggle with mid-level and low-level tasks such as counting, depth reasoning, and segmentation. Most MLMs can be classified as either end-to-end MLMS or tool-using MLMs.\nEnd-to-end MLMs. End-to-end MLMs use a single, unified architecture [1, 8, 24, 26, 27] that can be repeatedly used for different tasks without requiring any architectural changes. While training end-to-end MLMs can be costly due to the need for large amounts of multi-task data, the use of diverse datasets allows end-to-end MLMs to generalize effectively and learn nuanced visual representations.\nTool-using MLMs. Tool-using MLMs enable LLMs to perform vision tasks by attaching specialized vision mod-"}, {"title": "3. Perception Tokens & Aurora", "content": "We introduce Perception Tokens and our Aurora training algorithm (see Fig. 3), which augment multimodal language models with perception tokens, enabling the MLM to leverage these tokens effectively during training and incorporate them into its chain-of-thought reasoning process for enhanced visual reasoning.\n3.1. Problem formulation\nIn autoregressive large language models, chain-of-thought (CoT) reasoning can be formulated as a multi-step inferential process in which the model iteratively generates intermediate reasoning steps to arrive at a final answer. Given a task input x, the model generates a response y conditioned on the input and a sequence of m intermediate reasoning steps {si}i=1m, where x, y, and each si are sequences of tokens from the model's vocabulary V.\nExisting multimodal language models often rely on limited vocabulary tokens derived from text or pre-trained image embeddings like CLIP, restricting their capacity to interpret other representations crucial for reasoning. Mid-level and low-level visual features such as depth maps, instance segmentation masks, human poses and bounding boxes which could substantially improve visual reasoning, are currently incompatible with the model and cannot be integrated during training or inference. Our key insight is to introduce auxiliary perception tokens for these intermediate steps with an expanded vocabulary V' = V \u222a Vaux, bridging this gap by allowing the model to integrate richer visual representations into its reasoning process. Conditioning the final output on these tokens enhances the model's accuracy and interpretability across multimodal tasks.\n3.2. Perception token prediction and reasoning\nIntroducing an expanded vocabulary to enhance multimodal reasoning presents two main challenges. The first challenge is enabling the model to generate tokens from the new auxiliary vocabulary Vaux, which includes specialized tokens for low- and mid-level visual features, such as depth maps and bounding boxes. The second challenge is ensuring that the model can effectively condition on these auxiliary tokens to improve reasoning, particularly for multi-step inference tasks.\nPerception token prediction. To address the first challenge, we employ a specialist-to-generalist distillation approach, using pre-trained specialist models (e.g., depth estimation or instance segmentation) to guide auxiliary token generation through cross-entropy loss. For each input x, the specialist model provides a target probability distribution qi over its tokens. Let M : Vspec \u2192 Vaux denote a one-to-one mapping from the specialist model's vocabulary Vspec to the auxiliary token vocabulary Vaux. We define the distillation loss as:\nldist = minM ( - \u2211i qi log PM(i)),  (1)\nwhere PM(i) is the probability assigned by our model to the auxiliary token corresponding to the mapped token M(i). This consistent mapping allows the model to effectively align its predictions with the specialist model's output distribution, enhancing the relevance and accuracy of its auxiliary token predictions.\nIn addition to distillation, we incorporate a reconstruction loss to enhance the token prediction and the interpretability of our model. Each auxiliary token corresponds to a specific representation, such as a depth map or a bounding box vector, and is trained to directly predict this feature. To achieve this, we introduced a lightweight decoder g that maps the tokens into the feature space, allowing for efficient and interpretable transformations. Formally, for a token t \u2208 Vaux, decoder g, and its target feature f, the reconstruction loss is defined as:\nlrec = ||g(t) - f||2, (2)\nwhere g(t) is the decoded representation for token t in the feature space. This reconstruction process not only aligns each token with a meaningful feature, improving interpretability, but also reinforces the accuracy of auxiliary"}, {"title": "3.3. Tokenization", "content": "A unified tokenization space is crucial for multimodal models as it creates a consistent framework through which varied visual tasks can be represented, processed, and interpreted. Inspired by [38], we establish a unified tokenization space which enables the model to enable the model to learn varied visual features within a shared representation seamlessly. In our experiments, we implement two tailored tokenization schemes for commonly used visual representations. Importantly, our framework is designed with flexibility, allowing it to generalize to a broad range of visual representations beyond those presented here.\nPixel-level representation. This tokenization scheme captures fine-grained spatial information, such as depth maps and segmentation masks, providing the model with detailed pixel-level data essential for accurate visual processing. For these types of tokens, we leverage visual tokenizers like VQVAE and VQGAN, which take in the ground truth masks or depth maps and return discrete target tokens [10, 33, 34, 46].\nStructured representation. This scheme encodes structured yet abstracted visual features, such as human poses, bounding boxes, and coordinates, allowing the model to reason with higher-level spatial relationships and object hierarchies. For these tokens, we define the domain of the tokens based on specific properties; for example, the domain for coordinates can range from 0 to the maximum number of pixels in the image's height or width [5]."}, {"title": "3.4. Curriculum learning with progressive CoTs", "content": "The objective of training the model is to develop a data and computationally efficient method for learning to predict novel, fine-grained visual tokens and using them to complete complex visual reasoning tasks. We observed that the standard approach, which relies on a fixed mixture data, encounters a trade-off between the accuracy of novel tokens predictions and the model's reasoning capability, primarily due to catastrophic forgetting and challenges in reasoning with new tokens. Conversely, fine-tuning the model with the original training mixture significantly raises computational costs and may be impractical if the original data, particularly proprietary datasets, is unavailable, making this approach less scalable for incorporating new tokens in the future.\nWe propose a curriculum learning-inspired training scheme that begins with atomic tasks and gradually advances to more complex ones requiring sophisticated, multi-hop reasoning. Let dt represent the difficulty of task t, with difficulties d1 < d2 <\uff65\uff65\uff65<dT across T tasks, and let p(t,s) denote the probability of sampling data points from task t at training step s. We define p(dt, s) using a temperature-scaled Softmax formulation as follows:\np(dt, s) = exp(-dt/T(S)) / \u2211i=1m exp(-di/T(S)), (3)\nwhere T(S) modulates the task difficulty over time, allowing a smooth shift in the probability distribution toward harder tasks. This temperature function is defined as:\nT(S) = To / 1+ \u03bbs/S (4)\nHere, To is the initial temperature, \u03bb is the annealing rate, and S is the total number of training steps.\nOur approach for defining dt values is based on the inherent complexity of each task, which corresponds to the depth of reasoning involved. Specifically, we assign d1 to the most atomic task, involving the prediction of newly introduced tokens. At the other end of the spectrum, dm represents the final task, requiring the full chain-of-thought (CoT) reasoning steps. Between them, intermediate tasks {dt}t=1T, serves to bridge the gap between the atomic tasks and the comprehensive chain-of-thought responses.\nIn this project, we introduce three types of data subsets for each downstream task, organized in increasing levels of difficulty. The first and most atomic task involves teaching the model to generate tokens from the new auxiliary vocabulary. For instance, in depth-related tasks, we train the model to learn depth maps; in segmentation-related tasks, we teach the model to generate masks; and so on.\nThe other two data subsets involve Chain-of-Thought (CoT) prompts and direct labeling which help with reasoning with auxiliary tokens. In the CoT subset, we use the new intermediate visual perception tokens to answer downstream-specific questions, encouraging the model to reason step by step. In the direct labeling subset, we pose the same questions but instruct the model to provide direct answers without step-by-step reasoning.\nInspired by [15], we employ a multitasking approach for these two data subsets. For each image, we sequentially present both the CoT and direct labeling questions, allowing the model to tackle each image with both reasoning styles in sequence. We use a sequential sampler rather than a random sampler, shuffling the images beforehand. This strategy enables the model to learn from both types of reasoning tasks effectively, enhancing its ability to perform complex visual reasoning."}, {"title": "4. Experiments", "content": "We base our work on LLaVA 1.5 13B as the foundation for our model, which we refer to as LLaVA-AURORA. Our approach augments the MLM with perception tokens to enhance reasoning and improve performance across both 3D and 2D visual tasks. We evaluate our approach on relative depth estimation (3D) using pixel-level depth map tokens for fine-grained depth capture, and on object counting (2D) with mid-level bounding box tokens for precise localization. These tokens not only enhance task-specific results but also highlight our framework's potential to generalize effectively across a broad spectrum of visual reasoning tasks."}, {"title": "4.1. 3D reasoning task", "content": "We choose relative depth estimation as our 3D task because it enables the model to determine spatial relationships within a scene by identifying which points are closer to or farther from the camera. This foundational skill is crucial for scene understanding and applications requiring spatial awareness, such as robotics and autonomous systems. Specifically, this task involves identifying the point closest to the camera among multiple marked points in an image. To support the model's reasoning, we use discrete depth map tokens that capture spatial depth information, enhancing the model's understanding of proximity.\nTokenization. To capture fine-grained spatial details, we tokenize depth maps into sequences of discrete tokens. Inspired by the approach in AiT [38], we use a Vector Quantized Variational Autoencoder (VQVAE) [46] with a code-book size of 128. In this setup, each depth map is encoded as a grid of embeddings, with each embedding matched to the nearest entry in the codebook, yielding a compact depth representation. The VQVAE decoder reconstructs the depth map from this sequence of latent codes, and the entire model is optimized with a reconstruction loss to ensure precise encoding. During inference, each 320x320 depth map is compressed into a 10x10 grid of code indices, resulting in a 100-token sequence where each token represents one of the 128 discrete depth tokens, labeled DEPTH_0 to DEPTH_127. To organize the sequence, we encapsulate it with special tokens DEPTH_START and DEPTH_END, adding a total of 130 depth-related tokens to the model's vocabulary.\nTraining data. We train the VQVAE model on pseudo-depth maps generated from the ADE20k dataset [56, 57] using the Depth Anything model [51, 52]. This dataset provides a diverse range of scenes, enhancing the model's ability to generalize.\nFor fine-tuning, we prepare three types of data tailored for relative depth estimation (as detailed in Sec. 3.4). First, we generate depth maps for 20k ADE20k images, tokenize them with the pre-trained VQVAE, and format each sample in a Q&A structure, prompting the model with a depth estimation question and an answer sequence of depth tokens. Additionally, we construct a dataset of 500 ADE20k images for chain-of-thought (CoT) reasoning, with 2\u20135 markers in each image. Here, the prompt guides the model to generate the coordinates of the markers and then the depth map as intermediate steps, then identify the marker closest to the camera. This CoT training improves sequential reasoning and relative depth estimation accuracy. Finally, we use the same 500 images for direct labeling, prompting the model to directly identify and label the marker closest to the camera (More details in Supplementary)."}, {"title": "4.2. 2D reasoning task", "content": "We select counting as a critical 2D visual task. For object counting, we incorporate bounding box predictions as an intermediate reasoning step to improve accuracy in answering counting queries. Given an image and a question about the number of specific objects, the model first identifies and predicts bounding boxes for each instance of the target object. These bounding box tokens serve as structured, intermediate representations, enabling the model to understand spatial arrangements and accurately count object instances.\nTokenization. To represent bounding boxes as discrete tokens, we resize all input images to a fixed resolution of 336x336 pixels. This preprocessing step allows us to add 336 unique tokens to the model's vocabulary, each representing a specific pixel position within the resized image. These tokens, labeled PIXEL_0 to PIXEL_335, enable the model to uniquely reference each pixel location. Bounding boxes are encoded as tuples of four tokens, formatted as (PIXEL_i, PIXEL_j, PIXEL_k, PIXEL_m), where PIXEL_i and PIXEL_j denote the coordinates of the top-left corner and PIXEL_k and PIXEL_m represent the bottom-right corner (i.e., (x1, Y1, X2,Y2)). This discrete representation allows the model to interpret and use bounding box locations effectively, providing the spatial structure needed for accurate object counting.\nTraining data. To fine-tune the model for object counting, we draw on three types of data tailored to this task (as detailed in Sec. 3.4). First, we use task-specific data from the LVIS dataset [13], selecting 5k images with objects whose counts range from 0 to 15. For each selected image, we specify an object type (e.g., \"beds\") and structure the fine-tuning samples to prompt the model for bounding box predictions of the specified objects within the image.\nTo enhance the model's reasoning ability, we include a small subset of 250 LVIS images for chain-of-thought (CoT) training. Here, each question prompt encourages step-by-step reasoning by instructing the model to first generate bounding boxes for the target object, followed by providing the final count. Additionally, we create a direct labeling subset using the same 250 images. In this subset, we prompt the model to directly identify and label the total count of the specified objects without the intermediate step of bounding box generation."}, {"title": "4.3. Benchmarks", "content": "Relative depth. A recent benchmark, BLINK [11], introduces tasks designed to be intuitive for humans yet challenging for multimodal models, with relative depth estimation as one of its tasks. BLINK provides 124 images, each containing two marked points labeled as A and B, and asks which point is closer to the camera. To reduce biases that language models have toward answering multiple-choice questions [41, 55], we modify the original BLINK questions by removing the answer choices. To further evaluate the model's reasoning and 3D understanding in relative depth, we curated a series of more challenging benchmark sets, collectively called HardBLINK. We progressively increase task difficulty by altering the prompts and image configurations as follows:\n1. Prompt Modification: In the prompts, we exclude the number of markers and their labels, requiring the model to infer these details solely from the image.\n2. Increased Point Complexity: We generate four variations of BLINK by adding more markers to each image, using Depth Anything to produce pseudo-depth maps for precise placement. These curated sets\u2014 HardBLINK 3points, HardBLINK 4points, and HardBLINK 5points\u2014 contain the same images as BLINK but with 3, 4, and 5 randomly placed markers, respectively, each with reasonable depth and distance differences. This setup tests the model's depth reasoning across more complex spatial configurations.\n3. Mitigating Height-Based Bias: To prevent the model from assuming that higher points are farther from the camera, we place markers at mid-height within the image. This approach encourages reliance on depth information rather than positional cues [6].\nCounting. For counting, we evaluate the model on CV-Bench [42], SEED-Bench [23], and BLINK's counting [11] subtask. To better capture the model's capabilities, we also remove multiple-choice options, requiring it to generate an exact count."}, {"title": "4.4. Baselines", "content": "We evaluate a diverse range of models, including closed-source models like GPT-40 [39] and GPT-4 Turbo [39], as well as state-of-the-art open-source models such as LLaVA OneVision [25]. Another key baseline for our work involves fine-tuning the base model AURORA is applied on, solely on the direct labeling portion of the training data for each task, omitting the newly introduced tokens. This approach allows us to isolate the impact of our token-based enhancements. Additionally, we evaluate the base model for AURORA, LLaVA 1.5 13B, to assess its performance without task-specific adaptations. For comparison, we use a tool-augmented baseline with an LLM.\nIn relative depth estimation, we employ GPT-4 Turbo, providing it with the ground truth depth maps generated by Depth Anything for each image, allowing it to use this information in its responses. Details on the exact format are provided in the supplementary materials [17]. For counting, we also use GPT-4 Turbo in a tool-augmented setup. In this process, GPT-4 Turbo first identifies the object specified in the question, then uses Grounding DINO [31] to locate the bounding boxes for each instance of the object and finally counts them."}, {"title": "4.5. How new tokens improve 3D reasoning?", "content": "Our experiments demonstrate that incorporating new visual tokens significantly enhances the model's 3D reasoning abilities, specifically in the relative depth estimation task. Results in Tab. 1 show that our model outperforms both the"}, {"title": "4.6. How new tokens improve 2D reasoning?", "content": "In the 2D task of object counting, incorporating new visual tokens provides a significant advantage over the primary baselines. As shown in Tab. 2, our model outperforms both the baseline fine-tuned solely on direct labeling data and the original base model, as well as the state-of-the-art open-source model LLaVA-OneVision and GPT-4 Turbo + Tool, underscoring the value of these tokens in enhancing counting accuracy.\nAlthough our model does not yet surpass advanced closed-source models, the results demonstrate that the new tokens yield a meaningful improvement in 2D reasoning, enabling more reliable and accurate object counting compared to standard fine-tuning approaches and open-source alternatives."}, {"title": "4.7. Perception token decoding", "content": "Our approach enables the decoding of learned perception tokens into specialist features, such as depth maps and object bounding boxes, to assess their fidelity and utility. We, in particular, evaluate the accuracy and correctness of the depth maps generated by LLaVA-AURORA, specifically focusing on its ability to represent spatial relationships in visual scenes. For this evaluation, we use the relative depth images from the BLINK benchmark [11], ensuring consistency with our relative depth assessments. LLaVA-AURORA generates depth tokens from BLINK images, which are then reconstructed into full depth maps via the decoder of our pre-trained VQVAE model. Notably, the depth maps output by our pre-trained VQVAE provide an upper bound on the quality of the depth maps generated by LLaVA-AURORA. We use programmatic relative depth accuracy as our metric, which measures how well the reconstructed depth maps capture the relative depth of marked points. This evaluation spans several configurations in the benchmark, including images with 2, 3, 4, and 5 labeled points. By comparing the model's predicted depth with the ground-truth marker coordinates, we calculate relative depth accuracy, allowing us to assess the precision of depth map generation in reflecting spatial depth relationships. As shown in Tab. 3, LLaVA-AURORA outperforms Unified-IO [33, 34] in this task. Furthermore, qualitative analysis in Fig. 4 reveals that LLaVA-AURORA 's depth maps capture spatial details effectively highlighting its capacity to interpret and represent fine-grained depth information."}, {"title": "5. Conclusion", "content": "Our algorithm enables the lightweight and scalable integration of perception tokens, such as depth maps and bounding box coordinates, into MLMs, allowing them to perform intermediate reasoning steps akin to chain-of-thought processes. Our method achieves state-of-the-art results on challenging tasks like 2D object counting and 3D relative depth estimation. It also enhances model generalization and interpretability without relying on external tools or task-specific finetuning. The framework is inherently adaptable, incorporating new perception tokens as they emerge, making it a future-proof solution for advancing multimodal reasoning."}, {"title": "6. Ablation study", "content": "In this section, we analyze the impact of various design choices and data configurations on the performance of our proposed method. We focus on three aspects: (1) the impact of including or excluding specific steps in the chain-of-thought (CoT) reasoning process for the 3D task of relative depth estimation, (2) the use of standard text tokens versus new perception pixel tokens for the 2D task of object counting, and (3) the effect of incorporating a perception token reconstruction loss during fine-tunning.\n6.1. Chain-of-thought steps\nFor our 3D task of relative depth estimation, the chain-of-thought (CoT) questions in the fine-tunning data include two steps: (1) identifying the coordinates or locations of the points marked in the image, and (2) generating the depth map and determining which point is closer to the camera based on pixel values in the depth map. This study evaluates the impact of including or excluding these steps in the question prompts during fine-tunning.\nWe experiment with three variations of fine-tunning data configurations:\n1. Direct Labeling Baseline: The model is fine-tuned solely on direct labeling data, where the question prompts directly ask which point is closer to the camera and provide the label as the answer. These prompts do not include either step (1) or step (2), see baselines section.\n2. Step (2) Only: This model is fine-tuned with prompts that exclude step (1) (point location identification) but include step (2), asking the model to answer based on the depth map alone.\n3. AURORA: Our proposed AURORA technique uses prompts that include both steps (1) and (2), explicitly guiding the model through point location identification before generating the depth map.\nAll models are evaluated on the harder BLINK datasets we introduced. As shown in Tab. 4, the results demonstrate that having both steps in the prompts provides the most significant performance improvement. This suggests that guiding the model through a multi-step reasoning process in the prompts enables it to better capture spatial relationships and achieve more accurate depth estimations.\n6.2. Text tokens vs. Perception tokens\nIn this ablation study, we evaluate the impact of using perception tokens compared to standard text tokens for the object counting subtask. Perception tokens are represented in the format PIXEL_X, where X is a number between 0 and 336, indicating pixel locations for object bounding boxes. For comparison, we replace these perception tokens with regular text tokens in the fine-tunning data, such that PIXEL_100 is replaced with 100, and so on.\nAs shown in Tab. 5, models utilizing perception tokens achieve higher performance across all three counting benchmarks: BLINK [11], SEED-Bench [23], and CV-Bench [42]. This demonstrates the effectiveness of perception tokens in explicitly encoding spatial information for improved counting accuracy.\n6.3. Perception token reconstruction loss\nThe aim of this ablation study is to assess whether adding the perception token reconstruction loss, despite its increased computational cost, significantly improves model performance. Incorporating this loss requires adding the decoder for the specific task, which increases computation time and resource requirements. Not using it makes the system lighter and faster by just using the token classification loss. Therefore, we evaluate whether the performance gains justify the additional overhead.\nTo this end, we fine-tune two models based on LLaVA 1.5 13B [27] using a dataset of 20,000 samples only for depth map generation. Each sample includes a prompt such as \"What is the depth map for the image?\" and a response containing sequences of depth tokens. Both models are fine-tuned for 10 epochs: one with the reconstruction loss and one without it (both with cross entropy loss).\nThe reconstruction loss is computed as the mean squared error (MSE) between the ground truth depth map, which is the output of the VQVAE decoder when provided with the ground truth depth tokens, and the predicted depth map, which is generated by decoding the depth tokens produced by the LLM. A soft merging technique is used in reconstruction, where a \"soft token\u201d is created by averaging the embeddings of all potential tokens, weighted by their prediction probabilities from the LLM.\nThe models are evaluated on two datasets: (1) 124 images from the relative depth subtask of BLINK [11], and (2) 1000 random images from the Visual Genome dataset [21], for which depth maps were generated using Depth Anything [51]. The evaluation metric is the mean squared error (MSE) between the ground truth decoded depth maps and the depth maps reconstructed from the model's output tokens.\nAs shown in Tab. 6, the results indicate that incorporating the reconstruction loss does not significantly improve model performance. Fig. 5 further illustrates qualitative results, highlighting the visual differences in the predicted"}, {"title": "7. Cross-task generalization", "content": "To assess the generalizability of AURORA trained on depth generation and Chain of Thought (CoT) data for the relative depth task, we evaluate it on a different depth-related task. Specifically, we use the Depth subtask from CV-Bench [42], which involves identifying which of two objects, highlighted with red and blue bounding boxes, is closer to the camera. Similar to the BLINK evaluations for relative depth, we remove options from question prompts in these evaluations too.\nAs shown in Tab. 7, our model outperforms both the base LLaVA 1.5 13B model and the fine-tuning baseline, demonstrating its generalization capabilities across depth-related"}, {"title": "8. Implementation details", "content": "Computation resources. We train Aurora models on single-node machines equipped with 8 A40 GPUs. Each"}]}