{"title": "TruthFlow: Truthful LLM Generation via Representation Flow Correction", "authors": ["Hanyu Wang", "Bochuan Cao", "Yuanpu Cao", "Jinghui Chen"], "abstract": "Large language models (LLMs) are known to struggle with consistently generating truthful responses. While various representation intervention techniques have been proposed, these methods typically apply a universal representation correction vector to all input queries, limiting their effectiveness against diverse queries in practice. In this study, we introduce TruthFlow, a novel method that leverages the Flow Matching technique for query-specific truthful representation correction. Specifically, TruthFlow first uses a flow model to learn query-specific correction vectors that transition representations from hallucinated to truthful states. Then, during inference, the trained flow model generates these correction vectors to enhance the truthfulness of LLM outputs. Experimental results demonstrate that TruthFlow significantly improves performance on open-ended generation tasks across various advanced LLMs evaluated on TruthfulQA. Moreover, the trained TruthFlow model exhibits strong transfer-ability, performing effectively on other unseen hallucination benchmarks.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated remarkable performance across various natural language processing (NLP) tasks (Achiam et al., 2023; Bai et al., 2023; Liu et al., 2024a). However, they are also prone to hallucination (Ji et al., 2023; Huang et al., 2023; Rawte et al., 2023) phenomenon where the generated content appears plausible but is ultimately misleading or inconsistent with established knowledge (see an example in Figure 1). In particular, LLMs can generate non-truthful content with low factual inaccuracy. These issues significantly undermine the trustworthiness of LLMs, especially in critical scenarios such as generating medical advice or legal suggestions. For instance, Pal et al. (2023) observed ChatGPT frequently produced fabricated or inaccurate medical references, posing substantial risks if relied upon in clinical decision-making. Thus it's crucial to improve the truthfulness of the current LLMs to ensure their reliable deployment in practical applications.\nTill now, various methods have been proposed to mitigate hallucinations in LLMs. For example, one can fine-tune the LLM with carefully collected truthful knowledge to improve its truthfulness (Tian et al., 2023). Another popular strategy is to leverage external knowledge via retrieval-augmented generation (Lewis et al., 2020). However, such strategies usually come with a high computational burden by training the model or demand a large amount of accurate external knowledge, which is hard to collect and verify. Recently, representation intervention has emerged as a more popular strategy, which only edits the internal representations of LLMs at inference time to elicit truthful responses. For example, ITI (Li et al., 2024) aims to edit the representation of several truth-related attention heads inside the transformer blocks. Specifically, ITI computed a truthful correction vector and added it to these selected attention heads to steer LLMs toward more truthful outputs. Several follow-up works (Bayat et al., 2024; Hoscilowicz et al., 2024) further improve upon ITI by considering better strategies for selecting attention heads or the intensity of truthful intervention. Since representation intervention techniques only require editing the query representation at inference time, it is usually lightweight without heavy dependence on any external knowledge base or extra computational burden. It also preserves the LLM's general utility since the intervention only happens to the representation of a specific layer.\nDespite that representation intervention methods have achieved improved truthfulness in LLMs, the whole line of research (Zou et al., 2023; Zhang et al., 2024; Cai et al., 2024) relies on one important assumption: there exists some universal truthful intervention vector in the representation space of LLMs that turns any input query from its hallucinated state to the truthful state. However, no concrete evidence is provided in previous studies to show that such an assumption can be satisfied. Intuitively, given diverse input queries, it is hard to imagine that there exists one universal \"magical\" vector that fixes all truthfulness issues.\nTo further dig into the validity of this assumption, we conduct empirical analysis in Section 3.2. We figure that a unified truthful vector is not able to accommodate all input queries with their diverse representations. Although most truthful correction vectors follow a certain rough trend in direction, each query has its own best truthful correction direction, which, in many cases, contradicts the overall trend. Thus it is necessary to develop a query-specific correction strategy to further improve the effectiveness of the representation intervention methods.\nTo this end, we propose TruthFlow, a novel method that leverages the Flow Matching technique (Lipman et al., 2022; Liu et al., 2022) for query-specific truthful representation correction. Specifically, TruthFlow first uses a flow matching model to learn query-specific correction vectors that transition representations from hallucinated to truthful states. The trained flow model can take any specific query's representations as input and output its corresponding truthful representation correction vector. Then, during inference, TruthFlow leverages the generated query-specific correction vectors from the flow matching model to edit the representation of the current query and enhance the truthfulness of the outputs. By introducing flow matching, we achieved effective and flexible query-specific truthful representation intervention that is efficient and outperforms previous methods on hallucination benchmarks.\nWe summarize our contributions as follows.\n\u2022 We propose TruthFlow, a novel method that leverages the Flow Matching technique (Liu et al., 2022; Lipman et al., 2022) for query-specific truthful representation correction with high effectiveness.\n\u2022 To further improve the effectiveness of TruthFlow, we design a truth-related subspace projection step before applying the correction vectors to purify the noisy information gathered from query representations.\n\u2022 Experiments on TruthfulQA (Lin et al., 2021) demonstrate that TruthFlow enhances truthfulness, especially in open-ended generation tasks. Furthermore, transferability experiments show that TruthFlow can be generalized to other unseen datasets."}, {"title": "2. Related Work", "content": "Representation Intervention. Representation intervention aims to edit the LLMs' hidden representations at certain layers to guide their behavior (Panickssery et al., 2023; Zou et al., 2023; Cao et al., 2024; Li et al., 2024; Chen et al., 2024e). In particular, several efforts have been made to steer them toward more truthful generation. ITI (Li et al., 2024) utilizes fine-grained probing accuracy on each layer's attention heads to locate the most \"truthfulness-related\" attention heads and improves truthfulness. TruthX (Zhang et al., 2024) projects the LLM's internal representations into truthful and semantic latent spaces and refines the model within the truthful space, thereby improving its truthfulness. LITO (Bayat et al., 2024) aims to improve upon ITI and break the \u201cone-size-fits-all\" intervention solution by sweeping through several intervention intensities to generate candidate responses and trains LSTM to predict which response to select. NL-ITI (Hoscilowicz et al., 2024) adopts MLP to replace the logistics regression in ITI to improve the probing accuracy, which results in a more appropriate choice of attention heads.\nOther Approaches to Mitigate Hallucination. Traditionally, post-training or fine-tuning is the default method for mitigating hallucination issues in LLMs. Typical methods include Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), Direct Preference Optimization (Rafailov et al., 2024), and many other techniques to align LLMs with human values, especially truthfulness (Chen et al., 2024d; Tian et al., 2023; Hu et al., 2024). Although these methods have been successful in certain applications, they also exhibit significant shortcomings, such as high computational costs and instability during training (Casper et al., 2023). Aside from training-time mitigation and representation intervention, other inference-time approaches have been developed. Contrastive decoding aims to modify the output logits by contrasting strong and weak model outputs (O'Brien & Lewis, 2023; Zhang et al., 2023; Chen et al., 2024b). Li et al. (2022) attempted to contrast an expert LLM with an amateur LLM to improve fluency and coherence. DoLa (Chuang et al., 2023) contrasted the final layer and early layers to edit output logits, leading to more truthful generation. Kai et al. (2024); Chen et al. (2024c) refined output logits based on key tokens and context sharpness measured by contextual entropy, respectively."}, {"title": "3. Methodology", "content": "We organize this section as follows: we first present preliminaries on Flow Matching in Section 3.1. Then in Section 3.2 we analyze current representation intervention methods and explains the motivation of our method. In Section 3.3 we give comprehensive explanations on how to achieve query-specific truthful correction via flow matching model. In Section 3.4 we show how to integrate the flow model to elicit truthful generation from LLMs."}, {"title": "3.1. Preliminaries on Flow Matching", "content": "Flow matching (Lipman et al., 2022) refers to a class of generative models that use a vector field to capture a desired probability path from source distribution psource to target distribution ptarget. One typical flow matching model is rectified flow (Liu et al., 2022), which demonstrates strong generative capacity (Esser et al., 2024) via building a linear trajectory between the source and the target. Specifically, suppose we have drawn data samples x ~ psource and y ~ ptarget, we can calculate the linear interpolation zt = ty + (1-t)x for t \u2208 [0,1]. The vector field parameterized by v\u03a6 in flow matching, denoted as v\u03a6 : [0, 1] \u00d7 Rd \u2192 Rd, is trained to follow the trajectory of the linear interpolation, i.e., dzt/dt = y - x. Thus we can train the desired vector field with a neural network using the following objective:\nmin E x,y~Psource Ptarget [\u222b 0 1 ||(y - x) - V(t, zt) ||2 dt],\nwhere psource ptarg et denotes the joint distribution of the source and target. When we finish training the parameterized vector field v\u03a6, it allows us to generate samples following the target distribution given samples drawn from the source with the following ordinary differential equation (ODE):\ndzt/dt = v(t, zt)dt. (1)\nAny prebuilt numerical ODE solver such as Euler (Euler, 1845) or Runge-Kutta (Runge, 1895; Kutta, 1901) can be used to simulate the solution."}, {"title": "3.2. Motivation: Universal Correction Vector?", "content": "Current representation intervention methods (Li et al., 2024; Chen et al., 2024e; Hoscilowicz et al., 2024) rely on one unverified assumption that there exists some universal truthful intervention vector in the representation space of LLMs that turns any input query from its hallucinated states to the truthful states.\nTo verify whether this assumption holds in practice, we visualize the geometry of Llama-2-7b-chat model's representation states at a certain layer (that are edited by current representation intervention methods) in Figure 2. To be more specific, we first append a correct answer and an incorrect answer to a question, respectively. Then we extract the MLP activations for each token at the 13-th layer within the transformer. By averaging over the whole sentence tokens, we obtain a truthful representation (corresponding to the correct answer) vector and a hallucinated representation (corresponding to the incorrect answer) vector. Then we use PCA to reduce the high-dimensional representation vectors to 2-dimension. Specifically, we estimate the distribution along the two principle directions (which are the x and y axis in the figure, respectively) with Kernel Density Estimation (KDE) and plot the contour of hallucinated states and truthful states separately in red and purple. Furthermore, in order to compare the specific direction for each question and the overall trend from hallucination to truthfulness, we plot the arrows to represent the difference between the hallucinated and truthful representation states. The overall trend is plotted in a blue arrow while each specific direction is expressed in a light blue arrow. From Figure 2 we observe that some light blue arrows follow the general trend directed by the deep blue arrow while others do not follow that pattern. In other words, a single unified truthful correction vector is, in general, not enough to steer diverse input queries all toward their truthful states, which motivates us to design a query-specific representation intervention method."}, {"title": "3.3. Flow Matching for Query-Specific Correction Vectors", "content": "Based on the analysis in Section 3.2, we hope to obtain a query-specific correction solution. This requires us to capture not a universal correction vector, but a correction vector distribution, which is a perfect match for flow matching models (Lipman et al., 2022; Liu et al., 2022). Specifically, we hope to train a flow model that learns the linear trajectory from the query representation distribution to the corresponding correction vector distribution. After we obtain such a flow model, given any new input query, the trained flow will take the query's hidden representation as input and generate its corresponding truthful correction vector for truthful LLM generation.\nTraining Data for Flow Matching\nFor the query representation distribution, following prior works (Azaria & Mitchell, 2023; Chen et al., 2024a), we extract the input query q's hidden states at the last token of layer l (i.e., hql ) as the query representation distribution. In terms of the correction vector distribution, we first append the correct answer ac of length Tc and incorrect answer ai of length Ti to the query, then we calculate the average hidden states over all the answer tokens h a = 1/Tc \u2211Tj=1 h j and h a = 1/Ti \u2211Tj=1 h j, similar to Ren et al. (2022). We contrast these average states to get the truthful correction vector dq = ha - ha for query q and collect them for all queries as our correction vector distribution.\nFlow Model Training Once we collect query representations hql from the query representation distribution pq, and their truthful correction vectors dq from the correction vector distribution pa, we can train the flow model to capture the distribution transition between them using the following optimization objective:\nmin E h,d ~ Popa [\u222b 0 1 ||(da - ha ) - V(t, zt) ||2 dt],\nwhere zt = tda + (1 \u2212 t)ha is the linear interpolation between query representation and its corresponding truthful correction. The implementation of the training algorithm is shown in Algorithm 1. In practice, we follow the general architecture design in flow matching (Lipman et al., 2022) but modify the U-Net architecture to fit the size of our hidden state vectors (see Appendix A.1)."}, {"title": "3.4. Integrate Flow Model for Truthful LLM Generation", "content": "Once we obtain the trained flow matching model that learns the path from pq to pa, we can apply it to generate query-specific truthful correction vectors. During inference, the correction vector for a given input query is added back to the LLM's hidden representations at the l-th layer where hql and dq are extracted. Moreover, we further improve the query-specific vectors via projection onto truthfulness-related subspace formed by the top singular vectors.\nRepresentation Flow Correction In general, for each input query, the flow matching model is able to transfer the last token hidden state hql to a query-specific truthful direction dq = Flow(hql ) by solving Equation (1) using any prebuilt numerical ODE solver. Following Lipman et al. (2022), we choose the Midpoint method (Burden & Faires, 2010), a second-order Runge-Kutta solver, for our case. To edit LLM hidden representations and elicit truthful outputs, we further add the query-specific truthful correction vector dq to each token position at the l-th transformer layer with a multiplier \u03b1 \u2208 R, which controls the strength of intervention intensity. Formally, after representation flow correction, the l-th layer's input token hidden state is now h j l + \u03b1dq, \u2200j \u2208 {1, . . . , m} and going forward, all new generated tokens' hidden states are edited by hk l + \u03b1dq, \u2200k \u2208 {1, . . . , n}.\nTruthfulness-Related Subspace Projection Ideally, we hope that the truthful correction vector dq = ha - ha represents an accurate truthful correction direction. Yet in practice, such truthful correction vectors obtained by the mean difference of hidden states may be too \u201cnoisy\u201d and contains a lot of query-specific information aside from truthfulness (Manigrasso et al., 2024; Zou et al., 2024). We conjecture that the truthful information may only be located in an intrinsically low-dimensional manifold (Aghajanyan et al., 2020) while other dimensions of the vector contain some non-related high-frequency noisy information. Thus we propose applying singular value decomposition (SVD) on the truthful directions and then projecting our correction vector onto top singular vector directions to purify the potential noisy information. To be specific, we collect a set of mean differences {dq }q and construct a matrix D \u2208 RN\u00d7d where N is the number of dq and d is the dimension of d. Directly applying SVD gives us D = U\u03a3VT , where VT = [v1 , . . . , vN ], vi \u2208 Rd. Then we project the truthful correction vector dq to the subspace formed by these singular vectors to obtain the projected correction vector by\ndAprojq = \u2211k i=1 (vi , dq )vi , (2)\nwhere {vi }k i=1 are singular vectors corresponding to the largest k singular values of D. Intuitively, since D contains all the truthful correction vectors for each queries, we believe its largest few singular vectors represents the key truthfulness-related information while the other singular vectors may carry other unrelated noisy information.\nThe complete process to obtain query-specific directions using flow matching model is shown in Algorithm 2. After obtaining the project correction vector dAprojq , we similarly add \u03b1dAprojq to all tokens' hidden states at the l-th layer for representation intervention."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Settings", "content": "Datasets and models. In order to measure the truthfulness of LLMs, we mainly consider TruthfulQA (Lin et al., 2021). It is composed of 817 questions across 38 categories. Each question comes with one best answer, several correct answers, and some incorrect answers. TruthfulQA contains both multiple-choice (MC) questions and open-generation questions, both performances can reflect the truthfulness of the LLM model.\nWe conduct main experiments on various LLMs to validate our methods' effectiveness. We consider Llama-2-7B-Chat, Llama-2-13B-Chat (Touvron et al., 2023), Llama-3-8B-Instruct (Dubey et al., 2024), Mistral-7B-Instruct-v0.2, Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), and Gemma-2-9B-it (Team et al., 2024). We refer to them as Llama2-7B, Llama2-13B, Llama3, Mistral2, Mistral3, Gemma2 respectively. In the rest of the paper, \u201cbase\u201d models refer to these chat (or instruct) models with greedy decoding strategy instead of the pre-trained models for the seek of convenience.\nBaselines. We compare TruthFlow with a comprehensive collection of baseline methods, including (1) DoLa (Chuang et al., 2023); (2) Activation Decoding (Chen et al., 2024c) (AD); (3) ITI (Li et al., 2024); (4) NL-ITI (Hoscilowicz et al., 2024); (5) TruthX (Zhang et al., 2024).\nEvaluations. For open-ended text generation tasks, we follow the standard practice (Lin et al., 2021) and utilize two sets of metrics: (1) BLEURT score, which is determined by whether the generated text is closer to correct or incorrect answers, measured by BLEURT model (Sellam et al., 2020); (2) truthful and informative scores given by GPT-4 model. For the multiple-choice questions, we calculate the standard MC1 and MC2 scores which evaluate the LLM's ability to identify truthful statements. Specifically, the MC1 score is calculated by the proportion of the best answer having the highest probabilities; the MC2 score is defined as normalized total probability assigned to the set of true answers given a question and multiple correct and incorrect reference answers. All metrics are higher the better."}, {"title": "4.2. Results", "content": "Following the experiment settings of previous work (Zhang et al., 2024; Li et al., 2024), we divide the whole TruthfulQA dataset into half: 408 data as the training set and 409 remaining data as the test set. For the training set, we retain only the pairs of the best answer and the first incorrect answer (best_answer, incorrect_answers1) for each question. More experimental details are deferred to Appendix B.\nQuantitative Analysis. Table 1 demonstrates the comparative results of TruthFlow and previous baselines on TruthfulQA. Specifically, on open-ended generation tasks, TruthFlow yields significant improvements (7% on average) on the truthfulness score over the base model, largely outperforming other baselines. In certain cases, the Info score is slightly reduced (see analysis later in qualitative study), yet TruthFlow still largely improves on the True*Info score in all models we tested. In addition, TruthFlow also achieves over 5% improvement on average with respect to BLEURT score evaluation. On multiple-choice tasks, TruthFlow increases MC scores across most LLMs (5% on average over the base model for both MC1 and MC2) and outperforms most baselines."}, {"title": "Qualitative Study", "content": "We show some typical TruthfulQA examples in Table 2 to illustrate how TruthFlow elicits truthful answers. In the first question, the base model acknowledges the misconception that Einstein flunked some subjects in school and hallucinates saying \"French\" and \"geography\", while TruthFlow negates the statement in the question and answers correctly. In this case, TruthFlow successfully flips hallucination to truthful outputs without undermining informativeness. In the second question, the base model hallucinates to consider \u201cArabian Nights\u201d where rubbing an old lamp often causes a genie to appear. TruthFlow, in comparison, generates a truthful answer that \u201cnothing happens\", despite not being as informative as the hallucinated answer.\nWe also notice that some of the best answers in TruthfulQA are \"I have no comment\", which is considered as not informative during evaluation. This explains why TruthFlow demonstrates a slight decrease in Info score in certain cases: TruthFlow successfully flips the hallucinated answer to the truthful one but the truthful answer is sometimes not informative (e.g., \"I have no comment\").\""}, {"title": "5. Analysis and Ablations", "content": "In this section, we extend analyses to explore the improvements of TruthFlow further. We analyze transferability, selected intervention layer effect, impact of k, and ablation on flow matching technique and truthful subspace projection."}, {"title": "5.1. Transferability", "content": "To assess the generalizability of our method, we apply TruthFlow which is trained on the entire TruthfulQA dataset to HaluEval (Li et al., 2023), Natrual Questions (Kwiatkowski et al., 2019) (NQ), and TriviaQA (Joshi et al., 2017). The HaluEval dataset (QA track) consists of 10000 questions from some existing dataset (e.g. HotpotQA (Yang et al., 2018)). It equips each question with reference knowledge, a right answer, and a hallucinated answer which ChatGPT automatically generates. The NQ and TriviaQA datasets are two large-scale question-answering datasets with real user queries annotated with corresponding answers. To form truthful and hallucinated data pairs, Li et al. (2024) selected a subset of 3610 data from each of these two datasets and prompted GPT-4 to generate \"the most plausible sounding but false\" answers. We use the datasets they released for evaluating our method.\nWe consider these benchmarks in an open-ended generation format and evaluate the True score and True*Info score, which are the same as those used in our TruthfulQA experiments. The details of this evaluation can be found in Appendix D.\nTable 3 highlights TruthFlow's performance across the three benchmarks, showcasing its remarkable generalizability. We also compare TruthFlow with the base model and ITI to analyze the transferability. In the open-ended generation setting, ITI shows weak transferability and undermines the base model's performance heavily. In comparison, TruthFlow significantly enhances both True and True*Info scores, indicating that it achieves truthful improvements while balancing informativeness. The results reveal that TruthFlow maintains the LLM's performance even when applied to unseen domains. This exceptional generalizability may be attributed to the synergy between the flow-matching model and SVD: the former generates query-specific truthful correction vectors, while the latter captures general truthful information, ensuring consistent and reliable improvements in truthfulness."}, {"title": "5.2. Effect of Layers", "content": "We conduct experiments to explore the effect of different layers where flow matching model is applied. The generation performance achieves the peak at medium layers, such as layer 12 as is shown in Table 4. This phenomenon is aligned with previous findings that the intermediate layers process some complex, high-level abstractions (Chuang et al., 2023; Jin et al., 2024) while the deeper layers are more focused on prediction tasks (Liu et al., 2024b). Thus as is similar to previous one-layer steering methods (Panickssery et al., 2023; Cao et al., 2024), we only edit one certain layer in the intermediate layers and steer LLM to generate more truthful responses while maintaining informativeness."}, {"title": "5.3. Impact of the Number of Chosen Singular Vectors", "content": "We conduct comparative experiments on the number of top singular vectors we select to construct the truthful subspace. Intuitively, the main truthful information can be expressed in an intrinsic low-dimensional subspace. However, since the hidden states include a large amount of information, including but not limited to contextual, factual, and logical information, we cannot merely depend on the very few top singular vectors. Thus we conduct experiments to empirically figure out the influence of k.\nwhile increasing numbers of chosen singular vectors may not largely contribute to the performance. Therefore, selecting k around 10 to 20 is enough for capturing main truthful information while maintaining informativeness."}, {"title": "5.4. Ablations", "content": "We analyze the combined effects of the flow matching technique and truthful subspace projection. To assess the benefits of the query-specific but noisy truthful correction vectors dq provided by flow matching alone, we compare the base model to TruthFlow without truthful subspace projection. Additionally, we evaluate the influence of projection by comparing TruthFlow with and without its application.\nThe numerical results in Table 5 demonstrate that applying query-specific correction without projecting onto the truthful subspace significantly enhances the truthfulness of LLM outputs. Moreover, the True*Info score shows substantial improvement, indicating that the correction vectors, even if they are not truth-intensive enough, can still lead to better truthful and informative behavior. When the query-specific vector is further projected onto the subspace spanned by the top k singular vectors, truthfulness and informativeness improve even further."}, {"title": "6. Conclusions", "content": "In this paper, we propose TruthFlow, a novel representation intervention framework aimed at mitigating hallucinations in LLMs. Our approach introduces flow matching model to capture the query-specific correction vectors for truthful LLM generation. Specifically, TruthFlow first uses a flow model to learn query-specific correction vectors that transition representations from hallucinated to truthful states. Then, during inference, the trained flow model generates these correction vectors to enhance the truthfulness of LLM outputs. Experimental results reflect TruthFlow's significant improvements in truthfulness and the remarkable transfer-ability across different unseen domains."}, {"title": "Impact Statement", "content": "This paper introduces a novel framework to mitigate hallucinations and elicit truthful generations from LLMs. By addressing these critical issues, TruthFlow contributes to the development of more reliable and responsible LLM systems. Furthermore, the design underlying TruthFlow may also inspire researchers in the broader LLM community, fostering advancements in more reliable and trustworthy LLMs."}, {"title": "A. Configuration of TruthFlow", "content": ""}, {"title": "A.1. Architecture of 1D-UNet", "content": "We modify the architecture of the 2D-UNet (Ronneberger et al., 2015) used in flow matching to fit our LLM settings.\nIn general, we follow the 2D-UNet architecture. The whole network is composed of several down-sampling blocks, bottleneck blocks, and up-sampling blocks. A down-sampling block is made up of d residual blocks, where d refers to the \"depth\" of the UNet. Each residual block has two linear layers and two batch normalization layers with ReLU being the activation function. When we set \u201cfeature scale\u201d to \u03b1, each residual block changes the dimensionality of the input feature to alpha times its dimensionality. For example, if the input feature size is 4096 and the feature scale is 0.5, then the output feature size will be 2048. An up-sampling block is completely symmetrical to the down-sampling block. As for the middle bottleneck block, we design it as a residual block with the same input and output size.\nSince the flow matching framework requires time steps as part of the input to the neural network, we use Sinusoidal Positional Embedding (Vaswani, 2017) to achieve time embedding.\nTo fit the 1D-UNet to our experimental setting, we set depth d = 4, feature scale \u03b1 = 0.5, and time embedding dimension as 128 by default. The input feature size is dependent on the different LLMs' hidden dimension, which can be 3584 for Gemma2, 5120 for Llama2-13b, and 4096 for other LLMs used in this work. Regarding the scale of our neural network for training flow matching model, we evaluate both the number of parameters and memory usage. For the Gemma2 model, which features 3584-dimensional hidden states, the 1D-UNet has fewer than 0.09B parameters and occupies 336.59 MB of memory. For larger LLMs with 4096-dimensional hidden states, the network comprises approximately 0.11B parameters and consumes 437.92 MB of memory. In the case of the Llama-2-13b-chat model, which utilizes 5120-dimensional hidden states, the network contains fewer than 0.18B parameters and requires 680.52 MB of memory."}, {"title": "A.2. Training", "content": "We use 408 pairs (one pair for one question) to train the flow model. We use AdamW optimizer with learning rate 10-4 and 100 steps cosine schedule warmup. The training batch size is set to 136 and the number of epochs is 25 by default. The training process will take only a few seconds and does not call for extra large GPU memory.\nThe training time for flow matching model is shown in Table 6. We run the training process for three times and average the training time to avoid particularly long or short training periods for various reasons."}, {"title": "A.3. Sampling", "content": "We use the Midpoint method, which belongs to RK2 ODE solver class, to obtain the numerical solution to Equation (1) in 16 discretization time steps. The concrete algorithm is presented in Algorithm 3.\nThe local truncation error of Midpoint method is O(h3 ). This arises because the method matches the Taylor expansion of the true solution up to the quadratic term. As for the global truncation error, since the each time step contributes a O(h3 ) error, the total error accumulating over the total steps is O(h2 ). The Midpoint methods balance computational efficiency and accuracy, making it a good choice for our flow matching sampling here."}, {"title": "B. More Experiment Setting", "content": "All the experiments are done on a single Nvidia RTX A6000 48GB GPU.\nIn all the open-ended generation tasks, we utilize the greedy decoding strategy to generate new tokens. Besides, we set the maximum number of newly generated tokens to 256 to allow relatively long text generation, which is closer to current LLM generation paradigms in real-world applications.\nFor each LLM, we apply the following hyperparameters (see Table 7) to achieve the results reported in Table 1. \u201cNum Epochs\" refers to the number of epochs to train the flow matching model."}, {"title": "C. Evaluation of TruthfulQA", "content": ""}, {"title": "C.1. Multiple Choice Accuracy", "content": "For TruthfulQA, we follow the standard multiple-choice evaluation to calculate the MC1 score and the MC2 score.\n\u2022 MC1(%): the proportion of best answer having highest probabilities.\n\u2022 MC2(%): normalized total probability assigned to the set of true answers given a question and multiple correct and incorrect reference answers."}, {"title": "C.2. GPT Evaluation Prompts", "content": "To evaluate the truthfulness of TruthFlow on TruthfulQA, we prompt GPT to determine whether the generated answers are truthful according to the reference correct and incorrect answers in TruthfulQA. Previously, the standard practice for open-ended generation evaluation was to use a finetuned GPT-3 to judge whether the answer is truthful. However, OpenAI has shut down its original GPT-3 models including ada, babbage, curie, and davinci. Thus we turn to GPT-42 and use the prompts in Figure 4 and Figure 5 to urge it to evaluate the answers.\nOur prompt template focuses on hard-label judgement rather than telling GPT to rate the answer according to certain criteria. By giving explicit instructions and standards, GPT-4 is able to judge the correctness of the generated answers objectively.\nTo calculate the informativeness score, we prompt GPT-4 to evaluate the response in a few-shot manner following the evaluation samples provided by Lin et al. (2021). To be specific, we use the following prompt template."}, {"title": "D. Evaluation of Transferability", "content": "We use the same metrics as TruthfulQA evaluation above to evaluate open-ended generation performance on HaluEval, Natural Questions, and TriviaQA.\nFor NQ and TriviaQA, to calculate true score, we prompt GPT-4 to assign hard labels to whether the generated answer is truthful based on comparison between example correct and incorrect answers (see Figure 7, and Figure 8). For Info score, we use the same few shot prompt (see Figure 5) as in TruthfulQA evaluation to tell GPT-4 to judge whether the generated answer is informative. Finally, the True*Info score is calculated by multiplying True score and Info score.\nIn particular, since HaluEval has far more data than other datasets and the reference knowledge for each entry is long, we design the evaluation prompt in a more efficient way to evaluate several (here are 3 in our experiment setting) generated answers simultaneously to lower the cost. See Figure 6 and Figure"}]}