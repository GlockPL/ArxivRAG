{"title": "Fragmented Layer Grouping in GUI Designs Through Graph Learning Based on Multimodal Information", "authors": ["Yunnong Chen", "Shuhong Xiao", "Jiazhi Li", "Tingting Zhou", "Yanfang Chang", "Yankun Zhen", "Lingyun Sun", "Liuqing Chen"], "abstract": "Automatically constructing GUI groups of different granularities constitutes a critical intelligent step towards automating GUI design and implementation tasks. Specifically, in the industrial GUI-to-code process, fragmented layers may decrease the readability and maintainability of generated code, which can be alleviated by grouping semantically consistent fragmented layers in the design prototypes. This study aims to propose a graph-learning-based approach to tackle the fragmented layer grouping problem according to multi-modal information in design prototypes. Our graph learning module consists of self-attention and graph neural network modules. By taking the multimodal fused representation of GUI layers as input, we innovatively group fragmented layers by classifying GUI layers and regressing the bounding boxes of the corresponding GUI components simultaneously. Experiments on two real-world datasets demonstrate that our model achieves state-of-the-art performance. A further user study is also conducted to validate that our approach can assist an intelligent downstream tool in generating more maintainable and readable front-end code.", "sections": [{"title": "1 Introduction", "content": "Graphic User Interface (GUI) builds a visual bridge between software and end users. GUI design refers to the process of creating the interface layout, including the arrangement of visual elements like buttons, icons, and menus, as well as the interaction patterns that guide users through the software. A good GUI design makes the software efficient and easy to use, which has a significant influence on the success of applications and the loyalty of its users. In the industrial setting, the development of GUI starts from design prototypes produced with design software, such as Sketch [1] and Figma [2]. A design prototype contains multiple GUI artboards. Each GUI artboard has a view hierarchy capturing the arrangement of GUI layers and it shows a visual effect of GUI design in the Canvas (as shown in Figure 1). During the industrial GUI-to-code process, with a full understanding of the designer's intentions, front-end developers need to identify GUI components that should be instantiated on screen and rearrange them in a semantic structure to ensure correct visual displays [3]. To alleviate the burden on developers, some semi-automated code generation platforms (e.g. Imgcook [4]), which take design prototypes as inputs, are developed to generate code intelligently. In academia, intelligent code generation has also gained great attention of researchers [5-8], and these methods are mainly based on GUI design images and"}, {"title": "2 literature review", "content": null}, {"title": "2.1 GUI Understanding", "content": "GUI implementation is a time-consuming process, which prevents developers from devoting the majority of time to developing unique features of applications. It attracts researchers who adopt deep learning techniques to understand GUI and automate the developing process. Previous work can be summarized based on the resource where GUI originated from. Understanding GUI from screenshots or wireframe sketches is an active research field. For example, Magic layouts [13] detects and classifies UI"}, {"title": "2.2 GUI grouping", "content": "Forming GUI groups of different granularities are used for intelligence to automate GUI testing [17, 18], implementation, and automation tasks. To our knowledge, previous grouping methods can be summarized as three types: component-level, section-level, and layer-level. In the component-level category, UIED [11] detects and forms perceptual groups of GUI widgets based on a psychologically-inspired, unsupervised visual inference method. Some methods [5, 6] adopt image captioning models to generate GUI view hierarchy from GUI images. Methods in the section-level category group GUI elements into tab, bar, or layout sections. For example, REMAUI [19] group GUI widgets to three Android-specific layouts. Screen recognition [20] develops some heuristics for inferring tab and bar sections. Xiao et al. proposed the semantic component group [21, 22] to identify UI components that achieve certain interaction functions or visual information. They introduced a vision detector based on deformable-DETR for semantic component grouping, aiming to improve the performance of multiple UI-related software tasks [22]. While these methods are based on GUI design images, there are some GUI implementation-oriented methods to group GUI elements into sections. For example, ReDraw [3] and FaceOff [23] solve the layout problem by finding in the codebase the layouts containing similar GUI widgets. Some methods adopt specific layout algorithms to synthesize modular GUI code or layout [24, 25] and infer GUI duplication [26]. Recently, the fragmented layer grouping problem, which requires grouping low-granularity layers in GUI design prototypes during industrial GUI development, has attracted researchers' attention. Deep-learning-based techniques, such as object detection [9] and transformer [10], are adopted to group fragmented layers to facilitate an intelligent downstream tool to generate more maintainable and readable code. However, these methods have some limitations and drawbacks as described in the introduction. In this study, we propose a new algorithm to tackle fragmented layer grouping, which outperforms previous work."}, {"title": "2.3 Graph Neural Networks", "content": "Recent years have witnessed a great surge of promising graph neural networks (GNNs) being developed for a variety of domains including chemistry, physics, social sciences, knowledge graphs, recommendations, and neuroscience. Graph learning refers to the process of learning from graph-structured data by leveraging the relationships between nodes and edges to capture both local and global patterns. The first GNN model was proposed in [27], which is a trainable recurrent message-passing process. To generalize the convolution operation to non-Euclidean graphs, these works [28-30] defined spectral filters based on the graph Laplacian matrix. Spatial-based models define convolutions directly on the graph vertexes and their neighbors. Monti et al. [31] presented a unified generalization of CNN architectures to graphs. Hamilton et al. [32] introduced GraphSAGE, a method for computing node representations in an inductive manner that operates by sampling a fixed-size neighborhood of each node and performing a specific aggregator over it. Some methods attempted to enhance the original models with anisotropic operations on graphs, such as attention [33, 34] and gating mechanisms [35]. Xu et al. [36] aimed at improving upon the theoretical limitations of the previous model. Li et al. [37] and Chen et al. [38] tried to overcome the over-smoothing problem when GCN goes deeper, and some current work already attempts to introduce transformer network into graph learning [39, 40]. Researchers also have great interest in utilizing graph neural networks to tackle computer vision tasks, such as 3D object detection [41], skeleton-based action recognition [42], and semantic segmentation [43]. Graph neural networks can also be helpful for GUI understanding. Ang et al. [44] combines graph neural networks with scaled dot-product attention to learn the embeddings of heterogeneous nodes in GUI designs, which achieves state-of-the-art performance in UI representation learning tasks. Li et al. [45] introduce GNNs for multi-class node classification used for denoising GUI view hierarchy. Inspired by these studies, we also attempt to introduce a graph neural model to our proposed pipeline to group fragmented layers in the UI design prototypes. More details are described in Section 3."}, {"title": "3 Approach", "content": null}, {"title": "3.1 Pipeline Overview", "content": "The pipeline of our approach is visualized in Figure 2 and Figure 3. A design artboard is composed of UI layers that draw various UI components. We parse a design artboard into a layer list containing the following multimodal information, {(x, y, w, h), img-tensor, category}-1, in which (x,y,w,h) is the layer's wireframe information, img-tensor is the layer's image with a resolution of 64\u00d764, category denotes the type of a layer (such as Text, ShapePath, etc). We encode all the attributes into embedding vectors and sum them up to obtain the initial representation vectors for each layer. Our graph-learning-based model refines and updates the layer's representation vectors. It predicts whether a layer is fragmented and regresses a bounding box for each fragmented layer. After a box merging algorithm, we obtain the final results of bounding boxes representing merging groups' boundaries. Fragmented layers"}, {"title": "3.2 Graph Construction and Feature Extraction", "content": null}, {"title": "3.2.1 Graph Construction", "content": "In fragmented layer grouping, the core task of the proposed graph neural network is to discover the semantic consistency between fragmented layers. To more comprehensively explore the semantic similarity of GUI layers, we constructed a graph model based on the wireframe attributes of layers and enhanced the representation capabilities of node embeddings through the GNN. Our goal is to leverage the strengths of"}, {"title": "3.2.2 Multimodal Attributes Encoding", "content": "Unlike methods that solely parse GUI components from screenshots [13, 16], we leverage the rich multimodal information embedded within original design prototypes to address the fragmented layer grouping task. This subsection elaborates on how we encode multimodal attributes in detail.\nVisual information plays a pivotal role in fragmented layer grouping. To balance time and space complexity, we employ a pre-trained ResNet-50 model as the backbone to encode 64 \u00d7 64 layer images into visual feature vectors. In GUI design software, each layer is categorized to help designers create geometric shapes (e.g., Oval, Rectangle) or other GUI elements (e.g., text and images). There are 13 layer categories, and we learn an embedding matrix $E^{13\u00d7d}$, where each row represents the embedding vector of a corresponding category. For wireframe information, we convert 4-dimensional coordinates into high-dimensional vectors using high-frequency functions as described in [46]:\n$\\gamma(x) = (sin(2^0\\pi x), cos(2^0\\pi x), ..., sin(2^{L-1}\\pi x), cos(2^{L-1}\\pi x))$\nWe then use a parameter matrix $M^{8\u00d7L,d}$ to embed the high-dimensional vector into a d-dimensional space. While several approaches aim to design more sophisticated multimodal feature fusion strategies, we employ a simple yet empirically effective strategy by directly adding these feature embedding vectors.\nAdditionally, we assign a feature vector to each edge. For an edge connecting nodes $v_i$ and $v_j$, we utilize the high-frequency function in formula 1 to encode the differences in wireframe coordinates, (\u2206x, \u2206y, \u0394\u03c9, \u0394h), as the edge attribute vector. We acknowledge that layers within the same group tend to be spatially proximate. In fact, fragmented layer groups can be further divided into two categories: those where the internal layers are spatially adjacent (e.g., icon layers) and those where the internal layers are spatially distant (e.g., background layers). By encoding the spatial distances between layers and incorporating this information as edge embeddings, we significantly enhance the performance of the Graph Neural Network. This encoding enables the model to more effectively capture and differentiate the spatial relationships between layers, which is crucial for accurate layer grouping. We have integrated an attention mechanism within the GNN to focus on layer groups that may exhibit larger spatial separations but still demonstrate underlying correlations. This attention mechanism further improves the model's representational capacity and overall accuracy, ensuring that both spatially close and distant layer groups are appropriately identified and merged.\nIn summary, as depicted in Figure 4.(d), each node in the graph is represented by a multimodal feature embedding, incorporating image features, wireframes, and category information. Each edge corresponds to a feature embedding that encodes the spatial relationships between adjacent nodes."}, {"title": "3.3 Network Architecture and Loss Functions", "content": null}, {"title": "3.3.1 Graph Learning Blocks", "content": "As shown in Figure 5, the proposed graph learning module consists of a multi-head self-attention module and a message-passing neural network (MPNN) layer. Graph neural networks (GNNs) can be formalized within the message-passing framework, where node representations are updated through the following iterative formula:\n$h^{(k)}_v = COMBINE^{(k)} (h^{(k-1)}_v, AGGREGATE^{(k)} ({MESSAGE^{(k)} (h^{(k-1)}_v, h^{(k-1)}_u, e_{uv}) | u \\in N_v}))$\nIn the k-th iteration, for each node v, the GNN first computes messages from its neighboring nodes u \u2208 N and the associated edge attributes e, and then aggregates these messages. The aggregated information is combined with the node's previous representation $h^{(k-1)}_v$ to update its representation $h^{(k)}_v$.\nAfter k iterations, the representation of node v captures the structural information within its k-hop neighborhood. To ensure consistency, the AGGREGATE function must be permutation-invariant to the order of neighboring nodes, while the COMBINE function should be differentiable to facilitate gradient-based optimization. In our implementation: The MESSAGE function is defined as:\n$MESSAGE^{(k)} (h^{(k-1)}_v, h^{(k-1)}_u, luv) = h^{(k-1)}_u$\nwhich directly passes the features of the neighboring nodes.\nThe AGGREGATE function sums the messages:\n$AGGREGATE^{(k)} ({h^{(k-1)}_u | u \\in N_v}) = \\sum_{u\\in N_v} h^{(k-1)}_u$\nThe COMBINE function is implemented as a two-layer multilayer perceptron (MLP):\n$h^{(k)}_v = MLP^{(k)}(h^{(k-1)}_v + \\sum_{u\\in N_v} h^{(k-1)}_u )$\nThis design follows the Graph Isomorphism Network (GIN) framework [36], which has expressive power equivalent to the Weisfeiler-Lehman (WL) graph isomorphism test. However, traditional GNN methods face issues such as over-smoothing and over-compression [38]. To address these limitations, recent studies have introduced global attention mechanisms, allowing nodes to attend to all other nodes in the graph. Inspired by [39], we incorporated multi-head self-attention into our model. The node representations are updated according to the following formulas:"}, {"title": "3.3.2 Classification and Boundary Regression", "content": "After K iterations of updating node representation, the final node embedding vectors are fed into two MLP branches. The first branch predicts whether a layer is fragmented or not, and the second branch localizes the boundary of the merging groups."}, {"title": "3.3.3 Loss Functions", "content": "The total loss function $L_{total}$ in our model is a weighted sum of three components: the classification loss $L_{cls}$, the localization loss $L_{loc}$, and the confidence loss $L_{con}$.\nwhere $\u03bb_{cls}, \u03bb_{loc},$ and $\u03bb_{con}$ are hyperparameters that balance the contribution of each loss term.\nTo address the class imbalance between positive and negative samples, we employ the focal loss [48]. The focal loss modulates the standard cross-entropy loss by adding a factor that down-weights easy examples and focuses training on hard negatives. It is defined as:\n$L_{cls} = -a_t(1-p_t)^\u03b3 log(p_t)$,\nwhere:\n$p_t = {\\begin{array}{cc}p, & \\text{if } y = 1,\\\\1-p, & \\text{if } y = 0,\\end{array}}$\nand $p\u2208 [0,1]$ is the predicted probability for the class with ground-truth label y\u2208 {0,1}. $\u03b1_t \u2208 [0,1]$ is a weighting factor for class t to address class imbalance. \u03b3\u2265 0 is the focusing parameter that adjusts the rate at which easy examples are down-weighted.\nBy modulating the loss with $(1 - p_t)^\u03b3$, the focal loss focuses learning on hard examples where p is small, thus improving the model's performance on imbalanced datasets.\nFor bounding box regression, we utilize the Complete Intersection over Union (CIoU) loss [49], which enhances the standard IoU loss by incorporating the distance"}, {"title": "3.4 Box merging and UI Layer Grouping", "content": null}, {"title": "3.4.1 NMS Algorithm", "content": "Our model regresses a bounding box for each fragmented layer. Many boxes represent the same merging area boundary. We design an algorithm to obtain a final bounding box based on a non-maximum-suppress (NMS) algorithm that is used in the object detection task. The motivation for our algorithm has two aspects, the first one is that our final result box should be large enough so that it can contain the whole UI components. It doesn't matter that our predicted boxes' are not so accurate. On the other side, simply discarding boxes that don't have maximum confidence scores may"}, {"title": "3.4.2 Fragmented Layer Grouping", "content": "It is trivial to group fragmented layers because our model regresses the bounding boxes of merging groups. Considering the overlap between background UI components and others, we sort merged boxes in ascending order according to the rectangle area and group semantically consistent layers inside smaller boxes first. We traverse the sorted bounding box list and calculate the proportion of fragmented layers that intersect with the current box. If the value exceeds the threshold, we assign this fragmented layer to the current merging groups. In this way, we can effectively group fragmented layers in a design prototype and facilitate a downstream code-generation platform to generate code of higher quality. There is another advantage of our algorithm, which is the capability of correcting errors in grouping results based on some prior knowledge. We observe that a Text layer is usually not grouped with others, while layers depicting geometric shapes are part of merging groups. When searching inside merged boxes, we can consider grouping layers that are predicted as non-fragmented but belong to a specific category (such as Oval, Rectangle, etc)."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Implementation Details", "content": null}, {"title": "4.1.1 Data preparation", "content": "Following UILM [9] and EGFE [10], we conduct experiments on their collected real-world design artboards to validate our algorithm's effectiveness. The number of fragmented layers varies greatly in different design artboards. Therefore, we sort the data according to the number of fragmented layers, after which we split the dataset into three even collections. For each collection, we split it into a training set and a test set in a consistent proportion of 8:2. Considering that a design artboard may contain a substantial amount of layers, which can result in consuming too many computing resources during graph learning, we use a sliding window with a fixed window size to cut the artboard. Graphs are constructed for layers inside each window."}, {"title": "4.1.2 Training Details", "content": "The multimodal attributes of layers are embedded into 128-dim feature vectors. We adopt ResNet50 [50] pre-trained on ImageNet to extract the layer's visual image features and only train a linear layer to transform the features into a 128-dim vector. For the high-frequency encoding, we set L to be 9 and utilize a linear layer to encode the result into an initial vector. The attention module of each layer has 4 attention heads and a hidden dimension of 128. We adopt the GINE [51] layer as the graph neural module to learn neighborhood information around each node, and the hidden dimension is 128 as well. We conduct hyperparameter experiments to determine the optimal number of graph learning blocks. As shown in Table 5, our model achieves the best performance when the number of layers is set to 9. The classification branch and the localization branch both consist of a 3-layer MLP, where the hidden dimension is set to 256. For the loss function, we set Acls as 1, Aloc as 10.0, and Acon as 5.0 also based on the results of hyperparameter experiments (Table 5). We set the IoU threshold for the NMS-fine algorithm to 0.45 and set the IoU threshold for merging associative layers to 0.7.\nThe whole algorithm is implemented by Pytorch and Pytorch-geometric library. All experiments are conducted on a Linux server with 4 Geforce-3090 GPUs. We train the cls branch for the first 50 epochs with AdamW optimizer [52] and the learning rate is set as 1e-4. Next, we train the two branches together for another 1000 epochs to converge. The initial learning rate is 1e-5, and it drops down exponentially every 10 epochs by 0.99. The training process takes around 20 hours."}, {"title": "4.2 Experimental Setting", "content": null}, {"title": "4.2.1 Baselien Models", "content": "UILM [9] develops an objection detection method to detect the bounding boxes of merging groups based on the layer's boundary prior. Then all layers inside the same bounding box are grouped."}, {"title": "4.2.2 Metrics", "content": "Common metrics, which are precision, recall, f1-score, and accuracy, are used to evaluate the classification results. UILM cannot classify layers, so we considered a layer to be fragmented if 70% of it lies inside the predicted bounding box. EGFE categorizes layers into three classes. Layers labeled 'Start-merge' and 'Merge' are considered to be fragmented because EGFE only groups layers of these two classes.\nTo evaluate the performance of the fragmented layer grouping task, we propose the following evaluation methods. The first one is that we evaluate the accuracy of predicting a similarity matrix, in which Mij=1 denotes layer i and layer j belong to the same UI component, and Mij=0 means layer i and layer j should be separated into different merging groups. We compare the predicted matrix with the ground-truth matrix to evaluate the layer grouping results of different methods. Specifically, the following formulas\n$asso\\_precision = \\frac{\\sum_{i \\in S_{gt}}\\sum_{j \\in S_{gt}} 1(M_{ij} == M_{ij}^\\prime)}{(\\#S_{gt})^2}$\n$asso\\_recall = \\frac{\\sum_{i \\in S_{pred}}\\sum_{j \\in S_{pred}} 1(M_{ij} == M_{ij}^\\prime)}{(\\#S_{pred})^2}$\nevaluate the performance of our approach in two aspects, which is similar to precision and recall. Eq.18 shows how many correct merging pairs are predicted by the algorithms, and Eq.19 evaluates the quality of fragmented layer grouping results. In the formulas, Spred, Sgt denotes the set of layers in predicted and ground-truth merging groups respectively. # denotes the number of a set. A design artboard may contain several merging groups, so we just average the results to obtain a final score."}, {"title": "4.3 Quantitative and Qualitative Analysis", "content": null}, {"title": "4.3.1 Comparison with Previous Methods", "content": "UILM adopts the 2D object detection model to locate the boundary of merging groups and find fragmented layers inside the area. However, when the bounding box of the merging groups is large, it will also contain other unrelated GUI layers that should not be merged into that merging group. Conversely, EGFE and our model classify each UI layer directly according to the multimodal information in the original design prototypes. Therefore, the UI layer classification accuracy of UILM is smaller than EGFE and our model, as shown in Table 1, due to retrieving many non-fragmented layers inside detected bounding boxes. The precision of EGFE outperforms our model a little, however, our model improves the recall by around 5.3%. Furthermore, among graph-based methods, our approach also demonstrates the best performance, with an F1 score 2.2% higher than GCNII and 1% higher than GraphGPS. The primary issue with these methods is their over-reliance on the proper tuning of hyperparameters, which limits their ability to generalize to other graph tasks. These advanced mechanisms significantly amplify the influence of relationships between components on the layer classification results. Consequently, the features of one layer may excessively affect the connected or indirectly connected layers. Compared with previous methods such as EGFE and UILM, Table 1 can validate the effectiveness of introducing graph neural modules to learn the relationship between UI layers. For example, the two graph neural models (GCN and GAT) can improve the recall and consequently, the fl-score of UI layer classification. Our method also outperforms GCN and GAT, achieving F1 score improvements of 1.03% and 1.01%, respectively. Considering the performance of fragmented layer grouping, our method outperforms previous methods in all metrics greatly. We improve asso-precision and IoU-precision by 9.7% and 6.7% respectively compared with UILM. For asso-recall and IoU-recall, our method outperforms UILM by 3.2% and 3.9% respectively.\nHere we further analyze the reported results above. UILM can detect accurate bounding boxes of merging groups but it may make mistakes for background layer grouping due to the visual overlap of layers. It assumes that all layers inside a bounding box belong to the corresponding merging group, which is the main reason for the low precision of grouping results. The main obstacle of EGFE is the class imbalance"}, {"title": "4.3.2 Qualitative Analysis", "content": "As shown in Figure 6, we visualize a typical case to elaborate on the advantages of our algorithm over previous methods. UILM does well in detecting the bounding boxes of foreground merging groups. However, it is hard for UILM to distinguish between background and foreground layers directly due to the visual overlap. EGFE and our method can achieve higher accuracy in layer classification than UILM because we directly use the multi-modal information to classify layers. However, EGFE may sometimes miss some fragmented layers. For example, EGFE can not find two background layers (which depict the shadow of a tree). EGFE also makes mistakes in grouping stroke layers and signal layers due to the wrong prediction of class 'Start-merge'. To overcome"}, {"title": "4.4 Additional Study", "content": "In this section, we present additional experimental results. We first discuss the effectiveness of our proposed strategies, including the layer graph construction based on wireframe coordinates, the self-attention module introduced in the graph learning blocks, the wireframe coordinates used as edge attributes, the improved NMS algorithm, which takes the entire overlapped box cluster into account, and the multimodal information used as input graph representation learning. To further validate the performance of our method on a wider range of real-world UI designs, we obtain an additional 508 design prototypes from the Figma community. Compared to our training dataset, these prototypes exhibit more variability in design quality and cover a broader range of business domains."}, {"title": "4.4.1 Effectiveness of Layer Graph Construction", "content": "In our approach, we start by reconstructing the structure of design prototypes based on the coordinates between layers. The idea is to break down all the original containers, allowing layers that are close in distance and spatially nested to aggregate in the graph, facilitating information flow between them during the learning process. At the same time, this approach helps eliminate irregularities in these designs, which often arise"}, {"title": "4.4.2 Effectiveness of Self-attention", "content": "We remove the self-attention module to see how long-range dependency boosts the performance of our model. Due to the over-smoothing problem, we ultimately utilize a 5-layer GINE model for this experiment. As shown in Table 3, the recall of fragmented layer prediction falls down by around 5% while the recall of grouping UI layers also declines a lot. Inspired by [39], global self-attention can help alleviate the over-smoothing, over-squashing, and other fundamental problems in graph neural networks. It is a bottleneck for GNNs to capture long-range dependency between nodes that are distant from each other in the graph. Self-attention mechanism requires nodes to attend the key-query process of all other nodes, which naturally addresses the issue of information communication bottleneck due to the limitation of graph topology. Overall, self-attention can not only improve the accuracy of retrieving fragmented layers but can improve the quality of UI layer grouping as well."}, {"title": "4.4.3 Effectiveness of Edge Attributes", "content": "To validate the effectiveness of edge attributes, we set up three baseline groups: edges with XY as attributes, edges with WH as attributes, and no encoding for edges. We believe that encoding coordinate information between UI layers is crucial because it signifies the type of merging area in UI layers. It is trivial that UI layers in UI icons lie very close while background layers spread around. Encoding the difference of adjacent nodes' wireframe information as the edge attributes can facilitate our model to regress more accurate bounding boxes of merging groups. Table 3 shows that the quality of layer grouping can improve a lot after we encode the edge attributes, which proves the necessity of edge encoding during the graph learning process. Specifically, no encoding, only XY, and only WH as attributes result in a 7.6%, 4.2%, and 5.0% decrease in asso-recall, and a 5.1%, 0.8%, and 1.7% decrease in IoU-recall, respectively."}, {"title": "4.4.4 Evaluation on Improved NMS Algorithm", "content": "The third row of Table 3 shows the results of using a standard NMS algorithm to group fragmented UI layers. As described in Section 3, the key improvement is that we evaluate the whole overlapped box cluster to obtain the final box result. Instead, the original NMS algorithm discards overlapped boxes that have non-maximum classification probability or confidence score. The experimental results show that our algorithm can improve the asso-recall and IoU-recall fl-score by 2.4% and 2.2% respectively. It seems that the improvement is not very significant. One possible explanation is that we use the confidence score instead of the classification probability as the criteria to reserve the bounding box with the maximum value. The confidence score depicts the quality of the predicted bounding box more accurately than adopting the classification score. So the bounding box with the maximum confidence score is already accurate enough. The fourth row of Table 3 further reports the results of the original NMS algorithm that utilizes classification probability instead of predicted confidence score to non-maximum suppress overlapped boxes. The asso-recall and IoU-recall fall down by 2.7% and 2.6% respectively."}, {"title": "4.4.5 Effectiveness of Multimodal Information", "content": "Table 4 presents the experimental results of our model when visual features, categories, and wireframe information are removed from the input. The performance of our model decreases as multimodal information is removed, with the accuracy of UI layer classification remaining relatively stable while the performance of fragmented layer grouping is impaired. This may be due to inaccurate regressed bounding boxes of merging groups, which affects the quality of fragmented layer grouping results.\nVisual features have the greatest influence on the performance of our model among the three types of multimodal information, as observed from Table 4. Without integrating visual information, the classification accuracy of our model slightly degrades, but the asso-recall and IoU-recall decrease by 4.9% and 5.1%, respectively. This result is uniform with human perception as we predominantly understand UI based on screenshots rather than other underlying information. The category information about UI layers has less impact on our model due to the limited number of layer categories (only 13). Wireframe information plays a critical role in edge learning during the message-passing process, explaining the considerable performance drop across all metrics upon its removal."}, {"title": "4.4.6 Evaluation on Figma Dataset", "content": "To further validate the effectiveness of our method on a broader set of UI data, we collected additional design prototypes from the Figma community. Compared to the training data, these prototypes span a wider range of application scenarios, including shopping, finance, healthcare, and travel. Given the varying levels of expertise among community designers, these prototypes exhibit greater variability in quality and more pronounced structural errors compared to data from EGFE and UILM, which are derived from commercial applications developed by large enterprises. A total of 508 design prototypes were collected and subsequently inspected and annotated by five designers to label fragmented elements as ground truth. Our Figma dataset is comparable to the original test set, with one-hot encoding updated based on the layer categories in the Figma designs. These prototypes were used exclusively for testing to evaluate our model's generalization performance beyond the original training data. As shown in Table 6, our method maintained strong performance on the Figma dataset, with only a 4.0% reduction in asso-recall and a 3.6% reduction in IoU-recall compared to the original dataset. The performance decline is likely due to the significant distribution shift across different application scenarios in the UI design domain. Future research could explore domain adaptation techniques to further improve the model's robustness."}, {"title": "5 User study", "content": "In this section, we conduct a user study to evaluate the effectiveness of applying our approach to Imgcook which is an automated code generation platform."}, {"title": "5.1 Procedures", "content": "This study recruited 10 developers, all of whom are proficient in UI development using the Vue framework, with an average of approximately three years of development experience. As described in previous sections, fragmented layers exist in UI icons, decorative components, and background components. For each category, we randomly pick five design prototypes and fetch corresponding components which all contain fragmented layers. We then generate front-end code by imgcook automatically. For the control group, developers evaluate and modify the front-end code of the original UI components containing fragmented layers. For the experimental group, the original design prototypes are processed and merged by our approach before code generation. The developers modify the front-end code to reach acceptable industry standards based on their own experience. Before the evaluation, each developer is given enough time to get familiar with the UI components. We use the git service to record the lines of code modified, and we also record the time of modifying front-end code by the developers. The fewer lines of code are modified, the higher the code availability is. So we evaluate the code availability by the formula:\n$availability = 1 - \\frac{number \\:of \\:modified\\:lines}{total \\:number\\:of\\:lines}$The developers do not know we record the time as the time pressure may affect their modification speed. When the developers finish the code modification for one UI component, they mark the generated code on a five-point Likert scale for readability and maintainability respectively (1: not readable and 5: strongly readable, so is maintain-ability). All developers evaluate and modify front-end code individually and they all do not know which code is merged by our approach."}, {"title": "5.2 Results", "content": "Table 7 shows that the number of code lines modified in the experimental group is less than the control group for all three categories. Furthermore, the time spent on modifying code to reach acceptable industry standards in the experimental group is also shorter than that in the control group. For example, the availability of code is improved by 7.3% and 7.9% for UI icons and decorative UI patterns. The average modification time is reduced by 42.1% with our approach. It demonstrates that the availability of generated code is improved a lot by adopting our approach to merge fragmented layers in UI design prototypes before code generation. For readability and maintainability, Table 7 shows that the developers generally mark higher scores for front-end code after merging fragmented layers. The average score of readability and maintainability in the experimental group is 44.6% and 48.0% more than that in the control group respectively. Above all, the generated code by imgcook has higher availability, readability, and maintainability after merging associative fragmented layers in design prototypes by our approach.\nTo understand the"}]}