{"title": "Beyond Instruction Following:\nEvaluating Rule Following of Large Language Models", "authors": ["Wangtao Sun", "Chenxiang Zhang", "Xueyou Zhang", "Ziyang Huang", "Haotian Xu", "Pei Chen", "Shizhu He", "Jun Zhao", "Kang Liu"], "abstract": "Although Large Language Models (LLMs)\nhave demonstrated strong instruction-\nfollowing ability to be helpful, they are further\nsupposed to be controlled and guided by\nrules in real-world scenarios to be safe, and\naccurate in responses. This demands the\npossession of rule-following capability of\nLLMs. However, few works have made a clear\nevaluation of the rule-following capability of\nLLMs. Previous studies that try to evaluate\nthe rule-following capability of LLMs fail\nto distinguish the rule-following scenarios\nfrom the instruction-following scenarios.\nTherefore, this paper first makes a clarification\nof the concept of rule-following, and curates\na comprehensive benchmark, RuleBench, to\nevaluate a diversified range of rule-following\nabilities. Our experimental results on a variety\nof LLMs show that they are still limited in\nfollowing rules. Our further analysis provides\ninsights into the improvements for LLMs\ntoward a better rule-following intelligent\nagent. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-\nfollowing-B3E3/", "sections": [{"title": "Introduction", "content": "Benefiting from a vast amount of pre-training data\nand the enormous parameters, the Large Language\nModels (LLMs) can accomplish numerous Natu-\nral Language Processing (NLP) tasks because of\ntheir instruction-following ability. However, in\nreal-world applications, people often expect LLMs\nto generate outputs that conform to various rules.\nFor example, when planning behavioral actions for\nmultimodal agents, we expect LLMs to adhere to\nthe physical rules of the real world. Humans use\nthese rules to efficiently communicate with each\nother and quickly adapt to a specific domain. In\npursuit of achieving Artificial General Intelligence\n(AGI), we are expecting LLMs to possess such\nrule-following capabilities.\nIt leads to the research on the rule-following of\nLLMs. With rule-following capability, humans can\ndirectly and efficiently manipulate the behavior of\nLLMs using natural language rules as a medium,\nthereby correcting the policy exhibited by LLMs\nin specific downstream tasks.\nSome recent studies"}, {"title": "Related Work", "content": "2.1 Rule-enhanced LLM Reasoning\nWhile LLMs have demonstrated remarkable zero-\nshot reasoning capabilities in many downstream\ntasks, they still generate outputs that do not con-\nform to logic or human preference. Some re-\nsearch studies have found that compared with the\nreasoning enhancement methods based on LLMs\nthemselves like Chain-of-Thought, Self-reflection, and Self-\nrefinement, providing LLMs\nwith relevant rules with Retrieval-Augmented Gen-\neration (RAG) paradigm do better in helping them\nconduct reasoning in the downstream tasks.\nHowever, the rule-following capabil-\nity of LLMs is far from satisfactory. Few works\nhave comprehensively evaluated whether LLMs\ncan benefit from the provided rules under different\nscenarios and how LLMs can follow rules better.\nTo make up for this gap, this paper conducted a\nseries of experiments to evaluate the rule-following\ncapabilities of several State-of-The-Art LLMs and\nprovide some insights into how LLMs can follow\nrules better.\n2.2 LLMs Instruction-following\nInstruction-following has been generally consid-\nered an important capability of LLMs and some previous works have been\ndone to evaluate the instruction-following capabil-\nity of LLMs. However, only a few works have cast their attention\nto the question of rule-following. Recent works fo-\ncused on the rule-following capability of LLMs\nfailed to dis-\ntinguish rule-following from instruction-following.\nThis paper instead proposes the scenario of rule-\nfollowing and sets up useful baselines for future\nworks."}, {"title": "RuleBench", "content": "To construct RuleBench, we have leveraged and re-\nprocessed the existing reasoning benchmarks for\ndifferent rule-following scenarios, including rela-\ntion extraction, content moderation, commonsense\nQA, science QA, and judgment prediction. The de-\ntails of the construction of each benchmark are as\nfollows and the prompts used during constructing\nRuleBench can be found in Appendix A.\n\u2022 CLUTRR Suite CLUTRR\ncontains a large set of semi-synthetic stories in-\nvolving hypothetical families. Given a story, the\ngoal is to infer the kinship between two family\nmembers, which is not explicitly mentioned in\nthe story. The testing set of CLUTRR contains\n1048 samples in all, with their reasoning hops\nvarying from 2 to 10. As the suite CLUTRR\ncontains the oracle relation chain for each data\nsample itself, we write a deterministic function\nto transform this information into the rule for\neach data sample. For the answer evaluation, we\nextract all the kinships mentioned in the answer\ntexts and select the last one to compare with the\nground truth kinship.\n\u2022 SALAD We adopt SALAD, a\nsafety benchmark specifically designed for eval-\nuating LLMs, for the scenario of content moder-\nation. Given a piece of toxic text, the goal is to\nclassify it into one of 6 different categories. The\ntesting set of SALAD contains 5939 samples in\nall. As there is no auxiliary inference informa-\ntion contained in SALAD, we adopt ChatGPT\nto generate a corresponding inferential rule for\neach data sample. Specifically, we create a rule\ngeneration instruction and two demonstrations\nmanually. They are prompted to ChatGPT to-\ngether with each sample in SALAD. Based on\nIn-context Learning (ICL), ChatGPT will gen-\nerate a corresponding inferential rule for each\nsample. For the answer evaluation, we extract"}, {"title": "Evaluation", "content": "To comprehensively evaluate the rule-following ca-\npabilities of LLMs, this paper has designed 5 main\nparts of experiments. We evaluate the effects of\nrule quantity (\u00a74.2), rule form (\u00a74.3), the presence\nof CoT when applying rules (\u00a74.4), and rule fac-\ntuality (\u00a74.5). Besides, we analyzed the failure\ncases of rule-following from a behavioral perspec-\ntive, classifying them into Triggering Error and\nExecution Error (\u00a74.6). Finally, we categorize the\nrule-following capabilities into 5 dimensions and\ncompare the performances of 8 State-of-The-Art\nLLMs (\u00a74.7).\n4.1 Model Selections\nFor open-source LLMs, we adopt Llama-2-7b-\nchat, Meta-Llama-3-8B,\nMistral-7B-Instruct-v0.2, Yi, and Phi-\n3. For closed-source LLMs, we\nadopt gpt-3.5-turbo, gpt-4-turbo, and gpt-40 from OpenAI. The comprehensive per-"}, {"title": "Rules Are Helpful for the Reasoning of LLMS", "content": "To evaluate whether rules are helpful for the rea-\nsoning of LLMs, we adopt the following settings\nto test the LLMs.\n\u2022 No Rule. This setting simply prompts the LLMs\nwith the original question and without the rules.\n\u2022 Golden Rule. This setting prompts the LLMs\nwith the golden rule (i.e. a correct rule that\nshould be applied to the question) together with\nthe original question.\n\u2022 Few Rule. This setting prompts the LLMs with\nthe golden rule and two random irrelevant rules\ntogether with the original question.\n\u2022 All Rule. This setting is similar to Few Rule\nwhile the number of irrelevant rules increases to\n30. This setting simulates a scenario where users\nprompt the LLMs with all possible rules in the\ntasks instead of the relevant rules retrieved based\non the query.\nAll these rule settings are tested in a zero-\nshot manner. As shown in Figure 3, in most\ncases, LLMs enjoy great performance improve-\nments while being prompted with one golden infer-\nential rule (No Rule \u2192 Golden Rule). Neverthe-\nless, as the number of irrelevant rules increases,\nLLMs will find it hard to trigger and leverage\nthe golden rule and thus have a performance drop\n(Golden Rule \u2192 Few Rule \u2192 All Rule)."}, {"title": "LLMs Prefer Natural Language Rules than Formal Language Rules", "content": "Formal language is widely used in early Artifi-\ncial Intelligence, which is able to conduct efficient\nand generalized reasoning. However, LLMs have\nshown competitive or even superior reasoning per-\nformance over traditional formal language rule-\nbased engines, i.e. Knowledge Graphs. In contrast to formal language rule-based\nreasoning, reasoning with LLMs is more flexible\nand robust to various data and tasks. Therefore, we\nwould like to know if we can combine these two\nparadigms, i.e. whether LLMs can follow formal\nlanguage rules.\nTo evaluate whether LLMs can follow formal\nlanguage rules, we transform the natural language\nrules of each benchmark into the form of First-\nOrder Logic (FOL) by executing deterministic func-\ntions or prompting ChatGPT. Then we compare\nthe reasoning performances of LLMs which are\nprompted by different forms of rules in both zero-\nshot All Rule and Few Rule settings.\nAs shown in Figure 12, in most cases, LLMs con-\nduct reasoning better with natural language rules\nthan formal language rules. This aligns with our\nintuition that LLMs are mostly pre-trained with"}, {"title": "Chain of Thought Is Inadequate for LLMs to Apply Rules", "content": "Chain-of-Thought has been\nwidely verified as a useful prompting technique to\nhelp LLMs conduct multi-hop reasoning. To eval-\nuate whether LLMs can use CoT to apply rules in\nthe rule-following scenario, we choose the few-shot\nGolden Rule and Few Rule settings. We manu-\nally created two demonstrations with CoT and two\ndemonstrations without CoT under such settings\nfor LLMs to conduct In-context Learning.\nHowever, as shown in Figure 5, LLMs with CoT\nhave not exhibited stronger rule-following perfor-\nmances in most cases. This may be attributed to the\nlack of planning of CoT. CoT conducts straight-\nforward reasoning from the question to the answer\nwith multiple reasoning hops. However, when ap-\nplying rules, it involves trying to apply each rule\nto the current question and thinking about whether\nto execute this rule. Therefore, plain CoT is inad-\nequate for LLMs to apply rules. Prompting tech-\nniques or\ndecoding algorithms that involve planning steps are needed for helping\nLLMs to apply rules."}, {"title": "LLMs Struggle to Follow Counterfactual Rules", "content": "Although we have verified the effectiveness of the\nrules, it is still unclear whether LLMs completely\nfollow the given rules or use their parametric knowl-\nedge. Therefore, we designed the scenario of coun-\nterfactual rule-following.\nTo evaluate whether LLMs can follow counter-\nfactual rules, we construct corresponding coun-\nterfactual benchmarks and rule sets of CLUTRR,\nSALAD, ULogic, and CAIL2018. Specifically, we\nreplace the ground truth of each question and the\nconclusion of the corresponding rule with a ran-\ndom incorrect answer. For example, for the ques-\ntion: its ground truth:\ngrandson, and its corresponding rule: if A has a\ngranddaughter B, B has a brother C, and A is male,\nC is male, then C is the grandson of A., we replace\nthe word grandson in both ground truth and the\nrule with another random kinship to construct the\ncounterfactual data sample.\nAs shown in Figure 6, in most cases of both\nGolden Rule and Few Rule settings, LLMs have\nsignificant performance drops when following\ncounterfactual rules, compared with following fac-\ntual rules. These results indicate that the perfor-\nmance improvements brought by following rules\nare actually partly attributed to the parametric\nknowledge of LLMs, besides following rules."}, {"title": "Behavioral Analysis of LLMs Following Rules", "content": "To understand why LLMs fail to follow the given\nrule in the reasoning process, we made a behavioral\nanalysis of LLMs in the failure cases of LLMs rule-\nfollowing. Specifically, we adopt the few-shot Few\nRule settings for LLMs to follow the rule-applying\ndemonstrations to apply the given rules to the cur-\nrent question. We ordered the LLMs first to choose\na rule to follow and then reason with it. By pars-\ning the output of LLMs we can classify the failure\ncases of LLMs rule-following into two categories:\nTriggering Error and Execution Error. Triggering\nError indicates that the LLMs choose an irrelevant\nrule for the current case and therefore lead to an in-\ncorrect reasoning result. Execution Error indicates\nthat although LLMs have chosen the correct rule\nfor the current case, they fail to draw the correct\nconclusion of rule body. To faithfully describe the\nrule-following behavior of LLMs instead of being\naffected by the parametric knowledge of LLMs, we\nrun the analysis under the counterfactual settings\nof the selected benchmarks.\nFrom the results shown in Figure 7, we can tell\nthat when tackling different tasks, LLMs exhibit\ndifferent behaviors in following rules. While rules\nhave a heavy head for triggering (e.g. in CLUTRR\nand CAIL2018, the rule head will be a series of re-\nlation hops among characters), the LLMs are likely\nto make Triggering Errors. While the rule head is\ncommonsensical (e.g. in SALAD and ULogic), but\nthe conclusion of the rule body is ambiguous or\nconfused (the counterfactual scenario), the LLMs\nare likely to make Execution Errors.\nTo avoid Triggering Errors in the scenario\nof rule-enhanced reasoning with RAG paradigm\n(\u00a72.1), the rule retriever plays a crucial role. The\nTriggering Errors can be eliminated if the rule re-\ntriever only retrieved the golden rules. However,\nexisting works often employ simple sparse retriev-\ners such as BM25 which greatly compromises\nthe rule-following performance of LLMs.\nTo avoid Execution Errors in following rules, the\nLLMs need to faithfully execute the rule body and\navoid generating conclusions of illusions. There-\nfore, users may avoid letting LLMs follow the rules\nthat are counterfactual or out of the pre-trained dis-\ntribution of LLMs before they fine-tune the LLMs\nto adapt to those domains or specific tasks."}, {"title": "Rule Following Capabilities of LLMs", "content": "To make a comprehensive evaluation of the rule-\nfollowing capability of the LLMs, we categorize\nthe experimental results in the previous sections\ninto 5 dimensions:\n\u2022 Executing Rules. We average the results in all\nGolden Rule settings to obtain the capability\nof Execution Rules of LLMs. This capability\nindicates how much the LLMs can follow the\ngiven golden rule.\n\u2022 Triggering Rules. We average the results in all\nAll Rule settings to obtain the capability of Trig-\ngering Rules of LLMs. This capability indicates\nhow much the LLMs can resist the interruption\nof irrelevant rules and find the golden rule.\n\u2022 Following Formal Rules. We average all the\nresults with formal language rules to obtain the\ncapability of Following Formal Rules of LLMs.\nThis capability indicates how much the LLMs\ncan leverage the formal language rules to conduct\nreasoning.\n\u2022 Applying Rules. We average all the results\nwhere LLMs apply rules with CoT to obtain the\ncapability of Applying Rules of LLMs. This ca-\npability indicates how much the LLMs can apply\nthe rules with Chain-of-Thought.\n\u2022 Following Counterfactual Rules. We average\nall the results with counterfactual rules to ob-\ntain the capability of Following Counterfactual\nRules of LLMs. This capability indicates how\nmuch the LLMs can follow counterfactual rules.\nAs shown in Figure 2, while the closed-source\nLLMs show dominant performances in the sce-\nnario of rule-following, some open-source LLMs,\nlike Llama-3-8B, exhibit competitive performances\nand have balanced capabilities in all dimensions.\nAmong the closed-source LLMs, gpt-4-turbo is\nmore capable of following formal language rules\nwhile gpt-3.5-turbo shows a stronger capability of\nfollowing counterfactual rules.\nGenerally, LLMs are not very good at rule-\nfollowing. This may be attributed to the lack of\ntraining in rule-following in the current LLMs. As\nInstruction Fine-Tuning (IFT) has been a stan-\ndard step in the pipeline of training LLMs and thus\nensures their strong instruction-following capabil-\nity, we think that a Rule-Following Fine-Tuning\n(RFFT) steps could fundamentally enhance the rule-\nfollowing capability of LLMs."}, {"title": "Conclusion", "content": "In this paper, We introduce rule-following as a vital\ncapability of LLMs and distinguish it from the pre-\nvious labors on instruction-following. We then con-\nstruct and propose a new benchmark, RuleBench,\nfor evaluating the rule-following capabilities of"}, {"title": "Limitations", "content": "Although the evaluation results in this paper have\nillustrated the preference of LLMs in following\nrules, we have not yet proposed an effective method\nto help LLMs follow the rules better in a fixed given\nsetting."}, {"title": "Ethics Statement", "content": "Our research aims to evaluate the rule-following ca-\npability of LLMs. To mitigate risks associated with\nsome sensitive content in the benchmark, we re-\nstrict access to authorized researchers who adhere\nto strict ethical guidelines. These measures safe-\nguard research integrity while minimizing potential\nharm."}, {"title": "A Prompts for Constructing RuleBench", "content": null}]}