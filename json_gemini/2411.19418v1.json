{"title": "PROTO SUCCESSOR MEASURE: REPRESENTING THE\nSPACE OF ALL POSSIBLE SOLUTIONS OF\nREINFORCEMENT LEARNING", "authors": ["Siddhant Agarwal", "Harshit Sikchi", "Peter Stone", "Amy Zhang"], "abstract": "Having explored an environment, intelligent agents should be able to transfer their\nknowledge to most downstream tasks within that environment. Referred to as\n\u201czero-shot learning,\u201d this ability remains elusive for general-purpose reinforcement\nlearning algorithms. While recent works have attempted to produce zero-shot RL\nagents, they make assumptions about the nature of the tasks or the structure of the\nMDP. We present Proto Successor Measure: the basis set for all possible solutions\nof Reinforcement Learning in a dynamical system. We provably show that any\npossible policy can be represented using an affine combination of these policy\nindependent basis functions. Given a reward function at test time, we simply need\nto find the right set of linear weights to combine these basis corresponding to the\noptimal policy. We derive a practical algorithm to learn these basis functions using\nonly interaction data from the environment and show that our approach can produce\nthe optimal policy at test time for any given reward function without additional\nenvironmental interactions. Project page: agarwalsiddhant10.github.io/psm.html", "sections": [{"title": "1 INTRODUCTION", "content": "A wide variety of tasks can be defined within an environment (or any dynamical system). For\ninstance, in navigation environments, tasks can be defined to reach a goal, path following, reach a\ngoal while avoiding certain states etc. Once familiar with an environment, humans have the wonderful\nability to perform new tasks in that environment without any additional practice. For example,\nconsider the last time you moved to a new city. At first, you may have needed to explore various\nroutes to figure out the most efficient way to get to the nearest supermarket or place of work. But\neventually, you could probably travel to new places efficiently the very first time you needed to get\nthere. Like humans, intelligent agents should be able to infer the necessary information about the\nenvironment during exploration and use this experience for solving any downstream task efficiently.\nReinforcement Learning (RL) algorithms have seen great success in finding a sequence of decisions\nthat optimally solves a given task in the environment (Wurman et al., 2022; Fawzi et al., 2022). In\nRL settings, tasks are defined using reward functions with different tasks having their own optimal\nagent policy or behavior corresponding to the task reward. RL agents are usually trained for a given\ntask (reward function) or on a distribution of related tasks; most RL agents do not generalize to\nsolving any task, even in the same environment. While related machine learning fields like computer\nvision and natural language processing have shown success in zero-shot (Ramesh et al., 2021) and\nfew-shot (Radford et al., 2021) adaptation to a wide range of downstream tasks, RL lags behind in\nsuch functionalities. Unsupervised reinforcement learning aims to extract reusable information such\nas skills (Eysenbach et al., 2019; Zahavy et al., 2023), representations (Ghosh et al., 2023; Ma et al.,\n2023), world-model (Janner et al., 2019; Hafner et al., 2020), goal-reaching policies (Agarwal et al.,\n2024; Sikchi et al., 2024a), etc, from the environment using data independent of the task reward to\nefficiently train RL agents for any task. Recent advances in unsupervised RL (Wu et al., 2019; Touati\n& Ollivier, 2021; Blier et al., 2021b; Touati et al., 2023) have shown some promise towards achieving\nzero-shot RL."}, {"title": "3 PRELIMINARIES", "content": "In this section we introduce some preliminaries and define terminologies that will be used in later\nsections. We begin with some MDP fundamentals and RL preliminaries followed by a discussion on\naffine spaces which form the basis for our representation learning paradigm."}, {"title": "3.1 MARKOV DECISION PROCESSES", "content": "A Markov Decision Process is defined as a tuple $(S, A, P, r, \\gamma, \\mu)$ where S is the state space, A\nis the action space, $P : S \\times A \\rightarrow \\triangle(S)$ is the transition probability ($\\triangle(\\cdot)$ denotes a probability\ndistribution over a set), $\\gamma \\in [0, 1)$ is the discount factor, $\\mu$ is the distribution over initial states and\n$r:S\\times A\\rightarrow \\mathbb{R}$ is the reward function. The task is specified using the reward function r and the\ninitial state distribution $\\mu$. The goal for the RL agent is to learn a policy $\\pi_{\\theta} : S\\leftarrow A$ that maximizes\nthe expected return $J(\\pi_{\\theta}) = E_{s_0\\sim\\mu}E_{\\pi_{\\theta}} [\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_t, a_t)]$.\nIn this work, we consider a task-free MDP which does not provide the reward function or the initial\nstate distribution. Hence, a task-free or reward-free MDP is simply the tuple (S, A, P, $\\gamma$). A task-\nfree MDP essentially only captures the underlying environment dynamics and can have infinite\ndownstream tasks specified through different reward functions.\nThe state-action visitation distribution, $d^{\\pi}(s, a)$ is defined as the normalized probability of being in a\nstate s and taking an action a if the agent follows the policy $\\pi$ from a state sampled from the initial state\ndistribution. Concretely, $d^{\\pi}(s, a) = (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^{t}P(s_t = s, a_t = a)$. A more general quantity,\nsuccessor measure, $M^{\\pi}(s, a, s', a')$, is defined as the probability of being in state s' and taking\naction a' when starting from the state-action pair s, a and following the policy $\\pi$. Mathematically,\n$M^{\\pi}(s,a,s',a') = (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^{t}P(s_t = s', a_t = a'|s_0 = s, a_0 = a)$. The state-action\nvisitation distribution can be written as $d^{\\pi}(s, a) = E_{s_0\\sim\\mu(s),a_0\\sim\\pi(a_0|s_0)} [M^{\\pi}(s_0, a_0, s, a)]$.\nBoth these quantities, state-action visitation distribution and successor measure, follow the Bellman\nFlow equations:\n$d^{\\pi}(s, a) = (1 - \\gamma)\\mu(s) \\pi(a|s) +\\gamma \\sum_{s'\\in S, a'\\in A} P(s|s', a')d^{\\pi}(s', a')\\pi(a|s).$ (1)\nFor successor measure, the initial state distribution changes to an identity function\n$M^{\\pi}(s,a, s', a') = (1 - \\gamma)\\mathbb{1}[s = s', a = a'] + \\gamma \\sum_{s'\\in S, a'\\in A} P(s'|s', a')M^{\\pi}(s,a, s', a')\\pi(a'|s').$ (2)\nThe RL objective has a well studied linear programming interpretation (Manne, 1960). Given any\ntask reward function r, the RL objective can be rewritten in the form of a constrained linear program:\n$\\max_{d} E_{d(s, a)}r(s,a), \\text{ s.t. } d(s,a) \\geq 0 \\ \\forall s, a, \\\\ \\text{s.t. } d(s, a) = (1 - \\gamma)\\mu(s) \\pi(a|s) + \\gamma \\sum_{s'\\in S, a'\\in A} P(s|s', a')d(s', a')\\pi(a|s)$ (3)\nand the unique policy corresponding to visitation d is obtained by $\\pi(a|s) = \\frac{d(s,a)}{\\sum_{a} d(s,a)}$. The Q function\ncan then be defined using successor measure as $Q^{\\pi}(s, a) = \\sum_{s',a'} M^{\\pi}(s,a, s', a')r(s', a')$ or\n$Q^{\\pi} = M^{\\pi}r$. Obtaining the optimal policies requires maximizing the Q function which requires\nsolving $\\arg \\max_a Mr$."}, {"title": "3.2 AFFINE SPACES", "content": "Let V be a vector space and b be a vector. An affine set is defined as $A = b+V = {x|x = b+v, v \\in V}$.\nAny vector in a vector space can be written as a linear combination of basis vectors, i.e., $v = \\sum_{i}^{n} a_i v_i$\nwhere n is the dimensionality of the vector space. This property implies that any element of an affine\nspace can be expressed as $x = b + \\sum_{i}^{n} a_i v_i$. Given a system of linear equations Ax = c, with A\nbeing an m \u00d7 n matrix (m < n) and c \u2260 0, the solution x forms an affine set. Hence, there exists\nalphas a such that $x = b + \\sum_{i}^{n} a_i x_i$. The vectors ${x_i}$ form the basis set of the null space or kernel\nof A. The values ${a_i}$ form the affine coordinates of x for the basis ${x_i}$. Hence, for a given system\nwith known ${x_i}$ and b, any solution can be represented using only the affine coordinates ${a_i}$."}, {"title": "4 THE BASIS SET FOR ALL SOLUTIONS OF RL", "content": "In this section, we introduce the theoretical results that form the foundation for our representation\nlearning approach. The goal is to learn policy-independent representations that can represent any valid\nvisitation distribution in the environment (i.e. satisfy the Bellman Flow constraint in Equation 3). With\na compact way to represent these distributions, it is possible to reduce the policy optimization problem\nto a search in this compact representation space. We will show that state visitation distributions and"}, {"title": "5 METHOD", "content": "In this section, we start by introducing the core practical algorithm for representation learning inspired\nby the theory discussed in Section 4 for obtaining $\\Phi$ and b. We then discuss the inference step, i.e.,\nobtaining w for a given reward function."}, {"title": "5.1 LEARNING I AND b", "content": "For a given policy $\\pi$, its successor measure under our framework is denoted by $M^{\\pi} = \\Phi w^{\\pi} + b$ with\n$w^{\\pi}$ the only object depending on policy. Given an offline dataset with density $\\rho$, we follow prior\nworks (Touati & Ollivier, 2021; Blier et al., 2021b) and model densities $m^{\\pi} = M^{\\pi} /\\rho$ learned with\nthe following objective:\n$\\mathcal{L}^{\\pi} (\\Phi, b, w^{\\pi}) = -E_{s,a\\sim\\rho}[m_{\\Phi,b,w^{\\pi}} (s, a, s, a)] \\\\ + \\frac{1}{2} E_{s,a,s'\\sim\\rho,s', a'\\sim\\rho}[m_{\\Phi,b,w^{\\pi}} (s, a, s', a') - \\gamma m_{\\Phi,b,w^{\\pi}} (s', \\pi(s'), s',a')].$ (7)\nThe above objective only requires samples (s, a, s') from the reward-free dataset and a random\nstate-action pair (s', a') (also sampled from the same data) to compute and minimize $\\mathcal{L}(\\pi)$.\nA $\\Phi$ and b that allows for minimizing the $\\mathcal{L}(\\pi)$ for all $\\pi \\in \\Pi$ forms a solution to our representation\nlearning problem. But how do we go about learning such $\\Phi$ and b? A na\u00efve way to implement\nlearning $\\Phi$ and b is via a bi-level optimization. We sample policies from the policy space of $\\Pi$, for\neach policy we learn a $w^{\\pi}$ that optimizes the policy evaluation loss (Eq 7) and take a gradient update\nw.r.t $\\Phi$ and b. In general, the objective can be optimized by any two-player game solving strategies\nwith [$\\Phi$, b] as the first player and $w^{\\pi}$ as the second player. Instead, in the next section, we present an\napproach to simplify learning representations to a single-player game."}, {"title": "5.2 SIMPLIFYING OPTIMIZATION VIA A DISCRETE CODEBOOK OF POLICIES", "content": "Learning a new $w^{\\pi}$ for each specific sampled policy $\\pi$does not leverage precomputations and\nrequires retraining from scratch. We propose parameterizing w to be conditional on policy, which\nallows leveraging generalization between policies that induce similar visitation and as we show, will\nallow us to simplify the two player game into a single player optimization. In general, policies are\nhigh-dimensional objects and compressing them can result in additional overhead. Compression\nby parameterizing policies with a latent variable z is another alternative but presents the challenge\nof covering the space of all possible policies by sampling z. Instead, we propose using a discrete\ncodebook of policies as a way to simulate uniform sampling of all possible policies with support in\nthe offline dataset.\nDiscrete Codebook of Policies: Denote z as a compact representation of policies. We propose\nto represent z as a random sampling seed that will generate a deterministic policy from the set of\nsupported policies as follows:\n$\\pi(a|s, z) = \\text{Uniform Sample(seed} = z + \\text{hash}(s)).$ (8)\nThe above sampling strategy defines a unique mapping from a seed to a policy. If the seed generator\nis unbiased, the approach provably samples from among all possible deterministic policies uniformly.\nNow, with policy $\\pi_z$ and $w_z$ parameterized as a function of z we derive the following single-player\nreduction to learn $\\Phi$, b, $w_z$ jointly.\nPSM-objective: $\\arg \\min_{\\Phi,b,w(z)} E_z [\\mathcal{L}^{\\pi_z} (\\Phi, b, w(z))].$ (9)"}, {"title": "5.3 FAST OPTIMAL POLICY INFERENCE ON DOWNSTREAM TASKS", "content": "After obtaining $\\Phi$ and b via the pretraining step, the only parameter to compute for obtaining\nthe optimal Q function for a downstream task in the MDP is w. As discussed earlier, $Q^* =$\n$\\max_w (\\Phi w + b)r$ but simply maximizing this objective will not yield a Q function. The linear\nprogram still has a constraint of $\\Phi w + b > 0,\\forall s, a$. We solve the constrained linear program by\nconstructing the Lagrangian dual using Lagrange multipliers $\\lambda(s, a)$. The dual problem is shown in\nEquation 10. Here, we write the corresponding loss for the constraint as $\\min(\\Phi w + b, 0)$.\n$\\max_{\\lambda \\geq 0} \\min_W - \\Phi w r - \\sum_{s,a} \\lambda(s, a) \\min(\\Phi w + b, 0).$ (10)\nOnce $w^*$ is obtained, the corresponding $M^*$ and $Q^*$ can be easily computed. The policy can be\nobtained as $\\pi^* = \\arg \\max_a Q^* (s, a)$ for discrete action spaces and via DDPG style policy learning\nfor continuous action spaces."}, {"title": "6 CONNECTIONS TO SUCCESSOR FEATURES", "content": "In this section, we uncover the theoretical connections between PSM and successor features. Succes-\nsor Features (Barreto et al., 2017) ($\\psi^{\\pi} (s, a)$) are defined as the discounted sum of state features $\\phi(s)$,\n$\\psi^{\\pi} (s, a) = E_{\\pi}[\\sum_{t} \\gamma^{t}\\phi(s_t)]$. These state features can be used to span reward functions as $r = \\phi z$.\nUsing this construction, the Q function is linear in z as $Q(s, a) = \\psi^{\\pi}(s,a)z$. We can establish a\nsimple relation between $M^{\\pi}$ and $\\psi^{\\pi}$, $\\psi^{\\pi}(s,a) = \\int_{s'} M^{\\pi} (s,a, s')p(s')ds'$. This connection shows\nthat, like successor measures, successor features can also be represented using a similar basis."}]}