{"title": "Understanding Epistemic Language with a Bayesian Theory of Mind", "authors": ["Lance Ying", "Tan Zhi-Xuan", "Lionel Wong", "Vikash Mansinghka", "Joshua B. Tenenbaum"], "abstract": "How do people understand and evaluate claims about others' beliefs, even though these beliefs cannot be directly observed? In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents' goals, beliefs, and intentions: a language-augmented Bayesian theory-of-mind (LaBToM). By translating natural language into an epistemic \"language-of-thought\", then evaluating these translations against the inferences produced by inverting a probabilistic generative model of rational action and perception, LaBTOM captures graded plausibility judgments about epistemic claims. We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent's beliefs. In contrast with multimodal LLMs (GPT-40, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.", "sections": [{"title": "Introduction", "content": "People regularly use and interpret language about other agents' beliefs, evaluating rich linguistic constructions that may involve claims about what others' consider necessary (Grace thinks that if Katie didn't eat the cookie, it must have been Jane", "Tom believes Sam is more likely to win the election than the others\"), or the relationship of their beliefs to the world (\u201cJohn didn't know that today was a holiday\"). But given that these beliefs are not directly observable, how do people evaluate the truth or plausibility of epistemic claims? Philosophers and linguists have long investigated the semantics of epistemic language (Hintikka, 1962; Partee, 1973; Loar, 1981), offering detailed theories of how epistemic claims relate to the sets of worlds deemed possible by an agent, and how the truth of these claims derive from their constituent predicates or propositions (Von Fintel and Heim, 2011). However, these theories do not explain how people understand and evaluate epistemic language in its context of utterance. If someone says \u201cAlice believes it might rain\", how does her behavior (e.g. bringing an umbrella) render that statement more or less plausible?\nIn this paper, we introduce a cognitive model of how humans interpret epistemic language in context (Figure 1), grounded in inferences about what agents believe given their actions and observations. We build upon a framework known as Bayesian Theory-of-Mind (BToM), which casts human mentalizing as Bayesian inference over a generative model of rational action and perception (Baker et al., 2017; Jara-Ettinger et al., 2019). We combine this framework with the compositionality afforded by probabilistic extensions of the language-of-thought hypothesis (Piantadosi, 2011; Goodman and Lassiter, 2015), developing an epistemic language of thought (ELOT) to represent how others represent the world. Using large language models (LLMs) as flexible semantic parsers, we translate natural language into this ELoT representation (Wong et al., 2023), allowing us to quantitatively evaluate epistemic claims against BToM inferences. This gives us our full model: A language-augmented Bayesian theory-of-mind (LaBToM).\nTo evaluate our model, we run an experiment where participants watch animations of a player solving a gridworld puzzle called Doors, Keys, & Gems (Zhi-Xuan et al., 2020). In these puzzles (Figure 1b), the player has to pick up (single-use)\"\n    },\n    {\n      \"title\": \"Related Work\",\n      \"content\": \"Semantics of Epistemic Language. Sentences about belief and knowledge have been studied at length in philosophy and linguistics (Hawthorne et al., 2016; Egan and Weatherson, 2011; Yalcin, 2007). Standard treatments associate belief sentences with sets of compatible worlds (Hintikka, 1962; Von Fintel and Heim, 2011), while more recent work grounds the gradedness of belief claims in probabilities (Lassiter, 2017;\"\n    },\n    {\n      \"title\": \"Cognitive Models of Language Interpretation\",\n      \"content\": \"In contrast to the above line of research, our goal is to build a cognitive model of epistemic language understanding, drawing upon accounts of word meaning that prioritize the role of mental states (Loar, 1981; Block, 1987; Lake and Murphy, 2023), including cognitive semantics (Lakoff, 1988; Jackendoff, 2003), functional role semantics (Harman, 1982), and work that grounds linguistic meaning in a (probabilistic) language of thought (Fodor et al., 1975; Goodman and Lassiter, 2015; Wong et al., 2023; Zhang et al., 2023).\"\n    },\n    {\n      \"title\": \"Bayesian Theory-of-Mind\",\n      \"content\": \"To model how epistemic sentences relates to (inferred) mental states, our paper builds upon work in Bayesian Theory-of-Mind (Baker et al., 2017; Zhi-Xuan et al., 2022), along with related work on epistemic action understanding (Croom et al., 2023; Shvo et al., 2020), leveraging the connection between symbolic representations of the world (used in planning and inverse planning (McDermott et al., 1998)) and our representations of others' minds.\"\n    },\n    {\n      \"title\": \"Theory-of-Mind in Language Models\",\n      \"content\": \"With recent advances in the capabilities of neural large language models (LLMs), some researchers have suggested that LLMs might serve as cognitive models (Binz and Schulz, 2023), including as models of theory-of-mind (Strachan et al., 2024). However, while LLMs achieve human- like performance on some ToM tasks, they do not reliably generalize to simple alterations (Shapira et al., 2024), multi-step conversations (Kim et al., 2023), or multi-modal scenarios (Jin et al., 2023; Ying et al., 2024a). Our model represents an alternative structured approach, which leverages LLMs as flexible translators between natural language and formal representations of meaning (Wong et al., 2023; Ying et al., 2023a,b; Zhi-Xuan et al., 2024b).\"\n    },\n    {\n      \"title\": \"Computational Model\",\n      \"content\": \"Our LaBTOM model comprises two interlinked modules for reasoning about beliefs. The first module (Figure la), an epistemic language of thought, models our capacity to compositionally represent concepts that describe the world (including the contents of other agents' minds) by combining more basic concepts into richer thoughts and expressions (Goodman and Lassiter, 2015), and how we translate such thoughts from natural language (Wong et al., 2023). The second module (Figure 1b), a Bayesian theory-of-mind (Baker et al., 2017), captures our intuitive inferences about others' mental states via probabilistic inference over a generative model of how agents update their beliefs and act towards their goals. By combining these two modules, epistemic language understanding can be modeled by first translating natural language into our ELOT representation, then evaluating these representations against inferences produced by rational mentalizing.\"\n    },\n    {\n      \"title\": \"Interpreting Belief Sentences with an Epistemic Language of Thought\",\n      \"content\": \"To represent epistemic sentences in a way that admits systematic compositionality, we introduce a formal language (Table 1) that serves as our epistemic language of thought. This language can be viewed as a form of epistemic logic (Gochet and Gribomont, 2006) with a richer set of modal operators. We adopt a degree-based semantics for epistemic modals grounded in probability comparisons, due to its advantages in handling the gradability of epistemic expressions (Lassiter, 2010; Moss, 2015), but crucially also because it allows us to evaluate formulas using probabilities inferred by our BTOM module.\"\n    },\n    {\n      \"title\": \"Representing Epistemic Formulas\",\n      \"content\": \"We first define our non-epistemic base language. Building on the Planning Domain Definition Language (McDermott et al., 1998), our language is constructed from a set of predicates $P$ and functions $F$ used to describe a set of objects $O$. Predicates can be combined into formulas $\u03c6 \u2208 \u03a6$ via logical operators or quantification. For example, \\\"A key is in box 2\\\" can be represented as $\u2203k.key(k) \u2227 inside(k, box2)$. Conceptually, a state $s$ of the environment (or some agent's mental representation of state s) is just a large formula: A conjunction of predicates and function assignments which fully describe the state. We denote the truth value of $\u03c6$ in $s$ as $[\u03c6": "s$.\nOn top of this base language, we introduce a set of epistemic expressions to model assertions of belief, knowledge, or modal qualifications thereof (Table 1a). The semantics of these expressions are ultimately grounded in the probability function"}, {"title": "Translating Epistemic Language", "content": "Epistemic formulas in our ELoT representation are unambiguous and precise. In contrast, natural language might be used to communicate the same epistemic information in multiple ways. To handle this diversity of surface forms, we make use of LLMs as general-purpose semantic parsers (Shin et al., 2021) that translate a wide variety of expressions into formal representations, substituting for the human ability to flexibly interpret natural language in-context (Wong et al., 2023). In particular, we prompt an LLM (Gemini Flash) with example translations from English to our ELoT representation, and append a new sentence \u03c3 to translate. We then sample up to $n_0$ outputs, rejecting any which are syntactically or semantically invalid, retaining the first formula that is successfully translated. (See Appendix for the translation prompt and other details.)"}, {"title": "Inferring and Evaluating Beliefs with a Bayesian Theory-of-Mind", "content": "Our ELOT module gives us a way to interpret belief claims in terms of the probability $Pr(A, \u03c6)$ an agent $A$ assigns to sentences $\u03c6$. But how is the value of $Pr(A, \u03c6)$ computed? This is the function of our BTOM module: By modeling the functional role that belief plays in guiding an agent's actions, along with the causal influence of the agent's perceptions on their beliefs, an observer can infer what the agent thinks based on what the agent sees and does. Following the structure of Partially Observable Markov Decision Processes (POMDPs) (Kaelbling et al., 1998), this theory of approximately rational agency can be formalized as a probabilistic generative model:\nGoal Prior: $g \\sim P(g)$\nState Prior: $s_0 \\sim P(s_0)$\nBelief Prior: $b_0 \\sim P(b_0|s_0)$\nState Transition: $s_t \\sim P(s_t|s_{t-1}, a_{t-1})$\nBelief Update: $b_t \\sim P(b_t|s_t,b_{t-1})$\nAction Selection: $a_t \\sim P(a_t|b_t, g)$\nObservations: $o_t \\sim P(o_t|s_t)$"}, {"title": "Modeling Perception and Action", "content": "Two crucial aspects of our BTOM module are how it models belief updating (Eq. 5) as the result of perception, and how it models goal-directed action given the agent's uncertain beliefs (Eq. 6). To model perception, we represent an agent's belief $b_t$ as a probability-weighted collection ${(s_i, w_i)}_1^k$ of possible environment states $s_i$ (which are represented in turn as collections of ELOT predicates). Given an observation of the environment $s_t$ (e.g. observing that a box is empty), the agent updates its belief by filtering out inconsistent hypotheses $s_i$, setting $w_i = 0$.\nAs for goal-directed action, our model builds upon methods for epistemic planning (Bolander, 2017) and belief-space planning in POMDPS (Littman et al., 1995). Given a belief $b_t$, the agent engages in instrumental planning to achieve their goal $g$, which requires achieving instrumental subgoals (e.g. picking up keys), but also gathering goal-relevant information (e.g. finding out if a key is in a certain box). We model this by assuming that the agent acts by approximately minimizing a cost-to-go estimate $Q_g(b_t, a)$: An estimate of the optimal cost $Q(b_t, a)$ of reaching $g$ after action $a$ starting from one's (uncertain) belief $b_t$. Action selection can thus be modeled by a Boltzmann distribution over these Q estimates:\n$P(a_t|b_t, g) \\propto exp\\left(-\\beta Q_g(b_t, a)\\right)$"}, {"title": "Joint Inference of Goals and Beliefs", "content": "With this generative model, observers can jointly infer the agent's goal $g$, belief history $b_{0:T}$, and environment trajectory $s_{0:T}$ given observations of the agent's actions $a_{1:T}$ and partial observations $o_{0:T}$ of the environment:\n$P(g, b_{0:T}, s_{0:T}|a_{1:T}, o_{0:T}) \\propto P(g, s_0, b_0) \\prod_{t=1}^T P(b_t, a_t, s_t, o_t|b_{t-1}, s_{t-1})$\nComputing this posterior is intractable in general due to the large space of possible initial beliefs $b_0$, which may in turn be defined over a large space of environment states $s_0$. However, if the space of possible beliefs and states is sufficiently small, it can be reasonable to model human observers as approximating the Bayesian ideal (Blokpoel et al., 2010). Therefore, we consider only the set of initial states $S_0$ consistent with the initial observation $o_0$, and a discrete set $B_0$ of possible beliefs $b_0$ sufficient to model comparative likelihood claims (e.g. \"The key is more likely in box 1 than 2.\"). Specifically, we consider all beliefs formed by distributing $k$ particles across $n_s := |S_0|$ states, resulting in $n_b := |B_0| = \\binom{n_s+k-1}{k}$ distributions. We then perform exact Bayesian inference over all combinations of goals $g$, initial beliefs $b_0$, and states $s_0$, which we implement as a variant of Sequential Inverse Plan Search (Zhi-Xuan et al., 2020) using the Gen probabilistic programming system (Cusumano-Towner et al., 2019). More algorithmic details are provided in the Appendix."}, {"title": "Evaluating Epistemic Sentences", "content": "By inferring the agent's belief history $b_{0:T}$, we can now compute the probability $Pr(A, \u03c6)$ assigned to a formula \u03c6 at time t as the weighted sum of truth values under the agent's belief $b_t$:\n$Pr(A, \\Phi)_t = \\sum_{(s_i,w_i)\\in b_t} w_i \\cdot [[\\Phi]]_{s_i}$\nWe can thus evaluate a epistemic formula \u03c6 given a belief $b_t$ and environment state $s_t$ by replacing"}, {"title": "Experiments", "content": "To evaluate our model on a diverse set of belief statements, we conducted a two-part experiment. We first recruited participants to write English sentences describing the current and initial beliefs of a player character as it navigated a gridworld puzzle that required finding keys hidden in boxes, collecting a dataset of epistemic language (Table 2). Next, we asked two groups of participants to rate how likely these sentences were to be true given the player's observed behavior, with one group rating sentences about the player's current beliefs, and the other group rating sentences about initial beliefs. These ratings were collected at multiple points over the course of the player's trajectory. We then compared these ratings against the inferences produced by our model."}, {"title": "Scenario Construction", "content": "We constructed 20 scenarios in the Doors, Keys, & Gems environment with varied maze designs and item locations (Figure 3). In each scenario, there were 4 goal gems with different shapes (triangle, square, hexagon, circle), some of which were locked behind doors. Scenarios also had 2 to 3 boxes with up to 2 colored keys among them. The player's actions were varied across scenarios to elicit inferences about a diversity of epistemic states, such as ignorance about key locations or false confidence about the location of a key."}, {"title": "Collecting Epistemic Language", "content": "In the first part of the experiment, we recruited 42 US participants via Prolific (mean age: 36.02, SD: 10.1; 16 women, 26 men). Following a tutorial, participants watched 10 scenario animations, with each stopping before the player reached their goal and all relevant keys were revealed. Participants were then asked to write at least two sentences about the player's likely beliefs at the end of the scenario (current beliefs), and another two sentences about the player's beliefs at the start of the scenario (initial beliefs). To ensure that these sentences focused on beliefs about the environment, we instructed participants to"}, {"title": "Evaluating Epistemic Language", "content": "For the next part of our experiment, we recruited 94 US participants via Prolific to evaluate current belief statements (mean age = 35.7, SD = 12.4, 67 women, 27 men), and another 104 US participants to evaluate initial belief statements (mean age = 35.27, SD = 11.7, 69 women, 33 men, 2 non- binary). Each participant was shown 10 out of 20 scenario animations, and was asked to rate the goals and beliefs of the player at several judgment points during each animation. For goals, participants were shown a checkbox for each gem, and asked to select all gems likely to be the agent's goal. This served as both an attention check and an additional data source for model validation. For beliefs, participants were shown 2 belief statements selected from our dataset of human-written statements, and asked to rate how likely each statement was on a scale from 1 to 7. These ratings were normalized between 0 and 1 for our analysis. We excluded 7 participants from the current belief condition and 5 from the initial belief condition for low outlying scores on the goal inference subtask."}, {"title": "Statement Selection", "content": "To ensure that the belief statements evaluated by our participants were (i) diverse and (ii) rated enough times to ensure sufficient statistical power (88% power at Cohen's d = 0.8), we selected a set of 5 statements per scenario (3 plausible, 2 implausible) from our full dataset of collected statements. The plausible statements were chosen by repeatedly sampling sets of 3 statements out of all those written for a particular scenario, then selecting the set that scored highest on a diversity metric derived from the factors in Table 2 (see Appendix for the definition of this metric). We"}, {"title": "Model Fitting and Evaluation", "content": "We evaluated our LaBTOM model on all 20 scenarios, producing normalized likelihood scores for the 5 current and 5 initial belief statements per scenario. We then computed the correlation coefficient r between these scores and average human ratings. We fit our model parameters to maximize r, fitting the belief thresholds $\u0398 := (\u03b8_{believes}, \u03b8_{could}...)$ via coordinate ascent, and the inverse temperature \u03b2 of our action model via grid search. This produced the fitted values for $\u0398$ shown in Table 1, and $\u03b2 = 2^{3/2}$. For the set of possible initial agent beliefs $B_0$, we fixed the number of belief particles to $k = 3$ to ensure the tractability of exact Bayesian inference.\nAlongside this direct comparison with human- provided ratings, we evaluated our model on the full dataset of 469 human-written statements by pairing each statement with either the scenario it was written for (in-context) or 1-2 other scenarios with the same map layout but distinct agent trajectories (out-of-context). This allowed us to compare each statement's in-context normalized likelihood $L$ with its (average) out-of-context likelihood score. Reasonable models of epistemic language interpretation should assign higher likelihood scores to most statements when they are evaluated in-context vs. out-of-context."}, {"title": "Baselines", "content": "To assess the import of a sufficiently rich theory of mind for epistemic language understanding, we evaluated several ablations of our LabToM model which made simplified assumptions about the agent's beliefs or planning abilities. We also evaluated two state-of-the-art multi-modal LLMs, thereby testing the degree to which grounded evaluation of epistemic language can be achieved with sufficient scale and finetuning:\nTrue Belief. The True Belief ablation assumes that the observed agent has fully accurate beliefs about the environment (i.e. they already know where all the keys are located), equivalent to the full model from Ying et al. (2024b). The observer starts with a uniform prior over these true beliefs."}, {"title": "Results", "content": "LaBTOM correlates highly with human ratings of epistemic statements. As Table 3 shows, we find that that our full model produces statement scores that correlate highly with human ratings (r = 0.81) across both current and initial belief conditions (a per-factor breakdown can be found in the Appendix). Plotting average human judgments against LaBToM inferences (Figure 2, Row 1), we also find a strong qualitative fit: LaBTOM generally assigns high or low ratings to sentences when humans do, using the ends of the scale as appropriate. In contrast, our ablated models do poorly, either because they fail to track how the player updates their beliefs (True Belief, Current r = 0.09), or fail to infer the player's initial beliefs from their actions towards instrumental subgoals (Non-Planning, Initial r = 0.13).\nSotA multimodal LLMs struggle at grounded evaluation of epistemic language. The multi- modal LLM baselines also perform less well than LaBTOM despite extensive prompting, with the strongest LLM baseline (GPT-40 with images, narratives, and few-shot prompting) achieving human correlations of only 0.59 and 0.41 respectively. Increasing information in the prompt improves performance, although the Gemini models perform poorly regardless, and do not improve even when provided full video of the scenarios. We find that LLM performance is lower when evaluating initial belief sentences, suggesting that they are better at tracking how agents' current beliefs change in response to observations, but worse at inferring past beliefs."}, {"title": "Discussion", "content": "Our experiments show that, similar to humans, our LaBTOM model is able to coherently interpret and adjust its evaluations of natural language statements about agents' beliefs, whereas state-of- the-art multimodal LLMs struggle with this task. We also find that LaBTOM largely distinguishes in vs. out-of-context sentence usage on a large and diverse set of crowd-sourced epistemic language, indicating the generalizability of our approach.\nThat said, our model is not without limitations. As we discuss at greater length in the Appendix, LaBTOM's outputs depart from human judgments in several interesting ways, suggesting the need to account for (i) contextual adaptation of probability thresholds (Schuster and Degen, 2020), (ii) the role of justification in human's intuitive evaluation of knowledge claims (Alston, 1989), and (iii) bounded human reasoning about logical implications (Smets and Solaki, 2018). LaBTOM is also an ideal observer model that does not scale readily to large belief spaces, leaving open how humans tractably infer and evaluate claims about others' beliefs (Van Rooij, 2008), perhaps by focusing on occurent beliefs (Bartlett, 2018) that are relevant to others' goals. Finally, LaBTOM is a model of how people interpret epistemic language, but full understanding also includes the ability to produce such language. This could potentially be achieved by inverting the ELOT module of our model, using it to translate salient inferences about an agent's beliefs into natural language. By extending our model in this way, we stand to gain an even richer account of what it means to understand epistemic language."}, {"title": "Direct Translation w/o ELOT", "content": "To investigate the importance of our ELoT formalism, we also experimented with an ablated translation model which skips the ELoT representation and directly translates from natural language to the lower- level representation using Gemini 1.5 Flash with few-shot prompting. We find that this approach can handle most simple belief sentences, but makes errors when the statement becomes more complex, or involves distinguishing knowledge from belief. We highlight some failure cases in Table C2."}, {"title": "Impact of a Normalized Statement Prior", "content": "In the main text, we report results under the assumption that human observers respond as if they have a normalized 50-50 prior $U_P$ over whether each statement $\\varphi$ is true. Under this assumption, the posterior truth-value of a statement $P([\\varphi]_{(s_t,b_t)}|A_{1:T}, O_{0:T})$ can be interpreted as a normalized likelihood:\n$L([\\varphi]_{(s_t,b_t)}|A_{1:T}, O_{0:T}) = \\frac{P(A_{1:T}, O_{0:T}|[\\varphi]_{(s_t,b_t)})}{P(A_{1:T}, O_{0:T}|[\\varphi]_{(s_t,b_t)}) + P(A_{1:T}, O_{0:T}|[\\neg \\varphi]_{(s_t,b_t)})}$\nAn alternative assumption is to use a uniform prior $U_{S_0 \\times B_0}$ over all possible initial states $S_0$ and belief distributions $B_0$. This has the effect of up-weighting statements which are true in more possible worlds, e.g. \"The player believes that a red key might be in box 1, 2, or 3.\" \nTable C3 shows the impact of making either assumption, in terms of the Pearson's correlation coefficient (PCC) r and mean absolute error (MAE) with human judgments. Consistent with Ying et al. (2024b), using a normalized statement prior largely improves the correlation while reducing the mean absolute error for the full LaBToM model. In other words, people appear more willing to say that a statement is true only if they have evidence for \u03c6, and otherwise default to a 50-50 rating. We see sharper differences for initial belief statements, which is likely because priors have a stronger effect on initial beliefs, whereas an agent's current beliefs are more strongly determined by their percepts: If an agent sees that a box is empty, an observer's judgment about whether the agent believes that the box is empty should not depend on the observer's prior."}, {"title": "Per-Factor Breakdown of Model Performance", "content": "Table C4 shows the correlation between human judgments and model outputs broken down by the annotated factors described in Table 2. LaBToM robustly outperforms the baselines on almost all of these data splits, achieving a correlation around r = 0.8 in each case. The one exception is the set of statements about what the agent initially knows (Init. Know., r = 0.31), such as \u201cThe player initially knew that the red key was in box 3\". As we discuss in the next section, this is likely because human participants assume that direct perception or justification is necessary for other agents to know some proposition \u03c6. In contrast, our model treats knowledge claims as equivalent to claims of true belief."}, {"title": "Differences in Human Ratings vs. Model Inferences", "content": "While LaBTOM largely matches human evaluations of epistemic language both in aggregate and at the individual scenario level, there a number of interesting ways in which they differ.\nPeople are less certain than LaBToM. One difference is simply that our model tends to be more certain than people, using the extremes of the 0-1 scale in ways that our participants tended to avoid. This effect did not appear to be driven by the choice of the Boltzmann inverse temperature \u03b2, since lower values of \u03b2 (which increase model uncertainty) led to poorer fits with human data. Instead, humans may be evaluating the truth a statement \u03c6 less strictly than our model does, perhaps by maintaining uncertainty over the probability thresholds \u03b8 associated with each statement.\nPeople appear to adapt probability thresholds. Threshold uncertainty is closely related to another potential driver of difference: Unlike our model, participants appear to contextually adapt probability thresholds associated with modal words, in line with work on the pragmatics of epistemic modals (Schuster and Degen, 2020; Rudin, 2016; Lassiter, 2017). This is evinced by human responses for current belief statement S2 in Fig. 3b. People rate \"The player is unsure which box has a key\" highly despite the apparent confidence that the player exhibits in looking for a key in box 3 (at the expense of looking in box 1 or box 2). This is consistent with an upwards adjustment of $\u03b8_{uncertain}$ from 0.55, such that the player is judged as uncertain even when they seem to think it is quite likely for a key to be in box 3. In contrast, our model thinks it is unlikely that the player is uncertain. Similar effects can be seen for current statement S1 and S5 in the same scenario, except that people appear to adjust their threshold for $\u03b8_{may}$ and $\u03b8_{might}$ downwards from 0.3 and 0.2 to accommodate even highly unlikely possibilities."}, {"title": "Misclassified In-Context vs. Out-of-Context Statements", "content": "As Table 4 in the main text shows, LaBToM accurately classifies most statements by assigning them a higher likelihood in their original context. However, about 30% of statements were assigned either equal or higher likelihoods out-of-context. By inspecting these cases more closely, we found that misclassification largely occurred due to the following reasons:\nDataset Noise. A few participants wrote statements that were implausible in their original context.\nSimilar Plausibility Across Contexts. Some statements described beliefs that were equally likely to be true out-of-context, or plausible in-context but even more plausible out-of-context.\nTranslation Errors. The ELoT translation step sometimes resulted in syntactically or semantically invalid translations. These statements were assigned a 0.5 likelihood across all contexts.\nTo illustrate the second point, we use an example statement from the scenario in Figure C2: \"\"The player believes that box 1 or 2 contains the red key.\" By the end of the trajectory shown in Figure C2, this statement is assigned a normalized likelihood of 1.0 (since box 3 contains the blue key, the red key must be in box 1 or 2). In our two out-of-context trajectories, the player is shown finding a red key in box 1 and box 2 respectively. As such, it should be equally true that \u201c"}]}