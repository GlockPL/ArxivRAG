{"title": "Understanding Epistemic Language with a Bayesian Theory of Mind", "authors": ["Lance Ying", "Tan Zhi-Xuan", "Lionel Wong", "Vikash Mansinghka", "Joshua B. Tenenbaum"], "abstract": "How do people understand and evaluate claims about others' beliefs, even though these beliefs cannot be directly observed? In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents' goals, beliefs, and intentions: a language-augmented Bayesian theory-of-mind (LaBToM). By translating natural language into an epistemic \"language-of-thought\", then evaluating these translations against the inferences produced by inverting a probabilistic generative model of rational action and perception, LaBTOM captures graded plausibility judgments about epistemic claims. We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent's beliefs. In contrast with multimodal LLMs (GPT-40, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.", "sections": [{"title": "1 Introduction", "content": "People regularly use and interpret language about other agents' beliefs, evaluating rich linguistic constructions that may involve claims about what others' consider necessary (Grace thinks that if Katie didn't eat the cookie, it must have been Jane", "Tom believes Sam is more likely to win the election than the others\"), or the relationship of their beliefs to the world (\u201cJohn didn't know that today was a holiday\"). But given that these beliefs are not directly observable, how do people evaluate the truth or plausibility of epistemic claims? Philosophers and linguists have long investigated the semantics of epistemic language (Hintikka, 1962; Partee, 1973; Loar, 1981), offering detailed theories of how epistemic claims relate to the sets of worlds deemed possible by an agent, and how the truth of these claims derive from their constituent predicates or propositions (Von Fintel and Heim, 2011). However, these theories do not explain how people understand and evaluate epistemic language in its context of utterance. If someone says \u201cAlice believes it might rain\", how does her behavior (e.g. bringing an umbrella) render that statement more or less plausible?\nIn this paper, we introduce a cognitive model of how humans interpret epistemic language in context (Figure 1), grounded in inferences about what agents believe given their actions and observations. We build upon a framework known as Bayesian Theory-of-Mind (BToM), which casts human mentalizing as Bayesian inference over a generative model of rational action and perception (Baker et al., 2017; Jara-Ettinger et al., 2019). We combine this framework with the compositionality afforded by probabilistic extensions of the language-of-thought hypothesis (Piantadosi, 2011; Goodman and Lassiter, 2015), developing an epistemic language of thought (ELOT) to represent how others represent the world. Using large language models (LLMs) as flexible semantic parsers, we translate natural language into this ELoT representation (Wong et al., 2023), allowing us to quantitatively evaluate epistemic claims against BToM inferences. This gives us our full model: A language-augmented Bayesian theory-of-mind (LaBToM).\nTo evaluate our model, we run an experiment where participants watch animations of a player solving a gridworld puzzle called Doors, Keys, & Gems (Zhi-Xuan et al., 2020). In these puzzles (Figure 1b), the player has to pick up (single-use)\"\n    },\n    {\n      \"title\": \"2 Related Work\",\n      \"content\": \"Sentences\nabout belief and knowledge have been studied at\nlength in philosophy and linguistics (Hawthorne\net al., 2016; Egan and Weatherson, 2011;\nYalcin, 2007). Standard treatments associate\nbelief sentences with sets of compatible worlds\n(Hintikka, 1962; Von Fintel and Heim, 2011),\nwhile more recent work grounds the gradedness\nof belief claims in probabilities (Lassiter, 2017;\"\n    },\n    {\n      \"title\": \"3 Computational Model\",\n      \"content\": \"Our LaBTOM model comprises two interlinked modules for reasoning about beliefs. The first module (Figure la), an epistemic language of thought, models our capacity to compositionally represent concepts that describe the world (including the contents of other agents' minds) by combining more basic concepts into richer thoughts and expressions (Goodman and Lassiter, 2015), and how we translate such thoughts from natural language (Wong et al., 2023). The second module (Figure 1b), a Bayesian theory-of-mind (Baker et al., 2017), captures our intuitive inferences about others' mental states via probabilistic inference over a generative model of how agents update their beliefs and act towards their goals. By combining these two modules, epistemic language understanding can be modeled by first translating natural language into our ELOT representation, then evaluating these representations against inferences produced by rational mentalizing.\"\n    },\n    {\n      \"title\": \"3.1 Interpreting Belief Sentences with an Epistemic Language of Thought\",\n      \"content\": \"To represent epistemic sentences in a way that admits systematic compositionality, we introduce a formal language (Table 1) that serves as our epistemic language of thought. This language can be viewed as a form of epistemic logic (Gochet and Gribomont, 2006) with a richer set of modal operators. We adopt a degree-based semantics for epistemic modals grounded in probability comparisons, due to its advantages in handling the gradability of epistemic expressions (Lassiter, 2010; Moss, 2015), but crucially also because it allows us to evaluate formulas using probabilities inferred by our BTOM module.\"\n    },\n    {\n      \"title\": \"3.1.1 Representing Epistemic Formulas\",\n      \"content\": \"We first define our non-epistemic base language. Building on the Planning Domain Definition Language (McDermott et al., 1998), our language is constructed from a set of predicates Pand functions F used to describe a set of objects O. Predicates can be combined into formulas \u03c6\u2208\u03a6 via logical operators or quantification. For example, \\\"A key is in box 2\\\" can be represented as \u2203k.key(k) \u2227 inside(k, box2). Conceptually, a state s of the environment (or some agent's mental representation of state s) is just a large formula: A conjunction of predicates and function assignments which fully describe the state. We denote the truth value of \u03c6 in s as [\u03c6]s.\nOn top of this base language, we introduce a set of epistemic expressions to model assertions of belief, knowledge, or modal qualifications thereof (Table 1a). The semantics of these expressions are ultimately grounded in the probability function Pr(A, \u03c6) the probability assigned by agent A to formula \u03c6, and comparisons against term- specific thresholds\"\n    },\n    {\n      \"title\": \"3.1.2 Translating Epistemic Language\",\n      \"content\": \"Epistemic formulas in our ELoT representation are unambiguous and precise. In contrast, natural language might be used to communicate the same epistemic information in multiple ways. To handle this diversity of surface forms, we make use of LLMs as general-purpose semantic parsers (Shin et al., 2021) that translate a wide variety of expressions into formal representations, substituting for the human ability to flexibly interpret natural language in-context (Wong et al., 2023). In particular, we prompt an LLM (Gemini Flash) with example translations from English to our ELoT representation, and append a new sentence \u03c3 to translate. We then sample up to no outputs, rejecting any which are syntactically or semantically invalid, retaining the first formula that is successfully translated.\"\n    },\n    {\n      \"title\": \"3.2 Inferring and Evaluating Beliefs with a Bayesian Theory-of-Mind\",\n      \"content\": \"Our ELOT module gives us a way to interpret belief claims in terms of the probability Pr(A, \u03c6) an agent A assigns to sentences \u03c6. But how is the value of Pr(A, \u03c6) computed? This is the function of our BTOM module: By modeling the functional role that belief plays in guiding an agent's actions, along with the causal influence of the agent's perceptions on their beliefs, an observer can infer what the agent thinks based on what the agent sees and does. Following the structure of Partially Observable Markov Decision Processes (POMDPs) (Kaelbling et al., 1998), this theory of approximately rational agency can be formalized as a probabilistic generative model:\nGoal Prior: g~P(g) (1)\nState Prior: so ~ P(so) (2)\nBelief Prior: bo~ P(bo so) (3)\nState Transition: st ~ P(st|st-1, at-1) (4)\nBelief Update: bt ~ P(bt|st,bt\u22121) (5)\nAction Selection: at ~ P(at|bt, g) (6)\nObservations: ot ~ P(Ot St) (7)\"\n    },\n    {\n      \"title\": \"3.2.1 Modeling Perception and Action\",\n      \"content\": \"Two crucial aspects of our BTOM module are how it models belief updating (Eq. 5) as the result of perception, and how it models goal-directed action given the agent's uncertain beliefs (Eq. 6). To model perception, we represent an agent's belief bt as a probability-weighted collection {(\u0161i, Wi)}1 of possible environment states \u0161i (which are represented in turn as collections of ELOT predicates). Given an observation of the environment st (e.g. observing that a box is empty), the agent updates its belief by filtering out inconsistent hypotheses \u0161i, setting w\u2081 = 0.\nAs for goal-directed action, our model builds upon methods for epistemic planning (Bolander, 2017) and belief-space planning in POMDPS (Littman et al., 1995). Given a belief bt, the agent engages in instrumental planning to achieve their goal g, which requires achieving instrumental subgoals (e.g. picking up keys), but also gathering goal-relevant information (e.g. finding out if a key is in a certain box). We model this by assuming that the agent acts by approximately minimizing a cost-to-go estimate Qg(bt, a): An estimate of the optimal cost Q(bt, a) of reaching g after action a starting from one's (uncertain) belief bt. Action selection can thus be modeled by a Boltzmann distribution over these Q estimates:\n$$P(at|bt, g) \u03b1 exp (-\u03b2Qg(bt, a))$$\nTo estimate Q efficiently, we follow recent advances in inverse planning (Zhi-Xuan et al.,\"\n    },\n    {\n      \"title\": \"3.2.2 Joint Inference of Goals and Beliefs\",\n      \"content\": \"With this generative model, observers can jointly infer the agent's goal g, belief history bo:T, and environment trajectory 80:T given observations of the agent's actions 01:T and partial observations 00:T of the environment:\n$$P(g, b0:T, 80:T|a1:T, 00:T) \u03b1 P(g, so, bo) \u03a0-1 P(bt, at, St, Ot|bt\u22121, St\u22121)$$\nComputing this posterior is intractable in general due to the large space of possible initial beliefs bo, which may in turn be defined over a large space of environment states so. However, if the space of possible beliefs and states is sufficiently small, it can be reasonable to model human observers as approximating the Bayesian ideal (Blokpoel et al., 2010). Therefore, we consider only the set of initial states So consistent with the initial observation 00, and a discrete set Bo of possible beliefs bo sufficient to model comparative likelihood claims (e.g. \\\"The key is more likely in box 1 than 2.\\\"). Specifically, we consider all beliefs formed by distributing k particles across ns := |So| states, resulting in \u043f\u044c := |Bo = (ns+k-1) distributions. We then perform exact Bayesian inference over all combinations of goals g, initial beliefs bo, and states so, which we implement as a variant of Sequential Inverse Plan Search (Zhi-Xuan et al., 2020) using the Gen probabilistic programming system (Cusumano-Towner et al., 2019). More algorithmic details are provided in the Appendix.\"\n    },\n    {\n      \"title\": \"3.2.3 Evaluating Epistemic Sentences\",\n      \"content\": \"By inferring the agent's belief history bo:T, we can now compute the probability Pr(A, \u03c6) assigned to a formula \u03c6 at time t as the weighted sum of truth values under the agent's belief bt:\n$$Pr(A, \u03a6) = \u2211(\u0161i,wi)\u2208bt Wi\u00b7 [[$]s$$\nWe can thus evaluate a epistemic formula \u03c6 given a belief bt and environment state st by replacing\"\n    },\n    {\n      \"title\": \"4 Experiments\",\n      \"content\": \"To evaluate our model on a diverse set of belief statements, we conducted a two-part experiment. We first recruited participants to write English sentences describing the current and initial beliefs of a player character as it navigated a gridworld puzzle that required finding keys hidden in boxes, collecting a dataset of epistemic language (Table 2). Next, we asked two groups of participants to rate how likely these sentences were to be true given the player's observed behavior, with one group rating sentences about the player's current beliefs, and the other group rating sentences about initial beliefs. These ratings were collected at multiple points over the course of the player's trajectory. We then compared these ratings against the inferences produced by our model.\"\n    },\n    {\n      \"title\": \"4.1 Scenario Construction\",\n      \"content\": \"We constructed 20 scenarios in the Doors, Keys, & Gems environment with varied maze designs and item locations (Figure 3). In each scenario, there were 4 goal gems with different shapes (triangle, square, hexagon, circle), some of which were locked behind doors. Scenarios also had 2 to 3 boxes with up to 2 colored keys among them. The player's actions were varied across scenarios to elicit inferences about a diversity of epistemic states, such as ignorance about key locations or false confidence about the location of a key.\"\n    },\n    {\n      \"title\": \"4.2 Collecting Epistemic Language\",\n      \"content\": \"In the first part of the experiment, we recruited 42 US participants via Prolific (mean age: 36.02, SD: 10.1; 16 women, 26 men). Following a tutorial, participants watched 10 scenario animations, with each stopping before the player reached their goal and all relevant keys were revealed. Participants were then asked to write at least two sentences about the player's likely beliefs at the end of the scenario (current beliefs), and another two sentences about the player's beliefs at the start of the scenario (initial beliefs). To ensure that these sentences focused on beliefs about the environment, we instructed participants to\"\n    },\n    {\n      \"title\": \"4.3 Evaluating Epistemic Language\",\n      \"content\": \"For the next part of our experiment, we recruited 94 US participants via Prolific to evaluate current belief statements (mean age = 35.7, SD = 12.4, 67 women, 27 men), and another 104 US participants to evaluate initial belief statements (mean age = 35.27, SD = 11.7, 69 women, 33 men, 2 non-binary). Each participant was shown 10 out of 20 scenario animations, and was asked to rate the goals and beliefs of the player at several judgment points during each animation. For goals, participants were shown a checkbox for each gem, and asked to select all gems likely to be the agent's goal. This served as both an attention check and an additional data source for model validation. For beliefs, participants were shown 2 belief statements selected from our dataset of human-written statements, and asked to rate how likely each statement was on a scale from 1 to 7. These ratings were normalized between 0 and 1 for our analysis. We excluded 7 participants from the current belief condition and 5 from the initial belief condition for low outlying scores on the goal inference subtask.\"\n    },\n    {\n      \"title\": \"4.3.1 Statement Selection\",\n      \"content\": \"To ensure that the belief statements evaluated by our participants were (i) diverse and (ii) rated enough times to ensure sufficient statistical power (88% power at Cohen's d = 0.8), we selected a set of 5 statements per scenario (3 plausible, 2 implausible) from our full dataset of collected statements. The plausible statements were chosen by repeatedly sampling sets of 3 statements out of all those written for a particular scenario, then selecting the set that scored highest on a diversity metric derived from the factors in Table 2.\"\n    },\n    {\n      \"title\": \"4.4 Model Fitting and Evaluation\",\n      \"content\": \"We evaluated our LaBTOM model on all 20 scenarios, producing normalized likelihood scores for the 5 current and 5 initial belief statements per scenario. We then computed the correlation coefficient r between these scores and average human ratings. We fit our model parameters to maximize r, fitting the belief thresholds \u0398 := (@believes, could...) via coordinate ascent, and the inverse temperature \u03b2 of our action model via grid search. This produced the fitted values for shown in Table 1, and \u03b2 = 23/2. For the set of possible initial agent beliefs Bo, we fixed the number of belief particles to k = 3 to ensure the tractability of exact Bayesian inference.\nAlongside this direct comparison with human- provided ratings, we evaluated our model on the full dataset of 469 human-written statements by pairing each statement with either the scenario it was written for (in-context) or 1-2 other scenarios with the same map layout but distinct agent trajectories (out-of-context). This allowed us to compare each statement's in-context normalized likelihood L with its (average) out-of-context likelihood score. Reasonable models of epistemic language interpretation should assign higher likelihood scores to most statements when they are evaluated in-context vs. out-of-context.\"\n    },\n    {\n      \"title\": \"4.5 Baselines\",\n      \"content\": \"To assess the import of a sufficiently rich theory of mind for epistemic language understanding, we evaluated several ablations of our LabToM model which made simplified assumptions about the agent's beliefs or planning abilities. We also evaluated two state-of-the-art multi-modal LLMs, thereby testing the degree to which grounded evaluation of epistemic language can be achieved with sufficient scale and finetuning:\nTrue Belief. The True Belief ablation assumes that the observed agent has fully accurate beliefs about the environment (i.e. they already know where all the keys are located), equivalent to the full model from Ying et al. (2024b). The observer starts with a uniform prior over these true beliefs.\"\n    },\n    {\n      \"title\": \"5 Results\",\n      \"content\": \"LaBTOM correlates highly with human ratings of epistemic statements. As Table 3 shows, we find that that our full model produces statement scores that correlate highly with human ratings (r = 0.81) across both current and initial belief conditions (a per-factor breakdown can be found in the Appendix). Plotting average human judgments against LaBToM inferences (Figure 2, Row 1), we also find a strong qualitative fit: LaBTOM generally assigns high or low ratings to sentences when humans do, using the ends of the scale as appropriate. In contrast, our ablated models do poorly, either because they fail to track how the player updates their beliefs (True Belief, Current r = 0.09), or fail to infer the player's initial beliefs from their actions towards instrumental subgoals (Non-Planning, Initial r = 0.13).\nSotA multimodal LLMs struggle at grounded evaluation of epistemic language. The multi- modal LLM baselines also perform less well than LaBTOM despite extensive prompting, with the strongest LLM baseline (GPT-40 with images, narratives, and few-shot prompting) achieving human correlations of only 0.59 and 0.41 respectively. Increasing information in the prompt improves performance, although the Gemini models perform poorly regardless, and do not improve even when provided full video of the scenarios. We find that LLM performance is lower when evaluating initial belief sentences, suggesting that they are better at tracking how agents' current beliefs change in response to observations, but worse at inferring past beliefs.\"\n    },\n    {\n      \"title\": \"LaBToM captures how human evaluations of epistemic language change with agent behavior.\",\n      \"content\": \"Our model predicts that human evaluations of epistemic sentences should change systematically as they gain more information about an agent's percepts and actions. As Figure 3 illustrates, this is what we find. When the player sees that a box is empty, both humans and our model sharply decrease their ratings for statements claiming that the player believes a key is in that box (e.g. Fig. 3b, S4 Current). When the player approaches one box instead of another, both humans and LaBTOM gain confidence in statements about the relative likelihood of key locations\"\n    },\n    {\n      \"title\": \"LaBToM distinguishes in-context and out-of-context epistemic language.\",\n      \"content\": \"To investigate how our model generalizes to a larger set of epistemic expressions, we performed the in-context vs. out- of-context likelihood comparison described in Section 4.4 for our full dataset of 469 sentences. We tested the full LaBTOM model, ablations and the best applicable LLM baseline from Table 3 (GPT-40, I+Na). Results are shown in Table 4. For both current and initial belief statements, we find that LaBTOM assigns significantly higher scores when a statement is evaluated in-context vs. out- of-context, correctly classifying the context about 70% of the time by assigning a strictly higher in- context score. As we examine in the Appendix, many statements that are not correctly classified turn out to be plausible in either context, resulting in equal or close-to-equal scores. The ablated models and GPT-40 perform significantly worse than LaBTOM, especially for initial beliefs, where all baselines perform worse-than-chance.\"\n    },\n    {\n      \"title\": \"6 Discussion\",\n      \"content\": \"Our experiments show that, similar to humans, our LaBTOM model is able to coherently interpret and adjust its evaluations of natural language statements about agents' beliefs, whereas state-of- the-art multimodal LLMs struggle with this task. We also find that LaBTOM largely distinguishes in vs. out-of-context sentence usage on a large and diverse set of crowd-sourced epistemic language, indicating the generalizability of our approach. That said, our model is not without limitations. As we discuss at greater length in the Appendix, LaBTOM's outputs depart from human judgments in several interesting ways, suggesting the need to account for (i) contextual adaptation of probability thresholds (Schuster and Degen, 2020), (ii) the role of justification in human's intuitive evaluation of knowledge claims (Alston, 1989), and (iii) bounded human reasoning about logical implications (Smets and Solaki, 2018). LaBTOM is also an ideal observer model that does not scale readily to large belief spaces, leaving open how humans tractably infer and evaluate claims about others' beliefs (Van Rooij, 2008), perhaps by focusing on occurent beliefs (Bartlett, 2018) that are relevant to others' goals. Finally, LaBTOM is a model of how people interpret epistemic language, but full understanding also includes the ability to produce such language. This could potentially be achieved by inverting the ELOT module of our model, using it to translate salient inferences about an agent's beliefs into natural language. By extending our model in this way, we stand to gain an even richer account of what it means to understand epistemic language.\"\n    },\n     {\n      \"title\": \"A Dataset Collection\",\n      \"content\": null\n    },\n    {\n      \"title\": \"A.1 Experimental Procedure\",\n      \"content\": \"The interface useds for collecting the statements and evaluating statements are shown in Figure A1. Participants first completed a tutorial that explained the task and experimental interface, then answered 5 comprehension questions before proceeding to the main experiment. In the main experiment, they were shown 10 out of the 20 stimuli in a randomized order.\nTo incentivize accurate but calibrated responses, participants were rewarded for accurately guessing the true goal. Specifically, they earned 1/N bonus points if they selected N goals out of which one was the true goal, but 0 points if none of their selected goals was the true goal. Participants were paid US$1 for every 40 bonus points they earned, on top of a base pay of US$15/hr.\"\n    },\n    {\n      \"title\": \"A.2 Statement Post-Processing and Annotation\",\n      \"content\": \"Once statements were collected, two experimenters independently annotated whether each statement was valid for inclusion. We excluded invalid sentences based on two criteria: (i) whether the statement had the right tense (present for current beliefs, past for initial beliefs) and (ii) whether the statement referred to beliefs about the boxes. We also corrected minor grammatical errors and normalized statements to the form \u201cThe player + [believes/knows/thinks/expects/is sure/is uncertain, etc.]...\u201c for current beliefs and \u201cThe player initially + [believed/knew/thought/expected/was sure/uncertain, etc.]...": "or initial beliefs.\nAfter filtering and normalization, two experimenters independently annotated the statements based on four factors: possibility, probability, compositionality, and knowledge. These factors are not mutually exclusive, so a statement could be annotated with any combination of factors. The codes for possibility were [may, might, can, could, should, must, none] and the codes for probability were [certain, uncertain, likely, unlikely, none]. The codes for compositionality and knowledge were binary [0, 1]. The annotators agreed on 95% of the codes and discussed to resolve their differences."}, {"title": "A.3 Selecting Diverse Statements for Human Evaluation", "content": "After annotation, we selected a set of 3 plausible and 2 implausible statements per scenario for evaluation by human raters. Implausible statements were manually selected from other scenarios to be implausible in their scenario of evaluation. To select the 3 plausible statements, we sampled 100 subsets of 3 statements out of all statements written for that scenario, then computed a diversity score for each set S:\n$$Score(S) = |S_{possibility}| +  |S_{probability}|+ |S_{compositionality}| + |S_{knowledge}|$$\nwhere |Spossibility | indicates the number of unique possibility codes in set S and vice versa. We then chose the set with the highest diversity score among the 100 sampled sets."}, {"title": "B Model Configuration", "content": null}, {"title": "B.1 Belief-Space Sequential Inverse Plan Search (BSIPS)", "content": "Our BTOM inference algorithm is a belief-space policy variant of Sequential Inverse Plan Search (SIPS, Zhi-Xuan et al. (2020)), which uses policies instead partial plans to evaluate action likelihoods as in more recent extensions of SIPS (Zhi-Xuan et al., 2024b,a). Perhaps surprisingly, Belief-Space SIPS (BSIPS) is able to exactly compute the posterior over beliefs, goals, and states (Equation 10) without any Monte Carlo approximation: For the scenarios we considered, there were between 120 and 5940 possible combinations of goals, initial states, and belief distributions, and enumerative inference over these hypothesis spaces could run as fast as 0.1s per action (120 hypotheses), going up to 20s per action (for 5490 hypotheses). Experiments were conducted with a i7-1370P 1.90 GHz CPU and 64 GB RAM.\nAlgorithm 1 provides the pseudocode for BSIPS. At each step t, we simulate how the environment changes, and how the agent updates their beliefs based on what they see (L6-7). Next, we efficiently compute belief-space Q-values by leveraging the QMDP approximation described in Section 3.2.1. This involves averaging over Q-values for each state hypothesis \u0161 in the agent's belief b, which can be done cheaply by memoizing and reusing shortest-path computations across all belief hypotheses (L9-13). The Q-values allow us to compute the likelihood of the observed action at, allowing us to reweight each hypothesis by how well it explains the observations (L14-16).\nFor epistemic language evaluation at this scale, the technical challenges are mostly representational, not algorithmic. However, scaling to larger spaces of goals (Zhi-Xuan et al., 2024a) and (belief) states will require additional layers of Monte Carlo approximation. Our implementation in Gen (Cusumano- Towner et al., 2019) can naturally be extended to these cases, e.g. by leveraging Sequential Monte Carlo for approximate inference of initial states (Del Moral et al., 2006; Lew et al., 2023)."}, {"title": "B.2 Parameter Fitting", "content": "We performed a grid search over parameters for the LaBToM model. The range of inverse temperatures \u03b2 for the Boltzmann policy went from 0.5 to 4 in multiplicative increments of \u221a2. This produced human correlations between r = 0.75 (at \u03b2 = 0.5) and r = 0.81 (at \u03b2 = 4) for current beliefs, and between r = 0.64 (at \u03b2 = 0.5) and r = 0.80 (at \u03b2 = 23/2), with \u03b2 = 23/2 producing the best fit overall.\nWe also fitted the threshold parameters O used in our ELoT representation. We performed grid-based coordinate ascent with a step size of 0.05, starting from values derived from the literature mapping modal words to probabilities (Wesson and Pulford, 2009; Hahn and Engelmann, 2014; Meder et al., 2022), and limiting the search to a range of 0.2 above and below this starting point."}, {"title": "B.3 ELOT Translation Prompt", "content": "We used Gemini 1.5 Flash (Gemini Team et al., 2024) prompted with 34 examples to translate natural language statements into ELoT formulae. ELOT formulae were represented in a Prolog-like syntax analogous to the mathematical syntax we show in Table 1. The prompt used is shown below. We only show the first 10 out of 34 examples due to space constraints, and provide the full set in our code release."}, {"title": "C Additional Results", "content": null}, {"title": "C.1 Direct Translation w/o ELOT", "content": "To investigate the importance of our ELoT formalism, we also experimented with an ablated translation model which skips the ELoT representation and directly translates from natural language to the lower- level representation using Gemini 1.5 Flash with few-shot prompting. We find that this approach can handle most simple belief sentences, but makes errors when the statement becomes more complex, or involves distinguishing knowledge from belief."}, {"title": "C.2 Impact of a Normalized Statement Prior", "content": "In the main text, we report results under the assumption that human observers respond as if they have a normalized 50-50 prior Up over whether each statement 4 is true. Under this assumption, the posterior truth-value of a statement P([4] (st,bt)|A1:T, 00:T) can be interpreted as a normalized likelihood:\n$$L([4](st,bt)|A1:T, 00:\u0422) = \\frac{P(a1:T, 00:T[]](st,bt))}{P(a1:T, 00:T[]] (st,bt)) + P(A1:T, 00:T][4](st,bt))}$$\nAn alternative assumption is to use a uniform prior Uso\u00d7B0 over all possible initial states So and belief distributions Bo. This has the effect of up-weighting statements which are true in more possible worlds, e.g. \"The player believes that a red key might be in box 1, 2, or 3.\". Table C3 shows the impact of making either assumption, in terms of the Pearson's correlation coefficient (PCC) r and mean absolute error (MAE) with human judgments. Consistent with Ying et al. (2024b), using a normalized statement prior largely improves the correlation while reducing the mean absolute error for the full LaBToM model. In other words, people appear more willing to say that a statement is true only if they have evidence for 4, and otherwise default to a 50-50 rating. We see sharper differences for initial belief statements, which is likely because priors have a stronger effect on initial beliefs, whereas an agent's current beliefs are more strongly determined by their percepts: If an agent sees that a box is empty, an observer's judgment about whether the agent believes that the box is empty should not depend on the observer's prior."}, {"title": "C.3 Per-Factor Breakdown of Model Performance", "content": "Table C4 shows the correlation between human judgments and model outputs broken down by the annotated factors described in Table 2. LaBToM robustly outperforms the baselines on almost all of these data splits, achieving a correlation around r = 0.8 in each case. The one exception is the set of statements about what the agent initially knows (Init. Know., r = 0.31), such as \u201cThe player initially knew that the red key was in box 3\". As we discuss in the next section, this is likely because human participants assume that direct perception or justification is necessary for other agents to know some proposition 4. In contrast, our model treats knowledge claims as equivalent to claims of true belief.\""}, {"title": "C.4 Differences in Human Ratings vs. Model Inferences", "content": "While LaBTOM largely matches human evaluations of epistemic language both in aggregate and at the individual scenario level", "difference": "Unlike our model, participants appear to contextually adapt probability thresholds associated with modal words, in line with work on the pragmatics of epistemic modals (Schuster and Degen, 2020; Rudin, 2016; Lassiter, 2017). This is evinced by human responses for current belief statement S2 in Fig. 3b. People rate \"The player is unsure which box has a key\u201d highly despite the apparent confidence that the player exhibits in looking for a key in box 3 (at the expense of looking in box 1 or box 2). This is consistent with an upwards adjustment of uncertain from 0.55, such that the player is judged as uncertain even when they seem to think it is quite likely for a key to be in box 3. In contrast, our model thinks it is unlikely that the"}]}