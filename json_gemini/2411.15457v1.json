{"title": "Hindi audio-video-Deepfake (HAV-DF): A Hindi language-based Audio-video Deepfake Dataset", "authors": ["Sukhandeep Kaur", "Mubashir Buhari", "Naman Khandelwal", "Priyansh Tyagi", "Kiran Sharma"], "abstract": "Deepfakes, while technologically impressive, present a dual-edged sword. On the one hand, they have potential for innovation and creativity. However, their misuse poses significant risks to privacy, trust, and security, making it essential to develop detection tools and ethical frameworks to mitigate harm. With a vast Hindi-speaking population, India is particularly vulnerable to deepfake-driven misinformation campaigns. Fake videos or speeches in Hindi can have an enormous impact on rural and semi-urban communities, where digital literacy tends to be lower and people are more inclined to trust video content. The development of effective frameworks and detection tools to combat deepfake misuse requires high-quality, diverse, and extensive datasets. The existing popular datasets like FF-DF (FaceForensics++), and DFDC (DeepFake Detection Challenge) are based on English language, and there is a lack of regional language datasets. Hence, the aim of this paper is to create a first novel Hindi deep fake dataset, named \"Hindi audio-video-Deepfake\" (HAV-DF). The dataset has been generated using the faceswap, lipsyn and voice cloning methods. This multistep process allows us to create a rich, varied dataset that captures the nuances of Hindi speech and facial expressions, providing a robust foundation for training and evaluating deepfake detection models in a Hindi language context. It is unique of its kind as all of the previous datasets contain either deepfake videos or synthesized audio. This type of deepfake dataset can be used for training a detector for both deepfake video and audio datasets. Notably, the newly introduced HAV-DF dataset demonstrates lower detection accuracy's across existing detection methods like Headpose, Xception-c40 Mesonet, etc. Compared to other well-known datasets FF-DF (FaceForensics++), and DFDC (DeepFake Detection Challenge). This trend suggests that the HAV-DF dataset presents deeper challenges to detect, possibly due to its focus on Hindi language content and diverse manipulation techniques. Furthermore, the HAV-DF dataset represents a significant contribution to the field, addressing the previous lack of Hindi-specific deepfake datasets and enabling more effective development of multilingual deepfake identification systems.", "sections": [{"title": "1. Introduction", "content": "Recent advances in AI, particularly in Generative AI and deep learning, have enabled the creation and manipulation of large volumes of highly realistic fake data across various formats, including audio, text, and video. The quality of these synthetic data is often indistinguishable from real data, making it challenging for human perception to differentiate between the two. Although these developments initially focused on textual data, the proliferation of social networks has facilitated the generation of fake content in other modalities, such as audio, video, and images. These visual and auditory formats spread misinformation more rapidly than text, as individuals tend to respond more quickly and strongly to visual content. Despite its potential for harm, synthetic data also offers significant benefits in fields such as education, entertainment, animation, and accessibility for visually impaired individuals (Liz-Lopez, Keita, Taleb-Ahmed, Hadid, Huertas-Tato and Camacho, 2024).\nA notable example of the negative implications of deepfakes occurred in 2017, when face-swapping techniques were used to create explicit videos featuring celebrities, which were then published online. Another significant case involved a fake video of former President Barack Obama, generated using software primarily developed by Reddit users (Mirsky and Lee, 2021). Highlighting the seriousness of audio deepfake attacks, a recent cybercrime involving audio deepfakes led to a financial loss of $35 million for a UAE-based company(Rabhi, Bakiras and Di Pietro, 2024).\nThe increasing interest in deepfakes research has led to the emergence of numerous workshops, conferences, and dedicated international projects, such as MediFor, funded by the Defense Advanced Research Projects Agency (DARPA). Furthermore, competitions such as the Media Forensics Challenge (MFC2018) and the Deepfake Detection Challenge (DFDC), initiated by the National Institute of Standards and Technology (NIST) and Facebook, have further driven advancements in the field (Tolosana, Vera-Rodriguez, Fierrez, Morales and Ortega-Garcia, 2020). To develop an effective deepfake detection system, it is crucial to create large-scale high-quality data sets comprising authentic and manipulated media, allowing the distinction between real and synthetic content. Deepfakes are typically produced either by generating entirely synthetic data or by altering existing media. Training detection models on a diverse range of forgeries enhances their ability to generalize and perform well on previously unseen examples. This process also helps uncover specific features that characterize deepfakes, which are essential to designing robust detection systems (Zhou, Han, Morariu and Davis, 2017; Rossler, Cozzolino, Verdoliva, Riess, Thies and Nie\u00dfner, 2019). Furthermore, creating benchmark detection models involves generating deepfakes with varying levels of complexity, ranging from basic modifications to highly advanced forgeries, to allow thorough evaluation and refinement of detection techniques (Dolhansky, Bitton, Pflaum, Lu, Howes, Wang and Ferrer, 2020).\nResearchers can refine the deepfakes generated to assess the detection system's performance under different conditions, including compressed videos, low-resolution images, and noisy environments. With the continuous advancements in deepfake generation technologies, it is crucial to evaluate these systems against the most recent and sophisticated techniques to ensure the development of effective and up-to-date detection methods (Chesney and Citron, 2019). For audio-visual deepfakes, maintaining synchronization between audio and visual elements poses a major challenge, often leading to inconsistencies in lip-sync, facial expressions, or vocal patterns. Detecting such discrepancies during the creation process, such as misalignments between speech and facial movements, can improve the accuracy of future deepfake detection methods (Korshunov and Marcel, 2018). The creation of deepfakes is essential for thoroughly evaluating the robustness of detection models. By producing forgeries with different degrees of realism, researchers can benchmark detection systems across a broad spectrum of manipulations (Dolhansky et al., 2020). The most convincing deepfakes are often generated using diverse training datasets, efficient processes, and high-quality outputs that closely replicate real-world data."}, {"title": "1.1. Deepfakes Types and Overview", "content": "Masood, Nawaz, Malik, Javed, Irtaza and Malik (2023) conducted an in-depth survey on deepfake generation and detection, categorizing deepfakes into two main types: audio and visual. For visual deepfakes, they identified five methodologies: Face Swap, Puppet-Mastery, Lip Syncing, Entire Face Synthesis, and Facial Attribute Manipulation. For audio deepfakes, they highlighted two primary techniques: Text-to-Speech Synthesis and Voice Conversion. Additionally, a multimodal category was noted, combining audio and visual deepfakes. Figure 1 illustrates the various types of deepfakes generated. Mathematical representations use s and t to denote source and target identities, with $x_s$ and $x_t$ representing source and target images, respectively, and $x_g$ representing the generated deepfake created using source and target images. For generating HAV-DF dataset, we have considered the following deepfakes generation methods.\n1. Face reenactment: Face reenactment is a technique used to manipulate and transfer facial expressions, head movements, and gestures from one individual (source) to another (target) in a video or image. The goal is to make it appear as though the target person is performing the actions or expressions of the source while maintaining the target's identity. Puppet Master or Face Reenactment methods involve manipulating a person's facial features, such as head movements, eye motions, and gestures. These methods find extensive use in the animation and entertainment industries, particularly for tasks such as dubbing or modifying an actor's expressions in movie scenes. The mathematical process is outlined as follows:\n(a) Feature extraction: Extract expression features from the source face $x_s$ and identity features from the target face $x_t$:\n$Z_{exp} = E_{exp}(x_s)$ (1)\n$Z_{id} = E_{id}(x_t)$ (2)"}, {"title": null, "content": "where $Z_{exp}$ represents the source expressions and $Z_{id}$ represents the target identity.\n(b) Feature fusion: - Combine the source's expression features with the target's identity features:\n$Z_{fusion} = f(Z_{exp}, Z_{id})$ (3)\nHere, $Z_{fusion}$ is the combined representation of the target's identity and source's expressions.\n(c) Reenacted face generation: - Generate the reenacted face $x_g$ using a decoder D:\n$x_g = D(Z_{fusion})$ (4)\n2. Face swap or replacement: In Face Swap, the face from the source image $x_s$ is swapped with the face from the target video $x_t$. This process entails extracting the face from the source image and replacing key facial features, including the eyes, mouth, and nose of the target face, with those from the source. Various deepfake generation methods based on Face Swap have been examined in the literature, including Faceswap, FaceSwapGAN, DeepFaceLab, Fast Face Swap, FSNet, RSGAN, and FaceShifter. These approaches utilize architectures such as encoder-decoder networks, Generative Adversarial Networks (GANs), and Convolutional Neural Networks (CNNs) to create deepfakes. As described in Equation 5, the contents of the target image $x_t$ are replaced with those of the source image $x_s$. The encoder E maps the input image to a latent space vector. The process can be represented mathematically as follows:\n(a) Encoding source and target faces:\n$Z_s = E(x_s), Z_t = E(x_t)$ (5)\nwhere $Z_s$ and $Z_t$ are the latent features of the source $x_s$ and target $x_t$, extracted by the encoder E.\n(b) Combining features:\n$Z_{swap} = f(Z_{id,s}, Z_{attr,t})$ (6)\nwhere $Z_{id,s}$ represents the identity features of the source, $Z_{attr,t}$ represents the attributes (e.g., pose, lighting) of the target, and f is the fusion function.\n(c) Decoding to generate swapped face:\n$x_{swap} = D(Z_{swap})$ (7)\nwhere D is the decoder that synthesizes the swapped face from the combined latent representation $Z_{swap}$\n(d) Final composition:\n$x_{output} = B(x_{swap}, x_t)$ (8)\nwhere B is the blending function that integrates the swapped face $x_{swap}$ into the target image $x_t$, preserving its background and context.\n3. Face Synthesis and Editing: Face Synthesis and Editing involve generating new faces or modifying attributes of existing faces, such as age, gender, or expression. Face synthesis creates entirely new, realistic faces using models like GANs (e.g., StyleGAN). Face synthesis and editing involve generating new faces or modifying specific attributes of existing faces, such as age, expression, or hairstyle. These processes can be described mathematically as follows:\n(a) Face synthesis:\n$x_g = G(z)$ (9)\nwhere z is a latent vector sampled from a distribution P(z) (e.g., Gaussian), and G is the generator that outputs the synthesized face $x_g$.\n(b) Face editing:\n$x_{edit} = G(z + \\Delta z_{attr})$ (10)\nwhere z is the latent representation of the original image, $\\Delta z_{attr}$ represents the change in the latent space corresponding to a specific attribute (e.g., aging, expression), and $x_{edit}$ is the edited face output."}, {"title": null, "content": "(c) Attribute manipulation in latent space:\n$z_{edit} = z + \\Delta z_{attr}$ (11)\nThis modifies the original latent representation z by adding a vector $\\Delta z_{attr}$, which corresponds to the desired change in attributes.\n4. Lip-synced: Lip sync is the process of matching a speaker's lip movements in a video or animation to an audio track, such as spoken words or singing. The aim is to create a smooth and natural alignment between the visual lip movements and the audio, making it appear as though the person or character is authentically speaking the provided audio. The lip-synced process creates realistic lip movements in a video to match an audio input. The steps and mathematical equations are as follows:\n(a) Audio encoding: The audio a is analyzed to extract speech features:\n$Z_{audio} = E_{audio}(a)$ (12)\nwhere $Z_{audio}$ is latent representation of the audio and $E_{audio}$ is audio encoder.\n(b) Video encoding: The video frame $x_t$ is analyzed to extract lip features:\n$Z_{lip} = E_{lip}(x_t)$ (13)\nwhere $Z_{lip}$ is the latent representation of the visual features and $E_{lip}$ is visual encoder.\n(c) Feature fusion: The audio and lip features are combined to align lip movements with speech:\n$Z_{sync} = f(Z_{audio}, Z_{lip})$ (14)\nwhere $Z_{sync}$ is the latent representation combining audio and visual features and f is fusion function.\n(d) Lip-synced frame generation: A new video frame $x_{sync}$ is created with synchronized lips:\n$x_{sync} = G(Z_{sync})$ (15)\nwhere $x_{sync}$ is generated frame with lip-synced movements and G is generator model.\nThe final equation is:\n$x_{sync} = G(f(E_{audio}(a), E_{lip}(x_t)))$ (16)\nThis process aligns audio and video features to generate a lip-synced video.\n5. Voice cloning: Voice cloning is the process of replicating a person's voice using AI-based technologies. It involves capturing the unique characteristics of an individual's voice\u2014such as tone, pitch, accent, and speaking style\u2014to synthesize new speech that mimics the original speaker. This technology is widely used in personalized virtual assistants, content creation, and accessibility tools. A sample of the target speaker's voice is recorded. This sample can range from a few seconds (for zero-shot cloning) to several minutes for higher accuracy. This process can be described mathematically as follows:\n(a) Feature extraction: An encoder $E_{voice}$ processes the input speech s to extract a speaker embedding $Z_{voice}$, which captures the speaker's unique vocal characteristics:\n$Z_{voice} = E_{voice}(s)$ (17)\n(b) Text-to-speech synthesis: A text input t is converted into a latent representation $Z_{text}$ using a text encoder $E_{text}$:\n$Z_{text} = E_{text}(t)$ (18)\n(c) Speech generation: The voice embedding $Z_{voice}$ and text representation $Z_{text}$ are combined to generate the final audio output y using a vocoder V:\n$y = V(f(Z_{voice}, Z_{text}))$ (19)\nHere, f is a fusion function that aligns the speaker's vocal characteristics with the desired speech content."}, {"title": "1.2. Challenges in Deepfake Generation", "content": "The major challenges in creating deepfakes are:\n1. The trade-off between data and quality: To get the better results we need huge amount of training data which can be feasible for some particular target or group of individuals like celebrities etc. but not feasible for individuals.\n2. Speed and quality: The quality of the deepfake is highly effected by the speed and it depends more on the channel of deepfake generation such as online (interactive) or offline (stored data).\n3. Availability and quality: The availability of more code and data will help to the researchers to generate high quality deepfakes (Mirsky and Lee, 2021)."}, {"title": "1.3. Research Objectives", "content": "1. Create a Hindi-language based audio-video deepfake dataset named Hindi audio-video-Deepfake (HAV-DF).\n2. Validate the robustness of the dataset using the pre-trained models.\n3. Compare the performance of the HAV-DF dataset with the existing datasets on audio-video, only audio, and video only."}, {"title": "1.4. Deepfakes for Low Resource Languages", "content": "Most research on audio-visual deepfakes has focused on high-resource languages such as English and Chinese, while low-resource languages like Indic, Arabic, and many Asian and European languages lack adequate attention. Low-resource languages are characterized by limited digital resources, including datasets, research, and technological support, which restricts advancements in deepfake detection methods for these languages. A significant challenge in generating deepfakes for low-resource languages is the lack of training data. Generating deepfakes necessitates extensive datasets of video and audio recordings to train neural networks on language-specific features. Low-resource languages, such as Indic languages, pose unique challenges due to their intricate grammatical structures, distinct phonetic characteristics, and culturally embedded visual expressions like gestures. The absence of language-specific or culturally tailored models significantly hampers the ability to produce accurate deepfakes.\nA common approach to tackle this challenge is transfer learning, which involves fine-tuning a pre-trained model trained on high-resource languages for use with low-resource languages. This method leverages the generic features learned from extensive datasets and adapts them to the limited data available for low-resource languages (Jia, Zhang, Weiss, Wang, Shen, Ren, Nguyen, Pang, Lopez Moreno, Wu et al., 2018). Voice cloning provides an effective solution for audio data in low-resource languages. By synthesizing speech data, it facilitates deepfake generation even with limited resources. Techniques such as few-shot learning enable models to accurately replicate a speaker's voice in a low-resource language using only a few samples (Arik, Chen, Peng, Ping and Zhou, 2018).\nMultispeaker text-to-speech (TTS) models can be tailored for low-resource languages using cross-lingual training or language adaptation methods. These models are capable of synthesizing speech that accurately replicates real individuals' voices, even with limited data availability. However, implementing such solutions requires careful consideration of linguistic and cultural nuances to ensure accurate and ethical use of deepfake technology. For low-resource languages, it is crucial to involve the respective language communities in deepfake research to ensure ethical use of the technology. Addressing the unique challenges faced by low-resource languages not only enhances deep-fake detection capabilities, but also promotes equitable technological development."}, {"title": "2. Literature Review", "content": "Mubarak, Alsboui, Alshaikh, Inuwa-Dute, Khan and Parkinson (2023) have done in depth survey on detection and impact of deepfakes with respect to visual, audio and text. They have examined the impacts of deepfakes in social, technical, political and economic domains. All deepfakes detection methods for audio, visual and textual have been analyzed separately for deep learning as well as handcrafted fetaure extraction methods. Tolosana et al. (2020) have reviewed the techniques for face manipulation to generate deepfakes. Liz-L\u00f3pez et al have reviewed the generation and manipulation techniques for deepfakes in terms of forensic from 2018 to 2923 including the datasets. The in depth analysis of all the benchmarks datasets available for audio, visual, audio-visual has been given by Liz-Lopez et al. (2024). The features most of the researchers have used in the past for lip sync based deep fake generation are Mouth landmarks, MFCC audio features (28-D), VGG-M network, MFCC audio features, Mel-spectrogram representation etc. In Facial attribute manipulation, the particular facial attributes are manipulated such adding/removing eyeglasses, skin manipulation such as smoothening, removing scars, etc. (Masood et al., 2023).\nIn audio manipulations, the deepfakes are generated by cloning a person's voice with some other voice which he never said before. Although synthesis voice is very helpful for development of entertainment industry, chatbots, AI assistant, assisting specially abele people. So the two popular techniques available for audio synthesis are text to speech and voice cloning. In text to speech, it generates the synthetic voice similar to speaker's natural sound from a given input text while voice cloning works on manipulation of waveforms of the source speaker to convert it into target speaker waveforms (Masood et al., 2023).\nFurther the author had also mentioned the limitations of existing datasets and reviewed the lipsync based deep fake generation like LIpGAN, Wav2Lip, based on GAN models. The author had highlighted the many deepfake generation techniques based on face re-enactment such as Face2Face, ReenactGAN, GANimation, GANnotation, X2face, FaR-GAN etc. He further reviewed face synthesis deepfake generation technique CoGAN, ProGAN, StyleGAN, TP-GAN, SAGAN, BigGAN, StackGAN. Further, some researchers have done facial attribute manipulation base deepfake generation such as techniques used are IcGAN, StarGAN, AttGAN, STGAN, PA-GAN, etc. (Masood et al., 2023). Kwon, You, Nam, Park and Chae (2021) have designed KODF Korean deepfake dataset using FaceSwap, DeepFaceLab, FS-GAN, FOMM i.e First Order Motion Model (FOMM) is a video-driven face-reenactment model, Wav2Lip. Korshunov and Marcel (2018) have evaluated the various deepfake videos generated using VGG and Facenet neural networks."}, {"title": "2.1. Deepfake Generation of Videos", "content": "Most of the available deepfake datasets have been generated using tools like StyleGAN, GANimation, FaceSwapping, SimSwap etc. For generation the foundation models used are autoencoder and GANS. Some of the best notable examples are: variational autoencoders, VAE-GAN, cycleGAN. Cycle GANS are highly used for image-to-image translation where images of domain X are converted into images of domain Y. Further, Face swap GAN (FSGAN) which swaps the face of one person with another using facial landmarks of face. It has three major components i.e. reenactment generator, segmentation CNN and inpainting network. StarGAN is basically works on image translation framework for different domains. In this the style encoder is used to extract the required style from input and the mapping network extracts the style code from the latent vector and the corresponding domain given as input. Now the generator by using the mapping network or the style encoder generates the output image corresponding to input image. Another methods are STGAN and face swap-GAN (Patel, Tanwar, Gupta, Bhattacharya, Davidson, Nyameko, Aluvala and Vimal, 2023).\nMelnik, Miasayedzenkau, Makaravets, Pirshtuk, Akbulut, Holzmann, Renusch, Reichert and Ritter (2024) have conducted an in-depth analysis of face generation methods utilizing StyleGANs. The study discusses various StyleGAN variants, ranging from Progressive GAN to StyleGAN3, and explores their underlying latent representations, training metrics such as losses, and datasets. In addition, it delves into the inversion process, which involves converting an image into StyleGAN latent codes. The primary approaches highlighted include gradient-based optimization, which minimizes the loss between real and generated images to refine latent vectors, and training an encoder model on sample images to map them into latent space. A further technique discussed is fine-tuning the generator for specific tasks. Similarly, Cheng (2024) address the limitations of existing GAN-based face generation methods, particularly the issues of domain irrelevance and insufficient representation of facial detail. Their proposed enhancement includes refining the generator architecture by integrating attention mechanisms and adaptive residual blocks to extract more nuanced facial features. A convolutional attention module is introduced after the residual block to emphasize important features in both the channel and the spatial dimensions, overcoming the original model's inability to focus on key facial regions. Using the CelebA dataset that contains 202,599 face images with attribute labels, their algorithm demonstrates an improved ability to generate faces across various age groups and genders. The output images were visually and quantitatively evaluated, and the algorithm achieved a 20. 06% and 14. 63% improvement in the PSNR and SSIM values, respectively. Furthermore, Khalid, Tariq, Kim and Woo (2021) describe the Celeb-DF dataset, which uses 500 real YouTube videos of celebrities to generate deepfake videos using Faceswap methods. This dataset is widely used for evaluating the performance of deepfake generation and detection algorithms."}, {"title": null, "content": "Felouat, Nguyen, Le, Yamagishi and Echizen (2024) presented the eKYC-DF dataset, a pioneering resource designed to tackle identity fraud challenges in eKYC systems. This data set comprises real and synthetic facial videos, serving as a critical tool to advance and evaluate eKYC systems, especially in deep-fake detection and facial recognition. The paper outlines five significant contributions. First, it provides a comprehensive analysis of critical concepts and methods relevant to commercial eKYC solutions, deepfake generation and detection, commonly used datasets in deepfake detection, and the vulnerabilities of eKYC systems to deepfake-based spoofing. Second, the eKYC-DF dataset is significantly larger than most datasets used in deep learning, offering better support for pattern recognition and model generalization. Third, the dataset's diversity, encompassing a wide range of ages, genders, and ethnicities, facilitates robust deep learning model training, minimizes biases, and improves prediction accuracy across different demographics. Finally, the paper sets a benchmark for assessing the effectiveness of the dataset in deepfake detection and facial matching, thereby enhancing its value in strengthening the security and reliability of eKYC systems.\nTolosana et al. (2020) provided a comprehensive review of face manipulation techniques in deepfake generation and detection, classifying them into four primary categories: complete face synthesis, identity swapping, attribute manipulation, and expression swapping. Entire face synthesis involves the use of advanced models like GANs and StyleGAN to generate entirely non-existent, high-quality face images that exhibit remarkable realism. The identity swap replaces one person's face with another using techniques such as face swapping. Attribute manipulation focuses on altering specific facial features, such as hair or skin color, gender, age, or adding accessories like glasses, often employing StarGAN-based approaches. Expression swapping, although similar to identity swapping, focuses on altering a person's facial expressions while preserving their overall identity. These categories illustrate the diverse techniques used in facial manipulation within deep-fake technology."}, {"title": "2.2. Deepfake Generation of Audio", "content": "Synthetic audio data generation is primarily achieved through two approaches: text-to-speech (TTS) and voice conversion. Most synthetic speech generation methods utilize an encoder-decoder architecture, making GAN-based models well suited for this purpose. Examples of TTS models include Char2Wav, which employs a bidirectional RNN as the encoder and a conditional extension of SampleRNN as the decoder. Another notable model is WaveNet, which operates as a sequence-to-sequence encoder-decoder network. The first component of WaveNet predicts spectrograms using an encoder to process character sequences into intermediate features, which are then decoded into spectrograms. The second component, a modified version of WaveNet, generates time-domain waveform samples based on the input of the spectrogram from the first stage.\nOther advanced TTS methods include WaveGlow, MelNet, Deep Voice 3, HiFi-GAN, and MelGAN, all of which leverage GANs or similar architectures for high-quality audio generation. Furthermore, voice impersonation represents another form of manipulation that goes beyond voice conversion by mimicking not only the target speaker's speech and signal qualities but also their unique speaking style. For such tasks, GANs have been used for style transfer, with DiscoGAN being a notable example (Patel et al., 2023).\nThe most widely used pre-trained models for deep audio generation include Tacotron 2, WaveNet, FastSpeech 2, VALL-E, WaveGlow, and TalkNet. Wang, Skerry-Ryan, Stanton, Wu, Weiss, Jaitly, Yang, Xiao, Chen, Bengio et al. (2017) introduced Tacotron, a sequence-to-sequence model that synthesizes speech directly from input text, designed to enhance the vanilla seq2seq architecture. Process raw input characters and generate Mel spectrograms using an attention mechanism integrated into the seq2seq model.\nBuilding on this, Tacotron 2 combines a neural network-based architecture with RNN and WaveNet to synthesize speech. The RNN model predicts Mel spectrogram sequences from input text using a sequence-to-sequence feature prediction network, while a modified version of WaveNet generates time-domain waveform samples from the predicted mel spectrograms. To evaluate Tacotron 2, subjective human evaluations, similar to Amazon's Mechanical Turk, were performed, using the Mean Opinion Score (MOS) to assess the quality of the generated speech (Shen, Pang, Weiss, Schuster, Jaitly, Yang, Chen, Zhang, Wang, Skerrv-Ryan et al., 2018)."}, {"title": null, "content": "(Wu, Yamagishi, Kinnunen, Hanil\u00e7i, Sahidullah, Sizov, Evans, Todisco and Delgado, 2017) highlights the ASV spoof challenges as pivotal in driving advancements in speaker verification research. The inaugural dataset, ASVspoof 2015, featured audio samples from 106 speakers, including both genuine and spoofed samples created using 10 distinct spoofing algorithms. Additionally, (Nautsch, Wang, Evans, Kinnunen, Vestman, Todisco, Delgado, Sahidullah, Yamagishi and Lee, 2021) reports that the ASVspoof 2019 challenge broadened the scope by incorporating four types of spoofed audio data: text-to-speech synthesis, voice conversion, audio replay attacks, and advanced spoofing techniques. This expansion provided a comprehensive dataset for evaluating and enhancing speaker verification systems."}, {"title": "2.3. Deepfake Generation of Audio-videos", "content": "Previous studies have concentrated on generating deepfake videos, often overlooking the audio component. Many available datasets either feature only real audio or lack audio entirely. The FakeAVCeleb dataset was introduced to fill this gap, providing an audio-video deepfake dataset that includes both deepfake videos and synthesized, lip-synced fake audio. Created using generative models and real YouTube videos of celebrities, it offers a multimodal resource encompassing diverse ethnic backgrounds. The dataset is designed to aid in the development of multi-modal deepfake detectors (Khalid et al., 2021).\nThe DFDC dataset was developed as part of the Deepfake Detection Challenge (DFDC) in collaboration with researchers and organizations including Amazon Web Services, Facebook, and Microsoft (Dolhansky et al., 2020). The DFDC dataset includes deepfake videos, synthesized cloned audio, and in some instances, both. These videos were created using eight distinct synthesis methods and recorded in diverse environmental conditions. However, a major limitation of the dataset is the absence of detailed labeling, especially regarding the alignment between audio and video. Khalid et al. (2021) introduced FakeAVCeleb, a novel multimodal deepfake detection dataset designed to address this limitation by providing synchronized labeling of deepfake videos and their corresponding synthesized cloned audio.\nFake audio generation involves integrating lip-syncing techniques with deepfake videos. Facial reenactment methods are employed to synchronize the synthesized audio with the target speaker, producing a deepfake video accompanied by corresponding fake audio (Khalid et al., 2021). The DFDC challenge concentrated on identifying whether the audio or video in a deepfake was fake. In comparison, the FakeAVCeleb dataset expands on this by addressing additional challenges, including varying environmental conditions, greater diversity, detailed audio-video labeling, and representation of individuals from different ethnicities, age groups, and genders. To create these deepfakes, a cloned voice of the target speaker is synthesized and synchronized with the video through facial reenactment techniques. Face-swapping tasks are performed using tools such as FaceSwap and FaceSwap GAN (FSGAN), while Wav2Lip is employed for facial reenactment based on source audio. For voice cloning, the Real-Time Voice Cloning (RTVC) tool is used. Additionally, synthesis processes are customized separately for each gender to enhance realism (Khalid et al., 2021)."}, {"title": "2.4. Deepfake Generation for Indian/Multilingual/Low Resource Languages", "content": "Hou, Fu, Chen, Li, Zhang and Zhao (2024) have designed multilingual dataset for deepfakes i.e. PolyGlotFake. This dataset contains content in seven languages and designed using all the Text-to-Speech, voice cloning, and lip-sync techniques. The PolyGlotFake dataset comprises a total of 15238 videos, including 766 real videos and 14472 fake videos. The average duration of each video is 11.79 seconds, with a resolution of 1280\u00d7720. Apart from this we have many recent works in multilingual deepfakes such as (Shelar, Ghatole, Pachpande, Bhandari and Shinde, 2022; Seong, Lee and Lee, 2021)\nAuthors have proposed audio based deepfake dataset for Urdu language using text to speech synthesis models like Tacotron and VITS TTS. The proposed dataset have been evaluated qualitative and quantitative using human evaluator, Equal Error Rate(EER), t-SNE plotting and comparing L2 norms. It uses 495 text sentences from Urdu news sources (Munir, Sajjad, Raza, Abbas, Azeemi, Qazi and Raza, 2024)."}, {"title": "3. Methodology", "content": "The HAV-DF dataset comprises 508 high-quality videos, including 200 authentic and 308 synthetically manipulated samples. All source videos were carefully curated from YouTube, adhering to strict selection criteria to ensure consistency and quality across the dataset. The selection of the videos were based on the frontal view of the subject, single person per video, subject in a straight and upright position, clear Hindi audio, Unobstructed centered face and minimal use/absence of accessories (e.g., hats, glasses, masks) that might occlude facial features. Further, the dataset includes a mix of videos featuring both celebrities and ordinary individuals, ensuring a wide range of facial characteristics and speaking styles. The topics covered in the videos span news segments, technology discussions, and podcast excerpts, providing a diverse representation of Hindi speech patterns and content. The dataset is nearly equally balanced between male and female subjects."}, {"title": "3.2. Proposed Architecture for HAV-DF", "content": "Figure 2 demonstrate the comprehensive methodology employed to create novel Hindi-language audio-video deep-fake dataset", "swap": "Replace the original faces in source videos with target faces", "sync": "Lip features has been extracted from the face-swapped frames and combined with audio waveforms to ensure realistic lip movements that match the speech. For this", "Cloning": "For audio manipulation, this process analyzes the original Hindi speech alongside a target voice model to generate a synthetic voice that maintains the content of the original speech but mimics the characteristics of the target speaker. For this, RVC Model has been used (Kambali, Ansari, Srivastav, Aryan and Nanda, 2023).\nThe final output of the proposed pipeline is a diverse set of deepfake videos that form the HAV-DF dataset. Each video in the dataset combines face-swapped and lip-syn"}]}