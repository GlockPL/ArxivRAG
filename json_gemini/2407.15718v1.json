{"title": "Integrating AI Tutors in a Programming Course", "authors": ["Iris Ma", "Alberto Krone Martins", "Cristina Videira Lopes"], "abstract": "RAGMan is an LLM-powered tutoring system that can support a variety of course-specific and homework-specific AI tutors. RAGMan leverages Retrieval Augmented Generation (RAG), as well as strict instructions, to ensure the alignment of the AI tutors' responses. By using RAGMan's AI tutors, students receive assistance with their specific homework assignments without directly obtaining solutions, while also having the ability to ask general programming-related questions.\nRAGMan was deployed as an optional resource in an introductory programming course with an enrollment of 455 students. It was configured as a set of five homework-specific AI tutors. This paper describes the interactions the students had with the AI tutors, the students' feedback, and a comparative grade analysis. Overall, about half of the students engaged with the AI tutors, and the vast majority of the interactions were legitimate homework questions. When students posed questions within the intended scope, the AI tutors delivered accurate responses 98% of the time. Within the students used AI tutors, 78% reported that the tutors helped their learning. Beyond AI tutors' ability to provide valuable suggestions, students reported appreciating them for fostering a safe learning environment free from judgment.", "sections": [{"title": "1 INTRODUCTION", "content": "The number of students enrolling in Computer Science has increased significantly in the past decades. This surge is driven by the growing reliance on software in modern society and the"}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 LLMs in Education", "content": "Large Language Models (LLMs) refer to extensive deep learning models constructed on the foundation of the transformer"}, {"title": "2.1.1 Impacts of Using LLMs in Educational Settings.", "content": "Several studies have explored the opportunities and challenges associated with integrating AI tools into education [3, 12, 33, 34].\nJacob et al. [9] investigated the experience of a non-native English speaker using ChatGPT to improve their English writing skills. The authors found that such tools can positively impact second-language learners by facilitating writing practice, rather than hindering their development.\nWhile AI tools powered by LLMs offer potential benefits for student learning, concerns regarding their inclusion, usability, technical limitations, ethical implications, and user well-being have also been raised [6]. A study [29] revealed that ChatGPT could exhibit harmful behaviors like dishonesty, manipulation, and misinformation dissemination. Thus, the development of responsible and accountable AI tools for real-world applications is critical [2, 16]."}, {"title": "2.1.2 LLMs in Computer Science Education.", "content": "Research on the application of LLMs in computer science education is still in its early stages. Studies have primarily focused on exploring their capabilities [14], particularly in programming task solving [5, 7] and educational content generation [18].\nThree recent studies have implemented LLM-powered AI tools for programming classes; all of these works are similar to ours, although with significant differences.\nLiffiton et al. [21] developed CodeHelp for entry-level programming classes. CodeHelp presents the students a fixed number of forms with which they can get help about specific blocks of code - explanations, error messages, etc. It was designed with a number of guardrails that ensure that the complete solution to programming problems is not presented to the students, and it only responds to content related to programming. Also, code blocks are removed from the responses. In order to implement these guardrails, CodeHelp makes several requests to the LLM, with several prompts. The results indicate that students found CodeHelp to be a valuable complement to direct support from instructors and teaching assistants.\nKazemitabaar et al. [14] developed CodeAid, a system that leverages few-shot learning to assist students with programming assignments and enhance their conceptual understanding. CodeAid is very similar to CodeHelp, in that it presents the students with a fixed set of help actions. Their prompts include examples (few-shot learning) for improved responses. Their results indicate that the tool provided correct and helpful answers in 90% of the time.\nLiu et al. [23] introduced a suite of AI-powered educational tools within a web application named CS50.ai, designed to"}, {"title": "2.2 Retrieval-Augmented Generation (RAG)", "content": "When users want to explore detailed knowledge into current or specific topics with an LLM, they may encounter responses that are hallucinatory and misleading [1, 10, 20, 22]. According to Li et al. [20], about 19.5% responses from ChatGPT are hallucinatory. To mitigate this, Lewis et al. have developed RAG, which connects generative AI models to external knowledge database [19]. RAG equips models with sources they can reference to make their responses verifiable and more dependable."}, {"title": "3 RAGMAN AND \u0391\u0399 TUTORS", "content": "RAGMan is our RAG framework for developing AI tutors, and it is fairly generic it can be used for education as well as other purposes. Each AI tutor built on top of RAGMan has its own knowledge base, and behaves in ways that are tuned for specific goals that are not necessarily all the same for all AI tutors. Here we explain the generic framework, and how we obtain specific AI tutors from it.\nFigure 1 depicts the overall architecture. RAGMan is structured into two main subsystems: the front-end and the back-end applications. The front-end is a NextJS application that"}, {"title": "4 INTEGRATION OF AI TUTORS", "content": "As is usual in Computer Science / Software Engineering, in the first year of our program, students are required to take a series of three introductory programming courses, ICS 31, 32, and 33. Our deployment targeted ICS32. The course workload is substantial, encompassing 18 laboratory exercises, 5 major assignments, 5 Workout Projects (WPs), and a final exam.\nIn order to keep the course relatively stable with respect to how it has been taught for the past several years, and also mitigate the risk of disruption caused by new technology, we introduced AI tutors only for the 5 WPs. The score for WPs accounts for 4.5% of the final grade. There are 5 WPs, but students must complete only 3; if they do more than 3, only the 3 top scores are counted for the WP score. Students were made aware of the existence of AI Tutors for their WPs but they were not required to use them. AI tutors were available 24 hours a day.\nIRB status: our work was classified in our institution as \"Non Human Subjects Research,\" because it was simply the introduction of a new, optional instructional tool in an existing course, with an extremely limited scope."}, {"title": "4.1 Data Collection", "content": "We collected anonymized conversations between students and AI tutors, with prior notification to students that their interactions would be saved for analysis. All data was stored securely on a school server accessible only by authorized personnel. Additionally, the student survey we employed was entirely voluntary, with informed consent obtained through the survey itself.\nThe submission rates for each work package (WP) were as follows: WP1 (61%), WP2 (29%), WP3 (25%), WP4 (53%), and WP5 (44%). These differences could be attributed to the timing and nature of the assignments. Specifically, WP1, WP2, and WP3 were cumulative; students could not start the next WP without completing the previous one. Additionally, the deadline for WP3 coincided with midterms. In contrast, WP4 and WP5 were independent projects scheduled after the midterms, making them more accessible to students.\nIn total, we collected 2,072 conversations comprising 4,769 message pairs. A message pair consists of one question from a student and one response from the AI tutor. Each conversation contains one or more message pairs."}, {"title": "4.2 Data Processing", "content": "We sorted the message pairs alphabetically, facilitating the visual detection of repetitions and patterns. We then removed questions that had been asked more than three times using"}, {"title": "5 OBSERVATIONS", "content": "In this section, we discuss the results of the students' interactions with the AI tutors, including the AI tutors' response quality, students' self-reported experience, and a comparison between course outcomes of identical offerings of the course, one in 2023, which did not adopt AI tutors, and another in 2024, which used AI tutors."}, {"title": "5.1 AI Tutors Response Quality", "content": "To assess the qualitative effectiveness of AI tutors in addressing student inquiries, we analyzed a random sample of 248 conversations, which represents 37% of the total dataset. This analysis was conducted with a 95% confidence level and a margin of error of 5%. The conversations were proportionally selected from different WPs: WP1 (81), WP2 (78), WP3 (41), WP4 (23), and WP5 (25). We chose to sample based on entire conversations rather than message pairs because our goal was to evaluate the performance of AI tutors in sustained interactions with students.\nWhile AI Tutors were primarily designed to assist students with WPs, we observed instances of students using them for other assignments and even non-programming tasks. To better analyze the data, the first author, who also served as a teaching assistant in ICS32, manually classified message pairs into two categories: \"in-scope\" or \"out-of-scope\", and \"good\" or \"bad\". A question in a message pair can be \"in-scope\" or \"out-of-scope\", and a response in a message pair can be \"good\" or \"bad\" quality.\nQuestions directly related to WPs, general programming knowledge, or greetings were deemed \"in-scope.\" Examples of \"in-scope\" questions are provided in Figure 2. Conversely, questions pertaining to specific unrelated assignments or non-programming topics were classified as \"out-of-scope.\" Examples of \"out-of-scope\" questions can be found in Figure 3.\nA good quality response from AI tutors should exhibit three key characteristics, as exemplified in Figure 4. First, the response must accurately and effectively address the query. Second, honesty is crucial: if the AI tutor lacks sufficient context relevant to the specific assignment, it should acknowledge this limitation. Finally, the AI tutor should be equipped to identify and decline to answer queries that are either inappropriate or unrelated to the domain of programming. Responses that fail to meet any of these three criteria are considered ineffective.\nThe analysis indicated that 74% of student queries fell within the scope of AI tutors' capabilities, while conversely, 26% of queries were classified as out of scope. Moreover, 94% of responses were considered \"good.\" Responses to questions within the scope were more often \"good\" (98%) compared to responses to out-of-scope messages (81%). For example, in Figure 4, a student inquired about the functionality of the tell() function in WP1. The AI tutor effectively addressed this query by first explaining the purpose of tell() and the data it returns. Subsequently, the tutor provided a relevant example and establishes a connection between the method and the student's current project. Finally, the AI tutor concluded by posing a question to the student.\nAlthough rare, AI tutors sometimes fail to provide accurate responses, even for inquiries within their intended scope. A deeper investigation revealed instances where AI tutors offered potentially misleading advice. As exemplified in Figure 5, a student inquired about retrieving the length of text within a file for WP3. The AI tutor recommended using the len() function. However, the len() function is not the most suitable choice for this specific scenario. Firstly, employing len() necessitates loading the entire file content into memory at once. This approach could be problematic for exceptionally large files, potentially causing memory exhaustion. Secondly, a core objective of this project was to facilitate student comprehension of pointer concepts. Consequently, suggesting the use of seek() and tell() functions to determine file length would be more aligned with this learning objective.\nInterestingly, while 26% of questions fell outside the scope of AI tutors' capabilities, 81% of responses were still rated as \"good.\" Two primary reasons emerged for AI tutors' effectiveness in handling out-of-scope questions. First, when students sought assistance with assignments beyond WPs,"}, {"title": "5.2 Students' Feedback", "content": "A survey was distributed one week before the end of instruction, with 88% response rate. 79% of students who responded the survey indicated that they have used at least one AI tool in ICS32, with AI tutors being the most popular choice for these students (58%). 21% of them relied solely on tools like ChatGPT. The use of AI tools in programming classes is likely an inevitable trend, even in the absence of a designated classroom tool. However, unconstrained access to AI tools that provide solutions without guardrails is likely detrimental to student learning.\nThen we asked about their overall experience with the AI tutors. This included assessments of AI tutors' perceived helpfulness, advantages, and disadvantages. 79% of students reported the AI tutors had a positive impact on their learning process in ICS32. A summary of the survey results is as follows:\n(1) The majority students found that AI tutors were helpful with WPs. Specifically, 13% of them \"strongly agree,\" 45% \"agree,\" 30% \"neutral,\" 9% \"disagree,\" and 3% \"strongly disagree.\"\n(2) Even though AI tutors were not designed to assist with other assignments, some students still found they are helpful with 5% \"strongly agree,\" 18% agree,\" 45% \"neutral,\" 21% \"disagree,\" and 11% \"strongly disagree.\"\nMost students appreciated AI tutors' ability to clearly explain WPs and recommend effective methods. Some valued the step-by-step guidance for problem-solving and its contribution to their understanding of programming concepts. Notably, a significant advantage of AI tutors was that students did not not have to worry about being judged because of the questions they asked.\nOn the down side, most students expressed dissatisfaction with AI tutors' slowness. The implementation we had at the time was, indeed, very slow, and this affected the user experience. We have since fixed this issue, but not in time for ICS32. Another concern raised by students was the occurrence of contradictory responses. An example of this is illustrated in Figure 5, where AI tutor contradicted itself. Some students felt that AI tutors lacked detailed explanations. This could be attributed to the limitation of not incorporating context from past homework assignments."}, {"title": "5.3 Impact on grades", "content": "Note: the results presented here stand on a highly complex, multi-variate context. Nevertheless, since we had access to the grades of the previous year's course, we make a simple comparative analysis of the grades, to try to identify any statistically significant effects of the AI tutors.\nWe compared the grade distributions of the cohorts from the Winter 2023 (without AI tutors) and Winter 2024 (with AI tutors) offerings of the class by the same instructor. In this analysis, we compared the distribution of the grades of each WP between the two cohorts, the distribution of the combined WP grade, the grades of the final exam, and the distribution of the final course letter grades.\nTo compare the continuous distribution of numerical grades of the WPs, we used classical two-sample Kolmogorov-Smirnov tests [KS; 15, 27]. We also used Wilcoxon-Mann-Whitney and Anderson-Darling tests, resulting in similar conclusions. The two-sample KS tests indicated that the grade distributions showed no statistically significant difference between the cohorts in WP1, WP2, and WP5, with resulting p-values of 0.132, 0.248, 0.805. The other two WPS, WP3 and WP4, showed statistically significant differences, with p-values of 7.64 \u00d7 10-14 and 0.011, respectively. In WP3, the 2024 cohort showed a worse outcome, while in WP4, the 2024 cohort showed a better outcome. As mentioned before, there was a modification in the deadline for WP3 in 2024, which was increased by a week and made it overlap with midterms. This WP had the lowest submission rate among all assignments.\nIn this course, the student grades of the individual WPs are combined in a single WP grade, accounting for 4.5% of the student's final grade. To calculate this WP grade, the two lowest-graded WPs of each student are dropped, and the grades of the three highest-graded WPs are added together and then rescaled to a maximum of 4.5 points. The comparison of this combined WP grade distribution between the two cohorts shows a statistically significant difference with a p-value of 0.012 and with the 2024 cohort displaying a mode of 0.2% points higher than the 2023 cohort. The distribution of the numerical grades of the final exam showed no statistically significant difference between the two cohorts,"}, {"title": "6 CONCLUSION", "content": "This paper introduces RAGMan, an LLM-powered tutoring system designed to support entry-level programming students with homework assignments. AI tutors build on top of RAG-Man to offer students a conversational interface to help them with problem-solving without directly providing solutions.\nWe conducted a manual analysis to evaluate the quality of AI tutors' responses. The results show that 93.5% were categorized as good, meaning they correctly and effectively addressed student inquiries. We also distributed a survey to assess student experiences using AI tutors. Over 78% of students who have used AI tutors reported that the tutors facilitated their learning in class, which indicates that regardless of any real impact on the course outcomes, students felt that they are learning more when they are assisted by such technology. The survey results also show that students appreciated the AI tutors' ability to provide helpful study advice in a judgment-free environment where they felt comfortable asking questions. Efforts to improve AI tutors will focus on minimizing the generation of contradictory responses.\nFinally, we compared the grade distributions of a course cohort that used AI tutors and another that did not, offered in two different years. This comparison shows a statistically significant increase in the number of students approved in the course and an increase in the number of students receiving middle grades (i.e., B). This data suggests, although it does not demonstrate, that AI tutors can positively impact student success and provide important help, especially to students who would be struggling in challenging courses."}]}