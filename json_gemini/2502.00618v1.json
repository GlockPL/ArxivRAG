{"title": "DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models", "authors": ["Chiyuan He", "Zihuan Qiu", "Fanman Meng", "Linfeng Xu", "Qingbo Wu", "Hongliang Li"], "abstract": "Continual adaptation of vision-language models (VLMs) focuses on leveraging cross-modal pretrained knowledge to incrementally adapt for expanding downstream tasks and datasets, while tackling the challenge of knowledge forgetting. Existing research often focuses on connecting visual features with specific class text in downstream tasks, overlooking the latent relationships between general and specialized knowledge. Our findings reveal that forcing models to optimize inappropriate visual-text matches exacerbates forgetting of VLMs. To tackle this issue, we propose DesCLIP, which leverages general attribute (GA) descriptions to guide the understanding of specific class objects, enabling VLMs to establish robust vision-GA-class trilateral associations rather than relying solely on vision-class connections. Specifically, we introduce a language assistant to generate concrete GA description candidates via proper request prompts. Then, an anchor-based embedding filter is designed to obtain highly relevant GA description embeddings, which are leveraged as the paired text embeddings for visual-textual instance matching, thereby tuning the visual encoder. Correspondingly, the class text embeddings are gradually calibrated to align with these shared GA description embeddings. Extensive experiments demonstrate the advancements and efficacy of our proposed method, with comprehensive empirical evaluations highlighting its superior performance compared to existing pretrained and VLM-based continual learning methods.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, deep models pretrained on large-scale datasets have achieved remarkable success across visual, linguistic, and multi-modal domains. Pretrained vision-language models (VLMs), exemplified by CLIP [1] and ALIGN [2], have demonstrated substantial promise in handling open-vocabulary tasks. Despite their strong zero-shot capabilities in common domains, VLMs often underperform on specialized tasks, such as distinguishing low-quality images or identifying fine-grained object categories. Consequently, significant efforts [3], [4], [5], [6], [7], [8] have focused on adapting VLMs on downstream datasets to adapt to these new tasks. However, as the demand for and volume of data continue to grow, incorporating previous models and data for joint training imposes substantial storage and computational overhead. Considering the significant cost of repeatedly training foundation models, exploring continual learning (CL) becomes particularly valuable in this context.\nRecently, Zheng et al. [9] and Zhang et al. [10] have highlighted the risk of losing existing generalization capabilities when adjusting VLMs with generic knowledge to specialized domain models. This adjustment may result in the model losing effectiveness on prior tasks and lacking potential for optimization on subsequent tasks. This phenomenon, known as catastrophic forgetting in the field of continual learning, is particularly pronounced o n VLMs. Unlike conventional scenarios [11], [12], [13], [14], catastrophic forgetting o f VLMs impacts not only the task-specific knowledge of previously learned tasks but also the extensive pretrained knowledge, presenting significant challenges in adjusting VLMs for continual tasks.\nOver the past few years, research [16], [10], [17], [18], [9] has explored how VLMs can adapt to CL tasks. Conventional adapting approaches [3], [4], [7], [19], [20] have shown limited effectiveness in adapting pretrained VLMs for incremental tasks. This limitation is primarily due to their reliance on shared structures and prompts across all tasks, which often results in forgetting of previously learned knowledge when"}, {"title": "II. RELATED WORK", "content": null}, {"title": "A. Continual Learning", "content": "Continual Learning (CL) investigates how deep models can incrementally learn knowledge. Existing CL research can be categorized into several types based on the strategies they employ. Among these, regularization-based methods [11], [31], [32] introduce regularization terms during model training to penalize forgetting old knowledge. These regularization terms can either focus on protecting model parameters [31] or on output distributions [11], [33] (e.g., knowledge distillation). Dynamic network-based methods [34], [35], [36], [37] aim to learn the model by introducing new structures for new tasks while preserving old knowledge, although this incurs substantial overhead as model parameters increase with the number of tasks. Recently, replay-based methods have become increasingly common. Data replay methods [38], [37] assist models in retaining old knowledge by recalling a small number of real samples. Additionally, some methods [39], [40], [41] recall old knowledge by storing sample features and the distributions of these features. However, replay-based methods introduce storage costs and require repetitive computation for old data.\nIn recent years, studies such as [42], [43], [44], [45] have predominantly focused on integrating additional components for incremental tasks, such as learnable prompts [44], [45], [46], [47] and adapters [42], [43], into pretrained models. This integration necessitates the development of methods for selecting and evaluating the relevance of these components to ensure both their appropriateness and compatibility with the pretrained model. However, a significant limitation arises as the number of tasks increases: the associated computational and storage costs grow substantially, posing challenges to scalability and efficiency."}, {"title": "B. Vision-Language Models", "content": "With advancements in pre-training techniques, large-scale foundation models [1], [48], [49], [50], [51] have significantly impacted the industry. For instance, Vision-Language Models such as Contrastive Language-Image Pretraining (CLIP) [1] and Adversarially Learned Inference for Image-Text Matching (ALIGN) [2] have demonstrated remarkable zero-shot capabilities for general tasks. However, despite being pre-trained on over 400 million image-text pairs, CLIP still face challenges in specific downstream tasks, such as accurately identifying certain types of vehicles and lizards.\nTo better adapt VLMs for downstream tasks, various text prompt-based fine-tuning methods [3], [4], [6], [7], [52] have been proposed, which can enhance VLM performance on specific tasks. In more complex scenarios, learnable prompts can be inserted into intermediate layers [8] to incorporate more sophisticated general knowledge. Additionally, the integration of adapter structures [53], [19], [20] has also been shown to be an effective strategy. Other approaches [54], [55], [56], [57]"}, {"title": "C. Continual Adaptation for VLMs", "content": "Investigating the continual learning and adaptation of VLMs for diverse downstream tasks holds significant value, as it reduces data storage requirements and computational redundancy while addressing the challenge of inaccessible previous data. It is crucial to protect the model's gerenic pretrained knowledge and previously learned knowledge. The full fine-tuning strategies discussed in II-A will lead to significant forgetting of pre-trained knowledge, which is a notable distinction between pre-trained foundation models (e.g., CLIP) and small-scale deep models. Additionally, frameworks such as CoOp [3] and CoOpOp [4] have been shown to have limited adjustment capabilities for VLMs in incremental tasks due to their reliance on shared structures and contextual prompts across all tasks, leading to forgetting old knowledge during the process of fitting new knowledge. To solve this, Wang et al. [16] introduced AttriCLIP, which establishes a shared attribute bank for all tasks and selects suitable contexts based on visual images to bridge the gap between images and text. Yu et al. [18] proposed using a mixture of experts (MoE) framework to adapt knowledge for different tasks, decoupling the model's zero-shot capabilities from its specialized task abilities. From the perspective of parameter sparse updating, efforts from SPG [22], SparseCL [21], and SPU [10] have aimed to update VLM parameters selectively by employing appropriate \u201cimportant parameter\u201d selection patterns; for example, SPU selects more important parameters for updates based on the gradients accumulated by batches. Additionally, Zheng et al. [9] and Yu et al. [23] proposed the use of additional reference datasets to facilitate knowledge distillation in a VLM, effectively mitigating the forgetting of generic knowledge."}, {"title": "III. METHODOLOGY", "content": null}, {"title": "A. Preliminaries", "content": "1) Continual Learning Formulation: A sequence of task datasets is denoted as {$D_1,D_2,..., D_t$}. During training on task $D_t$ ($t \u2208 {1,2,..., T}$), access to data from previous tasks {$D_1, D_2,..., D_{t-1}$} is either highly restricted or entirely unavailable. In class-incremental learning (CIL), datasets for different tasks are introduced sequentially. Each task t is associated with a unique set of classes $C_t = {C_{t,1}, C_{t,2}, ..., C_{t,|c_t|}},$ where $|C_t|$ denotes the number of classes in task t. The classes associated with different tasks are disjoint:\n$C_t \u2229 C_{t'} = \u00d8, \u2200t \u2260 t', t,t' \u2208 {1,2,...,T}.$ \n2) CLIP for Incremental Tasks: CLIP [1] comprises an image encoder $F_e(\u00b7)$ and a text encoder $T(\u00b7)$. Specifically, an image $x \u2208 R^{H\u00d7W\u00d73}$ and a text prompt, referred to as the rudimentary prompt $RP_y$, are input into $F_e(\u00b7)$ and $T(\u00b7)$,"}, {"title": "B. Overview of proposed DesCLIP", "content": "The overall architecture of our proposed framework is shown in Fig. 3. Within our framework, the CLIP's textual encoder $T(\u00b7)$ remains fixed and comprises two input branches: one processes rudimentary prompts derived from basic class names, while the other generates a diverse pool of general attribute (GA) description candidates via a language assistant. To obtain highly relevant visual-GA text pairs, we introduce the anchor-based embedding filter (AEF). The AEF identifies the most relevant attribute description embeddings from the candidate pool with respect to the current visual features. These filtered embeddings are then paired with visual features to compute a class-agnostic instance-matching loss, which is utilized to fine-tune the visual encoder $F_e(\u00b7)$. Concurrently, the text embeddings are gradually calibrated to align with shared attribute embeddings, further enhancing the consistency among representations of vision, GA, and downstream classes."}, {"title": "C. General Attribute Description Generation", "content": "CLIP establish robust visual-textual associations during the pre-training phase through instance-level contrastive learning. However, most existing research overlooks this foundational capability, conventionally relying on fixed, hand-crafted templates combined with class names as prompts to derive \"prior\" classification weights via the textual encoder. Although Wang et al. [16] introduced an \"attribute bank\" to enable attribute sharing across different task instances, this approach lacks intrinsic relevance to specific classes. For instance, attributes such as \"white\" and \"grass\" fail to provide meaningful distinctions between classes like \"cat\" and \"dog\".\nTo address this limitation, we propose the use of a language assistant to generate rich, contextually relevant attribute descriptions for specific classes. The language assistant utilizes an advanced large language model (LLM) with a generalized understanding of downstream task entities. Drawing inspiration from [25], [26], [28], we design a variety of describe-request prompts (DRPs) to guide the language assistant in generating visually relevant attribute descriptions. Examples of basic DRPs include:"}, {"title": "D. Anchor-based Embedding Filter", "content": "The generated embedding candidates of GAs do not always align with visual representations due to potential domain discrepancies or unrelated information in the text descriptions produced by the language assistant. To establish robust vision-GA associations, we propose an anchor-based embedding filter (AEF) mechanism to refine the embedding candidates. This mechanism identifies candidates that sufficiently match the visual representations, enabling the construction of approximate image-text pairs tailored to the specific requirements of the task.\nFor a training sample $(x_i, y_i)$, where $y_i = c = {t,k}$ is assumed, the label c is considered to correspond to the k-th class of incremental task t, with k \u2208 {1,2,...,|$C_t$|}. As illustrated in Fig. 3, the inputs to AEF include the visual features $z_i = F_e(x_i)$, the rudimentary text embedding $w_c = T(RP_y)$, and the embedding candidates $EC_c$. The cosine similarity between the visual features $z_i$ and the rudimentary text embedding $w_c$ is calculated as:\n$CS (\\frac{z_i}{||z_i||}, \\frac{w_c}{||w_c||}) = \\frac{z_i^T w_c}{||z_i||' || w_c||}$\nSubsequently, the similarity scores between the visual features and each embedding candidate in EC are calculated:\n$EC\\_S_j^c = (\\frac{z_i}{||z_i||}, \\frac{EC_j^c}{||EC_j^c||})$\nwhere j\u2208 {1,2,...,$n_{dsc}$}. To mitigate the risk of overfitting in the CLIP's visual encoder, visual features with low relevance to either class text or GA descriptions should be filtered out. Hence, a condition $x(z_i)$ is defined to filter visual features as:\n$\\chi (z_i) = \\begin{cases} 1, & \\text{if } \\max(CS, \\max_j EC\\_S_j^c) > \\delta_{\\alpha},\\\\ 0, & \\text{otherwise.} \\end{cases}$"}, {"title": "E. General Attribute-Guided Progressive Visual-Textual Alignment", "content": "This section introduces the methodology for optimizing the visual and textual branches of CLIP in incremental tasks, leveraging the filtered embeddings identified for relevant training samples. Within the CLIP architecture, the optimization focuses on the visual encoder $F_e(\u00b7)$ (specifically, the initial MLP layers within each Transformer block [10]) and the rudimentary text embeddings introduced for the current task.\n1) Instance Matching: To align the visual features with highly relevant embedded GA descriptions, we select the most closest textual representation $h_1$ as the paired text embedding:\n$h_1 = FE [\u03b8].$\nThe instance matching loss $L_{IM}$ is computed across the batch as:\n$L_{IM} = E_{i\u2208P} [ -log \\frac{M_{i,i}}{M_{i,i} + \\sum_{j\u2208P, j\u2260i} M_{i,j}}];$\nwhere\n$M_{i,j} = exp(\\tau (\\frac{z_i}{||z_i||}, \\frac{h_j}{||h_j||}));$\n$M_{i,i} = exp(\\tau (\\frac{z_i}{||z_i||}^T \\frac{h_i}{||h_i||}));$\nHere, \u03c4 denotes an elevated temperature scaling factor, defined as \u03c4 = 107. The set P represents the indices of valid samples within a batch of size B and is specified as:\n$P = {i | i \u2208 {1, ..., B}, \\chi(z_i) = 1, FE^2 \u2260 0},$\nwhere $\\chi(z_i)$ is the condition defined in III-D. For each incremental task t, we tune the model using a contrastive learning framework similar to the pretraining strategy of CLIP [1]. To mitigate forgetting, we adhere to a \u201cnearest matching\u201d principle, aligning visual features with GA description embeddings exhibiting higher correlations. This approach minimizes the risk of overfitting visual features to specific class text embeddings, maintaining both generalization and previous knowledge.\n2) Text Embedding Calibration: General attribute descriptions play a pivotal role in guiding the calibration of text embeddings to achieve better alignment with visual representations. This alignment is particularly crucial because the original rudimentary text embeddings often exhibit weak correlations with visual features in \u201cunfamiliar\u201d downstream tasks. Such misalignment can lead to overfitting in the visual branch of the VLM to class labels, thereby exacerbating forgetting. To mitigate this issue, we propose a weight-shifting mechanism that calibrates the rudimentary text embeddings for the classes in task t. This mechanism repositions the text embeddings toward representative attributes shared across the corresponding visual features, fostering stronger alignment between shared general attributes and class-specific text embeddings. Specifically, we define a shifting weight $s_{t,k} \u2208 R^D$, and a shift transformation $\\psi(w,s)$ for the calibration of rudimentary text embedding $w_{t,k}$, where {t,k} representing the k-th class of incremental task t, k \u2208 {1,2, ..., |$C_t$|}. The calibrated text embedding $w'_{t,k}$ can be obtained as:\n$w'_{t,k} = \\psi (w_{t,k}, s_{t,k}) = \\frac{w_{t,k}}{||w_{t,k}||} + \\alpha \\cdot s_{t,k}.$\nThe key to text embedding calibration is to ensure a strong correlation with the visual representations of the class while preventing an excessive focus on any single attribute text. Therefore, $w'_{t,k}$ should be aligned with FE, which is filtered based on the visual features $z_i$ of the class c = {t, k}:\n$\\ell_{IA} = E_{u\u2208FE} [ -\\gamma \\frac{w_{t,k}^T u}{||w_{t,k}|| ||u||} + (1-\\gamma) |\\frac{w_{t,k}}{||w_{t,k}||} - \\frac{u}{||u||} |];$\nwhere \u03b2 is a parameter that determines whether the alignment of text embeddings places greater emphasis on their absolute distance in the text space (e.g., Euclidean distance) or on the directional consistency. Text alignment loss $L_{TA}$ Of the current batch is: $L_{TA} = \\sum_{i\u2208P} \\ell_{IA}$\u00b7\nSince, in the context of continual learning, data from previous tasks cannot be revisited during the current task, the weights $w'_t$ = {$w'_{t,1}, w'_{t,2}, ..., w'_{t,|C_t|}$} are calibrated solely within the scope of the current task t. Consequently, only the shifting weights $s_t$ = {$s_{t,1}, s_{t,2}, ..., s_{t,|C_t|}$} associated with the current task are learnable, whereas the shifting weights $S_{0:t-1}$ = {$S_0, S_1, ..., S_{t\u22121}$} from previous tasks remain fixed.\n3) Reconstructed Intra-task Classification: To ensure alignment between text embeddings and the visual branch during optimization, we reconstruct the classification loss for task t. This loss constrains the calibration of text embeddings to remain within the low-loss region of the classification space for the current task, which is a critical prerequisite. Specifically, for each visual feature $z_i$ in a batch, its similarity to all calibrated text embeddings for the current task is computed to generate predicted logits. These logits are aligned with the ground truth label $y_i$, and the prediction classification loss is"}, {"title": "IV. EXPERIMENTS", "content": null}, {"title": "A. Setup", "content": "1) Datasets: The evaluation experiments for continual learning are conducted on CIFAR100 [29], ImageNet-Subset [24] and CUB-200 [30]. ImageNet-full [24] is utilized as a control set to evaluate the retention of the pretrained generalization knowledge in CLIP."}, {"title": "B. Comparison with State-of-the-art Methods", "content": "The accuracy results for continual learning, including 'Last' and 'Avg', are summarized in Table I. These results are derived from comprehensive experiments conducted on multiple datasets under varying incremental task settings. Additionally, Fig. 5 depicts the forgetting curves, comparing recent methods and providing a detailed evaluation of accuracy after each incremental task."}, {"title": "C. Ablation Study", "content": "1) Effectiveness of Each Component: We conduct detailed ablation studies on various components of our DesCLIP framework. As shown in Table III, the zero-shot model is used as the baseline, and the relative improvements or declines are reported. It is evident that a fully fine-tuned visual encoder (V.E.(full)) suffers from catastrophic forgetting. Without data replay, merely fine-tuning the visual encoder (V.E.) partially (tuning the first MLPs in each Transformer block) proves inadequate for continual learning [10]. Based on V.E., by integrating instance matching (IM) with visually filtered attribute description embeddings, we achieve a relative improvement of 2~3% over the zero-shot baseline. This improvement highlights the effectiveness of IM in mitigating forgetting, which typically arises from overfitting specific downstream task classes. Furthermore, the impact of text embedding calibration (TEC) is notable. The combination of V.E.+TEC+IM+TA, leveraging the filtered attribute description embeddings, yields the best performance, showcasing the synergy between these components.\n2) Parameter Selection: Table IV presents the 10-task performance under different parameter configurations. Our analysis reveals that the effectiveness of the instance matching strength, \u03bb\u03b9\u03bc, is highly sensitive to the specific task context. Stronger instance matching proves advantageous in scenarios requiring finer granularity, such as CUB-200 [30]. On the other hand, the filtering of attribute description embeddings plays a crucial role. Excessively conservative filtering (e.g., \u03b3 = 0) or overly stringent filtering (e.g., \u03b3 = 0.05) leads to noticeable performance degradation.\n3) Types of Text Embedding Calibration: We demonstrate the comparisons of different types of TEC in Table V. \u201c\u03a8(w,s) = w/||w||+\u03b1\u00b7s\u201d is the optimal solution, which is an calibration of the rudimentary text embedding in cosine space, which can best approximate the alignment of the representative shared attribute description embedding.\n4) Generated Attribute Descriptions: We analyze the impact of the generated description amount on the performance of continual learning, as shown in Fig. 8. For coarse CIFAR100"}, {"title": "V. LIMITATIONS", "content": "Observed in experimental trials, The effectiveness of our DesCLIP framework hinges on the language assistant (or real experts) being knowledgeable about the general attribute features of objects. This can pose challenges in certain applications that require indirect reasoning, such as StanfordCars [63], where it is difficult to accurately describe the representative features of a vehicle model associated with a specific license plate. Additionally, there are high standards for the quality of attribute descriptions; inappropriate prompts can introduce domain bias, ultimately hindering the ability of the Anchor-based embedding filter (AEF) to select highly relevant attribute features."}, {"title": "VI. CONCLUSIONS", "content": "Current research for VLM-based continual learning predominantly emphasizes to connect visual inputs and specific new-class text in downstream tasks, frequently neglecting the latent relationship between general knowledge and specialized knowledge for VLMs. In this paper, we propose DesCLIP, a framework that harnesses general attribute (GA) descriptions to enhance VLMs in establishing robust vision-GA-class text associations. By going beyond the traditional connections between visual inputs and class texts, DesCLIP employs a language assistant to generate candidates for attribute descriptions through tailored prompting. Additionally, we implemented an anchor-based embedding filter (AEF) to extract highly relevant description embeddings, which serve as paired text embeddings for instance matching (IM). Addtionally, we perform text embedding calibration (TEC) which allows for the progressive calibration of rudimentary text embeddings to align with representative GA representations. Our extensive experiments validate the effectiveness and advancements of DesCLIP, demonstrating its superior performance over existing pretrained and VLM-based continual learning methods."}]}