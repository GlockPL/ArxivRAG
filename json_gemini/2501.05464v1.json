{"title": "LLM-MedQA: Enhancing Medical Question Answering through Case Studies in Large Language Models", "authors": ["Hang Yang", "Hao Chen", "Hui Guo", "Yineng Chen", "Ching-Sheng Lin", "Shu Hu", "Jinrong Hu", "Xi Wu", "Xin Wang"], "abstract": "Accurate and efficient question-answering systems are essential for delivering high-quality patient care in the medical field. While Large Language Models (LLMs) have made remarkable strides across various domains, they continue to face significant challenges in medical question answering, particularly in understanding domain-specific terminologies and performing complex reasoning. These limitations undermine their effectiveness in critical medical applications. To address these issues, we propose a novel approach incorporating similar case generation within a multi-agent medical question-answering (MedQA) system. Specifically, we leverage the Llama3.1:70B model, a state-of-the-art LLM, in a multi-agent architecture to enhance performance on the MedQA dataset using zero-shot learning. Our method capitalizes on the model's inherent medical knowledge and reasoning capabilities, eliminating the need for additional training data. Experimental results show substantial performance gains over existing benchmark models, with improvements of 7% in both accuracy and F1-score across various medical QA tasks. Furthermore, we examine the model's interpretability and reliability in addressing complex medical queries. This research not only offers a robust solution for medical question answering but also establishes a foundation for broader applications of LLMs in the medical domain.", "sections": [{"title": "I. INTRODUCTION", "content": "The advent of Large Language Models (LLMs) has revolutionized the field of natural language processing, offering unprecedented capabilities in understanding and generating human-like text across a multitude of domains. However, the medical domain poses unique challenges due to its specialized terminology, complex reasoning requirements, and the critical importance of accuracy in patient care. Medical Question Answering (QA) systems which are designed to provide accurate and reliable information in response to medical queries, must navigate this complexity while ensuring the safety and efficacy of the information delivered. Despite significant advancements, existing LLMs often struggle with the nuances of medical language and the need for precise reasoning, which is essential for high-quality medical QA.\nIn this study, we introduce a multi-agent framework, as shown in Fig. 1, to tackle the QA task on the medical domain. This framework leverages the Llama3.1:70B model, the off-the-shelf LLM model with 70 billion parameters, to enhance the performance of medical QA systems on the MedQA dataset. Our multi-agent approach incorporates specialized agents to handle the inherent complexity of medical QA. Each query in the system is assigned a series of experts, including question-specific analysis, option analysis, and case generation. A key innovation of our multi-agent system is the integration of a case generation module. This module autonomously generates supportive clinical cases tailored to the given question and selected options, as described in detail. Its purpose is to produce plausible and contextually accurate cases that substantiate the correct option for a specific medical problem. These cases, which are integrated into the reporting module, address a critical gap in existing systems by providing transparent and contextually rich explanations.\nFurthermore, the system leverages the Llama3.1:70B model's vast capacity for zero-shot learning, enabling it to reason through complex and specialized medical queries without requiring additional training examples. This capability is especially valuable for the MedQA dataset, where annotated data is scarce and costly to generate, allowing the system to adapt to diverse scenarios with minimal data preparation.\nOur primary research objective is to demonstrate the effectiveness of the Llama3.1:70B model, combined with a multi-agent framework, in addressing the challenges of medical QA. Specifically, we aim to illustrate how the architectural advantages of both the model and the multi-agent system contribute to improved accuracy, reliability, and interpretability in handling medical queries. Through a series of experiments, we evaluate the model's performance on the MedQA dataset and compare it with other state-of-the-art models. The contributions of the paper are summarized as follows.\n\u2022 We introduce a novel concept of case studies in the"}, {"title": "II. RELATED WORK", "content": "In recent years, large language models have brought transformative changes to the medical field [9], reshaping key areas such as diagnostics, treatment planning, and communication between healthcare professionals and patients [10]. By assisting physicians in symptom analysis and disease diagnosis [11], LLMs enhance the accuracy and efficiency of medical assessments [12], [13]. These models are also capable of supporting clinical decision-making by providing evidence-based recommendations tailored to individual patients [14]. Additionally, LLMs play a pivotal role in synthesizing and summarizing complex medical information, making it more accessible to both medical professionals and patients. Their ability to convey medical advice in a clear and understandable manner significantly improves doctor-patient communication [15], [16]. Moreover, LLMs facilitate the electronic documentation of patient records, streamlining administrative tasks and improving workflow efficiency [17]. Collectively, these advancements highlight the indispensable role of LLMs in optimizing healthcare delivery and outcomes [18].\nTraditionally, enhancing the performance of LLMs in specialized medical tasks has relied heavily on fine-tuning with domain-specific datasets [19]. This process involves curating high-quality medical data and adapting pre-trained models through transfer learning, allowing the models to perform more effectively in new and complex tasks [20], [21]. While fine-tuning has proven effective in refining the capabilities of a single model, it requires substantial computational resources and extensive retraining [22], [23]. However, novel approaches are emerging that bypass the need for additional training, offering a more efficient and cost-effective alternative [24], [25]. These approaches enable healthcare providers to benefit from advanced model applications without the need for extensive customization, thus making these tools more accessible across different medical environments [26].\nIn contrast to the traditional single-model fine-tuning approach, the multi-agent system offers a more robust framework for medical decision-making. By enabling multiple agents to collaborate, exchange information, and analyze clinical cases from diverse perspectives, multi-agent systems enhance the accuracy and reliability of medical decisions [27]. This collaborative approach harnesses the collective intelligence of various agents, resulting in more comprehensive and well-"}, {"title": "III. METHODOLOGY", "content": "In this section, we propose a specific multi-agent model framework to tackle the task of Medical Question Answering. The overall model consists of six components: (1) Multi-agent generation, which includes the creation of question experts and option experts. (2) Proposition analysis, this step involves a detailed analysis of the problem and the available options. (3) Case generation, which generates relevant cases based on the input questions and provided options. (4) Report digest, a report is generated by synthesizing insights from problem analysis, option analysis, and case generation. (5) Voting mechanism, where experts vote on the generated report, revising it as necessary if disagreements arise. This process continues iteratively until consensus is reached; and (6) Decision making, where the final report is used as the basis for selecting the correct answer. Apart from above components we described, we finally provide an overview of the algorithm used to facilitate expert voting and decision-making, and model selection explains why we chose to use the LLama 3.1:70B model."}, {"title": "A. Multi-Agent Generation", "content": "In the context of clinical medical problems, given a problem q and a set of options op = {01,02,..., Ok}, where k denotes the total number of available options, the goal of this process is to assemble a team of experts. These include question experts, specialized in clinical problem analysis, questionExperts = {qe1, qe2,..., qem}, as well as option experts specialized in analyzing the options, optionExperts = {oe1, oe2, ..., oen}, where m and n represent the respective numbers of question and option domain experts. Specifically, we assign a role to the model and provide instructions to guide it in generating the corresponding domain experts based on the input problems and options:\nquestionExperts = GenerateExpert(q, promptqe) (1)\noptionExperts = GenerateExpert(q, op, promptoe) (2)\nTwo prompts in equations are represented for generating the question experts and option experts respectively. They guide the model's behavior during the expert generation process, ensuring LLM performs the appropriate categorization tasks based on the given problem and options. To be specific, promptqe uses the format: Description <promptge: You need to classify the following question into one subfield of medicine based on the given medical scenario: \"'{question}'\". Consider relevant diagnoses and related fields. Provide the classification in the format \"'{question_domain_format}'\", keeping your response concise and under {max_words} words.\" >, promptoe uses the format: Description <promptoe: Classify the following options: \"'{options}'\", based on the medical scenario: \"'{question}'\". Output them in the format \"'{options_domain_format}'\".\" >\nQuestion Domain Experts: These experts specialize in clinical knowledge related to specific medical issues. They analyze symptoms, diagnoses, and treatment options, providing critical insights for decision-making. This group includes specialists from fields such as infectious diseases, gynecology, and hematology, and is crucial in identifying features requiring immediate attention, ensuring patient safety and care.\nOption Domain Experts: These experts analyze the clinical options available for a specific medical issue. Their primary role is to assess the relevance and correctness of each option, considering the nuances between them. By leveraging their extensive clinical experience, they help identify misleading options and provide critical insights that guide the team in selecting the most appropriate treatment pathways."}, {"title": "B. Proposition Analysis", "content": "Question Analyses: After consulting with the experts from relevant fields regarding the problem, we asked them to provide their individual analyses, which are then used to inform further reasoning. For each question q and corresponding question expert qei \u2208 questionExperts, we employ a large language model (LLM)to act as a domain-specific expert. Guided by the prompt promptqa, the LLM generates an analysis, represented by the following equation:\nqA\u2081 = AnalyzeQuestion(q, qei, promptqa) (3)\nThe promptqa directs the LLM to: (1) Identify the key components of the question, such as symptoms, potential"}, {"title": "C. Case Generation", "content": "A key component of our LLM-MQA system is the case generation. We introduce it to serve as supportive evidence that aids in the final diagnosis and enhances the interpretability of the overall system. The generated cases are not standalone outputs but work synergistically with the analyses from the problem and option experts. They offer context-rich, interpretable explanations that justify the recommended diagnosis or treatment and are seamlessly integrated into the final report. In addition, the entire system provides not only the correct answer but also a clear and well-supported reasoning process, improving both accuracy and interpretability.\nDuring the case generation phase, the LLM to autonomously creates clinical cases that align with a plausible and correct option based on the dataset. The LLM begins by identifying key clinical features, such as symptoms, examination findings, laboratory results, and other diagnostic factors. Using these elements, it generates one or two concise, realistic clinical cases. These cases are intended for use in the report generation phase, where they provide additional context to support final decision-making. Each generated case consists of the following components:\nContext: Provides a detailed clinical scenario, highlighting key symptoms, medical history, and diagnostic findings.\nKey Mechanism/Reasoning: Justifies the selected option by explaining how the clinical findings support the correct diagnosis or treatment, emphasizing the alignment between the case and the chosen outcome.\nNeutrality Check: Maintains objectivity by avoiding exaggerated claims about the selected option, while briefly acknowledging relevant alternatives when appropriate."}, {"title": "D. Report Digest", "content": "In the Report Generation phase, the LLM plays a crucial role as a \"synthesizer\", integrates insights derived from the different analysis modules-Question Analysis (QA), Option Analysis (OA), and Case Generation (CG). This phase is designed to create a coherent, well-supported report that is not only accurate but also interpretable. The generated clinical cases, produced by the Case Generation module, are particularly important because they provide contextually rich, clinical justifications for the selected options. These cases add depth to the final report, enhancing its transparency and interpretability.\nIn this process, the LLM first extracts the key information from each analysis module and identifies areas of agreement and disagreement among the experts. It then synthesizes these insights and generates a comprehensive report offering a nuanced and complete view of the problem. The LLM carefully balances the clinical data from the analyses with the generated cases to form a cohesive and informative report. The cases contribute significantly to this synthesis by grounding the theoretical analyses in real-world clinical contexts. In generating the report, LLM follows a structured promptrp which requires extracting key information from the problem analysis, option analysis, and case study analysis, and generating two core sections of the report based on this information:\nKey Knowledge: In this section, the most important diagnostic clues, clinical context, and reasoning are extracted from all three modules: Question Analysis, Option Analysis, and Case Generation. The Case Generation module plays a pivotal role here, as it provides detailed clinical scenarios that are aligned with the correct options, offering concrete examples that illustrate the reasoning behind the conclusions. This section ensures that all analyses are accurately represented and highlights the most relevant information to support the decision-making process.\nTotal Analysis: This section synthesizes the entire clinical scenario by incorporating clinical features from the Case Generation module. It evaluates each option by considering both supporting and refuting evidence and ranks them based on their clinical relevance. The generated cases ensure that the evaluation is grounded in realistic clinical situations, enabling a direct comparison of options within the context of the problem. The LLM then provides a ranked recommendation with clear justification, grounded in both the analyses and"}, {"title": "E. Voting Mechanism", "content": "After generating the report, we implement a voting decision-making mechanism with \"Yes\" and \"No\" as the voting options. If the experts find the report unreasonable, they will cast a negative vote (\"No\") and provide revision suggestions to address"}, {"title": "IV. EXPERIMENTS", "content": "We conducted experiments on the publicly available MedQA dataset, which is specifically designed for questions and answers in the medical field. The dataset consists of multiple-choice medical questions, as detailed in Fig. 2. Each instance includes a clinical query, a set of five answer options, and a correct answer for validation purposes. The MedQA dataset presents unique challenges due to the specialized nature of medical knowledge and the complexity of reasoning required to derive the correct answers. It provides a useful testbed for evaluating medical question answering systems, particularly in the context of leveraging large language models.\nConsidering the ultimate goal of model is to identify the best option from the multiple choices. To comprehensively evaluate the performance of our proposed system and compare it against other baselines, we adopt four widely used evaluation metrics in multi-class classification task:\n\u2022 Accuracy: This measures the overall proportion of correct predictions. For multi-class classification, it is computed by summing the correct predictions (true positives) across all classes and dividing by the total number of samples.\n\u2022 Macro Precision is the average of precision scores across all classes, without considering the class distribution. The formula is \u2211Precisioni, where C is the number of classes, and Precision; is the precision for the i-th class.\n\u2022 Macro Recall is the average of recall scores across all classes, without considering the class distribution. The formula is \u2211 Recalli, where C is the number of classes, and Recalli is the recall for the i \u2013 th class.\n\u2022 Macro F1-Score is the average of F1-scores across all classes, without considering the class distribution. The formula is \u2211 F1i, where C is the number of classes and F1 is the F1 for the i th class calculated by the equation:\n2 * Precisioni * Recalli / Precisioni + Recalli (9)\nB. Experiments Settings"}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce a multi-agent framework for medical question answering (MedQA), leveraging specialized domain experts, a case generation module, and a voting mechanism to enhance decision-making. Our approach integrates domain experts with a case generation module that uses clinical data to support the selection of the most plausible answers. The framework employs a joint optimization mechanism, where feedback from domain experts and the case generation module is utilized to refine problem and option analysis tasks, feeding insights back into the large language model. Additionally, a voting mechanism aggregates expert feedback and revisions, improving the quality and reliability of the generated reports.\nComprehensive experiments on the MedQA dataset demonstrate that our approach outperforms existing methods, such as direct inference and Chain of Thought (CoT), across key metrics, including accuracy, precision, recall, and F1-score. Our method achieves nearly 77% across all metrics, compared to approximately 70% for other approaches. Furthermore, we validate that employing a large-scale LLM significantly enhances performance, with the case generation step identified as a critical component driving these improvements. This framework enhances the system's explainability of reasoning and ensures robust decision-making through expert consensus, providing a reliable and effective solution for medical question-answering tasks.\nIn future work, we aim to further investigate the case generation module to support a wider range of clinical scenarios, incorporating diverse patient profiles and diagnostic complexities. Additionally, we plan to explore the scalability of our multi-agent framework in real-time medical environments, focusing on optimizing model efficiency and response times for clinical practitioners. Our approach offers a solid foundation for building advanced, explainable medical question-answering framework, which is general and can be applied to other complex decision-making tasks."}]}