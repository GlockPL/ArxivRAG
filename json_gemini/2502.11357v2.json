{"title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents", "authors": ["Vardaan Pahuja", "Yadong Lu", "Corby Rosset", "Boyu Gou", "Arindam Mitra", "Spencer Whitehead", "Yu Su", "Ahmed Awadallah"], "abstract": "Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of human-level capabilities in more realistic online settings. A key bottleneck is the lack of diverse and large-scale trajectory-level datasets across various domains, which are expensive to collect. In this paper, we address this challenge by developing a scalable recipe to synthesize the largest and most diverse trajectory-level dataset to date, containing over 94K successful multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and 33M web elements. In particular, we leverage extensive web exploration and refinement to obtain diverse task intents. The average cost is 28 cents per successful trajectory, making it affordable to a wide range of users in the community. Leveraging this dataset, we train Explorer, a multimodal web agent, and demonstrate strong performance on both offline and online web agent benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++. Additionally, our experiments highlight data scaling as a key driver for improving web agent capabilities. We hope this study makes state-of-the-art LMM-based agent research at a larger scale more accessible.", "sections": [{"title": "1 Introduction", "content": "Graphical User Interfaces (GUIs) serve as the primary medium for user interaction across digital environments. Within the GUI environment, LLM-based agents (Su et al., 2024) have shown great potential in automating complex workflows for human users. These agents are designed to operate across diverse interfaces, including the web (Deng et al., 2023; Zhou et al., 2024; Zheng et al., 2024), desktop (Xie et al., 2024; Wu et al., 2024; Bonatti et al., 2024), and mobile platforms (Rawles et al., 2023; Yan et al., 2023). Navigating modern GUI interfaces, which integrate textual, graphical, and interactive components, typically requires agents to possess visual grounding, long-term planning, and memory management capabilities.\nRecent work (Cheng et al., 2024; Gou et al., 2025) has demonstrated the effectiveness of synthetic data for enhancing visual grounding (Gou et al., 2025; Chen et al., 2024a; Kapoor et al., 2024; Chen et al., 2024b) and planning (Xu et al., 2024b; Zhang et al., 2024). Developing end-to-end GUI agents with long-term planning and grounding capabilities requires training on multi-step trajectory data (Xu et al., 2025, 2024b; Qin et al., 2025). However, existing trajectory datasets are primarily human-annotated (Deng et al., 2023; Li et al., 2024; Lu et al., 2024) or leverage synthetic data just for task proposal curation (Lai et al., 2024; Chen et al., 2024a). And human annotation is expensive to scale for collecting large and diverse training datasets. Therefore, synthetic data has emerged as a promising alternative to human-annotated data (Hartvigsen et al., 2022; Sahu et al., 2022; Ye et al., 2022; Tang et al., 2023; Mukherjee et al., 2023; Mitra et al., 2024). Collecting trajectory-level datasets presents unique challenges: 1) curating a diverse set of task intents at scale, 2) deploying an agent capable of interacting with a real-world environment to complete these tasks through a series of actions, and 3) verifying whether the task is accomplished by the executed action sequence.\nData diversity is essential for equipping generalist web agents with a broad range of skills. Existing work on synthetic web trajectory generation employs self-instruct for task proposal generation (He et al., 2024b). It formulates task proposals from homepages or parametric LLM knowledge, overlooking the richer content available in deeper"}, {"title": "2 Related Work", "content": "2.1 LLM-based Web Agents\nRecent advances in multimodal language models have facilitated the development of web agents autonomous systems designed to interact with real-world websites to perform everyday tasks (Deng et al., 2023; Hong et al., 2024; Cheng et al., 2024; Zheng et al., 2024). Key challenges for web agents include long-term planning, visual grounding, and memory management. To improve long-context understanding, WebAgent (Gur et al., 2024) utilizes multiple LLMs one for planning, summarization, and grounded program synthesis. SeeAct (Zheng et al., 2024) adopts a two-step procedure of planning followed by grounding at each step using GPT-4 to accomplish web agent tasks. Another line of work employs a vision-only approach to train a GUI grounding model that directly predicts pixel coordinates for executing GUI agent tasks (Cheng et al., 2024; Kapoor et al., 2024; Gou et al., 2025). However, a significant bottleneck remains the lack of large-scale, high-quality web trajectory data for training robust agents. Our work presents a new framework for synthesizing large-scale web trajectory data to train end-to-end web agents.\n2.2 Web Agent Benchmarks and Datasets\nEarly benchmarks for web tasks such as Mini-Wob++ (Liu et al., 2018) focused on testing low-level actions on simulated websites. However, these simulated websites fail to capture the complexity of the real-world web. Mind2Web (Deng et al., 2023) introduces a trajectory-level dataset with 2K tasks across 137 real-world websites and 31 domains. However, it employs a static evaluation method that penalizes alternative valid execution paths. To overcome this limitation, follow-up work has explored alternative evaluation approaches, including functional correctness-based evaluation in WebArena (Zhou et al., 2024) and key-node-based evaluation in Mind2Web-Live (Pan et al., 2024b). Towards the goal of making web agents more capable of performing realistic tasks, GAIA (Mialon et al., 2024) and AssistantBench (Yoran et al., 2024) introduce benchmarks that include time-consuming information-seeking tasks. In this work, we develop Explorer, a multimodal web agent trained on our synthetic dataset, and showcase its strong performance across online and offline benchmarks, including Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++.\n2.3 Data Synthesis for Web Agents\nEarly efforts to acquire trajectory data for training web agents primarily relied on crowd-sourcing (Deng et al., 2023; Lu et al., 2024). However, human annotation is cost-prohibitive, prompting the adoption of synthetic data generation approaches to facilitate large-scale data collection. AutoWebGLM (Lai et al., 2024) and GUIAct (Chen et al., 2024a) utilize LLMs to generate task proposals, which human experts subsequently annotate. OpenWebVoyager (He et al., 2024b) employs a web agent to execute auto-generated task descriptions. However, since these task descriptions are generated using LLMs without exploring a website, they fail to capture the full diversity of possible tasks on that website. Another line of work, including Synatra (Ou et al., 2024) and AgentTrek (Xu et al., 2025), leverages web tutorials to guide web trajectory generation. Meanwhile, concurrent"}, {"title": "3 Data Recipe", "content": "We design an automatic web trajectory synthesis pipeline that explores websites to generate diverse web trajectories. It utilizes Playwright* to execute actions and collect metadata from real-world websites, starting from an initial URL.\u2020 The metadata includes screenshots, HTML, Ally tree, and actions in grounded and natural language forms. The action space is given in Table 3.\n3.1 Website Selection\nWe use a combination of URL sources to generate the synthetic web trajectories. We obtain the top 100 URLs from similarweb.com corresponding to the high-traffic portion of the web with transactional tasks like booking flights, restaurant reservations, government services, sports, entertainment, etc. The Tranco (Pochat et al., 2019) URLs include 49K URLs representing the head portion of the web, which is less trafficked but popular nonetheless. We filter out harmful websites containing violent or explicit content to ensure safety compliance."}, {"title": "3.2 Data Generation Pipeline", "content": "We aim to develop a generalized pipeline for web exploration to collect diverse web trajectory data. To enhance diversity, we adopt a bottom-up approach, starting with low-level actions and progressively shaping them into high-level task descriptions while maintaining a coherent task intent. In the first step, the proposer agent generates an abstract task, which is refined to a more specific task through a refinement process. For instance, starting from the Amazon homepage, the initial task proposal might be \u201cFind today's deals on Amazon\", which is progressively refined into \u201cProceed to checkout for the Amazon eero Pro 6E mesh Wi-Fi router with 36% off\" (Figure 1). Since the agents execute actions alongside the refinement process, the generated tasks respect real-world constraints, such as product availability, available color options, and other specifications, ensuring practical applicability. Our pipeline consists of the following LLM-powered agents+:\nTask Proposer. Given a website homepage, including its screenshot and accessibility tree, the task proposer agent generates diverse initial tasks that could be performed on that website. The task descriptions at this stage are instructed to be high-level and abstract versions of the real-world tasks, which will be refined into more specific tasks in later stages. Along with generating the task proposal, the agent proposes and executes the first action toward completing that task. Furthermore, the agent is instructed to halt upon encountering robot detection mechanisms, CAPTCHA verification, login prompts, or payment requests.\nTask Refiner. The task refiner agent receives the initial task proposal or the refined task description from the previous step, along with the corresponding action history as inputs. It then predicts the next action consistent with the input task description and the updated refined task description while incorporating the complete action history. By iteratively refining the task description after each action, the agent ensures that the updated task remains aligned with the action history.\nTask Summarizer. This module processes the entire action and screenshot history to predict an overall task description that aligns with the trajectory. The task summary is expected to be high level, i.e., it should describe what the task entails while omitting how it is accomplished.\nTask Verifier. Inspired by Pan et al. (2024a), the task verifier agent receives the task description and action history, serving as a critic to evaluate whether the trajectory successfully completes the specified task. In addition to the screenshots of the trajectory, it also receives a markdown representation of the last page. This ensures the verifier has the full context of the website's final state, even when the viewport cannot capture all the content. Such automatic evaluation of web trajectories has been widely adopted in prior work (Xu et al., 2025; He et al., 2024a; Koh et al., 2024). Figure 1 illustrates the above pipeline. The prompts for the above agents are given in Appendix D."}, {"title": "3.3 Dataset Analysis", "content": "Explorer comprises web trajectories spanning diverse domains, including services, entertainment, shopping, travel, and information, ensuring broad task diversity. Sample tasks from Explorer are presented in Table 2. To the best of our knowledge, Explorer with 94K trajectories is the largest web trajectory dataset of this scale. Beyond diversity, Explorer is also highly scalable and cost-efficient. Our approach achieves a cost of $0.28 per successful trajectory, making it"}, {"title": "4 Experiments", "content": "We use the synthetic trajectories generated by our pipeline to train small multimodal language models (SLMs) for web agent tasks. To ensure computational efficiency, we select 40K trajectories from the full set for training. We further refine this subset by filtering out trajectories that contain more than two scroll actions to mitigate potential model bias toward excessive scrolling behavior. Finally, we use ~30K trajectories obtained after filtering to fine-tune multimodal language models like Phi-3.5V (Abdin et al., 2024) and Qwen2-VL-7B (Wang et al., 2024a). For brevity, we denote the models trained on Phi-3.5V and Qwen2-VL-7B as Explorer-4B and Explorer-7B, respectively. To test the effectiveness of our data for web-based agentic tasks, we evaluate Explorer-4B and Explorer-7B on Mind2Web-Live (Pan et al., 2024b), Multimodal-Mind2Web (Deng et al., 2023; Zheng et al., 2024), and MiniWob++ (Liu et al., 2018).\nMultimodal-Mind2Web. Multimodal-Mind2Web is an offline web agent benchmark comprising 2K open-ended tasks spanning 137 websites across 31 domains. Each task comprises a sequence of actions with screenshots, action type, and HTML. We follow the setting in Zheng et al. (2024) and report element accuracy, operation F1, and step success rate (SR) as evaluation metrics.\nMind2Web-Live. Mind2Web-Live is a benchmark modified from the original Mind2Web dataset to test web agents on live websites rather than static trajectories. The benchmark evaluates performance using a key-node-based evaluation approach rather than using a golden action sequence, requiring valid trajectories to reach annotated \"key nodes\" across 104 test tasks in Mind2Web. Since Mind2Web-Live relies on real-world dynamic websites, it encounters robot detection such as reCAPTCHA, which hinders testing (Xu et al., 2024b). To address this, we select a subset of 83 test set tasks that remain consistently accessible throughout our tests. Following Pan et al. (2024b), we report the average step success rate, completion rate, and full task success rate (SR) on the test set. The average step success rate and completion rate represent the proportion of completed key nodes, using macro and micro averages, respectively.\nMiniWob++. This benchmark consists of low-level tasks on a single webpage. Typical examples include clicking a sequence of buttons, selecting items from a drop-down list, and filling out a form. We use the subset of 46 tasks used for evaluation"}, {"title": "5 Results", "content": "5.1 In-domain Evaluation\nAs an intrinsic evaluation of the trajectory collection pipeline, we generate 100 test tasks using Explorer, disjoint from the train set. The SLM agents are tasked with executing the given tasks on live websites while an LLM-as-a-judge verifier (\u00a7 3.2) evaluates the correctness of their actions at the trajectory level. We observe that the fine-tuned agents significantly outperform their pre-trained counterparts. Thus, using in-domain web trajectory data training helps, which is a valuable sanity check. It also surpasses GPT-40, further underscoring our synthetic data's quality.\n5.2 Mind2Web-Live Results\nWe evaluate Explorer-4B and Explorer-7B trained on the synthetic trajectory dataset. We make the following observations from the results:\nImprovement over base pre-trained models. We observe that Explorer-7B yields improvements of 5.1% and 4.8% in average step success rate and key node completion rate, respectively, compared to the pre-trained Qwen2-VL-7B model. Similarly, Explorer-4B obtains gains of 15.5% and 15.9% in average step success rate and key node completion rate, respectively, over its pre-trained counterpart. In terms of full task success rate, Phi-3.5V improves significantly from 2.4% to 18.1%, while Qwen2-VL-7B improves from 14.5% to 19.3%. To the best of our knowledge, this represents the state-of-the-art performance on Mind2Web-Live for end-to-end web agents of this size that are trained exclusively on synthetic data.\nImprovement over higher capacity pre-trained models. Despite having much fewer parameters, we observe that Explorer-4B outperforms strong baselines such as Mistral-7B-Instruct-0.3 and Qwen2-72B-Instruct in full task SR by margins of 8.5% and 2.7%, respectively. The Phi-3.5V model obtains an 18.1% full task success rate, which is better than GPT-3.5 (15.4%), despite using orders of magnitude fewer parameters. The corresponding results for the entire set of 104 tasks, including unreachable websites, are given in Appendix A.\n5.3 Multimodal-Mind2Web Results\nFollowing Deng et al. (2023), we obtain the top-50 elements from a pre-trained DeBERTa (He et al., 2021) candidate generation model which are then used to construct the accessibility tree and SoM image inputs. The results are shown in Table 9.\nAmong baselines, we include API-based models for in-context learning \u2013 GPT-3.5, GPT-4, and SeeAct (Zheng et al., 2024). SeeAct is a web agent that performs web tasks using a two-step procedure of action generation and grounding using GPT-4V. Additionally, we include baselines that fine-tune small language models using synthetic data, followed by further fine-tuning on the Mind2Web training set. SeeClick (Cheng et al., 2024) introduces a visual grounding model (Qwen-VL) trained on synthetically-generated grounding data. EDGE (Chen et al., 2024b) synthesizes QA data on webpages to improve the grounded GUI understanding capabilities of MLLMs. ScribeAgent-Large (Shen et al., 2024) and MiniCPM-GUI (Chen et al., 2024a) use human-annotated trajectory data to train web agents. AgentTrek (Xu et al., 2025) is a GUI agent baseline that also utilizes synthetic trajectory data to fine-tune SLMs for Mind2Web, similar to our setting. We observe that Explorer-7B fine-tuned on synthetic data from Explorer plus Mind2Web outperforms all baselines in average step success rate. Notably, it surpasses Agent-Trek, which uses the same Qwen2-VL-7B MLLM backbone, highlighting the superior quality of our dataset. The broad domain coverage and task diversity in Explorer contribute to its superior generalization across environments.\n5.4 MiniWob++ Results\nTable 8 shows the results on the MiniWob++ benchmark in the zero-shot evaluation setting. Among baselines, we have API-based models, in-context learning using open-source LMs, and agentic models like AgentLM (Zeng et al., 2024), CodeActAgent (Wang et al., 2024b), Lemur-Chat (Xu et al., 2024a) and AgentFlan (Chen et al., 2024c) which include web-based demonstrations in their instruction tuning dataset. Synatra-CodeLlama-7B (Ou et al., 2024) and AgentTrek (Xu et al., 2025) also synthesize web-agent trajectories automatically. We observe that Explorer outperforms GPT-4 and"}, {"title": "5.5 Data Scaling Experiments", "content": "We conduct experiments with different data scales for Explorer-4B to analyze the impact of training data size. Specifically, we subsample the original trajectory dataset to utilize 50% and 25% of its original size. Our results show that, even with just 25% of the training data, the model exhibits rapid performance gains over the base pre-trained model. Increasing the dataset size further leads to gradual improvements across all reported metrics. However, the increase in the overall task success rate is more gradual compared to the stepwise metrics, as it is a more coarse-grained metric."}, {"title": "6 Analyses", "content": "6.1 Ablation Studies\nWe conduct ablation studies to assess the impact of various design choices on overall performance. To evaluate the importance of visual modality, we experiment with using just the textual modality for the Phi-3.5V model, replacing it with the text-only Phi-3-mini (Abdin et al., 2024). In addition to Qwen2-VL-7B and Phi-3.5V, we also evaluate LLaVA-Mistral-7B (Liu et al., 2023), a strong MLLM baseline. Our results show that omitting the visual modality leads to a sharp 4.8% drop in performance for Phi-3.5V, underscoring its importance for effective GUI grounding. Furthermore, LLaVA-Mistral-7B significantly underperforms compared to both Qwen2-VL-7B and Phi-3.5V, highlighting the necessity of a stronger MLLM backbone for improved GUI agent performance.\n6.2 Failure Modes of Trajectory Generation\nWe analyze cases where a generated trajectory is ultimately rejected by the task verifier agent. Our goal is to synthesize trajectory data that closely resembles human-annotated datasets for training web agents. However, since our pipeline collects trajectories through an exploration-driven approach, some trajectories result from random, incoherent action sequences that fail to align with a well-defined task intent. For instance, in shopping tasks, the agent may explore various products without demonstrating an intent to purchase (e.g., by adding items to the cart). Another failure case arises when the agent encounters errors on the final page due to automated browser detection, CAPTCHA verification, or an unresponsive website. We note that the verifier agent is instructed to judge a trajectory as successful if the task is completed, except for the final login and payment steps. The trajectories in failure modes are still valuable for web agents to learn low-level tasks such as form filling, basic interaction with web elements, and visual grounding.\n6.3 Case Studies for Mind2Web-Live\nWe randomly sample 20 error cases for Explorer on Mind2Web-Live to gain insights for future improvement. These errors fall into the following categories:\n\u2022 Task deviation: The agent executes actions unrelated to the given task, thus failing to complete it.\n\u2022 Missing key steps: The agent retrieves results that partially satisfy the required constraints, e.g., the agent finds women's clothes of the correct size but incorrect type or color.\n\u2022 Grounding error: The agent fails to interact with a valid element on the page.\n\u2022 Website unresponsive: The agent executes the correct action, but the website does not respond."}, {"title": "7 Conclusion", "content": "In this work, we introduce Explorer, a scalable framework for synthesizing web trajectories on a large scale. By leveraging thorough web exploration, Explorer ensures diversity in both domains and the skills acquired by web agents. Unlike previous approaches, our framework generates contextually grounded trajectories that adapt to real-world constraints, improving both task relevance and generalization. We instantiate this framework using URLs collected from diverse sources. Explorer outperforms existing web agent baselines by a significant margin on both online and offline web agent benchmarks. Furthermore, our results highlight the critical role of data scale in enhancing web agents' performance. Future work will focus on extending this framework to encompass a broader range of GUI environments, such as operating systems with diverse applications. GUI agents require specialized skills for different tasks, including information-seeking, operational, and navigation skills. Efficient exploration of the environment to acquire these skills presents another promising avenue for future research."}, {"title": "Limitations", "content": "Explorer explores the web environment autonomously, which may occasionally result in incoherent tasks. Synthetic data collection using closed-source LLMs can be costly due to associated API expenses. While this work serves as a proof of concept, future research will focus on developing tailor-made open-source LLMs for this task. Additionally, some website content remains inaccessible due to login requirements, leading to insufficient data for those websites."}, {"title": "Ethical Considerations", "content": "The synthetic data collection pipeline proposed in this paper is intended solely for academic research on GUI agents, with strict ethical safeguards to prevent unauthorized website interactions. To ensure ethical compliance and mitigate risks, we prompt our agents to automatically terminate upon encountering CAPTCHA verifications, login prompts, or payment requests, ensuring that no actual transactions or bookings occur. Additionally, we filter out websites containing violent or explicit content and strictly adhere to privacy regulations, ensuring that no personal information is used during action execution. To enforce responsible data collection,"}]}