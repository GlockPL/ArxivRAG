{"title": "TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation", "authors": ["Rong Zhou", "Weixiang Sun", "Yanfang Ye", "Zhengqing Yuan", "Xiang Li", "Kai Zhang", "Zhiling Yan", "Yiwei Li", "Lifang He", "Lichao Sun"], "abstract": "Biomedical image segmentation is crucial for accurately diagnosing and ana-lyzing various diseases. However, Convolutional Neural Networks (CNNs) and Transformers, the most commonly used architectures for this task, struggle to effectively capture long-range dependencies due to the inherent locality of CNNs and the computational complexity of Transformers. To address this limitation, we introduce TTT-Unet, a novel framework that integrates Test-Time Training (TTT) layers into the traditional U-Net architecture for biomedical image seg-mentation. TTT-Unet dynamically adjusts model parameters during the testing time, enhancing the model's ability to capture both local and long-range features. We evaluate TTT-Unet on multiple medical imaging datasets, including 3D ab-dominal organ segmentation in CT and MR images, instrument segmentation in endoscopy images, and cell segmentation in microscopy images. The results demonstrate that TTT-Unet consistently outperforms state-of-the-art CNN-based and Transformer-based segmentation models across all tasks. The code is available at https://github.com/rongzhou7/TTT-Unet.", "sections": [{"title": "Introduction", "content": "Image segmentation plays a crucial role in medical imaging, as it empowers medical professionals to identify biological structures and measure their morphology, aiding in the analysis and diagnosis of various diseases [1]. In recent years, convolutional neural networks (CNNs) [2] have emerged as a promising approach in the field of biomedical image segmentation. Among various CNN-based techniques, U-Net [3] stands out for its straightforward structure and significant adaptability. Many enhancements and iterations [4-11] have been developed based on this U-shaped architecture, typically featuring a symmetric encoder-decoder design to capture multi-scale image features through convolutional operations. Leveraging this foundation, significant advancements have been achieved across a wide range of medical imaging applications. These include cardiac segmentation in magnetic resonance (MR) imaging [12], multi-organ delineation in computed tomography (CT) scans [13], and others [14, 15].\nDespite the remarkable representational capabilities of CNN-based models, their architectural design exhibits an inherent limitation in modeling long-range dependencies within images, because convolu-tional kernels are inherently local [16]. While skip connections in the U-Net architecture facilitate the merging of low-level details with high-level features, they mainly serve to directly merge local features, which does not substantially boost the network's ability to model long-range dependencies.\nThis limitation becomes especially evident in scenarios with large inter-patient variations in shape, size, etc [16]. Such variability poses challenges to the CNN framework's ability to consistently and accurately capture information across extended spatial contexts, highlighting the need for innovative approaches to address this fundamental constraint.\nRecognizing the limitations of CNNs in capturing long-range dependencies, the research community has shifted interest towards Transformer models for their ability to naturally understand global contexts [17]. This transition is evidenced in biomedical image segmentation, where approaches like TransUNet [18], UNETR [8], SwinUNETR [9] demonstrate the potential of integrating Transformers. These hybrid models that blend CNNs for high-resolution spatial detail and Transformers for global context emerge as a more effective strategy.\nDespite their ability to capture global dependencies, Transformers are computationally intensive [19], especially in dense biomedical image segmentation tasks. Mamba [19], a state-space model designed for efficient sequence modeling, offers a more computationally efficient approach to long-range dependency modeling. Building on this, U-Mamba [20] integrates Mamba within U-Net, effectively combining high-resolution spatial detail with long-range dependency modeling to enhance biomedical image segmentation. Despite these advancements, U-Mamba and similar models, still face challenges in expressiveness, particularly over extended contexts, where their fixed-size hidden states limit their ability to capture complex and nuanced dependencies.\nRecently, TTT (Test-Time Training) [21] have emerged as a new class of sequence modeling layers with linear complexity and an expressive hidden state. TTT treats the traditional fixed hidden state as a machine learning model itself, which can be dynamically updated through self-supervised learning. This dynamic adjustment allows the model to refine its parameters based on test data, providing greater flexibility and expressiveness in capturing intricate long-range dependencies. In comparison to Transformers and Mamba, TTT layers not only maintain efficiency but also offer superior performance in handling long-context sequences.\nIn this paper, we introduce TTT-Unet, a novel hybrid architecture that incorporates TTT layers within the traditional U-Net framework to address the inherent limitations in modeling long-range dependencies in biomedical image segmentation tasks. The TTT layers dynamically adapt its parameters during test time, allowing it to more effectively capture both localized details and long-range dependencies. Our extensive experiments across various medical imaging datasets demonstrate that TTT-Unet consistently outperforms existing state-of-the-art models. The results highlight the model's effectiveness in handling complex anatomical structures and its robustness in diverse clinical scenarios. Particularly, TTT-Unet has shown significant improvements in biomedical image segmentation tasks, making it a versatile solution for medical image analysis. Our contributions are summarized as follows:\n\u2022 We introduce TTT-Unet, an enhanced U-Net architecture integrated with TTT layers, which allows the model to perform self-supervised adaptation during test time. This hybrid design effectively tackles the challenge of modeling long-range dependencies and improves the model's generalization capability across diverse data distributions.\n\u2022 TTT-Unet has been rigorously evaluated on a diverse set of medical imaging datasets, including 3D abdominal organ segmentation in CT and MRI scans, instrument segmentation in endoscopy images, and cell segmentation in microscopy images. The results demonstrate consistent improvements over state-of-the-art models in both 3D and 2D segmentation.\nIn summary, TTT-Unet represents a significant advancement in biomedical image segmentation, offering a robust and adaptable approach that leverages the strengths of CNNs and TTT layers. This work lays the foundation for future developments in adaptive and context-aware medical image analysis technologies."}, {"title": "Related Work", "content": ""}, {"title": "U-Net and variants", "content": "CNN-based and Transformer-based models have significantly advanced the field of biomedical image segmentation. U-Net [3], a representative among CNN-based approaches, features a symmetrical encoder-decoder architecture enhanced with skip connections to better preserve details. Various"}, {"title": "Hybrid models", "content": "SSMs, such as Mamba, have recently gained prominence as a powerful component for developing deep networks, achieving cutting-edge performance in analyzing long-sequence data [26, 27]. In the realm of biomedical image segmentation, U-Mamba [20] presents a novel SSM-CNN hybrid approach, signifying the first application of SSMs in the medical image domain. Further developments include SegMamba [28] and nnMamba [29], which combine SSMs in the encoder with CNNs in the decoder, illustrating the versatility and effectiveness of SSMs in enhancing medical imaging analysis."}, {"title": "Method", "content": "TTT-Unet follows the conventional U-Net structure, designed to effectively capture both local features and long-range dependencies. As shown in Figure 1, TTT-Unet integrates Test-Time Training (TTT) layers into the Mamba blocks within the U-Net network. This integration enables the model to continuously update its parameters based on test data, enhancing its feature extraction capabilities in the encoder and allowing it to adaptively learn long-range dependencies. Subsequently, we introduce the TTT layer and then describe how it is integrated into the Mamba blocks within the U-Net architecture."}, {"title": "TTT layers", "content": "Traditional sequence modeling layers, such as RNNs, compress the context of a sequence $x_1,..., x_t$ into a fixed-size hidden state $h_t$. For RNNs, the hidden state $h_t$ at time step t is updated based on the current input $x_t$ and the previous hidden state $h_{t-1}$ through linear transformation matrices $\\theta_h$ and $\\theta_x$ and a non-linear activation function $\\sigma$:\n$h_t = \\sigma(\\theta_h h_{t-1} + \\theta_x x_t)$,\nwhere $\\theta_h$ and $\\theta_x$ are learned parameters. The output $z_t$ is then generated from the hidden state:\n$z_t = \\phi(h_t)$,\nwhere $\\phi$ represents a linear or non-linear transformation.\nHowever, the fixed size of the hidden state limits performance when dealing with long contexts due to its finite capacity to represent contextual information. To address this limitation, a new class of sequence modeling layers, referred to as TTT layers [21] is introduced, where the hidden state is treated as a trainable model and is updated through self-supervised learning.\nSpecifically, in a TTT layer (Fig. 1c), the hidden state $h_t$ at time step t is treated as a trainable model f with weights $W_t$, which is updated based on the current input $x_t$:\n$W_t = W_{t-1} - \\eta \\nabla l(W_{t-1}; x_t)$\nThe output token $z_t$ is then generated using trainable model f with weights $W_t$:\n$z_t = f(x_t; W_t)$\nIn the basic naive version, the self-supervised loss l aims to reconstruct the corrupted input $x\\hat{}$. This approach is straightforward and focuses on learning to recover the original input from its corrupted version:\n$l(W; x_t) = ||f(x_t; W) - x\\hat{t}||^2$\nWhile this naive reconstruction method is effective in certain scenarios, it has inherent limitations in capturing the complex dependencies within the input data, especially in tasks requiring a more nuanced understanding of the input context.\nTo address these limitations, we follow a more sophisticated self-supervised task that leverages multiple views of the input data. Instead of directly reconstructing the corrupted input, we introduce learnable matrices $\\theta_K$ and $\\theta_V$ to project the input into different views. The training view $K = \\theta_K x_t$ captures the essential information needed for learning, while the label view $V = \\theta_V x_t$ provides a target for reconstruction:\n$l(W; x_t) = ||f(\\theta_K x_t; W) - \\theta_V x_t||^2$\nThis approach allows the model to selectively focus on the most relevant features of the input, improving its ability to capture long-range dependencies and subtle relationships within the data.\nThe output token $z_t$ is then generated as follow:\n$z_t = f(\\theta_Q x_t; W_t)$,\nwhere f is a function parameterized by $W_t$, which can be a linear model or a multi-layer perceptron (MLP). Here, the projection d is used to obtain the test view $Q = \\theta_Q x_t$, which introduces additional flexibility by allowing the model to emphasize different aspects of the input data during inference. This approach enables the model to focus on the most informative features in the context of the current test case, thereby enhancing its ability to adapt to new, unseen data."}, {"title": "TTT-Unet architecture", "content": "As shown in Fig. 1, the TTT-Unet architecture integrates the traditional U-Net structure with TTT layers, allowing the network to adapt during testing through self-supervised learning dynamically. The architecture is composed of an encoder-decoder structure, where the encoder is enhanced with TTT building blocks to improve adaptability, while the decoder follows the standard U-Net design focused on reconstructing the segmentation map.\nEncoder. The encoder in TTT-Unet follows the traditional U-Net design, comprising multiple convolutional layers. These layers are interspersed with TTT building blocks, which are critical components that enable the model to adjust its parameters dynamically during test time. Each layer in the encoder progressively downscales the input image while capturing both local and long-range features essential for segmentation tasks. Including TTT building blocks within the encoder ensures the model can adapt to varying data distributions encountered during testing.\nTTT building blocks. As illustrated in Fig. 1b, the TTT building blocks are the core components that allow for the test-time adaptability of the model. Initially, the input features pass through two successive Residual blocks [30], each comprising a standard convolutional layer, followed by Instance"}, {"title": "Experiments", "content": ""}, {"title": "Datasets", "content": "To evaluate the performance and scalability of TTT-UNet, we utilize four biomedical image datasets across a variety of segmentation tasks and imaging modalities, including Abdomen CT dataset [35], Abdomen MRI dataset [36], Endoscopy dataset [37] and Microscopy dataset [38]. The basic informa-tion of these datasets is summarized in Table 1.\nAbdomen CT. The Abdomen CT [35] dataset, from the MICCAI 2022 FLARE challenge, includes the segmentation of 13 abdominal organs from 50 CT scans in both the training and testing sets. The organs segmented include the liver, spleen, pancreas, kidneys, stomach, gallbladder, esophagus, aorta, inferior vena cava, adrenal glands, and duodenum.\nAbdomen MRI. The Abdomen MR [36] dataset, from the MICCAI 2022 AMOS Challenge, focuses on the segmentation of the same 13 abdominal organs, using MRI scans. It consists of 60 MRI scans for training and 50 for testing. Additionally, we generate a 2D version of this dataset by converting the 3D abdominal MRI scans into 2D slices. This conversion enables us to evaluate TTT-Unet under the common 2D segmentation setting, which is widely used in practice due to its lower computational requirements. The conversion retains the same 13 organs, ensuring consistent evaluation across both 2D and 3D modalities.\nEndoscopy images. From the MICCAI 2017 EndoVis Challenge [37], this dataset focuses on instrument segmentation within endoscopy images, featuring seven distinct instruments, including the large needle driver, prograsp forceps, monopolar curved scissors, cadiere forceps, bipolar forceps,"}, {"title": "Experimental setup", "content": "The setting of our experiments is the same as that in U-Mamba [20] and nnU-Net [23] to ensure a fair comparison, as shown in Table 2 We adopt an unweighted combination of Dice loss and cross-entropy loss for all datasets and utilize the SGD optimizer with an initial learning rate of le-2. The training duration for each dataset is set to 1000 epochs, conducted on a single NVIDIA A100 GPU. Leveraging the self-configuring capabilities from nnU-Net, the number of network blocks adjusts automatically according to the dataset. For evaluation metrics, we employ the Dice Similarity Coefficient (DSC) and Normalized Surface Distance (NSD) to assess performance in abdominal multi-organ segmentation for MR scans, as well as instrument segmentation in Endoscopy images. For the cell segmentation task, we utilize the F1 score to evaluate method performance."}, {"title": "Baselines and metrics", "content": "In our evaluation of TTT-UNet, we compare against two prominent CNN-based segmentation methods: nnU-NET [23] and SegResNet [22]. Additionally, we include a comparison with UNETR [8] and SwinUNETR [9], a Transformer-based network that has gained popularity in biomedical image segmentation tasks. U-Mamba [20], a recent method based on the Mamba model, is also included in our comparison to provide a comprehensive overview of its performance. For each model, we implement their recommended optimizers to ensure consistency in training conditions. To maintain fairness across all comparisons, we apply the default image preprocessing in nnU-NET [23]."}, {"title": "Quantitative segmentation results", "content": "Table 3 presents the results of 2D segmentation tasks, comparing the performance of various models across three datasets: organ segmentation in Abdomen MRI, instrument segmentation in endoscopy images, and cell segmentation in microscopy images. For the organ segmentation task in Abdomen MRI, TTT-Unet models significantly outperformed other methods. The TTT-Unet_Bot variant achieved the highest DSC of 0.7750\u00b10.1022 and NSD of 0.8452\u00b10.1080, while TTT-Unet_Enc closely followed with a DSC of 0.7725\u00b10.1044 and an NSD of 0.8540\u00b10.1032. These results suggest that the TTT-Unet's ability to adapt its parameters at test time provides a considerable advantage in accurately segmenting organs, where anatomical variability is common. This adaptability likely contributes to the model's superior performance, enabling it to better generalize across different patient scans and handle the complexities of MRI data.\nIn the instrument segmentation task for endoscopy images, TTT-Unet variants again demon-strated superior performance, with TTT-Unet_Bot achieving a DSC of 0.6643\u00b10.3018 and NSD of 0.6799\u00b10.3056. The TTT-Unet_Enc model further improved these metrics, with a DSC of 0.6696\u00b10.3018 and NSD of 0.6820\u00b10.3080. These results indicate that the TTT layers within the model effectively capture the fine details of surgical instruments, which are often challenging to segment due to their small size and variability in appearance. The test-time adaptation provided by TTT layers helps in refining the segmentation boundaries, making the model more precise in instrument delineation.\nFor the cell segmentation task in microscopy images, the TTT-Unet models once again outperformed their counterparts. The TTT-Unet_Bot achieved the highest F1 score of 0.5818\u00b10.2410, followed by TTT-Unet_Enc with an F1 score of 0.5773\u00b10.2435. The superior performance in this task highlights the robustness of TTT-Unet in handling high variability and noise in microscopy data. The ability to dynamically adjust to new test samples allows the model to focus on relevant features, thereby improving segmentation accuracy even in challenging scenarios like cell segmentation."}, {"title": "Qualitative segmentation results", "content": "As shown in Figure 2, the segmentation results on the Abdomen MRI dataset reveal the effectiveness of TTT-Unet in handling complex anatomical structures. The comparison between the ground truth labels and the TTT-Unet predictions indicates a strong alignment, particularly in regions with significant anatomical variability. This suggests that TTT-Unet's ability to adapt its parameters during"}, {"title": "Discussion and conclusion", "content": "The experimental results across multiple biomedical image segmentation tasks consistently demon-strate that TTT-Unet provides a significant improvement over state-of-the-art methods. The key factor driving this improvement is the integration of TTT layers, which allow the model to adapt to the characteristics of each test image dynamically. This capability leads to enhanced generalization, especially in tasks involving diverse and complex imaging modalities, such as 3D abdomen CT, abdomen MRI, endoscopy, and microscopy datasets.\nFurthermore, TTT-Unet's superior performance in capturing long-range dependencies and handling high anatomical variability positions it as a robust tool for clinical applications. In the case of both large-scale anatomical structures and smaller, intricate features, TTT-Unet has demonstrated the ability to adapt and deliver accurate segmentation results. This versatility is particularly crucial in clinical scenarios where precision and adaptability are essential for effective diagnosis and treatment.\nOne of the primary advantages of TTT-Unet lies in its capacity to dynamically adjust model parameters during the test phase, which significantly enhances segmentation accuracy. Additionally, the lower variance in performance across different datasets emphasizes the model's robustness and consistency. However, it is important to acknowledge that the computational cost associated with test-time training could be a limitation for real-time applications. Future work should focus on optimizing the TTT layers to minimize computational overhead without compromising performance.\nIn conclusion, TTT-Unet represents a significant advancement in biomedical image segmentation by offering a flexible and adaptive solution. Its ability to consistently outperform other models in both 2D and 3D segmentation tasks reinforces its potential as a state-of-the-art model for medical image analysis. As the model evolves, further optimization of test-time adaptation strategies and integration with large-scale datasets will pave the way for broader clinical adoption and deployment."}]}