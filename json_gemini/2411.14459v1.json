{"title": "Unveiling User Preferences: A Knowledge Graph and LLM-Driven Approach for Conversational Recommendation", "authors": ["Zhangchi Qiu", "Shirui Pan", "Linhao Luo", "Alan Wee-Chung Liew"], "abstract": "Conversational Recommender Systems (CRSs) aim to provide personalized recommendations through dynamically capturing user preferences in interactive conversations. Conventional CRSs often extract user preferences as hidden representations, which are criticized for their lack of interpretability. This diminishes the transparency and trustworthiness of the recommendation process. Recent works have explored combining the impressive capabilities of Large Language Models (LLMs) with the domain-specific knowledge of Knowledge Graphs (KGs) to generate human-understandable recommendation explanations. Despite these efforts, the integration of LLMs and KGs for CRSs remains challenging due to the modality gap between unstructured dialogues and structured KGs. Moreover, LLMs pre-trained on large-scale corpora may not be well-suited for analyzing user preferences, which require domain-specific knowledge. In this paper, we propose COMPASS (Compact Preference Analyzer and Summarization System), a plug-and-play framework that synergizes LLMs and KGs to unveil user preferences, enhancing the performance and explainability of existing CRSs. To address integration challenges, COMPASS employs a two-stage training approach: first, it bridges the gap between the structured KG and natural language through an innovative graph entity captioning pre-training mechanism. This enables the LLM to transform KG entities into concise natural language descriptions, allowing them to comprehend domain-specific knowledge. Following, COMPASS optimizes user preference modeling via knowledge-aware instruction fine-tuning, where the LLM learns to reason and summarize user preferences from both dialogue histories and KG-augmented context. This enables COMPASS to perform knowledge-aware reasoning and generate comprehensive and interpretable user preferences that can seamlessly integrate with existing CRS models for improving recommendation performance and explainability. Our experiments on benchmark datasets demonstrate the effectiveness of COMPASS in improving various CRS models.", "sections": [{"title": "1 Introduction", "content": "Providing personalized, context-aware recommendations that align with users' changing preferences and situational needs is a critical challenge across various domains such as e-commerce, streaming platforms, and other online services. In recent years, conversational recommender systems (CRSs) have emerged as a promising approach, harnessing the power of natural language interactions to unravel user preferences [12, 18]. By engaging the user in interactive conversations, CRSs enable a better understanding of the user's evolving interests and guide them toward products, services, and information that best meet their immediate requirements [4, 24, 26]. Despite the success, CRSs face unique challenges in precisely modeling user preferences from the semantically rich and dynamically evolving dialogues, due to the challenge of natural language understanding [18]. CRSs must capture user's intent and interest from their natural language inputs, which are often ambiguous and context-dependent [24, 48]. Moreover, as the conversation progresses, CRSs need to continuously update and refine their understanding of user preferences in real-time [11, 34]. As the user reveals more about their preferences, CRSs must dynamically adapt their preference modeling to integrate the user's immediate interests expressed in recent interactions with their overall preferences developed over time.\nConventional CRSs mainly rely on item-centric approaches [3, 48], focusing on items mentioned during the conversation to model user preferences. Recent studies have incorporated pre-trained language models (PLMs) to enhance both natural language understanding and user preference modeling [37, 41]. However, these methods often fail to infer implicit preferences or reason about underlying motivations beyond explicit item mentions, leading to a superficial understanding of user intent. Moreover, they typically represent user preferences as hidden embeddings, leaving it unclear what specific preference is being considered when making a recommendation, as shown at the top of Figure 1. This ambiguity makes it challenging to verify the underlying reasons and results behind the recommendations, hindering the transparency and accountability of the recommender system.\nRecent advancements in Large Language Models (LLMs), which significantly scale up PLMs in terms of parameter size and pre-training data, have demonstrated exceptional capabilities in both natural language processing [8, 20, 47] and reasoning [42, 43]. These models have shown promise in generating natural language explanations, particularly in recommendation systems where explainability is critical. Recent works like X-REC [28] and SLIM [42] have demonstrated the LLM's ability to reason over user historical behaviors and generate interpretable recommendation explanations, enhancing both explainability and performance. Nevertheless, these methods only focus on the traditional recommendation settings, where user preferences are static and inferred from their historical behaviors. CRSs face unique challenges, necessitating real-time preference modeling and recommendations that adapt to users' changing preferences during ongoing conversations. Although LLMs have shown promise in CRS tasks such as evaluation [40, 50], zero-shot recommendation capabilities [16], and task planning [9, 10, 23], the role of explainable user preferences in LLM-based CRSs still remains unexplored.\nWhile LLMs have shown promise in CRS tasks, they still face limitations in incorporating domain-specific knowledge and keeping up-to-date with item information, which are essential for accurate preference modeling and recommendations. Knowledge Graphs (KGs) have proven effective in addressing these limitations [3, 41, 48], as they provide a rich, structured context of items and their relationships, offering domain-specific insights that enable more accurate and explainable recommendations. Despite their demonstrated potential, integrating the structured knowledge of KGs with the reasoning and language capabilities of LLMs presents a new set of challenges:\n\u2022 Modality Gap (Challenge 1): There exists a significant modality gap between KGs and LLMs, which hinders the LLM's ability to understand and interpret KG information. While LLMs process sequences of tokens that represent natural language, KGs represent information in a structured, graph-based format. This difference makes it difficult for LLMs to directly interpret the entities and relationships encoded in KGs, limiting their ability to leverage domain knowledge for preference modeling.\n\u2022 Cross-Modal Reasoning (Challenge 2): Effectively reasoning over both KG information and conversation data to infer user preferences is a complex task. LLMs, despite their strong natural language processing capabilities, are not inherently designed to perform this cross-modal reasoning. Therefore, they face difficulties in analyzing and synthesizing insights from graph-structured knowledge alongside dialogue history. This limitation hinders their ability to identify relevant patterns across both sources and to perform the knowledge-aware reasoning necessary for comprehensive user preference modeling, in which both domain knowledge and user interactions are essential.\nTo address the above challenges, we propose Compact Preference Analyzer and Summarization System (COMPASS), a novel framework that synergizes LLMs and KGs to unveil user preferences, improving both recommendation performance and the explainability of existing CRS. COMPASS directly tackles the limitations of current CRSs and the challenges of integrating LLMs with KGs through a two-stage process. First, we introduce a graph entity captioning pre-training mechanism that transforms KG structures into natural language descriptions. This allows the LLM to comprehend domain-specific information and bridge the modality gap (Challenge 1). We leverage a Graph Neural Network (GNN) to capture structural information from the KG and represent it as entity embeddings. These embeddings are then input into the LLM to produce textual descriptions of the entities along with relevant details from their neighbors. In this way, we align graph-structured knowledge with natural language, enabling the LLM to better interpret KG information. Building upon this alignment, COMPASS employs knowledge-aware instruction fine-tuning to improve the LLM's ability to reason about user preferences from dialogue histories and KG-augmented contexts. These KG-augmented contexts consist of relevant entity information and relationships extracted from the KG, providing a rich background for inference beyond the conversation history alone. Through carefully designed instructions, we enhance the LLM's capability to perform cross-modal reasoning (Challenge 2) by analyzing conversation history and cross-referencing with KG-augmented information. This instruction tuning process enhances the LLM's ability to extract explicit mentions, infer implicit interests, and reason about preferences in relation to various item attributes. Consequently, as shown in the bottom of Figure 1, COMPASS generates comprehensive and interpretable user preferences in text that capture both overall preferences and current interests. To leverage these insights, we introduce an adaptive gating mechanism that integrates summarized preferences into existing CRS models, boosting recommendation performance and explainability without requiring architectural changes. Our main contributions can be summarized as follows:\n\u2022 New framework. We propose COMPASS, a novel framework for enhancing user preference modeling in CRSs. To the best of our knowledge, this is the first work to leverage LLMs and KGs for explainable preference generation in CRSs.\n\u2022 Effective cross-model reasoning and explanation. We develop a two-stage process that enables the LLM to perform cross-modal reasoning over KGs and conversations, generating explainable user preference summaries. This approach moves beyond abstract vector representations to provide clear, human-readable user preferences.\n\u2022 Flexible Plug-in. COMPASS generates user preference summaries that are compatible with existing CRS architectures without requiring modifications to the system, improving both recommendation performance and explainability."}, {"title": "2 Preliminaries", "content": "Conversational Recommendation. Let i denote a candidate item from the set of items I, and let w denote a word in the vocabulary V. A dialogue D between a user and the recommender system consists of a sequence of utterances D = [u\u2081,..., uT], where ut [w,..., wm] is the t-th utterance composed of m words, and T is the maximum number of turns in the dialogue. As the conversation progresses, the dialogue history up to the turn t is denoted as Ht = [u\u2081: ut], where [u\u2081 : ut] signifies the chronological sequence of utterances from the first to the t-th turn. The CRS estimates the user's preferences based on Ht, them recommend K items from I, which are used to generate the next utterance ut+1. Note that It can be empty when no recommendation is needed. In such cases, the CRS may raise a clarification question or generate a casual conversation response.\nKnowledge Graph. A knowledge graph is defined as G = (E, A, X) where &, R represents the set of entities and relation types in the graph, respectively. A is the adjacency matrix capturing the relationships between entities and X represents the textual descriptions of each entity. For each entity e \u2208 &, its description is denoted as xe \u2208 X, where xe = [w\u2081, ..., w], and we represents the k-th word in the entity description. The entity set & encompasses candidate items I (e.g., movies) and non-item entities that represent item attributes (e.g., actors, genres, keywords). Formally, I C &.\nExplainable User Preferences and Recommendations. Explainable user preferences are crucial for enhancing the transparency and effectiveness of CRSs. Our goal is to generate clear, human-understandable textual user preference summaries that provide insights for both recommendation and explanation. Specifically, for a given dialogue history Ht, we define the generation of user preference summaries as:\nPt = f (Ip, Ht, EM, G), (1)\nwhere f represents a model that reasons over the dialogue history Ht, the mentioned entities En C &, and their associated information from the knowledge graph G. Ip is denoted as instruction prompts. The resulting Pt represents the textual explainable user preference summary at the t-th turn. The summary Pt is then encoded using a preference encoder g() and integrated into the base CRS model fcrs, which uses it to adjust its recommendation strategy. Depending on the model, additional inputs, such as KG information or dialogue history, may also be used. Formally, the recommendation step is represented as:\nIt = fcrs (g(Pt), Ht, G), (2)\nwhere It represents the recommended items at turn t, and g(Pt) denotes the encoded preference summary."}, {"title": "3 Methodology", "content": "We present an overview of COMPASS, followed by detailed descriptions of each component and the training process."}, {"title": "3.1 Overview", "content": "The primary goal of COMPASS is to synergize the reasoning capabilities of LLMs with the structured knowledge from KGs to analyze and summarize user preferences. COMPASS comprises three core components: (1) Graph encoder processes a domain-specific KG, capturing complex relationships between items to augment user preference modeling. (2) Graph-to-Text adapter aims to bridge the modality gap between the graph encoder and the LLM, enabling the LLM to comprehend the graph structure and conduct reasoning. (3) Large Language Model (LLM) leverages the powerful reasoning and generative capabilities of advanced language models to generate interpretable user preference summaries.\nTo integrate the components cohesively, we employ a two-stage training process: (1) Graph entity captioning aligns KG structures with natural language representations, creating a shared semantic space for the LLM to comprehend and reason with domain-specific knowledge effectively. (2) Knowledge-aware instruction fine-tuning optimizes the LLM for cross-modal reasoning, allowing it to generate comprehensive user preference summaries by synthesizing information from dialogue history and KG-augmented context. Once trained, COMPASS can be integrated with existing CRS models to improve their recommendation performance and explainability by generating user preferences and incorporating them into the recommendation process with an adaptive gating mechanism. Figure 2 illustrates COMPASS's architecture and training process."}, {"title": "3.2 Model Architecture", "content": "3.2.1 Graph Encoder. In COMPASS, the KG is a crucial source of domain-specific information that provides extra context for understanding attributes and relationships of items. To efficiently convert the structured knowledge in the KG into a format understandable by the LLM for preference analysis and summarization, we utilize a Relational Graph Convolutional Network (R-GCN) [33] to capture the complex graph structure and generate entity embeddings. The R-GCN is well-suited for modeling KGs due to its ability to handle multi-relational data and capture higher-order dependencies between entities.\nTo initialize the entity embeddings, we leverage the textual descriptions X associated with the entities in the KG. Specifically, for each entity e \u2208 &, we encode its description xe using a pre-trained language model (PLM). This provides a rich semantic foundation for the graph learning process. The R-GCN then captures both entity-level information and the overall graph structure through iterative message passing, which is particularly important for understanding the relationships between items and attributes. Formally, the representation of an entity e at the l-th layer is calculated as follows:\nh(0)e = PLM(xe), (3)\nh(l+1)e = \u03c3(1Zer\u03a3r\u2208R\u03a3e'\u2208E Wh(l)rh(l)e' + W(l)h(l)e), (4)\nwhere he is the embedding of entity e at the l-th layer, & is the set of neighboring entities connected to e through relation r, W(l)r and W(l) are learnable weight matrices, Zer is a normalization factor, and o is an activation function.\n3.2.2 Graph-to-Text Adapter. The entity embeddings produced by the graph encoder, while rich in structural information, exist in a different representational space from the LLM's textual token representations. This makes it challenging for the LLM to effectively reason with the KG-augmented context to generate user preferences. To bridge this semantic gap and enable effective knowledge integration, we introduce an adapter module that creates a mapping between graph-structured entity embeddings and the LLM's textual domain. Specifically, the adaptation process is defined as:\nh = fp(he), (5)\nwhere he is the adapted entity embedding aligned with the LLM's semantic space, he is the entity embedding generated from graph encoder, and fp is the projection function implemented as a linear layer.\n3.2.3 Large Language Model (LLM). The LLM is the main reasoning engine that generates user preferences by synthesizing information from dialogue histories and KG-augmented contexts. It captures both the explicit user preferences expressed in the dialogue and the implicit preferences inferred from the KG-augmented context for accurate preference analysis. Our framework is compatible with various state-of-the-art LLMs, allowing flexibility in model choice. In this paper, we use Llama3.1-8B [8] for its natural language understanding and generation capabilities. To adapt the chosen LLM efficiently for our task, we employ Low-Rank Adaptation (LORA) [17] for fine-tuning."}, {"title": "3.3 Training Pipeline", "content": "3.3.1 Graph Entity Captioning. To obtain a Graph-to-Text adapter that effectively bridges the modality gap between the graph entity embeddings and the LLM's semantic space, inspired by the pre-training strategies employed in vision-language models [5, 25], we introduce a graph entity captioning pretraining mechanism, as shown in Figure 2a. This process creates a strong connection between graph-structured data and natural language by generating entity-specific captions that encompass both the entity's intrinsic information and aggregated data from its neighboring nodes, simulating a message-passing process within the graph structure. The pre-training stage enables the LLM to interpret graph entity embeddings in a semantic context, leading to a deeper understanding of the relationships and attributes encoded within the KG, which is crucial for subsequent preference modeling. The caption generation process differentiates between item entities (e.g., movies) and non-item entities (e.g., genres, directors). For item entities, we employ a structured template that captures key attributes and relationships.\nh = fp(he), (5)\n3.3.2 Knowledge-aware Instruction Tuning. After the graph entity captioning pre-training, the LLM has gained a basic understanding of the KG structure and content. However, it has not yet been explicitly trained to utilize this knowledge for downstream tasks such as preference modeling and recommendation generation. To this end, we introduce a knowledge-aware instruction tuning, which aims to enable the LLM with abilities of reasoning across modalities, integrating information from both the dialogue history and the KG to infer user preferences and interests, as shown in Figure 2b.\nThe knowledge-aware tuning process employs a carefully crafted instruction prompt Ip to guide the LLM in synthesizing and reasoning over inputs from multiple sources. Given a dialogue history Ht and the entities Er mentioned within it, the process retrieves the embeddings Et of these entities from Equation 5. These embeddings, along with the full dialogue history, serve as inputs for the LLM. The instruction prompt Ip directs the LLM to analyze both the KG-derived entity information and the dialogue history to generate a user preference summary Pt.\nThis prompt follows a coarse-to-fine structure containing four key steps: (1) reasoning, providing transparency in the model's decision-making; (2) overall preferences, offering a broad view of the user's tastes; (3) current interests, capturing recent and specific preferences to guide subsequent recommendations; and (4) recommendation, leveraging the LLM's reasoning capabilities to suggest relevant items aligned with user preferences, guiding downstream"}, {"title": "3.4 Integration with Existing CRS Models", "content": "To enhance the existing CRS models with the user preference summaries generated by COMPASS, we propose a two-step integration process: (1) transforming the natural language preference summaries into a format compatible with CRS models, and (2) incorporating these transformed preferences to enhance the base CRS model's recommendation performance via an adaptive gating mechanism. Note that COMPASS remains frozen during this process.\n3.4.1 Preference Representation. Traditional CRSs are not designed to directly utilize natural language preference summaries. Therefore, we explore two methods to encode these summaries into a format suitable for existing CRS architectures:\n1) PLM-based representation: This method leverages a PLM to extract rich semantic representations from text. The PLM is adaptable and can be implemented as either a frozen or trainable text encoding model. In our implementation, we employ BERT [6] to encode the user preference summary as follows:\nstext = PLM(P), (8)\nwhere stext is the encoded preference from text preference P, specifically the [CLS] token embedding. The contextual understanding of language models enables stext to capture comprehensive user preference summaries at both coarse-grained (i.e., overall user preferences) and fine-grained levels (i.e., current interests and specific items).\n2) EOS representation: Considering the auto-regressive nature of LLMs and the information-rich preference summaries generated by COMPASS, we implement a computationally efficient encoding"}, {"title": "3.4.2 Enhanced Recommendation", "content": "3.4.2 Enhanced Recommendation. We integrate the encoded preferences into the existing CRS models to enhance their recommendation performance. We employ an adaptive gating mechanism to enhance the preference representation st captured from the base CRS model\u00b2 with our COMPASS-generated representation sc. Formally, we have:\ny = \u03c3(W[sb; Sc]), (10)\nSu = ysp + (1 - y)sc, (11)\nwhere W are the learnable weight matrices, o is the sigmoid activation function, and y represents the gating probability. This adaptive mechanism controls the influence of each representation on the final user preference representation su. The recommendation score for each item is computed using dot-product similarity as:\nPrec (i) = softmax(su Ii), (12)\nwhere I\u00a1 are the item representations\u00b3. Finally, we optimize the recommendation loss Lrec as follows:\nLrec = - \u2211NMj=1 i=1yij log (Precrec(i)), (13)\nwhere N is the number of conversations, M is the number of items, and yij is the ground truth label indicating whether item i is relevant to conversation j."}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\n4.1.1 Datasets. We conduct our experiments on two widely used English CRS datasets: ReDial [24] and INSPIRED [14] datasets. The ReDial dataset, focused on movie recommendations, contains 11,348 conversations and is constructed through crowdsourcing on Amazon Mechanical Turk (AMT). The INSPIRED dataset contains 999 conversations, also about movie recommendations, but additionally provides recommendation strategies based on social science theories. We constructed the knowledge graph by scraping data from IMDB\u00b9, using movie titles and release years as key search terms.\n4.1.2 Baselines. We consider a comprehensive range of baseline models, including traditional CRS ReDial [24], knowledge-graph based methods such as KBRD [3] and KGSF [48], language model-based approaches like BERT [6], GPT-2 [31], Llama3.1-8B [8], and hybrid models combining KGs with language models such as BARCOR [38] and PECRS [32].\nSince no existing baselines specifically generate user preference summaries, we also compare COMPASS to Llama3.1-8B and GPT-40, with Llama3.1-8B denoted as Llama-Summary when used solely for generating user preference summaries without access to the KG. More details on these models can be found in Appendix B.1.\n4.1.3 Evaluation Metrics. Our evaluation framework assesses both the performance of the recommendations and the quality of generated user preference summaries. For recommendation tasks, we adopt widely used metrics from previous works [2, 48, 49], including HR@K, NDCG@K, and MRR@K, with K set to 10 and 50. To evaluate user preference summaries, we employ three types of metrics: (1) Lexical Similarity: ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Sum; (2) Semantic Understanding: Following [36], we use GPT-40-mini to assess Reasoning Proficiency (RP) and Factual Consistency (FC); (3) User-Centric Evaluation: LLM-based simulated user evaluations, inspired by [13, 19], measure Explainability and User Preference Alignment. More details on the metrics used can be found in Appendix B.4.\n4.1.4 Implementation Details. We implement COMPASS using the Llama3.1-8B model [8] as the base LLM, which consists of 32 transformer layers with an embedding dimension of 4096. We freeze all parameters of the base LLM and employ LoRA [17] for fine-tuning. For the graph encoder, we set the number of layers to 1, with a hidden dimension of 768. We use batch sizes of 256 for pre-training and 128 for fine-tuning on the Inspired dataset. For the ReDial dataset, we maintain a batch size of 128 for both pre-training and fine-tuning. Early stopping is implemented to optimize training. All experiments are conducted on Nvidia A100 GPUs. For ground truth user preference summaries, we utilize the OpenAI API. More details on this process are provided in Appendix B.3. Our code will be made publicly available upon acceptance.\n4.2 Recommendation Evaluation\n4.2.1 Improvement over Baseline Models. COMPASS is designed to be flexible, allowing integration with different CRS models. To assess its effectiveness, we evaluated COMPASS across various CRS models. Performance results are shown in Table 1. We have the following observations: (1) Baseline models incorporating external KGs, such as KBRD and KGSF, consistently outperform simpler language model-based approaches like BERT and GPT-2. This highlights the importance of structured knowledge in capturing user preferences and item relationships, particularly in conversational recommendation settings. (2) Methods that combine language models with KGs, such as BARCOR and PECRS, show further improvements over KG-only models, demonstrating the benefits of integrating both sources of information. (3) Llama3.1-8B demonstrates strong performance, surpassing BARCOR and performing competitively with PECRS on the ReDial dataset while outperforming it on the INSPIRED dataset. This indicates that the advanced\nlanguage understanding and extensive world knowledge of LLMs can effectively capture accurate user preferences, resulting in more accurate recommendations.\nBuilding upon these strong baseline performances, the integration of COMPASS with these models leads to substantial improvements across all evaluation metrics. Notably, when integrated with KBRD on the INSPIRED dataset, COMPASS achieves remarkable relative improvements of 44.76% in HR@10 and 47.92% in HR@50, outperforming all baseline models, including PECRS. When applied to the already strong Llama3-8B model, COMPASS still delivers considerable enhancements, with increases of 13.76% in HR@10 and 14.45% in NDCG@10 on the ReDial dataset, and even greater gains on the INSPIRED dataset. These consistent improvements are attributed to the high-quality, knowledge-enriched user preference representations generated by COMPASS. These representations not only capture user intent more effectively but also provide structured insights that enhance the overall recommendation process, leading to both better performance and explainability.\n4.2.2 Comparison of Enhancement Methods. To evaluate COMPASS's effectiveness, we compare it with Llama-Summary and GPT-40 as baseline enhancers, which generate user preference summaries without KG access. Figure 3 illustrates the performance of these methods across various CRS models on the INSPIRED dataset. While all enhancement methods show improvements over the base models, COMPASS consistently outperforms both alternatives. Notably, COMPASS consistently outperforms GPT-40 across all models, with improvements ranging from marginal (for Llama3.1-8B) to substantial (for KBRD and KGSF). This superior performance, achieved despite GPT-40 being a much larger model, demonstrates the importance of integrating domain-specific knowledge into preference modeling. The substantial performance gap between COMPASS and Llama-Summary highlights the effectiveness of our framework in leveraging both structured knowledge and LLM capabilities. These results show COMPASS's ability to generate more accurate and contextually relevant user preference summaries, leading to improved recommendation performance across different CRS architectures.\n4.2.3 Ablation study. To assess the contribution of different components in COMPASS, we conducted an ablation study by evaluating several model variants across three base models: KBRD, KGSF, and LLaMA3.1-8B. We compare the COMPASS model against three ablated versions: (1) COM w/o REC: The generated preference summary does not include recommended items; (2) COM w/o GEP: COMPASS without the graph entity captioning pre-training; (3) COM w/o KG: COMPASS without the graph encoder."}, {"title": "4.3 Preference Generation Evaluation", "content": "This study introduces a novel approach to enhance CRS by generating user preference summaries. As the first attempt in this direction, this evaluation focuses on comparing COMPASS with its variants to assess the effectiveness of its key components. Additionally, a case study that includes examples from Llama-Summary and GPT-40 provides additional qualitative insights.\n4.3.1 Automatic Evaluation. Table 3 presents a comparative analysis of COMPASS and its variants, COM w/o KG and COM w/o GEP, on the ReDial and INSPIRED datasets. COMPASS consistently outperforms its variants across all metrics on both datasets. Lexical similarity metrics (ROUGE scores) demonstrate that COMPASS generates summaries that align closely with reference texts, while higher reasoning proficiency and factual consistency scores illustrate its enhanced quality of reasoning and factual accuracy.\nThe performance of COMPASS compared to COM w/o KG indicates the value of KG integration in enhancing summary quality. This suggests that incorporating structured knowledge grounds the model's output in domain-specific information, which is crucial for accurate preference generation. Moreover, the comparative performance of COM w/o GEP and COMPASS indicates that, despite the presence of structured knowledge from the KG, there remains a modality gap between graph structures and natural language, posing challenges for LLMs in fully understanding and utilizing this information. These results validate the COMPASS framework, and demonstrating the synergistic effect of combining KGs with LLMs, and showing the critical role of our pre-training strategy in enhancing user preference generation.\n4.3.2 LLM-Simulated User Evaluation. To assess user-perceived quality, we employ LLM-simulated user evaluations on explainability and user preference alignment. Figure 4 presents the results across the ReDial and INSPIRED datasets. COMPASS is consistently preferred over COM w/o KG and COM w/o GEP in both metrics. On the ReDial dataset, COMPASS is most preferred in in approximately 75% of the cases for both alignment and explainability. Although the preference gap is smaller on INSPIRED, COMPASS maintains a clear advantage. Notably, COM w/o NP is consistently the least preferred, showing the importance of pre-training in generating meaningful preference summaries. These results demonstrate COMPASS's superior ability to generate preference summaries that align closely with user preferences while providing clear, user-friendly explanations for recommendations, enhancing both fidelity and transparency in CRSs."}, {"title": "4.3.3 Case Study", "content": "We present a detailed case study in Table 5 to visualize a sample dialogue along with the corresponding preference summaries generated by different models. It demonstrates that COMPASS accurately identifies the key information for capturing user preference and provides interpretable summaries. The detailed analysis can be found in Appendix C.2."}, {"title": "5 Conclusion", "content": "In this paper, we introduce COMPASS, a novel framework that synergizes KGs and LLMs to unveil user preferences in Conversational Recommender Systems (CRSs). To address the modality gap between structured knowledge and natural language, we propose a graph entity captioning mechanism that transforms KG structures into LLM-compatible representations. Through knowledge-aware instruction tuning, COMPASS becomes proficient in performing cross-modal reasoning, generating comprehensive and interpretable user preference summaries. Our proposed framework has been extensively evaluated as a plug-and-play enhancement for various existing CRS models across benchmark datasets. The results demonstrate COMPASS's effectiveness in significantly improving both the recommendation performance and the explainability of these base models. The adaptive integration mechanism of COMPASS allows for seamless enhancement of diverse CRS architectures without requiring structural modifications, showcasing its versatility and potential for widespread adoption in the field of CRSs."}, {"title": "A Related Work", "content": "A.1 Conversational Recommender System\nConversational Recommender Systems (CRSs) represent an evolution in recommendation technology, designed to understand user preferences through interactive dialogue and provide personalized recommendations [18]. These systems face a unique challenge in modeling user preferences from typically brief and sparse conversational data. To address this, recent works incorporate external knowledge sources into CRSs to enhance preference modeling. Knowledge graphs (KG) enable systems to infer implicit user interests and capture complex relationships between items [3, 48], while user reviews provide additional insights into item attributes and user opinions [21, 27, 49], helping to address the challenge of limited contextual information. Moreover, the integration of pre-trained language models (PLMs) such as DialoGPT [37, 41, 46] and BART [22, 30] has further improved contextual understanding and language generation capabilities in CRSs.\nMore recently, the advent of powerful LLMs has introduced new opportunities for CRSs. Studies have focused on improving LLM evaluation through enhanced user simulators [40, 50], exploring zero-shot recommendation capabilities [16], and utilizing LLMs for task planning in CRSs [9, 10, 23]. However, one critical aspect that remains underexplored is the role of explainable user preference in CRSs. Although some conceptual designs propose using LLMs to manage user preferences [11], they lack quantitative validation. [44] addresses user preference memories in sequential CRS but does not fully harness LLM's reasoning abilities for preference generation. Our work is the first to introduce a framework that synergizes the LLM and the KG for unveiling user preferences in CRSs, enhancing both preference modeling and recommendation explainability.\nA.2 Explainable Recommendation\nExplainable recommendation has emerged as a crucial areas of research, aiming to enhance user trust, satisfaction, and decision making by providing transparent rationales for recommendations. Early research focused on leveraging user-item interactions and content features to generate simple explanations [45], using techniques like recurrent neural networks [7], attention mechanisms [1], and graph neural networks [39]. Recent developments in LLMs have opened new avenues for explainable recommendation. Studies [28, 42] have demonstrated LLMs' potential in generating explanations by analyzing user's past interactions. However, current LLM-based methods primarily operate within the constraints of static user profiles, where preferences are inferred from historical behavior. Our work introduces a novel LLM-KG framework for CRSs, enhancing both preference modeling and recommendation explainability in dynamic interaction contexts."}, {"title": "B Details of Experimental Setting", "content": "B.1 Model Details\nB.1.1 Backbone Models. The detailed information for each backbone is as follows:\n\u2022 ReDial [24", "15": "as the recommendation module.\n\u2022 KBRD [3", "48": "This method employs a dual KG approach", "6": "This method utilizes a bidirectional Transformer model [35", "CLS": "token representation for the recommendation task.\n\u2022 GPT-2 [31", "8": "This is an open-source LLM that has been trained using both supervised fine-tuning and reinforcement learning with human feedback [29", "38": "This model introduces a unified framework that integrates BART with a knowledge graph", "32": "This model integrates recommendation and response generation into a single training phase using LoRA [17", "work": "first, as a backbone CRS model adapted through training for the recommendation task, and second, as Llama-Summary for generating user preference summaries without KG access.\nB.1.2 COMPASS Integration. We enhance CRS models performance by incorporating our COMPASS framework as described in Section 3.4. The integration method is consistent across KBRD, KGSF, and BERT, utilizing the adaptive gating mechanism to combine COMPASS-generated preference summaries with the original model representations. For preference-enhanced Llama3, we employ a different integration strategy. We directly concatenate the generated user preference summary with the conversation history. This approach leverages Llama3.1-8B's strong language understanding capabilities to process"}]}