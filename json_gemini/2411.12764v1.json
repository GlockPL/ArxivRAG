{"title": "SEFD: Semantic-Enhanced Framework for Detecting LLM-Generated Text", "authors": ["Weiqing He", "Bojian Hou", "Tianqi Shang", "Davoud Ataee Tarzanagh", "Qi Long", "Li Shen"], "abstract": "The widespread adoption of large language models (LLMs) has created an urgent need for robust tools to detect LLM-generated text, especially in light of paraphrasing techniques that often evade existing detection methods. To address this challenge, we present a novel semantic-enhanced framework for detecting LLM-generated text (SEFD) that leverages a retrieval-based mechanism to fully utilize text semantics. Our framework improves upon existing detection methods by systematically integrating retrieval-based techniques with traditional detectors, employing a carefully curated retrieval mechanism that strikes a balance between comprehensive coverage and computational efficiency. We showcase the effectiveness of our approach in sequential text scenarios common in real-world applications, such as online forums and Q&A platforms. Through comprehensive experiments across various LLM-generated texts and detection methods, we demonstrate that our framework substantially enhances detection accuracy in paraphrasing scenarios while maintaining robustness for standard LLM-generated content. This work contributes significantly to ongoing efforts to safeguard information integrity in an era where AI-generated content is increasingly prevalent. Code is available at https://github.com/hwq0726/SEFD.", "sections": [{"title": "I. INTRODUCTION", "content": "The proliferation of LLM-generated content, while beneficial in many ways, also raises significant concerns, especially in terms of the authenticity and accuracy of information. Instances of misleading or incorrect LLM-generated text, as evidenced in the study by [1], highlight the imperative need for effective detection mechanisms. Additionally, in human health-related research, the authenticity of data is paramount [2], [3], as reliance on fabricated or inaccurately generated data, such as AI-generated medical notes, could fundamentally compromise study outcomes. Ensuring data integrity is essential to maintain trustworthiness [4], [5], especially when findings directly impact patient care or public health policies [6].\nInnovations in this field include watermarking [7]\u2013[9], training based classifiers [10]\u2013[15], statistical test based detectors [14], [16]\u2013[18] and text heterogeneity based detectors [19]\u2013[21]. In an era where LLM-generated content is increasingly dominant, these tools represent a concerted effort to ensure the integrity and reliability of information.\nIdentifying LLM-generated text poses a significant challenge due to the rapid evolution of LLMs and widespread evasion tactics [22]. Concurrently, evasion techniques such as paraphrasing-where another LLM is used to rephrase the original LLM-generated text\u2014undermine the effectiveness of detection tools like GPTZero [23], DetectGPT [16], and OpenAI's text classifier [15]. These methods are non-watermarking techniques that rely on LLM-specific features. Paraphrasing can obscure these features by altering text attributes, which hampers detection. Furthermore, watermarking strategies that insert unique patterns into LLM outputs are vulnerable to evasion through paraphrasing [24] which can eliminate the markers by restructuring sentences and replacing synonyms.\nTo address the paraphrasing issue, we introduce a semantic similarity-based retrieval technique that computes the semantic similarity between the target text and LLM-generated texts from a pre-defined database to determine whether the target text is LLM-generated. Since paraphrasing largely preserves the original semantic meaning, this technique offers a viable approach to defend against paraphrasing attacks. By systematically integrating this technique with traditional detection methods, we aim to achieve robust detection performance for both regular LLM-generated text and paraphrased content. However, the effectiveness of the retrieval technique heavily depends on the comprehensiveness of its database, which presents significant practical challenges. On one hand, storing all LLM responses demands significant storage space and computational resources for retrieval from this extensive dataset. On the other hand, to save storage, a limited database may not be sufficient and can be outdated as the environment or distribution changes. Given the challenges posed by paraphrasing tactics and the limitations of the retrieval technique, we propose a framework that is able to not only systematically combine it with the traditional method but also update in real-time as detection progresses.\nWe also observe that in many detection scenarios, texts follow a sequential order, such as detecting the source of comments under a post or answers under a question. With this in mind, we focus on the AI text detection task within this sequential text context, assuming that inputs are strictly ordered. Based on that, we develop a semantic-enhanced framework for detecting LLM-generated text (SEFD) to improve the detector's performance against paraphrasing attacks. As shown in Fig. 1, our framework comprises a retrieval pool (colored by purple) and three detection steps (colored by green). Initial detection, which employs an existing detector to analyze the input text and generate a detection score; Semantic Similarity Computation, a pre-trained model (e.g., BERT-based) computes semantic similarity with texts in the retrieval pool, getting a similarity score; Semantic Enhanced Detection, a fusion function integrates the detection and similarity scores for the final score. The retrieval pool is updated using both detection and similarity scores to adapt to new LLM-generated text, enhancing paraphrase detection. Our contributions can be summarized as follow:\n\u2022\tWe introduce a novel semantic-enhanced framework for detecting LLM-generated text (SEFD) that integrates retrieval-based approaches with traditional detection methods, significantly improving robustness against paraphrasing attacks.\n\u2022\tWe develop an efficient and adaptive retrieval pool mechanism that balances comprehensive coverage with computational practicality, allowing real-time updates to adapt to new LLM-generated content.\n\u2022\tWe demonstrate the framework's effectiveness in sequential text scenarios common in real-world applications, and provide extensive experimental validation across various LLM-generated texts and detection methods.\nThe rest of this paper is organized as follows: Section II reviews related work in LLM-generated text detection. Section III details our proposed SEFD framework. Section IV presents our experimental setup and results. Finally, Section V discusses the limitations of our approach, outlines future research directions, and concludes the paper."}, {"title": "II. RELATED WORK", "content": "Watermarking Techniques. Current LLM-generated text detection methodologies fall into two categories: watermarking and non-watermarking. The studies [7], [25]\u2013[31] leveraging watermarking approach to address the detection issues involves subtly altering the generated text in a way that is undetectable to human readers but can be identified by specialized algorithms during a post-generation analysis. Effective watermarks are meticulously designed to be resistant to removal and to exert minimal impact on the overall quality of the text output. As an example, soft watermarking developed by [7], utilizes a novel token partitioning strategy, categorizing tokens into \"green\" and \"red\" lists. This partitioning aids in the creation of distinct watermark patterns, with a watermarked LLM typically selecting tokens from the \"green list\", which is determined by the preceding token, with a high degree of probability. In a recent development, Scott Aaronson [32] has announced his investigation into cryptographic methods of watermarking, in a collaborative effort with OpenAI. Their preliminary method is based only on biasing of the LLM output, diverging from the more deterministic approach seen in the work of [33]. While comprehensive details of this method are yet to be disclosed, preliminary information hints at the involvement of hashing techniques applied to n-gram sequences. It is important to note, however, that watermarking approaches are inherently targeted and are thus applicable only to specific LLMs. Moreover, the advancement of watermarking algorithms is somewhat constrained by the necessity for access to open-source LLMs.\nNon-watermarking Techniques. Compared to watermarking-based methods, non-watermarking techniques offer the distinct advantage of being capable of detecting text generated by various LLMs without necessitating modifications to the generative algorithms. Early non-watermark detection strategies focused on identifying statistical anomalies in metrics such as entropy [34] and perplexity [35]. A notable advancement in this field was the introduction of the GLTR visualizer [17], designed to aid human evaluators in distinguishing LLM-generated text. The emergence of ChatGPT led to the development of zero-shot detectors [14], [16], [18]\u2013[21], leveraging the statistical and topological properties of the LLM-generated text. Instead of these, classifier-based methods train supervised models to distinguish human-written text from LLM-generated text [10]-[15]."}, {"title": "III. METHOD", "content": "We define our detection task as a binary classification problem. Let X = (xi)\n  i=1 be a text sequence, where \u03a7 \u0395 \u03a9, and 2 is the text space. We define:\n1) \u03a9 = \u03a9H UfLM\n2) \u03a9H is the human-written text space.\n3) \u03a9fLM is the text space generated by an LLM fLM.\n4) \u03a9H \u03a0\u03a9 fLM = \u00d8.\nOur detection task is to define a classification function D : \u03a9\u2192 {0,1} such that for each xi:\nD(xi) =\n{\n0 if xi \u2208 NH\n1 if xi \u2208 NfLM\n(1)\nIn particular, for a text Yi, \u0177r is its paraphrased version by any LLM model. Since we only want to distinguish whether a text is human-written or not, we consider yi \u2208 NfLM for simplicity, which means in the detection phase, \u0177i should be classified as LLM-generated instead of human-written.\nFig. 2 illustrates the structure of our framework which is a detailed version of Fig. 1. In the following sections, we detail our detection framework, starting with the three detection steps, followed by an introduction to the retrieval pool and its updating rules.\nA. Initial Detection\nIn the initial detection phase, we employ a pre-studied detector capable of analyzing input text and generating a detection score, as presented in Step I of Alg. 1. Mathematically, this detector can be represented as a function f : \u03a9 \u2192 R, mapping from the text space to the real number. Formally, for a candidate text xi, the detection score is given by:\nSdet\ni = f(xi)\n(2)\nDetectors that meet our requirements should produce distinct score distributions for different text sources, enabling differ-entiation between them. For instance, in soft watermarking applications, the watermarked text (LLM-generated) scores are typically significantly higher than those for human-written text.\nThe choice of detectors can vary, with different detectors yielding unique score distributions. For real-world data, we curated 1300 questions from the r/explainlikeimfive subreddit 1 and collect answers from three different sources: original human-written answers, LLM-generated answers with GPT-2 XL model [36], and paraphrased versions of the LLM-generated answers using DIPPER [24], a widely used paraphrasing LLM. Fig. 3 illustrates the detection score distributions of these answers from four distinct detectors: Log-Likelihood [18], DetectGPT [16], Intrinsic Dimension [19], and Soft Watermarking [7]. These detectors show varying abilities to distinguish between different sources of text, as indicated by the overlapping and separation of their respective"}, {"title": "B. Semantic Similarity Computation", "content": "Semantic similarity is a well-studied area [37]\u2013[41], which is often used in pool-based retrieval method. Typically, computing the semantic similarity unfolds in a two-step process: first converting text into numerical form, which is a crucial step that lays the groundwork for understanding the underlying semantics; once texts are vectorized, the similarity is calculated using metrics such as cosine similarity, Euclidean distance, Manhattan distance, etc.\nRetrieval pool and pool size. Given an LLM fLm, for any given prompt qk, we use this LLM to generate a response Yk, represented as Yk = fLM(qk), where Yk \u2208 \u03a9fLM. Let fenc : \u03a9 \u2192 Rd be an encoder (e.g., sentence-transformers [37]) that embeds variable-length sequences into fixed-size vectors that encapsulate their semantic content, expressed as uk = fenc (yk). Let q1,...,qM be the set of prompts that have been inputted into the LLM with y1,..., ym as their output. We set the initialized pool size to be Mo, where 0 \u2264 Mo \u2264 M, and construct the retrieval pool y = {u\u2081, ..., uMo } by encoding Mo LLM responses with the specified encoder. Note that different LLMs may generate varying responses even with the same prompt, so it is necessary to create separate pools for each LLM.\nAfter defining the retrieval pool, for each given text, we compare its embedding with all the others in the retrieval pool to compute the semantic similarity. Specifically, let xi be a candidate text and v\u2081 = fenc (xi) be the vector after embedding, the maximum cosine similarity between vi and uj; over all uj \u2208 Y is:\n2\nSsim = max\nUjey\nsim (Vi, Uj) =\n(Vi,Uj)\n|| Vi||||uj||\n(3)\nIf sim is close to 1, then there is a high probability that Xi is generated by this LLM. Assuming the retrieval pool is large enough to cover all the distribution of the texts generated by the LLM (e.g., Mo = M), the similarity score of the LLM-generated text should always be 1. For better illustration, we generate answers to the 1300 questions men-tioned in Section III-A using four different LLMs: GPT-2 XL model [36], OPT-13B model [42], GPT-3.5 model [43], and GPT-40-mini [44] model, these answers are then paraphrased using DIPPER [24]. For each LLM, we initialize the retrieval pool that includes all the original answers. As shown in Fig. 4, the similarity scores of LLM-generated text are concentrated around 1. More importantly, the similarity score will still be high even if the LLM-generated text is paraphrased by another LLM, since the semantic of the text will remain essentially unchanged after paraphrasing, whereas the similarity scores for human-written text are much lower. This distinction makes the semantic similarity retrieval technique a robust method for defending against paraphrasing. The procedure for computing semantic similarity is summarized in Step II of Alg. 1.\nAs highlighted previously, achieving optimal detection re-sults necessitates a considerably large retrieval pool, a require-ment that stands as a significant limitation of this technique. This challenge is further compounded by the escalating usage of LLMs across various domains. The considerable magnitude of Mo will lead to high computing requirements for executing similarity searches. While efficient methodologies such as FAISS [45] provide some relief by utilizing adept nearest neighbor libraries, they still fall short in terms of scalability. If we use a relatively small pool to save storage and time, it may not cover all the distribution and therefore fails to provide an effective similarity score. In Section III-D we delineate our pool updating rule to address this issue."}, {"title": "C. Semantic Enhanced Detection", "content": "Initial detection (Step I) distinguishes itself by obviating the necessity for a training set or a similar resource pool. This characteristic is particularly beneficial in offsetting the substantial pool capacity demands of the semantic similarity computation (Step II). On the opposite, Step II demonstrates a remarkable resilience to paraphrase attacks, a feature that effectively compensates for Step I's vulnerability in similar scenarios.\nWhy combine two scores? To effectively integrate the detection score and similarity score, we observe that, as a result of the diminished capacity of the retrieval pool, a low similarity score typically indicates that the input is either human-generated or from an unincorporated LLM response. Under such conditions, the decision-making capability of Step II becomes less reliable, potentially exerting a significant influence on the judgment of initial detection. Consequently, we prioritize the insights from Step I when dealing with a lower similarity score. On the other hand, a similarity score close to 1 provides robust evidence of the input being LLM-generated, necessitating an increased dependence on Step II's assessment.\nFusion function. For the following discussion, we focus on detectors where human-written text has a lower detection score compared to LLM-generated text\u00b2 and use min-max to normalize the detection score to [0, 1] before integration (in the following discussion of this part, we still use sdet to denote the normalized detection score). Based on the above observations and analysis, we design the following fusion function. Given parameters A1, A2 > 0, the fusion function is defined as\nffus (Sdet, Ssim) =\nSdet\n(1 +\n10-11 \u2013 ssim)1/12\n(4)\nIn the fusion function ffus (Sdet, Ssim), the coefficient (1+10-11-5sim)1/2 serves as the weighting factor of Sdet. The parameters 1 and 2 control how strongly ssim influences Sdet. In this configuration, when ssim is close to 1, Sdet will be amplified by approximately 10^1/12 times. This allows a candidate to be classified as LLM-generated even with a small Sdet. Conversely, when ssim is near 0, Sdet is only slightly adjusted, placing greater reliance on Sdet for the final classification outcome. However, it is important to note that, since the distribution of sdet is various for different detectors, the parameters A1, A2 need to be tuned accordingly.\nWith the definition of the fusion function ffus, we can inte-grate the detection score and similarity score. For a candidate text xi with its detection score set and similarity score si the semantic enhanced detection score is:\nSiffus($det, Ssim)\nSsin\n(5)\nWith these results, we can determine if the candidate text xi is generated by the LLM LM by simply thresholding the semantic enhanced detection score si. This detection process is summarized in Step III of Alg. 1."}, {"title": "D. Retrieval Pool Updating Rule", "content": "Input text assumptions. Many real-world applications involve sequential text, where accurate detection is important. For example, in online forums or social media platforms like X and Instagram, comments are chronologically arranged under each post and determining whether a comment is generated by a user or an automated system is key to effective content moderation. Also, on question-answer websites like Stack Overflow and Quora, where users submit questions and receive answers in a sequential format, it is crucial to determine whether responses are from LLM or are original human-generated content. This distinction helps maintain the integrity of the information and prevents the spread of misleading content. Educational platforms and customer support systems, which also utilize sequential text, similarly benefit from robust detection mechanisms to uphold content accuracy and relevance. In this paper, we mainly focus on the LLM-generated text detection task in this sequential text scenario. Based on the observation, we first define input text assumptions for the input text sequence X = (xi)i=1:\nA. 1: The inputs are in a strictly sequential order, which means two or more texts cannot be input at the same time.\nA. 2: For an LLM-generated text, the paraphrased version always comes after the original text.\nFor example, let \"a,\" \"b,\" and \"c\" represent LLM-generated texts, and \u201ca,\u201d \u201cb\u201d and \u201cc\u201d represent their paraphrased ver-sions. A valid input sequence would be: \u201ca; b; \u00e2; b; c; \u00ea.\u201d\nThese two assumptions are important for updating the retrieval pool effectively. A. 1 ensures that we can examine each text sequentially, one at a time, maintaining the order of input. A. 2 is fundamental to our approach, as it stipulates that any paraphrased LLM-generated text will only appear after its original LLM-generated text. This sequential ordering is essential for defending against paraphrasing attacks. The logic behind this assumption is intuitive: to paraphrase an LLM-generated text using another LLM, the original text must already exist. Next, we introduce the updating rule.\nUpdating rule. For each input candidate text xi \u2208 X, we decide whether to add it into the retrieval pool or not based on the updating rule. We first define two thresholds Edet and Esim. Given the detection score s\u00e5et and similarity score sim of the text xi, the updating rule is presented in Table I and"}, {"title": "IV. EXPERIMENTS", "content": "We conduct experiments to gain deeper insights into var-ious aspects of detecting LLM-generated text. We study the effectiveness of SEFD for detecting LLM-generated text and defending against paraphrasing attacks across various datasets. To further understand the impact of the retrieval pool, we also examine the detection accuracy under different initial pool sizes. Finally, as an additional demonstration of our method's robustness, we evaluate SEFD's performance against recursive paraphrasing attacks, which can severely degrade the effectiveness of traditional detectors.\nA. Evaluation Metrics\nIn our experiment, we use two metrics to evaluate the performance of detection.\nAUROC Since detection is fundamentally a binary classi-fication task, the results are largely dependent on the chosen decision threshold (e in Alg. 1). The first metric is the AUROC (Area Under the Receiver Operating Characteristic Curve) which is commonly used to measure detection performance [16], [24], assessing detection performance across the spec-trum of potential thresholds.\nDetection Accuracy Typically, we anticipate the detection process to exhibit a high true positive rate (TPR) performance. However, in the detection of LLM-generated text, maintaining a low false positive rate (FPR) is also significant; in other words, human-written text must be rarely misclassified as LLM-generated [7], [22]. This requirement is straightforward and intuitive since an AI language detector without a low FPR can cause harm as it might wrongly accuse a human of plagiarizing using an LLM. Therefore, by adjusting the detection threshold, we compute the true positive rate (TPR) at 1% FPR, denoted as detection accuracy, which serves as our second metric. Compared with AUROC, detection accuracy is more important in real scenario detection, so in our experiment, we will focus more on detection accuracy.\nB. Baselines, Data, and Settings\nBase language models In our experiment, we want to assess and compare the performance of detectors in identifying text produced by different LLMs. We focus on four base language models: (1) GPT-2 XL model [36], which possesses 1.5B parameters; (2) OPT-13B model [42], renowned for its unique architecture; (3) text-davinci-003 variant from GPT-3.5 family [43], which has 175B parameters and has additionally been instruction tuned using reinforcement learning from human feedback (RLHF) [46]; (4) GPT-4o mini [44], a latest cost-efficient version of GPT-4 with only 8B parameters.\nLLM-generated text data Our study explores long-form text generation tasks, primarily due to their association with potentially harmful uses, such as the fabrication of false arti-cles. We specifically focus on long-form question answering. This involves a language model providing detailed responses of 250-300 words to complex how/why queries, like \"Why did it take so long for sunglasses to get widespread?\". To create a relevant dataset for this task, we extract questions from Reddit, targeting six prominent domains: biology, physics, chemistry, economics, law, and technology. From each domain, 500 questions are randomly selected. Each question is paired with its most comprehensive human-written response found on the subreddit, resulting in 3,000 long-form question-answer pairs. These questions are then used as prompts for the four"}, {"title": "C. Results", "content": "1) Main Results: We first present how SEFD can improve the detection performance across four initial detectors: Log-Likelihood (likelihood), DetectGPT, Intrinsic Dimension with MLE (ID-MLE), and Soft Watermarking (Watermarking). The results are shown in Table III. In this table, \"+ pool size -0\" indicates SEFD with an empty initial pool, while \"+ pool size -1/5\" indicates SEFD with an initial pool containing 1/5 of the LLM-generated text.\nEffective defense against paraphrase. From the results in Table III, we observe that SEFD significantly improves detection performance in both AUROC and detection accuracy under paraphrase attacks across all datasets and initial detec-tors. Intuitively, a larger pool size generally leads to better performance since when detecting text already present in the pool, the similarity score approaches 1, allowing for accurate classification. However, there are cases that SEFD performs better with an empty initial pool. This is primarily due to the lack of robustness in the semantic encoder we applied\u2074. Given that both human-written and LLM-generated texts answer the same question, they often share common terminology related to the query. A less robust encoder may cause the semantic similarity based technique to incorrectly identify high semantic similarity between a human-written answer and an LLM-generated text stored in the initial pool, potentially leading to misclassification. In this rare situation, an empty pool can avoid such mistake and thus lead to a better performance than a non-empty pool. The influence of the initial pool size will be further discussed in Section IV-C2.\nPerformance on detecting LLM-generated text. Besides the improvement on defending paraphrase, SEFD also en-hances detection of original LLM-generated text. With the initial pool size 1/5, detection accuracy shows significant improvement. However, with an empty initial pool, the detec-tion accuracy decreases for Log-likelihood, DetectGPT, and Intrinsic Dimension. This is because, with an empty pool, the semantic similarity retrieval technique cannot assist in detecting LLM-generated text. Additionally, as the detection process continues and texts are added to the pool, the similarity score becomes helpful for detecting paraphrases but may also negatively influence the initial detection decision, as discussed in Section III-C. Notably, if the initial detector is robust enough in classifying LLM-generated text, such as watermarking, this decrease will not occur, as shown in the results of Watermarking in Table III.\n2) Influence of Initial Pool Size: In this section, we inves-tigate the influence of initial pool sizes on detection perfor-mance. Fig. 6 illustrates our findings, with DetectGPT serving as the initial detector. The x-axis represents the proportion of LLM-generated text included in the initial pool. A non-empty initial retrieval pool is expected to enhance both original LLM-generated text detection and paraphrase detection: For LLM-generated text detection, the semantic similarity retrieval technique can precisely identify text that are already stored in the pool, outputting a similarity score of 1 to classify the text as LLM-generated. For paraphrase detection, when detecting a paraphrased version of text stored in the pool, the similarity score, while not equal to 1, will be close to it. This score is still larger than that of human-written text, thus aiding detection.\nAs evident in Fig. 6, detection accuracy improves as the pool size increases for both detection tasks. Notabaly, the degree of improvement for paraphrased detection is less pronounced compared to that of original LLM-generated text detection. Thanks to our updating rule, even with an empty initial pool SEFD already has a significant improvement when facing paraphrasing attacks. The updating rule allows the retrieval pool to incorporate new LLM-generated text, which aids in paraphrase detection. Consequently, paraphrase detection does not solely rely on the initial pool, explaining the more modest improvement compared to original text detection."}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "A. Limitations and Future Works\nLimitations. One limitation of our work is the lack of globally optimal parameters. Our semantic-enhanced detec-tion framework employs four tuning parameters: \u20acdet, sim, A1, and 12. In our experiments, we selected workable values to demonstrate our method's efficacy. However, these parameters warrant further investigation to potentially improve detection performance. It is important to note that there is no universally optimal choice for these parameters, as their ideal values largely depend on the initial detector used. Furthermore, we discuss only one type of fusion function in this study. While effective, this represents just one possible approach. The optimal choice of fusion function may vary depending on the characteristics of different initial detectors.\nFuture Work. While our proposed framework primarily focuses on improving detection performance, future work may address equitable detection across diverse groups of human-written text. Studies have shown that GPT detectors can be biased against non-native English writers [49]. This fairness concern may be widespread in LLM-generated text detection and has implications for human rights. Besides, the rela-tionship between prompting strategies and detection efficacy remains unexplored. Future work should examine whether clever prompts, such as instructing LLMs to generate answers in a more human-like tone, can successfully evade existing detection methods. Finally, the boundary between humans and AI needs to be further studied. When a human-written text is paraphrased by an LLM, classifying the paraphrase as either purely human-written or LLM-generated is not persuasive. Future research may develop more nuanced classification systems that go beyond the simple binary of human-written or LLM-generated.\nB. Conclusion\nThis study introduces SEFD, a semantic-enhanced frame-work for detecting LLM-generated text, addressing challenges like paraphrasing and recursive paraphrasing attacks. By inte-grating retrieval-based mechanisms with traditional detectors, SEFD leverages text semantics to enhance detection perfor-mance and robustness. Extensive experiments demonstrate its superior performance across various LLM-generated texts and detection methods, in both standard and paraphrasing scenarios. Notably, SEFD is highly adaptable and compatible with different detectors. We this work can inspire future re-search toward developing effective, general-purpose solutions to mitigate AI misuse."}]}