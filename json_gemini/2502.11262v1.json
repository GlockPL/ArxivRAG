{"title": "Generating Skyline Datasets for Data Science Models", "authors": ["Mengying Wang", "Hanchao Ma", "Yiyang Bian", "Yangxin Fan", "Yinghui Wu"], "abstract": "Preparing high-quality datasets required by various data-driven AI and machine learning models has become a cornerstone task in data-driven analysis. Conventional data discovery methods typically integrate datasets towards a single pre-defined quality measure that may lead to bias for downstream tasks. This paper introduces MODis, a framework that discovers datasets by optimizing multiple user-defined, model-performance measures. Given a set of data sources and a model, MODis selects and integrates data sources into a skyline dataset, over which the model is expected to have the desired performance in all the performance measures. We formulate MODis as a multi-goal finite state transducer, and derive three feasible algorithms to generate skyline datasets. Our first algorithm adopts a \"reduce-from-universal\" strategy, that starts with a universal schema and iteratively prunes unpromising data. Our second algorithm further reduces the cost with a bi-directional strategy that interleaves data augmentation and reduction. We also introduce a diversification algorithm to mitigate the bias in skyline datasets. We experimentally verify the efficiency and effectiveness of our skyline data discovery algorithms, and showcase their applications in optimizing data science pipelines.", "sections": [{"title": "1 INTRODUCTION", "content": "High-quality machine learning (ML) models have become criti-cale assets for various domain sciences research. A routine task in data-driven domain sciences is to prepare datasets that can be used to improve such data science models. Data augmenta-tion [36] and feature selection [27] have been studied to sug-gest data for ML models [7]. Nevertheless, they typically gen-erate data by favoring a pre-defined, single performance goal, such as data completeness or feature importance. Such data may be biased and not very useful to actually improve the model performance, and moreover, fall short at addressing multiple user-defined ML performance measures (e.g., expected accuracy, training cost). Such need is evident in multi-variable experiment optimization [23, 30, 33], feature selection [27], and AI bench-marking [9], among others.\nDiscovering datasets that can improve a model over multiple user-defined performance measures remains to be desirable yet less studied issue. Consider the following real-world example."}, {"title": "2 MODELS AND PERFORMANCE EVALUATION", "content": "We start with several notations used in MODis framework.\nDatasets. A dataset $D(A_1, ..., A_m)$ is a structured table instance that conforms to a local schema $R_D(A_1, ..., A_m)$. Each tuple $t \\in D$ is a m-ary vector, where $t.A_i = a (i \\in [1,m])$ means the $i^{th}$ attribute $A_i$ of $t$ is assigned a value $a$. A dataset may have missing values at some attribute A (i.e., $t.A = 0$).\nGiven a set of datasets $D = {D_1,... D_n}$, each dataset $D_i$ con-firms to a local schema $R_i$. The universal schema $R_U$ is the union of the local schemas of datasets in D, i.e., a set of all the attributes involved in D. The active domain of an attribute A from $D_U$, denoted as $adom(A)$, refers to the finite set of its distinct values occurring in D. The size of adom(A), denoted as $|adom(A)|$, is the number of distinct values of A in D.\nModels. A data science model (or simply \"model\") is a function in the form of $M : D \\rightarrow R^d$, which takes as input a dataset D, and outputs a result embedding in $R^d$ for some $d \\in N$. Here R and N are real and integer sets. In practice, M can be a pre-trained machine learning model, a statistical model, or a simulator. The input D may represent a feature matrix (a set of numerical feature vectors), or a tensor (from real-world physical systems), to be used for a data science model M as training or testing data. The output embedding can be conveniently converted to task-dependent output (e.g., labels for classification, discrete cluster numbers for clustering, or Boolean values for outlier detection) with post-processing.\nFixed Deterministic models. We say a model M is fixed, if its com-putation process does not change for fixed input. For example, a regression model M is fixed if any factors that determines its inference (e.g., number of layers, learned model weights) remain fixed. The model M is deterministic if it always outputs the same result for the same input. We consider fixed, deterministic mod-els for the needs of consistent performance, which is a desired property in ML-driven data analytical tasks.\nModel Evaluation. A performance measure p (or simply \"mea-sure\") is a performance indicator of a model M, such as accuracy e.g., precision, recall, F1 score (for classification); or mean average error (for regression analysis). It may also be a cost measure such as training time, inference time, or memory consumption.\nWe use the following settings.\n(1) We unify P as a set of normalized measures to minimize, with a range (0, 1]. Measures to be maximized (e.g., accuracy) can be easily converted to an inversed counterpart (e.g., relative error).\n(2) Each measure $p \\in P$ has an optional range $[p_l, p_u] \\in (0,1]$. It specifies desired lower bound $p_l$ or an upper bound $p_u$ for model performance, such as maximum training or inference time, memory capacity, or error ranges.\nEstimators. A performance measure $p \\in P$ can often be efficiently estimated by an estimation model $\\varepsilon$ (or simply \"estimator\"), in PTIME in terms of |D| (the number of tuples in D). An estimator $\\varepsilon$ makes use of a set of historically observed performance of M"}, {"title": "3 SKYLINE DATASET GENERATION: A FORMALIZATION", "content": "Given datasets D, an input model M and a set of measures P, we formalize the generation process of a skyline dataset with a \"multi-goals\" finite state transducer (FST). An FST extends ex-tends finite automata by associating outputs with transitions. We use FST to abstract and characterize the generation of Skyline datasets as a data transformation process. We introduce this for-malization, with a counterpart for data integration [7, 26], to help us characterize the computation of skyline dataset generation.\nData Generator. A skyline dataset generator is a finite-state transducer, denoted as $T = (S_M, S, O, S_F, \\delta)$, where (1) S is a set of states, (2) $S_M \\in S$ is a designated start state, (3) O is a set of operators of types {$\\oplus$, $\\ominus$}; (4) $S_F$ is a set of output states; and (5) $\\delta$ refers to a set of transitions. We next specify its components.\nStates. A state s specifies a table $D_s$ that conforms to schema $R_s$ and active domains $adoms$. For each attribute $A \\in R_s$, $adoms (A) \\subseteq adom(A)$ refers to a fraction of values A can take at state s. $adoms (A)$ can be set as empty set $\\O$, which indicates that the attribute A is not involved for training or testing M; or a wildcard '_' ('don't care'), which indicates that A can take any value in $adom(A)$.\nOperators. A skyline data generator adopts two primitive polynomial-time computable operators, Augment and Reduct."}, {"title": "4 SKYLINE DATA GENERATION PROBLEM", "content": "Given T and a configuration C, MODis aims find a running of T that ideally leads to a \"global\" optimal dataset, where M is expected to deliver the highest performance over all metrics. Nevertheless, a single optimal solution may not always exist. First, two measures in P may in nature conflict due to trade-offs (e.g., training cost versus accuracy, precision versus recall). Moreover, the \"no free lunch\" theorem [39] indicates that there may not exist a single test that demonstrate best performance over all measures. We thus pursue Pareto optimality for DF. We start with a dominance relation below.\nDominance. Given a data discovery system T and performance measures P, a state s = ($D_s$, $R_s$, $adoms$) is dominated by s' = ($D_{s'}$, $R_{s'}$, $adoms'$), denoted as $s \\lessdot s'$, if there are valuated tests t = (M, $D_s$) and t' = (M, $D_{s'}$) in T, such that\n\\begin{itemize}\n    \\item for each $p\\in P$, $t'.p \\leq t.p$; and\n    \\item there exists a measure $p^* \\in P$, such that $t'.p^* < t.p^*$.\n\\end{itemize}\nA dataset $D_s$ is dominated by $D_{s'}$, denoted as $D_s \\lessdot D_{s'}$, if $s \\lessdot s'$.\nSkyline set. Given T and a configuration C, let $D_F$ be the set of all the possible output datasets from a running of T, a set of datasets $D \\subseteq D_F$ is a skyline set w.r.t. T and C, if\n\\begin{itemize}\n    \\item for any dataset $D \\in D$, and any performance measure $p\\in P$, there exists a test $t \\in T$, such that $t.p \\in [p_l, p_u]$;\n    \\item there is no pair {$D_1$, $D_2$} $\\subset D$ such that $D_1 \\lessdot D_2$ or $D_2 \\lessdot D_1$; and\n    \\item for any other $D \\in D_F \\setminus D$, and any $D' \\in D$, $D \\lessdot D'$.\n\\end{itemize}\nWe next formulate the skyline data generation problem.\nSkyline Data Generation. Given a skyline data generator T and its configuration C = ($S_M$, O, M, T, $\\varepsilon$), the skyline data generation problem is to compute a skyline set $D_F$ in terms of T and C."}, {"title": "5 COMPUTING SKYLINE SETS", "content": "We next present our first algorithm that generates a size-bounded set, which approximates a Skyline set in $D_F$. To characterize the approximation quality, we introduce a notion of $\\epsilon$-skyline set.\n$\\epsilon$-Skyline set. Given a data discovery system T with a configuration C, Let $D_s$ be a set of N valuated datasets in the running of T. Given a pair of datasets ($D$, $D'$) from $D_s$, and a constant $\\epsilon > 0$, we say D' $\\epsilon$-dominates D, denoted as $D' \\geq_{\\epsilon} D$, if for the corresponding tests t = (M, D) and t' = (M, D'),\n\\begin{itemize}\n    \\item $t'.p \\leq (1+\\epsilon)t.p$ for each $p\\in P$, and\n    \\item there exists a measure $p^* \\in P$, such that $t'.p^* < t.p^*$.\n\\end{itemize}\nIn particular, we call $p^*$ a decisive measure. Note that $p^*$ can be any $p\\in P$ and may not be fixed.\nA set of datasets $D_e \\subset D_s$ is an $\\epsilon$-Skyline set of $D_s$, if\n\\begin{itemize}\n    \\item for any dataset $D \\in D_e$, and any performance measure $p \\in P$, there exists a corresponding test $t \\in T$, such that $t.p \\in [p_l, p_u]$; and\n    \\item for every dataset $D' \\in D_s$, there exists a dataset $D \\in D_e$ such that $D \\geq_{\\epsilon} D'$.\n\\end{itemize}\n(N, $\\epsilon$)-approximation. We say an algorithm is an (N,$\\epsilon$)-approximation for MODis, if it satisfies the following:\n\\begin{itemize}\n    \\item it explores and valuates at most N states;\n    \\item for any constant $\\epsilon > 0$, the system correctly outputs an $\\epsilon$-Skyline set, as an approximation of a Skyline set defined over N valuated states; and\n    \\item the time cost is polynomial determined by |D|, N, and $\\frac{log(p_m)}{\\epsilon}$.\n\\end{itemize}\nTHEOREM 1. Given datasets D, configuration C, and a number N, there exists an (N,$\\epsilon$)-approximation for MODis in time $O(\\frac{1}{\\epsilon^{|P|-1}} \\cdot N \\cdot |D| \\cdot log(\\frac{log(p_m)}{\\epsilon}))$, where $|R_U|$ is the size of the universal schema, $N_u = |R_U| + |adomm|$ ($adomm$ the largest active domain), $p_m = max(P)$ as the measure p ranges over P; and I is the unit valuation cost per test.\nRemarks. The above result captures a relative guarantee w.r.t. $\\epsilon$ and N. When N = $|D_F|$, an (N, $\\epsilon$)-approximation ensures to output a $\\epsilon$-Skyline set. The paradigm is feasible as one can ex-plicitly trade the 'closeness' of the output to a Skyline set with affordable time cost, by explicitly tuning $\\epsilon$ and N. Moreover, the worst-case factor $|adomm|$ can also be \"tightened\" by a bound determined by the value constraints posed by the literals. For ex-ample, an attribute A may contribute up to two necessary values in the search if the literals involving A only enforce two equality conditions \"A=2\" and \"A=5\", regardless of how large $|adom(A)|$ is (see Sections 3 and 6).", "5.2 Approximation Algorithm": "As a constructive proof of Theorem 1, we next present an (N, $\\epsilon$)-approximation algorithm, denoted as ApxMODis.\n\"Reduce-from-Universal\". Algorithm ApxMODis simulates the running of T from a start state $s_u$. The start state is initialized with a \"universal\" dataset $D_u$, which carries the universal schema $R_U$, and is populated by joining all the tables (with outer join to preserve all the values besides common attributes, by default). This is to enforce the search from a set of rows that preserve all the attribute values as much as possible to maximize the chance of"}, {"title": "5.3 Bi-Directional Skyline Set Generation", "content": "Given our cost analysis, for skyline data generation with larger (more \"tolerate\") ranges ($p_l$, $p_u$) and larger |D|, ApxMODis may still need to valuate a large number of datasets. To further reduce valuation cost, we introduce BiMODis, its bi-directional variant. Our idea is to interact both augment and reduct operators, with a \"forward\" search from universal dataset, and a \"backward\" coun-terpart from a single dataset in D. We also introduce a pruning strategy based on an early detection of dominance relation.\nAlgorithm. Algorithm BiMODis, as shown in Fig. 5, has the fol-lowing steps. (1) Initialization (lines 3). It first invokes a pro-cedure BackSt to initialize a back-end start state node $s_b$. Two queues $Q_f$ and $Q_b$ are initialized, seeded with start state $s_u$ for forward search, and a back state $s_b$ for backward search, respec-tively. They serve as the forward and backward frontiers, respec-tively. (2) Bi-directional Search (lines 4-13). BiMODis conducts an exploration from both directions, controlled by $Q_f$ for forward search, and $Q_b$ for backward search. Similar to ApxMODis, a Sky-line set $D_F$ is maintained in a levelwise manner. The difference is that it invokes a revised procedure OpGen (with original coun-terpart in ApxMODis in Fig. 3), which generates reduct operators for the forward search, and augment operators for the backward search. The search process terminates when both $Q_f$ and $Q_b$ are empty, or when a path is formed, the result $D_F$ is returned.\nProcedure BackSt. This procedure initializes a backend dataset $D_b$ for augmentation. This procedure can be tailored to the specific task. For example, for a classifier M with input features and a"}, {"title": "5.4 Diversified Skyline Dataset Generation", "content": "A Skyline dataset may still contain data that largely overlap or are similar, hence leading to bias and reducing the generality of the model if adopted. This may occur due to skewed value distri-bution in the active domains, common attributes, over specific performance metrics in the skyline data generation process. It is"}, {"title": "6 EXPERIMENT STUDY", "content": "We next experimentally verify the efficiency and effectiveness of our algorithms. We aim to answer three questions: RQ1: How well can our algorithms improve the performance of models in multiple measures? RQ2: What is the impact of generation settings, such as data size? RQ3: How fast can they generate skyline sets, and how scalable are they? We also illustrate the applications of our approaches with case studies\u00b9.\nDatasets. We use three sets of tabular datasets: kaggle [21], OpenData [1], and HF [19] (summarized in Table 2).\nTasks and Models. A set of tasks are assigned for evaluation. We trained: (1) a Gradient Boosting model (GBmovie) to predict movie grosses using Kaggle for Task T\u2081; (2) a Random Forest model (RFhouse) to classify house prices using OpenData with the same settings in [14] for Task T2; and (3) a Logistic Regression model (LRavocado) to predict Avocado prices using HF for Task T3. (4) a LightGBM model (LGCmental) [22] to classify mental health status using Kaggle for Task T4. We also introduced task T5, a link regression task for recommendation. This task takes as input a bipartite graph between users and products, and links indicate their interaction. A LightGCN [17] (LGRmodel), a variant"}, {"title": "7 CONCLUSION", "content": "We have introduced MODis, a framework that generate skyline datasets to improve data science models on multiple performance measures. We have formalized skyline data generation with trans-ducers equipped with augment and reduction operators. We show the hardness and fixed-parameter tractability of the problem. We have introduced three algorithms that compute approximate Sky-line sets in terms of $\\epsilon$-Skyline set, with reduce-from-universal, bi-directional, and diversification paradigms. Our experiments have verified their effectiveness and efficiency. A future topic is to enhance MODis with query optimization techniques to scale it for larger input with high-dimensional data. Another topic is to extend MODis for distributed Skyline data generation."}, {"title": "Appendix A: Algorithms and Proof", "content": "A.1 ApxMODis\nProof of Lemma 2 For any constant $\\epsilon$, ApxMODis correctly\ncomputes an $\\epsilon$-Skyline set $\\Pi$ that approximates a Skyline set defined\non the N states it valuated.\nPROOF. We establish the $\\epsilon$-approximability of ApxMODis by\nconstructing a reduction from MODis to the multi-objective\nshortest path problem (MOSP) [40].\nReduction. An instance of MOSP consists of an edge-weighted\ngraph $G_W$, where each edge $e_w$ is assigned a d-dimensional at-\ntribute vector $e_w.c$. The cost of a path $p_w$ in $G_W$ is defined as\n$p_w.c = \\sum_{e_w \\in p_w} e_w.c$. The dominance relation between two paths\n$p_w$ and $p'_w$ is determined by comparing their costs. Specifically,\n$p_w$ dominates $p'_w$ if $p_w$ has equal or lower costs than $p'$ in all\ndimensions and is strictly better in at least one dimension. The\nobjective is to compute a Skyline set of paths from a start node $u$\nto all other nodes in the graph.\nWe construct the reduction from our problem to MOSP. (1) We\ndefine $G_w$ as an edge weighted counterpart of a running graph\n$G_r$. (a) Each vertex in $G_r$ represents a unique state s during\nthe execution of ApxMODis, with each state corresponding to a\nspecific dataset configuration in the data discovery process. The\ngraph $G_r$ contains N vertices, corresponding to the N states\nthat ApxMODis has spawned and valuated. (b) Each edge $(s, s')$\nin $G_r$ represents a transition from state s to state $s'$, resulting\nfrom applying an operation (e.g., reduction or augmentation) that\nmodifies the dataset. The edge is weighted by the difference in\nperformance measures in P between the two states: $e_w = s.P -$\n$s'.P$. Here, s.P and $s'.P$ are the performance vectors of the states\ns and $s'$, respectively. The edge weight $e_w$ is a d-dimensional\nvector that quantifies how the performance metrics change as a\nresult of the transition. A path $p \\in G_r$ corresponds to a sequence\nof transitions between states, starting from the initial state $s_u$.\nSimilar to $p_w \\in G_w$, the cumulative cost of this path $p.c$ is defined\nas the sum of the edge weights along the path, which represents\nthe cumulative change in the performance measures P as the\ndataset evolves through different states.\nGiven a solution $\\Pi_w$ of an instance of MOSP, which is an $\\epsilon$-\nSkyline set of paths, we construct a solution for a corresponding\ninstance of MODis. For each path $p_w \\in \\Pi_w$, we establish a\ncorresponding path p in $G_r$ and identify the final state s that the\npath reaches. The final state s corresponds to a specific dataset D,\nwhich is the result of applying the sequence of operations from\np. We then include D in the set $D_F$. This forms a set of datasets\nas the solution to MODis.\nWe next prove that $\\Pi_w$ is an $\\epsilon$-Skyline set of paths $\\Pi_w$ in $G_w$,\nif and only if $D_F$ is an $\\epsilon$-Skyline set of $D_s$.\nIf condition. Let $D_F$ be an $\\epsilon$-Skyline set of $D_s$. By the definition\ngiven in sec 4, this means that for every dataset $D' \\in (D_s \\setminus D_F)$,\nthere exists at least one dataest $D \\in D_F$ such that D $\\epsilon$-dominates\nD'. Specifically, this means that D has costs that are at most\n(1 + $\\epsilon$) times the costs of D' in all performance measures in P,\nand D has a strictly lower cost in at least one measure. From\nthe reduction, each path $p_w \\in \\Pi_w$ corresponds to a sequence of\ntransitions in $G_r$ leading to a final state s, which represents a\ndataset $D \\in D_F$. Similarly, $p'_w \\notin \\Pi_w$ corresponds to a dataset\nD' $\\in$ ($D_s \\setminus D_F$). Since D $\\epsilon$-dominates D', the corresponding\npath $p_w$ $\\epsilon$-dominates $p'_w$. This dominance is preserved because\nthe performance measures P, directly corresponding to the edge"}]}