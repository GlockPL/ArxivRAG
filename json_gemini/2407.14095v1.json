{"title": "People use fast, goal-directed simulation to reason about novel games", "authors": ["Cedegao E. Zhang", "Katherine M. Collins", "Lionel Wong", "Adrian Weller", "Joshua B. Tenenbaum"], "abstract": "We can evaluate features of problems and their potential solutions well before we can effectively solve them. When considering a game we have never played, for instance, we might infer whether it is likely to be challenging, fair, or fun simply from hearing the game rules, prior to deciding whether to invest time in learning the game or trying to play it well. Many studies of game play have focused on optimality and expertise, characterizing how people and computational models play based on moderate to extensive search and after playing a game dozens (if not thousands or millions) of times. Here, we study how people reason about a range of simple but novel connect-n style board games. We ask people to judge how fair and how fun the games are from very little experience: just thinking about the game for a minute or so, before they have ever actually played with anyone else, and we propose a resource-limited model that captures their judgments using only a small number of partial game simulations and almost no lookahead search.", "sections": [{"title": "Introduction", "content": "Consider the two-player game shown in the top left of Fig. 1, where players alternate placing their pieces in grid squares and winning means being the first to connect 3 pieces in a row on a 5 by 5 board. Though you may be familiar with related games like Tic-Tac-Toe or Connect-4, chances are you have not played this particular game before. Still, with just a moment's thought, you can likely evaluate some basic but important aspects of what it would be like to actually play this game against a reasonable opponent-how many moves would it take for the game to finish? Would you rather go first or second? Would this be fun to play with another novice? Think for a little longer, and if you hadn't already, you might realize that the game is actually very biased in favor of the first player, and perhaps not fun at all to play for more than a few rounds. You could probably evaluate any of the games at the top of Fig. 1 in the same way, and you likely would think about these kinds of questions before deciding to master one of these games as a hobby, or to bet money on their outcomes.\nThe formal and empirical study of games often focuses on optimality and expertise. Game theory in mathematical economics, as studied by Nash (1950) for example, largely seeks to characterize equilibrium states reached under rational play. Computational game playing models, from classic AI expert systems like Deep Blue to more recent neurally guided models like AlphaGo and AlphaZero (Silver et al., 2016, 2017, 2018), aim to match or exceed the abilities of human experts, generally by evaluating millions of simulated games. A rich body of related cognitive science research documents how human experts play after extensive experience on a given game (Newell & Simon, 1972; Gobet et al., 2004).\nExperts seem to learn highly specialized and game-specific representations, like memorized chess states and openings, that enable them to effectively search or approximate the results of searching much further ahead in a game than novice players (Chase & Simon, 1973; van Opheusden et al., 2023).\nBut we simply aren't experts on most problems we encounter. While we might master a few domains over a lifetime, the sheer range of environments and tasks we take on also seems to demand capacities for efficiently but intelligently evaluating a much broader range of novel problems under limited experience. How do we predict, for instance, whether a given goal or cooperative task will be tractable, rewarding, or fair enough to invest time and resources in solving it more completely?\nThe present study focuses on this higher-level capacity to flexibly evaluate novel inference and decision making problems, using a set of novel strategy games that vary the environment, dynamics, and win conditions of more familiar grid games as a test bed for general problem evaluation. We propose a computational model (Fig. 1b) that implements novice but goal-directed agents as fast but search-limited planners. Our model estimates the value of intermediate game states based on simple but general concepts about game play, such as making progress towards a sparse overall goal (like making M in a row) and preventing opponent progress when in a competitive setting. We then model how people evaluate games overall by nesting this agent model within a sample-based inference procedure that estimates game outcomes from a limited number of partial game simulations. While we focus here on games, the underlying building blocks of this \"intuitive game theory\" model are designed to capture more basic multi-agent planning and probabilistic inference components applicable to a much broader range of problems-where people are called on to make good guesses and good bets in novel situations, and to evaluate their prospects of success along with the emotional reward of engaging (Allen et al., 2024).\nTo test our model, we ask human subjects to evaluate game outcomes for our novel game stimuli, such as how likely the first player is to win or draw in any give game, and to predict how fun a game would be for people to play. We allow participants to use an interactive scratchpad board in which they can place pieces during the experiment, collecting data on when and how they may choose to simulate games against imaginary opponents.\nWe find that our intuitive game theory model, despite using only a single step of search in its agent model and a small number of partial game simulations to estimate game play, well predicts participants' judgements of game outcomes on novel games. We compare our model against several alternative models, which either (i) consider even more naive game agent models, or conversely (ii) expend much more computational effort on search and game simulation to more optimally predict outcomes. We also compare against two language-model-based baselines with no explicit agent model that reason solely from background knowledge about other more familiar games.\nUsing our model, we also show that several game-related factors correlate with human fun ratings on these games: a fairness factor based on estimated entropy over game outcomes; a challengingness factor based on the estimated advantage our model has in playing against a random agent; and a fun factor based on language model predictions. Combined into a joint regression model, these factors together can predict most of the variance in human fun judgements."}, {"title": "Intuitive Game Theory Computational Model", "content": "How do we quickly evaluate key aspects of a novel problem-such as whether it is potentially rewarding or hugely unfair-before investing time and effort to acquire longer-term expertise? Even for the relatively simple grid games we study here, estimating potential rewards under optimal or expert-like game play presents several related computational challenges.\nTo choose any single action, an optimal agent model should estimate and then seek to maximize downstream utility with respect to all available actions from the current game state. Many general game play models estimate utility for novel games using expensive lookahead search, ideally to terminal reward-generating game states (Genesereth & Thielscher, 2014; Yannakakis & Togelius, 2018). For instance, the class of games we consider here defines very sparse utility functions-winning or losing is only determined based on a final state in which a player or their opponent has placed $M$ pieces in a row. In cases where $M$ is large or players are playing on large boards (e.g., the goal is to make 8 in a row on a 20 by 20 board), this means searching over potentially long trajectories of game play to evaluate any single action.\nTo then draw probabilistic judgements about game outcomes (such as how likely a given non-deterministic player is to win on average), generic sample-based inference based on this agent model would require running multiple full game simulations, wherein each agent action would require downstream search to these terminal states.\nIn contrast, our intuitive game theory model aims to produce fast, probabilistic inferences over a broad range of novel games. This model simulates agent game play based on basic but general assumptions about goal-oriented players that can be estimated directly from intermediate states, then draws graded judgements based on partial game simulations over a range of game depths rather than always simulating to terminal states."}, {"title": "Game specifications and game reasoning queries", "content": "We begin by defining a general notion of an intuitive game theory problem that encompasses the varying questions we seek to evaluate (e.g., how likely is this game to end in a draw?) on arbitrary games. We formalize a game specification as comprising an environment definition (e.g., the board shape), a game state transition function that defines the game dynamics (e.g., alternating actions over two players, and terminal states based on the win or draw conditions), and game utility functions defined for each player given game state, which in our dataset are only sparsely defined over terminal states. This formalization derives from standard multi-agent planning problems definitions used in the AI planning literature (e.g., Russell & Norvig (2020)).\nA problem is then a game specification combined with any general game reasoning query about that game."}, {"title": "Estimating game outcomes by simulating goal-directed but search-limited players", "content": "The core of our model is a simulated agent that can play from arbitrary game specifications. This agent model has a general search-limited way of placing utility over intermediate states.\nWe design the agent model around a collection of utility functions which capture general intuitions about goal-directed competitive game play, and can be quickly evaluated over any intermediate board state with at most a single step of lookahead search in the class of games we consider here. Together, these utility functions are closely related to features in models from a long tradition of works that study game-playing in specific \u201cm-in-a-row\" games (Amir et al., 2022; Crowley & Siegler, 1993; van Opheusden et al., 2023), which are often descriptively derived from empirical observations of game play. Here, we generalize these game-specific features into a model applicable across the entire, broad family of games we consider in this paper, and also ground each in more general intuitions about game play. The general rationales beyond the features also admit other utility function formulations that could be explored in future work.\nSpecifically, given an intermediate game state, our player model assigns an overall utility score to each possible next move (each open position on the board) based on the following functions:\n1. The normalized Euclidean distance of the position and the center of the board, $d \\in [0,1]$. This reflects the intuition, applicable across our family of games, that people often but not always place pieces around the center at the beginning. This function can be explained based on a more general rationale-agnostic to other aspects of the current game state, placing pieces closer to the center of the board allows that piece to participate in the most possible winning terminal states for any m-in-a-row win condition.\n2. The maximum number of connected contiguous pieces that the position entails on any of the allowed winning directions (horizontal, vertical, or diagonal in general-modulated with respect to any player-specific restrictions on win directions), $n_1 \\in Z^+$. If $n_1$ equals the winning $m$ form in a row, we add an additional 1 to $n_1$. This function captures the general intuition that players are goal-directed and try to make progress towards winning\u2014that is, that players derive utility from intermediate states towards the most rewarding terminal state. While there are many possible subgoals that a player could consider which might make partial progress towards terminal states (e.g., placing making 3 in a row by placing two unconnected pieces and then placing one more piece in the center), we choose a simple, easy to calculate function that also follows from players following an intuitive standard policy (making m in a row by previously building up to m \u2013 1 in a row, m-2 in a row, and so on via contiguous pieces). We also only consider the maximum number of pieces to reflect a relatively naive, shallow player who is considering a single, simple policy towards the nearest victory. This function could easily be augmented to support a number of related functions that reflect partial progress towards multiple terminal rewarding states.\n3. Relatedly, the maximum number of connected contiguous pieces that the position blocks the opponent from having, $n_2 \\in Q^+$, also considered with respect to the win-directions applicable to the opponent. This function captures the intuition that competitive players in antagonistic two-player games should also attempt to block opponent progress towards opponent goals. Note that this function is simple, symmetric to our definition of player progress (because players in our games have related and opposing goals). It also effectively evaluates just a single step of lookahead search relative to a opponent who is also relatively naive (that is, an opponent who at the next step would simply attempt to make progress towards their nearest victory along a simple policy). We subtract 0.5 from $n_2$ to reflect people's tendency to weigh offense over defense (Crowley & Siegler, 1993), so blocking the opponent's m in a row is not as good as making an m in a row for oneself (but is better than making an m \u2013 1 in a row). However, if $n_2$ equals the winning m, we do not subtract 0.5. One could consider other related or more complex formulations that also capture the notion of blocking opponent progress.\nThese components together reflect the intuitions that people often but not always place pieces around the center at the beginning, people naturally try to make progress towards winning, and people know they should secure winning moves and block the opponent from having those. The utility of a position p is then given by an exponential function:\n$V(p) = 2^{(1-d)+n_1+n_2)} $.\nWe apply a softmax (temperature = 1) to the set of position utilities to obtain a probability distribution over possible moves, and the agent samples a move from this distribution.\nGiven this model, how do we draw graded inferences about how fair a game is? For each given game, the model runs k simulations of two players playing the game. Here we set k = 20, building upon the idea that people use a relatively small number of simulations to form beliefs and make decisions (Vul et al., 2014; Icard, 2016). The model does not necessarily simulate until a terminal state of the game (i.e., a win, loss, or draw happens). Instead, the model randomly samples a number of moves it would play out from 1 to the size of the game board, and evaluates the game outcome based on the board state only up to that maximum number of moves (if a terminal win state hasn't been reached, we evaluate the board as a draw). This reflects the notion that one could stop a simulation after making a few moves, stop in the middle, or play until the end, and such behaviors are observed in the human scratchpad usage data. Given the numbers of win, loss, and draw outcomes in these 20 move-capped simulations, we can compute the expected payoff for the game (with scores 1 for win, -1 for loss, and 0 for draw)."}, {"title": "Human and Model Experiments", "content": "We conduct an experiment to test whether our model explains patterns of human reasoning about a variety of two-player grid-based games. We design n = 121 grid game specifications which vary the game environment (including square boards varying from 3\u00d73 to 10\u00d710; rectangular boards such as 2\u00d75 and 4\u00d79; and boards specified over an infinite grid); the game dynamics (including games in which a given player opens by going twice); and the player utility functions (including games in which the first or second player has differing win conditions, like requiring the first player to make a larger row than the second one.)"}, {"title": "Human game evaluations and game construction", "content": "We carry out a two-part study to evaluate how people reason about these novel games, and begin to to probe how humans create their own novel game variants to satisfy general game criteria (like constructing a reasonably fun game).\nParticipants We recruit 484 participants from Prolific (Palan & Schitter, 2018). Each participant was randomly presented with 10 games sampled from our stimuli, as well as regular Tic-Tac-Toe (won by making 3 in a row on a 3 \u00d7 3 board) to normalize game judgements. We collect approximately 20 judgements per game stimuli for each game reasoning query. Participants were paid at a base rate of $12.5/hr with an optional bonus up to $15/hr; the full experiment approximately took 25 minutes.\nGame outcome and game fun judgments Subjects were randomly assigned to one of two game reasoning conditions. In the game outcome evaluation condition, subjects produced judgements on a continuous 0-100 probability scale to predict the likelihood of a first player win (if the game does not end in a draw, assuming both players play reasonably, how likely is it that the first player is going to win (not draw)?) and a draw (assuming both players play reasonably, how likely is the game to end in a draw?). In the game fun condition, subjects instead assessed the likelihood that the game is fun (how fun is this game?) on a confidence scale spanning 0 (the least fun of this class of game) to 100 (the most fun of this class of game).\nParticipants produced judgements about each game based on a linguistic game specification. We additionally provided participants with an interactive scratchpad board that they were told they could, but were not required, to use to inform their judgements; the scratchpad permitted automatically placing pieces of different colors (to simulate players) and could be cleared entirely to begin a new game. Participants were required to consider each game for at least 60 seconds before producing game judgements; on average, participants took 87.2 (\u00b12.5 STE) seconds per game in the game outcome condition and 83.4 (\u00b1 2.41 STE) seconds per game in the fun rating condition.\nNovel game creation After answering all game reasoning queries, we additionally asked participants to create a new grid-based game variant that they would find fun. Participants wrote a linguistic game specification, describing the board size and win conditions. As in the game judgement queries, participants were again provided an optional scratchpad and required to spend 60 seconds before submitting a response. The scratchpad enabled participants to try out the game they intend to create. After specifying a game, participants were asked to answer the same game reasoning query (either game outcomes or game fun) about their own game."}, {"title": "Alternative computational models", "content": "We implement a range of alternative computational models designed to span both weaker estimates of game play (using more naive agent models, or without any explicit agent models) and more optimal estimates of game play (using additional search and game simulations to terminal states).\nPartial game simulations with random agent This baseline estimates outcomes using partial game simulations, but substitutes our sub-goaling agent model with a more naive (\"random\") agent model that selects actions from a uniform distribution over valid moves.\nFull game simulations with random agent This baseline infers outcomes using complete game simulations to terminal states, but using the random agent model.\nFull game simulations with depth-5 lookahead search This baseline infers outcomes using complete game simulations to terminal states, and chooses moves based on future state utilities (under our sub-goaling utility metric) using a depth-5 lookahead search (i.e., we model alternating agent play using our model up to 5 actions ahead, then choose actions based on utility estimates under our subgoaling function at the future state). The search-based agent model is inspired by the finding in van Opheusden et al. (2023), which suggests that experts playing a specific Connect-4 grid game are well-modeled by depth-5 search.\nFull game simulations with Monte Carlo Search (MCS) This baseline infers outcomes using complete game simulations to terminal states. The agent employs a standard MCS utility estimator (Genesereth & Thielscher, 2014), which runs full game simulations under a random agent model to estimate action utilities and then selects moves from the softmax distribution over these estimates. MCS is often used to approximate oracle, near-optimal game utility estimates with many simulations; we use k = 20 simulations per move.\nLarge Language Model (LLM) game evaluations We also implement two LLM baselines to evaluate game reasoning with no explicit agent model, but rather, models which can draw on background knowledge about classic grid games and variants (including extensive information, for instance, about the optimal solutions to games like Tic-Tac-Toe and Connect-4). We use GPT-4 (OpenAI, 2023) as our base LLM and implement two LLM-based game evaluation baselines: a zero-shot baseline (which is directly queried with the linguistic game specification and game reasoning query), and a zero-shot chain-of-thought (CoT) baseline (Kojima et al., 2022; Wei et al., 2022), which is first prompted to produce additional reasoning in text before answering a query (a standard method used to account for the computational limits in directly producing answers to questions given the LLM architecture). In both baselines, we prompt the model with a lightly-modified variant of the full experiment instructions shown to human participants, and collect judgements on the same 0-100 scale for all queries."}, {"title": "Results and Discussion", "content": "Here we report our findings and discuss their interpretations.\nPeople's game outcome judgements are best predicted by partial, goal-directed game simulations. Subjects provided judgements estimating both the likelihood of a draw, and the likelihood that the first player would win if the game did not end in the draw, which collectively estimate the distribution and expected utility over all possible outcomes (draw, first player wins, second player wins) for any given game. Fig. 3 shows correlations between human estimations of game payoffs (expected utility, where -1 is a loss and 1 is a win)-aggregated across all participants and game stimuli and our model (Ours, blue) as well as the alternative baselines. As shown in Fig. 3, our model correlates well with human estimates (R2 = 0.852, p < 5e-4).\nNotably, our model is a much better fit to human predictions than both the more naive models (such as the random partial simulation model, orange), and the more optimal models (both the depth-5 lookahead search, red, and MCS, pink). In particular, the game outcome predictions under the MCS baseline reveal that there is essentially no uncertainty about game outcomes under optimal play. That is, two true experts, or an agent estimating play from many more simulations, would expect to draw, or win based on their player ordering, every time. A qualitative analysis of human scratchpad data supports the conclusion that while most subjects spontaneously do seem to simulate games, they draw game estimates from very few games (almost no participants reset and then reused the scratchpad more than 5 times), and imperfect game simulations (see examples in Fig. 2).\nFig. 4 shows mean absolute error between the predicted payoff under models and human judgements on games, further split among the subcategories of our novel game variants. As seen in Fig. 4, our model (blue) is close (and usually closest) to human judgements within each of these subcategories. We also find that the LLM-based models (both the zero-shot and chain-of-thought baselines) are in fact relatively close to human judgements for the games that are the most obviously related to Tic-Tac-Toe (the M in a row games on a square N \u00d7 N board), but deviate much more from human judgements on seemingly minor variants\u2014M in a row games on rectangular boards or infinite boards possibly because these games are relatively out-of-distribution compared to the more familiar variants found in background linguistic data.\nPeople's game fun judgements can be predicted by their own and our model's game outcome estimates. We evaluate several game-play-based features which we find each correlate with subject's game fun estimates: 1) an estimate of whether games are fair and balanced based on the entropy over game outcomes-both predicted by humans themselves and predicted under our model; 2) an estimate of whether a game is reasonably challenging based on relative advantage between our model and a random agent (where expected utility is computed given random assignment of player order); and 3) expected game length. Interestingly, we also find the LLM-based CoT estimates of game fun to be well-correlated with human judgments. Fig. 5 summarizes these individual features, as well as fits from regression models fit to our model-derived features (and including the additional LLM-based prediction).\nPeople spontaneously invent fair games when asked to invent fun games. Finally, subject self-evaluations of their own invented game variants further supports the relationship between game outcomes and game fun: human evaluations suggest that people mostly generate games they believe are fair. That is, participants endorse an unbiased expected payoff between players, when asked to generate games that they also self-evaluate as fun."}, {"title": "Conclusion and future directions", "content": "Our findings demonstrate how people might make quick, probabilistic judgments about the expected value of complex novel problems\u2014well before they learn narrower, task-specific strategies and representations that constitute genuine expertise-by integrating efficient, general-purpose computations for simulating multi-agent decision making and approximate probabilistic inferences. We see many avenues for future work.\nOne is the question of how people grasp the constraints and goals of novel tasks, in order to reason about them. Here we assumed that people understood the linguistic descriptions of the games we presented them with, but this is a nontrivial cognitive process in its own right. Although large language models were limited in their ability to reason about new games as people do, they have proven capable of semantic parsing and program synthesis, and we see opportunities for modeling language-based game understanding using LLMs together with probabilistic planning and decision models like our intuitive game theory models, in a neurosymbolic framework (Austin et al., 2021; Chen et al., 2021; Collins et al., 2022; Wong et al., 2023).\nHere we also only modeled the earliest stages of playing and reasoning about novel games. It is plausible that people's skill in these games will rapidly increase as they play more. A deeper understanding of such fast concept and skill acquisition would be highly valuable (Lake et al., 2017). More broadly, the study of how people quickly learn to play novel games could contribute to contemporary work on resource rationality (Gershman et al., 2015; Lieder & Griffiths, 2020; Icard, 2023) and human efficient planning (Callaway et al., 2022; Ho et al., 2022). It is an open question to what extent our current model can properly be called \"resource rational\", or indeed, to what extent and in which ways naive humans approximate answers to these kinds of intuitive questions using \"resource rational\u201d representations. Overall, we believe general principles of efficient use of cognitive resources can be further applied to studying games, novel problem solving, and beyond."}]}