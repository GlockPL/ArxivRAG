{"title": "People use fast, goal-directed simulation to reason about novel games", "authors": ["Cedegao E. Zhang", "Katherine M. Collins", "Lionel Wong", "Adrian Weller", "Joshua B. Tenenbaum"], "abstract": "We can evaluate features of problems and their potential solutions well before we can effectively solve them. When considering a game we have never played, for instance, we might infer whether it is likely to be challenging, fair, or fun simply from hearing the game rules, prior to deciding whether to invest time in learning the game or trying to play it well. Many studies of game play have focused on optimality and expertise, characterizing how people and computational models play based on moderate to extensive search and after playing a game dozens (if not thousands or millions) of times. Here, we study how people reason about a range of simple but novel connect-n style board games. We ask people to judge how fair and how fun the games are from very little experience: just thinking about the game for a minute or so, before they have ever actually played with anyone else, and we propose a resource-limited model that captures their judgments using only a small number of partial game simulations and almost no lookahead search.", "sections": [{"title": "Introduction", "content": "Consider the two-player game shown in the top left of Fig. 1, where players alternate placing their pieces in grid squares and winning means being the first to connect 3 pieces in a row on a 5 by 5 board. Though you may be familiar with related games like Tic-Tac-Toe or Connect-4, chances are you have not played this particular game before. Still, with just a moment's thought, you can likely evaluate some basic but important aspects of what it would be like to actually play this game against a reasonable opponent-how many moves would it take for the game to finish? Would you rather go first or second? Would this be fun to play with another novice? Think for a little longer, and if you hadn't already, you might realize that the game is actually very biased in favor of the first player, and perhaps not fun at all to play for more than a few rounds. You could probably evaluate any of the games at the top of Fig. 1 in the same way, and you likely would think about these kinds of questions before deciding to master one of these games as a hobby, or to bet money on their outcomes.\nThe formal and empirical study of games often focuses on optimality and expertise. Game theory in mathematical eco-nomics, as studied by Nash (1950) for example, largely seeks to characterize equilibrium states reached under rational play. Computational game playing models, from classic AI expert systems like Deep Blue to more recent neurally guided models like AlphaGo and AlphaZero (Silver et al., 2016, 2017, 2018), aim to match or exceed the abilities of human experts, generally by evaluating millions of simulated games. A rich body of related cognitive science research documents how human experts play after extensive experience on a given game (Newell & Simon, 1972; Gobet et al., 2004).\nExperts seem to learn highly specialized and game-specific representations, like memorized chess states and openings, that enable them to effectively search or approximate the results of searching much further ahead in a game than novice players (Chase & Simon, 1973; van Opheusden et al., 2023).\nBut we simply aren't experts on most problems we en-counter. While we might master a few domains over a lifetime, the sheer range of environments and tasks we take on also seems to demand capacities for efficiently but intel-ligently evaluating a much broader range of novel problems under limited experience. How do we predict, for instance, whether a given goal or cooperative task will be tractable, rewarding, or fair enough to invest time and resources in solving it more completely?\nThe present study focuses on this higher-level capacity to flexibly evaluate novel inference and decision making problems, using a set of novel strategy games that vary the environment, dynamics, and win conditions of more familiar grid games as a test bed for general problem evaluation. We propose a computational model (Fig. 1b) that implements novice but goal-directed agents as fast but search-limited planners. Our model estimates the value of intermediate game states based on simple but general concepts about game play, such as making progress towards a sparse overall goal (like making M in a row) and preventing opponent progress when in a competitive setting. We then model how people evaluate games overall by nesting this agent model within a sample-based inference procedure that estimates game outcomes from a limited number of partial game simulations. While we focus here on games, the underlying building blocks of this \"intuitive game theory\" model are designed to capture more basic multi-agent planning and probabilistic inference com-ponents applicable to a much broader range of problems-where people are called on to make good guesses and good bets in novel situations, and to evaluate their prospects of success along with the emotional reward of engaging (Allen et al., 2024).\nTo test our model, we ask human subjects to evaluate game outcomes for our novel game stimuli, such as how likely the first player is to win or draw in any give game, and to predict how fun a game would be for people to play. We allow participants to use an interactive scratchpad board in which they can place pieces during the experiment, collecting data on when and how they may choose to simulate games against imaginary opponents.\nWe find that our intuitive game theory model, despite using only a single step of search in its agent model and a small number of partial game simulations to estimate game play, well predicts participants' judgements of game outcomes on novel games. We compare our model against several alternative models, which either (i) consider even more naive game agent models, or conversely (ii) expend much more computational effort on search and game simulation to more optimally predict outcomes. We also compare against two language-model-based baselines with no explicit agent model that reason solely from background knowledge about other more familiar games.\nUsing our model, we also show that several game-related factors correlate with human fun ratings on these games: a fairness factor based on estimated entropy over game outcomes; a challengingness factor based on the estimated advantage our model has in playing against a random agent; and a fun factor based on language model predictions. Com-"}, {"title": "Intuitive Game Theory Computational Model", "content": "How do we quickly evaluate key aspects of a novel problem-such as whether it is potentially rewarding or hugely unfair-before investing time and effort to acquire longer-term exper-tise? Even for the relatively simple grid games we study here, estimating potential rewards under optimal or expert-like game play presents several related computational challenges.\nTo choose any single action, an optimal agent model should estimate and then seek to maximize downstream utility with respect to all available actions from the current game state. Many general game play models estimate utility for novel games using expensive lookahead search, ideally to terminal reward-generating game states (Genesereth & Thielscher, 2014; Yannakakis & Togelius, 2018). For instance, the class of games we consider here defines very sparse utility functions-winning or losing is only determined based on a final state in which a player or their opponent has placed M pieces in a row. In cases where M is large or players are playing on large boards (e.g., the goal is to make 8 in a row on a 20 by 20 board), this means searching over potentially long trajectories of game play to evaluate any single action.\nTo then draw probabilistic judgements about game out-comes (such as how likely a given non-deterministic player is to win on average), generic sample-based inference based on this agent model would require running multiple full game simulations, wherein each agent action would require downstream search to these terminal states.\nIn contrast, our intuitive game theory model aims to produce fast, probabilistic inferences over a broad range of novel games. This model simulates agent game play based on basic but general assumptions about goal-oriented players that can be estimated directly from intermediate states, then draws graded judgements based on partial game simulations over a range of game depths rather than always simulating to terminal states."}, {"title": "Game specifications and game reasoning queries", "content": "We begin by defining a general notion of an intuitive game theory problem that encompasses the varying questions we seek to evaluate (e.g., how likely is this game to end in a draw?) on arbitrary games. We formalize a game spec-ification as comprising an environment definition (e.g., the board shape), a game state transition function that defines the game dynamics (e.g., alternating actions over two players, and terminal states based on the win or draw conditions), and game utility functions defined for each player given game state, which in our dataset are only sparsely defined over terminal states. This formalization derives from standard multi-agent planning problems definitions used in the AI planning literature (e.g., Russell & Norvig (2020)).\nA problem is then a game specification combined with any general game reasoning query about that game."}, {"title": "Estimating game outcomes by simulating goal-directed but search-limited players", "content": "The core of our model is a simulated agent that can play from arbitrary game specifications. This agent model has a general search-limited way of placing utility over intermediate states.\nWe design the agent model around a collection of util-ity functions which capture general intuitions about goal-directed competitive game play, and can be quickly evaluated over any intermediate board state with at most a single step of lookahead search in the class of games we consider here. Together, these utility functions are closely related to features in models from a long tradition of works that study game-playing in specific \u201cm-in-a-row\" games (Amir et al., 2022; Crowley & Siegler, 1993; van Opheusden et al., 2023), which are often descriptively derived from empirical observations of game play. Here, we generalize these game-specific features into a model applicable across the entire, broad family of games we consider in this paper, and also ground each in more general intuitions about game play. The general rationales beyond the features also admit other utility function formulations that could be explored in future work.\nSpecifically, given an intermediate game state, our player model assigns an overall utility score to each possible next move (each open position on the board) based on the follow-ing functions:\n1.  The normalized Euclidean distance of the position and the center of the board, $d \\in [0,1]$. This reflects the intuition, applicable across our family of games, that people often but not always place pieces around the center at the beginning. This function can be explained based on a more general rationale-agnostic to other aspects of the current game state, placing pieces closer to the center of the board allows that piece to participate in the most possible winning terminal states for any m-in-a-row win condition.\n2.  The maximum number of connected contiguous pieces that the position entails on any of the allowed winning directions (horizontal, vertical, or diagonal in general-modulated with respect to any player-specific restrictions on win directions), $n_1 \\in Z^+$. If $n_1$ equals the winning $m$ form in a row, we add an additional 1 to $n_1$. This function captures the general intuition that players are goal-directed and try to make progress towards winning\u2014that is, that players derive utility from intermediate states towards the most rewarding terminal state. While there are many possible subgoals that a player could consider which might make partial progress towards terminal states (e.g., placing making 3 in a row by placing two unconnected pieces and then placing one more piece in the center), we choose a simple, easy to calculate function that also follows from players following an intuitive standard policy (making m in a row by previously building up to m \u2013 1 in a row, m-2 in a row, and so on via contiguous pieces). We also only consider the maximum number of pieces to reflect a relatively naive, shallow player who is considering a single, simple policy towards the nearest victory. This function could easily be augmented to support a number of related functions that reflect partial progress towards multiple terminal rewarding states.\n3.  Relatedly, the maximum number of connected contiguous pieces that the position blocks the opponent from having, $n_2 \\in Q^+$, also considered with respect to the win-directions applicable to the opponent. This function captures the in-tuition that competitive players in antagonistic two-player games should also attempt to block opponent progress towards opponent goals. Note that this function is simple, symmetric to our definition of player progress (because players in our games have related and opposing goals). It also effectively evaluates just a single step of lookahead search relative to a opponent who is also relatively naive (that is, an opponent who at the next step would simply attempt to make progress towards their nearest victory along a simple policy). We subtract 0.5 from n\u2082 to reflect people's tendency to weigh offense over defense (Crowley & Siegler, 1993), so blocking the opponent's \u00ee in a row is not as good as making an \u00ee in a row for oneself (but is better than making an \u00een \u2013 1 in a row). However, if n2 equals the winning m, we do not subtract 0.5. One could"}, {"title": "Human and Model Experiments", "content": "We conduct an experiment to test whether our model ex-plains patterns of human reasoning about a variety of two-player grid-based games. We design n = 121 grid game specifications which vary the game environment (including square boards varying from 3\u00d73 to 10\u00d710; rectangular boards such as 2\u00d75 and 4\u00d79; and boards specified over an infinite grid); the game dynamics (including games in which a given player opens by going twice); and the player utility functions (including games in which the first or second player has differing win conditions, like requiring the first player to make a larger row than the second one.)"}, {"title": "Human game evaluations and game construction", "content": "We carry out a two-part study to evaluate how people reason about these novel games, and begin to to probe how humans create their own novel game variants to satisfy general game criteria (like constructing a reasonably fun game).\nParticipants We recruit 484 participants from Prolific (Palan & Schitter, 2018). Each participant was randomly presented with 10 games sampled from our stimuli, as well as regular Tic-Tac-Toe (won by making 3 in a row on a 3 \u00d7 3 board) to normalize game judgements. We collect approximately 20 judgements per game stimuli for each game reasoning query. Participants were paid at a base rate of $12.5/hr with an optional bonus up to $15/hr; the full experiment approximately took 25 minutes.\nGame outcome and game fun judgments Subjects were randomly assigned to one of two game reasoning conditions. In the game outcome evaluation condition, subjects pro-duced judgements on a continuous 0-100 probability scale to predict the likelihood of a first player win (if the game does not end in a draw, assuming both players play reasonably, how likely is it that the first player is going to win (not draw)?) and a draw (assuming both players play reasonably, how likely is the game to end in a draw?). In the game fun condition, subjects instead assessed the likelihood that the game is fun (how fun is this game?) on a confidence scale spanning 0 (the least fun of this class of game) to 100 (the most fun of this class of game).\nParticipants produced judgements about each game based on a linguistic game specification. We additionally provided participants with an interactive scratchpad board that they were told they could, but were not required, to use to inform their judgements; the scratchpad permitted automatically placing pieces of different colors (to simulate players) and could be cleared entirely to begin a new game. Participants were required to consider each game for at least 60 seconds before producing game judgements; on average, participants took 87.2 (\u00b12.5 STE) seconds per game in the game outcome condition and 83.4 (\u00b1 2.41 STE) seconds per game in the fun rating condition.\nNovel game creation After answering all game reasoning"}, {"title": "Alternative computational models", "content": "We implement a range of alternative computational models designed to span both weaker estimates of game play (using more naive agent models, or without any explicit agent models) and more optimal estimates of game play (using additional search and game simulations to terminal states).\nPartial game simulations with random agent This base-line estimates outcomes using partial game simulations, but substitutes our sub-goaling agent model with a more naive (\"random\") agent model that selects actions from a uniform distribution over valid moves.\nFull game simulations with random agent This baseline infers outcomes using complete game simulations to terminal states, but using the random agent model.\nFull game simulations with depth-5 lookahead search This baseline infers outcomes using complete game simula-tions to terminal states, and chooses moves based on future state utilities (under our sub-goaling utility metric) using a depth-5 lookahead search (i.e., we model alternating agent play using our model up to 5 actions ahead, then choose ac-tions based on utility estimates under our subgoaling function at the future state). The search-based agent model is inspired by the finding in van Opheusden et al. (2023), which suggests that experts playing a specific Connect-4 grid game are well-modeled by depth-5 search.\nFull game simulations with Monte Carlo Search (MCS) This baseline infers outcomes using complete game simula-tions to terminal states. The agent employs a standard MCS utility estimator (Genesereth & Thielscher, 2014), which runs full game simulations under a random agent model to"}, {"title": "Results and Discussion", "content": "Here we report our findings and discuss their interpretations.\nPeople's game outcome judgements are best predicted by partial, goal-directed game simulations. Subjects pro-vided judgements estimating both the likelihood of a draw, and the likelihood that the first player would win if the game did not end in the draw, which collectively estimate the distribution and expected utility over all possible outcomes (draw, first player wins, second player wins) for any given game. As shown in Fig. 3, our model correlates well with human estimates ($R^2 = 0.852, p < 5e-4$).\nNotably, our model is a much better fit to human predic-tions than both the more naive models (such as the random partial simulation model, orange), and the more optimal models (both the depth-5 lookahead search, red, and MCS, pink). In particular, the game outcome predictions under the MCS baseline reveal that there is essentially no uncertainty about game outcomes under optimal play. That is, two true experts, or an agent estimating play from many more simulations, would expect to draw, or win based on their player ordering, every time. A qualitative analysis of human scratchpad data supports the conclusion that while most subjects spontaneously do seem to simulate games, they draw game estimates from very few games, and imperfect game simulations\nFig. 4 shows mean absolute error between the predicted payoff under models and human judgements on games, fur-ther split among the subcategories of our novel game variants. As seen in Fig. 4, our model (blue) is close (and usually clos-est) to human judgements within each of these subcategories. We also find that the LLM-based models in fact relatively close to human judgements for the games that are the most obviously related to Tic-Tac-Toe , but deviate much more from human judgements on seemingly minor variants possibly because these games are relatively out-of-distribution compared to the more familiar variants found in background linguistic data.\nPeople's game fun judgements can be predicted by their own and our model's game outcome estimates. We eval-uate several game-play-based features which we find each correlate with subject's game fun estimates: 1) an estimate of whether games are fair and balanced based on the entropy over game outcomes-both predicted by humans themselves and predicted under our model; 2) an estimate of whether a game is reasonably challenging based on relative advantage"}, {"title": "Conclusion and future directions", "content": "Our findings demonstrate how people might make quick, probabilistic judgments about the expected value of com-plex novel problems\u2014well before they learn narrower, task-specific strategies and representations that constitute genuine expertise-by integrating efficient, general-purpose compu-tations for simulating multi-agent decision making and ap-proximate probabilistic inferences. We see many avenues for future work.\nOne is the question of how people grasp the constraints and goals of novel tasks, in order to reason about them. Here we assumed that people understood the linguistic descriptions of the games we presented them with, but this is a nontrivial cognitive process in its own right. Although large language models were limited in their ability to reason about new games as people do, they have proven capable of semantic parsing and program synthesis, and we see opportunities for modeling language-based game understanding using LLMs together with probabilistic planning and decision models like our intuitive game theory models, in a neurosymbolic framework Here we also only modeled the earliest stages of playing and reasoning about novel games. It is plausible that people's skill in these games will rapidly increase as they play more. A deeper understanding of such fast concept and skill acqui-sition would be highly valuable More broadly, the study of how people quickly learn to play novel games could contribute to contemporary work on resource rationality and human efficient planning. It is an open question to what extent our current model can properly be called \"resource rational\", or indeed, to what extent and in which ways naive humans approximate answers to these kinds of intuitive questions using \"resource rational\u201d representations. Overall, we believe general principles of efficient use of cognitive resources can be further applied to studying games, novel problem solving, and beyond."}]}