{"title": "Gazing at Rewards: Eye Movements as a Lens into Human and AI Decision-Making in Hybrid Visual Foraging", "authors": ["Bo Wang", "Dingwei Tan", "Yen-Ling Kuo", "Zhaowei Sun", "Jeremy M. Wolfe", "Tat-Jen Cham", "Mengmi Zhang"], "abstract": "Imagine searching a collection of coins for quarters (0.25), dimes (0.10), nickels (0.05), and pennies (0.01)\u2014a hybrid foraging task where observers look for multiple instances of multiple target types. In such tasks, how do target values and their prevalence influence foraging and eye movement behaviors (e.g., should you prioritize rare quarters or common nickels)? To explore this, we conducted human psychophysics experiments, revealing that humans are proficient reward foragers. Their eye fixations are drawn to regions with higher average rewards, fixation durations are longer on more valuable targets, and their cumulative rewards exceed chance, approaching the upper bound of optimal foragers. To probe these decision-making processes of humans, we developed a transformer-based Visual Forager (VF) model trained via reinforcement learning. Our VF model takes a series of targets, their corresponding values, and the search image as inputs, processes the images using foveated vision, and produces a sequence of eye movements along with decisions on whether to collect each fixated item. Our model outperforms all baselines, achieves cumulative rewards comparable to those of humans, and approximates human foraging behavior in eye movements and foraging biases within time-limited environments. Furthermore, stress tests on out-of-distribution tasks with novel targets, unseen values, and varying set sizes demonstrate the VF model's effective generalization. Our work offers valuable insights into the relationship between eye movements and decision-making, with our model serving as a powerful tool for further exploration of this connection. All data, code, and models will be made publicly available.", "sections": [{"title": "1. Introduction", "content": "Hybrid visual foraging is a ubiquitous challenge in our daily life, such as grocery shopping for a list of items, simultaneously scanning for traffic lights, parking spaces, and restaurants while driving, or looking for a specific amount of change among piles of coins (Fig. 1). These tasks involve searching for multiple instances of various target types stored in memory, where target values and prevalence can vary, and the exact number of target instances is often unknown. This raises a critical question about how to prioritize target selections during the search process. Understanding these dynamics is essential for optimizing search efficiency and decision-making in complex environments.\nTo tackle this question, eye movements could offer a unique window into the underlying perceptual, cognitive, and evaluative processes involved in decision-making, such as sensory evidence sampling and accumulation [75, 84, 103, 137, 143], decision timing and temporal expectation [6, 11, 107, 117, 123], response inhibitions [24, 46, 62, 82, 85], and decision certainty and confidence [10, 23, 60, 93, 107], offering high temporal and spatial resolution [44, 47, 72, 100, 116]. In hybrid visual foraging, while neuroscience and psychology works [77, 130, 134-136] have primarily examined the sequence of target selections within the same environment and the timing of search transitions across different environments especially when target values and prevalence vary, there is a notable lack of studies focusing on eye movements. Here, we design and conduct human psychophysics experiments to examine how foraging strategies and eye movements are influenced by the prevalence and value of targets.\nAlongside studies in psychology and neuroscience, many AI models have been developed to predict eye movements during decision-making tasks, including visual search [2, 35, 49, 57, 83, 118, 125, 142], object recognition and detection [8, 88, 96, 127], and visual question answering [18, 56, 58]. Notably, existing visual search models integrate both bottom-up saliency [2, 35, 57] and top-down feature modulations [49, 83, 142]. However, these models assume idealized scenarios where either a single target type is present or multiple target types have equal values. As a result, they often overlook the need to prioritize target selections based on varying target prevalences and values during the search process. In this work, we introduce a computational model called Visual Forager (VF), a transformer-based architecture trained with reinforcement learning, designed to perform hybrid visual foraging efficiently across varying combinations of target prevalence and values. Unlike prior visual search models [19, 26, 119, 139], which often rely on human data for supervised training, our VF approximates human foraging behaviors and biases, despite zero training on human data. We highlight our key contributions:\n1. Drawing from psychology, we introduce hybrid visual foraging tasks for AI models. The predicted eye movements offer a unique window into the decision-making process with high spatial and temporal resolution.\n2. We propose an AI model, Visual Forager (VF), for hybrid visual foraging tasks. VF uses actor-critic networks with a vision transformer backbone and integrates feature-based and value-based modulations to guide decision-making processes, determining where to fixate next and whether to collect currently fixated items during foraging tasks.\n3. To benchmark AI model performances, we design and conduct human eye-tracking experiments for hybrid visual foraging tasks. Despite no training on human data, our VF achieves cumulative rewards comparable to human participants and approximates their foraging behaviors, including eye movements and decision biases toward highly valued and prevalent targets.\n4. Humans can flexibly adapt their foraging strategies to maximize total rewards under varying target values and prevalence. Remarkably, our VF also performs efficient foraging, under out-of-distribution conditions it was never trained on. This capability is attributed to our newly introduced data augmentations applied to target values."}, {"title": "2. Related Work", "content": "Eye movements as a window into decision making. Many psychology and neuroscience works have been using eye movements as a unique and non-invasive means [37, 39, 40, 59, 66, 72, 73, 76, 92, 98, 101, 102, 113, 114] to reflect how efficiently we interpret decision instructions [68, 137], the timing and duration of decision formation [6, 11, 107, 117, 123], expected rewards [41, 109, 122, 128, 140], decision accuracy [107], and our confidence in the outcome [10, 23, 60, 93]. Although multiple studies analyze the decision making in hybrid visual foraging tasks [15, 17, 34, 97, 141], there is a lack of research specifically examining eye movements and their relationships to decision making. To close this gap, we design and conduct eye-tracking experiments to investigate eye movements and their interactions with foraging behaviors.\nComputational models for goal-directed visual search. Eye movement in visual search is typically guided by five primary factors [133]: bottom-up saliency [63, 121, 132], top-down feature guidance [29, 33, 81], scene properties [12, 54, 120], prior history [31, 67, 129], and item values [3, 4, 79]. A range of computational models have been proposed to model human eye movements in each of these aspects during visual search [2, 13, 16, 16, 21, 22, 42, 45, 49, 57, 64, 65, 71, 74, 83, 91, 115, 124-126, 142]. Recent deep-learning models, such as [9, 30, 48, 49, 90, 138, 142, 143], have demonstrated success in searching for a single target in complex, naturalistic scenes. However, these models fail to look for multiple instances of multiple target types based on varying values and prevalence. To close this gap, we introduce Visual Forager (VF), capable of searching for multiple instances across different target types. Moreover,"}, {"title": "3. Hybrid visual foraging", "content": "3.1. Human psychophysics experiments\nWe conducted an in-lab psychophysics experiment for hybrid visual foraging, schematically illustrated in Fig. 2. The items were randomly arranged on a 16\u00d716 grid, containing either 90, 105, or 120 items, with 20% to 30% of the squares filled with target instances. In each foraging trial, subjects searched for N\u2208 {1,2,4} target objects, each with a varying number of target instances. Targets and distractors in the hybrid foraging search arrays were randomly selected from a pool of 2,400 unique items in [14]. A total of 15 subjects were recruited, yielding 750 trials, containing 50514 eye fixations and 12851 mouse clicks. All the experiments are conducted with the subjects' informed consent and according to the protocols approved by the Institutional Review Board of our institution. See Appendix Sec. $1.1 for more details.\n3.2. Foraging environments for AI models\nHumans have accumulated years of visual experiences, making them efficient zero-shot visual searchers without requiring prior training for hybrid visual foraging [135]. In contrast, AI models require task-specific training. Hence, we introduced procedural generation of diverse foraging environments, subsequently used to train Al models. As demonstrated in the human psychophysics experiments described above, the procedural generation of a foraging trial depends on several key experimental parameters: the total number of items on the object arrays within a 16\u00d716-sized grid, the number of target objects on the search arrays, the prevalence of target instances for each target object, the values assigned to the target objects, and the selection of target and distractor items from a pool of 2,400 unique items. It is evident that the decision-making processes of both humans and AI models may be influenced by any of these experimental parameters. We sub-sample various combinations of these experimental parameters for the procedural generation of foraging environments: the total number of items on the search array is fixed at 105. A fixed set of 4 items is randomly selected as targets with their values set at 2, 4, 8, and 16 and their prevalence randomly determined. See Appendix Sec. $1.2 for details.\nTo benchmark AI model performance in hybrid foraging tasks, we introduce two in-domain hybrid foraging conditions that align with the distribution of the training environments that the AI models were optimized to solve. (1) In-domain Uneven Value, Equal Prevalence (UnValEqPre): the prevalence of all 4 targets is equal but their values vary. (2) In-domain Uneven Value, Unequal Prevalence (UnValUnPre): both target prevalence and values vary, with low-value targets being more common and high-value targets scarcer. To assess whether the AI models can generalize to out-of-distribution (OOD) hybrid visual search tasks, where experimental parameters differ from those encountered during training, we introduced five OOD conditions. (1) OOD - Even Value, UnEqual Prevalence (EqValUnPre): the 4 target types have the equal values with varying prevalence. (2) OOD - Unseen target objects (UTargets) The target images are unseen during testing. (3) OOD - Unseen value combinations (UValues) The target values exceeded the range used for training, with their"}, {"title": "4. Our proposed Visual Forager (VF)", "content": "We propose the Visual Forager (VF), a computational model for hybrid visual foraging (Fig. 3). At the current tth fixation location Ft, the model takes as input the search image Is and N target images IN with their corresponding target values V1:N, processes them with foveated vision, and modulates the decision making with visual features of target images and their corresponding values. Our VF outputs the predicted t + 1th fixation location Ft+1 and the mouse-click policies for the currently fixated item. VF comprises three modules: target feature modulation, target value modulation, and decision-making.\n4.1. Target feature modulation\nOur VF processes the N input target images IN with a feed-forward convolution neural network (2D-CNN) to extract their feature maps N at the last convolution layer. We used the VGG16 network [111], pre-trained on ImageNet [27] as the backbone of the feature extractor and froze its weights during training of hybrid foraging tasks.\nReceptive field sizes in the visual cortex increase progressively across brain areas [43]. This scaling is modeled in current visual recognition systems through eccentricity-dependent pooling operations [49]. Unlike uniform sampling in standard max pooling, receptive field size in eccentricity-dependent pooling layers grows with increasing distance from the current fixation Ft. Following [49], we replace VGG16's standard max pooling layers 110, 114, and 118 with eccentricity-dependent pooling [49]. This modified feature extractor produces feature maps os,t for the search image Is from the last convolutional layer, given the fixation location Ft. For simplification of mathematical notations, we omit the subscript t in os,t for the rest of the text, unless specified. We use pre-trained ImageNet weights for VGG16 and freeze them for semantic feature extraction. Since pooling layers lack learnable weights, freezing the pre-trained weights of eccentricity-dependent VGG16 does not affect feature extraction quality. Note that we do not use eccentricity-dependent VGG16 to process feature maps of input target images Z1:N as these target images are often small and they can be processed in high resolution within the foveated region.\nVisual search or feature-based attention is often modulated by the visual features of the targets [49, 142, 143]. Similar to [142], we implement the attentional modulation by computing the similarity between the features of the ith target image I and the features $s of the search image Is: Mi = M(\u03a6i,\u03a6s), where M(\u00b7) is the 2D convolution of stride 2 with \u03a6i serving as a convolution kernel applied to \u03a6s. We repeat the same attentional modulation for each target image, resulting in N similarity maps MF \u2208 RH\u00d7W\u00d7N, where H\u00d7W = 16\u00d716 denotes the grid size of the object arrays on Is.\n4.2. Target value modulation\nValues have proven to be a strong modulator of guidance in visual search [3]. To incorporate target values in our VF, we introduce a value encoder Ev (\u00b7) consisting of two fully connected layers and a ReLU activation layer in between. Ev takes N target values V1:N \u2208 RN as input and produces a D-dimensional value embedding Ev (V1:N) that encodes the target values. To embed Ev into MF, we also introduce a 2D-CNN encoder EF(\u00b7) to extract feature maps EF (MF) of size H \u00d7 W \u00d7 D from the similarity maps MF. The encoder comprises convolution blocks with ReLU activations, using 1 \u00d7 1 convolution kernels with stride 1 to maintain the spatial resolution of MF. No pooling layers are used. These convolution blocks facilitate feature fusion across all N similarity maps within each location but not across H \u00d7 W locations.\nSimilar to the way that positional embeddings are added to each feature vector of the image patches in the vision transformer (ViT) [32], we duplicate the value embedding over all H \u00d7 W locations and perform element-wise summation on EF (MF):\nMy = EF(MF) + duplicate(Ev (V1:N)) (1)\nThis results in the value-modulated feature maps My \u2208"}, {"title": "4.3. Policy network for decision making", "content": "Given My at the current fixation Ft as the states of the environment, VF must decide when to fixate next and whether to collect the currently fixated item. We model this as a Markov Decision Process with finite states and action spaces. Fixation locations are discretized to the H \u00d7 W grid, corresponding to the search array size on Is. The mouse click is a binary decision of whether to collect the fixated item. Here, we introduce the architecture of the decision-making module, also known as the policy network.\nWe employ the vision transformer (ViT) [32]) as the backbone for our decision-making network with its weights randomly initialized. We reshape the value-modulated similarity feature maps My into a sequence of patches Xp \u2208 RP\u00d7D, where P = H \u00d7 W. To retain positional information for all the patches, we add the standard learnable 1D positional embeddings to all the patches.\nSimilar to the role of the classification token in the original ViT, we introduce the extra click token to the patch sequence. A click action head is attached to the click token at the last layer of the transformer. The click action head is implemented with an average pooling layer over D dimensions of the click token embedding followed by a sigmoid function, outputting the probability Pc of the click. A click action ac is sampled based on Pc.\nIn parallel, we take the embeddings of all the patches from xp in the last layer of the ViT, apply average pooling over D dimensions for every patch, and obtain the logit of fixation probability over total P locations. Next, we use the softmax operation to normalize the logit map and reshape it to a 2D probabilistic attention map PF \u2208 RH\u00d7W indicating the most probable fixation location at t+1. The next fixation action af = Ft+1 is sampled based on PF.\n4.4. Training Details\nTo reduce overfitting and enhance the generalization of our VF, during training, we implement a channel augmentation technique that shuffles the input order of target images and their corresponding value pairs. We train VF with reinforcement learning (RL). The goal of RL is to jointly learn the fixation policy \u03c0\u0192(\u00b7|s) and the click policy \u03c0\u03bf(\u00b7|s), aiming to maximize the expected cumulative reward. Here, our VF uses the stochastic policy, where \u03c0f(af|s) = PF and \u03c0\u03b5(ac|s) = Pc. With these two separate policies, we can combine them into a joint probability over a multi-discrete action space PA = PCPF = \u03c0(ac, af|s).\nWe specifically used the Proximal Policy Optimization (PPO) algorithm [106] with the Adam optimizer [61] for training PA PPO is a policy gradient method designed to enhance the stability and efficiency of policy updates. Preliminaries on RL and PPO, as well as PPO hyperparameters, can be found in Appendix Sec. S2.1.\nIn practice, PPO often employs an actor-critic network [106]. The actor network selects actions based on the current policy, while the critic network evaluates these actions by learning the value function, assessing the quality of the chosen actions. We use the policy network outlined in Sec. 4.3 as our actor network. To estimate the current state's value (or state-value in RL), we add a special state-value token to the token sequence in the policy network. A critic head, implemented as a single fully connected layer, is attached to this token embedding at the final transformer"}, {"title": "4.5. Baselines and Evaluation Metrics", "content": "We introduced six baseline models. (1) Chance: A sequence of eye fixations is generated through uniform random sampling on a 16x16 grid, with a mouse click occurring at each fixated item with a 50% probability. (2) Visual Feature Modulation Only (FeatOnly): The method discards value modulations and predicts a sequence of eye fixations based solely on the items with the highest activation on the eccentricity-dependent similarity maps MF at Ft. The sequence of clicks directly corresponds to the sequence of eye fixations. (3) Max Value First (MaxVal) Eccentricity-dependent feature similarity maps MF are modulated by multiplying each of its maps with the value of the corresponding target object in V1:N. A sequence of eye fixations, equivalent to mouse clicks, is predicted by selecting the items with the highest values from value-modulated Mv. (4) Average Value First (AvgVal) We compute the average of value-modulated Mv in MaxVal over all target objects. The items with the highest averaged values get fixated and collected by the model. (5) Deep-Q learning (DQN) Instead of modular designs in our VF with the transformer architecture, we use a classical 2D-CNN-based deep-Q network [89] and train it end-to-end. See Appendix Sec. S2.3 for implementation details. (6) Upper bound (UpperBound) It is an oracle model with perfect target localization and recognition abilities, capable of making globally optimal decisions by always selecting the target instances with the highest values in the search arrays and never clicking on a distractor.\nAll the baseline models use infinite inhibition of return to track previous fixation locations and prevent revisiting them. This is accomplished by either masking the visited item locations in the action probability distribution for DQN or by removing the clicked or fixated items from the search arrays for the other baselines introduced above.\nWe propose two evaluation metrics. First, Normalized Score (Norm.Score) refers to the cumulative rewards as a function of the number of clicks within a foraging trial, normalized by the maximum score of UpperBound for that trial. The normalized score has an upper limit of 1, and a higher normalized score indicates that AI models or humans are more effective at making decisions to optimize cumulative rewards. Second, Click Bias Ratio (CBR) reflects the clicking biases of humans and AI models during foraging. We define the proportion of selections of target objects as Proportion Picked (PP) and the proportion of target objects left on the array as Proportion On-Screen (POS). CBR is then calculated as the normalized relative difference between PP and POS: CBR = PP\u2212POS. CBR can change over the course of clicks within a trial. If neither humans nor AI models exhibit clicking preferences, PP will equal POS, resulting in a CBR of 0. If either agent prefers one target object over others, PP will be greater than POS,"}, {"title": "5. Results", "content": "5.1. Humans and AI models are reward-seeking\nHumans and AI models are proficient foragers. We present the Norm.Score of humans and AI models under three conditions where target prevalence, value, or both vary, as shown in Fig. 4. Human subjects achieved Norm. Scores of 87.4%, 84.1%, and 93.1% across these conditions, significantly exceeding chance levels. Notably, even at the first mouse click, the Norm. Score was already well above chance. Similarly, all competitive baselines and our VF model consistently outperformed chance across all click numbers. These results suggest that both human subjects and AI models are effective at hybrid visual foraging, with their mouse click decisions strongly influenced by the values of target objects. However, humans never reached the upper bound of performance, indicating they are not perfect global optimizers. This may be due to imperfect object recognition [83], limited memory capacity [131], and foveated vision with restricted receptive fields [43.\nOur VF model outperform all the baseline models. The Norm. Scores of our VF model are 72.6%, 67.1%, and 81.6% across the three conditions, surpassing all baseline models. This indicates that our VF model effectively learns to optimize decision-making in foraging tasks and adapts well to variations in value and prevalence. Among the baseline models, FeatOnly achieved the highest Norm. Score, although it still performed worse than VF. This highlights the importance of visual feature modulation in foraging tasks, but also reveals that feature modulation alone is insufficient for optimal decision-making. While both AvgVal and MaxVal models incorporate value-based guidance into their decision strategies, their lower performance compared to FeatOnly suggests that simply relying on explicit values is ineffective. Rather, value and feature modulations must interact in a more sophisticated manner to achieve optimal performance.\n5.2. Eye movements are effected by target values\nEye fixations tend to be drawn to regions associated with higher rewards. Eye movements can serve as a valuable lens for examining the decision-making process [51]. Here, we analyzed eye fixation locations and their correlation with rewards in the corresponding areas. In Fig. S4, we present the average rewards of all target objects within the fixated areas, defined as those falling within a radius of 1.5 degrees of visual angle around each fixation. Surprisingly, we found that both humans and VF tend to fixate on regions associated with average rewards of 3.31 and 8.08, significantly higher than the averages derived from random fixations. This indicates that target values guide fixations during decision-making for both humans and VF. Despite limited visual coverage due to foveation, both agents can effectively explore more rewarding areas. We also conducted the human fixation duration analysis. Remarkably, we found that humans tend to spend more time fixating on higher-value targets compared to those with lower values (Appendix Sec. $3.2).\n5.3. Behavioral alignment between humans and VF\nBoth humans and AI models tend to overpick high-valued targets and underpick low-valued targets. In this analysis, we examine the items selected by humans and AI models from the search arrays and report their click bias ratios (CBR) averaged over all click numbers under the UnValEqPre and UnValUnPre conditions, as shown in Fig. 5A. Consistent with [135], we find that humans tend to overpick the highest-valued targets, as indicated by a positive CBR, while they underpick the lowest-valued targets, reflected by a negative CBR. Interestingly, our VF exhibits similar signs and magnitudes of CBR on these targets, suggesting that it displays analogous click biases based on target values as humans. However, both humans and VF do not exclusively select the highest-valued targets, as their Proportion Picked (PP) for less-valued targets is not exactly zero. This suggests that both humans and VF occasionally prioritize lower-valued targets during foraging (see Fig. S5 in Appendix). Moreover, we also noted a slight discrepancy in the signs of CBR when humans and VF select the second-highest-valued items. This indicates that VF is less sensitive to medium rewards compared to humans (also see Appendix Fig. S5).\nVF approximates the saccade size distributions of humans, without training on human eye movements. Human saccade sizes are restricted by physiological limitations of the eye muscles and the necessity for accurate visual processing in the foveal region [43]. We aggregate all saccade sizes from humans and our VF model under 3 conditions and plot their distributions in Fig. 5B. Despite lacking prior training on human eye movements, VF yields a mean saccade size of 4.06 degree, closely approximating the mean saccade size of 4.05 degree for humans. Additionally, to investigate the mechanisms in VF that constrain saccade sizes, we present the distribution of saccade sizes with eccentricity-dependent layers replaced with standard max-pooling layers in the visual feature extractor. As expected, and in line with [49], the saccade sizes increase, suggesting that smaller saccade sizes are partially due to foveal processing."}, {"title": "5.4. VF generalizes to OOD conditions", "content": "To assess the generalization performance of our VF model in out-of-distribution (OOD) hybrid foraging tasks, we benchmark humans, VF, and the best baseline model, FeatOnly, across five OOD conditions (Sec. 3.2). From Fig. 5C, VF outperforms FeatOnly in all experimental conditions, indicating that it learns generic decision-making strategies and adapts well to unseen foraging scenarios. However, VF still lags behind humans, suggesting that humans can more flexibly adjust their foraging strategies to maximize rewards based on the environments.\n5.5. Ablations reveal critical component designs\nWe systematically ablated several essential components in our VF model and reported their results in Tab. 1. (1) Rather than using reinforcement learning, we train VF on human eye movements and mouse clicks through supervised learning (Behavior Cloning). The lower Norm. Score of Behavior Cloning indicates that human eye movement data is limited, leading to model overfitting. Hence, the model struggles to generalize to unseen target value and prevalence combinations. (2) We replace the transformer-based decision-making module with a 2D-CNN, referred to"}, {"title": "6. Discussion", "content": "In hybrid visual foraging, humans are proficient foragers, directing attention to regions with significantly higher rewards than chance. When there is an imbalance in target prevalence or values, humans tend to over-exploit the most prevalent or high-valued target types. To explain the characteristics of human behavior in hybrid visual foraging, we propose a transformer-based Visual Forager (VF) trained in reinforcement learning. Its cumulative rewards match human performance and surpass other baseline models. Despite zero training on any human data, VF closely approximates human foraging behaviors, including foraging biases and eye movements. Remarkably, VF demonstrates exceptional generalization abilities, flexibly adjusting its foraging strategies to experimental conditions it has never encountered. Our work paves the way for several new research directions in psychology, neuroscience, and AI. We discuss these future works in Appendix Sec. S4."}, {"title": "S2. Reinforcement learning", "content": "We recall the Markov decision process (MDP) framework with finite state space S and action space A. An MDP is defined as M = (S, A, Pr, r, y), where Pr : S \u00d7 A \u2192 \u25b3(S) is the transition function, r : S \u00d7 A \u2192 R is the reward function, and \u03b3\u2208 (0, 1) is the discount factor. Given an initial state 80, the goal of reinforcement learning (RL) is to learn a policy \u3160 that maps a state s \u2208 S to a distribution \u03c0(\u00b7 | s) over the action space, aiming to maximize the expected cumulative discounted reward.\nFor any policy \u03c0, the action-value function Q\u2122 (s, a) represents the expected return starting from state s, taking action a, and thereafter following policy \u03c0. It is defined as Q\" (s, a) = \u0395\u03c0,Pr [\u2211t=0yh r(st, at) | 80 = s, a\u2081 = a], where \u0395\u03c0,Pr(\u00b7) denotes the expectation over trajectories generated by following \u3160 under the transition dynamics Pr. The state-value function V(s) is the expected return starting from state s and following \u03c0, while the advantage function A\" (s, a) is given by A\" (s, a) = Q\" (s, a) \u2013 V\" (s), quantifying the relative advantage of taking action a in state s under policy \u03c0.\nS2.1. Proximal Policy Optimization\nProximal Policy Optimization (PPO) is a policy gradient method designed to improve the stability and efficiency of policy updates. PPO ([106]) uses a surrogate objective function with a clipping mechanism to prevent large, destabilizing updates. The surrogate objective function is defined as:\nLCUP  Eaold min (x(a), clip(x(a) 1400)) 401(5, a)\nwhere Ais an estimate of the advantage function, and is a hyperparameter controlling the extent of clipping.\nIn this formulation, the first term inside the min operator is the standard policy gradient objective, while the second term applies the clipping mechanism to ensure that the policy update does not result in excessively large changes. This clipping mechanism is crucial for maintaining the stability of the learning process.\nS2.2. Additional training and implementation details\nIn practice, rather than learning two separate policies for actions at different times i.e., the mouse click at t and the fixation at t + 1, we modify the click policy \u03c0\u03bf(\u00b7|s) to output the binary click decision at t + 1, aligning it with the fixation policy. Empirically, this leads to more efficient training and faster convergence. Importantly, this modification does not alter the hybrid foraging setup, as VF can fixate on the same grid cell consecutively. In other words, VF may initially decide not to click the item fixated at t + 1 but can later decide to click it by fixating on the same item again at the next time step.\nThe search image Is has a resolution of 1024 \u00d7 1024 pixels, while the target images IT are 64 \u00d7 64 pixels, corresponding to the size of one cell within a 16\u00d716-sized grid in Is. The search feature map os has dimensions 32 \u00d7 32 \u00d7 512, while the target feature maps 1 are 2 \u00d7 2 \u00d7 512. We implemented the target modulation function M with a stride of 2, resulting in MF with dimensions 16 \u00d7 16 \u00d7 N, where the spatial size matches the grid size of the search image.\nOur VF was trained over 3 million timesteps in the first stage, taking approximately 3 days, and over 0.6 million timesteps in the second stage, taking approximately 1 day. All training was conducted on a single NVIDIA RTX A6000 GPU.\nS2.3. Deep Q-leaning\nValue-based reinforcement learning method solves MDP problem by getting an optimal value function. The optimal value function is defined by V*(s) = sup\u201e V (s) and similarly Q*(s, a) = sup\u201e Q\" (s, a). We use deep Q-learning (DQN) as a baseline method, which obtains Q* based on the update Qi+1 (st, at) = (1- at) Qi (St, at)+at (rt + y maxa Qi (St+1, a)), where at \u2208 (0, 1) is the learning rate. We employ the \u025b-greedy approach for action selection based on a value function, which means that we pick arg maxa Qi(s, a) with 1 \u025b probability and a random action with probability \u025b.\nAs our baseline, we do not incorporate target feature modulation or target value modulation. Instead, we designed a deep neural network (DNN) to predict the value function in an end-to-end fashion. This DNN takes as input a search image, target images, and target values. It uses two 2D-CNNs to extract features from the search and target images, respectively, then concatenates the search features, target features, and target values. An MLP with three fully connected blocks outputs an approximate state-action value for each action. Following the standard DQN used in [89], our approach incorporates the key techniques of target networks and experience replays.\nS2.4. PPO Hyperparameters\nThis hyperparameters used at two training stages are listed as follow:\""}, {"title": "S3. Additional experiment results", "content": "S3.1. Human motor response\nS3.2. Human fixation duration\nFixation durations are longer on targets with higher values. We also investigate human fixation durations on targets with varying values under the UnValEqPre and UnValUnPre conditions. From Appendix Fig. S3, surprisingly, we found that humans tend to spend more time fixating on higher-value targets compared to those with lower values. For example, under the UnValEqPre condition, the mean eye fixation duration is 344 milliseconds on targets valued at 16, while the duration is 309 milliseconds on targets valued at 2. This may be attributed to the enhancement of learning and memory, where longer fixation durations facilitate cognitive processing and reinforce associations between previous decision-making strategies and positive outcomes.\nS3.3. Average reward within fixation area\nS3.4. Click behavior\nFirst, we observed that humans occasionally clicked on items they were not directly fixating on, while VF assumes eye movements always align with the locations at which foraging decisions are made. Second, a strong priming effect was evident in humans, especially when target values were equal, showing the long-lasting influence of prior experiences on human decisions. Our VF currently lacks the ability to model such long-term dependencies, as it does not have a working memory integrating reinforcements from past actions into current decisions. Third, in hybrid foraging, humans actively compare fixated items with those in memory, a process known as memory search. Our VF assumes perfect memory search, where all targets are compared to the fixated item simultaneously. Lastly, real-world environments may present additional challenges, such as target occlusions and physical constraints imposed by scene contexts. Extending the study of hybrid visual foraging beyond simplistic stimuli in controlled experimental settings remains an intriguing research direction."}]}