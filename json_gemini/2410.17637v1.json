{"title": "MIA-DPO: MULTI-IMAGE AUGMENTED DIRECT PREFERENCE OPTIMIZATION FOR LARGE VISION-LANGUAGE MODELS", "authors": ["Ziyu Liu", "Yuhang Zang", "Xiaoyi Dong", "Pan Zhang", "Yuhang Cao", "Haodong Duan", "Conghui He", "Yuanjun Xiong", "Dahua Lin", "Jiaqi Wang"], "abstract": "Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing optimization algorithms like direct preference optimization (DPO). Existing visual alignment methods, primarily designed for single-image scenarios, struggle to effectively handle the complexity of multi-image tasks due to the scarcity of diverse training data and the high cost of annotating chosen/rejected pairs. We present Multi-Image Augmented Direct Preference Optimization (MIA-DPO), a visual preference alignment approach that effectively handles multi-image inputs. MIA-DPO mitigates the scarcity of diverse multi-image training data by extending single-image data with unrelated images arranged in grid collages or pic-in-pic formats, significantly reducing the costs associated with multi-image data annotations. Our observation reveals that attention values of LVLMs vary considerably across different images. We use attention values to identify and filter out rejected responses the model may have mistakenly focused on. Our attention-aware selection for constructing the chosen/rejected pairs without relying on (i) human annotation, (ii) extra data, and (iii) external models or APIs. MIA-DPO is compatible with various architectures and outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the recent InternLM-XC2.5. Moreover, MIA-DPO has a minimal effect on the model's ability to understand single images.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent progress in Large Vision Language Models (LVLMs) marks a significant breakthrough in Al research. While proprietary models (e.g., GPT-40 (OpenAI, 2024)) excel at handling multi-image contexts, current open-source LVLMs (Liu et al., 2024b;a) yield promising results but are primarily focused on single-image visual question answering. In real-world environments, such as digital documents and web pages, multiple figures and texts are interleaved to convey complex information effectively. The ability to understand multi-image contexts is a crucial direction for the future development of LVLMs.\nLVLMs typically have three stages: (1) Pre-Training, (2) Supervised Fine-Tuning (SFT), and (3) Preference Alignment (i.e., Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) or from AI Feedback (RLAIF) (Bai et al., 2022)). Correspondingly, to enhance the multi-image ability of LVLMs, several recent multi-image pre-training (Awadalla et al., 2023a; 2024) and instruction fine-tuning (Jiang et al., 2024; Li et al., 2024a; Chen et al., 2024b; Liu et al., 2024c) datasets and evaluation benchmarks (Jiang et al., 2024; Fu et al., 2024; Song et al., 2024; Ma et al., 2024) have been proposed. Pre-training and SFT on multi-image data can enhance the model's ability to handle multiple images to some extent. Nevertheless, similar to single-image scenarios, hallucinations remain an inevitable issue. Additionally, incorporating multi-image data during SFT may adversely affect performance on single-image tasks. For example, previous work (Jiang et al.,"}, {"title": "2 RELATED WORKS", "content": "Large Vision Language Models (LVLMs), like GPT-4V (Achiam et al., 2023), signify a major breakthrough in the development of Large Language Models (LLMs) by incorporating both visual and textual data. LVLMs significantly enhance the quality of human-AI interactions, making these exchanges more intuitive and seamless. To enable LVLMs to handle multi-image tasks, several multi-image datasets suitable for pre-training and supervised fine-tuning (SFT) have gradually emerged (Jiang et al., 2024; Liu et al., 2024c; Song et al., 2024). However, due to the lag in the development of multi-image datasets, data and methods tailored for multi-image tasks during the RLHF/RLAIF phase remain unexplored. Therefore, we designed a dedicated MIA-DPO framework for multi-image tasks, aimed at improving the ability of LVLMs to handle multi-image scenarios.\nVisual Preference Alignment is a multi-modal extension of preference alignment techniques with image inputs. Preference alignment aligns LLMs with human values and reduces hallucinations by collecting pairs of preferred and rejected data, using optimization techniques including PPO (Schulman et al., 2017) and DPO (Rafailov et al., 2024) to guide the model's adjustments. Earlier approaches, such as LLaVa-RLHF (Sun et al., 2023) and RLHF-V (Yu et al., 2024a), required human labeling of preferred data, which incurs high labor costs. HA-DPO (Zhao et al., 2023) mitigates this by using GPT-4's API to generate the necessary DPO data, but it still faces high API costs. RLAIF-V (Yu et al., 2024b) employs a text-splitting approach to scoring individual text segments and using open-source LVLMs for data generation. POVID (Zhou et al., 2024) uses blurred images and GPT-4 to inject hallucinations to construct the DPO data. The previous approaches focus solely on single-image scenarios and require costly chosen/rejected data. Our MIA-DPO first enables visual preference alignment for multi-image scenarios and achieves low-cost DPO data construction."}, {"title": "3 METHODS", "content": "We first introduce the background of visual preference alignment in Sec. 3.1. We analyze the multi-image hallucinations in Sec. 3.2. We present our MIA-DPO framework in Sec. 3.3."}, {"title": "3.1 PRELIMINARY", "content": "To enhance LVLMs' understanding of multi-image inputs, we employ visual preference alignment. This section introduces the concept of visual preference alignment and highlights the DPO approach as a representative example.\nVisual Preference Alignment Preference alignment aims to align a model's preferences with human preferences. Representative approaches include Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and Reinforcement Learning from AI Feedback (RLAIF) (Bai et al., 2022). Given a dataset $D$, where each sample consists of an input prompt x, the chosen answers $y_w$ and the rejected output $y_l$. We can represent $D$ as follows: $D = \\{x, y_w, y_l\\}$. The input prompt x can be an interleaved sequence of images v and texts t. When an LVLM processes an input x and generates an output y, a reward r(x, y) is assigned. The reward model r assesses the chosen (high value of r(x, y)) and rejected (low r(x, y)) samples. Visual preference alignment aims to maximize the reward r(x, y):\n$\\max_{\\theta} E_{x\\sim D, y\\sim \\pi_{\\theta} (y|x)} [r(x, y)],$ (1)"}, {"title": "3.2 ANALYSIS ON MULTI-IMAGE HALLUCINATIONS", "content": "In this section, we conduct various studies to analyze the characteristics of multi-image hallucinations in LVLMs and reveal that the attention mechanism is a proper indicator to determine when hallucinations occur.\nTwo-types of Multi-Image Hallucinations Some previous studies (Li et al., 2023c; Ouali et al., 2024) have explored different types of single-image hallucinations, such as object hallucination which means the model incorrectly describes objects that are not present in the image. Compared to"}, {"title": "3.3 \u039c\u0399\u0391-DPO FRAMEWORK", "content": "As illustrated in Fig. 3, MIA-DPO initially extends single-image prompts to multi-image prompts (Sec. 3.3.1), followed by attention-based filtering of rejected data and post-selection processing (Sec. 3.3.2). Finally, we apply the DPO algorithm (Sec. 3.3.3) to the constructed multi-image prompts and chosen/rejected pairs, resulting in a stronger model."}, {"title": "3.3.1 FROM SINGLE-IMAGE PROMPTS TO MULTI-IMAGE PROMPTS", "content": "Rather than expending effort on collecting and annotating new multi-image prompts, we efficiently convert existing single-image datasets, such as LLaVA-665k (Liu et al., 2024a), by incorporating unrelated images. Our low-cost, scalable approach enriches data forms and allows us to comprehensively explore the various types of multi-image hallucinations that LVLMs might produce.\nAs shown in Fig. 4, we construct multi-image prompts in three formats: (1) Sequence: Multiple images are arranged sequentially, with questions targeting specific images. The number of images varies from 2 to 5. (2) Grid Collage: Multiple images are merged into a single image, each labeled with a number description. Questions focus on specific images based on language descriptions."}, {"title": "3.3.2 ATTENTION-AWARE SELECTION FOR REJECTED SAMPLES", "content": "As we analyzed in Sec. 3.2, the model's attention values are clues for detecting multi-image hallucinations. Inspired by our observation, we present an attention-aware selection mechanism for constructing the rejected samples of the DPO algorithm.\nGiven the input question x and a set of generated answers $(y_1, y_2, ...) \\sim \\pi_{\\theta}(y|x)$. For each answer sample y, we compute the attention value metric $R(y) = \\frac{A_{target}}{A_{total}}$, where $A_{target}$ be the amount of attention directed toward the target defined in x, and $A_{total}$ be the total amount of attention values. By setting an attention ratio threshold $\\tau$, we can select cases $y_l$ that the LVLMs did not correctly focus on the image or region specified:\n$y_l = \\{y | y \\sim \\pi_{\\theta}(y|x) \\text{ and } R(y) \\leq \\tau\\}.$ (4)\nWe use $y_l$ as the rejected answer for the DPO algorithm. We use the ground truth of question x as the chosen sample $y_w$. Finally, we construct the DPO pair data $D = \\{x, y_w, y_l\\}$ in Eq. (3)."}, {"title": "3.3.3 OPTIMIZATION", "content": "As discussed in Sec. 3.3.1 and Sec. 3.3.2, we have outlined how to construct multi-image input prompts x, and select chosen $y_w$ and rejected $y_l$ pairs. By applying Eq. (3), we can update the policy $\\pi_{\\theta}$.\nTo improve the stability of DPO training, following the approach in (Dubey et al., 2024; Pang et al., 2024), we add a negative log-likelihood(NLL) loss $L_{NLL}(\\pi_{\\theta}) = - \\log \\pi_{\\theta}(y_w|x)$. We use a parameter $\\gamma$ to balance the $L_{DPO}$ and $L_{NLL}$. The final loss $L_{total}$ is defined in Eq. (5):\n$L_{total} = L_{DPO}(\\pi_{\\theta}; \\pi_{ref}) + \\gamma L_{NLL}(\\pi_{\\theta}).$ (5)"}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUP\nBenchmarks We evaluate our method on the following representative benchmarks. First, we select five multi-image benchmarks: MMMU (Yue et al., 2024), BLINK (Fu et al., 2024), Mantis (Jiang et al., 2024), NLVR2 (Suhr et al., 2018), and MVBench (Li et al., 2024c). The MMMU benchmark includes questions involving both single-image and multi-image scenarios. Subsequently, we also test the model on several single-image benchmarks: MMStar (Chen et al., 2024a), ScienceQA (Lu et al., 2022), MMVet (Yu et al., 2023), POPE (Li et al., 2023c), MMBench (Liu et al., 2023), MathVista (Lu et al., 2023), and AI2D (Kembhavi et al., 2016). We evaluate our method on a diverse set of benchmarks, demonstrating its effectiveness across both scenarios. These evaluations confirm the model's improved performance, particularly in multi-image contexts.\nBaseline Methods We compare MIA-DPO with three preference optimization baselines. (1) LLaVA-RLHF (Sun et al., 2023) improves model performance by augmenting GPT-4-generated data"}, {"title": "4.2 RESULTS ON MULTI-IMAGES BENCHMARKS", "content": "Results on LLaVA-v1.5 As present in Tab. 1, applying MIA-DPO to LLaVA-v1.5 achieves improvements of 1.2%/5.8%/2.3%/2.1%/3.5% on five multi-image benchmarks, which demonstrates the effectiveness of MIA-DPO. As for the challenging MMMU benchmark that requires complex domain-specific knowledge, MIA-DPO enables LLaVA-v1.5 to achieve a 1.2% improvement. The experimental results on MMMU demonstrate that MIA-DPO enhances the LLaVA-v1.5's reasoning ability on multi-image problems. Additionally, on the BLINK dataset that includes multi-view and spatial relationship reasoning, MIA-DPO significantly boosts the performance of LLaVA-v1.5 by 5.8%. Such an improvement highlights the effectiveness of MIA-DPO in enhancing the model's ability to understand and reason under multi-image scenarios.\nComparison with Preference Optimization Baselines In Tab. 1, we compare MIA-DPO with three preference optimization baselines (LLaVA-RLHF, HA-DPO, POVID) on LLaVA-v1.5. Thanks to our multi-image attention-based method for constructing the DPO data, MIA-DPO achieves significant advantages on the reported five multi-image benchmarks compared to the baselines.\nMore LVLM Architectures We also applied MIA-DPO to other LVLM architectures, such as the recent InternLM-XC2.5 model. As shown in Tab. 1, MIA-DPO boosts the performance of 1.2%/0.8%/11.1%/4.5%/4.1% across the five benchmarks, resulting in an average improvement of 4.3%. The results on LLaVA-1.5 and InternLM-XC2.5 demonstrate that MIA-DPO is general and effective for different LVLM architectures. Notably, despite the Supervised Fine-tuning (SFT) phase of InternLM-XC2.5 involving multi-image data, our MIA-DPO still further boosts performance on multi-image benchmarks."}, {"title": "4.3 RESULTS ON SINGLE-IMAGES BENCHMARKS", "content": "While MIA-DPO is effective in multi-image scenarios, we also report the performance on single-image benchmarks. As shown in Tab. 2, MIA-DPO outperforms the LLaVA-v1.5 baseline and DPO methods, including LLaVA-RLHF and HA-DPO, in average results across seven single-image benchmarks. As for the InternLM-XC2.5 model, MIA-DPO achieves a 1.4% increase on MMStar but performs slightly below baseline on average across all single-image benchmarks. The slight degradation in InternLM-XC2.5's single-image performance suggests that while the model benefits greatly in multi-image scenarios, there may be a trade-off in optimizing for more complex, interleaved inputs. Overall, our findings highlight the robustness of our MIA-DPO, which not only excels in improving multi-image performance but also preserves proficiency on single-image tasks. Our MIA-DPO serves as a strong candidate for real-world applications requiring versatile multimodal abilities across both single and multiple image tasks."}, {"title": "4.4 ABLATION STUDIES", "content": "Ablation Studies on Post-Selection In our ablation study, we experimented with the post-selection process for DPO data. As illustrated in Fig. 3, our post-selection process includes three components: perplexity (ppl), text length, and edit distance. We conduct ablation studies to compare the impact of whether to use the post-selection or not. In Tab. 3, the results show that while MIA-DPO without post-selection (row 1) still led to improvements across multiple multi-image benchmarks, its performance was consistently lower than that of MIA-DPO with post-selection (row 2). Our findings highlight that post-selection effectively removes outlier and low-quality data, further enhancing the overall quality of the DPO pair data and boosting model performance.\nAblation Studies on Data Types In the process of constructing multi-image DPO data for MIA-DPO, we created three types of data: Sequence, Grid Collage, and Pic-in-Pic Data. These three types of data work together to specifically eliminate the two types of multi-image hallucinations we identified: Sequence Confusion and Element Interference. To study the impact of each data type on overall performance, we trained the LLaVa-v1.5 model separately with 20k instances of each data type and summarized the results in Tab. 3.\nThe experimental results indicate that using each data type individually for DPO on LLaVa-v1.5 yields similar average scores of 42.6, 42.4, and 42.7 across five benchmarks. However, when combining all three data types, the model achieves a higher average score of 43.4, as shown in Tab. 1. This suggests that the three data types address different hallucination types, and their combination produces better results than using them separately."}, {"title": "4.5 VISUALIZATION OBSERVATIONS", "content": "We visualize the reasoning process of the LLaVA-v1.5 model before and after applying MIA-DPO on multi-image cases. In Fig. 6, we show the attention map of the generated text tokens relative to the input image tokens. The top and second rows display the attention distribution before and after applying MIA-DPO, respectively. The attention difference (delta value) in the third row indicates which areas receive increased attention due to applying our preference optimization process.\nUsing MIA-DPO, the LLaVA-v1.5 model adjusts its focus to specific image regions corresponding to the given instruction. In both the first and second cases, we observe an increased focus on the instruction-targeted areas of Image 1 after applying MIA-DPO. In the third case, attention gravitates more toward Image 2, which is specified in the language instruction. The visualization results indicate that MIA-DPO effectively improves the model's ability to correctly allocate attention to the relevant image regions, reducing the likelihood of multi-image hallucinations."}, {"title": "5 CONCLUSION", "content": "Aligning models with human preferences is a critical goal. In this paper, we are the first to propose a multi-image DPO framework. We conducted an in-depth analysis of the differences between hallucinations in multi-image and single-image reasoning for LVLMs, exploring the root causes of multi-image hallucinations through the lens of attention. Our findings reveal that a lack of attention-aware capabilities is a key factor contributing to hallucinations in multi-image reasoning. Based on these insights, we introduced MIA-DPO (Multi-Image Augmented Direct Preference Optimization). Results from tests on five multi-image benchmarks and seven single-image benchmarks demonstrate that MIA-DPO significantly improves the model's performance in multi-image reasoning while maintaining its original single-image reasoning capabilities."}, {"title": "A MORE EXPERIMENTS", "content": "A.1 ABLATION STUDIES\nAblation Studies on y and Epochs We perform ablation studies on the key hyper-parameters, including the NLL loss coefficient y and the number of training epochs. As shown in Tab. 4, we observe that a larger value of y negatively impacts the training process, while the number of epochs has a minor effect on the final results. Based on the experimental results, we set 3 epochs and y = 0.1 as the default values for the parameters.\nGPT-40-mini Selection and MIA-DPO To validate the effectiveness of MIA-DPO, we introduce an ablation experiment using GPT-40-mini for DPO data selection. The process begins with the model generating answers to our multi-image questions, followed by presenting both the model's responses and the ground truth to GPT-40-mini. GPT-40-mini then assesses the accuracy of the model's responses and their similarity to the ground truth, assigning a score between 0 and 10 based on various criteria. We classify responses with scores below 7 as rejected data and use them to construct the DPO data. The results are presented in Tab. 5. Our observations indicate that MIA-DPO not only offers a cost advantage over the GPT-40-mini-based data selection method but also outperforms it across five benchmarks.\nThe prompt we use to guide GPT-40-mini in data selection is as follows:\nAssume you are an expert in evaluating the accuracy of answers. You will be provided with a question and two answers: one is the ground truth, and the other is a model-generated response. You need to score the model's response based on its similarity to the ground truth, using a scale from 0 to 10. The specific requirements are as follows:\nThe closer the model's response is to the ground truth, the higher the score.\n1.If there are obvious errors and the model's response is completely different from the ground truth, score 0-3.\n2. If there are errors and the model's response is far from the ground truth, score 4-6.\n3. If there are some errors, and they have some negative impact on the overall response, score 6-8.\n4. If the model's answer is very close to the ground truth, score 9.\n5. If the model's response is identical to the ground truth, or even richer in content and better expressed, score 10.\nPlease return the score directly in the following format without any extra information, for example:\"Score\":\"2\".\nA.2 EXPERIMENTS DETAILS\nAll single-image experimental results presented in Tab 2 are obtained using the VLMEvalKit (Duan et al., 2024). For the five multi-image benchmarks, MMMU (Yue et al., 2024) is also tested using"}, {"title": "B MODEL AND DATA SOURCES", "content": "B.1 MODEL SOURCES\nFor the experimental section, we present the testing results of multiple LVLMs on several multi-image and single-image benchmarks. The models involved in the experiments are listed in Tab. 6 of the paper.\nB.2 BENCHMARK SOURCES\nThe benchmarks involved in the experiments are diverse and include 5 multi-image benchmarks and 7 single-image benchmarks. These benchmarks cover various domains, allowing for a comprehensive assessment of the models' actual capabilities. We list all the benchmarks and their detailed information in Tab. 7, along with a further introduction to some of the benchmarks:\nMMMU MMMU (Yue et al., 2024) is a benchmark for assessing multimodal models on college-level tasks that require advanced reasoning and domain-specific knowledge. It features 11,500 questions across six disciplines and includes diverse image types. Initial evaluations show that even advanced model GPT-4V struggles, achieving only 56% accuracy, indicating substantial room for improvement. In addition, MMMU includes both single-image and multi-image test questions.\nBLINK BLINK (Fu et al., 2024) is a benchmark for multimodal language models (LLMs) that tests core visual perception tasks solvable by humans \u201cwithin a blink,\" like depth estimation and visual correspondence. It reformats 14 classic computer vision tasks into 3,807 multiple-choice questions with images. While humans achieve 95.70% accuracy, top models like GPT-4V and Gemini perform significantly worse, highlighting a gap in visual perception abilities among current LLMs.\nNLVR2 NLVR2 (Suhr et al., 2018) is a dataset designed for joint reasoning involving natural language and images, focusing on semantic diversity and visual reasoning challenges. It contains\""}, {"title": "B.3 \u039c\u0399\u0391-DPO DATA STATISTIC", "content": "In constructing our MIA-DPO dataset with three types of multi-image data (Sequence Data, Grid Collage Data, and Pic-in-Pic Data), we used the LLaVa665k (Liu et al., 2024b) dataset as the foundational single-image data. The LLaVa665k dataset contains 665k training samples, with a negligible"}, {"title": "C MIA-DPO DATA CASES", "content": "C.1 SEQUENCE IMAGE DATA\nSequence Image Data is the first type of MIA-DPO data we constructed, where multiple images are combined into a sequence, and questions are posed about a randomly selected image within that sequence. The number of images included in Sequence Image Data ranges from 2 to 5. This approach increases the difficulty of answering questions for LVLMs by adding interference from other images beyond the one indicated in the instructions. Additionally, inputting multiple images in sequence significantly increases the length of image tokens, posing a greater challenge for LVLMs. At the same time, the Sequence Image Data type is primarily designed to address the Sequence Confusion type of multi-image hallucination, while also mitigating the Element Interference type of hallucination to some extent. We provide several examples of Sequence Image Data in Fig. 7.\nC.2 GRID COLLAGE IMAGE DATA\nGrid Collage Image Data is the second type of MIA-DPO data we constructed, where multiple images are stitched together, and each image is assigned a label such as 'Imagel' to indicate which image the instructions refer to for the LVLMs. The number of images in the Grid Collage Data ranges from 2 to 9, forming a large image composed of 1 to 3 rows or columns of smaller images. By combining multiple images, Grid Collage Image Data mixes a vast array of visual elements and details, posing high demands on LVLMs. The instructions for Grid Collage Data involve questioning specific visual elements within the image, with other visual elements serving as interference factors. This data type primarily targets the Element Interference type of hallucination, while the numbered labels for each sub-image also assist the model in addressing the Sequence Confusion type of hallucination. We provide several examples of Grid Collage Image Data in Fig. 8.\nC.3 PIC-IN-PIC IMAGE DATA\nPic-in-Pic Image Data is the third type of MIA-DPO data we constructed. We randomly select two images, resizing one to about half the size of the other, and then paste the smaller image in the center of the larger one. The instructions for Pic-in-Pic Image Data involve questioning the central image, while the background image adds numerous visual elements and details that serve as interference. LVLMs need to carefully distinguish the relationships between these images and integrate the correct visual information to generate answers. Pic-in-Pic Image Data is primarily designed to address the Element Interference type of hallucination. We provide several examples of Pic-in-Pic Image Data in Fig. 9."}, {"title": "D MORE OBSERVATION", "content": "D.1 ATTENTION OBSERVATION\nIn the MIA-DPO architecture, a key step is the selection of chosen and rejected data based on attention. The core idea is to filter data according to the attention-aware capability of LVLMs. For"}, {"title": "D.2 HALLUCINATIONS OBSERVATION", "content": "In the context of multi-image reasoning, the types of hallucinations that LVLMs may produce are more diverse and varied. Therefore, during the data construction process, we need to specifically analyze these hallucination types. In addition to hallucinations that may occur in single-image tasks, such as existence, attributes, and relation Hallucination, we believe that two unique types of hallucinations may exist in multi-image tasks: Sequence Confusion and Element Interference. These two types of hallucinations are primarily caused by an excessive number of input images that the LVLMs cannot follow in sequence, as well as the overwhelming number of image tokens and visual elements.\nIn the process of constructing DPO data, we take hallucination types as our starting point and thoroughly consider solutions for these two types of hallucinations. More hallucination cases are already presented in Fig. 7, Fig. 8, Fig. 9."}]}