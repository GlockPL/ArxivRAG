{"title": "Exploring and Controlling Diversity in LLM-Agent Conversation", "authors": ["KuanChao Chu", "Yi-Pei Chen", "Hideki Nakayama"], "abstract": "Diversity is a critical aspect of multi-agent communication. In this paper, we focus on controlling and exploring diversity in the context of open-domain multi-agent conversations, particularly for world simulation applications. We propose Adaptive Prompt Pruning (APP), a novel method that dynamically adjusts the content of the utterance generation prompt to control diversity using a single parameter, \u03bb. Through extensive experiments, we show that APP effectively controls the output diversity across models and datasets, with pruning more information leading to more diverse output. We comprehensively analyze the relationship between prompt content and conversational diversity. Our findings reveal that information from all components of the prompt generally constrains the diversity of the output, with the Memory block exerting the most significant influence. APP is compatible with established techniques like temperature sampling and top-p sampling, providing a versatile tool for diversity management. To address the trade-offs of increased diversity, such as inconsistencies with omitted information, we incorporate a post-generation correction step, which effectively balances diversity enhancement with output consistency. Additionally, we examine how prompt structure, including component order and length, impacts diversity. This study addresses key questions surrounding diversity in multi-agent world simulation, offering insights into its control, influencing factors, and associated trade-offs. Our contributions lay the foundation for systematically engineering diversity in LLM-based multi-agent collaborations, advancing their effectiveness in real-world applications.", "sections": [{"title": "1 Introduction", "content": "Communication is central to multi-agent collaboration. Particularly, diversity plays a pivotal role for multi-agent communication, as it directly influences the adaptability and creativity of agents in addressing complex, dynamic tasks. Diverse communication allows agents to explore a broader solution space, avoid redundancy, and introduce unique perspectives, thereby enhancing collective problem-solving capabilities for goal-oriented tasks and increasing realism for world simulation.\nIn this work, we define diversity as the range of variations generated under identical initial conditions, with a specific focus on multi-agent systems for world simulation (Park et al. 2023). The prompt for open-domain multi-agent conversations typically comprises several key components: an environment description, agent profiles and memory, dialogue history, and the current dialogue. While most previous works integrate these components into the prompt, it is unclear how these components affect the diversity. Does reducing the provided information lead to generalized but less diverse responses? Or does it encourage more open and varied outputs? Although previous studies have explored the influence of communication structures, the impact of communication content on interaction quality remains underexplored (Guo et al. 2024).\nTo address this gap, we propose Adaptive Prompt Pruning (APP), a removal-based approach for controlling diversity by dynamically adjusting the prompt content via a single parameter, \u03bb. We structured the prompt into blocks, each containing one or more items. Leveraging attention weights from raw output utterances, APP selectively removes items from the modularized prompt. A higher A corresponds to more aggressive pruning and, consequently, a greater potential for diversity. We investigate various design choices for the pruning selection, and comprehensively analyze the relation between prompt content and output diversity.\nUsing data from Park et al. (2023) and Wang, Chiu, and Chiu (2023), we demonstrate that APP effectively modulates the degree of diversity by pruning influential prompt components. Our findings reveal that all prompt components constrain diversity to some extent, with the Memory block having the most significant impact. In addition, APP is compatible with established diversity control techniques, such as temperature sampling (Ackley, Hinton, and Sejnowski 1985) or top-p sampling (Holtzman et al. 2020).\nWhile increasing diversity through prompt pruning can result in inconsistencies with omitted information, we mitigate this issue by introducing a correction step post-dialogue generation. Experimental results show that this approach balances the trade-off between enhancing diversity and preserving information consistency.\nBeyond pruning, we investigate the role of prompt structure, including the order and length of components, in influencing diversity. Our results indicate that component order significantly affects diversity, while excessively lengthy prompts hinder it. Moreover, we analyze the role of pre-existing knowledge within LLMs and its interaction with diversity by replacing agents' names with well-known or rare ones.\nIn summary, this paper tackles three fundamental questions related to diversity in multi-agent simulation: (1) How can diversity be effectively controlled in multi-agent communication? (2) How does prompt content influence the level of conversational diversity? (3) What trade-offs arise in diversity management, and how can they be mitigated? Our contributions are as follows:\n\u2022 We introduce Adaptive Prompt Pruning (APP), a novel approach for controlling output diversity in multi-agent communication while maintaining consistency.\n\u2022 We provide one of the first systematic investigations into the relationship between prompt content and diversity in multi-agent simulations.\nBy addressing these questions, we aim to lay the groundwork for understanding and engineering diversity in LLM-based multi-agent systems."}, {"title": "2 Data, Model, and Task for Diversity Evaluation", "content": "Data We leveraged the simulation logs released by Generative Agents (Park et al. 2023) as our primary dataset, referred to as GA. The logs consist of 290 dialogues simulating a day in a small town, which we treated as independent cases. From these, we evenly sampled 20 cases in chronological order for generation. In a conversation, each utterance generated by an LLM agent involves several dynamic steps simulating the internal cognitive behaviors, such as querying related memories, verifying the current environmental states, and integrating these pieces of information into a prompt to produce the final response. For each case, we extracted all necessary contextual information from the logs, including memory bases, location context, and dialogue history, ensuring accurate simulations.\nWe also utilized an extended dataset based on Humanoid Agents (Wang, Chiu, and Chiu 2023), referred to as HA, which extends GA by introducing new agent states such as basic needs, emotions, and relationship closeness. Following the same methodology, we augmented GA's 20 cases with these states, collectively referred to as human needs. Together, these two datasets cover key components of LLM agents and simulation content for human-like behavior (Xi et al. 2023; Cheng et al. 2024; Sumers et al. 2024).\nTo better manipulate the prompt for response generation, we adapted GA's templates by modularizing its content. We treated the prompt as a sequence comprising distinct blocks, each of which is a subsequence of multiple units. A unit represents the smallest element, which could be either a piece of information (an \u201citem\u201d, e.g., a single memory string) or an instruction (a \"text\", e.g., \"Here is the memory that is in Eddy Lin's head:\").\nModel We employed LLaMA 3 and LLaMA 3.1 (Dubey et al. 2024) as the backbone LLMs. Released in mid-2024, these models are among the most powerful and widely adopted open-source LLM families. LLaMA 3.1 demonstrates superior performance at the same scale, offering enhanced capabilities (AI 2024) and a significantly extended context window (128,000 compared to 8,192 tokens). For practical purposes, we used the 8B-Instruct models in half precision.\nTask We define diversity as the variation between dialogues generated under identical initial conditions across trials. In other words, it measures how different the dialogues are when simulating among the same set of LLM agents at the same moment. For each case, we conducted n = 10 simulations and measured diversity among these n dialogues. Two metrics were employed: sim and dist-N, which quantify diversity from lexical and semantic perspectives. The former calculates the mean pairwise cosine similarity of dialogue embeddings (Reimers 2019; Wang et al. 2021), while the latter computes the proportion of unique N-grams across all n dialogues (Li et al. 2016). We report the average scores on all cases."}, {"title": "3 Adaptive Prompt Pruning", "content": "While longer prompts offer more contextual clues and topics (Weston and Sukhbaatar 2023), potentially enriching outputs, they can also impose stronger constraints, leading to more deterministic results.\nWe conducted a preliminary ablation study: in each utterance generation, specific blocks were pruned from the prompt, and we observed the resulting changes in diversity. The findings are recorded in Table 2. It was observed that removing different blocks led to varying degrees of changes in diversity, with pruning all four blocks (RMbmpe) resulting in a significant increase in diversity. Under this condition, the prompt retained only the instructions and the current dialogue, leaving no agent-related information. This suggests such information collectively plays a constraining role in multi-agent simulation.\nBuilding on these findings, we aim to design a more granular approach to control the transition in diversity using a single parameter. We propose leveraging attention scores to guide content removal, targeting overemphasized portions of the prompt to regulate diversity. This strategy avoids directly altering attention mechanisms, thereby preserving the model's general abilities, while operating independently of specific block content, offering greater generality and applicability.", "3.1 Method": "We calculate the attention score for each unit using the response generated by the model. Using the full prompt as the input, the output response is a sequence of tokens denoted as r = {tr\u2081, tr\u2082,\u2026\u2026, tr\u2099 }. For any unit u, defined as u = {tu\u2081, tu\u2082, ..., tu\u2098 }, the attention values from r to u can be represented as a tensor a \u2208 R\u1d38\u00d7\u1d34\u00d7\u1d50\u00d7\u207f, where L is the number of attention layers in the model, and H is the number of attention heads. To facilitate comparison among units, we further compress the two-dimensional attention values between token sequences by applying a' = R(a), where a' \u2208 R\u1d38\u00d7\u1d34. Here, R(\u00b7) is a \u201csum-mean\u201d reducer, which first sums over the m dimension and then averages over the n dimension. This operation aggregates the total impacts from all tokens in u, averaged across r. Finally, we take the mean across the heads and sum across the layers\u00b9 to obtain the scalar attention score \u03b1\u1d64 for unit u,\n\\alpha_u = \\frac{1}{LH} \\sum_{i=1}^L \\sum_{j=1}^H a'_{ij} \\qquad(1)\nAfter obtaining scores for all units, a single parameter is introduced to control the intensity of removal. First, a subset of all units, denoted as U\u2090\u1d63\u2098 and referred to as \"removable units\", is selected based on user requirements. Units outside U\u2090\u1d63\u2098, such as task or output instruction units, are excluded from consideration for removal. Next, the elements in U\u2090\u1d63\u2098 are sorted in descending order based on their corresponding \u03b1\u1d64 values. A parameter \u03bb\u2208 [0, 1] is defined to determine the units to remove, such that the cumulative score of the selected units reaches \u03bb times the total score of U\u2090\u1d63\u2098. To meet this condition, elements are selected sequentially from the top of the sorted list (i.e., units with higher scores) until the cumulative score satisfies the threshold. Finally, the selected units are removed from the full prompt, after which utterance generation proceeds. This process is applied individually to each utterance generation in the dialogues. A detailed description is provided in Alg. 1.\nIn our implementation, the \u03b1\u1d64 values are computed as the average of results from three output responses. All items in the original prompt, except from \u201cCurrent dialogue\", are included in U\u2090\u1d63\u2098, as removing it would prevent the conversation from continuing. If all items within a single block are removed, the block is removed entirely."}, {"title": "3.2 Discussion", "content": "Main results We measured dialogue diversity under different \u03bb values. Fig. 2a and Fig. 2b represent the diversity scores and the corresponding unit/word removal ratio. As \u03bb increases, diversity generally rises until all removable units are eliminated, demonstrating that \u03bb effectively controls diversity enhancement. Since the units were selected for removal in descending order of attention scores, only a small fraction of content needed to be removed in the early stages of increasing \u03bb, as these units had higher scores. This observation inspired a new efficiency criterion: in general, a method that enhances diversity with less content removal is likely to be more efficient. Consequently, Fig. 2c plots diversity changes with the word removal ratio on the x-axis.\nTo validate the rationale behind descending-order selection, we compared it with ascending-order selection (dashed lines in the figures), where lower-score units are prioritized for removal. While ascending-order selection appears to yield greater diversity improvement, particularly for the dist-3 metric, this comes at the cost of removing more units for the same \u03bb. From an efficiency standpoint, Fig. 2c shows that descending-order selection is generally more efficient, except when \u03bb approaches 1.0.\nWe repeated the experiments across different models and datasets, with the results shown in Fig. 3. Among the models, LLaMA 3.1 exhibits higher initial diversity (X = 0.0). For HA, despite its longer prompts, it also achieves higher initial diversity (e.g., dist-3 increases from 0.535 to 0.546 on LLaMA 3). This can be attributed to the additional information regarding human needs, which provides more options for dialogue content. Nevertheless, as units are gradually removed, diversity continues to increase, peaking at x = 1.0. This demonstrates that such information primarily functions as a conditioning factor when generating utterances.\nReducer We evaluate the effects of different reducers R(.) on the selection results of unit removal. As shown in Fig. 4a, the solid line represents the \u201csum-mean\u201d method we adopted, while the dotted lines correspond to the \"mean-mean\" method. Unlike \"sum-mean\u201d, the \u201cmean-mean\u201d method averages the attention scores across all tokens within a unit instead of summing them, thereby reducing the scoring advantage of longer units. However, we observe that \"mean-mean\" achieves inferior improvements in diversity when the \u03bb value is large. Additionally, changes under the sim metric initially decrease and then increase, indicating a weaker linear relationship with \u03bb. Given the goal of serving as a control parameter, we argue that the \"sum-mean\" method, which preserves the length bias of units, is a more suitable choice.\nPost-removal attention scores Although Fig. 2 shows positive correlations of diversity with \u03bb (a) and word removal ratio (c), the reasons for the subtle changes in diversity when \u03bb or the word removal ratio is low remain unclear. Fig. 4b presents two metrics after pruning: (1) the proportion of the total scores for the remaining removable units (U\u2090\u1d63\u2098) relative to the total scores in U\u2090\u1d63\u2098 before removal (independent of \u03bb), represented by the red line, and (2) the proportion of scores contributed by the top three removable units. Since attention scores are redistributed after unit removal, the actual reduction in attention allocated to removable units does not perfectly align with \u03bb. The figure shows that this reduction is consistently smaller than \u03bb. For example, when \u03bb = 0.6, attention decreases by only 19%. This phenomenon is particularly evident for smaller \u03bb values, which may explain the limited growth in diversity during the early stages shown in Fig. 2: attention on removable units decreases only marginally, and the score proportion of the top-1 unit even increases. Moreover, when \u03bb exceeds 0.8, the attention proportion of the top units begins to rise, contradicting the trend of increasing diversity. This behavior may result from the substantial reduction in the number of remaining removable units.\nRetain-1 We conducted a specialized pruning experiment to investigate the impact on diversity when only a single item remains. This setup was designed to minimize the confounding effects of attention redistribution. Figure 4c illustrates the outcomes of keeping items with the highest (Hi) or lowest (Lo) attention scores from each block. The findings indicate that, for nearly all blocks and metrics, retaining items with the highest score (Hi) results in lower diversity compared to those with the lowest scores (Lo). This reinforces the notion that items with high attention scores adversely affect diversity.\nFor the Previous Dialogues block, the inter-group differences (p1Lo and plHi) are smaller, likely due to cases where the number of items is zero. By contrast, the Memory block has the most detrimental effect on diversity across all groups, even when only a single Memory item remains (e.g., dist-3 drops from 0.800 to 0.595 for m1Hi). This result likely reflects the behaviors learned by the pre-trained model for different block types.\nInterestingly, this outcome also explains the efficiency reversal between the two sorting settings observed in the tail of Fig. 2c. When x = 0.95, under the \u201cdescending\" setting, approximately 83.4% of the remaining items belong to the Memory block. In contrast, under the \"ascending\" setting, the dominant block type is Previous Dialogues (59.3%), with Memory accounting for only 1.6%. This disparity likely underpins the \"ascending\" setting's advantage toward the end."}, {"title": "4 Trade-off of Diversity Management", "content": "Using unit removal is an effective method to control and enhance dialogue diversity. However, the generated responses may conflict with the pruned information. To address this issue, we introduce a second step for revision to rectify potential discrepancies in the generated utterances."}, {"title": "4.1 Method", "content": "After generating a response controlled by \u03bb, we collect the removed units and the generated utterance to assess whether the utterance conflicts with the content of the removed units. If a conflict is detected, the utterance undergoes revision; otherwise, it is accepted as is. Figure 5 illustrates this workflow.\nIn our implementation, we use the same LLM for conflict detection, utilizing the following task prompt: \"{name of agent A} is now in a chat with {name of agent B} and going to say '{response}'. Are there any inconsistencies between this response and the statements above?\u201d The LLM generates a comment and assigns a score from 1 to 10, where higher scores indicate greater inconsistency. We take the average of three scoring runs as the final score and set a threshold \u03b8 = 6.67. If the score exceeds \u03b8, a conflict is identified. When a conflict occurs, there are two common approaches to revision: (1) Regenerating the utterance: Reverting to the previous stage to generate a new response. (2) Comment-based modification: Revising the utterance based on the generated comments (Pan et al. 2023). For simplicity, this study adopts the first approach by preparing multiple backup responses during the initial generation. The rollback process is repeated up to three times until the score drops below \u03b8, or the utterance with the lowest score is selected."}, {"title": "4.2 Discussion", "content": "Figure 6a compares the average inconsistency scores of dialogues before and after applying revision. As a baseline, we also estimate the score for \u03bb = 0.0, which does not involve unit removal but uses the same task prompt to assess consistency between the content of all items in the full prompt and the response. The results indicate that x = 0.0 and x = 1.0 correspond to the lowest and highest inconsistency scores without revision, respectively. However, the correlation between the degree of removal and the inconsistency score is not significant (e.g., the second-highest score occurs for X = 0.15, where fewer words are removed compared to higher \u03bb values). This may be because the error space for open-ended conversations is smaller than that for task-oriented ones, making higher A values unnecessary for introducing errors.\nAfter revision, the scores are consistently reduced, indicating that the model finds the revised responses more faithful. Notably, the revised scores are even lower than those for X = 0.0, suggesting that the model perceives flaws in outputs generated with the full prompt, which the revision process helps to improve. Regarding diversity, Figure 6b shows the diversity metrics with and without revision. While some metrics reveal slight reductions within certain A ranges, the overall results demonstrate that our method effectively enhances diversity while maintaining consistency between the utterance and all items.\nDespite these promising results, several directions warrant further exploration. First, investigating potential biases in the LLM's judgments and their correlation with dialogue diversity presents a valuable avenue for future research. Second, attention is needed for utterances that are difficult to revise solely by rolling back, such as when the agent is asked, \"What is your major?\" and lacks relevant information to respond faithfully. Drawing on the distinction between discrimination and criticism (Saunders et al. 2022), the LLM could be queried to assess its ability to \u201cknow\" the appropriate revision direction using the removed units. If capable, a comment-based modification could be applied; otherwise, rolling back could be used to benefit from diversity in generation. Combining these two approaches may improve pipeline efficiency."}, {"title": "5 Extended Analysis on Removal", "content": "In this section, we present additional perspectives to deepen the understanding of the unit removal method. Since the results for \"Remove memory (RMm)\" exhibit the most significant differences, we use this setting as a representative case to conduct the following experiments.\nOur method is compatible with established diversity control approaches. We evaluated unit removal alongside other prevalent methods for enhancing generation diversity, specifically (1) adjusting decoding parameters and (2) sequential generation. The results are summarized in Table 3.\nAdjusting decoding parameters (e.g., increasing temperature T, top-p) is a widely adopted strategy for enhancing diversity. This approach increases the likelihood of selecting low-probability tokens but may compromise coherence within a single sentence. As shown in Table 3, neither increasing T nor p achieved diversity improvements as significant as using RMm alone. Notably, combining RMm with these methods led to further enhancements in diversity. For instance, in LLaMA 3, the dist-3 metric increased from 0.578 to 0.674 when RMm was combined with T = 1.0.\nSequential generation modifies the generation process by producing multiple responses simultaneously rather than a single response. Under this setup, the model conditions on previous responses, deliberately varying topics to avoid duplication. For this approach, our implementation appends \"Please output TEN candidates\" to the task instruction and randomly selects one of the generated candidates as the final output utterance. The results demonstrate that this method yields a notable improvement in diversity, consistent with findings in Yao et al. (2023), which observed better performance for this setup when the space of response generation was more constrained. Moreover, combining sequential generation with RMm further enhances diversity. However, sequential generation has its drawbacks: response lengths tend to shorten due to the simultaneous generation of multiple candidates; some candidates may lack coherence with the given context; and the increased generation complexity occasionally leads to challenges when producing outputs in the correct JSON format.\nDiversity improvement is driven by the first few rounds. In previous sections, we examined diversity across different dialogue trials. But at what point does the divergence between dialogues occur? Using utterances as the unit of analysis, we calculated the diversity of utterances at corresponding positions across dialogues, employing the same similarity and dist-N metrics. As shown in Fig. 7, we compared the differences between the full prompt and RMm. Regardless of whether the removal operation was applied, diversity consistently increased during the initial rounds of dialogue, with index 1 (the listener's first response) being particularly critical. Building on this foundation, RMm further amplifies its divergence from the full prompt around indices 2 to 3, before stabilizing in the later stages of the dialogue."}, {"title": "Measuring the exclusiveness of content across settings", "content": "After applying the RMm setting, the diversity among different trials increases significantly. To further investigate whether RMm generates more novel content or whether most of the generated content overlaps with the dialogues produced under the full-prompt setting, we measure the exclusiveness of the generated dialogues between two settings. Given N dialogues generated under settings A and B, respectively, we compute the following metrics:\n1. Avg. B-to-A max similarity: The average of the maximum similarity scores for each dialogue in B compared to the dialogues in A.\n2. Exclusive unique n-gram ratio for B: The proportion of unique n-grams in all dialogues of B that do not appear in A.\nThe calculations for similarity and unique n-grams follow the same methodology used in this study. We compare the differences between the full-to-full (averaged over three different seeds) and full-to-RMm settings, with the results presented in Table 4. These findings indicate that RMm indeed generates more exclusive content."}, {"title": "6 Factors Affecting Diversity in Text", "content": "In addition to employing unit removal to control and enhance diversity, we also explored the factors influencing diversity from the perspective of the original text space. Specifically, we examined the effects of block order, block length, and name frequency. The results are presented in Table 5.\nBlock order critically affects diversity. The reasoning abilities of LLMs are known to be influenced by the order of premises (Chen et al. 2024b) and the placement of critical information (Liu et al. 2024a). In this experiment, we investigated whether the order of input elements also impacts dialogue diversity. To this end, we rearranged the blocks in the prompt (denoted by the sequence of their initials) in various orders and observed that the sequence in which the model processes agent information substantially influences diversity. For instance, reversing the order from bpmec to cempb resulted in a dramatic decline in quality and diversity, with the dist-3 metric dropping from 0.535 to 0.191. Under the cempb configuration, the generated dialogue started to repetitively cycle through the same round\u00b2, leading to a significant degradation in dist-N. Notably, the amplified context differences caused by such repetition also reduced sim scores, an embedding-based measure. One notable negative pattern was placing c at the beginning and b at the end. Additionally, a comparison of bmepc and bmecp (with dist-3 scores of 0.514 and 0.413, respectively) revealed that positioning p before c mitigates significant drops in diversity. This pattern aligns with the chronological order, underscoring that carefully adjusting the block order is crucial for a greater initial diversity.\nEffect of block length. We simulate variations in block length by randomly duplicating or deleting items within the blocks. The word count for each block containing items is adjusted to either 250 or 750 words (BLN250 and BLN750). For blocks other than memory, these operations effectively result in either an increase or no change in length. To isolate the effect of memory, we exclude it from the analysis. The results indicate that, compared to RMm, BLN250+RMm exhibits minimal differences in diversity, whereas BLN750+RMm shows a significant decline in the dist-N metric. This finding underscores the detrimental impact of excessive redundant content on diversity.\nA frequent name can enhance diversity as parametric knowledge is amplified. We employed name replacement to analyze the agent's reliance on parametric and in-context knowledge during dialogue generation, as well as its impact on diversity. Inspired by frequency sensitivity experiments in (McCoy et al. 2023), we replaced prompt names with two sets of fictional characters: \u201cHarry Potter with Severus Snape (HPSS)\" and \"Tifa Lockhart with Cloud Strife (TLCS).\" According to the C4 dataset (Dodge et al. 2021), a widely-used LLM pretraining corpus, these names vary significantly in frequency: \"Harry Potter\" appears 762,023 times, while \"Tifa Lockhart\" appears only 432 times. This disparity suggests differing learned strengths for these names and is one potential factor affecting the model's ability to leverage parametric knowledge.\nResults show that replacing names alone did not improve diversity (HPSS to full). However, when prompt content was further pruned (RMbmp), name replacement significantly increased diversity, as measured by dist-N (HPSS+RMbmp to RMbmp). Comparing name combinations (HPSS+RMbmp to TLCS+RMbmp) revealed that high-frequency names produced a more pronounced diversity boost. This suggests that pruning prompt content strengthens parametric knowledge, enabling outputs to integrate both parametric and in-context information, enhancing diversity. Notably, this improvement largely manifests as additional vocabulary\u00b3 in dialogue generation, increasing distinct n-grams but with minimal impact on dialogue embeddings. In summary, this experiment highlights how LLM agents utilize both knowledge resources, offering insights into their interplay and impact on diversity.\""}, {"title": "7 Related Work", "content": "Research in LLM-based multi-agents has explored effective collaboration and meaningful interaction between multiple agents to achieve a predefined goal or to simulate human behavior. The former are task-oriented, studying the communication strategy (Liu et al. 2024b) or the collaboration between agents of different roles such as a program manager and a software engineer for software development (Chen et al. 2024a; Hong et al. 2024). The latter are open-domain, investigating emergent human behavior or social simulation (Park et al. 2023; Gao et al. 2024). However, most of these works focus on task performance metrics rather than the intrinsic qualities of agent interactions. Chu, Chen, and Nakayama (2024) revealed the repetition, inconsistency, and hallucination problems in LLM-based multi-agent conversations.\nDiversity in natural language generation has long been a critical research challenge. Techniques such as temperature scaling (Ackley, Hinton, and Sejnowski 1985) or nucleus sampling (Holtzman et al. 2020) have been explored to generate varied responses while maintaining coherence. To reduce the cost of enhancing diversity, Lee et al. (2022) further improves upon nucleus sampling, achieving better trade-offs between generation diversity and factuality. Similarly, Chung, Kamar, and Amershi (2023) increases text generation diversity while maintaining data accuracy through human interventions.\nBalancing diversity and relevance in multi-turn dialogues remains non-trivial. Studies such as Li et al. (2016) have investigated diversity-promoting objectives like Maximum Mutual Information (MMI) to address response repetition in dialogue systems. Zhou et al. (2023) generated a large number of utterance candidates and selected the best one using NLI entailment scores to achieve the generation of diverse and coherent dialogues. However, controlling diversity in multi-agent conversations is still underdeveloped. Chu, Chen, and Nakayama (2024) applied dynamic similarity threshold to remove overly repetitive utterances. Our work bridges the gap of diversity control while maintaining consistency."}, {"title": "8 Conclusion", "content": "Diversity is crucial in LLM-based multi-agent systems. We introduced Adaptive Prompt Pruning (APP), a novel approach to control diversity in multi-agent conversations. APP modularly removes prompt items based on a single parameter, \u03bb, providing a flexible way to balance diversity and coherence.\nExperiments confirmed APP enhances diversity while retaining most of the original prompt. Our analysis validated the descending selection method and sum-mean attention reducing mechanism underpinning APP, as attention scores correlate positively with diversity. We also demonstrated that APP integrates well with existing methods like temperature and top-p sampling.\nTo address pruning-induced inconsistency, we proposed a post-generation correction step, effectively maintaining coherence. Additionally, we found that block order significantly affects diversity, while lengthy prompts hinder it, emphasizing concise, structured inputs.\nAPP offers a practical solution to manage diversity, fostering improved communication and collaboration among LLM-based agents, with implications for future advancements in multi-agent systems."}]}