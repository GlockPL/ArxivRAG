{"title": "Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models", "authors": ["Malak Mansour", "Ahmed Aly", "Bahey Tharwat", "Sarim Hashmi", "Dong An", "Ian Reid"], "abstract": "Large Language Models (LLMs) such as GPT-4, trained on huge amount of datasets spanning multiple domains, exhibit significant reasoning, understanding, and planning capabilities across various tasks. This study presents the first-ever work in Arabic language integration within the Vision-and-Language Navigation (VLN) domain in robotics, an area that has been notably underexplored in existing research. We perform a comprehensive evaluation of state-of-the-art multilingual Small Language Models (SLMs), including GPT-40 mini, Llama 3 8B, and Phi-3 medium 14B, alongside the Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure LLM-based instruction-following navigation agent, to assess the impact of language on navigation reasoning through zero-shot sequential action prediction using the R2R dataset.\nThrough comprehensive experiments, we demonstrate that our framework is capable of high-level planning for navigation tasks when provided with instructions in both English and Arabic. However, certain models struggled with reasoning and planning in the Arabic language due to inherent limitations in their capabilities, sub-optimal performance, and parsing issues. These findings highlight the importance of enhancing planning and reasoning capabilities in language models for effective navigation, emphasizing this as a key area for further development while also unlocking the potential of Arabic-language models for impactful real-world applications.", "sections": [{"title": "Introduction", "content": "With the rise of AI-driven robotics in smart cities, multilingual interaction systems have become increasingly crucial, particularly in the Middle East and North Africa (MENA) region, where investment in autonomous systems is growing rapidly. However, the scarcity of models trained on Arabic data presents a significant barrier to their deployment.\nDespite Arabic's importance, spoken by over 400 million people (Koto et al. 2024), its under-representation in Vision-and-Language Navigation (VLN) research limits the effectiveness of autonomous systems in Arabic-speaking regions.\nThis research addresses these gaps by examining how Arabic and English inputs influence the planning and reasoning capabilities of Language Models (LMs) in robotic VLN tasks. The goal is to contribute to developing more inclusive autonomous systems that address the linguistic and cultural diversity of the Arabic-speaking world by seamlessly understanding and executing navigation instructions in Arabic.\nSmall Language Models (SLMs) are more efficient and cost-effective than Large Language Models (LLMs) due to their smaller size and reduced computational requirements, making them suitable for deployment on resource-constrained devices (Lu et al. 2024). Their smaller footprint enables faster response times and easier integration into existing systems. Despite these advantages, SLMs have received comparatively less attention in research, presenting an opportunity to explore their potential for optimization, efficiency, and domain-specific applications in both language understanding and high-level planning for navigation tasks.\nIn this work, we focus on SLMs rather than Vision-Language Models (VLMs) due to SLMs' superior ability to process and reason about complex linguistic structures across multiple languages (Lu et al. 2024). As noted by (Rahmanzadehgervi et al. 2024), VLMs often face performance limitations in real-world navigation tasks, particularly because their approach to extracting visual features tends to neglect instruction prompts, thereby reducing adaptability and planning efficiency. Moreover, excluding vision from the reasoning process mitigates potential simulation-to-reality gaps during robot deployment, ensuring that systems are more robust in their ability to execute plans effectively in diverse real-world scenarios.\nOur evaluation involves inferencing and comparing state-of-the-art multilingual SLMs and Core42's Arabic-centered LLM, Jais 30B (Sengupta et al. 2023), in the NavGPT framework. The SLMs are OpenAI's GPT-40 mini (Vidhyashree 2024), Meta's Llama 3 8B (Dubey 2024), and Microsoft's Phi-3 medium 14B (Abdin et al. 2024). Using the Room-to-Room (R2R)-VLN dataset (Anderson et al. 2018), which provides English-language navigation instructions, we augment the data with Arabic translations generated using the Groq API for comparative analysis. The models are evaluated within the NavGPT framework, which operates in a zero-shot manner to predict sequential actions based on"}, {"title": "Related Works", "content": "textual descriptions of visual observations, navigation history, and navigable viewpoints, enabling a systematic assessment of their planning and reasoning capabilities, as illustrated in Figure 1 (Zhou, Hong, and Wu 2023).\nClassical planners\nRecent classical planning systems in robotics commonly employ Planning Domain Description Language (PDDL) or Answer Set Programming (ASP) as the underlying action language for planners (Ghallab, Nau, and Traverso 2016; Jiang et al. 2019). PDDL is a formal language designed for expressing planning problems and domains. It provides a structured way to define the initial state, goal state, and possible actions, enabling planners to generate valid sequences of actions to transition from the initial state to the goal state. It has been widely used due to its expressiveness and ability to handle various planning paradigms. ASP, on the other hand, is a declarative programming approach rooted in logic programming and non-monotonic reasoning. It allows for the encoding of complex planning tasks as sets of logical rules and constraints, making it well-suited for tasks requiring reasoning over discrete domains and handling combinatorial complexity.\nThese methods are strong in ensuring soundness, completeness, and efficiency in structured environments where tasks and states are well-defined. However, they struggle in open-world or dynamic scenarios, where predefined actions and states cannot always accommodate the unpredictability of the real world as they do not consider scene understanding. On the other hand, VLN models, which combine scene understanding and language instruction, have gained attraction in robotics (Zhou, Hong, and Wu 2023). These models allow robots to interpret and execute actions sequentially in every state based on visual inputs and natural language instructions, making them more adaptable to dynamic environments.\nVLN Task\nVLN tasks use natural language instructions to guide agents through diverse environments, testing the reasoning and comprehension capabilities of LMs (Gu et al. 2022). These tasks typically evaluate agents in indoor settings, such as navigating through rooms in a building (Anderson et al. 2018), or outdoor scenarios, such as traversing open landscapes or urban areas (Vargas-Munoz et al. 2021). Models must interpret navigation commands, align them with visual context, and generate appropriate actions in these varied environments.\nDespite their promise, VLN tasks pose significant challenges. Ambiguities in language instructions, such as vague descriptions or inconsistent phrasing, can hinder accurate alignment with visual cues (Anderson et al. 2018). Sparse or incomplete visual data, such as in dim indoor settings or cluttered outdoor scenes, complicate navigation (Rahmanzadehgervi et al. 2024). These challenges are exacerbated when operating in multilingual contexts, where instructions may carry linguistic nuances that models struggle to capture or disambiguate. Addressing these limitations is crucial for improving real-world deployment of VLN systems (Ku et al. 2020; Tan, Yu, and Bansal 2019).\nLanguage Models in Navigation\nLMs play a pivotal role in VLN tasks by interpreting instructions and guiding agents through diverse environments. SLMs, LLMs, and VLMs each have distinct capabilities. SLMs are lightweight and efficient, enabling real-time processing in resource-constrained settings (Lu et al. 2024). Their streamlined architecture ensures practical deployment, though they may lack the extensive training and generalization capabilities of LLMs. By contrast, LLMs excel in multilingual reasoning and zero-shot tasks due to their vast training on diverse datasets (Zhou, Hong, and Wu 2023; Zhou et al. 2024). However, their computational demands and reliance on text-only reasoning limit their suitability for scenarios requiring multimodal integration (Lu et al. 2019).\nVLMs specialize in combining visual and textual inputs, excelling in spatial reasoning tasks where strong visual grounding is critical. Despite this, VLMs often underperform in linguistically complex or multilingual scenarios, as their architectures prioritize visual processing over comprehensive language understanding (Rahmanzadehgervi et al. 2024; Koto et al. 2024). This study focuses on SLMs' ability to handle linguistic challenges like Arabic's morphological richness and syntactic complexity. This can enhance model robustness through exposure to diverse linguistic structures while ensuring efficient deployment in real-world applications, particularly in resource-constrained environments common in the MENA region.\nMultilingual Challenges and Arabic-Specific Context\nWhile most VLN studies focus on English-language instructions, they overlook the complexities and opportunities of non-English languages (Zhang et al. 2024). This work explores multilingual capabilities, particularly Arabic. Arabic,"}, {"title": "Problem Statement", "content": "The MENA region's growing reliance on autonomous systems emphasizes the need for multilingual VLN models capable of understanding Arabic, a language spoken by over 400 million people (Koto et al. 2024). Arabic's morphological richness and syntactic complexity present unique challenges for NLP (Khalati, Ali, and Al-Romany 2024), making it a valuable test case for evaluating language model reasoning. Despite this importance, existing VLN datasets, such as R2R, are predominantly in English (Anderson et al. 2018), leaving a critical gap in resources for Arabic-language tasks.\nLLMs like GPT (OpenAI 2024) and Jais (Sengupta et al. 2023) have demonstrated strong multilingual reasoning capabilities, but their performance in Arabic navigation contexts remains under-explored. SLMs are cost-effective and practical for deployment on resource-constrained devices, such as for real-time navigation; however, their potential remains underrepresented in VLN research. Few studies have compared SLMs to LLMs in the context of multilingual navigation, highlighting a gap in the literature (Vidhyashree 2024). Addressing these gaps, this study explores how Arabic and English instructions affect reasoning capabilities in state-of-the-art SLMs and LLMs, aiming to improve inclusivity and adaptability in robotic navigation systems."}, {"title": "Methodology", "content": "In this section, we outline the problem formulation, method for translating the English R2R dataset to Arabic, and how NavGPT, a state-of-the-art LM-based navigation system, is adapted to process Arabic instructions. Our approach encompasses dataset translation, model setup, inference execution, and performance evaluation laid out in the pipeline in Figure 2.\nVLN Problem Formulation\nNavGPT addresses the VLN problem by framing it as follows (Zhou, Hong, and Wu 2023). Given a natural language instruction W, represented as a sequence of words W1, W2, W3, ..., wn, the agent retrieves an observation O at each step st by interpreting its current location through a simulator. This observation consists of N alternative viewpoints, representing the surrounding environment of the agent in varying angles.\nEach unique view observation is denoted as oi (i < N), with its corresponding angle direction represented as ai (i < N). Consequently, the observation at step t can be expressed as:\nOt = ((o1, a\u2081), (o2, a\u2082), ..., (oN, aN))\nDuring navigation, the agent's action space is restricted to the navigation graph G, where G is a predefined graph of navigable viewpoints. At each step, the agent selects the next action from the M = |Ct+1| set of navigable viewpoints, Ct+1. This selection is guided by aligning the current observation Oc\u2081 with the provided instruction W. The agent predicts the next action by identifying the relative angle ac from Oc\u2081, executes this action through interaction with the simulator, and transitions from the current state st = (vt, \u03b8t, \u03a6t) to the next state st+1 = (vt+1, \u03b8t+1, \u03a6t+1), where v, \u03b8, and \u03a6 represent the agent's current viewpoint, heading, and elevation angle, respectively.\nTo support navigation, the agent maintains a history of its previous states ht and updates the conditional transition probability between states as follows:\nSt = T (St+1|ac\u2081, St, ht)\nwhere T represents the conditional transition probability distribution.\nIn summary, the agent learns a policy \ud835\udf0b parametrized by \u0398 that relies on the instruction W and the current observation Oct, expressed as:\n\u03c0(at|W, Ot, OC\u2081, St; \u0398)\nThis study conducts the VLN task in a zero-shot setting, where \u0398 is not trained using VLN-specific datasets but is"}, {"title": "Experimental Setup", "content": "instead derived from the language corpus used to train the LMs.\nDataset Translation\nTo convert the English R2R dataset to Arabic, we used the Groq API, specifically the Llama-3.2-90B-text-preview model. We then developed a simple prompt-based code to translate the English instructions, objects list, and observations in the R2R dataset to Arabic, ensuring that the translated dataset maintained the same format as the original English version for seamless compatibility with NavGPT. This alignment facilitates using both the original and translated datasets in comparative experiments.\nIncorporating LMs\nTo explore different language models with NavGPT, we utilized LMs deployed on Azure for inference, enabling a comparative analysis. Following NavGPT's pipeline shown in Figure 1, we configured the SLMs (GPT-40 mini, Llama, Phi-3) and Jais 30B LLM, and tested them on the English and Arabic datasets to facilitate a direct comparison of their performances.\nThe selected SLMs were chosen for their diverse architecture sizes, multilingual capabilities, and unique approaches to processing input. GPT-40 mini, with its compact architecture, demonstrates efficiency in understanding and reasoning across languages by leveraging large-scale multilingual training (Vidhyashree 2024). Llama 3 8B excels in instruction-following tasks, combining a medium-sized model with strong contextual understanding (Dubey 2024). Phi-3 Medium 14B balances scalability and reasoning power, enabling nuanced task-specific performance (Abdin et al. 2024). Jais 30B, optimized for Arabic, enhances linguistic diversity by deeply integrating Arabic-specific datasets, ensuring accurate comprehension and generation (Sengupta et al. 2023). These models allow for analyzing the effects of multilingual input on reasoning, focusing on whether instructions are directly reasoned upon or internally translated.\nInference with NavGPT\nThe NavGPT framework integrates natural language instructions and visual observations for autonomous navigation. Instructions are processed alongside environmental data using Visual Foundation Models, which extract key features from the current viewpoint. A Prompt Manager formats this information into structured inputs for an LM, which reasons over the trajectory to decide the next action or stop. A history buffer tracks previous states to ensure consistent decision-making. This pipeline facilitates robust multimodal navigation with real-time reasoning capabilities tailored to user instructions.\nWe ran NavGPT in inference mode using the SLMs and Jais for a subset of the data. We assessed the configured LMs' effectiveness on English and Arabic instructions and evaluated their performance to see if language affects reasoning in navigation. We used 100 sample trajectories from the val unseen dataset in the R2R dataset. Each trajectory output includes:"}, {"title": "Evaulation Metrics", "content": "\u2022 Action Input: The upcoming trajectory ID\n\u2022 Observation: Textual descriptions of the environment at each location\n\u2022 Thought: The robot's thoughts, reasoning, and planning about reaching the target location and identifying obstacles\n\u2022 Evaluation Metrics: Action steps, total steps, path lengths, navigation error, oracle error, success rate (SR), oracle success rate (oracle SR), success weighted by path length (SPL), normalized dynamic time warping (nDTW), success weighted by dynamic time warping (SDTW), and coverage length score (CLS)\nThrough these steps, our methodology combines translation, inference, and evaluation, providing a structured approach to deploying NavGPT for Arabic-language navigation tasks. This approach ultimately allows us to measure the model's efficacy across languages and guide improvements for multilingual robotic navigation.\nExperimental Setup\nThe experiments were conducted using a combination of local hardware and cloud-based APIs. The local setup included machines with NVIDIA Quadro 6000 GPUs, each with 24 GB of memory, primarily for dataset preparation and evaluation tasks. The models evaluated\u2014GPT-40 mini (Vidhyashree 2024), Llama 3 8B (Dubey 2024), Phi-3 Medium 14B (Abdin et al. 2024), and Jais 30B (Sengupta et al. 2023) were hosted on Azure's serverless platform and accessed via APIs in the same configuration. This setup ensured consistency in model performance while leveraging Azure's scalability. For dataset augmentation, the Groq API was used to generate Arabic translations of English instructions in the R2R dataset.\nThis study evaluates the zero-shot reasoning capabilities of pre-trained language models, focusing on their ability to handle navigation tasks in both English and Arabic. No training or fine-tuning was performed. Input instructions and navigation trajectories were fed directly to the models via APIs without modifications to the underlying model architecture. To ensure outputs were in the correct format, different prompts were used depending on the model, aligning responses with the required structure for evaluation. This consistent configuration allowed for a controlled and fair comparison across all models.\nDataset\nThe evaluation utilized the R2R dataset alongside its Arabic-augmented counterpart. Arabic translations were generated using the Groq API, maintaining alignment with the original English instructions. A total of 100 navigation trajectories were evaluated in each language. The same 100 trajectories from the English dataset were used in the augmented Arabic dataset to ensure consistency. This framework allowed for a direct comparison of language-specific model reasoning and navigation capabilities.\nEvaulation Metrics\nQuantitative assessment We compared the models with each other using the following standard evaluation metrics for VLN tasks (Anderson et al. 2018):\n\u2022 Trajectory Length (TL): the average distance traveled by the agent during navigation\n\u2022 Navigation Error (NE): the mean distance between the agent's final location and the target location\n\u2022 Success Rate (SR): measures the percentage of completed trajectories where the robot reaches its goal\n\u2022 Oracle Success Rate (OSR): evaluates whether the agent was on the right path even if it didn't stop at the exact target location\n\u2022 Success weighted by Path Length (SPL): considers the length of the trajectory relative to the shortest path\nQuantitative results are shown in Table 1.\nQualitative Assessment We also conducted a qualitative assessment to examine the models' performance, focusing on their reasoning and decision-making processes. This evaluation highlights subjective observations that help identify weaknesses in the models' reasoning and planning capabilities. Specifically, we assessed the following aspects:\n\u2022 Reasoning: Analyzed how effectively the models interpreted navigation instructions, integrated visual observations, and decomposed complex instructions into actionable sub-goals.\n\u2022 Spatial Awareness: Evaluated the models' ability to comprehend their current environment, maintain navigation history, and use this information to make accurate decisions.\n\u2022 Planning: Investigated how the models structured sequential steps, anticipated future states, and adjusted their plans dynamically based on the environment."}, {"title": "Results and Discussion", "content": "This study explored the performance of various language models in reasoning and understanding complex navigation instructions in English and Arabic. The models tested included GPT-40 mini, Llama 3, Phi-3, and Jais, with significant variations in their ability to parse and execute instructions. Furthermore, the qualitative assessment provides textual examples of some of the model outputs, including failed cases. Based on the models' performance, we categorized them into three groups: Working, Partially Working, and Not Working. We show that the relative limitations of LMs depend on the input language and planning aspects, which affect their response to the navigation instructions.\nQuantitative Assessment\nTable 1 presents the aggregated results of quantitative evaluation metrics. During inferencing, NavGPT outputs thoughts that demonstrate its reasoning process as it navigates the environment. We prompted the model to output its thoughts in Arabic whenever we were inferencing with the Arabic-translated dataset as input, creating a monolingual Arabic"}, {"title": "Qualitative Assessment", "content": "context. Only with GPT-40 mini did we mix input languages, combining Arabic datasets and English thoughts, to observe how this robust model performs.\nWe evaluated the models on 100 trajectories. However, some of the language models frequently ran out of context window when reasoning through more complex long instructions. This occurred primarily with smaller or less robust models, such as Phi and Jais, as reflected in their lower number of successful predictions out of 100 trajectories (Table 1).\nNavGPT relies on structured prompts to perform optimally in navigation tasks. Specifically, the input should include a well-defined task description, such as goal location and intermediate waypoints. When these structured inputs are missing or incomplete, the model often struggles to generate accurate predictions, as shown in Figures 5 and 6. These errors typically happen when the model cannot fully reason over the provided instructions, resulting in its failure to output the necessary information in the required format for continued navigation.\n\u2022 Working:\n1. GPT-40 mini: This model is trained on a large multilingual dataset, eliminating the need for explicit translations of non-English inputs. As a result, it successfully handles both English and Arabic datasets. Its performance metrics for monolingual English and Arabic scenarios were comparable, achieving the highest values for Trajectory Length (TL), Success Rate (SR), Oracle Success Rate (OSR), and Success weighted by Path Length (SPL), while maintaining the lowest Navigation Error (NE) compared to other models, showcasing its robustness. However, the mixed scenario of Arabic data with English reasoning experienced slightly higher navigation error, lower SR and SPL, and a marginally higher OSR (37.00) than the pure Arabic scenario (36.00). This discrepancy could be attributed to a misalignment between the Arabic dataset and English reasoning. Nevertheless, GPT-40 mini (SR=21) outperformed the next best model, Phi-3 (SR=7), by approximately three times and Llama 3 (SR=4) by approximately five times.\n2. Llama 3 8B: Llama 3 exhibited reasonable reasoning and planning capabilities when processing both English and Arabic instructions. However, it fell short of GPT-40 mini's performance metrics, likely due to its smaller multilingual training dataset and less optimization for diverse linguistic tasks. Its SR (4 for English and 3.12 for Arabic) indicates limited success in executing goal-oriented tasks. Despite these challenges, Llama 3's ability to handle Arabic instructions suggests it holds promise for future development in multilingual reasoning. Even with its smaller size (8B), Llama 3 successfully completed nearly all 100 trajectories with both datasets, demonstrating its robust capabilities and large context window.\n\u2022 Partially Working (Phi-3 medium):\nPhi-3 medium demonstrated competitive performance in processing English instructions but faced challenges due"}, {"title": "Limitations and Future Work", "content": "to a smaller number of successful predictions (41/100) and parsing issues with the viewpoint ID. These issues were likely caused by the model's strict format requirements for input alignment, highlighting its lack of robust natural language understanding. This limitation led to incorrect outputs, as detailed in the qualitative assessment. For Arabic tasks, Phi-3 failed entirely, which can be attributed to its non-multilingual nature and insufficient exposure to Arabic language data during training. Consequently, it was unable to process or generate meaningful outputs in Arabic. Moreover, the model only evaluated 18 out of 100 trajectories, revealing its limited robustness in handling complex tasks in Arabic.\n\u2022 Not Working (Jais):\nJais 30B, the only Arabic-centric LLM in this experiment, surprisingly exhibited poor reasoning capabilities in both Arabic and English, performing the worst across all critical metrics. Although it was expected to perform well due to its Arabic focus, Jais 30B struggled with reasoning in the context of navigation tasks. This poor performance may be attributed to its initial training by Core42, which was not specifically optimized for navigation-related tasks. However, despite these limitations, Jais 30B's large size allowed it to complete 82 out of 100 trajectories.\nThis analysis demonstrates that the underlying architecture and multilingual training of the LMs are more influential than the input language in determining performance. This conclusion is supported by the smaller performance variation observed within robust LMs like GPT-40 mini across English and Arabic datasets compared to the larger differences between models. Therefore, selecting high-capacity, multilingual LMs that can generalize across languages can mitigate the need for language-specific fine-tuning for navigation tasks.\nTable 1: Quantitative Analysis of the LMs with English and Arabic Datasets\nModel Data Succ. TL NE SR\u2191 OSR\u2191 SPL\u2191\nGPT-40 mini Eng 100 17.6 6.98 21.0 46.0 13.0\nAr 100 17.7 7.18 20.0 36.0 9.34\nMixed 100 17.1 7.87 16.0 37.0 8.08\nPhi-3 med Eng 41 6.89 7.65 7.32 7.32 5.66\nAr 18 2.36 8.51 0.00 0.00 0.00\nLlama 3 8B Eng 100 8.21 8.20 4.00 8.00 2.73\nAr 96 7.54 8.34 3.12 5.21 1.33\nJais 30B Eng 95 0.68 8.45 0.00 0.00 0.00\nAr 82 0.78 8.35 0.00 0.00 0.00\nQualitative Assessment\nSimilar to the quantitative evaluation, the qualitative evaluation was performed using both Arabic and English instructions, providing insights into how the input language affected the models' behavior and reasoning processes. The LM's planning capabilities and whether it is multilingual or"}, {"title": "Conclusion", "content": "not were observed to have a greater impact than the input language of instruction.\nWe evaluated the models on three planning specific metrics:\n\u2022 Goal Decomposition Accuracy: Measured how well the model broke down a high-level goal into actionable steps.\n\u2022 Sequential Consistency: Evaluated whether the model followed a logical sequence of actions without contradicting previous decisions.\n\u2022 Adaptability: Assessed the model's ability to dynamically adjust its plan when encountering unforeseen environmental changes.\nSuccessful: English Instructions and Thoughts Figure 3 shows an example where the agent successfully understood the instruction and navigated to the desired area using English instructions and thoughts. The model demonstrated accurate goal decomposition and sequential consistency, effectively aligning its actions with the given instructions.\nParsing Error As shown in Figure 5, parsing errors occur when the model fails to output the thought and action in the expected format. These errors include incorrect formatting, missing action outputs, or hallucinating non-existent viewpoints. Such issues directly affect the sequential consistency and adaptability of the model, highlighting areas for improvement in prompt design and system parsing mechanisms.\nJais Failing Jais often fails as shown in Figure 6. These failures indicate an inability to perform navigation tasks or comprehend the question. This behavior likely stems from the lack of instruction-based fine-tuning for navigation tasks"}, {"title": "Limitations and Future Work", "content": "and system prompt constraints, which limit its planning capabilities. Specifically, Jais struggled with sequential consistency and goal decomposition, suggesting that enhancements in task-specific fine-tuning could mitigate these issues.\nBy discussing planning-specific metrics and weaknesses, this assessment provides actionable insights into improving language models for navigation and task-planning scenarios.\nLimitations and Future Work\nThe limitations of the proposed work can be summarized as follows:\n\u2022 Lack of Visual Features: The visual images are not directly processed using an image encoder; instead, an image-to-text descriptor is used, resulting in information loss. As a result, we only depend on the textual depiction of visual scenes for language models.\n\u2022 Zero-Shot Reasoning Ability: The language models used in this study were not fine-tuned for the specific downstream task. Instead, we relied on their zero-shot reasoning and planning capabilities for unseen tasks, which were falling behind fine-tuned models (Zhou, Hong, and Wu 2023)."}, {"title": "Conclusion", "content": "This work explored the impact of language on VLN tasks by comparing multilingual SLMs with the Arabic-focused LLM, Jais, in processing and planning navigation instructions in both English and Arabic. We augmented the R2R dataset with machine-translated Arabic instructions and evaluated performance within the NavGPT framework. The results revealed that the robustness of a model's reasoning and planning capabilities mattered more than the"}]}