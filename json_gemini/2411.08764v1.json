{"title": "Flow reconstruction in time-varying geometries using graph neural networks", "authors": ["Bogdan A. Danciu", "Vito A. Pagone", "Benjamin B\u00f6hm", "Marius Schmidt", "Christos E. Frouzakis"], "abstract": "The paper presents a Graph Attention Convolutional Network (GACN) for flow reconstruction from very sparse data in time-varying geometries. The model incorporates a feature propagation algorithm as a preprocessing step to handle extremely sparse inputs, leveraging information from neighboring nodes to initialize missing features. In addition, a binary indicator is introduced as a validity mask to distinguish between the original and propagated data points, enabling more effective learning from sparse inputs. Trained on a unique data set of Direct Numerical Simulations (DNS) of a motored engine at a technically relevant operating condition, the GACN shows robust performance across different resolutions and domain sizes and can effectively handle unstructured data and variable input sizes. The model is tested on previously unseen DNS data as well as on an experimental data set from Particle Image Velocimetry (PIV) measurements that were not considered during training. A comparative analysis shows that the GACN consistently outperforms both a conventional Convolutional Neural Network (CNN) and cubic interpolation methods on the DNS and PIV test sets by achieving lower reconstruction errors and better capturing fine-scale turbulent structures. In particular, the GACN effectively reconstructs flow fields from domains up to 14 times larger than those observed during training, with the performance advantage increasing for larger domains.", "sections": [{"title": "1. Introduction", "content": "The reconstruction of spatial fields from sparse and limited data is a major challenge in the analysis, estimation and control of complex physical systems. In various fields such as atmospheric research (Tello Alonso et al., 2010; Mishra et al., 2014), autonomous aerial navigation (Achermann et al., 2019, 2024) and fluid dynamics (Fukami et al., 2019), conventional linear methods such as linear stochastic estimation (Adrian and Moin, 1988), Delaunay triangulation (Saini et al., 2016) and proper orthogonal decomposition (Bui-Thanh et al., 2004; Dru-ault et al., 2005) face challenges in accurately reconstructing extensive spatial patterns. This is especially true when dealing with very sparse data and prob-lems such as system nonlinearity and boundary effects. Neural Networks (NN) have emerged as a promising nonlinear alternative that has proven to be effective in efficiently reconstructing chaotic data from sparse information. Building on this foundation, Machine Learning (ML) has achieved remarkable success in generating flow fields using data from experimental observations and numerical simulations (e.g. Kissas et al. (2020); Shengnan et al. (2019); Cai et al. (2019)).\nRecent work has demonstrated the power of ML approaches in fluid dynamics applications. Morimoto et al. (2021) used a Convolutional Neural Network (CNN) to analyze artificial Particle Image Velocimetry (PIV) data, and propose a novel approach for reconstructing flow fields from snapshots containing regions with missing data. Kochkov et al. (2021) employed an end-to-end CNN-based model to improve approximations inside Computational Fluid Dynamics (CFD) domains for modeling two-dimensional (2D) turbulent flows. Manickathan et al. (2022) proposed CNN as an alternative to the image cross-correlation methods commonly used in processing PIV data to reconstruct the fluid velocity field, and it was shown to outperform conventional methods in terms of robustness to data noise and providing reconstructed velocity fields with significantly higher spatial resolution. Super-Resolution (SR) methods have also been used to reconstruct high-resolution flow fields from low-resolution data. Fukami et al. (2019) utilized a CNN and a hybrid downsampled skip-connection/multi-scale (DSC/MS) CNN model, for the SR analysis of turbulent flow fields from very coarse data. Kim et al. (2021) used the cycle-consistent generative adversarial network (CycleGAN) to reconstruct flow fields from low-resolution DNS and Large Eddy Simulation (LES) data. Bode et al. (2021, 2023) developed a physics-based super-resolution generative adversarial network (PIESRGAN) for subfilter-scale turbulence reconstruction that uses a loss function based on the continuity equation residue. Even though only homogeneous isotropic data was used to train the model, it was able to make better predictions of scalar mixing in a reacting jet.\nThe aforementioned models rely on convolutional layers and can therefore be applied to unstructured data only to a limited extent. Since conventional ML methods require a feature matrix with a specific size and order of input samples, they cannot be readily applied to unstructured data. However, flow field data can be highly unstructured due to irregular meshes in curved or complex geometries. Several approaches have been developed to address the limitations of CNN when handling unstructured data in fluid dynamics. Heaney et al. (2024) proposed using space-filling curves to transform multi-dimensional solutions on unstructured meshes into a one-dimensional (1D) representation, allowing for the application of 1D convolutional layers. Xu et al. (2021b) developed an unstructured CNN that aggregates and exploits features from neighboring nodes through a weight function, enabling convolutions on irregular grids. Kashefi et al. (2021); Kashefi and Mukerji (2022) explored point cloud deep learning frameworks where CFD grid vertices are treated as point clouds and used as inputs to neural networks. Graph Convolutional Networks (GCN) have also emerged as a powerful tool for unstructured data. He et al. (2022) introduced the flow completion network (FCN) that employs a GCN to deduce fluid dynamics from sparse data sets. Duth\u00e9 et al. (2023) also used a GCN to predict the flow field and far-field boundary conditions based on the pressure distribution at the surface of airfoils.\nDespite these advancements, several challenges remain in applying ML techniques to fluid dynamics problems. One significant limitation is the difficulty in handling extremely sparse data, where traditional interpolation methods often fail to capture complex flow features. Additionally, time-varying geometries pose a unique challenge, as most current ML models are designed for static configurations and struggle to adapt to dynamic changes in the flow domain. Another critical issue is the generalization of ML models across different types of data sets, from high-fidelity numerical simulations to experimental measurements. The discrepancies in data quality, resolution, and underlying physics between these sources can lead to poor accuracy when models trained on one type of data are applied to another.\nIn this paper, we introduce a Graph Attention Convolutional Network (GACN) trained on a unique data set of three-dimensional (3D) Direct Numerical Simulations (DNS) modeling the compression-expansion stroke of a single-cylinder Internal Combustion Engine (ICE) under practically relevant operating conditions. The DNS data presents specific challenges for conventional ML applications due to its unstructured grid that dynamically changes during compression, leading to variations in resolution. GACN effectively addresses both aspects: the unstructured nature of the data is captured by the position of the graph nodes, while the changing resolution is accounted for by the distance features of the edges between nodes. Furthermore, we present a method to handle extremely sparse data, which performs remarkably well even when 99% of the data is missing. This is achieved by incorporating a Feature Propagation (FP) algorithm as a preprocessing step and adding a Binary Indicator (BI) as an extra feature. The FP algorithm initializes absent features with values that are both reasonable and physically consistent, leveraging data from neighboring nodes. The BI serves as a validity mask, providing crucial information to the network about which data points are original and which are propagated, enabling a more effective learning from sparse inputs without the limitations of using default values for missing data. Notably, we demonstrate the robustness and versatility of the approach by successfully applying the model trained on DNS data to experimental PIV measurements, achieving promising results despite the inherent differences between numerical and experimental data sources.\nThe rest of the paper is organized as follows. Section 2 introduces the DNS and PIV data sets used for training and evaluating the model. In Sec. 3, we provide a detailed description of the GACN model, as well as the alternative models used for comparison, specifically a CNN and classical cubic interpolation. Section 4 presents a comprehensive analysis of the model predictive accuracy using both numerical data derived from DNS simulations and experimental data from PIV measurements which was not used during training. We compare the performance of the GACN model to a CNN architecture for which data was interpolated onto a uniform mesh and classical cubic interpolation. The main conclusions and future research directions are outlined in Sec. 5."}, {"title": "2. Data Sets", "content": ""}, {"title": "2.1. DNS data", "content": "The DNS data set consists of a multi-cycle simulation of a motored laboratory-scale engine at practically relevant conditions (1500 rpm and full load 0.95 bar intake pressure) (Danciu et al., 2023, 2024). The single-cylinder, optically-accessible engine studied at TU Darmstadt has a four-valve pent-roof cylinder head and an intake duct that promotes the formation of a tumbling flow. The cylinder of the square engine has a bore of B = 86 mm. The computational domain is shown in Fig. 1."}, {"title": "2.2. PIV data", "content": "High-speed PIV was used to measure the two in-plane velocity components of the flow field in the central tumble plane of the engine. The beam of a dual-cavity Nd:YAG laser (IS4II-DE Edgewave) operating at 4.8 kHz, frequency doubled to 532 nm, was passed through a combination of a telescope and a negative cylindrical lens to form a light sheet (0.8 mm thick at 13.5% of maximum intensity) introduced through the piston window, coaxial to the cylinder centerline via the Bowditch extension and piston mirror. The crank-angle dependent time interval \u0394t between pulses was set between 3\u201351 \u03bcs to achieve optimal particle displacement and minimize out-of-plane losses. DOWSIL 510 (Dow Corning) silicone oil was atomized by a fluid seeder (AGF 10.0, Palas) with an average particle size of 0.5 \u03bcm and introduced into the intake system as tracer particles. A high-speed CMOS camera (Phantom v1610) with a Sigma lens (105 mm F2.8 Macro, f/11) was used to record the Mie scattering of the tracer particles perpendicular to the light sheet through the quartz glass cylinder.\nDaVis 8.4 (LaVision) was used to calculate the flow fields using cross-correlation with multiple pass iterations with decreasing window size (twice: 64 \u00d7 64 pixels, 50% overlap; twice: 32 \u00d7 32 pixels, 75% overlap), a peak ratio threshold of 1.3, and a universal outlier median filter to remove false vectors. Possible sources of error and statistical uncertainties are discussed in detail in Haussmann and et al. (2020). The instantaneous velocity magnitudes uncertainty is estimated to be about 5%.\nIn our study we only use the PIV data for testing purposes, as the GACN model never sees it during training. In this way, we can test the generalization capabilities"}, {"title": "2.3. Training, validation and test data sets", "content": "Two-dimensional slices along the xz tumble plane were extracted from the 3D DNS data. A total of 19 slices, spaced 4 mm apart, were selected along the y-direction. To maintain reasonable computational cost, depending on the piston position, up to 9 panels were selected on each slice (Fig. 1). Due to the dynamic piston movement, the number of grid points in each panel varies between 130,000 and 470,000 during mid-compression. Panels were sampled every 5 crank-angle degrees (CAD) from -120 CAD (before top dead center) up to -60 CAD. The bulk Reynolds number, $Re = BV_p/\\nu$, calculated using the mean piston speed $V_p$, bore B, and the kinematic viscosity \u03bd of the gas, ranges between 35,000 and 120,000. Of the 12 simulated cycles, 10 were allocated for training and validation, while 2 were reserved for testing. In total, 9,754 panels were created for training and validation, and 2,237 panels for testing. Additionally, 840 xz slices covering the whole vertical extent were selected exclusively for testing purposes. To evaluate model performance on experimental data across different resolutions and domain sizes, panels and complete xz slices were selected from the central tumble plane of the PIV data. A total of 612 panels and 204 slices from 17 cycles were sampled every 5 CAD from -120 CAD to -60 CAD."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. The GACN model", "content": ""}, {"title": "3.1.1. Input and label composition", "content": "The input and label for the GACN model consist of graphs, where each node corresponds to a grid point as described in Sec. 2.3. The nodes carry five features:"}, {"title": "3.1.2. GACN architecture", "content": "The GACN architecture is shown in Fig. 2. We use the FP algorithm proposed by Rossi et al. (2022) as a preprocessing step before feeding the input to the network. The algorithm is an efficient iterative process for reconstructing missing node features in graphs, and operates on the principle of Dirichlet energy minimization to diffuse known features throughout the entire graph and effectively reconstruct missing features even when a large fraction of the features is not available."}, {"title": "3.1.3. Loss function", "content": "For the GACN model, we employ as the primary optimization criterion the Mean Squared Error (MSE) $L_{MSE}$, i.e. the $L_2$ norm of the error to ensures that the network predictions closely match the actual velocities:\n$L_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (u_i - \\hat{u}_i)^2$.\nHere, ui and \u00fb denote the actual and the predicted velocity at graph node i, respectively."}, {"title": "3.2. CNN model used for comparison", "content": ""}, {"title": "3.2.1. Input and label composition", "content": "The input to the CNN model consists of three features: the two velocity components (ux and uz) and a binary mask that serves a purpose similar to the BI in the GACN model, marking the points that contain valid data or are zero-valued. To prepare the data for the CNN, we interpolated the DNS panels to a uniform 512 \u00d7 512 grid, resulting in 262,144 points. This number is chosen to match the average number of nodes used as inputs to the GACN model (about 250,000 nodes per panel.) To simulate sparse data conditions and maintain consistency with the GACN input, we randomly set 99% of the input velocity values to zero, retaining only 1% of the original velocity data. The binary mask is then used to specify these non-zero velocity locations. This approach ensures a consistent resolution and sparsity level in both architectures and preserves the small-scale velocity features of the input data. The interpolation process maintains the spatial structure of the flow field while adapting it to the required input format for the CNN. The labels for the CNN model are, similar to the GACN, the two predicted velocity components."}, {"title": "3.2.2. CNN architecture", "content": "The CNN architecture consists of eight layers evenly distributed between the encoder and decoder parts (Fig. 3). The network input comprises three channels: two for the velocity components and one for the binary mask. We opted against a U-shaped network architecture, as initial experiments have shown that such designs tend to produce more pronounced checkerboard artifacts in the reconstructed flow fields. Instead, the architecture maintains the spatial dimensions throughout all layers, preserving the original size of the input. The four-layer encoder section of the CNN gradually increases the channel depth from 4 to 128, allowing the network to capture complex hierarchical patterns in the data. Each of these layers consists of a convolutional layer followed by a ReLU activation function that ensures effective feature extraction. The decoder, which also consists of four layers, methodically reduces the channel depth back to two in order to adapt to the original velocity channels while maintaining spatial dimensions. This is achieved by transposed convolutional layers. To preserve important spatial details that might otherwise be lost during encoding, and to improve the network ability to accurately reconstruct patterns, we implement skip connections between the corresponding layers in the encoder and decoder. These connections allow the model to utilize both high-level abstract features and low-level detailed information during the reconstruction process.\nThe resulting CNN model contains 243,881 trainable parameters. This number is higher than that of the GACN model (148,881 parameters), but was intentionally kept comparable in size to ensure a fair comparison between the two architectures. This similarity in model size allows us to evaluate the differences in performance primarily in terms of architectural approaches rather than differences in model complexity."}, {"title": "3.2.3. Loss function", "content": "For the CNN, the loss functions for training consists of two components. The MSE loss ($L_{MSE}$) (Eq. 2) is combined with the Total Variation (TV) loss $L_{TV}$ (Chambolle et al., 2009) defined as the sum of the absolute differences between neighboring pixel values,\n$L_{TV} = \\frac{2}{N} \\sum_{i=1}^{N} (|u_{i+1,j} - u_{i,j}| + |u_{i,j+1} \u2013 u_{i,j}|)$,\nwhere $u_{i,j}$ represents the predicted velocity value at pixel $(i, j)$, and N is the total number of pixels. This ensure that the predictions do not exhibit abrupt spatial changes, which is unlikely in real fluid flows. By penalizing high-frequency fluctuations, the $L_{TV}$ term results in more coherent and realistic flow fields and avoids pixelated results. The incorporation of the TV loss not only improved the visual quality of the predictions but also enhanced the overall performance of the network. We observed that models trained with this loss demonstrated better generalization capabilities and produced more accurate reconstructions.\nThe complete loss function for the CNN is thus formulated as a weighted sum of the aforementioned losses:\n$L_{total} = L_{MSE} + \u03b1L_{TV}$,\nwhere the hyperparameter \u03b1 controls the relative importance of the TV loss by balancing its contribution to the total loss."}, {"title": "3.3. Implementation details", "content": "The GACN and CNN models were implemented with Python, using the PyTorch (Paszke and et al., 2019) and PyTorch Geometric (Fey and Lenssen, 2019) libraries. PyTorch Geometric, a specialized extension of PyTorch, was crucial for the efficient handling of graph data structures and facilitated the training and evaluation of the GACN model. We trained the GACN for 100 epochs and the CNN for 150 epochs. The training processes started from scratch by randomly initializing the weights of the networks using the Glorot (Xavier) initializer (Glorot and Bengio, 2010), with the biases initialized to zero. Both models were trained end-to-end using the Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of $10^{-4}$. To optimize the learning process, we employed an adaptive learning rate strategy that reduced the rate when validation performance plateaued. This approach, commonly known as learning rate decay, helps to fine-tune the convergence of the model. The training was performed on a single node of the JUWELS Booster GPU cluster at the J\u00fclich Supercomputing Centre, equipped with 2 AMD EPYC Rome CPUs (48 cores) and 4 NVIDIA A100 GPUs, each with 40 GB of memory. The training process required 48 and 50 node hours for the CNN and GACN models, respectively."}, {"title": "3.4. Performance metrics", "content": "To evaluate the accuracy of the model in reconstructing the flow fields, we used two complementary metrics: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). MAE measures the average magnitude of errors in a set of predictions and is defined as,\n$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |u_i - \\hat{u}_i|$.\nRMSE provides a quadratic scoring rule that measures the average error magnitude. It is particularly useful in our case as it assigns higher weight to large errors, which is relevant for capturing significant deviations in flow structures. It is defined as\n$RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (u_i - \\hat{u}_i)^2}$.\nThe individual metrics are averaged across all test cases to provide an overall assessment of the model performance."}, {"title": "4. Results and discussion", "content": ""}, {"title": "4.1. Ablation study", "content": "We conducted an ablation study to understand the impact of different components in the GACN model focusing on the effects of the FP algorithm, the BI, and the graph attention mechanism. Table 2 shows the model performance with different combinations of the FP algorithm and BI on the DNS panel test set consisting of 2237 samples. The results demonstrate that both the FP algorithm and the BI contribute to improved model performance, with the combination of both yielding the best results."}, {"title": "4.2. Reconstruction of DNS data", "content": "Figure 4 illustrates the reconstruction capabilities of the GACN model compared to two baseline methods: standard cubic interpolation, a common benchmark for modern ML-based flow reconstruction methods and the more competitive CNN model presented in Sec. 3.2. The input for this comparison comes from a test panel with 99% of the node velocity features set to zero. This sparse input is fed directly into the CNN and interpolation methods. For the GACN, the input is preprocessed by the FP algorithm to initialize the flow field before it enters the network. The results show that the ML-based methods significantly outperform cubic interpolation in flow reconstruction, with the GACN achieving better results overall. While the CNN has difficulty capturing the smallest turbulent scales and the finest features, the GACN shows a distinct ability to reconstruct these complex flow features. Cubic interpolation, although capable of capturing larger-scale features, it cannot accurately reconstruct smaller turbulent structures. In the last column of Fig. 4, the differences in accuracy are further highlighted by comparisons of the mean absolute error between the ground truth and predictions for each method.\nThe reconstruction capabilities of the GACN model was also compared using as input an xz slice that includes the cylinder head and the spark plug, the dark grey region from the slices in Fig. 5. The area that is approximately 14 times larger than the training data and contains about 3.6 million unique grid points, was processed with 99% of the velocity features set to zero (Fig. 5.) The GACN processed the entire slice without data pre-processing. For the CNN model, we interpolated the xz slice to a uniform 2048 \u00d7 2048 grid, which is 16 times larger than the grid used in training, to meet the fixed input size requirements and obtain a similar resolution as for the GACN model. Despite the considerable increase in domain size, both networks accurately reconstruct the complex flow, with the GACN performing better. The cubic interpolation method exhibits larger errors in this extended domain. The last column of Fig. 5 shows the mean absolute error (MAE) between the ground truth and predictions.\nThe scatter plots of the predictions from Figs. 4 and 5 depicted in Fig. 6 provide a direct comparison and emphasize the superior predictive capabilities of the GACN model. For both the analyzed panel and the larger slice, the GACN model shows a tighter clustering of points around the diagonal. Its higher $R^2$ scores compared to the CNN and interpolation methods for the analyzed panel and slice show that a larger proportion of the variance of the true velocities is captured by the model.\nFigure 7 illustrates the prediction error metrics averaged over the entire DNS test sets, which comprise 2237 test panels and 840 slices. For the panel test set, the GACN achieves an average MAE of 0.15m/s, which is significantly lower than the 0.32 m/s and 0.92 m/s observed for the CNN and cubic interpolation, respectively. In addition, the GACN exhibits a narrower MAE distribution compared to the CNN, highlighting its effectiveness in minimizing prediction errors and improving the overall reliability of the flow reconstruction. For the slice test set, which is the larger domain, the performance difference increases further. The GACN maintains its superior performance with an MAE of 0.42 m/s, while the CNN and interpolation methods yield an MAE of 0.78 m/s and 1.18 m/s, respectively. This significant difference in predictive accuracy at larger domains could be partly due to the graph structure of the GACN model. By including the distances between nodes as a training feature, the GACN gains a sense of scale, potentially facilitating better generalization to larger domain sizes."}, {"title": "4.3. Reconstruction of PIV data", "content": "The reconstruction capabilities were also tested using the PIV test set without fine tuning or retraining the network, using both 20 \u00d7 20 mm\u00b2 panels and entire xz slices as input. The PIV data has a significantly lower spatial resolution than the DNS with 1521 grid points on each panel and 16,641 on each xz slice. To evaluate the reconstruction accuracy and error of the networks, we randomly split the PIV data into two equal size subsets. One subset was used to generate the input (illustrated by the blue nodes in Fig.8, right.) The other half of the points was reserved for error testing (green nodes in Fig.8, right). In order to provide the networks an input with a size closer to what was seen during training, evenly spaced nodes were introduced between the PIV points (grey nodes in Fig. 8, right) increasing the total number of nodes to 76,000 for panels and 832,000 for slices. The features at the intermediate nodes were initialized with zero velocity values. After the addition of these extra nodes, the amount of valid information (non-zero velocity data) in the input was reduced to approximately 1% of the total nodes to match the sparsity level used in the DNS data set and during network training.\nTo evaluate the reconstruction accuracy of the network for this super-resolution-like task, we compared the predictions only for half of the PIV measurement points that were not used as input (green nodes in Fig.8, right). Figure 9 illustrates the results for one of the measured panel and slice. Both networks are able to reconstruct the flow with high accuracy, considering that more than 99% of the input domain contains zero values. The GACN model shows superior performance in capturing smaller flow features, resulting in a more accurate overall prediction. The computed MAE and RMSE values for the panel and slice, shown in Table 4, further confirm the improved predictive capabilities of the GACN model. It is important to note that this super-resolution-like task presents an interesting interpretative challenge. The ability of the network to generate realistic flow structures in these regions suggests a form of physics-informed interpolation that may provide insights beyond the original measurement resolution. However, this also emphasises the need for careful interpretation of the results, especially in regions for which no PIV data is available. Future work could investigate methods to validate these interpolated regions using higher resolution PIV measurements."}, {"title": "5. Conclusions", "content": "The paper presents a Graph Attention Convolutional Network (GACN) for flow reconstruction from very sparse data. The model employs a preprocessing step based on a Feature Propagation (FP) algorithm that effectively handles sparse inputs by leveraging information from neighboring nodes, improving the initialization of missing features, and ensuring physical consistency. Additionally, we introduce a binary indicator (BI) as an extra feature, serving as a validity mask to differentiate between original and propagated data points. This combination enables more effective learning from sparse inputs.\nThe network is trained and tested on one of the largest DNS data sets for Internal Combustion Engines (ICE), showcasing its ability to handle unstructured data and perform well at different resolutions and domain sizes. A key feature of the model is its capacity to effectively process time-varying engine data, where the computational domain changes due to piston motion. This results in variable input sizes throughout the engine cycle, which the GACN can seamlessly accommodate without requiring data padding or interpolation. Notably, the model exhibits robust generalization capabilities, performing effectively even when tested with experimental PIV data that were not used during training.\nThe comparative analysis shows that the GACN consistently outperforms both a conventional CNN and cubic interpolation methods. The GACN achieves lower reconstruction errors on both the DNS and PIV test sets, with the additional major advantage of not requiring data interpolation onto a structured grid, as is necessary for CNN. This ability to process unstructured data directly makes the GACN particularly suitable for complex geometries and time-varying domains, which are characteristic of ICE. In addition, the GACN model effectively reconstructs flow fields not only from small panels used in training, but also from entire slices with domains up to 14 times larger than those seen during the training phase. A key factor contributing to this generalization ability is likely the inclusion of distance features between nodes in the graph structure, giving the network a sense of scale and allowing it to adapt to larger domains by understanding the spatial relationships between data points, regardless of the overall size of the domain. The difference between GACN and other methods becomes more pronounced in these larger domains, highlighting its superior ability to capture and reconstruct complex flow patterns across different spatial scales.\nWhile this work focuses on flow reconstruction in ICE, the developed GACN model can potentially have broader applicability for various engineering applications where reconstruction from sparse or insufficiently resolved data is required, especially for unstructured domains. As a next step, we will investigate the predictive and modeling capabilities of the GACN for near-wall phenomena. An extension of the approach to full 3D flow field reconstruction is also of interest at the expense of a significant increase in the computational cost of model training. The application to reactive flows, using a newly developed version of the reactive flow solver, would be particularly interesting. This is challenging for various reasons, as in fast chemistry the source terms depend on the smallest scales, which means that these must be predicted correctly for such multi-scale fields."}]}