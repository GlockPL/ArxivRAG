{"title": "LLM-ABBA: Understand time series via symbolic approximation", "authors": ["Erin Carson", "Xinye Chen", "Cheng Kang"], "abstract": "The success of large language models (LLMs) for time series has been demonstrated in previous work. Utilizing a symbolic time series representation, one can efficiently bridge the gap between LLMs and time series. However, the remaining challenge is to exploit the semantic information hidden in time series by using symbols or existing tokens of LLMs, while aligning the embedding space of LLMs according to the hidden infor-mation of time series. The symbolic time series approximation (STSA) method called adaptive Brownian bridge-based symbolic aggregation (ABBA) shows outstanding efficacy in preserving salient time series features by modeling time series patterns in terms of amplitude and period while using existing tokens of LLMs.\nIn this paper, we introduce a method, called LLM-ABBA, that integrates ABBA into large language models for various down-stream time series tasks. By symbolizing time series, LLM-ABBA compares favorably to the recent state-of-the-art (SOTA) in UCR and three medical time series classification tasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to avoid obvi-ous drifting during prediction tasks by significantly mitigating the effects of cumulative error arising from misused symbols during the transition from symbols to numerical values. In time series regression tasks, LLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER) benchmarks. LLM-ABBA also shows competitive prediction capability compared to recent SOTA time series prediction results. We believe this framework can also seamlessly extend to other time series tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Time series are fundamental mathematical objects with appli-cations across diverse disciplines such as classification [1], regression [2], and prediction [3]. Recently, the power of large language models (LLMs) in time series applications has been recognized. One recent review [4] concludes that there are three main LLM-based approaches to learn intricate semantic and knowledge representations from time series to perform various tasks. The first approach is to patch and tokenize numerical signals and related text data, followed by\nfine-tuning on time series tasks [5]\u2013[7]; the second one is preprocessing time series data to fit LLM input spaces by adding a customized tokenizer [8]; the last one is to build foundation models from scratch, and this approach aims to create large, scalable models, both generic and domain-specific [9], [10].\nThese three techniques each come with their own limitations. Patching and tokenizing time series segments can build the mapping between time series and the latent embedding of LLMs, instead of discrete language tokens. When outputting the numerical value, this method should generate the digit one by one, which eventually reduces the generation speed [6]. Furthermore, by adding a customized tokenizer, LLMs can handle positions of time series patterns and reproduce the internal logic of given time series signals [11]. Because LLM tokenizers, not designed for numerical values, separate continuous values and ignore the temporal relationship of time series, this method should convert tokens into flexible continuous values [12]. It inevitably requires token transitions from time series feature space to the latent embedding space of LLMs and cannot avoid the risk of semantic loss. Building foundational time series models from scratch can essentially solve these problems. But considering that one should bal-ance the high development costs and their applicability, the challenge of expensive training persists and should be tackled [4].\nBy aligning time series and native language, large language models and specialized time series models constitute a new paradigm, where the LLMs are prompted with both time series and text-based instructions [4]. Following this paradigm, time series and textual information provide essential contexts, LLMs contribute to internal knowledge and reasoning ca-"}, {"title": "II. RELATED WORK", "content": "LLMs for time series methods have seen significant advances in recent years. The work [8] argues that this success stems from the ability of LLMs to naturally represent multimodal distributions of time series. By framing a time series fore-casting task as a sentence-to-sentence task, AutoTimes [18] minimizes the tunable parameters needed to generate time series embeddings while freezing the parameters of the LLM, and FPT [19] fine-tunes LLM parameters to serve as a gen-eral representation extractor for various time series analysis tasks. These approaches maximize the use of inherent token transitions, leading to improved model efficiency. In terms of multivariate time series forecasting, UniTime [20] trains and fine-tunes a language model to provide a unified forecasting framework across multiple time series domains. Leveraging advanced prompting designs and techniques, PromptCast [21] transforms time series data into text pairs, and TEMPO [22] models specific time series patterns, such as trends and sea-sonality, by using weighted scatterplot smoothing [23].\nTuning-based predictors use accessible LLM parameters, typ-ically involving pre-processing and tokenizing numerical sig-nals and related prompt text, followed by fine-tuning on time series tasks [4]. In summary, there are four steps needed to adapt LLM to time series:\n(i) $T_{inp}$ = Pre-processing(T): With a Patching operation [5], [18] or a weighted scatterplot smoothing processing [22], the time series set T is pre-processed to specific knowledge-contained inputs $T_{inp}$;\n(ii) $M_{inp}$ = Tokenizer (Prompt, $T_{inp}$): An additional op-tion is to perform a Tokenizer operation on time series $T_{inp}$ and related prompt text to form text sequence tokens $M_{inp}$;\n(iii) $M_{outp}$ = $f_{ALM}(M_{inp})$: With the instruction prompt Prompt, time series processed tokens and optional text tokens are fed into $f_{ALM}(\u00b7)$ with partial unfreezing or"}, {"title": "III. METHODOLOGIES", "content": "Our research is inspired by the observation that speech sig-nals often contain a plethora of semantic information [24], which enables the language model to perform extremely well across a multitude of tasks; see [4] and references therein. However, directly applying language models to time series is not possible due to the fact that time series are made up of numerical values and lack useful embedding patterns; further, the high dimensionality of time series makes it difficult for the sequential and recurrent model to capture the dependencies of time series features. Thus learning an informative sym-bolic time series representation while having dimensionality reduced is a practical yet challenging problem. ABBA-a symbolic approximation method is designed to address this as it compresses the time series to a symbolic presentation in terms of amplitude and period, and each symbol describes the oscillatory behavior of time series during a specific period.\nABBA utilizes adaptive polygonal chain approximation fol-lowed by mean-based clustering to achieve symbolization of time series. The reconstruction error of the representation can be modeled as a Brownian bridge with pinned start and end points. ABBA symbolization contains two dominant procedures, namely compression and digitization, to aggregate a time series $T = [t_1,t_2,...,t_n] \\in R^n$ into its symbolic representation $A = a_1a_2...a_N$ where $N < n$ and $a_i$ is an element in a specific letter set $L$, which is referred to as a dictionary in the ABBA procedure.\n1) Compression: The ABBA compression is performed to compute an adaptive piecewise linear continuous approx-imation (APCA) of T. The ABBA compression plays a critical role in dimensionality reduction in ABBA symbolic approximation-a user-specific tolerance, denoted by tol, is given to determine the degree of the reduction. The ABBA compression proceeds by adaptively selecting $N + 1$ indices $I=  {i_0 = 0 < i_1 < ... < i_N=n}$ given a tolerance tol such that the time series T is well approximated by a polygonal chain going through the points $(i_j,t_{i_j})$ for $j = 0,1,..., N$. This leads to a partition of T into N pieces $P_j = (len_j,inc_j)$ that represents cardinality and increment of $T_{[i_{j-1}:i_j]} = [t_{i_{j-1}},t_{i_{j-1}+1},...,t_{i_j}]$, which is calculated by $len_j \\in N := i_j \u2013 i_{j-1} \\geq 1$ and $inc_j \\in R := t_{i_j} - t_{i_{j-1}}$. As such, each piece $p_j$ is represented by a straight line connecting the endpoint values $t_{i_{j-1}}$ and $t_{i_j}$. Given an index $i_{j-1}$ and starting with $i_0 = 0$, the procedure seeks the largest possible $i_j$ such that $i_{j-1} < i_j < n$ and\n$\n\\sum_{i=i_{j-1}}^{i_j} (t_i - t_{i_{j-1}} - \\frac{i-i_{j-1}}{i_j-i_{j-1}} (t_{i_j}-t_{i_{j-1}}))^2 \\leq (i_j - i_{j-1} \u2013 1) \\cdot tol^2.\n$"}, {"title": "B. Error analysis reconstruction", "content": "We are concerned with the reconstruction error of ABBA's symbolization since a symbolic representation with a higher reconstruction error is a less informative representation. It is worth noting that the reconstruction of time series from the compression procedure proceeds by establishing a polygonal chain $\\hat{T}$ going through the chosen tuples $\\{(i_j,t_{i_j})\\}_{j=0}^N$ from the original time series T and $len_j = i_{j+1} \u2013 i_j$. As indicated in [13], a polygonal chain $\\tilde{T}$ stitching together $\\{(i_j, t_{i_j})\\}_{j=0}^N$ via a tuple sequence P is reconstructed by the inverse sym-bolization.\nTheorem III.1 ([13]). Let $ {(\\bar{len}_{i_l}, \\bar{inc}_{i_l} ) }_{l=1}^N =\\arg\\min_{S_i} \\sum_{(len, inc) \\in S_i} (len, inc)$, we denote the mean set for len and inc by $U_{len} = {(\\bar{len}_{i_l})_{l=1}^N}$ and $U_{inc} = {(\\bar{inc}_{i_l})_{l=1}^N}$, respectively. Since $i_0 = 0$, the reconstruction indices and size of time series values are given by\n$(i_j, t_{i_j}) = (\\sum_{l=1}^{j} \\bar{len}_l, t_0 + \\sum_{l=1}^{j} \\bar{inc}_l)$ for j = 0, ..., N,\nwhere $(\\bar{len}_{i_l}, \\bar{inc}_{i_l})$ are the computed cluster centers, i.e., $\\bar{len}_{i_l} \\in U_{len}$ and $\\bar{inc}_{i_l} \\in U_{inc}$.\nTheorem III.1 shows the accumulated deviations from the true lengths and increments are canceled out (as analyzed in [13]) at the right endpoint of the last piece $P_N$, thus $(i_N,t_{i_N}) = (i_N,t_{i_N}) = (n,t_n)$, which indicates the start and ending point between T, $\\hat{T}$ and $\\tilde{T}$ are identical. We thus have the following result.\nWe now denote the local deviation of the increment and length by\n$d^{len}_i := \\bar{len}_{i_l} - \\bar{len}_l, d^{inc}_i := \\bar{inc}_{i_l} - \\bar{inc}_l.$\nTheorem III.2 ([13]).\n$\\sum_{(len, inc) \\in S_i} \\sum_{i} (d^{len}_i, d^{inc}_i) =(0,0)$."}, {"title": "C. ABBA to LLM", "content": "In the following, we write a single time series containing n data points as T, and use $T = {T_i}_{i=1}^q$ to denote a set of time series of cardinality q, associated with its corresponding symbolic representation set $A = {A_i}_{i=1}^q$\n1) Fixed-point adaptive polygonal chain: In time series pre-diction settings, the value-based prediction is converted into a token-based prediction using STSA. However, it is very desir-able to mitigate the negative effect of the preceding mistakenly predicted symbol on the subsequent time series recovery since the recovery proceeds from front to back. However, APCA and the symbolic recovery often lead to a cumulative error for symbolic prediction, that is, an incorrect replacement of a previous symbol will influence the subsequent reconstruc-tion. A fixed-point polygonal chain trick is introduced to mitigate this issue. We still partition the time series into"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we study three time series tasks to validate the efficiency of ABBA in LLM. We also fine-tune three language models on the training data using QLoRA [17] with inhibition [30]. All experiments are simulated in PyTorch with a single NVIDIA A100 40GB GPU. The benefits of LLM-ABBA include (1) avoiding the need for LLMs to learn time series from scratch, and (2) only utilizing compression and decompression without the need for the training of extra embedding layers [6]. For a fair comparison, we evaluate our models on the same settings for each task. In the following, unless otherwise stated, we assume that the greedy aggregation is used for the ABBA digitization.\nA larger dataset needs more symbols or LLM tokens, as a larger time series dataset contains more information and symbolic semantics. ROBERTaLarge is based on BERT [31]"}, {"title": "F. QLoRA Fine-Tuning", "content": "Because the low rank of adapter fine-tuning will influence the efficiency of passing information [17], [30] from the previous layer, we use different low rank settings of QLoRA on the corresponding tasks during the fine-tuning progress. But for time series regression and prediction tasks, we select r\u2208 {16,46,256} for the corresponding data input. We find that there is no obvious over-fitting problem, and more tunable parameters are not able to improve the performance of LLM-ABBA.\nIn medical time series domains, ptb-db and MIT-BIH arrhyth-mia data sets are mostly used. EEG eye state data set has two categories, and because of its high complexity, the accuracy always stays at around 60%. EEG eye state data and MIT-BIH has more than one channel, which indicates that LLM-ABBA might have the ability to process complicate features across channels. Table VI presents the full medical time series classification results using LLM-ABBA.\nLLM-ABBA achieves comparable time series prediction re-sults to the SOTAs, and there is no over-fitting in these tasks when using different low rank r. Because ABBA tends to"}, {"title": "V. LIMITATIONS", "content": "ABBA is assessed carefully via performance profiles with respect to its reconstruction evaluated via 2-norm, DTW, and"}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose LLM-ABBA for time series classi-fication, regression, and forecasting tasks. We discuss how to seamlessly integrate time series symbolization with LLMs and enhance its performance. To mitigate the drift phenomenon of time series, we introduce the FAPCA method to improve ABBA symbolization. The empirical results demonstrate our method achieves a performance comparable to the SOTA on classification and regression tasks. We refer readers of interest to the Appendix for further discussion of the reconstruction error of ABBA symbolization, how it relates to the dominant parameters, and the congenital defect of LLM-ABBA. In terms of convenience and universality, LLM-ABBA improves the multi-modality of LLMs on time series analysis. We believe the potential of ABBA extends to other time series applications, which will be left as future work."}]}