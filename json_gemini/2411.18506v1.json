[{"title": "LLM-ABBA: Understand time series via symbolic approximation", "authors": ["Erin Carson", "Xinye Chen", "Cheng Kang"], "abstract": "The success of large language models (LLMs) for time series has been demonstrated in previous work. Utilizing a symbolic time series representation, one can efficiently bridge the gap between LLMs and time series. However, the remaining challenge is to exploit the semantic information hidden in time series by using symbols or existing tokens of LLMs, while aligning the embedding space of LLMs according to the hidden information of time series. The symbolic time series approximation (STSA) method called adaptive Brownian bridge-based symbolic aggregation (ABBA) shows outstanding efficacy in preserving salient time series features by modeling time series patterns in terms of amplitude and period while using existing tokens of LLMs.\nIn this paper, we introduce a method, called LLM-ABBA, that integrates ABBA into large language models for various downstream time series tasks. By symbolizing time series, LLM-ABBA compares favorably to the recent state-of-the-art (SOTA) in UCR and three medical time series classification tasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to avoid obvious drifting during prediction tasks by significantly mitigating the effects of cumulative error arising from misused symbols during the transition from symbols to numerical values. In time series regression tasks, LLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER) benchmarks. LLM-ABBA also shows competitive prediction capability compared to recent SOTA time series prediction results. We believe this framework can also seamlessly extend to other time series tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Time series are fundamental mathematical objects with applications across diverse disciplines such as classification [1], regression [2], and prediction [3]. Recently, the power of large language models (LLMs) in time series applications has been recognized. One recent review [4] concludes that there are three main LLM-based approaches to learn intricate semantic and knowledge representations from time series to perform various tasks. The first approach is to patch and tokenize numerical signals and related text data, followed by fine-tuning on time series tasks [5]\u2013[7]; the second one is preprocessing time series data to fit LLM input spaces by adding a customized tokenizer [8]; the last one is to build foundation models from scratch, and this approach aims to create large, scalable models, both generic and domain-specific [9], [10].\nThese three techniques each come with their own limitations. Patching and tokenizing time series segments can build the mapping between time series and the latent embedding of LLMs, instead of discrete language tokens. When outputting the numerical value, this method should generate the digit one by one, which eventually reduces the generation speed [6]. Furthermore, by adding a customized tokenizer, LLMs can handle positions of time series patterns and reproduce the internal logic of given time series signals [11]. Because LLM tokenizers, not designed for numerical values, separate continuous values and ignore the temporal relationship of time series, this method should convert tokens into flexible continuous values [12]. It inevitably requires token transitions from time series feature space to the latent embedding space of LLMs and cannot avoid the risk of semantic loss. Building foundational time series models from scratch can essentially solve these problems. But considering that one should balance the high development costs and their applicability, the challenge of expensive training persists and should be tackled [4].\nBy aligning time series and native language, large language models and specialized time series models constitute a new paradigm, where the LLMs are prompted with both time series and text-based instructions [4]. Following this paradigm, time series and textual information provide essential contexts, LLMs contribute to internal knowledge and reasoning ca-"}, {"title": "II. RELATED WORK", "content": "LLMs for time series methods have seen significant advances in recent years. The work [8] argues that this success stems from the ability of LLMs to naturally represent multimodal distributions of time series. By framing a time series forecasting task as a sentence-to-sentence task, AutoTimes [18] minimizes the tunable parameters needed to generate time series embeddings while freezing the parameters of the LLM, and FPT [19] fine-tunes LLM parameters to serve as a general representation extractor for various time series analysis tasks. These approaches maximize the use of inherent token transitions, leading to improved model efficiency. In terms of multivariate time series forecasting, UniTime [20] trains and fine-tunes a language model to provide a unified forecasting framework across multiple time series domains. Leveraging advanced prompting designs and techniques, PromptCast [21] transforms time series data into text pairs, and TEMPO [22] models specific time series patterns, such as trends and seasonality, by using weighted scatterplot smoothing [23].\nTuning-based predictors use accessible LLM parameters, typically involving pre-processing and tokenizing numerical signals and related prompt text, followed by fine-tuning on time series tasks [4]. In summary, there are four steps needed to adapt LLM to time series:\n(i) $T_{inp}$ = Pre-processing($T$): With a Patching operation [5], [18] or a weighted scatterplot smoothing processing [22], the time series set $T$ is pre-processed to specific knowledge-contained inputs $T_{inp}$;\n(ii) $M_{inp}$ = Tokenizer(Prompt, $T_{inp}$): An additional option is to perform a Tokenizer operation on time series $T_{inp}$ and related prompt text to form text sequence tokens $M_{inp}$;\n(iii) $M_{outp}$ = $f_{ALM}(M_{inp})$: With the instruction prompt Prompt, time series processed tokens and optional text tokens are fed into $f_{ALM}(\u00b7)$ with partial unfreezing or"}, {"title": "III. METHODOLOGIES", "content": "Our research is inspired by the observation that speech signals often contain a plethora of semantic information [24], which enables the language model to perform extremely well across a multitude of tasks; see [4] and references therein. However, directly applying language models to time series is not possible due to the fact that time series are made up of numerical values and lack useful embedding patterns; further, the high dimensionality of time series makes it difficult for the sequential and recurrent model to capture the dependencies of time series features. Thus learning an informative symbolic time series representation while having dimensionality reduced is a practical yet challenging problem. ABBA\u2014a symbolic approximation method is designed to address this as it compresses the time series to a symbolic presentation in terms of amplitude and period, and each symbol describes the oscillatory behavior of time series during a specific period.\nABBA utilizes adaptive polygonal chain approximation followed by mean-based clustering to achieve symbolization of time series. The reconstruction error of the representation can be modeled as a Brownian bridge with pinned start and end points. ABBA symbolization contains two dominant procedures, namely compression and digitization, to aggregate a time series $T = [t_1,t_2,...,t_n] \\in R^n$ into its symbolic representation $A = a_1a_2...a_N$ where $N < n$ and $a_i$ is an element in a specific letter set $L$, which is referred to as a dictionary in the ABBA procedure.\n1) Compression: The ABBA compression is performed to compute an adaptive piecewise linear continuous approximation (APCA) of $T$. The ABBA compression plays a critical role in dimensionality reduction in ABBA symbolic approximation\u2014a user-specific tolerance, denoted by $tol$, is given to determine the degree of the reduction. The ABBA compression proceeds by adaptively selecting $N + 1$ indices $i_0 = 0 < i_1 < ... < i_N = n$ given a tolerance $tol$ such that the time series $T$ is well approximated by a polygonal chain going through the points $(i_j,t_{i_j})$ for $j = 0,1,..., N$. This leads to a partition of $T$ into $N$ pieces $P_j = (len_j,inc_j)$ that represents cardinality and increment of $T_{[i_{j-1}:i_j]} = [t_{i_{j-1}},t_{i_{j-1}+1},...,t_{i_j}]$, which is calculated by $len_j \\in N := i_j \u2013 i_{j-1} \\ge 1$ and $inc_j \\in R := t_{i_j} - t_{i_{j-1}}$. As such, each piece $p_j$ is represented by a straight line connecting the endpoint values $t_{i_{j-1}}$ and $t_{i_j}$. Given an index $i_{j-1}$ and starting with $i_0 = 0$, the procedure seeks the largest possible $i_j$ such that $i_{j-1} < i_j < n$ and\n$\\sum_{i=i_{j-1}}^{i_j} (t_i - t_{i_{j-1}} - \\frac{i-i_{j-1}}{i_j - i_{j-1}}(t_{i_j} - t_{i_{j-1}}))^2 \\le (i_j - i_{j-1} - 1) \\cdot tol^2.$\nThis means that this partitioning criterion indicates that the squared Euclidean distance of the values in $p_j$ from the straight polygonal line is upper bounded by $(len_j \u2013 1)\\cdot tol^2$.\nFollowing the above, the whole polygonal chain can be recovered exactly from the first value $t_0$ and the tuple sequence $[P_1,P_2,...,P_N]$ in the sense that the reconstruction error of this representation is with pinned start and end points and can be naturally modeled as a Brownian bridge. In terms of (1), a lower $tol$ value is required to ensure an acceptable compression of time series with a great variety of features such as trends, seasonal and nonseasonal cycles, pulses and steps. As indicated in [13], the error bound between the reconstruction and original time series is upper bounded by $(n \u2212 N)\\cdot tol^2$.\n2) Digitization: The ABBA compression is followed by a reasonable digitization that leads to a symbolic representation. Prior to digitizing, the tuple lengths and increments are separately normalized by their standard deviations $\u03c3_{len}$ and $\u03c3_{inc}$, respectively. After that, further scaling is employed by using a parameter $scl$ to assign different weights to the length of each piece $p_i$, which denotes the importance assigned to its length value in relation to its increment value. Hence, the clustering is effectively performed on the scaled tuples $p_i^s = (scl\\cdot len_i, inci), i = 1,..., N$. In particular, if $scl = 0$, then clustering will be only performed on the increment values of $p_i^s$, while if $scl = 1$, the lengths and increments are treated with equal importance."}, {"title": "A. ABBA symbolic approximation", "content": "additional adapter layers. $M_{outp}$ can be either a fine-tuned result or a intermediate result;\n(iv) \u0176 = Task ($M_{outp}$): To generate or output required label \u0176, an extra task operation, denoted as Task(\u00b7), is finally introduced to perform different analysis tasks."}, {"title": "1) Compression:", "content": "Algorithm 1: Greedy sorting-based aggregation\n1) Scale and sort data points, and assume they are denoted $p_1^s,..., p_n^s$. Label all of them as \"unassigned\".\n2) For $i \\in \\{1, ..., n\\}$ let the first unassigned point $p_i^s$ as starting point and set $j := i$. If there are no unassigned points left, go to Step 6.\n3) Compute $d_{ij} := d(p_i^s,p_j^s)$\n4) If $d_{ij} \\le a$,\n\u2022 assign $p_j^s$ to the same group as $p_i^s$\n\u2022 increase $j:= j + 1$\n5) If $j > n$ or termination condition is satisfied, go to Step 2. Otherwise go to Step 3.\n6) For each computed group, compute the group center as the mean of all its points.\nThe step after normalization works with a mean-based clustering technique in Euclidean space. In the ABBA setting, letting the input of N vectors be $P_s = [p_1^s,\u2026\u2026,p_N^s] \\in R^{2\u00d7N}$, one seeks a codebook of k vectors, i.e., $C = [C_1, . . ., C_k] \\in R^{2\u00d7k}$ ($k 0$ indicates that the sample is from a larger dataset or represents time series characteristics. The default $scl$ is set to 3, which is used in other LLM tasks. $tol$ and $\u03b1$ are set to be the same."}, {"title": "B. Error analysis reconstruction", "content": "We are concerned with the reconstruction error of ABBA's symbolization since a symbolic representation with a higher reconstruction error is a less informative representation. It is worth noting that the reconstruction of time series from the compression procedure proceeds by establishing a polygonal chain $\\hat{T"}, "going through the chosen tuples $\\{(i_j,t_{i_j})\\}_{j=0}^{N}$ from the original time series $T$ and $len_j = i_{j+1} \u2013 i_j$. As indicated in [13], a polygonal chain $\\bar{T}$ stitching together $\\{(i_j, t_{i_j})\\}_{j=0}^{N}$ via a tuple sequence $P$ is reconstructed by the inverse symbolization.\nLet $(\\overline{len}^N_{j=1}, \\overline{inc}^N_{j=1}) \\overset{\\triangle}{=} \\frac{1}{N}(\\sum_{j=1}^{len_k}\\, inc_k,\\}_{\\forall\\ k \\in S_i}\\,\\}$, then we have $\\max_{len_j, inc_j) \\in S_i}\\,\\{ (d^{inc})^2 + (d^{len})^2 \\} \\le \\alpha^2$,\nand further\n$\\sigma = \\frac{1}{ |S_i|}\\sum_{(len,inc) \\in S_i}\\sqrt{ (|len - \\mu_{len}|^2 + |inc - \\mu_{inc}|^2)}$\n    },\n    {", "title\": \"C. ABBA to LLM", "content", "In the following, we write a single time series containing $n$ data points as $T$, and use $\\mathcal{T} = \\{T_i\\}_{i=1}^{q}$ to denote a set of time series of cardinality $q$, associated with its corresponding symbolic representation set $\\mathcal{A} = \\{A_i\\}_{i=1}^{q}$.\n1) Fixed-point adaptive polygonal chain: In time series prediction settings, the value-based prediction is converted into a token-based prediction using STSA. However, it is very desirable to mitigate the negative effect of the preceding mistakenly predicted symbol on the subsequent time series recovery since the recovery proceeds from front to back. However, APCA and the symbolic recovery often lead to a cumulative error for symbolic prediction, that is, an incorrect replacement of a previous symbol will influence the subsequent reconstruction. A fixed-point polygonal chain trick is introduced to mitigate this issue. We still partition the time series into pieces following (1) while $p_j = (len_j, inc_j)$ is replaced with $p_j = (len_j, t_{i_j})$ before normalization. We call the new approximation method fixed-point adaptive piecewise linear continuous approximation (FAPCA). The resulting tuples $p_i$ will be normalized and one can be recovered from the other since $inc_j = t_{i_j} - t_{i_{j-1}}$. Fig. 4 shows that FAPCA eliminates the cumulative errors arising from the preceding mistaken symbol and improves the recovery.\n2) Symbolizing multiple time series: Existing work on symbolic approximation focuses on converting a single time series; it can not convert another time series with consistent symbolic information (i.e., each symbol is associated with a unique symbolic center). To allow the manipulation of co-evolving time series or multiple time series, it is necessary to keep consistent symbolic information for multiple symbolic time series representations.\nWe illustrate a unified approach towards a consistent symbolic approximation for multiple time series.\n\u2022 Step 1: Use APCA or FAPCA to compress each time series $T_i$ into $P_i$ for $i = 1,... q$\n\u2022 Step 2: Compute normalized $P_i^s$ and concatenate $P_i^s$ to form $P_s := [P_i^s]_{i=1}^{q}$"]}, {"title": "D. Linguistics investigation: Zipf's law", "content": "In nearly all corpora, the most common word appears approximately twice as frequently as the next common word; this phenomenon is explained by Zipf's law [29]. Zipf's law asserts that the frequencies of certain events are inversely proportional to their rank, and further, the rank-frequency distribution is an inverse power law relation.\nIn Fig. 5, we can see unigrams generated by ABBA symbolization from 7 different time series datasets from the UCR Archive coarsely meet Zipf's law. This showcases an appealing alignment between ABBA symbols and the native language words."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we study three time series tasks to validate the efficiency of ABBA in LLM. We also fine-tune three language models on the training data using QLoRA [17] with inhibition [30]. All experiments are simulated in PyTorch with a single NVIDIA A100 40GB GPU. The benefits of LLM-ABBA include (1) avoiding the need for LLMs to learn time series from scratch, and (2) only utilizing compression and decompression without the need for the training of extra embedding layers [6]. For a fair comparison, we evaluate our models on the same settings for each task. In the following, unless otherwise stated, we assume that the greedy aggregation is used for the ABBA digitization.\nA larger dataset needs more symbols or LLM tokens, as a larger time series dataset contains more information and symbolic semantics. ROBERTaLarge is based on BERT [31] which considers two directions of the input language sen-"}, {"title": "A. Hyperparameters", "content": "1) Hyperparameters of ABBA: There are four interactive parameters that establish the transition of time series when integrating ABBA into LLMs. The tolerance $tol$ is chosen from $\\{1 \u00d710^{\u22122}, 1 \u00d7 10^{\u22124}, 1 \u00d7 10^{\u22126}\\}$ to control the degree of the compression and dimension reduction, and the digitization parameter \u03b1 is chosen from $\\{1 \u00d710^{\u22122},1 \u00d710^{\u22124},1\u00d710^{-6}\\}$ to determine the number of distinct symbols. L is a finite letter set that can be specified as the LLMs' tokens, and $scl \u2208 \\{1,2,3\\}$ is used as the normalized scaling for the length of each piece.\n2) Hyperparameters of LLMs: There are three time series analysis tasks: classification, regression, and prediction. We quantize LLMs by 4-bits using the bitsandbytes package2. In order to fine-tune LLMs, the shunting inhibition mechanism [30] is utilized during the QLoRA adapter fine-tuning progress. The modified embedding layer is also saved after fine-tuning on the corresponding task. For the classification task, the metric is accuracy rate (%). Root-mean-square-error is used as the metric for regression tasks. Mean-square-error and mean-absolute-error are used as the metrics for prediction tasks, and we also visualize the correlation coefficient of prediction tasks on ETTh1 data in terms of their seven features. We control the fine-tuning epoch and apply a small batch size on every task. The alpha of QLoRA is set to 16."}, {"title": "B. Compression and Recovery", "content": "To transform the numerical time series to symbolic time series, we use tokens of LLMs as the initial dictionary of ABBA for the symbolic representation, and there are no extra tokens that will be used to represent the numerical input. ABBA shows a strong symbolic transition on time series signals (See Figure 6 and Table IV).\nTo visualize the performance of ABBA on time series transition processes, we employ ETTh1 time series data to compute the correlation coefficient and reconstruction error of ABBA. This multivariate data has seven features, and in terms of these seven features, the average of mean-absolute-error (MSE), mean-square-error (MAE), and correlation coefficient"}, {"title": "C. Time Series Classification Tasks", "content": "For the classification task, we evaluate these three pretrained LLMs on UCR Time Series Archive datasets [36], EEG eye state [37], and MIT-BIH [38], [39] which have been ex-"}, {"title": "D. Time Series Regression Tasks", "content": "For the regression task, we evaluate these three pretrained LLMs on the Time Series Extrinsic Regression (TSER) benchmarking archive [2], which contains 19 time series datasets from 5 application domains, including Health Monitoring, Energy Monitoring, Environment Monitoring, Sentiment Analysis, and Forecasting4. To use as few symbols as possible, we initialize the setting of $tol = 0.01$ and $\u03b1 = 0.01$. We also utilize the L2 loss for the regression training. Details of the implementation and datasets can be found in Table II. The evaluation metric is root-mean-square-error (RMSE).\nExperimenting on the TSER benchmark archive [2], the empirical results are shown in Table VII, in which for 15 out of 19 use-cases, LLM-ABBA outperforms the machine learning SOTA results. We believe that LLM-ABBA can exploit the semantic information hiding beneath the time series in the task of time series regression. ABBA is able to provide COPs to LLMs by compressing and digitizing time series to symbols, which finally results in the change of embedding space by using adaption fine-tuning methods."}, {"title": "E. Time Series Forecasting Tasks", "content": "For time series forecasting, we experimented on 4 well-established benchmarks: ETT datasets (including 4 subsets: ETTh1, ETTh2, ETTm1, ETTm2) [45], [46]. Details of the implementation and datasets can be found in Table III. The input length of the time series is 168, and we use three different prediction horizons H \u2208 {24, 96, 168}. The evaluation metrics include MSE and MAE.\nAlthough LLM-ABBA cannot obtain a new SOTA on time series forecasting tasks, it compares favorably to the Informer architecture which is trained from scratch. The congenital defect of ABBA is that the symbolization tends to be affected by the fluctuation and oscillation of time series signals, which eventually leads to higher MSE and MAE scores. Because LLM-ABBA utilizes a totally different technical roadmap to existing methods, it only remolds the construction of the LLM's tokens. However, remodeling pretrained tokens inevitably brings the previous pretrained semantics to the LLM-ABBA design. Thus, we discussed the semantic consistency of LLM-ABBA using extra symbols or tokens to overcome this problem."}, {"title": "F. QLoRA Fine-Tuning", "content": "Because the low rank of adapter fine-tuning will influence the efficiency of passing information [17], [30] from the previous layer, we use different low rank settings of QLoRA on the corresponding tasks during the fine-tuning progress. But for time series regression and prediction tasks, we select r\u2208 {16,46,256} for the corresponding data input. We find that there is no obvious over-fitting problem, and more tunable parameters are not able to improve the performance of LLM-ABBA.\nIn medical time series domains, ptb-db and MIT-BIH arrhythmia data sets are mostly used. EEG eye state data set has two categories, and because of its high complexity, the accuracy always stays at around 60%. EEG eye state data and MIT-BIH has more than one channel, which indicates that LLM-ABBA might have the ability to process complicate features across channels. Table VI presents the full medical time series classification results using LLM-ABBA.\nLLM-ABBA achieves comparable time series prediction results to the SOTAs, and there is no over-fitting in these tasks when using different low rank r. Because ABBA tends to"}, {"title": "G. Semantic consistency", "content": "When using pretrained tokens as the input symbols, fine-tuning on no language content (such as time series signals) will generally bring semantic loss to LLMs. Therefore, we use ASCII codes to generate new symbols by adding more digits and expanding the used alphabet table. Following the same fine-tuning process to the above experiment settings, we compute the forecasting performance by fine-tuning on Mistral-7B. Compared to Table VIII, Table IX shows that the difference is not noticeable."}, {"title": "V. LIMITATIONS", "content": "ABBA is assessed carefully via performance profiles with respect to its reconstruction evaluated via 2-norm, DTW, and"}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose LLM-ABBA for time series classification, regression, and forecasting tasks. We discuss how to seamlessly integrate time series symbolization with LLMs and enhance its performance. To mitigate the drift phenomenon of time series, we introduce the FAPCA method to improve ABBA symbolization. The empirical results demonstrate our method achieves a performance comparable to the SOTA on classification and regression tasks. We refer readers of interest to the Appendix for further discussion of the reconstruction error of ABBA symbolization, how it relates to the dominant parameters, and the congenital defect of LLM-ABBA. In terms of convenience and universality, LLM-ABBA improves the multi-modality of LLMs on time series analysis. We believe the potential of ABBA extends to other time series applications, which will be left as future work."}]