{"title": "ASVspoof 5: Crowdsourced Speech Data, Deepfakes, and Adversarial Attacks at Scale", "authors": ["Xin Wang", "H\u00e9ctor Delgado", "Hemlata Tak", "Jee-weon Jung", "Hye-jin Shim", "Massimiliano Todisco", "Ivan Kukanov", "Xuechen Liu", "Md Sahidullah", "Tomi Kinnunen", "Nicholas Evans", "Kong Aik Lee", "Junichi Yamagishi"], "abstract": "ASVspoof 5 is the fifth edition in a series of challenges that promote the study of speech spoofing and deepfake attacks, and the design of detection solutions. Compared to previous challenges, the ASVspoof 5 database is built from crowdsourced data collected from a vastly greater number of speakers in diverse acoustic conditions. Attacks, also crowdsourced, are generated and tested using surrogate detection models, while adversarial attacks are incorporated for the first time. New metrics support the evaluation of spoofing-robust automatic speaker verification (SASV) as well as stand-alone detection solutions, i.e., countermeasures without ASV. We describe the two challenge tracks, the new database, the evaluation metrics, baselines, and the evaluation platform, and present a summary of the results. Attacks significantly compromise the baseline systems, while submissions bring substantial improvements.", "sections": [{"title": "1. Introduction", "content": "The ASVspoof initiative was conceived to foster progress in the development of detection solutions, also referred to as countermeasures (CMs) and presentation attack detection (PAD) solutions, to discriminate between bona fide and spoofed or deepfake speech utterances. ASVspoof 5 is the fifth edition in a series of previously-biennial challenges [1-4] and has evolved in the track definition, the database and spoofing attacks, and the evaluation metrics.\nWhile the 2021 challenge edition involved distinct logical access (LA), physical access (PA), and speech deepfake (DF) sub-tasks [5], ASVspoof 5 takes the form of a single, com- bined LA and DF task, but encompasses two tracks: (i) stand- alone spoofing and speech deepfake detection (CM, no ASV) and (ii) spoofing-robust automatic speaker verification (SASV).\nTrack 1 is similar to the DF track of the previous 2021 chal- lenge. It reflects a scenario in which an attacker has access to the voice data of a targeted victim, e.g. data posted to social media. The attacker is assumed to use public data and speech deepfake technology to generate spoofed speech resembling the voice of the victim, and then to post the generated recordings to social media, e.g. to defame the victim. Speech data, both bona fide and spoofed, may be compressed using conventional codecs (e.g., mp3) or contemporary neural codecs.\nTrack 2 shares the same goal as the LA sub-task of previous ASVspoof editions and the SASV2022 Challenge [6]. Track 2 assumes a telephony scenario where synthetic and converted speech are injected into a communication system (e.g. a telephone line) without any acoustic propagation. Participants can elect to develop single classifiers or separate, fused ASV and CM sub-systems. They can use either a pre-trained ASV sub- system provided by the organisers or can optimize their own bespoke system.\nParticipants are furthermore provided with an entirely new ASVspoof 5 database. Source data and attacks, both crowdsourced, encompass greater acoustic variation than ear- lier ASVspoof databases. The objectives are to evaluate the threat of spoofing and deepfake attacks forged using non-studio- quality data and optimised to compromise not just ASV sub- systems but also CM sub-systems. Source data, collected from a vastly greater number of speakers than for earlier ASVspoof databases, is extracted from the Multilingual Lib- rispeech (MLS) English partition [7]. In addition to the use of new spoofing attacks implemented using the latest text-to- speech (TTS) synthesis and voice conversion (VC) algorithms, adversarial attacks are introduced for the first time and com- bined with spoofing attacks.\nAlso new is an open condition for both tracks 1 and 2. In contrast to the traditional closed condition, for which partici- pants are restricted to use the specified data protocol, for the open condition participants have the opportunity to use exter- nal data and pre-trained speech foundation models, subject to there being no overlap between training data (i.e. that used for training foundation models) and challenge evaluation data.\nA new suite of evaluation metrics is also introduced. In- spired by the NIST SREs [8], we adopt the minimum detec- tion cost function (minDCF) as the primary metric for Track 1. The log-likelihood-ratio cost function (Cllr) and actual DCF are also used to gauge not only discrimination but also calibration performance. The recently proposed architecture-agnostic DCF (a-DCF) [9] is used as the primary metric for Track 2, with the tandem detection cost function (t-DCF) [10] and tandem equal error rate (t-EER) [11] being complementary.\nWe present an overview of the two challenge tracks, the database, and the adopted metrics. Spoofing and deepfake at- tacks built by database contributors and their performance in fooling an ASV system are also described. Finally, we report a summary of system performance for the baselines and those submitted by 54 challenge participants."}, {"title": "2. Database", "content": "The new ASVspoof 5 database has evolved in two aspects: source data and attack algorithms. In terms of the source data, it is built upon the MLS English dataset [7] to evaluate the per- formance of CM and SASV systems on detecting spoofing at- tacks forged using non-studio-quality data. The MLS English dataset incorporates data from more than 4k speakers, recorded\ntrained systems, namely YourTTS [18] and XTTS [19], are used to clone target speakers' voices in a zero-shot manner.\nTo evaluate the CM and SASV systems' performance when both bona fide and spoofed data are (lossy) encoded or com- pressed, the evaluation sets contain data treated with codecs listed in Table 3. C1-C7 operates with a 16kHz sampling rate, while C8-C11 operates in an 8kHz narrow band setting. To cre- ate the narrow band data, bona fide and spoofed data are down- sampled to 8 kHz, processed with the codec, and up-sampled to 16 kHz. Condition CO replicates the scenario without en- coding or compression. Bona fide and spoofed utterances are treated with one of the codec conditions. All the data are saved in a FLAC format with a sampling rate of 16 kHz. The lead- ing and trailing non-speech segments in the evaluation set utter- ances have been removed.\nParticipants of the closed condition of both Track 1 and 2 are required to use the same training and development sets to build their systems. For both tracks, participants in the open conditions can use external training data given the condition that it doesoverlapthe challenge database. They can use pre- trained speech foundation models built on some publicly avail- able databases [33, \u00a74.2]. The evaluation set for the two tracks covers the same set of utterances, except that Track 2 ignores a few codec conditions listed in Table 3."}, {"title": "3. Performance measures", "content": "This section provides a brief summary of the performance mea- sures used in the two challenge tracks.\n3.1. Track 1: from EER to DCF\nTrack 1 submissions assign a real-valued bona fide-spoof de- tection score to each utterance. Different from past ASVspoof challenge editions for which EER was used as the primary met- ric for the comparison of spoofing CMs, Track 1 builds upon a normalized detection cost function (DCF) [8]. While further details are available in [33, Appendix], the DCF has a simple form:\n$DCF(T_{cm}) = \\beta \\cdot P_{miss} (T_{cm}) + P_{fa} (T_{cm}),$\nwhere $P_{miss} (T_{cm})$ is the miss rate (false rejection rate of bona fide utterances) and $P_{fa} (T_{cm})$ is the false alarm rate (false acceptance rate of spoofed utterances). Both are functions of a detection threshold, $T_{cm}$, and the constant $\u03b2$ in (1) is defined as\n$\\beta = \\frac{C_{miss}}{C_{fa}} \\cdot \\frac{\\pi_{miss}}{ \\pi_{spf}},$\nwhere $C_{miss}$ and $C_{fa}$ are, respectively, the costs of miss and false alarm, and where $\u03c0_{spf}$ is asserted prior probability of spoofing attack. The scenario envisioned in Track 1 lays on the assump- tion that, compared to spoofed utterances, bona fide speech ut- terances are, in general, far more likely in practice (low spf). But, when encountered but not detected, the relative cost is high. We set $C_{miss} = 1$, $C_{fa} = 10$, $\u03c0_{spf} = 0.05$, which gives $\u03b2 \u2248 1.90$.\nThe normalized DCF in (1) is used to compute both the minimum and actual DCFs. The former is the primary met- ric of Track 1, defined as minDCF = min$T_{cm}$ DCF($T_{cm}$). The latter, actDCF = DCF($T_{Bayes}$), is the DCF evaluated at a fixed threshold $T_{Bayes} = -log(\u03b2)$ under the assumption that the detection scores can be interpreted as log-likelihood ratios (LLRs). Whereas minDCF measures performance using an 'or- acle' threshold (set based on ground-truth), actDCF measures the realised cost obtained by setting the threshold to $T_{Bayes}$ [8]. Note that this is meaningful only when the scores can be in- terpreted as calibrated LLRs [34, 35]. Similar to the past chal- lenge editions, ASVspoof 5 did not require participants to sub- mit LLR scores rather, it was encouraged for the first time. Another complementary metric, cost of log-likelihood ra- tios (Cllr) [34], was used to assess the quality of detection scores when interpreted as LLRs:\n$C_{llr} = \\frac{1}{2 log2} [ \\frac{1}{\\mid B \\mid} \\sum_{s_i \\in B} log (1 + e^{-s_i}) + \\frac{1}{\\mid S \\mid} \\sum_{s_j \\in S} log (1 + e^{s_j}) ] ,$\nwhere $B = {s_i}$ and $S = {s_j}$ denote, respectively, the sets of bona fide and spoofed trial scores. The lower the $C_{llr}$, the better calibrated (and more discriminative) the scores are. In addition to minDCF, actDCF, and $C_{llr}$, EER is also reported.\n3.2. Track 2: from SASV-EER to a-DCF\nFor Track 2, participants were allowed to submit either single real-valued SASV scores or a triplet of scores which, in addition\nto SASV scores, contains two additional sets of spoofing (CM sub-system) and speaker (ASV sub-system) detection scores. While the former applies to any model architecture which out- puts a single detection score, the latter assumes specific tandem (cascade) architecture [10] consisting of two clearly-identified sub-systems intended to detect spoofing attacks and to verify the speaker, respectively. In the latter case, the final SASV score is formed by combining the outputs of the two sub-systems (e.g., embeddings or scores) using an arbitrary combination strategy designed by the participants.\nFor both types of submission, the SASV scores are used to compute the primary challenge metric. Track 2 takes a step for-ward from EER-based metrics used in the first SASV challenge [6] to DCF-based metrics. Extending upon the two-class DCF (1), [9] recently proposed normalized architecture-agnostic de- tection cost function (a-DCF) [9], defined as\n$a-DCF(T_{sasv}) = \\alpha \\cdot P_{miss_{tar}}(T_{sasv}) + (1-\\alpha) \\cdot P_{fa,non_{non}} (T_{sasv}) + \\gamma \\cdot P_{fa,spf} (T_{sasv}),$\nwhere $P_{miss_{tar}}(T_{sasv})$ is the ASV miss (target speaker false rejec- tion) rate and where $P_{fa,non_{non}} (T_{sasv})$ and $P_{fa,spf} (T_{sasv})$ are the false alarm (false acceptance) rates for non-targets and spoofing at- tacks, respectively. All three error rates are functions of an SASV threshold $T_{sasv}$. The constants \u03b1 and \u03b3 are given by\n$\\alpha = \\frac{C_{miss_{tar}} \\pi_{tar}}{C_{fa,non} \\pi_{non} + C_{fa,spf} \\pi_{spf}},$\n$\\gamma = \\frac{C_{fa,spf} \\pi_{spf}}{C_{fa,non} \\pi_{non} + C_{fa,spf} \\pi_{spf}},$\nwhere $C_{miss}, C_{fa,non}$, and $C_{fa, spoof}$ are the costs of miss, falsely acceptance of non-target speaker, and false acceptance of spoofing attack. Moreover, $\u03c0_{tar}, \u03c0_{non}$, and $\u03c0_{spoof}$ are the asserted priors of targets, non-targets (zero-effort impostors), and spoofing at- tack. The assumptions are similar to those in Track 1. We set $\u03c0_{tar} = 0.9405$, $\u03c0_{non} = 0.0095$, $\u03c0_{spf} = 0.05$, $C_{miss} = 1$ and $C_{fa,non} = C_{fa,spf} = 10$. This gives $\u03b1 \u2248 1.58$ and $\u03b3 \u2248 0.84$.\nThe primary metric of Track 2 is the minimum of the a-DCF, min a-DCF = min$T_{sasv}$ a-DCF($T_{sasv}$).\nFor the submissions that contain clearly-identified ASV and CM sub-systems, ASV-constrained minimum tandem detection cost function (t-DCF) [10] and tandem equal error rate (t-EER) [11] metrics are additionally reported. Whereas the former has served as the primary metric since ASVspoof 2019, the latter provides a complementary parameter-free measure of class dis- crimination. To compute the t-DCF metric, we adopt the same costs and priors as above and use ASV scores produced by a common ASV system of the organiser in place of scores pro- vided by participants. This allows computation of the minimum 'ASV-constrained' t-DCF in the same way as for the previous ASVspoof challenges and enables the comparison of different CM sub-systems when they are combined with a common ASV sub-system.\nFor computation of the t-EER metric, both the CM and ASV sub-system scores are used to obtain a single concurrent t- EER value, denoted by t-EERx. It has a simple interpretation as the error rate at a unique pair of ASV and CM thresholds, $T_x := (T_{asv}, T_{cm})$, at which the miss rate and the two types of false alarm rates (one for spoofing attacks, the other for non-targets) are equal: t-EERx = $P_{miss_{tar}}(T_x) = P_{fa,non} (T_x) = P_{fa,spoof} (T_x)$. The superscript 'tdm' is used to emphasize the assumed tandem architecture. The t-EER can be seen as a generalisation of the conventional two-class, single system EER which provides an application-agnostic discrimination measure."}, {"title": "4. Common ASV, surrogate systems, and challenge baselines", "content": "4.1. Common ASV system by organisers\nThe common ASV system uses an ECAPA-TDNN speaker en- coder [36] and cosine similarity scoring. The ECAPA-TDNN model is trained using the training partitions of VoxCeleb 1 and 2 [37]. The computed ASV cosine scores are subsequently nor- malised using an s-norm. Figure 1 illustrates the ASV EER values on the evaluation data. The EER (5%) is low when dis- criminating between bona fide target and non-target speakers' data (the leftmost bar). However, the EERs are much higher when spoofing attacks are mounted. Note that although A25 is the least effective attack, it is more threatening when enhanced as an adversarial attack A32.\n4.2. Baseline systems\nTrack 1 adopts two CM baseline systems: RawNet2 [38] (B01) and AASIST [39] (B02). Both systems are end-to-end systems operating directly on raw waveforms. Inputs to these baseline systems are raw waveforms of 4 seconds duration (64,000 sam- ples). RawNet2 is composed of a fixed bank of 20 sinc filters, six residual blocks followed by gated recurrent units, which convert the frame-level representation to utterance-level rep- resentation. The CM output scores are generated using fully- connected layers.\nAASIST uses a RawNet2-based encoder [38] to extract spectro-temporal features from the input waveform. A spectro-temporal heterogeneous graph attention layers and max graph operations are then used to integrate temporal and spectral rep- resentations. CM output scores are generated using a readout operation and a linear output layer. Both baselines were trained with a weighted cross-entropy loss for binary classification.\nIn Track 2, a fusion-based [6] (B03) and a single inte- grated [40] (B04) systems are adopted. B03 is adopted from the SASV 2022 challenge baseline but fuses the common ASV and the baseline AASIST of Track 1 using an LLR-based fu- sion tool [41]. B04, which is based on MFA-Conformer [42], extracts a single embedding from the input waveform and pro- duces a single SASV score. It is trained in three stages: speaker classification-based pre-training, copy synthesis [43] training with adapted SASV loss functions, and in-domain fine-tuning. VoxCeleb and copy synthesis data are used in the first and sec- ond stages, respectively. The in-domain fine-tuning is con- ducted using ASV spoof 5 training data. The source codes for all baselines are accessible from the ASVspoof 5 git repository."}, {"title": "4.3. Surrogate systems", "content": "The surrogate ASV system is based on ECAPA-TDNN and a probabilistic linear discriminant analysis scoring backend [44]. The surrogate CM systems include AASIST, RawNet2, LCNNS with LFCC, all of which are trained on bona fide data from the MLS partition A and spoofed attacks created by the first group of data contributors (see \u00a7 2). Note that the surrogate CMs do not see attacks in the development and evaluation sets."}, {"title": "5. Evaluation platform", "content": "ASVspoof 5 used the CodaLab website through which partic- ipants could submit detection scores and receive results. The challenge was run in two phases, with an additional post- evaluation phase (not addressed in this paper). During the first progress phase, participants could submit up to four submis- sions per day. Results determined from a subset of the evalua- tion set (i.e., the progress subset) were made available to partic- ipants who could opt to submit their results to an anonymised leaderboard. The evaluation phase ran for only a few days, dur- ing which participants could make only a single submission. This submission was evaluated using the whole evaluation set. Figure 2 illustrates the number of submissions during the progress and evaluation phases. In Track 1, closed and open conditions received comparable amount of submissions. In con- trast, in case of Track 2, open condition received considerably higher number of submissions, demonstrating the need for ad- ditional data for training SASV systems."}, {"title": "6. Challenge results", "content": "6.1. Track 1\nResults on Track 1 are listed in Table 4. The baseline sys- tems achieved minDCF higher than 0.7 and EERs higher than 29%. Although they use the RawNet2 and AASIST architec- tures, which have been demonstrated to be effective on the previous ASVspoof challenge databases, the non-studio-quality data sourced from MLS and the more advanced spoofing attacks may have led to their unsatisfactory performance.\nIt is encouraging that most of the submissions in the closed"}, {"title": "6.2. Track 2", "content": "Results on Track 2 are listed in Table 5. Spoofing-robust ASV is technically more demanding than a stand-alone CM, which may be the reason for lower numbers of submissions to Track 2. B03 performed similarly to a reference system (REF) that is the same as B03 except using a random guessing CM sub-system. This indicates that the CM sub-system in B03 does not provide useful information for detection spoofing attacks. In contrast, the single integrated B04 performed better. However, note that the results on the baselines do not support the claim that fusion- based approach is inferior. In fact, all the top submissions are fusing ASV and CM sub-systems, including T45, which opted not to submit their ASV and CM scores.\nMost of the submitted systems outperformed the baselines. Compared with the baseline, the top-3 submissions in the closed have 50% relative improvement on the min a-DCF values. Similar to the findings in Track 1, submissions in the open condi- tion reached lower metrics. The usage of SSL-based features is common among the top submissions."}, {"title": "7. Conclusions", "content": "This paper outlines the ASVspoof 5 challenge, which is de- signed to support the evaluation of both stand-alone speech spoofing and deepfake detection and SASV solutions. The fifth edition was considerably more complex than its predecessors, including not only a new task, but also more challenging crowd- sourced data collected under variable conditions, spoofing at- tacks generated with a variety of contemporary algorithms opti- mised to fool surrogate ASV and CM sub-systems, and new ad- versarial attacks. Despite the use of lower-quality data to create spoofs and deepfakes, detection performance for the baseline systems, all top-performing systems reported in recent years, is relatively poor. Encouragingly, results for most challenge sub- missions outperform the challenge baselines, sometimes by a substantial margin. We look forward to learning about the tech- nical details from the challenge participants in their forthcom- ing research articles. Results also reveal the hitherto ignored issue of score calibration, an essential consideration if detection solutions are deployed in real, practical scenarios.\nWith a particularly tight schedule for ASVspoof 5, more detailed analyses will be presented at the ASV spoof 5 workshop and reported in future work."}]}