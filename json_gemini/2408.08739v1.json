{"title": "ASVspoof 5: Crowdsourced Speech Data, Deepfakes, and Adversarial Attacks at Scale", "authors": ["Xin Wang", "H\u00e9ctor Delgado", "Hemlata Tak", "Jee-weon Jung", "Hye-jin Shim", "Massimiliano Todisco", "Ivan Kukanov", "Xuechen Liu", "Md Sahidullah", "Tomi Kinnunen", "Nicholas Evans", "Kong Aik Lee", "Junichi Yamagishi"], "abstract": "ASVspoof 5 is the fifth edition in a series of challenges that promote the study of speech spoofing and deepfake attacks, and the design of detection solutions. Compared to previous challenges, the ASVspoof 5 database is built from crowdsourced data collected from a vastly greater number of speakers in diverse acoustic conditions. Attacks, also crowdsourced, are generated and tested using surrogate detection models, while adversarial attacks are incorporated for the first time. New metrics support the evaluation of spoofing-robust automatic speaker verification (SASV) as well as stand-alone detection solutions, i.e., countermeasures without ASV. We describe the two challenge tracks, the new database, the evaluation metrics, baselines, and the evaluation platform, and present a summary of the results. Attacks significantly compromise the baseline systems, while submissions bring substantial improvements.", "sections": [{"title": "1. Introduction", "content": "The ASVspoof initiative was conceived to foster progress in the development of detection solutions, also referred to as countermeasures (CMs) and presentation attack detection (PAD) solutions, to discriminate between bona fide and spoofed or deepfake speech utterances. ASVspoof 5 is the fifth edition in a series of previously-biennial challenges [1-4] and has evolved in the track definition, the database and spoofing attacks, and the evaluation metrics.\nWhile the 2021 challenge edition involved distinct logical access (LA), physical access (PA), and speech deepfake (DF) sub-tasks [5], ASVspoof 5 takes the form of a single, combined LA and DF task, but encompasses two tracks: (i) stand-alone spoofing and speech deepfake detection (CM, no ASV) and (ii) spoofing-robust automatic speaker verification (SASV).\nTrack 1 is similar to the DF track of the previous 2021 challenge. It reflects a scenario in which an attacker has access to the voice data of a targeted victim, e.g. data posted to social media. The attacker is assumed to use public data and speech deepfake technology to generate spoofed speech resembling the voice of the victim, and then to post the generated recordings to social media, e.g. to defame the victim. Speech data, both bona fide and spoofed, may be compressed using conventional codecs (e.g., mp3) or contemporary neural codecs.\nTrack 2 shares the same goal as the LA sub-task of previous ASVspoof editions and the SASV2022 Challenge [6]. Track 2 assumes a telephony scenario where synthetic and converted speech are injected into a communication system (e.g. a telephone line) without any acoustic propagation. Participants can elect to develop single classifiers or separate, fused ASV and CM sub-systems. They can use either a pre-trained ASV sub-system provided by the organisers or can optimize their own bespoke system.\nParticipants are furthermore provided with an entirely new ASVspoof 5 database. Source data and attacks, both crowdsourced, encompass greater acoustic variation than earlier ASVspoof databases. The objectives are to evaluate the threat of spoofing and deepfake attacks forged using non-studio-quality data and optimised to compromise not just ASV sub-systems but also CM sub-systems. Source data, collected from a vastly greater number of speakers than for earlier ASVspoof databases, is extracted from the Multilingual Librispeech (MLS) English partition [7]. In addition to the use of new spoofing attacks implemented using the latest text-to-speech (TTS) synthesis and voice conversion (VC) algorithms, adversarial attacks are introduced for the first time and combined with spoofing attacks.\nAlso new is an open condition for both tracks 1 and 2. In contrast to the traditional closed condition, for which participants are restricted to use the specified data protocol, for the open condition participants have the opportunity to use external data and pre-trained speech foundation models, subject to there being no overlap between training data (i.e. that used for training foundation models) and challenge evaluation data.\nA new suite of evaluation metrics is also introduced. Inspired by the NIST SREs [8], we adopt the minimum detection cost function (minDCF) as the primary metric for Track 1. The log-likelihood-ratio cost function (Cllr) and actual DCF are also used to gauge not only discrimination but also calibration performance. The recently proposed architecture-agnostic DCF (a-DCF) [9] is used as the primary metric for Track 2, with the tandem detection cost function (t-DCF) [10] and tandem equal error rate (t-EER) [11] being complementary.\nWe present an overview of the two challenge tracks, the database, and the adopted metrics. Spoofing and deepfake attacks built by database contributors and their performance in fooling an ASV system are also described. Finally, we report a summary of system performance for the baselines and those submitted by 54 challenge participants."}, {"title": "2. Database", "content": "The new ASVspoof 5 database has evolved in two aspects: source data and attack algorithms. In terms of the source data, it is built upon the MLS English dataset [7] to evaluate the performance of CM and SASV systems on detecting spoofing attacks forged using non-studio-quality data. The MLS English dataset incorporates data from more than 4k speakers, recorded\nEnglish dataset is scraped from the same source as Librispeech [15]. Because challenge participants in the open condition may use models pre-trained on Librispeech, we remove speakers in the evaluation set who also appear in Librispeech."}, {"title": "3. Performance measures", "content": "This section provides a brief summary of the performance measures used in the two challenge tracks.\n3.1. Track 1: from EER to DCF\nTrack 1 submissions assign a real-valued bona fide-spoof detection score to each utterance. Different from past ASVspoof challenge editions for which EER was used as the primary metric for the comparison of spoofing CMs, Track 1 builds upon a normalized detection cost function (DCF) [8]. While further details are available in [33, Appendix], the DCF has a simple form:\n$\\DCF(cm) = \\beta \\cdot P_{miss} (T_{cm}) + P_{fa} (T_{cm}),$\nwhere $P_{miss} (T_{cm})$ is the miss rate (false rejection rate of bona fide utterances) and $P_{fa} (T_{cm})$ is the false alarm rate (false acceptance rate of spoofed utterances). Both are functions of a detection threshold, $T_{cm}$, and the constant $\\beta$ in (1) is defined as\n$\\beta = \\frac{C_{miss}}{C_{fa}} \\cdot \\frac{\\pi_{non}}{spf},$\nwhere $C_{miss}$ and $C_{fa}$ are, respectively, the costs of miss and false alarm, and where $\\pi_{spf}$ is asserted prior probability of spoofing attack.2 The scenario envisioned in Track 1 lays on the assumption that, compared to spoofed utterances, bona fide speech utterances are, in general, far more likely in practice (low spf). But, when encountered but not detected, the relative cost is high. We set $C_{miss} = 1$, $C_{fa} = 10$, $\\pi_{spf} = 0.05$, which gives $\\beta \\approx 1.90$.\nThe normalized DCF in (1) is used to compute both the minimum and actual DCFs. The former is the primary metric of Track 1, defined as $\\minDCF = \\min_{T_{cm}} DCF(T_{cm})$. The latter, $\\actDCF = DCF(\\T_{Bayes})$, is the DCF evaluated at a fixed threshold $T_{Bayes} = -\\log(\\beta)$ under the assumption that the detection scores can be interpreted as log-likelihood ratios (LLRs). Whereas minDCF measures performance using an 'oracle' threshold (set based on ground-truth), actDCF measures the realised cost obtained by setting the threshold to $T_{Bayes}$ [8]. Note that this is meaningful only when the scores can be interpreted as calibrated LLRs [34, 35]. Similar to the past challenge editions, ASVspoof 5 did not require participants to submit LLR scores rather, it was encouraged for the first time.3\nAnother complementary metric, cost of log-likelihood ratios (Cllr) [34], was used to assess the quality of detection scores when interpreted as LLRs:\n$C_{llr} = \\frac{1}{2\\log 2} \\bigg[\\frac{1}{|B|}\\sum_{s_i \\in B} \\log(1+e^{-s_i}) + \\frac{1}{|S|}\\sum_{s_j \\in S} \\log(1+e^{s_j})\\bigg]$\nwhere $B = \\{s_i\\}$ and $S = \\{s_j\\}$ denote, respectively, the sets of bona fide and spoofed trial scores. The lower the $C_{llr}$, the better calibrated (and more discriminative) the scores are. In addition to minDCF, actDCF, and Cllr, EER is also reported.\n3.2. Track 2: from SASV-EER to a-DCF\nFor Track 2, participants were allowed to submit either single real-valued SASV scores or a triplet of scores which, in addition\nsspfis the asserted prior of the bona fide class.\nParticipantscould post-process their raw detection scores intoLLRs using implementations such as [35] in order to reduce actDCF.Note, however, that any order-preserving score calibration does not af-fect the primary minDCF metric.to SASV scores, contains two additional sets of spoofing (CM sub-system) and speaker (ASV sub-system) detection scores. While the former applies to any model architecture which outputs a single detection score, the latter assumes specific tandem (cascade) architecture [10] consisting of two clearly-identified sub-systems intended to detect spoofing attacks and to verify the speaker, respectively. In the latter case, the final SASV score is formed by combining the outputs of the two sub-systems (e.g., embeddings or scores) using an arbitrary combination strategy designed by the participants.\nFor both types of submission, the SASV scores are used to compute the primary challenge metric. Track 2 takes a step forward from EER-based metrics used in the first SASV challenge [6] to DCF-based metrics. Extending upon the two-class DCF (1), [9] recently proposed normalized architecture-agnostic detection cost function (a-DCF) [9], defined as\n$\\alpha\\u001dCF(T_{sasv}) = \\alpha \\cdot P_{miss}^{sasv}(T_{sasv}) + (1-\\alpha)( \\gamma \\cdot P_{fa,non}^{sasv}(T_{sasv})+ P_{fa,spf}(T_{sasv})$\nwhere $P_{miss}^{sasv}(T_{sasv})$ is the ASV miss (target speaker false rejection) rate and where $P_{fa,non}^{sasv}(T_{sasv})$ and $P_{fa,spf}(T_{sasv})$ are the false alarm (false acceptance) rates for non-targets and spoofing attacks, respectively. All three error rates are functions of an SASV threshold $T_{sasv}$. The constants $\\alpha$ and $\\gamma$ are given by\n$\\alpha = \\frac{C_{miss}}{C_{fa,non}\\pi_{non} + C_{fa,spf}\\pi_{spf}}$\n$\\gamma = \\frac{C_{fa,spf}\\pi_{spf}}{C_{fa,non}\\pi_{non} + C_{fa,spf}\\pi_{spf}}$\nwhere $C_{miss}, C_{fa,non}$, and $C_{fa,spf}$ are the costs of miss, falsely acceptance of non-target speaker, and false acceptance of spoofing attack. Moreover, $\\pi_{tar}, \\pi_{non}$, and $\\pi_{spoof}$ are the asserted priors of targets, non-targets (zero-effort impostors), and spoofing attack. The assumptions are similar to those in Track 1. We set $\\pi_{tar} = 0.9405, \\pi_{non} = 0.0095, \\pi_{spf} = 0.05, C_{miss} = 1$ and $C_{fa,non} = C_{fa,spf} = 10$. This gives $\\alpha \\approx 1.58$ and $\\gamma \\approx 0.84$.\nThe primary metric of Track 2 is the minimum of the a-DCF, $\\min a\\u001dCF = \\min_{T_{sasv}} a\\u001dCF(T_{sasv})$.\nFor the submissions that contain clearly-identified ASV and CM sub-systems, ASV-constrained minimum tandem detection cost function (t-DCF) [10] and tandem equal error rate (t-EER) [11] metrics are additionally reported. Whereas the former has served as the primary metric since ASVspoof 2019, the latter provides a complementary parameter-free measure of class discrimination. To compute the t-DCF metric, we adopt the same costs and priors as above and use ASV scores produced by a common ASV system of the organiser in place of scores provided by participants. This allows computation of the minimum 'ASV-constrained' t-DCF in the same way as for the previous ASVspoof challenges and enables the comparison of different CM sub-systems when they are combined with a common ASV sub-system.\nFor computation of the t-EER metric, both the CM and ASV sub-system scores are used to obtain a single concurrent t-EER value, denoted by $t\\u001dER_x$. It has a simple interpretation as the error rate at a unique pair of ASV and CM thresholds, $T_x := (T_{asv}, T_{cm})$, at which the miss rate and the two types of false alarm rates (one for spoofing attacks, the other for non-targets) are equal: $t\\u001dER_x = P_{miss}^{tdm}(T_x) = P_{fa,non}^{tdm}(T_x) = P_{fa,spoof}^{tdm}(T_x)$. The superscript 'tdm' is used to emphasize the assumed tandem architecture. The t-EER can be seen as a generalisation of the conventional two-class, single system EER which provides an application-agnostic discrimination measure."}, {"title": "4. Common ASV, surrogate systems, and challenge baselines", "content": "4.1. Common ASV system by organisers\nThe common ASV system uses an ECAPA-TDNN speaker encoder [36] and cosine similarity scoring. The ECAPA-TDNN model is trained using the training partitions of VoxCeleb 1 and 2 [37]. The computed ASV cosine scores are subsequently normalised using an s-norm. Figure 1 illustrates the ASV EER values on the evaluation data. The EER (5%) is low when discriminating between bona fide target and non-target speakers' data (the leftmost bar). However, the EERs are much higher when spoofing attacks are mounted. Note that although A25 is the least effective attack, it is more threatening when enhanced as an adversarial attack A32.\n4.2. Baseline systems\nTrack 1 adopts two CM baseline systems: RawNet2 [38] (B01) and AASIST [39] (B02). Both systems are end-to-end systems operating directly on raw waveforms. Inputs to these baseline systems are raw waveforms of 4 seconds duration (64,000 samples). RawNet2 is composed of a fixed bank of 20 sinc filters, six residual blocks followed by gated recurrent units, which convert the frame-level representation to utterance-level representation. The CM output scores are generated using fully-connected layers.\nAASIST uses a RawNet2-based encoder [38] to extract spectro-temporal features from the input waveform. A spectro-temporal heterogeneous graph attention layers and max graph operations are then used to integrate temporal and spectral representations. CM output scores are generated using a readout operation and a linear output layer. Both baselines were trained with a weighted cross-entropy loss for binary classification.\nIn Track 2, a fusion-based [6] (B03) and a single integrated [40] (B04) systems are adopted. B03 is adopted from the SASV 2022 challenge baseline but fuses the common ASV and the baseline AASIST of Track 1 using an LLR-based fusion tool [41]. B04, which is based on MFA-Conformer [42], extracts a single embedding from the input waveform and produces a single SASV score. It is trained in three stages: speaker classification-based pre-training, copy synthesis [43] training with adapted SASV loss functions, and in-domain fine-tuning. VoxCeleb and copy synthesis data are used in the first and second stages, respectively. The in-domain fine-tuning is conducted using ASV spoof 5 training data. The source codes for all baselines are accessible from the ASVspoof 5 git repository."}, {"title": "4.3. Surrogate systems", "content": "The surrogate ASV system is based on ECAPA-TDNN and a probabilistic linear discriminant analysis scoring backend [44]. The surrogate CM systems include AASIST, RawNet2, LCNNS with LFCC, all of which are trained on bona fide data from the MLS partition A and spoofed attacks created by the first group of data contributors (see \u00a7 2). Note that the surrogate CMs do not see attacks in the development and evaluation sets."}, {"title": "5. Evaluation platform", "content": "ASVspoof 5 used the CodaLab website through which participants could submit detection scores and receive results. The challenge was run in two phases, with an additional post-evaluation phase (not addressed in this paper). During the first progress phase, participants could submit up to four submissions per day. Results determined from a subset of the evaluation set (i.e., the progress subset) were made available to participants who could opt to submit their results to an anonymised leaderboard. The evaluation phase ran for only a few days, during which participants could make only a single submission. This submission was evaluated using the whole evaluation set. Figure 2 illustrates the number of submissions during the progress and evaluation phases. In Track 1, closed and open conditions received comparable amount of submissions. In contrast, in case of Track 2, open condition received considerably higher number of submissions, demonstrating the need for additional data for training SASV systems."}, {"title": "6. Challenge results", "content": "6.1. Track 1\nResults on Track 1 are listed in Table 4. The baseline systems achieved minDCF higher than 0.7 and EERs higher than 29%. Although they use the RawNet2 and AASIST architectures, which have been demonstrated to be effective on the previous ASVspoof challenge databases, the non-studio-quality data sourced from MLS and the more advanced spoofing attacks may have led to their unsatisfactory performance.\nIt is encouraging that most of the submissions in the closed"}, {"title": "6.2. Track 2", "content": "Results on Track 2 are listed in Table 5. Spoofing-robust ASV is technically more demanding than a stand-alone CM, which may be the reason for lower numbers of submissions to Track 2. B03 performed similarly to a reference system (REF) that is the same as B03 except using a random guessing CM sub-system. This indicates that the CM sub-system in B03 does not provide useful information for detection spoofing attacks. In contrast,"}, {"title": "7. Conclusions", "content": "This paper outlines the ASVspoof 5 challenge, which is designed to support the evaluation of both stand-alone speech spoofing and deepfake detection and SASV solutions. The fifth edition was considerably more complex than its predecessors, including not only a new task, but also more challenging crowdsourced data collected under variable conditions, spoofing attacks generated with a variety of contemporary algorithms optimised to fool surrogate ASV and CM sub-systems, and new adversarial attacks. Despite the use of lower-quality data to create spoofs and deepfakes, detection performance for the baseline systems, all top-performing systems reported in recent years, is relatively poor. Encouragingly, results for most challenge submissions outperform the challenge baselines, sometimes by a substantial margin. We look forward to learning about the technical details from the challenge participants in their forthcoming research articles. Results also reveal the hitherto ignored issue of score calibration, an essential consideration if detection solutions are deployed in real, practical scenarios.\nWith a particularly tight schedule for ASVspoof 5, more detailed analyses will be presented at the ASV spoof 5 workshop and reported in future work."}]}