{"title": "EDITROOM: LLM-PARAMETERIZED GRAPH DIFFUSION FOR COMPOSABLE 3D ROOM LAYOUT EDITING", "authors": ["Kaizhi Zheng", "Xiaotong Chen", "Xuehai He", "Jing Gu", "Linjie Li", "Zhengyuan Yang", "Kevin Lin", "Jianfeng Wang", "Lijuan Wang", "Xin Eric Wang"], "abstract": "Given the steep learning curve of professional 3D software and the time-consuming process of managing large 3D assets, language-guided 3D scene editing has significant potential in fields such as virtual reality, augmented reality, and gaming. However, recent approaches to language-guided 3D scene editing either require manual interventions or focus only on appearance modifications without supporting comprehensive scene layout changes. In response, we propose EditRoom, a unified framework capable of executing a variety of layout edits through natural language commands, without requiring manual intervention. Specifically, EditRoom leverages Large Language Models (LLMs) for command planning and generates target scenes using a diffusion-based method, enabling six types of edits: rotate, translate, scale, replace, add, and remove. To address the lack of data for language-guided 3D scene editing, we have developed an automatic pipeline to augment existing 3D scene synthesis datasets and introduced EditRoom-DB, a large-scale dataset with 83k editing pairs, for training and evaluation. Our experiments demonstrate that our approach consistently outperforms other baselines across all metrics, indicating higher accuracy and coherence in language-guided scene layout editing.", "sections": [{"title": "1 INTRODUCTION", "content": "Traditionally, editing 3D scenes requires manual intervention through specialized software like Blender (Community, 2024), which demands substantial expertise and considerable time for resource management. As a result, language-guided 3D scene editing has emerged as a promising technology for next-generation 3D software. To build an automated system capable of interpreting natural language and manipulating scenes, the system must be able to align complex, diverse, and often ambiguous language commands with various editing actions while also comprehending the global spatial structure of the scene. Additionally, the relatively small size of available 3D scene datasets presents a challenge for developing large-scale pretrained models necessary for fully auto-mated, end-to-end language-guided scene editing.\nRecently, there have been some works (Zhuang et al., 2023; Bartrum et al., 2024; Karim et al., 2023) focusing on leveraging pretrained image generation models to edit single object appearance inside the scene, but they fail to modify the layout of original scenes. Meanwhile, other approaches (Chen et al., 2023; Ye et al., 2023) further incorporate pretrained segmentation models to enable individual object manipulation. However, they require manual intervention to determine the edited object and editing type for any layout adjustments, like adding a new object or changing the object pose. Furthermore, these methods are all limited to executing a single editing step and are hard to deal with commands with multiple potential steps.\nTherefore, we propose EditRoom, a unified framework that can execute all editing types by complex natural language commands without intermediate manual interventions. It consists of two main modules: the command parameterizer and the scene editor. The command parameterizer employs an pretrained LLM, specifically GPT-40 (OpenAI, 2024), to transform natural language commands into sequences of breakdown commands for six basic editing types on single object: adding, removing, replacing, translating, rotating, and scaling. These breakdown com-mands, along with the source scenes, are then fed sequentially into the scene editor for execution. To unify all editing types, we convert scenes into graph representations and construct scene editor as conditional graph generation models, where we take source scenes and text commands as conditions and train diffusion models to generate the target scene graphs with the layout.\nAnother challenge is the lack of language-guided scene editing datasets, which constrains both train-ing and evaluation of scene editing models. To enable the scene editor to accurately execute every basic editing type, we construct an automatic data generation pipeline and collect a synthetic scene editing dataset named EditRoom-DB for both training and evaluation, which includes approxi-mately 83k editing pairs with commands. We implement several functions to simulate the single editing process on the existing 3D scene synthetic dataset, 3D FRONT (Fu et al., 2021a), which contains 16k indoor scene designs equipped with high-quality object models, and generate corre-sponding language commands using predefined templates. To mimic the human inputs, we employ LLMs to transform the templated commands into natural language forms, serving both as training material for our baselines and as test cases for single-operation evaluations.\nIn the experiments, we quantitatively assess the performance of EditRoom in scenarios with single-operation commands. From the results, we find that EditRoom outperforms other baselines in all metrics across different room types and editing types, which indicates higher precision and co-herence in single-operation editing. Furthermore, we qualitatively evaluate EditRoom in scenarios involving multi-operation commands. We find that the model can successfully generalize to these scenarios even though we do not train the model on multi-operation data.\nOur contributions are summarized as follows:\n\u2022 We propose a new framework, named EditRoom, consisting of the command parameterizer and scene editor, which accepts scene inputs and can edit scenes using natural language commands by leveraging LLM for planning.\n\u2022 We propose a unified graph diffusion-based module that serves as the scene editor, capa-ble of executing every basic editing type, including adding, removing, replacing, translating, rotating, and scaling.\n\u2022 We introduce the EditRoom-DB dataset with 83k editing pairs for the first language-guided 3D scene layout editing dataset by augmenting the existing 3D scene synthesis dataset."}, {"title": "2 RELATED WORK", "content": "Language-guided 3D Scene Generation Currently, some language-guided 3D Scene generation works that claim for language-guided scene editing capablility can be abstractly categorized into two main approaches. The first approach starts with generating the scene configurations by utilizing large language models, and prompt large language model for following editing (Vilesov et al., 2023; Zhou et al., 2024b; Aguina-Kang et al., 2024). However, they are limited to editing its own generated scenes and do not accept source scenes as input. The second approach is about training diffusion-based model to learn text-conditional scene generation (Haque et al., 2023; Tang et al., 2023; Zhai et al., 2024). During the editing, they needs to manually mask the editing part and leverage the com-pletion ability of diffusion models. Besides, their editing types are limited. InstructScene (Haque et al., 2023) does not add language conditions to the scene layout, so translating, rotating, and scal-ing cannot be applied to the scenes. EchoScene's manipulations only apply to object relations so that they can only add an object or change the relative positions of objects (Zhai et al., 2024). In contrast to these works, EditRoom can accept an existing scene as input and apply free-form editing commands for 3D scene layout without manual interventions.\nLanguage-guided 3D Scene Editing Recent language-guided 3D scene editing works mostly in-coprate neural field representations and pretrained image generation models for object appearance editing. Some works are mainly focusing on replacing single object (Zhuang et al., 2023; Bartrum et al., 2024; Karim et al., 2023) but failing to edit the layout. Other approaches (Chen et al., 2023; Ye et al., 2023) leverage pretrained segmantation models to obtain the individual object representation so that they can remove the objects by manually selecting the target objects. Besides, all these meth-ods can only take one single-operation command at each interaction. Compared to these previous works, EditRoom can leverage LLM to deal with multi-operation natural language commands and automatically execute all editing types through a unified graph diffusion-based module.\nLLMs for 3D Scene Understanding Recent works demonstrate that existing LLMs can facilitate 3D spatial reasoning. These works usually leverage the pretrained image caption models to convert 3D scenes into text descriptions and ask the LLM to generate navigation steps (Zhou et al., 2023; 2024a), provide room layout (Feng et al., 2024), or ground 3D objects (Yang et al., 2023; Hong et al., 2023; Huang et al., 2023). In our work, we are the first work to leverage LLM for natural language-guided 3D layout editing, where LLM takes source scenes in text format and breaks the natural language commands into basic editing operations.\nDiffusion Models for Graph In recent years, denoising diffusion models have shown impressive generative capability in the graph generation (Liu et al., 2023a; Kong et al., 2023; Vignac et al., 2022). Compared to the previous VAE-based Verma et al. (2022) or GAN-based (Martinkus et al., 2022) models, diffusion-based models have advantages like stable training processes and generaliz-ability to various graph structures. Some works have presented that graph diffusion-based models can be used in molecule generation (Guo et al., 2022; Akhmetshin, 2023), protein modeling (Zhang et al., 2022), 3D scene generation (Haque et al., 2023; Zhai et al., 2024), and etc. In this work, we first propose to use graph diffusion-based models for language-guided 3D scene layout editing."}, {"title": "3 THE EDITROOM METHOD", "content": "In this section, we introduce EditRoom, shown in Figure 1, comprising two primary modules: Command Parameterizer and the Scene Editor. Given a natural language command C and source scene S, we aim to estimate the target scene T with conditional distribution $q(T|S, C')$. Our command pa-rameterizer takes the source scene S and natural command C to generate the breakdown commands L. Then, the scene editor conditions on breakdown commands L to obtain the final target scene T, where the whole pipeline can be written as $q(T|S, C) = q(L|S, C) \\times q(T|S, L)$. All objects inside source scenes and target scenes are retrieved from a high-quality 3D furniture dataset (Fu et al., 2021b)."}, {"title": "3.1 LLM AS COMMAND PARAMETERIZER", "content": "In order to process open natural language commands, we use GPT-40 (OpenAI, 2024) to convert natural language command C into a set of combinations of basic editing types with breakdown commands $L := \\{l_i\\}_{i=1}^{N_L}$, where $N_L$ is the number of breakdown commands. To cover the general manipulations on the scene, we design six basic editing operations:\n\u2022 Rotate an object: [Rotate, Target Object Description, Angle]\n\u2022 Translate an object: [Translate, Target Object Description, Direction, Distance]\n\u2022 Scale an object: [Scale, Target Object Description, Scale Factor]\n\u2022 Replace an object: [Replace, Source Object Description, Target Object Description]\n\u2022 Add an object: [Add, Target Object Description, Target Object Location]\n\u2022 Remove an object: [Remove, Target Object Description]\nWhen the target object is not unique, we ask the LLM to use another unique object as a reference to describe the spatial relation. During the inference phase, we prompt the LLM with attributes of objects within the source scene along with the natural language command, tasking the model to analyze the scene and delineate basic editing operations through breakdown commands in specified formats. The attributes include categories, locations, sizes, rotations, and object captions. Detailed descriptions of the full prompt and examples are provided in Figure 5 of the appendix."}, {"title": "3.2 GRAPH DIFFUSION AS 3D SCENE EDITOR", "content": "Given the breakdown command l and source scene S, our objective is to determine the conditional target scene distribution $q(T|S, l)$. Drawing inspiration from recent advancements in language-guided 3D scene synthesis (Lin & Yadong, 2023), we transform scenes into semantic graphs and employ a graph transformer-based conditional diffusion model to learn the conditional target scene"}, {"title": "Scene Graph Representation", "content": "Each scene is represented as a combination of a layout B and a scene graph G (Lin & Yadong, 2023). The layout B encapsulates the position, size, and orientation of each object, while the scene graph G encodes additional high-level semantic information. Formally, a semantic scene graph G := (V, E) comprises nodes $v_i \\in V$, where each $v_i$ corresponds to an object $o_i$ with high-level attributes. Directed edges $e_{ij} \\in E$ represent spatial relationships: [\"in front of\", \"behind\", \"right of\", \"left of\", \"closely in front of\", \"closely behind\", \"closely right of\", \"closely left of\", \"above\u201d, \u201cbelow\u201d], connecting the i-th object to the j-th object, where \u201cclosely\u201d means the distance between two object centers are less than 1 meter. Each node $v_i$ is characterized by a discrete category $c_i$ and continuous semantic features $f_i$, derived from a pretrained multimodal-aligned point cloud encoder, OpenShape (Liu et al., 2024c), which features a 1280-dimensional representation space."}, {"title": "Target Graph Diffusion", "content": "In this stage, we aim to learn target scene graphs $G_{tg}$ by giving source scene graphs $G_s$ and language commands l through a discrete graph diffusion model $\\varepsilon_g$, where $G_{tg}$ includes category $C_{tg}$ and semantic features $F_{tg}$ for each node and the edges $E_{tg}$ for object relative relations. Since high-dimensional object semantic features (d = 1280) are too complicated to learn from limited data, we use a VQ-VAE model (Lin & Yadong, 2023; Wang et al., 2019) to compress them into low-dimensional features $z \\in R^{n_f\\times d_z}$, which consists of $n_f$ vectors extracted from a learned codebook $Z \\in R^{K_f\\times d_z}$ by a sequence of feature indices $fid\\chi := \\{1, ..., K_f\\}^{n_f}$, where $K_f$ and $d_z$ are the size and dimension of codebook. Then, we use the feature indices to replace the orig-inal object semantic features as targets for training, denoted as F. Therefore, $G_{tg} = (C_{tg}, F_{tg}, E_{tg})$ and $G_s = (C_s, F_s, E_s)$, and our goal is to learn the conditional distribution $q(G_{tg}|G_s,l)$. During the training process, at timestep t, the gaussian noises are added to the $G_{tg}$ to get $G_{t^g}$, and the model $\\varepsilon_g$ aims to reconstruct $G_{t-1^g}$, by conditioning on $G_s$ and l. To add the conditions, we concatenate each element of source scene graphs into noisy target scene graphs as context and use cross-attention layers to incorporate language features. The loss function can be written as:\n$L_g :=E_{q(G_0^g)} \\left[\\sum_{t=2}^{T} L_{t-1} - E_{q(G_{t-1^g}|G_t^g)} [log \\ p_{\\varepsilon_g}(G_{t-1^g}|G_t^g, G_s, l)]\\right] $ (1)\n$L_{t-1}:=D_{KL}[q(G_{t-1^g}|G_{tg}, G_t^g)||p_{\\varepsilon_g}(G_{t-1^g}|G_t^g, G_s, l)]$ (2)\nwhere $D_{KL}$ indicates the KL divergence."}, {"title": "Target Layout Diffusion", "content": "In this stage, we aim to estimate the target scene layout $B_{tg}$ using another graph diffusion model $\\varepsilon_b$, conditioning on target scene graph $G_{tg}$, source scene graph $G_s$, source layout $B_s$, and language command l. The target scene layout $B_{tg} \\in R^{M\\times 8}$ consists of position $T_{tg} \\in R^{M\\times 3}$, size $S_{tg} \\in R^{M\\times 3}$, and rotation $R_{tg} \\in R^{M\\times 2}$. During the training process, gaussian noises \\epsilon will be added to the target layout, and the layouts are encoded into the node features by MLP layers. Similar to the Target Graph Diffusion, we concatenate the source scene graph and source layout to the target scene graph as context and corrupted target layout. The language features are incorporated through cross-attention layers. The objective target is to estimate the added noises at each time step. The loss function can be written as:\n$L_b := E_{B_{tg},t,\\epsilon} [||\\epsilon - \\varepsilon_b (B_t^g, t, G_{tg}, G_s, B_s, l)||^2]$ (3)"}, {"title": "Inference Process", "content": "During the inference phase, the first step consists of transforming the source scene into a scene graph $G_s$ and a corresponding layout $B_s$. Subsequently, the Target Graph Gener-ation model predicts the target scene graph $G_{tg}$, conditioned on the source scene graph $G_s$ and the language command l. This is followed by the Target Layout Generation model, which computes the target layout $B_{tg}$, leveraging all available variables as inputs. The final step in constructing the tar-get scene, denoted as $T := (G_{tg}, B_{tg})$, involves retrieving the object meshes based on the estimated object features and arranging them according to the generated layout. This systematic approach enables the dynamic generation of scenes that are aligned with verbal instructions, ensuring that the resulting scenes accurately represent the specified conditions."}, {"title": "4 THE EDITROOM-DB DATASET", "content": "To facilitate comprehensive training and evaluation of our framework, we present the EditRoom-DB dataset, designed to support a wide range of basic 3D scene editing operations. The dataset is generated through an automated data augmentation pipeline that produces editing pairs based on object-level modifications applied to scenes from the 3D-FRONT dataset (Fu et al., 2021a). We utilize scenes from the bedroom, dining room, and living room categories and enhance them with high-quality object models from the 3D-FUTURE dataset (Fu et al., 2021c) to simulate real-world editing workflows.\nOur data generation pipeline supports a variety of editing operations, including Add and Remove Objects, Pose and Size Changes, and Object Replacement. Each modification produces a new scene paired with a detailed textual description of the changes made, following a predefined template. For each scene, objects are selected randomly and modified iteratively through basic editing operations. In the case of Add and Remove Objects, the modified scene serves as the target for the operation, with the original scene acting as the source. For Pose and Size Changes, random adjustments are applied to selected objects, and collision checking ensures that the final scenes are free from object overlap. Similarly, in Object Replacement, new objects from the same category are substituted for existing ones, with collision checks ensuring high-quality outputs. More details about automatic pipelines can be found in Appendix B.\nTo create diverse and realistic language instructions for the editing tasks, we first employ the mul-timodal understanding model LLAVA-1.6 (Liu et al., 2024b;a; 2023b), which captions front-view images of the objects in the scenes. Then, these object captions are used to construct the template-based commands. The template-based commands are then transformed into natural language com-mands using GPT-40, making the dataset suitable for training and testing language-guided scene editing models. Detailed prompts and additional examples are shown in Figure 6 in the appendix.\nThe resulting dataset consists of approximately 83,000 training samples and 1,500 test samples (ran-domly sampled 500 for each room type). Table 1 provides a detailed breakdown of the dataset statis-tics across the three scene categories. Each sample includes a source scene, an edited target scene, and corresponding language commands. This comprehensive dataset enables the development and evaluation of robust models capable of performing various scene editing tasks. By simulating real-istic workflows and providing detailed text commands, EditRoom-DB serves as a valuable resource for advancing language-guided 3D scene editing."}, {"title": "5 EXPERIMENTS", "content": null}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Baselines To the best of the authors' knowledge, EditRoom is the first work that can automatically execute all editing types with natural language commands. Therefore, we construct two baselines by modifying language-guided 3D scene synthesis methods for comparisons: DiffuScene-E and SceneEditor-N:"}, {"title": "A LLM AS COMMAND PLANNER", "content": "(Referred by Section 3.1) A detailed dialog between user and LLM (GPT-40) is shown in Figure 5."}, {"title": "B EDITROOM-DB PIPELINE DETAILS", "content": "(Referred by Section 4) A detailed example of using LLM to generate natural language description from template command is shown in Figure 6.\nAdd and Remove Objects Removing each object in the scene separately could generate the mod-ified scenes as the result after removal compared to the original scene. Conversely, the original scene could be treated as the result after object addition. The formatted editing description will be 'Add/Remove [object description]'. In order to consider the location of the addition and potential multiple objects in the scene, we will add the relative location description with the closest unique object in the scene, like 'location: [relative description] [reference object description]'.\nPose and Size Changes We can similarly repeat the pose change operation for every object in the scene as add/remove. Specifically, we design three operations: translation, rotation, and scaling. For translation, we create random translations as the mix of distances, sampled from 0.1 meters to 1.5 meters with step 0.1, and directions, sampled along the two axes directions (front/back and left/right). Then, collision checking is done for every translated object until we find a collision-free sample. The translation will be skipped if all the samples fail in collision checking. The formatted editing description will be 'Move object towards the front/back/left/right direction for [distance] : [object description]'\nSimilarly, we create random rotation angles as the mix of uniform direction samples, clockwise or counterclockwise, and random values between 15 180 degrees with the step of 15 degrees, and check collision for each sample. The check stops when we find a collision-free sample or all samples fail the checking. The formatted editing description will be \u2018Rotate object [angle] degrees : [object description]'\nFor scaling, we separate it as shrinking and enlarging. The scaling factor is randomly generated be-tween 0.5-0.8 or 1.2-1.5. The scaling factor uniformly applies to three dimensions. Since shrinking won't cause a collision with other objects, it can always result in a successfully modified scene. For enlarging, if collision checking fails on all trials, the enlarging is skipped. Otherwise, we save the largest collision-free scaling factor. The formatted editing description will be 'Shrink/Enlarge object by [scale_factor] times : [object description]'\nObject Replacement For the replace operation, we access an object dataset with semantic class labels and 3D meshes. The system will retrieve several objects from the dataset with the same class label as the replaced object, and check their collision with other existing objects in the scene. If none of the objects could be placed without collision, we randomly select one object and shrink its bounding box to be equal or smaller than the replaced object to avoid collision. The formatted description is 'Replace source with target : [source object description] to [target object description]'.\nCollision Detection Module The objects are abstracted as 3D bounding boxes and further decom-posed into 2D bounding boxes on a horizontal plane and vertical range, as the objects can only rotate about the vertical axis. Then, the two objects are only in collision if their 2D bounding boxes overlap and their vertical ranges overlap. For 2D bounding box collision detection, we apply the Separating Axis Theorem Huynh (2009) to determine if the boxes intersect."}, {"title": "Ethics Statement", "content": "Our work presents a framework for automated 3D scene editing guided by natural language. The primary focus of this research is to advance technical capabilities in scene manipulation. We do not anticipate any ethical concerns or negative societal impacts arising from this work. All datasets used in our research are synthetic and publicly available, and no personally identifiable information or human-related data were involved."}, {"title": "Reproducibility Statement", "content": "All language commands are encoded through the pretrained CLIP-ViT-B32 text encoder. Each graph diffusion model includes a five-layer graph transformer model with 512 hidden dimensions and 8 attention heads. Training is conducted using the AdamW op-timizer over 300 epochs, with a batch size of 512 and a learning rate of 2 \u00d7 10-4. All models are individually trained and tested on each room type. For EditRoom, template commands are em-ployed for the scene editor during training, whereas other baseline models utilize natural language commands generated by GPT-40. During testing, all models receive natural language commands as input. We set the node number of denoise graphs to the maximum object number for different room types (bedroom = 12, living room = dining room = 21). During training, we pad the objects with zeros to facilitate the batch process."}]}