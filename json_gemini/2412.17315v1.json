{"title": "CODEV: Issue Resolving with Visual Data", "authors": ["Linhao Zhang", "Daoguang Zan*", "Quanshun Yang", "Zhirong Huang", "Dong Chen", "Bo Shen", "Tianyu Liu", "Yongshun Gong", "Pengjie Huang", "Xudong Lu*", "Guangtai Liang", "Lizhen Cui", "Qianxiang Wang"], "abstract": "Large Language Models (LLMs) have advanced rapidly in recent years, with their applications in software engineering expanding to more complex repository-level tasks. GitHub issue resolving is a key challenge among these tasks. While recent approaches have made progress on this task, they focus on textual data within issues, neglecting visual data. However, this visual data is crucial for resolving issues as it conveys additional knowledge that text alone cannot. We propose CODEV, the first approach to leveraging visual data to enhance the issue-resolving capabilities of LLMs. CODEV resolves each issue by following a two-phase process: data processing and patch generation. To evaluate CODEV, we construct a benchmark for visual issue resolving, namely Visual SWE-bench. Through extensive experiments, we demonstrate the effectiveness of CODEV, as well as provide valuable insights into leveraging visual data to resolve GitHub issues\u00b9.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have advanced rapidly in recent years, with their applications in the field of software engineering becoming increasingly widespread (Zan et al., 2023; Zheng et al., 2023; Zhang et al., 2023b; Chen et al., 2024b). Currently, LLMs' applications in software engineering have gradually expanded tasks at the code line and function level to more challenging repository-level tasks (Zhang et al., 2023a; Liu et al., 2024). Within repository-level tasks, GitHub issue resolving is a key challenge, where LLMs are tasked to resolve the issue based on the issue description and the defective codebase. (Jimenez et al., 2024; Xia et al., 2024). This task can accelerate program repair and is crucial for improving development efficiency. Although recent approaches have made progress on this task, they focus exclusively on textual data coding capabilities. Currently, only the latest commercial models, GPT-40 (OpenAI, 2024) and Claude 3.5 Sonnet (Anthropic, 2024), barely meet these requirements, but their capabilities remain highly limited. Moreover, these models are less suitable for issue resolving due to high computational costs. Based on our analysis, using these models within the popular SWE-agent approach (Yang et al., 2024a) to run through all issues in SWE-bench once is estimated to cost an average of over $4,700 (Xia et al., 2024), which imposes a significant financial burden on researchers. To address this, we propose CODEV, the first approach that leverages visual data to enhance the issue-resolving capabilities of LLMs at low cost. To resolve each issue, CODEV follows a two-phase process: data processing and patch generation. In the data processing phase, CODEV processes the visual data within the issue from both local and holistic perspectives. This phase produces fine-grained descriptions of the visual data and a structured summary of the entire issue. In the patch generation phase, CODEV leverages the processed information to assist LLMs in generating a patch to resolve the issue.\nTo evaluate our approach, we construct a benchmark specifically designed for evaluating visual GitHub issue resolving, called Visual SWE-bench. The benchmark comprises 133 task instances spanning 11 open-source GitHub repositories, each of which has undergone rigorous selection. Finally, we conduct a series of experiments to validate the effectiveness of our approach. Experimental results demonstrate that CODEV achieves a round 63.13% relative improvement in the percentage of resolved instances on Visual SWE-bench compared to Agentless. Additionally, through case studies, we analyze the role of each component of CODEV, providing insights into leveraging visual data to resolve issues. Overall, the contributions of this paper are as follows:\n\u2022 We propose CODEV, a simple yet novel approach that leverages visual data to enhance the issue-resolving capabilities of LLMs.\n\u2022 We construct a benchmark designed to evaluate the performance of LLMs in resolving visual GitHub issues, namely Visual SWE-bench. The benchmark comprises 133 realistic software engineering tasks sourced from 11 open-source GitHub repositories.\n\u2022 We validate the effectiveness of our approach through a series of experiments and conduct in-depth analysis and summarization of the experimental results."}, {"title": "2 Approach", "content": "Figure 2 illustrates an overview of CODEV, which consists of two phases: data processing and patch generation. The first phase processes visual data and the second phase uses the processed information to assist LLMs in generating a patch. Below is a detailed description of each phase."}, {"title": "2.1 Data Processing", "content": "To process the issue's visual data, we adopt two components: fine-grained description and structured summarization. For fine-grained description, the Vision-Language Model (VLM) first generates an independent description for each piece of visual data based on its content. It then provides a contextual description that relates this data to the issue, resulting in a fine-grained description. In structured summarization, the VLM produces a summary that breaks down the complex issue into several clear sections. Below, we detail the implementation of each component."}, {"title": "2.1.1 Fine-Grained Description", "content": "We design the fine-grained description component to generate textual representations of the visual data. This component draws inspiration from how humans process visual data. When encountering visual data in an issue, humans first identify its raw features and then analyze its function in the context of the problem. Inspired by this, we design a two-step process to generate fine-grained descriptions.\nStep 1: Independent Description. In the first step, we instruct the VLM to describe each piece of visual data based solely on its content. For visual data consisting purely of text, such as a screenshot of error logs, the VLM extracts the text and outputs it in Markdown format. For other types of visual data, like images or videos with non-textual content, the VLM generates a detailed description capturing all details.\nStep 2: Contextual Description. In the second step, the VLM is prompted with the complete problem statement to establish the context. Subsequently, it is tasked with providing a comprehensive description and analysis of the visual data based on this contextual understanding. During this step, the VLM tightly connects the visual data with the problem's context to analyze its function."}, {"title": "2.1.2 Structured Summarization", "content": "Some GitHub issues are described in a structured format, including reproduction steps, expected results, actual results, and so on. This format enhances clarity and makes the key aspects of issues easier to understand. Inspired by this, we propose generating a structured summary to enrich issues and reduce the difficulty of understanding them.\nIn the structured summarization component, the VLM is prompted with the complete problem statement, including the visual data. It is then tasked with understanding and analyzing the issue to generate a structured summary. To guide this process, we supply the VLM with a template that consists of the following fields: a brief problem summary, background information, reproduction steps, expected results, actual results, descriptions of visual data, and additional notes. However, not all issues may fit perfectly with this template. Therefore, we allow the VLM to skip irrelevant or unclear fields. Additionally, the summary can also include new fields if needed, as long as it remains clear and useful for resolving the issue.\nUnlike fine-grained description, which focuses on generating representations of visual data, structured summarization aims to provide an overview of the entire issue. It not only covers visual data but also gives a deeper understanding of the problem."}, {"title": "2.2 Patch Generation", "content": "After generating fine-grained descriptions and a structured summary in the data processing phase, the patch generation phase leverages this information to generate a patch. To support LLMs in efficiently utilizing this information, we splice them into the original issue. Specifically, the visual data is converted into fine-grained descriptions, and the issue is enriched with a structured summary, with an example provided in Appendix B.\nTo enhance the ability of LLMs to resolve textual issues, various approaches have been proposed. These approaches take different forms: some are agent-based, equipping LLMs with a set of tools that allow the agent to autonomously perform actions (Chen et al., 2024a; Yang et al., 2024a; Zhang et al., 2024); others are agentless (Xia et al., 2024). Regardless of their form, they typically input the issue and codebase, with the output being a generated patch. CODEV combines these approaches through a unified interface, automating patch generation. At this point, the newly generated issue and its corresponding codebase are fed into the textual issue-resolving approach, where LLMs follow predefined instructions to generate a patch."}, {"title": "3 Visual SWE-bench Benchmark", "content": "In current benchmarks for the issue-resolving task, only the recently released SWE-bench Multimodal (Yang et al., 2024b) focuses on visual issues. However, as of writing, SWE-bench Multimodal\u2074 lacks evaluation fields, and its evaluation script has not been made public, making it unsuitable for evaluation. To evaluate CodeV, we construct a benchmark for resolving visual GitHub issues, namely Visual SWE-bench. Below, we detail our benchmark construction process and its key features."}, {"title": "3.1 Construction", "content": "From the 2,294 instances in SWE-bench, we identify 128 task instances whose problem statement contains visual data. These visual data are presented through hyperlinks, with images embedded using HTML or Markdown syntax and videos provided as plain text hyperlinks. Building on these identified task instances, we adopt a four-stage construction process to further expand the tasks and conduct rigorous verification on all instances.\n1. Repositories selection and pull requests Collection. We analyze the 128 task instances from SWE-bench, and the results show that most of them originate from visualization libraries. To expand our benchmark, we select three additional popular open-source visualization libraries (plotly.py, networkx, and altair) and crawl all their pull requests (PRs) from GitHub. Since SWE-bench only includes PRs created before August 2023, we select repositories with at least 10 visual task instances from SWE-bench (matplotlib, sympy, sphinx, and seaborn) and collect recent PRS from these repositories. These two rounds of collection yield approximately 10,000 PRs.\n2. Candidate instance construction. Candidate instances are constructed from the collected PRs through the following steps:\n(1) We select only merged PRs that resolve at least one issue and include modifications to test files.\n(2) For each PR, we extract the text of all resolved issues, retaining only those PRs where the issue text contained hyperlinks to images or videos.\n(3) For qualifying PRs, we gather detailed information, including \u201cinstance ID\u201d, \"patch\", \u201ctest patch\u201d, and so on.\nThis process results in 38 candidate instances from approximately 10,000 PRs.\n3. Execution verification. For each candidate instance, we meticulously set up the runtime environment and testing commands, removing any instances that failed due to installation or runtime errors. Next, we apply the test patch to each instance and record the test results both before and after applying the gold patch. Instances without any tests where the status changes from fail to pass are excluded. This process leaves 31 viable candidate instances.\n4. Human verification. We conduct human verification on 159 instances, comprising 128 task instances from SWE-bench and 31 candidate instances filtered through the previous stages. Each instance is evaluated based on the following criteria:\n(1) Whether the visual data can be fully converted to text.\n(2) Whether the visual data is essential for resolving the instance.\n(3) Whether the problem description contains sufficient information for effective resolution.\nUsing these criteria, we exclude 4 instances where visual data can be fully converted to text via Optical Character Recognition (OCR) and 4 instances where visual data is not essential for resolution. Additionally, based on the content of their test cases, we exclude 18 instances that cannot be resolved due to insufficient problem descriptions. This process results in a curated, high-quality benchmark of 133 task instances."}, {"title": "3.2 Features", "content": "As shown in Figure 3, Visual SWE-bench comprises 133 visual task instances sourced from 11 open-source GitHub repositories. These instances cover a wide range of functionalities, including but not limited to data visualization, machine learning, and document generation. This diverse set of tasks provides a comprehensive benchmark for evaluating the performance of LLMs in resolving visual issues automatically."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": "Models. To execute CODEV for resolving visual issues, two model types are required: a VLM for processing visual data and an LLM for generating patches. To demonstrate the effectiveness of CODEV, we specifically avoid commercial models and use open-source models in our experiments. For the VLM, we select Qwen2-VL (Wang et al., 2024), a model renowned for its robust visual understanding capabilities, using three versions: 2B, 7B, and 72B. For the LLM, we choose two models: DeepSeek-V2.5 (DeepSeek-AI, 2024) and Qwen2.5-Coder-32B (Hui et al., 2024), both recognized for their powerful coding capabilities."}, {"title": "4.2 Evaluation", "content": null}, {"title": "4.2.1 Main Results", "content": "Table 2 presents the results of all approaches. The results show that CODEV significantly enhances the issue-resolving capabilities of the LLM by leveraging visual data. Compared to all benchmarks, CODEV achieves the best performance. When combined with Agentless, CODEV achieves over a 50% relative improvement, whether using DeepSeek-V2.5 or Qwen2.5-Coder-32B to resolve issues. The performance of CodeV highlights the value of leveraging visual data to help LLMs understand and resolve issues.\nFigure 4(a) depicts the distribution of issues resolved by CODEV compared to both closed-source and open-source baseline approaches. Notably, CODEV can resolve certain issues that either open-source or closed-source approaches cannot resolve. Furthermore, CODEV successfully resolves some complex issues that neither category of approaches could solve. This highlights not only the advantages of CODEV but also the importance of leveraging visual data to resolve issues.\nFrom Table 2, it is evident that the performance of the VLM does not significantly impact CODEV. Among the three versions of Qwen2-VL, the 72B model is the most powerful, while the 7B and 2B models exhibit progressively weaker capabilities. However, even with the lower-performing 7B and 2B models, CODEV maintained robust issue-resolving capabilities, even outperforming the 72B model. Additionally, Figure 4(b) further illustrates the distribution of resolved issues across different VLMs. The issues resolved do not overlap entirely, indicating that each VLM has its strengths in processing different types of issues. This indicates that despite differences in VLM performance, CODEV can still exert the capabilities of VLM, resolve issues stably, and demonstrate strong robustness.\nAdditionally, we observe that using the VLM alone, while it leverages visual data, does not yield satisfactory results. For example, Agentless Plus combined with Qwen2-VL-72B resolves only one issue. This is primarily due to its weak coding capabilities. In comparison, CODEV effectively combines the VLM's visual understanding ability with the LLM's coding capabilities. This integration allows LLMs to leverage visual data to resolve issues at a low cost, making it a promising solution."}, {"title": "4.2.2 Analysis of Ablation Studies", "content": "We conduct a series of ablation studies on Visual SWE-bench, and the results in Table 3 show that removing any component of CODEV leads to a decline in performance. This led us to further investigate the functions of the components in the data processing phase.\nAnalysis of Fine-Grained Description. The fine-grained description process consists of two steps: independent description and contextual description. In the independent description, the VLM captures the raw features of visual data, providing a direct and detailed representation. However, why is contextual description also necessary? Figure 5 shows an issue that is difficult to resolve without the contextual description. The figure shows two images, and the contextual description analyzes their respective function, explaining the information conveyed by each. In contrast, the independent description provides only a general overview, missing critical details needed for a complete understanding of the issue. These details are essential for LLMs to grasp the issue accurately. Thus, while the independent description captures the raw features of the visual data, the contextual description extracts deeper, more nuanced information. Together, these two steps work in tandem to provide a comprehensive understanding of the visual data.\nAnalysis of Structured Summarization. As shown in Table 3, removing structured summarization significantly undermines the performance of CODEV. To explain this phenomenon, Figure 6 presents an issue that is more easily resolved with a summary. The summary generated by CODEV breaks down the complex issue into clear, digestible sections, providing LLMs with a full understanding of the issue's background, expected outcomes, and actual results. This structured format also helps LLMs grasp the core content more effectively. While the fine-grained description component attempts to convey the meaning of the visual data, relying solely on this still presents challenges in fully understanding the issue. By combining visual and textual data, the structured summary offers LLMs a more holistic understanding of the issue."}, {"title": "5 Related Works", "content": "Issue Resolving Approaches. To assist LLMs in resolving GitHub issues, many approaches have already been proposed. Retrieval Augmented Generation (RAG) (Jimenez et al., 2024) is a direct approach that resolves the issue by first extracting relevant code snippets from the repository and then using them to prompt LLMs to generate a patch. SWE-agent (Yang et al., 2024a) meticulously designs an agent-computer interface (ACI) that enables LLM agents to interact with repository environments to solve software engineering tasks. AutoCodeRover (Zhang et al., 2024) combines LLMs with code search, utilizes program structure, and conducts iterative searches for program improvement. CodeR (Chen et al., 2024a) is a multi-agent approach for issue-resolving tasks, adopting a multi-agent framework and pre-defined task graphs. Agentless (Xia et al., 2024) points out the limitations of using agents and proposes a simple two-phase process of localization and repair to solve software development problems. However, these existing approaches overlook visual data within issues. CODEV bridges this gap by processing visual data from both local and holistic perspectives, enhancing the capabilities of LLMs to resolve complex visual issues.\nCode Generation Benchmarks. Code generation has long been a measure of LLMs performance (Austin et al., 2021). The emergence of HumanEval (Chen et al., 2021) provides a standardized framework for evaluating code generation models. In subsequent years, various benchmarks have been developed to enhance HumanEval by adding extensions to different languages (Cassano et al., 2022; Athiwaratkun et al., 2023; Orlanski et al., 2023), introducing variations in edit scope (Yu et al., 2024; Du et al., 2023), presenting similar yet novel code completion tasks (Muennighoff et al., 2024), and conducting more extensive testing (Liu et al., 2023). With the development of LLMs, existing benchmarks struggle to explore the boundaries of state-of-the-art LLMs' capabilities. To address this, SWE-bench (Jimenez et al., 2024) offers a direction by researching real-world GitHub issues, serving as a challenging benchmark for evaluating next-generation LLMs. Building on this, SWE-bench-Java (Zan et al., 2024) extends the benchmark to the Java ecosystem, creating a multilingual benchmark. Similarly, the latest work, SWE-bench Multimodal (Yang et al., 2024b) offers a multimodal upgrade to the benchmark, focusing on visual JavaScript problems. Given Python's increasing role in fields like data science, machine learning, and visualization, where visual data is crucial, we construct Visual SWE-bench focusing on visual issues in Python. By incorporating real-world visual issues, Visual SWE-bench encourages researchers to leverage visual data in solving complex software challenges."}, {"title": "6 Conclusion", "content": "We propose CODEV, an approach that leverages visual data to resolve issues automatically. It processes visual data and provides LLMs with valuable information that enhances their ability to resolve issues. To evaluate CODEV, we construct a benchmark for visual issue resolving, namely Visual SWE-bench. Through extensive experiments, we demonstrate the effectiveness of CODEV and find that it maintains robust performance across VLMs with varying model sizes. Additionally, through case studies, we analyze the function of each component of CODEV, offering insights on leveraging visual data to resolve issues."}, {"title": "Limitations", "content": "Although this study offers valuable insights into leveraging visual data to resolve GitHub issues, several limitations should be acknowledged:\n\u2022 Due to the randomness in the responses generated by LLMs, there is a potential threat to the experimental results. Despite repeating each experiment twice to mitigate this, minor fluctuations in results may still occur.\n\u2022 Due to the lack of suitable benchmarks, our experiments are conducted solely on the self-constructed benchmark. However, we conduct comprehensive experiments and analyses to validate our approach, and we hope future research will develop more publicly available benchmarks to further explore this direction.\n\u2022 Due to the high costs of GPT-40 and Claude 3.5 Sonnet, we don't include them in our comparative experiments. Based on our estimates, using these models within the SWE-agent approach under similar experimental conditions would cost thousands of dollars. Nevertheless, we evaluate CODEV using two LLMs and three VLMs, conducting extensive experiments that confirm its effectiveness."}, {"title": "Appendix", "content": null}, {"title": "A Prompts", "content": "Figures 7-10 show the prompts we use for images. The prompts for videos are almost identical, with \"image\" replaced by \"video\" in the text."}, {"title": "A.1 Independent Description", "content": "Figure 7 illustrates the prompt we use to generate independent descriptions, instructing the VLM to provide descriptions based on the image content."}, {"title": "A.2 Contextual Description", "content": "Figure 8 and Figure 9 present the prompts used for generating contextual descriptions. The prompt in Figure 8 instructs the VLM to describe images based on the contextual information, while the prompt in Figure 9 guides the VLM to analyze the function of images."}, {"title": "A.3 Structured Summary", "content": "Figure 10 illustrates the prompt designed for generating a structured summary. It instructs the VLM to produce a summary of the issue based on a referenced format."}, {"title": "B Example", "content": "Figure 11 presents an example where visual data is processed by the VLM, and the resulting information is appended to the original issue."}, {"title": "C Other Experimental Details", "content": "For models like Qwen2-VL and Qwen2.5-Coder-32B, we use vLLM for deployment on servers equipped with four NVIDIA H800 GPUs (each with 80GB of memory). For the DeepSeek-V2.5 model, we utilize the official API service provided by its developers. All experiments are conducted twice to determine the maximum number of instances that can be resolved. When using the Agentless approach, we employ version 1.0."}]}