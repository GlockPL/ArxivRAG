{"title": "Concept-Based Interpretable Reinforcement Learning with Limited to No Human Labels", "authors": ["Zhuorui Ye", "Stephanie Milani", "Geoffrey J. Gordon", "Fei Fang"], "abstract": "Recent advances in reinforcement learning (RL) have predominantly leveraged neural network-based policies for decision-making, yet these models often lack interpretability, posing challenges for stakeholder comprehension and trust. Concept bottleneck models offer an interpretable alternative by integrating human-understandable concepts into neural networks. However, a significant limitation in prior work is the assumption that human annotations for these concepts are readily available during training, necessitating continuous real-time input from human annotators. To overcome this limitation, we introduce a novel training scheme that enables RL algorithms to efficiently learn a concept-based policy by only querying humans to label a small set of data, or in the extreme case, without any human labels. Our algorithm, LICORICE, involves three main contributions: interleaving concept learning and RL training, using a concept ensembles to actively select informative data points for labeling, and decorrelating the concept data with a simple strategy. We show how LICORICE reduces manual labeling efforts to to 500 or fewer concept labels in three environments. Finally, we present an initial study to explore how we can use powerful vision-language models to infer concepts from raw visual inputs without explicit labels at minimal cost to performance.", "sections": [{"title": "1 Introduction", "content": "In reinforcement learning (RL), agents are tasked with learning a policy, a rule that makes sequential, reactive decisions in complex environments. In recent RL work, agents typically represent the policy as a neural network, as such representations tend to lead to high performance [17]. However, this choice can come at a cost: such policies are challenging for stakeholders to interpret - particularly when the inputs to the network are also complex, such as high-dimensional sensor data. This opacity can become a significant hurdle, especially in applications where understanding the rationale behind decisions is critical, such as healthcare [32] or finance [15]. In such applications, decisions can have significant consequences, so it is essential for stakeholders to fully grasp the reasoning behind actions in order to confidently adopt or collaborate on a policy.\nIn this work, we aim to address this challenge of requiring frequent human interventions during the training of interpretable policies using RL. We propose LICORICE (Label-efficient Interpretable Concept-based ReInforCEment learning), a novel training paradigm consisting of three main contributions. First, LICORICE interleaves concept learning and RL training: it alternately freezes the network layers corresponding to either the concept learning part or the decision-making part. We believe this scheme improves our ability to learn from limited labeled data by reducing interference between the learning tasks. Additionally, concept learning uses data that is more on-policy, which means it is more relevant and useful for learning accurate concepts that directly impact the decision-making process. Second, LICORICE utilizes concept ensembles to actively select the most informative data points for labeling. By focusing on samples that are predicted to provide the most valuable information for model improvement, this technique substantially reduces the number of labels needed to achieve both high performance and high concept accuracy. Third, LICORICE includes a strategy to decorrelate the concept data collected under the current policy. By generating a diverse set of training data, this approach ensures the data remains unbiased and representative of various scenarios. We demonstrate how these changes yield both higher concept accuracy and higher reward while requiring fewer queries on three environments with image input, including an image input version of CartPole and two Minigrid environments.\nGiven these results, we ask whether we can further reduce the reliance on manual concept labeling by leveraging the potential of vision-language models (VLMs). This capability is important in scenarios where manual concept annotation is impractical. We present an initial exploration of whether VLMs can further reduce the concept annotation burden. We find that VLMs can indeed serve as concept annotators for some but not all of the above environments. In these cases, the resulting policies can achieve 83 to 93% of the optimal performance."}, {"title": "2 Preliminaries", "content": "In RL, an agent learns to make decisions by interacting with an environment [27]. The environment is commonly modeled as a Markov decision process [21], consisting of the following components: a set of states $\\mathcal{S}$, a set of actions $\\mathcal{A}$, a state transition function $T : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0,1]$ that indicates the probability of transitioning from one state to another given an action, a reward function $R : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}$ that assigns a reward for each state-action-state transition, and a discount factor $\\gamma \\in [0, 1]$ that determines the present value of future rewards. The agent learns a policy $\\pi: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$, which maps states to actions with the aim of maximizing the expected cumulative discounted reward. We evaluate a policy via its value function, which is defined as $V^{\\pi}(s) = \\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^{k}r_{t+k+1} | s_{0} = s], \\forall s \\in \\mathcal{S}$. The ultimate aim in RL is to determine the optimal policy, $\\pi^{*}$. To do so, the agent iteratively refines its policy based on feedback from the environment."}, {"title": "2.2 Concept Bottleneck Models", "content": "Concept-based explanations have emerged as a common paradigm in explainable AI [20]. They explain a model's decision-making process through human-interpretable attributes and abstractions. Concept bottleneck models [13] are an example of concept-based explanations: they learn a mapping from samples $x \\in \\mathcal{X}$ to labels $y \\in \\mathcal{Y}$ through two functions, $g$ and $f$. The concept encoder function $g : \\mathcal{X} \\rightarrow \\mathcal{C}$ maps samples from the input space to an intermediate space of human-interpretable concepts. The label predictor function $f : \\mathcal{C} \\rightarrow \\mathcal{Y}$ maps samples from the concept space to a downstream task space, such as labels for supervised classification. As a result, the prediction $\\hat{y} = f(g(x))$ relies on the input $x$ entirely through the bottleneck $\\hat{c} = g(x)$.\nTraining these models requires a dataset of $\\mathcal{X} \\times \\mathcal{C} \\times \\mathcal{Y}$, in which each sample consists of input features $x$, a ground truth concept vector $c \\in \\{0,1\\}^{k}$, and a task label $y$. The functions $f$ and $g$ are parameterized by neural networks. These models are trained independently, jointly, or sequentially. The independent training method trains $f$ and $g$ independently. Critically, $f$ is trained using the true $c$, but at test time it takes $\\hat{g}(x)$ as input. If directly applied to the RL setting, this approach would require continuous access to ground-truth concepts for training $f$, which we would like to avoid. In joint training, the model minimizes the weighted sum of the concept losses with the task performance loss. This paradigm can be problematic, as the losses can interfere with each other, and it may require careful tuning to find the right balance between the two losses. In sequential training, the model first learns $g$. It then uses the concept predictions $\\hat{g}(x)$ to learn $f(\\hat{g}(x))$. As we will later show, this setup is problematic for RL, as concepts may only emerge after the policy is sufficiently performant."}, {"title": "3 LICORICE", "content": "We now describe LICORICE, our algorithm for interactively querying for concept labels under a limited concept label budget during RL training. In Section 3.1, we describe the architecture of the concept bottleneck policy. We then detail our iterative training process (Section 3.2) and explain our active learning approach for choosing informative training points (Section 3.3). Finally, in Section 3.4, we detail how we use VLMs as a substitute for human concept labeling."}, {"title": "3.1 Concept Bottleneck Policies", "content": "We insert a concept bottleneck layer into the policy network, such that $\\pi$ maps from states $\\mathcal{S}$ to concepts $\\mathcal{C}$ to actions $\\mathcal{A}$, $\\pi = f(g(s))$. These concepts serve as intermediaries in the policy network, which subsequently maps the concept vector to a distribution over possible actions. This setup allows the policy to base its decisions on understandable and meaningful features. We describe additional modifications necessary to accommodate the concept bottleneck in Appendix A.2. As a result, we can use any RL algorithm as long as we include an additional loss function for concept prediction.\nLoss Function We now describe how we use two loss functions for training in our iterative training scheme, described in more detail in Section 3.2. Because we employ an iterative training scheme, we disentangle the two loss functions - the concept prediction loss and the standard RL loss - to prevent interference between them. In this way, the concept prediction loss $\\mathcal{L}_{C}$ only affects $g$, the part of the model responsible for predicting concepts, and the standard RL loss $\\mathcal{L}_{RL}$ only affects $f$. For training $g$, we employ two types of loss functions depending on the nature of the concepts (MSE for continous concepts; cross-entropy loss for categorical concepts). If the problem requires mixed-type concepts, we could discretize the continuous attributes, converting them into categorical forms suitable for classification. This approach ensures that our method accommodates a diverse range of concept types."}, {"title": "3.2 Training Process", "content": "We now present LICORICE, our novel algorithm for optimizing concept-based models for RL within a fixed concept query budget. Our iterative training approach extends the sequential bottleneck training paradigm by incorporating multiple phases. Intuitively, this strategy can be advantageous for obtaining accurate on-policy concept estimates. To collect candidate training data for $g$, we employ our current policy for rollouts. To minimize data point correlation and collect a diverse range of data, we use a decorrelation strategy. To select more informative points to query for concept labels, we use a concept ensemble to calculate disagreement.\nMore specifically, LICORICE (Algorithm 1) proceeds in three main stages within each iteration $m \\in \\mathcal{M}$: data collection, concept ensemble training for data selection, and concept bottleneck policy training. First, in the data collection stage (lines 4 to 8), we collect a dataset of unlabeled concept data $\\mathcal{U}_{m}$ by rolling out our current policy $\\pi_{m}$. Crucially, during this step, we decorrelate the data to ensure the data points are diverse (line 6). Specifically, we set an acceptance probability $p$ for adding the data point to the dataset, as they are generated by the temporally-correlated RL policy. This prepares a diverse dataset for the next stage, with the aim of promoting the training of a more performant concept ensemble for data selection and, eventually, a more accurate concept bottleneck portion of the policy.\nThe second stage is concept ensemble training for data selection (lines 9 to 16). In this stage, we train the concept ensemble \u2014 consisting of $N$ independent concept models \u2014 from scratch on the dataset of training points $\\mathcal{D}_{train}$ that has been aggregated over all iterations (line 10). We use this ensemble to calculate the disagreement-based acquisition function, which evaluates whether each candidate in our unlabeled dataset $\\mathcal{U}_{m}$ ought to receive a ground-truth concept label (line 11). This function targets samples where prediction disagreement is highest, detailed in Section 3.3. We then query for $B_{m}$ ground-truth concept labels (line 13) to prepare us for the next stage.\nWe are now prepared for the third stage: concept bottleneck policy training (lines 17 to 19). We aggregate the concept datasets from the second stage with data from previous iterations, and continue training the concept network $g$ on $\\mathcal{D}_{train}$ (line 18). After early stopping dictates that we stop $g$ training, we freeze $g$ and train $f$ using any standard RL algorithm for example, PPO to obtain the policy for collecting data in the next iteration (line 19). With that, we are prepared to start the process again from the first stage."}, {"title": "3.3 Active Concept Learning", "content": "We propose to leverage an ensemble of concept models to calculate the disagreement-based acquisition function for actively querying for concept labels. To quantify the concept disagreement of a state $U(s)$, we use two different formulations, depending on the concept learning task.\nClassification For concept classification, we use a query-by-committee [24] approach. When the models produce diverse predictions, it indicates that the instance is difficult and more information would be particularly valuable. Conversely, if all models agree, the instance is likely already well understood, and additional labels would be less beneficial. More specifically, we prioritize points with a high proportion of predicted class labels that are not the modal class prediction (also called the variation ratio [3]). This is given by $U(s) = 1 - \\frac{1}{N} \\max_{c \\in \\mathcal{C}} \\sum_{i=1}^{N} [\\hat{g}_{i}(s) = c]$, where $\\hat{g}_{i}(s)$ is the concept prediction of the i-th model on state $s$.\nRegression For concept regression, we observe that we can instead directly use variance as a measure of disagreement. Specifically, the concept disagreement of a state is quantified by the unbiased estimation of variance $U(s) = \\sigma^{2}(s)$ of the predictions across the concept models, due to Bessel's correction, defined as: $U(s) = \\sigma^{2}(s) = \\frac{1}{N-1} \\sum_{i=1}^{N}(\\hat{g}_{i}(s) - \\mu(s))^{2}$, where $\\mu(s) = \\frac{1}{N} \\sum_{i=1}^{N} \\hat{g}_{i}(s)$ is the mean prediction of the N concept models."}, {"title": "3.4 Using Vision-Language Models for Concept Labeling", "content": "Equipped with our overall algorithm, we now seek to further reduce human annotation burden. To do so, we turn to VLMs due to their remarkable performance in understanding and generating human-like descriptions of visual content [22]. Within the pipeline of LICORICE, we keep all aspects of our algorithm the same but use the VLM as the concept annotator in the training loop. By doing so, we effectively decrease the human annotation effort to zero, albeit with some labeling inaccuracy introduced.\nIn our experiments, we use GPT-40 [1], a closed-source model that is possibly the most capable vision-language model in the world at the time of writing the paper. During the training loop of LICORICE, we query GPT-40 each time the algorithm requires a concept label (line 13 in Algorithm 1). As we are using a pre-specified concept set, we design prompts with detailed general descriptions of the scene layout and definitions of all concepts, in a similar fashion to giving labeling instructions to real humans. We then prompt the GPT-40 to obtain the generated labels. In environments where continuous concept values are required, we ask GPT-40 to give as accurate estimates as possible; in environments with concepts that are discrete and more intuitive to label, we provide clear instructions of how to read the concept numbers according to the input image. More details about our prompts can be found in Appendix A.3."}, {"title": "4 Experiments", "content": "In our experiments, we investigate the following questions:\nRQ 1 Does LICORICE enable both high concept accuracy and high environment reward?\nRQ 2 To what extent does LICORICE enable a reduction in human labeling effort without substantially impacting performance?\nRQ 3 Can LICORICE be used alongside powerful vision-language models to further alleviate the labeling burden?"}, {"title": "4.1 Environments", "content": "We evaluate our approach on three environments. For each environment, we define an interpretable concept set, where we define concepts to describe properties of objects of interest.\nPixelCartPole In PixelCartPole-v1 [31], the states are images, and the concepts are the original continuous features in the standard CartPole environment: the cart position, the cart velocity, the pole angle, and the pole angular velocity. To obtain the mapping from images to concepts, we use a fixed window of the four most recent images for the temporal concepts, such as the cart velocity. We also include the last action as input to ensure that the information is sufficient to infer concept values. This domain is deceptively difficult due to the temporal concepts.\nDoorKey In DoorKey-7x7 [5], the agent operates in a 5x5 grid to pick up an item to unlock the door, then reach the green goal square. The states are fully observable images. We define 6 groups of concepts with 12 concept variables in total: 1) the agent's position, 2) the agent's direction, 3) the key's position or a fixed position (0, 0) outside the grid if the key has been picked up, 4) the door's position, 5) whether the door is open, and 6) whether the agent can move along each of four directions, i.e., the corresponding cell is empty. Each group contains one or more concept variables. For example, position groups contain x and y coordinates.\nDynamicObstacles In DynamicObstacles-5x5 [5], the agent operates in a 3x3 grid to avoid two moving obstacles while navigating to the goal. Colliding with the dynamic obstacles yields a large penalty, so the agent must correctly learn concepts to safely reach the goal. The states are fully observable images. We define 5 groups of concepts with 11 concept variables in total: 1) the agent's position, 2) the agent's direction, 3) the first obstacle's position, 4) the second obstacle's position, and 5) whether the agent can move along each of four directions, i.e., the corresponding cell is empty. Each group contains one or more concept variables. For example, position groups contain x and y coordinates."}, {"title": "4.2 Experiment Details", "content": "In each experiment, we run each algorithm 5 times, each with a random seed. More implementation details and hyperparameters are in Appendix A.2.\nArchitecture For the backbone model and algorithm, we use PPO (implementation provided by Stable Baselines 3 [23]). We preserve most of the network architecture, but we add a concept layer in the policy network. Specifically, after passing the image through the feature extractor, which consists of 3 CNN layers, we introduce a linear layer to map to the concept layer and obtain predicted concept values. Each predicted concept is represented as a real value for the regression case and one categorical value derived from a classification head for the classification case. This portion of the network corresponds to $g$. For $f$, we utilize an MLP extractor with 2 fully connected layers, each with 64 neurons and a Tanh activation function. We share the default CNN feature extractor between policy and value functions, as we found this to be beneficial in preliminary experiments.\nConcept Representation We model concept learning for PixelCartPole-v1 as a regression problem (minimizing mean squared error) as the concepts are real-valued, and concept learning for DoorKey-7x7 and DynamicObstacles-5x5 as classification problems as the concepts are categorical. The observation images have resolutions of (240, 160) for PixelCartPole-v1, and (160, 160) for DoorKey-7x7 and DynamicObstacles-5x5. These dimensions maintain a short side of 160 pixels while preserving the aspect ratio of the original rendered images, as we observe lower resolutions may impact performance.\nPerformance Metrics To obtain an upper bound on the reward for each environment, we directly use ground-truth concept labels to learn a policy, leading to a reward of 500, 0.97, and 0.91 for the three environments respectively. In the following sections, we report the reward as a ratio of this upper bound. Percentages (or ratios) make sense here since the minimum reasonable reward is 0 in all environments: in PixelCartPole-v1 and DoorKey-7x7, all rewards are nonnegative; in DynamicObstacles-5x5, a random policy would have negative reward due to collisions, but the agent can ensure nonnegative reward by simply staying in place. We additionally report the concept error (MSE for regression; 1 - accuracy for classification). All the reported numbers are calculated during the testing stage, where we evaluate the models on 100 episodes."}, {"title": "4.3 Results", "content": "We first validate that LICORICE can achieve high reward in all test environments while accurately identifying concepts. We first compare against baselines with a fixed budget of $B = [500, 300, 300]$"}, {"title": "4.3.1 Balancing Concept Performance and Environment Reward", "content": "queries for PixelCartPole-v1, DoorKey-7x7, and DynamicObstacles-5x5, respectively, as depicted in the first section of Table 1. We choose these budgets by starting from 500 and then decreasing by units of 100 until we find that LICORICE can no longer achieve 99% of the reward upper bound if we further decrease the budget. We treat the minimum budget under which LICORICE can achieve 99% of the reward upper bound as the maximum budget considered for that environment.\nComparison with Budget-Constrained Baselines To our knowledge, no previous algorithms exist that seek to minimize the number of train-time labels for concept-based RL training, so we implement our own baselines. In Sequential-Q, the agent spends all of $B$ queries on the first $B$ states it encounters during the initial policy rollout. In Disagreement-Q, the agent similarly spends its budget at the beginning of its learning process; however, it uses active learning with the same acquisition function as LICORICE to strategically choose the training data. In Random-Q, the agent receives $B$ concept labels at random points in the training process using a probability to decide whether to query for a concept for each state. We show the results in the first section of Table 1. In the three environments, LICORICE performs similarly to or better than all baselines in terms of both reward and concept error. The main performance differences can be seen for PixelCartPole-v1 and DoorKey-7x7, where LICORICE achieves 100% and 99% of the optimal reward, respectively, while the second best algorithm achieves 36% and 89%, respectively. Random-Q, Disagreement-Q, and LICORICE perform similarly on DynamicObstacles-5x5, indicating that this environment is relatively simple and may not benefit from a more advanced strategy. Its simplicity may be due to the lack of concept distribution shift that is present in the other two environments. For example, in PixelCartPole-v1, the policy must be further refined to improve its estimate of the more on-policy concepts. In the initial training stages, when the policy is more random, the agent needs to estimate a wide range of possible values for the pole angle, as the pole starts upright and frequently falls. As the agent's policy improves, it requires more precise estimates of the pole angle primarily around the vertical position.\nComparison with Budget-Unconstrained Baseline To better understand the performance of our approach, we compare it against existing work that is not constrained by concept label budgets. Specifically, we implement Concept Policy Model (CPM) from previous work in multi-agent RL [33] but for the single-agent setting. This approach jointly trains the concept bottleneck and the policy, assuming unlimited access to concept labels. It represents an upper bound on concept accuracy, as the agent receives continuous concept feedback throughout learning. Surprisingly, LICORICE outperforms CPM in PixelCartPole-v1and has similar performance to CPM in the other two environments in this unfair comparison. We emphasize that CPM is given an unlimited budget, and in fact, it uses over 1M concept labels for each environment, whereas LICORICE uses 500 or fewer. We therefore answer RQ 1 affirmatively: compared to baselines, LICORICE demonstrates both high concept accuracy (low concept error) and high reward on our three test environments, all while using substantially fewer concept labels than existing algorithms."}, {"title": "4.3.2 Budget Allocation Effectiveness", "content": "To understand the extent to which LICORICE enables a reduction in human labeling effort without substantially impacting performance, we conduct experiments across three different environments with varying concept labeling budgets. We start with the baseline budget $B$ from our previous experiments and test two additional budgets, decreasing by steps of 100. The only component of our algorithm that we vary is the number of iterations $M$. For each budget and environment, we report a single performance value that maximizes the sum of the relative reward and inverse concept error, both scaled to [0, 1]. The results of this experiment are shown in Table 2.\nOur findings reveal that the impact of budget reductions (human labeling effort) varies across environments. In general, we see an increase in concept error and a decrease in reward for all environments with decreasing budget. However, both DynamicObstacles-5x5 and DoorKey-7x7 show more resilience to budget reductions, while PixelCartPole-v1 exhibits higher sensitivity. Specifically, we find that even with $B = 100$ for DynamicObstacles-5x5, LICORICE can still achieve 96% of the optimal reward. With $B = 200$, LICORICE can still achieve 93% of the optimal reward on DoorKey-7x7. In contrast, we see large decreases in reward from the budget reductions for PixelCartPole-v1. We therefore provide a more nuanced answer to RQ 2: LICORICE can indeed enable reductions in human labeling effort without substantially impacting performance, but the extent of this reduction is environment-dependent. In simpler environments, human effort can be dramatically reduced while maintaining near-optimal performance. However, in more complex environments like PixelCartPole-v1, the trade-off between reduced human effort and maintained performance becomes more pronounced. This finding aligns with the intuition that different environments have varying levels of complexity and would require different levels of human input to accurately capture all relevant concepts and their relationships."}, {"title": "4.3.3 Integration with Vision-Language Models", "content": "We now seek to answer the question of whether VLMs can successfully provide concept labels in lieu of a human annotator within our LICORICE framework. For consistency, we use the same hyperparameters for LICORICE as in Section 4.3.2. Because using VLMs incurs costs and users requiring interpretable policies for their environments may still face budget constraints, we operate within the same budget-constrained setting as described in Section 4.3.2. Here, the reduction in the number of \"human\" labels required translates to cost savings.\nWe show the results in Table 3. We find that GPT-40 can indeed serve as an annotator for some but not all environments. In DoorKey-7x7 and DynamicObstacles-5x5, LICORICE with GPT-40 labels achieves 83% and 88% of the optimal reward, respectively. The concept error rate is comparable to GPT-40's labeling error, both evaluated on the same rollout observations, with sampling applied to the latter due to cost constraints. Additionally, the concept error gap between our trained concept model and GPT-40 almost diminishes with"}, {"title": "4.3.4 Ablation Study", "content": "Given the previous results, we now conduct ablations to confirm the effectiveness of our three main contributions: iterative training, decorrelation, and active learning. LICORICE-IT corresponds to LICORICE with only one iteration, LICORICE-DE corresponds to LICORICE without decorrelation, and LICORICE-AC corresponds to LICORICE without active learning (instead, it uses the entire unlabeled dataset for querying). We show the learning curves for PixelCartPole-v1 in Figure 1; the full set of curves is in Appendix B.\nTable 4 depicts the results of our ablation study on all environments. The bottom row corresponds to the upper bound on performance by LICORICE, with all components included. All of our contributions are critical to achieving both high reward and low concept error. However, the component that most contributes to the reward or concept performance differs depending on the environment. For example, compared with LICORICE, LICORICE-IT exhibits the largest reward gap for PixelCartPole-v1; however, LICORICE-AC yields the largest reward gap for DynamicObstacles-5x5, and LICORICE-DE yields the largest gap for DoorKey-7x7. We suspect that this difference is because the concepts in DynamicObstacles-5x5 are simple enough such that one iteration is sufficient for learning, meaning that the largest gains can be made by using active learning. In contrast, PixelCartPole-v1 requires further refinement of the policy to better estimate on-distribution concept values, so the largest gains can be made by leveraging multiple iterations."}, {"title": "5 Related Work", "content": "Interpretable Reinforcement Learning Interpretable RL has gained significant attention in recent years [10]. One prominent approach uses rule-based methods such as decision trees [26, 29], logic [8], and programs [30, 19] - to represent policies. These works either assume that the state is already interpretable or that the policy is pre-specified. Unlike prior work, our method involves learning the interpretable representation (through concept training) for policy learning.\nConcept Learning for Reinforcement Learning Inspired by successes in the supervised setting [6, 25, 34], concept-based explanations have recently been incorporated into RL. One approach [7] learns a joint embedding model between state-action pairs and concept-based explanations to expedite learning via reward shaping. Unlike our work, their policy is not a strict function of the concepts, allowing our techniques to be combined to provide both concept-based explanations and a concept-based interpretable policy. Another example, CPM [33], is a multi-agent RL concept architecture that focuses on trade-offs between interpretability and accuracy. They assume that concept labels are available continuously during training. As we have shown, this approach uses over 1M concept labels in our test environments, whereas our approach reduces the need for continuous human intervention, requiring only 500 or fewer concept labels to achieve similar or better performance in single-agent environments.\nLearning Concepts with Human Feedback Another line of work explores how to best leverage human concept labels but not in the RL setting, and does not focus on reducing the labeling burden. In contrast, our approach aims to reduce the concept labeling burden during training. One work [14] instead has users label additional information about the relevance of certain feature dimensions to the concept label. Another work [4] develops an intervention policy at prediction time to choose which concepts to request a label for with the goal of improving the final prediction. Future work could explore using these techniques alongside our method."}, {"title": "6 Discussion and Conclusion", "content": "In this work, we proposed LICORICE, a novel RL algorithm that addresses the critical issue of model interpretability while minimizing the reliance on continuous human annotation. We introduced a training scheme that enables RL algorithms to learn concepts more efficiently from little to no labeled concept data. Our approach interleaves concept learning and RL training, uses an ensemble-based active learning technique to select informative data points for labeling, and uses a simple sampling strategy to better decorrelate the concept data. We demonstrated how this approach reduces manual labeling effort. Finally, we conducted initial experiments to demonstrate how we can leverage powerful VLMs to infer concepts from raw visual inputs without explicit labels in some environments. There are broader societal impacts of our work that must be considered. These include both the impacts of using VLMs in real-world applications, as well as considerations around interpretability more generally. For a more detailed discussion, please refer to Appendix C.\nLimitations and Future Work Although VLMs can be successfully used for automatic labeling of some concepts, there are still hallucination issues [2] and other failure cases, such as providing inaccurate counts. We believe that future work that seeks to improve general VLM capabilities and mitigate hallucinations would also help overcome this limitation. Another exciting direction is the work in human-AI complementarity and learning-to-defer algorithms [18] to train an additional classifier for deferring concept labeling to a person when the chance of making an error is high."}, {"title": "A Experimental Result Reproducibility", "content": "In this section, we provided detailed descriptions to achieve reproducibility."}, {"title": "A.1 Concepts Definitions", "content": "Table 5 provides more details on the concepts used in each environment, categorizing them by their names, types, and value ranges. For the PixelCartPole-v1 environment, all concepts such as Cart Position, Cart Velocity, Pole Angle, and Pole Angular Velocity are continuous. In contrast, the DoorKey-7x7 environment features discrete concepts like Agent Position (x and y), Key Position (x and y), and Door Open status, each with specific value ranges. Similarly, the DynamicObstacles-5x5 environment lists discrete concepts, including Agent and Obstacle positions, with corresponding value ranges.\nWe also visualize the start configurations for DoorKey-7x7 in Figure 2 to illustrate the importance of concept definitions. As shown, the concepts must be defined in such a way to allow the agent to generalize to all possible environment configurations."}, {"title": "A.2 LICORICE Details", "content": "In this section, we provide additional implementation details for LICORICE. The model architecture has been mostly described in the main text and we state all additional details here. The number of neurons in the concept layer is exactly the number of concepts. For continuous concept values, we directly use a linear layer to map from features to concept values. For discrete concept values, since different concepts have different numbers of categories, we create one linear classification head for each single concept, and to predict the final action, we calculate the class with the largest predicted probability for each concept.\nValue-Based Methods If we were to use a value-based method as the RL backbone, we would need to make the following changes. First, we would need to modify V(s, a) or Q(s, a) to include a concept bottleneck, such that $Q(s, a) = f(g(s))$. Then, we can conduct interleaved training in a similar way to LICORICE.\nFeature Extractor If we use the actor-critic paradigm, we propose to share a feature extractor between the policy and value networks, shown in Figure 3. Intuitively, this choice can offer several advantages compared with using image or predicted concepts as input for both networks. Sharing a feature extractor enables both networks to benefit from a common, rich representation of the input data, reducing the number of parameters to be trained. More importantly, it balances the updates of the policy and value networks. In experiments, we observed that directly using the raw image as input for both networks complicated policy learning. Conversely, relying solely on predicted concepts for the value network may limit its accuracy in value estimation, particularly if the concepts do not capture all the nuances relevant to the value predictions."}, {"title": "A.3 VLM Details", "content": "We detail our prompts for each environment here."}, {"title": "PixelCartPole-v1", "content": "Prompt: Here are the past 4 rendered frames from the CartPole environment. Please use these images to estimate the following values in the latest frame (the last one):\n\u2022 Cart Position, within the range (-2.4, 2.4)\n\u2022 Cart Velocity\n\u2022 Pole Angle, within the range (-0.2095, 0.2095)\n\u2022 Pole Angular Velocity\nAdditionally, please note that the last action taken was [last action].\nPlease carefully determine the following values and give concise answers one by one. Make sure to return an estimated value for each parameter, even if the task may look challenging.\nFollow the reporting format:\n\u2022 Cart Position: estimated_value\n\u2022 Cart Velocity: estimated_value\n\u2022 Pole Angle: estimated_value\n\u2022 Pole Angular Velocity: estimated_value"}]}