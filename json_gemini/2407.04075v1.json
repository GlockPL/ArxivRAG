[{"title": "Sparsest Models Elude Pruning: An Expos\u00e9 of Pruning's Current Capabilities", "authors": ["Stephen Zhang", "Vardan Papyan"], "abstract": "Pruning has emerged as a promising approach for compressing large-scale models, yet its effectiveness in recovering the sparsest of models has not yet been explored. We conducted an extensive series of 485,838 experiments, applying a range of state-of-the-art pruning algorithms to a synthetic dataset we created, named the Cubist Spiral. Our findings reveal a significant gap in performance compared to ideal sparse networks, which we identified through a novel combinatorial search algorithm. We attribute this performance gap to current pruning algorithms' poor behaviour under overparameterization, their tendency to induce disconnected paths throughout the network, and their propensity to get stuck at suboptimal solutions, even when given the optimal width and initialization. This gap is concerning, given the simplicity of the network architectures and datasets used in our study. We hope that our research encourages further investigation into new pruning techniques that strive for true network sparsity.", "sections": [{"title": "1. Introduction", "content": "The burgeoning complexity of state-of-the-art deep learning models has made their training and deployment prohibitively expensive. To counteract this increasing demand for resources, model compression has become increasingly important in optimizing the computational efficiency of these networks. Among these techniques, a popular and proven option is network pruning which induces sparsity in the model parameters (Hoefler et al., 2021).\nWhilst pruning is effective, it is difficult to assess how close current pruning algorithms are to obtaining the sparsest of models due to the complexity of the datasets and models used. For the same reason, analyzing and interpreting pruning's effects on trained models has been challenging which has allowed for potential shortcomings to go unnoticed.\nThis paper aims to scrutinize how closely various pruning algorithms approach the ideal, sparsest network, defined as the model with the fewest nonzero parameters that can achieve a specific target accuracy, and reveal the true efficacy of current pruning techniques."}, {"title": "1.1. Method Overview", "content": "To achieve our goals, we engineer the following tools that will be the basis for our analysis:\nCubist Spiral A synthetic dataset named the Cubist Spiral, depicted in Figure 2b. The simplicity inherent in the dataset leads to interpretable sparse models that are amenable to visualization and analysis.\nCombinatorial Search A novel combinatorial search algorithm that searches across model sparsity masks for an optimal and maximally sparse model. Diverging from existing naive benchmarks such as random pruning, where the pruned weights are selected randomly, our algorithm leverages structured sparsity to perform an efficient exploration across sparsity masks for the model.\nSparse Model Visualization A visualization tool, similar to TensorFlow Playground (Smilkov et al., 2017), designed for inspecting sparse models by graphically representing their non-zero subnetwork. An example of a model visualization is shown in Figure 1."}, {"title": "1.2. Contributions", "content": "Through an empirical study (code available on GitHub), we uncover the following deficiencies:\nAlgorithms Fail to Get Sparsest Model There exists a disparity between the achievable outcomes and the current capabilities of pruning techniques in terms of recovering a sparsest model.\nOverparameterization Impedes Pruning Unstructured pruning techniques are unable to adequately perform structured pruning resulting in a deterioration of their performance under overparameterization.\nPruning Fails Under Optimal Conditions Pruning is unable to recover the sparsest sparsity masks for the model even when provided with the optimal width and initialization.\nThrough our visualization, we show:\nDisconnected Paths Pruning algorithms are unable to correctly align the parameters of consecutive layers resulting in disconnected paths. This leads to an inflated number of nonzero parameters that are not contributing to the expressivity of the network.\nPruning Algorithms Foregoes Sparsity Pruned networks can be further pruned after training without harming model performance."}, {"title": "2. Background: Pruning Algorithms", "content": "Pruning algorithms are commonly classified into two main categories: structured and unstructured. In unstructured pruning, individual weights are pruned, whereas structured pruning operates at a higher level by pruning entire filters or channels (Wen et al., 2016; Li et al., 2017; Luo et al., 2017).\nBeyond structured and unstructured, pruning algorithms can also be classified into the following three categories based on their pruning strategies."}, {"title": "2.1. Dense to Sparse", "content": "This category encompasses the methods of Optimal Brain Damage by LeCun et al. (1989) and its successor, Optimal Brain Surgeon by Hassibi & Stork (1992); Hassibi et al. (1993), which are seminal works not only in dense-to-sparse pruning but in pruning in general. More recently, magnitude-based pruning approaches have proven to be extremely effective (Han et al., 2015), leading to state-of-the-art methods being developed such as Gradual Magnitude Pruning (GMP) by Zhu & Gupta (2018) and the Lottery Ticket Hypothesis (LTH) by Frankle & Carbin (2018). These techniques typically start with a dense network configuration and implement pruning either progressively during the training process or upon its completion. While they offer resource savings during the inference stage, they do not reduce resource utilization during the training phase."}, {"title": "2.2. Pruning at Initialization", "content": "Representative methods in this category include Gradient Signal Preservation (GraSP) by Wang et al. (2019), Prospect Pruning (ProsPr) by Alizadeh et al. (2021), Single-shot Network Pruning (SNIP) by Lee et al. (2019), Iterative Synaptic Flow Pruning (SynFlow) by Tanaka et al. (2020) and Iter SNIP and FORCE by de Jorge et al. (2021). In contrast to the previous category, these algorithms involve pruning neural networks at the initialization stage, followed by training the already-pruned models. This approach is beneficial as it conserves resources both during the training and inference phases, assuming the initial pruning overhead is negligible."}, {"title": "2.3. Sparse to Sparse", "content": "Sparse Evolutionary Training (SET) by Mocanu et al. (2018) was the pioneer algorithm in this category. Subsequently, several other algorithms have been introduced, such as Deep-R by Bellec et al. (2018), Sparse Networks From Scratch (SNFS) by Dettmers & Zettlemoyer (2019), and Dynamic Sparse Reparameterization (DSR) by Mostafa & Wang (2019). The Rigged Lottery (RigL) by Evci et al. (2020a) has emerged as a state-of-the-art method in this group. Distinguishing itself from other categories, this approach initiates with a sparsely connected neural network and maintains the total number of parameters while dynamically altering the nonzero connections throughout training."}, {"title": "3. Methodology", "content": "The objective of this study is to evaluate the effectiveness of pruning algorithms in identifying the sparsest possible network. Finding it requires a combinatorial search which is only practical for smaller network architectures, due to its complexity.\nWe therefore train four-layer Multilayer Perceptrons (MLPs) with ReLU activation functions, which take as input two coordinates and predicts a class label.\nAll combinatorial search experiments are done on MLPs of width 16. The pruning algorithms, on the other hand, are run on MLPs of varying widths:\n{3, 4, 5, 6, 7, 8, 16, 32, 64, 128, 256},\nto examine the impact of overparameterization on their efficacy."}, {"title": "3.2. Dataset", "content": "The simplicity of the architecture calls for a simple dataset as well. We opt for the classical synthetic spiral dataset, notable for its non-linear separability. To better suit sparse modeling techniques, we have adapted the spiral by straightening its naturally curved edges. This modification gives rise to what we call the Cubist Spiral dataset, a nod to the Cubism art movement that emphasized the use of minimal geometric shapes when depicting objects of interest. The classical spiral and its Cubist counterpart are juxtaposed in Figure 2.\nWe pick 50,000 points spaced evenly along the spiral divided equally between the two classes. This deliberate choice of a large training set stems from our desire to separate any issues related to generalization when evaluating the efficacy of pruning algorithms."}, {"title": "3.3. Combinatorial Search", "content": "The combinatorial search is encapsulated in a function which obtains as input the width of the network D and a desired target accuracy p and returns a list of model sparsity masks for the MLP. The function involves two phases.\nFirst Phase: Structured Sparsity The first phase performs a grid search over the number of neurons in each layer that span over the set {1, 2, ..., D}. We denote the number of neurons in layer l as $d^{[l]}$ with $d^{[0]} = 2$ and $d^{[4]} = 1$. For each neuron configuration, a four-layer MLP is randomly initialized and masked such that only the first $d^{[l-1]}$ columns and $d^{[l]}$ rows in layer $W^{[l]}$ are nonzero. The MLP is then trained and a final accuracy is computed. Given the results from all the trainings, the configuration that achieves the desired target accuracy with the fewest nonzeros is selected. A schematic showing how phase one operates is displayed above in Figure 3.\nSecond Phase: Unstructured Sparsity The second phase iterates over a list of unstructured sparsity masks for each weight matrix.\u00b9 For weight $W^{[l]}$, this list is generated by the function ELIGIBLEMASKS($d^{[l-1]}$, $d^{[l]}$) where $d^{[l-1]}$ and $d^{[l]}$ are determined based on the optimal configuration established in the first phase.\nTo ensure that the combinatorial search is done efficiently, ELIGIBLEMASKS($d^{[l]}$, $d^{[l-1]}$) ensures that each mask for the weight is confined to the rows and columns essential for fulfilling the neuron configuration. Furthermore, each required row and column contains at least one nonzero element. This assumption is grounded in the notion that the neuron configuration, as determined in the first phase of the search, is inherently minimal.\nELIGIBLEMASKS($d^{[l]}$, $d^{[l-1]}$) further optimizes the combinatorial search by eliminating masks that are functionally identical but differ merely by permutations of channels. The symmetry is broken by selecting from all row permutations the specific arrangement that results in a sequentially decreasing count of nonzero elements. If two rows have an equal count of nonzeros, the algorithm converts the binary vector representations of these masks into their decimal equivalents and arranges them in descending order. A schematic for phase two is depicted below in Figure 4.\nThe combinatorial search iterates only over the masks of the weight matrices. As for the biases, we assign a value of zero to the i-th bias entry if and only if the i-th row of the weight matrix in that layer is zero in the mask."}, {"title": "3.4. Selection of Pruning Algorithms", "content": "We benchmark the following pruning algorithms: GMP, LTH, GraSP, SNIP, SynFlow, Iter SNIP, FORCE, ProsPr, and RigL. We depict in Table 1 below each technique's categorization along with the FLOPS required to prune and train an MLP of width 16. We prune multiple models with different budgets of nonzeros for the weights. The budgets are specifically chosen to be centered around the range where the combinatorial search can reconstruct the spiral. Further details are provided in Appendix B and C.\nThe aforementioned pruning techniques do not include bias parameters in the pruning process. To ensure that the comparison to the combinatorial search is fair, entries of the bias are masked based on whether the corresponding column in the succeeding weight matrix is fully pruned, i.e., $b^{[l]}$ is masked if and only if $W^{[l+1]}_{:,i}$ = 0. The bias corresponding to the classifier layer always remains fully dense."}, {"title": "3.5. Initialization Experiments", "content": "The combinatorial search trains models on a large number of model masks, where each trained model starts at a different initialization of the parameters. One might speculate that the success of the combinatorial search could be tied to the initialization rather than the actual mask of the parameters.\nTo study the effect of the parameter initialization on the success of the pruning algorithms, we pick the best initialization from the combinatorial search \u2013 the one that led to the sparsest model for a given target accuracy, p \u2013 and use it to initialize the pruning experiments. If a good initialization is all that is needed for successful pruning, then the pruning algorithms should succeed and be able to match the combinatorial search.\nWe equalize the comparison with the combinatorial search by running another round of the combinatorial search, but this time using the most successful initialization from the first combinatorial search to account for giving the pruned models the initialization. This also serves as a sanity check to verify whether the initialization is advantageous compared to a typical random initialization."}, {"title": "3.6. Optimization", "content": "We train the model parameters for 50 epochs using stochastic gradient descent (SGD) with momentum 0.9 and a batch size of 128. Parameters outside of the determined model mask are constrained to be zero. A weight decay is applied for all experiments and set to 5e-4. For the pruning experiments, learning rates {0.05, 0.1, 0.2} are used while for the combinatorial search, only {0.05, 0.1} are used. We also utilize three learning rate schedulers: constant learning rate, a cosine annealing scheduler, and a decay of 0.1 applied at epochs 15 and 30."}, {"title": "4. Combinatorial Search Results", "content": "Preliminary experiments, which involve running the first phase of the combinatorial search with varying target sparsities, reveal three categories of model performance:\nBelow 95% Accuracy: Models in this group were unable to even approximately reconstruct the spiral."}, {"title": "4.2. Phase Two of the Combinatorial Search", "content": "Phase two of the algorithm provides a total of 25,992 model masks to try for the 95% target accuracy and 266,004,066 model masks for the 99.5% target accuracy. Due to computational limits, we check only a subset of size 63,208 of the possible model masks for the 99.5% target accuracy. Details on the subset are given in Appendix D"}, {"title": "4.3. Analysis of Sparse Models", "content": "Several observation can be deduced from the minimal models presented in Figures 6 and 7:\nSelective Connectivity Both models exhibit selective connectivity and do not form connections with every neuron in the preceding layer. This suggests a more refined and efficient architectural design of the network.\nEdges to Spiral The initial layers predominantly capture the spiral's edges. As we move deeper into the network, these edges are progressively integrated, forming polygonal shapes. In the last layers, these polygons are subtracted from one another to, roughly, reconstruct the spiral structure.\nSuboptimal Sparsity The bottom neurons in the second and third layer of the model in Figure 6, and the bottom neuron in the third layer of the model in Figure 7, can be pruned to obtain a sparser model without significantly impacting the prediction of either model."}, {"title": "5. Pruning Algorithms Versus Combinatorial Search", "content": "Given the results of the combinatorial search, we run the pruning experiments with the following budgets of nonzero weights:\n{15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 30, 33, 37, 40, 44, 50, 53, 55, 57, 60, 65}.\nHence, although the models are sparse, they can be further pruned. We comment on this further in Sections 5.2 and 7.2."}, {"title": "5.1. Suboptimality of Pruning Algorithms", "content": "All pruning techniques suffer greatly in the 95% \u2013 99.5% regime relative to the combinatorial search. In particular, the combinatorial search achieves above 95% accuracy with just 30 nonzeros. The second best is the dense-to-sparse method LTH requiring 44 nonzeros to reach the accuracy threshold. The sparse-to-sparse method RigL reaches the threshold at 52 nonzeros, while the sparsest pruning at initialization method that reaches the accuracy threshold is SynFlow with 51 nonzeros.\nFor accuracies above 99.5%, we again see a gap between pruning and the combinatorial search. The combinatorial search obtains above 99.5% accuracy with 45 nonzeros.\nThe second-best method is GMP, which achieves a similar level of accuracy with 61 nonzeros, while RigL reaches this accuracy threshold with 84 nonzeros. The only pruning-at-initialization method that could reach the threshold within the tested nonzero budgets is Iter SNIP with 78 nonzeros and 99.69% accuracy."}, {"title": "5.2. Visualization of Pruned Models", "content": "To gain further insight as to why pruned models are struggling to match the combinatorial search, we visualize failed models generated by various pruning methods in Figures 9 and 10 below and comment on some key observations. Further examples are provided in Appendix H.\nDisconnected Paths Current pruning algorithms are unable to properly align the weights between consecutive lay-ers leading to disconnected paths. This occurs when there is a path in the network that is either disconnected from the model input or output. Nonzero parameters in a disconnected path do not contribute to the expressiveness of the network and inflate the number of nonzeros in the model.\nSuboptimal Sparsity Similar to the models found by the combinatorial search, the model depicted in Figure 9 is foregoing a lot of sparsity that could be attained by magnitude pruning the model after training. This in part is due to the sub-optimal nature of current pruning algorithms requiring the final nonzero budget to be determined prior to any pruning or training being done."}, {"title": "6. Impact of Overparameterization on Pruning", "content": "In theory, overparameterization should be beneficial to pruning as it increases the number of combinatorial options for sparsity masks from which to find the optimal mask for a sparse model. Contrary to this belief, Figure 11 below shows the results from the experiments measuring the impact of overparameterization on the success of pruning algorithms."}, {"title": "6.1. Overparameterization Hinders Pruning", "content": "Figure 11 shows that overparameterization harms the performance of most pruning techniques and that at width 256, all pruning algorithms are largely failing. Furthermore, reducing the network width to 3 increases the performance of current pruning algorithms indicating that current unstructured pruning approaches are inadequately performing structured pruning. In Appendix L, we further prove that overparameterization leads to more disconnected paths for pruning methods that utilize a random mask at initialization, like RigL."}, {"title": "6.2. Optimal Width Limitations", "content": "Even when given the optimal width identified by the combinatorial search, the pruning methods are still unable to consistently match the accuracies that were shown to be empirically possible through the combinatorial search. Out of a total of 18,954 experiments, only two instances of pruning were able to match or beat the combinatorial search: SNIP with 29 nonzeros and an accuracy of 95.14% and GMP with 40 nonzeros and an accuracy of 99.68%. Both models were provided with the optimal widths of 3 and 7 respectively."}, {"title": "7. Impact of Initialization", "content": "In this section, we follow the experimental protocol detailed in Section 3.5, using the unmasked initialization of BENCH-995. Analogous experiments for BENCH-95 are included in Appendix A."}, {"title": "7.1. Combinatorial Search Using Optimal Initialization", "content": "Running another round of the combinatorial search\u00b2 but this time training all models from the initialization of BENCH-995 recovers a sparser model that is visualized in Figure 12 below. Due to the neuron configuration found (6,3,2), increasing the MLP width beyond 16 would not lead to the combinatorial search finding sparser solutions. Further explanation can be found in Appendix J."}, {"title": "7.2. Pruning after Training is Insufficient", "content": "Figure 7 shows that pruning BENCH-995 obtained from the first combinatorial search after training will lead to a sparser model with a neuron configuration of (7,3, 2) and 42 nonzeros. However, the second combinatorial search reveals a model that has a neuron configuration of (6, 3, 2) containing less nonzeros, 38, than what would be obtained by pruning BENCH-995. This indicates that magnitude pruning after training is insufficient to find a minimal model."}, {"title": "7.3. Pruning Fails with Optimal Initialization", "content": "Using the BENCH-995 initialization, Figure 13 below captures pruning's inability to recover a minimal sparsity mask for the model despite being given an ideal initialization. We can see that none of the pruning techniques are able to recover the minimal sparsity masks that the combinatorial search is able to find even when provided with the optimal width by masking the rows and columns of each layer down to the largest width of the weight matrices of the sparsest model found by the combinatorial search."}, {"title": "8. Limitations of the Combinatorial Search", "content": "Synthetic Datasets and Small Models The high cost associated with performing the combinatorial search restricts our experiments to only synthetic datasets and small models.\nStill, the shortcomings that are already being exhibited by pruning techniques in such a simplistic task should raise concerns and inquiries into pruning's current efficacy. While it is possible that the deficiencies of pruning algorithms observed do not extend to more complicated datasets and larger models, generally speaking, we would not expect an algorithm that does not work in a simple setting to work in a more complicated one.\nNo Guarantee of Sparsest While our combinatorial search can find a model that is sparser than any of the models found by pruning, there is no guarantee that the combinatorial search finds the optimally sparse model. Rather the combinatorial search only provides a lower bound on the sparsity that is attainable for the four-layer MLP yet is not achieved by existing pruning algorithms \u2013 the sparsest models elude pruning."}, {"title": "9. Related Works", "content": "Sparse Representations and Compressed Sensing This work is predicated on the assumption that pruning algorithms ought to be able to identify the sparsest model. It is natural to question why such an assumption is even feasible. The rationale stems from both empirical and theoretical works in the fields of sparse representations and compressed sensing where it is known that, within the framework of linear models, if the underlying sparse solution is sufficiently sparse, then pruning algorithms will recover it. For further details, refer to (Donoho, 2006; Elad, 2010; Candes & Tao, 2005; Tropp, 2006; 2004) and the cited literature.\nStrong Tickets Sparked by interest in the lottery ticket hypothesis (Frankle & Carbin, 2018), numerous works have shown that, with high probability, there exists a subnetwork that can attain competitive performance within a sufficiently overparameterized randomly initialized network, called a strong lottery ticket (Malach et al., 2020; Ramanujan et al., 2020; Orseau et al., 2020). Our experiments reveal that, while the probability of identifying strong lottery tickets increases with the network's width, the effectiveness of current pruning methods in fact diminishes.\nRandom Pruning In line with our work, previous works have also benchmarked existing pruning techniques with naive pruning methods like random pruning (Liu et al., 2022; Gale et al., 2019). What sets our work apart is that random pruning, much like existing pruning techniques, remains susceptible to disconnected paths. This poses a challenge in achieving the recovery of a maximally sparse model, especially at high levels of overparameterization. Our approach to the combinatorial search guarantees that misalignment between weights is impossible making the search significantly more efficient in finding a minimal model.\nElucidating Pruning Several recent studies have expressed concerns about the current state of pruning, particularly with inconsistent benchmarking. Both Liu et al. (2023) and Blalock et al. (2020) proposed benchmarks for pruning, the former proposing SMC-Bench and the latter proposing ShrinkBench. Frankle et al. (2021) assessed several pruning at initialization techniques and remarked how they all perform similarly and are struggling to prune effectively at initialization. Evci et al. (2020b) showed that networks that were pruned at initialization have poor gradient flow leading to significantly worse generalization. For structured pruning, Liu et al. (2019) observed that the common pipeline of fine-tuning the pruned model is, at best, comparable to just training the model from scratch and encouraged a more careful evaluation of structured pruning. We differentiate ourselves from prior works by comparing pruning against the sparsest of models, enabling us to underscore fundamental issues inherent in current pruning methods.\nPlant 'n' Seek In Fischer & Burkholz (2022), the authors handcraft sparse networks to solve synthetic problems and plant them within a larger randomly initialized network. They find that current pruning techniques are unable to extract the sparse subnetwork from the larger network either at initialization or after training. Our work, on the other hand, does not require handcrafting sparse subnetworks and all training starts from a completely random initialization. This experimental setup is more representative of the standard pruning paradigm, where model sizes might not be large enough for strong tickets to exist at initialization with high probability.\nDisconnected Paths The tendency of pruning techniques to induce disconnected paths has previously been observed in prior works (Frankle et al., 2021; Vysogorets & Kempe, 2023; Pham et al., 2023). Both Frankle et al. (2021) and Vysogorets & Kempe (2023) propose measuring effective sparsity, which accounts for the disconnected paths when assessing sparsity. In Pham et al. (2023), the authors found that the ratio of the number of connected paths to the number of active neurons in the model is crucial for the success of pruning (Node-Path Balancing Principle) and introduced a novel pruning method that maximizes both quantities.\nPruning and Layer-Collapse It has been shown that current pruning techniques can inadvertently prune an entire layer at higher sparsity rates, effectively turning every path in the network into a disconnected one (Hayou et al., 2021). In Lee et al. (2020), the authors showed that an initialization that preserves layerwise dynamical isometry can assist in preventing this while Tanaka et al. (2020) proposed the pruning technique SynFlow as a solution. Figure 11 confirms that SynFlow is more robust to overparameterization compared to other pruning techniques but still fallible. Our work highlights that the problem is currently manifesting itself even at lower sparsity rates through disconnected paths and is more prevalent than just the catastrophic case where an entire layer is pruned."}, {"title": "10. Conclusion", "content": "We provided a comprehensive assessment of state-of-the-art pruning algorithms against the backdrop of ideal sparse networks obtained from a novel combinatorial search. Our findings reveal that current pruning algorithms fail to attain achievable sparsity levels even when given the optimal width and initialization. We associate this discrepancy with unstructured pruning's inadequacy at performing structured pruning, their failure to benefit from overparameterization, and their tendency to induce disconnected paths while also foregoing sparsity. Despite the simplicity of the dataset and network architectures employed in our study, we believe that the issues highlighted in our work are only exacerbated at larger scales and we hope that our methods and findings will be of assistance for future forays into the development of new pruning techniques."}, {"title": "Algorithm 1: Combinatorial Search", "content": "Input: Desired Accuracy, p, and maximal number of channels per layer, D.\nOutput: The set of all possible model masks S.\nFunction COMBINATORIALSEARCH (p, D) :\n// Phase One: Loop through all possible numbers of neurons in each layer.\nN\u2190 {}\n// Two-dimensional input and one-dimensional output.\n$d^{[0]} = 2$, $d^{[4]} = 1$\nfor ($d^{[1]}$, $d^{[2]}$, $d^{[3]}$) \u2208 {1, ..., D}\u00b3 do\n\u03b8\u2190 MLP(depth=4, width=D)\n// Mask layers according to neuron configuration.\nfor l\u2208 {1, 2, 3, 4} do\n$W^{[l]}[:, d^{[l]} : D] = 0$, $W^{[l]}[d^{[l-1]} : D, : ] = 0$, $b^{[l]}[d^{[l]} : D] = 0$\nif ACCURACY(\u03b8) > p then\nN\u2190 N\u222a {($d^{[1]}$, $d^{[2]}$, $d^{[3]}$)}\n// Find successful configuration that minimizes model nonzeros.\n$d^{[1]}, d^{[2]}, d^{[3]}$ \u2190 arg $\\min_{(d^{[1]},d^{[2]},d^{[3]})\u2208N}$ (2 * $d^{[1]} + 1$) + ($d^{[1]}$ * $d^{[2]} + d^{[2]}$) + ($d^{[2]}$ * $d^{[3]} + d^{[3]}$) + ($d^{[3]}$ * 1 + 1)\n// Phase Two: Generate eligible masks for each layer.\nfor l\u2208 {1, 2, 3, 4} do\nsupp[$l$]\u2190 ELIGIBLEMASKS($d^{[l-1]}$, $d^{[l]}$)\nreturn {($s^{[1]}$, $s^{[2]}$, $s^{[3]}$, $s^{[4]}$) | $s^{[1]}$ \u2208 supp[1], $s^{[2]}$ \u2208 supp[2], $s^{[3]}$ \u2208 supp[3], $s^{[4]}$ \u2208 supp[4]}\nFunction ELIGIBLEMASKS ($d^{[in]}$, $d^{[out]}$):\nsupp \u2190 {}\n// Calculate min and max possible nonzeros for each layer.\nmin = max($d^{[in]}$, $d^{[out]}$)\nmax = $d^{[in]}$. $d^{[out]}$\n// Loop through all nonzero counts for the layer's weights.\nfor n \u2208 {min, . . ., max} do\n// Loop over all row-wise nonzero distributions.\nfor ($k_1$,..., $k_{d^{[out]}}$) \u2208 { k \u2208 {1, 2, ..., $d^{[in]}$ }$^{d^{[out]}}$ | $\\sum_{i=1}^{d^{[out]}} k_i  = n$ } do\n// B(k) = {v \u2208 {0,1}$^{d^{[in]}}$ | $\\sum_{i=1}^{d^{[in]}} v_i = k$}\n// Loop over all possible masks for each row in the layer.\nfor $s_1$ \u2208 B($k_1$) \u2227 ... \u2227 $s_{d^{[out]}}$ \u2208 B($k_{d^{[out]}}$) do\nif ELIGIBLE (STACK($s_1$,..., $s_{d^{[out]}}$)) then\nsupp\u2190 supp \u222a {PADWITHZEROS ($s_1$,..., $s_{d^{[out]}}$)}\nreturn supp\nFunction ELIGIBLE (S):\nif S contains zero columns then\nreturn False\n// Ensure non-increasing nonzeros across the rows.\nfor i\u2208 {1,..., Rows(S) \u2013 1} do\nif || $S_{i,:}$||0 > ||$S_{i+1,:}$||0 then\nreturn False\nelse if || $S_{i,:}$||0 = ||$S_{i+1,:}$ ||0 then\nif BINARYTODECIMAL($S_{i,:}$) > BINARYTODECIMAL($S_{i+1,:}$) then\nreturn True"}, {"title": "L. Overparameterization Leads to Disconnected Paths", "content": "Theorem L.1. Consider an L-layer multilayer perceptron with weights $W^{[1", "W^{[L": ""}, "where $W^{[1"]}, {"infinity.\nProof": "For the proof", "satisfying": "n$\\mathcal{A"}, {"w}": "M_{.1"}, {"follows": "n$\\sum_{k=1}^{n_l} \\frac{\\binom{w}{k}}{\\binom{w^2}{n_l}} P(\\text{No Connected Paths between } \\widehat{W}^{[l]} \\text{ and } \\widehat{W}^{[l+1]} | M^{[l]} \\in \\mathcal{A}_{k}^{[l]}) \\cdot P(M^{[l]} \\in \\mathcal{A}_{k}^{[l]})$\nFor there to be no connected paths between $\\widehat{W}^{[l]}$ and $\\widehat{W}^{[l+1]}$, that means that all $n_{l+1}$ nonzero entries in $M^{[l+1]}$ must lie in the remaining $w - k$ columns that are not aligned with the k nonzero rows of $M^{[l]}$. Thus, we find that\n$P(\\text{No Connected Paths between } \\widehat{W}^{[l]} \\text{ and } \\widehat{W}^{[l+1]} | M^{[l]} \\in \\mathcal{A}_{k}^{[l]}) = \\frac{\\binom{w^2 - k \\cdot w}{n_{l+1}"}, {}, "k)}{n"]