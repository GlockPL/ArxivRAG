{"title": "Establish seedling quality classification standard for Chrysanthemum efficiently with help of deep clustering algorithm", "authors": ["Yanzhi Jing", "Hongguang Zhao", "Shujun Yu"], "abstract": "Establishing reasonable standards for edible chrysanthemum seedlings helps promote seedling development, thereby improving plant quality. However, current grading methods have the several issues. The limitation that only support a few indicators causes information loss, and indicators selected to evaluate seedling level have a narrow applicability. Meanwhile, some methods misuse mathematical formulas. Therefore, we propose a simple, efficient, and generic framework, SQCSEF, for establishing seedling quality classification standards with flexible clustering modules, applicable to most plant species. In this study, we introduce the state-of-the-art deep clustering algorithm CVCL, using factor analysis to divide indicators into several perspectives as inputs for the CVCL method, resulting in more reasonable clusters and ultimately a grading standard Sevel for edible chrysanthemum seedlings. Through conducting extensive experiments, we validate the correctness and efficiency of the proposed SQCSEF framework.", "sections": [{"title": "1. introduction", "content": "Chrysanthemum is one of the most popular flower in the world(Spaargaren and van Geest (2018)). With the advancement of modern medical and chemical technology, researchers have found that edible chrysanthemum is rich in functional health ingredients(Jingyun, Baiyi and Baojun (2021)), such as a variety of vitamins, minerals and amino acids, chlorogenic acid, quercetin and baicalin, etc(Rop, Ml-cek and Jurikova (2012)). The beneficial effects of Chrysanthemum are primarily attributed to its phenolic bioactive compounds, such as flavonoids and phenolic acids(Tian, Li, Li, Zhi, Li, Tang, Yang, Yin and Ming (2018)). These compounds are believed to possess antibacterial, antiviral, anti-inflammatory, and antioxidant properties, as well as free radical scavenging capabilities. They contribute to cardiovascular protection, prevention of coronary heart disease, cholesterol and lipid reduction, and benefit for diabetes management (Yamamoto, Yamane, Oishi, Shimizu, Tadaishi and Kobayashi-Hattori (2015)). Additionally, they have a positive impact on human health (Hadizadeh, Samiei and Shakeri (2022)). Overall, edible Chrysanthemum is a multi-functional material that not only contains various nutrients but also promotes physical and mental health, alleviating some common health issues (Lu, Li and Yin (2016)).\nDue to their various health benefits, edible Chrysanthemum has gradually become a highly favored health food (Jiang, Le, Wan, Zhai, Hu, Xu and Xiao (2015)), available in a variety of forms (Acikgoz (2017)). The growing demand for edible Chrysanthemum has made it one of the most common flowers in the edible flower market (Fernandes, Casal, Pereira, Saraiva and Ramalhosa (2020)). This trend has spurred the development of related industries, with researchers focusing on improving Chrysanthemum quality, increasing yield, and exploring cultivation techniques suitable for different regions.\nThe establishment of seedling quality classification standards aims to ensure that the growth and yield of crops, horticultural plants, and forestry trees meet expected levels, thereby promoting the sustainable development of agriculture, horticulture, and forestry (Sutton (1980)).These standards not only ensure production quality and increase yield and quality but also enhance plant resistance to pests and diseases, promote varietal improvement, reduce production risks, regulate market order, and facilitate international trade(Novikov, Sokolov, Drapalyuk, Zelikov and Iveti\u0107 (2019)). Overall, the implementation of seedling quality classification standards helps optimize production, protect the environment, and improve economic benefits, thereby laying a solid foundation for the sustainable development of agriculture and related industries.\nCurrently, numerous scholars have developed various indicators to assess seedling quality and performance potential, primarily based on morphological characteristics, e.g., plant height or ground diameter. These characteristics are easy to measure and widely applicable (Pinto, Marshall, Dumroese, Davis and Cobos (2011)). However, traditional assessment methods primarily rely on manual inspection and basic indicators, which are highly subjective, labor-intensive, and inconsistent (Zaerr (2003)).In practical production environments, seedling quality classification mainly relies on morphological indicators, while classification methods based on physiological indicators are still in the experimental research stage (Grossnickle and MacDonald (2018)).As seedling quality classification methods continue to evolve, it is increasingly necessary to further explore and refine a scoring system that comprehensively considers both morphological and physiological indicators (Rose, Carlson and Morgan (1990)). Researchers need to establish a comprehensive and more applicable scoring system that considers both morphological and physiological indicators. However, existing grading standard establishment methods have the following issues:\n\u2022 Only support a limited number of indicators. Clark et al. used visually selected indicators to establish grading standards for Northern Red Oak seedlings (Clark, Schalarbaum and Kormanik (2000)). However, describing plant development with only a few indicators inevitably results in information loss.\n\u2022 The grading indicators are only applicable to specific species.\n\u2022 Some grading standard methods incorrectly use mathematical formulas. For example, principal component analysis (PCA) is designed for data dimensionality reduction, but a significant amount of work erroneously uses it for indicator selection.\nTo address these issues, we first defined a simple, efficient, and generic framework: Seedling Quality Classification Standard Establishment Framework, abbreviated as SQCSEF, which is suitable for establishing grading standards for most plants. Specifically, we first use clustering algorithms to divide the sample data into several clusters and compute the cluster centers and radii, then calculate the lower bounds of the clusters to determine the boundaries between grades. With the support of SQCSEF, we developed a grading standard for fresh chrysanthemum seedlings. Initially, we used the classic K-Means clustering method as the clustering module within SQCSEF, obtaining a grading standard Skmeans as a baseline. Subsequently, we introduced the state-of-the-art deep clustering algorithm CVCL, employing factor analysis to divide indicators into several views as inputs for the CVCL method, achieving more reasonable clustering results and ultimately a new grading standard Sevel. Upon comparison, we found that Sevel is more precise and reasonable than Skmeans, demonstrating that the SQCSEF framework is both efficient and generic. Moreover, the CVCL method effectively uncovers the intrinsic relationships between samples from different views. We summarize the contributions of this paper as follows:\n\u2022 To the best of our knowledge, SQCSEF is the first framework that support to establish seedling quality classification for most variety of plants.\n\u2022 We employ K-Means clustering method as the clustering module within SQCSEF, and obtain a grading standard Skmeans for edible Chrysanthemum seedlings.\n\u2022 We leverage CVCL, which is a deep clustering method, as the clustering module within SQCSEF. To meet the requirements of CVCL, we design a novel trick to partition samples into several views with help of factor analysis. Then we obtain a more precise standard Sovel\nThe rest of the paper is organized as follows. In Section 2, we first present materials for experiment and some basic mathematical preliminaries. The SQCSEF framework is designed in subsequent description. Then we introduce classic K-Means clustering and state-of-the-art deep clustering in detail. Section 3 shows the experimental evaluation results. In the end, we conclude our work in Section 4."}, {"title": "2. Materials and methods", "content": "To ensure the consistency of plant growth, the materials utilized in this study were exclusively derived from Chrysanthemum morifolium cultivars, collected in March 2023 from the Zhongshan city. These were cultivated in the greenhouse at the College of Horticulture Experimental Base at South China Agricultural University, located at coordinates 113\u00b021'30\" E and 23\u00b09\u203223\" N. Propagation by cutting was initiated on March 10th, and data were collected on May 10th. During the cultivation period, the average annual temperature was observed to be between 19\u00b0C and 28\u00b0C, with precipitation levels ranging from 1,623.6mm to 1,899.8mm. The plant was identified as a member of the Chrysanthemum genus within the Asteraceae family.\nWe referred to the Technical Regulations for Cultivation of Fresh Edible Chrysanthemum Flowers in Zhongshan City (DB4420/T 18-2022) for plant cultivation and management methods. Furthermore, Liu Xiaobing et al. investigated the effects of different cultivation substrates on the growth of Huangqiu. Therefore, to ensure excellent water retention and air permeability of the cultivation substrate during the planting period of Huangqiu, we chose a mixture of peat soil, perlite, vermiculite, and pond mud in a ratio of 3:1:0.5:1.\nTo ensure good rooting effects and significant reduction in production costs, cultivation containers should possess characteristics such as environmental friendliness, low cost, and ease of use. Non-woven planting bags meet these criteria, so we selected them as the planting containers. During actual planting, Huangqiu seedlings were planted in non-woven planting bags, with bag dimensions of 30\u00d725 cm and substrate filling up to 3/5 of the bag height. This ensures both good rooting effects and significant reduction in production costs. To prevent weed growth, maintain substrate air permeability, and avoid over-wetting of the planting bags, they were placed on a 1.5\u00d73m iron rack, isolating the roots from the soil.\nConsidering the preference of chrysanthemums for moist but not waterlogged conditions, an automatic sprinkler system was installed in the greenhouse. Due to the high temperatures from July to September, water evaporates quickly, so automatic watering was scheduled for 5 minutes every afternoon at 6 pm. After October, as temperatures decrease, the frequency of automatic watering was reduced to once every two days, with each session lasting for 8 minutes. In September, when nighttime temperatures are higher, there is a tendency for willow leaf buds to appear. If found, they should be promptly removed from above the normal leaves. To improve the quality of chrysanthemum flowers, solar supplementary lighting was installed above the greenhouse for one and a half months, from early October to mid-November, from 6 pm to 3 am.\nPrior to the grading of seedlings, it is imperative to eliminate any plants afflicted with diseases or exhibiting poor growth. For comprehensive assessment, 200 species were randomly selected for measurement from each category, encompassing six parameters: seedling height, stem diameter,number of lateral branches, root length, fresh weight, and leaf chlorophyll content. The specific measurement procedures were as follows: seedling height was measured using a measuring tape with a precision of 0.1 cm; stem diameter was assessed with an electronic digital caliper accurate to 0.01 mm; root length was likewise measured using a digital caliper, with precision to 0.01 mm; fresh weight was determined using an electronic scale, precise to 0.01 g; leaf chlorophyll content was quantified using the SPAD-502 chlorophyll meter. These measurement techniques were selected to ensure the accuracy and reliability of the data for subsequent analysis and evaluation."}, {"title": "2.2. Basic preprocessing and statistical analysis of dataset", "content": "It is deserved to note that we ought to transform all metrics into maximization objectives before the preprocessing. For example, the seedling height and ground diameter are in category of maximization objectives since that they have a positive effect on seedling growth, while number of lateral branches is a minimization objective, which will consume nutrients of seedlings, block the sunlight, and increase the risk of diseases and pests. A simple formula to finish this work is shown as Eqs.1.\n$x_{i}$ = max(x) \u2212 $x_{i}$  (1)\nTo compare and analyze indices with different scales fairly, the min-max normalization method is called at the beginning of preprocessing. Suppose that raw dataset $D_{raw}$ is a $M \u00d7 N$ matrix where each row is an element and each column corresponds to an index, we scale it to the standardized dataset $D_{std}$ according to Eqs.2\n$e_{ij}$ =\n$\\frac{e_{ij} - min(e_{j})}{max(e_{j}) \u2013 min(e_{j})}$  (2)\nwhere $e_{j}$(resp., $e_{ij}$) is the value at the i-th row and j-th column of $D_{raw}$(resp., $D_{std}$).\nIdeally, there is always some correlation between indices of plants, so we firstly attempt to introduce Pearson correlation coefficient(PCC) to describe correlation among indices. Assume that there are two variables X and Y, the correlation coefficient \u03c1 is defined as\n$\u03c1_{X,Y}$ = $\\frac{cou(X, Y)}{\u03c3_{X}\u03c3_{Y}}$ (3)\nwhere cou(X, Y) is the covariance of X and Y which could be calculated according to Eqs.4, and \u03c3 is the standard deviation.\ncou(X, Y) =$\\frac{\\sum_{i=1}^{M}(X_{i} \u2212 E(X))(Y_{i} \u2212 E(Y))}{M}$ (4)\nThen we can obtain a correlation coefficient matrix. To estimate the robust of correlation coefficient, hypothesis testing should be conducted(Wilcox (2017)). For simplicity, we show the calculation method of hypothesises testing directly. Define two hypothesis as\n$H_{o}$:r= 0\n$H_{1}$:r\u22600 (5)\nwhere r is the correlation coefficient between two variables X and Y. Construct a new variable t as\n$t = r\\sqrt{\\frac{M-2}{1-r^{2}}}$ (6)\nwhich actually obeys the t-distribution with M \u2212 2 degree of freedom(DOF). Figure out the probability p(t) at value t in t-distribution mentioned before, then the significance level of correlation coefficient is revealed. For example, if we calculate p of some variable less than 0.05, the hypothesis $H_{o}$ is rejected at 95% confidence level, in other word, the correlation coefficient is not equal to 0 at 95% significance level.\nNote that the dataset ought to meet two requirements if use PCC: (1) the correlation type between two variables must be linear correlation and (2) samples of each variable should obey a normal distribution. Hence we need to do some inspection before calculating PCC. To briefly examine the correlation type, we choose to plot a scatterplot matrix which represent correlation between any two variables vividly. It is slightly complicated to check the distribution type that a variable obey. Jarque-Bera testing is conducted to test if a variable obeys normal distribution. Define two hypothesises as\n$H_{o}$: X obeys normal distribution\n$H_{1}$: X doesn't obey normal distribution (7)\nand construct a new variable JB as\n$JB = \\frac{M}{6} [sk^{2} + \\frac{(ku - 3)^{2}}{4}]$ (8)\nwhere sk and ku is the skewness and kurtosis of X respectively. If X obeys normal distribution, variable JB must obey the chi-square distribution with 2 DOF, i.e., JB ~ $X^{2}$(2). For example, we can figure out the probability p(JB) in chi-square distribution mentioned before, and test if p(JB) is larger than 0.01. The p(JB) >= 0.01 indicates that X obeys normal distribution at 99% confidence level..\nWhen the correlation between two variables is non-linear, PCC should be replaced by Spearman correlation coefficient(SCC) which is defined as\n$r_{s}$=1-6$\\frac{\\sum_{i=1}^{M}d_{i}^{2}}{M(M^{2}-1)}$ (9)\nwhere $d_{i}$ is the rank difference between sample $X_{i}$ and $Y_{i}$. Similar to the robust estimation phase of PCC, we still use hypothesises the same as Eqs.5 shown, and construct a new variable st according to Eqs.10.\n$s_{t} = r_{s}VM-1 ~ N(0, 1)$  (10)\nThen we figure out the probability p(st) in standard normal distribution and compare it with 0.05. If p(st) < 0.05, the correlation coefficient is not equal to 0 at 95% significance level."}, {"title": "2.3. Framework of establishing seedling quality classification standard", "content": "In recent years, some studies on seedling quality classification standard for forests, cash crops, flowers and herbs have been proposed(cite papers). However, there doesn't exist a framework that could be employed to establish the standard for any plant. So we attempt to define a simple, efficient and universal framework(namely SQCSEF, i.e., seedling quality classification standard establishment framework) to address this issue. Assuming that we have done the preprocessing work of raw dataset $D_{raw}$ as we described in section 2.2, and obtained the dataset $D_{std}$ containing M elements. The element is a N-dimensional vector where each dimension actually is an attribute. Firstly, SQCSEF takes $D_{std}$ as input and calls the clustering method to partition the dataset $D_{std}$ into K clusters $C_{1}$, $C_{2}$, ..., $C_{k}$ and figure out the centroid $c_{i}$, i = 1,2,..., K of each cluster. Note that the specific clustering method is chosen by the user, which means our framework is suitable for different varieties as long as calling appropriate clustering method. In practice, K is likely to set to 3 corresponding to 3 levels of seedling quality. Then we calculate the radius of each cluster according to Eqs.11\n$r_{i}$ = $\\sqrt{\\sum_{k=1}^{N}\u03c3_{ik}^{2}}$ (11)\nwhere $\u03c3_{ik}$ is the standard deviation of cluster $C_{i}$'s k-th attribute according to Eqs.12. We use notion $M_{i}$ to denote the number of elements in cluster $C_{i}$, and notion $e[k]$ to denote the k-th attribute value of element e.\n$\u03c3_{ik}$ =$\\sqrt{\\frac{1}{M_{i} - 1}\\sum_{j=1}^{M_{i}}(e_{j}[k] - \\bar{e}[k])^{2}}$ (12)\nAfter that, we calculate the lower bound of each level using radiuses and centroids of clusters. The traditional calculation method of lower bound draws a circle and straight line through point centroids on a chequered paper, then measures the point of intersection as lower bound. However, it has two limitations: poor precision and unable to support high dimensional data. To address this limitations, we derive a formula as shown in Eqs.13 to support data of any dimension while maintaining high precision. The notion $lb_{ik}$ denotes the lower bound of cluster $C_{i}$'s k-th attribute.\n$lb_{ik} = c_{i}[k] - \\frac{c_{i}[k].r_{i}}{2}$ (13)\nThe lower bound is set as the boundary point between levels in classification standard. Note that $lb_{ik}$ values are corresponding to the standardized dataset $D_{std}$, so we need to transform them to real value corresponding to raw dataset $D_{raw}$ according to Eqs.2. In practice, we intend to follow the principle of half meeting, i.e., if half or more of the indices of a sample meet the level X, we will consider this sample belongs to level X. The remaining challenge is how to choose an appropriate and efficient clustering method. Aiming to compare the performance of traditional clustering and emerging deep clustering, we introduce two clustering methods, K-Means and CVCL in section 2.4 and section 2.5 respectively."}, {"title": "2.4. Traditional cluster analysis algorithm", "content": "Cluster analysis(Driver (1932)), which is also named as clustering, is an algorithm in category of unsupervised learning. Given a set of elements, cluster analysis is supposed to form some clusters such that for arbitrary cluster $C_{i}$ and element $e_{j}$ in $C_{i}$, $e_{j}$ is more similar to other elements in cluster $C_{i}$ than elements in other clusters. There are several distances to represent the similarity between elements, such as Euclidean distance and Hamming distance. During the processing, the algorithm tries to reduce the sum of distance between each elements in the cluster as less as possible until the algorithm converged. Cluster analysis is extensively utilized in knowledge discovery and data mining to partition the dataset into several clusters corresponding to some classes. For example, given descriptions of apple, banana, cat and dog, the cluster analysis algorithm could classify apple and banana as cluster \"Fruit\" while classify cat and dog as cluster \"Animal\".\nK-Means clustering is a wildly used cluster analysis algorithm proposed by Stuart Lloyd(Lloyd (1982)). Recall that we have a dataset of M elements $e_{1}, e_{2}, ..., e_{M}$ and each element is a N-dimensional vector corresponding to N attributes, the key idea of K-Means clustering is to partition these elements into K clusters s.t.\n$arg min  \\sum_{i=1}^{K} \\sum_{e \u2208 C_{i}}||e - \\bar{e}||_{2}^{2}$ (14)\nwhere $\\bar{e}$ is the mean of elements in cluster $C_{i}$ and $||v||_{2}$ is the Euclidean norm of vector v. Evidently it is NP-hard to solve this multi-objective optimization problem, but K-Means clustering gets the local optimum intelligently using the Heuristic algorithm. More concretely, the algorithm choose K elements from the dataset randomly as the initial K centroids(Step I). Through assigning every elements to nearest cluster, we can obtain K clusters of preliminary version(Step II). Then the algorithm updates K centroids with mean of elements in each cluster(Step III). Step II and Step III are executed repeatedly until the difference between result of two successive iterations is below the Threshold value predefined by the user, and actually we get the local optimum."}, {"title": "2.5. Deep Clustering", "content": "While K-Means clustering is efficient and easy to implement, it has one drawback that can not be ignored: it uses $||e-\\bar{e}||_{2}$, i.e., Euclidean distance, as a measure of similarity, which could induce severe fault when some attributes is abnormal. For instance, supposed that we have an element $e_{1}$ = [2, 2, 10] and two centroids $c_{1}$ = [1, 1, 1], $C_{2}$ = [8, 8, 8] of cluster $C_{1}$, $C_{2}$ respectively. Each dimension of vector represents seedling's Height, Ground diameter and Weight respectively. So we can calculate the Euclidean distance from $e_{1}$ to $c_{1}$ and $c_{2}$, i.e., $d_{1}$ = $||e_{1} - c_{1}||_{2}$ = $\\sqrt{83}$, $d_{2}$ =\n$||e_{2} - c_{2}||_{2}$ = $\\sqrt{76}$. Based on the algorithm of K-Means described above, $e_{1}$ should be assigned to cluster $C_{2}$ as $d_{2}$ \u2264 $d_{1}$. However, the result is contrast to common sense since that when a seedling's Height and Ground diameter are lower than normal, it has a poor grouth even though weight of which has a good level. In fact, $e_{1}$ should be assigned to cluster $C_{1}$ in practice because of its poor Comprehensive grouth.\nReviewing the role of K-Means clustering in the establishment of seedling quality classification standard, we found that it's mainly used to form three clusters corresponding to three levels of seedlings. Due to the potential negative impact of some abnormal attribute values in the sample dataset, K-Means may give unreasonable answers. So it is of vital significance to find an appropriate clustering algorithm to achieve higher accuracy.\nWith the rapid development of Deep Learning, deep clustering is proposed to replace the traditional clustering methods which suffered from negative impact of abnormal elements and huge computational overhead on large-scale sample datasets. Xie et al. proposed Deep Embedded Clustering for the first time in 2016, the key idea of which is to map elements to lower-dimensional feature space using neural network(Xie, Girshick and Farhadi (2016)). Then a sequence of methods were proposed, such as DEeP Embedded Regularized ClusTering proposed by Kamran et al. based on convolutional autoencoder and multinomial logistic regression function(Dizaji, Herandi, Deng, Cai and Huang (2017)), deep manifold clustering proposed by Chen et al. which utilized a clustering-oriented objective and a locality preserving objective as model's optimization objective(Chen, Lv and Zhang (2017)), USNID proposed by Zhang et al. which designed a centroid-guided clustering mechanism(Zhang, Xu, Wang, Long and Gao (2023)), and Secu proposed by Qi which adds a pre-training stage for representation learning(Qian (2023)), etc.\nChen et al. proposed cross-view contrastive learning (CVCL) method in 2023 that concentrated on semantic label consistency of multi-view data(Chen, Mao, Woo and Peng (2023)). The core idea of CVCL is utilizing contrastive learning to extract common semantic labels from multi views. Obviously, CVCL method can address our issue suitably just with a few modifications since that we can divide indices of seedling into several views from different aspects: physiology view and morphology view, or above-ground part view and underground part view. For example, the leaf chlorophyll content belongs to physiology view while the height and ground diameter of seedlings belong to morphology view. To appropriately decide the views, the descending dimension method such as principal component analysis(PCA) or factor analysis(FA) is called before conducting CVCL method. Compared with PCA, factor analysis can identify correlations between latent factors and observed variables, and analysis result is easier to interpret. So we conducts the factor analysis on indices to extract underlying factors, and analyzes every factor to explain the information contained in this factor. More specifically, we observe the scree plot and total variance explained matrix to determine the number of factors nv in the first round, and obtain nv factors and corresponding eigenvalue $\u03bb_{1}$, $\u03bb_{2}$, ..., $\u03bb_{nv}$, in the second round. Then we analyze the rotated component matrix and extract primary indices of each factor according to its factor loading.\nWe describe the CVCL method here in detail and the framework of CVCL is presented in Fig1. Given a dataset in which arbitrary element can be seen as combination of multiple views, i.e., $D = {E^{(v)}}_{v=1}^{nv}$ with $n_{v}$ views, CVCL can partition the elements considering every view comprehensively without being affected by abnormal values. Denote the cardinality of dataset D as M, and each view is a M \u00d7 dv matrix $[e_{1}^{(v)}, e_{2}^{(v)}, ..., e_{M}^{(v)}]^{T}$ where $d_{v}$ is the dimension of view v. The framework of CVCL mainly has two components: autoencoder and contrastive learning module. Autoencoder,consisting of an encoder and a decoder, was employed to transform the raw data to semantic features of raw dataset in the pre-training stage, while contrastive learning module takes semantic features as input and partition them into K clusters through contrasting temporary clusters corresponding to different views. Contrastive learning module firstly pushes semantic features of each view into a multi-layer perception which contains several linear layers and a softmax layer, and obtains a M \u00d7 K matrix $H^{(v)}$ for each view as the probability distribution of each cluster. More concretely, denote $h_{rc}^{(v)}$ as the element in r-th row and c-th column, and we define $h_{rc}^{(v)}$ as the probability assigning element $e_{r}$ to c-th cluster in view v. However, $H^{(v)}$ matrix may suffer from a problem that the difference among {h,j},j = 1,2, ..., K is not significant, so a unified target distribution is appended to the multi-layer perception as shown in Eqs.15 to enhance the discriminability of cluster assignments. Then we can obtain a M \u00d7 K matrix $P^{(v)}$.\n$P_{rc}^{(v)} = \\frac{(H_{rc}^{(v)})^{2}/\\sum_{i=1}^{M}H_{ic}^{(v)}}{((\\sum_{j=1}^{K}(H_{rj}^{(v)})^{2}) / (\\sum_{i=1}^{M}H_{ij}^{(v)}))}$ (15)\nFor expressing the similarity between cluster assignments more precisely, we utilize Cosine Similarity as similarity calculation formula instead of the one used in CVCL(Chen et al. (2023)). Assume that $p_{c}^{(v)}$ is the c-th column of $P^{(v)}$. The similarity can be measured according to Eqs.16.\ns$(p_{c}^{(v1)}, p_{c}^{(v2)})$ =$\\frac{(p_{c}^{(v1)})^{T}(p_{c}^{(v2)})}{||p_{c}^{(v1)}||_{2}||p_{c}^{(v2)}||_{2}}$ (16)\nCVCL method simply assigns the element $e_{i}$ to the cluster which has maximal mean probability among all views, i.e.,\n$arg max_{C}$ \\frac{\\sum_{v=1}^{nv}P_{ic}^{(v)}}{nv}$ (17)\nObviously, CVCL method treats each view equally, i.e., the weight of each view is the same. But some attributes actually contribute to plant's growth more than others in the real world, so it's impractical to set the same weights. To enhance the performance of the CVCL method, we figure out the weight $w_{1}$, $w_{2}$, ..., $w_{nv}$ of each view through factor analysis method and modify Eqs.17 as:\n$arg max_{C}$ \\frac{\\sum_{v=1}^{nv}w_{v}P_{ic}^{(v)}}{nv}$ (18)\nwhere weight of view $v_{i}$ can be calculated as:\n$\u03c9_{v} = \u03bb_{i}/\\sum_{i=1}^{nv}\u03bb_{i}$ (19)\nExperiment shows that the assignment of each element is more close to practice via introducing weight of views.\nTo train the network efficiently, we continue to define loss function the same as CVCL's. More specifically, the total loss function is composed by three parts, i.e.,\nL = $L_{pre}$ + \u03b1$L_{c}$ + \u03b2$L_{a}$ (20)\nwhere $L_{pre}$ is the pre-training loss, $L_{c}$ is the cross-view contrastive loss, $L_{a}$ is the regularization term, and \u03b1 and \u03b2 are parameters decided by the user. Define $f^{(v)}(.)$ and $f^{(v)}(.)$ as the encoder and decoder respectively. The pre-training loss expresses the reconstruction capacity of decoder and can be measured as\n$L_{pre}$ =$\\sum_{v=1}^{nv} \\sum_{i=1}^{M}||e^{(v)} - f^{(v)}(f^{(v)}(e^{(v)}))||^{2}$ (21)\nIntroduced to maximize the difference of single view's assignment and minimize the difference of inter-view's assignments, cross-view contrastive loss is measured as follows:\n$l(v_{1},v_{2})$ = $\\frac{1}{1-K} \\sum_{k=1}^{K} log \\frac{e^{s(p_{k}^{(v_{1})},p_{k}^{(v_{2})})}}{\\sum_{j=1,j\u2260k}^{K}e^{s(p_{j}^{(v_{1})},p_{k}^{(v_{2})})}}$\n$L_{c}$ = $\\sum_{v_{1}=1}^{nv} \\sum_{v_{2}=1, v_{2}\u2260v_{1}}^{nv}l(v_{1},v_{2})$ (22)\nIt is straightforward that with the purpose of minimize the cross-view contrastive loss, network ought to maximize $e^{s(p_{k}^{(v_{1})},p_{k}^{(v_{2})})}$ corresponding to the difference of single view's assignment and minimize $\\sum_{j=1,j\u2260k}^{K}e^{s(p_{j}^{(v_{1})},p_{k}^{(v_{2})})}_{j=1,j\u2260k}$"}, {"title": "3. Results", "content": "In this section, we represent and analyze intermediate results of each step in section 2. And as a result, we obtain the seedling quality classification standard for Chrysanthemum, whose performance and reasonability had been examined carefully. The total experiment was conducted on a laptop computer with Intel(R) Core(R) i5-12th CPU and 32 GB RAM."}, {"title": "3.1. Analysis of dataset", "content": "At the preprocessing stage, we firstly describe basic statistical properties of raw dataset, i.e., minimum, maximum, mean, standard deviation and correlation. IBM(R) SPSS(R) is employed to do these calculation due to its excellent statistical analysis ability. As shown in Table 1, it is obvious that seedling height, ground diameter and leaf chlorophyll content have moderate distribution ranges, which indicates that these indices have less dispersion degree, while distribution ranges of root length and fresh weight is slightly large. Especially, the dispersion degree of number of lateral branches is extremely high, so we guess that this index is independent of other indices.\nFor intuition, we plot the scatterplot matrix as Fig.2 shown. Obviously, except for number of lateral branches, each index basically obey linear correlation with other indices. Meanwhile, result of kernel density estimation(KDE) states that indices may obey normal distribution, of course except for number of lateral branches. So we firstly tried to figure out the PCC among seedling height, ground diameter, root length, fresh weight and leaf chlorophyll content, then figure out SCC between number of lateral branches and the other 5 indices. Table.2 shows the conclusion of Jarque-Bera testing.\nJust as we suspected, seedling height, ground diameter, root length, fresh weight and leaf chlorophyll content obey the normal distribution at 99% confidence level since that their p-values are larger than 0.01, while number of lateral branches does not. Of course, we also plotted the Probability plot(P-P plot) in Fig.3 to estimate the distribution type roughly. The PCC and SCC of indices are presented in Table.3 and Table.4 respectively, and correlation heat map(Fig.4) is employed to enhance visibility.\nSeedling height has an extremely strong correlation with leaf chlorophyll content, and a fairly strong correlation with fresh weight and root length. It conforms to common sense since that the taller seedling is, the easier it is to receive illumination, so more chlorophyll can be synthesized. And the height of the seedling affects the volume of its above-ground part, which indirectly affects the fresh weight of the seedling. In general, the growth of the aboveground part of the seedling requires a well-developed root system, so the root length is positively correlated with the height of the seedling. Ground diameter has a fairly strong correlation with root length and fresh weight. Strong roots usually have longer root system, which can obtain enough nutrients for the seedlings and also contributes to fresh weight. Especially, number of lateral branches has a very weak correlation with the other 5 indices. In practice, lateral branches will consume nutrients of seedlings, block the sunlight which affects photosynthesis, and increase the risk of diseases and pests. So it is advised to trim lateral branches regularly."}, {"title": "3.2. Performance of SQCSEF using different clustering method", "content": "It is convenient to conduct K-Means clustering with help of SPSS, so we concentrate on the CVCL method. CVCL method requires the data be divided into several views, so factor analysis is called before SQCSEF. We firstly test if indices are suitable for factor analysis and decide the number of factors. The results of KMO test and Bartlett's test is reported in Table.5.\nSince the KMO Measure of sampling adequacy is 0.807 > 0.80 and significance of Bartlett's test is 0.000 \u2264 0.050, factor analysis can be conducted on the raw dataset. The scree plot is plotted in Fig.5, from which we observe that the curve begins to leave off after the second point, hence we consider to set the number of factors as 2 or 3. The total variance explained table is reported in Table.6. Cumulative variance explained of the first three factors reaches 90.402% \u2265 80.000%, which means that these three factors can describe the raw dataset more comprehensively. To simplify the factor structure and make the relationships between factors more clearer and easier to understand, we rotate the factor matrix and report it in Table.7. In factor1, the loadings of seedling height, fresh weight and leaf chlorophyll content are relatively larger than other indices, so we suppose that factor"}]}