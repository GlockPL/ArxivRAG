{"title": "Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers", "authors": ["Kai Yan", "Alexander G. Schwing", "Yu-Xiong Wang"], "abstract": "Decision Transformers have recently emerged as a new and compelling paradigm for offline Reinforcement Learning (RL), completing a trajectory in an autoregressive way. While improvements have been made to overcome initial shortcomings, online finetuning of decision transformers has been surprisingly under-explored. The widely adopted state-of-the-art Online Decision Transformer (ODT) still struggles when pretrained with low-reward offline data. In this paper, we theoretically analyze the online-finetuning of the decision transformer, showing that the commonly used Return-To-Go (RTG) that's far from the expected return hampers the online fine-tuning process. This problem, however, is well-addressed by the value function and advantage of standard RL algorithms. As suggested by our analysis, in our experiments, we hence find that simply adding TD3 gradients to the fine-tuning process of ODT effectively improves the online finetuning performance of ODT, especially if ODT is pretrained with low-reward offline data. These findings provide new directions to further improve decision transformers.", "sections": [{"title": "1 Introduction", "content": "While Reinforcement Learning (RL) has achieved great success in recent years [55, 31], it is known to struggle with several shortcomings, including training instability when propagating a Temporal Difference (TD) error along long trajectories [14], low data efficiency when training from scratch [67], and limited benefits from more modern neural network architectures [12]. The latter point differs significantly from other parts of the machine learning community such as Computer Vision [17] and Natural Language Processing [11].\nTo address these issues, Decision Transformers (DTs) [14] have been proposed as an emerging paradigm for RL, introducing more modern transformer architectures into the literature rather than the still widely used Multi-Layer Perceptrons (MLPs). Instead of evaluating state and state-action pairs, a DT considers the whole trajectory as a sequence to complete, and trains on offline data in a supervised, auto-regressive way. Upon inception, DTs have been improved in various ways, mostly dealing with architecture changes [37], the token to predict other than return-to-go [22], addressing the problem of being overly optimistic [46], and the inability to stitch together trajectories [5]. Significant and encouraging improvements have been reported on those aspects.\nHowever, one fundamental issue has been largely overlooked by the community: offline-to-online RL using decision transformers, i.e., finetuning of decision transformers with online interactions. Offline-to-online RL [72, 41] is a widely studied sub-field of RL, which combines offline RL learning from given, fixed trajectory data and online RL data from interactions with the environment. By first training on offline data and then finetuning, the agent can learn a policy with much greater data efficiency, while calibrating the out-of-distribution error from the offline dataset. Unsurprisingly, this sub-field has become popular in recent years."}, {"title": "2 Preliminaries", "content": "Markov Decision Process. A Markov Decision Process (MDP) is the basic framework of sequential decision-making. An MDP is characterized by five components: the state space S, the action space A, the transition function p, the reward r, and either the discount factor \u03b3 or horizon H. MDPs involve an agent making decisions in discrete steps t \u2208 {0, 1, 2, . . . }. On step t, the agent receives the current state $s_t \u2208 S$, and samples an action $a_t \u2208 A$ according to its stochastic policy \u03c0(at|st) \u2208 \u2206(\u0391), where (A) is the probability simplex over A, or its deterministic policy \u00b5(st) \u2208 A. Executing the action yields a reward r(st, at) \u2208 R, and leads to the evolution of the MDP to a new state st+1, governed by the MDP's transition function p(st+1|St, at). The goal of the agent is to maximize the total reward $\\sum_t \\gamma^t r(s_t, a_t)$, discounted by the discount factor \u03b3\u2208 [0,1] for infinite steps, or $\\sum_{t=1}^H r(s_t, a_t)$ for finite steps. When the agent ends a complete run, it finishes an episode, and the state(-action) data collected during the run is referred to as a trajectory \u0442.\nOffline and Online RL. Based on the source of learning data, RL can be roughly categorized into offline and online RL. The former learns from a given finite dataset of state-action-reward trajectories, while the latter learns from trajectories collected online from the environment. The effort of combining the two is called offline-to-online RL, which first pre-trains a policy using offline data, and then continues to finetune the policy using online data with higher efficiency. Our work falls into the category of offline-to-online RL. We focus on improving the decision transformers, instead of Q-learning-based methods which are commonly used in offline-to-online RL.\nDecision Transformer (DT). The decision transformer represents a new paradigm of offline RL, going beyond a TD-error framework. It views a trajectory as a sequence to be auto-regressively completed. The sequence interleaves three types of tokens: returns-to-go (RTG, the target total return), states, and actions. At step t, the past sequence of context length K is given as the input, i.e., the input is (RTGt-K,St\u2212k,at\u2212k,...,RTGt, St), and an action is predicted by the auto-regressive model, which is usually implemented with a GPT-like architecture [11]. The model is trained via supervised learning, considering the past K steps of the trajectory along with the current state and the current return-to-go as the feature, and the sequence of all actions a in a segment as the labels. At evaluation time, a desired return RTGeval is specified, since the ground truth future return RTGreal isn't known in advance."}, {"title": "Online Decision Transformer (ODT)", "content": "ODT has two stages: offline pre-training which is identical to classic DT training, and online finetuning where trajectories are iteratively collected and the policy is updated via supervised learning. Specifically, the action at at step t during rollouts is computed by the deterministic policy $\u00b5_{DT} (s_{t-T:t}, a_{t-T:t-1}, RTG_{t-T:t}, T = T_{eval}, RTG = RTG_{eval})$, or sampled from the stochastic policy $\\pi_{DT} (a_t|s_{t-T:t}, a_{t-T:t-1}, RTG_{t-T:t}, T = T_{eval}, RTG = RTG_{eval})$. Here, T is the context length (which is Teval in evaluation), and RTGeval \u2208 R is the target return-to-go. The data buffer, initialized with offline data, is gradually replaced by online data during finetuning.\nWhen updating the policy, the following loss (we use the deterministic policy as an example, and thus omit the entropy regularizer) is minimized:\n$\\sum_{t=1}^{T_{train}} ||DT (s_{0:t}, a_{0:t-1}, RTG_{0:t}, RTG= RTG_{real}, T = t) - a_t||^2$ (1)\nNote, Ttrain is the training context length and RTGreal is the real return-to-go. For better readability, we denote {$x+1,Sx+2,...,Sy}, x,y \u2208 N as sx:y (i.e., left exclusive and right inclusive), and similarly {ax+1,Ax+2,...,ay} as ax:y and {RTGx+1,...,RTGy} as RTGx:y. Specially, index x = y represents an empty sequence. For example, when t = 1, a0:0 is an empty action sequence as the decision transformer is not conditioned on any past action.\nOne important observation: the decision transformer is inherently off-policy (the exact policy distribution varies with the sampled starting point, context length and return-to-go), which effectively guides our choice of RL gradients to off-policy algorithms (see Appendix C for more details)."}, {"title": "TD3", "content": "Twin Delayed Deep Deterministic Policy Gradient (TD3) [21] is a state-of-the-art online off-policy RL algorithm that learns a deterministic policy a = \u00b5RL(s). It is an improved version of an actor-critic (DDPG [32]) with three adjustments to improve its stability: 1) Clipped double Q-learning, which maintains two critics (estimators for expected return) $Q_{\u03d51}, Q_{\u03d52}: |S|\u00d7 |A| \u2192 R$ and uses the smaller of the two values (i.e., min (Q1, Q42)) to form the target for TD-error minimization. Such design prevents overestimation of the Q-value; 2) Policy smoothing, which adds noise when calculating the Q-value for the next action to effectively prevent overfitting; and 3) Delayed update, which updates \u00b5RL less frequently than $Q_{\u03d51}, Q_{\u03d52}$ to benefit from a better Q-value landscape when updating the actor. TD3 also maintains a set of target networks storing old parameters of the actor and critics that are soft-updated with slow exponential moving average updates from the current, active network. In this paper, we adapt this algorithm to fit the decision transformer architecture so that it can be used as an auxiliary objective in an online finetuning process."}, {"title": "3 Method", "content": "This section is organized as follows: we will first provide intuition why RL gradients aid online finetuning of decision transformers (Sec. 3.1), and present our method of adding TD3 gradients (Sec. 3.2). To further justify our intuition, we provide a theoretical analysis on how ODT fails to improve during online finetuning when pre-trained with low-reward data (Sec. 3.3)."}, {"title": "3.1 Why RL Gradients?", "content": "In order to understand why RL gradients aid online finetuning of decision transformers, let us consider an MDP which only has a single state so, one step, a one dimensional action a \u2208 [-1,1] (i.e., a bandit with continuous action space) and a simple reward function r(a) = (a + 1)\u00b2 if a \u2264 0 and r(a) = 1 - 2a otherwise, as illustrated in Fig. 1. In this case, a trajectory can be represented effectively by a scalar, which is the action. If the offline dataset for pretraining is of low quality, i.e., all actions in the dataset are either close to -1 or 1, then the decision transformer will obviously not generate trajectories with high RTG after offline training. As a consequence, during online finetuning, the new rollout trajectory is very likely to be uninformative about how to reach RTGeval, since it is too far from RTGeval. Worse still, it cannot improve locally either, which requires $\\frac{\\partial RTG}{\\partial a}$. However, the decision transformer yields exactly the inverse, i.e., $\\frac{\\partial a}{\\partial RTG}$. Since the transformer is not invertible (and even if the transformer is invertible, often the ground truth RTG(a) itself is not), we cannot"}, {"title": "3.2 Adding TD3 Gradients to ODT", "content": "In this work, we mainly consider TD3 [21] as the RL gradient for online finetuning. There are two reasons for selecting TD3. First, TD3 is a more robust off-policy RL algorithm compared to other off-policy RL algorithms [54]. Second, the success of TD3+BC [20] indicates that TD3 is a good candidate when combined with supervised learning. A more detailed discussion and empirical comparison to other RL algorithms can be found in Appendix C.\nGenerally, we simply add a weighted standard TD3 actor loss to the decision transformer objective. To do this, we follow classic TD3 and additionally train two critic networks $Q_{\u03d51}, Q_{\u03d52}: SXAR$ parameterized by $\u03d51, \u03d52$ respectively. In the offline pretraining stage, we use the following objective for the actor:\n$\\min_{\u00b5_{DT}} E_{\u03c4\u223cD} \\sum_{t=1}^{T_{train}} [-\u03b1Q_{\u03d51}(s_t, \u00b5_{DT}(s_{0:t}, a_{0:t-1}, RTG_{0:t}, RTG = RTG_{real}, T = t))+ ||\u00b5_{DT} (s_{0:t}, a_{0:t-1}, RTG_{0:t}, RTG = RTG_{real}, T = t) - a_t||^2]$ (2)\nHere, \u03b1 \u2208 {0,0.1} is a hyperparameter, and the loss sums over the trajectory segment. For critics $Q_{\u03d51}, Q_{\u03d52}$, we use the standard TD3 critic loss\n$\\min_{\u03d5_1,\u03d5_2} E_{\u03c4\u223cD} \\sum_{t=1}^{T_{train}} [(Q_{\u03d51} (s_t, a_t) - Q_{min,t})^2 + (Q_{\u03d52} (s_t, a_t) \u2013 Q_{min,t})^2]$, with\n$Q_{min,t} = r_t + \u03b3(1 - d_t) \\min_{i\u2208\\{1,2\\}} Q_{i,tar} (s_t, clip (\u00b5_{\u03b8}(z_t) + clip(\u03f5, -c, c), a_{low}, a_{high}))$,(3)\nwhere T = {$s_{0:T_{train}+1}, a_{0:T_{train}}, RTG_{0:T_{train}+1}, d_{0:T_{train}}, r_{0:T_{train}}, RTG = RTG_{real}$} is the trajectory segment sampled from buffer D that stores the offline dataset. Further, dt indicates whether the trajectory ends on the t-th step (true is 1, false is 0), Qmin is the target to fit, Qi, tar is produced by the target network (stored old parameter), zt is the context for \u201cnext state\u201d at step t. par is the target network for the actor (i.e., decision transformer). For an n-dimensional action, clip(a, x, y), a \u2208 Rn, y \u2208 Rn, z\u2208RN means clip a\u017c to [yi, zi] for i \u2208 {1,2,..., n}. alow \u2208 Rn and @high \u2208 Rn are the lower and upper bound for every dimension respectively.\nTo demonstrate the impact on aiding the exploration of a decision transformer, in this work we choose the simplest form of a critic, which is reflective, i.e., only depends on the current state. This essentially makes the Q-value an average of different context lengths sampled from a near-uniform distribution (see Appendix D for the detailed reason and distribution for this). The choice is based on the fact that training a transformer-based value function estimator is quite hard [45] due to increased input complexity (i.e., noise from the environment) which leads to reduced stability and slower convergence. In fact, to avoid this difficulty, many recent works on Large Language Models (LLMs) [13] and vision models [48] which finetune with RL adopt a policy-based algorithm instead of an actor-critic, despite a generally lower variance of the latter. In our experiments, we also found such a critic to be much more stable than a recurrent critic network (see Appendix G for ablations).\nDuring online finetuning, we again use Eq. (2) and Eq. (3), but always use a = 0.1 for Eq. (2).\nWhile the training paradigm resembles that of TD3+BC, our proposed method improves upon TD3+BC in the following two ways: 1) Architecture. While TD3+BC uses MLP networks for single steps, we leverage a decision transformer, which is more expressive and can take more context into account when making decisions. 2) Selected instead of indiscriminated behavior cloning. Behavior cloning mimics all data collected without regard to their reward, while the supervised learning process of a decision transformer prioritizes trajectories with higher return by conditioning action generation on higher RTG. See Appendix G.9 for an ablation."}, {"title": "3.3 Why Does ODT Fail to Improve the Policy?", "content": "As mentioned in Sec. 3.1, it is the goal of ODT to \"prompt\" a policy with a high RTG, i.e., to improve a policy by conditioning on a high RTG during online rollout. However, beyond the intuition provided"}, {"title": "4 Experiments", "content": "In this section, we aim to address the following questions: a) Does our proposed solution for decision transformers indeed improve its ability to cope with low-reward pretraining data. b) Is improving"}, {"title": "4.1 Adroit Environments", "content": "Environment and Dataset Setup. We test on four difficult robotic manipulation tasks [49], which are the Pen, Hammer, Door and Relocate environment. For each environment, we test three different datasets: expert, cloned and human, which are generated by a finetuned RL policy, an imitation learning policy and human demonstration respectively. See Appendix F.1 for details.\nResults. Fig. 3 shows the performance of each method on Adroit before and after online finetuning. TD3+BC fails on almost all tasks and often diverges with extremely large Q-value during online finetuning. ODT and PDT perform better but still fall short of the proposed method, TD3+ODT. Note, IQL, TD3 and TD3+ODT all perform decently well (with similar average reward as shown in Tab. 2 in Appendix B). However, we found that TD3 often fails during online finetuning, probably because the environments are complicated and TD3 struggles to recover from a poor policy generated during online exploration (i.e., it has a catastrophic forgetting issue). To see whether there is a simple fix, in Appendix G.7, we ablate whether an action regularizer pushing towards a pretrain policy similar to TD3+BC helps, but find it to hinder performance increase in other environments. IQL is overall much more stable than TD3, but improves much less during online finetuning than TD3+ODT. ODT can achieve good performance when pretrained on expert data, but struggles with datasets of lower quality, which validates our motivation. DDPG+ODT starts out well in the online finetuning stage but fails quickly, probably because DDPG is less stable compared to TD3."}, {"title": "4.2 Antmaze Environments", "content": "Environment and Dataset Setup. We further test on a harder version of the Maze2D environment in D4RL [19] where the pointmass is substituted by a robotic ant. We study six different variants, which are umaze, umaze-diverse, medium-play, medium-diverse, large-play and large-diverse.\nResults. Fig. 4 lists the results of each method on umaze and medium maze before and after online finetuning (see Appendix C for reward curves and Appendix B for results summary on large antmaze). TD3+ODT works the best on umaze and medium maze, and significantly outperforms TD3. This shows that RL gradients alone are not enough for offline-to-online RL of the decision transformer. Though TD3+ODT does not work on large maze, we found that IQL+ODT works decently well. However, we choose TD3+ODT in this work because IQL+ODT does not work well on the random datasets. This is probably because IQL aims to address the Out-Of-Distribution (OOD) estimation problem [28], which makes it better at utilizing offline data but worse at online exploration. See Appendix C for a detailed discussion and results. DDPG+ODT works worse than TD3+ODT but much better than baselines except IQL."}, {"title": "4.3 MuJoCo Environments", "content": "Environment and Dataset Setup. We further test on four widely recognized standard environments [58], which are the Hopper, Halfcheetah, Walker2d and Ant environment. For each environment, we study three different datasets: medium, medium-replay, and random. The first and second one contain trajectories of decent quality, while the last one is generated with a random agent."}, {"title": "5 Related Work", "content": "Online Finetuning of Decision Transformers. While there are many works on generalizing decision transformers (e.g., predicting waypoints [5], goal, or encoded future information instead of return-to-go [22, 5, 57, 36]), improving the architecture [37, 16, 53, 65] or addressing the overly-optimistic [46] or trajectory stitching issue [63]), there is surprisingly little work beyond online decision transformers that deals with online finetuning of decision transformers. There is some loosely related literature: MADT [31] proposes to finetune pretrained decision transformers with PPO. PDT [64] also studies online finetuning with the same training paradigm as ODT [74]. QDT [66] uses an offline RL"}, {"title": "Transformers as Backbone for RL", "content": "Having witnessed the impressive success of transformers in Computer Vision (CV) [17] and Natural Language Processing (NLP) [11], numerous works also studied the impact of transformers in RL either as a model for the agent [45, 38] or as a world model [39, 50]. However, a large portion of state-of-the-art work in RL is still based on simple Multi-Layer Perceptrons (MLPs) [35, 28]. This is largely because transformers are significantly harder to train and require extra effort [45], making their ability to better memorize long trajectories [42] harder to realize compared to MLPs. Further, there are works on using transformers as feature extractors for a trajectory [37, 45] and works that leverage the common sense of transformer-based Large Language Model's for RL priors [10, 9, 70]. In contrast, our work focuses on improving the new \u201cRL via Supervised learning\u201d (RvS) [7, 18] paradigm, aiming to merge this paradigm with the benefits of classic RL training."}, {"title": "Offline-to-Online RL", "content": "Offline-to-online RL bridges the gap between offline RL, which heavily depends on the quality of existing data while struggling with out-of-distribution policies, and online RL, which requires many interactions and is of low data efficiency. Mainstream offline-to-online RL methods include teacher-student [51, 6, 59, 72] and out-of-distribution handling (regularization [21, 29, 62], avoidance [28, 23], ensembles [2, 15, 24]). There are also works on pessimistic Q-value initialization [69], confidence bounds [26], and a mixture of offline and online training [56, 73]. However, all the aforementioned works are based on Q-learning and don't consider decision transformers."}, {"title": "6 Conclusion", "content": "In this paper, we point out an under-explored problem in the Decision Transformer (DT) community, i.e., online finetuning. To address online finetuning with a decision transformer, we examine the current state-of-the-art, online decision transformer, and point out an issue with low-reward, sub-optimal pretraining. To address the issue, we propose to mix TD3 gradients with decision transformer training. This combination permits to achieve better results in multiple testbeds. Our work is a complement to the current DT literature, and calls out a new aspect of improving decision transformers."}, {"title": "Limitations and Future Works", "content": "While our work theoretically analyzes an ODT issue, the conclusion relies on several assumptions which we expect to remove in future work. Empirically, in this work we propose a simple solution orthogonal to existing efforts like architecture improvements and predicting future information rather than return-to-go. To explore other ideas that could further improve online finetuning of decision transformers, next steps include the study of other environments and other ways to incorporate RL gradients into decision transformers. Other possible avenues for future research include testing our solution on image-based environments, and decreasing the additional computational cost compared to ODT (an analysis for the current time cost is provided in Appendix H)."}, {"title": "A Broader Societal Impacts", "content": "Our work generally helps automation of decision-making by improving the use of online interaction data of a pretrained decision transformer agent. While this effort improves the efficiency of decision-makers and has the potential to boost a variety of real-life applications such as robotics and resource allocation, it may also cause several negative social impacts, such as potential job losses, human de-skilling (making humans less capable of making decisions without AI), and misuse of technology (e.g., military)."}, {"title": "B Performance Summary", "content": "In this section, we summarize the average reward achieve by each method on different environments and datasets, where the result for Adroit is shown in Tab. 2, and the result for Antmaze is shown in Tab. 3. As the summary table for MuJoCo is already presented in Sec. 4, we show the reward curves in Fig. 6. For a more rigorous evaluation, we also report other metrics including the median, InterQuartile Mean (IQM) and optimality gap using the rliable [3] library. See Fig. 7 for details. Breakdown analysis for each environment can be downloaded by browsing to https://kaiyan289. github.io/assets/breakdown_rliable.rar."}, {"title": "C Why Do We Choose TD3 to Provide RL Gradients?", "content": "In this section, we provide an ablation analysis on which RL gradient fits the decision transformer architecture best. Fig. 8 illustrates the result of using a pure RL gradient for online finetuning of a pretrained decision transformer (for those RL algorithms with stochastic policy, we adopt the same"}, {"title": "D Why Our Critic Serves as an Average of Policies Generated by Different Context Lengths?", "content": "As we mentioned in Sec. 3.2, When updating a deterministic DT policy, the following loss is minimized:\n$\\sum_{t=1}^{T_{train}} ||DT (s_{0:t}, a_{0:t-1}, RTG_{0:t}, RTG= RTG_{real}, T = t) \u2013 a_t||^2$, (8)\nwhere Ttrain is the training context length and RTGreal is the real return-to-go."}, {"title": "E Mathematical Proofs", "content": "In this section, we will state the theoretical analysis summarized in Sec. 3.3 more rigorously. We will first provide an explanation on how the decision transformer improves its policy during online finetuning, linking it to an existing RL method in Sec. E.1 and Sec. E.2. We will then bound its performance in Sec. E.3."}, {"title": "E.1 Preliminaries", "content": "Advantage-Weighted Actor Critic (AWAC) [40] is an offline-to-online RL algorithm, where the replay buffer is filled with offline data during offline pretraining and then supplemented with online experience during online finetuning. AWAC uses standard Q-learning to train the critic $Q : |S| \u00d7 |A| \u2192 R$, and update the actor using weighted behavior cloning, where the weight is exponentiated advantage (i.e., $exp(\\frac{A(s,a)}{\u03bb})$ where x > 0 is some constant)."}, {"title": "E.2 Connection between Decision Transformer and AWAC", "content": "We denote \u1e9e as the underlying policy of the dataset, and PB as the distribution over states, actions or returns induced by B. Note such PB can be either discrete or continuous. By prior work [7], for decision transformer policy \u03c0DT, we have the following formula holds for any return-to-go RTG \u2208 R of the future trajectory:\n$\\pi_{DT} (a|s, RTG) = P(a|s, RTG) = \\frac{P_\u03b2(a|s)P_\u03b2(RTG|s, a)}{P_\u03b2(RTG|s)} = \\frac{P_\u03b2(a|s)}{P_\u03b2(RTG|s)} \\frac{P_\u03b2(RTG|s, a)}{\u03b2(a|s)}$ (10)\nBased on Eq. (10), we have the following lemma:"}, {"title": "E.3 Failure of ODT Policy Update with Low Quality Data", "content": "As mentioned in Sec. E.2, we study the performance of decision transformers in online finetuning when PB(RTGs) and P3(RTG|s, a) is small. Specially, in this section, r(s, a) is not a reward function, but a reward distribution conditioned on (s,a); ri ~ r(si, ai) is the reward obtained on i-th step. Such notation takes the noise of reward into consideration and forms a more general framework. Also, as the discrete or continuous property of \u1e9e is important in this section, we will use Pr\u1e9e to represent probability mass (for discrete distribution or cumulative distribution for continuous distribution) and ps to represent probability density (for probability density function for continuous distribution).\nBy prior work [7], we have the following performance bound tight up to a constant factor for decision transformer for every iteration of updates:\nTheorem E.1. For any MDP with transition function p(\u00b7|s, a) and reward random variable r(s, a), and any condition function f, assume the following holds:\n\u2022 Return coverage: $P_\u03b2(g = f(s_1)|s_1) \u2265 \u03b1_f$ for any initial state s1;\n\u2022 Near determinism: for any state-action pair (s, a), \u2203 s' such that $Pr(s'|s, a) \u2265 1 \u2013 \u03f5$, and \u2203ro(s, a) such that $Pr(r(s, a) = ro(s, a)) \u2265 1 \u2013 \u03f5$;\n\u2022 Consistency of f: f(s) = f(s') + r for all s when transiting to next state s'.\nThen we have\n|E_{s_1\u223cP_{ini}} [f(s_1)] - E_{\u03c4=(s_1,a_1,\u2026\u2026\u2026,s_H,a_H)\u223c\u03c0_{DT}(.\\s,f(s))}  \\sum_{i=1}^H r(s_i,a_i)| < (\\frac{4}{\u03b1_f} +2) \u03f5 \\frac{H^2}{8}$, (12)\nwhere af > 0, \u20ac > 0 are constants, Pini is the initial state distribution, and H is the horizon of the MDP. \u03c0DT is the learned policy by Eq. (10).\nProof. See Brandfonbrener et al. [7].\nIn our case, we define f(s) as follows:\nDefinition E.2. f (81) = RTGeval for all initial states 81, f(si+1) = f(si) \u2013 ri for the (i + 1)-th step following i-th step (i \u2208 {1, 2, . . ., T \u2013 1})."}, {"title": "F Experimental Details", "content": "F.1 Environment and Dataset Details\nF.1.1 Single-State MDP\nThe single-state MDP studied in Sec. 3.1 motivates why RL gradients are useful for online finetuning. It has a single state, a single action a \u2208 [\u22121, 1], and a reward function r(a) = (a + 1)\u00b2 if a \u2264 0 and r(a) = 1 - 2a otherwise.\nDatasets. The dataset has a size of 128, with 100 actions uniformly sampled in (-1,0.95), and the remaining 28 actions uniformly sampled in (0.5, 1). The dataset is designed to conceal the reward peak in the middle. DDPG and ODT+DDPG successfully recognized the reward peak but ODT failed."}, {"title": "F.1.2 Adroit Environments", "content": "Environments. Adroit is a set of more difficult benchmark than Mujoco in D4RL, and is becoming increasingly popular in recent offline and offline-to-online RL works [28, 23]. We test four environments in adroit in our experiments: pen, hammer, door and relocate. Fig. 15 shows an illustration of the four environments.\n1. Pen. Pen is a locomotion environment where the agent needs to control a robotic hand to manipulate a pen, such that its orientation matches the target. It has a 24-dimensional action space, each of which controls a joint on the wrist or fingers. The state space is 45-dimensional, which contains the pose of the palm, the angular position of the joints, and the pose of the target and current pen.\n2. Hammer. Hammer is an environment where the agent needs to control a robotic hand to pick up a hammer and use it to drive a nail into a board. The action space is 26-dimensional, each of which corresponds to a joint on the hand. The state space is 46-dimensional, which describes the angular position of the fingers, the pose of the palm, and the status of hammer and nail.\n3. Door. In the door environment, the agent needs to use a robotic hand to open a door by undoing the latch and swinging it. The environment has a 28-dimensional action space, which are the absolute angular positions of the hand joints. It also has 39-dimensional observation space which describes each joint, the pose of the palm, and the door with its latch.\n4. Relocate. In the relocate environment, the agent needs to control a robotic hand to move a ball from its initial location towards a goal, both of which are randomized in the environment. The environment has a 30-dimensional action space which describes the angular position of the joints on the hand, and a 39-dimensional space which describes the hand as well as the ball and target."}, {"title": "F.1.3 Antmaze Environments", "content": "Environments. Antmaze is a more difficult version of Maze2D", "antmaze": "Umaze", "Umaze": "Medium", "Large": "escribes the size of the maze (see Fig. 16 for an illustration)", "Diverse": "nd \u201cPlay\u201d describes the type of the dataset. More specifically"}, {"Diverse": "eans that in the offline dataset", "Play\" means that the goal is generated by a handcraft design. \u201cUmaze\u201d without suffix is the simplest environment where both the starting point and the goal are fixed.\nDatasets. Similar to Adroit and MuJoCo, we test our method on datasets provided by D4RL. Tab. 5 shows the size and normalized reward of each dataset. Note, following IQL [28": "and CQL [29", "shaping": "subtracting from all rewards in the dataset and environment the value 1 during training of both our method and baselines to provide denser reward signal for all antmaze environments. However"}]}