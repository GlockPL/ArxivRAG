{"title": "TempoGPT: Enhancing Temporal Reasoning via Quantizing Embedding", "authors": ["Haochuan Zhang", "Chunhua Yang", "Jie Han", "Liyang Qin", "Xiaoli Wang"], "abstract": "Multi-modal language model has made advanced progress in vision and audio, but still faces significant challenges in dealing with complex reasoning tasks in the time series domain. The reasons are twofold. First, labels for multi-modal time series data are coarse and devoid of analysis or reasoning processes. Training with these data cannot improve the model\u2019s reasoning capabilities. Second, due to the lack of precise tokenization in processing time series, the representation patterns for temporal and textual information are inconsistent, which hampers the effectiveness of multi-modal alignment. To address these challenges, we propose a multi-modal time series data construction approach and a multi-modal time series language model (TLM), TempoGPT. Specially, we construct multi-modal data for complex reasoning tasks by analyzing the variable-system relationships within a white-box system. Additionally, proposed TempoGPT achieves consistent representation between temporal and textual information by quantizing temporal embeddings, where temporal embeddings are quantized into a series of discrete tokens using a predefined codebook; subsequently, a shared embedding layer processes both temporal and textual tokens. Extensive experiments demonstrate that TempoGPT accurately perceives temporal information, logically infers conclusions, and achieves state-of-the-art in the constructed complex time series reasoning tasks. Moreover, we quantitatively demonstrate the effectiveness of quantizing temporal embeddings in enhancing multi-modal alignment and the reasoning capabilities of TLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-modal language models can leverage the robust semantic understanding and reasoning capabilities of large language models (LLMs) to facilitate the processing of diverse modalities and support multiple downstream tasks [1\u20135]. Time series data, a ubiquitous form of modal information, is essential in numerous domains, such as anomaly detection [6], medical diagnosis [7], and financial economics [8]. Consequently, numerous researchers aim to take use of LLMs to improve the effectiveness of time series analysis [9\u201311].\nHowever, as illustrated in Figure 1, when introducing the reasoning capabilities of LLMs to the time series domain, while multimodal time series language models (TLMs) perform well on trend related tasks, they underperform in complex reasoning tasks and struggle to effectively utilize LLMs\u2019 reasoning capabilities [12]. Despite extensive research on TLMs and numerous efforts to leverage advanced model frameworks and training methods, the performance of TLMs in time series reasoning has yet to show significantly improvement [13]. This indicates that it is necessary to identify and address the fundamental issues underlying the limitations of TLMs in complex reasoning tasks.\nBy analyzing the crucial technologies in constructing TLMs,\nthere are two significant challenges. 1) Multi-modal data. The community lacks high-quality multi-modal time series data as the foundation for training TLMs [14\u201317]. Most datasets use basic classification labels, such as diagnoses like \"sinus rhythm\" and \"sinus irregularity\" in electrocardiogram (ECG) data [18]. Those datasets lack a thorough analysis of complex relationships, such as those between variables and between variables and physical systems [19]. Training with those data turns TLMs into classifiers, rather than improving their semantic understanding and reasoning capabilities. 2) Multi-modal alignment. As shown in the left side of Figure 2, unlike text sequences where individual words can be tokenized into discrete tokens [20], temporal representation methods lack precise tokenization techniques due to the absence of inherent semantic information in time series data points and the infinite variability of patches [21]. Those limitations result in inconsistent representation patterns for textual and temporal information in TLMs. As Figure 1 illustrates, while this issue has minimal impact on basic time series analysis tasks for TLMs, it poses challenges when strong multi-modal alignment and reasoning capabilities are required.\nTo effectively address the challenges faced by TLMs in complex reasoning tasks, it is crucial to tackle the aforementioned issues. Considering the significance of multi-modal data involving com plex analysis and reasoning processes in improving the reasoning capabilities of TLMs, we advocate for generating time series data within a white-box physical system. In this system, the relationships between variables and the system can be systematically analyzed using rule-based methods and human-in-the-loop strategies, thereby facilitating the creation of comprehensive multi-modal data. Furthermore, considering the potential risks posed by inconsistent information representation patterns to the reasoning capabilities of TLMs, as shown in the right side of Figure 2, we advocate for quantizing temporal embeddings into discrete tokens. This enables a precise tokenization and facilitates the capture of inherent semantic information of time series [22, 23]. These discrete tokens can be integrated into the vocabulary of LLMs and subsequently processed alongside textual tokens by a shared embedding layer, thus achieving a consistent representation pattern with textual information. In this way, temporal information can be better aligned with textual information, thereby enhancing the reasoning capabilities of TLMs.\nBased on the above motivations, this paper presents a multi modal time series data construction method and a multi-modal time series language model, TempoGPT. Our contributions are primarily reflected in the following aspects:\n\u2022 Multi-Modal Time Series Data. We propose a novel multimodal data construct approach, which enables effective analysis of variable-system relationships and facilitates the creation of multi-modal data for time series reasoning tasks.\n\u2022 Time Series Language Model. We design a multi-modal time series language model, TempoGPT. TempoGPT quantizes temporal embeddings into discrete tokens and expands the embedding layer to process both temporal and textual tokens, achieving a consistent representation of temporal and textual information.\n\u2022 Time Series Reasoning. Experimentally, TempoGPT achieves state-of-the-art on constructed time series reasoning tasks. Additionally, we extensively analyze the factors that influence the time series reasoning capabilities of TLMs and demonstrate the effectiveness of quantizing temporal embeddings in enhancing multi-modal alignment and reasoning capabilities."}, {"title": "2 RELATED WORK", "content": "2.1 Time Series Language Model\nThe remarkable capabilities of LLMs attract numerous researchers to explore how LLMs\u2019 reasoning capabilities can enhance the effectiveness of time series analysis [24\u201327].\nInitially, LLMs are utilized as innovative tools for specific time series analysis tasks, such as forecasting and classification, by designing specialized encoders and output heads [28\u201330]. However, these studies do not effectively utilize the natural language processing and reasoning capabilities of LLMs [31, 32]. Subsequently, some studies attempt to utilize the reasoning capabilities of LLMs through alignment or prompt-based methods, without altering the output heads of the LLMs [33\u201335]. Although prompt-based methods do not add extra components to LLMs, they struggle to scale to multivariate time series analysis due to limit input tokens. Furthermore, challenged with coarse multi-modal data labels, both alignment and prompt-based methods are limited to fundamental time series analysis tasks. Recent studies successfully utilize TLMs to analyze fundamental characteristics of time series [16, 17], such as trend and seasonality. However, research on TLMs in complex time series reasoning tasks is insufficient, and their performance is currently suboptimal [12].\n2.2 Multi-modal Alignment\nMulti-modal alignment methods, such as BLIP-2 [36] and CLIP [37], are widely studied in the visual domain, and demonstrate outstanding performance [38\u201340]. By emulating alignment methods from the visual domain, researchers achieve notable effectiveness in specific time series analysis tasks. However, they struggle to scale these methods to complex reasoning tasks, which place high demands on multi-modal alignment and the reasoning capabilities of TLMs [12, 41]. Observing inconsistencies in the representation patterns between temporal and textual information, some studies utilize quantization-based methods to tokenize temporal embeddings and unify the representation pattern [22, 42]. These studies primarily concentrate on specific tasks in time series analysis, where the extraction of temporal information is more critical than multi-modal alignment, resulting in insignificant benefits from a consistent representation pattern [43]. However, for the time series reasoning tasks that require strong alignment between temporal and textual information, current studies seldom explore whether inconsistent representation patterns hinder TLMs\u2019 reasoning capabilities."}, {"title": "3 MULTI-MODAL DATA GENERATION", "content": "Considering the scarcity of multi-modal time series data and the insufficient attention to the complex analysis and reasoning pro cess in time series, we suggest simulating multi-modal data within a white-box system with known mechanisms, generating labels through rule-based and human-in-the-loop approaches, and using ChatGPT [44] to enhance the diversity of the data further ultimately. In this manner, the relationships between time series and physical systems can be effectively explored and analyzed.\nInitially, as shown on the right side of Figure 3, a basic linear circuit is constructed. The circuit includes two AC voltage sources with the same maximum amplitude but different phases. There are three loads within the circuit: Load 1 is connected in series in the main loop, while Load 2 and Load 3 are connected in parallel and then in series with Load 1. The voltages across Voltage Source 1 and 2, Load 1, 2, and 3, as well as the current in the main loop, are observable. Additionally, to simulate abnormal operating conditions of the circuit, intrinsic parameters of the components within the circuit are randomly adjusted within a specific range, such as modifying the maximum amplitude of the voltage sources and the resistance of the loads. Time series data, consisting of six electrical variables, are generated based on the constructed electrical system: the voltage across Voltage Source 1, the voltage across Voltage Source 2, the total electromotive force of the circuit (sum of the voltages of Voltage Source 1 and 2), the voltage across Load 1, the voltage across Loads 2 and 3, and the current in the main loop of the circuit.\nSubsequently, we develop a series of templates for textual data to be used in both pre-training and fine-tuning stages. Textual data are derived from these templates by analyzing the generated time series data with rule-based and human-in-the-loop methods. The pre-training data comprise two main components: anomaly information related to electronic components and temporal information concerning the aforementioned six electrical variables. In the fine tuning stage, we implement a chain-of-thought (CoT) [45] approach to simulate the human reasoning process and develop five types of"}, {"title": "4 TEMPOGPT", "content": "4.1 Architecture\nTempoGPT is constructed to achieve a consistent representation pattern between temporal and textual information, and to accomplish complex time series reasoning tasks. As shown in Figure 4, TempoGPT involves three key technologies: 1. quantization encoding; 2. shared embedding layer. 3. large language model.\n4.1.1 Quantization Encoding. The purpose of quantization encoding is to tokenize time series into discrete tokens utilizing a predefined encoder and temporal codebook. The encoder, which is based on a weight-sharing 1-D convolutional network, transforms the time series into high-dimensional temporal embeddings. Furthermore, we employ patching and channel-independence techniques to enable the encoder to better represent the temporal information [21]. The codebook consists of a fixed-size set of codewords, where each codeword is represented as a vector in the discrete latent space and is associated with a corresponding token (index) [46]. Each of the aforementioned temporal embeddings is then mapped to the nearest discrete codeword based on the predefined codebook, thereby forming a temporal token. This would provide a precise tokenizer for the processing of time series. Notably, the encoder and temporal codebook in the quantization component can be developed by training a Vector Quantised-Variational AutoEncoder (VQ-VAE), as detailed in [23]. In other words, we load the encoder and codebook of VQ-VAE in the quantization component and freeze these parameters during the subsequent training process of TempoGPT.\nSpecifically, for a time series $X = (x_1, x_2, ..., x_M) \\in R^{T \\times D}$ with T timestamps and D variables, we firstly encode it into $d_E$-dimensional temporal embeddings $X_E = (x_1^E, x_2^E, ..., x_P^E) \\in R^{P \\times D \\times d_E}$, where P is the number of patches of each variable. Subsequently, each embedding vector in $X_E$ is converted into a temporal token using the predefined codebook, forming $X_I \\in \\{<0>,<1>,<2>, ..., <K>\\}$.\nThrough this process, quantization encoding effectively transforms the input from each patch of every variable into a temporal token.\n4.1.2 Shared Embedding Layer. The shared embedding layer functions to extend the LLMs\u2019 embedding layer to process both textual and temporal tokens simultaneously, thereby ensuring a consistent representation pattern for textual and temporal information. Temporal tokens are added to the existing vocabulary of LLMs, and the word embedding matrix is correspondingly expanded to ensure compatibility with the vocabulary."}, {"title": "4 TEMPOGPT", "content": "Assuming the length of the original vocabulary $V_0$ of the LLM is $|V_0|$, the vocabulary $V_1$, consisting of additional temporal tokens and special tokens, has a length of $|V_1|$, and $V_0 \\cap V_1 = \\emptyset$. The original vocabulary $V_0$ is expanded by merging it with $V_1$ to ultimately form $V_2$.\n$V_2 = V_0 \\cup V_1$.  (1)\nWhile expanding the vocabulary, it is necessary to simultaneously expand the corresponding word embedding matrix to ensure that the dimensions of the vocabulary and the embedding matrix are compatible. Assume that the word embedding matrices corresponding to $V_0$, $V_1$ and $V_2$ are $W_0 \\in R^{|V_0| \\times d}$, $W_1 \\in R^{|V_1| \\times d}$ and $W_2 \\in R^{|V_2| \\times d}$, respectively, where d is the dimensions of word embedding. According to Eq.2, expand $W_0$ to derive $W_2$.\n$W_2 = [W_0; W_1]$.  (2)\nSubsequently, the process of representing information is the same for both text and time series data: initial textual/temporal data is converted into textual/temporal tokens, each of which is mapped to a specific textual/temporal embedding vector through the shared embedding layer. In this way, TempoGPT can represent temporal information by combining a finite set of discrete embedding vectors. Adjusting these finite embedding vectors allows for better alignment of temporal and textual information, which is much more straightforward than representing temporal information in the continuous embedding space.\n4.1.3 Large Language Models. Finally, the constructed textual and temporal embedding vectors are fed into the main body of LLMs. LLMs further integrate and process temporal and textual information, ultimately generating the corresponding text response. TempoGPT aims to explore whether a consistent pattern for temporal and textual representation can enhance performance on time series reasoning tasks, positioning it as a general model framework. Consequently, we study five types of base models in TempoGPT, including GPT-2 (125M) [47], LLaMA-3.2-1B (1.2B) [48], LLaMA 3.2-3B (3.2B) [49], tiny-llama (1.1B) [50], and Phi-2 (2.7B) [51].\n4.2 Training\nWe train TempoGPT on our constructed electrical multi-modal time series dataset through pre-training and fine-tuning stages. The primary goal of TempoGPT during pre-training stage is to align temporal with textual information, while its fine-tuning stage focuses on enabling it to follow user instructions and process complex time series reasoning tasks. We do not discuss the training details of VQ-VAE here, assuming it has already been trained on time series data. For more details, please refer to [23].\n4.2.1 Pre-training. During the pre-training stage, only the parameters $\\Theta$ of the embedding layer in TempoGPT are adjusted, while the parameters of the remaining parts are frozen. This approach helps preserve LLMs\u2019 inherent capabilities while aligning textual information with temporal information. A sample in the pre-training dataset consists of the pair (x, y), where x is an input time series with T timestamps and D variables, and y is the corresponding text labels, as shown in Figure 3. The probability of the"}, {"title": "4 TEMPOGPT", "content": "target response y during the pre-training stage can be calculated as follows:\n$P(y|x) = \\prod_{i=1}^{|y|} P(y_i | y_{<i}; x)$.  (3)\nwhere $y_i$ indicates the i-th token in y; |y| is the number of tokens in y; $y_{<i}$ indicates the tokens before $y_i$ in y.\n4.2.2 Fine-tuning. During the fine-tuning stage, the embedding layer parameters $\\Theta$ and the main body of LLMs\u2019 parameters $\\Phi$ in TempoGPT are adjusted, which can help TempoGPT better adapt to multiple downstream tasks. Notably, when the base model is GPT-2, we employ full fine-tuning; otherwise, we utilize LoRA (Low Rank Adaptation) for fine-tuning [52]. Additionally, to enhance the instruction-following capabilities of TempoGPT during the fine tuning stage, the data sample is constructed as ((x,q),y), where q represents the user command, randomly selected from the five types of time series reasoning tasks. The probability of the target response y during the fine-tuning stage can be calculated according to Eq.4\n$P(y|x; q) = \\prod_{i=1}^{|y|} P(y_i | y_{<i}; x; q)$.  (4)"}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate the time series reasoning capabilities of TempoGPT by addressing the following research questions (RQs):\n\u2022 RQ1. How does TempoGPT perform in time series reasoning tasks?\n\u2022 RQ2. Do TempoGPT\u2019s responses exhibit logical coherence?\n\u2022 RQ3. What impact do our innovative components have on TLMs in time series reasoning tasks?\n\u2022 RQ4. What other factors influence the performance of TLMs on time series reasoning tasks?\n5.1 Experimental Setup\n5.1.1 Dataset. To explore TempoGPT\u2019s time series reasoning capabilities, we train it using the constructed multi-modal electrical dataset and evaluate it on five types of downstream tasks: 1) trend analysis; 2) trend forecast; 3) fault judgement; 4) fault diagnosis; 5) fault analysis. Among them, the first two tasks are mainly related to trend, primarily involving the analysis of time series; the latter three tasks are mainly related to reasoning, involving complex reasoning processes. Detailed descriptions of those data can be found in Section 3.\n5.1.2 Evaluation. To systematically evaluate TLMs\u2019 performance on time series reasoning tasks, we define three metrics to evaluate their capabilities:\n\u2022 Conclusion Accuracy (CA). CA provides a convenient method to evaluate the time series reasoning capabilities by calculating the proportion of correct conclusions in the responses of TLMs.\n\u2022 Logical Reasoning Accuracy (LRA). Compared to CA, LRA offers a more rigorous evaluation. It only considers responses that accurately perceive time series information and derive correct conclusions from this perceived data as logically correct; all other responses are deemed incorrect.\n\u2022 Deception Rate (DR). DR, which measures the proportion of responses that are conclusion-correct yet logically incorrect, serves as a complement to LRA.\nWe calculate CA through string matching, while LRA and DR are calculated via manual evaluation. Among the three evaluation metrics mentioned, higher values for CA and LRA are preferable, whereas a lower DR is desirable. For more details about evaluation, please refer to Appendix B.1.\n5.1.3 Baseline. Depending on the method of representing time series data, our baseline methods are divided into two categories:\n\u2022 Based on Textual Prompt. We choose GPT-3.5 [53] and GPT-4 [54] as representatives of this approach, which directly converts time series data into textual prompt for input.\n\u2022 Based on Continuous Embedding. These methods represent temporal information within a continuous embedding space, thereby aligning temporal and textual information. Due to the current limitations of research on TLMs in time series reasoning tasks, we adopt the time series encoding methods of advanced TLMs as our baseline, including 1) Linear-based. Encoding time series data with linear layers (e.g., GPT4MTS [55]); 2) Attention based. Encoding time series data with multi-head self-attention (e.g., Time-LLM [29]); 3) MLP-based. Encoding time series data with Multilayer Perceptron (MLP) (e.g., ChatTS [17])."}, {"title": "5 EXPERIMENTS", "content": "5.1.4 Implementation. We train all models in 4 \u00d7 V100 GPUs. For TempoGPT based on GPT-2, in the pre-training stage, models are trained for approximately 6.6K steps with a learning rate of 1e-3 and a batch size of 24, which takes 1 hour; In the fine-tuning stage, the models are trained for approximately 7.2K steps with a learning rate of 1e-4 and a batch size of 12, which takes 30 minutes.\n5.2 Time Series Reasoning Performance (RQ1)\nAs shown in Table 1, to evaluate the performance of TLMs in time series reasoning, we test these models across five types of time series reasoning tasks, comprising a total of 500 test questions, and evaluate them by conclusion accuracy.\nFor methods based on textual prompts, owing to the challenges posed by multiple variables and extensive redundant tokens, both GPT-3.5 and GPT-4 exhibit suboptimal performance in trend analy sis and trend forecast tasks. We did not evaluate GPT-3.5 and GPT-4 on the remaining tasks, as LLMs face difficulties in zero-shot han dling of relationships between temporal information and physical systems. For TempoGPT, compare to TLMs based on continuous embedding, we only quantize continuous temporal embeddings to achieve consistent representation between textual and temporal information. As a result, regardless of the base model employed, the performance of TempoGPT surpasses TLMs based on continuous embedding.\nNotably, tasks related to trends focus more on mining informa tion from time series, while tasks involving temporal information and physical systems involve complex reasoning processes. Although TLMs based on continuous embedding excellently handle the fundamental time series analysis tasks related to trend, they perform poorly when dealing with the relationships between variables and physical systems. This indicates ineffective alignment between temporal information and textual information. Therefore, quantiz ing temporal embeddings to achieve a consistent representation of temporal and textual information enables better multi-modal alignment for TLMs, thereby enhancing the time series reasoning capabilities of TLMs.\n5.3 Study of Logical Reasoning (RQ2)\nMerely determining the correctness of conclusion is insufficient for effectively evaluating the reasoning capabilities of TLMs. Therefore, in this section, we systematically evaluate TLMs from both qualitative and quantitative perspectives to determine whether they accurately perceive temporal information and logically reason to draw correct conclusions.\n5.3.1 Qualitative Evaluation. As shown in Figure 5, we list some examples of time series reasoning quiz for TempoGPT (GPT-2) and Baseline (GPT-2 based on linear), the best-performing method based on GPT-2 and continuous embedding in Section 5.2. We can draw attention to the following two observations through interpreting and comparing these responses: 1) Temporal Information Perception. The model must perceive the provided time series data. However, the baseline\u2019s perception of the time series data often differs significantly from the actual values. Although quantizing time series inevitably results in some loss of temporal information for TempoGPT, it still delivers a more precise perception of temporal information, closely approximating the actual values. 2) Logical Reasoning. The perception and logical reasoning capabilities influence the time series reasoning capabilities of TLMs. As shown on the left side of Figure 5, the baseline\u2019s incorrect perception of the voltage value for voltage source 1 leads to an incorrect conclusion. Additionally, as the diagram in the middle and right of Figure 5 illustrates, although the baseline model generates a correct conclusion, it incorrectly perceives the temporal information, and the logic is inconsistent. Conversely, TempoGPT perceives data with greater accuracy and uses this information to reason logically and draw correct conclusions.\n5.3.2 Quantitative evaluation. As tasks related to trends primarily concentrate on extracting information from time series rather than engaging in complex reasoning processes, we select the re maining tasks for quantitative evaluation. We randomly choose 25 test samples for each task, totaling 75 samples, and evaluate the responses of TLMs by LRA and DR. The statistical results for LRA and DR are shown in Table 2.\nTempoGPT considerably surpasses all TLMs based on continuous embeddings in terms of average LRA, demonstrating its superior"}, {"title": "5 EXPERIMENTS", "content": "ability to derive correct conclusions through its inherent logical reasoning capabilities. Furthermore, TempoGPT exhibits low DR values, indicating a reduced likelihood of random guessing and a stronger reliance on logical reasoning to provide accurate responses. In contrast, TLMs based on continuous embedding show DR values exceeding 20% in certain tasks, suggesting that even if their conclusions are correct, there is a high probability that their responses are logically inconsistent. As a result, TempoGPT more effectively aligns temporal and textual information, harnessing the reasoning capabilities of LLMs to tackle complex time series reasoning tasks.\n5.4 Ablation Study (RQ3)\nIn this section, we primarily investigate whether the quantization component and pre-training have a positive effect on the time series reasoning capabilities of TLMs.\nTo investigate the influence of quantization on the time series reasoning capabilities of TLMs, we train VQ-VAE using linear, self attention, and MLP as core components to generate the corresponding temporal codebook. Subsequently, TLMs can tokenize temporal embeddings into discrete tokens using this temporal codebook. We also expand the embedding matrix to accommodate these temporal tokens, thus completing the quantization of the time series data. Since TLMs perform well on trend-related tasks, we only test the TLMs before and after adding quantization on three reasoning related tasks. The experimental results are shown in Table 3. With few exceptions, it is evident that the introduction of quantization generally yields significant improvements for nearly all TLMs based on continuous embeddings, with some metrics exhibiting improvement effects surpassing 100%. This demonstrates the effectiveness of the quantization component, showing that quantizing temporal embeddings to achieve consist representation pattern with textual information can significantly enhance the time series reasoning capabilities of TLMs.\nMeanwhile, to further explore the impact of pre-training and quantization on the time series reasoning capabilities of TLMs, as shown in the Figure 6, we present CA in tasks related to trend and reasoning during the fine-tuning stage for TempoGPT (GPT2) and its two variants. After removing pre-training and quantization,"}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In response to the challenges of multi-modal time series language models (TLMs) in complex reasoning tasks, we propose a multimodal time series data construction method and design a multimodal time series language model, TempoGPT. Specially, we construct complex time series reasoning data by exploring the relationships between time series and physical systems within a white-box system, while also release an electrical multi-modal time series dataset. Additionally, proposed TempoGPT tokenize time series into temporal tokens via quantization encoding, then employs a shared embedding layer to process both temporal and textual tokens, achieving a consistent representation of temporal and textual information. Experimentally, TempoGPT exhibits robust perception and reasoning capabilities, achieving state-of-the-art in the constructed time series reasoning tasks. Under systematic analysis, TempoGPT demonstrates outstanding framework generality, effectively enhancing the multi-modal alignment and time series reasoning capabilities of TLMs through the quantization of temporal embeddings. In the future, we will explore systematic time series reasoning benchmark, involving more complex time series data and challenging downstream tasks, requiring TLMs to have robust temporal perception and reasoning capabilities."}]}