{"title": "FedMobileAgent: Training Mobile Agents Using Decentralized Self-Sourced Data from Diverse Users", "authors": ["Wenhao Wang", "Zijie Yu", "William Liu", "Rui Ye", "Tian Jin", "Siheng Chen", "Yanfeng Wang"], "abstract": "The advancement of mobile agents has opened new opportunities for automating tasks on mobile devices. Training these agents requires large-scale high-quality data, which is costly using human labor. Given the vast number of mobile phone users worldwide, if automated data collection from them is feasible, the resulting data volume and the subsequently trained mobile agents could reach unprecedented levels. Nevertheless, two major challenges arise: (1) extracting high-level and low-level user instructions without involving human and (2) utilizing distributed data from diverse users while preserving privacy. To tackle these challenges, we propose FedMobileAgent, a collaborative framework that trains mobile agents using self-sourced data from diverse users. Specifically, it includes two techniques. First, we propose Auto-Annotation, which enables the automatic collection of high-quality datasets during users' routine phone usage with minimal cost. Second, we introduce adapted aggregation to improve federated training of mobile agents on non-IID user data, by incorporating both episode- and step-level distributions. In distributed settings, FedMobileAgent achieves performance comparable to centralized human-annotated models at less than 0.02% of the cost, highlighting its potential for real-world applications.", "sections": [{"title": "1 Introduction", "content": "Mobile agents (Bai et al., 2024; Wang et al., 2024b) have experienced significant advancements, propelled by recent progress in Vision-Language Models (VLMs). Mobile agents, which are designed to simulate human mobile phone usage behavior, can automate complex tasks on mobile phones, saving tremendous human labor and change everyday lives. Compared to non-agent solutions, mobile agents offer significantly better adaptability and generalizability, enabling them to effectively handle various mobile environments and operation scenarios (Zhang et al., 2023).\nThe training of mobile agents heavily depends on large-scale, high-quality datasets (Chai et al., 2024; Song et al., 2024; Lu et al., 2024a). To build such datasets, existing approaches rely on centralized data collection followed by human annotation, resulting in high costs and limited scalability. To achieve large-scale data acquisition more efficiently, a paradigm shift from centralized to distributed data collection is necessary, enabling diverse users to participate in data contribution. Additionally, replacing human annotation with automatic annotation is crucial for efficiently processing the vast amount of collected data, allowing direct data sourcing from real user interactions.\nIn this work, we aim to automatically construct high-quality datasets from diverse users' routine phone usage in a decentralized manner. However, two technical challenges remain:\n1. Although the users' phone usage provides real-world screenshots and actions, it is difficult to extract the real intentions behind the actions in natural language;\n2. Data collected from a single user is both limited in scale and privacy-sensitive. The challenge lies in how to utilize distributed data from diverse users to boost performance while protecting privacy.\nTo tackle these challenges, we propose FedMobileAgent, a collaborative learning framework that trains mobile agents using automatically collected user data from daily phone interactions while preserving users' privacy. Specifically, FedMobileAgent involves two novel techniques.\nFirst, we propose Auto-Annotation, an automated method for data collection and annotation that leverages locally deployed VLMs to annotate"}, {"title": "2 Formulating Mobile Agents Trained on Distributed Auto-Annotated User Data", "content": "task instructions. The key technical innovation lies in combining step-wise low-level instruction breakdowns with episode-wise summarization, allowing even small local VLMs to better understand the user's intent. The step-wise description decomposes complex user instructions into simpler steps, enabling the VLM to comprehend and extract information more accurately. Meanwhile, the episode-wise summarization provides a global perspective on the entire task, generating a more comprehensive caption of the user's ultimate instruction. Compared with human annotation, Auto-Annotation can generate data of comparable quality with minimal cost requirement.\nSecond, to effectively utilize decentralized data from diverse users, we pioneer the integration of Federated Learning (FL) (Kairouz et al., 2021) to enable collaborative training of mobile agents, while ensuring rigorous user privacy protection. We further propose a novel aggregation method, termed adapted global aggregation, which integrates both episode- and step-level distributions to handle the two-level heterogeneity in diverse users' data, overcoming the limitations of traditional one-level aggregation methods (Karimireddy et al., 2021). Adapted aggregation adjusts the global aggregation weights using a weighted sum of episode and step counts for each client, thereby enhancing the performance of mobile agents trained in non-IID scenarios.\nExtensive experiments on Android Control and Android in the Wild (Li et al., 2024a; Rawles et al., 2023) datasets demonstrate that: (1) In FedMobileAgent, each participating user can obtain a mobile agent with strong performance at less than 0.01% of the original cost, (2) Auto-Annotation surpasses human-annotated data in performance while reducing annotation costs by 99%. (3) Adapted aggregation achieves a 5% relative improvement over FedAvg in non-IID scenarios. These promising results highlight the immense potential of our methods for real-world applications. To summarize, our contributions are as follows:\n1. We formulate the problem of self-sourced data collection from distributed mobile phone users and propose Auto-Annotation, an automatic data collection method, which achieves data quality comparable to human-annotated data at a significantly lower cost.\n2. We introduce FedMobileAgent, a collaborate"}, {"title": "2.1 Preliminaries", "content": "Data Composition. The mobile agent, powered by a VLM, simulates human users and completes tasks in a step-wise process. To train the core VLM, one data episode, denoted as \\(D\\), comprises multiple steps, each serving as a basic training unit. A step consists of three components: a task instruction \\(T\\), a screenshot \\(s\\), and a corresponding action \\(a\\). The data episode is defined as: \\(D = \\{\\(T, a_i, s_i\\) | i \\in [1,n]\\}\\), where \\((T, a_i, s_i)\\) represents the \\(i\\)-th step, with \\(a_i\\) and \\(s_i\\) denoting the action and screenshot respectively.\nTraditional Approach. As automating mobile phones is challenging, the training data for mobile agents places a strong emphasis on accuracy, which, at present, are all annotated by humans. Therefore, to obtain the three components of training data, traditional methods (Li et al., 2024a,b) rely on (1) human-annotated task instructions, followed by (2) centralized data collection and training.\nAs illustrated in Figure 1, traditional methods employ human outsourcing to generate human-annotated instructions using predefined rules or heuristics to enhance quality and diversity. These instructions are then executed in a centralized man-"}, {"title": "2.2 Primary Problem", "content": "ner to collect corresponding screenshots and actions. To ensure accuracy, each task is carried out step by step by human annotators, incurring substantial costs.\nTo address the high cost associated with traditional approach, we propose a new problem for promoting mobile agents, that is: How to harness distributed high-quality training data from diverse users without human annotation?\nAs shown in Figure 1, we further decompose our primary problem into two subordinate problems: (1) How to automatically collect data from individual users, while eliminating the need for expensive human annotation. (2) How to utilize distributed data from diverse users and optimize the mobile agent, while adhering to users' privacy constraints."}, {"title": "2.3 Automatic Data Annotation on User Side", "content": "During phone interactions, users spontaneously generate screenshots and actions, which are assumed to be easily collectible. However, users do not receive explicit natural language instructions and only act based on their underlying intentions, making task annotation challenging. Since users are generally reluctant to articulate their intentions and such intentions are non-trivial to infer, the first subordinate problem is how to automatically derive user intentions without human intervention, thereby constructing the training dataset. The objective is to learn a function \\(f(*)\\) that predicts user intention \\(T^*\\), an approximation of task instruction \\(T\\), based on \\(n\\) steps of actions and screenshots \\((a_i, s_i)\\):\n\\[T^* = f(\\{(a_i, s_i)\\}_{i=1}^n).\\]"}, {"title": "2.4 Distributed Training of Mobile Agents", "content": "The daily phone usage of an individual generates a limited dataset, constraining the performance of a mobile agent trained solely on it. Fortunately, with millions of users worldwide, there is immense potential to collaboratively train a mobile agent using their combined data, enabling virtually unlimited scalability. Nevertheless, directly sharing user data poses significant privacy risks, necessitating its use in a distributed manner. Therefore the second subordinate problem is to leverage distributed data collected from diverse users and optimize the performance of collaboratively trained mobile agents, particularly in non-IID scenarios.\nNew Heterogeneity Introduced. Conventional distributed learning models the distribution of individual steps while overlooking repetitive patterns"}, {"title": "3 Methodology", "content": "within the same episode. However, the actual distribution exists at two levels: among steps within an episode and among episodes within the dataset, which we define as the two-level distribution.\nSpecifically, let the dataset for user \\(u_k\\) contain \\(n^{epi}\\) episodes. The \\(j\\)-th episode, where \\(j \\in [1, n^{epi}]\\), consists of \\(n_i\\) steps under a fixed task instruction \\(T_0\\). Since \\(T_0\\) remains constant within an episode, its distribution simplifies to \\(P_{S,A}^{(j,k)} = P(j,k)\\). We define \\(P_{T,S,A}^{(k)}\\) as the marginal distribution of \\(T, S\\), and \\(A\\). \\(T, S\\), and \\(A\\) represent task, screenshot, and action spaces, respectively. The two-level distribution \\(P_{T,S,A}^{(k)}\\) is then given by:\n\\[P_{T,S,A}^{(k)} = \\sum_{j} P_{S,A}^{(j,k)} \\cdot P_{T}^{(k)}(T),\\]\nWe empirically observe that this two-level distribution gives rise to new heterogeneity issues that need to be addressed."}, {"title": "3.1 System Overview", "content": "The proposed FedMobileAgent is a collaborative learning framework for training mobile agents that automatically collects individual user data and achieves effective utilization of distributed data from diverse users. It mainly involve two procedures: Auto-Annotation and federated training with adapted global aggregation.\nFirst, as shown on the left of Figure 2, to reduce the high cost produced by human annotation, we propose Auto-Annotation to automatically build dataset on each participating user. We utilize screenshots and actions from distributed users' daily phone usage, eliminating the need for traditional centralized human execution. As the actual user instruction is unknown, we then employ a locally deployed VLM to automatically derive user instructions, thereby reducing the annotation cost. Inspired by human reasoning process, we decompose the entire task into multiple steps, allowing the VLM to better interpret user intention.\nSecond, depicted in the right part of Figure 2, users with auto-annotated data collaborate via federated learning to jointly train the target mobile agent with enhanced capabilities. In FedMobileAgent, we leverage federated learning as it achieves effective collaboration while ensuring privacy protection. Moreover, as federated mobile agents encounter new heterogeneity, we introduce"}, {"title": "3.2 Auto-Annotation: Automatic Mobile Data Collection from Daily Phone Usage", "content": "an adapted aggregation method that improves traditional approaches by considering both episode- and step-level distributions for each user.\nAuto-Annotation functions by automatically building datasets from users' phone usage. To annotate user instructions, the idea is to employ a local VLM to interpret user intentions step by step. Auto-Annotation is composed of two procedures: (1) During or after a user's daily phone usage, it incrementally generates low-level instructions, step by step. (2) These low-level instructions are then consolidated into a high-level instruction for the entire episode. Note: A low-level instruction is a specific, atomic directive that corresponds to an individual step in the task, whereas a high-level instruction represents the overall task objective.\nStep-Wise Instruction Description. We aim to annotate low-level instructions through step-wise description, a novel technique that decomposes complex user tasks into multiple steps. At each step, a local VLM is designed to automatically generate a low-level instruction in consideration of the current screenshot and converted action. This procedure enables the VLM to better interpret each step, leading to more accurate instructions.\nAs indicated by previous works (Zheng et al., 2024), some VLMs, such as GPT-4V (202, 2023), are unable to effectively identify the location of operations. Therefore, to make the original action interpretable to the VLM, also referred to as annotation model, we propose using a rule-based technique instead of using models Wang et al. (2024a) to transform the action (e.g., click x, y) into a natural language sentence. Specifically, for click actions, we align the exact point of click with a specific unit in the interface, based on XML files. If the unit contains text or a function call, we use the corresponding text or function name to formulate the action description. For other actions, such as navigate_home or wait, we slightly adjust the phrasing to improve clarity and comprehension. After this conversion, we obtain the converted action \\(A_i\\).\nWe subsequently employ the annotation model \\(M_a\\), which we refer to as the \"Descriptor\", to generate low-level instructions. At each step \\(i\\), the Descriptor is prompted to describe the user's intention, which serves as an approximation of the actual low-level instruction \\(T^{low}_i\\), based on the current"}, {"title": "3.3 Federated Training of Mobile Agents on Distributed User Data with Adapted Global Aggregation", "content": "screenshot \\(s_i\\) and the associate converted action \\(A_i\\):\n\\[Descriptor : (s_i, A_i) \\underset{M_a}{\\rightarrow} T^{low}_i.\\]\nThis step-wise process ensures that the information is more finely processed, allowing \\(M_a\\) to provide a more accurate description of each step. As a result, the VLM can later perform improved summarization. Details of our prompt templates can be found in Appendix D.3.\nEpisode-Wise Intention Summarization. Episode-wise intention summarization generates high-level instructions by summarizing the low-level instructions from all steps. The novelty is providing global context enriched with step-wise details, enabling the annotation model to effectively extract the user intention.\nTo provide global visual context for the annotation model \\(M_a\\), referred to as \"Summarizer\" in this step, we first concatenate all relevant screenshots into a single image \\(s_c\\), arranged in chronological order. Note that this approach (1) allows Summarizer to develop a comprehensive understanding of the entire task sequence, and (2) eliminates the need for multiple inferences by performing inference only once.\nFinally, we compile the concatenated screenshot \\(s_c\\) and the list of low-level instructions \\(\\{T^{low}_i\\}_{i=1}^n\\) into a single prompt and feed it into \\(M_a\\) to summarize the user's overall intention \\(T^{high}\\):\n\\[Summarizer : (s_c, \\{T^{low}_i\\}_{i=1}^n) \\underset{M_a}{\\rightarrow} T^{high}.\\]\nAs users do not provide explicit instructions, \\(T^{high}\\) serves as the high-level instruction, simulating what the user would convey if asking an agent to perform the same task.\nCombined with step-wise description, episode-wise summarization produces high-quality instructions comparable to human annotation, all while exclusively using locally deployed VLMs, thereby substantially reducing cost.\nDue to limited resources and phone usage time, the self-sourced data from a single user is restricted in scale, which in turn hinders the performance of locally trained mobile agents. To overcome this limitation, we introduce a novel collaborative framework within FedMobileAgent, that leverages federated learning to facilitate training on distributed"}, {"title": "4 Experiments", "content": "data from diverse users. Our key innovation is the introduction of an adapted aggregation method, designed to address the heterogeneity discussed in Section 2.4.\nWe consider a setup with \\(K\\) clients (users) and a central server, where all clients communicate with the server to collaboratively train a mobile agent without directly sharing private data. Let \\(D_k\\) denote the private dataset for client \\(u_k\\), which is automatically constructed using Auto-Annotation.\nLocal Agent Training. For each communication round \\(l\\), the server broadcasts the global model \\(M^{(l)}\\) to each available client \\(u_k \\in S^{l}\\), where \\(S^{l}\\) denotes the set of all participating clients at round \\(l\\). Then, each client \\(u_k\\) uses the global model to initialize its local model as: \\(M_k^{(l,0)} := M^{(l)}\\) where \\(M_k^{(l,r)}\\) denotes the local model at the \\(l\\)-th round and \\(0\\)-th training iteration. Starting from the initial local model \\(M_k^{(l,0)}\\), client \\(u_k\\) conducts multiple iterations of stochastic gradient descent (SGD) updates on its local dataset \\(D_k\\). At each iteration \\(r\\), with learning rate \\(\\eta\\), the local model is updated as follows:\n\\[M_k^{(l,r+1)} = M_k^{(l,r)} - \\eta \\nabla \\ell(M_k^{(l,r)}; T, s, a),\\]\nwhere \\(\\ell(M_k^{(l,r)}; t, s, a)\\) represents the computed loss based on a data sample \\((T, s, a)\\). After \\(T_k\\) iterations, the final local model is denoted as \\(M_k^{(l,T_k)}\\). When the local training finishes, each user \\(u_k\\) uploads \\(M_k^{(l,T_k)}\\) to the server, completing the local training of FedMobileAgent.\nAdapted Global Aggregation. In this step, the server updates global model by aggregating local"}, {"title": "4.1 Basic Setups", "content": "models, which is subsequently broadcast to available clients for the next round. Our innovation lies in adapting the aggregation strategy to accommodate the two-level structure of datasets used for training mobile agents, encompassing both step-level variations and episode-level distributions.\nTraditional FL methods, such as FedAvg and FedProx (Li et al., 2020), use the sample number of client as the aggregation weight. This insight has been proven successful over the past several years (Li et al., 2019, 2023). However, previous aggregation methods (Hsu et al., 2019; Reddi et al., 2020), which perform well on tasks such as image classification, overlook the two-level distribution discussed in Section 2.4. They treat all samples equally, regardless of whether they come from the same episode or not. This strategy fails in the case of heterogeneity.\nFor instance, consider two clients, each having 100 data samples (steps). One client's data contains 25 short, intensive episodes, while the other has 5 long episodes with repeated steps. The model trained on the first client's data is expected to behave better due to exposure to a more diverse set of tasks. Yet, in traditional FL, these models are merged equally, ignoring the critical performance distinction.\nTo address this limitation, we propose a novel aggregation technique adapting to the new scenario in FedMobileAgent. Within federated training of mobile agents, the data samples can be measured by both step count \\(n_k\\) and episode count \\(n_k^{epi}\\). \\(n_k^{epi}\\) is as well as, or even more important as it indicates how many tasks the agent has learned on. As \\(n_k\\) and \\(n_k\\) are measured in different scales, we empirically set a hyper-parameter \\(\\lambda\\) to align them, which is calculated around the average step length of all episodes. Then we redefine the sample count as \\(n_k^*\\) and reformulate the aggregation weight based on our adapted sample count \\(n_k^*\\); that is:\n\\[n_k^* := \\lambda n_k^{epi} + n_k, \\quad W_k = \\frac{n_k^*}{\\sum_{k \\in S^l} n_k^*},\\]\nwhere \\(W_k\\) denotes the weight for client \\(u_k\\) and \\(S^l\\) is the sampled participating clients. This design smoothly improves upon traditional aggregation and inherits its convergence property. When \\(\\lambda=0\\), it degrades to normal aggregation. Finally, the global model \\(M^{(l)}\\) is adaptively aggregated as:\n\\[M^{(l+1)} := \\sum_{k \\in S^l} W_k M_k^{(l)}.\\]\nThe adapted aggregation in FedMobileAgent balances both episode and step counts, achieving a better utilization of decentralized data from heterogeneous users."}, {"title": "5 Related Work", "content": "Models and Datasets. The base model for most of our experiments is Qwen2-VL-Instruct-7B (Wang et al., 2024c) (referred to as Qwen2-VL-7B for simplicity in our tables). We also compare results with models from the InternVL2 family (Chen et al., 2024) and API-based models, such as GPT-40, as discussed in Section 4.4 and Appendix A.6.\nAs user data is both costly to collect and sensitive to reveal, we do not collect actual user data by ourselves. Instead, we utilize two popular datasets from Google: Android Control (Li et al., 2024a) and Android in the Wild (AitW) (Rawles et al., 2023). These datasets are collected by crowdsourcing and serve well as a simulation of real-world mobile data.\nTraining Framework. We build upon the highly-starred VLM training framework, ms-swift (Zhao et al., 2024), and extend it into a repository capable of training federated VLMs. Our extension follows Metrics. To evaluate the performance of trained mobile agents, following Li et al. (2024a), we provide both step-wise and episode-wise accuracy (%) for high-level and low-level training. Step-wise accuracy is calculated by comparing the predicted action for current step with the ground truth action. If the TF-IDF similarity between the predicted and the ground truth action exceeds 0.9, we consider the model's prediction to be accurate. When all steps in an episode are predicted correctly, this episode is considered correct, thereby yielding the episode-wise accuracy.\nWe also compare the annotation costs between our methods and the baselines. The cost of a single human-annotated sample is derived from a Refuel-AI technical report. We use multiple RTX 4090 GPUs, each costing approximately $0.2857 per hour. The costs for our methods are estimated by calculating the average GPU usage during generation as:\n\\[Anno. Cost = \\frac{Price}{3600} \\times Time \\times \\frac{Memory_{Use}}{Memory_{Total}},\\]\nwhere Price is the GPU rent per hour. \\(Memory_{Use}\\) and \\(Memory_{Total}\\) represent the average occupied GPU memory and the total memory of the system, respectively. Time is the generation duration measured in seconds. All cost numbers in our tables are presented in terms of cents (\u00a2)."}, {"title": "4.2 FedMobileAgent in IID Settings", "content": "5.1 Development of Current Mobile Agents\nSetups. All federated experiments are conducted on our generated data. Since the data produced by\nThe advent of VLMs (Zhang et al., 2024) has marked a significant shift in phone automation, enabling more dynamic, context-aware, and sophisticated interactions with mobile devices (Liu et al., 2025). Research on mobile agents has progressed through key milestones, with models becoming more proficient at interpreting multi-modal data, understanding user intent, and autonomously executing complex tasks. VLM-based mobile agents typically follow two approaches: (1) Prompt Engineering (Lee et al., 2024; Lu et al., 2024b), where pre-trained models are guided by carefully designed prompts, and (2) Training-Based Methods (Hong et al., 2023; Cheng et al., 2024), where VLMs are further optimized using large-scale mobile datasets. While training-based methods offer higher potential and generalizability by improving the VLM through fine-tuning, they require a large amount of training data, which can be very costly."}, {"title": "5.2 Efforts in Building Datasets for Mobile Agents", "content": "https://www.refuel.ai/blog-posts/llm-labeling-technical-report\nAcquiring training trajectories for mobile agents presents significant challenges. Existing approaches are often reliant on manual curation, making data collection both costly and inefficient. Some works have explored the possibility of automatically constructing datasets using VLMs or Application Programming Interfaces (APIs) (Wang et al., 2021; Lai et al., 2024). But these approaches either halfway to completing the datasets or depend on pre-defined tasks.\nOS-Genesis (Sun et al., 2024), the most advanced in this area, proposes reverse task synthesis to eliminate the need for pre-defined instructions. However, this method still requires an agent to execute synthetic tasks in a simulated mobile environment, to obtain the corresponding screenshots and actions. This process does not guarantee the accuracy of executed actions, while also incurs additional computational and resource costs. In contrast, we propose collecting real-world data from mobile users. This approach offers both (1) unlimited data scale, given the billions of mobile users worldwide, and (2) ground truth accuracy, as the data is directly generated through human execution."}, {"title": "6 Conclusion", "content": "To overcome the scalability and efficiency limitations of traditional data collection methods, we emphasize the necessity of transitioning from human to automatic annotation and from centralized to distributed data collection.\nWe propose FedMobileAgent, a framework that collaboratively trains mobile agents using self-sourced data from diverse users. Specifically, we introduce Auto-Annotation, an efficient approach for generating high-quality datasets from routine phone usage at minimal cost. Additionally, we present a federated learning framework with adapted global aggregation to handle new data heterogeneity. Extensive experiments validate FedMobileAgent's effectiveness. The results highlight the scalability and practicality of our approach, offering a privacy-preserving and cost-efficient solution for training large-scale mobile agent."}]}