{"title": "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures", "authors": ["Ekaterina Sviridova", "Anar Yeginbergen", "Ainara Estarrona", "Elena Cabrio", "Serena Villata", "Rodrigo Agerri"], "abstract": "Explaining Artificial Intelligence (AI) decisions is a major challenge nowadays in AI, in particular when applied to sensitive scenarios like medicine and law. However, the need to explain the rationale behind decisions is a main issue also for human-based deliberation as it is important to justify why a certain decision has been taken. Resident medical doctors for instance are required not only to provide a (possibly correct) diagnosis, but also to explain how they reached a certain conclusion. Developing new tools to aid residents to train their explanation skills is therefore a central objective of AI in education. In this paper, we follow this direction, and we present, to the best of our knowledge, the first multilingual dataset for Medical Question Answering where correct and incorrect diagnoses for a clinical case are enriched with a natural language explanation written by doctors. These explanations have been manually annotated with argument components (i.\u0435., premise, claim) and argument relations (i.\u0435., attack, support), resulting in the Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases in four languages (English, Spanish, French, Italian) with explanations, where we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106 attack relations. We conclude by showing how competitive baselines perform over this challenging dataset for the argument mining task.", "sections": [{"title": "1 Introduction", "content": "There is an increasingly large body of research on AI applied to the medical domain with the objective of developing technology to assist and support medical doctors in explaining their decisions or how they have reached a certain conclusion. For example, resident medical doctors preparing for licensing exams may get AI support to explain what and why is the treatment or diagnosis correct given some background information (Safranek et al., 2023; Goenaga et al., 2024).\nA prominent example of this is the recent proliferation of Medical Question Answering (QA) datasets and benchmarks, in which the task often involves processing and acquiring relevant specialized medical knowledge to be able to answer a medical question based on the context provided by a clinical case (Singhal et al., 2023a; Nori et al., 2023; Xiong et al., 2024).\nThe development of Large Language Models (LLMs), both general purpose and specialized in the medical domain, has enabled rapid progress in Medical QA tasks which has led in turn to claims about LLMs being able to pass official medical exams such as the United States Medical Licensing Examination (USMLE) (Singhal et al., 2023b; Nori et al., 2023). Thus, publicly available LLMs such as LLaMa (Touvron et al., 2023) or Mistral (Jiang et al., 2023) and their respective medical-specific versions PMC-LLaMa (Wu et al., 2024) and BioMistral (Labrak et al., 2024), or proprietary models such as MedPaLM (Singhal et al., 2023b) and GPT-4 (Nori et al., 2023), to name but a few, have been reporting high-accuracy scores in a variety of Medical QA benchmarks\u00b9(Singhal et al., 2023a,b; Xiong et al., 2024).\nWhile these results constitute impressive progress, currently the Medical QA research field still presents a number of shortcomings. First, experimentation has been mostly focused on providing the correct answer in medical exams, usually in a multiple-choice setting. However, as doctors are also required to explain and argue about their predictions, research on Medical QA should also address the identification and generation of argumentative explanations. Unfortunately, and to the best of our knowledge, no Medical QA dataset"}, {"title": "2 Related Work", "content": "In this section, we will focus on reviewing datasets for Medical QA and on Explanatory Argumentation, the two features of our main contribution, CasiMedicos-Arg."}, {"title": "2.1 Medical Question Answering", "content": "Several of the most popular Medical QA datasets (Jin et al., 2019; Abacha et al., 2019b,a; Jin et al., 2021; Pal et al., 2022) have been grouped into three multi-task English benchmarks, namely, MultiMedQA (Singhal et al., 2023a), MIRAGE (Xiong et al., 2024), and the Open Medical-LLM Leaderboard (Pal et al., 2024), with the aim of providing comprehensive experimental evaluation benchmarks of LLMs for Medical QA.\nMultiMedQA includes MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022), PubMedQA (Jin et al., 2019), LiveQA (Abacha et al., 2019b), MedicationQA (Abacha et al., 2019a), MMLU clinical topics (Hendrycks et al., 2020) and HealthSearchQA (Singhal et al., 2023a). Except for the last one, all of them consist of a multiple-choice format and MedQA, MedMCQA and MMLU's source data come from licensing medical exams. In terms of size, MedQA includes almost 15K questions, MedMCQA 187K while the rest of them are of more moderate sizes, namely, 500 QA pairs in PubMedQA, around 1200 in MMLU, 738 in LiveQA and 674 in MedicationQA.\nWhile every dataset except MedQA and HealthSearchQA includes long form correct answers, they are not considered really usable for benchmarking LLMs because they were not optimally constructed as a ground-truth by medical doctors or professional clinicians (Singhal et al., 2023a).\nThe Open Medical-LLM Leaderboard also includes MedQA, MedMCQA, PubMedQA and MMLU clinical topics. General purpose LLMs such as GPT-4 (Nori et al., 2023), PaLM (Chowdhery et al., 2022), LLaMa (Touvron et al., 2023) or Mistral (Jiang et al., 2023) report high-accuracy scores on these Medical QA benchmarks, although recently a number of specialized LLMs for the medical domain sometimes appear with even stronger performances. Some popular models include MedPaLM (Singhal et al., 2023a), MedPaLM-2 (Singhal et al., 2023b), PMC-LLaMa (Wu et al., 2024), and more recently, BioMistral (Labrak et al., 2024).\nThe MIRAGE benchmark includes subsets of MedQA, MedMCQA, PubMedQA, MMLU clinical topics and adds the BioASQ-YN dataset (Tsatsaronis et al., 2015) with the aim of evaluating Retrieval Augmented Generation (RAG) techniques for LLMs in Medical QA tasks. According to the authors, their MEDRAG method not only helps to address the problem of hallucinated content by grounding the generation on specific contexts, but it also provides relevant up-to-date knowledge that may not be encoded in the LLM (Xiong et al., 2024). By employing MEDRAG, they are able to clearly improve the zero-shot results of some of the tested LLMs, although the results for others are rather mixed.\nFinally, MedExpQA (Alonso et al., 2024) presents the first multilingual Medical QA benchmark with reference gold explanations of incorrect and incorrect options written by Spanish medical doctors. The benchmark is translated into French,"}, {"title": "2.2 Explanatory Argumentation in the Medical Domain", "content": "Explanatory argumentation in natural language refers to the process of generating or analyzing explanations within argumentative texts. In recent years, natural language explanation generation has gained significant attention due to the advancements of generative models that are leveraged to develop specialized explanatory systems. The need for explanation generation is also driven by the predominant use of non-transparent algorithms which lack interpretability, thus being unsuitable for sensitive domains such as medical.\nCamburu et al. (2018) tackle the task of explanation generation by introducing an extension of the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which includes a new layer of annotations providing explanations for the entailment, neutrality, or contradiction labels. The generation of these explanations is addressed with a bi-LSTM encoder trained on the new e-SNLI dataset. e-SNLI (Camburu et al., 2018) is also exploited to generate explanations for a NLI method, which first generates possible explanations for predicted labels (Label-specific Explanations) and then takes a final label decision (Kumar and Talukdar, 2020). The authors use GPT-2 (Radford et al., 2019) for label-specific generation and classify explanations with RoBERTa (Liu et al., 2019).\nNarang et al. (2020) focus on generating complete explanations in natural language following a prediction step, utilizing a T5 model. The model is trained to predict both the label and the explanation. Li et al. (2021) also propose to generate explanations along with predicting NLI labels. The generation step is leveraged for the question-answering task exploiting domain-specific or commonsense knowledge, while the NLI step allows to predict relations between a premise and a hypothesis. Kotonya and Toni (2024) propose a framework to rationalize explanations taking into account not only free-form explanations, but also argumentative explanations. Furthermore, authors provide metrics for explanation evaluation.\nIn the medical domain, Molinet et al. (2024) propose generating template-based explanations for medical QA tasks. Their system incorporates medical knowledge from the Human Phenotype Ontology, making the explanations more verifiable and sound for the medical domain. At the same time, quality assessment of medical explanations remains challenging, as the process of decision-making is not transparent. In this regard, Marro et al. (2023) propose a new methodology to evaluate reasons of explanations in clinical texts.\nDespite the extensive research proposing various approaches to generate explanations, these approaches are not grounded on any argumentation model. This is particularly important in sensitive domains like medicine, where sound and well-founded explanations are essential to justify the taken decision. Moreover, medical explanations require verified medical knowledge at their core, which the described methods lack, as discussed in Molinet et al. (2024)."}, {"title": "3 CasiMedicos-Arg Annotation", "content": "The Spanish Ministry of Health yearly publishes the Resident Medical or M\u00e9dico Interno Residente (MIR) licensing exams including the correct answer. Every year the CasiMedicos MIR Project 2.02 takes the published exams by the ministry and provide gold explanatory arguments written by volunteer Spanish medical doctors to reason about the correct and incorrect options in the exam.\nThe Antidote CasiMedicos corpus consists of the original Spanish commented exams by the CasiMedicos doctors which were cleaned, structured and freely released for research purposes (Agerri et al., 2023). The original Spanish data was automatically translated and manually revised into English, French, and Italian. The corpus includes 622 documents each with a short clinical case, the multiple-choice questions and the expla"}, {"title": "3.1 Argumentation Annotation Guidelines", "content": "In line with the guidelines proposed by Mayer et al. (2021) for Randomized Controlled Trials (RCT) annotation, we identify two main argument components: Claims and Premises, and their relations, Support and Attack. Furthermore, we also propose to annotate Markers and labels specific to the medical domain, namely, Disease, Treatment and Diagnostics. In the following, we define and describe each type of annotation.\nA Claim is a concluding statement made by the author about the outcome of the study (Mayer et al., 2021):\n1. The patient's presenting picture is presumably erythema nodosum. (CasiMedicos)\n2. We propose immunotherapy with thymoglobulin and cyclosporine as a proper treatment.\n(CasiMedicos)\nA Premise corresponds to an observation or measurement in the study, which supports or attacks another argument component, usually a Claim. It is important that they are observed facts, therefore, credible without further evidence (Mayer et al., 2021):\n3. In addition, pancytopenia is not observed.\n(CasiMedicos)\n4. What is important is that the eye that has received the blow does not go up, and therefore there is double vision in the superior gaze.\n(CasiMedicos)\nAnalyzing the CasiMedicos dataset, we found certain ambiguity between claims and premises. Thus, statements representing general medical knowledge about a disease, symptoms, or treatments must be annotated as claims. Although these statements may support or attack the main claim, they are not premises since they do not involve case-specific evidence but represent medical facts:\n5. [The patient's presenting picture is presumably erythema nodosum]. [About 10% of cases of erythema nodosum are associated with inflammatory bowel disease, both ulcerative colitis and Crohn's disease]. [As mentioned, in most cases, erythema nodosum has a self-limited course]. [When associated with inflammatory bowel disease, erythema nodosum usually resolves with treatment of the intestinal flare, and recurs with disease recurrences. Local measures include elevation of the legs and bed rest]. (CasiMedicos)\nHere the first statement in square brackets represents a claim that asserts the patient's diagnosis (erythema nodosum). The following ones represent information about the diagnosis, its symptoms and its possible treatment. They are not based on the evidences given in the case, but on general medical knowledge available to the doctor. Therefore, these examples should be annotated as Claims.\nAdditionally, long statements with multiple self-contained pieces of evidence must be divided into single premises to differentiate their relations to specific claims. For example, a given evidence in a sentence may support a claim while others may attack it. To preserve these distinctions, such sentences should be split into independent premises.\nAs well as Claims and Premises we annotate Markers - discourse markers that are relevant for arguments as they help to identify the spans of argument components and the type of argumentative relations. In the following examples markers are written in bold:\n6. Other causes related to this picture are autoimmune diseases leading to transverse myelitis (Behcet's, FAS, SLE,...) or inflammatory diseases such as sarcoidosis, although our patient does not seem to meet the criteria for them. (CasiMedicos)\n7. Although this usually gives a subacute or chronic picture. (CasiMedicos)\nThe possible answers proposed in the CasiMedicos multiple-choice options correspond to predicting a Disease, a Treatment or a Diagnosis. We decided to also annotate them as they help to identify the type of doctor's arguments (whether to look justification of a diagnosis or about a possible treatment) and the type of argumentative relations.\nFor advanced reasoning comprehension, we need to explore argumentative relations connecting"}, {"title": "3.2 CasiMedicos Real Case Example", "content": "In this section we describe a real CasiMedicos case annotated with argument components \u2013 Premises (in square brackets in italics) and Claims (in square brackets in bold), as well as Markers (M).\nWe consider this case to be exemplary because its explanation includes reasons on why the correct answer is correct and why the incorrect answers are incorrect. We do not include argumentative relations for the sake of space and clarity.\nQUESTION TYPE: PEDIATRICS\nCLINICAL CASE\n[A woman comes to the office with her 3 year old daughter because she has detected a slight mammary development since 3 months without taking any medication or any relevant history.] Indeed, [the physical examination shows a Tanner stage IV, with no growth of pubic or axillary hair.] [The external genitalia are normal.] [Ultrasonography reveals a small uterus and radiology reveals a bone age of 3 years.] What attitude should be adopted?\n1- [Follow-up every 3-4 months, as this is a temporary condition that often resolves on its own.]\n2- [Breast biopsy.]\n3- [Mammography.]\n4- [Administration of GnRh analogues.]\nCORRECT ANSWER: 1\n[It seems that they want to present us with precocious puberty (or premature telarche)] (M)but [they do not provide any analytical data] and [the ultrasound data are ambiguous] ([we should assume that by a small uterus they are referring to a prepubertal uterus], (M)but [they do not provide any data on ovarian size]). [We are presented with the case of a three-year-old girl with advanced mammary development, in principle without any associated cause] ([in principle she does not take drugs that can increase the level of estrogen in the blood], [she does not seem to use body creams or eat a lot of chicken meat]). [If we follow the diagnostic scheme for a premature telarche or suspicion of precocious puberty, we request bone age and abdominal ultrasound] ([the EO is not advanced as in precocious puberty, and we assume that with a small uterus they mean a prepubertal uterus]); [according to the complementary examinations that we are given, it does not seem to be precocious puberty, except for the clinical (Tanner IV)]. [Strictly speaking, without analytical hormonal data, it seems that we could mark option 1, being necessary to follow the girl closely.]"}, {"title": "3.3 Annotation Process and Results", "content": "The annotation process consisted of three stages: training, reconciliation, and complete dataset annotation. During training, annotators worked on 10 CasiMedicos cases. We then calculated the inter-annotator agreement (IAA) results of the training phase to highlight weak spots, guideline flaws, and any issues in the dataset needing further analysis.\nAt the reconciliation phase, the descriptions of Claim and Premise labels were discussed and agreed upon. After this, we started the complete dataset annotation. As mentioned earlier, the original CasiMedicos dataset included 622 medical cases, but 64 cases were excluded during the annotation phase. Some of them did not have gold explanations while others were cases with confusing relations: the correct answer is a wrong disease, treatment, or diagnosis as asked in a question, thus, it is attacked by its premises instead of being supported. Therefore, the final number of annotated cases is 558. In the following subsections, we present the IAA of the entire dataset (3.4), annotation results and their description (3.5)."}, {"title": "3.4 Inter-Annotator Agreement (IAA)", "content": "The IAA is calculated over a random batch of 100 CasiMedicos cases. Since one instance (e.g. a claim) is usually an entire self-contained sentence, we measured the IAA at both the instance level and at the token level. In other words, we compute agreement over entire instances and over the tokens of each instance.\nTable 1 illustrates the IAA at instance level. Since instances are very long, annotators may be uncertain about which elements to include, leading to lower agreement scores for some labels. However, the major labels Claim and Premise have relatively good results with scores of 0.765 and 0.659, respectively. The mean F1 over all labels is 0.669.\nTable 2 shows the IAA at token level. Here we compute the agreement over tokens of each instance. The highest agreement score is of a Claim"}, {"title": "3.5 Annotation Results", "content": "In this part, we report the stats about label distribution over entire cases (documents) and the label distribution over the doctor's explanations only. Additionally, we also discuss the distribution of argumentative relations.\nTable 3 reports the total number of entities over the dataset and the average number of entities per case. Table 4 shows the label distributions only for the explanations, namely, the total number of entities in explanations and the average number of entities per explanation. In both tables, we notice that the discrepancy between the average number of claims per explanation and of premises per explanation is rather high. This may seem strange since premises are needed to accept or reject claims in order to complete one argumentation unit.\nHowever, there are plausible reasons for such distribution. First, there is a certain number of cases where the explanation is based on the evidence from the doctor's knowledge rather than clinical facts described in the case itself. Such explanations take into account the information given about the patient (e. g. age, symptoms, vital signs), but do not repeat any of these facts (as in Example 1 in Appendix A). Second, explanations that do not repeat evidence from the case are frequent, e.g. \"Here we must suspect ... disease. All the symptoms"}, {"title": "4 Experimental Setup", "content": "We first describe the process of projecting the manually annotated argumentation labels from the source English data to the other three target languages, namely, French, Italian and Spanish. Since the annotators of the argument components were English speakers, we treated it as the source when projecting labels to the target languages. This process will result in the Multilingual Casimedicos-Arg which will then be leveraged to produce strong baselines on argument component detection using a variety of LMs, including encoders (Devlin et al., 2019; He et al., 2021), encoder-decoders (Garc\u00eda-Ferrero et al., 2024) and decoder-only LLMS (Touvron et al., 2023; Jiang et al., 2023)."}, {"title": "4.1 Multilingual CasiMedicos-Arg", "content": "Taking the manually annotated English CasiMedicos-Arg as a starting point, we first needed to project the annotations to Spanish (original text), French and Italian (revised translations) following the method described in Yeginbergenova and Agerri (2023) and Yeginbergen et al. (2024). Second, and to ensure that the projection method correctly leveraged the annotations to the new data we additionally performed an automatic post-processing step of the newly generated data to correct any misalignments. Finally, to guarantee the quality of annotations and the validity of our evaluations, the translated and projected data is manually revised by native speakers.\nLabel projection is performed using word alignments calculated by AWESOME (Dou and Neubig, 2021) and Easy Label Projection (Garc\u00eda-Ferrero et al., 2022) to automatically map the word alignments into sequences (argument components) and project them from the source (English) to the target language (French, Italian and Spanish).\nA particular feature of argument components is that the sequences could span over the entire length of the sentences. Therefore, after revising the automatically projected data, an extra post-processing step was performed by correcting the projections in the sequences where some annotations were placed incorrectly. The most common correction was fixing articles at the beginning of the argument components, which were systematically missed out during the automatic projection step. Other sequences were labeled only by half instead of the whole sequence. This post-processing step was essential to minimize human labor during manual correction."}, {"title": "4.2 Sequence Labelling with LLMs", "content": "We leverage Multilingual CasiMedicos-Arg to perform cross-lingual and multilingual argument component detection, a task that, due to the heterogeneity and length of the sequences, is usually a rather challenging task (Stab and Gurevych, 2017; Eger et al., 2018; Yeginbergenova and Agerri, 2023). Furthermore, in addition to classic encoder-only models like mBERT (Devlin et al., 2019) and mDeBERTa (He et al., 2021), we decided to also perform the task using encoder-decoder and decoder-only models. For the encoder-decoder category, we chose two variants of Medical mT5, a multilingual text-to-text model adapted to multilingual medical texts: med-mT5-large and med-mT5-large-multitask (Garc\u00eda-Ferrero et al., 2024). For the decoder-only architecture, we selected the LLaMa-2 (Touvron et al., 2023) and Mistral (Jiang et al., 2023) models with 7B parameters. The domain-specific versions of these models produced less promising results, so we opted to report the results of the aforementioned models.\nPrevious work in sequence labeling with LLMs has demonstrated that discriminative approaches based on encoder-only models still outperform generative techniques based on LLMs (Wang et al., 2023). The motivation behind it is usually the nature of the sequence labeling task that even though LLMs possess some linguistic knowledge they suffer from a number of problems, notably, hallucinated content. In this paper, we use the LLMs for Sequence Labelling library to fine-tune the generative models with unconstrained decoding4.\nWe structure the experiments as follows. First, we perform monolingual experiments in which we train and test for each language separately. Note that for English we use the gold standard annotations, while for French, Italian and Spanish we are fine-tuning the models on projected data, which in cross-lingual transfer research is usually called"}, {"title": "5 Empirical Results", "content": "In this section we report the results obtained after performing the steps described in Section 4. All the results and standard deviations reported in this section are obtained by averaging three randomly initialized runs. We evaluate using sequence level F1-macro score, a common metric for argument component detection.\nWe first show the results on monolingual (using the manually annotated English data) and multilingual (fine-tuning on all four languages and evaluating in English) in Table 6. Overall, it can be observed that the decoder-only generative models outperform the rest, though the Medical mT5 models are nearly as effective. Furthermore, the multilingual method of pooling all languages into a single dataset proves to be beneficial for every model, improving over the results obtained when training using the gold standard English data only."}, {"title": "6 Conclusion", "content": "In this paper, we present CasiMedicos-Arg, a multilingual (French, English, Italian and Spanish) Medical QA dataset including gold reference explanations written by medical doctors which has been annotated with argumentative structures. This dataset aims to bridge a glaring gap in the Medical QA ecosystem by facilitating the evaluation of explanations generated to argue or justify a given prediction.\nThe final dataset includes 558 documents (parallel in four languages) with reference gold doctors' explanations which are enriched with manual annotations for argument components (5021 claims and 2313 premises) and relations (2431 support and 1106 attack).\nBoth inter-annotator agreement results and the baselines provided for argument component detection demonstrate the validity of our annotations. Furthermore, experiments show the advantage of performing argument component detection from a multilingual data-transfer perspective."}, {"title": "Limitations", "content": "We consider two main limitations in our work that we would like to address in the short term future. First, the choice of languages. We would have liked to include languages from different language families and with different morphological and grammatical characteristics, but we were limited by the native expertise available to us to perform the manual corrections of the projected labels and translations. Second, the size of the dataset (558 documents) could be larger.\nRegarding the first limitation, we still think that our experiments demonstrate the superiority of performing multilingual data-transfer over cross-lingual model transfer, at least with the LLMs currently available. With respect to the size of the dataset, we would like to point out that its size is similar to other datasets reviewed in Section 2, which are being widely used to benchmark LLMs for Medical QA.\nAnother issue worth considering in the future is the need to further research the generation of explanations for the predictions while taking into account a crucial unsolved issue, namely, the evaluation explanation generation in the highly specialized medical domain."}, {"title": "A Appendix. CasiMedicos Real Cases", "content": "Example 1:\nQUESTION TYPE: DERMATOLOGY\nCLINICAL CASE:\nA 62-year-old man with a history of significant alcohol abuse, carrier of hepatitis C virus, treated with Ibuprofen for tendinitis of the right shoulder, goes to his dermatologist because after spending two weeks on vacation at the beach he notices the appearance of tense blisters on the dorsum of his hands. On examination, in addition to localization and slight malar hypertrichosis. The most likely diagnosis is:\n1- Epidermolysis bullosa acquisita.\n2- Porphyria cutanea tarda.\n3- Phototoxic reaction.\n4- Contact dermatitis.\n5- Acute intermittent porphyria.\nCORRECT ANSWER: 2\nPorphyria Cutanea Tarda: 60% of patients with PCT are male, many of them drink alcohol in excess, women who develop it are usually treated with drugs containing estrogens. Most are males with signs of iron overload, this overload reduces the activity of the enzyme uroporphyrinogen decarboxylase, which leads to the elevation of uroporphyrins. HCV and HIV infections have been implicated in the precipitation of acquired PCT. There is a hereditary form with AD pattern. Patients with PCT present with blistering of photoexposed skin, most frequently on the dorsum of the hands and scalp. In addition to fragility, they may develop hypertrichosis, hyperpigmentation, cicatricial alopecia and sclerodermal induration.\nExample 2:\nQUESTION TYPE: PEDIATRICS\nCLINICAL CASE:\n6-month-old infant presenting to the emergency department for respiratory distress. Examination: axillary temperature 37.2\u00b0C, respiratory rate 40 rpm, heart rate 160 bpm, blood pressure 90/45 mmHg, SatO2 95% on room air. He shows moderate respiratory distress with intercostal"}, {"title": "B Number of corrections after annotation projection", "content": "The number of corrections required after automatically projecting the annotations."}]}