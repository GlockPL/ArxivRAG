[{"title": "Fairness Through Matching", "authors": ["Kunwoong Kim", "Insung Kong", "Jongjin Lee", "Minwoo Chae", "Sangchul Park", "Yongdai Kim"], "abstract": "Group fairness requires that different protected groups, characterized by a given sensitive attribute, receive equal outcomes overall. Typically, the level of group fairness is measured by the statistical gap between predictions from different protected groups. In this study, we reveal an implicit property of existing group fairness measures, which provides an insight into how the group-fair models behave. Then, we develop a new group-fair constraint based on this implicit property to learn group-fair models. To do so, we first introduce a notable theoretical observation: every group-fair model has an implicitly corresponding transport map between the input spaces of each protected group. Based on this observation, we introduce a new group fairness measure termed Matched Demographic Parity (MDP), which quantifies the averaged gap between predictions of two individuals (from different protected groups) matched by a given transport map. Then, we prove that any transport map can be used in MDP to learn group-fair models, and develop a novel algorithm called Fairness Through Matching (FTM)\u00b9, which learns a group-fair model using MDP constraint with an user-specified transport map. We specifically propose two favorable types of transport maps for MDP, based on the optimal transport theory, and discuss their advantages. Experiments reveal that FTM successfully trains group-fair models with certain desirable properties by choosing the transport map accordingly.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) technologies based on machine learning algorithms have become increasingly prevalent as crucial decision-making tools across diverse areas, including credit scoring, criminal risk assessment, and college admissions. However, when observed data contains unfair biases, the resulting trained models may produce discriminatory decisions (Calders et al., 2009; Feldman et al., 2015; Angwin et al., 2016; Barocas & Selbst, 2016; Chouldechova, 2016; Kleinberg et al., 2018; Mehrabi et al., 2019; Zhou et al., 2021). For instance, several cases of unfair preferences favoring specific groups, such as white individuals or males, have been reported (Angwin et al., 2016; Ingold & Soper, 2016; Dua & Graff, 2017). In response, there is a growing trend in non-discrimination laws that calls for the consideration of fair models (Hellman, 2019).\nUnder this social circumstance, ensuring algorithmic fairness in AI-based decision-making has become a crucial mission. Among several notions of algorithmic fairness, the notion of group fairness is the most explored one, which requires that certain statistics of each protected group should be similar. For example, the ratio of positive predictions should be similar across each protected group (Calders et al., 2009; Barocas & Selbst, 2016; Zafar et al., 2017; Donini et al., 2018; Agarwal et al., 2018).\nVarious algorithms have been proposed to learn models achieving group fairness. Existing methods for group fairness are roughly categorized into three: pre-processing, in-processing and post-processing. Pre-processing approaches (Zemel et al., 2013; Feldman et al., 2015; Webster et al., 2018; Xu et al., 2018; Madras et al., 2018; Creager et al., 2019; Kim et al., 2022a) aim to debias a given dataset, typically by learning fair representations whose distribution is independent of a given sensitive attribute. The debiased data (or fair representation) is then used to learn models. In-processing approaches (Kamishima et al., 2012; Goh et al., 2016; Zafar et al., 2017; Agarwal et al., 2018; Wu et al., 2019; Cotter et al., 2019; Celis et al., 2019; Zafar et al., 2019; Jiang et al., 2020a; Kim et al., 2022b) train models by minimizing a given objective function under a specified group fairness constraint. Post-processing approaches (Kamiran et al., 2012; Hardt et al., 2016b; Fish et al., 2016; Corbett-Davies et al., 2017; Pleiss et al., 2017; Chzhen et al., 2019; Wei et al., 2020; Jiang et al., 2020a) transform given prediction scores, typically provided by an unfair model, to satisfy a certain fairness level.\nMost group-fair algorithms correspond to specific group fairness measures, typically defined by explicit quantities such as prediction scores and sensitive attributes. For example, demographic parity (Calders et al., 2009; Feldman et al., 2015; Angwin et al., 2016) considers the gap between two protected groups in terms of the positive prediction ratio.\nHowever, a shortcoming of such group fairness measures is that they only concern statistical disparities without accounting for implicit mechanisms about how a given model achieves group fairness. Consequently, models can achieve high levels of group fairness in very undesirable ways (see Section B of Appendix for an example). Dwork et al. (2012) also noted that focusing solely on group fairness can lead to issues such as subset targeting and the self-fulfilling prophecy. These observations serve as the motivation of this study. Moreover, our empirical investigations on real-world datasets reveal that unfairness on subsets can be observed in group-fair models learned by existing algorithms, which are designed to achieve group fairness merely (see Section 5).\nIn this paper, we first propose a new group fairness measure that reveals implicit behaviors of group-fair models. Based on the proposed measure, we develop an in-processing algorithm to learn group-fair models with less undesirable properties (e.g., unfairness on subsets), that cannot be explored or controlled by existing fairness measures.\nTo do so, we begin by introducing a notable mathematical finding: every group-fair model implicitly corresponds to a transport map, which moves the measure of one protected group to another. Building on this observation, we propose a new measure for group fairness called Matched Demographic Parity (MDP), which quantifies the average prediction gap between two individuals from different protected groups matched by a given transport map. We further prove that the reverse of this finding also holds: any transport map can be used in MDP to learn a group-fair model. Consequently, we develop an algorithm called Fairness Through Matching (FTM), designed to learn a group-fair model under a fairness constraint based on MDP with a given transport map. FTM can provide group-fair models having specific desirable properties (e.g., higher fairness on subsets) by selecting a transport map accordingly. Note that FTM is not designed to achieve the"}, {"title": "2 Preliminaries", "content": "In this section, we outline the mathematical notations used throughout this paper. We focus on binary classification task in this study. We denote $X \\in \\mathcal{X} \\subset \\mathbb{R}^d$ and $Y \\in \\mathcal{Y} = \\{0,1\\}$ as the d-dimensional input vector and the binary label, respectively. Whenever necessary, we split $\\mathcal{X}$, i.e., the domain of $X$, with respect to $S$ and write $\\mathcal{X}_s$ as the domain of $X|S=s$. For given $s\\in \\{0,1\\}$, we let $s' = 1 - s$. Assuming the pre-defined sensitive attribute is binary, we denote $S\\in \\{0,1\\}$ as the binary sensitive attribute. For a given $s\\in \\{0,1\\}$, the realization of $S$, we write $s' = 1 - s$. For the probability distributions of these variables, let $P$ and $P_X$ represent the joint distribution of $(X,Y,S)$ and the marginal distribution of $X$, respectively. Furthermore, let $P_s = P_{X|S=s}, s \\in \\{0,1\\}$ be the conditional distributions of $X$ given $S = s$. We write $E$ and $E_s$ as the corresponding expectations of $P$ and $P_s$, respectively. For observed data, denote $\\mathcal{D} = \\{(x_i, Y_i, S_i)\\}_{i=1}^n$ as the training dataset, consisting of $n$ independent copies of the random tuple $(X,Y,S) \\sim P$. Let $||\\cdot||_p$ denote the $L_p$ norm. We also write the $L_2$ norm by $||\\cdot||$, i.e., $||\\cdot|| = ||\\cdot||_2$ for simplicity.\nWe denote the prediction model as $f = f(\\cdot, s), s \\in \\{0, 1\\}$, which is an estimator of $P_\\Theta(Y = 1|X = \\cdot)$, belonging to a given hypothesis class $\\mathcal{F} \\subset \\{f : \\mathcal{X} \\times \\{0,1\\} \\rightarrow [0,1]\\}$. Note that the output of $f$ is restricted to the interval $[0,1]$, making $f$ bounded. For simplicity, we sometimes write $f(\\cdot, s) = f_s(\\cdot)$ whenever necessary. For given $f$ and $s\\in \\{0,1\\}$, we denote $P_{f_s}$ as the conditional distribution of $f(X,s)$ given $S = s$. Furthermore, let $C_{f,\\tau} := I(f(\\cdot, s) \\geq \\tau)$ be the classification rule based on $f$, where $\\tau$ is a specific threshold (typically, $\\tau = 0.5$)."}, {"title": "2.2 Measures for group fairness & Definition of group-fair model", "content": "In the context of group fairness, various measures have been introduced to quantify the gap between predictions of each protected group. The original measure for Demographic Parity (DP), $\\Delta_{DP}(f) :=$"}, {"title": "2.3 Optimal transport", "content": "The concept of the Optimal Transport (OT) provides an approach for geometric comparison between two probability measures. For given two probability measures $Q_1$ and $Q_2$, a map $T$ from $\\text{Supp}(Q_1)$ to $\\text{Supp}(Q_2)$ is called a transport map from $Q_1$ to $Q_2$ if the push-forward measure $T_{\\#}Q_1$ is equal to $Q_2$ (Villani, 2008). Here, the push-forward measure is defined by $T_{\\#}Q_1(A) = Q_1(T^{-1}(A))$ for any measurable set $A$. In other words, $T_{\\#} Q_1$ is the measure of $T(X), X \\sim Q_1$. For given source and target distributions $Q_1$ and $Q_2$, the OT map from $Q_1$ to $Q_2$ is the optimal one among all transport maps from $Q_1$ to $Q_2$. In this context, 'optimal' means minimizing transport cost, such as $L_p$ distance in Euclidean space.\nMonge (1781) originally formulates this OT problem: for given source and target distributions $Q_1, Q_2$ in $\\mathbb{R}^d$ and a cost function $c$ (e.g., $L_2$ distance), the OT map from $Q_1$ to $Q_2$ is defined by the solution of $\\min_{T:T_{\\#}Q_1=Q_2} E_{X \\sim Q_1}(c(X, T(X)))$. If both $Q_1$ and $Q_2$ are discrete with the same number of support points, the OT map exists as a one-to-one mapping. For the case when $Q_1$ and $Q_2$ are discrete but have different numbers of supports, Kantorovich relaxed the Monge problem by seeking the optimal coupling between two distributions (Kantorovich, 2006). The Kantorovich problem is formulated as $\\inf_{\\pi \\in \\Pi(Q_1, Q_2)} E_{X,Y \\sim \\pi} (c(X, Y))$, where $\\Pi(Q_1, Q_2)$ is the set of all joint measures of $Q_1$ and $Q_2$. Note that this formulation can be also applied to the case of $Q_1$ and $Q_2$ with an identical number of support. See Section C of Appendix for more details about the Kantorovich problem."}, {"title": "2.4 Related works", "content": "Algorithmic fairness Group fairness is a fairness notion aimed at preventing discriminatory predictions over protected (demographic) groups divided by pre-defined sensitive attributes. Among various notions of group fairness, Demographic Parity (DP) (Calders et al., 2009; Feldman et al., 2015; Agarwal et al., 2019; Jiang et al., 2020b; Chzhen et al., 2020) quantifies the statistical gap in predictions between two different protected groups. Other measures, including Equal opportunity (Eqopp) and Equalized Odds (EO), consider groups conditioned on both the label and the sensitive attribute (Hardt et al., 2016a). Various algorithms have been proposed to learn group-fair model satisfying these group fairness notions (Zafar et al., 2017; Donini et al., 2018; Agarwal et al., 2018; Madras et al., 2018; Zafar et al., 2019; Chuang & Mroueh, 2021; Kim et al., 2022a)."}, {"title": "3 Learning group-fair model through matching", "content": "The goal of this section is to explore and specify the correspondence between group-fair models and transport maps. In Section 3.1, we show that every group-fair model has a corresponding implicit transport map in the input space that matches two individuals from different protected groups. We then introduce a new fairness measure based on the correspondence. In Section 3.2, we show the reverse, i.e., any given"}, {"title": "3.1 Matched Demographic Parity (MDP)", "content": "Proposition 3.1 below shows that for a given perfectly group-fair model $f$ (i.e., $P_{f_0} = P_{f_1}$ or equivalently $\\Delta = 0$), there exists an corresponding transport map in the input space that matches two individuals from different protected groups. Its proof is in Section A of Appendix. Throughout this section, we assume a condition (C): $P_s, s = 0, 1$, are absolutely continuous with respect to the Lebesgue measure. This regularity condition is assumed to simplify the discussion, since it guarantees the existence of transport maps between two distributions. See Proposition A.1 in Section A of Appendix, which presents a similar result for the case where $P_s, s = 0, 1$ are discrete. Let $\\mathcal{T}^{\\text{trans}}$ be the set of all transport maps from $P_s$ to $P_{s'}$.\nProposition 3.1 (Fair model \u2192 Transport map: perfect fairness case). For any perfectly group-fair model $f$, i.e., $P_{f_0} = P_{f_1}$, there exists a transport map $T_f \\in \\mathcal{T}^{\\text{trans}}$ satisfying $f(X, s) = f(T_f(X), s'), \\text{a.e.}$\nThe key implication of this mathematical proposition is that all perfectly group-fair models are not the same and the differences can be identified by their corresponding transport maps. We can similarly define a transport map corresponding to a given not-perfectly group-fair model (i.e., $\\Delta > 0$), by using a novel fairness measure termed Matched Demographic Parity (MDP) defined as below.\nDefinition 3.2 (Matched Demographic Parity). For a given model $f \\in \\mathcal{F}$ and a transport map $T_s \\in \\mathcal{T}^{\\text{trans}}$, the measure for MDP is defined as\n$\\Delta_{\\text{MDP}}(f, T_s) := E_S |f(X, s) - f(T_s(X), s')|.$   (1)\nThe idea behind MDP is that two individuals from different protected groups are matched by $T_s$, and $\\Delta_{\\text{MDP}}(f, T_s)$ quantifies the similarity between the predictions of these two matched individuals. Subsequently, Theorem 3.3 below presents a relaxed version of Proposition 3.1, showing that any group-fair model $f$ has a transport map $T_s$ such that $\\Delta_{\\text{MDP}}(f, T_s)$ is small. Refer to Section A of Appendix for the proof of Theorem 3.3 and see Figure 1 for the illustration of MDP.\nTheorem 3.3 (Fair model \u2192 Transport map: relaxed fairness case). Fix a fairness level $\\delta \\geq 0$. For any given group-fair model $f$ such that $\\Delta_{\\text{TVDP}}(f) \\leq \\delta$, there exists a transport map $T_s \\in \\mathcal{T}^{\\text{trans}}$ satisfying $\\Delta_{\\text{MDP}}(f, T_s) \\leq 2\\delta$."}, {"title": "3.2 Fairness Through Matching (FTM): learning a group-fair model with a transport map", "content": "The goal of this section is to formulate our proposed algorithm. Before introducing it, we provide a theoretical support, which shows that a group-fair model can be constructed by MDP using any transport map. Theorem 3.7 below, which is the reverse of Theorem 3.3, shows that any transport map in the input space can construct a group-fair model. Precisely, for a given transport map, if a model provides similar predictions for two individuals who are matched by the transport map, then it is group-fair. The proof is given in Section A of Appendix.\nTheorem 3.7 (Transport map \u2192 Group-fair model). For a given $T_s \\in \\mathcal{T}^{\\text{trans}}$, if $\\Delta_{\\text{MDP}}(f, T_s) \\leq \\delta$, then we have $\\Delta_{\\text{WDP}}(f) \\leq \\delta$ and $\\Delta_{\\text{DP}}(f) \\leq \\delta$.\nAgain, it is remarkable that a group-fair model and its corresponding transport map are closely related, i.e., every group-fair model has its corresponding implicit transport map, and vice versa. This finding can be mathematically expressed as follows. Let $\\Delta$ be a given (existing) fairness measure, and define $\\mathcal{F}_\\Delta(\\delta) := \\{f \\in \\mathcal{F} : \\Delta(f) \\leq \\delta\\}$ as the set of group-fair models of level $\\delta$ (with respect to $\\Delta$). Similarly, for MDP, define $\\mathcal{F}_{\\Delta_{\\text{MDP}}}(T_s, \\delta) := \\{f \\in \\mathcal{F} : \\Delta_{\\text{MDP}}(f, T_s) \\leq \\delta\\}$. Then, following from Theorem 3.3 and 3.7, we can conclude that the three measures (i.e., TVDP, WDP, and MDP) are closely related: $\\mathcal{F}_{\\Delta_{\\text{TVDP}}}(\\delta) \\subseteq \\bigcup_{T \\in \\mathcal{T}^{\\text{trans}}} \\{f : \\Delta_{\\text{MDP}}(f, T_s) \\leq 2\\delta\\} \\subseteq \\mathcal{F}_{\\Delta_{\\text{WDP}}}(2\\delta)$.\nAs discussed in Section 1 as well as several previous works (e.g., Dwork et al. (2012)), there exist group-fair models having undesirable properties such as subset targeting or self-fulfilling prophecy. The advantage of MDP is that we can screen out such undesirable group-fair models during the learning phase, to consider desirable group fair models only. And, using MDP achieves this goal by considering group-fair models whose transport maps have low transport costs. That is, we can search for a group-fair model only on $\\bigcup_{T \\in \\mathcal{T}^{\\text{good trans}}} \\{f: \\Delta_{\\text{MDP}}(f, T_s) \\leq 2\\delta\\}$, where $\\mathcal{T}^{\\text{good trans}} \\subseteq \\mathcal{T}^{\\text{trans}}$ is a set of specified \u2018good' transport maps. See Section 4 for such good transport maps that we specifically propose. It is worth noting that such screening would be challenging with existing group-fair measures.\nFTM algorithm Based on Theorem 3.7, we develop a learning algorithm named Fairness Through Matching (FTM), which learns a group-fair model subject to MDP being small with a given transport map. FTM consists of two steps. First, we select a (good) transport map. Then, we learn a model under the MDP constraint with the selected transport map. The precise objective of FTM is formulated below.\nSuppose a transport map $T_s$ is selected (see Section 4 for the proposed transport maps). FTM solves the following objective for a given loss function $l$ (e.g., cross-entropy) and a pre-defined fairness level $\\delta \\geq 0$:\n$f^{\\text{FTM}}(T_s) := \\arg \\min_{f \\in \\mathcal{F}} E_{S} l(Y, f(X, S)) \\text{ subject to } \\min_{s \\in \\{0,1\\}} \\Delta_{\\text{MDP}}(f, T_s) \\leq \\delta.  (2)$\nUnless there is any confusion, we write $f^{\\text{FTM}}$ instead of $f^{\\text{FTM}}(T_s)$ for simplicity. By Theorem 3.7, it is clear that $f^{\\text{FTM}}$ is fair (i.e., $\\Delta_{\\text{WDP}}(f^{\\text{FTM}}), \\Delta_{\\text{DP}}(f^{\\text{FTM}}) \\leq \\delta$) for any transport map $T_s$. In practice, we estimate $f^{\\text{FTM}}$ with observed data $\\mathcal{D}$ using mini-batch technique along with a stochastic gradient descent based algorithm (see Section 4 for details)."}, {"title": "3.3 Conceptual comparison of existing approaches and FTM", "content": "Individual fairness FTM and individual fairness are similar in the sense that they try to treat similar individuals similarly. A difference is that FTM aims to treat two individuals from different protected groups similarly, while the individual fairness tries to treat similar individuals similarly regardless of sensitive attribute (even when it is unknown). That is, similar individuals in FTM could be dissimilar in view of individual fairness, especially when the two protected groups are significantly different.\nOn the other hand, a limitation of individual fairness is that group fairness is not guaranteed. FTM can be also understood as a tool to address this limitation by finding models with higher individual fairness among group-fair models. Empirical results support this conjecture that FTM improves individual fairness compared to baseline methods for group fairness (see Table 4 in Section 5.4.2)."}, {"title": "4 Empirical algorithm with transport map selection", "content": "The implication of Theorems 3.3 and 3.7 is that any transport map corresponds to a group-fair model. However, an improperly chosen transport map can result in undesirable outcomes, even if group fairness is satisfied, as shown in Theorem 3.7 (see Section B of Appendix for an example of a problematic group-fair model due to an unreasonable transport map). Thus, selecting an appropriate transport map is crucial when using FTM, which is the primary focus of this section.\nSpecifically, we propose two favorable choices of the transport map $T_s$ used in FTM. In Section 4.1, we suggest using the OT map in the input space $\\mathcal{X}$, resulting in a group-fair model with a higher fairness level on subsets. In Section 4.2, we propose using the OT map in the product space $\\mathcal{X} \\times \\mathcal{Y}$, to improve prediction performance and the level of equalized odds, when compared to the OT map in the input space."}, {"title": "4.1 OT map on X", "content": "For a given $T_s \\in \\mathcal{T}^{\\text{trans}}$, the transport cost (on $\\mathcal{X}$) of $T_s$ is defined by $E_S||X - T_s(X)||_2$. First, we propose using the OT map between the two input spaces, which is the minimizer of the transport cost on $\\mathcal{X}$ among all transport maps. From now on, we call this OT map on $\\mathcal{X}$ as the marginal OT map.\nIn this section, we explore a benefit of using the marginal OT map (i.e., low transport cost) by showing a theoretical relationship between the transport cost and fairness on subsets. Many undesirable behaviors of group-fair models have been recognized and discussed (Dwork et al., 2012; Kearns et al., 2018a; Wachter et al., 2020; Mougan et al., 2024). Subset fairness, which is a similar concept to subset targeting in Dwork et al. (2012), is one of such undesirable behaviors. We say that a group-fair model is subset-unfair if it is not group-fair against a certain subset (e.g., aged over 60s) even if it is group-fair overall. The definition of subset fairness can be formulated as follows.\nDefinition 4.1 (Subset fairness). Let $A$ be a subset of $\\mathcal{X}$. The level of subset fairness over $A$ is defined as\n$\\Delta_{DPA}(f) := |E(f(X, 0)|S = 0, X \\in A) - E(f(X, 1)|S = 1, X \\in A)|$.\nIntuitively, we expect that a group-fair model with a low transport cost would exhibit a high level of subset fairness. This is because the chance of two matched individuals (from different protected groups) belonging to the same subset $A$ tends to be higher when the transport cost is smaller. Theorem 4.2 theoretically supports this conjecture, whose proof is provided in Section A of Appendix.\nTheorem 4.2 (Low transport cost benefits subset fairness). Suppose $\\mathcal{F}$ is the collection of L-Lipschitz\u00b2 functions. Let $A$ be a given subset in $\\mathcal{X}$. Then, for all $f$ satisfying $\\Delta_{\\text{MDP}}(f, T_s^\\ddagger) \\leq \\delta$, we have\n$\\Delta_{DPA}(f) \\leq L (E_S |X - T^\\ddagger(X)||_2)^{1/2} + TV(P_{0,A}, P_{1,A}) + U\\delta,  (3)$\n\u00b2A function $g: X \\rightarrow \\mathbb{R}$ is L-Lipschitz if $|g(x_1) - g(x_2)| \\leq L||x_1 - x_2|| \\text{ for all } x_1, x_2 \\in \\mathcal{X}$."}, {"title": "4.2 OT map on X x Y", "content": "One might argue that using the marginal OT map could degrade the prediction performance much, since the matchings done by the marginal OT map do not consider the similarity in $Y$. As a remedy for this issue, we consider incorporating the label $Y$ into the cost matrix calculation to avoid substantial degradation in prediction performance.\nFor this purpose, we define a new cost function on $\\mathcal{X} \\times \\mathcal{Y}$. Let $\\alpha$ be a given positive constant. The new cost function $c^\\alpha : \\mathbb{R}^{d+1} \\times \\mathbb{R}^{d+1} \\rightarrow \\mathbb{R}_+$ is defined by: $c^\\alpha ((X_1,Y_1), (X_2,Y_2)) := ||X_1 - X_2||^2 + \\alpha |y_1 - y_2|$, for given"}, {"title": "4.3 Empirical algorithm for FTM", "content": "In practice, we learn $f$ with a stochastic gradient descent algorithm. For each update, to calculate the expected loss, we sample a random mini-batch $\\mathcal{D}' \\subset \\mathcal{D}$ of size $n' \\leq n$. Then, we update the solution using the gradient of the following objective function\n$L(f) := \\frac{1}{n'}\\sum_{(x_i,Y_i, S_i) \\in \\mathcal{D}'} l(Y_i, f(x_i, S_i)) + \\frac{\\chi}{m} \\sum_{x^{(s)} \\in \\mathcal{D}_s} |f(x^{(s)}, s) - f(T_{s, p}(x^{(s)}), s')|   (7)$\nfor any $s \\in \\{0,1\\}$, where $\\chi > 0$ is the Lagrange multiplier and $T_{s,p}$ is a pre-specified transport map from $\\mathcal{D}_s$ to $\\mathcal{D}_{s'}$ (e.g., the marginal OT map from Section 4.1 or the joint OT map from Section 4.2)."}, {"title": "5 Experiments", "content": "This section presents our experimental results, showing that FTM with the proposed transport maps in Section 4 empirically works well to learn group-fair models. The key findings throughout this section are summarized as follows.\n\u2022 FTM with the marginal OT map successfully learns group-fair models that exhibit (i) competitive prediction performance (Section 5.2.1) and (ii) higher levels of subset fairness (Section 5.2.2), when compared to other group-fair models learned by existing baseline algorithms. Beyond subset fairness, we further evaluate the self-fulfilling prophecy (Dwork et al., 2012) as an additional benefit of low transport cost (see Table 10 and 11 in Section E of Appendix).\n\u2022 FTM with the joint OT map has the ability to learn group-fair models with improved prediction performance as well as improved levels of equalized odds, when compared to FTM with the marginal OT map (Section 5.3)."}, {"title": "5.1 Settings", "content": "Datasets We use four real-world benchmark tabular datasets in our experiments: ADULT (Dua & Graff, 2017), GERMAN (Dua & Graff, 2017), DUTCH (Van der Laan, 2001), and BANK (Moro et al., 2014). The basic information about these datasets is provided in Table 6 in Section D.1 of Appendix. We randomly partition each dataset into training and test datasets with the 8:2 ratio. For each split, we learn models using the training dataset and evaluate the models on the test dataset. This process is repeated 5 times, and the average performance on the test datasets is reported.\nBaseline algorithms and implementation details For the baseline algorithms, we consider three most popular state-of-the-art methods: Reduction (Agarwal et al., 2018), Reg (minimizing cross-entropy + $\\lambda \\Delta_{ADP}^2$, which is considered in Donini et al. (2018); Chuang & Mroueh (2021)), and Adv (adversarial learning for ensuring that the model's predictions are independent of sensitive attributes, which is proposed by Zhang et al. (2018)). Additionally, we consider the unfair baseline (abbr. Unfair), the ERM model trained without any fairness regularization or constraint. For the measure of prediction performance, we use the classification accuracy (abbr. Acc). For fairness measures, we consider ADP, $\\overline{\\text{ADP}}$ and AWDP, which are defined in Section 2.2.\nFor all algorithms, we employ MLP networks with ReLU activation and two hidden layers, where the hidden size is equal to the input dimension. We run all algorithms for 200 epochs and report their final performances on the test dataset. The Adam optimizer (Kingma & Ba, 2014) with the initial learning rate of 0.001 is used. To obtain the OT map for each mini-batch, we solve the linear program by using the POT library (Flamary et al., 2021). We utilize several Intel Xeon Silver 4410Y CPU cores and RTX 3090 GPU processors. More implementation details with Pytorch-style psuedo-code are provided in Section D.2 and D.3 of Appendix."}, {"title": "5.2 FTM with the marginal OT map", "content": "This section presents the performance of FTM with the marginal OT map, in terms of (i) fairness-prediction trade-off and (ii) improvement in subset fairness."}, {"title": "5.2.1 Fairness-prediction trade-off", "content": "In this section, we empirically verify that learned models by FTM successfully achieves group fairness (demographic parity). Figure 3 below shows that FTM successfully learns group-fair models for various fairness levels.\nAnother main implication is that using the marginal OT map does not hamper prediction performance much. Figure 3 supports this assertion in terms of fairness-prediction trade-off, that is, FTM is competitive with the three baselines. In most datasets, the performance of FTM is not significantly worse than that of the top-performing algorithm (i.e., Reduction). Notably, on GERMAN dataset, FTM performs the best, whereas Reduction fails to learn group-fair models with fairness level under 0.06. Additionally, FTM mostly outperforms the other two baseline algorithms, Reg and Adv. Hence, we can conclude that FTM is also a promising algorithm for achieving group fairness."}, {"title": "5.2.2 Improvement in subset fairness", "content": "This section highlights the key advantages of using the marginal OT map in terms of subset fairness", "4.1": 1, "i": "overline{\\mathbf{v"}]}, {"dataset": 0.06}]