{"title": "Fairness Through Matching", "authors": ["Kunwoong Kim", "Insung Kong", "Jongjin Lee", "Minwoo Chae", "Sangchul Park", "Yongdai Kim"], "abstract": "Group fairness requires that different protected groups, characterized by a given sensitive attribute, receive equal outcomes overall. Typically, the level of group fairness is measured by the statistical gap between predictions from different protected groups. In this study, we reveal an implicit property of existing group fairness measures, which provides an insight into how the group-fair models behave. Then, we develop a new group-fair constraint based on this implicit property to learn group-fair models. To do so, we first introduce a notable theoretical observation: every group-fair model has an implicitly corresponding transport map between the input spaces of each protected group. Based on this observation, we introduce a new group fairness measure termed Matched Demographic Parity (MDP), which quantifies the averaged gap between predictions of two individuals (from different protected groups) matched by a given transport map. Then, we prove that any transport map can be used in MDP to learn group-fair models, and develop a novel algorithm called Fairness Through Matching (FTM), which learns a group-fair model using MDP constraint with an user-specified transport map. We specifically propose two favorable types of transport maps for MDP, based on the optimal transport theory, and discuss their advantages. Experiments reveal that FTM successfully trains group-fair models with certain desirable properties by choosing the transport map accordingly.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) technologies based on machine learning algorithms have become increasingly prevalent as crucial decision-making tools across diverse areas, including credit scoring, criminal risk assessment, and college admissions. However, when observed data contains unfair biases, the resulting trained models may produce discriminatory decisions (Calders et al., 2009; Feldman et al., 2015; Angwin et al., 2016; Barocas & Selbst, 2016; Chouldechova, 2016; Kleinberg et al., 2018; Mehrabi et al., 2019; Zhou et al., 2021). For instance, several cases of unfair preferences favoring specific groups, such as white individuals or males, have been reported (Angwin et al., 2016; Ingold & Soper, 2016; Dua & Graff, 2017). In response, there is a growing trend in non-discrimination laws that calls for the consideration of fair models (Hellman, 2019).\nUnder this social circumstance, ensuring algorithmic fairness in AI-based decision-making has become a crucial mission. Among several notions of algorithmic fairness, the notion of group fairness is the most explored one, which requires that certain statistics of each protected group should be similar. For example, the ratio of positive predictions should be similar across each protected group (Calders et al., 2009; Barocas & Selbst, 2016; Zafar et al., 2017; Donini et al., 2018; Agarwal et al., 2018).\nVarious algorithms have been proposed to learn models achieving group fairness. Existing methods for group fairness are roughly categorized into three: pre-processing, in-processing and post-processing. Pre-processing approaches (Zemel et al., 2013; Feldman et al., 2015; Webster et al., 2018; Xu et al., 2018; Madras et al., 2018; Creager et al., 2019; Kim et al., 2022a) aim to debias a given dataset, typically by learning fair representations whose distribution is independent of a given sensitive attribute. The debiased data (or fair representation) is then used to learn models. In-processing approaches (Kamishima et al., 2012; Goh et al., 2016; Zafar et al., 2017; Agarwal et al., 2018; Wu et al., 2019; Cotter et al., 2019; Celis et al., 2019; Zafar et al., 2019; Jiang et al., 2020a; Kim et al., 2022b) train models by minimizing a given objective function under a specified group fairness constraint. Post-processing approaches (Kamiran et al., 2012; Hardt et al., 2016b; Fish et al., 2016; Corbett-Davies et al., 2017; Pleiss et al., 2017; Chzhen et al., 2019; Wei et al., 2020; Jiang et al., 2020a) transform given prediction scores, typically provided by an unfair model, to satisfy a certain fairness level.\nMost group-fair algorithms correspond to specific group fairness measures, typically defined by explicit quantities such as prediction scores and sensitive attributes. For example, demographic parity (Calders et al., 2009; Feldman et al., 2015; Angwin et al., 2016) considers the gap between two protected groups in terms of the positive prediction ratio.\nHowever, a shortcoming of such group fairness measures is that they only concern statistical disparities without accounting for implicit mechanisms about how a given model achieves group fairness. Consequently, models can achieve high levels of group fairness in very undesirable ways (see Section B of Appendix for an example). Dwork et al. (2012) also noted that focusing solely on group fairness can lead to issues such as subset targeting and the self-fulfilling prophecy. These observations serve as the motivation of this study. Moreover, our empirical investigations on real-world datasets reveal that unfairness on subsets can be observed in group-fair models learned by existing algorithms, which are designed to achieve group fairness merely (see Section 5).\nIn this paper, we first propose a new group fairness measure that reveals implicit behaviors of group-fair models. Based on the proposed measure, we develop an in-processing algorithm to learn group-fair models with less undesirable properties (e.g., unfairness on subsets), that cannot be explored or controlled by existing fairness measures.\nTo do so, we begin by introducing a notable mathematical finding: every group-fair model implicitly corresponds to a transport map, which moves the measure of one protected group to another. Building on this observation, we propose a new measure for group fairness called Matched Demographic Parity (MDP), which quantifies the average prediction gap between two individuals from different protected groups matched by a given transport map. We further prove that the reverse of this finding also holds: any transport map can be used in MDP to learn a group-fair model. Consequently, we develop an algorithm called Fairness Through Matching (FTM), designed to learn a group-fair model under a fairness constraint based on MDP with a given transport map. FTM can provide group-fair models having specific desirable properties (e.g., higher fairness on subsets) by selecting a transport map accordingly. Note that FTM is not designed to achieve the"}, {"title": "2 Preliminaries", "content": "optimal fairness-prediction trade-off, rather, the focus is on mitigating undesirable properties when achieving group fairness (e.g., unfairness on subsets).\nIn contrast, existing algorithms that focus solely on group fairness may lack such flexibility. In fact, FTM with a transport map that is not carefully selected could result in unreasonable group-fair models, even though group fairness is achieved.\nTherefore, the key to effectively using FTM lies in selecting a good transport map. To this end, we propose two specific options for the transport map: one is the optimal transport (OT) map in the input space, and the other is the OT map in the product space of input and output. Each is designed to achieve specific goals. For example, the former achieves higher fairness levels on subsets, while the latter yields better prediction performance and attains higher levels of equalized odds compared to the former. Experiments on real benchmark datasets support our theoretical findings, showing that FTM successfully learns group-fair models with more desirable properties, than those learned by existing algorithms for group fairness.\nMain contributions\n1. We introduce a notable mathematical observation: every group-fair model has a corresponding implicit transport map. Building on this finding, we present a novel measure of group fairness called Matched Demographic Parity (MDP).\n2. We prove that any transport map can be used in MDP to learn group-fair models. Subsequently, we devise a novel algorithm called Fairness Through Matching (FTM), designed to find a group-fair model using a constraint based on MDP with a given transport map. We propose two favorable transport maps tailored for specific purposes.\n3. Experiments on benchmark datasets illustrate that FTM successfully learns group-fair models. Furthermore, we examine the benefits of the two proposed transport maps in learning more desirable group-fair models, compared to those learned by existing algorithms that focus solely on achieving group fairness."}, {"title": "2.1 Notations & Problem setting", "content": "In this section, we outline the mathematical notations used throughout this paper. We focus on binary classification task in this study. We denote $X \\in \\mathcal{X} \\subset \\mathbb{R}^d$ and $Y \\in \\mathcal{Y} = \\{0,1\\}$ as the d-dimensional input vector and the binary label, respectively. Whenever necessary, we split $\\mathcal{X}$, i.e., the domain of $X$, with respect to $S$ and write $\\mathcal{X}_s$ as the domain of $X | S = s$. For given $s \\in \\{0,1\\}$, we let $s' = 1 - s$. Assuming the pre-defined sensitive attribute is binary, we denote $S \\in \\{0,1\\}$ as the binary sensitive attribute. For a given $s \\in \\{0,1\\}$, the realization of $S$, we write $s' = 1 - s$. For the probability distributions of these variables, let $P$ and $P_X$ represent the joint distribution of $(X, Y, S)$ and the marginal distribution of $X$, respectively. Furthermore, let $P_s = P_{X|S=s}, s \\in \\{0,1\\}$ be the conditional distributions of $X$ given $S = s$. We write $\\mathbb{E}$ and $\\mathbb{E}_s$ as the corresponding expectations of $P$ and $P_s$, respectively. For observed data, denote $\\mathcal{D} = \\{(x_i, Y_i, S_i)\\}_{i=1}^n$ as the training dataset, consisting of $n$ independent copies of the random tuple $(X, Y, S) \\sim P$. Let $||\\cdot||_p$ denote the $\\mathbb{L}_p$ norm. We also write the $\\mathbb{L}_2$ norm by $||\\cdot||$, i.e., $||\\cdot|| = ||\\cdot||_2$ for simplicity.\nWe denote the prediction model as $f = f(\\cdot, s)$, $s \\in \\{0, 1\\}$, which is an estimator of $\\mathbb{P}(Y = 1 | X = \\cdot)$, belonging to a given hypothesis class $\\mathcal{F} \\subset \\{f : \\mathcal{X} \\times \\{0,1\\} \\rightarrow [0,1]\\}$. Note that the output of $f$ is restricted to the interval $[0,1]$, making $f$ bounded. For simplicity, we sometimes write $f(\\cdot, s) = f_s(\\cdot)$ whenever necessary. For given $f$ and $s \\in \\{0,1\\}$, we denote $P_{f_s}$ as the conditional distribution of $f(X, s)$ given $S = s$. Furthermore, let $\\mathbb{C}_f := \\mathbb{I}(f(\\cdot, s) \\geq \\tau)$ be the classification rule based on $f$, where $\\tau$ is a specific threshold (typically, $\\tau = 0.5$)."}, {"title": "2.2 Measures for group fairness & Definition of group-fair model", "content": "In the context of group fairness, various measures have been introduced to quantify the gap between predictions of each protected group. The original measure for Demographic Parity (DP), $\\Delta_{DP}(f) :="}, {"title": "2.3 Optimal transport", "content": "The concept of the Optimal Transport (OT) provides an approach for geometric comparison between two probability measures. For given two probability measures $Q_1$ and $Q_2$, a map $T$ from $Supp(Q_1)$ to $Supp(Q_2)$ is called a transport map from $Q_1$ to $Q_2$ if the push-forward measure $T_{\\#} Q_1$ is equal to $Q_2$ (Villani, 2008). Here, the push-forward measure is defined by $T_{\\#}Q_1(A) = Q_1(T^{-1}(A))$ for any measurable set $A$. In other words, $T_{\\#} Q_1$ is the measure of $T(X), X \\sim Q_1$. For given source and target distributions $Q_1$ and $Q_2$, the OT map from $Q_1$ to $Q_2$ is the optimal one among all transport maps from $Q_1$ to $Q_2$. In this context, 'optimal' means minimizing transport cost, such as $\\mathbb{L}_p$ distance in Euclidean space.\nMonge (1781) originally formulates this OT problem: for given source and target distributions $Q_1, Q_2$ in $\\mathbb{R}^d$ and a cost function $c$ (e.g., $\\mathbb{L}_2$ distance), the OT map from $Q_1$ to $Q_2$ is defined by the solution of $\\min_{T:T_{\\#}Q_1 = Q_2} \\mathbb{E}_{X\\sim Q_1} (c(X, T(X)))$. If both $Q_1$ and $Q_2$ are discrete with the same number of support points, the OT map exists as a one-to-one mapping. For the case when $Q_1$ and $Q_2$ are discrete but have different numbers of supports, Kantorovich relaxed the Monge problem by seeking the optimal coupling between two distributions (Kantorovich, 2006). The Kantorovich problem is formulated as $\\inf_{\\pi \\in \\Pi(Q_1, Q_2)} \\mathbb{E}_{X,Y\\sim\\pi} (c(X, Y))$, where $\\Pi(Q_1, Q_2)$ is the set of all joint measures of $Q_1$ and $Q_2$. Note that this formulation can be also applied to the case of $Q_1$ and $Q_2$ with an identical number of support. See Section C of Appendix for more details about the Kantorovich problem.\nVarious feasible estimators for the OT map have been developed (Cuturi, 2013; Genevay et al., 2016), and applied to diverse tasks such as domain adaptation (Damodaran et al., 2018; Forrow et al., 2019) and computer vision (Su et al., 2015; Li et al., 2015; Salimans et al., 2018), to name a few."}, {"title": "2.4 Related works", "content": "Algorithmic fairness Group fairness is a fairness notion aimed at preventing discriminatory predictions over protected (demographic) groups divided by pre-defined sensitive attributes. Among various notions of group fairness, Demographic Parity (DP) (Calders et al., 2009; Feldman et al., 2015; Agarwal et al., 2019; Jiang et al., 2020b; Chzhen et al., 2020) quantifies the statistical gap in predictions between two different protected groups. Other measures, including Equal opportunity (Eqopp) and Equalized Odds (EO), consider groups conditioned on both the label and the sensitive attribute (Hardt et al., 2016a). Various algorithms have been proposed to learn group-fair model satisfying these group fairness notions (Zafar et al., 2017; Donini et al., 2018; Agarwal et al., 2018; Madras et al., 2018; Zafar et al., 2019; Chuang & Mroueh, 2021; Kim et al., 2022a)."}, {"title": "3 Learning group-fair model through matching", "content": "Individual fairness, initially introduced by Dwork et al. (2012), is another fairness notion based on the philosophy of treating similar individuals similarly. It is noteworthy that Dwork et al. (2012) highlighted that focusing solely on group fairness is not always a complete answer, pointing out issues such as subset targeting and the self-fulfilling prophecy. Subsequent researches (Yona & Rothblum, 2018; Yurochkin et al., 2020; Yurochkin & Sun, 2021; Petersen et al., 2021) have been studied in both theories and methodologies. However, individual fairness has two bottlenecks: (i) it strongly depends on the choice of the similarity metric, which hinders its practical applicability, and (ii) by itself, it does not ensure group fairness when the distributions of protected groups differ significantly. See Section 3.3 for detailed comparison between individual fairness and our proposed framework.\nCounterfactual fairness, which requires treating a counterfactual individual similarly to the original individual, has been studied by Kusner et al. (2017); Chiappa & Gillam (2018); von K\u00fcgelgen et al. (2022); Nilforoshan et al. (2022). Its application, however, is limited since it requires causal models, which are difficult to obtain only with observed data. Instead of using graphical models to define counterfactuals, this paper suggests using a transport map from $\\mathcal{X}_s$ to $\\mathcal{X}_{s'}$ having certain desirable properties. See Proposition 4.3 in Section 4.1 for the relationship between counterfactual fairness and our proposed algorithm.\nOn the other hand, in presence of multiple sensitive attributes, the concept of subgroup fairness can be considered (Kearns et al., 2018a;b; Shui et al., 2022; Mehrotra et al., 2022; Molina & Loiseau, 2022; Carvalho et al., 2022), emphasizing the need for models that satisfy fairness for all subgroups defined over the multiple sensitive attributes. In addition, Wachter et al. (2020); Simons et al. (2021); Mougan et al. (2024) have suggested that not only statistically equal outcome but also the notion of equal treatment, i.e., treating individuals with equal reasons, should be considered in algorithmic fairness.\nFair representation learning (FRL) FRL algorithm aims at searching a fair representation space, in the sense that the conditional distributions of the encoded representation with respect to the sensitive attribute are similar (Zemel et al., 2013). After learning the fair representation, FRL constructs a fair model by applying a supervised learning algorithm to the fair representation space. Initiated by Edwards & Storkey (2016), various FRL algorithms have been developed (Madras et al., 2018; Zhang et al., 2018) motivated by the adversarial-learning technique used in GAN (Goodfellow et al., 2014). See Section 3.3 for the detailed comparison between FRL and our proposed framework.\nApplications of the OT map to algorithmic fairness Several studies, including Gordaliza et al. (2019); Jiang et al. (2020b); Chzhen et al. (2020); Silvia et al. (2020); Buyl & Bie (2022), have employed the OT map for algorithmic fairness. Gordaliza et al. (2019) introduced a fair representation learning method that aligns inputs from different protected groups using the OT map. Jiang et al. (2020b); Chzhen et al. (2020); Silvia et al. (2020) proposed aligning prediction scores from different protected groups using the OT map or OT-based barycenter. Buyl & Bie (2022) developed a method that projects prediction scores onto a fair space by optimizing the projection through minimizing the transport cost calculated on all pairs of inputs.\nMost of these algorithms (e.g., Jiang et al. (2020b); Chzhen et al. (2020); Silvia et al. (2020)) focus on applying the OT map in the prediction space, i.e., aligning two conditional distributions $P_{f_0}$ and $P_{f_1}$. These methods fundamentally differ from our proposed approach, which focuses on applying the OT map in the input space. On the other hand, Gordaliza et al. (2019) and our approach are particularly similar in the sense that both apply the OT map on the input space. See the detailed comparison between Gordaliza et al. (2019) and our approach in Section 3.3.\nIt is worth noting that our proposed algorithm becomes the first to define a group fairness measure based on the transport map in the input space and to propose reasonable choices for the transport map.\nThe goal of this section is to explore and specify the correspondence between group-fair models and transport maps. In Section 3.1, we show that every group-fair model has a corresponding implicit transport map in the input space that matches two individuals from different protected groups. We then introduce a new fairness measure based on the correspondence. In Section 3.2, we show the reverse, i.e., any given"}, {"title": "3.1 Matched Demographic Parity (MDP)", "content": "transport map can be used to learn a group-fair model, then present our proposed algorithm for learning group-fair models under a fairness constraint based on a given transport map.\nProposition 3.1 below shows that for a given perfectly group-fair model $f$ (i.e., $P_{f_0} = P_{f_1}$ or equivalently $\\Delta = 0$), there exists an corresponding transport map in the input space that matches two individuals from different protected groups. Its proof is in Section A of Appendix. Throughout this section, we assume a condition (C): $P_s, s = 0, 1$, are absolutely continuous with respect to the Lebesgue measure. This regularity condition is assumed to simplify the discussion, since it guarantees the existence of transport maps between two distributions. See Proposition A.1 in Section A of Appendix, which presents a similar result for the case where $P_s, s = 0,1$ are discrete. Let $\\mathbb{T}^{trans}$ be the set of all transport maps from $P_s$ to $P_{s'}$.\nProposition 3.1 (Fair model $\\rightarrow$ Transport map: perfect fairness case). For any perfectly group-fair model $f$, i.e., $P_{f_0} = P_{f_1}$, there exists a transport map $T_f \\in \\mathbb{T}^{trans}$ satisfying $f(X, s) = f(T_f(X), s')$, a.e.\nThe key implication of this mathematical proposition is that all perfectly group-fair models are not the same and the differences can be identified by their corresponding transport maps. We can similarly define a transport map corresponding to a given not-perfectly group-fair model (i.e., $\\Delta > 0$), by using a novel fairness measure termed Matched Demographic Parity (MDP) defined as below.\nDefinition 3.2 (Matched Demographic Parity). For a given model $f \\in \\mathcal{F}$ and a transport map $T_s \\in \\mathbb{T}^{trans}$, the measure for MDP is defined as\n$\\Delta_{MDP}(f, T_s) := \\mathbb{E}_S |f(X, s) - f(T_s(X), s')|$.\nThe idea behind MDP is that two individuals from different protected groups are matched by $T_s$, and $\\Delta_{MDP}(f, T_s)$ quantifies the similarity between the predictions of these two matched individuals. Subsequently, Theorem 3.3 below presents a relaxed version of Proposition 3.1, showing that any group-fair model $f$ has a transport map $T_s$ such that $\\Delta_{MDP}(f, T_s)$ is small. Refer to Section A of Appendix for the proof of Theorem 3.3 and see Figure 1 for the illustration of MDP.\nTheorem 3.3 (Fair model $\\rightarrow$ Transport map: relaxed fairness case). Fix a fairness level $\\delta \\geq 0$. For any given group-fair model $f$ such that $\\Delta_{TVDP}(f) \\leq \\delta$, there exists a transport map $T_s \\in \\mathbb{T}^{trans}$ satisfying $\\Delta_{MDP}(f, T_s) \\leq 2\\delta$."}, {"title": "Definition 3.4 (Fair matching function of $f$)", "content": "For a given $f$, denote $\\mathbb{T}_f := \\arg \\min_{T_s \\in \\mathbb{T}^{trans}} \\Delta_{MDP}(f, T_s), s \\in \\{0,1\\}$. For $\\hat{s} := \\arg \\min_{s \\in \\{0,1\\}} \\Delta_{MDP}(f, T_s)$, the fair matching function of $f$ is defined as $T_f := \\mathbb{T}_{\\hat{s}}$.\nNote that the existence of this fair matching function is guaranteed by Brenier's theorem (Villani, 2008; H\u00fctter & Rigollet, 2021). To be specific, since $P_s, s \\in \\{0,1\\}$ are absolutely continuous by (C), and the cost function in MDP, i.e., $c(x, y) := |f(x, s) - f(y, s')|$, is lower semi-continuous and bounded from below, the minimizer of $\\Delta_{MDP}(f, T_s)$ uniquely exists."}, {"title": "Practical computation of the fair matching function", "content": "In practice, we estimate the fair matching function using the observed data $\\mathcal{D} = \\{(x_i, Y_i, S_i)\\}_{i=1}^n$. Let $\\mathcal{D}_0 = \\{x_i : s_i = 0\\} = \\{x_i^{(0)}\\}_{i=1}^{n_0}$ and $\\mathcal{D}_1 = \\{x_j : s_j = 1\\} = \\{x_j^{(1)}\\}_{j=1}^{n_1}$ be the set of inputs given the sensitive attribute, where $n_0 + n_1 = n$. We here introduce practical methods for computing the fair matching function in Definition 3.4, by considering two cases:\n1. Case of $n_0 = n_1$: When the sizes of two protected groups are equal ($n_0 = n_1$), we can easily find the fair matching function by quantile matching. We first sort the scores in $\\{f_s(x)\\}_{x \\in \\mathcal{D}_s}$ for each group $s \\in \\{0,1\\}$. Then, we match the individuals having the same rank (i.e., quantile) in each set, thereby obtaining the fair matching function of $f$. This straightforward procedure is theoretically guaranteed by the definition of 1-Wasserstein distance, which is calculated by quantile matching (Rachev & R\u00fcschendorf, 1998; Chzhen et al., 2020; Jiang et al., 2020b). We formally present this procedure in Proposition 3.5 below. Its proof is provided in Section A of Appendix. Let $F_s$ represents the cumulative distribution function of $f_s(x), x \\in \\mathcal{D}_s$. For technical simplicity, we assume that there is no tie in $\\{f_s(x) : x \\in \\mathcal{D}_s\\}$.\n2. Case of $n_0 \\neq n_1$: When the sizes differ, we can consider a stochastic fair matching function (a stochastic transport map that minimizes AMDP), which matches individuals with probability, not deterministically. In fact, a stochastic transport map corresponds to a joint distribution between two protected groups, i.e., a stochastic transport map can be defined by a joint distribution, as follows:\nDenote $Q$ as a joint distribution between $\\mathcal{D}_0$ and $\\mathcal{D}_1$, and let $X_0$ and $X_1$ be the random variables following the empirical distributions on $\\mathcal{D}_0$ and $\\mathcal{D}_1$, respectively. For a given joint distribution $Q$, the stochastic transport map $T_s$ (corresponding to the $Q$) is defined by $T_s(x^{(0)}) = x^{(1)}$ with probability $Q(X_0 = x^{(0)}, X_1 = x^{(1)})$. Once we find the stochastic fair matching function, i.e., the stochastic transport map (joint distribution $Q$) minimizing $\\Delta_{MDP}(f, Q) = \\mathbb{E}_{(X_0, X_1)\\sim Q} |f(X_0, 0) - f(X_1, 1)|$, we can compute its transport cost as $\\mathbb{E}_{(X_0, X_1)\\sim Q} ||X_0 - X_1 ||_2$. Note that the minimization of AMDP with respect to the stochastic transport map is technically equivalent to solving the Kantorovich problem, which can be easily solved by the use of linear programming. See Section A and Section C for more details about the stochastic transport map and the Kantorovich problem, respectively.\nAlternatively, in practice, we can apply a mini-batch sampling technique. We first sample two random mini-batches $\\mathcal{D}_0' \\subset \\mathcal{D}_0$ and $\\mathcal{D}_1' \\subset \\mathcal{D}_1$ with identical size $m$. Then, we follow the process in \u2018(1) Case of $n_0 = n_1$\u2019 above. The transport cost of the fair matching function can be then estimated by the average transport costs computed on many random mini-batches. See Remark A.2 in Section A for the statistical validity of this mini-batch technique."}, {"title": "Remark 3.6 (Usage of the transport cost of the fair matching function)", "content": "Furthermore, Remark 3.6 below explains how the transport cost can serve as a metric for assessing the desirability of given group-fair models.\nFor example, when choosing a model between two group-fair models with similar levels of group fairness or/and prediction accuracies, the model with the lower transport cost would be preferred. In Section 5.3.2, we compare transport costs of fair matching functions for two different group-fair models, using the mini-batch technique."}, {"title": "3.2 Fairness Through Matching (FTM): learning a group-fair model with a transport map", "content": "The goal of this section is to formulate our proposed algorithm. Before introducing it, we provide a theoretical support, which shows that a group-fair model can be constructed by MDP using any transport map. Theorem 3.7 below, which is the reverse of Theorem 3.3, shows that any transport map in the input space can construct a group-fair model. Precisely, for a given transport map, if a model provides similar predictions for two individuals who are matched by the transport map, then it is group-fair. The proof is given in Section A of Appendix.\nTheorem 3.7 (Transport map $\\rightarrow$ Group-fair model). For a given $T_s \\in \\mathbb{T}^{trans}$, if $\\Delta_{MDP}(f, T_s) \\leq \\delta$, then we have $\\Delta_{WDP}(f) \\leq \\delta$ and $\\Delta_{DP}(f) \\leq \\delta$.\nAgain, it is remarkable that a group-fair model and its corresponding transport map are closely related, i.e., every group-fair model has its corresponding implicit transport map, and vice versa. This finding can be mathematically expressed as follows. Let $\\Delta$ be a given (existing) fairness measure, and define $\\mathcal{F}_{\\Delta}(\\delta) := \\{f \\in \\mathcal{F} : \\Delta(f) \\leq \\delta\\}$ as the set of group-fair models of level $\\delta$ (with respect to $\\Delta$). Similarly, for MDP, define $\\mathcal{F}_{\\Delta_{MDP}}(T_s, \\delta) := \\{f \\in \\mathcal{F} : \\Delta_{MDP}(f, T_s) \\leq \\delta\\}$. Then, following from Theorem 3.3 and 3.7, we can conclude that the three measures (i.e., TVDP, WDP, and MDP) are closely related: $\\mathcal{F}_{\\Delta_{TVDP}}(\\delta) \\subseteq \\bigcup_{T \\in \\mathbb{T}^{trans}} \\{f : \\Delta_{MDP}(f, T_s) \\leq 2\\delta\\} \\subseteq \\mathcal{F}_{\\Delta_{WDP}}(2\\delta)$.\nAs discussed in Section 1 as well as several previous works (e.g., Dwork et al. (2012)), there exist group-fair models having undesirable properties such as subset targeting or self-fulfilling prophecy. The advantage of MDP is that we can screen out such undesirable group-fair models during the learning phase, to consider desirable group fair models only. And, using MDP achieves this goal by considering group-fair models whose transport maps have low transport costs. That is, we can search for a group-fair model only on $\\bigcup_{T \\in \\mathbb{T}^{good \\,trans}} \\{f: \\Delta_{MDP}(f, T_s) \\leq 2\\delta\\}$, where $\\mathbb{T}^{good \\,trans} \\subseteq \\mathbb{T}^{trans}$ is a set of specified \u2018good\u2019 transport maps. See Section 4 for such good transport maps that we specifically propose. It is worth noting that such screening would be challenging with existing group-fair measures.\nBased on Theorem 3.7, we develop a learning algorithm named Fairness Through Matching (FTM), which learns a group-fair model subject to MDP being small with a given transport map. FTM consists of two steps. First, we select a (good) transport map. Then, we learn a model under the MDP constraint with the selected transport map. The precise objective of FTM is formulated below.\nSuppose a transport map $T_s$ is selected (see Section 4 for the proposed transport maps). FTM solves the following objective for a given loss function $l$ (e.g., cross-entropy) and a pre-defined fairness level $\\delta \\geq 0$ :\nUnless there is any confusion, we write $f^{FTM}$ instead of $f^{FTM}(T_s)$ for simplicity. By Theorem 3.7, it is clear that $f^{FTM}$ is fair (i.e., $\\Delta_{WDP}(f^{FTM}), \\Delta_{DP}(f^{FTM}) \\leq \\delta$) for any transport map $T_s$. In practice, we estimate $f^{FTM}$ with observed data $\\mathcal{D}$ using mini-batch technique along with a stochastic gradient descent based algorithm (see Section 4 for details)."}, {"title": "3.3 Conceptual comparison of existing approaches and FTM", "content": "Individual fairness FTM and individual fairness are similar in the sense that they try to treat similar individuals similarly. A difference is that FTM aims to treat two individuals from different protected groups similarly, while the individual fairness tries to treat similar individuals similarly regardless of sensitive attribute (even when it is unknown). That is, similar individuals in FTM could be dissimilar in view of individual fairness"}]}