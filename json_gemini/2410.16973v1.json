{"title": "Learning Mathematical Rules with Large Language Models", "authors": ["Antoine Gorceix", "Bastien Le Chenadec", "Ahmad Rammal", "Nelson Vadori", "Manuela Veloso"], "abstract": "In this paper, we study the ability of large language models to learn specific mathematical rules such as distributivity or simplifying equations. We present an empirical analysis of their ability to generalize these rules, as well as to reuse them in the context of word problems. For this purpose, we provide a rigorous methodology to build synthetic data incorporating such rules, and perform fine-tuning of large language models on such data. Our experiments show that our model can learn and generalize these rules to some extent, as well as suitably reuse them in the context of word problems.", "sections": [{"title": "Introduction", "content": "We focus on a specific aspect of mathematical reasoning, namely the ability of large language models (LLMs) to learn specific abstract mathematical rules and to reuse them while answering word problems, namely questions formulated in natural language that are usually made up of a few sentences describing a scenario that needs to be solved through mathematics. We will also refer to these mathematical rules as \"skills\". Thus, we study the following research question: Can LLMs learn and generalize specific mathematical rules, and apply them in contexts that have not been seen during training, for example when answering word problems?\nWe will fine-tune models on carefully built synthetic data reflecting the mathematical rules of interest, and we provide a detailed description of our methodology for building such data in section 2 and appendix B. Broadly speaking, we want our training data to be presented similarly to what you would find in a mathematics textbook. For example, focusing on how to rearrange or simplify an equation, or how to use the distributivity property of the addition operator. But without word problems.\nThe data at test time, however, will contain word problems where the model first needs to translate the question in natural language into one or more equations, and then use various rules to solve these equations. This is what we will call bottom-up generalization, namely the model's ability to go from mathematical rules to answering word problems. We also provide an empirical study on the model's ability to generalize when increasing the mathematical complexity of the task. That is, when we increase parameters such as the number of variables, the number of equations, the variable name token length, and so on. We will call this top-down generalization, as we zoom into a given mathematical rule to make it more complex. We comment on the related work in appendix A.\nOur contributions. We provide a rigorous methodology to create synthetic data containing specific mathematical rules such as manipulating equations (section 2). We show how fine-tuning models on those rules allows them to be reused in the context of word problems, while maintaining performance"}, {"title": "Building Synthetic Data Incorporating Mathematical Rules", "content": "We aim at constructing synthetic data reflecting specific mathematical rules that we would like our model to learn. The detailed description of the synthetic data is provided in appendix B, and is presented similarly to what you would find in a mathematics textbook, without word problems.\nIn section 3 we focus on bottom-up generalization. To this extent we consider the following rules presented in section B.1: a) finding the roots of a quadratic polynomial; b) solving for a given variable in a linear equation in that variable, for example: solve for the variable x in $13 \\cdot x \\cdot y + 24 \\cdot y^2 = 17 \\cdot xz + 12 \\cdot yz$. $x = \\frac{12 \\cdot y \\cdot (z - 2 \\cdot y)}{13 \\cdot y - 17 \\cdot z}$; c) simplifying terms in an equation, for example: simplify the following expression: $-9 \\cdot x^2 - 8 \\cdot y + 5 \\cdot y$. By grouping terms: $-9 \\cdot x^2 - 3 \\cdot y$; d) isolating a variable in an equation of the form a = b * (c + d + . . . ) or a = b/(1/c + 1/d + ...).\nIn section 4 we focus on top-down generalization. We consider rules such as distributivity, exponentiation, manipulation of single and pairs of equations as well as solving single steps of Gaussian elimination, all presented in section B.2. In particular, we focus on the model's ability to apply those rules when the variables appearing in the equations are arbitrary strings, and study the impact on generalization performance. For example: Q: Expand this expression: (-5+soccer-dog)*(blue-sky). A: By the distributivity property: -5*blue+5*sky+soccer*blue- soccer * sky - dog*blue+dog*sky.\nThe goal is to learn the fundamental nature of the corresponding operators, which do not depend on the variable names."}, {"title": "Bottom-Up Generalization: Going from Mathematical Rules to Word Problems", "content": "We consider three classes of word problems. Finding a solution to these problems involves mainly two steps. First, the model needs to correctly translate the question in natural language into one or multiple equations. Second, the model needs to apply one or more mathematical skills to correctly manipulate the equations to answer the question. We choose our examples such that the baseline model is able to perform the first step but struggles on the second. Our goal when performing fine-tuning is as follows: keep the general knowledge required to solve the first step, while providing new tools to also solve the second. We will fine-tune Llama-3 8B Instruct on on a mix of synthetic data described in section B.1 together with data from the Orca dataset Mukherjee et al. (2023) acting as a regularizer in order to avoid catastrophic forgetting and maintain performance on common benchmarks (see section C.4). Additional details are provided in appendix C.\nQuadratic Polynomials. We consider simple geometric problems involving the calculation of areas for shapes with the same unknown side length. The total area of all shapes is known, while the unknown side length is to be determined. The solution involves two main steps. The first one is to formulate a quadratic equation that models the relationship between the known areas and the unknown side length. This equation is always a quadratic polynomial. The second step is to find the roots of this polynomial to find the unknown side length. Problems are constructed to always yield one positive and one negative root, ensuring no ambiguity in the solution. We generate many such problems by varying the names (pools, fields, etc.) and shapes (square, triangle, parallelogram, rectangle) of the surfaces, the number of such surfaces per shape (in average, 2.5 surfaces per shape per word problem), as well as the values of the areas and side lengths appearing in the problem. We present an example of prompt given to the model in Figure 1.\nWe provide three examples of prompts and responses to the model (3-shot), and assess the validity of the numerical solution. We evaluate the performance of our fine-tuned model against a baseline across varying levels of difficulty. The difficulty scale ranges from 0 to 100%, in increments of 20%, corresponding to the proportion of problems with non-integer roots. Non-integer roots present a"}, {"title": "Top-Down Generalization: Increasing the Mathematical Complexity of the Task", "content": "We fine-tune Llama-2 7B Chat on all the synthetic data described in section B.2 (and that data only) and evaluate the model's ability to perform the following rules: distributivity, commutativity, division, exponentiation, variable evaluation, remarkable identities, single equation and two equations manipulation. We check in section D.4 that the performance on general knowledge benchmarks remains relatively stable, and present experimental details in section D.6.\nSolving systems of equations by recursive call of our model. Here, we train the model on individual steps of the Gaussian elimination algorithm and show at test time that we can solve a full system by recursively calling the model. An example is presented in section D.2.\nExperiments on various mathematical rules. We analyze the ability of our model to perform the rules of interest under different configuration regarding the training vocabulary. We find that our fine-tuned Llama-2 model consistently outperforms other models across all considered mathematical rules. The full results are presented in section D.3. In section D.5, we present the model's performance on some word problems, in particular we emphasize its ability to combine skills in figures 15 and 16.\nAblation study on the distributivity rule. In section D.1 we focus on the distributivity rule. We train the model on data where the variables appearing in the equations take value in subsets of increasing"}, {"title": "Related Work", "content": "In recent years, researchers have trained large language models (LLMs) on data of unprecedented size, and studying the emergent capabilities of these models on various tasks has been a central focus Bubeck et al. (2023). Nevertheless, mathematical reasoning remains a challenge, although the rate of improvement of LLMs on solving mathematical problems has been significant Ahn et al. (2024). A prize was even launched with the goal of getting AI to perform at gold medal level at the International Mathematical Olympiad XTX Investments (2024), emphasizing the importance of this research topic.\nProperly evaluating LLMs on mathematical reasoning tasks presents significant challenges, among which: i) data contamination: since LLMs are trained on vast amounts of data, it is often not clear that typical middle/high-school-level problems have not been seen during training; ii) assessment of proof correctness: it is not easy to automatically determine whether a sequence of mathematical reasoning steps - and more generally a proof - is correct. On this topic we note the work conducted with proof assistants such as Lean Yang et al. (2023), for which we can immediately determine whether a proof is true or false as mathematical reasoning is seen as a computer program; iii) the variety of mathematical problems is significant and their complexity/difficulty is not trivial to establish. On this topic we believe that work allowing to suitably categorize problems would be beneficial to the community.\nTORA Gou et al. (2024) combines natural language reasoning (rationale) with symbolic solvers (programmatic reasoning). WizardMath Luo et al. (2023) applies Reinforcement Learning from Evol-Instruct Feedback (RLEIF) to the domain of mathematics. MathPrompter Imani et al. (2023) uses the zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions to solve the same problem in different ways and thereby raises the confidence level in the output results. Alphageometry Trinh et al. (2024) significantly advances the state-of-the-art on geometry problems at the level of International Mathematical Olympiads. Llemma Azerbayeva et al. (2024) is a suite of models tailored for mathematical reasoning. Galactica Taylor et al. (2022) is a large language model that can store, combine and reason about scientific knowledge. Minerva Lewkowycz et al. (2022) solves scientific and mathematical questions in natural language, generates step-by-step solutions using Latex notation, and was trained on data composed of scientific and mathematical information. Yuan et al. (2023) applies rejection sampling fine-tuning (RFT), that uses supervised models to generate and collect correct reasoning paths in order to augment fine-tuning datasets. Zheng et al. (2023) proposes progressive-hint prompting that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide towards correct answers. Wang et al. (2023) proposes plan-and-solve prompting, that consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. Bubeck et al. (2023) presents among other things an experimental study of GPT4's mathematical reasoning capabilities. Jelassi et al. (2023) examine how transformers cope with two challenges: learning basic integer arithmetic, and generalizing to longer sequences than seen during training . Lample and Charton (2020) presents a deep learning approach for tasks such as symbolic integration and solving differential equations. LeanDojo Yang et al. (2023) explores mathematical reasoning using the Lean proof assistant, among other things they suitably retrieve relevant premises from a vast math library. Finally, the recent survey paper Ahn et al. (2024) presents the state of the field on the topic of mathematical reasoning."}, {"title": "Detailed Description of the Synthetic Data Incorporating Mathematical Rules", "content": "Our data is a combination of text and mathematical expressions, which are built from a set of types and operations. A type is a fundamental object that can be manipulated (a variable, an integer, a decimal number, etc.). By applying arithmetic operations to these types, we can construct complex mathematical expressions in the form of Abstract Syntax Trees (AST).\nSpecifically we consider the following types for our synthetic data:\n\u2022 integers,\n\u2022 decimals with up to two decimal places,\n\u2022 arbitrary strings, either made from the concatenation of k tokens taken from a subset of the tokenizer's vocabulary \u00b9 which we call full vocabulary, or from the concatenation of a latin or greek letter with a digit which we call restricted vocabulary,\nWe overload our programming language's arithmetic operators (+, -, *, /, ^) to implement a set of simplification rules. For instance, when adding two integers, we generally simplify the expression by evaluating the sum (though each simplification rule can be turned off). When no simplification applies, a node is added to the AST. This custom data structure allows us to control precise simplification details such as the order of terms, whether to evaluate numerical expressions, etc. In contrast, using a symbolic mathematics library such as Sympy would not allow us to control these details, as some simplifications are performed systematically. We also implement a translation function that converts our custom data structure to Sympy's data structure, and vice versa, to handle more advanced simplification rules externally, striking a balance between control and flexibility.\nTypes only hold their own value (e.g. 4, \"x\", 12.51), and the negative sign is considered a unary operation, while the other operations are binary. Thus our expressions are ASTs where each leaf is a type, and each internal node is a unary or binary operation. In simpler cases we might consider expressions of the form $w_1 o_1 w_2 o_2 ... o_{n-1} w_n$, where each $w_i$ is a type, and each $o_i$ is an arithmetic operation.\nWe are then able to represent our data structure as a string, taking into account operator precedence and associativity. We can thus construct mathematical expressions programatically, and generate prompts for our models. All of our prompts are of the form *start Question *mid Answer *end, where *start, *mid and *end are model-specific strings that separate the question from the answer. For example *start =[INST], *mid =[/INST] and *end is empty in the case of Llama-2 Chat. Similar string are used in the case of Llama-3 Instruct. In the below, $w_i$ and $o_i$ will denote respectively types and operations, which value can change depending on the context. For each one of the cases below, the number of terms appearing in the expressions is sampled randomly between some pre-specified lower and upper bounds."}, {"title": "Rules Used in Section 3", "content": "For the rules below, when relevant we use Sympy to solve equations symbolically and factorize the result.\nQuadratic polynomials. We want our model to solve quadratic polynomials. $w_1, w_2, w_3$, are random integers and $w_4$ is an arbitrary token.\n\u2022 Question. Solve this quadratic equation: $w_1 \\times w_4^2 + w_2 \\times w_4 + w_3 = 0$.\n\u2022 Answer. We first calculate the discriminant $\\Delta = w_2^2 - 4 w_1 w_3$.\n  - If $\\Delta > 0$: The equation has two distinct real roots:\n    $w_4 = \\frac{-w_2 \\pm \\sqrt{\\Delta}}{2w_1}$\n  - If $\\Delta = 0$: The equation has exactly one real root:\n    $w_4 = \\frac{-w_2}{2w_1}$"}, {"title": "Rules Used in Section 4", "content": "Distributivity. We expand an expression $w_A$ consisting of the product of two brackets $w_A = (\\sum_{i=1}^n w_i) * (\\sum_{j=1}^m w_{n+j})$, to obtain the distributed expression $w_B = \\sum_{i=1}^n \\sum_{j=1}^m w_i * w_{n+j}$.\n\u2022 Question. Expand this expression: $w_A$.\n\u2022 Answer. By the distributivity property: $w_B$.\n\u2022 Example. Question. Expand this expression: (-5+ soccer dog) * (blue - sky). Answer. By the distributivity property: -5 * blue + 5 * sky + soccer * blue - soccer * sky - dog * blue + dog * sky."}, {"title": "Single equation manipulation", "content": "We consider two rules. The first consists in computing an affine transformation $w_3 * w_A + w_4$ of an equation $w_A$ of the form $w_1 = w_2$, namely $w_3 * w_1 + w_4 = w_3 * w_2 + w_4$. The second rule consists in \"simplifying\" an equation $w_A$ of the form $w_1 = w_2$ and performs three steps at once: putting the equation in standard form \u00b2, canceling out terms on both sides, and factorizing the remaining terms. We use the convention that equations are always encapsulated within two semicolons, in order to help the model recognize equations.\n\u2022 Questions. a) Assumptions: E1 ;wi;. Compute: $w_3 * E1 + w_4$. b) Simplify: ;wi;.\n\u2022 Answers. a-b) We get: ;wi;.\n\u2022 Example a). Question. Assumptions: E1 ;cat + dog - 2 = tree;. Compute: sky * E1 + 8. Answer. We get: ;sky * cat + sky * dog \u2013 sky * 2 + 8 = sky * tree + 8;.\n\u2022 Example b). Question. Simplify the equation: ;7*dog+sky*cat+sky*dog-sky*2+8=\nsky * tree + 7 * dog + 8 \u2013 blueberry;. Answer. We get: ; sky * (cat + dog - 2 - tree) +\nblueberry = 0;."}, {"title": "Commutativity", "content": "We study the commutativity of an operation $o_A \\in {*, \\pm}$ in an expression $w_A := w_1 o_1 w_2 o_2 ... o_{n-1} w_n$. If $o_A = *$, then $o_i = * \\forall i$, and if $o_A = \\pm$, then each i is either + or \u2013 with equal probability. We specify the two types wi, wj among n to which we want to apply commutativity, and then commuted expression $w_B$ is the same as $w_A$ but where we switch the positions of wi, wj. If one of the latter appears more than once in the expression (i.e. $w_i = w_k$ or $w_j = w_k$ for some $k \\neq i, j$), we choose the first from the left.\n\u2022 Question. Apply the commutativity property of $o_1$ to $w_1, w_2$ in $w_A$.\n\u2022 Answer. By the commutativity property of $o_1$: $w_A = w_B$.\n\u2022 Example. Question. Apply the commutativity property of * to cat, 5 in 5 * cat * soccer. Answer. We get: cat * 5 * soccer."}, {"title": "Division", "content": "We consider three properties of the division: $\\frac{w_1 o_1 o_2}{w_3} = \\frac{w_1}{w_3} o_1 \\frac{o_2}{w_3}, where o_1 \\in {+, -};  \\frac{w_1 w_2}{w_3 w_4}= \\frac{w_1}{w_3} * \\frac{w_2}{w_4};  \\frac{w_1}{w_2 w_3} =  \\frac{w_1 w_2}{w_3}$\n\u2022 Question. Use a fundamental property of the division in $w_A$.\n\u2022 Answer. By a property of the division: $w_B$.\n\u2022 Example. Question. Use a fundamental property of the division in (-sky * blue)/(tree * -cloud). Answer. By a property of the division: (-sky)/(tree) * (blue)/(-cloud)."}, {"title": "Exponentiation", "content": "We consider five properties of the exponentiation: $w^0 = 1$; the definition of exponentiation $w^n = w * ... * w$ if n is a positive integer, the reciprocal of the latter if n is a negative integer; if n, m are signed integers: $w^n * w^m = w^{n+m}; (w_1 * w_2)^n = w_1^n * w_2^n; \\frac{w^n}{w^m}= w^{n-m}$.\n\u2022 Questions. a) Apply the definition of the exponentiation to: $w_A$. b) Give the value of: $w^0$. c) Use a fundamental property of the exponentiation: $w_A$.\n\u2022 Answers. a) By definition of the exponentiation: $w_B$. b-c) By a property of the exponentiation: $w_B$..\n\u2022 Examples.\nQuestion. Use a fundamental property of the exponentiation: (bluesky^{-3})/(bluesky^0). Answer. By a property of the exponentiation: bluesky^{-12}.\nQuestion. Apply the definition of the exponentiation to: bluesky\u00b3. Answer. By definition of the exponentiation: bluesky * bluesky * bluesky.\nQuestion. Give the value of: bluesky\u2070. Answer. By fundamental property of the exponentiation: bluesky = 1."}, {"title": "Variable evaluation", "content": "We teach the model to substitute k \u2265 0 types in a given equation, based on some assumptions on these types (if k = 0, nothing occurs). Precisely, assume that we have an equation $w_A$ of the form $w_1 o_1 w_2 o_2 ... o_{n-1} w_n = w_{n+1} o_{n+1} w_{n+2} o_{n+2} ... o_{n+m-1} w_{n+m}$, and that by assumption $w_{n_i} = w'_{n_i}$ for a set of indexes ${n_i}$ and some types $w'_{n_i}$. Then, the new equation $w_B$ is the same as $w_A$ but with $w'_{n_i}$ replaced by $w_{n_i}$.\n\u2022 Question. Assumptions: $w_{n_1} = w'_{1},...,w_{n_k} = w'_{n_k}$. Based on the assumptions, evaluate $w_A$.\n\u2022 Answer. The evaluated expression: $w_B$.\n\u2022 Example. Question. Assumptions: sky = 2, blueberry = cat. Based on the assumptions, evaluate dog - tree + sky = blueberry. Answer. The evaluated expression: dog \u2013 tree +\n2 = cat."}, {"title": "Remarkable identities", "content": "We consider the remarkable identity $(w_1 o_1 w_2)^2 = w_1^2 + w_2^2 o_1 2 w_1 w_2$, where $o_1 \\in {+,-}$.\n\u2022 Question. Expand this expression: $w_A$.\n\u2022 Answer. By the remarkable identity properties: $w_B$.\n\u2022 Example. Question Expand this expression: (sky \u2013 blueberry) \u00b2. Answer. By the remarkable identity properties: sky\u00b2 + blueberry\u00b2 - 2 * sky * blueberry."}, {"title": "Combination of two equations", "content": "We consider two skills. The first skill consists in computing an affine transformation $w_5 * w_A + w_6 * w_B$ of two equations $w_A, w_B$ of respective forms $w_1 = w_2$ and $w_3 = w_4$. Here, $w_1, w_2, w_3, w_4$ are expression types. This yields the equation $w_5 * w_1 + w_6 * w_3 = w_5 * w_2 + w_6 * w_4$. The second skill consists in being able to determine whether two equations are equivalent. This is done in several steps: first, the two equations are put into standard form. Then, we compute the difference of their left-hand sides. If we get 0 = 0, then we conclude that the two equations are equivalent, otherwise they are not as the left-hand residual is not zero. In the latter case, the residual is provided in factorized form."}, {"title": "Systems of equations", "content": "Remember that Gaussian elimination is a method to solve a system of n linear equations in k steps. Each step transforms the system at step i to a new, simpler system at step i + 1. Our aim is to teach the model to perform any step i \u2192 i + 1. For this, we create a system of k equations with n \u2265 k variables. The latter are always taken to be of the m-token base type. Then, we generate a matrix M of coefficients of size k \u00d7 (n + 1) associated with the system, where the coefficients are arbitrary types:\n$M = \\begin{bmatrix}\na_{11} & a_{12} & ... & a_{1n} & b_1\\\\\na_{21} & a_{22} & ... & a_{2n} & b_2\\\\\\vdots & : & & : & : \\\\\\na_{k1} & a_{k2} & ... & a_{kn} & b_k\n\\end{bmatrix}$\nLet $x_1,..., x_n$ be the variables, thus our system of equations at step 0 is:\n$a_{11}x_1 + a_{12}x_2 + ... + a_{1n}x_n = b_1$\n$a_{21}x_1 + a_{22}x_2 + ... + a_{2n}x_n = b_2$\n:\n$a_{k1}x_1 + a_{k2}x_2 + ... + a_{kn}x_n = b_k$\nWe then apply a step of Gaussian elimination, which is a method that transforms the coefficient matrix of the system into row-echelon form and then back-substitutes. First, we perform row operations to transform the augmented matrix into row-echelon form. The goal is to create zeros below the diagonal elements. To do this, we first divide the first row $L_1$ by the coefficient of $x_1$ and then \\\\\\$Vi \\in [2, k]$ we update the expression of the ith row $L_i$, $L_i \\rightarrow L_i - a_{i1} * L_1$. This eliminates the $x_1$ variable from equation i. Then we do the same $\\forall i \\in [2, k], \\forall j \\in [i, k]$ we update $L_j \\rightarrow L_j - a_{ji} * L_i$. This eliminates variable i from each equation j. At the end of this first phase, we obtain a new matrix"}, {"title": "Solving Systems of Equations by Recursive Call", "content": "System solving is a difficult task for most language models, as it requires the elaboration of a series of reasoning steps. Language models have a finite context window, which constrains the amount of information they can process. Furthermore, numerous variables must be maintained and manipulated over multiple steps. Language models struggle with long-range dependencies, where the relevant information from earlier steps needs to be accurately memorized and applied in later steps. Even state-of-the-art models have difficulty solving systems with more than two variables and equations. To overcome these problems, our idea is to break down system solving into elementary steps and teach our model to correctly transition from each one of the steps to the next (as opposed to training it on the full resolution). The training data contains examples where the system is already resolved (termination condition), and when it is the case, the model recognizes that there is nothing left to do and returns The system is already simplified.\nThe detailed construction of the corresponding data from steps i \u2192 i + 1 of the Gaussian elimination is provided in section B.2. We considered systems from 1 to 5 variables. For each step transition i \u2192 i + 1, we report a score of 1 if the model predicted string matches exactly the ground truth, and 0 otherwise. We obtain an average score of 88.5 \u00b10.5%. The latter was computed over 100 examples and 2 random seeds."}, {"title": "Experiments on All Mathematical Rules", "content": "We fine-tune our model on all the mathematical rules detailed in section B. To evaluate the models, we considered eight different configurations:\n\u2022 4 train configurations: We use the same hyperparameters as the training dataset: 3 to 5 variables per instance. We consider two vocabulary settings:\n  - Full vocabulary: The full Llama-2 vocabulary. Each term is a concatenation of 1 to 3 Llama-2 tokens.\n    * With integers: 10% of the variables in an expression are integers, while the rest are a concatenation of Llama-2 tokens.\n    * Without integers: All the variables are a concatenation of Llama-2 tokens.\n  - Restricted vocabulary: The vocabulary consists of Latin and Greek letters, one token per variable, and no integers allowed.\n    * With digits: The vocabulary consists of Latin and Greek letters and a concatenation of these letters with numbers from 0 to 9 (e.g., $\\alpha0$).\n    * Without digits: The vocabulary consists of only Latin and Greek letters without any numbers.\n\u2022 4 test configurations: We increase the number of variables to 6 to 7 variables per instance. We consider two vocabulary settings:\n  - Full vocabulary: The full Llama-2 vocabulary. Each term is a concatenation of 3 to 5 Llama-2 tokens.\n    * With integers: 10% of the variables in an expression are integers, while the rest are a concatenation of Llama-2 tokens.\n    * Without integers: All the variables are a concatenation of Llama-2 tokens.\n  - Restricted vocabulary: Same as the training configuration\nThe goal of excluding integers in some configurations is to ensure robust evaluation of symbolic reasoning, avoiding cases where models might output correct numerical results without demonstrating an understanding of the underlying properties. For example, in evaluating the division property $(1+5)/3 = 1/3 + 5/3$, we expect models to show understanding of this property rather than simply computing numerical expressions. On the other hand, the restricted vocabulary relying on the Latin and Greek alphabetical letters ensures a fair comparison to baseline models likely unfamiliar with unusual tokens in Llama 2's tokenizer. The baseline models considered are the instruct versions of Llemma Azerbayeva et al. (2024), Llama-2, Llama-3, Mistral, and WizardLM Luo et al. (2023). Each model generated responses with a max_new_tokens set to 512. This token limit was determined to be sufficient for generating correct answers based on tests with multiple lengths (up to 1500 tokens), taking into consideration the verbose nature of these models. Each mathematical rule was evaluated on 100 examples over 2 seeds.\nOur fine-tuned Llama-2 model consistently outperforms other models across all considered mathematical rules, irrespective of the configuration. Notably, our model demonstrates strong generalization capabilities across various configurations. Baseline models demonstrate moderate performance on the restricted vocabulary and significantly underperform on the full vocabulary, which is expected due to their limited grasp of what a mathematical variable is. In contrast, our model maintains consistently high scores across the configurations. Additionally, we conduct a safety check by verifying string equality as a lower-bound metric to ensure the accuracy of our metrics.\nGiven the lengthy nature of the prompts in distributivity and two equations tasks, we decided to add another test configuration with the full vocabulary with integers in which we only increase the number of variables taking it from 6 to 7, while keeping the variables in the same shape as the ones in the training (a concatenation of 1 to 3 tokens). We only test this configuration on the two skills of distributivity and two equations."}, {"title": "Benchmark Performance", "content": "Catastrophic forgetting is a well-known issue when fine-tuning large language models, where the model's performance on the pre-training task decreases after fine-tuning on a new task. We evaluate our models on 3 established benchmarks before and after fine-tuning. The results are presented in table 15. While there is a small performance drop after fine-tuning, the model still performs well on the general knowledge tasks."}, {"title": "Some Word Problems", "content": "Here we discuss cases where our trained model successfully applies the mathematical rules seen during training in new situations. We see that our model is able to extract \"abstract mathematical skills\" and apply them on practical examples, which is striking.\nWe provide in figures 12 and 13 qualitative examples for the distributivity rule. The main takeway is that despite being trained on the distributivity rule only (with a single prompt template), the model is able to infer the distributivity property on unseen prompts and apply it correctly, whereas the pre-trained model fails to do so. What is more striking is that these unseen prompts are word problems, as opposed to \"mathematical\" ones seen during training.\nFigures 15 and 16 present examples of the model's ability to combine multiple rules it has been trained on (the all model refers to the model trained on all mathematical rules, see section D.3). Figure 16 is particularly striking and shows the benefit of training on multiple rules: the model trained on distributivity only doesn't simplify $a^5 * a^2$, contrary to the model trained on all rules."}, {"title": "Experimental Details and Compute", "content": "Below are the hyperparameters used during our experiments:\n\u2022 One epoch of training for models trained on distributivity only, two epochs otherwise.\n\u2022 Quantized fine-tuning with low-rank adaption (QLORA Dettmers et al. (2024)) with a rank of 256 and a dropout of 0.1\n\u2022 Constant learning rate of 10-5\n\u2022 Additionally following the work of Hayou et al. (2024) we use a learning rate ratio of 16 between the matrices A and B of the low rank decomposition\n\u2022 AdamW optimizer quantized with 8 bits\n\u2022 Batch size of 8 or 32 depending on the experiment\nWe report in table 16 the compute ressources used for our experiments. Each experiment was run on 4 Nvidia A10 GPUs with 24GB of video memory on an AWS g5.12xl instance."}, {"title": "Evaluation", "content": "To evaluate and compare the performance of our fine-tuned model against baseline models, we developed a streamlined pipeline. The primary objective of this pipeline is to assess the correctness of answers generated by models in response to mathematical prompts. Our approach heavily relies on SymPy for handling symbolic mathematics. The pipeline is summarized in figure 17. First we provide an overview of the main elements.\nExtract Relevant Mathematical Expressions. Models such as LLaMA, Mistral, WizardLM, and Llemma do not produce outputs in a standardized format. Therefore, isolating mathematical formulas from their often noisy outputs is generally challenging. The ambiguity and variability of notation and conventions in mathematical expressions, and the handling of nested parenthesis make"}]}