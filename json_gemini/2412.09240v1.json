{"title": "VLMS meet UDA: Boosting Transferability of Open Vocabulary Segmentation with Unsupervised Domain Adaptation", "authors": ["Roberto Alcover-Couso", "Marcos Escudero-Vi\u00f1olo", "Juan C. SanMiguel", "Jesus Bescos"], "abstract": "Segmentation models are typically constrained by the categories defined during training. To address this, researchers have explored two independent approaches: adapting Vision-Language Models (VLMs) and leveraging synthetic data. However, VLMs often struggle with granularity, failing to disentangle fine-grained concepts, while synthetic data-based methods remain limited by the scope of available datasets. This paper proposes enhancing segmentation accuracy across diverse domains by integrating Vision-Language reasoning with key strategies for Unsupervised Domain Adaptation (UDA). First, we improve the fine-grained segmentation capabilities of VLMs through multi-scale contextual data, robust text embeddings with prompt augmentation, and layer-wise fine-tuning in our proposed Foundational-Retaining Open Vocabulary Semantic Segmentation (FROVSS) framework. Next, we incorporate these enhancements into a UDA framework by employing distillation to stabilize training and cross-domain mixed sampling to boost adaptability without compromising generalization. The resulting UDA-FROVSS framework is the first UDA approach to effectively adapt across domains without requiring shared categories.", "sections": [{"title": "Introduction", "content": "Semantic segmentation, the task of assigning a categorical label to every pixel in an image, is critical for multiple applications. However, its class dviersity is constrained by the number and nature of the annotated classes learned during training. To handle this limitation, two approaches have been explored: (1) training in synthetic domains where classes can be created at will and instances are annotated at creation; so-trained models need to be later adapted to the target domain. (2) leverage the foundational encoded knowledge of Vision Language Models (VLMs) by adapting their outcomes to a segmentation setting. First, Unsupervised Domain Adaptation (UDA) [1, 2, 3] aims at leveraging synthetic datasets, where simulated environments enable the automatic generation of pixel-perfect annotations at a fraction of the cost of manual labeling [4, 5, 6]. While UDA has shown remarkable success in mitigating domain shifts between tasks and data domains, UDA has a significant drawback: its limited ability to adapt to new categories absent in the synthetic dataset [7]. A potential solution would be to regenerate the synthetic data with the new categories, but the models would have to be re-trained, as UDA models are notoriously known for overspecificity [8]. This limitation impairs the performance of UDA-based models in real-world applications where coping with unseen categories is crucial. Second, recent advancements in open vocabulary semantic segmentation (OVSS) have shown that VLMs can be adapted to perform dense prediction tasks. However, these methods typically require task-"}, {"title": "Related Work", "content": "Open Vocabulary Semantic Segmentation Early CLIP [18] extensions to pixel-level prediction primarily involved its use as an image classifier with masked regions [19, 20, 21, 22, 23, 24]. These methods typically utilized a region proposal network to identify image segments, which were masked and classified using a static CLIP model. This process required processing each segment through the CLIP encoder, making it computationally intensive. Recent studies [25, 26, 13, 27, 11] have shifted focus to harnessing CLIP's features for more detailed, object-level semantic analysis. Initial efforts involved using CLIP's attention maps to create segmentation maps [25, 26], which were classified based on similarity values. However, as CLIP is trained for global image representations, these methods struggled to capture intricate image details. Alternative approaches [28, 29, 30] introduce category-agnostic region proposals as support inputs to the CLIP encoder. This process has the limitation of losing the image global context as each support region is processed independently. In contrast, MaskCLIP [11] addressed this by modifying CLIP's final pooling layer to extract dense features directly. To obtain accurate dense"}, {"title": "Method", "content": "In this paper, we enhance the adaptability of CLIP to interpret semantics at pixel level across novel domains. We initialize the training with a pre-trained VLM as it contains rich semantics learned from large-scale training. We follow [11] and modify the last layer of the image encoder to obtain dense image features which are combined with text embeddings to generate cost volume embeddings as in [13]. These cost volumes are then fed to a decoder to generate segmentation maps. Additionally, to improve performance we propose paraphrasing en-"}, {"title": "Open Vocabulary Semantic Segmentation", "content": "Problem overview In open vocabulary semantic segmentation, the task is to map each pixel of an input image x to a label from a set of N potential categories P = {P1, ..., Pn, ..., PN}, where for each n-th category, Pn = {P1,n, \u2026\u2026\u2026, Pm,n, ..., PM,n} is a set of M different text prompts pm,n (equal M for all categories). This task expands upon traditional semantic segmentation by being able to cope with unseen categories during inference, posing a challenge beyond conventional segmentation capabilities.\nFine-Tuning of CLIP Prior works utilizing CLIP [11, 13, 9, 10, 12] highlight that conventional fine-tuning of the image encoder can degrade performance due to potential misalignment of the image and text encoders. Therefore, guided by the insight that tuning layers responsible for spatial interaction (e.g., attention layers and positional embeddings) suffices for transferring image-level representations to pixel-level [49], we freeze the MLP layers in the encoder. Additionally, we follow the hypothesis that deeper layers in the image encoder encapsulate task-specific filters, while shallow layers represent task-agnostic filters which should be less tuned to obtain optimal performance [50, 51]. Therefore, we propose to decrease the learning rate in each layer by a factor of B with respect of the previous layer:\n$lr_{l} \\leftarrow Ir_{l+1} \\cdot \\beta$,\nwhere $Ir_l$ is the learning rate assigned to layer l and \u03b2 a training hyper-parameter. For the framework we only set the initial learning rate of the last layer of the encoder and then the learning rate is propagated to the following layers.\nLanguage-Guided Cost Volume and Semantic Decoding The vision-language pre-training allows for aligned vision and language embeddings; such alignment enables the reasoning of semantics of the image given a description. We employ a decoder to disentagle the relationships derived from the VLM into accurate pixel-level predictions. To do so, first we modify the CLIP image encoder as per [11], so that it generates patch-level image features by removing the attention pooling of the last layer of the ViT encoder [52]. For each input image x of H \u00d7 W spatial resolution, the modified CLIP image encoder V (x) processes image patches of size k \u00d7 k to obtain sets of visual features {EV, i \u2208 [1, H' \u00d7 W']}, with H' = and W' = . Regarding the text branch, the encoder \u03a6(P) is kept unaltered, yielding a set"}, {"title": "Prompt Definition", "content": "We find two major drawbacks in prompt definition of the current CLIP-based segmentators. First, they do not employ mean text embeddings nor prompt augmentations to generate more reliable text embeddings [13]. Second, they do not account for conflicting category names given by different datasets, e.g. the Cityscapes dataset [55] differentiates between the categories rider and pedestrian, whereas other datasets group both under the category person, without accounting for the situational context of the individual. To address the identified issues, we incorporate the descriptions provided to annotators into the text"}, {"title": "Unsupervised Domain Adaptation", "content": "Addressing the challenge of applying open vocabulary segmentation to unseen datasets, we advocate the use of UDA techniques [31, 32, 33]. UDA leverages knowledge from a source-labeled domain to train models that can effectively generalize to unlabeled target domains by coping with a covariant distribution shift. To that end, pseudo-labels [34] from a teacher model are used to guide the learning of a model being trained (i.e., the student) [35, 36, 37, 38, 7, 39]. As UDA frameworks typically lack a reliable teacher, the common choice is to define the teacher model as an exponential moving average of the student's weights. This allows integrating learnt knowledge on the fly, while mitigating concept drifting by hampering the impact of pseudo-labels in the teacher model [40]. Concept drift is the phenomenon where accumulating inaccuracies, particularly false positives, misguides the model's learning trajectory [41, 42]. Among other canonical UDA strategies, cross-domain mixed sampling (image mixup) randomly overlays images from both domains to enforce the model to learn domain-invariant features [43, 44, 45, 46, 47]. Moreover, domain randomization introduces controlled variability in the training data, such as changes in lighting, textures, and other environmental factors. Additionally, it proposes overlaying source objects on target images to further introduce variability [47, 44]. These variations ensure that the model remains adaptable even when faced with data different from the training set. We blend these UDA techniques into our VLM-based OVSS method: UDA-FROVSS, yielding large performance gains across all analyzed domains."}, {"title": "Robust Text Embeddings", "content": "To improve open vocabulary capabilities, state-of-the-art methods [18, 48] employ multiple descriptions of the image to generate a mean embedding. These mean embeddings are supposed to be more reliable descriptors of the object as the only common factor of the sentences is the text identifying the target object, therefore reducing the noise introduced into the text embedding of the prompt. Differently, the state-of-the-art method for open vocabulary segmentation [13] does not employ mean text embeddings and computes similarity on each prompt, thus neglecting the cascaded benefits of better textual representations. To overcome this drawback we employ different prompt augmentation techniques based on object characteristics, e.g. \"A photo of a Vintage car\", and descriptions for annotators."}, {"title": "Method", "content": "In this paper, we enhance the adaptability of CLIP to interpret semantics at pixel level across novel domains. We initialize the training with a pre-trained VLM as it contains rich semantics learned from large-scale training. We follow [11] and modify the last layer of the image encoder to obtain dense image features which are combined with text embeddings to generate cost volume embeddings as in [13]. These cost volumes are then fed to a decoder to generate segmentation maps. Additionally, to improve performance we propose paraphrasing en-"}, {"title": "Open Vocabulary Semantic Segmentation", "content": "Problem overview In open vocabulary semantic segmentation, the task is to map each pixel of an input image x to a label from a set of N potential categories P = {P1, ..., Pn, ..., PN}, where for each n-th category, Pn = {P1,n, \u2026\u2026\u2026, Pm,n, ..., PM,n} is a set of M different text prompts pm,n (equal M for all categories). This task expands upon traditional semantic segmentation by being able to cope with unseen categories during inference, posing a challenge beyond conventional segmentation capabilities.\nFine-Tuning of CLIP Prior works utilizing CLIP [11, 13, 9, 10, 12] highlight that conventional fine-tuning of the image encoder can degrade performance due to potential misalignment of the image and text encoders. Therefore, guided by the insight that tuning layers responsible for spatial interaction (e.g., attention layers and positional embeddings) suffices for transferring image-level representations to pixel-level [49], we freeze the MLP layers in the encoder. Additionally, we follow the hypothesis that deeper layers in the image encoder encapsulate task-specific filters, while shallow layers represent task-agnostic filters which should be less tuned to obtain optimal performance [50, 51]. Therefore, we propose to decrease the learning rate in each layer by a factor of B with respect of the previous layer:\n$lr_{l} \\leftarrow Ir_{l+1} \\cdot \\beta$,\nwhere $Ir_l$ is the learning rate assigned to layer l and \u03b2 a training hyper-parameter. For the framework we only set the initial learning rate of the last layer of the encoder and then the learning rate is propagated to the following layers.\nLanguage-Guided Cost Volume and Semantic Decoding The vision-language pre-training allows for aligned vision and language embeddings; such alignment enables the reasoning of semantics of the image given a description. We employ a decoder to disentagle the relationships derived from the VLM into accurate pixel-level predictions. To do so, first we modify the CLIP image encoder as per [11], so that it generates patch-level image features by removing the attention pooling of the last layer of the ViT encoder [52]. For each input image x of H \u00d7 W spatial resolution, the modified CLIP image encoder V (x) processes image patches of size k \u00d7 k to obtain sets of visual features {EV, i \u2208 [1, H' \u00d7 W']}, with H' = and W' = . Regarding the text branch, the encoder \u03a6(P) is kept unaltered, yielding a set"}, {"title": "Prompt Definition", "content": "We find two major drawbacks in prompt definition of the current CLIP-based segmentators. First, they do not employ mean text embeddings nor prompt augmentations to generate more reliable text embeddings [13]. Second, they do not account for conflicting category names given by different datasets, e.g. the Cityscapes dataset [55] differentiates between the categories rider and pedestrian, whereas other datasets group both under the category person, without accounting for the situational context of the individual. To address the identified issues, we incorporate the descriptions provided to annotators into the text"}, {"title": "Unsupervised Domain Adaptation", "content": "Addressing the challenge of applying open vocabulary segmentation to unseen datasets, we advocate the use of UDA techniques [31, 32, 33]. UDA leverages knowledge from a source-labeled domain to train models that can effectively generalize to unlabeled target domains by coping with a covariant distribution shift. To that end, pseudo-labels [34] from a teacher model are used to guide the learning of a model being trained (i.e., the student) [35, 36, 37, 38, 7, 39]. As UDA frameworks typically lack a reliable teacher, the common choice is to define the teacher model as an exponential moving average of the student's weights. This allows integrating learnt knowledge on the fly, while mitigating concept drifting by hampering the impact of pseudo-labels in the teacher model [40]. Concept drift is the phenomenon where accumulating inaccuracies, particularly false positives, misguides the model's learning trajectory [41, 42]. Among other canonical UDA strategies, cross-domain mixed sampling (image mixup) randomly overlays images from both domains to enforce the model to learn domain-invariant features [43, 44, 45, 46, 47]. Moreover, domain randomization introduces controlled variability in the training data, such as changes in lighting, textures, and other environmental factors. Additionally, it proposes overlaying source objects on target images to further introduce variability [47, 44]. These variations ensure that the model remains adaptable even when faced with data different from the training set. We blend these UDA techniques into our VLM-based OVSS method: UDA-FROVSS, yielding large performance gains across all analyzed domains."}, {"title": "Robust Text Embeddings", "content": "To improve open vocabulary capabilities, state-of-the-art methods [18, 48] employ multiple descriptions of the image to generate a mean embedding. These mean embeddings are supposed to be more reliable descriptors of the object as the only common factor of the sentences is the text identifying the target object, therefore reducing the noise introduced into the text embedding of the prompt. Differently, the state-of-the-art method for open vocabulary segmentation [13] does not employ mean text embeddings and computes similarity on each prompt, thus neglecting the cascaded benefits of better textual representations. To overcome this drawback we employ different prompt augmentation techniques based on object characteristics, e.g. \"A photo of a Vintage car\", and descriptions for annotators."}, {"title": "Synergy between UDA and Open Vocabulary", "content": "We bridge the gap between UDA and OVSS by developing a unified framework that eliminates the need for shared categories between the source and target domains, making our UDA framework the first to enable models to recognize and segment objects beyond the categories encountered during training. By combining the domain generalization capabilities of UDA with the flexibility of OVSS, we train models which present high effectiveness on the target dataset while preserving their generalization prowers (see Figures 2b and 2c UDA-FROVSS).\nAlltogether, these contributions enable open vocabulary models to significantly benefit from the use of unlabeled images common in UDA, thus enhancing their performance and generalization across diverse datasets. As a result, the models trained using FROS and UDA-FROS demonstrate superior segmentation accuracy on the training dataset but also enhanced transferred performance in previously unseen domains. The proposed methods are extensively evaluated across multiple semantic segmentation datasets showcasing improvements across all analyzed benchmarks: PAS-20"}, {"title": "Related Work", "content": "Open Vocabulary Semantic Segmentation\nEarly CLIP [18] extensions to pixel-level prediction primarily involved its use as an image classifier with masked regions [19, 20, 21, 22, 23, 24]. These methods typically utilized a region proposal network to identify image segments, which were masked and classified using a static CLIP model. This process required processing each segment through the CLIP encoder, making it computationally intensive. Recent studies [25, 26, 13, 27, 11] have shifted focus to harnessing CLIP's features for more detailed, object-level semantic analysis. Initial efforts involved using CLIP's attention maps to create segmentation maps [25, 26], which were classified based on similarity values. However, as CLIP is trained for global image representations, these methods struggled to capture intricate image details. Alternative approaches [28, 29, 30] introduce category-agnostic region proposals as support inputs to the CLIP encoder. This process has the limitation of losing the image global context as each support region is processed independently. In contrast, MaskCLIP [11] addressed this by modifying CLIP's final pooling layer to extract dense features directly. To obtain accurate dense"}, {"title": "Method", "content": "In this paper, we enhance the adaptability of CLIP to interpret semantics at pixel level across novel domains. We initialize the training with a pre-trained VLM as it contains rich semantics learned from large-scale training. We follow [11] and modify the last layer of the image encoder to obtain dense image features which are combined with text embeddings to generate cost volume embeddings as in [13]. These cost volumes are then fed to a decoder to generate segmentation maps. Additionally, to improve performance we propose paraphrasing en-"}, {"title": "Open Vocabulary Semantic Segmentation", "content": "Problem overview In open vocabulary semantic segmentation, the task is to map each pixel of an input image x to a label from a set of N potential categories P = {P1, ..., Pn, ..., PN}, where for each n-th category, Pn = {P1,n, \u2026\u2026\u2026, Pm,n, ..., PM,n} is a set of M different text prompts pm,n (equal M for all categories). This task expands upon traditional semantic segmentation by being able to cope with unseen categories during inference, posing a challenge beyond conventional segmentation capabilities.\nFine-Tuning of CLIP Prior works utilizing CLIP [11, 13, 9, 10, 12] highlight that conventional fine-tuning of the image encoder can degrade performance due to potential misalignment of the image and text encoders. Therefore, guided by the insight that tuning layers responsible for spatial interaction (e.g., attention layers and positional embeddings) suffices for transferring image-level representations to pixel-level [49], we freeze the MLP layers in the encoder. Additionally, we follow the hypothesis that deeper layers in the image encoder encapsulate task-specific filters, while shallow layers represent task-agnostic filters which should be less tuned to obtain optimal performance [50, 51]. Therefore, we propose to decrease the learning rate in each layer by a factor of B with respect of the previous layer:\n$lr_{l} \\leftarrow Ir_{l+1} \\cdot \\beta$,\nwhere $Ir_l$ is the learning rate assigned to layer l and \u03b2 a training hyper-parameter. For the framework we only set the initial learning rate of the last layer of the encoder and then the learning rate is propagated to the following layers.\nLanguage-Guided Cost Volume and Semantic Decoding The vision-language pre-training allows for aligned vision and language embeddings; such alignment enables the reasoning of semantics of the image given a description. We employ a decoder to disentagle the relationships derived from the VLM into accurate pixel-level predictions. To do so, first we modify the CLIP image encoder as per [11], so that it generates patch-level image features by removing the attention pooling of the last layer of the ViT encoder [52]. For each input image x of H \u00d7 W spatial resolution, the modified CLIP image encoder V (x) processes image patches of size k \u00d7 k to obtain sets of visual features {EV, i \u2208 [1, H' \u00d7 W']}, with H' = and W' = . Regarding the text branch, the encoder \u03a6(P) is kept unaltered, yielding a set"}, {"title": "Prompt Definition", "content": "We find two major drawbacks in prompt definition of the current CLIP-based segmentators. First, they do not employ mean text embeddings nor prompt augmentations to generate more reliable text embeddings [13]. Second, they do not account for conflicting category names given by different datasets, e.g. the Cityscapes dataset [55] differentiates between the categories rider and pedestrian, whereas other datasets group both under the category person, without accounting for the situational context of the individual. To address the identified issues, we incorporate the descriptions provided to annotators into the text"}, {"title": "Unsupervised Domain Adaptation", "content": "Addressing the challenge of applying open vocabulary segmentation to unseen datasets, we advocate the use of UDA techniques [31, 32, 33]. UDA leverages knowledge from a source-labeled domain to train models that can effectively generalize to unlabeled target domains by coping with a covariant distribution shift. To that end, pseudo-labels [34] from a teacher model are used to guide the learning of a model being trained (i.e., the student) [35, 36, 37, 38, 7, 39]. As UDA frameworks typically lack a reliable teacher, the common choice is to define the teacher model as an exponential moving average of the student's weights. This allows integrating learnt knowledge on the fly, while mitigating concept drifting by hampering the impact of pseudo-labels in the teacher model [40]. Concept drift is the phenomenon where accumulating inaccuracies, particularly false positives, misguides the model's learning trajectory [41, 42]. Among other canonical UDA strategies, cross-domain mixed sampling (image mixup) randomly overlays images from both domains to enforce the model to learn domain-invariant features [43, 44, 45, 46, 47]. Moreover, domain randomization introduces controlled variability in the training data, such as changes in lighting, textures, and other environmental factors. Additionally, it proposes overlaying source objects on target images to further introduce variability [47, 44]. These variations ensure that the model remains adaptable even when faced with data different from the training set. We blend these UDA techniques into our VLM-based OVSS method: UDA-FROVSS, yielding large performance gains across all analyzed domains."}, {"title": "Robust Text Embeddings", "content": "To improve open vocabulary capabilities, state-of-the-art methods [18, 48] employ multiple descriptions of the image to generate a mean embedding. These mean embeddings are supposed to be more reliable descriptors of the object as the only common factor of the sentences is the text identifying the target object, therefore reducing the noise introduced into the text embedding of the prompt. Differently, the state-of-the-art method for open vocabulary segmentation [13] does not employ mean text embeddings and computes similarity on each prompt, thus neglecting the cascaded benefits of better textual representations. To overcome this drawback we employ different prompt augmentation techniques based on object characteristics, e.g. \"A photo of a Vintage car\", and descriptions for annotators."}, {"title": "Up-Sampling Module", "content": "Given that the underlying CLIP visual transformer encoder operates on a k\u00d7k times smaller feature resolution than the input, the similarity volume F' is up-sampled by a module U to recover the original image resolution. This module is the concatenation of bilinear up-sampling followed by a set of transposed convolutions. This process iterates as many times required \u00b9 to yield an output of the same resolution as the input image for each hidden dimension and category: F\" \u2208 R(H\u00d7W)\u00d7D\u00d7N\n$F_{i,d,n} = U(F',d,n)$\nIncorporating visual guidance [54], the auxiliary feature embeddings & are concatenated with F' and processed by a convolutional layer to return to the hidden dimension D before the up-sampling module. These features are concatenated and processed in the same order they are extracted. Intuitively, following the extraction order in the auxiliary decoder refines finer details of the image. Finally, each category's similarity Fin \u2208 R(H\u00d7W)\u00d7N is computed for each spatial position by the sigmoid activation of a weighted combination of the hidden dimension D in F\", implemented as a learnable 1 \u00d7 1 convolution."}, {"title": "Prompt Definition", "content": "We find two major drawbacks in prompt definition of the current CLIP-based segmentators. First, they do not employ mean text embeddings nor prompt augmentations to generate more reliable text embeddings [13]. Second, they do not account for conflicting category names given by different datasets, e.g. the Cityscapes dataset [55] differentiates between the categories rider and pedestrian, whereas other datasets group both under the category person, without accounting for the situational context of the individual. To address the identified issues, we incorporate the descriptions provided to annotators into the text"}, {"title": "Unsupervised Domain Adaptation", "content": "Our UDA-FROVSS framework, illustrated in Figure 5, integrates the improved CLIP capabilities in the UDA framework to utilize labeled images from the source domain to guide the learning on unlabeled images on the target domain. While UDA already achieves good segmentation quality on shared categories, current UDA frameworks struggle to segment target-private categories not present in the source domain. To address this, we propose to combine open vocabulary segmentation with UDA techniques into our UDA-FROVSS framework. Specifically, we adapt UDAs teacher-student training scheme and image mixup to enhance domain generalizatio by employing pseudo-labels generated by the teacher on the"}, {"title": "Open Vocabulary", "content": "Open Vocabulary Model Enhancements: We introduce a novel decoder architecture that leverages convolutional layers for aiding transformer layers learning with limited data [15, 16, 17]. Additionally, our fine-tuning strategy is designed to preserve the integrity of VLM's pre-trained weights, avoiding catastrophic forgetting while enhancing pixel-level predictions.\nTextual Relationship Improvements: To improve the cross-dataset generalization ability of our model, we propose a concept-level prompt augmentation strategy. By using Large Language Models (LLMs) and providing specific instructions for annotators, we generate diverse and contextually enriched textual prompts. Our approach enhances the model's ability to recognize categories through semantic relationships across datasets. \nSynergy between UDA and Open Vocabulary: We bridge the gap between UDA and OVSS by developing a unified framework that eliminates the need for shared categories between the source and target domains, making our UDA framework the first to enable models to recognize and segment objects beyond the categories encountered during training. By combining the domain generalization capabilities of UDA with the flexibility of OVSS, we train models which present high effectiveness on the target dataset while preserving their generalization prowers. Altogether, these contributions enable open vocabulary models to significantly benefit from the use of unlabeled images common in UDA, thus enhancing their performance and generalization across diverse datasets. As a result, the models trained using FROS and UDA-FROS demonstrate superior segmentation accuracy on the training dataset but also enhanced transferred performance in previously unseen domains.\nThe proposed methods are extensively evaluated across multiple semantic segmentation datasets showcasing improvements across all analyzed benchmarks: PAS-20"}, {"title": "Prompt Definition", "content": "We find two major drawbacks in prompt definition of the current CLIP-based segmentators. First, they do not employ mean text embeddings nor prompt augmentations to generate more reliable text embeddings [13]. Second, they do not account for conflicting category names given by different datasets, e.g. the Cityscapes dataset [55] differentiates between the categories rider and pedestrian, whereas other datasets group both under the category person, without accounting for the situational context of the individual. To address the identified issues, we incorporate the descriptions provided to annotators into the text"}, {"title": "Object characteristics augmentation.", "content": "These augmentations are based on including different adjectives before the class name into the prompt, e.g. \"A photo of a Vintage car\". These object characteristics should provide robustness to the concept as these variations are averaged with the concept as a common anchor."}, {"title": "Photometry of the image augmentation.", "content": "At the end of the prompt followed by a comma we include visual characteristics of the global image, e.g. \"A photo of a car, with High Contrast\". These characteristics may be useful to provide robustness towards style changes."}, {"title": "Background characteristics augmentation.", "content": "We include positional information into the prompt, e.g. \"A photo of a car in the Countryside\". These characteristics can be useful for extrapolating from datasets captured on specific geographical points, such as Cityscapes captured on Germany [55], to more diverse datasets as Mapilliary [57] captured on a global scale."}, {"title": "Prompt", "content": "Prompt Definition\nFormally, we propose to generate A variations of each category textual description pm,n, generating an augmented set P\u201e = {P1,n,1,\u2026\u2026, P1,n,A, \u2026\u2026\u2026, P'm,n,A}\u00b7 Then, these are fed to the text encoder to extract text features. Before combining them with the visual ones in Equations 2 and 4, we calculate the mean of the augmented set of text features to obtain a robuster set of text features:\n$E_{m,n}^L = \\frac{1}{A} \\sum_{a=1}^{A}L(\\text{P}'_{m,n,a})$\nTo unify the responses across the different augmentations, a 1\u00d71 convolution is trained, thereby relying on a learnable weighted combination."}, {"title": "Prompt Definition", "content": "Table 5: Performance comparison of different prompt definition strategies. Prompt augmentation is applied during training and testing. Key, Obj: Object, PH: Photometry, BG: Background.\nexclusively during testing enhances the model's generality at the expense of reduced specificity in relation to the training dataset. This is attributed to the decoder's increased specialization with the training-specific prompts. Conversely, implementing prompt augmentation throughout both training and testing phases enhances the model's performance in terms of both specificity and generality."}, {"title": "Teacher Update", "content": "Based on observations that updating the teacher encoder improves performance on source domain but diminishes the model's open vocabulary potential, we propose to keep the encoder of the teacher model unaltered, whereas the teacher decoder is updated through an exponential moving average (\u0395.\u039c.\u0391.) of the student weights implementing a temporal ensemble at every time step \u03b4 to stabilize predictions:\nD_{t+1} \\leftarrow \\alpha \\cdot T_{t} + (1 - \\alpha ) \\cdot D^S_{t}.\nAs the teacher decoder becomes increasingly unaligned with its encoder, we propose to progressively incorporate the student predictions in the pseudo-labels. Our aim is to first only weight teacher predictions in the learning, to seamlessly employ the student predictions once its learning stablizes. We propose to implement this as a weighted linear combination of the teacher (M) and student predictions:\n\\hat{y}_{t} = \\gamma \\cdot M^T(x) + (1 - \\gamma ) \\cdot M^{S}(x),\nInitially, generated pseudo-labels only takes into account the teacher pseudo-label, as it has been shown that CLIP possesses somewhat reliable zero-shot segmentation capabilities [11]. To do so, we define \u03b3 as a parameter reduced at every time-step:\n\\gamma_{\\delta + 1}(\\frac{1}{\\delta, \\delta + 1}),\nwhere o is a training hyper-parameter. We select this update, as it is a smoothed version of the teacher decoder E.M.A. update. Therefore, the more"}, {"title": "Ablation Study on the UDA Techniques.", "content": "Ta-ble 10 presents the ablation results for the different components presented in the paper. We notice that Prompt Augmentation and Image MixUp significantly drive performance. First, the definition of the prompts help the transfer of categories cross-domain. Second, Image MixUp acts as a data augmentation technique helping the model learn the semantic edges of objects and improving its classification accuracy. As a limitation, we find that our model is not able to distinguish between terrain (not included in the Synthia dataset) and vegetation (semantically related and highly prevalent in the Synthia dataset)."}, {"title": "Teacher Update", "content": "In our work we decided to only update the teacher decoder. This is mainly due to two reasons: First, CLIP has shown remarkable zero shot segmentation performance [66]. Second, we have found that early iterations with AdamW optimizer lead to the model over-fitting to the training dataset. In our first approximation, we changed the optimizer to SGD, which helped alleviating such over-fitting. However, this seemed to affect the encoded knowledge, leading to worse performing models. Therefore, we opted to only update the decoder of the teacher while maintaining the encoder frozen. This helped the teacher model preserving the encoded knowledge of the target"}, {"title": "Results on OVSS enhanced with UDA", "content": "In this subsection, we explore including UDA techniques, leveraging un-labeled color images from a given target domain for training."}, {"title": "Conclusions", "content": "In this paper we introduced the first framework for unsupervised domain adaptation open vocabulary semantic segmentation, marking a significant integration of these two research areas. By combining UDA with open vocabulary segmentation, we alleviate the necessity for shared categories between source and target domains, as demonstrated by our improvement over the state-of-the-art UDA frameworks by over 8% for the Synthia to Cityscapes. Conversely, the open vocabulary approach benefits from UDA's capacity to utilize large volumes of unlabeled data, enabling our models to be successfully trained with less than 2K annotated images. Our approach surpasses previous state-of-the-art for open vocabulary semantic segmentation in all analyzed benchmarks. To achieve these results, we propose a decoder for refined segmentation, a strategic fine-tuning approach to retain CLIP's original weight integrity, and enhanced text embeddings to bolster open vocabulary segmentation. Additionally, we also adapted the teacher-student framework and pseudo-label protocol to effectively train VLMs. For future research, inter-dataset similarity and tuning of the textual encoder emerge as critical factors for further performance enhancements."}]}