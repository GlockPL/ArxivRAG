{"title": "A New Paradigm in Tuning Learned Indexes: A Reinforcement Learning Enhanced Approach", "authors": ["Taiyi Wang", "Liang Liang", "Guang Yang", "Thomas Heinis", "Eiko Yoneki"], "abstract": "Learned Index Structures (LIS) have significantly advanced data management by leveraging machine learning models to optimize data indexing. However, designing these structures often involves critical trade-offs, making it challenging for both designers and end-users to find an optimal balance tailored to specific workloads and scenarios. While some indexes offer adjustable parameters that demand intensive manual tuning, others rely on fixed configurations based on heuristic auto-tuners or expert knowledge, which may not consistently deliver optimal performance.\nThis paper introduces LITUNE, a novel framework for end-to-end automatic tuning of Learned Index Structures. LITUNE employs an adaptive training pipeline equipped with a tailor-made Deep Reinforcement Learning (DRL) approach to ensure stable and efficient tuning. To accommodate long-term dynamics arising from online tuning, we further enhance LITUNE with an on-the-fly updating mechanism termed the O2 system. These innovations allow LITUNE to effectively capture state transitions in online tuning scenarios and dynamically adjust to changing data distributions and workloads, marking a significant improvement over other tuning methods. Our experimental results demonstrate that LITUNE achieves up to a 98% reduction in runtime and a 17-fold increase in throughput compared to default parameter settings given a selected Learned Index instance. These findings highlight LITUNE'S effectiveness and its potential to facilitate broader adoption of LIS in real-world applications.", "sections": [{"title": "1 INTRODUCTION", "content": "The intersection of data management and machine learning has given rise to learned index structures. These indexes integrate machine learning, replacing traditional algorithmic components, to capture data distributions and optimize search times. Notable examples include RMI [21], ALEX [10] and PGM [11], etc., which have become subjects of extensive research.\nThe effective design of a learned index involves deliberate trade-offs to achieve optimal performance for varying workloads. For instance, ALEX favors combined search and update performance by introducing gaps at the expense of space efficiency [10]. On the other hand, the dynamic PGM Index prioritizes update efficiency over search performance [11]. These design trade-offs also lead to more complex structures which generate configurable parameters. Tuning these parameters is the key to balancing the trade-offs that ensure higher performance over traditional indexes.\nBeyond the primary parameters, learned indexes like ALEX have more subtle tunable factors that are often overlooked for simplicity. These parameters affect various aspects of the index performance, from operation cost (e.g., search and insertion cost) to the structure of the index (e.g., heights of the tree). For example, for ALEX, the Max Node Size parameter changes the size of the nodes, thereby affecting the height of the tree. On the other hand, Split Policy and Gap Ratio affect how insertion is carried out. These parameters are intertwined, and adjusting them in real-world scenarios can lead to substantial performance improvements, though it requires a more complex tuning process.\nSelecting the right tuning approach for learned indexes involves navigating a myriad of parameter configurations [16, 41]. For example, in practice, parameterized indexes can exhibit vastly different performance due to parameter choices. This is illustrated in Figure 1(a), where adjusting just two parameters leads to significant variability in runtime\u00b9. This variability underscores the complexities and potential performance swings when considering the full spectrum of high-dimensional and continuous parameter configurations, which can scale into thousands or millions. This complexity is compounded by the fact that a misconfiguration can lead to drastic performance deviations, emphasizing the importance of precise\nWhile learned indexes strive to be user-friendly by abstracting away underlying parameters, this abstraction can inadvertently lead to significant performance degradation, as hiding these critical parameters from the user may result in suboptimal default settings for various scenarios. By fine-tuning these parameters, we can achieve significant performance improvements. Figure 1(b) emphasizes the substantial performance gains (measured by query runtime speedup) achieved by our tuning system over SOSD [18] through comparisons between the optimal solution found by LITUNE and the default settings provided by experts. Importantly, unlike many DBMSs whose default configurations are often overly conserva-tive [40, 49], the default settings in learned indexes are decided based on instance-optimized designs [9] which allow them to adapt to specific system environments given a \"good\" tuning algorithm. Currently, these algorithms are chosen by experts to optimize for common system environments. However, there hasn't been any end-to-end tuning system that is adaptive across data distributions. Thus, tuning is necessary and can lead to considerable enhance-ments.\nMoreover, in real-world usage, data distributions and query types are not constant, which makes tuning more challenging. Figure 1(c) shows the degradation of tuning performance among existing out-of-box tuning methods during continuous online tuning when fac-ing dynamic workloads, highlighting the need for adaptive tun-ing to workloads and data distributions (Challenge C.2). We in-troduce four different workloads derived from mixture-distributed data from the SOSD dataset [18], involving various query types by adjusting the read-write ratio over time. To ensure fair comparisons, each tuning method is provided with a preparation period, depicted as grey areas within the workload intervals. During this period, a preliminary smaller dataset reflecting future trends is used for warming up, initial tuning will happen during this period to make sure the system begins with reasonably optimized parameters.\nAnother challenge arises from the need for safe tuning, especially when dealing with a large parameter space and concurrent tuning demands. Recently, Reinforcement Learning enhanced by deep neu-ral networks (DRL) has already been proved by many works [15, 49] as a good tuner when working within a large parameter space due to its intrinsic exploration abilities. Equipped with a learned module, an RL-based approach can be easily generalized and deployed to various data conditions. However, DRL, as a trial-and-error-based approach, normally presents aggressive tuning towards the optimal solutions, whose potential risks to the existing system were largely ignored. In such cases, it is crucial to ensure tuning remains both safe and stable during exploration within the extensive parameter space (Challenge C.3). Figure 1(d) shows the failure rates caused by improper parameter settings from aggressive tuning"}, {"title": "2 RELATED WORKS", "content": "Parameter tuning is a common practice to optimize systems. For example, knob tuning in database systems, where specific values or knobs can be adjusted to optimize for specific query operators and improve data access efficiency [50]. Traditional search strategies include using random and grid search to find the optimal parame-ters for a given workload. Advanced frameworks such as GPTune [25], Spearmint [33], and Sequential Model-Based Optimization (SMBO) [29] attempt to refine this process through Bayesian Op-timization, integrating multi-task and transfer learning. However, these methods struggle with accuracy and computational efficiency, as they require starting anew with each shift in workload or data\nRecently, Deep reinforcement learning (DRL) has shown promis-ing results in optimizing complex systems under dynamic condi-tions [6, 15]. Notably, CDBTune [49] uses deep deterministic policy gradients (DDPG [24]) to automatically navigate and tune the high-dimensional continuous space of database configurations. It has shown the adaptability and efficiency of DRL methods over tra-ditional tuning tools and expert DBA interventions for handling dynamic conditions. However, CDBTune cannot easily adapt to the unique challenges in index tuning, not present database configura-tion tuning. Index tuning requires rapid and precise adjustments without system resets or prolonged downtime while answering queries and updating records within dynamic data distributions and workload patterns. Misconfiguration has severe consequences for performance and must be avoided. LITUNE, on the other hand, ef-ficiently tackles these challenges while keeping the full advantages of DRL tuning.\nLearned indexes [21] replace traditional indexing algorithms (like B+Trees) with models that predict the approximate location of a key using the Empirical Cumulative Distribution Function (CDF), potentially reducing search operations. Research on both static [11, 19, 21, 34] and updatable indexes [10, 11, 14, 22, 23, 38, 44, 46, 48] demonstrates that performance heavily depends on data distribution, and to achieve the best performance, the indexes must be \"tuned\" according to the data distribution.\nCurrently, there are two approaches for considering data distri-butions when designing learned indexes: (1) exposing parameters for adjustment by users or future research, as seen in [11, 21, 48], and (2) implementing self-tuning mechanisms through cost models, utilized by indexes such as [10, 23, 46]. In both approaches, the de-fault parameter settings are crucial, with index designers asserting that tuning is unnecessary for satisfactory performance. However, this assumption only holds when the empirical cost of the default parameters is effective [22, 26, 38], and for updatable learned in-dexes, the insert cost model must also be valid [10, 23]. Early studies like [36] use grid search to find optimal default parameters but fail to capture the complexities of tuning learned indexes. Automating this tuning process remains under-explored and is fundamental to our work with LITune. This challenge is exacerbated for in-dexes supporting dynamic workloads, which must be reorganized to maintain model accuracy as data distributions shift. Unlike tradi-tional indexes, learned index parameters depend not only on dataset size but also on rapidly changing data distributions. Additionally, updatable indexes require structural modifications\u2014such as gaps [10, 23], hierarchical structures [11, 45], and buffers [14, 23]\u2014to mitigate the impact of distribution shifts. These complexities create intricate dependencies between parameters (as shown in Figure 1(a)), making automated tuning particularly challenging.\nCDFShop [27] automates learned index optimization by fine-tuning cumulative distribution function (CDF) models specifically"}, {"title": "3 LITUNE SYSTEM", "content": "Unlike existing index tuning works that concentrate on a limited set of observable parameters directly reflected in data structures, our work tackles the more complex challenge of tuning within a vast parameter space where parameters interact in non-independent and intertwined ways. This complexity demands stable and effi-cient tuning strategies, particularly in the context of online and continuous learning tasks. Since parameter metrics are difficult to capture and do not readily lend themselves to the integration of strong heuristics or expert knowledge, we focus on capturing stateful transitions and propose tailored end-to-end tuners.\nFurthermore, navigating complex parameter spaces poses sig-nificant challenges for traditional search strategies and advanced model-based approaches [49]. Traditional search strategies (random search, grid search, heuristics) fall short in navigating the extensive parameter space of learned indexes, while advanced out-of-box tuning frameworks (e.g., SMBO [29]) require starting a new naviga-tion cycle with each shift in workload or data distribution, making them resource-intensive. Furthermore, establishing universal states for different index structures with different parameter sets creates complications. In this regard, LITUNE is designed to offer online and stateful index tuning using deep reinforcement learning for dynamic workloads and provides fast and safe configurations for multiple learned indexes. Our approach mitigates the instabilities and aimless explorations that often accompany the use of generic, out-of-the-box tuning methods.\nAt the core of LITUNE is reinforcement learning (RL), which en-hances adaptability to dynamic workloads beyond the capabilities of traditional cost models. Specifically, we design a Markov De-cision Process framework that involves the RL agent interacting with an environment to maximize rewards. The decisions are made based on observed states and chosen parameters [37], which, in this case, are the structural responses to shifting data distributions and tuned parameters, respectively.\nLITUNE operates in two primary phases: the Training Stage and the Online Tuning Stage. In the Training Stage, we implement an efficient adaptive training pipeline to generate a generalizable pre-trained model. Once this model is deployed, it undergoes continuous fine-tuning in the Online Tuning Stage, ensuring it remains cur-rent and effective under evolving operational conditions. To avoid misconfigurations during tuning, LITUNE adopts a context-aware RL system that prevents early terminations, ensuring stability and speed throughout the process. Additionally, to keep the RL agent"}, {"title": "3.3 Training Stage", "content": "Part A of Figure 3 depicts the initial generation of a pre-trained RL agent. This agent is crucial as it forms the foundation of our adaptive RL-based tuner, designed to efficiently handle diverse tuning scenarios right from deployment.\nThe foundational step in our approach involves preparing a Deep Reinforcement Learning (DRL) agent for optimizing learned index configurations. This process commences with the generation of varied datasets and query sets, establishing a diverse training environment. Each training episode, defined by a unique dataset-query set combination, allows the agent to explore a spectrum of configurations through a trial-and-error strategy, thereby accumulating a rich set of initial training data.\nUnlike supervised learning, our methodology relies on the automatic collection of training quintuples (d, q, a, s, r) by the RL agent. d denotes the dataset, q a set of queries, a the parameters for index construction, s the index states, and r the performance metric. This approach ensures comprehensive feedback for the agent, which is pivotal for its learning process. Given the NP-hard nature of optimizing learned index configurations in a continuous parameter space, our model employs general DRL tech-niques. This choice enables the exploration of novel configurations and mitigates the risk of entrapment in local optima.\nAs mentioned in Introduction, the C.2 requires robust generalization of Deep Re-inforcement Learning (DRL) agents in LITUNE to handle real-world scenarios with unseen queries and variable data distributions [47]. To achieve this, we introduce an adaptive training design based on Meta Reinforcement Learning (Meta-RL), which enhances the tuner's adaptability to new situations and addresses.\nOur method employs the Model-Agnostic Meta-Learning (MAML) approach [13], which trains agents to rapidly adapt with minimal updates. In the context of learned index tuning, the \"tuning in-stances\" represent specific scenarios with unique data distributions and query types. MAML integrates into the RL training process through a two-level training loop:\nAgents perform tuning-instance-specific updates to optimize performance on sampled tun-ing instances. This involves adjusting policy parameters based on interactions characterized by specific workload types and data dis-tributions.\nThe initial policy parameters are updated across all tuning instances to improve general performance. This enhances the agent's ability to generalize to new, unseen tuning scenarios.\nThis dual-loop process enables the agent to handle individual tuning scenarios effectively while maintaining broad adaptability across diverse operational conditions, which is essential for effi-ciency in the dynamic landscape of learned index tuning. Here is a quick example on how it works in practice:"}, {"title": "3.4 Online Tuning Stage", "content": "After establishing a strong foundation in the training stage, we now transition to the online tuning stage, where the pre-trained model is put into practical use. This section describes how the LITUNE system operates and adapts to continuous tuning needs on the fly.\nParts B and C of Figure 3 illustrate the architecture of the online tuning system. The dashed box at the top represents the client and data storage system where end users send their queries to the LITUNE system below. This system is designed to handle the continuous adaptation required by varying data environments.\nThe operational flow is presented in Part B of Figure 3: Once the data storage setup and tuning requests are confirmed, the learned index and tuning system process the current data as the underlying distribution. The system executes queries on this data, generat-ing performance data and states. Based on these observations, the tuning system automatically suggests adjustments to the learned index parameters to optimize future query handling and improve performance.\nFurthermore, LITUNE leverages the pre-trained model to provide real-time recommendations for parameter settings during online tuning scenarios. It also continuously refines this model by incorpo-rating feedback from each tuning request into its training process. This integration of Online Tuning and Offline Training (O2 Sys-tem) ensures that LITUNE dynamically adapts to any changes in workload and data distribution, maintaining high efficiency and adaptability over time.\nEnd users can easily tune the target learned index by submitting a request to LITUNE. Upon receiving a request, the system gathers the necessary data and utilizes the pre-trained RL model for online tuning, ultimately recommending the best-performing parameters.\nAs depicted in Part C of Figure 3, the O2 system integrates Online and Offline RL models to address C.2, enhancing real-time adaptability and performance optimization. The system employs the online tuner with a pre-trained model for immediate index adjustments when no data changes occur. Conversely, signif-icant data changes activate both the online and offline models: the offline model refines itself with new data, while the online model handles real-time optimizations."}, {"title": "3.5 LITUNE Working Process", "content": "Summarizing the learned index parameter tuning in LITUNE, the learned index (tuning target) serves as the RL environment, while the deep RL model acts as the agent, recommending configurations based on the learned index state. As the index is constructed and queries are executed, the learned index state alters, reflected in the metrics. These metrics evaluate learned index performance, calcu-lating the corresponding RL reward value, and the agent updates its policy accordingly, continuing until the tuning time budget is exhausted, ultimately revealing the most fitting parameter settings. Thus, the RL-based tuner can easily capture and memorize the state responses to the data distribution and workload shifts.\nTo illustrate how our RL-based tuner is specifically designed for learned index scenarios, Figure 4(a) demonstrates a tuning exam-ple using ALEX. As the workload transitions from a balanced to a write-heavy workload, LITUNE detects changes in ALEX's states and performance metrics. Initially, it increases the maximum node size from the default 16MB and reduces the threshold for out-of-domain inserts before triggering node expansion. This adjustment leads to a notable decrease in the \"no_expand_and_retrain\" metric. With positive feedback from the training environment, LITUNE stores new data and asynchronously fine-tunes the model using the O2 system, evaluating potential updates based on pre-defined criteria. This enables continuous learning with minimal disrup-tion, allowing swift adaptation to workload changes. Eventually, LITUNE recommends further increasing the maximum node size to 64MB and raising the minimum number of out-of-domain inserts required before expansion for future trials. The Safe RL module ensures system safety by automatically preventing the selection of dangerous states when configuring more aggressive values for the maximum node size and minimum number of out-of-domain inserts. We avoid these aggressive settings, which may yield im-mediate rewards but could lead to system failure in the long run. While some learned indexes (like ALEX) require full reconstruction for structural parameter changes due to their codebase constraints, LITUNE provides flexible on-the-fly reconfiguration mechanisms across different index implementations. For cases where reconstruc-tion is unavoidable, LITUNE minimizes overhead through efficient"}, {"title": "4 METHODOLOGY", "content": "In this section, we explore the integration of novel Reinforcement Learning (RL) models to address the unique challenges of tuning learned indexes. While vanilla RL frameworks like Deep Deter-ministic Policy Gradient (DDPG) methods [24, 49] are adept at managing high-dimensional spaces and continuous actions, they often fall short in ensuring safety for tuning systems. To address these limitations, we have augmented the standard DDPG frame-work by incorporating context-aware learning. Notably, our online tuner components are designed with flexibility in mind and can be replaced by any existing vanilla DRL methods, showcasing our framework's adaptability to accept similar DRL approaches. De-tailed discussions on these DDPG framework enhancements are presented in the subsequent sections.\nUsing RL in LITUNE requires a nuanced formalization of learned index-tuning scenarios. Part B in Figure 3 illustrates the interaction diagram of LITUNE components and the functional workflow of LITUNE in practice.\nViewed as the tuning system, the agent receives rewards and states from the learned index, updating the policy to steer parameter adjustments towards higher rewards.\nRepresenting the tuning target, the environment is an execution instance of the learned index.\nIn LITUNE, the state of the agent, denoted as $s_t$, represents the current condition of the learned index after applying recom-mended parameter settings. We categorize states into two main categories: structural and operational. Structural metrics might in-clude features such as the number of internal nodes or tree height that represent the structure and measurable mechanisms of the index. However, not all features can be captured with just struc-tural features. Therefore, we define a new category of operational metrics that captures which parameters affect the actual operation of the index. For each of the key operations (search, insert, update, and delete), we define a set of features that capture the cost of these operations using non-index-specific metrics. For example, we use search distance to capture the search cost, which is universal to all indexes regardless of the exact search algorithm used (linear, binary, or exponential). These metrics act as empirical proxies due to the complex inter-dependencies among features, offering a de-tailed snapshot of the index's internal states necessary for effective tuning across various scenarios.\nWe define the reward $r_t$ as a scalar that accounts for performance changes from both the initial baseline ($D_o$) and the immediately preceding step (t - 1). The idea is to capture the sort of incremental, forward-looking decisions that human experts make when tuning complex systems, balanc-ing short-term gains with a broader trajectory of improvement."}, {"title": "4.1 RL formalization for LITUNE", "content": "Concretely, the RL agent starts from an initial performance $D_o$ and seeks an improved state $D_n$. Once tuning begins, the system transi-tions to $D_1$ and the RL-agent computes $\\Delta(D_1, D_0)$. From there, each iteration aims to surpass its predecessor, reflecting the principle that $D_i$ should exceed $D_{i-1}$ for all i < n. This design effectively encodes both immediate and foundational improvements in a single reward function.\nOur focus metric for performance, denoted as R, signifies the end-to-end runtime, which is paramount for understanding and optimizing query performance. To track optimization progress, we define two key differential metrics: $\\Delta_{t \\rightarrow o} =  \\frac{-R_t + R_o}{R_o}$ , $\\Delta_{t \\rightarrow t-1} = \\frac{-R_t + R_{t-1}}{R_{t-1}}$\nInspired by [49], the reward, r, is articulated as:\n$r=\\begin{cases}   ((1 + \\Delta_{t \\rightarrow o})^2 - 1) *(1 + \\Delta_{t \\rightarrow t-1})^x, & \\text{if }\\Delta_{t \\rightarrow o} > 0 \\\\  -((1 - \\Delta_{t \\rightarrow o})^2 - 1) *(1 - \\Delta_{t \\rightarrow t-1})^x, & \\text{if }\\Delta_{t \\rightarrow 0} \\leq 0 \\end{cases}$\nHere, $\\Delta_{t \\rightarrow o}$ and $\\Delta_{t \\rightarrow t-1}$ measure performance changes relative to the initial setting and the previous step. The scalar parameters w (odd) and x (even) control how strongly the reward emphasizes near-term versus longer-term improvements: w governs the sig-nificance of changes from the initial baseline, and x dictates the impact of recent performance gains or losses. By adjusting these scalars, practitioners can configure how aggressively the system pursues performance gains, how quickly it penalizes regressions, and how it balances near-term improvements against long-term objectives. In our practice, w = 1 and x = 2 often strike a useful balance between achieving notable gains and maintaining stability.\nThe reward function underpins our RL-based tuning pro-cess by connecting the search mechanism to diverse performance objectives. By maximizing the expected cumulative discounted re-ward, $max_\\pi \\mathbb{E}_{\\tau \\sim \\pi}  \\sum_{t=0}^{H} \\gamma^{t} r_{t}$ where $\\pi$ is the policy, $\\tau$ the trajectory, $\\gamma$ the discount factor, and $r_{t}$ the immediate reward, the RL agent explores parameter configurations aligned with user priorities. It refines its policy via trial-and-error (temporal-difference methods) to approximate expected returns across diverse states. Practitioners can adjust the performance metric R to emphasize or de-emphasize latency or throughput (e.g., R = 0.8 latency + 0.2 throughput-1), steering the tuner's optimization without altering the underlying RL framework, ensuring that the tuner can effectively meet the final goal-be it latency-sensitive, throughput-oriented, or a balanced mix of both.\nDenoted as $a_t$, actions derive from the parameter con-figurations space and correspond to parameter tuning operations, influencing all tunable parameters concurrently.\nPolicy, mapping from state to action, maintains state transitions and is represented by a deep neural network. RL aims to learn the optimal policy."}, {"title": "4.2 Backbone: Safe RL approach for LITUNE", "content": "As mentioned in the introduction, we face the challenge of opti-mizing performance while ensuring safety in the context of tuning learned index (C.3), i.e., avoiding configurations that lead to sys-tem instability or failures such as out-of-memory errors or endless\nTo address this, we propose a minimalist approach by employing an Early Terminated Markov Decision Process (ET-MDP) solver that triggers an early termination whenever the learning policy violates predefined constraints. Early termination has been previously used to improve sample efficiency in solving regular MDPs [42], as it accelerates learning by reducing the policy search space and shortening the time horizon. Moreover, an ideal policy should never violate the constraints, eliminating the need to learn to proceed or recover after violations.\nTo effectively handle the constraints in our tuning problem, we model it as a Constrained Markov Decision Process (CMDP) and transform it into its early terminated counterpart, the ET-MDP [35]. This transformation allows us to apply standard RL algorithms while ensuring safety through the early termination mechanism.\nA Constrained Markov Decision Process (CMDP) is a deterministic MDP with a fixed horizon $H \\in \\mathbb{N^+}$, defined by the tuple (S, A, H, r, c, C, T), where S and A represent the state and action spaces; $r : S\\times A \\rightarrow \\mathbb{R}$ is the reward function; $c: S \\times A \\rightarrow \\mathbb{R}$ is the cost function rep-resenting constraints; $C \\in \\mathbb{R^+}$ is the upper bound on the permit-ted expected cumulative cost; and $T : S \\times A \\rightarrow S$ is the tran-sition function. The policy class $ \\Pi$ consists of stationary policies $\\pi: S \\times A \\rightarrow [0, 1]$ such that $ \\Sigma_a \\pi(a|s) = 1 $ for all $s \\in S$.\nIn our learned index tuning problem, constraints such as out-of-memory errors and endless runtime define dangerous or con-strained states, as illustrated in Figure 4(b). We incorporate these constraints into the CMDP framework by defining appropriate cost functions. Specifically, we assign costs (e.g., $c_m$ for memory violations and $c_r$ for runtime violations) which are set to 1 upon violation, ensuring that each type of violation contributes equally to the cumulative cost. This approach penalizes the policy for entering unsafe areas and guides it toward safer trajectories.\nBy incorporating system constraints into the cost func-tion, we can effectively model the safe learned index tuning problem as a CMDP.\nTo handle the constraints and ensure safety during the learning process, we transform the CMDP into an Early Terminated MDP (ET-MDP), which introduces an absorbing termination state whenever the cumulative cost exceeds a predefined threshold.\nFor any CMDP, we de-fine its Early Terminated MDP (ET-MDP) as a new unconstrained MDP ($S \\cup \\{s_e\\}$, A, H, $r'$, $T'$), where $s_e$ is the absorbing state after termination. The transition function and reward function in the ET-MDP are adjusted to handle terminations:\n$T' (s, a) = T(s, a) \\mathbb{1} (b_t \\leq C) + s_e \\mathbb{1} (b_t > C),$\n$r' (s, a) = r(s, a) \\mathbb{1} (b_t \\leq C) + r_e \\mathbb{1} (b_t > C).$\nwhere $b_t =  \\sum_{i=1}^{t} c_m + c_r$ records the cumulative costs up to time t, and $r_e \\in R$ is a small termination reward. The parameter C represents the total tolerated failures we can accept during training.\nThese adjustments integrate the costs into the ET-MDP frame-work to ensure that once a constraint is violated, the associated cost is counted and added to the cumulative costs. This guides the agent to avoid actions leading to high-penalty states, effectively steering the policy towards safer and more optimal trajectories.\nBy converting the CMDP into an ET-MDP and solving it using an appropriate RL algorithm, we can effectively ensure that the learned policy respects the constraints by avoiding actions that lead to early termination.\nThe transformation of the CMDP into an ET-MDP simplifies the problem by converting it into an uncon-strained MDP where constraints are implicitly handled via early termination. The goal is to find an optimal policy $\\pi$ that maximizes the expected cumulative reward while respecting the constraints:\n$\\underset{\\pi\\in\\Pi}{max} \\mathbb{E}_{\\tau\\sim\\pi,T} [\\sum_{t=1}^{H}r_t]$\n$s.t.  \\sum_{t=1}^{H} c_t  \\leq C,$\nwhere $\\tau = (s_1, a_1, r_1, . . ., s_H, a_H, r_f)$ represents the trajectory gen-erated by policy $\\pi$.\nTo solve this constrained optimization problem, the Lagrangian method relaxes it to an unconstrained one with a penalty term:\n$\\pi^* =  arg \\underset{\\pi\\in\\Pi}{max} \\underset{\\lambda \\geq 0}{min} \\mathbb{E}_{\\tau\\sim\\pi,T} [\\sum_{t=1}^{H}r_t - \\lambda (\\sum_{t=1}^{H} c_t - C) + \\lambda C],$   (1)\nWhere $\\lambda \\geq 0$ is the Lagrangian multiplier. In practice, if the policy $\\pi$ is parameterized by $\\theta$, i.e., $\\pi = \\pi_\\theta$, the optimization over $\\theta$ and $\\lambda$ can be conducted iteratively through policy gradient ascent for $\\theta$ and stochastic gradient descent for $\\lambda$ according to Eqn. (1)."}, {"title": "5 EXPERIMENTAL STUDY", "content": "In this section, we evaluate the performance of LITUNE in compari-son to existing tuning approaches, focusing on two specific learned index instances across various workloads and datasets.\nBefore presenting our experimental results, we highlight several essential insights from our study:\nEffective default parameters are hard to set due to the complexity and variability of systems and prob-lems. Unlike claims in [10, 48], default settings often fail to achieve optimal performance because they can't handle parameter inter-dependencies and dynamic data conditions. This highlights the crucial need for tuning to enhance learned index performance by synchronously adjusting parameters. (details in section 5.4.1)\nContrary to the belief that deep models sacrifice tuning efficiency for quality, LITUNE uses online tuning methods to significantly improve efficiency. Experiments demon-strate that LITUNE outperforms other methods in performance gains, regardless of the tuning budget. (details in section 5.4.3, cor-responding to C.1)\nLITUNE effectively handles online and continuous tuning scenarios, adapting to different data distribu-tions and workload dynamics. Its ability to adjust to varying tuning"}, {"title": "5.1 Key Insights", "content": "This robust performance amid diverse and shifting scenarios can be ascribed to the foundational Deep Reinforcement Learning (DRL) framework, which enables LITUNE to continually refine its policies and swiftly adapt its parameter settings, ensuring optimal performance amidst varied query contexts and data scenarios. The LITUNE not only promotes intelligent and dynamic exploration and exploitation of the parameter space but also facilitates quick convergence to optimal or near-optimal configurations, making it especially adept at navigating complex, heterogeneous, and dynam-ically evolving data and query environments, thereby addressing C.2. Specifically, LITUNE demonstrates:\nDifferent rows in Figure 6 correspond to diverse query types (B, RH, WH) when read vertically. Notably, LITUNE'S framework efficiently navigate through different query types, adapting its parameter settings in real time to ensure optimal performance amidst shifting query contexts.\nDifferent columns in Figure 6 correspond to varied data distributions (OSM, books, fb, MIX) when read horizontally. LITUNE also showcases adaptability to var-ied data distributions, adeptly managing complex scenarios like the MIX distribution by autonomously identifying and applying optimal parameter configurations.\nAs evident in Figure 9, LITUNE sustains high performance across continuous data chunks in online tuning, quickly adapting to evolving data distributions without necessitating re-initialization. It is important to note that we did not include DDPG or vanilla-DRL methods for comparison here, as their extreme instability in handling dynamic scenarios without tailored designs made them unsuitable for this evaluation."}, {"title": "5.2 Experimental Settings", "content": "Table 3 presents a comprehensive comparison of tuning overhead across different methods. We evaluate LITUNE under different sam-pling rates to demonstrate the effectiveness of our sampling strategy. For instance, to achieve a 20% runtime reduction, LITUNE with 1% sampling requires only 22 seconds of tuning time, compared to 25 seconds for vanilla DDPG and several minutes to hours for tradi-tional approaches. We choose the 1% sampling rate as it achieves nearly identical performance (212s vs 208s) to LITune-Full while having training and tuning overhead close to LITune-0.1%, which sacrifices too much performance (288s vs 212s). Our results also showcase the highest attainable performance for various tuning methods when given substantial tuning budgets (1000s), while not-ing Grid Search's limitation of becoming computationally infeasible due to its expansive search domain.\nThe training phase represents a one-time investment in computa-tional resources, where LITUNE develops generic tuning strategies across various index scenarios. As shown in Table 3, LITUNE's train-ing time with 1% sampling (6 hours) is half that of DDPG used in RusKey [28] (12 hours), while providing superior tuning ca\u0440\u0430-bilities - achieving 47% runtime reduction compared to DDPG's 19%. Furthermore, our O2 System (detailed in Section 3.4.2) allows instant model updates for real-time applications without additional training overhead."}, {"title": "5.4 Tuning and Training Costs of LITune", "content": "LITUNE consistently exhibits supe-rior performance and adaptability across varied queries, data dis-tributions, and data shifts, as demonstrated in Figures 6, 7, and 9.\nSpecifically, LITUNE demonstrates:\nDifferent rows in Figure 6 correspond to diverse query types (B, RH, WH) when read vertically. Notably, LITUNE'S framework efficiently navigate through different query types, adapting its parameter settings in real time to ensure optimal performance amidst shifting query contexts.\nDifferent columns in Figure 6 correspond to varied data distributions (OSM, books, fb, MIX) when read horizontally. LITUNE also showcases adaptability to var-ied data distributions, adeptly managing complex scenarios like the MIX distribution by autonomously identifying and applying optimal parameter configurations.\nAs evident in Figure 9, LITUNE sustains high performance across continuous data chunks in online tuning, quickly adapting to evolving data distributions without necessitating re-initialization. It is important to note that we did not include DDPG or vanilla-DRL methods for comparison here, as their extreme instability in handling dynamic scenarios without tailored designs made them unsuitable for this evaluation."}, {"title": "5.5 Ablation Study", "content": "This integration of Online Tuning and Offline Training (O2 Sys-tem) ensures that LITUNE dynamically adapts to any changes in workload and data distribution, maintaining high efficiency and adaptability over time.\nhow that the O2 system's online component quickly adapts to new trends using a sliding window of recent queries, enhancing resilience to unforeseen data and ensuring adaptability.\nDemonstrated in the MIX dataset with ALEX (See Figure 10), the O2 system adeptly handles data distribution shifts. The offline tuner, enriched with diverse"}, {"title": "5.5.1 Effects of"}]}