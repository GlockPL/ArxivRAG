{"title": "Harnessing Diverse Perspectives: A Multi-Agent Framework for Enhanced Error Detection in Knowledge Graphs", "authors": ["Yu Li", "Yi Huang", "Guilin Qi", "Junlan Feng", "Nan Hu", "Songlin Zhai", "Haohan Xue", "Yongrui Chen", "Ruoyan Shen", "Tongtong Wu"], "abstract": "Knowledge graphs are widely used in industrial applications, making error detection crucial for ensuring the reliability of downstream applications. Existing error detection methods often fail to effectively utilize fine-grained subgraph information and rely solely on fixed graph structures, while also lacking transparency in their decision-making processes, which results in suboptimal detection performance. In this paper, we propose a novel Multi-Agent framework for Knowledge Graph Error Detection (MAKGED) that utilizes multiple large language models (LLMs) in a collaborative setting. By concatenating fine-grained, bidirectional subgraph embeddings with LLM-based query embeddings during training, our framework integrates these representations to produce four specialized agents. These agents utilize subgraph information from different dimensions to engage in multi-round discussions, thereby improving error detection accuracy and ensuring a transparent decision-making process. Extensive experiments on FB15K and WN18RR demonstrate that MAKGED outperforms state-of-the-art methods, enhancing the accuracy and robustness of KG evaluation. For specific industrial scenarios, our framework can facilitate the training of specialized agents using domain-specific knowledge graphs for error detection, which highlights the potential industrial application value of our framework.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) [1] represent facts in the real world as triples, such as (Paris, capital_of, France), facilitating the organization and scaling of information [5], and have gained paramount importance in knowledge-based systems,"}, {"title": "3 Problem Statement", "content": ""}, {"title": "3.1 Knowledge Graph Error", "content": "We define a knowledge graph G as a set of triples (h, r,t), where h is the head entity, r the relation, and t the tail entity. Error detection involves determining if a given triple is correct or incorrect, with the output being a binary label.\nA triple is incorrect if the head or tail entity does not align with the relation [22]. For example, (Harvard University, is_located_in, New York). Conversely, a triple is correct if all components align appropriately."}, {"title": "3.2 Subgraph Definitions", "content": "To analyze the context of a triple, we define two key concepts for each entity: Out_Neighbor Subgraph and In_Neighbor Subgraph.\nOut Neighbor Subgraph: The set of triples where the entity serves as the head. For an entity e, the Out_Neighbor Subgraph is {(e,r',t') | (e,r',t') \u2208 G}, where r' is outgoing relations from e, and t' is the corresponding tail entity.\nIn_Neighbor Subgraph: The set of triples where the entity serves as the tail. For an entity e, the In_Neighbor Subgraph is {(h',r', e) | (h',r', e) \u2208 G}, where h' is the corresponding head entity, and r' represents incoming relations to e.\nBased on these concepts, for a given triple (h,r,t), we define the following subgraphs for both the head h and the tail t:\n(a) Head Forward_Subgraph: The Out_Neighbor Subgraph of the head entity h, excluding the current triple (h, r, t). Formally:"}, {"title": null, "content": "{(h,r',t') | (h, r',t') \u2208 G, (r', t') \u2260 (r,t)}\n(1)"}, {"title": "(b) Head_Backward_Subgraph", "content": "The In_Neighbor Subgraph of the head entity h, capturing all incoming relations to h. Formally:"}, {"title": null, "content": "{(h',r',h) | (h',r', h) \u2208 G}\n(2)"}, {"title": "(c) Tail Forward Subgraph", "content": "The Out_Neighbor Subgraph of the tail entity t, capturing all outgoing relations from t. Formally:"}, {"title": null, "content": "{(t,r', t') | (t,r', t') \u2208 G}\n(3)"}, {"title": "(d) Tail Backward_Subgraph", "content": "The In_Neighbor Subgraph of the tail entity t, excluding the current triple (h, r, t). Formally:"}, {"title": null, "content": "{(h',r', t) | (h',r',t) \u2208 G, (h',r') \u2260 (h,r)}\n(4)"}, {"title": "3.3 Agent Construction", "content": "We construct four agents based on the above subgraphs: Head_Forward_Agent, Head_Backward_Agent, Tail_Forward_Agent and Tail_Backward_Agent. Each agent analyzes the corresponding subgraph for the triple (h,r,t), enabling a multi-angle evaluation of the triple by considering both head and tail entities' forward and backward contexts."}, {"title": "4 Method", "content": ""}, {"title": "4.1 Design of the Framework", "content": "In our framework, we employ multiple LLM-based agents working collaboratively to detect errors in KGs. Using the structural information of the graph, we construct four bidirectional subgraph agents for both the head and tail entities. These agents analyze the contextual information of triples from different perspectives, and a final decision on the correctness of the triples is made through a voting mechanism. The detailed explanation of this process is provided below:\nBidirectional Subgraph Agents: In our MAKGED framework, we design four bidirectional subgraph agents to evaluate triples in the knowledge graph. Each of these agents is responsible for analyzing triples from a specific directional perspective, including the Head_Forward_Agent and Head_Backward_Agent for the head entity, and the Tail_Forward_Agent and Tail_Backward_Agent for the tail entity, as illustrated in Figure 1.\nFirst, we construct bidirectional subgraphs for both the head and tail entities of each triple. For the head entity, the Head_Forward_Agent extracts the Out_Neighbor subgraph, where the edges represent outgoing relations from the head entity; concurrently, the Head_Backward_Agent extracts the In_Neighbor subgraph, where the edges represent incoming relations directed toward the head entity. Similarly, for the tail entity, the Tail_Forward_Agent and Tail_Backward-Agent generate forward and backward subgraphs, representing the tail entity as either a head node or a tail node in related subgraphs.\nOnce the subgraphs are constructed, we process them using a Graph Convolutional Network to generate the corresponding subgraph embedding vectors. Let the subgraph embeddings be denoted as zg. These subgraph embeddings are then concatenated with the embedding vectors generated by Llama2, denoted"}, {"title": null, "content": "econcat = [ZG; Etext]\n(5)"}, {"title": null, "content": "Sit Iit econcat\n(6)"}, {"title": null, "content": "\u2211log PM (Si | S", "i=1": "\u2211log PM (Si | S"}, {"title": null, "content": "Lit\n(7)"}, {"title": "5 Experiments", "content": "To validate the effectiveness of our proposed MAKGED framework, we conducted comprehensive experiments on two representative knowledge graph datasets in this section, as well as in industrial scenarios such as China Mobile. Specifically, we aim to answer the following research questions through experiments:\nRQ1: How does MAKGED perform compared to state-of-the-art KG error detection methods?\nRQ2: How does each component of the MAKGED framework contribute to its overall performance?\nRQ3: Can the MAKGED framework successfully detect specific errors in knowledge graphs, especially in industrial applications such as those at China Mobile?"}, {"title": "5.1 Experimental Settings", "content": "Datasets: We use two real-world knowledge graph datasets: FB15K [17] and WN18RR [6]. We chose these two datasets because they are highly representative in the field of knowledge graph error detection, encompassing most typical scenarios and possible graph structural representations found in knowledge graph data. In each dataset, we simulate realistic errors by replacing entities and relations with similar ones selected based on cosine similarity within the dataset, resulting in approximately 30% of the data being erroneous. We split each dataset into training, validation, and test sets with a ratio of 8:1:1. The fine-tuning process used only the training set, while the test set was reserved for final evaluations. FB15K is derived from Freebase and contains a rich set of entities and relations, while WN18RR is a subset of WordNet with corrected inverse relations, increasing the complexity. Additionally, we conducted experiments on a knowledge graph dataset from China Mobile's business scenarios, achieving the best results compared to other methods.\nBaselines: We compare MAKGED against various baseline methods, including traditional knowledge graph embedding models such as TransE [2], DistMult [23], and Complex [18], which learn triple embeddings to compute confidence scores. Additionally, we compared recent embedding-based KG error detection"}, {"title": "5.2 Effectiveness Analysis", "content": "Experiment Setup: To study RQ1, we conducted comprehensive experiments on two KG datasets, comparing it with the previously mentioned baseline models across four key metrics. The experimental results are presented in Table 1.\nComparison to Embedding-based Methods: MAKGED combines subgraph structural information with semantic insights from LLMs, leading to a 10-20% improvement in accuracy and a significant increase in the F1-Score."}, {"title": "5.3 Ablation Study", "content": "Experiment Setup: To address RQ2, we conducted an ablation study evaluating several model variants under the same experimental setup as the full framework. The Head as Head variant refers to the results where all four agents"}, {"title": "5.4 Case Study", "content": "Running Example: To study RQ3, we select an incorrect triple (Huawei Honor 10, network support, 5G) from the industrial KG of China Mobile to demonstrate the effectiveness of our framework in industrial applications. Fig 3 shows the evaluation and discussion paths for this triple under our framework.\nSubgraph-Aided Error Correction: The framework effectively uses subgraph information for in-depth analysis, enabling agents to correct initial errors and reach the correct conclusion. In contrast, using the original Llama2 model without subgraph fine-tuning leads to a lower accuracy performance.\nValue of Multi-Agent Collaboration: If we had relied only on the initial model output, the result would have been \"correct\", conflicting with the ground truth. However, after three rounds of discussion, the agents reached the correct conclusion, demonstrating the effectiveness of multi-agent collaboration."}, {"title": "6 Conclusion", "content": "In this paper, we propose MAKGED, a novel framework for knowledge graph error detection. By combining subgraph embeddings from a GCN with LLM embeddings, we train four agents to evaluate triples through multi-agent discussions, enabling multi-perspective analysis. Experiments demonstrate that MAKGED significantly outperforms traditional and LLM-based methods, improving accuracy, F1-Score, precision, and recall across two datasets. Moreover, our framework has also shown excellent performance in industrial scenarios, validating the industrial application value of our method."}]}