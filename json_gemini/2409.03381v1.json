{"title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks", "authors": ["Yongxin Deng", "Xihe Qiu", "Xiaoyu Tan", "Chao Qu", "Jing Pan", "Yuan Cheng", "Yinghui Xu", "Wei Chu"], "abstract": "Abstract\u2014Cognitive psychology investigates perception, attention, memory, language, problem-solving, decision-making, and reasoning. Kahneman's dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive System 1 and the deliberative, rational System 2. Recent advancements have positioned large language Models (LLMs) as formidable tools nearing human-level proficiency in various cognitive tasks. Nonetheless, the presence of a dual-system framework analogous to human cognition in LLMs remains unexplored. This study introduces the CogniDual Framework for LLMs (CFLLMs), designed to assess whether LLMs can, through self-training, evolve from deliberate deduction to intuitive responses, thereby emulating the human process of acquiring and mastering new information. Our findings reveal the cognitive mechanisms behind LLMs' response generation, enhancing our understanding of their capabilities in cognitive psychology. Practically, self-trained models can provide faster responses to certain queries, reducing computational demands during inference.", "sections": [{"title": "I. INTRODUCTION", "content": "Cognitive psychology seeks to elucidate the processes by which humans acquire, retain, and retrieve knowledge [1], [2]. Kahneman's dual-system theory [3]\u2013[8] emerges as a seminal framework within this realm, offering a nuanced understanding of cognitive operations. This theory outlines two distinct cognitive systems: System 1, which is instinctual and facilitates rapid decision-making with minimal cognitive effort, and System 2, which is methodical and requires deliberate focus for complex reasoning tasks.\nIn the realm of artificial intelligence, the advent of deep learning and the influx of extensive datasets have precipitated the swift advancement of language models (LMs). These models, particularly those utilizing the Transformer architecture [9] such as GPT-4 [10], have garnered significant attention for their advanced language processing abilities [11]-[14], achieving near-human proficiency across numerous linguistic tasks. Trained on expansive natural language corpora, these models demonstrate an ability to comprehend and produce symbol sequences with an intuition akin to human System 1's pattern processing. Furthermore, when prompted to employ CoT problem-solving, LLMs exhibit deep reasoning capabilities paralleling human System 2 [12], [15], [16]. Nevertheless, the persistence of such efficient and accurate outputs in the absence of CoT remains uncertain. Should LLMs achieve this, it would suggest the integration of an intuitive operational process comparable to human System 1.\nCan LLMs internalize System 2's complex reasoning into System 1's intuitive responses through iterative training? We hypothesize that LLMs, by mimicking human rapid skill acquisition, can generate fast, intuitive answers without additional training data, thus enhancing resource efficiency and reducing dependence on chains of thought (CoT). Our methodology was straightforward yet robust: we commenced by prompting the model with specific reasoning questions, both with and without CoT cues, and subsequently assessed the accuracy of the responses generated under each condition. Following this, the model employed CoT as a scaffold to reengineer non-CoT responses. Theoretically, this self-editing could facilitate the internalization of CoT reasoning steps, potentially enhancing the model's future problem-solving precision without explicit CoT prompts. The final phase involved reevaluating the model's performance post self-improvement to determine if there was an enhancement in CoT-independent operations. Our experiments are designed to uncover whether LLMs can emulate the human cognitive system by internalizing complex reasoning processes, particularly when functioning without direct reasoning instructions.\nWe applied our methodology to the Vicuna and Llama2 models of varying sizes and evaluated their performance enhancements on reasoning datasets such as GSM8K, ReClor, and LogiQA 2.0. Our findings indicate that LLMs display marked discrepancies in response accuracy when utilizing CoT compared to when it is absent. Following a period of self-training, LLMs exhibited a substantial increase in response"}, {"title": "II. COGNIDUAL FRAMEWORK", "content": "Our CogniDual framework replicates the human learning curve, as depicted in Figure 1. Initially, we prompt untrained LLMs to answer questions from reasoning datasets without CoT instructions, even compelling the LLMs to provide immediate answers without rationale. We designate this question set as $Q_n = \\{q_i, \\text{for } i = 1,2,..., n\\}$, with n denoting the total number of questions. The corresponding answer set is labeled $A1_n = \\{al_i, \\text{for } i = 1,2,..., n\\}$, symbolizing the LLMs' initial responses, akin to human cognitive System 1. These question-answer pairs are preserved.\nSubsequently, we introduce CoT directives, guiding LLMs to derive correct answers sequentially. The resulting answers are categorized as $A2_n = \\{a2_i, \\text{for } i = 1, 2, ..., n\\}$. We also maintain these pairs. In the third phase, we furnish the LLMS with standard answers from the dataset, denoted as $A_n = \\{a_i, \\text{for } i = 1, 2, ..., n\\}$.\nTo quantify the proficiency of our LLMs in responding to $Q_n$, we define the accuracy metric as follows:\n\\begin{equation}\nAcc(A1_n, A_n) = \\frac{1}{n} \\sum_{i=1}^{n} \\text{SemanticMatch}(ali, ai), (1)\n\\end{equation}\n\\begin{equation}\nAcc(A2_n, A_n) = \\frac{1}{n} \\sum_{i=1}^{n} \\text{SemanticMatch}(a2i, ai), (2)\n\\end{equation}\nwhere SemanticMatch(,) assesses the semantic similarity between the LLM's initial response and the standard answer.\nGiven that standard answers usually include comprehensive reasoning and do not conform to a 'yes or no' format, we cannot rely on character matching scripts to evaluate the LLMs' responses. Instead, we engage the LLMs in semantic synonymy judgments to assess the accuracy of $A1_n$ and $A2_n$ against $A_n$, specifically identifying instances where $A2_n$ is accurate, and $A1_n$ is not."}, {"title": "A. Model Self-Iteration", "content": "Our CogniDual framework replicates the human learning curve, as depicted in Figure 1. Initially, we prompt untrained LLMs to answer questions from reasoning datasets without CoT instructions, even compelling the LLMs to provide immediate answers without rationale. We designate this question set as $Q_n = \\{q_i, \\text{for } i = 1,2,..., n\\}$, with n denoting the total number of questions. The corresponding answer set is labeled $A1_n = \\{al_i, \\text{for } i = 1,2,..., n\\}$, symbolizing the LLMs' initial responses, akin to human cognitive System 1. These question-answer pairs are preserved.\nSubsequently, we introduce CoT directives, guiding LLMs to derive correct answers sequentially. The resulting answers are categorized as $A2_n = \\{a2_i, \\text{for } i = 1, 2, ..., n\\}$. We also maintain these pairs. In the third phase, we furnish the LLMS with standard answers from the dataset, denoted as $A_n = \\{a_i, \\text{for } i = 1, 2, ..., n\\}$.\nTo quantify the proficiency of our LLMs in responding to $Q_n$, we define the accuracy metric as follows:\n$Acc(A1_n, A_n) = \\frac{1}{n} \\sum_{i=1}^{n} SemanticMatch(a_{li}, a_i), (1)$\n$Acc(A2_n, A_n) = \\frac{1}{n} \\sum_{i=1}^{n} SemanticMatch(a_{2i}, a_i), (2)$\nwhere SemanticMatch(,) assesses the semantic similarity between the LLM's initial response and the standard answer.\nGiven that standard answers usually include comprehensive reasoning and do not conform to a 'yes or no' format, we cannot rely on character matching scripts to evaluate the LLMs' responses. Instead, we engage the LLMs in semantic synonymy judgments to assess the accuracy of $A1_n$ and $A2_n$ against $A_n$, specifically identifying instances where $A2_n$ is accurate, and $A1_n$ is not."}, {"title": "B. Pre-training Model Distillation", "content": "The framework outlined in Section II-A enables LMs to self-train independently of external interaction. Nevertheless, our framework necessitates that models complete two supplementary tasks when addressing dataset questions: 1. Synonymy Semantic Judgment: LMs must determine if Aln, A2n, and An are semantically equivalent to assess reasoning accuracy both with and without the CoT; 2. Answer Rewriting: Recognizing the impracticality of manually rewriting numerous answers containing reasoning processes into concise responses, we expect LMs to autonomously perform answer rewriting. Suppose we have an open-source LLM denoted by $p_\\theta$, which is parameterized by $\\theta$. To perform synonymy semantic judgment and achieve SemanticMatch, we can design a specific prompt, Prompt semantic. This prompt will evaluate whether the two responses ai and aj have identical semantic meanings:\n$SemanticMatch(a_i, a_j) = p_{\\theta}(a_j, a_i| Prompt_{semantic}). (3)$\nFor answer rewriting, which is also a common task for chat base LLM, we can design a PromptRewrite to align the generated answer aj to the target answer $a_i$ with the identical format, and acquire the updated answer $a_i'$:\n$a_i' = p_{\\theta}(a_j, a_i| Prompt_{Rewrite}). (4)$\nWhile large-scale models readily accomplish these tasks, smaller models, such as the Llama2-7B, may find them challenging [18]. A model that struggles to understand standard answers is unlikely to enhance its capabilities from System 2 to System 1 through self-training alone. To improve training outcomes, we advocate for the pre-training of smaller models using knowledge distillation, equipping them with essential skills for synonymy semantic judgment and answer rewriting.\nKnowledge distillation [19], [19]\u2013[24] is a technique for transferring knowledge from a large, complex 'teacher' model to a smaller, simpler \u2018student' model, facilitating deployment in resource-limited settings without greatly impacting performance. We employ a simplified approach akin to Distilling step-by-step [25]. For synonymy semantic judgment, smaller models generate sample $A_{ln}$, $A_{2n}$, and $A_n$, after which GPT-3.5's advanced generative power yields precise judgments and comprehensive explanations. By creating multiple explanations per question, we ensure a clear delineation of the reasoning pathway. GPT-3.5 also supplies sample rewrites and their justifications for the answer rewriting task. Subsequently, smaller model $\\theta$ can be trained through supervised fine-tuning using the larger models' outputs $A'$, preparing them for self-improvement and independent practice:\nmin -E(qi,a)~A' [log po (a|qi)] . (5)\n\u03b8"}, {"title": "III. EXPERIMENT", "content": "This experiment is designed to examine various questions concerning the cognitive and reasoning capabilities of LLMs such as Llama2. Specifically, we aim to determine whether such models exhibit characteristics analogous to the dual-system cognitive framework observed in humans (Q1), if self-practice in the absence of Chain of Thought (CoT) guidance enhances reasoning abilities (Q2), whether learning curves indicate improved accuracy with additional examples post self-practice (Q3), if larger models benefit more from self-practice without CoT guidance in terms of performance (Q4), and whether the enhanced reasoning abilities generalized across different reasoning tasks (Q5)."}, {"title": "A. Experimental Objectives", "content": "This experiment is designed to examine various questions concerning the cognitive and reasoning capabilities of LLMs such as Llama2. Specifically, we aim to determine whether such models exhibit characteristics analogous to the dual-system cognitive framework observed in humans (Q1), if self-practice in the absence of Chain of Thought (CoT) guidance enhances reasoning abilities (Q2), whether learning curves indicate improved accuracy with additional examples post self-practice (Q3), if larger models benefit more from self-practice without CoT guidance in terms of performance (Q4), and whether the enhanced reasoning abilities generalized across different reasoning tasks (Q5)."}, {"title": "B. Experimental Setting", "content": "To investigate Q1 and Q2 outlined in Section III-A, we employed untrained LLMs as a baseline to evaluate their efficacy both with and without implementing the CoT method. The few-shot methodology was consistently applied in prompt construction, irrespective of CoT method utilization. Given that the GSM8K dataset's solutions entail reasoning sequences, in instances where the CoT method was not applied, we modified the prompting using GPT-4 [10] to exclude the reasoning pathway, employing an 8-shot technique. In contrast, for the Reclor and LogiQA2.0 datasets, which naturally lack reasoning pathways in their answers, we engaged GPT-4 to fabricate corresponding reasoning sequences to assess LLMs\u2019 proficiency under the CoT paradigm, adopting a 3-shot approach for these datasets. This baseline was then juxtaposed with our CFLLMs framework. To tackle Q3, we experimented with diverse data volumes within the CFLLMs framework to cultivate the LLMs and scrutinized their performance.\nIn pursuit of Q4, our framework was applied to LLMs of varying sizes, including Vicuna models (7B, 13B, 30B) [26], [27] and Llama2 models (7B, 13B) [28]. To facilitate deployment on a consumer-grade Nvidia RTX 4090 GPU [29] while minimizing memory usage and inference time, we employed the GPTQ [30] approach to quantize the models to 4-bit precision. It is important to note that, despite the ability of 7B-sized models to operate on consumer-grade GPUs without quantization, we opted for 4-bit quantization across all models to maintain a uniform comparison scale and minimize quantization errors.\nTo address the variability in dataset sizes and their potential influence on experimental results, we standardized our approach by extracting a consistent sample of 1000 data entries from each dataset to form the training set for the LLMs\u2019 self-practice. An additional 1000 data entries were selected to comprise the test set. To maintain experimental uniformity, each data entry within these subsets was numbered. For each experiment, we consistently used the first n numbered data entries, with n representing the requisite volume of data for the specific experimental conditions.\nFor Q5, we selected datasets encompassing various reasoning tasks, such as GSM8K [31], ReClor [32], and LogiQA2.0 [33], [34]. GSM8K comprises over 8,000 quality elementary mathematics problems crafted by human authors to assess arithmetic reasoning in LLMs. ReClor features questions from logical reasoning sections of standardized tests like the GMAT and LSAT, challenging the LLMs' critical thinking and complex logical reasoning skills. LogiQA2.0, based on questions from the Chinese civil service exam translated and validated by professional translators and human experts, evaluates the LLMs' capacity for generalizing natural language reasoning."}, {"title": "C. Results", "content": "We conducted experiments across a variety of LLMs, differing in type and size, as well as on diverse datasets, to evaluate their reasoning capabilities. This evaluation was based on the mean answer accuracy derived from five experimental trials, detailed in Table I. It is important to note that the figures succeeding \"CFLLMs\u201d in the table signify the volume of data utilized for the LLMs' self-practice. The underscored values denote the peak accuracy attained with this methodology for the consistent model and dataset, whereas the bolded values represent the maximum accuracy achieved without employing the CoT method to prompt incremental reasoning, compelling the model to directly generate answers. Red-highlighted numbers in the table reveal that our framework, under the given experimental conditions, did not improve but rather diminished performance.\nWith this groundwork, we can address Q1 and Q2 introduced in Section III-A. The implementation of CoT markedly influences the models' reasoning proficiency on tasks that entail natural language inference, such as reading comprehension and logical deduction. For instance, on the LogiQA2.0 dataset, the accuracy rates for smaller models like Llama2-7B and Vicuna-7B plummet to near zero in the absence of CoT. However, the deployment of the CogniDual Framework, has resulted in a substantial enhancement of performance without CoT. Despite the models' reasoning accuracy not equalling that of CoT use, their ability to intuitively respond to certain questions suggests an inherent decision-making logic akin to the human dual-system cognitive framework. This insight indicates the potential for transforming System 2 capabilities into System 1 through sustained practice, thereby bolstering the LLMs' rapid response to specific queries and diminishing the time and computational resources required for reasoning.\nMoreover, we observed a negligible improvement from the CogniDual Framework on the GSM8K dataset, attributed to the models' propensity for step-by-step reasoning even when instructed to directly answer. The prevalence of LLMs producing answers with comprehensive derivations is likely due to task contamination, as postulated by Liu et al. [35], where mathematical problems are consistently presented with accompanying detailed solutions throughout the training phase. Our framework aims to enhance the System 1 capabilities of LLMs, rather than augment System 2 directly. Consequently, we can deduce from Q5 that only tasks exhibiting a substantial discrepancy in accuracy between CoT usage and non-usage enable LLMs to advance their internalized reasoning abilities through self-practice.\nFor Q3 and Q4, the results in Table I indicate that, in general, an increase in additional examples correlates with a more pronounced enhancement in the LLMs' reasoning abilities without CoT, achieved through self-practice. Larger models require fewer examples to approach their System 1 capacity ceiling; beyond this point, further example data yield minimal benefits. This finding suggests that larger models are more adept at leveraging limited data to improve performance without CoT guidance through self-practice, aligning with the research by Jaimovitch et al. [36]."}, {"title": "IV. CONCLUSION", "content": "This study explores the dual cognitive characteristics of LLMs. Our experimental results indicate that once LLMs internalize CoT reasoning through self-training, they can retain CoT-enhanced problem-solving abilities even without CoT prompts. This finding supports the hypothesis that, with appropriate training, LLMs can convert complex, deliberative System 2 reasoning into faster, more intuitive System 1-like responses. Leveraging this property, we designed a self-training framework to reduce the cognitive load of LLM reasoning. Despite these advancements, further research is necessary to address the study's limitations, including examining how this framework influences the cognitive processing preferences of LLMs."}]}