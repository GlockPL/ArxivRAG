{"title": "LLM Gesticulator: Leveraging Large Language Models for Scalable and Controllable Co-Speech Gesture Synthesis", "authors": ["Haozhou Pang", "Tianwei Ding", "Lanshan He", "Qi Gan"], "abstract": "In this work, we present LLM Gesticulator, an LLM-based audio-driven co-speech gesture generation framework\nthat synthesizes full-body animations that are rhythmically aligned with the input audio while exhibiting natural\nmovements and editability. Compared to previous work, our model demonstrates substantial scalability. As the\nsize of the backbone LLM model increases, our framework shows proportional improvements in evaluation metrics\n(a.k.a. scaling law). Our method also exhibits strong controllability where the content, style of the generated\ngestures can be controlled by text prompt. To the best of our knowledge, LLM gesticulator is the first work that\nuse LLM on the co-speech generation task. Evaluation with existing objective metrics and user studies indicate\nthat our framework outperforms prior works.", "sections": [{"title": "1. INTRODUCTION", "content": "Gesture, a fundamental aspect of human communication, transcends linguistic barriers and enriches the con-\nveyance of thoughts, emotions, and intentions. It is an intricate movement of the human body that complements\nspeech, providing a nuanced layer of non-verbal interaction. The significance of gesture in communication is\nunderscored by its role in emphasizing speech content, clarifying complex ideas, and facilitating social cohesion.\nThe accurate depiction of gestures is paramount for creating immersive and interactive experiences in various\nsectors, including gaming, film production, robotics, and virtual reality.\nThe quest to generate realistic co-speech gestures has led to the development of rule-based and data-driven\nmethods. Rule-based approaches often rely on predefined rules that govern the synchronization of gestures with\nspeech patterns. Conversely, data-driven methods leverage machine learning to learn the complex mappings\nfrom speech to gestures, offering a more organic synthesis process.\nIn this paper, we propose a novel framework for generating full-body co-speech gestures that are rhythmically\naligned with speech while maintaining scalability and editability. Our approach leverages recent advancements in\nLarge Language Models (LLM) by modeling the gesture generation problem as a sequence-to-sequence translation\ntask. To the best of our knowledge, this is the first attempt of tackling the co-speech generation task using LLMs.\nOur experiment shows that by proper tokenization of input and delicate design of the LLM training framework,\nour system can generate high quality gestures that are natural, rhythmic and contextually appropriate. Thanks\nto the text comprehension ability of pre-trained LLM model and our training framework, our system extends\nthe generation editable capability by allowing user to control motion generation by giving text prompt.\nIn summary, our main contributions are as follows:\n\u2022 We present a framework that is built based on a large language model to generate full body (body + hand)\ngestures according to input audio and text prompt. Our method outperforms prior works on existing\nevaluation metrics and user studies.\n\u2022 Our proposed training scheme supports controllable gesture generation based on text prompts.\n\u2022 We propose a new data augmentation paradigm by utilizing rendering engines and VLLM models. We\nannotate the motion descriptions of BEAT dataset and we will release it to the community for future\nresearch."}, {"title": "2. RELATED WORK", "content": ""}, {"title": "2.1 Co-speech Gesture Generation", "content": "Synthesizing co-speech gestures is an essential task across various fields, such as games, films, robotics, and\nvirtual reality applications, to create interactive and immersive user experiences. Early methods use rule-\nbased approaches to synthesize gestures, which primarily rely on a carefully hand-crafted heuristic to map\ninput speech to a set of appropriate gesture clips. Wagner et al. provide a comprehensive summary of rule-\nbased methods. Such methods are highly explainable and controllable, however, the generation diversity and\nnaturalness is limited by the design of rules, and adding more gesture units requires extra manual work, which\nis infeasible for large-scale systems.\nTo reduce the manual work in the process of designing rules, and improve the generation naturalness, data-\ndriven methods have drawn much attention recently because of its ability to synthesis gestures from implicit\nrepresentation of training datasets. Nyatsanga et al. gives a comprehensive review of data-driven methods.\nEarly data-driven methods try to build a statistical system to predict a gesture label that is used to index\npre-recorded gesture clips. Compared to rule-based methods, statistical approaches make fewer assumptions\nabout the speech-gesture association, but they still require a well crafted collection of gesture units to generate\nthe final motion. Subsequently, the advanced capabilities of deep neural networks enable the training of models\ndirectly using raw gesture data in an end-to-end manner. The development of deep-learning based gesture\ngeneration systems is analogous to that of other generation tasks. We briefly introduce some representative\nworks using different network architectures. Deterministic models including MLP, CNN, RNN and"}, {"title": "2.2 Dataset", "content": "The volume and quality of datasets is fundamental to any data-driven method. Current public available datasets\ncan be categorized into two groups, pose-estimated from monocular video and motion-captured. The former use\n3D pose estimation algorithm to extract motions from videos, Speech2Gesture and TED estimate the 2D poses\nfirst, and then lift to 3D poses and SMPL-X. Although pose-estimated methods largely enrich the diversity of\nmotion data, but they are generally limited by the motion quality, and do not contain fingers animations. Motion-\ncaptured data usually have better motion quality, Trinity dataset propose 4 hours of motion data performed\nby a male actor along with audio data. The TalkingWithHand dataset record conversational motions from\ntwo speakers. The ZEGGS dataset propose 2 hours of motion data captured from a single speaker performing\nwith 12 different styles. BEAT dataset is by far the largest mocap dataset, it contains 76 hours motion data\nalong with facial blend-shapes and emotions.\nTo the best of our knowledge, none of the previous dataset contains audio, motion, and text annotations\n(describing the content of motion) at the same time. In our work, we render the motion clips in BEAT dataset\nand use a VLLM model through prompt engineering to describe and summary the detailed motions in each clip\n(Fig. 5). Thus, another modality is added to the BEAT dataset. The text description is shown to be useful to\nextend our framework's controllability such that the style and content of generated gestures can be guided by\nuser's prompt at inference time. We will release the text annotations for BEAT dataset for research purpose."}, {"title": "2.3 Multi-Modal Data Representation", "content": "Co-speech generation task takes input from multiple modalities, including audio, text, and other speaker-wise\ninformation like speaker ids, emotions. The representation and alignment of different modalities have been\nstudied in previous works. For audio input, Mel-spectrogram and MFCC features are commonly used. As\nthe development of audio compression technology, audio codec is also commonly used in audio-driven tasks for\na more abstract and tokenized feature representation. Text input is usually encoded by pre-trained language\nmodels like BERT. The audio and text features are resampled and aligned into the same length of the motion."}, {"title": "2.4 Evaluation of Motion Generation Model", "content": "Evaluating co-speech gestures is challenging due to the stochastic nature of human gesture perception. Most\nprevious works use user studies to evaluate different aspects of motion quality, such as human-likeliness and"}, {"title": "3. SYSTEM OVERVIEW", "content": "In this section, we provide a comprehensive overview of how our system processes text prompts and speech audio\ninputs to predict motion sequences(as shown in Fig. 2). First, we perform motion tokenization, with the detailed\nmethodology outlined in Section 3.1. Subsequently, we describe the pretraining of large language models (LLMs)\nto enable them to comprehend and generate audio data related to motion, as discussed in Section 3.2. Finally,\nwe explore the generation of co-speech motion using text prompts, with specific methods and experimental\nresults presented in Section 3.3. Our system effectively integrates text guidance with audio-synchronized motion\ngeneration, facilitating the production of high-quality motion representations."}, {"title": "3.1 Motion Tokenization", "content": "With the rapid advancement in language model research, LLMs have exhibited exceptional performance. Inspired\nby the strategies employed in LLMs, we attempt to transform the task of audio-synchronized motion generation\ninto a translation problem solvable by LLMs. Consequently, we adopt the approach of Residual Vector Quantized\nVariational Autoencoder (Residual VQVAE) to construct MotionRVQ for the tokenization of motion data.\nFor a given motion sequence, we utilize the rotations and offsets of each joint as features, represented as\nfollows:\n$m_i = \\{R_0, R_1,..., R_N\\}$\n$M = \\{m_0, m_1,,\u2026\u2026\u2026,m_t\\}$"}, {"title": "", "content": "where $R_i$ denotes the rotation and offset of the i-th joint and $m_i$ denotes the motion feature of the i-th\nframe. The MotionRVQ network can be represented as:\n$Z = \\Phi(M)$,\nwhere is a function representing our MotionRVQ encoder. The tokenization process can be modeled as:\n$Z^* = \\sum_{l=1}^{L} q_l(\\epsilon_l)$,\nwhere L is the number of quantization levels, $q_l(\\cdot)$ denotes the quantization function at level l, and $\\epsilon_l$\nrepresents the residual encoding at level l. The quantization at each level is defined as:\n$q_l(\\epsilon_t) = arg \\underset{c \\in C_l}{min} ||\\epsilon_i - c||^2,$\nwhere $C_i$ is the codebook at level l, and c represents the embeddings in the codebook. The residual encodings\nare computed recursively as follows:\n$\\epsilon_1 = \\Phi(M)$,\n$\\epsilon_l = \\epsilon_{l-1} - q_{l-1}(\\epsilon_{l-1}), for l = 2, . . ., L,$\nwhere $(\\cdot)$ is the encoder function.\nMotion tokenizer's architecture comprises convolutional networks and a Transformer Encoder, employing\na downsampling rate of 8. Training techniques such as exponential moving average (EMA) and random re-\ninitialization of the inactive codebook entries are used to ensure training stability and enhance quality.\nTo train our MotionRVQ model effectively, we define a loss function that balances reconstruction accuracy\nand codebook utilization. The overall loss function L consists of three components: the reconstruction loss $L_{rec}$,\nthe commitment loss $L_{commit}$, and the codebook loss $L_{codebook}$. The reconstruction loss measures the discrepancy\nbetween the original motion sequence M and the reconstructed sequence M, defined as:\n$L_{rec} = ||M - M||^2,$\nwhere $M = \\Psi(Z^*)$, and $\\Psi(\\cdot)$ is the decoder function. The commitment loss ensures that the encoder's\noutputs commit to the codebook entries, encouraging efficient codebook usage. It is defined as:\n$L_{commit} = \\beta \\sum_{l=1}^{L} ||\\epsilon_i - sg [q_l(\\epsilon_l)]||^2,$\nwhere $\\beta$ is a hyperparameter controlling the strength of the commitment, and sg[\u00b7] denotes the stop-gradient\noperation, which prevents gradients from flowing through its argument during backpropagation. The codebook\nloss updates the codebook entries to match the encoder outputs:\n$L_{codebook} \\sum_{l=1}^{L} ||sg [\\epsilon_l] - q_q(\\epsilon_l)||^2.$\nThe total loss function combines these components:\n$L = L_{rec} + L_{commit} + L_{codebook}.$\nBy optimizing this loss function, the model learns to reconstruct motion sequences accurately while effectively\nutilizing the quantization codebooks. This balance is crucial for generating high-quality motion representations\nsuitable for translation by LLMs."}, {"title": "3.2 Pretraining the LLM on Motion and Audio Data", "content": "For the co-speech Motion generation task, alongside the tokenization of motion, it is essential to also tokenize\nthe audio. Recent advancements in audio representation have led to the development of effective methods for\ntokenizing speech.\nUsing the motion tokenizer and audio tokenizer, we convert motion and audio data into discrete token\nsequences. Let M denote the tokenized motion sequence and A denote the tokenized audio sequence This\ntransformation allows us to frame audio-driven motion generation as a sequence-to-sequence generation task.\nFollowing prior work to enhance the LLM's understanding of motion and audio, we first perform motion\nsequence pre-train and audio sequence pre-train before fine-tuning. For the motion completion task, the model\nlearns to predict the next element in the motion sequence, formalized as estimating the conditional probability\n$P(m_t | m_1, ..., m_{t-1})$, where $m_t$ is the element at time step t and $m_1,..., m_{t-1}$ is the history of the motion\nsequence up to time step t - 1. Similarly, for the audio completion task, the model estimates $P(a_t | a_1, ..., a_{t-1})$\nto predict the next element in the audio sequence. This process helps the LLM build comprehension of the newly\nintroduced motion and audio tokens. Our experiments (Fig 7.) demonstrate that this process can enhance the\ngeneration quality."}, {"title": "3.3 Text Prompted Co-Speech Motion Generation using LLM", "content": "Subsequently, we continue fine-tuning on subtasks such as generating motion from audio and generating motion\nbased on prompts and audio. Specifically, the audio-driven motion generation task is modeled as learning\nthe conditional distribution P(M | \u00c0), aiming to generate a motion sequence conditioned on a given audio\nsequence.\nFor the task of generating motion based on prompts and audio, we model P(M | \u00c0, T), where the\nmodel generates motion sequences conditioned on both the audio sequence and textual prompts.\nOur data composition is illustrated in Figure 2. Since this task focuses primarily on motion generation, we\nperform supervised fine-tuning solely on the motion token portion. The fine-tuning objective is to minimize the\nnegative log-likelihood loss:\n$L_{SFT} \\sum_{t=1}^{T_M}log P(m_t | \u00c0, T, m_{<t}),$"}, {"title": "4. EXPERIMENT", "content": ""}, {"title": "4.1 Dataset", "content": "Co-Speech Motion Dataset. We utilize the BEAT dataset and the ZEGGS dataset for training. The\nBEAT dataset is a high-quality corpus of speech motion data, featuring 30 characters and over 70 hours of\nmotion data, along with corresponding audio, dialogue data, and annotations for key motion nodes. For the\nMotionRVQ, in alignment with the base-line setting in CaMN, we select characters with speaker IDs 2, 4, 6,\nand 8 for training. For the LLM, we tokenize the motions of all characters using the aforementioned MotionRVQ\nand tokenize the audio from all characters using Encodec. The ZEGGS dataset contains approximately 2 hours\nof motion data, encompassing 19 different styles. Due to the smaller size of the ZEGGS dataset, we utilize all\navailable data for MotionRVQ training and LLM fine-tuning.\nGenerated text prompt dataset. We retarget all motions from the BEAT dataset to our character assets and\nrender them into video using Unity. We use different characters for different gender of speakers in the dataset.\nSubsequently, we slice the videos and annotate each slice using the QWen VL 7B model to obtain natural\nlanguage descriptions of the video content (Fig. 5)."}, {"title": "4.2 System Setup", "content": "Regarding the MotionRVQ, we employ a three-layer convolutional neural network and a six-layer Transformer\narchitecture as the encoder, with a symmetric structure serving as the decoder. Our codebook size is set to\n512, and the embedding space size is also 512. The depth of our residual layers is 4. We utilize a learning rate\nscheduler with a cosine decay and warmup, and employ the AdamW optimizer with a learning rate of 2 \u00d7 10-4\nfor training. Training the MotionRVQ on the ZEGGS dataset requires 2 A100 GPUs for 10 hours, whereas the\nsame setup takes over a day to complete training on the BEAT dataset."}, {"title": "4.3 Quantitative Results", "content": "We quantitatively evaluate the generated results with existing metrics including FGD, Beat Alignment Score,\nand Diversity. FGD is calculated by comparing features of ground truth data and generated data, where the\nfeatures are extracted by an auto-encoder network that is trained for reconstruction task. Beat Alignment Score\nwas introduced by to measure the alignment of audio beats and motion beats. Diversity measures the average\ndistance between each pair of gesture samples in the test dataset using the Euclidean distance metric.\nThe test set comes from the BEAT dataset including 1183 5-second unseen motion sequences of 4 speakers.\nAs shown in Table 1, our method achieves a better FGD. The Diversity and Beat Alignment Score of ours are\ncloser to the gound truth."}, {"title": "4.4 User Study", "content": "Following the method in previous works, we conduct user studies using pair-wise comparison. We create a\ndataset comprising 30 video pairs, with each pair containing our model's output, and another model's output or"}, {"title": "4.5 Ablation Study", "content": "Validation of the scaling law. We trained LLM models of different scales under the same configuration, as\nshown in Figure 7. The results indicate that larger models produce higher quality action sequences. In contrast,\nsmaller models often face issues such as sequence repetition and excessively long sequences. This demonstrates\nthe importance of model scale in generating diverse and coherent action sequences.\nTraining from scratch versus LLM Finetuning. The primary reason for choosing to construct the system\nusing large language models (LLMs) is that training a model from scratch often fails to achieve a sufficient\nunderstanding of language. We conducted a simple validation experiment by constructing a motion sequence"}, {"title": "5. CONCLUSION", "content": "In this paper, we present LLM Gesticulator, a novel framework to synthesize controllable co-speech gestures.\nWith the goal of generating natural, rhythmic, and controllable gestures, we formulate the co-speech generation\ntask as a sequence-to-sequence translation problem so that by proper tokenization of data in different modalities,\nwe could leverage pre-trained large language models to accomplish this task. We conduct experiments on LLM\nmodels of different scales and observe performance gains as the model size increases. This finding is consistent\nwith the scaling law and demonstrates the scalability of our approach. We render and annotate the motion clips\nin BEAT dataset and use the video content description as text prompt in our training framework, thanks to\nthe text understanding ability of pre-trained LLM models, our method demonstrates strong text controllability\nwhere the style and content of generated gestures can be controlled by user prompts. Our method outperforms\nprior works on existing quantitative evaluation metrics and is more preferable in user studies in terms of human\nlikeness and audio alignment.\nHowever, there is still room for improvement in our framework. Our method cannot achieve real-time stream\ninference. We believe that with the advancement of acceleration techniques such as quantization and distillation,\nand better engineering practices, real-time stream co-speech synthesis is possible to achieve. In addition, we\nconsider the text and audio modalities of input data in this work, but our framework can be easily extended to\nsupport other modalities such as images or videos by proper tokenization and alignment process. Generating\nfull-body gestures that include facial-expressions is also an interesting topic, and we left them as a directions\nworth exploring in the future."}]}