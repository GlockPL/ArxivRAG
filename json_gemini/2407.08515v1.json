{"title": "15M Multimodal Facial Image-Text Dataset", "authors": ["Dawei Dai", "YuTang Li", "YingGe Liu", "Mingming Jia", "Zhang YuanHui", "Guoyin Wang"], "abstract": "Currently, image-text-driven multi-modal deep learning models have demonstrated their outstanding potential in many fields. In practice, tasks centered around facial images have broad application prospects. This paper presents FaceCaption-15M, a large-scale, diverse, and high-quality dataset of facial images accompanied by their natural language descriptions (facial image-to-text). This dataset aims to facilitate a study on face-centered tasks. FaceCaption-15M comprises over 15 million pairs of facial images and their corresponding natural language descriptions of facial features, making it the largest facial image-caption dataset to date. We conducted a comprehensive analysis of image quality, text naturalness, text complexity, and text-image relevance to demonstrate the superiority of FaceCaption-15M. To validate the effectiveness of FaceCaption-15M, we first trained a facial language-image pre-training model (FLIP, similar to CLIP) to align facial image with its corresponding captions in feature space. Subsequently, using both image and text encoders and fine-tuning only the linear layer, our FLIP-based models achieved state-of-the-art results on two challenging face-centered tasks. The purpose is to promote research in the field of face-related tasks through the availability of the proposed FaceCaption-15M dataset. All data, codes, and models are publicly available.", "sections": [{"title": "1 Introduction", "content": "In recent years, large-scale data-driven pre-training models have achieved significant success in fields of computer vision and natural language processing [1, 20, 21, 50, 55, 71, 82]. In practical applications, tasks centered around faces have broad application across various aspects, such as social media, mobile payment systems and security monitoring. These applications heavily rely on the availability of datasets, such as ImageNet [14], CelebA [43], CelebA-HQ [31], LAION-Face [86], FFHQ [32] and CelebV-Text [81].\nA multimodal image-text model exhibit the potential to achieve better generalization capabilities. However, training a multi-modal model for aligning face image-text remains challenging owing to the limitations in existing open-source face image datasets. These limitations include the weak correlation between text and facial images, and a shortage of samples. A large-scale, high-quality facial image-text dataset can significantly promote the efficient development of face-related research. In this work, \"large-scale\" refers to the dataset size that meets the requirements for training large models, such as CLIP, with tens of millions or more samples; \"high-quality\" refers to a dataset primarily comprising facial images"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Face-related Tasks", "content": "Facial images have given rise to numerous practical applications. For example, image retrieval [12, 62, 83], face recognition [36, 42], facial image generation [16, 54, 66], facial image parsing [41, 85, 86] and editing [27, 30, 66, 69]. For task of facial attribute recognition, many studies [7, 16, 46, 59] have been explored to improve the accuracy, which are primarily based on datasets such as CelebA [43] and LFWA [76], with widespread applications across multiple fields. For sketch-based facial image retrieval task, widely used datasets include FS2K [17] and CUFSF [84]. Efforts [11, 12, 17, 22, 35] in this area mainly focus on (1) learning efficient representations for sketch and image. This task holds great potential applications, including digital entertainment and suspect identification. In recent years, the way of fine-tuning a pretrained models has demonstrated significant advantages [4, 13, 45, 51, 65]."}, {"title": "2.2 Datasets", "content": "Single-modal datasets. Single-modal face datasets mainly consist of images, which played a pivotal role in early research. For example, CelebA [43] and LFWA [76] datasets provide a large number of facial attribute annotations; VGGFace2 [8] and CASIA-WebFace [78] datasets were specifically designed for face recognition tasks. They include a large number of celebrity face images collected from the Web, as well as face images captured under different conditions, such as different poses, expressions, and lighting conditions.\nMulti-modal datasets. The Flickr30k [80] dataset includes approximately 31,000 facial images collected from Flickr, each of which has been annotated using five reference sentences by human annotators. However, these images often feature complex backgrounds, and the corresponding text do not naturally capture facial features. Although MM-CelebA [77] and CelebA-Dialog [28] contain multiple pairs of face description labeled by humans, the sample sizes are insufficient, making them inadequate for training a large model. LAION-Face [86] dataset, a subset of LAION-400M [61], stands as the largest existing face image-text dataset, containing approximately 50M pairs of image-text. However, the text within this dataset was directly extracted from the Internet and exhibit very weak correlation with the face. Moreover, the images often contain complex backgrounds, which may not suffice to support delicate facial-related tasks.\nIn Summary. Owing to the lack of large-scale and high quality facial image-text datasets, researchers of-ten first train a model (such as ResNet [25], VIT [19] and CLIP [52]) on the general large-scale datasets such as LAION-5B [60], CC [64], ImageNet22K [56], and COCO [40] as pre-trained modules. Subsequently, they fine-tune the pre-trained models on a smaller-scale dataset for specific facial-related tasks. However, these pre-trained models tend to exhibit insufficient generalization capabilities on facial-related tasks. Overall, various limitations underscore the pressing need for a large-scale, high-quality, multi-modal facial dataset featuring natural language descriptions of facial details to support more complex facial-related tasks [38, 75]."}, {"title": "3 Dataset Construction", "content": "Our purpose is to construct a large-scale and high-quality facial image-text dataset featuring natural language descriptions that accurately describe facial features. The general approach of building such a large-scale facial image-text dataset is to first collect as many images containing people as possible, then select an excellent-performing algorithm to segment the face part of the image, and finally select or design a suitable algorithm to generate high-quality text descriptions for the face images [29, 32, 81]. As shown in Figure 2, for the construction of our FaceCaption-15M."}, {"title": "3.1 Facial Images Collection", "content": "Image Collection. Specifically, we accessed the LAION-Face [86] dataset, which contains over 50M image-text pairs that obtained through web crawling, as our source of raw data. LAION-Face is of a"}, {"title": "3.2 Facial Attributes Annotation", "content": "Attributes play a pivotal role in generating the description text for facial image, thereby determining the correlation between the image and text. We designed 40 appearance attributes for facial features. Given the considerations of annotating a vast amount of data, we selected an automatic annotation method. In terms of efficiency and accuracy, we employed an open-source algorithm [26] for predicting image attributes. To enhance the reliability of annotations, we retained only the labels predicted by the model with a probability exceeding 0.85. Furthermore, to generate more accurate natural language text, we retained samples with more than five valid predicted labels. Finally, we reduced the dataset size to 15M."}, {"title": "3.3 Facial Caption Generation: Raw Text Generation and Rewriting", "content": "Since, image-text pairs in LAION-Face dataset were obtained through subtitle crawling, and the text showed a weak correlation with the accompanying image. Our aim is to generate the caption of fa-cial images. The manual annotation, while accurate, is time-consuming and labor-intensive, making it unsuitable for constructing large-scale datasets. However, automatic methods often offer efficiency and scalability. Nevertheless, the diversity, complexity, and naturalness of description sentences generated by traditional automatic text generation methods are limited by grammatical templates. With the de-velopment of LLM [2, 3, 20, 23, 50, 72], text generated by these models is endowed with high diversity and naturalness. Here, we propose a text generation strategy that combines grammatical templates with LLM. Specifically, (1) we first input the attribute annotations generated by Section 3.2 into the designed grammatical template to generate the raw text, and then (2) we input the raw text into the LLM to generate natural, diverse, and accurate text descriptions.\nTo ensure the generation of high-quality description text using LLM, the quality of the raw text generated by the grammatical template is paramount. Here, we adopted the probabilistic context-free grammars (PCFG [70]) algorithm to generate the raw text as multiple short sentences, each constructed using different attributes. The performance of the LLM model itself affects the quality of the generated caption. We conducted research on existing open-source LLMs and finally selected the Qwen-7B-Chat model [2]."}, {"title": "4 Statistical Analysis for FaceCaption-15M Dataset", "content": "In this section, we conducted a comprehensive analysis for our FaceCaption-15M dataset. To provide an overview, we compared the overall statistics of FaceCaption-15M with some existing popular face datasets, as presented in Table 1. More comparisons are as the following sections."}, {"title": "4.1 Image Quality Comparisons", "content": "We first conducted a quantitative comparison of image quality across some facial image datasets: CelebA-Dialog [28], MM-CelebA [77], CelebV-Text [81] (randomly selecting 10 frames from each video to evaluate their quality), and LAION-Face [86]. We employed two general no-reference image quality assessment methods, BRISQUE [47] and CLIPIQA [73], to evaluate the image quality within these datasets. The BRISQUE method evaluates image quality by calculating the local normalized brightness coefficient of the image pixels, where lower scores indicate better image quality. CLIPIQA method [53] assesses image"}, {"title": "4.2 Text Comparison", "content": "Firstly, we performed a statistical analysis of the attribute annotations within FaceCapition-15M. As shown in Figure 4(a), we divided the 40 facial-appearance attributes into five categories. Among these categories, facial characteristics (such as a double chin and big nose) account for approximately 33.9%. The elementary characteristics of the portrait (including young, male, and blurry) accounted for approx-imately 26.7%, while beard and hair characteristics each account for approximately 14.9% and 13.9%, respectively. Accessory characteristics made up 10.6% of the annotations.\nIn comparison to CelebA-Dialog [28], MM-CelebA [77], CelebV-Text [81] and LAION-Face [86], the text within FaceCaption-15M are more extensive and detailed (See Figure 4(b)). Specifically, the average"}, {"title": "4.3 Text-Image Relevance Comparison", "content": "To quantitatively verify the correlation between facial images and text, we adopted the image-text matching (ITM) score [38, 39] as a metric to measure the degree of alignment between the two modalities. A larger ITM score indicates a greater correlation between text and images. In this regard, we selected an equal number of samples from the CelebA-Dialog [28], LAION-Face [86], CelebV-Text [81] (randomly selecting some frames from each video), FaceCaption* and FaceCaption-15M datasets to fine-tune the last linear layer of BLIP [38], obtaining five new BLIP models. These models were then used for the text-based image retrieval task on the test set of MM-CelebA [77], which exhibits a high correlation between text and images within FaceCapition* and FaceCapition. As shown in Figure 5, the distribution of ITM scores in FaceCaption-15M surpassed that of CelebA-Dialog, CelebV-Text and LAION-Face. Notably, the distribution of ITM scores in CelebA-Dialog performed the worst, further proving that FaceCaption-15M exhibits a high correlation between the descriptions and facial images."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 FLIP: Facial Language-Image Pre-training Model", "content": "Architecture: Based on FaceCaption-15M, we trained a multimodal representation model (FLIP), sim-ilar in concept to CLIP [53], designed for aligning facial images with semantics. As shown in Figure 6(a), FLIP contains the following components: (1) Image Encoder [19]: Composed of a visual transformer, this component processes the image. (2) Text Encoder: When handling text input alone, this encoder follows the standard BERT [18] module and uses the [CLS] token to summarize the entire sentence. In the case of multimodal input, a cross-attention layer is introduced between the self-attention layer and the feed-forward network of the text encoder to fuse visual information (Image-grounded Text Encoder). To adapt to specific tasks, an [ENC] token is added to the text input, serving as the multimodal representation for the image-text pair."}, {"title": "Loss functions", "content": "Two distinct loss functions were designed to train the proposed FLIP model: (1) Image-Text Contrastive loss (ITC): It aims to align the feature spaces of facial images and caption text [53]. (2) Image-Text Matching loss (ITM): It is intended to facilitate the learning of image-text multimodal representations and align fine-grained features between image and text. ITM essentially represents a classification task, where the model predicts whether an image-text pair matches via an ITM head [38, 39].\nFLIP Settings: The visual encoder of FLIP adopts a 12-layer ViT-B/16 [19] with a token dimension of 768, and the input is a 224 \u00d7 224 image (about 223M parameters). A learnable [CLS] token was added before these 196 embeddings, resulting in 197 embeddings. The text encoder adopts a 12-layer BERT [18] model with an embedding dimension of 768. We fixed the number of input text tokens to 65 and truncated or padded the input if it did not match. Finally, we projected the image [CLS] and text [CLS] tokens into a 256-dimensional space, computed the ITC loss, inputted the 197 feature embeddings of ViT-B/16 into the BERT model, replaced the [ENC] token with the text feature embedding [CLS] token, and computed the ITM loss through cross-attention.\nTraining FLIP: We implemented FLIP using the PyTorch framework and conducted the training on 8 \u00d7 NVIDIA A100 GPUs. First, regarding the hyperparameter settings, we used a batch size of 8 \u00d7 80 for the 8 \u00d7 NVIDIA A100 GPU. Similar to ALBEF [39], we set the queue size to 61440 to preserve the text and image features of multiple historical samples, which helped improve the robustness of the model. The image and text encoders were initialized using ViT-B [19] and BERT-base [18], respectively. We employed the AdamW [44] optimizer with a weight decay of 0.05. The learning rate was initialized to 1e-6, warmed to 3e-5 in 2,000 steps, and the cosine decayed to 1e-6 over 15 epochs. Notably, we used mixed precision to accelerate the training and save memory."}, {"title": "5.2 Text-Image Retrieval", "content": "Facial image-text retrieval tasks involve finding an image (text) based on a given text (image). To demonstrate the superior performance of FaceCaption-15M on this task, we conducted evaluations on CelebA-Caption and MM-CelebA [77] datasets. For CelebA-Caption, we used 162,770 samples for training and 19,962 samples for testing. For the MM-CelebA [77] dataset, we used 24,000 training samples and 6,000 testing samples. We adopted the Top-K retrieval index, where R@5 and R@10 represent the top-5 and top-10 accuracy, respectively.\nBaselines: We selected three benchmark pre-trained models for comparison: (1) ALIGN, which was trained on a large-scale dataset containing 1 billion image-text pairs. (2) BLIP [38], which was trained"}, {"title": "5.3 Facial Attributes Prediction", "content": "This task involves predicting attributes of a given facial image, such as gender, hairstyle, and so on, which can be regarded as a multilabel classification task. It has a wide range of applications in fields such as recommendation systems and security monitoring. To verify the effectiveness of our FLIP on this task, we conducted evaluations on the CelebA [43] and LFWA [76] datasets. For CelebA [43], we utilized 162,770 samples for training and 19962 samples for testing. For LFWA [76], we employed 6263 samples for training, reserving the rest for testing. Average precision (AP) was used as the evaluation index.\nFine-tuning FLIP. For this task, we only fine-tuned the final linear layer of FLIP. As shown in Fig-ure 6(c), we frozen image encoder and used a randomly initialized new projection head as the output layer. This projection head utilized the features from K layers of the ViT module, where K={4,6,8,12}. We obtained three types of feature vectors: (1) the [CLS] token feature vector; (2) the average feature vector of all non-[CLS] tokens; (3) the max pooling feature vector of all non-[CLS] tokens. And then, we perform layer-normalize the 12 feature vectors and linearly combine them into one feature vector, followed by a fully-connected layer. Using AdamW [44] optimizer, an initial learning rate of 0.3, and a weight decay of 0.05, with cosine decay to 0 after 100 epochs.\nPerformance Analysis. In the case of the CelebA [43] and LFWA [76] datasets, we adopted the same ways with [86] that three different scale training subsets (1%, 2%/10%, and 100% for CelebA and 10%, 50%, and 100% for LFWA) to fine-tune the linear layer of the FLIP model and subsequently evaluated its performance on the testing dataset. As shown in Table 3, we can observe that: (1) The performance gradually improves with the increase in the amount of training data. (2) Interestingly, FLIP-based model exhibits a greater advantage when fine-tuned with less data. For example, the improvement of the FLIP model on LFWA is greater than that on CelebA when using a smaller percentage of data for fine-tuning. (3) Despite fine-tuning only the linear layer of FLIP, without further optimization of these features, our proposed model outperforms all the listed models trained on other datasets. These experiments demonstrate the capacity of FaceCaption-15M to enable the learning of more effective facial"}, {"title": "5.4 Sketch Less Facial Image Retrieval (SLFIR)", "content": "Traditional sketch-based facial image retrieval (SBIR) depend on the high-quality sketch [10, 57, 58], which hinders its widespread applicability. SLFIR [12] framework was designed to break these barriers by dynamic interactions during the sketching process. Its goal is to retrieve the image using a partial (poor) sketch with as few strokes as possible. For SLFIR, the main challenge is that the partial sketch often contains only localized information. In this section, we develop a multimodal learning method based on FLIP for the SLFIR problem.\nDataset. Based on FS2K dataset [17], Dai et al. [12] constructed FS2K-SDE1 and FS2K-SDE2 datasets to address SLFIR problem. For FS2K-SDE1, 75,530 sketches and 1,079 images were used for training, the rest being used for testing. For FS2K-SDE2, 23,380 sketches and 334 images were used for training, the rest being used for testing. Owing to the significant differences in sketching among painters, Dai et al. [12] constructed User-SDE that invited 50 professional painters to submit 110 sketch-drawing episodes. Following the method in Section 3, we generated corresponding caption for the three datasets.\nFine-tuning FLIP. As shown in Figure 6(d), we frozen the image and text encoders, and initialized two linear layers as the output layers of the image and the text encoders, respectively. The fused feature vector obtained by adding the sketch and text feature vectors was used for the final retrieval. And we make the training settings of all linear layers consistent: using the AdamW [44] optimizer with an initial learning rate of 1e-06 and a weight decay of 0.05, the learning rate is adjusted to 0 by cosine decay within 100 epochs. The specific architecture of the fine-tuning model is illustrated in Figure D1. As mentioned in [12], m@A (the ranking percentile) and m@B (1/rank versus percentage of sketch) were used to capture the retrieval performance for the partial sketches.\nPerformance Analysis. The performance of our FLIP-based model on the SLFIR problem is shown in Figure 7 and Table 4. We can observe that: (1) Compared to single-modal sketch representations"}, {"title": "6 Discussion", "content": "We have proposed FaceCaption-15M, a comprehensive facial image-text dataset featuring 15 M image-text pairs with high-quality and diverse natural language descriptions of facial attributes. Our rig-orous statistical analysis and experimentation demonstrated the superior and quality effectiveness of FaceCaption-15M. In future, we have plans to further improve both the image quality and richness of text within FaceCaption-15M. We also plan to explore new tasks and develop new models based on this dataset. We will make the dataset available after undergoing a rigorous legal check at our institution."}, {"title": "Ethical Consideration", "content": "We only annotated generic attributes such as gender, hair color, and wears. Additionally, facial images and captions generated in this work are devoid of bias or certain biometric information, alleviating ethical concerns. We are committed to carefully controlling the application and acquisition procedures for FaceCaption-15M to prevent any potential misuse or abuse.\nWe are aware that the scale of FaceCaption-15M could have dual impacts on society, both advancing facial recognition technology and digital content innovation, and potentially being misused, such as for Deepfake fraud. In terms of fairness, thanks to the comprehensiveness of the LAION dataset and our strict requirements for fairness during the dataset creation process, FaceCaption-15M demonstrates a high level of fairness. We also recognize the limitations in achieving absolute fairness with the current dataset, and the selection of displayed images may exacerbate this impression of unfairness. Additionally, in the subsequent maintenance of the dataset, we will continuously introduce new technologies to improve data annotation and reduce linguistic biases in the dataset."}]}