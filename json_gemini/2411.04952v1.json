{"title": "M3DOCRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding", "authors": ["Jaemin Cho", "Debanjan Mahata", "Ozan \u0130rsoy", "Yujie He", "Mohit Bansal"], "abstract": "Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction tools such as optical character recognition (OCR). However, there are difficulties in applying these methods in real-world scenarios: (a) questions often require information across different pages or documents, where MLMs cannot handle many long documents; (b) documents often have important information in visual elements such as figures, but text extraction tools ignore them. We introduce M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG finds relevant documents and answers questions using a multi-modal retriever and an MLM, so that it can efficiently handle single or many documents while preserving visual information. Since previous DocVQA datasets ask questions in the context of a specific document, we also present M3DOCVQA, a new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. In three benchmarks (M3DOCVQA/MMLongBench-Doc/MP-DocVQA), empirical results show that M3DOCRAG with ColPali and Qwen2-VL 7B achieves superior performance than many strong baselines, including state-of-the-art performance in MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully handle various scenarios, such as when relevant information exists across multiple pages and when answer evidence only exists in images.", "sections": [{"title": "1. Introduction and Background", "content": "Document visual question answering (DocVQA) [14, 31, 40, 42, 57] is a multi-modal task that answers textual questions by interpreting information contained within document images. Existing methods on DocVQA either focus on visual question answering (VQA) on a single-page document (Fig. 1 (a)) or extract text from documents (e.g., via optical character recognition (OCR) [43, 53] or PDF text extraction [18, 49]) and use retrieval-augmented generation (RAG) [35], where a retrieval model finds relevant paragraphs and a language model answers questions given the paragraphs (Fig. 1 (b)). However, there are difficulties in applying these methods in real-world document understanding scenarios: (a) questions often require information across different pages or documents, where existing VQA methods cannot handle many long documents; (b) some documents feature complex visual formats such as tables, charts, and mixed layouts, but text extraction methods such as OCR ignore these nuances, leading to incomplete or inaccurate document interpretations. Accurately and efficiently answering questions across numerous, lengthy documents with intricate layouts would greatly benefit many domains such as finance, healthcare, and law, where document A\u0399 assistants can streamline the daily processing of large volumes of documents, improving productivity and enabling faster, more informed decision-making.\nTo overcome these limitations of existing DocVQA approaches, we introduce M3DOCRAG (Multi-modal Multi-page Multi-Document Retrieval-Augmented Generation; Sec. 2), a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). As illustrated in Fig. 1 (c), the M3DOCRAG framework retrieves relevant document pages using a multi-modal retrieval model, such as ColPali [17], and generates answers to questions from the retrieved pages using a multi-modal language model (MLM), such as Qwen2-VL [59]. M3DOCRAG operates in three stages: In (1) document"}, {"title": "2. M3DOCRAG: A Unified Framework for Multi-modal, Multi-page, Multi-document Understanding", "content": "We propose M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). As illustrated in Fig. 3, M3DOCRAG operates in three stages: (1) encoding document images into visual embeddings (Sec. 2.1), (2) retrieving relevant document pages (Sec. 2.2), and (3) generating answers to questions based on the retrieved pages (Sec. 2.3). Below, we explain the problem definition and the details of each stage.\nProblem definition. We define a corpus of documents as $C = {D_1,D_2,...,D_M}$, where M is the total number of documents, and each document Di consists of a set of pages, Pi, represented as RGB images. From the documents in C, we construct a global set of page images $P = \\bigcup_{i=1}^M P_i = {P_1, P_2, ..., P_N}$, where each pj represents an individual page image, and N is the total number of page images across all documents in C (i.e., $N = \\sum_{i=1}^M |P_i|$). The objective of M3DOCRAG is to accurately answer a given question q using the multi-modal information available in the corpus of documents C. First, we identify $P_K^*$, the top K ($\\ll N$) pages that are most relevant to answering the query q from the global page set P. Then, we obtain the final answer with a question answering model that takes retrieved page images Pr and query q as inputs. The problem of question answering can be categorized into two settings with different document context sizes:\nClosed-domain question answering \u2013 The query q should be answerable from a given single document Di. The retrieval model outputs the top K relevant page images"}, {"title": "2.1. Document Embedding", "content": "In M3DOCRAG, both textual query q and page images P are projected into a shared multi-modal embedding space using ColPali [17]. ColPali is a multi-modal retrieval model based on a late interaction mechanism, which encodes the text and image inputs into unified vector representations and retrieves the top K most relevant images. ColPali adopts both training objective and similarity scoring from Col-BERT [29, 50], which utilizes a shared architecture to encode either textual or visual inputs. In our framework, each page p \u2208 Pi of a document Di is treated as a single image with fixed dimensions (width \u00d7 height).\nFrom an image of a page, we extract a dense visual embedding $E^P \\in \\mathbb{R}^{n^u \\times d}$, where $n^u$ represents the number of visual tokens per page (which remains constant across all pages), and d denotes the embedding dimension (e.g., 128). For a textual query q, we similarly obtain an embedding $E^q \\in \\mathbb{R}^{n^q \\times d}$, where $n^q$ is the number of text tokens.\nFor efficiency, we treat each page of a document independently. This allows us to flatten all pages in the document corpus C into a single page-level embedding tensor: $E^C \\in \\mathbb{R}^{N \\times n^u \\times d}$, where N represents the total number of pages in the entire document corpus, $n^u$ is the number of visual tokens per page, and d is the embedding dimension. M3DOCRAG can flexibly adapt to different retrieval settings, such as a single-page document (N = 1), a single document with multiple pages (e.g. N = 100), and a large corpus of multi-page documents (e.g. N > 1,000)."}, {"title": "2.2. Page Retrieval", "content": "The relevance between the query q and the page p is computed using the MaxSim score s(q,p):\n$s(q,p) = \\frac{1}{n^q} \\sum_{i=1}^{n^q} \\max_{j \\in [n^u]} E^q_{i,.} \\cdot E^p_{j,.}$\nwhere \u00b7 denotes the dot product, and Ei,. \u2208 Rd denotes the i-th row (vector) of the embedding matrix E\u2208 Rn\u00d7d. We then identify $P_K^*$, the top K ($\\ll N$) pages that are most relevant to answering the query q; i.e. we search K pages scoring highest s(q, p). That is,\n$P_K^* = {p_1^*, p_2^*, ...,p_K^*} = argtop-kp\\in P S(q, P)$\nApproximate indexing for open-domain page retrieval. Searching pages over in a large document corpus can be"}, {"title": "2.3. Question Answering", "content": "We run visual question answering by giving the text query q and retrieved page images Pr to a multi-modal language model to obtain the final answer. For this, we employ multi-modal language models (e.g. Qwen2-VL [59]) that consist of a visual encoder Encvis and a language model LM. The visual encoder takes K-retrieved page images Pr as inputs and outputs visual embeddings (different from ColPali encoder's outputs). The language model takes the visual embeddings and text embeddings of query q as inputs and outputs the final answer a in the autoregressive manner:\n$a = LM(Enc_{Vis} (P_r), q).$"}, {"title": "3. M3DOCVQA: A New Benchmark for Open-domain Document Understanding", "content": "We present M3DOCVQA (Multi-modal Multi-page Multi-Document Visual Question Answering), a new open-domain DocVQA benchmark designed to evaluate the ability to answer questions using multi-modal information from a large corpus of documents.\nAs illustrated in Fig. 2, existing DocVQA datasets [31, 40, 42, 57] primarily focus on evaluating question answering within the context of a single document (i.e., closed-domain). These datasets are not well-suited for benchmarking open-domain visual question answering, where relevant information, often in multiple modalities such as text, images, and tables, must be retrieved from multiple documents. This limitation stems from their questions being designed around specific content on certain pages within a single document. In real-world scenarios, users often seek answers that span across multiple documents and modalities, making open-domain settings critical. However, the questions in the existing DocVQA datasets are not applicable in such an open-domain setting. For example, a question from MP-DocVQA, such as \"What was the gross profit in the year 2009?\" assumes that the model already has access to specific information within the document.\nM3DOCVQA challenges models in an open-domain DocVQA setting, where they must navigate a large 'haystack' of multi-modal documents and retrieve relevant"}, {"title": "4. Experiment Setup", "content": "Datasets. We benchmark M3DOCRAG on three PDF document understanding datasets that represent different scenarios: (1) M3DocVQA (Open-domain DocVQA); (2) MMLongBench-Doc [40] (Closed-domain DocVQA); (3) MP-DocVQA [57] (Closed-domain DocVQA). In M3DOCVQA, M3DOCRAG processes over 3,000 PDFs, totaling more than 40,000 pages. For MP-DocVQA, models handle a single PDF with up to 20 pages for each question. For MMLongBench-Doc, models handle a single PDF with up to 120 pages for each question.\nEvaluation Metrics. For M3DocVQA, we follow the evaluation setup of MultimodalQA [54]. For MMLongBench-Doc [40] and MP-DocVQA [57], we follow their official evaluation setups. For M3DOCVQA, we evaluate answer accuracy with exact match (EM) and F1. For MMLongBench-Doc, we extract short answers with GPT40 [46] from the model outputs and report answer accuracy with generalized accuracy (based on a rule-based"}, {"title": "5. Results and Key Findings", "content": "In the following, we describe experiment results of M3DOCRAG and baselines in both open-domain (Sec. 5.1) and closed-domain settings (Sec. 5.2). Next, we provide ablation studies (Sec. 5.3) about different page indexing strategies and different multi-modal LMs and retrieval models. Lastly, we show qualitative examples (Sec. 5.4) where M3DOCRAG can tackle M3DOCVQA questions whose answer source exists in various modalities."}, {"title": "5.1. Open-domain DocVQA", "content": "Multi-modal RAG outperforms text RAG, especially on non-text evidence sources. Table 1 shows the evaluation results on M3DOCVQA. As a model needs to find relevant documents from 3,000+ PDFs for each question, we focus solely on RAG pipelines. We observe that our M3DOCRAG (ColPali + Qwen2-VL 7B) significantly outperforms text RAG (ColBERT v2 + Llama 3.1 8B), across all different evidence modalities / question hops / # pages. The performance gap is especially big when the evidence involves images, underscoring that M3DOCRAG addresses the information loss over non-textual content by text-only pipelines. We also notice that providing more retrieved pages as context generally increases the performance of both text RAG and M3DOCRAG (using the top 4 pages gives higher performance than the top 1 and 2 pages)."}, {"title": "5.2. Closed-domain DocVQA", "content": "Multi-modal RAG boosts long document understanding of MLMs. In MMLongBench-Doc, the models must handle a long PDF document (up to 120 pages) for each ques-"}, {"title": "5.3. Additional analysis", "content": "Different page indexing: speed and accuracy. In Table 4, we analyze the speed and accuracy of ColPali+Qwen2-VL 7B pipeline with different document embedding indexing methods. While the naive indexing with exact search (Flat IP) is slow (21s per query), we find that using approximate indexing such as inverted file [52, 66] (IVFFlat) and product quantization [27] (IVFPQ) can retain most of the accuracy, while making the search significantly faster (< 2s per query). We use"}, {"title": "5.4. Qualitative Examples", "content": "In Fig. 5, Fig. 6, and Fig. 7, we provide qualitative examples of M3DOCRAG (ColPali + Qwen2-VL 7B)'s question answering results on several M3DOCVQA examples. In Fig. 5, the answer information is only visually stored within the game logo ('man is leaning on a motorcycle'), and M3DOCRAG could find the information. In Fig. 6, the question requires multi-hop reasoning across different pages/documents, and M3DOCRAG could combine information from multiple retrieved pages. In Fig. 7, although ColPali did not retrieve the page that contains information about a team whose logo features a bat, Qwen-2 VL leverages its own knowledge 'Valencia CF has a logo featuring a bat', and could provide the final answer. Overall, the qualitative examples showcase that M3DOCRAG can successfully tackle different questions whose answer sources exist in various modalities."}, {"title": "6. Related Work", "content": "Document visual question answering. Mathew et al. [42] proposed document visual question answering (DocVQA) task, where a model extracts information from documents by treating them as images, like in generic visual question answering [1]. Most research on DocVQA focuses on handling a single-page document [22, 23, 30, 34, 41, 42, 55, 58, 63], and it has been now a common practice to include the single-page DocVQA [42] as a part of the image understanding evaluation suite among recent MLMs [7, 12, 20, 32, 46, 59]. Several recent works study applying MLMs for DocVQA on multi-page documents [31, 40, 57]. However, all previous works on DocVQA have focused on handling questions in the context of a specific document, such as \"What was the gross profit in the year 2009?\" [14, 40, 42, 57]. While this is probably due to the limited context length of the backbone multi-modal LMs, this does not reflect real-world scenarios, where users often ask questions that require information across different pages/documents. We address the limitation and propose M3DocRAG framework and M3DOCVQA dataset for effective, efficient, and flexible document understanding under various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.)."}, {"title": "7. Conclusion", "content": "We introduce M3DOCRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). In M3DocRAG, a multi-modal retrieval model identifies relevant pages from single or multiple documents, which are then processed by a multimodal language model, where all documents are represented as pixels. Next, we introduce M3DOCVQA, the first benchmark that evaluates open-domain multi-modal document understanding capabilities. M3DOCVQA consists of 2,000+ questions and 3,000+ PDF documents, and the questions need to be answered with various modalities such as images, text, and tables. Our experiments in three datasets (M3DOCVQA, MP-DocVQA, and MMLongBench-Doc) demonstrate significant advantages of M3DOCRAG over existing methods, including the state-of-the-art performance in MP-DocVQA. We also provide analysis comparing different indexing strategies, multi-modal LMs, and multi-modal retrieval models. Finally, we show qualitative examples where M3DOCRAG can successfully tackle different questions whose answer sources exist in various modalities. We hope that our work encourages future advancements in multi-modal frameworks for document understanding, paving the way for more robust, scalable, and practical solutions in real-world applications."}, {"title": "Ethical Considerations", "content": "Limitations. Since our multimodal retrieval models and multimodal LMs were trained with English-heavy datasets, they might not understand prompts or documents written in non-English. While our M3DOCRAG framework can benefit many document understanding applications, the model components could present false or biased information. Thus, the framework should be used with human supervision in real-world applications. Note that M3DOCRAG is designed with flexibility so that users can update or replace components as more accurate solutions for each element of the framework become available in the future.\nData collection. We do not involve human subjects during data collection. We do not claim ownership/rights of the Wikipedia documents, and we attribute the source Wikipedia document URLs to all pages."}]}