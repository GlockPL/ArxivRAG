{"title": "Investigating learning-independent abstract reasoning in artificial neural networks", "authors": ["Tomer Barak", "Yonatan Loewenstein"], "abstract": "Humans are capable of solving complex abstract reasoning tests. Whether this ability reflects a learning-independent inference mechanism applicable to any novel unlearned problem or whether it is a manifestation of extensive training throughout life is an open question. Addressing this question in humans is challenging because it is impossible to control their prior training. However, assuming a similarity between the cognitive processing of Artificial Neural Networks (ANNs) and humans, the extent to which training is required for ANNs' abstract reasoning is informative about this question in humans. Previous studies demonstrated that ANNs can solve abstract reasoning tests. However, this success required extensive training. In this study, we examined the learning-independent abstract reasoning of ANNs. Specifically, we evaluated their performance without any pretraining, with the ANNs' weights being randomly-initialized, and only change in the process of problem solving. We found that naive ANN models can solve non-trivial visual reasoning tests, similar to those used to evaluate human learning-independent reasoning. We further studied the mechanisms that support this ability. Our results suggest the possibility of learning-independent abstract reasoning that does not require extensive training.", "sections": [{"title": "Introduction", "content": "Recent developments in the field of machine learning are revolutionizing the cognitive sciences because, for the first time, we have access to non-human agents capable of performing complex cognitive functions\u00b9. These machines are constructed of deep Artificial Neural Networks (ANNs) trained using stochastic gradient descent methods on large datasets\u00b2. Whether or not these machines exhibit \u201cintelligence\u201d in a similar way to humans' intelligence is still a matter of debate3\u20135. What is clear, however, is that the training of humans and machines differs on several levels. One of the prominent differences is the size of the training set. For example, current Large Language Models (LLM) pretrain on datasets that are orders of magnitudes larger than those children are exposed to, a difference that echoes Chomsky's arguments several decades ago about \u201cpoverty of the stimulus\u201d in language learning7.\nThe topic of this paper is abstract reasoning, sometimes referred to as \u201cfluid intelligence\u201d. Abstract reasoning is, broadly speaking, the ability to solve complex problems by identifying regularities and relations in the problem being solved and utilizing them for deducing the solution9,10. It is often studied using intelligence tests that comprise word analogy tests (e.g., infer that the relationship between \u201ccow\u201d and \u201cmilk\" is the same as between \u201cchicken\u201d and \u201cegg", "abstract reasoning\", is used for all these various tasks. Moreover, this general \u201cabstract-reasoning\" ability does not seem to improve by practice \u2013 training on one task does not generalize to other tasks15.\nStrong support for the hypothesis that abstract reasoning requires extensive training comes from machine learning. Recent developments in this field demonstrate that extensively-trained deep networks can successfully solve many types of intelligence tests 16-21. The performance of these networks relies on extensive prior training, suggesting that their success in solving abstract reasoning benchmarks might be related to exposure to similar questions (an issue called contamination)3,4, thus memory- dependent. Indeed, changes in how problems are phrased that seem minor to humans and do not affect their performance can render them unsolvable to the networks5.\nTrivially, solving some forms of intelligence tests requires some form of training. A human who does not speak English or has never heard about the relationship between \u201ccow": "nd \u201cmilk", "chicken": "o \u201cegg\u201d. The extent to which such training is required for solving visual reasoning tests is less clear. Visual reasoning tests utilize abstract shapes like squares and triangles that are presumably more general than language. However, even this is unclear because the everyday visual experience may be a form of extensive training22.\nTo contribute to this debate, here we study the extent to which ANN models can solve visual abstract reasoning tests without prior training. To that goal, we constructed visual abstract reasoning tests that require identifying relations in a sequence of stimuli, a sub-task shared among intelligence tests8\u201310,23. We studied the ability of Relation Networks (RNs)24 to solve them, as members of this class of models were shown to be capable of identifying abstract relations and solving intelligence tests after extensive training18,25. Furthermore, RNs were shown to successfully solve word-analogy problems without specific training on those problems, relying on general pretrained relations26,27. In contrast to these previous studies, our focus was the ability of \"naive\" RNs, who were not exposed to any pre-training, to identify relations in visual reasoning tests and use them for solving the tests.\nThe paper is constructed as follows: We introduce the problems and the model that we tested. Then, we present its performance and dissect the underlying requirements for the model's successful problem-solving. Finally, we add training to the model and discuss a similarity between the effect of the training schedule on performance and a phenomenon known as the interleaving effect in the cognitive sciences."}, {"title": "Results", "content": "Sequential visual reasoning tests\nWe constructed a set of artificial problems in which the task is to evaluate the consistency of an input with a sequence of its preceding inputs (Fig. 1). Each problem comprises 5 gray-scale images and 4 optional-choice images. The images, 224 \u00d7 224 pixels each, are composed of identical abstract objects and differ along several dimensions: the shape of the objects, their size, their color, their number, and their arrangement. By construction, one of these features changes predictably over the 5 images. Formally, an image is characterized by a low-dimensional vector of features, fj, where f denotes the value of feature i in image j. An image xj is constructed according to its characterizing features by a generative function xj = G (fj). One of the features fP changes predictably along the sequence according to a simple deterministic rule $f_{i+1} = U(f)$ while the other features are either constant over the images or change randomly (values are i.i.d). Considering the optional-choice images, the predictable rule is followed in only one of them, and the task is to select this image. The other features are either constant in all 9 images (5 of sequence and 4 of optional choices) or change randomly (see Methods). We refer to a problem's predictably changing feature as the problem's Predictive Feature (PF) and to the randomly changing features as distractors. Intuitively, the number of distractors is a measure of a problem's difficulty.\nEach image is characterized by a small number of features, of which one changes predictably. The challenge is to simultaneously identify the features and the rule that relates the features of the different images. Relation Networks24 do exactly that. Taking a set of stimuli, they learn two functions: an encoder function $Z\u00f3 (x)$ that extracts relevant feature(s) from the stimulus, that is, a low-dimensional representation of the stimuli x and a relation module $R_\theta (Z_\\phi(x_i), Z_\\phi(x_j))$ that characterizes the relationship between the features of pairs of stimuli xi and xj. In practice, the encoder and the relation modules are functions (typically networks) whose parameters (\u03c6 and \u03b8, respectively) are learned from examples. It should be noted that, to some extent, the complexities of the encoder and the relation module are interchangeable. The reason is that a sufficiently-complex relation module can incorporate the feature extraction. Similarly, a sufficiently complex encoder can operate on the extracted features as to simplify the relation between them. For example, any monotonous relation between the features is also a linear relation between a (nonlinear) transformation of the features.\nPrevious studies have shown that with sufficient examples, relational networks can learn to extract the relevant features and their relations at a level sufficient for solving intelligence tests18,25,27. The challenge here is to perform a similar task without any pre-training. To do so, we defined the following loss function on a sequences of 5 images:\n$\\mathcal{L}(\\theta, \\phi) = \\frac{1}{4} \\sum_{i=1}^4 [R_\\theta (Z_\\phi (x_i), Z_\\phi(x_{i+1}))]$(1)\n$x_i$ is the $i^{th}$ image in the sequence, the encoder $Z_\\phi : \\mathbb{R}^{224 \\times 224} \\rightarrow \\mathbb{R}^n$ is a function that takes 224 \u00d7 224 pixel images to an n dimensional latent space and the relation module $R_\\theta : \\mathbb{R}^{2n} \\rightarrow \\mathbb{R}^+$ takes two consecutive latent variables, each of dimension n, and outputs a positive 1D relation score.\nThis loss is minimized for a relation function $R_\\theta (Z_\\phi (x_i), Z_\\phi (x_j))$ that outputs a minimal relation score for consecutive sequence images (j = i + 1), requiring the identification of the regularity that characterizes these consecutive images. We updated the networks' weights \u03b8 and \u03c6 with 10 optimization steps over the loss $\\mathcal{L}(\\theta, \\phi)$ using the RMSprop optimizer28 (learning rate of 1e\u20135, the rest of the parameters are set to PyTorch29 default). Eventually, after optimization, we evaluated the consistency of each choice image with the sequence based on their relation value R when they were placed as the sixth sequence image $R_\theta (Z_\\phi (x_5), Z_\\phi (\\cdot ))$ and selected the choice image with the lowest relation value as the answer.\nTo clarify, in these settings, the model does not need to learn the features and their relation in the generative sense to solve a test successfully. Instead, it is enough to find image representations and rules that are sufficiently correlated with a problem's predictive feature for selecting the most consistent image out of four options."}, {"title": "Vanilla model performance", "content": "The success of the model would depend on the specific choice of R and Z (their network structure), as they can be inductively biased towards certain types of features and rules. In our vanilla model, the encoder Z was a small CNN from input space to a 1D latent neuron, composed of 3 convolutional layers followed by 5 fully-connected (FC) layers with a single output neuron (see Methods and Supplementary Information Fig. S1). For the relation module, we used a simple function that asserts a linearly changing relation between the latent variables,\n$R_\\theta (Z_\\phi (x_i), Z_\\phi (x_j)) = (Z_\\phi (x_i) \u2013 Z_\\phi (x_j) + \\Theta)^2$(2)\nwhere \u0398 is a trainable constant that does not depend on Z.\nWe evaluated the performance of the vanilla model on the different tests, in which the predictive feature's values increased linearly, and found that it performed substantially better than chance (0.25) in almost all tasks and all levels of difficulty (Fig. 2). Without distractors, its performance on some tasks was close to perfect. We also found that performance decreased with the number of distractors, verifying that the number of distractors is a good measure of the task's difficulty. All these results were obtained using networks that we randomly initialized before each problem, thus demonstrating that learning-independent abstract reasoning is possible, at least to some extent. Averages over all conditions, the model's performance was 0.58\u00b10.01. From this point in the paper, we use this global performance measure for comparisons (see Methods; complete performance results are in the Supplementary Information)."}, {"title": "Determinants for success", "content": "Our model consists of two main components: the encoder Z and the relation module R. The parameters of both were changed in the direction of minimizing the loss function on the images of each problem, a process that we will refer to as optimization. To study the relative contribution of these components to problem-solving, we studied the model's performance when the parameters of only one of these components, either Z or R, were optimized. We found that optimizing the encoder was essential: when the parameters of the encoder Z remained unchanged, the model's performance, averaged over all conditions, was close to the chance level, 0.30\u00b10.01 (Fig. S2). By contrast, using random parameters for the relation module R had no significant effect on performance, resulting in an average performance of 0.58\u00b10.01 (Fig. S3), which is not significantly different from that of the vanilla model. These results motivated us further to study the role of the encoder in the task.\nThe encoder\nThe encoder is an 8-layer network with 3 convolutional layers followed by 5 Fully-Connected (FC) layers. Removing the convolutional layers and connecting the FC layers directly to the inputs impaired the average performance of the model, reducing its performance to 0.48\u00b10.01 (Fig. S4), indicating that the convolutional layers are important for performance. In the vanilla model, the parameters of both the convolutional layers and the FC layers are optimized in the direction of minimizing the loss function. However, it turns out that the optimization of the parameters of the convolutional layers does not contribute to the performance. The average performance when the weights of the convolutional layers remained random, 0.57\u00b10.01, was not significantly different than that of the vanilla model (Fig. S5). By contrast, keeping the FC network weights fixed at their randomly-initialized values during problem-solving was detrimental to the performance (0.34\u00b10.01, Fig. S6).\nSo far, we saw that freezing either the weights of the convolutional layers or the relation module at their initial random values does not impair performance. This insensitivity does not change when both are frozen (0.58\u00b10.01, Fig. S7).\nWe conclude that the convolutional layers effectively operate as frozen feature extractors (features in the more general sense \u2013 not necessarily the features used for constructing the images) while the parameters of the FC layers are optimized to solve the task.\nTo test how the FC layers contribute to this task, we note that the task could be perfectly solved if the encoder could learn to identify the inverse generative function of the problem images $G^{-1}(x)$ and use it to extract the underlying predictive feature $f^P$ and its rule U ($f^P$). If this is done, we expect the optimization steps to increase the correlation of the encoder's output neuron with the predictive feature (but not with the distracting features). We tested this hypothesis in the vanilla model for all the predictive features. Indeed, the absolute Pearson correlation of the output neuron with the predictive feature (see Methods) increases with optimization steps, as depicted in Fig. 3a (black).\nTo better understand how such correlations emerge in the FC network, we also computed the absolute Pearson correlations of these features with the activities of all other neurons in the FC network (see Methods. Comprehensive results in Supplementary Fig. S8). These correlations, averaged over all neurons in a layer, are depicted in Fig. 3a. We found that the correlations with the predictive feature increase with the layer depth.\nThe higher the correlation of the output neuron with the predictive feature, the easier it is for the relation module to identify the regularity in the sequence of images. Along the same lines, we also expected the optimization process to decrease the correlation of the encoder output neuron with the other irrelevant features. This, however, is not the case. Considering the same features in problems in which they are not predictive features (either constant or distracting), we found that the correlation of the output neuron with these features also increases on average in the optimization process, albeit to a lesser extent (Fig. 3b). Considering the correlations of these features with neurons in the hidden layers of the encoder, we found that the correlations with these irrelevant features also increased with the layer depth."}, {"title": "The relation module", "content": "By construction, the vanilla model's relation module is simple, implicitly assuming that the features change linearly. Therefore, one may naively expect that identifying a non-linear change in the predictive feature will be more challenging. However, any monotonically changing rule can be mapped into a linearly changing rule with a sufficiently complex encoder. We, therefore, tested our vanilla model in problems in which the change in the feature was non-linear (Fig. 6). We found that when the relevant feature was size, the performance for an exponential increase or a square root increase of this feature was comparable to that of a linear increase (Linear: 0.53\u00b10.01; Exp: 0.54\u00b10.01; Sqrt: 0.52\u00b10.01. Fig. S9-10 right). Similarly, when the relevant feature was color, the model achieved comparable performance to the linear case, although with higher variability: performance was better for an exponential increase and worse for a square root increase (Linear: 0.70\u00b10.01; Exp: 0.75\u00b10.01; Sqrt: 0.64\u00b10.01. Fig. S9-10 left). These results suggest that a relation module that assumes linear relationships can capture general monotonic relationships, substantially downsizing the hypothesis space of possible relationships. It would, however, be more difficult for the model to deal with non-monotonic rules. Indeed, when tested in problems where the predictive feature alternated between two of its values, the vanilla model performance was at a chance level (0.24\u00b10.01. Fig. S11).\nIn the vanilla model, the relation module is simple and general, and the encoder that finds appropriate image representations carries most of the \"computational load\". However, we expect the complexity of the encoder and the complexity of the relation module to be interchangeable, to some extent. Thus, we can move some of the computational load from the encoder to the relation module without changing the performance. To test this, we simplified the encoder by removing the fully-connected layers, leaving only the convolutional layers, and complicated the relation module, by making it a more complex and expressive,\n$R_\\theta (Z_\\phi (x_i), Z_\\phi (x_j)) = H_\\Theta (Z_{conv} (x_i) \\oplus Z_{conv}(x_j))$(3)\nwhere the relation module $H_\u0398$ takes a concatenation of the convolutional layers' outputs to a single output neuron and has a network architecture similar to the vanilla model's encoder's FC layers (with twice the input dimension). Rather than optimizing both the encoder and the relation module, as in the vanilla model, we optimized only the relation module. This version of the model achieved an average accuracy of 0.59\u00b10.01 (Fig. S12) comparable to that of the vanilla model, demonstrating that it is possible to move the computational load from the encoder to the relation module without paying in performance.\nTo conclude this section, we demonstrated two ways for carrying the computational load: either the encoder carries most of the load by extracting the relevant feature such that a simple linear relation module is sufficient for capturing multiple monotonic"}, {"title": "Knowledge crystallization", "content": "Our focus so far was the ability of the networks to solve problems without any training, that is, without any accumulation of information between problems. Embedded in our model, however, is the ability to accumulate knowledge. This is because problem-solving in our model is achieved through changes in synaptic weights. This motivated us to study how solving multiple problems affects performance. In humans, the improvement of performance due to the accumulation of knowledge by training is referred to as knowledge crystallization30.\nWe first studied the extent to which the model can improve its performance on one predictive feature by practicing on that feature. Notably, in these practice sessions there was no feedback about the correct answer (in fact, the networks were exposed only to the sequences of 5 images and not to the possible answers). We found that networks that solved 1,000 easy problems with a specific predictive feature (without resetting the weights between problems) improved their accuracy on problems with that same predictive feature to 0.74\u00b10.01 (averaged over the three predictive features, Fig. 7), a substantial improvement from the average accuracy without prior training (0.58\u00b10.01). Notably, the improvement was not uniform across features. While performance on Number and Size substantially improved (Size: from 0.53\u00b10.01 to 0.69\u00b10.01, Number: from 0.50\u00b10.01 to 0.84\u00b10.01), training on Color did not affect performance in Color problems (0.70\u00b10.01 in both conditions).\nInterestingly, freezing the weights of the relation module resulted in an even better performance (0.80\u00b10.01, Fig. S13). On the other hand, the improvement was only modest when the convolutional layers' weights were frozen (0.65\u00b10.01, Fig. S14).\nThis improvement in performance is analogous to knowledge crystallization. However, will training on one predictive feature improve performance when other predictive features are used? Recall that in humans, training on one task does not generalize to other tasks15. Similarly, we found that while training on one predictive feature improved performance in problems with that same predictive feature, it was detrimental when the networks were tested on problems with a different predictive feature (Fig. S15).\nWill training on several predictive features improve network performance on those several trained features? To address this question, we focused on the two predictive features that exhibited improvement with training, Number and Size. We used block training and tested performance on the most difficult problems of both types. Considering the first block of training, extensively training the network with one predictive feature improves performance on that feature but not on the other feature (Fig. S16). Considering the second block, when this network is trained on the other predictive feature, the network quickly improves on that feature, but improvement on the first predictive feature quickly diminishes. when the network trains on the other feature (Fig. S16a, b). Trying to resolve this by interleaving these two predictive-feature problems in short blocks of 5 problems does not change the result and the network seems unable to simultaneously improve on two predictive features (Fig. S16c, d).\nTo minimize conflict between the two features, we trained and tested the network in problems in which the competing non-predictive feature (Size or Number) was set at the same constant value (see Methods). We found that when training was done in two long blocks, the network only improved on the trained feature (Fig. 8a, b). By contrast, when training was done by interleaving many short blocks of 5 problems, the network improved in both features (Fig. 8c, d)."}, {"title": "Discussion", "content": "The main result of this paper is that abstract reasoning is possible even in naive (and random) ANNs. This is a result that has implications both for the cognitive sciences and for machine learning. In the cognitive sciences, whether abstract reasoning is a learning-independent capability or whether it requires extensive training has long been a matter of contention7,13. It is difficult to address this question in humans, because our interaction with the environment, both in development and in evolutionary time scales, is a form of extensive training. Our work shows that some non-trivial aspects of abstract reasoning are possible (at least in ANNs) without prior learning. In machine learning, much progress in recent years was based on using ever larger datasets to train ever more complicated ANNs35,36. Our work shows that ANNs are computationally powerful even in the absence of large datasets.\nTraditionally, abstract reasoning in the brain was considered a symbolic computation: a digital computation that is very different from the analog computation that ANNs perform37,38. More recently, however, it was demonstrated that computations that were attributed to complex symbolic manipulations could be achieved by extensively-trained ANNs39,40. This ability is particularly pronounced when considering LLMs, that seem capable of abstract reasoning21. Criticizing abstract reasoning in ANNs, it has been argued that the success of these networks reflects effective memory-retrieval rather than \"true\" abstract reasoning3,4. Our contribution is that we show that such networks can achieve what looks like symbolic abstract reasoning without any training and, hence, without any memory recall.\nFundamental to the ability of our network to perform the task is the encoder, which extracts features that co-vary with the images. The random convolutional layers were found to be instrumental in extracting features that are correlated with relevant latent features. Highlighting the features that are relevant for the particular sequence of images of a particular problem is done in the deep layers of the encoder, together with the relation module.\nThese results resonate with human-brain studies. In humans, the primary regions in the visual cortex are considered generic feature extractors that are sensitive to low-level features of the stimulus, such as orientation and direction. It seems unlikely that this low-level feature extraction changes with every problem presented to a human participant (they may, however, change with extensive training41). Imaging studies have revealed that higher cortical regions, such as the lateral prefrontal cortex, play an important role in abstract reasoning42 and rule learning43. We hypothesize that these higher cortical regions perform the computation of highlighting the relevant feature (Fully-connected layers of the encoder) and identifying its regularity (the relation module).\nNotably, the computations performed by the fully-connected layers of the encoder and the relation module are somewhat interchangeable. This is because either of those networks could carry the main computational load. This suggests an interesting approach for finding relations by implementing a few very simple and general (applicable to different problems) relation modules, transferring a significant computational load of finding appropriate input representations to the encoder. Given the interchangeability of complexity in the fully-connected layers of the encoder and the relational module, the separation between the encoder and relational module in the brain may be somewhat artificial. A clear border between an encoder and a relational module may not exist.\nOur framework naturally generalizes to explaining knowledge acquisition through problem-solving (in an unsupervised way, without feedback). We found that training with many short interleaved blocks was substantially more effective than training with two long blocks. This resembles a similar observation in the cognitive sciences known as the interleaving effect33,34. In the cognitive sciences, two competing theories have been used to explain this effect. In one, the interleaving effect is due to enhanced problem identification and feature distinction required when solving two types of problems in close proximity44. The second theory explains the interleaving effect by proposing that with interleaved training, the brain is continually engaged at retrieving the responses from memory - a process that enhances the consolidation of those memories45,46. In contrast to these theories, our model has no explicit problem-identification or memory-consolidation mechanisms implemented. Rather, it is another manifestation of the well-known catastrophic forgetting phenomenon in machine learning31,32.\nWhile explaining some aspects of abstract reasoning, our model does not encapsulate other facets of abstract reasoning observed in humans. Specifically, our model does not incorporate working memory, limiting the regularities it can identify. It also does not explicitly perform the mapping computation required for analogical reasoning27. Additionally, the model cannot solve a problem by breaking it into its sub-components47. For example, to solve a Raven Progression Matrix, humans use the strategy of identifying common regularities in the rows and the columns. Our model was constructed only to find a regularity in a sequence. As with most ANN models, the model cannot interpret its choices. Finally, it lacks the ability to generate new images that follow the regularity it identifies. These limitations present opportunities for future research and suggest areas for improvement.\nRegarding human abstract reasoning, the recent success of extensively trained deep learning models in solving abstract reasoning problems supports the hypothesis that human abstract reasoning requires extensive training. Our work shows that similar deep learning models can also exhibit some form of learning-independent abstract reasoning. As deep networks are currently considered leading models for human cognition, our work indicates that learning-independent abstract reasoning is also possible in humans."}, {"title": "Methods", "content": "The code for this paper was written using PyTorch29. The code that generates test problems and applies the model to solve them is available at https://github.com/Tomer-Barak/learning-independent_abstract_reasoning.\nNetwork architectures\nThe encoder (Z(x)) consisted of two main components (see Supplementary Fig. S1): three convolutional layers (kernel sizes: 2, 2, and 3; strides: all 1; padding: all 1) and five Fully-Connected (FC) layers (number of neurons: 200, 100, 50, 10, 1). Three ReLU activation functions were applied after each convolutional layer, and two Max-Pool layers (kernels: 4 and 6, strides: all 1) were applied after the second and third convolutional (+ReLU) layers. Four tanh activation functions were applied after each FC layer, except the last one, which had no activation function and remained a linear transformation.\nThe vanilla model's relation module consisted of a single parameter as written in equation (2). The more complex relation module written in equation (3) was implemented by a five-layer fully-connected network (number of neurons: 200, 100, 50, 10, 1). Four tanh activation functions were applied after each of this relation module's layers, except the last one, which had no activation function and remained a linear transformation."}, {"title": "Sequential visual reasoning tests", "content": "Each image of the tests was constructed using the following five features: the number of objects in an image (possible values: 1 to 9), their shade (6 linearly distributed grayscale values), their shape (circle, triangle, square, star, hexagon), their size (6 linearly distributed values for the shapes' enclosing circle circumference), and arrangement (a vector of grid positions that was used to place the shapes in order).\nAs written in the paper, the choice images' non-predictive features followed the same rules they abide by in the sequence (constant or randomly changing). The predictive feature followed the sequence rule only in the correct choice and was randomly chosen from the remaining feature values in the incorrect choices. We restricted the possibility of having a repeated choice image in the same problem. If a repeated image was generated by chance, we generated another one to replace it."}, {"title": "Average accuracies", "content": "In the paper, we report networks' average accuracies/performance in different experiments. For example, the vanilla model's average accuracy was 0.58\u00b10.01. These numbers were obtained (except in the knowledge crystallization section, discussed below) in the following way. For each predictive feature relevant to the experiment, we considered all its test conditions of different difficulties. There were five features, one predictive and the other four either constant or distracting, amounting to 24 = 16 test conditions per predictive feature. We tested randomly initialized networks in 500 problems in each test condition (each problem with a different initialized network) and obtained their success rate in that test condition. To estimate the errors, we calculated the standard error of the mean of a sample of Binomial random variables based on the success rate and the number of samples (500). To obtain the average accuracy of that predictive feature, we averaged the success rates over all test conditions and propagated the errors accordingly. For the total average accuracy, we averaged the accuracies of the experiment's relevant predictive features and propagated the errors.\nIn the knowledge crystallization section, the average accuracies (e.g., Fig. 7) were obtained by training 50 networks in each predictive feature on 1000 easy problems (without distractors) of that predictive feature. After training, the networks solved 10 test problems in each of the 16 test conditions of a given predictive feature (results for networks that trained on one predictive feature and tested on another are shown in Fig. S15). We then calculated the average success rate of the networks in each test condition, using the standard error of the mean of Binomial random variables as errors. For the total average accuracy, we averaged across the different test conditions and all the relevant predictive features of the experiment, propagating the errors.\nIn the blocks versus interleaving experiments (Fig. 8, S16-S18), we trained 20 networks (in each of the figure panels) on 30,000 easy problems of two training predictive features. The training was either in two big blocks or interleaved into small five-problem blocks. After every 1875 training problems, we tested the networks on 25 difficult problems of the two predictive features. In Fig. S16, the easy and difficult problems were such that there were no distractors or all of the distractors. In Fig. 8, both the easy and difficult problems of Size had a fixed value of Number (5 shapes). Accordingly, the easy and difficult problems of Number had a fixed value of Size (the 5th size value)."}, {"title": "Correlations", "content": "To calculate an encoder neurons' correlations with a particular feature (color, number, or size; Fig. 3 and S9), we generated artificial testing examples corresponding to that feature: 20 images for each of the feature's six possible values (120 examples overall) where the rest of the features' values were drawn randomly. We applied these examples to the network and recorded its neurons' activity. Based on the neurons' activity, we calculated each of the neurons' correlation with the feature values. Finally, we averaged the correlations across the layers.\nTo generate Fig. 3a, we average the correlations of networks that solved the three possible predictive features with the predictive features they solved. For Fig. 3b, we calculated the correlations with the other (non-predictive) features. In both figures, we averaged over the 16 test conditions of a given predictive feature, 50 problems per test condition, each problem solved by a different naive network. Thus, overall, the results are average over 3 \u00d7 50 \u00d7 6 = 900 networks. The complete results of these simulations, before averaging over networks and test conditions, are shown in Fig. S8. To calculate the errors of the correlations, we estimated the standard error of the mean of the average correlations of the different networks. We propagated these errors when we averaged the correlations across test conditions and different predictive features.\nTo calculate the correlations (or covariance) of neurons with the sequence (Fig. 5b-d), we applied the sequence images to the network and recorded its neural activity. Then, we calculated for each neuron the correlation (or covariance) between its activity and the sequence order indices of the images. In Fig. 5b-c, we calculated the covariance with the sequence of neurons taken from the output of the encoder's 3rd convolutional layer, while in Fig. 5d, we calculated correlations with the sequence of the FC layers' neurons. The errors in the latter case were calculated like those of the feature correlations."}, {"title": "Figure 4", "content": "To obtain the values of this plot, we considered test conditions with one distracting feature that was either Color, Number, or Size (as those are the features for which we were able to calculate networks' correlations). In total, there were 6 such test conditions"}, {"title": "The relation module", "content": "$\\mathcal{R}_\\theta(\\mathbf{Z}_\\phi(x_i), \\mathbf{Z}_\\phi(x_j)) = H_\\Theta(\\mathbf{Z}_{conv}(x_i) \\oplus \\mathbf{Z}_{conv}(x_j))$"}]}