{"title": "AnyMatch \u2013 Efficient Zero-Shot Entity Matching with a Small Language Model", "authors": ["Zeyu Zhang", "Iacer Calixto", "Paul Groth", "Sebastian Schelter"], "abstract": "Entity matching (EM) is the problem of determining whether two records refer to same real-world entity, which is crucial in data integration, e.g., for product catalogs or address databases. A major drawback of many EM approaches is their dependence on labelled examples. We thus focus on the challenging setting of zero-shot entity matching where no labelled examples are available for an unseen target dataset. Recently, large language models (LLMs) have shown promising results for zero-shot EM, but their low throughput and high deployment cost limit their applicability and scalability. We revisit the zero-shot EM problem with ANYMATCH, a small language model fine-tuned in a transfer learning setup. We propose several novel data selection techniques to generate fine-tuning data for our model, e.g., by selecting difficult pairs to match via an AutoML filter, by generating additional attribute-level examples, and by controlling label imbalance in the data. We conduct an extensive evaluation of the prediction quality and deployment cost of our model, in a comparison to thirteen baselines on nine benchmark datasets. We find that ANYMATCH provides competitive prediction quality despite its small parameter size: it achieves the second-highest F1 score overall, and outperforms several other approaches that employ models with hundreds of billions of parameters. Furthermore, our approach exhibits major cost benefits: the average prediction quality of ANYMATCH is within 4.4% of the state-of-the-art method MatchGPT with the proprietary trillion-parameter model GPT-4, yet ANYMATCH requires four orders of magnitude less parameters and incurs a 3,899 times lower inference cost (in dollars per 1,000 tokens).", "sections": [{"title": "1 INTRODUCTION", "content": "Entity matching (EM), often also referred to as deduplication or entity resolution, is the problem of determining whether two records refer to the same real-world entity. EM is a well-studied problem [6, 8, 12, 19, 21, 23, 35] and of high practical importance in data integration [2, 13, 14, 32, 42].\nZero-shot entity matching. A typical restriction in entity matching scenarios is the dependence on labelled examples. A less restrictive yet more challenging setting is zero-shot entity matching [37], where the matcher has to work with an unseen target dataset for which no labelled examples are available. Zero-shot matchers are essential to improve data integration services in the cloud (e.g., AWS Glue [29]), which provide automated integration capabilities on enterprise data lakes [34], but currently require end users to manually label examples for entity matching [30]. Additionally, zero-shot matchers can be applied for duplicate detection [15] as part of data cleaning in machine learning pipelines [1, 18], and are also valuable as a primitive for entity alignment in tasks such as table reclamation [10]. All of these use cases focus on ingesting and processing large amounts of heterogeneous data from various data sources, such as Excel and CSV files in data lakes, log files or web crawls. In such settings, schema and type information may often be missing or unreliable. Therefore, we extend the recently introduced zero-shot setting [37] with the additional restriction that no column name or type information is available for the unseen target dataset (see Section 2 for details), and argue that improving the performance and applicability of zero-shot matchers in this setting is of high practical relevance.\nThe high cost of LLM-based matchers. Recently, the promising capabilities of large language models (LLMs) [11] have led to a resurgence of research on EM [17, 22, 26, 41]. Approaches such as MatchGPT [26] or TableGPT [17] show promising results for zero-shot entity matching by prompting LLMs [22]. However, these approaches rely on extremely large proprietary models with hundreds of billions or even trillions of parameters [36], which require expensive accelerator hardware for deployment. As a result, these approaches incur a hefty cost at a low throughput during inference. The latest commercial LLMs often come with a \u201cthroughput of less than 1 KB per second\u201d [20] at a high cost imposed by their pay-per-token model, which results in prices of \u201c5 USD for processing just 5MB of data\u201d [20]. LLM-based entity matchers inherit this high computational cost, which severely limits their scalability and applicability. An indication of this is the fact that even relatively"}, {"title": "2 PROBLEM STATEMENT", "content": "We introduce entity matching and detail the restrictions for our tackled zero-shot setting. Furthermore, we discuss a set of use cases for zero-shot entity matching.\nEntity matching. The entity matching problem is to predict whether the pair of records $(r_l, r_r)$ with $r_l \\in R_{left}$ and $r_r \\in R_{right}$ refers to the same real-world entity or not. $R_{left}$ and $R_{right}$ denote two input relations with k aligned attributes $A = \\{a_1, ..., a_k\\}$.\nEntity matching is often modelled as a binary classification problem with a labelled training set $D_{train} \\subset R_{left} \\times R_{right} \\times \\{0, 1\\}$. State-of-the-art approaches [19] featurise the example pairs based on their attribute names $a_1,..., a_k$ and the aligned attribute values $v_{l_1} = r_l[a_1], ..., v_{lk} = r_l[a_k], v_{r_1} = r_r[a_1], ..., v_{rk} = r_r[a_k]$. Furthermore, the attribute values may be augmented with additional data, e.g., domain information. Note that entity matching can also be used for the deduplication of a single relation, when we use pairs of records from this single relation as input. Furthermore, real-world entity matching systems typically first apply a blocking function to the set $R_l \\times R_r$ to form smaller candidate sets as input to the matcher. We focus on the matcher itself and present a general approach which can be easily plugged into existing matching systems and applied to blocked candidates sets.\nZero-shot entity matching. In contrast to classical entity matching, we tackle a more challenging setting, referred to as zero-shot entity matching [37]. In particular, we tackle the entity matching problem under the following restrictions:\nRestriction 1 - Unseen target data: A zero-shot matcher will not have access to labelled example pairs for the target relations $R_{left}$ and $R_{right}$, which means there is no training set available for the unseen target data $D_{target}$.\nRestriction 2 - Lack of type information: There is no column name or column type information accessible for the target relations $R_{left}$ and $R_{right}$. A zero-shot matcher can only enumerate the attribute values $r[a_1], ..., r[a_k]$ of a record r from the target relations in a string representation.\nPrevious research like ZeroER [37] already tackles zero-shot entity matching based on Restriction 1 (no training data for the target relations), but still needs column type information to select appropriate similarity functions (violating Restriction 2).\nUse cases. We argue that our proposed zero-shot EM setup is crucial in scenarios where a high level of automation required, and where it is unlikely or impractical to force a domain expert to manually label training data. Furthermore, schema and type information may often be missing or unreliable in these scenarios. Examples for such use cases include data integration services in the cloud, such as AWS Glue [29], which provide automated integration capabilities on enterprise data lakes. Currently, such services require end users to manually label examples for entity matching [30]. They match our restrictions well as they need to be able to ingest and automatically process heterogeneous data from various data sources"}, {"title": "3 APPROACH", "content": "We give an overview of ANYMATCH in Section 3.1. Next, we detail how we choose our base model and serialization format in Section 3.2 and discuss our proposed techniques for selecting the fine-tuning data $D_{fine-tune}$ in Section 3.3."}, {"title": "3.1 Zero-shot Entity Matching as Sequence Classification in a Transfer Learning Setup", "content": "Analogous to existing approaches [19], we treat entity matching as a sequence classification problem, where the input records $r_l$ and $r_r$ are serialised into an ordered sequence $x = (x_1,\u22ef, x_t)$ of t tokens, and a classification model $f : X \u2192 \\{0, 1\\}$ predicts whether they refer to the same real-world entity or not. Note that $X$ denotes the space of possible sequences here. Recall from Section 2 that we aim for a matcher $f$ which can be applied to unseen target $D_{target}$ originating from two relations $R_{left}$ and $R_{right}$, for which no labelled training data $D_{train}$ is available.\nIn order to tackle this challenge, we use a language model as sequence classifier and treat zero-shot entity matching as a transfer learning problem. We assume that one can access a large set of m labelled datasets $D^{(1)}_{transfer}, ..., D^{(m)}_{transfer}$ from other relations (e.g., all publicly available academic benchmark datasets) to fine-tune a language model as the zero-shot matcher $f$. Directly concatenating samples from all these available datasets is prone to overfitting, and special care is required to create high quality fine-tuning data. The following steps, illustrated in Figure 2, detail how to create and use ANYMATCH:\n(1) We generate a single dataset $D_{fine-tune}$ from the available labelled datasets $D^{(1)}_{transfer}, ..., D^{(m)}_{transfer}$ by applying several data selection techniques.\n(2) We fine-tune a language model on $D_{fine-tune}$ as the matcher $f$.\n(3) We use the resulting matcher $f$ in a zero-shot setting for inference on the unseen target data $D_{target}$."}, {"title": "3.2 Base Model and Serialization Format", "content": "We leverage the language model GPT-2 [27] as our base model. GPT-2 is a decoder-only model with 124 million parameters pre-trained auto-regressively. In contrast to encoder-decoder models like T5 [28] used in [34] with an equivalent number of transformer layers, GPT-2 has no encoder (and therefore only needs about half the number of parameters compared to the 220 million parameters of T5-base). GPT-2 is designed for zero-shot tasks and its auto-regressive training enables the model to compress general knowledge and respond to tasks based on the previous context.\nSerialization format. We apply a general, domain-independent serialisation format to turn the pair of tuples to match into a prompt for our base model. Our prompt starts with a representation of both records. The first record $r_l$ with values $v_{l_1},..., v_{lk}$ is serialised as Record A is <p>COL $v_{l_1}$, ..., COL $v_{lk}$</p>. The second record $r_r$ with values $v_{r_1},..., v_{rk}$ is serialised analogously as Record B is <p>COL $v_{r_1}$, ..., COL $v_{rk}$</p>. and appended. Here, the string COL is used as a marker for a column value, the comma separates column values, while <p> and </p> enclose the record values, to make them more recognisable to the model. Moreover, we represent"}, {"title": "3.3 Selecting High-Quality Fine-Tuning Data", "content": "We now detail the main ideas and motivation for our techniques to turn the set of labelled datasets $D^{(1)}_{transfer}, ..., D^{(m)}_{transfer}$ into the data $D_{fine-tune}$ to fine-tune ANYMATCH. We design a set of data selection techniques to make sure that $D_{fine-tune}$ contains high-quality examples that result in a generalisable matcher. Note that the detailed algorithms to generate this data are discussed in the following in Section 3.4.\nSelection of difficult matching examples via AutoML. A general principle in many EM systems is to categorise candidate pairs based on the difficulty of distinguishing between the represented entities. Unlikely matches are usually pruned early via blocking functions [24]. Moreover, recent studies [16, 21, 23] indicate that many entity matching benchmark datasets can already be solved using linear models.\nThese observations motivate us to think about EM datasets as a mix of record pairs that are easily classified by conventional machine learning models, such as linear models, and another set of pairs that require non-linearity and increased model capacity for accurate prediction. We prioritise the inclusion of the latter, more challenging examples into the fine-tuning data $D_{fine-tune}$ via an AutoML filter. we train an AutoML model on examples from a labelled dataset $D^{(i)}_{transfer}$ and include the positively labelled samples misclassified by the AutoML model in $D_{fine-tune}$. Such samples often highlight the boundaries or edge cases that the model struggles with, and including them can lead to more robust performance and higher transferability.\nAugmentation with attribute-level samples. Even though LLMs show promising potential for data wrangling [22], there still remain fundamental structural mismatches in applying them to relational data [3], e.g., that relational data has an additional column/attribute structure (which is not present in text) and that the relational model does not impose an order over these attributes. We accommodate for this structural mismatch with an augmentation approach that is inspired by how humans tackle the entity matching task: they will not immediately make decisions at the record level, but will use a combination of comparisons across different attributes.\nIn order to account for this observation, we generate several additional attribute-level examples which only contain pairs of attribute values $(r_l[a], r_r[a])$ for each attribute a from a labeled record pair $(r_l, r_r)$ in $D^{(i)}_{transfer}$ and augment the fine-tuning data $D_{fine-tune}$ with these additional samples.\nControlling label imbalance. In general, the candidate set of record pairs for a matcher is generated through the Cartesian product of the two involved relations (filtered by blocking functions). This typically results in significantly more negatively labelled (non-matching) record pairs than positively labelled matching pairs. Directly using all such entity pairs to fine-tune a model could lead to serious overfitting (e.g., via instance memorisation) on the minority class of positively labelled samples. Moreover, since we make no assumptions about the label distribution in the target data $D_{target}$, it is crucial for the final model to perform well on both positive and negative samples.\nWe address this label imbalance with a heuristic to control the ratio of negative to positive samples and include twice as many negative as positive examples from each dataset $D^{(i)}_{transfer}$ in the fine-tuning data $D_{fine-tune}$. The intuition behind this rate is as follows: given a positive pair $(r_l, r_r)$, the model should be able to make different predictions for $(\\tilde{r_l}, r_r)$ and $(r_l, \\tilde{r_r})$, where $\\tilde{r_l}$ and $\\tilde{r_r}$ are similar but distinct entities from the two tables. Given that such negative samples may not have been annotated with golden labels in real datasets, which have golden label from the data annotator, this ratio serves as a practical measure to ensure a consistent balance between positive and negative pairs."}, {"title": "3.4 Data Selection Algorithms", "content": "We now detail the concrete algorithms to turn the set of labelled transfer datasets $D^{(1)}_{transfer} ... D^{(m)}_{transfer}$ into the data $D_{fine-tune}$ which we use for fine-tuning. The function FINETUNE_ANYMATCH in Algorithm 1 shows the high-level procedure as outlined previously: we first generate record-level examples (Algorithm 2) which are then augmented with attribute-level examples (Algorithm 3), which we finally use to fine-tune our base model GPT-2."}, {"title": "4 IMPLEMENTATION", "content": "We implement ANYMATCH based on the existing GPT2Tokenizer and GPT2ForSequenceClassification classes in PyTorch [25] for our chosen base model GPT-2 [27] from the transformers library. We implement the AutoML filter, which selects difficult training examples, based on the TabularPredictor from Amazon's Auto-Gluon library [9]. When filtering individual datasets, we convert them into a TabularDataset instance and subsequently construct a TabularPredictor by specifying the name of the label column that indicates matches. We do not constrain the time budget for the AutoML training (e.g., we do not set a time_limit argument in the fit() method), but observe that the fitting procedure can be completed within minutes for the datasets we are using.\nWe fine-tune GPT-2 via stochastic gradient descent using Adam as optimiser with a fixed learning rate of 0.00002 and weight decay of 0.01. We run fine-tuning for 50 epochs during which we keep track of the best model by evaluating the F1 score on held-out validation data. We apply early stopping if we observe no significant performance improvement on the validation set for six epochs."}, {"title": "5 RELATED WORK", "content": "We summarise existing entity matching approaches (both with and without zero-shot capabilities).\nEntity matching approaches applicable in the zero-shot setting. We start by discussing approaches that are applicable in the zero-shot setting which is in the focus of our paper.\nA seminal work in this area is ZeroER [37], which is a parameter-less method, explicitly designed for the zero-shot case without any training data for the target dataset. The approach is built on the observation that the similarity vectors for matching records are distributed differently than the similarity vectors for non-matching records. In contrast to our approach, there are several drawbacks though: the method requires information about the types of the columns and a decision on the similarity functions to use, is only applicable in a batch setting and cannot match single record pairs in isolation (which for example makes debugging false predictions difficult), and relies on distributional assumptions which may not hold on every dataset.\nDitto [19] is a state-of-the-art entity matching approach, based on fine-tuning a Bert encoder [7]. Ditto injects domain knowledge into the data during serialisation and augments the training data to enhance the model's ability to differentiate between challenging entity pairs. This process includes actions such as dropping columns and editing spans of tokens. Since Ditto models do not rely on a hard-coded schema during training, they can also be applied to unseen target datasets with a different schema in a zero-shot setting. In a similar direction, [43] recently proposed a vision to leverage a selection of LoRA-tuned domain-specific models for entity matching.\nJellyfish [41] is a general LLM-based approach targeting four data preprocessing tasks (including entity matching). Jellyfish leverages two LLAMA2-13B models, which are fine-tuned in an instruction tuning fashion. The corresponding data is specifically created to accommodate multiple data preprocessing tasks. Essentially, one LLAMA model is tasked with classification, providing detailed reasoning, while the second model interprets this output to refine the reasoning process further. Jellyfish is explicitly designed to tackle zero-shot data preparation scenarios on unseen datasets.\nNarayan et al. [22] showed that prompting large commercial LLMs with serialised records (and carefully chosen few-shot examples) can lead to competitive matching performance. MatchGPT [26] enhances the chosen prompts and evaluates a wide variety of base models and prompt formats for both zero-shot and few-shot entity matching. TableGPT [17] applies a similar approach, but enhances the LLMs with \u201ctable fine-tuning\u201d to teach them various data preparation tasks. This approach is designed for both zero-shot and few-shot scenarios.\nApproaches without zero-shot capabilities. Next, we discuss a set of methods for entity matching, which are not able to adapt to the zero-shot setting on unseen data, since their feature encoding depends on the schema of the target data to match. Magellan [8] focuses on building an end-to-end system for entity matching, based on classical machine learning methods. GNEM [6] employs a graph neural network approach to entity matching, where each node represents an entity pair and encodes semantic information and interactions. HierMatch [12] introduces a novel approach to entity"}, {"title": "6 EXPERIMENTAL EVALUATION", "content": "In the following we evaluate the prediction quality of ANYMATCH in comparison to several baselines on a variety of entity matching datasets in Section 6.1. We additionally investigate the computational performance and deployment cost of our method in Section 6.2. Finally, we conduct an ablation study to validate our model design and our choices for the fine-tuning data generation in Section 6.3. The source code of our experiments is available at https://github.com/Jantory/anymatch."}, {"title": "6.1 Prediction Quality", "content": "In our first experiment, we measure the prediction quality of ANYMATCH for zero-shot entity matching on nine benchmark datasets. Our goal is to confirm that our method offers an attractive trade-off between model size and prediction quality."}, {"title": "6.2 Inference Throughput & Deployment Cost", "content": "The goal of the next two experiments is to evaluate the computational performance and hardware requirements of ANYMATCH in comparison to other zero-shot entity matching approaches. We focus on MatchGPT with GPT-4 (which outperforms ANYMATCH by 4.4 percent in prediction quality) and MatchGPT with other LLMs, which also scored high in the evaluation. The ability to deploy zero-shot EM models in a cost-efficient and scalable way is especially important for the use cases discussed in Section 2, such as data integration services in the cloud or deduplication as a data cleaning step in machine learning pipelines [18, 39]. We focus on throughput as a measure of computational performance, as the input for entity matching are typically large candidate sets of potentially"}, {"title": "6.2.1 Inference Throughput", "content": "The goal of our first experiment is to measure the inference throughput in terms of tokens per second of ANYMATCH and the LLM-based methods Jellyfish (which uses the Llama2-13B model [33]) and MatchGPT with three open-weight models (Mixtral-8x7B, SOLAR, Beluga2).\nExperimental setup. We deploy each matcher (in combination with a given model) with exclusive access to a machine with four A100 GPUs with 40 GB GPU RAM in a large academic HPC cluster. Note that the A100 GPU is a common choice for ML, is the most powerful hardware available to us in academic context, and also constitutes a common choice in cloud instances designed for ML workloads. We leverage implementations based on PyTorch and the transformers library. We deploy quantised (16-bit precision) versions of the models and use model parallelism to distribute a model over multiple GPUs if it cannot fit into the 40 GB memory"}, {"title": "6.2.2 Inference Cost", "content": "The goal of the following discussion is to compare ANYMATCH against entity matching with approaches like MatchGPT that use commercial models from OpenAI. As discussed, we cannot reliably measure the throughput for these models since they are deployed behind proprietary APIs on unknown hardware. As a consequence, we compare the dollar cost of inference with these models to the dollar cost of inference with ANYMATCH and MatchGPT with the open-weights models Mixtral-8x7B, SOLAR and Beluga2.\nSetup. We lookup the costs for the commercial models from OpenAI at https://openai.com/api/pricing/. As of July 2024, batch inference with the GPT-4 model costs $0.015 per 1,000 tokens and inference with GPT-3.5-turbo-0613 costs $0.00075 per 1,000 tokens. Note that these models have different costs for input and output tokens; we use the cheaper input token cost, since entity matching is modelled as sequence classification task, which only generates a single output.\nWe estimate the cost for ANYMATCH and the open-weight models as follows. We assume that such a model is deployed on a cloud instance that is constantly used for inference (e.g., as part of the use cases described in Section 2). We use the cost for a p4d. 24xlarge instance from the Amazon Web Services cloud as a reference. This machine is designed for ML workloads and comes with eight A100 (40GB) GPUs (exactly twice the amount of GPUs which we used for our throughput experiment). As of June 2024, such a machine has an hourly cost of $19.22 in a scenario where the instance is reserved for a year (which would be common in a corporate setup). Since the cloud instance has the exact same type of GPU (only twice the amount), we can extrapolate our throughput numbers from Section 6.2.1 to this machine by simply doubling them, as inference in entity matching is an embarrassingly parallel workload. We therefore estimate the cost per 1,000 tokens for models deployed on this machine as $(p/(2t_m \\cdot 3600)) \\cdot 1000$ where p is the hourly instance price, $t_m$ is the throughput in tokens/s observed for model m and 2 is the extrapolation factor from our previous experiments (as the cloud instance has twice the amount of GPUs).\nFor the open-weight models, we additionally lookup the hosting price on the cloud platform together.ai6 and choose this option if the resulting price per 1,000 tokens would be lower than our self-hosting setup (e.g., because a more favourable GPU can be chosen)."}, {"title": "6.3 Ablation Study", "content": "Finally, we conduct an ablation study for ANYMATCH in order to validate our design decisions from Section 3. For that, we remove and/or replace different components of our model and show that this removal and replacement results in a performance decrease. We evaluate the resulting model variants analogously to Section 6.1, where we measure the average F1 score over the nine benchmark datasets in a zero-shot setting. We summarise the tested variants together with the corresponding results for the individual datasets in Table 5. In addition, we report the performance delta (the reduction in average F1 score) for all tested variants, in comparison to the proposed design of ANYMATCH."}, {"title": "6.3.1 Choice of base model", "content": "The goal of our first experiment is to validate the choice of the decoder-only language model GPT-2 [27] as base model for ANYMATCH.\nExperimental setup. We evaluate the impact of replacing GPT-2 in ANYMATCH with different similarly sized alternative models. In particular, we evaluate Google's T5 [28] model with an encoder-decoder architecture and the encoder-only model Bert [7], which is for example also used by [19].\nResults and discussion. We find that both alternative models result in a decrease of the overall F1 score, as detailed in Table 5: using T5 results in a decrease of 2.38%, while using Bert leads to the drastic decrease of 9.04% in average F1 score. The performance loss observed when switching to the T5 model likely stems from its better suitability for sequence-to-sequence tasks [28], whereas we treat our task as sequence classification and have specifically tailored the prompt design for this purpose. Moreover, we attribute the significant decrease in predictive quality when using Bert to the following factors: Bert encodes the input into a vectorised representation, to which a prediction head is subsequently appended for making predictions. However, in our approach, we incorporate a task description into the input sequence, accounting for roughly 5% of the total input, which potentially negatively influences the downstream classification. Note while using a different serialization method for the Bert model might improve performance, this is not the focus of this ablation study."}, {"title": "6.3.2 Choice of serialization format", "content": "The goal of this experiment is to validate the design of our serialization format. In particular, we enclose the serialized record pairs in <p> tags (referred to as <p> enclosement), use the placeholder COL to denote the starting position of each attribute value, and add the question \u201cGiven the attributes of the two records, are they the same?\u201d as a suffix to the end of the model prompt.\nExperimental setup. We evaluate three alternative serialization formats. The first variant (prefix, <p> enclosement, column name) replaces the COL placeholder with the actual column name to specify attributes. In the second variant called (prefix, <p> enclosement), we prepend the question to the prompt. In the last variant (prefix), we additionally remove the <p> enclosement from the format.\nResults and discussion. As detailed in Table 5, all variants lead to a loss in prediction quality. Including the column name lead to a 0.43% performance loss. This is an indication that the reliance on attribute names at training time undermines the model's transferability as the target data lacks such information. Changing the suffix format to prefix results in a drop of 1.39% in terms of average F1, while additionally removing the <p> enclosement decreases the drop to 1.71%."}, {"title": "6.3.3 Benefits of difficult pairs selection and attribute-level matching during training data generation", "content": "Next, we aim to validate the choice of the different techniques used during the generation of training data for ANYMATCH.\nExperimental setup. We show that these techniques contribute to the predictive performance of ANYMATCH. For that we implement different variants of our training data generation algorithm and evaluate the zero-shot performance of our model trained on this data on the nine datasets analogous to the setting in Section 6.1. As discussed, our main model uses difficult pair selection via AutoML (automl) for record-level instances, augments the data with \u201cflipped\u201d record pairs (flip) and mixes attribute-level training instances into the training data (attr_mix). In this experiment, we evaluate the following reduced variants:\n\u2022 (automl, attr_mix) - This variant does not include flipped record pairs.\n\u2022 (attr_mix) - This variant does not use our AutoML-based approach to select difficult matching examples and does not include flipped record pairs.\n\u2022 (attr_seq) - This variant also does not use our AutoML-based approach to select difficult matching examples and does not include flipped record pairs. Additionally, it does not mix in the attribute-level training instances with the record-level training instances. Instead, it uses sequential training to fine-tune the model on the attribute-level pairs before continuing the fine-tuning on record-level examples.\n\u2022 () - This variant neither uses the AutoML-based approach to select difficult matching examples nor any attribute-level training examples or flipped record pairs.\nResults and discussion. We list the resulting F1 scores (and the deltas in F1 compared to the main model) in Table 5. The results validate the benefits of our training data generation techniques, as we find that the removal of any of our proposed techniques results in a performance decrease. Removing the flipped records pairs in (automl, attr_mix) leads to a decrease of 0.72%, additionally removing the selection of difficult examples via AutoML in (attr_mix) further decreases the performance by 1.62%. Removing or not mixing in the attribute level instances (e.g., the variants (attr_seq) and ()) leads to a performance loss of more than 3%.\nIn summary, the ablation study confirms that all our design decisions for the model and fine-tuning data generation contribute to the predictive performance of ANYMATCH."}, {"title": "7 CONCLUSION", "content": "We revisited the zero-shot EM problem with ANYMATCH, a small language model fine-tuned in a transfer learning setup, and proposed several novel data selection techniques to generate high-quality fine-tuning data for our model.\nWe conducted an extensive evaluation of the prediction quality and deployment cost of our model, in a comparison to thirteen baselines on nine benchmark datasets. We find that ANYMATCH provides competitive prediction quality despite its small parameter size and exhibits drastic deployment cost benefits compared to the latest EM approaches, which leverage trillion parameter LLMs.\nFuture work. In future work, we aim to investigate whether we can combine ANYMATCH with expensive LLMs like GPT-4 in a smart way that still retains the overall cost-efficiency, e.g., by using GPT-4 only for samples where our method has low confidence. Furthermore, we also plan to extend ANYMATCH to make use of additional labelled samples, e.g., in few-shot scenarios."}]}