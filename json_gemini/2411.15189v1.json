{"title": "Order Is All You Need for Categorical Data Clustering", "authors": ["Yiqun Zhang", "Mingjie Zhao", "Hong Jia", "Yiu-ming Cheung"], "abstract": "Categorical data composed of nominal valued attributes are ubiquitous in knowledge discovery and data mining tasks. Due to the lack of well-defined metric space, categorical data distributions are difficult to intuitively understand. Clustering is a popular technique suitable for data analysis. However, the success of clustering often relies on reasonable distance metrics, which happens to be what categorical data naturally lack. Therefore, the cluster analysis of categorical data is considered a critical but challenging problem. This paper introduces the new finding that the order relation among attribute values is the decisive factor in clustering accuracy, and is also the key to understanding the categorical data clusters. To automatically obtain the orders, we propose a new learning paradigm that allows joint learning of clusters and the orders. It turns out that clustering with order learning achieves superior clustering accuracy, and the learned orders provide intuition for understanding the cluster distribution of categorical data. Extensive experiments with statistical evidence and case studies have verified the effectiveness of the new \"order is all you need\u201d insight and the proposed method.", "sections": [{"title": "1 INTRODUCTION", "content": "Categorical attributes with qualitative values (e.g., {lawyer, doctor, computer scientist} of attribute \"occupation\") are very common in clustering analysis on real datasets. As the categorical values are cognitive conceptual entities that cannot directly attend arithmetic operations [1], most existing clustering techniques proposed for numerical data are not directly applicable to the qualitative-valued categorical data. Therefore, the development of categorical data clustering techniques, including clustering algorithms, distance measures, and representation learning approaches, continues to attract attention in the data mining and machine learning fields [8].\nIn the literature, there are two main types of clustering algo-rithms for categorical data: 1) k-modes and its subsequent vari-ants [15][4][18] that partition data samples into clusters based on certain distance measures, and 2) cluster quality assessment-based algorithms [14] [26] that adjust the sample-cluster affiliations based on certain cluster quality measures. Their performance is com-monly dominated by the metric space represented by the adopted measures. Thus, the focus of subsequent research has shifted from designing clustering algorithms to proposing reasonable measures that suit the categorical data clustering [29].\nThe conventional Hamming distance measure and one-hot en-coding strategy are very common in categorical data clustering, but they both simply represent the distance between two attribute values based on whether the two values are identical or not, which is far from informative. Later, on the one hand, more advanced dis-tance measures have been proposed to discriminatively indicate the value-level differences by considering the intra-attribute probability of possible values [32], inter-attribute relationship [3] [25] [17], or both [19][35]. On the other hand, advanced representation strate-gies that encode distances among data samples [30], and couplings in the attribute and value levels [23] [21], have also been proposed Although all the above advanced techniques have considerably im-proved the clustering performance, they are all task-independent and thus incompetent in adapting different clustering tasks on various categorical datasets.\nThe learning-based approaches usually optimize the representa-tion and clustering in a joint way to further achieve improvements in clustering performance. Typical works in this stream include the distance learning-based methods [6] [18][36] that learn the distance structures of the categorical values, and the representative learning-based methods [39] [40] [38] that learn to encode the categorical data into numerical one. In the literature, a phenomenon that the possible values of a categorical attribute are with semantic order (e.g., attribute \u201cdecision\u201d with values {strong_accept, accept, ..., strong_reject}) has also been presented [2][24]. Such value order has been empirically proven to be crucial in obtaining appropri-ate distance structures and accurate clustering performance for categorical data [33] [34].\nIt is noteworthy that order is the actual basis for clustering in most cases. For instance, when a k-means-type algorithm tries to"}, {"title": "2 RELATED WORK", "content": "This section overviews the related works in data encoding, rep-resentation learning, and consideration of value order, for categorical data clustering.\nThe data encoding techniques convert the categorical data into numerical values based on certain strategies, and then perform clustering on the numerical encoding. The conventional one-hot encoding and Hamming distance construct similar distance struc-tures for categorical data by assigning distances \u201c1\u201d and \u201c0\u201d to any pair of different and identical values, respectively. As they cannot distinguish different dissimilarity degrees [8][5] of data samples, information loss is usually unavoidable during cluster-ing. Some statistic-based distance measures [27][6][8] consider the intra-attribute occurrence frequency of values to better distinguish the data samples. They take a common principle that two similar values may have more consistent statistics within the correspond-ing attribute or cluster. Some metrics [3] [25] further define distance considering the relevance between attributes. The representative measure proposed in [17] selects more relevant attributes for the distance measurement. Later, the measures [19][20] that more com-prehensively consider both the intra- and inter-attribute couplings reflected by the data statistics have been presented. Coupling-based encoding strategies [23] [21] have also been designed accordingly to encode the couplings. All the above techniques perform value-level encoding to reflect sample-level distances, while the work [30]"}, {"title": "3 PROPOSED METHOD", "content": "This section first formulates the problem, then proposes the order distance definition and order learning mechanism, and finally introduces the joint learning of the orders and clusters. Analyses of time complexity and convergence are also provided."}, {"title": "3.1 Problem Formulation", "content": "The problem of categorical data clustering with order learning is for-mulated as follows. Given a categorical dataset $X = {X_1, X_2, ..., X_n}$ with n data samples (e.g., a collection of n bank clients). Each data sample $x_i$ can be denoted as a d-dimensional row vector $x_i = [x_{i,1}, x_{i,2}, .., x_{i,d}]$ represented by d attributes $A = {a_1, a_2, ..., a_d}$ (e.g., occupation, gender, priority, etc.). Each attribute $a_r$ (e.g., occu-pation) can be denoted as a column vector $a_r = [x_{1,r}, x_{2,r}, ..., x_{n,r}]$ composed of the occupation description of all the n clients. For the categorical attribute \"occupation\" $a_r$, all its n values are taken from a limited number of qualitative possible values $V_r = {v_{r,1}, v_{r,2}, ..., v_{r,l_r}}$, e.g., {lawyer, doctor, ..., scientist}. The subscripts indicate the se-quential number of an attribute and the sequential number of the attribute's possible value. For example, doctor is the second value"}, {"title": "3.2 Order Distance", "content": "We first define the sample-cluster distance $\u0398(x_i, C_m; O)$ with consid-ering the order O, to connect the order with the clustering objective. The distance $\u0398(x_i, C_m; O)$ is defined as:\n$\u0398(x_i, C_m; O) = \\sum_{r=1}^{d} \u03b8(x_{i,r}, P_{m,r})$\nwhere\n$\u03b8(x_{i,r}, P_{m,r}) = d_{i,r} p_{m,r}.$\nis the order distance between $x_i$ and all the samples in $C_m$ reflected by the r-th attribute. $d_{i,r}$ is an $l_r$-dimensional vector containing the order distances between $x_{i,r}$ and possible values in $V_r$, and $p_{m,r}$ is an $l_r$-dimensional vector describing the probability distribution of $V_r$ within cluster $C_m$, which can be written as:\n$d_{i,r} = [d_{i,r,1}, d_{i,r,2}, ..., d_{i,r,l_r}]$\n$p_{m,r} = [p_{m,r,1}, p_{m,r,2}, ..., p_{m,r,l_r}]$\nrespectively. The g-th value of $d_{i,r}$ is defined as the normalized order distance between $x_{i,r}$ and $v_{r,g}$, which can be written as:\n$d_{i,r,g} = \\frac{|o(x_{i,r}) - o(v_{r,g})|}{l_r - 1}$\nwhere $l_r - 1$ in the denominator is to normalize the order distance into the interval [0, 1] to ensure distance comparability of different attributes. The g-th value of $p_{m,r}$ is the occurrence probability of $v_{r,g}$ in cluster $C_m$, which can be written as:\n$p_{m,r,g} = \\frac{card(X_{r,g} \u2229 C_m)}{card(C_m)}$\nwhere $X_{r,g} = {x_i|x_{i,r} = v_{r,g}}$ is a sample set collecting all the samples in X with their r-th values equal to $v_{r,g}$, and the cardinality function card() counts the number of elements in a set.\nREMARK 1. Rationality of the order distance. The intra-cluster probability distribution and the value order are introduced to form the order distance. The probability descriptor $p_{m,r}$ is adopted to in-formatively represent the value distribution in different clusters for appropriate distance measurement and accurate order learning in Sec-tion 3.3. By making the distance $d_{i,r}$ dependent on the order O through Eq. (7), we facilitate a direct connection between the order learning and the clustering tasks. Such a connection leverages the optimization of the order O for more accurate clustering in Section 3.4."}, {"title": "3.3 Order Learning", "content": "Based on the above-defined order distance, we discuss how to obtain the optimal order:\n$O^* = arg \\min_{O} L(Q, O)$\nA simple but extremely laborious solution is to search different order combinations across all the attributes. There are a total of $\u03a0_{r=1}^{d} l_r!$ combinations, and each combination should be evaluated on n samples by accumulating their sample-cluster distances cal-culated by Eq. (3), which makes the search infeasible. A relaxed efficient solution is to search the optimal order within clusters by assuming independence of attributes. Accordingly, optimal order of $a_r$ w.r.t. $C_m$ denoted as $O_{r,m} = {o_m(v_{r,1}), o_m(v_{r,2}), ..., o_m(v_{r,l_r})}$ can be obtained through the efficient intra-cluster traversal search:\n$O_{r,m} = arg \\min_{O_r} L_{r,m}$\nwhere $L_{r,m}$ is an inner sum component of the objective L. More specifically, $L_{r,m}$ is jointly contributed by the r-th attribute and m-th cluster, which can be derived from Eqs. (1) and (3) as:\n$L_{r,m} = \\sum_{x_i \u2208 C_m} \u03b8(x_{i,r}, P_{m,r})$\nBy searching for the optimal ranking in each cluster and then performing a linear combination based on the cluster size, the overall ranking of each possible value $v_{r,g}$ can be obtained by:\n$\u014d(v_{r,g}) = \\sum_{m=1}^{k} (o_m(v_{r,g}). \\frac{card(C_m)}{n})$\nSince the value of $\u014d(v_{r,g})$ is not necessarily an integer, we sort the obtained rankings ${\u014d(v_{r,1}), \u014d(v_{r,2}), ..., \u014d(v_{r,l_r})}$ of an attribute in ascending order to obtain the final ranking of values in Vr, and the corresponding integer order $\u00d5_r = {\u00f5(v_{r,1}), \u00f5(v_{r,2}), ..., \u00f5(v_{r,l_r})}$ is obtained. Correspondingly, we collectively refer to all orders obtained through Eqs. (10) - (12) as $\u00d5 = {\u00d5_1, \u00d5_2, ..., \u00d5_d}$.\nREMARK 2. Principle of the order learning. In the process of obtaining \u00d5, the optimal order of each attribute is determined for each cluster by Eqs. (10) - (12). Through Eq. (11), the specific distribution of samples within each cluster is fully considered (see Remark 1) to appropriately reflect the component $L_{r,m}$ of the objective L. Hence, an expected minimization on a series of $L_{r,m}$s can be achieved by the determined orders. The overall order of an attribute is obtained by integrating the contributions of all the clusters through Eq. (12). Here,"}, {"title": "3.4 Joint Order and Cluster Learning", "content": "We facilitate joint learning of clusters and orders by integrating the order learning into the clustering objective, which can be rewritten in a more detailed form based on Eqs. (1), (3), and (4):\n$L(Q,O) = \\sum_{m=1}^{k} \\sum_{i=1}^{n} q_{i,m} \\sum_{r=1}^{d} d_{i,r}p_{m,r}$\nwhere the order distance $D = {d_{i,r}|i \u2208 {1, 2, ..., n}, r \u2208 {1, 2, ..., d}}$ is dependent to the order O according to Eqs. (5) and (7), and the probability distribution $P = {p_{m,r}|m = {1, 2..., k}, r \u2208 {1, 2, ..., d}}$ is dependent to the partition Q according to Eqs. (6) and (8). Typically, Q and O are iteratively computed to minimize L following a k-modes-type optimization process. Specifically, given \u00d4, the order distance D can be updated accordingly, and then a new Q can be computed by:\n$q_{i,m} = \\{\\begin{array}{ll} 1, & \\text{if } m = arg \\min_{y} \\sum_{r=1}^{d} d_{i,r}P_{y,r} \\\\ 0, & \\text{otherwise} \\end{array}$\nwith $i = {1, 2, ..., n}$ and $m = {1, 2, ..., k}$. Eq. (14) is a detailed version of Eq. (2) that adopts the order distance defined by Eqs. (3) and (4). Then, P is updated according to the new Q. When the iterative updating of Q and P converges, we obtain O according to Eqs. (10) - (12), and update D accordingly. In summary, L is minimized by iteratively solving the following two sub-problems:\n(1) Fix Q and P, solve the minimization problem L(Q,O) by searching O and updating D accordingly;\n(2) Fix \u00d4 and I, solve the minimization problem L(Q, \u00d4) by iteratively computing Q, and updating P according to Q, until convergence;\nThe whole algorithm named Order and Cluster Learning (OCL) is summarized as Algorithm 1. The next subsection provides analyses of time complexity and convergence to further explain OCL."}, {"title": "3.5 Complexity and Convergence Analysis", "content": "We analyze the time complexity to show the scalability of OCL, and then provide its convergence analysis.\nTHEOREM 1. The time complexity of OCL is $O(EInkds + Edkns!)$.\nProof. Assume it involves I inner iterations (lines 4 - 9 in Algo-rithm 1) to compute Q and P, and E outer iterations (lines 2 - 13 in Algorithm 1) to search O and update D. For worst-case analysis, we adopt s to indicate the maximum number of possible values of a dataset by s = max(11, 12, ..., la).\nFor each of the I inner iterations, we obtain P upon the n data samples with complexity O(nkds), and obtain D upon the s values of d attributes on n samples with complexity O(nds). Then the n samples are partitioned to k clusters with complexity O(nkds). For I iterations in total, the complexity is O(Inkds).\nFor each of the E outer iterations, a new order of s possible values of each attribute within each cluster is searched by Eqs. (10) - (12), and the complexity for d attributes and k clusters is thus O(dkns!). For E outer iterations with considering the I inner iterations, the overall time complexity of OCL is $O(EInkds + Edkns!)$.\nREMARK 3. Scalability of OCL. The essential characteristic of categorical data is that the number of possible values for an attribute is finite and small, usually ranging from 2 to 8 for real benchmark datasets as shown in the Appendix. Accordingly, s can be treated as a small constant, and the time complexity is simplified to O(EInkd), which is linear to the scaling of n or d.\nTHEOREM 2. OCL algorithm converges to a local minimum in a finite number of iterations.\nProof. We prove the convergence of the inner and outer loop, i.e., lines 4 - 9 and lines 2 - 13 of Algorithm 1, respectively.\nLemma 1. The inner loop converges to a local minimum of L(Q, \u00d4) in a finite number of iterations with fixed \u00d4. Note that the number of possible partitions Q is finite for a dataset with a finite num-ber of objects n. We then illustrate that each possible partition Q occurs at most once by OCL. Assume that two optimizers satisfy Q{1} = Q{2}, where I\u2081 \u2260 I2. Then it is clear that L(Q{11}, O{E}) =\nL(Q{2}, O{E}). However, the value sequence of the objective function {L(Q{1}, O{E}), L(Q{2}, O{E}), ..., L(Q{I}, O{E})} generated by OCL is strictly decreasing. Hence, the result follows.\nLEMMA 2. The outer loop converges to a local minimum of L(Q, O) in a finite number of iterations. We note that the status of orders O is in a discrete space and thus the number of possible orders is finite. The number of possible partitions Q is also finite. Therefore, the number of possible combinations of Q and O is finite. We then prove that each possible combination of Q and O appears at most once by OCL. We note that given Q{E1} = Q{E2}, where E1 \u2260 E2, the corresponding O{E1+1} and O{E2+1} can be computed accord-ingly, satisfying O{E1+1} = O{E2+1}. Then we can obtain their cor-responding partitions Q{E1+1} and Q{E2+1} at the convergence of the next inner loop, satisfying Q{E1+1} = Q{E2+1}. It is clear that"}, {"title": "4 EXPERIMENTS", "content": "Different experiments have been designed to evaluate the perfor-mance of the proposed order learning strategy as well as the corre-sponding clustering algorithm. Detailed experimental settings and the obtained results are introduced as follows."}, {"title": "4.1 Experimental Settings", "content": "Four types of experiments have been conducted to evaluate the proposed method. A brief outline of the experiments is given be-low: (1) Clustering performance evaluation with significance test (Section 4.2): We compared the proposed OCL with conven-tional and state-of-the-art methods on different types of datasets. To statistically analyze the performance differences between variant approaches, we also conducted a significance test on the clustering results; (2) Ablation studies (Section 4.3): To more specifically demonstrate the effectiveness of the core components of OCL and investigate the role of learned order in improving clustering perfor-mance, we conduct ablation studies from the algorithm aspect and data aspect. (3) Convergence and efficiency evaluation (Sec-tion 4.4): The values of the objective function after each learning iteration have been recorded to study the convergence of OCL. Its efficiency has also been studied with experiments on large datasets with scaling sizes of samples n and attributes d. (4) Visualization of clustering effect (Section 4.5): To visually demonstrate the effectiveness of the order distance metric learned by OCL, we use it and the counterparts to numerically encode the data, and utilize the t-SNE for visualization.\nNine counterparts have been selected for the comparative study. The counterparts include traditional K-MoDes (KMD) algo-rithm [15], popular entropy-based metric LSM [27], and context-based distance measurement CBDM [16]. The latter two are rep-resentative measures proposed for categorical data, both of which"}, {"title": "4.2 Clustering Performance Evaluation", "content": "In this section, we investigate the clustering performance of differ-ent algorithms and statistically analyze the superiority of OCL."}, {"title": "4.2.1 Clustering performance on different datasets", "content": "The cluster-ing results obtained by different algorithms on various datasets in terms of different validity indices are shown in Tables 3 and 4. The best and second-best results on each dataset are highlighted in bold and underlined, respectively. The observations include the following four aspects: (1) Overall, OCL reaches the best result on most datasets, indicating its superiority in clustering. (2) On the CA index, although OCL does not perform significantly bet-ter than the second-best method on the AP and TT datasets, the second-best methods are different on these datasets, but OCL has stable performance on them. (3) On the CA index, although OCL does not perform the best on CS dataset, its performance is close to the best result (the difference is less than 0.005), which still proves OCL's competitiveness. (4) In terms of the ARI index, there are four datasets on which OCL does not perform the best, namely AP, CS, BC, and LG. However, similarly, the differences between OCL and the best-performing algorithms on AP, CS, and BC datasets are very small. In addition, the result of CBDM is not reported on the NS dataset because all the NS's attributes are independent of each other, making the inter-attribute dependence-based CBDM fail in measuring distances."}, {"title": "4.2.2 Significance study", "content": "We first implement the Friedman [11] test on the average ranks reported in the last rows in Tables 3 and 4. The corresponding p-values are 0.000017 and 0.000091 respectively, both passing the test under the confidence of 99% (p-value = 0.01). On this basis, results of the two-tailed BD test [7] with confidence inter-vals 0.95 (\u03b1 = 0.05) and 0.9 (a = 0.1) are shown in Figure 2, with the corresponding CD intervals CD_95%=3.4275 and CD_90%=3.1383, respectively. According to the criterion presented in [7], OCL is con-sidered to perform significantly better than all the counterparts that fall outside the right boundary of the CD intervals. It can be seen that the proposed OCL is significantly superior to all counterparts."}, {"title": "4.3 Ablation Studies", "content": "To demonstrate the effectiveness of the core components of OCL and to verify our hypothesis on the orders of categorical data at-tributes, we design several variants of OCL for ablation studies from both algorithm and data perspectives.\nFrom the algorithm aspect, to evaluate the effectiveness of the proposed Order Distance (OD), we compare OCL with its variant OCL\u00b9, which uses normalized equidistant order distance without considering the probability distribution of possible values. To eval-uate the effectiveness of the Order Learning (OL) mechanism, we further remove the process of order learning from OCL\u00b9 and only update the order once in the first iteration, which forms another variant OCLII. To completely discard Order Information (OI), we further make OCLII use only the traditional Hamming distance, thus forming the third variant OCLIII. Table 5 compares the clus-tering performance of OCL and the three variants.\nIt can be observed that the performance of OCL is superior to all variants. More specifically, OCL outperforms OCL\u00b9 significantly on 11 out of all 12 datasets. This indicates that OD can effectively utilize the distribution information of the attribute values within different clusters for reasonable order distance measurement. Furthermore,"}, {"title": "4.4 Convergence and Efficiency Evaluation", "content": "To evaluate the convergence of OCL, we execute it on all 12 datasets and present the convergence curves in Figure 3. The horizontal and vertical axes represent the number of learning iterations and the value of the objective function L, respectively. In each sub-figure of Figure 3, the blue triangles mark the iterations of order updates, while the red square indicates the iteration that OCL converges. It can be observed that, after each order update, the value of L further decreases. This indicates that the proposed order learning mechanism is consistent with the optimization of the clustering objective function. Moreover, for all the tested datasets, OCL com-pletes the learning process within 30 iterations with at most three updates of the orders. This reflects that OCL converges efficiently with guarantee, which conforms to the analysis in Section 3.5.\nFurthermore, to evaluate the execution efficiency of OCL, we con-struct large synthetic datasets with different numbers of samples and attributes to investigate the execution time of OCL in compari-son with other counterparts. The results reported in Figure 4 show that the execution time of OCL is better than UDMC and DLC on large-scale datasets, and better than H2H, UDMC, and CBDM on high-dimensional datasets. Additionally, as the number of samples or attributes increases, the execution time of OCL increases linearly, which is consistent with the time complexity analysis in Section 3.5. In summary, OCL is efficient compared to the state-of-the-art meth-ods and does not incur too much additional computational cost compared to the simplest methods."}, {"title": "4.5 Visualization of Clustering Effect", "content": "To demonstrate the cluster discrimination ability of the OCL method, we utilize the distance between attribute values learned by OCL, CBDM, and ADC respectively to encode the attributes of the VT dataset. Subsequently, the t-SNE [28] is introduced to process the en-coded data into two-dimensional space and visualize it by marking the samples with real labels in different colors. It can be seen from Figure 5 that OCL has significantly better cluster discrimination ability than CBDM and ADC."}, {"title": "5 CONCLUDING REMARKS", "content": "This paper introduces a new finding that knowing the correct order relation of attribute values is critical to cluster analysis of categori-cal data. Accordingly, we propose the OCL paradigm that allows joint learning of data partitions and order relation of attribute val-ues for accurate clustering. The proposed learning mechanism can infer a more appropriate order among categorical attribute values based on the current clusters, thus yielding more compact clusters based on the updated order. The data partition and order learning are iteratively executed to approach the clustering objective, i.e., maximizing the overall similarity of intra-cluster samples. As a result, more accurate clustering results and insightful orders can be obtained simultaneously. OCL converges quickly with guaran-tee, and extensive experiments conducted on 12 real benchmark datasets fully demonstrate its superiority in comparison with state-of-the-art clustering methods. A case study has also been provided to show the rationality of the learned order in understanding unla-beled categorical data.\nThe proposed OCL method is also not exempt from limitations. Although it demonstrates the potential to enhance the performance of mixed data clustering, the deep connection between the treat-ment of categorical and numerical attributes remains to be explored. In addition, it would also be promising to extend OCL to more com-plex datasets with non-static distribution, an unknown number of clusters, and extremely imbalanced clusters."}]}