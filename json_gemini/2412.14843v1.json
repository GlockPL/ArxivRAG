{"title": "Mapping and Influencing the Political Ideology of Large Language Models using Synthetic Personas", "authors": ["Pietro Bernardelle", "Leon Fr\u00f6hling", "Stefano Civelli", "Riccardo Lunardi", "Kevin Roitero", "Gianluca Demartini"], "abstract": "The analysis of political biases in large language models (LLMs) has primarily examined these systems as single entities with fixed viewpoints. While various methods exist for measuring such biases, the impact of persona-based prompting on LLMs' political orientation remains unexplored. In this work we leverage PersonaHub, a collection of synthetic persona descriptions, to map the political distribution of persona-based prompted LLMs using the Political Compass Test (PCT). We then examine whether these initial compass distributions can be manipulated through explicit ideological prompting towards diametrically opposed political orientations: right-authoritarian and left-libertarian. Our experiments reveal that synthetic personas predominantly cluster in the left-libertarian quadrant, with models demonstrating varying degrees of responsiveness when prompted with explicit ideological descriptors. While all models demonstrate significant shifts towards right-authoritarian positions, they exhibit more limited shifts towards left-libertarian positions, suggesting an asymmetric response to ideological manipulation that may reflect inherent biases in model training.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have been shown to exhibit distinct political biases when evaluated through the Political Compass Test (PCT) [3, 9], typically leaning towards economically left and socially liberal positions. Parallel research has demonstrated that these models can dynamically adopt the perspective of different personas through zero-shot prompting, increasing diversity in their outputs and behavior patterns [4] while accurately simulating responses across demographic groups [1]. However, this adaptability comes with potential drawbacks, as it can amplify stereotypes [7] and significantly alter political expression [2]. This malleability in LLMs' behavior raises questions about the stability of their political orientations and the potential for deliberate manipulation through persona-based prompting.\nRecent work has introduced PersonaHub [5], a collection of synthetic persona descriptions generated through LLM bootstrapping. While these personas have been shown to increase diversity in various natural language processing (NLP) tasks, their influence on LLMs' political biases remains unexplored. Our study bridges this gap by investigating how different PersonaHub personas affect LLMs' responses to the PCT. Specifically, we examine both the distribution of political orientations across personas and the potential for manipulation through explicit ideological prompting. We focus on testing ideological malleability towards right-authoritarian and left-libertarian positions, as these represent maximally distant points on the political compass while also allowing us to examine both reinforcement and resistance to LLMs' inherent left-libertarian bias. Our investigation addresses two questions:\nRQ1: How do diverse persona descriptions affect LLMs' positions on the Political Compass Test?\nRQ2: To what extent the introduction of explicit ideological ele-ments in persona descriptions can influence LLMs' Political Compass Test outcomes?\nWe show that synthetic personas predominantly cluster in the left-libertarian quadrant (Figure 1), with Mistral, Llama, and Qwen exhibiting similar distributions. Zephyr demonstrates the highest concentration of personas around its centroid, while Llama's personas distribution displays the greatest variance. Additionally, explicit ideological prompting can substantially alter this baseline distribution, though models respond differently to such interventions. In our experiments, Llama exhibited the strongest shift toward right-authoritarian positions, while Mistral showed the most pronounced movement toward left-libertarian orientations."}, {"title": "2 RELATED WORK", "content": "Personas in LLMs. Recent work has shown that LLMs can effectively adopt and simulate different personas through appropriate prompting. PersonaHub [5] introduced a collection of over 1 billion synthetically generated persona descriptions, showing that these personas can help diversify model outputs in a controlled way. This approach has been successful in diversifying data annotations, where personas help elicit a wider range of valid perspectives [4]. The ability of LLMs to reliably simulate different personas has also been explored in the context of simulating human samples [1], where LLMs successfully predicted survey responses across different demographic groups with high accuracy. Our work builds on these findings by systematically mapping personas from PersonaHub onto the political compass, providing the first large-scale analysis of how synthetic personas distribute across political ideological space over multiple LLMs.\nLLMs & Political Compass Test. Multiple studies have investi-gated the political biases of LLMs using the PCT as an evaluation framework. Initial work by Hartmann et al. [6] found that Chat-GPT exhibited consistently left-libertarian leanings. Subsequent studies confirmed this tendency across different LLMs while highlighting challenges in reliably eliciting political stances from more recent models with stronger guardrails [9]. Recent methodological innovations have focused on making PCT evaluations more robust, including through masked token prediction [3] and ensuring stability across different prompting approaches [9]. Our work advances this line of research by combining PCT evaluation with persona-based prompting, offering a novel methodology for understanding not just the default political biases of LLMs, but also their capacity to simulate diverse political perspectives through persona adoption."}, {"title": "3 METHODOLOGY", "content": "Data. We utilize PersonaHub, a collection of 1 billion diverse personas\u00b9, to examine how persona descriptions shape the political orientations of LLMs. To systematically evaluate these influences, we employ the PCT, which assesses political perspectives through 62 distinct statements across six key domains. While previous research by Lunardi et al. [8] has shown that LLMs' responses may vary based on question phrasing, we maintain the original PCT formulations to ensure methodological consistency. Our analysis includes 12.4 million categorical responses (200.000 personas x 62 statements), collected by prompting models to select from predefined political stance options.\nModels. For our analysis, we select four open-source language models: Mistral-7B-Instruct-v0.3, Llama-3.1-8B-Instruct, Qwen-2.5-7B, and Zephyr-7B-beta. These models were chosen for their open-source nature and relatively small parameter size (7-8B), enabling easy reproducibility while maintaining sufficient diversity for generalizable insights.\nExperimental setup. Our experimental setup comprises of two phases. First, we establish a baseline by having each model complete the PCT while impersonating each persona description. Second, we investigate ideological malleability by injecting explicit political descriptors (\"right authoritarian\" or \"left libertarian\") to the persona descriptions and measuring the resulting shifts in political positioning.\nComputational resources. For our experiments, we used 8\u00d7A100 GPU cards. The inference time was approximately four hours to run the PCT across each persona-based LLM, with a total experimental runtime of approximately 36 hours for the complete set of experiments."}, {"title": "4 RESULTS", "content": "Personas political distribution. The baseline political distribution of personas, shown in Figure 1, reveals several patterns on how LLMs inherently position them in the political compass. Mistral, Llama, and Qwen exhibit similar distributions, with the majority of personas clustering in the left-libertarian quadrant-a reflection of the widely recognized left-libertarian bias encoded in LLMs. While Zephyr maintains this left-libertarian tendency, its distribution is notably more concentrated (as shown in Table 1), and positioned closer to the center of the compass compared to the other three models. This tighter clustering and more moderate positioning suggests that Zephyr either has stronger internal constraints on ideological expression or is less responsive to the political variations implied by different personas. Notably, while the personas distribution remain remarkably consistent across models, they do not align with the models' default positions (white dots in Figure 1). This difference between models' default political positions and their persona-based stances indicates that using personas may trigger consistent political behavioral patterns, regardless of the models' initial biases. While the majority of personas align with left-libertarian tendencies, all four models demonstrate a minor presence in the right-authoritarian quadrant. This deviation from default positions indicates that, despite their biases, the models are capable of associating personas with politically diverse descriptors.\nPersonas manipulation. Figure 2 illustrates the results of injecting ideological descriptors like \"right authoritarian\" or \"left libertarian\" to the personas descriptions, while Table 2 provides a detailed statistical analysis of the observed shifts. When personas were prompted to adopt \"right-authoritarian\" descriptor, all models exhibited significant vertical (authoritarian-libertarian) and horizontal (left-right) shifts. Notably, the shifts in the authoritarian dimension were consistently larger across most of the models. This suggests a higher susceptibility to ideological repositioning along the vertical axis compared to the horizontal axis. The only exception was Zephyr, which showed greater movement along the horizontal axis ($\\Delta \\mu_x = 1.60$) compared to the vertical one ($\\Delta \\mu_y = 1.13$).\nAmong the models, Llama displayed the most substantial overall movement ($\\Delta \\mu_x = 2.19$, $\\Delta \\mu_y = 3.20$), indicating that it is particularly sensitive to ideological manipulation. In contrast, Zephyr demonstrated the most resistance, with relatively modest changes ($\\Delta \\mu_x = 1.60$, $\\Delta \\mu_y = 1.13$). The pronounced vertical and horizontal movements align with the idea that right-authoritarian ideology contrasts sharply with the left-libertarian bias intrinsic to LLMs, as evidenced in their baseline distributions (Figure 1), thus facilitating more distinct repositioning relative to the original placement.\nFor the \"left-libertarian\" condition, the shifts were generally smaller in magnitude. Llama and Qwen showed minimal movement on the horizontal axis ($\\Delta \\mu_x = 0.16$ and -0.15, respectively) and moderate changes on the vertical axis ($\\Delta \\mu_y = 0.68$ and 0.47). This discrepancy might stem from the models' pre-existing left-libertarian inclinations, making further adjustments less pronounced. This asymmetry may reflect biases in pretraining data, where left-libertarian narratives are more prevalent, leading to stronger internal representations of these ideologies. Additionally, the directional differences in responsiveness between left-right shifts versus authoritarian-libertarian shifts reveal that LLMs' internal conceptualization of political ideologies may not fully align with the political compass framework."}, {"title": "5 CONCLUSION", "content": "In this study we investigated how adaptable LLMs are when impersonating certain profiles that may carry certain political ideologies. Our experimental results show that persona descriptions significantly influence LLMs' political compass placements, with models exhibiting varying degrees of malleability to both implicit persona characteristics and explicit ideological prompting. This has important implications for understanding and controlling political bias in language models.\nLimitations. While the PCT provides valuable insights, it represents just one possible measure of political orientation. Alternative frameworks, like the 8values test, could provide additional dimensions beyond the economic and social axes of the PCT, offering a more nuanced political positioning across multiple value spectrums. Additionally, models' internal conceptualizations of political descriptors may not perfectly align with PCT definitions, potentially affecting our results' interpretation. While we recognize this limitation, our study's primary focus is not on evaluating whether language models accurately map personas to their real-world demographic political leanings. Instead, we aim to analyze broader patterns in the distribution of responses and examine the range of variability that different personas can elicit from these models.\nFuture studies. Looking forward, we identify three promising research directions: i) extending this analysis to larger models (70B+ parameters) could reveal how model scale affects political malleability; ii) deeper investigation of persona-specific biases could uncover important patterns in how models stereotype different roles and professions; and iii) these insights could inform techniques for de-biasing LLMs toward more neutral political orientations, potentially improving their utility across diverse applications."}]}