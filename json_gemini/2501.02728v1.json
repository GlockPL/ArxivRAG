{"title": "OpenGU: A Comprehensive Benchmark for Graph Unlearning", "authors": ["Bowen Fan", "Yuming Ai", "Xunkai Li", "Zhilin Guo", "Rong-Hua Li", "Guoren Wang"], "abstract": "Graph Machine Learning is essential for understanding and analyzing relational data. However, privacy-sensitive applications demand the ability to efficiently remove sensitive information from trained graph neural networks (GNNs), avoiding the unnecessary time and space overhead caused by retraining models from scratch. To address this issue, Graph Unlearning (GU) has emerged as a critical solution, with the potential to support dynamic graph updates in data management systems and enable scalable unlearning in distributed data systems while ensuring privacy compliance. Unlike machine unlearning in computer vision or other fields, GU faces unique difficulties due to the non-Euclidean nature of graph data and the recursive message-passing mechanism of GNNs. Additionally, the diversity of downstream tasks and the complexity of unlearning requests further amplify these challenges. Despite the proliferation of diverse GU strategies, the absence of a benchmark providing fair comparisons for GU, and the limited flexibility in combining downstream tasks and unlearning requests, have yielded inconsistencies in evaluations, hindering the development of this domain. To fill this gap, we present OpenGU, the first GU benchmark, where 16 SOTA GU algorithms and 37 multi-domain datasets are integrated, enabling various downstream tasks with 13 GNN backbones when responding to flexible unlearning requests. Based on this unified benchmark framework, we are able to provide a comprehensive and fair evaluation for GU. Through extensive experimentation, we have drawn 8 crucial conclusions about existing GU methods, while also gaining valuable insights into their limitations, shedding light on potential avenues for future research.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs are versatile mathematical structures that represent com- plex interactions and relationships between entities, offering an abstract yet intuitive framework for modeling real-world systems. In the context of database systems, graphs provide a powerful means to represent and analyze relational data, enabling insights into structured information. To effectively capture the rich and in- terconnected information inherent in graph data, GNNs [24, 48, 63] have emerged as transformative tools, achieving remarkable success in optimizing database query performance [4, 6, 22, 59], enhancing data management [2, 3], and supporting other fields such as social networks [42, 72], recommendation systems [9, 31, 62], biological networks [27, 55, 94] and data mining [35, 57]. Nowadays, the majority of machine learning paradigms are pri- marily driven by data for the acquisition of knowledge, fundamen- tally transforming decision-making processes across various fields [5, 78, 92]. However, the indiscriminate use of data has intensified the conflict between data rights and user privacy [46, 66, 73]. In response to these pressing issues, a range of pioneering regulatory frameworks has been proposed, including the European Union's General Data Protection Regulation (GDPR) [56], and the Califor- nia Consumer Privacy Act (CCPA) [51] in California. Among these regulations, one of the most critical and contentious provisions is the right to be forgotten [38]. To technically align the effectiveness of machine learning with data rights and privacy regulations, a revolutionary frontier-machine unlearning [10, 79] has emerged. Despite advances in traditional machine unlearning for indepen- dently distributed data, the growing reliance on graph-structured data in various fields like database systems and data mining high- lights the significance of GU. As graphs underpin relational schemas and mined patterns, GU enables selective removal of sensitive infor- mation while preserving the integrity of analytical results. Unlike other domains, removing specific information from a graph re- quires not only deleting or modifying individual nodes or edges but also needs to consider how these changes ripple through the entire graph. In addition, GU is jointly determined by downstream tasks and unlearning requests. Downstream tasks include node classification, link prediction, and graph classification (See Sec. 2.2), while unlearning requests span multiple levels, such as node, edge, and feature (See Sec. 2.3). The interplay of these two aspects makes GU even more complex. These challenges position GU as a par- ticularly demanding area of research, requiring tailored solutions for its structural complexities and task-specific needs. Recently, a"}, {"title": "2 DEFINITIONS AND BACKGROUND", "content": "In this section, we will briefly review several key concepts and definitions to better explain the fundamentals of GU. Generally, we define a graph as \\(G = (V, &, X)\\), where V represents a set of nodes with \\(|V| = n\\), and & denotes the edge set containing \\(|8| = m\\) edges. The feature matrix, \\(X \u2208 R^{n\u00d7d}\\), represents the feature vectors of all nodes, and d represents the dimension of node features. We also define \\(Y = {Y_1, Y_2, ..., Y_n}\\) as the label set, where each \\(y_i\\) corresponds to the node \\(v_i \u2208 V\\) in a one-to-one manner. Apart from the aforementioned attributes, the graph is also characterized by its adjacency matrix \\(A \u2208 R^{n\u00d7n}\\), where each element \\(A_{ij}\\) indicates the edge between nodes i and j. Formally, we define M as the original model trained on a complete graph G or a graph dataset denoted as \\(G = {G_1, G_2, ..., G_z}\\) with the corresponding labels \\(L = {l_1, l_2, ..., l_z }\\)."}, {"title": "2.1 Graph Neural Networks", "content": "In this part, we mainly focus on Message Passing Neural Networks, a widely adopted paradigm within GNNs for learning graph data. These models leverage the structural properties of graphs to learn node representations effectively. Broadly, such GNNs can be defined through three key components: initialization, aggregation, and update, offering a unified framework to capture relational de- pendencies in graphs.\n1.Initialization. To initialize node representations in a graph, each node is typically assigned an embedding based on its features or attributes. This process can be formalized as:\n\\(h_v^0 = \\chi_v\\ \\forall v \\in V.\\)\n(1)\n2.Aggregation. To capture information from the neighborhood of node u, an aggregation function \\(f_{Agg}^{(k-1)}\\) is applied to combine the embeddings of N(u) from the previous iteration. Common ag- gregators include permutation-invariant functions like sum, mean, or max, ensuring that the aggregated result is independent of the order of neighbors. This operation can be expressed as:\n\\(m_u^{(k)} = f_{Agg}^{(k-1)}(\\{h_v^{(k-1)}, v \\in N(u)\\})\\).\n(2)\n3.Update. This process enables each node to refine its represen- tation using \\(f_{Up}^{(k-1)}\\) which combines the node's embedding and the neighborhood's aggregated message. Specifically, the updated embedding for node u is computed as follows:\n\\(h_u^{(k)} = f_{Up}^{(k-1)}(h_u^{(k-1)}, m_u^{(k-1)})\\).\n(3)\nThe combination of initialization, aggregation, and update func- tions forms the foundation of GNNs. Through iterative refinement, node embeddings progressively incorporate structural and feature information from their local neighborhoods, enabling GNNs to learn rich and expressive representations for downstream tasks."}, {"title": "2.2 Downstream Tasks", "content": "Building upon the iterative process of node representation, the resulting node embeddings \\(h_v^{(k)}\\) at the final iteration k serve as the foundation for downstream tasks. These refined embeddings are rich in information and capture both local and global graph structures, making them highly suitable for various graph-based prediction tasks. The downstream tasks typically involve node classification, link prediction, and graph classification, each utilizing these node embeddings in different ways.\nNode classification aims to assign labels to individual nodes in the graph based on their features and structural context. The learned node representations \\(h_v^{(k)}\\) are fed into a classifier, often a fully connected layer, to predict node labels. For instance, in financial transaction networks, this task can help determine whether a given transaction is fraudulent or not.\nLink prediction seeks to predict the existence of edges between nodes using their embeddings and graph structure. This is critical in tasks like drug-drug interaction prediction in biomedical net- works, where nodes represent drugs, and edges indicate known interactions. Predicting potential edges can aid in discovering new therapeutic drug combinations.\nGraph classification predicts a label for an entire graph by ag- gregating node embeddings \\(h_v^{(k)}\\) via a readout function \\(f_{Read}\\) (e.g., sum or mean). In chemical compound classification, each molecule can be represented as a graph, where nodes correspond to atoms, and edges represent chemical bonds. Graph classification can help predict whether a molecule is toxic or not."}, {"title": "2.3 Unlearning Requests", "content": "When receiving unlearning request \\(\\Delta G = \\{\\Delta V, \\Delta E, \\Delta X\\}\\), the re- trained model M is trained from scratch on the pruned graph after the deletion and the unlearning model M' updates its parameters from original W to W' based on the unlearning algorithm. The goal of GU is to minimize the discrepancy between M' and M. Common unlearning requests in graph unlearning typically fall into three cat- egories: node-level \\(\\Delta G = \\{\\Delta V, \\O, \\O\\}\\), edge-level \\(\\Delta G = \\{\\O, \\Delta E, \\O\\}\\), and feature-level \\(\\Delta G = \\{\\O, \\O, \\Delta X\\}\\).\nNode unlearning targets the removal of individual nodes, facili- tating the precise deletion of specific data points, such as individual users or entities, while preserving the broader structure of the graph. In social network platforms, if users (nodes) decide to delete their accounts, node unlearning ensures that the model no longer uses their interactions or behaviors to influence recommendations or predictions, safeguarding user privacy. Edge unlearning, on the other hand, targets the removal of relationships between nodes-an approach essential for privacy-sensitive applications where certain connections require erasure without altering node attributes. A key application of edge unlearning is in financial fraud detection, where certain transactional relationships (edges) between users may need to be erased if they are identified as erroneous or compromised. Feature unlearning, meanwhile, involves the elimination of specific node features, facilitating controlled deletion of attribute-based information linked to individual nodes. For example, in health- care, sensitive medical attributes can be excluded to comply with regulations while retaining the utility of remaining features."}, {"title": "2.4 GU Taxonomy", "content": "To provide a comprehensive understanding of GU, we categorize existing methodologies into five types based on their operational frameworks: (1) Partition-based algorithms, (2) Influence Function- based (IF-based) unlearning algorithms, (3) Learning-based algo- rithms, (4) Projection-based algorithms, and (5) Structure-based algorithms. In Partition-based methods, GraphEraser [15], GUIDE [70], and GraphRevoker [89] draw inspiration from SISA [7] to implement different partitioning strategies, enabling training and unlearning on independent shards. In IF-based methods, GIF [75], CGU [18], CEU [76], and others [26, 50, 86] leverage rigorous math- ematical formulations to quantify the impact of data removal on model, allowing for efficient model updates. In Learning-based al- gorithms methods, GNNDelete [16], MEGU [43], SGU [], and others [61, 83, 91] achieve a trade-off between forgetting and reasoning with specialized loss functions. As for Projection-based algorithm, Projector [21] adapts to unlearning by orthogonally projecting the weights into a different subspace. Finally, Structure-based method UtU [65] manipulate the graph structure directly without the ex- tensive retraining or complex optimizations. This categorization provides a structured landscape of GU, as illustrated in Figure 1."}, {"title": "3 BENCHMARK DESIGN", "content": "In this section, we present a comprehensive overview of OpenGU, emphasizing the key aspects of the benchmark design, as outlined in Table 1. First, we detail the datasets and the associated prepro- cessing techniques (Sec. 3.1), followed by an examination of the various GU methods (Sec. 3.2). Lastly, we outline the evaluation metrics and experimental configurations (Sec. 3.3) that structure the benchmarking process, offering the core principles and the holistic view of our OpenGU framework."}, {"title": "3.1 Dataset Overview for OpenGU", "content": "GU scenarios are fundamentally data-driven, making the meticulous selection of datasets indispensable for evaluating the GU strategies. To assess GU methods for node or edge-related tasks, we have care- fully selected 19 datasets [36]. Citation Networks include Cora, Cite- seer, and PubMed [84], while Co-author Networks are represented by CS and Physics [60]. Image Networks feature Flickr [88], while E-commerce and Product Networks incorporate Photo, Computers [60], ogbn-products [34], and Amazon-ratings [53], capturing con- sumer and product interactions. Scientific Networks leverage DBLP [87] and ogbn-arxiv [34] to reflect publication patterns. To further broaden the scope, we add Squirrel and Chameleon [52], reflect- ing webpage networks; Actor [52], focusing on film connections; and digital engagement data such as Minesweeper [53] for online gaming, and Tolokers [53], from a crowdsourcing platform where edges signify task collaborations. Additionally, historical and social Q&A contexts are represented by Roman-empire and Questions [53], respectively, enriching our dataset diversity.\nConsidering the scenario of graph classification tasks, OpenGU includes 18 datasets spanning various domains, Specifically, the compounds networks (MUTAG, PTC-MR, BZR, COX2, DHFR, AIDS, NCI1, ogbg-molhiv and ogbg-molpcba) [23, 33, 34, 58, 64, 69], focus on molecular structures and chemical properties; the protein net- works (ENZYMES, DD, PROTEINS and ogbg-ppa) [11, 25, 32, 34] are concerned with biology data; the movie networks (IMDB-BINARY, IMDB-MULTI) [82] and collaboration networks (COLLAB) [39] pertain to social media and community structures; additionally, ShapeNet [85] and MNISTSuperPixels [49] represent tasks related to 3D shapes and image superpixels. To provide a clearer under- standing of the datasets, a detailed overview is showed in Tables 2 and 3 and further details about datasets can be found in [1] (\u0391.1).\nIn terms of data preprocessing, our work introduces several key enhancements to address existing limitations in splitting, inference, and special scenes with noise or sparsity, making OpenGU more adaptable and comprehensive. GU methods currently lack a unified approach for dataset splitting: for instance, GraphEraser applies an 80%/20% training-test split, whereas GIF uses 90%/10%. Although some datasets provide default splitting, these fixed ratios can limit the flexibility needed for diverse experimentation. To achieve a standardized and versatile splitting in OpenGU, we implemented code that allows arbitrary dataset split ratios, enabling researchers"}, {"title": "3.2 Algorithm Framework for OpenGU", "content": "GNN Backbones. To evaluate the generalizability of GU algo- rithms, we incorporate three predominant paradigms of GNNS within our benchmark: traditional GNNs, sampling GNNs, and decoupled GNNs. For the traditional GNNs, we implement widely- recognized models such as GCN [36], GAT [68], GIN [80] and others [8, 13, 31]. Sampling GNNs include GraphSAGE [30], GraphSAINT [88] and Cluster-gcn [17]. Additionally, to accommodate GU meth- ods which rely on linear-GNN, we further incorporate scalable, decoupled GNN models into the benchmark, specifically SGC [74], SSGC [93], SIGN [28] and APPNP [37]. These models offer scalabil- ity advantages and efficient decoupling for handling larger datasets and supporting diverse GU methods. More detailed descriptions of these GNN backbones are provided in [1] (A.2).\nGU Algorithms. For GU algorithms, our framework encompasses 16 methods, each meticulously reproduced based on source code or detailed descriptions in the relevant publications. The detailed de- scriptions can be found in [1] (A.3). Moreover, we deliver a unified interface for GU methods, merging them under a cohesive API to facilitate easier access, experimentation, and future expansion. By standardizing these methods within OpenGU, we provide a stream- lined and highly efficient platform for researchers and practitioners to conduct robust, reliable, and reproducible benchmarking studies."}, {"title": "3.3 Evaluation Strategy for OpenGU", "content": "To assess GU algorithms in diverse real-world scenarios, our bench- mark evaluation spans three critical dimensions tailored to GU contexts: effectiveness, efficiency, and robustness. Each dimension includes tailored evaluation methods reflecting OpenGU's mission to serve as a flexible, high-standard benchmark.\nCross-over Design. In previous GU studies, node and feature unlearning typically align with node classification tasks, while edge unlearning is often evaluated in the context of link prediction. However, real-world applications frequently demand the removal of data in scenarios where unlearning requests and downstream tasks intersect. To address this gap, we designed cross-task evaluations in OpenGU, allowing us to measure GU algorithm performance in more complex, realistic scenarios where different unlearning types may apply across diverse downstream tasks. This approach provides a comprehensive and practical evaluation framework to assess the flexibility of GU algorithms in real-world applications.\nEffectiveness. For the effectiveness of algorithms within OpenGU, we conduct evaluations tailored to key downstream tasks while specifically examining GU performance on retained data. We lever- age F1-score, AUC-ROC, and Accuracy to evaluate the model's predictive performance at the node, edge, and graph levels, respec- tively. To evaluate unlearning effects more rigorously, we incorpo- rate membership inference attack (MIA) and poisoning attack (PA). MIA examines whether specific nodes are in the training set, with an AUC-ROC close to 0.5 indicating effective unlearning. Poisoning attack, on the other hand, degrades model predictions by introduc- ing mismatched edges. Improved link prediction after removing these edges validates the algorithm's ability to erase unwanted relationships effectively. This multi-faceted approach provides a thorough assessment of GU effectiveness, ensuring comprehensive evaluation of both retained and unlearned information. Detailed explanation of metrics and attacks is provided in [1] (A.4 and A.5)."}, {"title": "4 EXPERIMENTS AND ANALYSES", "content": "In this section, we delve into a series of targeted experiments de- signed to rigorously evaluate the effectiveness, efficiency, and ro- bustness of GU algorithms within OpenGU. By posing key ques- tions, we aim to uncover insights into how these algorithms respond to diverse unlearning scenarios, data complexities, and practical deployment challenges, ultimately providing a comprehensive un- derstanding of their efficacy. More detailed information about the experimental setting is presented in [1] (A.6).\nFor effectiveness, Q1: How effective are GU algorithms in pre- dicting retained data under different unlearning requests? Q2: Do existing GU strategies achieve forgetting in response to unlearn- ing requests? Q3: Do current GU algorithms effectively balance the trade-off between forgetting and reasoning? For efficiency, Q4: How do GU algorithms perform in terms of space and time complex- ity theoretically? Q5: How do the GU algorithms perform regarding space and time consumption in practical scenarios? For robust- ness, Q6: As the intensity of forgetting requests increases, can the GU algorithms still maintain their original performance levels? Q7: How do GU algorithms perform under sparse and noisy settings?"}, {"title": "4.1 Reasoning Performance Comparison", "content": "To address Q1, we conducted a comprehensive comparison and analysis of existing GU methods across three representative combi- nations of downstream tasks and unlearning requests. For clarity, we denote these combinations as downstream task-unlearning re- quest (e.g. node-node). In the subsequent sections, we will conduct an in-depth comparison and analysis of GU algorithms centered around a meticulously designed experimental setup, ultimately deriving insightful conclusions.\nPrevious studies on GU have often employed varying dataset splits, different GNN backbones, and inconsistent unlearning re- quest configurations, hindering direct comparisons between differ- ent methods. To begin with, we outline the unified experimental setup employed in our study. We design tasks across three levels (i,e, node, edge, and graph) using datasets split into 80% for training and"}, {"title": "4.2 Forgetting Performance Comparison", "content": "To address Q2, we evaluate whether GU algorithms effectively forget the unlearning entity by employing commonly used attack strategies in the GU domain, including Membership Inference At- tack and Poisoning Attack. These assessments are conducted from both node and edge perspectives to determine whether existing GU methods can genuinely prevent information leakage and protect privacy. The basic experimental setup is the same as Q1."}, {"title": "4.3 Trade-off between Forgetting and Reasoning", "content": "To address Q3, we synthesize the insights from the previous two questions and adopt a unified perspective to analyze their inter- play. Due to space constraints, we focus on the node classification where most GU methods demonstrate strong performance, and utilize SGC as the backbone for consistency across comparisons. To provide a more comprehensive and precise evaluation of the progress achieved by these methods, present the performance of the GU algorithms on Cora, Citeseer, and PubMed respectively. Ad- ditionally, we calculate the average performance of the algorithm across nine datasets (consistent with Q1). The summarized results are illustrated in Figure 4, offering a clear overview of the relative performance of different methods in balancing forgetting and rea- soning. The methods that are not shown in the figure indicate their AUC exceeds the right boundary.\nGiven that the most reasonable result of MIA is 0.5, GU methods which are positioned closer to the red centerline and higher up on the graph exhibit superior overall performance. From a vertical"}, {"title": "4.4 Algorithm Complexity Analyses", "content": "To answer Q4 about how the GU algorithms perform in terms of time and space complexity, we analyze the time complexity from the perspectives of preprocessing, training, unlearning, and infer- ence. Since partition-based methods are unique, their partitioning is included in the preprocessing, while aggregation is incorporated into inference. Additionally, we provide a comprehensive summary of the space complexity for all methods, encompassing the required memory for model parameters, data storage, and auxiliary struc- tures, ensuring a thorough evaluation. Except for the methods that can only act on edge-level, we take node unlearning as the perspec- tive of analysis.\nFor clarity in the complexity analysis, we introduce a set of key notations that will be used consistently throughout the discussion. To ensure a uniform evaluation of complexity, we consider GCN as the backbone model, with the number of layers denoted by L. In terms of the dataset, n represents the total number of nodes, m refers to the number of edges, f is the feature dimension of the nodes, and d denotes the average degree of the nodes. The number of classes is denoted by c, while u signifies the number of unlearning requests, representing the nodes, edges or features to be deleted during the unlearning process. Since part of the methods involve sampling, we define the number of samples as ns."}, {"title": "4.5 Practical Efficiency Analyses", "content": "In addressing Q5, we evaluated the time and memory overhead of GU algorithms under a 10% node unlearning request on a range of datasets, with a focus on the node classification. As depicted in Figure 5, we selected four datasets of increasing scale, from the small Cora dataset with thousands of nodes to the large-scale ogbn- products dataset with millions of nodes, to ensure a fair compari- son in terms of time cost and scalability. Partition-based methods display substantial time overhead on account of partitioning, ag- gregation, and shard training operations, whereas most IF-based and Learning-based methods exhibit greater temporal efficiency. On large-scale datasets like ogbn-products, only 6 GU methods were able to run, with comparable performance, while others faced challenges such as timeouts or memory overflows. Regarding mem- ory usage in Figure 6, GUIDE consistently exhibits low memory overhead across various datasets, while GraphEraser (without vi- sualization) incurs considerably higher costs, exceeding 4000MB even on smallest dataset Actor. Notably, methods that are capable of handling large datasets, such as ScaleGUN and SGU, maintain stable memory consumption across all datasets, thereby demonstrating their scalability."}, {"title": "4.6 Impact of Unlearning Intensity", "content": "To address Q6, we conducted node unlearning experiments on Cora and ogbn-arxiv and edge unlearning experiments on Citeseer and Chameleon, employing various backbones for the node classifica- tion task. The unlearning ratio was incrementally increased from 0 to 0.5, and the results are presented in Figure 7. The illustration reveals a consistent downward trend in performance for all GU methods as the unlearning ratio increases, indicating that higher deletion intensities negatively impact prediction capabilities. No- tably, methods such as Projector, GIF, and CEU exhibit greater sensitivity to changes in certain datasets, while Learning-based methods demonstrate a more gradual decline, highlighting their robustness under higher unlearning intensities. However, we also"}, {"title": "4.7 Robustness Analyses", "content": "To address Q7, we simulate more realistic noise and sparsity sce- narios by introducing perturbations at both the label and feature levels to comprehensively evaluate the robustness of existing GU methods. For label noise, a certain proportion of training samples are randomly assigned incorrect labels, while for feature noise, Gaussian noise is injected based on the dimensionality of node features. Sparsity is introduced by varying the proportion of train- ing nodes and simulating partial feature absence. Given the large number of GU methods, we select representative approaches for analysis based on their categories, using the same settings as the node-node experiments in Q1 and adopting SSGC as the backbone. The results, presented in Figure 8, reveal that nearly all methods experience a decline in predictive performance under the influence of noise and sparsity, with label noise exerting the most pronounced impact. Specifically, CGU and Projector exhibit a pattern of slow initial degradation followed by a rapid decline, while other methods demonstrate a steady and sharp drop. When the noise ratio reaches"}, {"title": "5 CONCLUSION AND FUTURE DIRECTIONS", "content": "In this paper, we first provide a comprehensive review of the cur- rent advancements in the field of Graph Unlearning, highlighting its practical applications and systematically categorizing existing algorithms based on their technical characteristics. Building on this foundation, we introduce OpenGU, the first unified and compre- hensive benchmark for GU, which integrates 16 state-of-the-art GU algorithms and 37 datasets across multiple domains. OpenGU further extends the flexibility of GU algorithms, allowing seamless combinations across three downstream tasks and three types of un- learning requests. Using the standardized and fair evaluation frame- work provided by OpenGU, we conduct extensive experiments to assess the advancements of GU methods from the perspectives of effectiveness, efficiency, and robustness. These experiments not only reveal the notable breakthroughs in the field but also expose critical limitations in existing approaches. To inspire further re- search, we outline the major challenges currently faced by GU and propose promising directions for future exploration.\nDesigning Generalized GU Frameworks for Diverse Tasks (C1 and C2). While some existing methods demonstrate strong predictive performance for specific tasks, their underlying princi- ples often make it difficult to adapt to varying downstream tasks and unlearning requests. Furthermore, the current design of un- learning requests remains relatively simplistic, whereas real-world applications often require handling mixed unlearning requests or subgraph-level unlearning. Achieving consistently superior predic- tive capabilities across such complex and varied scenarios remains a considerable challenge. Therefore, developing more generalized frameworks becomes imperative.\nUnified Metrics for Evaluating Forgetting (C3 and C4). The current methods for assessing the forgetting capability of GU ap- proaches remain insufficient and leave significant gaps to be ad- dressed. These methods are often tightly coupled with specific unlearning requests and downstream tasks, making them less ef- fective in handling diverse combinations of scenarios. Moreover, determining whether the information targeted for removal has been thoroughly unlearned from theoretical perspective remains a critical yet underexplored challenge. Furthermore, GU algorithms must strike a balance between reasoning and forgetting. Future re- search should move beyond the current paradigm of independently assessing these two aspects, striving instead for a unified metric that evaluates models from an integrated perspective, ensuring a comprehensive understanding of their capabilities.\nEnhancing Algorithm Efficiency (C5 and C6). While theoreti- cal analysis provides valuable insights, the practical performance of current GU methods often falls short, especially when scaling to large datasets with millions of nodes. These methods commonly en- counter OOT or OOM issues. To enable GU's effective deployment in large-scale scenarios, algorithms must be optimized for both efficiency and scalability, ensuring they can handle the demands of real-world data while avoiding these performance bottlenecks.\nAddressing Realistic Scenarios (C7 and C8). In practical ap- plications, the presence of noise and incomplete datasets is an unavoidable challenge. However, current GU algorithms lack suffi- cient exploration and adaptation to such scenarios. Experimental results highlight significant weaknesses when dealing with noise and sparsity, particularly in terms of label and feature robustness. Future research should aim to broaden the scope of investigation, extending robustness analysis to encompass a wider range of real-world challenges and data imperfections.\nOpenGU embodies the collective efforts and expertise of the current GU field, offering a comprehensive platform for research and development. As we move forward, we will continue to ex- pand and enhance OpenGU, strengthening its support for future advancements in this area. Finally, we welcome feedback and en- courage suggestions to refine our benchmark, further improving its effectiveness and user experience."}, {"title": "A.4 Evaluation Metric Details", "content": "Our OpenGU includes three downstream tasks in total: node clas- sification, link prediction, and graph classification. These three downstream tasks are evaluated using F1-score, ROC-AUC, and accuracy Acc, respectively.\nF1-score, a metric used to evaluate classification problems, is the harmonic mean of precision and recall. In node classification tasks, there may be an imbalance in categories, that is, the number of samples in some categories is much larger than that in other categories. In this case, using accuracy alone may lead to misjudg- ment of model performance. F1-score can more comprehensively reflect the performance of the model in each category by considering both precision and recall, especially when dealing with data with imbalanced categories.\nROC-AUC, the area under the ROC curve, is used to evaluate the performance of a classification model. AUC values range from 0 to 1, with larger values indicating better performance. In link prediction tasks, the AUC value measures the model's ability to predict whether there is a link between node pairs. A higher AUC value indicates that the model is more effective in distinguishing nodes that actually have links from node pairs that do not.\nAccuracy, one of the basic indicators for measuring model per- formance. In graph classification tasks, accuracy is the most intu- itive evaluation indicator and is easy to understand and calculate.\nPrecision, a crucial indicator for model performance assessment. In some tasks, it shows the proportion of correctly predicted positive instances among all predicted positives. It is a significant metric that helps to understand the exactness of the model's predictions."}, {"title": "A.5 Attack Details", "content": "We implemented two attack strategies, Membership Inference At- tack and Poison Attack, and evaluated them respectively.\nMembership Inference Attack, a privacy attack technique against machine learning models in which the attacker attempts to determine whether a specific data record was previously used to train the machine learning model. We use Membership Infer- ence Attack to gauge the efficacy of unlearning by quantifying the probability ratio of unlearned part presence before and after the unlearning process.\nPoisoning Attack, a security threat against machine learning models. Its main purpose is to disrupt the model training process by contaminating the training data during the model training phase. Add heterogeneous edges to the original data as negative samples, and judge the quality of the forgetting effect by comparing the effects of the model before and after forgetting the heterogeneous edges."}, {"title": "A.6 Experimental Setting Details", "content": "All experiments were conducted on a system equipped with an NVIDIA A100 80GB PCIe GPU and an Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz, with CUDA Version 12.4 enabled. The software environment was set up with Python 3.8.0 and PyTorch 2.2.0 to en- sure optimal compatibility and performance for all GU algorithms. Additionally, hyperparameters for each GU algorithm were config- ured based on conclusions drawn from prior research to provide consistent and reliable results."}]}