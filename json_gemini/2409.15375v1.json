{"title": "DS2TA: Denoising Spiking Transformer with Attenuated Spatiotemporal Attention", "authors": ["Boxun Xu", "Hejia Geng", "Yuxuan Yin", "Peng Li"], "abstract": "Vision Transformers (ViT) are current high-performance models of choice for various vision applications. Recent developments have given rise to biologically inspired spiking transformers that thrive in ultra-low power operations on neuromorphic hardware, however, without fully unlocking the potential of spiking neural networks. We introduce DS2TA, a Denoising Spiking transformer with attenuated SpatioTemporal Attention, designed specifically for vision applications. DS2TA introduces a new spiking attenuated spatiotemporal attention mechanism that considers input firing correlations occurring in both time and space, thereby fully harnessing the computational power of spiking neurons at the core of the transformer architecture. Importantly, DS2TA facilitates parameter-efficient spatiotemporal attention computation without introducing extra weights. DS2TA employs efficient hashmap-based nonlinear spiking attention denoisers to enhance the robustness and expressive power of spiking attention maps. DS2TA demonstrates state-of-the-art performances on several widely adopted static image and dynamic neuromorphic datasets. Operated over 4 time steps, DS2TA achieves 94.92% top-1 accuracy on CIFAR10 and 77.47% top-1 accuracy on CIFAR100, as well as 79.1% and 94.44% on CIFAR10-DVS and DVS-Gesture using 10 time steps.", "sections": [{"title": "1 Introduction", "content": "Originally designed for natural language processing applications Vaswani et al. (2017), transformers have gained popularity in various computer vision tasks, including image classification Dosovitskiy et al. (2021), object detection (Carion et al., 2020), and semantic segmentation Xie et al. (2021). As a key mechanism of transformers, self-attention selectively focuses on relevant information, enabling capturing of long-range interdependent features.\nSpiking neural networks (SNNs) are more biologically plausible than their non-spiking artificial neural network (ANN) counterparts (Gerstner & Kistler, 2002). Notably, SNNs can harness powerful temporal coding, facilitate spatiotemporal computation based on binary activations, and achieve ultra-low energy dissipation on dedicated neuromorphic hardware (Furber et al., 2014; Davies et al., 2018; Lee et al., 2022). The recent emergence of spiking transformer architectures represents a logical progression (Zhou et al., 2023; Zhang et al., 2022; Zhu et al., 2023). Particularly, promising results have been demonstrated by the spiking transformers of Zhou et al. (2023), which incorporate a spike-based self-attention mechanism. This self attention captures correlations between spatial input patches occurring at the same time point, which we refer to as \"spatial-only\" attention.\nMotivated by the recent progress on spiking transformers, this work proposes a new architecture called Denoising Spiking transformer with Attenuated SpatioTemporal Attention (DS2TA). DS2TA enables fully-fledged spiking temporally attenuated spatiotemporal attention (TASA) as opposed to \"spatial-only\" attention of Zhou et al. (2023). TASA computes spiking queries, keys, values, and the final output of each attention block while taking into account correlations in input firing activities occurring in both time and space. Thus, it fully exploits the spatiotemporal computing power of spiking neurons for forming attentions, which are at the core of any transformer. Equally importantly, we facilitate parameter-efficient spatiotemporal attention computation and employ nonlinear denoising to enhance the robustness and expressive power of spiking attention mechanisms, thereby boosting the overall model's performance.\nThe proposed DS2TA spiking transformer introduces several key contributions:\nDeparture from \"Spatial-Only\" Attention: In contrast to existing spiking transformers that exhibit \"spatial-only\" attention, such as those by Zhou et al. (2023), DS2TA moves beyond this limitation. It introduces temporally attenuated spatiotemporal attention (TASA) that considers not only spatial but also temporal correlations in input firings when computing queries, keys, values and final output of each attention block, providing a more comprehensive approach to attention.\nParameter-Efficient Spatiotemporal Attention Computation: DS2TA is designed to facilitate parameter-efficient computation of spatiotemporal attention via a technique called Attenuated Tempo-ral Weight Replica. This approach dramatically reduces the number of temporally-dependent synaptic weights employed in TASA. This efficiency contributes to the overall optimization of the transformer, enhancing its scalability and resource utilization."}, {"title": "Nonlinear Spiking Attention Denoisers (NSAD)", "content": "DS2TA utilizes efficient hashmap-based nonlinear denoisers with learnable nonlinearity to enhance the robustness and expressive power of spatiotemporal attention maps, thereby further improving performance.\nExtensive experimental evaluations on various static image and dynamic neuromorphic datasets, consistently demonstrate the superior performance of the DS2TA architecture in comparison to prior spiking transformer approaches."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Spiking Neural Networks", "content": "In the brain, computations typically occur in a spatiotemporal fashion Buzsaki (2006). As a computational model inspired by the brain, spiking neural networks (SNNs) are well suited for processing spatiotemporal information Maass (1997). Contrary to traditional deep learning models in ANNs, which relay information between layers through continuous values, SNNs operate on binary spikes across multiple time steps. To address non-differentiability of spiking activities, activation-based (or surrogate gradient) (Zenke & Ganguli, 2018; Li et al., 2021) and timing-based backpropagation training methods Shrestha & Orchard (2018); Zhang & Li (2020); Yang et al. (2021) and their combination have been proposed (Kim et al., 2020). Fang et al. (2021b) enhanced spiking neural networks by incorporating learnable membrane time constants. Zheng et al. (2020) introduced a threshold-dependent batch normalization method (STBP-tdBN) for directly training deep SNNs. Duan et al. (2022) proposed Temporal Effective Batch Normalization (TEBN) for enhancing training efficiency by rescaling the presynaptic inputs with different weights at every timestep. Yao et al. (2021) introduced a Temporal-wise Attention SNNs (TA-SNN) model that proposes a method of assigning significance to frames during training to reduce timesteps on DVS datastreams."}, {"title": "2.2 Spiking Transformers", "content": "Transformers Vaswani et al. (2023) and their variants Han et al. (2023); Khan et al. (2022); Lin et al. (2022) have emerged as a powerful model in deep learning based on the non-spiking artificial neural network (ANN) implementation. However, spiking-based transformers have not been well explored. Several recent studies have undertaken investigations into transformer-based spiking neural networks for tasks such as image classification (Zhou et al., 2023), object tracking (Zhang et al., 2022), and the utilization of large language models (LLMs) (Zhu et al., 2023). Specifically, Zhang et al. (2022) introduced a non-spiking ANN based transformer designed to process spiking data generated by Dynamic Vision Sensors (DVS) cameras. On the other hand, Zhou et al. (2023) and Yao et al. (2023) have proposed a spiking vision transformer while incorporating only spatial and linear self-attention mechanisms. Wang et al. (2023) considered pairwise similarity between queries and keys in time and space. Zhu et al. (2023) developed an SNN-ANN fusion language model, integrating a transformer-based spiking encoder with an ANN-based GPT-2 decoder to enhance the operational efficiency of LLMs.\nHowever, while the aforementioned spiking transformer models have made significant contributions to the field, they have not fully explored the potential of spatiotemporal self-attention mechanisms or addressed the issues of noise suppression and nonlinear scaling of attention maps, which are central to the research presented in this work."}, {"title": "3 Method", "content": "The conventional non-spiking Vision Transformer (ViT) architecture (Dosovitskiy et al., 2021) consists of patch-splitting modules, encoder blocks, and linear classification heads. Each encoder block includes a self-attention layer and a multi-layer perceptron (MLP) layer. Self-attention empowers ViT to capture global dependencies among image patches, thereby enhancing feature representation (Katharopoulos et al., 2020)."}, {"title": "3.1 Temporal Attention Window", "content": "The spiking spatio-temproal attention is confined within a Temporal Attention Window (TAW) to limit computational complexity.\nSpecifically, the input to the query/key/value neuron at location (i, k) in block l is based upon the firing activations of D output neurons from the prior (l \u2013 1)-th block that fall under a given TAW TAW:\n$I_{ik}^{(l)}[t] = \\sum_{m=t-TAW+1}^{t} \\sum_{j=0}^{D} w_{(kj)}^{(l)}(m) s_j^{(l-1)}[m]$,\nwhere $w_{(kj)}^{(l)}(m)$ is the temporally-attenuated synaptic weight specifying the efficacy of a spike evoked by the j-th output neuron of block (l \u2013 1) m time-steps before on the neuron at location (i, k) in block l. With the above synaptic input, each query/key/value neuron is emulated by the following discretized leaky integrate-and-fire dynamics (Gerstner & Kistler, 2002):\n$V_{ik}^{(l)}[t] = (1 - s_{ik}^{(l)}[t-1])\\frac{V_{ik}^{(l)}[t-1]}{T_m} + I_{ik}^{(l)}[t]$,\n$s_{ik}^{(l)}[t] = H(V_{ik}^{(l)}[t] - 0)$."}, {"title": "3.1.2 Attenuated Temporal weight replica", "content": "The spatiotemporal attention in Eq. 1 involves Taw temporally-dependent weights $W_{(kj)(m)}$, m \u2208 [0..Taw] for a pair of presynaptic and postsynaptic neurons. We introduce a learnable scheme, called attenuated temporal weight replica, to reduce the number of temporally-dependent weights by a factor of Taw. This amounts to set $w W_{(kj)(m)}$, m \u2208 [1..Taw], to be a temporally decayed value of $w_{(kj)(0)}$:\n$W_{(kj)(m)}^{(l)} = 1(m > 0)w_{(kj)(0)}^{(l)} df (m)$.\nHere, as shown in Fig 2, df(m) is the decay factor for $w w_{(ik)(im)}^{(l)}$. We make all decay factors a power-of-two, which can be efficiently implemented by low-cost shift operations:\n$df (m) = 2^{-(\\lambda^{(l)})m}$,\nwhere $\\lambda^{(l)}$ is a layer-wise learnable integer decay exponent."}, {"title": "3.2 Nonlinear Spiking Attention Denoiser (NSAD)", "content": "In the attention layers of existing spiking transformers (Zhou et al., 2023), a timestep-wise spiking attention map is generated by multiplying the outputs of the query neuron array (Q) with those of the key neuron array (K). Each entry in this map corresponds to a pairing of query and key neurons, where a one-to-one spatial correspondence is maintained."}, {"title": "3.2.1 Denosing with Element-wise Nonlinear Transformation", "content": "Recognizing the central role of spiking attention maps, we propose a learnable hashmap-based Nonlinear Spiking Attention Denoiser (NSAD) to improve the overall transformer performance. NSAD serves the dual-purpose of denoising a given computed attention map, and equally importantly, introducing efficient element-wise nonlinear transformation to enhance expressive power.\nFirstly, it's important to note that a nonzero value in the attention map signifies the simultaneous activation of one or multiple query-key neuron pairs. However, the computed spike-based attention maps are not necessarily devoid of noise, and the existing spiking transformers lack efficient noise suppression mechanisms.\nSecondly, it has been shown that applying row or column-based nonlinear softmax operations to attention maps improves performance in ANN-based transformers. However, softmax induces exponential operations and non-local memory access and data summations, which are costly and not hardware-friendly (Dao et al., 2022)."}, {"title": "3.2.2 Learning the Nonlinear Spiking Attention Denoiser", "content": "The proposed nonlinear spiking attention denoiser (NSAD) offers an efficient solution to addressing the above issues via element-wise hashmap-based nonlinear transformation without non-local memory access and computation, as illustrated in Figure 4.\nEach head in a transformer with H heads may have unique focuses and parameter distribution Naseer et al. (2021). As such, we establish a small hashmap $AD^{<h>}$ with d = D/H, say d = 32, entries dedicated to each head h \u2208 [1..H]. Each entry in $AD^{<h>}$ is indexed (addressed) by a specific integer value falling within the range of possible attention values of 0 and d, i.e., $AD^{<h>}[s]$ specifies the integer value to which all entries with value s in the attention map associated with head h are transformed to. In other words, the specific (i, j) entry in head-h's attention map, i.e., $S^{<h>}[i, j]$ is transformed to the $AD^{<h>}[S^{<h>}[i, j]]$.\nSince NSAD produces nonlinear transformed denoised maps using simple integer-based lookups of small hashmaps, it is computationally efficient and hardware-friendly. For a block of 12-head attention, only 12 x 32 = 384 integer values need to be stored in the hashmaps while there are 1.77M block-level weight parameters. The complexity of computing a denoised attention map is O(N \u00d7 N) per head, which can also be easily parallelized on hardware. This is in sharp contrast to the overall complexity of $O(12ND\u00b2 + N2D)$ for the block."}, {"title": "4 Experiments", "content": "We assess the performance of our DS2TA spiking transformer by comparing it with existing SNN networks using various methods trained from scratch and the recent spiking transformer model employing spatial-only attention (Zhou et al., 2023) on several static image datasets such as CIFAR10 and CIFAR100 in Section 4.1, and dynamic neuromorphic datasets such as CIFAR10-DVS and DVS-Gesture in Section 4.2, commonly adopted for evaluating spiking neural networks. We perform our experiments using the Spikingjelly (Fang et al., 2023) frameworks supporting an activation-based gradient surrogate training for SNNs. We further make a sparsity analysis in Section 4.3 and ablation study in Section 4.4."}, {"title": "4.1 Results on Static Classification Datasets", "content": "CIFAR10/100 The CIFAR dataset (Krizhevsky, 2009) contains 50,000 training images and 10,000 testing images, assigned into 10 or 100 categories, respectively, and are commonly adopted for testing directly trained SNNs. The pixel resolution of each image is 32 \u00d7 32. A spiking patch splitting module consisting of 4 sequential Conv+BN+MaxPool blocks with gradually improving channels to embedding dimensions, and a relative position generator to patchifies each image into 64 tokens with each token packing 4\u00d74 pixels. In the experiment, we employ a batch size of 256 and adopt the AdamWKingma & Ba (2014) optimizer to train the baseline spikformer model and our DS2TA transformer over 300 epochs, and compare them with several prior SNN works. We employ a standard data augmentation method, such as random augmentation, mixup, or cutmix for a fair comparison. The learning rate is initialized to 0.001 with a cosine learning rate scheduler. We initialize denoising threshold $u_n$ = 3 in NSAD for all attention heads; we initialize temporally attenuated attention window size Taw = 3 for all neurons and decay exponent T = 4 for all layers.\nWe compare the performance of our DS2TA model on CIFAR-10 and CIFAR-100 with a baseline spiking transformer model and other CIFARNet, VGG-11 and ResNet-19 SNNs as shown in Table 1, where the * subscript highlights our reproduced results. The transformer's architecture incorporates 384-dimensional embedding and four encoders (ViT-4-384). DS2TA gains a significant improvement on top-1 accuracy on CIFAR10, improving the baseline spiking transformer's accuracy from 94.19% to 94.92% when executing over 4 time steps. DS2TA obtains a significant accuracy improvement of 0.50% or more over ResNet-19, and improves the accuracy by 2.38% of the ResNet-20 model with 5 timesteps. On CIFAR-100, based on the same ViT-4-384 architecture, DS2TA gains noticeable accuracy improvements over these SNNs and the baseline spiking transformer when executed over 4 time steps. Compared with the baseline spiking transformer model, DS2TA improves by 1.42% on top-1 accuracy; compared with ResNet-19 SNNs, it improves by 3.00% to 3.99% on different training methods, and it improves by 13.4% over ResNet-20 DietSNN."}, {"title": "4.2 Results on Dynamic Neuromorphic Datasets", "content": "CIFAR10-DVS and DVS-Gesture CIFAR10-DVS (Li et al., 2017) is a neuromorphic dataset containing dynamic spike streams captured by a dynamic vision sensor camera viewing moving images from the CIFAR10 datasets. It contains 9,000 training samples and 1,000 test samples. The Dynamic Vision Sensor (DVS) Gesture dataset Amir et al. (2017) consists of 11 gestures from multiple human subjects as seen through a dynamic vision sensor, an event-based camera that responds to localized changes in brightness. For images from the two neuromorphic datasets, the image size is 128\u00d7128, we adapt a 16\u00d716 patch size. We utilize a ViT-2-256 as the backbone, which indicates utilizing two blocks with an embedding dimension of 256. The attention head number for both datasets is set to 16. We employ an AdamW optimizer and a batch size of 256. The learning rate is 0.001 with a cosine decay scheduler. The initial denoising threshold $u_n$ is set to 2 in NSAD; the temporally attenuated attention window TAW is set to 3 for all 16 heads; the decay exponent Tis set to 2. Neuromorphic data augmentation is applied as in Li et al. (2022); Zhou et al. (2023). The training is performed over 200 epochs.\nThe classification performances of DS2TA and the spiking transformer baseline, as well as other state-of-the-art SNN models are shown in Table 2. DS2TA outperforms the existing SNN models under various settings. Compared with the baseline Spikformer transformer model with the same number of 2.58M parameters, it improves by 0.2% and 0.69% the top-1 accuracy on CIFAR10-DVS and DVS-Gesture, respectively. Compared with 12.63M ResNet-19 model, DS2TA improves by 1.1%, 4.5%, and 11.3% top-1 accuracy on CIFAR10-DVS using 2.58M parameters, respectively."}, {"title": "4.3 Sparsity, Efficiency and Energy Consumption", "content": "Thanks to our proposed NSAD, the sparsity of the attention map is improved significantly as shown in Figure 3, enabling an opportunity of efficient processing within the attention block. We evaluate the sparsity of spike-based attention maps of our baseline spiking transformer and DS2TA model across multiple encoder blocks in ViT-4-384 backbone (CIFAR-100) in Table 3. It's noticeable that the sparsity is improved by 22.73%, 21.00%, 19.16% and 15.95% across four attention maps when it goes from shallow to deep. The activations across many attention heads in DS2TA are extremely sparse, and have such opportunities to be pruned completely, introducing a structured sparsity. We build an energy consumption model by Energy = $E_{AC}$ \u00d7 #OPs \u00d7 (1 - sparsity) for attention-map related computations. The energy consumption for attention map-related computation is trimmed down by 88.3%, 90.6%, 91.1%, and 91.9%, for blocks ranging from shallow to deeper, respectively."}, {"title": "4.4 Ablation Study", "content": "Proposed key elements in DS2TA. We analyze the effect of each proposed key element in the DS2TA on CIFAR10 and CIFAR100 in Table 4. Overall, the proposed two key techniques can incrementally lead to further performance improvements of the spiking transformer backbones. Under the mode of only enabling NSAD, the top-1 accuracy is improved by 0.45% and 1.14% on CIFAR10 and CIFAR100, respectively; after adding TASA, the top-1 accuracy is incrementally improved by 0.28% and 0.28% on CIFAR10 and CIFAR100, respectively."}, {"title": "5 Conclusion", "content": "In this work, we introduce DS2TA, a denoising spiking transformer that incorporates parameter-efficient attenuated spatiotemporal attention. We place a particular emphasis on exploring spatiotemporal attention mechanisms within spiking transformers. The mechanism enables spiking transformers to seamlessly fuse spatiotemporal attention information. Furthermore, we introduce a non-linear denoising technique for attenuating the noise and enhancing expressive power of spiking attention maps based on the proposed multi-head hashmap-based denoising mechanism with little extra computational overhead. Moreover, denoising significantly improves sparsity in attention maps, improves computational efficiency, and reduces energy dissipation. Our DS2TA spiking transformer outperforms the current state-of-the-art SNN models on several image and neuromorphic datasets. We believe that the presented work provides a promising foundation for future research in the domain of computationally efficient high-performance SNN-based transformer models."}]}