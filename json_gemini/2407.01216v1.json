{"title": "Let Hybrid A* Path Planner Obey Traffic Rules:\nA Deep Reinforcement Learning-Based Planning Framework", "authors": ["Xibo Li", "Shruti Patel", "Christof B\u00fcskens"], "abstract": "Deep reinforcement learning (DRL) allows a sy-\nstem to interact with its environment and take actions by\ntraining an efficient policy that maximizes self-defined rewards.\nIn autonomous driving, it can be used as a strategy for high-\nlevel decision making, whereas low-level algorithms such as\nthe hybrid A* path planning have proven their ability to\nsolve the local trajectory planning problem. In this work, we\ncombine these two methods where the DRL makes high-level\ndecisions such as lane change commands. After obtaining the\nlane change command, the hybrid A* planner is able to generate\na collision-free trajectory to be executed by a model predictive\ncontroller (MPC). In addition, the DRL algorithm is able to\nkeep the lane change command consistent within a chosen time-\nperiod. Traffic rules are implemented using linear temporal\nlogic (LTL), which is then utilized as a reward function in DRL.\nFurthermore, we validate the proposed method on a real system\nto demonstrate its feasibility from simulation to implementation\non real hardware.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past few years, Advanced Driver Assistance\nSystems (ADAS) have improved road transportation safety\nand efficiency [1]. Autonomous driving, considered to be the\nnext step of ADAS, has also made great progress due to the\ndevelopment of sensor technology, control theory, and deep\nlearning [2]. Despite the advances in autonomous driving,\nwhen and how to choose the current driving behavior is\nstill quite a challenging task in complex driving scenarios\n[3]. Rule-based methods are usually implemented in order\nto solve straightforward scenarios such as lane change, lane\nmerging, emergency braking, and so on. However, these\nhand-crafted rules can be inefficient in dense traffic with\nmultiple moving vehicles.\nRecently, reinforcement learning (RL) has shown its po-\ntential in solving complicated tasks and achieving a better\nperformance than human experts while playing Atari games\n[4] and the game of Go [5]. Meanwhile, those RL methods\nare also extended in behavior planning in order to make\nfeasible decisions in different driving scenarios [6] [7].\nSimulation tools such as SUMO [8] and CARLA [9] are used\nto construct various driving scenarios in order to train the RL\nagent. Referring to real-world implementations, mainly end-\nto-end approaches have been applied [10] [11]. However,\nseveral challenges persist when applying RL methods to\nautonomous driving.\nFirstly, the RL policy trained in simulation is hard to\ndirectly apply in real-world situations because the simu-\nlation environment does not adequately mirror real-world\nconditions. Secondly, when the steering and acceleration are\ngenerated by end-to-end approaches, a reliable, smooth, and\ncomfortable trajectory is hard to ensure. Thirdly, the RL\nagent is not able to obey basic traffic rules. To address the\nfirst challenge, we propose an RL platform based on the\nEB Assist Automotive Data and Time-Triggered Framework\n(ADTF), commonly used in the automotive industry [12].\nThe RL agent is initially trained in the ADTF simulation\nenvironment and then directly implemented on an ADAS\nmodel car, as depicted in Fig. 1. To tackle the second challenge,\nwe propose a framework for combining deep reinforce-\nment learning with local trajectory planning. The proposed\nframework is structured into two components, high-level\nbehavior planning and low-level trajectory planning. High-\nlevel behavior planning employs DRL methods to generate\nlane change commands, while the hybrid A* planner serves\nas the low-level trajectory planner tasked with generating\nthe local trajectory [14]. Specifically, the Proximal Policy\nOptimization (PPO) [15] algorithm is implemented to gene-\nrate discrete lane change actions, aiding the local trajectory\nplanning module in generating reliable and efficient lane\nchange trajectories. As for the third challenge, the RL agent\nis trained to comply with traffic rules, which are encoded\nusing LTL and incorporated into the agent's reward function\n[16] [17]. This approach ensures that our agent obeys traffic\nrules when making decisions.\nConcretely, the main contributions of our work are sum-"}, {"title": "II. RELATED WORKS", "content": "A. Heuristic-based Behavior Planning\nHeuristic-based behavior planning methods attempt to\nsolve the behavior planning problem by developing a set of\nrules for various driving scenarios. For example, in [18], a\nrule-based behavioral layer with three sub-components: lane\ndriving, intersection handling, and goal selection, is desi-\ngned in order to make tactical decisions. Additionally, other\nheuristic methods such as decision trees [19] or finite state\nmachines [20] are also explored in other autonomous driving\nsystems. However, these methods are primarily employed to\ntackle limited, uncomplicated scenarios. As the complexity\nof driving situations increases, the effectiveness of hand-\ncrafted rules diminishes due to the challenges associated\nwith managing their growing complexity and tuning hyper-\nparameters.\nB. Learning-based Behavior Planning\nBy interacting with the environment, learning-based me-\nthods enable a system to execute actions based on pre-defined\nrewards. Deep Q Network (DQN) is first implemented to\nexecute reasonable lane change decisions across various\nsimulated environments [6] [7]. In addition to DQN, safe,\nefficient, and comfortable lane change decisions are executed\nusing PPO in mandatory lane change scenarios within SUMO\nSimulation [21], where in [22] safe lane change decisions\nare ensured by designing an additional safety layer between\nthe agent and the environment. By combining RL with\nImitation Learning, [23] employs bird's eye view images\nas the state input and achieves impressive performance for\ndifferent traffic scenarios such as the stop signs, lane chan-\nging, merging, and US-style junctions in CARLA Simulati-\non. Another imitation learning method, Adversarial Inverse\nReinforcement Learning (AIRL) is utilized in [24] in order\nto learn both a behavioral policy and a reward function. For\nthe autonomous overtaking problem in high-speed situations,"}, {"title": "III. PROBLEM DESCRIPTION", "content": "A lane change action in road traffic can occur either\nmandatorily or discretely. In the mandatory scenario, the\ndecision is typically made when exiting the highway or\nmerging from multiple lanes into a single lane. These si-\ntuations involve a high volume of vehicles with a complex\ntraffic flow. To simplify the scenario, we initially demonstrate\nit with two vehicles before expanding to more complex\nsituations. Therefore, in this paper, we primarily focus on\nthe discretionary lane change scenario, which is suitable for\ntesting with two ADAS model cars.\nHere we formulate the discretionary lane change scenario\nas a partially observed Markov decision process [28] defined\nby a tuple < S,A,O,T,E,R, \u03b3 >. The state, action and\nobservation spaces are represented by S, A and O, where\nthe state vector at timestep t is denoted as \\(S_t \u2208 S\\), the action\ntaken by the ego agent at timestep t is defined as \\(a_t \u2208 A\\)\nand the observation received by the ego agent at timestep t\nis \\(O_t \u2208 O\\). The transition model T at timestep t is defined\nas \\(p_t(S_{t+1}| S_t, a_t)\\), where \\(p_t\\) denotes the current transition\nprobability from state-action pair \\((s_t, a_t)\\) to the next state\\(\\(S_{t+1}\\). The emission probability \\(& \\) at timestep t is defined as\n\\(P_t(O_t| S_t)\\), which represents the probability of an observation\n\\(O_t\\) being generated from a state \\(s_t\\). The reward function is\ndefined by \\(r(s_t, a_t) \u2208 R\\) and the discount factor is denoted\nby \\(\u03b3 \u2208 [0, 1]\\). The overall objective function of reinforcement\nlearning is to maximize the expected discounted return by\nfinding an optimal policy \u03c0*:\n\u03c0* = arg max E \\(E Err(St, at)\\)\n\u03c0"}, {"title": "IV. METHODOLOGY", "content": "A. Overview of the Reinforecement Learning Framework in\nADTF\nTo the best of our knowledge, no relevant literature has\ndescribed how to integrate reinforcement learning with the\nADTF middleware. We propose a framework that enables\nADTF simulation to interact with the RL agent. As is shown\nin Fig. 2, the RL agent communicates with the ADTF\nsimulation through TCP/IP communication. The action gene-"}, {"title": "B. PPO-based Hierarchical Planning Framework", "content": "As presented in Fig. 3, a hierarchical planning framework\nis proposed based on the PPO algorithm. Tab. I defines the\nrequired action and observation space. At timestep t, the RL\nagent receives the observation \\(o_t \u2208 O = \\{r_s,r_d, d_l, d_r, v\\}\\)\nfrom ADTF simulation. To simplify the geometric relation-\nship between the ego and target vehicle, states \\(\\{r_s, r_d,d_l, d_r\\}\\)\nare calculated in Frenet coordinates system. Fig. 4 depicts\nthe geometric relationship between the vehicle and lane\nboundary. After receiving the observation, the high-level\naction \\(a_t \u2208 A = \\{0,1,2\\}\\) is generated by the RL agent using\nthe PPO algorithm. Later on it will be denoted as \\(a_t^{RL}\\) to\ndistinguish it from the low-level action \\(a_{ft^*}\\). The optimization\nobjective of PPO \\(L^{CLIP+VF+S}(\u03b8)\\) is given as:\n\\(L^{CLIP+VF+S}(\u03b8) = \\hat{E}_t [L^{CLIP}(\u03b8) \u2013 C_1L^{VF}(\u03b8)\n+C_2S [\u03c0_\u03b8] (S_t)],\\)\nwhere \\(L^{CLIP}\\) is the clipped objective (see Eq. 3), \\(L^{VF}\\) is\na squared-error loss (see Eq. 4) and S is an entropy bonus.\nHere \\(C_1, C_2\\) are coefficients and \u03b8 represents the trainable\nweights of the actor-critic network. The clipped objective\nand the squared error loss are defined as follows:\n\\(L^{CLIP} (\u03b8) = min (r_t (\u03b8) A_t, clip (r_t (\u03b8), 1 \u2013 \u03f5, 1 + \u03f5) A_t),\\)\n\\(L^{VF} (\u03b8) = (V_\u03b8(s_t) - V_t^{targ})^2,\\)\nwhere \\(r_t(\u03b8) = \\frac{\u03c0_\u03b8(a_t|s_t)}{\u03c0_{\u03b8^{old}}(a_t|s_t)}\\) denotes the probability ratio. As\nstated in Eq. 1, the policy \\(\u03c0_\u03b8\\) is generated from the actor-\ncritic network in the current iteration and \\(\u03c0_{\u03b8^{old}}\\) is the old\npolicy before the update. \\(A_t\\) is an estimator of the advantage\nfunction at timestep t, which can be computed by a general\nadvantage estimator [33]. The explanation of the function\n\\(clip (r_t (\u03b8), 1 \u2013 \u03f5, 1 + \u03f5) A_t\\) can be found in the original paper\nof PPO [15]. As for the second loss term \\(L^{VF}\\), \\(V_\u03b8(s_t)\\) is the"}, {"title": "C. Time Period Reinforcement Learning", "content": "The conventional reinforcement learning methods require\nthe RL agent to communicate with the environment in every\nsingle step. For example, for continuous control tasks such as\nbalancing a pendulum or cart-pole, it is reasonable to genera-\nte the control signal in each control step. However, this poses\na problem when it comes to the high-level behavior planning\ntask. For instance, the hybrid A* path planner generates a\ntrajectory to execute that takes more than 100 simulation\nsteps. During these simulation steps, the behavior planning\ncommand should remain consistent. However, conventional\nRL methods cannot achieve this goal. In order to overcome\nthis problem, we propose the TPRL method to sample the\naction in a certain time period and remain consistent in this\ntime period. Inspired by [27] and [34], we also divide the\npolicy into two levels. One is generated from RL Agent with\nPPO algorithm and the other is generated from the hybrid\nA* path planner. Although the output of hybrid A* path\nplanner is not like the RL policy, we consider it as a policy\nin this hierarchical framework. As is shown in Fig. 5, the\nenvironment state is partially observed by the RL agent, and\nthe reward and observation are saved in the set of samples\nDk every N time steps. Here we call N as action sampling\ninterval. In this hierarchical framework, the action \\(a_{fL}\\) from\nthe RL agent only gives an intermediate behavior planning\naction. The hybrid A* trajectory planner actually generates\nthe action \\(a_{ft^*}\\) and interacts with the environment. As for the\nRL Level, the reward \\(r_{St:t+(N-1)t, a_{t:t+(N-1)At}}^{RL}\\) of\nthe TPRL algorithm should be accumulated from time t until\ntime t + (N \u2212 1)\u25b3t. The accumulative reward of each high-\nlevel decision step is summarized as:\n\\(r_{St:t+(N-1)t, at:t+(N-1)At}^{RL} = \\sum_{n=0}^{N-1}Yr (St+nt, a*_{t+n_t})\\)"}, {"title": "D. Reward Definition by LTL", "content": "In some situations where RL is applied, the goal is quite\nclear, so the definition of the reward function is straightfor-\nward. However, when applying RL in high-level behavior\nplanning, the reward function can get complicated when the\nRL agent needs to understand traffic rules. To incorporate\nthe reward function with current traffic rules, LTL is used to\nformulate the traffic rules logically. We formulate the traffic\nrules referring to [16]. At first we define AP as a set of\natomic propositions. The syntax of LTL formula is given by\n\\(\u03c6 ::=\u03c3|\u00ac\u03c6|61^2|12|41 \u21d2 42|\n\u039f\u03c6\u03c6\u03c6\uc774,\\)\nwhere each atomic proposition \u03c3\u2208 AP is a Boolean\nstatement, \u00ac, A, V, \u21d2 denote the Boolean operators \"not\",\n\"and\", \"or\", \"implies\". Meanwhile, \u25cb, U,, and\u25ca denote"}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "To demonstrate the performance of the proposed TPRL\nalgorithm, we designed several experiments based on our\nsimulation environment and also validated it in real-world"}, {"title": "A. Experimental Setup", "content": "1) Training setups: To validate the generalization ability\nof the proposed algorithm, we train our RL agent on a very\nsimple oval map and validate it with a more complex cross\nroad map in ADTF simulation. As is shown in Fig. 7a, the\ntarget vehicle is initialized with 0.278 m/s (1 km/h), and\nafter 8 seconds the ego vehicle is initialized with 0.556\nm/s (2 km/h). Due to the time gap, the target vehicle is\nin front of the ego vehicle in the very beginning. Then,\nthe ego vehicle will consistently overtake the target vehicle\nbecause the target vehicle is slower than the ego vehicle.\nThe purple trajectory in Fig. 7a is generated when the ego\nagent receives the lane change decision from our RL agent.\nOtherwise, the ego vehicle will follow the target vehicle\nin front with 0.278 m/s. Thanks to our hybrid A* path\nplanner [14] and the emergency stop function, a collision-\nfree trajectory is ensured. The ego vehicle drives 5 circles\nalong the map for each episode. Referring to [35] and [36],\nwe also implemented a similar RL framework with the gym\nlibrary. The selection of hyperparameter parameters is also"}, {"title": "B. Results and Discussions", "content": "1) Training results: As stated in subsection V-A, we\ntrain our RL agent with DDQN and PPO based on three\ntime skip variants and each is trained with three random\nseeds. In Fig 8 and Fig. 9, the colored lines represent the"}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "We present a hierarchical decision-making algorithm that\ncombines DRL with the Hybrid A* trajectory planner. Therein, the time period reinforcement learning is proposed\nin order to sample a single action in a fixed time period.\nThis method maintains consistency in lane change actions,\nenabling the local trajectory planner to successfully plan and\nexecute trajectories within specific time periods. Furthermore, LTL rules are implemented as reward functions and tested.\nWe find that the agent is able to follow the traffic rules\nbased on LTL and the TPRL algorithm, whereas the simple\nperiod skip or no skip methods fail to achieve a sufficient\nrule compliance rate. In addition, we also implement the\nalgorithm in a real-world scenario and validate its real-time\nfeasibility. Future work will involve utilizing a bird's-eye\nview as input and scaling up the scenario, testing it in more\ncomplex real-world traffic situations."}]}