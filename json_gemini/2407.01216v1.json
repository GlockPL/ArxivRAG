{"title": "Let Hybrid A* Path Planner Obey Traffic Rules: A Deep Reinforcement Learning-Based Planning Framework", "authors": ["Xibo Li", "Shruti Patel", "Christof B\u00fcskens"], "abstract": "Deep reinforcement learning (DRL) allows a system to interact with its environment and take actions by training an efficient policy that maximizes self-defined rewards. In autonomous driving, it can be used as a strategy for high-level decision making, whereas low-level algorithms such as the hybrid A* path planning have proven their ability to solve the local trajectory planning problem. In this work, we combine these two methods where the DRL makes high-level decisions such as lane change commands. After obtaining the lane change command, the hybrid A* planner is able to generate a collision-free trajectory to be executed by a model predictive controller (MPC). In addition, the DRL algorithm is able to keep the lane change command consistent within a chosen time-period. Traffic rules are implemented using linear temporal logic (LTL), which is then utilized as a reward function in DRL. Furthermore, we validate the proposed method on a real system to demonstrate its feasibility from simulation to implementation on real hardware.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past few years, Advanced Driver Assistance Systems (ADAS) have improved road transportation safety and efficiency [1]. Autonomous driving, considered to be the next step of ADAS, has also made great progress due to the development of sensor technology, control theory, and deep learning [2]. Despite the advances in autonomous driving, when and how to choose the current driving behavior is still quite a challenging task in complex driving scenarios [3]. Rule-based methods are usually implemented in order to solve straightforward scenarios such as lane change, lane merging, emergency braking, and so on. However, these hand-crafted rules can be inefficient in dense traffic with multiple moving vehicles.\nRecently, reinforcement learning (RL) has shown its potential in solving complicated tasks and achieving a better performance than human experts while playing Atari games [4] and the game of Go [5]. Meanwhile, those RL methods are also extended in behavior planning in order to make feasible decisions in different driving scenarios [6] [7]. Simulation tools such as SUMO [8] and CARLA [9] are used to construct various driving scenarios in order to train the RL agent. Referring to real-world implementations, mainly end-to-end approaches have been applied [10] [11]. However, several challenges persist when applying RL methods to autonomous driving.\nFirstly, the RL policy trained in simulation is hard to directly apply in real-world situations because the simulation environment does not adequately mirror real-world conditions. Secondly, when the steering and acceleration are generated by end-to-end approaches, a reliable, smooth, and comfortable trajectory is hard to ensure. Thirdly, the RL agent is not able to obey basic traffic rules. To address the first challenge, we propose an RL platform based on the EB Assist Automotive Data and Time-Triggered Framework (ADTF), commonly used in the automotive industry [12]. The RL agent is initially trained in the ADTF simulation environment and then directly implemented on an ADAS model car, as depicted in Fig. 1. Further details of the model car can be found in [13]. To tackle the second challenge, we propose a framework for combining deep reinforcement learning with local trajectory planning. The proposed framework is structured into two components, high-level behavior planning and low-level trajectory planning. High-level behavior planning employs DRL methods to generate lane change commands, while the hybrid A* planner serves as the low-level trajectory planner tasked with generating the local trajectory [14]. Specifically, the Proximal Policy Optimization (PPO) [15] algorithm is implemented to generate discrete lane change actions, aiding the local trajectory planning module in generating reliable and efficient lane change trajectories. As for the third challenge, the RL agent is trained to comply with traffic rules, which are encoded using LTL and incorporated into the agent's reward function [16] [17]. This approach ensures that our agent obeys traffic rules when making decisions.\nConcretely, the main contributions of our work are sum-"}, {"title": "II. RELATED WORKS", "content": "A. Heuristic-based Behavior Planning\nHeuristic-based behavior planning methods attempt to solve the behavior planning problem by developing a set of rules for various driving scenarios. For example, in [18], a rule-based behavioral layer with three sub-components: lane driving, intersection handling, and goal selection, is designed in order to make tactical decisions. Additionally, other heuristic methods such as decision trees [19] or finite state machines [20] are also explored in other autonomous driving systems. However, these methods are primarily employed to tackle limited, uncomplicated scenarios. As the complexity of driving situations increases, the effectiveness of hand-crafted rules diminishes due to the challenges associated with managing their growing complexity and tuning hyperparameters.\nB. Learning-based Behavior Planning\nBy interacting with the environment, learning-based methods enable a system to execute actions based on pre-defined rewards. Deep Q Network (DQN) is first implemented to execute reasonable lane change decisions across various simulated environments [6] [7]. In addition to DQN, safe, efficient, and comfortable lane change decisions are executed using PPO in mandatory lane change scenarios within SUMO Simulation [21], where in [22] safe lane change decisions are ensured by designing an additional safety layer between the agent and the environment. By combining RL with Imitation Learning, [23] employs bird's eye view images as the state input and achieves impressive performance for different traffic scenarios such as the stop signs, lane changing, merging, and US-style junctions in CARLA Simulation. Another imitation learning method, Adversarial Inverse Reinforcement Learning (AIRL) is utilized in [24] in order to learn both a behavioral policy and a reward function. For the autonomous overtaking problem in high-speed situations, [25] implements curriculum reinforcement learning in a racing game and achieves a comparable performance to the experienced human driver. Besides the end-to-end methods, the authors of [26] [27] also implement some hierarchical structures in simulated urban environments. These previous works have shown the potential of using learning-based methods to solve the behavior planning problem. However, very few combine learning-based methods together with local trajectory planning. Most of the works only test the performance in simulation environments. In this paper, we present a comprehensive learning-based framework that combines PPO with trajectory planning and control. Furthermore, the performance is also validated both in simulation and real-world situations."}, {"title": "III. PROBLEM DESCRIPTION", "content": "A lane change action in road traffic can occur either mandatorily or discretely. In the mandatory scenario, the decision is typically made when exiting the highway or merging from multiple lanes into a single lane. These situations involve a high volume of vehicles with a complex traffic flow. To simplify the scenario, we initially demonstrate it with two vehicles before expanding to more complex situations. Therefore, in this paper, we primarily focus on the discretionary lane change scenario, which is suitable for testing with two ADAS model cars.\nHere we formulate the discretionary lane change scenario as a partially observed Markov decision process [28] defined by a tuple < S,A,O,T,E,R, \u03b3 >. The state, action and observation spaces are represented by S, A and O, where the state vector at timestep t is denoted as \\(S_t \\in S\\), the action taken by the ego agent at timestep t is defined as \\(a_t \\in A\\) and the observation received by the ego agent at timestep t is \\(O_t \\in O\\). The transition model T at timestep t is defined as \\(p_t(S_{t+1} | S_t, a_t)\\), where \\(p_t\\) denotes the current transition probability from state-action pair \\((s_t, a_t)\\) to the next state \\(S_{t+1}\\). The emission probability & at timestep t is defined as \\(p_t(O_t | S_t)\\), which represents the probability of an observation \\(O_t\\) being generated from a state \\(s_t\\). The reward function is defined by \\(r(s_t, a_t) \\in R\\) and the discount factor is denoted by \u03b3 \u2208 [0, 1]. The overall objective function of reinforcement learning is to maximize the expected discounted return by finding an optimal policy \u03c0*:\n\\(\u03c0^* = arg max E_\u03c0 [ \\sum_{t=0}^{\u221e} \u03b3^t r(S_t, a_t)]\\)   (1)"}, {"title": "IV. METHODOLOGY", "content": "A. Overview of the Reinforecement Learning Framework in ADTF\nTo the best of our knowledge, no relevant literature has described how to integrate reinforcement learning with the ADTF middleware. We propose a framework that enables ADTF simulation to interact with the RL agent. As is shown in Fig. 2, the RL agent communicates with the ADTF simulation through TCP/IP communication. The action gene-"}, {"title": "B. PPO-based Hierarchical Planning Framework", "content": "As presented in Fig. 3, a hierarchical planning framework is proposed based on the PPO algorithm. Tab. I defines the required action and observation space. At timestep t, the RL agent receives the observation \\(o_t \\in O = {r_s, r_d, d_l, d_r, v}\\) from ADTF simulation. To simplify the geometric relationship between the ego and target vehicle, states \\({r_s, r_d, d_l, d_r}\\) are calculated in Frenet coordinates system. Fig. 4 depicts the geometric relationship between the vehicle and lane boundary. After receiving the observation, the high-level action \\(a_t \\in A = {0,1,2}\\) is generated by the RL agent using the PPO algorithm. Later on it will be denoted as \\(a_t^{RL}\\) to distinguish it from the low-level action \\(a_{t}^{A^*}\\). The optimization objective of PPO \\(L^{CLIP+VF+S} (\\theta)\\) is given as:\n\\(L^{CLIP+VF+S} (\\theta) = \\hat{E_t} [L^{CLIP} (\\theta) \u2013 c_1L^{VF} (\\theta) + c_2 S[\u03c0_\u03b8] (S_t)]\\),  (2)\nwhere \\(L^{CLIP}\\) is the clipped objective (see Eq. 3), \\(L^{VF}\\) is a squared-error loss (see Eq. 4) and S is an entropy bonus. Here \\(c_1, c_2\\) are coefficients and \u03b8 represents the trainable weights of the actor-critic network. The clipped objective and the squared error loss are defined as follows:\n\\(L^{CLIP} (\\theta) = min (r_t (\\theta) \\hat{A}_t, clip (r_t (\\theta), 1 \u2013 \u03b5, 1 + \u03b5) \\hat{A}_t)\\),  (3)\n\\(L^{VF} (\\theta) = (V_\u03b8(s_t) - V_t^{targ})^2\\),   (4)\nwhere \\(r_t(\u03b8) = \\frac{\u03c0_\u03b8(a_t|s_t)}{\u03c0_{\u03b8^{old}}(a_t | S_t)}\\) denotes the probability ratio. As stated in Eq. 1, the policy \\(\u03c0_\u03b8\\) is generated from the actor-critic network in the current iteration and \\(\u03c0_{\u03b8^{old}}\\) is the old policy before the update. \\(\\hat{A}_t\\) is an estimator of the advantage function at timestep t, which can be computed by a general advantage estimator [33]. The explanation of the function clip \\((r_t (\\theta), 1 \u2013 \u03b5, 1 + \u03b5) \\hat{A}_t\\) can be found in the original paper of PPO [15]. As for the second loss term \\(L^{VF}\\), \\(V_\u03b8(s_t)\\) is the"}, {"title": "C. Time Period Reinforcement Learning", "content": "The conventional reinforcement learning methods require the RL agent to communicate with the environment in every single step. For example, for continuous control tasks such as balancing a pendulum or cart-pole, it is reasonable to generate the control signal in each control step. However, this poses a problem when it comes to the high-level behavior planning task. For instance, the hybrid A* path planner generates a trajectory to execute that takes more than 100 simulation steps. During these simulation steps, the behavior planning command should remain consistent. However, conventional RL methods cannot achieve this goal. In order to overcome this problem, we propose the TPRL method to sample the action in a certain time period and remain consistent in this time period. Inspired by [27] and [34], we also divide the policy into two levels. One is generated from RL Agent with PPO algorithm and the other is generated from the hybrid A* path planner. Although the output of hybrid A* path planner is not like the RL policy, we consider it as a policy in this hierarchical framework. As is shown in Fig. 5, the environment state is partially observed by the RL agent, and the reward and observation are saved in the set of samples Dk every N time steps. Here we call N as action sampling interval. In this hierarchical framework, the action \\(a_t^{RL}\\) from the RL agent only gives an intermediate behavior planning action. The hybrid A* trajectory planner actually generates the action \\(a_t^{A^*}\\) and interacts with the environment. As for the RL Level, the reward \\(r^{RL} (s_{t:t+(N-1)\u0394t}, a_{t:t+(N-1)\u0394t})\\) of the TPRL algorithm should be accumulated from time t until time t + (N \u2212 1)\u25b3t. The accumulative reward of each high-level decision step is summarized as:\nr^{RL} (s_{t:t+(N-1)\u0394t}, a_{t:t+(N-1)\u0394t}) = \\sum_{n=0}^{N-1} \u03b3_r (s_{t+n\u0394t}, a_{t+n\u0394t})\n (5)"}, {"title": "D. Reward Definition by LTL", "content": "In some situations where RL is applied, the goal is quite clear, so the definition of the reward function is straightforward. However, when applying RL in high-level behavior planning, the reward function can get complicated when the RL agent needs to understand traffic rules. To incorporate the reward function with current traffic rules, LTL is used to formulate the traffic rules logically. We formulate the traffic rules referring to [16]. At first we define AP as a set of atomic propositions. The syntax of LTL formula is given by\n\u03c6 ::= \u03c3 | \u00ac\u03c6 | \u03c6_1 \u2227 \u03c6_2 | \u03c6_1 \u2228 \u03c6_2 | \u03c6_1 \u21d2 \u03c6_2 | O\u03c6 | \u03c6_1 U \u03c6_2 | \u25a1\u03c6 | \u25ca\u03c6,(6)\nwhere each atomic proposition \u03c3\u2208 AP is a Boolean statement, \u00ac, \u2227, \u2228, \u21d2 denote the Boolean operators \"not\", \"and\", \"or\", \"implies\". Meanwhile, \u25cb, U, \u25a1, and \u25ca denote the temporal operators \u201cnext\u201d, \u201cuntil\u201d, \u201cglobally\", and \"finally\", respectively. Similarly to [16], we also separate the rules into premise and conclusion,\n\u03c6 \u2192 (P \u2192 \u03c6C)(7)\nwhere \u03c6P means the current state of the environment or the prerequisite condition. When this condition is fulfilled, the corresponding traffic rule (conclusion) will be checked. \u03c6C denotes the conclusion that defines the legal behavior of the ego agent when the premise is fulfilled. After the LTL format is defined, the formalized rules can be concluded in Tab. II. Fig. 6 shows some examples of the geometric relati-onship of LTL labels_in-front(i\u2192j), behind(i\u2192j), left(i\u2192j) and right(i\u2192j) between the ego and target vehicle. The r'dense denotes the distance between the ego and surrounding vehicles.\nBased on these basic labels, the required traffic rules can be formalized in the format of premise and conclusion. In Tab. III, we summarize two rules that can be used as the reward terms. When the ego agent violates R\u2081 or R2 once, the reward will be -1 for the current timestep. If it fulfills the traffic rules, then the reward is to be 0, indicating that it will not get punished when it obeys the traffic rules. Therefore,\""}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "To demonstrate the performance of the proposed TPRL algorithm, we designed several experiments based on our simulation environment and also validated it in real-world the reward can be written as\nr^{RL} (s^{RL}, a^{RL}) = R_1 + R_2(8)\n\nB. Results and Discussions\n1) Training results: As stated in subsection V-A, we train our RL agent with DDQN and PPO based on three time skip variants and each is trained with three random seeds. In Fig 8 and Fig. 9, the colored lines represent the"}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "We present a hierarchical decision-making algorithm that combines DRL with the Hybrid A* trajectory planner. Therein, the time period reinforcement learning is proposed in order to sample a single action in a fixed time period. This method maintains consistency in lane change actions, enabling the local trajectory planner to successfully plan and execute trajectories within specific time periods. Furthermore, LTL rules are implemented as reward functions and tested. We find that the agent is able to follow the traffic rules based on LTL and the TPRL algorithm, whereas the simple period skip or no skip methods fail to achieve a sufficient rule compliance rate. In addition, we also implement the algorithm in a real-world scenario and validate its real-time feasibility. Future work will involve utilizing a bird's-eye view as input and scaling up the scenario, testing it in more complex real-world traffic situations."}, {"title": "VII. ACKNOWLEDGEMENT", "content": "The work of Xibo Li is supported by the Federal Ministry of Economic Affairs and Climate Action on the basis of a decision by the German Bundestag. The work of Shruti Patel is supported by funds of the German Governments Special Purpose Fund held at Landwirtschaftliche Rentenbank and"}]}