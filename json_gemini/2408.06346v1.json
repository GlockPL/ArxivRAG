{"title": "Closing the Affective Loop via Experience-Driven Reinforcement Learning Designers", "authors": ["Matthew Barthet", "Diogo Branco", "Roberto Gallotta", "Ahmed Khalifa", "Georgios N. Yannakakis"], "abstract": "Autonomously tailoring content to a set of pre-determined affective patterns has long been considered the holy grail of affect-aware human-computer interaction at large. The experience-driven procedural content generation framework realises this vision by searching for content that elicits a certain experience pattern to a user. In this paper, we propose a novel reinforcement learning (RL) framework for generating affect-tailored content, and we test it in the domain of racing games. Specifically, the experience-driven RL (EDRL) framework is given a target arousal trace, and it then generates a racetrack that elicits the desired affective responses for a particular type of player. EDRL leverages a reward function that assesses the affective pattern of any generated racetrack from a corpus of arousal traces. Our findings suggest that EDRL can accurately generate affect-driven racing game levels according to a designer's style and outperforms search-based methods for personalised content generation. The method is not only directly applicable to game content generation tasks but also employable broadly to any domain that uses content for affective adaptation.", "sections": [{"title": "I. INTRODUCTION", "content": "One of the most challenging tasks within affective com-puting (AC) [1] is to effectively leverage affect models to autonomously generate new contexts that are, in turn, capable of eliciting a desired emotional response [2]. In other words, the challenge for AC is to successfully enable an affect-aware closed-loop adaptive system, largely known as the affective loop [3], [4]. What makes the design of such affect-aware adaptive interactions very difficult is the unpredictability and subjectivity of human users, both behaviourally and emotively.\nMotivated by this challenge, we introduce a novel method for autonomously generating content that, when experienced, elicits a desired sequence of emotional responses to a par-ticular user. To test the proposed framework, we focus on the domain of games, as they offer rich forms of human-computer interaction and have proven to be an ideal test bed for AC research in the past [4], [5]. In particular, we leverage human arousal demonstrations from over 100 annotators of a real-time 3D rally driving game from the Arousal video Game AnnotatIoN (AGAIN) corpus [6] to generate racetracks that elicit a desired arousal trace for a particular player type. To ac-complish this, we build upon the experience-driven procedural content generation (PCG) via reinforcement learning (EDRL) framework [7] by using a simulation-based approach to reward the racetracks EDRL generates, as visualised in Fig. 1. We compare the performance of the EDRL approach against an experience-driven PCG [7] method that uses genetic search for creating racetracks. Both methods employ the K-Nearest Neighbours (KNN) algorithm [8] to generate arousal traces from provided game states encountered during simulations. The result is a generative AI framework that can adapt its generated outcome to multiple player and annotator types, and can be refined over time with more data.\nWe assess the efficacy of our methods by testing them across three different clusters of human annotators and three indicative target arousal patterns. We both quantitatively com-"}, {"title": "II. RELATED WORK", "content": "In this section, we first cover related studies at the in-tersection of AC and games in general (Section II-A) and then specifically AC studies as applied to procedural content generation (Section II-B).\nApplications of AC span a vast array of contexts, user modalities, and domains, including text and natural language [9], videos [10], audio [11], and games [4], [12]. Video games present their own rich and unique form of human-computer interaction [13] in that they allow the user to play an active role during consumption and encompass multiple modalities. Traditionally, models of affect learn to map one or more of these input modalities (e.g., facial input [14], pixels [15], game states [6]) to an affect label reported by a human annotator using supervised learning. Reliably collecting such labels for affect, however, is a significant challenge with a dedicated field of research within AC. Some modern data collection platforms, such as CARMA [16] and the PAGAN framework [17], enable the collection of such labels in real-time. Recent tools such as RankTrace [18] allow for the collection of unbounded time-continuous signals, which can be used, in turn, for regression or classification tasks, or converted into ordinal labels for preference learning methods [19].\nDue to the several challenges involved in the collection of reliable affect labels, an alternative practice in AC is the analysis of an existing affective corpus. Standardized Emotion Elicitation Databases (SEEDs), allow the study of emotion by replicating real life in controlled settings. SEEDs have been solicited through multiple affect elicitors including images [20]\u2013[22], videos [23]\u2013[25], and even 3D objects [26]\u2013[28]. In this study, we use the AGAIN dataset [6], comprising 1, 100 in-game videos and self-reported annotations of arousal across 124 participants and 9 games. The data consists of in-game telemetry, synchronised with video recordings of the participants' gameplay and time-continuous arousal signals using RankTrace [18]. Models trained on affect labels from the Solid Rally racing game of AGAIN have already shown promising results on predicting arousal by solely relying on pixel and in-game audio [15] but also training of human-like game playing agents [29].\nIn this paper, we build on earlier studies [29], [30] and extend the functionality of affect models for the purpose of generating environments that are tailored toward desired affect patterns. We thus attempt to close the affective game loop [4] through the game-level generation capacities of the algorithms introduced here."}, {"title": "B. PCG for Affective Computing", "content": "Since its inception a few decades ago, PCG has evolved to be a hugely influential and critical area of research within the domain of generative media. The initial focus of PCG methods was on adding replayability and novelty to games, such as by generating infinite levels in Rogue (Epyx, 1980), which spawned an entire genre of games referred to as Rogue-likes [31]. With the recent advent of large language models, the scope for generative systems in games has expanded substantially [32]. Large language models such as GPT have proven themselves capable of both generating text based on emotions [33] and processing and recognizing emotions [9]. Whilst affect-driven generation models for other domains remain in its infancy [34], such methods have shown promise in domains such as music generation [35], facial expression transformation [36], as well as dialogue generation [37].\nWithin games, affect-based generation has been instantiated by and mostly been explored through the experience-driven PCG (EDPCG) framework [2]. EDPCG aims to generate content that elicits a particular player experience when played. EDPCG primarily takes the form of a search-based PCG method [38], which generates content through algorithmic means such as local search or evolutionary search, allowing for more complex outputs given the right fitness function. Early examples of EDPCG applied in games involve generating personalised levels using some theory-based metric, such as generating racetracks according to Koster's [39] fun metric [40], and evolving interesting first-person shooter maps that maximize the duration of close fighting between players [41]. Shaker et al. also generated levels for Super Mario Bros. (Nintendo, 1985) according to predicted states of the player including engagement, frustration, and challenge [42]. More recent examples of EDPCG include the generation of video game levels for the Sonancia [43] system according to the tension experienced by players [44].\nRecently, EDPCG was combined with PCGRL [45] to gen-erate game levels for Super Mario Bros (Nintendo, 1985) [7] using a reward function which moderates diversity, inspired by Koster's principle of fun [39]. The EDRL framework, in short, has also been extended to generate levels using an episodic soft-actor critic algorithm, allowing it to better tailor itself to individual players [46]. To our knowledge, however, EDRL has yet to tackle affect-aware generation using human affect annotations in a continuous manner. This paper introduces the first EDRL agent that relies on time-continuous affect models that, in turn, allow an RL designer to incrementally build content according to a target player type and target affect pattern. Our introduced EDRL agent can either act as a novel-level generator or an affect-aware design assistant."}, {"title": "III. EXPERIENCE-DRIVEN CONTENT GENERATION", "content": "In this paper, we propose a novel approach for generating video game content using a model of human affect demonstra-tions. The algorithm builds upon the EDRL [7] framework by using a data-driven approach for evaluating generated levels through an evaluator agent combined with an affect model, shown in Fig. 2. We describe our approach in detail in Section III and the different reward functions tested in Section III-C.\nThe designer is responsible for generating the stimuli (race-track in case of Solid Rally) that will be passed to the evaluator agent (hereafter the evaluator) during optimisation. Whilst the designer can use any generation method (e.g., PCGML [47], PCGRL [45]), we test two implementations in this paper: one which takes the form of a search-based generator using a evolutionary RL method (i.e. genetic algorithm), and an RL designer using a variant of the Go-Explore algorithm [48] called Go-Blend [29] (see Section IV-B). Regardless of implementation, the output of the designer is a level (L) that is assigned a reward based on a function used by the evaluator. Based on this loop of generating new levels followed by feedback from the evaluator's simulation data, the designer should be able to optimise its output over time. Once complete, it should be capable of generating new stimuli that satisfy the target affect trace for a particular player type.\nThe role of the evaluator is two-fold. First, the evaluator ensures the feasibility of the generated stimuli (i.e., the gen-erated racetrack) if required during training. If the evaluator identifies a stimulus as infeasible (e.g., an unplayable level featuring a track intersecting itself), the individual is penalised to discourage the designer from re-using this design. Second, the evaluator is responsible for generating the state (SL) and affect (AL) traces for the given stimulus. In this paper, we test an evaluator that generates traces by simulating the playback of the stimuli using an Al agent.\nOne more critical design decision for the evaluator is its behaviour during simulation. Due to the data-driven nature of this framework, the affect model and behaviour evaluator can be tailored to the human demonstrations of a specific player type through clustering. Training the affect model on a specific cluster (i.e., player type) will push the system to generate tracks that are tailored to their specific affective patterns. The specifics of the implementation of the evaluator are highly dependent on the environment and the type of content being generated, as discussed in Section IV-B.\nThe reward function is the final important component of our framework, which guides the designer to generate the desired stimuli. For example, if the generator is required to generate content that maximises arousal, it must be trained to produce stimuli that elicit AL close to the maximum arousal value. Beyond affect, the reward function can also be used to"}, {"title": "C. Reward Function", "content": "The reward function is the final important component of our framework, which guides the designer to generate the desired stimuli. For example, if the generator is required to generate content that maximises arousal, it must be trained to produce stimuli that elicit AL close to the maximum arousal value. Beyond affect, the reward function can also be used to reward the similarity of state traces (i.e., in the behaviour of the evaluator). Generally speaking, the reward function, denoted by RL, measures the similarity, D between a generated affect trace (AL) and the desired affect trace (AT) as follows:\n$R_L = -D(A_L, A_T)$ (1)\nSimilarity can be measured using any distance metric, D, considered appropriate for each use case. In this paper, we use the area between curves method outlined in [49] as it is robust to noise and outliers. To reward for similarity, we then assign a negative value based on the distance (i.e., the greater the distance from Ar, the harsher the penalty) as seen in Eq. 1. It is important to note that the reward function proposed here can be used by either a traditional RL method (as a reward) or an evolutionary RL method (as a fitness function)."}, {"title": "IV. AROUSAL-DRIVEN RACETRACK GENERATION", "content": "In this section, we briefly describe our case study platform, the Solid Rally racing game from the AGAIN dataset [6], along with an overview of our implementation (Section IV-B) and our arousal model (Section IV-A). Solid Rally is a 3D real-time rally driving game built on the Unity game engine. The player must race against three other opponent cars around a racing circuit featuring corners, straights, loops, and bridges (see Fig. 3). As the cars drive around the racetrack, they are awarded points for passing through checkpoints located at the end of each component they drive through. The game features two sets of controls, one for steering (left, right, straight) and one for the gas pedal (forward, backward, neutral). The races have a maximum time limit of three laps, or 2 minutes if the player fails to finish before the time limit."}, {"title": "A. Arousal Model for Solid Rally", "content": "In this section, we describe the model of arousal used for evaluating the quality of the generated racetracks. As mentioned above, this system is built on top of Solid Rally, a game from the AGAIN dataset [6] containing over 100 human demonstrations of continuous arousal traces.\nThe observation data of the model consists of a set of 29 game features returned from the game engine about the cars' speed, score, distance to opponents, and various other in-game metrics. Each record in the dataset corresponds to a 3-second time window where the above features and arousal values are averaged. We then convert the records into preferences by comparing pairs of consecutive windows and giving them an ordinal affect label as denoted by the difference between their arousal values: increase, decrease, or stable arousal [19]. Inspired by earlier studies [15], [29] we use a preference threshold of 0.15, which means that for a time window to be labelled as an increase or a decrease, the corresponding absolute change must be greater than 0.15. This threshold helps us combat reporting biases across subjects in the affect annotations provided. Finally, the dataset is normalised using min-max normalisation to ensure all the features lie within [0, 1].\nTo better differentiate between different annotator types, we cluster the dataset based on the player's score and arousal traces. We perform clustering by computing the distance between all possible pairs of annotators using the area between curves method described in Section III-C. Doing so yields three clusters of annotators, two of which are a group of high-performing players with a substantial difference in their annotated arousal traces. We label these two clusters as Excited experts and Unexcited experts. Figure 4 shows the mean arousal traces of these clusters, where we can clearly observe that Excited experts have a constant increase in arousal over time, whereas unexcited experts have a sharp rise at the start, followed by a gradual descent as players of this cluster get less and less aroused as gameplay progresses. The third cluster we identified hosts a group of poor-performing players in terms of behaviour, which we call Beginners.\nFor the affect model, we implement a distance-weighted KNN model [8] to generate the change in arousal from a given pair of states, in this case, the evaluator's current state and the previous one. The KNN takes as input two state vectors (i.e., 29 game features across 3-second windows each as described above) and searches the corpus of affect annotations for the closest K entries using a pairwise Euclidean distance measure. We down-sample the dataset to only include the samples with"}, {"title": "B. Arousal-Driven PCG for Solid Rally", "content": "In this paper, we implement a generator using two designer approaches, the first is an evolutionary algorithm, often seen as an EDPCG approach, and the second is an EDRL agent using a variant of the Go-Explore algorithm [48] called Go-Blend [29]. To accomplish this task, a framework had to be built around Solid Rally to facilitate the creation of new racetracks, and communication between the internal game state and the generator's code. The game was converted into an Open-AI Gym environment using the Unity-Gym package, which allows for the training of Unity ML agents [50] and custom agents using Python.\nRacetracks are represented as a string of track components, the types of which are visualised in Fig. 3. Each component has a start position and end position, which are used to connect the pieces during generation. The components are scaled to be the same unit size, i.e., the straight, start/finish, and curves are all 1 tile long, whereas the loop and ramp are 3 tiles long. This tile-based formatting allows us to represent the environment as a 2-dimensional grid of tiles, simplifying the process of detecting collisions and determining feasibility. The grid is initialised to contain zeros (i.e., empty tiles) at the start of training.\nOur implementation of the EDPCG designer for this case study is as follows. The genotype is a"}, {"title": "1) EDPCG Designer:", "content": "string of IDs of a fixed target length, which are mapped to the components and instantiated in sequence in the environment. The phenotype is generated by iteratively replacing the com-ponent ID found in the genome with the corresponding track piece and updating the 2D grid accordingly. Collisions are detected by checking whether the current location already has a component (i.e., an ID \u2260 (\u00d8) before placing the next piece. If the racetrack is found to collide with itself, the stimulus is considered infeasible and is assigned a high penalty value (i.e., the worst possible reward of -1000).\nWe employ a genetic algorithm to allow the EDPCG de-signer to search the solution space for highly fit tracks. Our implementation follows the (\u03bc \u2013 \u03bb) evolutionary strategy [51], with the addition of a crossover operation during reproduction. This works by first initialising a population consisting of random individuals. Then, across many generations, we pass each individual to the evaluator if they are considered feasible, and assign them a fitness as the reward RL (see Eq. (1)) based on their generated AL. After the entire population is evaluated, we select the \u03bc best individuals to act as parents for the next generation. The next generation of individuals is created by repeatedly selecting pairs of parents and using one-point crossover and mutation to generate offspring. We use uniform mutation, meaning that we randomly change components of an individual to a different ID with a small chance set by the mutation rate. We preserve the best individual between generations (elitism) to ensure the population retains feasible and high-quality individuals after reproduction. Once this pro-cess is completed (i.e., the maximum number of generations is reached), the designer outputs a set of high-quality tracks that should elicit the desired affect pattern.\nThe hyperparameters for the EDPCG designer in this use case are selected based on preliminary experiments and are as follows. We use a parent population size (\u03bc) of the best 10 individuals from the current population. Our offspring popu-lation size (\u03bb) was set to 50 individuals, with the constraint that each individual in the population must be unique (i.e., when a new individual is generated, it is only inserted into the population if an identical copy does not already exist). We use one-point cross-over and mutation, with a mutation rate of 10% during reproduction. Parents are randomly chosen for reproduction from the \u00b5 best individuals. We ran 10 separate runs of 50 generations per scenario described above, and present the average results across runs, including the 95% confidence interval."}, {"title": "2) EDRL Designer:", "content": "As an alternative to the search-based generator, we use Go-Explore as the EDRL designer algorithm for this experiment, as it has proven itself a strong candidate for exploring deceptive and challenging environments. This designer follows the same collision rules as the EDPCG designer, but is driven by a different optimisation process. The EDRL designer iteratively builds racetracks using the Go-Explore exploration loop. First, a cell is sampled from the archive and its state is produced in the environment by repeating its trajectory of actions. From there it explores a single action, assigns a reward (RL; see Eq. (1)), and checks whether to store it in the archive or not. Cells are stored if they are unique, or if they have a larger reward than the existing cell. Our cells' state representation is a state vector containing the number of straights, turns, loops, bridges, and the length of the Dijkstra path [52]; described in the following section. This more abstract representation was chosen to group similar racetracks into the same cell and only separate meaningfully different layouts, resulting in a smaller archive and better search space exploration. Finally, we cap the cells' trajectory length to a maximum of 10 pieces to maintain a fair comparison with the EDPCG and Random designers, meaning that Go-Explore cannot explore a cell if it already contains 10 actions. Once complete, we take the best cell from the archive with 10 actions as our elite individual."}, {"title": "3) Playable Racetracks via Dijkstra:", "content": "Since the tiles placed by the designer are not guaranteed to create a racetrack that forms a circuit (i.e., a connected start and finish tile), the PCG algorithms are required to find the shortest path between the first and last tile, connect them, and form a playable track. With this in mind, we use the well-established Dijkstra algorithm [52] to search the grid for the shortest path between the start and the end tile. Once the shortest path is found, Dijkstra's algorithm places appropriate tiles to fill the path and complete the circuit. Dijkstra is only allowed to use simple tiles (i.e., straight, curve left, curve right) to implicitly encourage the designer to not rely heavily on this method and, preferably, close the circuits through its own designs. In case no path can be found using Dijkstra due to the absence of a feasible route between the start and the end tile-the track is marked as infeasible and is discarded."}, {"title": "4) Evaluator:", "content": "The evaluator consists of an in-game agent that drives around the generated racetracks and queries the arousal affect model described in Section IV-A. The agent's behaviour is governed by a checkpoint system that forms part of the original Solid Rally implementation, which works as follows. Each component of the generated level has a checkpoint placed at its endpoint. The agent drives such that it minimises the distance to the next checkpoint. When the agent drives through this checkpoint, it is given an increment to its score, and its target is set to the next checkpoint. This simple system allows the agent to behave reliably across any kind of level generated. This is the same system used by the opponent cars which the evaluator is racing against and-like the original game-the race takes place over 3 laps. As the evaluator simulates a race, it queries the KNN affect model every 3 seconds of in-game time. Note that due to the relatively deterministic nature of the evaluations, we only perform one simulation per track; however, in less deterministic game environments averaging the R\u2081 across multiple independent runs would be necessary for yielding more stable training signals."}, {"title": "V. EXPERIMENTAL PROTOCOL", "content": "We evaluate generators by testing them across three different scenarios by which they must generate content that elicits a desired (i.e., target) affect trace to a game-playing agent. As mentioned in Section III-C, we use the area between curves as the distance measure between the generated affect trace (AL) and the target trace (AT). The content was tested by simulating three laps around the racetrack to remain consistent with the original dataset. The first scenario is the Minimise Arousal experiment, where the generator must create a race-track that causes the agent's arousal to constantly decrease over time. The Maximise Arousal scenario mirrors the former by requiring the agent's arousal to constantly increase over time. Finally, the Fluctuating Arousal requires the racetrack to elicit an arousal trace that varies over time. In particular, the level must first maximise arousal for its first third, then minimise arousal for the second third, and finally, for the last third of the track, maximise arousal for the player. We compare our affect-driven level designers against a baseline random designer agent, which places a series of random tiles and then uses Dijkstra to close the circuit.\nTo evaluate the generators, we pick the best individual based on the accuracy of the output arousal signal elicited from the generated track to the target signal. Specifically, we treat the arousal modelling of the generated track as a binary classification problem, and we measure the rate at which the output signal and the target signal agree on the change in affect (i.e., increase versus decrease). Since our arousal model outputs a value between 0 and 1, an increase (or decrease) is indicated by any output over (below) 0.5. An accuracy of 100% (or 0%) means that the generated arousal trace and the desired arousal trace agree (disagree) completely on the change of arousal across the complete racetrack. Comparisons are made across 10 runs for each experiment configuration. We report average accuracy values and corresponding 95% confidence intervals. We denote significance when we observe non-overlapping confidence intervals between experiments at the p < 0.05 level. Beyond this, we also perform an expressive range analysis on the output of the designers [53]. The expressive range analysis offers us complementary qualitative insights about the variation of the components used on the racetrack for each target signal."}, {"title": "VI. RESULTS", "content": "The results of our experiments comparing the three dif-ferent track designers can be seen in Table I: we test each designer across four different player clusters and three target arousal scenarios. As expected, we observe that both the EDPCG and EDRL designers outperform the random designer (significantly based on the 95% confidence intervals) across all player clusters and scenarios. It is also clear from the results that certain scenarios were more challenging than others across player clusters. For example, the excited and unexcited experts produce the most varied results across the three different scenarios tested. Unexcited experts are easier to satisfy as a player group when we attempt to maximise their arousal with accuracy values over 95% for both EDPCG and EDRL. The opposite holds for the excited experts and the beginners, as it seems that minimising arousal is easier than maximising arousal for these player groups. Experiments with all players available in our corpus yields overall more consistent results across designers and scenarios (i.e., accuracy values in between 68% and 87%) due to the larger and more diverse set of human demonstrations. This larger and more diverse corpus most likely allows the designer to explore a larger variety of arousal states and therefore yield more consistent performances.\nWe argue that the varying performance of the tested methods across player types is due to their dissimilar affective patterns (see Fig. 4). We argue that the inherent subjectivity of reported affect as elicited by the layout of the circuit causes such dissimilarities. For example, compared to other player types, the arousal changes of the beginner players appear to be more consistent to the design of the circuit (i.e., the context). Consistent affect demonstrations, in turn, make the task of content generation far easier as predicting any target arousal trace is much simpler. On the other end of the spectrum, the unexcited experts generally showcase less arousal variation with respect to racetrack changes. We argue that this is due to habituation effects, as this group of expert players appears to be stimulated less after driving their first lap. Another potential reason for the observed differences is that the skill level of the player clusters had a strong effect on the annotated arousal levels. Specifically, our evaluator agent showcases only marginally better performance to the opponent AI cars, which is roughly equivalent to the behaviour of the beginners cluster. Such an agent cannot replicate how either of the expert groups would play in any generated racetrack, and it thus results in skewed arousal levels and larger performance discrepancies across scenarios for these groups.\nAs seen in Table I, EDRL yields superior performances compared to the random baseline while it matches or outper-forms the performance of EDPCG across nearly all scenarios and player types. Results show that EDRL is, on average, able"}, {"title": "VII. DISCUSSION", "content": "The proposed approach is novel in that it generates affect-aware content in a continuous manner via the EDRL frame-work. Importantly, the generated content can be tailored to a target affective pattern or trace for a particular user. Our results show that EDRL is capable of matching and sometimes outperforming EDPCG when it comes to designing racetracks with a specific arousal trace as a target. Incorporating Go-Explore's [48] robustification phase would allow our EDRL agent to function in an online capacity. This superior perfor-mance paired with the ability of EDRL to act as an online content generator (e.g., as in [2], [46]) highlight the promise of this approach in a real-world setting. Future investigations employing more complex reward functions, tested across dis-similar game genres and other domains, will help us validate the potential of EDRL for affect-driven generation in a general fashion. While tested initially in the domain of games, the EDRL method introduced here is applicable to any affective interaction task that envisions closing the affective loop via content generation mechanisms. As EDRL has shown promise to adapt itself to clusters of users and target affect patterns in this paper, we envision methods relying on the EDRL principle to be able to cope with dynamic shifts of user preferences and behaviours over time.\nOne of the limitations of the current approach lies in the simple behaviour of the evaluator agent. Whilst we chose the checkpoint system that governs its behaviour-due to its reliability to generalise to new levels\u2014the resulting behaviour of the agent is very deterministic and does not follow human-like patterns of play. More specifically, the agent does not factor in opponent cars, so it is prone to collide with them in situations that most human players would likely avoid. The agent is also constantly accelerating (a behaviour that most human players would likely not follow) especially if they get stuck or need to slow down for a sharp turn. Consequently, since the agent tends to play through components with very similar behaviour, it limits the generator's ability to thoroughly explore the solution space as the full range of possible sce-narios per component is not sufficiently explored. One way of addressing this limitation in the future is to train agents that behave similarly to the target clusters, thus having a more representative evaluator in terms of behaviour and, therefore, more reliable affect-driven generation.\nAnother limitation of this work lies in the generalisability of our arousal model. Since the dataset used in this paper only contains annotations and telemetry from sessions on a single-track layout, the arousal model does not have a large distribution of contextual data available, which in turn limits its performance when assessing new content. As a result, whilst we show that EDRL is capable of generating content using this arousal model, it remains to be seen whether these results will accurately reflect human feedback. This is an important avenue for future work, as quantitatively correlating the generated tracks via a human study (e.g., in [54]) will confirm that the generator is capable of matching content to human emotions.\nFinally, collecting more human (behavioural and affect) demonstrations on a diverse set of new racetracks will help us build more general representations between arousal, playing behaviour and game content. Yet another approach could be a mixed initiative one [55], where a human user provides feedback on the generated tracks to help steer affect-driven op-timisation in the right direction. This approach could gradually help us build a more personalised model of affect, and with enough data obtained could remove the need for running simulations and instead learn to generate affect directly from the stimuli presented."}, {"title": "VIII. CONCLUSIONS", "content": "We introduced a novel framework that expands current experience-driven content generation frameworks [2], [7] able to generate content that elicits tailor-made continuous affective patterns to a particular type of user. We test the capacity of two such generative methods-one based on a genetic algorithm and a second one driven by RL exploration\u2014to create affect-aware and personalised racetrack levels in games. Our results demonstrate that both approaches can accurately match a number of desired affect traces for certain types of players, but the EDRL method appears to be more efficient and robust. Both methods, however, face challenges to elicit certain affect patterns (e.g., maximise arousal throughout the level) to player types that have not experienced such patterns through their demonstrations (e.g., low-aroused beginner players). Findings of this paper suggest that it is possible to continuously generate affect-driven content and, importantly, it appears that the proposed EDRL method can create personalised and tailor-made content for various user types. While the results of this paper are specific to game content generation, the methods proposed are directly applicable to any affective interaction domain in need of personalised content creation."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "This paper makes use of an existing dataset (AGAIN dataset [6]) of human demonstrations collected from crowd workers on the Amazon Mechanical Turk (mTurk) platform. This dataset is publicly available, and participants gave their consent for their data to be stored and utilised in an anonymous fashion. To the best of our knowledge, there is no significantly negative application of the methods we use in this paper and no added privacy or discrimination risk. The data used in the paper and environment are publicly available for scientific reproducibility and for further extensions of this study."}]}