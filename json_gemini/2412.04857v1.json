{"title": "Neuro-Symbolic Data Generation for Math Reasoning", "authors": ["Zenan Li", "Zhi Zhou", "Yuan Yao", "Yu-Feng Li", "Chun Cao", "Fan Yang", "Xian Zhang", "Xiaoxing Ma"], "abstract": "A critical question about Large Language Models (LLMs) is whether their apparent deficiency in mathematical reasoning is inherent, or merely a result of insufficient exposure to high-quality mathematical data. To explore this, we developed an automated method for generating high-quality, supervised mathematical datasets. The method carefully mutates existing math problems, ensuring both diversity and validity of the newly generated problems. This is achieved by a neuro-symbolic data generation framework combining the intuitive informalization strengths of LLMs, and the precise symbolic reasoning of math solvers along with projected Markov chain Monte Carlo sampling in the highly-irregular symbolic space. Empirical experiments demonstrate the high quality of data generated by the proposed method, and that the LLMs, specifically LLaMA-2 and Mistral, when realigned with the generated data, surpass their state-of-the-art counterparts.", "sections": [{"title": "1 Introduction", "content": "Despite recent progress [1-6], both proprietary and open-source LLMs are still far from satisfactory in mathematical reasoning [7\u20139]. It is an open question whether LLM's subpar reasoning capability is inherent or due to the the extreme scarcity of high-quality mathematical datasets [10\u201313]. As an initial step towards answer this question, a data generation framework that could create high-quality math datasets is required. To this end, current two lines of research struggle in the diversity-validity dilemma: (1) to produce diverse math data, the prompt-based method effectively rephrases math problems using LLMs, but may induce errors thus ruining the validity, especially considering the rigor of maths; (2) to ensure the validity, template-based methods are often used by rewriting math problems with certain rules, sacrificing the diversity and thus confining data scale.\nTo address this dilemma, we propose a novel neuro-symbolic framework that automatically generates high-quality, supervised mathematical data. The merit of this paradigm lies in leveraging both neural and symbolic strengths: (1) the math problem is generated in the symbolic space, achieving diversity through systematic sampling, while maintaining validity through symbolic solvers; (2) the translation from the symbolic space back to the natural language space can be effectively supported by LLMs, ensuring the consistency between newly generated formal problems and their corresponding natural language versions.\nOur framework, as illustrated in Figure 1, initiates with the formalization of the original problem expressed via the math symbolic tools. Next, it mutates the formal problem into an evolved version, and then derives a new natural language problem by informalization. Specifically, we design a mutation mechanism, including various simplification and complication strategies, such that the new problems can be generated with a controllable complexity. As shown in Figure 2, our mutation"}, {"title": "2 Mutation", "content": "Compared to existing data generation methods, the key feature of our framework lies in the mutation of math problems within the symbolic space, enabling systematic sampling and symbolic solving. Technically, our mutation mechanism includes several simplification and complication strategies, to control the complexity of the generated math problems. The overall framework of our problem mutation method is summarized in Algorithm 1."}, {"title": "2.1 Formalization", "content": "We first provide the formalization of math problems, based on which the mutation mechanism is operated. Specifically, we adopt the SMT-LIB language [19], a standard notation compatible with prevalent SMT solvers (e.g., Z3 [20], CVC5 [21], and MathSAT [22]). It can also be easily extended for symbolic calculators (e.g., SymPy [23]) and numerical solvers (e.g., SciPy [24]). With SMT-LIB language, the math problem in the following structure is enabled:\nGoal  g := min | max | solve f(x)\nConstraints h := h\u2081 \u2227 h2 | h\u2081 Vh2 | ite(h1, h2, h3) |\n\u2203x.e\u2081(x)  e2(x) | \u2200x. e1 (x)  e2(x) | e1 \u22c8 e2 , \u22c8\u2208 {>, <, >, <, =,\u2260}\nExpressions e := c | x := (x1,...,xn) | foo(x) | e1 \u20ac2,\u2208 {+,-, \u00d7, \u00f7}\nDomains D := N | N+ | R|C\nwhere c denotes a constant, x denotes an n-dimensional variable, ite denotes the if-then-else structure, foo refers to an interpreted function (e.g., trigonometric, logarithmic or user-defined ones) on the domain, and g and h represent any function of interest (can include quantifiers). In particular, we pre-defined a series of interpreted functions, such as summation, binomial, gcd, 1cm, derivate, and integral, which facilitate the formalization of most high-school level mathematical problems (excluding geometry) within the above SMT-LIB language."}, {"title": "2.2 Simplification", "content": "We perform simplification by systematically considering expression reduction and constraint reduc-tion, which can be attained through heuristic tactics provided by standard symbolic solvers [25].\nSpecifically, we apply the simplify tactic for expression reduction, which involes operations such as constant or variable folding (e.g., x + 0 \u21d2 x or y + x \u2212 x \u21d2 y), expression expansion (e.g., (x + 1)\u00b2 = x\u00b2 + 2x + 1), and function application (e.g., (x = 2) \u2227 (y = log(x)) \u21d2 y = log(2)); we also perform symbolic and numerical computations for further reductions (e.g., gcd(2x, 6y) \u21d2 2gcd(x, 3y) and sin(\u03c0/6) \u21d2 0.5).\nFor constraint reduction, we mainly employ the Gaussian elimination tactic gaussian_elim (e.g., x = 2^ y \u2264 x + z \u21d2 y \u2264 2 + z). To handle the if-then-else term, we apply the elim_term_ite tactic to decompose it by introducing a fresh variable (e.g., ite(x > y, x, y) > z \u21d2 (k > z) \u2227 (x > y \u2192 k = x) \u2227 (x < y \u2192 k = y)). For constraints involving quantifiers, we strive to eliminate them using the qe tactic (e.g., \u2203y.(y > 0) \u2227 (x = y + 2) \u21d2 x > 2). Appendix B provides more examples illustrating these simplifications."}, {"title": "2.3 Complication", "content": "To complicate the expressions, a straightforward strategy is to incorporate additional operators. For example, given an atomic constraint h = e\u2081 e2, we can introduce an additional expression, denoted by e', and derive a more complex constraint h = e\u2081 (e2e').\nHowever, such a strategy is non-trivial in practice. The first challenge lies in the validity aspect, i.e., the math problem is often carefully designed, and thus a random mutation may ruin their well-defined structure. Consider the running example problem (M\u2081), which has been normalized for simplicity. In this problem, a reckless mutation can easily violate the positive integer constraints, causing the problem ill-defined and unsolvable.\nTo address this issue, we equip each mutation with an auxiliary variable, followed by symbolic solvers to ensure the problem remains well-defined. Continuing with the previous example, we introduce three auxiliary variables, denoted by Z1, Z2, Z3, and then mutate the problem as (M2), where 1, 2, 3 \u2208 {+, -, \u00d7, \u00f7} represent three random operators. Furthermore, we instantiate e1, es, es by interpreted functions, i.e., e\u2081 = f001(21), e2 = foo2(22), and e3 = f003(23), where f001, f002, f003 are randomly selected from foo(z) = z | log(z) | exp(z) | arcsin(z) | . For our running example, we simply choose the identity function for f001, f002, f003, and set 1 = -, 2 = +, +3 = x. Using symbolic solvers to compute a feasible solution of (21, 22, 23), we derive a new and well-defined problem (M3).\nThe subsequent challenge is to ensure the diversity of the mutated problems, which now becomes how to make the solutions of auxiliary variables sufficiently diverse. This is essentially a model counting problem [26, 27], and current symbolic solvers still underperform in this regard [28]. To this end, we instead opt for auxiliary variable solution generation via the projected Markov chain Monte Carlo (projected MCMC) [14, 15]. Simply put, projected MCMC first perturbs a subset of variables (projected random walk), and then resolves the remaining part (inverse projection via symbolic solvers), which ensures both diversity and validity of the variable solutions.\nFinally, to complicate the constraints, one can easily reverse the process of simplification. For our running example, we can reverse the Gaussian elimination with refreshed variables, obtaining the final form (M4), which is then included as a new problem in the dataset."}, {"title": "3 Informalization", "content": "Informalization aims to translate a formal problem back to natural language without the loss of soundness [29]. As shown in Example 1, a simple, one-line instruction follows the formally posed SMT-LIB problem, serving as the input. Then, GPT-4 interprets the formal problem as a new math word problem.\nThe key challenge of informalization lies in ensuring a consistent conversion, i.e., the natural language problem informalized by GPT-4 should align with the formal solution given by symbolic solvers. Since it is difficult to directly measure this consistency, we instead use GPT-4 to generate a solution for each informalized problem, and then calculate the consistency rate between the solutions from GPT-4 and those from symbolic solvers as a surrogate metric. Furthermore, we observe that, if the problem is incorrectly informalized, GPT-4's solutions almostly cannot be confirmed by symbolic solvers (i.e., zero false positive). Therefore, the surrogate consistency rate can be regarded as a lower bound to the true consistency rate."}, {"title": "4 Experiments", "content": "In this section, we conduct a series of experiments to answer the following four research questions:\nRQ1: Efficacy \u2013 Using our data generation framework, can the fine-tuned model achieve better performance compared with existing models?\nRQ2: Efficiency \u2013 Given the same data generation budget, is the generated data from our framework better than that from the state-of-the-art data generation framework?\nRQ3: Generability \u2013 Is the effecitveness achieved by our framework due to potential data contami-nation introduced during the generation process?\nRQ4: Scalability \u2013 With more data generated, can our approach be continually effective in further improving model performance?"}, {"title": "4.1 Experimental Setup", "content": "Dataset. We conduct our data generation on the training sets of two popular mathematical reasoning benchmarks: GSM8K [10] and MATH [11]. GSM8K is a dataset comprising high-quality grade school math problems, which contains 7,473 training data and 1,319 testing data. MATH is a dataset comprised of challenging competition math problems, spanning seven subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus. There are 7,500 training data and 5,000 testing data in the MATH dataset. Additionally, we include two mathematical reasoning datasets, i.e., SVAMP [12] and ASDiv [18], to evaluate the out-of-domain generalizability of the models fine-tuned on the data generated from GSM8K and MATH datasets.\nComparison Methods. In our experiments, we compare the models trained using our generated data with existing state-of-the-art open-source mathematical reasoning models, including WizardMath [30], MuggleMATH [31], MAmmoTH [32], and MetaMath [33]. We also conduct a thorough comparison between our math generation method and the bootstrapping method employed in MetaMathQA [33], which is presently the most extensive open-source dataset for mathematical reasoning.\nData Generation Details. We use our mutation mechanism to generate a series of problems with varying levels of difficulty, and the specifics are as follows. Starting with a problem from the original dataset as a seed, we first perform simplification to the problem, and define this new version as level-0. Then, we randomly apply one expression complication step and one constraint complication step to the level-0 version, deriving a more difficult problem (level-1 version); and such complications can be repeated to obtain more difficult problems. For the GSM8K dataset, we create datasets across five levels of difficulty, with 30K examples at level-0 and 100K examples for the remaining four levels. As for the MATH dataset, we establish four levels of difficulty, where level-0, level-1, level-2, and level-3 contain 70K, 120K, 120K, and 120K examples, respectively. Particularly, for some problems in the MATH dataset that cannot be solved by symbolic solvers, we directly prompt GPT-4 to rephrase the problem and ignore the solution verification. The number of generated problems without solution verification varies across problem categories, and the details can be referred to Appendix D. In total, we generated 860K math problems based on the proposed framework to construct our dataset.\nEach generated math problem consists of a natural language problem description informalized by GPT-4 (version 0710), a final answer outputted by the symbolic solver, and a reasoning path from the problem to the final answer. The reasoning path for each problem is also generated by GPT-4, which is further verified by the corresponding answer derived from symbolic solvers.\nMore implementation details about training hyperparameters, instruction prompts, and symbolic solver integration, are included in Appendix D."}, {"title": "4.2 Empirical Results", "content": "RQ1: Efficacy. Using the generated math dataset, we fine-tune the LLaMA-2 base models of 7B and 13B parameter sizes, as well as the Mistral 7B base model. The fine-tuned models, as well as the comparison methods, are evaluated on the GSM8K and MATH datasets."}, {"title": "5 Related Work", "content": "Recent surveys [37-39] have comprehensively discussed the current advances in the mathematical reasoning of LLMs. Here, we review three main lines of existing work on enhancing the mathematical reasoning for LLMs related to our study: prompt-based methods, rephrasing-based methods, and tool-based methods.\nPrompt-based Method. Prompt-based methods aim to harness the inherent capabilities of LLMs by carefully designing appropriate input prompts without tuning the model parameters. This line of work starts from the observation that LLMs can effectively tackle more math problems when provided with a simple Chain-of-Thought (CoT) prompt, i.e., \u201cLet's think step by step\" [40]. Building upon the CoT prompt, Wang et al. [41] further propose to consolidate multiple reasoning paths based on the self-consistency of correct answers. Later, several researchers propose to prompt LLMs to decompose complex problems. For example, Zhou et al. [42] introduce the least-to-most strategy that prompts LLMs to break down the original problem into a series of sub-problems. Khot et al. [43] further boost this strategy, by assigning each sub-problem to the corresponding LLM that is specifically optimized for it. Finally, few-shot prompting, e.g., Few-shot CoT [44] and Complex CoT [45], has also been studied to enhance the reasoning performance of LLMs. To further improve the few-shot prompting, the prompt retrieval is proposed to automatically select high-quality examples [46, 47], while the prompt compression is explored to include more examples in restricted context by pruning each example [48].\nRephrasing-based Method. The second line of existing work aims to generate additional math data, based on which the mathematical reasoning capability of LLMs can be established via supervised fine-tuning. To address the data scarcity issue, current research mainly focuses on rephrasing the problem or the answer. For the answer rephrasing, Magister et al. [49] adopt the PaLM and GPT-3 to generate CoT math data, resulting in improved performance of the T5 model on math reasoning tasks. To mitigate the inclusion of incorrect answers during the supervised fine-tuning, RFT [50] introduces a rejection sampling strategy, whereas AFT [51] trains an LLM to categorize them. Regarding the problem rephrasing, WizardMath [30] proposes a reinforced evol-instruct method. It instructs ChatGPT and trains a new LLM to rephrase the problem, equipped by a reward model for evaluating the quality of generated problems. Combining the rephrasing of problems and answers together, MuggleMATH [31] builds the AugGSM8K dataset based on prompting GPT-3.5 and GPT-4. MetaMath [33] develops a question bootstrapping method based on LLMs, unifying rephrasing, self-verification [52], FOBAR [53], and answer augmentation strategies, obtaining the MetaMathQA. Xwin-Math [54] is a peer study that significantly enhances the reasoning capacity of LLMs using problems generated by GPT-4 Turbo. In contrast, our work focuses on generating verifiable problems through controllable mutations, rather than relying entirely on the GPT model.\nOur proposed method also falls into this category. In contrast to existing methods directly prompting LLMs to rephrase the problem, we mutate the problem in the formal symbolic space, resulting in a more controllable mutation mechanism that ensures both the validity and diversity of the generated problems. Moreover, the quality of reasoning paths is also guaranteed by the symbolic solvers.\nTool-based Method. Tool-based methods aim to enhance the math solving performance of LLMs by instructing them to use external math tools. For instance, PoT (Program of Thought) [55] and PAL [56] propose to prompt the LLMs to delegate the computation to a program interpreter, which can be executed to obtain the final answer. To further improve the tool-using ability of LLMs, MathCoder [57] constructs a math dataset containing problems and their code-based solutions for the supervised fine-tuning; MAmmoTH [32] builds a dataset that combines CoT and PoT reasoning, enabling LLMs to perform hybrid inference. Since the interaction with math tools can further boost the performance of LLMs, TVA [58] includes the Isabelle theorem prover to check each reasoning step and guide the reflection of LLMs; Tora [59] generates interactive tool-use trajectories on mathematical datasets and then performs imitation learning on the annotations.\nOur proposed method shares some similarities with tool-based approaches as both involve symbolic solvers. However, rather than using external tools to solve mathematical problems, our approach aims"}, {"title": "6 Limitations", "content": "The Capability of Symbolic Solvers. The effectiveness of our approach significantly hinges on the symbolic solvers. However, existing mathematical tools (e.g., Z3 [20], SymPy [23], and SciPy [24]) face limitations when it comes to expressing and solving a wide array of mathematical problems. For instance, the Z3 SMT solver struggles with expressing higher-order concepts like gcd and 1cm, while the SymPy encounters difficulties in solving inequalities involving multiple variables. In our framework, we integrate five mathematical tools, i.e., Z3, CVC4 [60], MathSAT [22], SymPy, and SciPy, and employ SMT-LIB [19] as a unified formal language to enhance the performance of symbolic solving.\nThe Expressiveness of Mutations. The mutation operators used within our framework remain limited, especially in generating more difficult problems (e.g., college- and even IMO-level math problems). One of our future work is to introduce more mutation operators, further increasing the problem difficulty. A possible strategy is the problem fusion [61], which fuses two formal problems into a single, new problem, rather than merely modifying an individual problem. Moreover, the informalization facilitated by LLMs can effectively mitigate the unnaturalness issue stemming from brute-force fusion.\nThe Dependence on GPT-4. GPT-4 is involved in our framework to carry out the informalization and generate the reasoning paths. We also consider the possible solutions that the dependence on GPT-4 can be gradually removed. First, by leveraging our generated formal-informal pairs, we can fine-tune a new LLM specifically for the informalization. Second, it is possible to bypass the generation of reasoning paths, through curriculum learning [62, 63] instead of supervised fine-tuning. Particularly, the reward in the curriculum learning can be determined by whether the generated solution is consistent with symbolic solvers, and the curriculum progresses by incorporating problems of various difficulty levels."}, {"title": "Broader Impact", "content": "The paper aims to advance the field of math data generation. There are many potential societal consequences of our work, and we firmly believe that the majority of these impacts are positive and none which we feel must be highlighted here."}, {"title": "7 Conclusion", "content": "This paper explores the question of whether sufficient exposure to high-quality mathematical data could enhance LLMs' inherent mathematical reasoning capability. We identify a key challenge in balancing diversity and validity in current math problem generation methods. To tackle this challenge, we propose a neuro-symbolic framework that initially generates formal mathematical problems and then informalizes them back into natural language versions. By casting the data generation into the formal language space, the diversity and validity of the generated math problems can be effectively ensured by the systematic sampling and symbolic solvers. Building upon this, we carefully devise a mutation mechanism, establishing the math dataset encompassing various difficulty levels, and prompt the LLMs to accomplish informalization. Through empirical experiments, we demonstrate that our neuro-symbolic data generation framework significantly enhances the performance of various LLMs in mathematical reasoning tasks, surpassing the current state-of-the-art open-source models. The results also suggest a promising pathway for further enhancing LLMs' mathematical capabilities.\nIn future work, we intend to expand the expressiveness of mutations and enhance the capability of symbolic solvers to support more types of problems, such as inequality problems. Our goal is to offer a data generation framework to automatically generate high-quality, supervised datasets for LLMs. We expect that our neuro-symbolic data generation framework can provide a potential solution for LLMs to solve the problem of data scarcity, and thereby facilitate in building more LLMs in downstream tasks. Further, our framework has the potential to be integrated with recent studies [64], which only require problems and final answers."}, {"title": "A Broader Impacts", "content": "The paper aims to advance the field of math data generation. There are many potential societal consequences of our work, and we firmly believe that the majority of these impacts are positive and none which we feel must be highlighted here."}, {"title": "B Examples of Simplification", "content": "Example 2: Simplification strategy on Problem (No. 41) of the GSM8K dataset.\nAutoformalization:\nTranslate the natural language problem into SMT-LIB language: \"Sara bought a pair of shoes for $50.00 and a dress for $200.00. If Rachel has twice the amount that Sara spent in total, how much is Rachel's budget?\"\nWe elucidate the simplification strategy through Example 2, which is selected from the GSM8K dataset. In this formal problem, we can conduct one step of Gaussian elimination, i.e., randomly solving and removing a variable (e.g., rachel_budget = 500). Then, we derive the new problem with its informalized version. Furthermore, we present another example, specifically Example 3, sourced from the MATH dataset. Here, the new problem is generated through the simplification strategy that randomly performs some calculations."}, {"title": "C Examples of Informalization", "content": "Various prompts yield different styles of informalization. To illustrate this, we provide two specific examples as follows. In Example 4, we do not refresh the variable, and instruct the informalization by adding the prompt \u201cEnsure to be a math word problem.\u201d The result of GPT-4's informalization tends"}, {"title": "D Additional Experimental Details", "content": "Generation Details. In the GSM8K dataset, each problem can be accurately formalized into the SMT-LIB format and successfully mutated into a new version. However, some problems, particularly in the precalculus and geometry categories of the MATH dataset, cannot be formalized or mutated effectively. Overall, out of a total of 7,500 problems, 822 cannot be formalized into the SMT-LIB format, and approximately 3,600 formalizations are inaccurate although they remain usable for the data generation. To address this issue, we strategically added a proportional number of mutated problems derived by directly prompting GPT-4, bypassing solution verification. The detailed counts of these problems are presented in Table 4.\nTraining Details. In this paper, we fully fine-tune the LLAMA-2-7B and LLAMA-2-13B models using four H800 NVIDIA GPUs. Each model is trained for 3 epochs with a batch size of 128 and a learning rate of 2e-5. For the fine-tuning of the LLAMA-2-70B model, we adopt the QLoRA [65] method with a learning rate of 1e-4. The rank and alpha of LoRA [66] are set to 96 and 16, respectively, with a dropout rate of 0.05 between the two matrices. The LoRA modules are added to both the attention and MLP layers. The 70B model is fine-tuned using eight A800 NVIDIA GPUs."}, {"title": "E Additional Experimental Results", "content": "E.1 Detailed results on MATH dataset\nWe present detailed results across different categories in the MATH dataset in Table 5. The numbers 1-7 correspond to Algebra, Counting and Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra, and Precalculus, respectively. The results indicate that Algebra is easier to improve, as evidenced by its higher mutation success rate. It is worth noting that improvements in Counting and Number Theory are reasonable because related mutation operators (e.g., binomial, gcd, lcm, etc.) are included in our framework. However, Precalculus and Geometry are still not well-supported. For example, the concept of triangle cannot currently be correctly expressed in the SMT-LIB format, resulting in relatively low improvement rates."}, {"title": "E.2 Comparison to tool-based methods", "content": "We compare our model with the tool-based models in Table 6. Although tool-based models achieve good performance with tools, they meet severe performance degradation when tools are not available. This result indicates that training a model using code-based and language-based rationales does not necessarily enhance the intrinsic reasoning ability; instead, it often promotes excessive dependence on external tools. For datasets that involve complex calculations, such as the MATH dataset, tool-based methods offer certain advantages due to their utilization of the strong capabilities of external tools. For datasets that emphasize knowledge reasoning but involve simpler calculations, such as the"}, {"title": "E.3 Experiments on DyVal Datasets", "content": "We also compare our models and existing mathematical reasoning models using DyVal [9] datasets to further evaluate the generalization ability of our models. DyVal is a flexible evaluation protocol for dynamic evaluation of LLMs, which generates evaluation samples with controllable complexities using directed acyclic graphs (DAGs). We focus on Arithmetic tasks using three DAGs' orders: topological (TOPO), reversed topological (REVERSED), and random orders (RAND). Following the same setting, we generate testing four increased complexity levels {D1, D2, D3, D3}, with tree depths and widths set to (2, 2), (3, 2), (3, 3), (4, 2). The performance comparison between models fine-tuned on LLaMA2-7B and Mistral-7B are respectively shown in Table 7 and Table 8. The results show that our models achieve the best performance in 11/12 cases and give a competitive performance in the remaining one case. As the complexity increases, our models achieves a relatively robust performance compared with the other models."}, {"title": "E.4 Diversity Gain across Various Difficulty Levels", "content": "To illustrate the need for various difficulty levels, we calculate the diversity gain relative to the original dataset for each difficulty level. We apply the same method as in MetaMath [33] to compute diversity"}]}