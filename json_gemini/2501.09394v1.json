{"title": "Quantum-Enhanced Transformers for Robust Acoustic Scene Classification in IoT Environments", "authors": ["Minh K. Quan", "Mayuri Wijayasundara", "Sujeeva Setunge", "Pubudu N. Pathirana"], "abstract": "The proliferation of Internet of Things (IoT) devices equipped with acoustic sensors necessitates robust acoustic scene classification (ASC) capabilities, even in noisy and data-limited environments. Traditional machine learning methods often struggle to generalize effectively under such conditions. To address this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene Classifier that leverages the power of quantum-inspired transformers. By integrating quantum concepts like superposition and entanglement, Q-ASC achieves superior feature learning and enhanced noise resilience compared to classical models. Furthermore, we introduce a Quantum Variational Autoencoder (QVAE) based data augmentation technique to mitigate the challenge of limited labeled data in IoT deployments. Extensive evaluations on the Tampere University of Technology (TUT) Acoustic Scenes 2016 benchmark dataset demonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5% under challenging conditions, outperforming state-of-the-art methods by over 5% in the best case. This research paves the way for deploying intelligent acoustic sensing in IoT networks, with potential applications in smart homes, industrial monitoring, and environmental surveillance, even in adverse acoustic environments.", "sections": [{"title": "I. INTRODUCTION", "content": "While deep learning (DL) has significantly advanced acoustic scene classification (ASC), real-world deployments, especially in the context of the Internet of Things (IoT), face substantial challenges. Noise, a ubiquitous element in acoustic environments, often masks or distorts crucial acoustic features, impeding accurate scene identification [1]. This is particularly problematic for traditional machine learning (ML) methods reliant on handcrafted features, as these might not capture the subtle nuances induced by noise [2]. Furthermore, the scarcity of labeled training data, often expensive and time-consuming to acquire, can lead to overfitting, where models excel on training data but struggle with unseen scenarios [3]. This is further exacerbated in IoT deployments, where data collection and annotation can be constrained by resource limitations and privacy concerns. The pressing need for robust ASC solutions in these challenging conditions motivates the exploration of novel approaches that can effectively handle noise and generalize from limited data, unlocking the full potential of ASC in real-world IoT applications.\nTo address these challenges, current research actively explores various approaches within the realm of deep learning for ASC in noisy and limited-data scenarios. Ensemble methods, such as convolutional recurrent neural networks with Log Mel filterbank energies, have been employed to improve robustness to noise by combining the strengths of multiple models [4]. Attention mechanisms, exemplified by Transformer-based ASC models like pretrained audio neural networks [5], have demonstrated success in capturing long-range dependencies in audio signals, aiding in scene understanding. To further enhance performance in noisy environments, multi-task learning has been explored to improve the robustness of deep neural networks by leveraging knowledge from related tasks. Additionally, data augmentation techniques, such as adding noise or changing pitch, have proven valuable in mitigating the issue of limited labeled data [6]. Despite these advancements, existing methods still grapple with achieving consistent performance in the face of complex noise conditions and data scarcity, particularly in capturing fine-grained acoustic features and modeling intricate temporal relationships in audio signals. The unique constraints of IoT devices, including limited computational power and storage, further necessitate the development of models that are not only accurate and robust but also efficient and deployable on resource-constrained hardware.\nIn this context, quantum-inspired machine learning (QiML) and transformer learning have emerged as powerful tools in AI [7], [8], offering a promising solution to these challenges. QiML leverages quantum principles like superposition and entanglement, enhancing classical algorithms' capabilities. At the same time, transformer learning, with its self-attention mechanism, excels at capturing long-range dependencies in sequential data. The combination of these techniques can address the limitations of current ASC methods by providing more robust representations and greater noise resilience. Specifically, QiML's ability to model complex distributions and non-local correlations complements the transformer's capacity to extract crucial contextual information, especially with limited data. By synergistically combining these advanced techniques, we aim to develop a novel ASC model that outperforms existing methods in challenging real-world scenarios, offering enhanced accuracy and robustness."}, {"title": "A. Motivation and main contributions", "content": "Motivated by the persistent challenges of noise and limited labeled data in ASC, particularly the degradation of performance in real-world environments with overlapping sound sources and varying noise levels, this paper introduces Q-ASC, a novel Quantum-Inspired Acoustic Scene Classifier. Q-ASC leverages quantum-enhanced transformers to achieve superior performance by addressing these limitations. Our key contributions include:\n\u2022\n\u2022\n\u2022\nWe propose a unique QiT architecture that integrates quantum concepts like superposition and entanglement, enabling richer feature representations and improved noise robustness compared to traditional methods.\nWe introduce a novel QVAE-based data augmentation technique to mitigate the issue of limited training data, enhancing the model's generalization capabilities, particularly in scenarios with few labeled examples.\nWe demonstrate that Q-ASC significantly outperforms state-of-the-art ASC methods through extensive evaluation on the TUT Acoustic Scenes 2016 [9] benchmark dataset, showcasing its effectiveness in handling both noisy and data-limited conditions."}, {"title": "B. Paper structure", "content": "The remainder of this paper is structured as follows. Section II outlines the Q-ASC methodology, including the quantum-inspired transformer architecture and QVAE-based data augmentation. In Section III, we describe the experimental setup, datasets, and evaluation metrics, and analyze the results, highlighting Q-ASC's superior performance. Finally, Section V concludes with a summary of key findings, implications, and future research directions."}, {"title": "II. PROPOSED METHODOLOGY", "content": ""}, {"title": "A. Quantum-inspired Transformer (QiT) Architecture", "content": "The Q-ASC model features a QiT architecture that integrates quantum principles into the transformer framework, addressing ASC challenges in noisy, data-limited environments. It includes a quantum embedding layer, a quantum-enhanced transformer encoder, a measurement and pooling layer, and a classical classifier."}, {"title": "1) Quantum embedding layer:", "content": "The input to the Quantum-in-the-Loop Transformer (QiT) is a mel-spectrogram patch $x_i \\in \\mathbb{R}^{P \\times P}$. These patches are generated using Short-Time Fourier Transform (STFT) [10] and mel-frequency filter banks, where p denotes the patch size. Each patch $x_i$ is encoded into a quantum state $|\\psi_i\\rangle$ within a Hilbert space $\\mathcal{H}^{\\otimes n}$ [11], where n represents the number of qubits. The encoding process utilizes a parameterized quantum circuit (PQC) $U_e(\\theta_e)$:\n$|\\psi_i\\rangle = U_e(\\theta_e)|0\\rangle^{\\otimes n}$,\n(1)\nHere, $|0\\rangle^n$ is the initial state of all qubits, and $\\theta_e$ are the trainable parameters of the PQC."}, {"title": "2) Quantum-enhanced transformer encoder:", "content": "This encoder consists of multiple layers, each comprising:\n\u2022 Quantum self-attention: This mechanism calculates attention weights between quantum states using a PQC $U_a(\\theta_a)$. The attention score $a_{ij}$ between two quantum states $|\\psi_i\\rangle$ and $|\\psi_j\\rangle$ is determined using the SWAP test [12]:\n$a_{ij} = |\\langle \\psi_i | SWAP | \\psi_j \\rangle|^2$.\n(2)\nThe attention matrix $A = [a_{ij}]$ is then used to compute a weighted sum of the input states:"}, {"title": "3) Measurement and pooling:", "content": "The final quantum states $|\\psi_i'\\rangle$ are measured in the computational basis, resulting in probability distributions:\n$p_i(j) = |\\langle j | \\psi_i'\\rangle|^2, \\quad j \\in \\{0, 1, ..., 2^n - 1\\}$,\n(5)\nwhere $p_i(j)$ indicates the probability of the i-th patch being classified into class j. Pooling operations aggregate these probabilities across patches. Let $P \\in \\mathbb{R}^{N \\times C}$ denote the matrix of probabilities, where N is the number of patches and C is the number of classes:\n$z_j = \\frac{1}{N} \\sum_{i=1}^N p_i(j), \\quad j \\in \\{1, ..., C\\}$.\n(6)\nThe aggregated probabilities form a feature vector $z \\in \\mathbb{R}^C$, which is subsequently input to the classical classifier."}, {"title": "4) Classical classifier:", "content": "In the final stage of Q-ASC, the pooled feature vector $z \\in \\mathbb{R}^C$ is processed by a classical classifier. A fully connected layer with weights $W \\in \\mathbb{R}^{C \\times m}$ and biases $b \\in \\mathbb{R}^C$ transforms the feature vector into logits $u = Wz + b$. The softmax function is applied to these logits to compute the predicted probabilities for each class:\n$\\hat{y}_j = \\frac{e^{u_j}}{\\sum_{k=1}^C e^{u_k}}, \\quad j \\in \\{1, ..., C\\}$.\n(7)\nThe final predicted acoustic scene class $\\hat{c}$ is determined by identifying the class with the highest predicted probability:\n$\\hat{c} = \\arg \\max_j \\hat{y}_j$.\n(8)"}, {"title": "B. QVAE-based data augmentation", "content": "The QVAE-based data augmentation for ASC aims to generate synthetic acoustic scenes to enrich the training dataset, thereby improving the generalization capabilities of the Q-ASC model, especially in scenarios with limited labeled data.\nAs detailed in Algorithm 1, we begin by sampling a latent vector z from a prior distribution $p(z)$, which is typically chosen to be a standard normal distribution $\\mathcal{N}(0, I)$. This latent vector serves as a compact representation of the acoustic scene. The quantum encoder, represented by the parameterized quantum circuit $U_{enc}(\\theta_{enc})$, maps the classical latent vector z to a quantum state $|\\psi_{latent}\\rangle$ in a lower-dimensional Hilbert space $\\mathcal{H}^{\\otimes m}$, where m < n. Mathematically, this can be expressed as an equation in Line 6 of Algorithm 1.\nThe quantum encoder $U_{enc}(\\theta_{enc})$ is a unitary operator composed of a sequence of quantum gates, whose parameters $\\theta_{enc}$ are learned during training. The latent quantum state $|\\psi_{latent}\\rangle$ is then measured in the computational basis $\\{|0\\rangle, |1\\rangle, ..., |2^m - 1|\\}$. The probability of measuring the j-th basis state is given by:\n$p_j = |\\langle j | \\psi_{latent} \\rangle|^2$.\n(9)\nThis results in a probability distribution $p = [p_0, p_1, ..., p_{2^m-1}]$, which represents the compressed information about the acoustic scene.\nThe classical decoder, a neural network $D(\\theta_{dec})$, takes the probability distribution p as input and generates a synthetic mel-spectrogram patch $\\hat{x}_i$. This can be expressed as an equation in Line 8 of Algorithm 1. The decoder's parameters $\\theta_{dec}$ are also learned during training. Finally, an inverse Short-Time Fourier Transform (ISTFT) is defined as:\n$\\hat{x}(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\sum_{m=-\\infty}^{\\infty} \\hat{X}_i(m, \\omega) w(t - mH) e^{j\\omega t} d\\omega$,\n(10)\nwhere $\\hat{x}(t)$ is the reconstructed time-domain signal, $\\hat{X}(m, \\omega)$ is the complex-valued STFT of the signal, $w(t)$ is the window function used in the STFT, H is the hop size (the time shift between successive STFT frames), m is the frame index, and $\\omega$ is the angular frequency. Equation (10) is applied to the synthetic mel-spectrogram patch $\\hat{x}_i$ to obtain the corresponding synthetic audio waveform $\\hat{a}_i$."}, {"title": "III. EXPERIMENTS AND RESULTS", "content": ""}, {"title": "A. Experimental settings", "content": "We evaluated Q-ASC using the TUT Acoustic Scenes 2016 dataset, which includes 10-second recordings from 15 diverse acoustic scenes, totaling 4680 clips with about 312 per class. These scenes include bus, cafe/restaurant, car, city center, forest path, grocery store, home, lakeside beach, library, metro station, office, residential area, train, tram, and urban park. To test noise robustness, we added white Gaussian noise at signal-to-noise ratios (SNRs) ranging from 0 to 20 dB. For data limitation studies, we varied training set sizes from 10% to 100% of the dataset. Each audio was converted into a 32x32 pixel mel-spectrogram patch for input into the QiT. Besides, Table I outlines the various configurations of the Q-ASC model and its corresponding QVAE employed in our experiments. Quantum components were implemented in Qiskit with simulation on Qiskit Aer's qasm_simulator, while classical components were handled by PyTorch on a GPU-equipped cluster."}, {"title": "B. Evaluation of Various System Configurations", "content": "The performance of Q-ASC configurations on the TUT Acoustic Scenes 2016 dataset, as detailed in Table II, demonstrates that Q-ASC models outperform the baseline in all noise conditions. Specifically, Q-ASC (6 qubits) leads with an accuracy of 88.5% in clean conditions and consistently performs better across noisy environments, achieving 76.9% at 5 dB SNR, 81.3% at 10 dB, 84.7% at 15 dB, and 86.8% at 20 dB. In comparison, the baseline model's accuracy ranges from 82.5% in clean conditions to 68.3% at 5 dB SNR, with incremental improvements up to 80.2% at 20 dB SNR. Among Q-ASC configurations, Q-ASC + QVAE also shows strong performance with 87.2% accuracy in clean conditions and holds a competitive edge in noisy settings, while Q-ASC (5 layers) and Q-ASC (angle encoding) have slightly lower accuracies, indicating that specific Q-ASC enhancements are critical for robust performance.\nFurthermore, our Q-ASC's performance is rigorously benchmarked against state-of-the-art baselines, including a VGG-16 based CNN, a ResNet-18 model pre-trained on AudioSet, the Audio Spectrogram Transformer (AST), and an ensemble of the CNN and LSTM models. All models are trained using the Adam optimizer with an initial learning rate of 0.001, employing a step-wise learning rate scheduler that reduces the learning rate by a factor of 0.5 every 10 epochs without improvement on the validation set. We utilize early stopping with a patience of 5 epochs to prevent overfitting. For the quantum circuits in Q-ASC, gradients are estimated using the parameter-shift rule. The primary evaluation metric is classification accuracy, supplemented by precision, recall, and F1-score to provide a nuanced performance assessment across different acoustic scenes."}, {"title": "IV. CONCLUSION", "content": "This paper presented Q-ASC, a pioneering Quantum-Inspired Acoustic Scene Classifier, designed to address the challenges inherent in noisy and data-scarce environments. By combining quantum-inspired transformers with QVAE for synthetic data augmentation, Q-ASC achieved a significant boost in classification accuracy, reaching 68.3% to 88.5% on the TUT Acoustic Scenes 2016 dataset and outperforming the existing state-of-the-art by over 5% in the best case. These results signified a substantial advancement in the field of acoustic scene classification, offering promising applications in industrial monitoring, environmental sound analysis, and healthcare. Future research should focus on further enhancing Q-ASC's capabilities through the integration of self-supervised learning techniques and multi-modal data fusion. Such advancements could improve the classifier's robustness and adaptability across diverse and complex acoustic environments, thereby extending its applicability and effectiveness in real-world scenarios."}, {"title": "C. Comparative Analysis with Other Models", "content": "We compared Q-ASC with baselines such as VGG-16\nCNN, ResNet-18, AST, and CNN + LSTM Ensemble. Q-\nASC consistently outperforms all models across epochs,\ndemonstrating superior feature learning for ASC, enhanced\nnoise resilience, and efficient training. Q-ASC's high early\naccuracy suggests shorter training times compared to other\nmodels. Although not detailed in the image, QVAE-based\ndata augmentation likely contributes to Q-ASC's perfor-\nmance by addressing data limitations. In contrast, tradi-\ntional models show steady but plateauing improvements,\nindicating their limitations in capturing complex acoustic\npatterns under noisy or limited data conditions. The CNN +\nLSTM Ensemble performs better than individual CNN and\nRNN models but still falls short of Q-ASC, highlighting the\nadvantages of the quantum-inspired approach.\nWhile Q-ASC proves effective, quantum-inspired models\nface challenges with computational complexity and resource\nrequirements, especially as qubit numbers and circuit depth\nincrease. The reliance on classical simulations may hinder\nscalability, underscoring the need for advanced quantum\nhardware to fully leverage Q-ASC's capabilities in the future."}]}