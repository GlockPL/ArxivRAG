{"title": "The Impact of Large Language Models on Open-source Innovation: Evidence from GitHub Copilot", "authors": ["Doron Yeverechyahu", "Raveesh Mayya", "Gal Oestreicher-Singer"], "abstract": "Generative AI (GenAI) has been shown to enhance individual productivity in a guided setting. While it is also likely to transform processes in a collaborative work setting, it is unclear what trajectory this transformation will follow. Collaborative environment is characterized by a blend of origination tasks that involve building something from scratch and iteration tasks that involve refining on others' work. Whether GenAI affects these two aspects of collaborative work and to what extent is an open empirical question. We study this question within the open-source development landscape, a prime example of collaborative innovation, where contributions are voluntary and unguided. Specifically, we focus on the launch of GitHub Copilot in October 2021 and leverage a natural experiment in which GitHub Copilot (a programming- focused LLM) selectively rolled out support for Python, but not for R. We observe a significant jump in overall contributions, suggesting that GenAI effectively augments collaborative innovation in an unguided setting. Interestingly, Copilot's launch increased maintenance-related contributions, which are mostly iterative tasks involving building on others' work, significantly more than code-development contributions, which are mostly origination tasks involving standalone contributions. This disparity was exacerbated in active projects with extensive coding activity, raising concerns that, as GenAI models improve to accommodate richer context, the gap between origination and iterative solutions may widen. We discuss practical and policy implications to incentivize high-value innovative solutions.", "sections": [{"title": "1. Introduction", "content": "Rapid advances in Generative Artificial Intelligence (GenAI) technologies are promising to transform various aspects of economic activities. Burgeoning research has explored how GenAI can affect the future of work and labor markets (Brynjolfsson et al., 2018; Eloundou et al., 2024), productivity and creativity (Brynjolfsson et al., 2023; Noy and Zhang 2023; Zhou and Lee 2024, Doshi and Houser 2024), and knowledge dissemination (Burtch et al., 2024; Horton 2023; Quinn and Gutt 2023).\nAn intriguing gap within the burgeoning literature is the lack of empirical investigation into GenAI's impact on the collaborative work environment\u2014a setting where multiple contributors make a variety of contributions to different aspects of a project. Most of what people do in a collaborative setting often involves a blend of origination tasks where solutions are conceptualized and executed from scratch and iterative tasks, where solutions are built on top of existing work by others.\u00b9 While GenAI can improve individual productivity in guided settings (Brynjolfsson et al. 2023; Noy and Zhang 2023) and has shown mixed outcomes on individual creativity (Chen and Chan, 2023; Doshi and Houser, 2024; Zhou and Lee, 2024), a clear intuition of GenAI's impact on collaborative tasks is missing. The divergent findings on individual tasks suggest that the impact of GenAI on collaborative innovation are likely to be complex.\nThis research adopts a unique empirical approach to provide key insights into the nature of GenAI's effects on collaborative innovation. Our approach is motivated by the intuition that GenAI is likely to excel in iterative tasks that build upon others' work because it assists in understanding the context of other's work better, compared to origination tasks where tasks need to be conceptualized and executed from scratch. To excel in iterative tasks, one needs to understand the solution that other have provided in prior iterations and refine it in the current iteration. Such tasks benefit from \"interpolative\" thinking (i.e., inside-the-box thinking), which involves finding solutions within existing knowledge inside the task. To excel in origination tasks, one needs to conceptualize and architect the solutions from scratch without a precedence"}, {"title": "2. Literature Survey", "content": "2.1 Early Impact of GenAI on Productivity and Creativity\nSeveral well-run experiments provide compelling evidence that GenAI positively impacts individual productivity in guided tasks. For example, Brynjolfsson et al. (2023) examined the staggered deployment of a chat assistant using data from over 5,100 agents at a Fortune 500 software firm that provides business process software. They noted that productivity, measured as issues resolved per hour, increased by 14% on"}, {"title": "2.2 Interpolative and Extrapolative Solutions", "content": "The tasks and its solution-space can be broadly categorized into two distinct types: iterative tasks needing interpolative solutions and origination tasks needing extrapolative solutions (Schockaert and Prade 2013). Interpolative solutions are derived from existing knowledge and patterns. For example, solutions to tasks such as customer service and routine writing often involve searching through past solutions to address present issues (Brynjolfsson et al. 2023, Noy and Zhang 2023). This method relies on interpolative or within-the-box thinking, where the task is largely about finding the closest match to the present issue from"}, {"title": "3. Research Context and Data", "content": "3.1 Empirical Context: The Natural Experiment\nOur investigation contrasts the community contributions to open-source packages in Python and R, the two prominent open-source data-oriented programming languages, each significantly influencing various programming fields. They are both cultivated by robust, active communities, a factor that contributes"}, {"title": "3.2 Data Collection", "content": "We compile two primary datasets on which the analyses are performed.\nThe Updates Dataset: We collected the version releases of packages and the timing of these releases, from October 2019 to October 2022, a period spanning two years before and one year after the introduction of GitHub Copilot. For Python packages, we focused our analysis on the 2000 most popular packages as of 2021 (van Kemenade et al. 2021) 8. The detailed data on these 2000 packages were retrieved from the Python Package Index (PyPI), a central repository to which developers upload their open-source Python packages. Similarly, we collected data for all R packages from the Comprehensive R Archive Network (CRAN), a central repository for R packages 9. We excluded packages that had not been updated in the two years preceding GitHub Copilot's release, as they were unlikely to be influenced by this new tool. This resulted in the removal of 348 Python packages and over 7,965 R packages. We dropped the top 1% of the Python packages and the equal count of R packages, as they had extreme high activity pre-policy. After the matching (described in Section 3.1), we are left with 1610 Python and its matched R packages. We retrieved the package update data for the Python and R packages from PyPI and CRAN repositories, respectively, and aggregated the data at a quarterly level to account for the cyclic nature of package releases."}, {"title": "3.3 Commit Classification using Large Language Models", "content": "Annotating over 622,000 GitHub commits into specific categories is a challenging task given the absence of pre-defined criteria and established categorization for commits. GitHub commit messages are often unstructured and do not explicitly convey the type of change introduced. Understanding the types of code committed by developers is, however, central to understanding whether and how LLMs impact the trajectory of innovation on open-source projects. To address this challenge, we employed an LLM approach for efficient and accurate multi-label classification of commit comments."}, {"title": "4. Identification Strategies", "content": "4.1 Classical Propensity Score Matching with Difference in Differences Technique\nGiven that our natural experiment has a well-defined, treated group (i.e., Python packages) and a control group (i.e., R packages) with one-shot policy adoption, we employ the classic Difference-in-Differences (DiD) framework (Meyer 1995) and estimate the Average Treatment Effect on Treated (ATT) using a two- way fixed effects (TWFE) model. One potential threat to causal estimation using DiD in a natural experiment setting is that the assignment of treatment could be non-random. In our context, while the assignment of treatment (i.e., language supported by GitHub Copilot) was a business decision and is therefore exogenous to the package themselves, the choice for the package to be on a programming language is likely endogenous. To address the potential selection, we employed the Propensity Score Matching to pre-process the two package groups (Dehejia and Wahba 2002), a well-recognized technique in the literature. Specifically, we use the nearest neighbor method without replacement to obtain one control"}, {"title": "4.1.1 Testing the Parallel Trends", "content": "To ensure that the treated and control packages have a similar trend before the release of GitHub Copilot, we employ the lead-lag model proposed by Autor (2003). Specifically, we treated the quarter before the launch of GitHub Copilot as the baseline and estimated the lead-lag model as follows:\nlog (yit + 1) = ai + Yt + \u03a3\u03b1 PythonPackage\u00a1 X RelativeQuarterj + Nit (2)"}, {"title": "3.2 Synthetic Difference in Differences (SDiD) as an Alternative Identification Strategy", "content": "We extend our empirical analysis by employing Synthetic Difference-in-Differences (SDiD), a recent and cutting-edge framework introduced by Arkhangelsky et al. (2021). SDiD is a cutting-edge non-parametric estimation technique that relies on creating a \u201csynthetic\u201d control group (Abadie et al. 2010). By juxtaposing the treated unit with this synthetically generated control group, SDiD estimates the Average Treatment Effect (ATE). SDiD also generates weights to time-periods such that observations closer to the treatment periods receive higher weights compares to observations which are a few quarters away. This approach allows us to rigorously assess the impact of our intervention and has been applied in several recent studies (Berman and Israeli, 2022; Lambrecht et al., 2023).\nFormally, we follow Arkhangelsky et al. (2021) and analyze a balanced panel with N units and T time periods, where the outcome for unit i in period t is donated by yit, and exposure to the binary treatment, the"}, {"title": "5. Results", "content": "5.1 The Effect of LLMs on the Volume of Open-Source Innovation\nFor the first research question, which measures the impact of GitHub Copilot on the volume of innovation, we use the log-transformed value of count of updates per quarter as the dependent variable and estimate equation (1). Table 2 presents the estimation results of the ATT using the classical TWFE DiD (Column 1), a more restrictive TWFE DiD using the balanced panel sample because of the restriction of the SDiD strategy (Column 2), and the SDiD estimation (Column 3). The coefficient of 0.164 in column (1) means that GitHub Copilot increases the number of new package releases by 17.82% (calculated as (e(0.164) -1)*100=17.82%)). The results in columns (2) and (3) are consistent: the coefficients reflect a 9.52%, and 8.54% increase in the number of new Python package versions after the release of GitHub Copilot, compared to their matched R counterpart."}, {"title": "Table 2. The Effect on the Volume of Innovation", "content": "Dependent Variable\nModels\nPythonPackage; X afterCopilott\nTime Fixed Effect\nPackage Fixed Effect\n# of Obs."}, {"title": "5.2 The Effect of LLMs on the Type of Innovation", "content": "To study how the programming focused LLMs affect two of the most relevant programming tasks (code development and maintenance), we modify equation (1) into a triple difference model to contrast the trajectory of maintenance commits with code development commits. We formalize the model as follows:\nlog(yijt + 1) = aij + Yt + \u1e9e\u2081PythonPackage\u00a1 X afterCopilott\n+B2PythonPackage\u00a1 X CategoryMaintenance; X afterCopilott (3)\n+Y\u0165 X CategoryMaintenance; X afterCopilott + Nijt\nwhere i indexes the package, j indexes the category and t indexes the quarter. The dependent variable, yijt is the number of commits for the package i made in the category j and in a quarter t. We introduce a triple difference term (i.e., PythonPackage X afterCopilotit X Category_Maintenance) where the dummy Category_Maintenance; carries a value of 1 for the maintenance commit count and 0 for the code development commit count.\nWe present the results in Table 3. Our TWFE estimation reveals that, while code development commits increase by 16.76% (calculated as (e(0.155) -1)*100 = 16.76%), the increase in maintenance commit surpasses the features development commits by 15.14% (calculated as (e(0.141) -1)*100 = 15.14%)\u00b9\u00ba. We find similar results using Balanced TWFE model, as shown in column (2) of the table.\u00b9\u00b9 Overall, we see a persistent difference in the increase in maintenance compared with the increase in code development. Finally, we find"}, {"title": "5.3 A deep dive into LLM-Augmented Contributions with better Context Provision", "content": "As noted earlier, contributors may be drawn to use LLMs on projects with more user activity footprint, given the abundant resources available to facilitate the creation of precise prompts for LLMs. Alternatively, simpler projects with fewer dependencies and clearer features may better capitalize on LLM assistance. These projects often benefit from a more straightforward review process and quicker turnaround, making them more conducive to experimentation with LLM-generated solutions. A priori, it is unclear whether LLMs affect projects with above or below-median user activities.\nTo understand how these two types of contributions vary depending on user activity footprint, 12 we broke down the analysis into sub-samples based on the project-activity levels (proxied by the pre-treatment package release volume). After splitting the packages into two groups based on the median volume of pre- treatment activity, we use equation (3) that utilizes the triple difference model to contrast the trajectory of maintenance commits compared to code development commits for the above or below median sub-sample.\nWe present the results in Table 4. We observed a consistent pattern: in both the above-median and below-median user-activity packages, maintenance contributions significantly outweigh code development contributions. However, the gap between these two types of contributions is starker for packages with above-median user activities. Specifically, contrasting the DiD coefficients in columns (1) and (3), the difference between the code development and maintenance contributions is starker for the above-median projects (column 1) compared to the below-median projects (column 3). Specifically, while more-active projects saw a 35.93% increase in code development compared to less-active projects (at 2.83% statistically insignificant increase), they saw a significantly larger increase in maintenance commits at 22.01%, compared to the increase in less-active projects (at 8.32%).\nTogether, these results suggest that Copilot's assistance is the most beneficial for maintenance and refinement activities in projects with high coding related activity, which tend to provide rich context to"}, {"title": "Table 4. The Effect on the type of Innovation\u2014Subsample based on Popularity", "content": "Dependent Variable\nModels"}, {"title": "5.4 Robustness Checks", "content": "We perform several additional analyses, apart from Synthetic DiD. We repeated our analyses by estimating using alternative model specifications (non-logged dependent variables), testing for temporal variation (dropping initial quarters, and shifting treatment timing) and ruling out some alternative explanations including the commit rollbacks."}, {"title": "5.4.1 Alternative Model Specification: Raw value of Dependent Variable", "content": "Appendix Table A2 presents our analysis for the impact of LLMs on the volume of open-source innovation using the raw value of the dependent variable instead of the logged version. The TWFE DiD estimation (column 1) shows a 0.248 boost in quarterly new version releases for Python packages compared to matched R packages. This effect is robust across specifications, with the balanced panel TWFE and SDiD models showing increases of 0.11 and 0.11, respectively (columns (2) and (3)). With the count of commits as the dependent variable, the TWFE DiD model indicates a 6.296 increase in commits for Python packages (column (4)). The increase is consistent across balanced TWFE and SDiD analyses in columns (5) and (6), respectively. Similarly, Appendix Tables A3 and A4 replicates Tables 3 and 4 but with the raw value of the dependent variables. The results are consistent with the main tables, giving us confidence that the results are robust to alternative model specifications."}, {"title": "5.4.2 Temporal Variations: Dropping Initial three Quarters (0,1 and 2)", "content": "To avoid selection from early adopters of GitHub Copilot that may result in unique usage patterns, we perform the analysis by dropping the initial 3 quarters when GitHub Copilot was in its technical preview period. We present the results of the first specification in Appendix Table A5. The TWFE DiD estimation (column 1) shows a 21.4% boost in quarterly new version releases (calculated as (e(0.194) -1)*100 = 21.4%) for Python packages compared to matched R packages. This effect is robust across specifications, with the balanced panel TWFE and SDiD models showing increases of 10.1% and 9%, respectively (columns (2) and (3)) as well as with the count of commits as the dependent variable. Similarly, Appendix Tables A6 and A7 replicates Tables 3 and 4 but after dropping the quarters 0 through 2, as in Appendix Table A5. The results are consistent with the main specification and have larger coefficients, giving us confidence that the results are not primarily driven by the initial adopters of GitHub Copilot. The larger coefficients, in fact, point towards an accelerating trend of contributions towards Python packages owing to LLM assistance."}, {"title": "5.4.3 Temporal Variations: Moving the Treatment to June 2021", "content": "While GitHub Copilot was made available via various IDEs in October 2021, the announcement happened in June 2021 with a selective set of programmers receiving early access. Hence, we advance our treatment window by one quarter (to June 2021) to test the robustness of our estimation. Appendix Table A8 presents our analysis of the impact of LLMs on the volume of open-source innovation. The TWFE DiD estimation (column 1) shows a 17.82% boost in quarterly new version releases (calculated as (e(0.164) -1)*100= 17.82%) for Python packages compared to matched R packages. This effect is robust across specifications, with the balanced panel TWFE and SDiD models showing increases of 9.52% and 9%, respectively (columns (2) and (3)), as well as across the count of commits as the DV. Similarly, Appendix Tables A9 and A10 replicate Tables 3 and 4, but with June 2021 as the treatment quarter. The results are consistent with the main specification."}, {"title": "5.4.4 (Ruling out) Alternative Explanation: Analysis using Commit Rollbacks", "content": "Finally, to rule out the possibility that the increase in maintenance contributions is driven from an increase in the number of errors introduced by GitHub Copilot itself, we explore the effect of the release of GitHub Copilot on the number of rollbacks. Specifically, we conduct a difference-in-differences analysis where the dependent variable is the number of commits with commit comments containing the word \u201crollback,\u201d referring to reversing a previously committed change to the GitHub repository. The estimation result for this analysis is presented in Appendix Table A11. We find that the number of rollbacks did not significantly change after the release of GitHub Copilot."}, {"title": "6. Discussion and Conclusion", "content": "6.1 Discussion\nOur work provides critical new insights regarding the causal effects of LLMs on innovation and provides a much-needed framework for understanding what to look for in a task to predict the impact of GenAI on that task, a question marked with contrasting outcomes in literature. As LLM adoption accelerates among programmers and software developers, understanding the impact of such LLMs on innovation in this space is critical. Extant research has highlighted the positive impact of LLMs on the swiftness of programming"}, {"title": "6.2 Implications", "content": "Our project has many theoretical and practical implications. To the best of our knowledge, we are among the first to leverage the natural experiment related to the rollout of GitHub Copilot to answer questions related to the impact of LLMs on open innovation. Unlike some of the earlier findings that report a precipitous drop in knowledge dissemination and exchange on User Generated Platforms (UGCs), we note an increase in the open-source commits after the LLM release. Furthermore, we extend extant research on"}, {"title": "6.3 Limitations", "content": "Our current findings have certain limitations common across all research related to innovation. Our commits only capture successful pull requests, in that the project's owner accepted the changes some contributor made. The rejected pull requests, just as failed innovation efforts elsewhere, are not easily visible, which may bias the results. We are investigating ways to address this bias, which may also be an exciting avenue for future work. Another notable limitation of our analysis is that not every contributor might have used Copilot, meaning our findings likely represent a lower bound of the true effect. As LLM adoption increases, we expect an even more significant effect on innovation patterns. To this end, our results present a lower bound. Based on GitHub's announcement, we know that the adoption is in millions. As a next step, researchers could work with GitHub to obtain information about the sign-up details on GitHub Copilot. Additionally, generalizing these findings to other innovation domains than open-source requires caution, as the unique characteristics of open-source innovation, such as voluntary contributions and high degree of autonomy given to contributors, may not fully translate to other settings with varied motivations and organizational structures.\nLimitations notwithstanding, our current findings have significant implications for open innovation. The imminent step for this research agenda is understanding the mechanism by which LLMs disproportionately affect iterative contributions via maintenance over origination contributions via code development. It could be that GitHub copilot makes it easier for contributors by instilling confidence about identifying the flaw in the logic when working with an AI \u201cpair programmer.\u201d Extant research on pair programming suggests that such a practice reduces subtle errors that make debugging much more difficult (Cockburn and Williams 2000). The reduction in the cognitive load of finding and rectifying complex bugs may be driving these findings."}]}