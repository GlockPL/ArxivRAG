{"title": "Towards Physics-Guided Foundation Models", "authors": ["Majid Farhadloo", "Arun Sharma", "Mingzhou Yang", "Bharat Jayaprakash", "William Northrop", "Shashi Shekhar"], "abstract": "Traditional foundation models are pre-trained on broad\ndatasets to reduce the training resources (e.g., time, energy,\nlabeled samples) needed for fine-tuning to a wide range of\ndownstream tasks. However, traditional foundation models\nstruggle with out-of-distribution prediction and can produce\noutputs that are unrealistic and physically infeasible. We\npropose the notation of physics-guided foundation models\n(PGFM), that is, foundation models integrated with broad or\ngeneral domain (e.g., scientific) physical knowledge applica-\nble to a wide range of downstream tasks.", "sections": [{"title": "Introduction", "content": "Driven by the availability of large-scale datasets, advance-\nments in computational power, and innovations in deep\nlearning architectures, traditional foundation models (FMs)\nhave significantly advanced the field of artificial intelligence\n(Chen et al. 2020; Vaswani 2017). For a comprehensive sur-\nvey on FMs and their history, interested readers may refer to\n(Zhou et al. 2024).\nLimitations of Foundation Models: Despite their ver-\nsatility, purely data-driven FMs exhibit major limitations in\nscientific and engineering domains. One is their struggle\nwith out-of-distribution situations, in which their benefits\nto downstream applications may be limited. This shortcom-\ning is particularly problematic in specialized tasks that re-\nquire nuanced understanding and adaptation to specific do-\nmain knowledge, such as climate science and healthcare. For\ninstance, the Geospatial Foundation Model (Prithvi) (Hsu,\nLi, and Wang 2024), trained exclusively on satellite im-\nagery data from Landsat and Sentinel with multi-spectral\nbands, may fail to generalize to other data types, such\nas hyperspectral imagery or newer satellites with different\nspectral resolutions. Second, purely data-driven foundation\nmodels (FMs) often violate fundamental physical principles\nsuch as energy conservation and motion dynamics. Without\nphysics-based constraints, these models produce unrealis-\ntic and physically infeasible outputs. For example, models\ntrained to estimate energy consumption and generate veloc-\nity profiles using transportation data such as onboard diag-\nnostics and trajectory datasets may display rapid and un-\nrealistic variations in velocity."}, {"title": "Vision", "content": "A physics-guided foundation model is a foundational model\nincorporating broad domain knowledge and a general un-\nderstanding of the domain's fundamental concepts and prin-\nciples. For example, popular knowledge graphs and tax-\nonomies (Ji et al. 2021) may strengthen foundation models\nfor a broad range of downstream applications. Also, con-\nservation laws (e.g., mass, energy) apply to a broad range\nof downstream physical science tasks. Concepts of speed,\nacceleration, and laws of motion are applicable to a broad\nrange of tasks related to moving objects. Maxwell's laws are\nalso widely applicable to tasks related to electromagnetism.\nA generalizable model of biogeochemistry may be used as a\nPGFM with extensive data training.\nA variety of approaches can be employed to embed physi-\ncal knowledge into foundation models. Physics-constrained\nlearning enforces real-world constraints by introducing do-\nmain rules into the learning process through the loss func-\ntion or regularization that safeguards model predictions to\nensure physical feasibility (e.g., (Li et al. 2023)). Similarly,\narchitecture-level integration (e.g., (Li et al. 2023; Jia et al.\n2019)) incorporates domain principles directly into model\nstructures, aligning network design with physics-based rules\nand patterns. In recent work (Li et al. 2023), we demon-\nstrated that incorporating a physical constraint, such as a\njerk penalty, in velocity profiling reduced the sharp fluctu-\nations common in unconstrained models"}, {"title": "Conclusion and Future Work", "content": "PGFM enhances foundation models by systematically inte-\ngrating domain-specific physcial knowledge, improving per-\nformance, robustness, and interpretability for diverse appli-\ncations. Future research in PGFM offers promising advance-\nments in AI across fields like healthcare (Farhadloo et al.\n2024b), geospatial analysis, and engineering. We plan to\ndevelop and evaluate a PGFM model, focusing on domain\nknowledge integration using metrics like performance sam-\nple complexity.\nAdvancing PGFM in future work will involve develop-\ning hybrid frameworks that combine the adaptability of\nretrieval-augmented generation (Lewis et al. 2020) with\nreal-time updates to optimize domain adaptation and reduce\ntraining resource demands. We will also explore PGFMS\nin the context of recent multi-modal foundation models\n(Ravirathinam et al. 2024), focusing on integrating physi-\ncal knowledge (Sharma et al. 2024; Sharma and Shekhar\n2022; Sharma et al. 2022) from diverse input sources and\nits impact on training procedures and downstream applica-\ntions. We will also explore additional criteria for compar-\ning PGFMs with other machine learning models, including\ntraining methods and challenges such as model complexity,\ncomputational requirements, location dependency, and cal-\nibration needs (Farhadloo et al. 2024a,b). In the long term,\nwe aim to address challenges such as explainability (Arri-\neta et al. 2020; Farhadloo et al. 2022), robustness, bias, data\nquality, and adaptability."}]}