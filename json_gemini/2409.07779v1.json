{"title": "ASSNet: Adaptive Semantic Segmentation Network\nfor Microtumors and Multi-Organ Segmentation", "authors": ["Fuchen Zheng", "Xinyi Chen", "Xuhang Chen", "Haolun Li", "Xiaojiao Guo", "Guoheng Huang", "Chi-Man Pun", "Shoujun Zhou"], "abstract": "Medical image segmentation, a crucial task in com-\nputer vision, facilitates the automated delineation of anatomical\nstructures and pathologies, supporting clinicians in diagnosis,\ntreatment planning, and disease monitoring. Recent advance-\nments in deep learning, particularly convolutional neural net-\nworks (CNNs) with U-shaped architectures, vision transformers,\nand Segment Anything Models (SAM), have significantly en-\nhanced segmentation accuracy. Notably, transformers employing\nshifted window-based self-attention have demonstrated excep-\ntional performance. However, their reliance on local window\nattention limits the fusion of local and global contextual informa-\ntion, crucial for segmenting microtumors and miniature organs.\nTo address this limitation, we propose the Adaptive Semantic\nSegmentation Network (ASSNet), a transformer architecture\nthat effectively integrates local and global features for precise\nmedical image segmentation. ASSNet comprises a transformer-\nbased U-shaped encoder-decoder network. The encoder utilizes\nshifted window self-attention across five resolutions to extract\nmulti-scale features, which are then propagated to the decoder\nthrough skip connections. We introduce an augmented multi-\nlayer perceptron within the encoder to explicitly model long-\nrange dependencies during feature extraction. Recognizing the\nconstraints of conventional symmetrical encoder-decoder designs,\nwe propose an Adaptive Feature Fusion (AFF) decoder to\ncomplement our encoder. This decoder incorporates three key\ncomponents: the Long Range Dependencies (LRD) block, the\nMulti-Scale Feature Fusion (MFF) block, and the Adaptive\nSemantic Center (ASC) block. These components synergistically\nfacilitate the effective fusion of multi-scale features extracted\nby the decoder while capturing long-range dependencies and\nrefining object boundaries. Comprehensive experiments on di-\nverse medical image segmentation tasks, including multi-organ,\nliver tumor, and bladder tumor segmentation, demonstrate that\nASSNet achieves state-of-the-art results.", "sections": [{"title": "I. INTRODUCTION", "content": "By accurately delineating anatomical structures, segmen-\ntation enhances the clarity of medical images, significantly\nimproving diagnostic accuracy and efficiency for healthcare\nprofessionals. Current research in medical image segmentation\nfocuses on critical tasks such as tumor segmentation and organ\ndelineation. Consequently, neural network architectures from\nthe broader field of computer vision are being increasingly\nadapted for medical image analysis. Vision transformers [1],\nexemplified by the Swin-transformer [2], have gained sig-\nnificant traction due to their robust feature extraction capa-\nbilities. However, while advancements in window attention\nmechanisms within these transformers have yielded impressive\nresults, challenges remain. These models often struggle to\ncapture features of small objects due to limitations in modeling\nlong-range dependencies [3] and accurately delineating image\nedges. Consequently, effectively integrating multi-scale local\nand global features remains an ongoing challenge.\nTo address these limitations, we propose the Adaptive Se-\nmantic Segmentation Network (ASSNet), a novel Transformer-\nbased [4] architecture specifically designed for medical image\nsegmentation. Inspired by the strengths of ResUnet [5] and\nSwin-transformer [2], ASSNet leverages Transformer blocks\nwithin a U-shaped residual structure to enhance feature learn-\ning across multiple scales.\nFurthermore, recognizing the limitations of simply repli-\ncating encoder structures in the decoder, we introduce a\nnovel Adaptive Feature Fusion (AFF) decoder. This decoder\ncomprises three key components: the Long Range Depen-\ndencies (LRD) block, the Multi-Scale Feature Fusion (MFF)\nblock, and the Adaptive Semantic Center (ASC) block. These\ncomponents work synergistically to leverage encoder-derived\nfeatures effectively, enabling the accurate segmentation of\nsmall structures, particularly at edges, and facilitating robust\nmulti-scale feature fusion.\nOur straightforward residual U-shaped Transformer archi-\ntecture, without relying on complex multi-scale structures or\nintricate loss functions, achieves state-of-the-art performance\non various medical image segmentation tasks. Notably, ASS-\nNet surpasses previous state-of-the-art models, demonstrating\nimpressive improvements on the LiTS2017, ISICDM2019 and\nSynapse datasets, respectively. The main contributions of this\npaper are as follows:\n1. We introduce ASSNet, a hybrid model that combines\nthe strengths of ResUnet and Swin-transformer, incorporating\nwindow attention, spatial attention, U-shaped architecture, and\nresidual connections for efficient segmentation."}, {"title": "II. RELATED WORK", "content": "Medical image segmentation is a fundamental task that\ninvolves partitioning medical images into distinct regions\nrepresenting different anatomical structures or tissues. The U-\nNet architecture [6], with its elegant encoder-decoder structure\nand skip connections, has emerged as a leading approach for\nthis task. Its ability to capture both fine-grained details and\nglobal context has led to widespread adoption and numerous\nextensions. For instance, ResUNet [7] combines the strengths\nof U-Net [6] and ResNet [5], leveraging the power of residual\nconnections [8] to facilitate the training of deeper networks\nand mitigate the vanishing gradient problem [9]. These resid-\nual connections allow for unimpeded information flow across\nlayers, significantly enhancing the network's ability to learn\ncomplex representations and improve segmentation accuracy.\nThe success of ResUNet can be attributed to the synergistic\ncombination of residual and skip connections. While residual\nconnections ensure efficient information propagation across\nlayers, skip connections enable the fusion of features from\ncorresponding levels in the encoder and decoder pathways.\nThis fusion of multi-scale features is crucial for capturing\nboth local details and global context, leading to improved\nsegmentation performance. UNet++ [10] further refines the\nU-Net architecture by introducing dense skip connections and\na nested architecture. These connections enhance information\nflow between encoder and decoder layers, facilitating the\ncapture of multi-scale features and improving segmentation\naccuracy. Building upon these foundational concepts, ASSNet\nalso incorporates skip connections and residual connections to\noptimize its segmentation capabilities."}, {"title": "B. Vision Transformer and Hybrid Architectures", "content": "Unlike Convolutional Neural Networks (CNNs) that process\nimages locally, Vision Transformer (ViT) models [1] leverage\na self-attention mechanism to capture long-range dependencies\nwithin images. This global receptive field has enabled ViT\nto achieve state-of-the-art performance in image classification\ntasks. The success of ViT has inspired its adaptation to medical\nimage analysis, with Swin Transformer [11] demonstrating\nimpressive results in various medical imaging applications.\nThe Swin Transformer employs a hierarchical approach, com-\nputing self-attention within local windows and then shifting\nthese windows to capture relationships across different image\nregions. This strategy reduces computational complexity while\npreserving the ability to model long-range dependencies.\nRecognizing the complementary strengths of CNNs and\ntransformers, researchers have explored hybrid architectures\nthat combine these paradigms. ResT [12], [13] exemplifies this\ntrend, integrating ResNet and transformer modules to leverage\nthe feature extraction capabilities of both. In the realm of U-\nNet, the incorporation of attention mechanisms has proven\nbeneficial. For instance, Attention U-Net [14] introduces atten-\ntion gates that enable the network to focus on salient regions\nduring segmentation. This integration of attention mechanisms\nhas paved the way for U-shaped transformer architectures,\nsuch as TransUNet [4]. TransUNet utilizes a convolutional\nencoder to extract features and a transformer to model global\ncontext.\nHowever, a common limitation in TransUNet and similar\narchitectures like TransClaw [15] and TransAttUnet [16] is\nthe suboptimal integration of attention mechanisms, preventing\nthe full realization of the transformer's potential. To address\nthis, our proposed network introduces a novel residual U-\nshaped transformer architecture designed for effective atten-\ntion fusion. This architecture leverages the strengths of the\nwindow attention mechanism employed in Swin Transformer\nand enhances it with an Enhanced Forward Feedback Network\n(EFFN), resulting in superior performance for medical image\nsegmentation."}, {"title": "III. METHODOLOGY", "content": "This section outlines the architecture and functionality of\nASSNet. We first describe the network's overall pipeline,\nfollowed by a detailed exposition of the Multi-scale Win-\ndow Attention (MWA) Transformer block-the core encoder\ncomponent. Subsequently, we elucidate the Adaptive Feature\nFusion (AFF) decoder, which is crucial for modeling long-\nrange dependencies and enhancing the network's ability to\ncapture fine-grained details amidst complex edge structures."}, {"title": "A. Overall Pipeline", "content": "ASSNet leverages a hierarchical U-shaped architecture, as\nillustrated in Figure 1, incorporating skip connections and\nresidual connections between the encoder and decoder to\nfacilitate efficient information propagation.\nThe input image, of size $C \\times H \\times W$ (where C, H, and\nW represent channels, height, and width, respectively), is\nprocessed through patch partition and linear embedding layers\nbefore entering the window attention module embedded in the\nMWA block. Following the original Swin Transformer [11],\nthe encoder consists of four stages, each performing $2 \\times C$\nspatial downsampling in the patch merging layer. This layer\nconcatenates features from $2 \\times 2$ spatially neighboring patches\nand applies a linear projection to halve their dimension.\nThe decoder mirrors the encoder with four symmetrical\nstages, incorporating our proposed Adaptive Feature Fusion\n(AFF) decoder. The AFF decoder facilitates the fusion of\nhigh-level semantic information with low-level spatial details,\nsurpassing the decoders of current state-of-the-art models [11],\n[17], [18]. Finally, an output convolution layer processes the\nconcatenated features to generate the segmentation prediction."}, {"title": "B. MWA Transformer Block", "content": "The MWA Transformer block forms the backbone of ASS-\nNet. As shown in Figure 2, it substitutes the Multi-Head Self-\nAttention (MSA) module [19] in the standard Transformer\nlayer with a shifted window attention-based MSA module,\nwhile preserving other attention-related components. Each\nMWA block consists of a shifted window-based MSA module\nfollowed by an Enhanced Feed-Forward Network (EFFN).\nRecognizing the limitations of standard FFNs in capturing\nlocal context [20], [21], we enhance the MLP within our\nTransformer block by incorporating depth-wise and pixel-wise\nconvolutions [22]\u2013[24]. As depicted in Figure 2, the EFFN\nfirst projects input tokens to a higher dimensional space. The\nprojected tokens are then reshaped into 2D feature maps and\nprocessed by a 3\u00d73 pixel-wise convolution followed by a 3\u00d73\ndepth-wise convolution, effectively capturing local contextual\ninformation. Subsequently, the features are reshaped back into\ntokens and projected back to the original channel dimension.\nFinally, a GELU activation function [25] introduces non-\nlinearity.\nMathematically, the computation within an MWA trans-"}, {"title": "C. Adaptive Feature Fusion (AFF) Decoder", "content": "To address the limitations of vision transformers in cap-\nturing local dependencies [3], [26] and the inadequacies of\nexisting decoders in integrating multi-scale local and global\nfeatures [11], [17], [18], we propose an Adaptive Feature\nFusion (AFF) Decoder. The AFF decoder comprises Long-\nRange Dependencies (LRD) block, Multi-scale Feature Fusion\n(MFF) block, and Adaptive Semantic Center (ASC) block, as\nillustrated in Figure 3.\nThe AFF decoder begins with a standard deconvolution\noperation to restore the feature map to the original image size\nwhile preserving resolution. Subsequently, skip connections\nare employed to concatenate MWA encoder feature maps\nfrom different scales, enriching the feature map with multi-\nscale information. This enriched feature map then undergoes\nthree parallel operations. First, spatial attention using dilated\nconvolutions with dilation rates of 1, 6, 12, and 18 is applied\nto capture multi-scale details [27]. LeakyReLU [28] is utilized\nas the activation function in the decoder to mitigate the\nvanishing gradient problem and enhance model stability and\ngeneralization. Second, an LRD block, implemented using a\nseries of convolutions and LeakyReLU activations, models\nlong-range dependencies. Finally, inspired by ResNet [5], a\nthird parallel thread acts as a mask prompt, aiding the decoding\nprocess of the first two threads. The outputs from these three\nthreads are then concatenated and processed by a convolution\noperation.\nThe resulting feature map is then passed to the ASC\nblock. Drawing inspiration from the Sobel edge detection\nmethod [29], the ASC block extracts local region information\nand performs channel-wise enhancement. It achieves this by\nutilizing an enhanced filter generated from adaptive average\npooling [30] and a fully connected layer [31]."}, {"title": "D. Objective Function", "content": "During training, ASSNet employs the BCE Dice loss\n$L_{BD}$ [32], a combination of Binary Cross-Entropy (BCE)\nloss $L_{BCE}$ and Dice loss $L_{D}$, widely used in medical image\nsegmentation tasks. This loss function is defined as:\n$L_{BD} = L_{D} + L_{BCE}(Y,P)$\n$\\begin{aligned}\nL_{D} &= 1-\\frac{1}{N} \\sum_{i=1}^{N} \\frac{2 \\sum_{j} y_{i, j} p_{i, j}}{\\sum_{j} y_{i, j}+\\sum_{j} p_{i, j}} \\\\\nL_{BCE}(y, p) &= - (y \\log (p)+(1-y) \\log (1-p))\n\\end{aligned}$\nwhere y represents the ground truth segmentation mask, p\ndenotes the predicted segmentation mask, and N is the number\nof pixels in the image. BCE loss penalizes discrepancies\nbetween the predicted and actual label distributions, while\nDice loss encourages overlap between the predicted and actual\nsegmentation regions. This combination effectively enhances\nboth pixel-wise classification accuracy and boundary delin-\neation."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we present the experimental framework\nand discuss the results. First, we describe the datasets and"}, {"title": "E. Ablation Study", "content": "To investigate the contribution of each module within ASS-\nNet, we conducted an ablation study on the ISICDM2019 and\nLiTS2017 datasets. We used the same experimental setup as\ndescribed in Section IV-A and evaluated the performance of\nASSNet by removing one component at a time. The results,\nsummarized in Table III, demonstrate that all components\ncontribute to the overall performance of ASSNet.\nThe ablation study clearly shows that the Embedded Feature\nFusion Network (EFFN) significantly enhances ASSNet's abil-"}, {"title": "V. CONCLUSION", "content": "This paper presented ASSNet, a novel Transformer-based\narchitecture purpose-built for accurate and efficient medical\nimage segmentation. The hallmark of ASSNet lies in its\ninnovative MWA encoder and AFF decoder. By effectively in-\nte grating an efficient feedforward network block, a long-range\ndependency block, a multi-scale feature fusion block, and\nan adaptive spatial-channel attention block, ASSNet exhibits\nsuperior capacity in modeling long-range dependencies and\ncapturing salient features across varying scales and spatial con-\ntexts. Rigorous evaluation on three publicly available medical\nimage segmentation datasets demonstrated that ASSNet con-\nsistently achieves state-of-the-art segmentation performance,\nsurpassing existing methods in accurately delineating diverse\nanatomical structures and pathologies. These promising results\nhighlight ASSNet's potential as a robust and valuable tool\nfor assisting medical professionals in critical tasks such as\ndiagnosis, treatment planning, and disease monitoring. Future\nresearch will investigate the generalizability of ASSNet to\nother medical imaging modalities and further evaluate its\nperformance in more complex clinical scenarios."}], "equations": ["Attention(Q, K, V) = SoftMax\\left(\\frac{Q K^{T}}{\\sqrt{d}}+B\\right) V", "L_{BD} =L_{D} + L_{BCE}(Y,P)", "DSC = \\frac{2x PNG}{|P| + |G|}", "mIoU = \\frac{1}{C} \\sum_{i=1}^{C} \\frac{P_{i} \\cap G_{i}}{\\left|P_{i}\\right|+\\left|G_{i}\\right|-\\left|P_{i} \\cap G_{i}\\right|}", "\\begin{aligned}\n\\hat{X}^l &= W-MSA(LN(X^{l-1})) + X^{l-1}, \\\\\nX^l &= EFFN(LN(\\hat{X}^l)) + \\hat{X}^l, \\\\\n\\hat{X}^{l+1} &= SW-MSA(LN(X^l)) + X^l, \\\\\nX^{l+1} &= EFFN(LN(\\hat{X}^{l+1})) + \\hat{X}^{l+1},\n\\end{aligned}", "L_{BD} =L_{D} + L_{BCE}(Y,P)", "\\begin{aligned}\nL_{D} &= 1-\\frac{1}{N} \\sum_{i=1}^{N} \\frac{2 \\sum_{j} y_{i, j} p_{i, j}}{\\sum_{j} y_{i, j}+\\sum_{j} p_{i, j}} \\\\\nL_{BCE}(y, p) &= - (y \\log (p)+(1-y) \\log (1-p))\n\\end{aligned}"]}