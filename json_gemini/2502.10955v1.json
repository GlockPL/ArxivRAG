{"title": "A recurrent vision transformer shows signatures of primate visual attention", "authors": ["Jonathan Morgan", "Badr Albanna", "James P. Herman"], "abstract": "Attention has emerged as a core component of both biological and artificial intelligences (Als). Despite decades\nof parallel research, studies of animal and Al attention remain largely separate. The self-attention mecha-\nnism ubiquitous in contemporary Al applications is not grounded in biology, and the powerful capabilities of Als\nequipped with self-attention have yet to offer fresh insight into the biological mechanisms of attention. Here,\nwe offer a unifying perspective, drawing together insights from primate neurophysiology and contemporary ma-\nchine learning in a Recurrent Vision Transformer (Recurrent ViT). Our model extends self-attention by allowing\nboth input and memory to guide attention allocation. Our model learns purely through sparse reward feedback,\nemulating how animals must learn in a laboratory environment. We benchmark our model by challenging it to\nperform a spatially cued orientation-change detection task widely used to study attention in the laboratory and\ncomparing its performance to non-human primates (NHPs). The model exhibits hallmark behavioral signatures\nof primate visual attention improved accuracy and faster responses for cued stimuli, both scaling with cue\nvalidity. Analysis of self-attention maps reveals rich attention dynamics, with the model maintaining spatial pri-\norities through delays and stimulus onsets, reactivating them prior to anticipated change events. Perturbing\nthese attention maps produces performance changes that mirror effects of causal manipulations in primate at-\ntention nodes such as the frontal eye fields (FEF) or superior colliculus (SC). These findings not only validate\nthe effectiveness of integrating recurrent memory with self-attention for emulating primate-like attention, but also\nestablish a promising framework for probing the neural underpinnings of attentional control. Ultimately, our work\nattempts to bridge the gap between biological and artificial attention, paving the way for more interpretable and\nneurologically informed Al systems.", "sections": [{"title": "1 Introduction", "content": "Visual attention is a foundational cognitive func-\ntion that supports behavioral flexibility by allowing bio-\nlogical organisms to guide behavior selectively on the\nbasis of a subset of visual input. Perceptual judgments\nare more accurate and reaction times are faster for\nattended stimuli compared to unattended [1-5]. Neu-\nronal correlates include heightened spiking activity and\ndecreased spike-count correlations for attended stim-\nuli [6-10]. Classic paradigms use spatial cues to direct\nattention [11], where a delay separates cue and stimu-\nlus. This delay is crucial as it allows any difference in\nthe behavioral response to cued versus uncued stimuli\nto be ascribed to the organism's internal \"attentional\u201d\nstate. As a consequence of this delay, the cue's lo-\ncation must be maintained in visual working memory\n(VWM). Visual attention and VWM are unsurprisingly\nstrongly linked [12\u201315], as working memory contents\nguide attention and vice versa [16-22.\nTransformers [23\u201325] have achieved remarkable\nsuccess in both language and vision by employing self-\nattention mechanisms that bear a superficial resem-\nblance to how biological systems allocate attentional\nresources [26\u201328]. However, whether the self-attention\nin these models truly mirrors the selective, goal-driven\nattention observed in humans or non-human primates\nremains a subject of debate. For instance, while\ntransformers can exhibit human-like patterns in text-\nbased tasks [29], in vision they tend to emphasize\nlow-level grouping rather than task-dependent selec-\ntion [30]. Similarly, unsupervised methods such as"}, {"title": "2 Task and Model", "content": null}, {"title": "2.1 Cued Orientation Change Detection Task Environment", "content": "We trained our model on a spatially cued ori-\nentation change detection task similar to those used\nin primate neurophysiology labs Figure 1. Each trial\ncomprised 7 time steps. At each time step, a 50x50\ngrayscale image was shown to the agent. At t = 0,\nthe trial began with a black image. At t = 1, a spa-\ntial cue was displayed. The cue appeared at stimulus\nposition 1 (Cue S\u2081) in one-half of the trials and at po-\nsition 4 (Cue S4) in the other half. At t = 2, a black\nscreen was displayed again. At t = 3, stimulus onset\noccurred, displaying four \u201cGabor\u201d stimuli in randomly\nchosen orientations at fixed positions: top left (S1), bot-\ntom left (S2), top right (S3), and bottom right (S4). At\nt = 4, the stimuli remained unchanged with the ex-\nception of orientation \u201cnoise\u201d added in each time step\nof stimulus presentation to control task difficulty (see\nMethods). If the trial was a no-change trial, stimuli re-\nmain unchanged from t = 4 to t = 6. In a change\ntrial, at t = 5 the orientation of one of the four stimuli\nchanged by \u2206 degrees (where \u2206 varied from trial to\ntrial). The orientations then remained unchanged from\nt = 5 to t = 6. Half of all trials were \u201cchange trials\u201d and\nthe other half were \u201cno-change trials\u201d (balanced across\ncue presentation positions).\nVisually distinct cues indicated different levels of\n\"cue validity\": the probability of an orientation-change\nevent occurring at the cued position. Cue validity lev-"}, {"title": "3 Model", "content": "Our Recurrent ViT has three parts: the self-\nattention (SA) module, the patch-based long-short-\nterm-memory (LSTM) module (\"working memory\u201d mod-\nule), and an actor-critic RL agent. The environment\ngenerates the current visual scene and the agent\nconverts this scene to a visual representation, X(t),\nthrough low-level convolutional operations (see Sup-\nplement). Processing results in 4 visual patches:\nX(t) = {x(t)}_1^4, and this 4-patch structure is retained\nthroughout the attention and working memory modules\n(number of patches is a hyperparameter). The primary\nbenefit of our choice to structure the visual patches\naround stimulus positions is the interpretability it affords\nto the model's \u201cattention map\u201d. Specifically, it allows the\nattention map to be visualized as a 4x4 array in which\nwe can interpret as the bias, \u03b1(t), assigned to an in-\nternal representation (\u03be(t)) associated with stimulus S\u2081.\nAn important distinction here is that \u03b1(t) does not just\ndescribe the the attention on the current visual patch\nx(t)). Instead it describes the attention on an internal\nrepresentation, \u03be(t) that consists of the immediate vi-\nsual information in x(t) and information derived from ac-\ntivated memory describing relevant past temporal and\nspatial context, h(t-1).\nThe patch-based LSTM module receives a trans-\nmission, Z(t) = {z(t)}_1^4, that contains visual informa-\ntion derived from the immediate visual scene in addition\nto spatial and temporal context derived from the self-\nattention mechanism. This information is utilized to up-\ndate the internal states of the LSTM, C(t) = {c(t)}_1^4\n(see Methods). Activated memory, H(t) = {h(t)}_1^4\nis derived from these internal states and sent: (1) re-\ncurrently back into the LSTM; (2) to the self-attention\nmodule; (3) to the actor and critic networks. This allows\nattention to be allocated both on the basis of visual in-\nputs as in traditional transformer architectures [23,24],\nand on the basis of memory.\nTo understand which model architecture elements\nwere required to recapitulate primate-like attention, we\nalso tested several alternative models. We briefly de-"}, {"title": "4 Results", "content": null}, {"title": "4.1 A recurrent ViT exhibits behavior signature of visual attention", "content": "Our model exhibited orderly \"psychometric\u201d and\n\"chronometric\" functions with characteristic sigmoidal\nshapes commonly observed in human and NHP exper-\niments (Figure 3A,D). Larger orientation-change \u2206 val-\nues were associated with higher hit rates and shorter\nreaction times, qualitatively comparable to those seen\nin countless human and NHP psychophysics experi-\nments. For cued orientation changes, higher cue va-\nlidity improved correct response rate (Figure 3A) and\nspeed reaction time modestly (Figure 3D). This pattern\nmirrors experimental findings that attentional benefits\nin biological systems are most pronounced when dis-"}, {"title": "4.2 A recurrent ViT deploys attention in an human/NHP-like strategy", "content": "To visualize how the model allocates attention in\neach timestep, we generated averaged self-attention\nheatmaps from trials with the cue at location S\u2081 and\nno orientation change occurred (\u2206 = 0; Figure 4A)."}, {"title": "4.3 Manipulating Bias Affects Response Rate and Reaction Times", "content": "The ability to causally manipulate activity in pri-\nmate brain regions like the frontal eye fields (FEF)\nand superior colliculus (SC) has provided founda-\ntional insights into the neuronal mechanisms of at-\ntention [43-45, 66\u201370]. The clear interpretability of\nour model's spatial and temporal attention allocation\ndynamics offers a unique opportunity to test whether\ntargeted perturbations of its self-attention mechanism\nproduce effects analogous to these biological interven-\ntions. Demonstrating such parallels would provide a\nstringent validation that our Recurrent ViT captures not\nonly correlational but also causal principles underlying\nprimate attentional control.\nAgain resembling the results of NHP experiments,\nthe behavioral consequences of attention manipulation"}, {"title": "4.4 Alternative Architectures and Training Approaches Fail to Capture Primate-like Attention", "content": "To validate our architectural choices and training\napproach, we systematically evaluated several alter-\nnative model variants. We tested different memory-\nattention integration schemes (tokens, additive, and\nmultiplicative feedback) and we examined whether re-\ninforcement learning was necessary by training two\nsupervised variants of our architecture. Supervised\nmodels trained either on trial-type labels (change /\nno-change) or target action sequences achieved rea-\nsonable task performance but did not show a \"cue-\ning effect\u201d, instead detecting orientation changes sim-\nilarly regardless of cue validity (Table 1). Alternative\nmemory-attention architectures showed similar limita-\ntions - while the additive attention model demonstrated\na weak version of anticipatory attention reallocation,\nboth it and the token-based model failed to capture the\nrich temporal dynamics observed in primates. Only our\nRL-trained Recurrent ViT with multiplicative feedback\nproduced the full-compliment of primate-like features\nwe have documented above. These results suggest\nthat both reinforcement learning and multiplicative in-\nteractions between memory and attention are critical\nfor developing temporally structured attentional control\nthat mirrors primate behavior."}, {"title": "5 Discussion", "content": "In this work, we introduce a Recurrent Vision\nTransformer (Recurrent ViT) enhanced with a spa-\ntial memory module designed for a cued orientation\nchange-detection task. Our central goal was to deter-\nmine whether augmenting standard vision transform-\ners\u2014which typically rely on feedforward processing of\nsingle frames [24]\u2014with a recurrent feedback mech-\nanism can enable top-down, internally guided atten-\ntional control akin to that observed in human and non-\nhuman primate (NHP) vision [1, 30, 73]. Our experi-\nments demonstrate that the proposed Recurrent ViT\nsuccessfully recapitulates many hallmark effects of pri-"}, {"title": "5.1 Recovering Hallmark Signatures of Primate Attention.", "content": "First, our trained model shows improved perfor-\nmance and faster detection of orientation changes at\ncued locations, mirroring the well-documented behav-\nioral effects of selective spatial attention [1, 3, 5, 74].\nThese benefits emerge in situations where high-validity\ncues bias internal representations toward the cued\nlocation, but they taper off or reverse if competing\nsalience signals (e.g., a large orientation change else-\nwhere) dominate the model's self-attention. This inter-\nplay between cue validity and exogenous salience res-"}, {"title": "5.2 Relevance to Neural Mechanisms of Attention and Working Memory.", "content": "The success of our Recurrent ViT underscores\nthe deep links between attention and working mem-\nory reported in neuroscience [12, 14, 15, 77]. Much\nlike the \"attentional template\u201d theory, which proposes\nthat memory representations guide attention to rele-\nvant features and locations [51,78,79], our model main-\ntains a set of spatial codes over time. These memory\nstates re-enter a self-attention module to bias ongo-\ning visual processing, effectively bridging top-down and"}, {"title": "5.3 Subcortical and Dopaminergic Influences.", "content": "Although our model already uses reward feed-\nback to guide learning, we have not explicitly inte-\ngrated dopaminergic-like prediction error signals or ex-\namined how reward history might adaptively modulate\nattentional policies in the superior colliculus and re-\nlated circuits [84\u201387]. In biological systems, dopamine\ncritically mediates plasticity, enabling more nuanced\nshaping of attentional priorities over extended time"}, {"title": "5.4 Constraints, Biological Plausibility, and Interpretability.", "content": "A central feature of our model is the introduction\nof a recurrent spatial memory component that con-\nstrains information flow between image frames shown\nat different points in time. Unlike standard transform-\ners-which can re-attend to entire sequences without\nconstraint [23, 24, 96] or models that process full se-\nquences of images [32]\u2014our approach assigns each\nspatial patch of an immediate image to a single hidden\nstate slot within an LSTM. This imposed bottleneck\nencourages competition among representations, echo-\ning the biased competition framework which posits that\na finite \"priority map\u201d mediates interactions between\nbottom-up inputs and top-down influences [5, 80, 97\u2013"}, {"title": "5.5 Broader Implications and Future Directions.", "content": "This Recurrent ViT opens several avenues for fu-\nture research. First, richer scenarios-such as multi-\nobject tracking [104, 105], dynamic scene understand-\ning [76, 106], or tasks requiring mid-trial updates to\nmemorized stimuli [107, 108]\u2014would extend our ap-\nproach and further test its alignment with primate atten-\ntional performance. Incorporating saccadic eye move-\nments, akin to real-world visual search, could allow the\nmodel to learn optimal covert and overt strategies in\ntandem [81, 109]. Additionally, scaling up to deeper,\nmultilayer recurrent architectures may capture the in-"}, {"title": "5.6 Conclusion.", "content": "Taken together, these findings demonstrate that\nrecapitulating human/NHP-like attention in transformer\narchitectures is possible by introducing explicit top-\ndown influences and recurrent feedback. Our Recur-\nrent ViT significantly narrows the gap between stan-\ndard, feedforward models of attention [23,24] and the it-\nerative, memory-intensive processes that characterize\nprimate visual cognition [5, 15]. By unifying principles\nof biased competition, working memory, and reward-\ndriven learning within a single framework, we not only\nadvance the biological plausibility of deep vision mod-"}, {"title": "6 Methods", "content": null}, {"title": "6.1 Model Overview", "content": "The objective of the model is to utilize immediate\nvisual inputs in order to update an internal state with\nsufficient immediate and past visual information such\nthat downstream decoders can estimate value and take\naction. We utilize a self-attention (SA) mechanism to\nconstruct the visual percept used to update an inter-\nnal state of an RNN. Self-attention is computed based\non the immediate visual inputs and feedback from the\nRNN. The following sections will describe the motiva-\ntions and details of this process in more depth."}, {"title": "6.2 Pre-Processing, Content Selection, and Construction for Visual Working Memory", "content": "Given a visual scene (an image) of dimension\nH \u00d7 W \u00d7 C, denoted by O(t) \u2208 \u211d^(H\u00d7W\u00d7C) at time t,\nthe agent views the entire scene through a fixation at\ncenter field. During preprocessing, the image is parti-\ntioned into a set of patches, {o\u1d62(t)}_(patch), where each\npatch is of size H_(patch) \u00d7 W_(patch) \u00d7 C_(patch). The orig-\ninal visual patches are then transformed into a com-\npact set of internal representations, {x\u1d62(t)}_(patch), each\nx\u1d62(t) \u2208 \u211d^(H_(patch)\u00d7W_(patch)\u00d7C_(patch)) and typically satisfying\ndim(x\u1d62(t)) < dim(o\u1d62(t)).\nTogether, the collection of these feature patches forms\nthe immediate visual information available to the agent\nat time t, denoted by X(t) = {x\u1d62(t)}_(patch)."}, {"title": "6.3 Spatially Oriented Visual Working Memory", "content": "Following numerous experimental findings, we al-\nlow our model to maintain a spatially arranged visual\nworking memory [22,78,121, 122], in which each patch\nlocation i has a corresponding patched memory com-\nponent c\u1d62(t) \u2208 \u211d^(d_mem) within the RNN. For updating the\ninternal state of our RNN, we utilize the operations and\nfunctions described in the LSTM architecture [40, 41].\nFor the remainder of our description, we will refer to\nthe collection C'(t) = {c\u1d62(t)}_(patch) the VWM state and"}, {"title": "6.4 A Disjoint Memory", "content": "Following the ideas presented by Knudsen [75],\nthe activated subset of working memeory is central for\ndecision-making and planning. However, a patched\nRNN as described above presents a clear shortcom-\ning: each h\u1d62(t) encodes the content of its own patch\nindependently, without explicit awareness of neighbor-\ning patches. If downstream processes (e.g., a decoder\n\u03c0) require spatial or contextual relationships among\npatches, they must construct these relationships en-\ntirely from the population of VWM patches. More-\nover, the problem intensifies over time. If the network\nmust integrate information from patches across multi-\nple timesteps (e.g., x\u2081\u207d\u1d57\u207e and x\u2081\u207d\u03c4\u207e for \u03c4 < t), then each\nVWM patch must retain all potentially significant cur-"}, {"title": "6.5 Self-Attention and Spatially Aware activated memory", "content": "We wish to obtain a scene-level representation\nZ\u207d\u1d57\u207e = {z\u1d62\u207d\u1d57\u207e}_(patch) such that each z\u1d62\u207d\u1d57\u207e encodes the\ntask-relevant spatial relationships among the visual\nfeature patches x\u2081\u207d\u1d57\u207e,..., x_(n_patch)\u207d\u1d57\u207e. By doing so, the\npatch-based LSTM will be able to utilize immediate vi-\nsual information within a patch and task-relevant spatial\ncontext to update internal states. Formally, we want\nz\u1d62\u207d\u1d57\u207e = f_(sa)(x\u1d62\u207d\u1d57\u207e, {x\u2c7c\u207d\u1d57\u207e}_(j\u2260i)),\nA straightforward way to implement this is via self-\nattention:\nz\u1d62\u207d\u1d57\u207e = x\u1d62\u207d\u1d57\u207e + \u03a3_(j=1)^(N_patch) \u03b1\u1d62\u2c7c\u207d\u1d57\u207e x\u2c7c\u207d\u1d57\u207e,\nwhere v\u2c7c\u207d\u1d57\u207e = V(x\u2c7c\u207d\u1d57\u207e) is a function that maps the fea-\nture patch into a latent space and \u03b1\u1d62\u2c7c\u207d\u1d57\u207e = A(x\u1d62\u207d\u1d57\u207e, x\u2c7c\u207d\u1d57\u207e)\nindicates the relative importance of x\u2c7c\u207d\u1d57\u207e with respect\nto x\u1d62\u207d\u1d57\u207e. To ensure a proper probability-like weighting,\nwe impose \u03a3_(j=1)^(N_patch) \u03b1\u1d62\u2c7c\u207d\u1d57\u207e = 1 with \u03b1\u1d62\u2c7c\u207d\u1d57\u207e \u2208 (0, 1). A typical\nchoice for \u03b1\u1d62\u2c7c\u207d\u1d57\u207e is:\n\u03b1\u1d62\u2c7c\u207d\u1d57\u207e = exp((q\u1d62\u207d\u1d57\u207e, k\u2c7c\u207d\u1d57\u207e))\n\u03a3_(m=1)^(N_patch) exp((q\u1d62\u207d\u1d57\u207e, k_m\u207d\u1d57\u207e)),\nwhere q\u1d62\u207d\u1d57\u207e = Q(x\u1d62\u207d\u1d57\u207e) and k\u2c7c\u207d\u1d57\u207e = K(x\u2c7c\u207d\u1d57\u207e) are query\nand key functions, respectively. Interpreting \u03b1\u1d62\u2c7c\u207d\u1d57\u207e as a\nsalient feature map has strong parallels to the saliency"}, {"title": "6.6 Recurrent Feedback From Memory", "content": "Knudsen [75] describes a feedback loop in which\nworking memory provides top-down signals that bias\nneural representations relevant to the organism's cur-\nIn the context of the biased compe-\ntition model [80], working memory holds an atten-\ntional template that biases competition in favor of task-\nrelevant representations. However, in practice it is\nnot clear how this mnemonic feedback is/should be\nimplemented. In this we simplify (and constrain) the\nproblem to implementing recurrent feedback from the\npatch-based LSTM to the self-attention mechanism of\nthe ViT. Hence, we evaluate three different methods in\nterms of their ability to yield primate-like behavior sig-"}, {"title": "6.7 Mnemonic Guidance", "content": null}, {"title": "6.7.1 Visual working memory as tokens", "content": "The first recurrent feedback method we evaluate is one\nin which we concatenate the mnemonic percept to the\nvisual input. Thus, the input to the self-attention mech-\nanism is\nX = Concatenate [X\u207d\u1d57\u207e, H\u207d\u1d57\u207b\u00b9\u207e]"}, {"title": "6.7.2 Additive Feedback from Visual Working Memory", "content": "We split the standard self-attention operation into two\nparallel pathways: one for the bottom-up immediate vi-\nsual inputs, x\u1d62\u207d\u1d57\u207e, and one for the top-down mnemonic\ninputs, h\u1d62\u207d\u1d57\u207e. Define:\nq_X^(i) = Q_X (x\u1d62\u207d\u1d57\u207e),\nk_(X,j)^((t)) = K_X (x\u2c7c\u207d\u1d57\u207e),\nv_(X,j)^((t)) = V_X (x\u2c7c\u207d\u1d57\u207e).\nand\nq_(H,i)^((t)) = Q_H(h\u1d62\u207d\u1d57\u207e),\nk_(H,j)^((t)) = K_H (h\u2c7c\u207d\u1d57\u207e),\nv_(H,j)^((t)) = V_H (h\u2c7c\u207d\u1d57\u207e).\nThe attention weights are given by\n\u03b1_(Aij)^((t)) = exp (q_X^(i) + q_H^(i)) \u00b7 (k_(X,j)^((t)) + k_(H,j)^((t)))\n\u03a3_(m=1)^(n_patch) exp(q_X^(i) + q_H^(i)) \u00b7 (k_(X,m)^((t)) + k_(H,m)^((t))"}, {"title": "6.7.3 Multiplicative Feedback from Visual Working Memory", "content": "To incorporate multiplicative feedback, we instead de-\nfine:\n\u03b1_(Ai,j)^((t)) = exp(q_(X,i)^((t)) \u2299 q_(H,i)^((t)) \u00b7 (k_(X,j)^((t)) \u2299 k_(H,j)^((t))))\n\u03a3_(m=1)^(n_patch) exp(q_(X,i)^((t)) \u2299 q_(H,i)^((t)) \u00b7 (k_(X,m)^((t)) \u2299 k_(H,m)^((t)))),\nand\nz\u1d62\u207d\u1d57\u207e = x\u1d62\u207d\u1d57\u207e + \u03a3_(j=1)^(n_patch) \u03b1\u1d62\u2c7c\u207d\u1d57\u207e (v_(X,j)^((t)) \u2299 v_(H,j)^((t))).\nHere, the top-down feedback pathway multiplicatively\ngates the bottom-up signals. As a result, larger\n(smaller) magnitudes in the memory pathway can am-"}, {"title": "7 Model Architecture", "content": "Our model integrates a Vision Transformer (ViT)\nwith a patch-based LSTM. First, a VAE is used to pre-\nprocess the raw visual features in a purely feed-forward\nmethod. Secondly, we utilize a recurrent ViT in which\nself-attention has been modified to incorporate immedi-\nate and recurrent inputs in order to construct the visual\npercept transmitted to the patch-based LSTM. Thirdly,\nthe LSTM utilizes the projection from the recurrent ViT\nto update the patch-based internal states."}, {"title": "7.1 VAE Pre-Processing", "content": "A Variational Autoencoder (VAE) is a generative\nmodel that learns to encode input data into a latent\nspace and reconstructs the data from this latent rep-\nresentation. It combines principles from deep learning\nand probabilistic inference, making it suitable for mod-\neling complex data distributions. It consists of two pri-\nmary components, and encoder (F) that encodes vi-\nsual inputs to a probabilistic latent space (zlatent), and\na decoder (G) that decodes a sampled latent vector\ninto a visual input.\nThe encoder network F entails multiple opera-\ntions, f \u2208 F which serve to map an input image patch\noi\u2208 \u211d^(H_(patch)\u00d7W_(patch) \u00d7C) to a latent representation char-\nacterized by a mean vector zu \u2208 \u211d^(d_latent) and a log-\nvariance vector Zlogvar \u2208 \u211d^(d_latent), where dlatent is the\ndimensionality of the latent space. The encoder con-\nsists of convolutional and fully connected layers as fol-"}, {"title": "7.2 ViT", "content": "Input images O(t) \u2208 \u211d^(50\u00d750) are sub-divided\ninto four equal patches {o\u2081(t), o\u2082(t), o\u2083(t), o\u2084(t)}, with\no\u1d62(t) \u2208 \u211d^(25\u00d725). We found that our RL agent learned\nfastest, was most interpretable, and demonstrated best\nperformance when we used the second flattend en-\ncoder layer (Oflat,2) as input to the ViT (as oppose to\nthe latent encoding). Hence, for a given patch input\no\u1d62(t) at time t, we have the encoding\n\u00f4\u1d62\u207d\u1d57\u207e = f*(o\u1d62\u207d\u1d57\u207e)\nwhere f*(\u00b7) includes encoder components (1)\u2013(4). We\nalso concatenate a (one-hot) positional (pi) and tem-\nporal (\u03c4) encoding. Thus the full pre-processed patch\ninput at timestep t is\nx\u1d62\u207d\u1d57\u207e = Concat[\u00f4\u1d62\u207d\u1d57\u207e, p\u1d62, \u03c4]\nThe complete input to the ViT at time step t is:\nX\u207d\u1d57\u207e = (x\u2081\u207d\u1d57\u207e, x\u2082\u207d\u1d57\u207e, x\u2083\u207d\u1d57\u207e, x\u2084\u207d\u1d57\u207e)\u1d40 \u2208 \u211d^(4\u00d7140)\nThe transformer computes queries, keys, and values\nas:\nQ = (X\u207d\u1d57\u207eW_XQ) \u2299 (H\u207d\u1d57\u207b\u00b9\u207eW_HQ)\nK = (X\u207d\u1d57\u207eW_XK) \u2299 (H\u207d\u1d57\u207b\u00b9\u207eW_HK)\nV = (X\u207d\u1d57\u207eW_Xv) \u2299 (H\u207d\u1d57\u207b\u00b9\u207eW_Hv)\nwhere W_X. \u2208 \u211d^(140\u00d7140), W_H. \u2208 \u211d^(1024\u00d7140), H\u207d\u1d57\u207b\u00b9\u207e is\nthe activated memory from the previous timestep, \u2299\ndenotes Hadamard product, and we have dropped the\ntemporal superscript (implicit). Self-attention is com-"}, {"title": "7.3 Spatial LSTM", "content": "We adapt the xLSTM architecture [41] for spa-\ntial memory. The LSTM operations are shown in Ta-\nble 2. The projection matrices have dimensions Wx \u2208\n\u211d^(140\u00d71024), and Rx \u2208 \u211d^(140\u00d71024). This ensures all output\nvariables have shape 4 \u00d7 1024. As described above,\nwe call this a patch-based LSTM because there is a\nhidden state for each patch of the visual scene. Impor-\ntantly, within the LSTM the hidden states are updated\nindependently. The matrices Z(t), C(t), H(t), M(t), and\nN(t) are of shape npatch by d, where d\u2208 dlatent, dmem.\nRight multiplication by the matrices Wx or Rx projects\nthe latent embedding or hidden state of a specific patch\nto another space, independent of the other patches.\nBy constructions, self-attention is the only mechanism\nby which information from visual patches (or mnemonic\\patches) is communicated to other patches."}, {"title": "7.4 Actor-Critic Reinforcement Learning", "content": "Our model is trained using an actor-critic rein-\nforcement learning (RL) framework [125] in which the\nagent learns to select actions that maximize long-term\nrewards. At each timestep, the agent observes a\nmnemonic percept\nH\u207d\u1d57\u207e \u2208 \u211d^(4\u00d71024),\nwhich encodes spatial and temporal context, and se-\nlects an action from a binary set:\n\u03b1_t =\n{\n0,\n(\"wait\" action)\n1,\n(\"declare change\" action)\nThe value function\nV(H\u207d\u1d57\u207e) = E [\u03a3_(\u03c4=t)^T \u03b3^(\u03c4-t) r\u03c4 | H\u207d\u1d57\u207e]\n\u00ce\u207d\u1d57\u207e = Z\u207d\u1d57\u207eWi + H\u207d\u1d57\u207b\u00b9\u207eRi\nF\u207d\u1d57\u207e = Z\u207d\u1d57\u207e Wf + H\u207d\u1d57\u207b\u00b9\u207eRf\n\u00d5\u207d\u1d57\u207e = z\u207d\u1d57\u207eWo + H\u207d\u1d57\u207b\u00b9\u207eRo\n\u016a\u207d\u1d57\u207e = Z\u207d\u1d57\u207eWu + H\u207d\u1d57\u207b\u00b9\u207eRz\nI\u207d\u1d57\u207e = exp(\u00ce\u207d\u1d57\u207e \u2013 M\u207d\u1d57\u207e)\nF\u207d\u1d57\u207e = exp(\u0159\u207d\u1d57\u207e + M\u207d\u1d57\u207b\u00b9\u207e \u2013 M\u207d\u1d57\u207e)\nM\u207d\u1d57\u207e = max(F\u207d\u1d57\u207e + M\u207d\u1d57\u207b\u00b9\u207e, \u00ce\u207d\u1d57\u207e)\nC\u207d\u1d57\u207e = C\u207d\u1d57\u207b\u00b9\u207e F\u207d\u1d57\u207e + U\u207d\u1d57\u207e I\u207d\u1d57\u207e\n0\u207d\u1d57\u207e = \u03c3(\u00f5\u207d\u1d57\u207e)\nN\u207d\u1d57\u207e = F\u207d\u1d57\u207e N\u207d\u1d57\u207b\u00b9\u207e + I\u207d\u1d57\u207e\nU\u207d\u1d57\u207e = tanh(\u016a\u207d\u1d57\u207e)\nH\u207d\u1d57\u207e = O\u207d\u1d57\u207e (C\u207d\u1d57\u207e/N\u207d\u1d57\u207e)"}, {"title": "7.5 Distributional Framing, Network Architecture, and Loss Functions", "content": "In our approach the critic network estimates a dis-\ntributional Q-function rather than a single scalar value.\nFor a state-action pair (Ht, at), the critic outputs a\nprobability distribution over 15 discrete Q-value bins.\nThe critic network is defined as follows:\nPo(q | Ht, at) = Softmax(1/(\u03c3(t) [Wout qt + bout]),\na' = atWa + ba,\nq0 = Concat[H', \u03b1'],\nq1 = ELU(q0W1 + b\u2081),\nq2 = ELU(q1W2 + b2),\nq3 = ELU(q2W3 + b3),\nwhere H' \u2208 \u211d^(4096) is the flattened mnemonic percept.\nThe improved (target) policy is defined as\n\u03c0_(imp) (a_t | s_t) \u03b1 [exp (Q_(\u03b8') (s_t, a_t))\n\u03b7\n] * \u03c0_\u03b8(a_t | s_t),\nwhere \u03b8' denotes the parameters of a target network\nand \u03b7 > 0 is a temperature parameter."}, {"title": "7.6 Task Difficulty", "content": "To control task difficulty, Gabor stimuli were cor-\nrupted with rotational \"noise\u201d. Defnining as the \"true\u201d\nGabor orientation for S\u2081, the orientation in the input im-\nage shown to the agent is:\n\u03b8\u2081 = 0 + \u03b4it\nwhere \u03b4it ~ N(0, \u03c3) is the rotational noise at time step\nt. If the stimulus is selected for change, then at t = 5\nand t = 6:\n\u03b8\u2081 = 0 + \u0394 + \u03b4it\nThe orientation noise parameter \u03c3 is set to 5. The\norientation change parameter \u0394 is a random variable\ndrawn at the beginning of a change trial, with \u0394 ~"}]}