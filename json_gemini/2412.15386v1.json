{"title": "Systematic Evaluation of Long-Context LLMs on Financial Concepts", "authors": ["Lavanya Gupta", "Saket Sharma", "Yiyun Zhao"], "abstract": "Long-context large language models (LC LLMs) promise to increase reliability of LLMs in real-world tasks requiring processing and understanding of long input documents. However, this ability of LC LLMs to reliably utilize their growing context windows remains under investigation. In this work, we evaluate the performance of state-of-the-art GPT-4 suite of LC LLMs in solving a series of progressively challenging tasks, as a function of factors such as context length, task difficulty, and position of key information by creating a real world financial news dataset. Our findings indicate that LC LLMs exhibit brittleness at longer context lengths even for simple tasks, with performance deteriorating sharply as task complexity increases. At longer context lengths, these state-of-the-art models experience catastrophic failures in instruction following resulting in degenerate outputs. Our prompt ablations also reveal unfortunate continued sensitivity to both the placement of the task instruction in the context window as well as minor markdown formatting. Finally, we advocate for more rigorous evaluation of LC LLMs by employing holistic metrics such as F1 (rather than recall) and reporting confidence intervals, thereby ensuring robust and conclusive findings.", "sections": [{"title": "1 Introduction", "content": "Recently, there has been a growing interest in extending the context window sizes of large language models (LLMs) to produce long-context LLMs (LC LLMs) (gpt, 2024; OpenAI, 2024; Gemini Team, 2024). This is especially promising as it allows to extend the \u201cworking memory\u201d of LLMs. Real world use cases need LC LLMs to be able to follow increasingly complicated instructions while reasoning over their long context length windows with high degree of reliability."}, {"title": "2 Methodology", "content": "Real world use of LLMs, commonly requires them to locate, reason over and synthesize relevant information across their context window, while accounting for constraints specified in the prompt. Our methodology is inspired to mimic such realistic use-cases of LC LLMs. We now outline the pivotal characteristics of our tasks, experimental setup, prompts and evaluation strategy."}, {"title": "2.1 Tasks", "content": "We introduce concepts that constitute the foundational building blocks of our framework. In this work, we experiment with 3 concepts: \"companies\", \"time\" and \"sentiment\" that are relevant in the context of financial news. We combine them as search clauses - single or multiple - to create increasingly difficult long-context retrieval tasks. Unlike other benchmarks that are restricted by the predefined choices of tasks, our framework allows us to flexibly combine concepts to create tasks of varying difficulties, that are a close proxy for realistic tasks.\n1. Company Company recognition (\"Which companies are mentioned in this article?\") is a fundamental concept in finance. We covert it into a long-context task that requires LC LLMs to: \"Find all articles about <company_of_interest> from the articles below.\"\n2. Time Temporal queries are frequently encountered in news applications. We create two long-context tasks using this concept: \"Find all articles since <time_range> from the articles below.\" and \"Find all articles about <company_of_interest> since <time_range> from the articles below.\"\n3. Sentiment Finally, sentiment (\"What is the sentiment of  in this article?\") is another key concept in financial news, that is usually defined in a highly-specialized domain-specific interpretation, inherently making it a more complex concept. Combining with company, we convert our 5-class sentiment concept into a long-context task as follows: \"Find all  articles about  from the articles below.\"\nIn summary, we create combinations of our predetermined financial concepts in curated settings to result into the following 4 functional tasks: Company (C), Time (T), Company+Time (CT), Company+Sentiment (CS).\nTask Difficulty To provide a background on task difficulty, we share baseline and skyline performance on the underlying concepts (short-context) using zero-shot in-context learning and fine-tuning respectively. Firstly, we find that both C and T are relatively simpler concepts for LLMs to understand. We notice that combining concepts results into somewhat harder tasks than its single-concept constituents. Finally, we also note that both C and CS tasks benefit from fine-tuning, showing an improvement of ~10% and ~30% over out-of-the-box GPT-4 models respectively. This further suggests that specialized company sentiment classification is a much harder concept for LLMs without fine-tuning. Overall, we observe task difficulty to be a function of the choice of concept as well as the number of concepts for generalized LC LLMs."}, {"title": "2.2 Dataset Creation", "content": "We first sample N=20 unique queries for each long-context task described in Sec. 2.1. Correspondingly, we then perform controlled sampling of news articles from our corpus to create test records of lengths 4K, 8K, 16K, 32K, 64K and 128K tokens respectively. For each of the above context lengths, we feed (110, 110, 85, 85, 85, 85) test records at (4K, 8K, 16K, 32K, 64K, 128K) context lengths respectively, thereby mitigating concerns related to low sample strength."}, {"title": "2.3 Prompts", "content": "We manually craft a prompt for each task, following a generic schema as show in Fig. 3. Prompts are run in zero-shot setting to replicate real-world use of LC LLMs. Outputs are requested as JSONS with an inline example of the expected output structure"}, {"title": "3 Results", "content": "To delve into the factors affecting the performance of LC LLMs, our results center on three key research questions presented in Sec. 1."}, {"title": "3.1 RQ1: LC LLMs are sensitive to position of the task instruction as well as minor prompt formatting", "content": "As discussed in Sec. 2.3, we experiment with four configurations of prompt placement. Our experiments show that while all the three prepend configurations (i.e. \"Prepend\", \"Prepend+Append\", \"OpenAI Best Practices\") are closely comparable, the \"Append\" configuration is considerably worse. We also record an overall improvement in performance using \"OpenAI Best Practices\" configuration over its vanilla \"Prepend\" counterpart. This exposes the unfortunate sensitivity of state-of-the-art LC LLMs to minor formatting. As a result, all the subsequent results in our work are reported on the \"OpenAI Best Practices\" configuration."}, {"title": "3.2 RQ 2: LC LLMs do not treat all context lengths equally", "content": "We observe that LC LLMs do not perform equally reliably at short vs. long context lengths on any given task. Even on simpler tasks like C, LC LLMs achieve almost a perfect F1-score at smaller context lengths (<= 32K tokens), but start breaking down at longer contexts. This breakdown is observed across all tasks where model performance declines consistently with increasing context length, thereby performing poorly on almost 75% of their claimed context window."}, {"title": "3.3 RQ3: LC LLMs perform poorly on difficult nuanced tasks", "content": "As discussed in Sec. 2.2, our design of (re-)using the same haystack context for all tasks allows us to disentagle and study the model's task ability in isolation to other factors. We observe that for more difficult multi-concept tasks (such as CT and CS), model performance collapses (almost) to 0 at context lengths greater than 32K, rendering models completely unusable at longer contexts, However, such stark drops in performance are not observed for relatively simpler single-concept tasks (such as C and T)."}, {"title": "3.4 Zero-needle", "content": "Previous works have shown that even powerful LC LLMs are imperfect at rejecting to answer. We witness similar behavior in our experiments wherein GPT-4-Turbo successfully returns empty JSONs for easy tasks such as C, but struggle on difficult tasks such as CS. We emphasize such tests are crucial to ensure robustness to distracting text in real-world industrial applications."}, {"title": "4 Towards Better Evaluation", "content": "Recall is not a reliable metric for difficult retrieval tasks at longer contexts. Most research in long-context retrieval primarily report their results using recall as the evaluation metric , since the basic goal is to assess if LC LLMs can \"remember\" key information in long contexts. We argue that a simple recall metric is often artificially inflated and hence of limited pragmatic value in real-world systems. For this reason, we also report bootstrap median F1-score throughout this work."}, {"title": "5 Conclusion", "content": "Our methodical framework characterized by real-world financial news concepts allows for flexible configurations to setup a range of different complexity tasks. Our study reveals that long context retrieval and reasoning is still a challenging task for long-context LLMs. Our dataset requires 32K tokens to challenge state-of-the-art GPT-4 models on easier tasks, and only 16K tokens on difficult tasks."}, {"title": "6 Limitations", "content": "In our experiments, we focused on GPT-class models due to organizational constraints, with plans to evaluate other model families in the future. While our tasks are based on real-world scenarios, they do not fully assess the long-form generation capabilities of LC LLMs, which are difficult to evaluate precisely. Additionally, we only begin to explore the complexity of real-world instructions where various constraints are applied. Nonetheless, we hope our evaluations offer valuable insights to guide future research."}, {"title": "7 Disclaimer", "content": "This paper was prepared for informational purposes by the Machine Learning Center of Excellence (MLCOE) group of JPMorgan Chase & Co. and its affiliates (\u201cJP Morgan\u201d) and is not a product of the Research Department of JP Morgan. JP Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Dataset Details", "content": "In our dataset, each news article title is about 21 tokens on average, with shortest title having 14 tokens and longest title having 90 tokens. Below are a few (anonymized) example news articles used in this work:"}, {"title": "A.2 Model Failures", "content": "A deep dive into model failures revealed two patterns of unloadable JSONs errors: (1) Continuous sequence of generations (2) Repeating their generations indefinitely. An example is shown in Fig. 9."}, {"title": "A.3 Assigning credits to invalid JSONS", "content": "Fig. 8 shows the pseudo-code that we use to handle and parse invalid JSON outputs returned by the models."}, {"title": "A.4 Prompts", "content": ""}, {"title": "A.4.1 Output format instruction", "content": "We show below the common output format instruction that we use for all four tasks: C, T, CT and CS."}, {"title": "A.4.2 Long Context", "content": "We show below in Table 3 the four different long-context task templates. The other parts that constitute the full prompt are common to all tasks as shown in Fig. 3 and Fig. 10."}, {"title": "A.5 Hard Negatives Examples", "content": "Our experiments rely on two types of hard negatives: Natural and Induced. Table 4 explains their definitions with a few examples."}, {"title": "A.6 Model parameters", "content": "Model versions and decoding strategy is shared in Table 5. For all our experiments, we set the maximum output generation token length to 100 tokens."}, {"title": "A.7 GPT-4-Turbo Results", "content": "We report results of GPT-4-Turbo on our benchmark in Fig. 11."}, {"title": "A.8 Prompt Ablations", "content": "Fig. 12 shows our prompt ablation results on GPT-4-Turbo. We note that there are no clear trends of 'lost-in-the-middle' phenomenon."}, {"title": "A.9 Zero Needle", "content": "In Fig 13, we show the ability of GPT-4-Turbo to reject or refuse answering when ground-truth (or evidence) is absent from the context."}]}