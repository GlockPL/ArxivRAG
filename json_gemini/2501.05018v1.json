{"title": "Finding Needles in Emb(a)dding Haystacks: Legal Document Retrieval via Bagging and SVR Ensembles", "authors": ["Kevin B\u00f6nisch", "Alexander Mehler"], "abstract": "We introduce a retrieval approach leveraging Support Vector Regression (SVR) ensembles, bootstrap aggregation (bagging), and embedding spaces on the German Dataset for Legal Information Retrieval (GerDaLIR). By conceptualizing the retrieval task in terms of multiple binary needle-in-a-haystack subtasks, we show improved recall over the baselines (0.849 > 0.803 | 0.829) using our voting ensemble, suggesting promising initial results, without training or fine-tuning any deep learning models. Our approach holds potential for further enhancement, particularly through refining the encoding models and optimizing hyperparameters.", "sections": [{"title": "1. Introduction", "content": "Information Retrieval (IR) fundamentally involves the process of identifying and extracting information units from an extensive collection, often in response to a specific query [1]. Legal Information Retrieval (LIR) represents the application of this concept within the legal domain, wherein the document collection consists of relevant judicial passages [2]. Among these documents, some are particularly relevant to a given legal case (query), necessitating the task of identifying and retrieving these specific documents. Consequently, a variety of Natural Language Processing (NLP) methodologies, such as term frequency-inverse document frequency (TF-IDF) [3] (cf. [4] for applying this in the legal domain) and BM25 ranking [5], are utilized to rank and compare collections of documents in order to retrieve the most pertinent ones. With the advent of the transformer architecture [6], language modeling has become a crucial component of LIR as well [7]. In this context, pre-trained bidirectional transformer models such as BERT [8], RoBERTa [9] and DeBERTa [10] are especially valuable, as they contextually capture semantic features of potentially relevant documents through high-dimensional embedding vectors, making them distinguishable and comparable on a computational level. Through fine-tuning, these models can be further adapted to more domain-specific downstream tasks such as LIR, as exemplified by LEGAL-BERT [11] and DISC-LawLLM [12]. Despite the significant"}, {"title": "2. Related Work", "content": "For Information Retrieval (IR) in general, TF-IDF stands as one of the primary methodologies [23, 24], owing to its simplicity and adaptability to various corpora and information sources. Consequently, within the aforementioned COLIEE competition, TF-IDF was widely used to directly retrieve or pre-process and pre-rank a given collection of documents [25, 26]. Following this, various BERT variants are employed for re-ranking [7] or downstream task fine-tuning [11], often in combination with methodologies such as TF-IDF. Additionally, contextualized embeddings from bidirectional pre-trained transformers like BERT [27, 28] are used for re-ranking or leveraging closeness metrics, such as cosine similarity, to fetch relevant documents. Finally, to enhance these models, feature engineering is applied to introduce potentially unique features, such as adding metadata to the documents, incorporating external data, or using model ensembles [4]. In the following sections, we will outline in detail the relevant technologies used for our retrieval model."}, {"title": "2.1. Embeddings", "content": "Transforming words and documents into a non-textual representation is a crucial task for nearly any NLP application. Among various methods, the vector space model introduced by Salton [29, 3] is arguably the most relevant and influential approach for achieving this. As a result, much effort has been invested in projecting textual data into high-dimensional spaces to capture semantic and contextual features. This trend was exemplified in 2014 with Word2Vec [30] and has since evolved with more recent models such as BERT and DeBERTa. These models generate high-dimensional embedding spaces for linguistic units, enabling spatial Euclidean"}, {"title": "2.2. Supported Vector Machine Regression", "content": "Support Vector Machines (SVM) are a popular machine learning tool for classification and regression (SVR), introduced by [31] in 1995. However, to our knowledge, they have not been widely used for LIR and IR in general. Akin to linear regression, Support Vector Machines (SVMs) aim to find the optimal hyperplane that effectively separates the two classes in the data. This hyperplane maximizes the margin, which is the distance between the hyperplane and the closest data points from each class, known as support vectors. SVR (Support Vector Regression) utilizes a similar concept where the margin is defined as an error tolerance of the model, known as the $\\epsilon$-insensitive tube [31]. Given training data $(x_i, y_i)$ for $i = 1, ..., n$, SVR seeks to find a function $f(x) = w \\cdot x + b$ that approximates the true values $y_i$ with minimal error, subject to the following constraints outlined in [31, p. 153 onward] and [32, p. 157]:\n\n$y_i - (w \\cdot x_i + b) \\leq \\epsilon + \\xi_i$\n$(w \\cdot x_i + b) - y_i \\leq \\epsilon + \\zeta_i$\n\nwhere $\\xi_i \\geq 0$ are slack variables representing the deviation from the margin, and $\\epsilon$ controls the width of the tube. The objective is to minimize the following regularized error function:\n\n$\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n (\\xi_i + \\zeta_i)$\n\nsubject to the constraints above, where C is a regularization parameter that balances the trade-off between the margin and the training error. The dual problem formulation involves computing Lagrange multipliers $\\alpha_i, \\alpha_i^*$ for each constraint, leading to the dual problem:\n\n$\\min_{\\alpha,\\alpha^*} \\frac{1}{2} \\sum_{i,j} (\\alpha_i - \\alpha_i^*)(\\alpha_j - \\alpha_j^*) x_i x_j + \\epsilon \\sum_i (\\alpha_i + \\alpha_i^*) - \\sum_i (y_i(\\alpha_i - \\alpha_i^*))$\n\nwith constraints $\\sum_i (\\alpha_i - \\alpha_i^*) = 0$ and $0 \\leq \\alpha_i, \\alpha_i^* \\leq C$ for $i = 1, ..., n$. The solution $w$ to the primal problem can be expressed in terms of the support vectors $x_i$ and their corresponding Lagrange multipliers $\\alpha_i^*, \\alpha_i$.\nIn practice, SVR effectively handles nonlinear relationships through kernel functions, mapping the input space into a higher-dimensional feature space where a linear model is constructed."}, {"title": "2.3. Bagging", "content": "Bootstrap Aggregating (Bagging), introduced by [21] in 1996, is an ensemble learning technique designed to enhance model accuracy by combining multiple base learners which also provides the capability to partition the problem into smaller sub-problems. The key aspects of bagging involve:\n\n\u2022 Creating multiple subsets of the original dataset, thereby forming smaller sub-problems for base learners.\n\u2022 Training each base learner independently on these subsets.\n\u2022 Combining the predictions of the individual learners in the ensemble to make a final prediction.\n\nThe strength of bagging lies in its ability to utilize an ensemble of smaller models [33], each trained on a subset of the dataset, particularly effective in scenarios with large feature sets and class imbalance. In such cases, overfitting can be a potential concern, and bagging can mitigate this risk effectively and hence improve generalization [34]."}, {"title": "2.4. GerDaLIR", "content": "GerDaLIR [19] is a German Dataset for Legal Information Retrieval based on the Open Legal Data platform. The dataset consists of 123,000 queries and 131,000 case documents, which are segmented into over 3 million passages. Each query is labeled with at least one document and hence multiple passages. The provided task is a precedent retrieval task based on case documents from the Open Legal Data\u00b2 platform. The authors provide several baseline models for LIR, including TF-IDF, BM25, and deep learning approaches such as BERT re-ranking [7]."}, {"title": "3. Methodology", "content": "In this section, we outline our approach to LIR on the GerDaLIR dataset, following the structure of a data science project report from start to finish, to demonstrate our findings and the chain of thought that led to our conclusions and final models."}, {"title": "3.1. Explorative Data Analysis", "content": "We begin by experimenting with various encoder transformer models. The intuition is to determine whether a given query is projected next to or at least very close to relevant documents in the collection embedding space, allowing for the possibility of retrieving documents based on their location in this space. We tested a total of eight different models and placed exemplary query embeddings into the collection space to observe their relation to relevant passages. It can be seen that while none of the queries inserted into the collection space are positioned directly adjacent to their relevant document passages, the relevant passages are often in close proximity to the queries \u2013 this is consistent with the approach of Salton's vector space model [3]. This is particularly evident when examining the longformer model [36], which appears to generate the most effective clustering for this task. The longformer was specifically trained to handle longer text sequences beyond lexical tokens or (short) sentences, which benefits the handling of queries and passages that typically range from two to six sentences in length.\nAnother observation is that the embedding space is heavily influenced by the length of the passage or query. Texts of the same length, regardless of their semantic similarity, tended"}, {"title": "3.2. Finding the Needle in a Haystack", "content": "Similar to other re-ranking techniques [7], we use the embeddings generated by the longformer model, as it seemed to work best for our task, to formulate the LIR procedure as outlined in Figure 2.\nFirst, we generate an embedding for each passage, resulting in a 3.095.383 \u00d7 768 embedding"}, {"title": "3.3. Results", "content": "The seemingly perfect scores for class 0 are a result of the nature of the needle-in-a-haystack task and should not be misinterpreted as indicating perfect model accuracy. Given that we sample, on average, $k$ \u2013 1 negative labels for each positive label, the precision and recall for this highly imbalanced class are distorted. Therefore, it is necessary to focus primarily on the results for class 1, where it is evident that even with a relatively small $k$ of just 50, our ensemble consistently identifies relevant passages with high recall, precision, accuracy, and F1 score."}, {"title": "4. Conclusion and Future Work", "content": "We showcased a novel approach for LIR, combining several machine learning methods such as SVR, bagging, and embedding spaces. While these initial results are promising, we need to work on the following areas:\n\n1. As demonstrated by our exploratory data analysis in Section 3.1, the embedding spaces heavily depend on the length of the texts. If we were to further partition the passages and queries into texts of equal length, the overall ensemble and retrieval process could benefit from it. Further investigation into this approach is necessary. Additionally, our approach is based on the observation that relevant passages are generally proximate to the queries within the embedding space. Nevertheless, as illustrated in Figure 1a, there are instances where outlier passages are positioned far from their corresponding queries, making it exceedingly difficult for our ensemble model to identify them. This is one of the key constraints that we need to address, particularly through the investigation of the impact of increasing the value of k.\n2. Due to time and hardware constraints, we had to opt for a relatively small k = 50 for first level retrieval. Increasing the radius of the initial retrieval layer, within which the models search for relevant information, could significantly enhance recall.\n3. The initial idea was to utilize multiple embedding spaces generated by various encoder models and concatenate them, thereby further increasing the feature space by higher dimensions. This approach has proven effective in several high-profile NLP competitions. For these preliminary results, we opted for a single embedding model, but we aim to expand this approach.\n\nFinally, we recognize the need for more German-based models and approaches, particularly within the field of law. The encoding models currently employed primarily process English text and rarely specialize in legal domains. While fine-tuned versions like LEGAL-BERT exist, they predominantly cater to non-German texts. The development of a German pre-trained encoder model, akin to LEGAL-BERT, could help bridge this gap and presents a promising avenue for future research.\nOur present approach combines SVMs with transformer-based embedding models to develop an appropriate feature space for the classification task at hand. The very simple method evolves as a process in which better and better embedding models can improve the second main part of our method, just as more efficient classification methods can improve the classical ML part that makes up the first main part of our approach. The obvious question as to why we do not rely directly on fine-tuning a pretrained embedding model lies in the transparency that the SVM-based approach offers us as a classification tool. This transparency is necessary because we divide the original document space into overlapping subspaces that compete, so to speak, for the processing of a query by the classifiers assigned to them. Thus, we divide the overall task into a series of subtasks, each of which can be transparently processed by classifiers, where the partitioning of the overall document space can reflect the specific topological conditions of the underlying embedding space, providing additional transparency. However, by using"}]}