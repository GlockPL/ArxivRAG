{"title": "On the Emergence of Thinking in LLMs I: Searching for the Right Intuition", "authors": ["Guanghao Ye", "Khiem Duc Pham", "Xinzhi Zhang", "Sivakanth Gopi", "Baolin Peng", "Beibin Li", "Janardhan Kulkarni", "Huseyin A. Inan"], "abstract": "Recent advancements in AI, such as OpenAI's new o models, Google's Gemini Thinking model, and Deepseek R1, are transforming LLMs into LRMs (Large Reasoning Models). Unlike LLMs, LRMS perform thinking or reasoning during inference, taking additional time and compute to produce higher-quality outputs. This work aims to discover the algorithmic framework behind training LRMs. Approaches based on self-consistency, process reward modeling, AlphaZero, highlight that reasoning is a form of guided search. Building on this principle, we ask: what is the simplest and most scalable way to implement search in the context of LLMs?\nTowards answering these questions, we propose a post-training framework called Reinforcement Learning via Self-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with human or synthetic demonstrations of the reasoning process, whenever possible (2) using an exploration reward signal to encourage diverse and efficient reasoning behaviors, and (3) RL training with an outcome verifier to ensure correctness while preventing reward hacking. Our key innovation is to decouple exploration and correctness signals during PPO training, carefully balancing them to improve performance and efficiency.\nWe perform empirical studies of the RLSP framework in the math domain, and show that the models trained with the RLSP framework demonstrated improved reasoning abilities. On Llama-3.1-8B-Instruct model the RLSP framework can boost performance by 23% in MATH-500 test set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due to RLSP technique.\nThe more important finding of this work is that the models trained using RLSP technique, even with the simplest exploration reward that encourages the model to take more intermediate steps before arriving at a solution, showed several emergent behaviors such as backtracking, exploration of ideas, and verification. Furthermore, our framework enables such emergent behaviors across multiple model families, sizes, and domains. These findings demonstrate that RLSP framework might be enough to enable the emergence of complex reasoning abilities in LLMs when scaled appropriately.\nLastly, we propose a theory as to why RLSP search strategy is more suitable for LLMs compared to previous approaches considered in the literature, inspired by a remarkable recent result that says that COT provably increases computation power of LLMs, and hence reasoning, and these abilities grow as the number of steps in CoT [LLZM24, MS23]. Our code is available at: https://github.com/ GuanghaoYe/Emergence-of-Thinking.", "sections": [{"title": "Introduction", "content": "With the release of o-series of models from OpenAI [Ope24d, Ope24a], Gemini Thinking model from Google [Goo24], and Deepseek R1 [DA25], LLMs are rapidly evolving into thinking machines, now referred to as LRMS (Large Reasoning Models). The key technical difference between LLMs and LRMs is the ability of LRMs to do thinking during the inference, which we define as the ability to take more time and compute during (inference) with the goal of producing a higher quality output to a given input, which is a fair definition of thinking process in all systems capable of reasoning both artificial and biological.\nThe main goal of this work is to discover the computational process behind reasoning or thinking in the context of LLMs. There have been several attempts in the past towards understanding this process, ranging from techniques such as self-consistency [WWS+22, LYF+23], (automated) process reward modeling (PRM) [UKK+22, LKB+23, SLXK24, BTR24], and adapting AlphaZero style search for LLMs [FWW+23, CLLF24, TWL+24, Dee24]. All of these techniques have one principle in common: reasoning or thinking is some form of search. We build on the principle of defining thinking as a form of guided search and ask the question:\nWhat is the simplest and most scalable framework for training LLMs that leads to the emergence of thinking or search behavior?"}, {"title": "1.1 Our Contributions", "content": "The RLSP Framework Towards answering these questions, we propose a post-training framework called Reinforcement Learning via Self-Play (RLSP)\u00b9. Our framework is a natural extension and generalization of the RLHF [OWJ+22] and RL from Constitutional AI feedback [BKK+22] frameworks and consists of three simple steps:\n\u2022 Step 1: If high-quality demonstrations of the process of thinking are available, either via human annotations or synthetic traces constructed via tree search procedures, do SFT (supervised fine-tuning) on the demonstration dataset.\n\u2022 Step 2: Use an exploration reward that is independent of the correctness of the solution to implicitly encourage diverse search behaviors such as backtracking, consideration of alternative possibilities, verification, etc.\n\u2022 Step 3: Do RL (reinforcement learning) with PPO (proximal policy optimization, [SWD+17]) as the training algorithm, using an outcome verifier that gives an unambiguous binary signal of the correctness of the solution.\nIn this work, we focus on domains where outcome verification is possible (and easier) during training. Our key insight is that any reward engineering in RL training should encourage the model to synthetically create novel CoT data that it can learn from during the PPO training; that is, it incentivizes self-play over new CoT reasoning traces. A simple way to implement this is to decouple the exploration reward signal that encourages search behavior from the correctness of the solution, and incentivize it. During PPO training, we do a careful weighing of the score from the outcome verifier and the score from exploration reward model for the optimal performance. By design, both components of the reward signal mitigate issues such as reward hacking and overfitting to the training data. More importantly, both signals provide minimal but essential feedback for the model to learn better with scale consuming more compute and data.\nIn subsection 1.2 we propose an argument supporting these intuitions and how we arrived at the RLSP framework guided by some remarkable recent results [LLZM24, MS23]."}, {"title": "1.2 A Theory of RLSP", "content": "We propose a theory to explain how we arrived at the RLSP search strategy and how it can lead to continuous self-improvement.\nThe guiding principle behind RLSP is that any RL training technique should incentivize the model to synthetically create novel CoT reasoning trajectories that are not already present in the training data, and learn from it. Our intuition comes from a recent elegant mathematical result that states CoT can provably enhance the computational power of transformers [LLZM24, MS23]. Broadly speaking, [MS23, LLZM24] argument says that the length of chain-of-thought trace impacts its reasoning power, and more intermediate steps lead to more computational power under standard computational complexity assumptions. Recall that CoT is an empirical implementation of $\\arg \\max P_\\theta(answer, rationale | problem)$ for an autoregressive LLM parameterized by $\\theta$.\nSuppose we assume that as the difficulty of a reasoning problem increases, arriving at the right rationale or intuition necessary to solve the problem becomes harder. In particular, commonly oc-curring ideas in the pretraining data, which is what standard CoT finds, fail to lead to the correct so-lution. Then it is natural to train the model to search over the space of rationales thus maximizing $P_\\theta(correct rationale, trajectory over rationales | problem)$, which can be interpreted as CoT in the space of trajectories over rationales. However, such trajectories may not be present in training data. Here, it is beneficial to think of settings where the problems are so difficult that no human can solve it. Taking cue from [MS23, LLZM24], we can still design reward signals that encourage the model to use more intermediate steps as the problem difficulty increases and explore diverse rationales to solve the problem. This was"}, {"title": "2 Details of the RLSP Framework", "content": "In this section, we describe full RLSP framework that we used to train our best reasoning models. In section 3, we do ablation studies to understand how individual steps of RLSP change the thinking behavior of models, and show why all 3 steps may be the smoothest way to empower search behavior in LLMs. However,"}, {"title": "2.1 SFT of the Base Model", "content": "In the first step of RLSP framework, we perform a supervised fine-tuning using cross-entropy loss. A high-quality SFT dataset should contain demonstrations of the thinking process that incorporates typical reasoning principles such as backtracking, abandoning a reasoning chain, self-verification, etc. We note that SFT dataset need not be exhaustive in terms of all possible reasoning traces; In simpler terms, this step can be thought of as studying chapters in a textbook or attending lectures on a particular topic before asking the student to solve homework problems. In fact, this step is not even necessary, but helps in (RL) training models that exhibit better search behavior as we will see in coming sections.\nSFT datasets can be constructed through various methods: 1) Human demonstrations. 2) Depth First Search (DFS) traversal of MCTS or other tree search techniques 3) Synthetic data via agentic workflows or using a thinking model that is already trained. Although the quality of data produced by each of these steps can vary, with proper curation one could hope to yield high-quality demonstration data. Extensive research on the relative efficacy of these techniques is beyond the scope of this paper, and we leave it as an open problem. In our experiments, we create an SFT dataset using a filtered version of publicly available reasoning traces QwQ-LongCoT [Qwe24c]."}, {"title": "2.2 Reward Function", "content": "The most important component of RL training is establishing the reward function $R$. Suppose for a given prompt $q$ the model outputs $o$. Suppose we have an outcome verifier $Ver$ which objectively decides whether the model response $o$ given prompt $q$ is correct or incorrect (which can be done in math domain if we know the final answer for a problem and in coding domain by running the code on a few test cases). We will also utilize an exploration reward $R_{ex}(q, o)$ which judges the effort and creativity shown by the response $o$ in answering the prompt $q$. During training, the output reward signal $R(q, o)$ consists of two components:\n$R(q, o) = \\alpha \\cdot 1[Ver(q, o) = True] + (1 - \\alpha) \\cdot R_{ex}(q, o)$.   (1)\nThe key insight in our work compared to most previous approaches based on PRM is to give a reward signal independent of the output correctness that encourages desirable properties of the process of reasoning. This is related to concept of auxiliary rewards in RL, we refer the reader to [JMC+16] and references therein. Unlike PRM, the exploration reward does not directly measure the progress the policy model is making towards the outcome, but rather meta-properties of the reasoning trajectory that increases the success probability, as discussed in subsection 1.2. Thus, we reward the process itself instead of process reward modeling (PRM). Moreover, in the beginning of RL training, when the reward signal from the outcome verifier is very sparse, the exploration reward serves as a dense signal guiding the model towards longer and better reasoning trajectories.\nTo prevent reward hacking of the exploration reward signal, we carefully balance the correctness signal from the outcome verifier and exploration reward. In practice $\\alpha$ is a hyperparameter that needs to be tuned and decayed over time, but we set it as 0.8 in our experiments for simplicity; that is bulk of the reward signal comes purely from the outcome verifier which makes the learning process more challenging but enables better generalization."}, {"title": "2.3 RL Training with PPO", "content": "We use the PPO algorithm [SWD+17] to train our policy model. Our implementation of the PPO algorithm for training the policy is similar to the setup used in RLHF fine-tuning using PPO [OWJ+22]. Suppose the model response can be decomposed into tokens as $o = o_1, o_2, ..., o_T$ with $o_T = EOS$ being the end-of-string token. The loss function in PPO is given by:\n$L^{PPO}(\\theta) = E_t [\\min (p_t(\\theta) \\hat{A}_t, clip(p_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t)]$\nwhere $p_t(\\theta) = \\frac{\\pi_\\theta(o_t|q, o_{<t})}{\\pi_{\\theta_{old}}(o_t|q, o_{<t})}$ and $\\hat{A}_t$ is the GAE estimate for the advantage function given by: $\\hat{A}_t = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + ... + (\\gamma \\lambda)^{T-t} \\delta_T$ where $\\delta_t = r_t + V(q, o_{<(t+1)}) - V(q, o_{<t})$ and $r_t$ is the per-token reward at step t and $V_\\theta(q, o_{<t})$ is the value function predicted by critic model which is trained along with the policy model using squared loss with target given by reward-to-go $R_t = r_t + \\gamma r_{t+1} + ... + \\gamma^{T-t} r_T$. We use the implementation of PPO from OpenRLHF framework [HWW+24] for our experiments and set $\\gamma = 1$ and $\\lambda = 0.95$. As in previous works, we also use a KL penalty term to stop the model from drifting too far from the base SFT model $(\\pi_{\\theta_{SFT}})$. Therefore the per-token reward is given by:\n$r_t = R(q, o) \\mathbb{1} (o_t = EOS) - \\beta \\log \\frac{\\pi_{\\theta_{old}}(o_t|q, o_{<t})}{\\pi_{\\theta_{SFT}}(o_t|q, o_{<t})}$,\nNote that the expected total reward is\n$E_{\\pi_{\\theta}(q)} [\\sum_{t=1}^{T} r_t] = E_{\\pi_{\\theta_{SFT}}(.|q, o_{<t})} R(q,o) - \\beta D_{KL} (\\pi_{\\theta_{old}} (.|q,o_{<t}) || \\pi_{\\theta_{SFT}}(.|q,o_{<t}))$.   (2)"}, {"title": "3 Can Pure RL Lead to Thinking Behavior?", "content": "An immediate question that arises from the RLSP framework is: Are SFT and exploration reward signals necessary or can LLMs learn to search directly with pure PPO training on the binary output of a verifier? Recall that Equation 2 sheds some light on this question in a theoretical sense. To empirically answer these questions, we perform following set of experiments on Llama and Qwen models. In these experiments we use response length as an objective metric to measure the search behavior. We will not concern ourselves with accuracy in this section."}, {"title": "Llama Models", "content": "Starting from Llama-3.1-8B-Instruct [Met24] as the base model on which we perform no SFT, we do a) PPO training on the training dataset of MATH with no exploration reward of any kind; that is $\\alpha = 1$ in Eq. (1). b) PPO training on the training dataset of MATH with the creativity reward proportional to length: $R_{cr}(q, o) \\propto \\frac{|o|}{C}$. Our findings are in Figure 6."}, {"title": "Qwen Models", "content": "We also carried out a similar experiment with Qwen2.5-7B-Instruct model in math domain. The result is presented in Figure 7."}, {"title": "4 Empirical Evaluation of RLSP", "content": "Having established that RL training even with simple exploration reward signal can lead to emergent search behaviors, in this section we evaluate the impact of full RLSP training on model performance within the math domain. Our main goal in this section is to establish that search behavior leads to improved problem solving abilities.\nOur experiments focus on two different model types with varying sizes: an 8B parameter model and a larger 32B parameter model. Each model is trained and evaluated on a corresponding dataset tailored to its size, enabling a comprehensive analysis of the effectiveness of RLSP across different model scales. The results are presented in two subsections, each dedicated to a specific model and dataset configuration. The general experimental settings are provided in subsection B.1, while the specific hyperparameters for each experiment will be detailed in their respective sections.\nWe remark that our goal is not to compare against the SOTA performance of models on these benchmarks but relative improvement due to RLSP. Therefore, in all our experiments we compare the performance of same model trained using various strategies. We did not optimize prompts and various other factors that can influence the performance of models both in base models and in RLSP training. All our experiments are conducted on a cluster with 6 nodes (48 H100 GPUs), though most of our experiments utilize 24 H100 GPUs. Despite being performed at a small scale with only a few thousand training samples and iterations, our empirical evaluation provides strong evidence that RLSP can yield substantial gains when scaled up."}, {"title": "4.1 Performance Analysis: 8B Model", "content": "For this experiment, we use Llama-3.1-8B-Instruct [Met24] as our base model. As observed in section 3, while pure RL with an exploration reward signal encourages thinking behavior, the overall performance remains suboptimal due to the model's moderate size and limited capabilities. Therefore, we conduct additional SFT on a math-specific dataset that includes long Chain-of-Thought (CoT) reasoning traces [Qwe24c], sourced from the QwQ-32B-preview model [Qwe24b]. To ensure the integrity of our evaluation, we decontaminate the dataset to remove any overlap with a wide range of test datasets. Further details on the dataset curation process are provided in subsection B.2. Hyperparameters for the fine-tuning is reported in subsection B.3.\nDuring the RL stage, the SFT model is trained with PPO on the training set of the MATH dataset [HBK+21], utilizing only the (problem, answer) pairs. Problems from the training set that are already correctly solved by the SFT model are excluded. Details on the hyperparameters used for RL training are provided in subsection B.4.\nFor RLSP training, we use structured prompts with the GPT-40 model [HLG+24] to generate exploration reward scores. Further details about the prompts can be found in subsection B.6.\nFor evaluation, we measure the model's accuracy on the 500 test splits of the MATH dataset (a.k.a. MATH-500)."}, {"title": "4.1.2 Evaluation Results", "content": "Table 1 presents the results of this section.\nWe now analyze these findings and highlight key observations. Firstly, SFT training yields a significant improvement in the base model's performance. This result underscores the importance of high-quality supervised demonstrations in enhancing the model's reasoning capabilities, especially for small models."}, {"title": "4.2 Performance Analysis: 32B Model", "content": "In this setup, we use Qwen2.5-32B-Instruct [Qwe24a] as the base model. Given its larger size and enhanced capabilities, we omit the SFT stage and focus on directly improving performance through RL training within our RLSP framework.\nDuring the RL stage, the base model is trained using PPO on AIME 918 problems from the years 1983 to 2003. We transition to the AIME dataset since the model already performs exceptionally well on the MATH dataset, nearing saturation. Thus, we focus on a more challenging setting to further assess and improve its capabilities. Details on the hyperparameters used for RL training are provided in subsection B.5.\nFor evaluation, we assess the model's accuracy on AIME problems from the year 2024, as well as on the 500 test splits of the MATH dataset (referred to as MATH-500)."}, {"title": "4.2.2 Evaluation Results", "content": "Table 2 presents the results of this section."}, {"title": "4.3 Token Efficiency of RLSP vs Self-Consistency", "content": "A key intuition behind the RLSP framework is incentivizing the model to learn and apply search behavior during inference. We evaluate the token efficiency of the RLSP-trained model by asking: For a similar compute budget, what is the accuracy achieved by an RLSP-trained model versus a model using standard CoT with majority voting? We discuss the results in the following."}, {"title": "5 Related Work", "content": "In this section, we give a detailed literature survey."}, {"title": "Large Reasoning Models", "content": "OpenAI's o1 series [Ope24d, Ope24a, JKL+24] and other reasoning models [Goo24, DA25, Qwe24d] represent a significant leap in reasoning capabilities, excelling in structured reasoning, systematic problem decomposition, and reliable handling of complex tasks. Testing is conducted on high-stakes benchmarks, including mathematics [HBK+21, Ope22, AM24a, AM23, Ope24b], competitive programming [JHG+24, Ope24c, Cod24], and scientific problem-solving [HBB+20, RHS+23], often achieving performance levels that surpass human experts.\nRecently, numerous open-source frameworks strive to replicate ol's reasoning capabilities through diverse methodologies. At the post-training stage, frameworks such as [WFW+24, ZZH+24, ZWL+24] utilize automated data augmentation with MCTS, while [HZL+24, HGH+25, Nov25, MCJ+24] exploit reasoning traces in long-CoT data. Process reward models are integrated into the training process by [CYW+24, GZL+25], boosting self-exploration. [Kim25] introduces an effective RL framework emphasizing"}, {"title": "Scaling Test-Time Compute", "content": "Scaling test-time compute enhances reasoning capabilities by allocating more computational resources during inference. The test-time scaling laws demonstrate that increased deliberate reasoning (e.g., through additional token generation or iterative steps) directly improves accuracy, especially in complex tasks like mathematics, coding, and planning [SLXK24, WSL+24, BJE+24, BTR24]. Recent work on simple test-time scaling [MYS+25] shows that even small models can achieve significant improvements in reasoning tasks by strategically allocating inference budgets. In addition, RL-based scaling approaches [HLL+25] show that inference scaling trends becomes more evident as training scales, reinforcing the connection between RL and test-time compute.\nVarious test-time search methods exploit this principle [KLC+24, WDL+24]. Majority vote aggregates predictions from multiple inference traces to refine accuracy. Tree search methods such as [YYZ+24, HGM+23, ZLH+24, QMX+24] systematically explore reasoning paths. Beam search [SD61] leverages the PRM and retains top-K paths at each step to improve sequence generation. Lookahead search [SLXK24] further enhances exploration depth by evaluating paths using simulations. While these methods improve reasoning accuracy, they increase computational demand, highlighting the trade-off between performance and resource efficiency."}, {"title": "Math Data with Long Reasoning Traces", "content": "Research indicates that combining System 2-inspired deliberate reasoning with System 1's fast, heuristic-based thinking significantly enhances reasoning performance [SSR+24]. To equip language models with System 2 reasoning techniques, recent advancements have explored approaches such as supervised fine-tuning (SFT) with extended chain-of-thought (CoT) reasoning [BZL+24, AAB+24, MCJ+24, HZL+24, QLZ+24, WCW+24, XJH+24]. Training datasets with long reasoning traces, often distilled from advanced models like GPT [LBT+24, LSX+23, YJS+23] and Qwen-qwq [Qwe24c], are critical for fostering complex reasoning capabilities. To address the limitations of generator models, exploration-based techniques like MCTS [GZL+25, ZZH+24] and rejection sampling [YYL+23, BJE+24] systematically enhance reasoning by expanding decision paths. These methods enable language models to improve reasoning abilities and generate high-quality solutions to challenging problems, surpassing the constraints of their training data.\nRecent studies further support the effectiveness of long CoT data. [ZHL+25] demonstrates that reinforcement learning on long CoT-augmented data enables small models to exhibit emergent reasoning capabilities with significantly fewer examples. Similarly, [BvWT25] highlights that long CoT SFT leads to substantial improvements in problem-solving accuracy across mathematics and programming tasks. Furthermore, [YTN+25] systematically investigates the factors driving long CoT emergence, revealing that while long CoT is not strictly necessary for reasoning ability, it substantially enhances training efficiency and stabilizes reinforcement learning by providing structured, verifiable reasoning trajectories. Meta-CoT [XSG+25] provides a theoretical foundation for why long reasoning traces may enhance reasoning abilities. It argues that standard CoT fails to fully capture complex, non-linear thought processes and that explicitly modeling latent reasoning steps improves performance in high-difficulty tasks."}, {"title": "Self-Correction in Language Models", "content": "Self-correction in LLMs has gained significant attention as a mechanism to enhance reasoning and problem-solving abilities. A range of techniques has been explored, spanning from fine-tuning methods to advanced reinforcement learning strategies. Fine-tuning approaches leverage curated data to train models for iterative corrections, improving their ability to refine responses [MCJ+24, QZGK24, ZKL+24]. Prompt-based approaches focus on eliciting better outputs through iterative feedback loops [HCM+23], while inference strategies like MCTS are employed to refine reasoning by exploring diverse solution paths [ZLH+24, YZH+24, TPS+24, WST+24]. Reinforcement learning methods such as Self-Correction via Reinforcement Learning (SCoRe) [KZA+24], enhance self-correction through multi-turn RL using intrinsic rewards to guide learning.\nWhile extrinsic feedback mechanisms remain effective, intrinsic self-correction\u2014where models refine their outputs without external input\u2014has proven challenging yet promising. Notable advancements include reward-based RL [YZH+24, CAG+24, KZA+24] and curriculum preference learning [WST+24], which enable iterative refinement of reasoning steps. Unlike these approaches, we observe that self-correction behavior emerges naturally through unsupervised RL guided solely by a length penalty signals. This suggests that our method can be easily adapted to other domains without the need for domain-specific agent design or reward hacking."}, {"title": "Reinforcement Learning with Auxiliary rewards", "content": "Reinforcement Learning (RL) with auxiliary rewards improves policy optimization by incorporating pseudo-reward signals that guide learning beyond task completion. The foundational work by Jaderberg et al. [JMC+16] uses auxiliary control and prediction tasks to accelerate learning. Recently, RL has been applied to enhance reasoning in large language models (LLMs). [HDR+24] trained LLMs with outcome-based reward models (ORMs) and utilized a dense reward signal derived from comparing partial solutions to reference solutions. [SNF+24] introduced process advantage verifiers (PAVs), which assign stepwise rewards based on changes in correctness probability judged by a stronger prover policy. [LSX+23] employed process-supervised RL by distilling a process reward model (PRM) from GPT-4 annotations. In contrast, our work introduces a dense exploration reward that does not require a reference solution, making it more unsupervised and adaptable. Furthermore, rather than distilling PRM from GPT-4, we propose a more general approach to measuring creativity and reasoning effort, enhancing applicability across diverse domains."}, {"title": "6 Conclusions, Limitations, and Future Work", "content": "In this work we proposed a post-training technique called RLSP to enable thinking behavior for LRMs, showed promising results both in terms of performance and emergent behaviors. More large scale experiments and analysis are necessary to fully understand capabilities and limitations of our work. Needless to say, our work is a small step towards complex reasoning in LLMs and opens up several fascinating research directions: How do we enable finer-grained test time search in LLMs where search time can have a direct influence on the quality of the solution so that model learns to differentiate between 1+1 = ? and the Riemann hypothesis. What is the impact of context length on reasoning? Can pure RL with no exploration reward lead to thinking behavior at some model scale? and what is the precise influence of pretraining data? While all our models show interesting search behaviors such as backtracking and verification, none of those search strategies are surprising to us, and indirectly present in the pretraining data as humans use those strategies as well. Is there a truly emergent behavior akin to \"move 37\" that surpasses human reasoning or at least unexpected? Finally, looking back at Figure 5, what other training recipes are needed to unlock even higher forms of reasoning such as creating abstractions and theories, and solving open ended problems like climate change or grand unified theory of everything."}, {"title": "A Examples of RLSP Emergence Behavior", "content": "In this section, we present three examples demonstrating the emergence of self-verification, backtracking, and self-correction behaviors. The models are trained solely using PPO with a length penalty reward on the Llama-3.1-8B-Instruct model, without any supervised fine-tuning. All steps exhibiting these behaviors are highlighted in red.\n\u2022 Self-Verification: The model explicitly checks its own reasoning steps to confirm correctness before finalizing an answer. This often involves re-evaluating computations or validating logical consistency.\n\u2022 Consideration of alternative possibilities: The model searches over multiple rationales and explores alternative approaches and recognizes inconsistencies or similarities.\n\u2022 Self-Correction: The model identifies errors in its intermediate steps and revises its solution in the subsequent steps.\n\u2022 Backtracking: Upon identifying errors or dead ends, the model goes back to an earlier step and considers a different reasoning path.\nThese examples provide strong evidence that RL facilitates the emergence of more systematic and reflective problem-solving behaviours without explicit supervision."}, {"title": "B Experimental Details", "content": "Libraries We use OpenRLHF repo [HWW+24] for both SFT and RL training, leveraging Ray [MNW+18] for distributed training.\nTraining Configurations All training procedures used AdamW with \u03b2\u2081 = 0.9, \u03b22 = 0.95 and a cosine learning rate schedule with warm-up (3% of the training steps) and a minimum learning rate set to 10% of the"}, {"title": "B.2 Dataset Preparation", "content": "We utilize the publicly available QwQ-LongCOT [Qwe24c] dataset with responses generated using QwQ-32B-Preview [Qwe24d] model. We keep the entries belonging to the problems in the NuminaMath-CoT dataset [AM24b] and filter out the rest of the entries. We then de-contaminate the dataset by excluding any problems that appears in the test sets of the following collection of benchmarking datasets: math, math500, gaokao2024_I, gaokao2024_mix, gaokao_math_cloze, cn_middle_school, minerva_math, tabmwp, svamp, carp_en, mmlu_stem, openai_math_splits, amc23, math, gaokao_math_qa, gaokao2023en, gsm8k, aqua, sat_math, olympiadbench, asdiv, mawps, aime24, cmath, gaokao2024_II, and college_math. Finally, we remove any duplicate problems so that each question appears exactly once in the final filtered dataset. The final version contains 88k samples in total. We will release the code for reproducing the preparation steps."}, {"title": "B.3 Hyperparameters for the SFT stage of subsection 4.1", "content": "We perform the SFT training for 5 epochs with a global batch size of 64. The optimizer is AdamW with a learning rate of 2 \u00d7 10\u22125."}, {"title": "B.4 Hyperparameters for the RL stage of subsection 4.1", "content": "For PPO training, we use Generalized Advantage Estimation (GAE) with \u03bb = 0.95 and a discount factor \u03b3 = 1. The PPO objective used an = 0.2 clipping, value clippoing of 0.2, and initial KL coefficent to be 0.05. The actor learning rate was set at 2 \u00d7 10\u20137, while the critic learning rate was 2 \u00d7 10\u20136. We use a rollout batch size of 512, and training batch size of 128. The model was trained for one epoch over 200 PPO episodes. We use C = 1000 for the response length penalty. Reward normalization and clipping in the range [-10, 10] were applied. The training was performed with DeepSpeed ZeRO-3 for memory efficiency."}, {"title": "B.5 Hyperparameters for the RL stage of subsection 4.2", "content": "The training setting is identical to the previous experiment. Additionally, optimizer states are offloaded to the CPU to mitigate memory pressure."}, {"title": "B.6 Sample Prompts", "content": "Real-time Creativity Score Rating (subsection 2.2)\nYou are a **Thinking-Effort Grading Assistant**. Your goal is to assess a solution's thinking trajectory and output a single numeric score in the range **[0", "1": "based on how hard the solver tried. You must **not** evaluate correctness of the final answer. Instead", "thinking outside the box.\"\nUse the following steps and guidelines": "n### 1. Understand the Inputs\n**Problem Statement**: A description of the task or question the solver was trying to address.\n**Solution Trajectory**: The step-by-step reasoning, sketches, or approaches the solver used. You will be given both pieces of information. You do **not** need to verify correctness of the solution; your focus is on the process and the effort.\n### 2. Key Dimensions to Evaluate\n1. **Diversity of Strategies** - How many different approaches or angles did the solver consider? Did they pivot or switch methods after encountering difficulties?\n2. **Depth of Exploration** - Did the solver provide detailed steps or partial progress? - Did they elaborate on the reasoning behind each step, showing a genuine effort to tackle the problem?\n3. **Creativity and Novelty** - Did the solver propose any unusual or \u201cout-of-the-box\u201d ideas? Were there any signs of creative leaps or innovative methods?\n4. **Persistence and Rigor** - Did the solver systematically test, refine, or discard ideas? - Did they keep trying to move forward despite challenges or dead"}]}