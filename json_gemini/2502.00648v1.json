{"title": "AGENCY IN THE AGE OF AI", "authors": ["Samarth Swarup"], "abstract": "There is significant concern about the impact of generative AI on society. Modern AI tools are capable of generating ever more realistic text, images, and videos, and functional code, from minimal prompts. Accompanying this rise in ability and usability, there is increasing alarm about the misuses to which these tools can be put, and the intentional and unintentional harms to individuals and society that may result. In this paper, we argue that agency is the appropriate lens to study these harms and benefits, but that doing so will require advancement in the theory of agency, and advancement in how this theory is applied in (agent-based) models.", "sections": [{"title": "1 The Problem", "content": "We are living in a time of rapid and far-reaching change, the \u201cAge of AI\u201d, driven by emerging AI technologies and tools. This is naturally giving rise to a range of concerns about the impacts of these tools, particularly generative Al tools, on society. Our goal, and blue sky idea, is to lay out a research perspective for addressing these concerns, especially to mitigate the potential harms. We begin by summarizing the problem, before laying out the theoretical and methodological advancements we imagine for addressing it.\nThere are multiple kinds of harms from generative AI that have been hypothesized and discussed in the recent literature. These can be grouped into the following categories:\nMisuse by malicious actors: A commonly discussed example is, emerging threats to democracy. Kreps and Kriner [2023] summarize the three main challenges as threats to representation, accountability, and trust. Lawmakers (elected representatives) and rule-making agencies respond to public concerns, which are expressed through messages sent to them by the public, by making new laws and regulations. This process can be subverted by malicious actors through the use of AI tools by generating large numbers of biased or fake messages. Similarly, accountability, i.e., the ability of the public to choose their representation through fair elections, can be subverted by, e.g., the spread of misinformation to influence opinions on social media. The repeated misuse of these tools, or even the perception of misuse can lead to the erosion of trust in the democratic process. Other examples of misuse by malicious actors include the spread of misinformation during crises such as epidemics [Blauth et al., 2022], leading to inefficient decision-making and implementation of interventions, and the facilitation of cyberterrorism by the increased ability to find exploits [Bernardi et al., 2024], making people and organizations more vulnerable.\nExploitation of the changed information landscape: Just the presence of generative AI tools and their possible use creates some pernicious effects. The main example here is the \"liar's dividend\", where, e.g., a politician or a company can claim that any evidence of wrongdoing by them is fake (created by their opponents using generative AI tools) [Schiff et al., 2024]. This effectively makes it hard for the public to judge evidence and to properly evaluate their choices when voting, buying products, etc.\nCorruption of the tools themselves: As these tools are being trained and iteratively improved, they can also be prone to corruption. Attackers can try to manipulate ML models by, e.g., providing bad training data intended to"}, {"title": "2 The Idea", "content": "We believe that agency is the most appropriate theoretical lens to view the problems outlined above. There are multi-ple theories and definitions of agency [e.g., Kiser, 1999, Shapiro, 2005, Hodges, 2022, Emirbayer and Mische, 1998, Tasselli and Kilduff, 2021]. The most commonly accepted theory in the multi-agent systems community is the Plan-ning Theory of Agency, due to Bratman [1987, 2007, 2013]. Broadly, autonomous agency is the ability to choose your own goals and make effective plans to achieve them, i.e., \u201cthe agent herself directs and governs her practical thoughts and action\" [Bratman, 2007, p.4]. This theory has been computationally operationalized as the Belief-Desire-Intention (BDI) model [Rao et al., 1995], and its many variants.\nBriefly, a BDI agent has beliefs about (the current state of) the world, desires for preferred states of the world, from which it selects a goal, and intentions or (partial) plans about how to achieve its goals. This model has been tremen-dously successful in driving research in MAS. To see the usefulness (and limitations) of this model in unifying the problems from the previous section, let us briefly take an adversarial perspective.\nConsider a thought experiment with an agent and an adversary. The agent has its beliefs, goals, plans, etc. The adversary's goal is to limit or reduce the agent's agency. What are the ways in which it could do this? We refer to the following as the adversary's attacks on agency.\n(A1) It could counteract the agent's plan by preventing its actions from succeeding somehow.\n(A2) It could try to affect the agent's planning process so that it fails to come up with effective plans.\n(A3) It could try to influence the agent's desires and goal selection, so that the agent only develops desires and goals that align with the adversary's choices.\n(A4) It could try to influence the agent's beliefs so that the agent thinks its desires are unachievable.\n(A5) It could try to affect the agent's belief formation system, so that the agent is unable to develop true beliefs by, e.g., providing misinformation or affecting the agent's ability to distinguish good and bad information.\n(A6) It could change the environment such that the agent only has bad options available.\nLet us consider the problems from the previous section in light of this approach. The threats to representation and accountability in democracy fall under attacks A3, A4, and A5, as does the spread of misinformation during crises. The threat of trust erosion and that of increased cyberterrorism falls under A6. The liar's dividend example is an instance of attack A5, as it affects the agent's ability to distinguish true and false information.\nTools extend agency by extending our cognitive abilities and by extending the range of actions we can take, thereby allowing new desires and correspondingly new plans. Thus, corruption of the tools results in a reduction of agency in"}, {"title": "3 The Approach", "content": "Different strands of research have developed different and complementary pieces of the blue sky theory we envision. These have to be brought together, extended, and integrated into a cohesive whole and then operationalized into a model. We summarize these strands below.\nIn keeping with observation O1, the sociological Relational Theory of Agency [Emirbayer and Mische, 1998, Burkitt, 2016] gives primacy to the interactions among agents. Their definition of agency is surprisingly similar to the Planning Theory, with some key differences. In their view, human agency consists of three parts [Emirbayer and Mische, 1998]. The first is an \u201citerational element", "the selective reactivation of past patterns of thought and action\", which is very similar to a plan library. The second is a \u201cprojective element\", which consists of \u201cthe generation of possible future trajectories of actions\", i.e., the generation of (partial) plans. The third is a \u201cpractical-evaluative element\", which is the capacity \u201cto make practical and normative judgments among alternative possible trajectories of action\". The key differences are that this perspective emphasizes social interactions (in the practical-evaluative element) and the cognitive aspects of planning (in the iterational and projective elements). A limitation is that the social interactions considered are not adversarial in particular, thus extra work is needed to apply it to our problems of\"\n    },\n    {\n      \"title\"": "3.1 Agent-based Modeling and Simulation"}, {"content": "While agent-based models (ABMs) are used in many domains, we are not aware of their use yet in modeling the harms due to generative AI tools. LLMs are increasingly being used in ABMs to model human behavior, but they are not being used as LLMs qua LLMs [Gao et al., 2024]. Our idea is to develop ABMs that include representations of \"baseline\" humans using the extended BDI models described above, representations of humans augmented with AI tools, and autonomous AI agents, all interacting in social situations of interest, such as elections or epidemics.\nThis blue sky idea also introduces a set of challenges for ABMs and simulation, which we discuss below. To start with, we note that ABMs are also generative models that produce complete and richly structured large-scale data sets. However, the theoretical and methodological challenges they pose are different from generative models in machine learning.\nRealistic simulation design: For answering specific questions, it is good to have the specifics represented in the model. For example, to assess the impact of school closures on mitigating an epidemic, it is good to have a model that represents schools well, and also represents the activity patterns of children when schools are closed. Designing realistic simulations of the problems listed in Section 1 is going to be quite challenging in terms of representing the population accurately with demographics, behaviors, information flow, etc. The use of synthetic populations or digital twins might be appropriate in this regard [Raghunathan, 2021].\nScaling: The ABMs and simulations we are proposing would be large and complex. Scaling these to large numbers of agents is an active area of research, even in the current BDI framework [e.g., de Mooij et al., 2023]. Constructing"}, {"title": "4 The Path Ahead", "content": "We have laid out an argument for using agency as the lens through which to evaluate the potential harms of generative AI tools to society. We have also discussed the advancements needed both theoretically and methodologically to be able to use the concept of agency in this way. Agent-based simulations will put these new models into motion, so that we can evaluate particular scenarios. The path ahead is not sequential; several of these problems can and should be worked on in parallel. At the same time, it is worth keeping in mind that generative AI tools can also be enormously beneficial in multiple ways:\n\u2022 By enabling computing substrates with intuitive, seamless interactions, thus truly democratizing computing-based infrastructure.\n\u2022 By providing information in an easy-to-understand and unbiased way, thus creating a more informed and educated population.\n\u2022 By improving decision-making in complex systems, thus increasing efficiency and reducing waste.\nThus, perhaps the best application of the new theory, model, and simulations would be:\nEvaluating better futures: How can AI systems and humans act collaboratively to augment agency, instead of just adversarially? What new questions does that pose for a theory of agency?"}]}