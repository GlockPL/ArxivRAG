{"title": "AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People", "authors": ["Seonghee Lee", "Maho Kohga", "Steve Landau", "Sile O'Modhrain", "Hari Subramonyam"], "abstract": "People with visual impairments often struggle to create content that relies heavily on visual elements, particularly when conveying spatial and structural information. Existing accessible drawing tools, which construct images line by line, are suitable for simple tasks like math but not for more expressive artwork. On the other hand, emerging generative AI-based text-to-image tools can produce expressive illustrations from descriptions in natural language, but they lack precise control over image composition and properties. To address this gap, our work integrates generative AI with a constructive approach that provides users with enhanced control and editing capabilities. Our system, AltCanvas, features a tile-based interface enabling users to construct visual scenes incrementally, with each tile representing an object within the scene. Users can add, edit, move, and arrange objects while receiving speech and audio feedback. Once completed, the scene can be rendered as a color illustration or as a vector for tactile graphic generation. Involving 14 blind or low-vision users in design and evaluation, we found that participants effectively used the AltCanvas's workflow to create illustrations.", "sections": [{"title": "1 INTRODUCTION", "content": "Spatial constructs play a crucial role in human cognition and communication [37]. We rely on spatial information for describing scenes, learning complex concepts in science and mathematics, and artistically expressing ourselves. For instance, when describing a pet-friendly home in a blog article, we can sketch out where to place pet beds, food stations, and play areas showing dogs and cats at play. For sighted users, several tools exist for creating visual graphics across different fidelity, ranging from digital pen-based drawing applications (e.g., SketchPad [59]) to direct manipulation tools such as Adobe Illustrator [3] for authoring rich vector graphics. However, based on a recent survey [67], while blind or vision impaired (BVI) people are motivated to create visual content for school, work, and art, current access barriers in content creation tools hinder their ability to do so. Although linguistic references such as \"next to\" or \"jagged\" are commonly used for spatial and structural descriptions, they can also be abstract and ambiguous. The act of visualizing allows us to be more precise in articulating spatial information.\nExisting drawing tools designed for BVI users take a guided constructive approach (i.e., line-by-line drawing) and primarily concentrate on basic graphics, such as shapes and geometric figures [9]. While this allows a high degree of control within specific drawing tasks, it can be tedious to create more expressive open-ended graphics, for instance, the illustration of the pet-friendly home in the earlier example. On the other hand, tools based on emergent text-to-image generative models [18] allow for rich, expressive illustration and digital artwork using text prompts [14]. However, the trade-off is in the degree of control. With a generative AI approach, users may have to describe entire scenes in natural language and have less control over specific spatial and visual attributes such as composition, size, color, relative distance, orientation, etc. In other words, it would be difficult to generate a highly specific illustration of the intended pet-friendly home. Our own exploration has shown that the models don't always accurately interpret text prompts (e.g., a prompt for \"image with five apples and one apple sliced in half\" resulted in one apple sliced into five pieces). In fact, recent work in generative AI-based image tools for BVI users has looked at question-answering mechanisms to validate the generated output across complex scenes [29]. Alternatively, creating accessible visual content like printed tactile graphics is also challenging, as direct conversion is often impossible, making iterative design difficult.\nIdeally, we would like tools that can leverage generative AI capabilities to create expressive content but also support a constructive approach for better control and composition. In this regard, for sighted users, researchers have explored techniques such as users providing a rough doodle of scene composition to inform the semantics and style of generation [49]. Turning our thoughts into tangible visuals requires such forms of control through iterative representation, feedback, and adjustments [42]. Thus, our motivating question for this work is \"How might we support a constructive approach to authoring visual content while also leveraging the generative capabilities of AI models?\" By conducting a formative study with five blind participants who have created visual content in the past, we learned about current content authoring workflows, associated challenges, and opinions about generative text-to-image models. Specifically, experienced users expressed (1) a need for precise control and guidance to optimize the design, (2) the ability to use pre-existing graphics with options for deleting backgrounds and tools for rescaling and editing, and (3) enhanced usability of generative Al features through verbal descriptions and explanations of graphics without needing to print each iteration. Based on these requirements, we developed AltCanvas, using an iterative design and evaluation approach.\nAltCanvas is a generative AI-powered illustration tool that implements a dynamic tile-based interface and sonification features to support the authoring of visual graphics. It consists of a side-by-side layout of a tile view (i.e., an alternative to a direct manipulation drawing canvas AltCanvas) and an image view to render the image under construction. The tile view is designed based on an understanding of blind spatial cognition and relational aspects of objects in images (i.e., \u201cthe vast majority of visual information is really spatial information [22] \"). At the start of creating a new illustration, the tile view consists of a single tile, and dynamically expands along eight directions to allow adding additional objects to relative locations. As shown in Figure 1, the user starts by adding a ball object to the canvas using speech-to-text and text-to-image. They then add a dog to the right of it by navigating to the adjacent tile on the right. AltCanvas implements speech-based descriptions of the generated image and, through keyboard navigation and sonification, allows creators to compose complex images through resizing, repositioning, and other edit features. Finally, once the image is composed, AltCanvas allows creators to generate a representation suitable for printing as tactile graphics or can render full-color graphics for sharing with sighted people. In developing AltCanvas, we prototyped a range of interaction techniques for different authoring tasks and conducted a preliminary study to understand the preferred interactions of BVI content creators. Based on feedback, we revised our designs and evaluated the final prototype through a user study. Across the formative and iterative evaluation studies, we worked with 14 participants.\nOur main contributions include (1) a tile-based interaction paradigm that provides an alternative representation of the visual canvas, (2) a novel constructive and generative image authoring workflow using speech and sonification, and (3) results from authoring content with AltCanvas and tactile graphic evaluation from BVI users.\""}, {"title": "2 RELATED WORK", "content": "2.1 Image Editing Challenges for BVI users\nWhen editing images, users must continually adjust their input based on visual feedback. However, many current editing software lack the capability to offer real-time visual feedback through screen-readers [2, 34, 55]. This is a serious limitation as visual feedback is crucial for editing operations such as alignment and overlap [43, 50]."}, {"title": "2.2 BVI Image Editing Interfaces", "content": "Grid-based interfaces can help BVI users make more precise point selections [33]. Prior work has explored variations such as IC2D's grid-based system [33] and haptic display with a grid system that divides the drawing surface into m \u00d7 n sections [26]. To navigate around different regions on the grid, keyboard commands coupled with verbalizations of grid locations were used. Alternately, the use of a table structure to convey information has also been used in prior work to make data flow diagrams accessible for BVI students [54]. Considerations for table-based interfaces include screen-reader accessibility, but it is challenging to operationalize well even if tables have been properly marked up [21, 64]. Innovative navigational aids like those in TextSL offer BVI users the ability to move through virtual spaces safely and efficiently using natural language for collision-free navigation and relative location information, replacing traditional coordinate systems with directional cues such as \"north\" or \"northeast\" [20]. This suite of tools and methodologies reflects a growing commitment to enhancing the usability of image editing software for BVI users through multi-sensory and accessible interfaces. Yet there is more to be explored in terms of effective guidelines for creating BVI-accessible editing software.\nImage description and tactile feedback are crucial components of accessible image editors for visually impaired users. Early innovations in this field include TDraw, which offered tactile feedback without erasure capabilities [36], and Watanabe's system, which introduced full erasure functionality via a Braille display and stylus [63]. Recent advancements have expanded these capabilities: the Draw and Drag Touchscreen provides interactive manipulation with text-to-speech output for geometric shapes [24], \"Playing with Geometry\" utilizes vibrotactile feedback for freehand drawing [12], and AudioDraw employs audio feedback to assist in creating educational diagrams [25]. Parallel developments in image description systems have further enhanced accessibility. The 'RegionSpeak' system enables users to crowdsource object labels and navigate spatial layouts [68], while 'TactileMaps' introduces a novel tactile exploration method using a raised-line map overlaid on a multi-touch screen with auditory feedback [11]. Leveraging advanced technology, 'Image Explorer' employs deep learning to create a multi-layered, touch-based exploration system [38]. These innovative tools, spanning from tactile interfaces to sophisticated image description systems, significantly improve visually impaired users' ability to interact with and understand visual content through a combination of touch and auditory feedback."}, {"title": "2.3 Image Generation and Editing with AI tools", "content": "Generative Al has opened new avenues for creating accessible technology. Recent research has explored its use in making short-form videos accessible through video summaries, as demonstrated in projects like AVscript and ShortScribe [30, 62]. It has also been employed to enhance image generation processes (GenAssist) and to design effective tactile graphics using image-to-image generation techniques (Text to Haptics) [29, 30, 60]. Complementary tools such as BeMyEyes, Apple VoiceOver, Android Talkback, Chromebook Chromevox, and Braille displays further assist users in navigating and understanding various visual mediums [19, 39, 56]. An innovative approach is exemplified by WorldSmith, a multimodal tool that enables users to design and refine complex fictional worlds through layered editing and hierarchical compositions, incorporating a prompt-based model that integrates text, sketches, and region masks as inputs [15]. Similarly, Crosspower explores leveraging language structure to facilitate graphic content creation, highlighting both the potential and limitations of language as a primary medium for image editing [65]. While natural language processing offers precision, its inherent limitations underscore the need for tools that provide greater control and flexibility in visual content creation and editing for visually impaired users."}, {"title": "3 FORMATIVE STUDY WITH BLIND VISUAL CONTENT CREATORS", "content": "To better understand existing visual content authoring workflows and associated challenges with representation, feedback, and iteration, we conducted semi-structured interviews with five blind experienced visual content creators."}, {"title": "3.1 Participant Recruitment", "content": "We recruited five participants by email using contacts in our immediate network. All participants self-identified as blind and had extensive experience authoring tactile graphics as part of their professional work or personal interest. Each semi-structured interview lasted between 45 and 60 minutes, and participants were compensated with a $50 Amazon Gift Card for their time. The interviews were conducted over the Zoom video-conference platform [69]. Table 1 provides a consolidated summary of all participants across different studies with specific columns indicating whether or not they were involved in each of the studies. Many of our participants engaged throughout the iterative design process."}, {"title": "3.2 Procedure", "content": "Our semi-structured interviews consisted of three parts. In the first part, we asked participants questions about their visual content authoring workflow. Example questions include \"Can you tell us about the last time you authored a tactile graphic? What was the graphic about?\", \"Can you walk us through your process for authoring that tactile graphics using your current tools?\", \"Can you tell us about the tools you usually use and how they help you with the authoring process?\" Based on this context and common ground, we proceeded to ask them questions to understand current challenges and needs for visual content authoring tools. We asked, \"What are some pain points or challenges in your authoring workflow?\" and asked follow-up questions depending on their pain points. In the last phase of the interview, we asked participants about their experience of co-creating tactile graphics with AI, if any. We then brainstormed with the participants using guiding questions such as, \"What is the input you would initially provide to ChatGPT, and what output would you expect it to return?\" and \"how do you imagine iterating and editing on the graphics using ChatGPT?\" in order to understand their expectation and perceptions towards AI. All sessions were video recorded, and in a few instances, participants also emailed us artifacts of their creations."}, {"title": "3.3 Analysis", "content": "After the interviews, video recordings of the sessions were transcribed using the Zoom transcription features, and we read through the transcripts to ensure correctness (approximately 300 minutes of recordings). Using an open-coding approach [17], two of the authors then independently coded all of the transcripts using ATLAS.ti [31]. Over multiple cycles of discussion amongst the authors, we synthesized the key emergent themes around visual content authoring. We continued refining these themes until every category and subtopic was addressed, and no additional themes surfaced. Specific themes included qualitative descriptions and characteristics of good tactile graphics, resources used to learn about visual content, tooling, color, visual attributes, specific features for editing, perception during authoring, limitations of current workflows, unexpected outputs from generative AI, and imagined uses."}, {"title": "3.4 Findings", "content": "Across all sessions, participants reported needing to navigate multiple tools to author spatial content (both visual and tactile). Depending on the fidelity of the graphics they were creating, participants made use of low-fidelity tools, such as Wikki Stix and Sensational Blackboards. In many cases, they would later switch to more high-fidelity vector graphics authoring tools, but that often required assistance from sighted users. When digitally authoring, participants reported frequently printing out the in-progress content as tactile graphics to assess and provide feedback or revise on their own. Based on these authoring workflows, here we focus our findings on key low-level steps during authoring and associated challenges:"}, {"title": "3.4.1 Composition and Layout.", "content": "In many of the content that participants reported creating, they used existing graphic elements as a starting point, and the focus was more on composition. For instance, in one session, P3 designed holiday-themed greeting cards: \u201cI was given a bunch of images, and I am using a physical grid to position them. I then work with a sighted individual to run it through Inkscape and print it out to create a holiday greeting card.\" Other participants also described using a similar grid-based approach for layout composition. In such cases, they reported needing to perform several calculations about the dimensions of the image and grid coordinates to place the items in the correct position. According to P5: \"So okay, 50 units is half an inch. 25 units is a quarter of an inch, and with that kind of spatial reasoning and a lot of time in practice, you kind of get a sense of what the numbers you're dialing in are going to create on your canvas... however, this becomes complicated to re-calibrate when editing the location many times.\" During composition, participants reported needing agency in specifying in detail where different elements will go. As P1 mentioned: \"Say I am creating a seascape, in my mind, I can imagine how it will look, I know where everything goes, it is all in my head, this is what I want.\" Based on these insights, we infer that authoring tools that combine constructive and generative approaches should provide precise control over the composition and layout of generated content without needing to manually calculate specific coordinates (D1).\nSpecific to using existing graphics, participants regularly looked for online content as a starting point to create their illustrations. According to P3: \"my first process is to look for images online, and I'll look for keywords like black and white line art coloring pages and outline drawings.\" Further, P3 adds that they often need additional processing to make the visual compatible with tactile graphics: \u201cI did re-scale and do some color management with a software called Tactile view to eliminate some unwanted color. And I use the latest version of Windows photos to zap some backgrounds that were creating distracting background dots\u201d. As an alternative, two participants also reported having learned how to code support vector graphics (SVG) manually to create digital content but the process can be cognitively intensive. P1 also added that they don't always want a hand-drawn look and feel. Therefore, we want tools to make it easy for creators to access tactile graphics compatible illustrations (D2)."}, {"title": "3.4.2 Iterative Editing.", "content": "Across sessions, participants mentioned a range of intents for editing the graphics being created. The majority of editing was done to resize elements. When digitally authoring, participants reported frequently printing out the in-progress graphics to assess the level of detail and whether elements were too close to each other, or if the element was too small to perceive the detail. In the seascape example, P1 mentioned: \"We want it to be fun when we interact with it, feeling the curve of the shell or legs of the crab. So if the image is too small, those details get lost, and I have to make it bigger.\" Other edit operations included changing the appearance of the objects, such as curves and edges, or making some details less prominent through the use of primary and secondary lines to clarify which details should stand out. Based on these insights authoring tools should support \"dimension-based editing as well as feature-based editing (D3)\""}, {"title": "3.4.3 Feedback During Authoring.", "content": "A key challenge in digital authoring tools is the lack of feedback during authoring. All participants reported seeking inputs from sighted individuals to assess and describe the graphic being authored. Alternatively, they have to print it out as tactile graphics to assess on their own. According to P5: \"..., she [sighted spouse] can look at it and tell me you know where to move it or if it needs to be adjusted. But if I'm by myself, then yeah, it's a lot of iteration, just a lot of printing back and forth.\" Participants aspired future tools would require fewer print cycles. Specific to editing tasks, participants commented that it would be helpful for tools to provide feedback on well-known constraints such as overlapping edges or extending beyond the border. According to P1: \"have visual spell-checks... have categories of things like what lines extending beyond borders.\" Participants also mentioned detailed verbal descriptions would be helpful (e.g., \"on the top left"}, {"title": "3.4.4 Use of Generative Al.", "content": "Three of the five participants reported having explored different ways to use ChatGPT in their authoring workflow. For instance P3 reporting using GPT to get an initial SVG representation of an object and then iterated on their own. P1 was enthusiastic but also cautioned about its limitations based on their experience. According to P3: \"It's fun and interesting to write a description and get a sense of what AI has created, but it is also challenging. It doesn't do a good job of spatially putting them where you want them to be. Thing A and Thing B and how you want them to be related to each other. Being able to specify that would be very helpful.\" They further added that describing takes a lot of work; it can be cognitively intensive and ambiguous. Along similar lines, P4 expressed that AI might help with creating some initial shapes to help guide their own authoring but not doing anything elaborate. Lastly, participants felt that if they were trying to create something unique, AI may not be able to accurately generate the illustration."}, {"title": "4 USER EXPERIENCE", "content": "Based on the guidelines from the formative study, we developed AltCanvas to help BVI users create visual content. As shown in Figure 3, AltCanvas's interface consists of two main regions: (1) on the left half of the interface is a tile view for authoring, and (2) on the right is an image view that will render the image being authored.\nThe tile view itself is comprised of dynamically expanding interactive tiles that the user can navigate through directional commands (up, down, left, right). Each tile represents a single object on the image canvas. It provides a functional proxy for the creation and manipulation of that object, enabling users to control the object's properties effortlessly. The layout of the tiles on the tile view closely aligns with the relative positions of objects on the canvas. The tile view starts with a single tile, and once an object is associated with that tile, eight new tiles are added adjacent to that tile (left, right, top, down, top-left, top-right, bottom-left, bottom-right) to place objects relative to that tile object. Note that the tiles are all uniform and do not represent the distance or size of the object on the canvas. Rather, they provide users with an easy way to navigate through images on a canvas and to select regions on the canvas for new generations by utilizing relative locations of image objects. In an earlier iteration, we experimented with absolute positioning based on the object arrangement on canvas by encoding size and distance (similar to the current grid-based techniques our participants reported). For instance, we can imagine dividing the canvas into a 5 x 5 grid and assigning one or more tiles to an object depending on how much space it occupies. However, this made the navigation less consistent and required large changes in the tile positioning as objects moved around on the canvas. In the final design, we opted to encode just the relative position as tiles (based on design consideration D1). Size and distance are encoded using sonification and speech, which we will describe later in this section (based on design consideration D5).\nTo better understand how AltCanvas supports a constructive and generative image authoring workflow, let us look at how Otto, a blind user, can create an artwork of his dog playing with a Frisbee in a park."}, {"title": "4.1 Setup and Tutorial", "content": "To begin with, Otto opens AltCanvas on his web browser. Because this is the first time Otto is using the system, he presses SHIFT + K to hear the keyboard commands in the system. This will open a popup with the keyboard commands listed. These shortcuts were designed with reference to existing blind-accessible games and industry-defined accessibility practices, ensuring consistency and learnability (e.g., using SHIFT+ [LETTER] of the function name). Using the arrow keys, Otto can navigate through the 10 commands in order (see Appendix for a list of keyboard shortcuts). The first down arrow key will read the first command SHIFT + G Global canvas description to Otto. Further, AltCanvas supports stereo audio with panning across the left and right microphone to support directional navigation on the canvas."}, {"title": "4.1.1 System Setting: Speech Rate.", "content": "Since users have different speeds at which they recognize speech, depending on their proficiency with a screen reader, a system should allow the user to control different levels of speech rate. Before starting the prototype, users can choose their desired speech rate. Users can select from three distinct speech rate options within the system. The highest setting, rated as 3, matches the rapid delivery typical of screen readers. The intermediate option moderates the speed to a level between typical human speech and the swift pace of screen readers. The lowest setting, rated as 1, is calibrated to the comfortable listening speed for the everyday user."}, {"title": "4.1.2 System Setting: Image Style.", "content": "As different users have varying needs for image creation with editors, our system offers primarily two methods of image authoring support: images to share with general audiences and tactile graphics. Users can select the type of image they wish to create-a colored version to share with general audiences or a tactile version that can assist with tactile graphic generation."}, {"title": "4.1.3 System Setting: Canvas Size.", "content": "While the canvas size is set to default based on the user's screen width, on the system setting popup, users have the option to change the size of the canvas through keyboard input."}, {"title": "4.2 Adding Objects to the Canvas", "content": "Once familiar with the keyboard navigation commands, Otto exists the tutorial view and sees AltCanvas's the main authoring interface. By default, AltCanvas focuses on the single new tile on the tile view and greets Otto with an auditory guide: \"You are currently focused on the first tile. Press Enter to generate the image on the 600 by 600 canvas.\" Otto presses Enter, and a distinct beep earcon confirms the system is ready for voice input. Following the beep, Otto voices his request, \"Create an image of a dog.\" The system quickly processes his command and seeks confirmation, audibly prompting, \u201cDetected: Create an image of a dog. Press Enter to confirm or the Escape key to cancel.\" Otto confirms by pressing Enter, signaling the system to generate the image. Based on design consideration D2, we use a text-to-image model to generate the image.\nOnce the image generation is complete, it is rendered on the canvas. By default, the first image is positioned at the center of the canvas. AltCanvas provided Otto with a detailed description of the generated image (D4, D5): \"Dog has been generated. The coordinates of the image are 250 by 250. The dog is a golden retriever with golden hair and a smiling expression. The image measures 100 by 100.\" Initially, all images are generated at a size of 100 by 100. The coordinates of these images correspond to their relative positions on the canvas. Otto can retain this initial generation and proceed with further image creation and editing, or he can press the Enter key to regenerate the image with a different description or press Shift + X to delete the image if he wishes to try other objects for his artwork.\nOnce the dog image is generated, 8 new tiles are added adjacent to the dog image tile. This allows Otto to begin adding other objects to the scene relative to the dog object. During keyboard navigation of the tiles, when Otto reaches the edge of the existing tiles, he will hear an audio cue, a \"thump\u201d sound indicating he has reached the edge of the tile blocks. On a current empty tile, Otto will hear a navigation sound depending on whether he is moving up, down, left, or right. Based on iterative feedback, users preferred different directional sounds as they were navigating the tile view. When Otto enters a tile with an image, he will hear the name of the image itself. With the initial image accepted, Otto can now continue authoring the scene. He decides to add more elements to his canvas. By focusing on the tile to the right of the dog tile, he voices another command, \"Add an image of a tree with a thick trunk\", and follows the same confirmation process as before. The system seamlessly integrates this new element into the existing canvas, maintaining spatial awareness and providing Otto with real-time updates."}, {"title": "4.3 Perception of AI Generated Content", "content": "Now that Otto has generated a few images, he wishes to understand the layout and orientation of objects on the canvas. There are a total of four commands that Otto can use to understand the image: Global Canvas Descriptions, Local Information, Radar Scan, and Chat (to support design consideration D5). By using the SHIFT + G command, Otto activates the Global Canvas Description feature. The command provides an auditory overview of the entire canvas. The system describes the overall layout, any overlapping images, their relative positions, and the overall ambiance of the canvas, giving Otto a sense of visual aesthetics and composition. Once Otto presses this command, he hears the global description: \"Your canvas currently features a golden retriever in the center with a medium-sized tree to its right. Both images are well-separated with no overlap, set against a clear background.\" At any time, Otto can press the Escape key to exit the narration. Second, the SHIFT + I command allows Otto to obtain detailed information about a specific image or tile he navigates to. When this command is activated, the system provides a description that includes the image's precise coordinates on the canvas and its current dimensions. This localized information helps Otto understand the specific details of individual elements within his artwork. Otto focuses on the dog tile and presses the SHIFT + I command. The system speaks: \"The image depicts a golden retriever sitting upright and facing the viewer. The fur of the retriever is long, and it has a smiling face, with its tongue sticking out.\u201d\nThird, to get a sense of objects surrounding the active tile, Otto can activate the Radar Scan feature by pressing the SHIFT+R keys. Based on the user's current location, the radar scan identifies the nearest objects by name and measures the distance from the current image to others sequentially. For instance, pressing SHIFT+R while focusing on a dog would result in the scan announcing \u201cTree, 20 pixels away.\" Finally, for inquiries beyond the scope of automated descriptions, Otto can use the SHIFT + C command. This feature is designed for interactive engagement with the canvas (D5). Upon pressing SHIFT + C at an image tile, the system prompts, \"Ask a question about the image and I will answer.\" Following a beep earcon, Otto can vocalize specific questions about the image. This could include inquiries about color, style, or any other detailed attributes not covered in standard descriptions. Otto asks: \"Describe the shape of the dog's tail?\" and the system responds, \"The dog's tail in this image is raised and curved upwards, reflecting a sense of excitement or alertness.\"\""}, {"title": "4.4 Editing and Composition", "content": "Using the above perceptual feedback features, Otto can iteratively and flexibly edit images as he adds them to the canvas. AltCanvas provides Otto with four essential editing operations to refine his artistic creations. These include modifying the location and size of objects, pushing images around the canvas, and deleting unwanted elements. These operations were most commonly described by the formative study participants to compose objects in the scene. We deliberately opted not to support fine-grain edits of individual objects through direct manipulation, such as enlarging the size of the dog's ear. Instead, we use generative features and natural language prompts for such modifications. In section 8, we discuss these design trade-offs."}, {"title": "4.4.1 Editing the Location of an Image.", "content": "To adjust the position of an image, Otto presses SHIFT + L to enter location edit mode. In this mode, using the arrow keys moves the image across the canvas. Each movement triggers an auditory notification confirming the action. If Otto's adjustments cause the image to overlap with another object or reach the canvas edge, he receives specific audio cues. These cues include warnings of overlaps (\"Overlapping with Image X\") and sounds indicating the edge of the canvas. To check the image's coordinates during editing, Otto can press SHIFT, which tells him the current position, with each press of an arrow key shifting the image by 20 units."}, {"title": "4.4.2 Editing the Size of an Image.", "content": "To modify an image's size, Otto initiates the process by pressing SHIFT + S, activating the size edit mode. Here, the UP and DOWN arrow keys adjust the image's size. Increases in size are accompanied by a rising earcon frequency, while decreases produce a lowering frequency. This sensory feedback helps Otto visualize the changes in real-time. If he needs to know the exact dimensions during resizing, pressing SHIFT provides this information. Adjustments are made in increments of 10 units per arrow key press while maintaining the overall aspect ratio of the generated image."}, {"title": "4.4.3 Rearranging the Image.", "content": "When Otto needs to make space between images or reorganize the layout, he can use the push operation. By selecting an image and pressing SHIFT + ARROW KEY in the desired direction, the image shifts, clearing space for additional elements. This operation is confirmed audibly, \"Pushed Image to the [direction],\u201d providing Otto clear feedback on his action. This allows Otto to add new images in between existing images (e.g., adding a Frisbee object between the dog and the tree). The tile view serves as a persistent reference to augment his spatial cognition as he makes the edits."}, {"title": "4.4.4 Deleting the Image.", "content": "If an image no longer fits Otto's vision, he can remove it by pressing SHIFT + X. This command deletes the selected image, and Otto immediately hears, \"Deleted Image on the tile.\" The removal of the image results in an empty tile, which alters the auditory navigation landscape, helping Otto understand that the space is now available for new creations."}, {"title": "4.5 Rendering the Final Image", "content": "After Otto completes editing his images on AltCanvas, he moves to the final stage, where the images are rendered into their finished forms. This section includes two specialized rendering options tailored to enhance tactile and visual experiences. For tactile graphic rendering, AltCanvas uses a dedicated model designed to optimize the image for tactile graphics. This rendering process adjusts the texture and relief of the image, making it suitable for tactile perception and interpretation. This option is particularly valuable for BVI users like Otto, as it transforms the digital image into a vector graphics format that can be printed using a tactile graphics printer. Additionally, Otto can choose to re-render the image to enhance its visual qualities, such as incorporating more naturalistic backgrounds or adjusting the color palette. This operation refines the visual elements of the image, making them more appealing and realistic. To do this, Otto provides a speech description of the type of background rendering he wishes to do on this final image. The re-rendering process might include adding shadow effects, lighting adjustments, and blending elements to create a cohesive scene that visually communicates Otto's artistic intent. This version can be shared with a sighted audience or embedded into other content, such as talk slides or blog articles on the web. Example results of tactile and color graphics using our AltCanvas are shown in Figure 6."}, {"title": "5 SYSTEM IMPLEMENTATION", "content": "AltCanvas is a web-based application employing a client-server architecture. Figure 7 illustrates the input-output pipeline, highlighting the system's two primary components: the image generation module and the image description module. The process begins when a user issues a speech command, which is then parsed to create an image prompt for the Large Language Model (LLM) image generation model. Following image creation, the image description module generates a comprehensive, accessible description of the generated image."}, {"title": "5.1 Image Generation", "content": "The user's speech input (e.g., \"create an image of a cat\") is initially transcribed and then passed along to a prompt rewriting pipeline (see Appendix section A.5). For tactile graphics generation, our base prompts align with the Braille Authority of North America (BANA) guidelines [10], emphasizing key features such as the absence of perspective, clear outlines, simplification, and elimination of unnecessary details. For other image types, our prompt focuses on limiting generation to a single object and excluding text. Once generated, the image undergoes background removal. The resulting image is then placed on the canvas. This refined prompt is passed to the GPT-40 model (parameters: n=1, style=natural, quality=hd) for image generation. Due to model limitations in infographic generation, our system currently focuses on object images."}, {"title": "5.2 Image Descriptions", "content": "After image generation, the resulting URL is input into the GPT-40 model to create a description tailored for visually impaired users. When the global canvas description mode is activated, the entire canvas is captured and processed through the GPT-4 API to generate a comprehensive description. The canvas's HTML element is captured using the html2canvas library and rendered as an image before being sent to the system for description generation [1]. Users can employ this feature during the editing process to understand the current canvas state, checking for image overlap and positioning. The prompt used for description generation is provided in the Appendix (section A.5). During location and size editing, users require more rapid system responses for efficient manipulation."}, {"title": "5.3 Image Editing", "content": "5.3.1 Size Editing. Sound frequency maps to object size - higher frequencies for larger objects, lower for smaller ones as preferred by participants. Each time the"}]}