{"title": "MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts", "authors": ["Rachel S.Y. Teo", "Tan M. Nguyen"], "abstract": "Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled scalability in deep learning. SMoE has the potential to exponentially increase in parameter count while maintaining the efficiency of the model by only activating a small subset of these parameters for a given sample. However, it has been observed that SMoE suffers from unstable training and has difficulty adapting to new distributions, leading to the model's lack of robustness to data contamination. To overcome these limitations, we first establish a connection between the dynamics of the expert representations in SMoEs and gradient descent on a multi-objective optimization problem. Leveraging our framework, we then integrate momentum into SMoE and propose a new family of SMoEs, named MomentumSMoE. We theoretically prove and numerically demonstrate that MomentumSMoE is more stable and robust than SMoE. In particular, we verify the advantages of MomentumSMoE over SMoE on a variety of practical tasks including ImageNet-1K object recognition and WikiText-103 language modeling. We demonstrate the applicability of MomentumSMoE to many types of SMoE models, including those in the Sparse MoE model for vision (V-MoE) and the Generalist Language Model (GLaM). We also show that other advanced momentum-based optimization methods, such as Adam, can be easily incorporated into the MomentumSMoE framework for designing new SMoE models with even better performance, almost negligible additional computation cost, and simple implementations. The code is publicly available at https://github.com/rachtsy/MomentumSMoE.", "sections": [{"title": "1 Introduction", "content": "Scaling up deep models has demonstrated significant potential for enhancing the model's performance on a wide range of cognitive and machine learning tasks, ranging from large language model pre-training [13, 58, 59, 30, 5, 51, 70] and vision understanding [15, 2, 3, 39, 1, 40] to reinforcement learning [6, 28] and scientific applications [66, 74]. However, increasing the model's size requires a higher computational budget, which can be often challenging to meet. As a result, Sparse Mixture of Experts (SMoE) has been recently studied as an efficient approach to effectively scale up deep models. By modularizing the network and activating only subsets of experts for each input, SMoE maintains constant computational costs while increasing model complexity. This approach enables the development of billion-parameter models and achieves significant success in various applications, including machine translation [35], image classification [61], and speech recognition [34]."}, {"title": "1.1 Sparse Mixture of Experts", "content": "A MoE replaces a component in the layer of the model, for example, a feed-forward or convolutional layer, by a set of networks termed experts. This approach largely scales up the model but increases the computational cost. A SMoE inherits the extended model capacity from MoE but preserves the computational overhead by taking advantage of conditional computation. In particular, a SMoE consists of a router and E expert networks, $u_i$, i = 1, 2, ..., E. For each input token $x_t \\in R^D$ at layer t, the SMoE's router computes the affinity scores between $x_t$ and each expert as $g_i(x_t)$, i = 1, 2, . . ., E. In practice, we often choose the router $g(x_t) = [g_1(x_t), g_2(x_t),...,g_E(x_t)] = Wx + b$, where $W \\in R^{E \\times D}$ and $b \\in R^E$. Then, a sparse gating function TopK is applied to select only K experts with the greatest affinity scores. Here, we define the TopK function as:\n\n$\\text{TopK}(g_i) := \\begin{cases}  g_i & \\text{if } g_i \\text{ is in the K largest elements of g} \\\\  -\\infty, & \\text{otherwise}.  \\end{cases}$\n\nThe outputs from K expert networks chosen by the router are then linearly combined as\n\n$x_{t+1} = x_t + \\sum_{i=1}^{E} \\text{softmax} \\left( \\text{TopK} \\left( g_i \\left( x_t \\right) \\right) \\right) u_i \\left( x_t \\right) = x_t + u \\left( x_t \\right),$ (2)\n\nwhere $\\text{softmax}(g_i) := \\exp(g_i) / \\sum_{j=1}^{E} \\exp(g_j)$. We often set K = 2, i.e., top-2 routing, as this configuration has been shown to provide the best trade-off between training efficiency and testing performance [35, 16, 76].\n\nLimitations of SMoE. Despite their remarkable success, SMoE suffers from unstable training [11, 78] and difficulty in adapting to new distributions, leading to the model's lack of robustness to data contamination [55, 75]. These limitations impede the application of SMoE to many important large-scale tasks."}, {"title": "1.2 Contribution", "content": "In this paper, we explore the role of the residual connection in SMoE and show that simple modifica- tions of this residual connection can help enhance the stability and robustness of SMoE. In particular, we develop a gradient descent (GD) analogy of the SMoE, showing that the dynamics of the expert representations in SMoE is associated with a gradient descent step toward the optimal solution of a multi-objective optimization problem. We then propose to integrate heavy-ball momentum into the dynamics of SMoE, which results in the Momentum Sparse Mixture-of-Experts (MomentumSMoE). At the core of MomentumSMoE is the use of momentum to stabilize and robustify the model. The architecture of MomentumSMoE is depicted in Fig. 1. MomentumSMoE can be extended beyond heavy-ball momentum to integrate well with other advanced momentum-accelerated methods such as AdamW [33, 42] and Robust Momentum [10]. Our contribution is three-fold:\n\n1.  We incorporate heavy-ball momentum in SMoE to improve the model's stability and robustness.\n\n2.  We theoretically prove that the spectrum of MomentumSMoE is better-structured than SMOE, leading to the model's stability enhancement.\n\n3.  We show that the design principle of MomentumSMoE can be generalized to other advanced momentum-based optimization methods, proposing AdamSMoE and Robust MomentumSMoE.\n\nOur experimental results validate that our momentum-based SMoEs improve over the baseline SMoE in terms of accuracy and robustness on a variety of practical benchmarks, including WikiText-103 language modeling and ImageNet-1K object recognition. We also empirically demonstrate that our momentum-based design framework is universally applicable to many existing SMoE models, including the Sparse MoE model for vision (V-MoE) [61] and the Generalist Language Model (GLaM) [16], just by changing a few lines of the baseline SMoE code.\n\nOrganization. We structure this paper as follows: In Section 2, we establish the connection between SMoE and gradient descent and derive our MomentumSMoE. In Section 3, we theoretically prove the stability advantage of MomentumSMoE over SMoE. In Section 4, we introduce AdamSMoE and Robust MomentumSMoE. In Section 5, we present our experimental results to justify the advantages of our momentum-based SMoE models over the traditional SMoE and other SMoE baselines. In Section 6, we empirically analyze our MomentumSMoE. We discuss related works in Section 7. The paper ends with concluding remarks. More experimental details are provided in the Appendix."}, {"title": "2 Momentum Sparse Mixture of Experts", "content": ""}, {"title": "2.1 Background: Multiple-Gradient Descent Algorithm for Multi-objective Optimization", "content": "A multi-objective optimization problem comprises of the concurrent optimization of E objective functions, $F_i(x)$, i = 1, 2, . . ., E, which might be formulated as the following minimization problem\n\n$\\min_{x \\in D} F(x) := \\sum_{i=1}^{E} c_i F_i(x)$ (3)\n\nwhere D is the feasible region and $c_i \\in R$ are weights representing the importance of each objective function. The optimal solution to the multi-objective optimization problem above is a Pareto-optimal point such that there is no other solution that can decrease at least one of the objective functions without increasing any other objective functions. [12] shows that a necessary condition for a solution to be Pareto-optimal is for it to be Pareto-stationary, which is defined as:\n\nDefinition 1 (Pareto-stationary) Let x be in the interior of the feasible region, D, in which the E objective functions, $F_i$, are smooth, and $f_i(x) = \\nabla_x F_i(x)$ be the local gradients for i = 1, . . ., E. x is said to be Pareto-stationary if there exists a vector $a = [\\alpha_1,...,\\alpha_E] \\in R^E$ such that $\\alpha_i \\geq 0, \\sum_{i=1}^{E} \\alpha_i = 1$ and $ \\sum_{i=1}^{E} a_i f_i(x) = 0$. That is, there exists a convex combination of the gradient-vectors $f_i(x)$ that is equal to 0.\n\nTherefore, it would be intuitive to extend the steepest descent algorithm to a multi-objective setting by finding a descent direction, that is common to all objectives, in the convex hull of the normalized local gradients $\\bar{f}_i(x) = \\frac{f_i(x)}{|| f_i(x)||}$. We denote such a set as $U = \\{v \\in R^N|v = \\sum_{i=1}^{E} a_i \\bar{f}_i(x); a_i \\geq 0, \\forall i; \\sum_{i=1}^{E} a_i = 1\\}$. Indeed, [12] developed the Multiple-Gradient Descent Algorithm (MGDA) from such an understanding, proving that there does exist such a descent direction in U, which is the direction with the smallest norm in the set. Then, the update rule of MGDA is\n\n$x_{t+1} = x_t - \\gamma \\sum_{i=1}^{E} a_i^* f_i(x_t)$ (4)\n\nwhere $a^* = (a_1, ..., a_E)$ minimizes $\\{||v|| | v \\in U\\}$."}, {"title": "2.2 Background: Momentum Acceleration for Gradient-Based Optimization", "content": "Among the simplest learning algorithms is gradient descent, also termed the steepest descent method. It typically starts with an objective function F(x) whose minima we aim to find by modifying our iterate $x_t$ at each time step t through its gradient $f(x_t) = \\nabla_x F(x_t)$, scaled by a step size $\\gamma > 0$:\n\n$x_{t+1} = x_t - \\gamma \\nabla_x F(x_t) = x_t - \\gamma f(x_t)$. (5)\n\nHowever, following this update rule might result in slow convergence. A classical acceleration method to speed up the steepest descent, known as the heavy-ball method [53, 67], includes a momentum term in the algorithm. This takes the form of\n\n$p_t = - f(x_t) + \\mu p_{t-1}; x_{t+1} = x_t + \\gamma p_t,$ (6)"}, {"title": "2.3 (S)MoE as (Stochastic) Gradient Descent on Multi-objective Optimization and MomentumSMoE", "content": "We will now consider the SMoE model from the multi-objective optimization perspective.\n\nMoE as Gradient Descent. In viewing each expert in an SMoE as specializing in optimizing an objective function, we are going to establish a connection between MoE and GD, and further leverage momentum to enhance MoE and SMOE. We rewrite Eqn. 2 of MoE as follows:\n\n$x_{t+1} = x - \\sum_{i=1}^{E} \\text{softmax}(g_i(x_t)) [-\\nabla F_i(x_t)].$ (8)\n\nIf we regard $-\\nabla F_i(x_t)$ as the local \u201cgradient\u201d $\\nabla F_i(x_t)$ at the t-th iteration and $\\text{softmax}(g_i(x_t))$ as to be learned to approximate $\\alpha_i^*$, then we can consider the MoE in Eqn. 2 and 8 as the dynamical system which updates $x_t$ using the MGDA to minimize the multi-objective optimization in Eqn. 3.\n\nSMoE as Stochastic Gradient Descent. Given the analogy between MoE and GD, SMOE can be interpreted as a stochastic version of MoE, which corresponds to an SGD algorithm applied to the multi-objective optimization problem in Eqn. 3. SMoE is then reformulated as:\n\n$x_{t+1} = x - \\sum_{i=1}^{K} \\text{softmax}(\\text{TopK}(g_i(x_t))) [-u_i(x_t)] = x_t - \\gamma \\bar{f}(x_t)$. (9)\n\nHere, $-\\bar{f}(x_t) = \\gamma \\sum_{i=1}^{K} \\text{softmax}(\\text{TopK}(g_i(x_t)))u_i(x_t)$ is the SMoE output."}, {"title": "2.4 MomentumSMoE", "content": "We propose the new MomentumSMoE layer, depicted in Fig. 1, to accelerate the dynamics of 8, which is principled by the accelerated gradient descent theory (see Section 2.2):\n\n$p_t = -f(x_t) + \\mu p_{t-1}; x_{t+1} = x_t + \\gamma p_t,$ (10)\n\nwhere $\\mu > 0$ and $\\gamma > 0$ are hyperparameters corresponding to the momentum coefficient and step size in the momentum-accelerated GD, respectively. The formulation of MomentumSMoE can be applied to MoE to derive the MomentumMoE."}, {"title": "3 Stability Analysis: MomentumSMoE vs. SMoE", "content": "In this section, we demonstrate the theoretical advantages of MomentumSMoE over SMOE. In particular, we show that the spectrum of MomentumSMoE is better-structured than that of SMOE, thus MomentumSMoE is more stable than SMOE. We rewrite MomentumSMoE using the equivalent form of momentum acceleration given in Eqn. 7 as follows:\n\n$x_{t+1} = x_t - \\gamma f(x_t) + \\mu (x_t - x_{t-1})$. (11)\n\nTaking inspiration from [57], we then expand f(xt) around the Pareto-stationary solution $x^*$ at which f(x*) = 0 (see Definition 1) using Taylor expansion to obtain an approximation of f(xt):\n\n$f(x_t) \\approx f(x^*) + \\nabla_x f(x^*)(x_t - x^*) = \\nabla_x f(x^*)(x_t - x^*)$. (12)\n\nSubstituting the Taylor expansion of f(xt) in Eqn. 12 into Eqn. 11, we attain\n\n$x_{t+1} = x_t - \\gamma \\nabla_x f(x^*)(x_t - x^*) + \\mu (x_t - x_{t-1})$.\n\nWithout loss of generality, we further let $x^* = 0$ as we can always replace $x_t$ with $x_t + x^*$. The formula of MomentumSMoE can then be simplified as\n\n$x_{t+1} = x_t - \\gamma \\nabla_x f(x^*)x_t + \\mu (x_t - x_{t-1})$. (13)\n\nSuppose that $ \\nabla_x f(x^*)$ does not have any defective eigenvalues and hence is diagonalizable. Then, $ \\nabla_x f(x^*) = Q \\Sigma Q^{-1}$, for some invertible matrix Q and the diagonal matrix $\\Sigma$ with diagonal entries being the eigenvalues $\\sigma(n)$, n = 1, 2, . . ., N, of $ \\nabla_x f(x^*)$. We can then rewrite Eqn. 13 as\n\n$x_{t+1} = x_t - \\gamma \\Sigma x_t + \\mu (x_t - x_{t-1})$. (14)\n\nSince we have decoupled the N features in $x_t$, we can consider each feature $x_t(n)$, separately. Introducing a dummy equation $x_t = x_t$, we rewrite Eqn. 14 as follows:\n\n$\\begin{pmatrix} x_{t-1}(n) \\\\ x_t(n) \\\\ x_{t+1}(n) \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ \\mu & (1+\\mu)-\\gamma \\sigma(n) & -\\mu \\end{pmatrix} \\begin{pmatrix} x_{t-2}(n) \\\\ x_{t-1}(n) \\\\ x_t(n) \\end{pmatrix} = A \\begin{pmatrix} x_{t-1}(n) \\\\ x_t(n) \\end{pmatrix}.$ (15)\n\nThe convergence of $x_t (n)$ then depends on the eigenvalues $ \\lambda_1 (A)$ and $ \\lambda_2(A)$ of A. In particular, we require $\\max\\{|\\lambda_1(A)|, |\\lambda_2(A)|\\} < 1$. It should be noted that omitting the momentum parameter, i.e., $\\mu = 0$, recovers the standard, unaccelerated SMoE layer."}, {"title": "4 Beyond Heavy-ball Momentum: AdamSMoE and Robust MomentumSMoE", "content": "In addition to heavy-ball momentum, there are several advanced momentum-based algorithms in optimization that can be utilized for SMoE design. In this subsection, we propose two additional variants of MomentumSMoE, AdamSMoE and Robust MomentumSMoE, which are derived from the AdamW [33, 42] and Robust Momentum [10], respectively."}, {"title": "4.1 Adam Sparse Mixture of Experts (AdamSMoE)", "content": "Adam [33] accelerates the gradient dynamics by utilizing the moving average of historical gradients and element-wise squared gradients. Adam with a decoupled weight decay regularization (AdamW) is more commonly used in practice thanks to its better generalization over Adam. We employ AdamW to derive the AdamSMoE as follows:\n\n$\\begin{aligned}  p_t &= \\mu p_{t-1} + (1 - \\mu)[-f(x_t)];  \\\\  x_{t+1} &= x_t + \\frac{p_t}{\\sqrt{m_t} + \\epsilon}  \\\\  m_t &= \\beta m_{t-1} + (1 - \\beta) f (x_t) \\odot f (x_t) - \\kappa x_t  \\end{aligned}$\n\nwhere $\\epsilon$ is a small constant to prevent numerical instability, $\\kappa$ the weight decay parameter, $\\gamma$ the step size, and $\\mu$ and $\\beta$ are the decay parameters for the moment estimates."}, {"title": "4.2 Robust Momentum Sparse Mixture of Experts (Robust MomentumSMoE)", "content": "Deep learning models, including SMoE, are known to not be robust to distribution shifts and data distortions [60, 19, 14]. Utilizing the connection between (S)MoE and (S)GD in Section 2.3, we develop the new Robust MomentumSMoE from the Robust Momentum Method [10].\n\nThe Robust Momentum Method proposed by [10] has the following update rule\n\n$y_t = x_t + \\alpha (x_t - x_{t-1}); x_{t+1} = x_t - \\gamma f(y_t) + \\mu(x_t - x_{t-1}),$ (16)\n\nwhere $\\gamma$, $\\mu$ and $\\alpha$ are parameterized by an additional hyperparameter p as follows:\n\n$\\gamma = \\frac{k(1 - p)^2(1 + p)}{L}; \\mu = \\frac{kp^3}{k-1}; \\alpha = \\frac{p^3}{(k - 1)(1 - p)^2(1+p)}.$ (17)\n\nHere, $k = L/m$ is a condition ratio of the objective function assuming that it is m-strongly convex and L-smooth. Compared with the heavy-ball momentum in Eqn. 7, there is an additional variable $y_t$ that can be interpreted as a feedback signal to steer the $x_t$ toward a robust solution. The parameters $\\gamma$, $\\mu$, and $\\alpha$ are designed such that the new system is robust."}, {"title": "5 Experimental Results", "content": "In this section, we numerically justify the advantages of our momentum-based SMoE over the baseline SMOE on both WikiText-103 language modeling and ImageNet-1k object recognition tasks. We aim to show that: (i) MomentumSMoE improves model performance across both language and vision tasks; (ii) AdamSMoE significantly outperforms the baseline and accelerates convergence in language models, even surpassing MomentumSMoE; (iii) Robust MomentumSMoE is highly effective at improving robustness of vision models to data corruption; (iv) MomentumSMoE is universal and can be easily integrated into many state-of-the-art SMoE and MoE models.\n\nThroughout the experiments, we compare our momentum-based SMoE with the baseline SMoE of the same configuration, replacing SMoE layers with our momentum-based SMoE. For Adam-based SMOE models, we use AdamSMoE in the first layer of the model and MomentumSMoE for the subsequent layers. We provide an explanation for this implementation in Appendix D.1. We find that implementing AdamSMoE in the first layer is enough to significantly improve the model's overall performance and accelerate its convergence. Our results are averaged over 5 runs. Details on datasets, models, and training are provided in Appendix D, along with Table 4 summarizing the momentum methods implemented on SMoE/MoE models for different tasks in our experiments. More results can be found in Appendix E. All experiments are conducted on a server with 8 A100 GPUs."}, {"title": "5.1 WikiText-103 Language Modeling", "content": "We use the Switch Transformer [18], referred to as SMoE in our tables and figures below, and GLaM [16] baselines. We consider 2 configurations: medium (6 layers) and large (12 layers). We report the perplexity (PPL) of MomentumSMoE and AdamSMoE in comparison with the baseline SMoE on word-level WikiText-103 validation and test datasets for both model sizes in Table 1. We also include experiments on the medium-sized GLaM. A lower PPL indicates a better performance of the model. To further demonstrate the robustness of our method, we test the models on word swap attacked WikiText-103 data and present their results. Across all metrics, AdamSMoE and AdamGLaM outperform the baseline by a significant margin, verifying the strength of our method. Additionally, in Figure 4(Left), we provide the training and validation PPL during the first 5 training epochs of MomentumSMoE and AdamSMoE compared to the baseline SMoE to illustrate the accelerated convergence of our momentum-based models."}, {"title": "5.2 ImageNet-1K Object Recognition Task", "content": "In this section, we investigate our momentum-based models on two popular vision models, Vision MoE (V-MoE) [61] and Soft MoE [56] on the ImagetNet-1K (IN-1K) object recognition task. We focus on i) improving the robustness of V-MoE using Robust MomentumSMoE (18) and ii) demonstrating that our momentum method is not limited to sparse models but can be generalized to MoE models such as Soft MoE. To benchmark robustness to data corruptions, we use the standard datasets, ImageNet-R (IN-R) [24], ImageNet-A (IN-A) [26], and ImageNet-C (IN-C) [25].\n\nVision Mixture of Experts (V-MoE). We use a V-MoE (small) model as the baseline. This V-MoE consists of 8 Vision Transformer (ViT) blocks [15] with every odd block's MLP being replaced by a SMoE layer. In Table 2, we provide the top-1 accuracy (%) on the training and validation set of IN-1K, IN-R, IN-A, and IN-C, as well as the mean Corruption Error (mCE) for IN-C. While MomentumV-MoE and Robust MomentumV-MoE have marginal gains on clean IN-1K data, we see significant improvement on IN-R, IN-A, and IN-C with at least a 1% increase in accuracy across these metrics. Specifically, Robust MomentumV-MoE has an almost 2% increase and 2 mCE decrease on IN-C, justifying the advantage of our method. Furthermore, we visualize the top-1 accuracy and mCE across increasing severity of two corruption types in Fig. 14 in Appendix E.3 to illustrate the increasing effectiveness of our method with escalating data corruption. The results of Robust MomentumSMoE on WikiText-103 can also be found in Appendix E.4, Table 9.\n\nSoft Mixture of Experts (Soft MoE). We use the Soft MoE-tiny with 12 layers. The first 6 layers consist of standard ViT blocks, and the last 6 layers replace the MLP in those blocks with a Soft MoE layer. We train a Momentum-Soft MoE and a baseline Soft MoE model on ImageNet-1K and present their results in Table 3. In addition, we plot the training loss and top-1 accuracy of both models for 120 training epochs in Fig. 4(Right). Notably, there is a considerable increase in the accuracy of Momentum-Soft MoE over the baseline, as well as a clear acceleration to a good solution during"}, {"title": "6 Empirical Analysis", "content": "We conduct empirical analysis based on the SMoE-medium trained on WikiText-103 in Section 5.1.\n\nNorm-based Load Imbalance. Load imbalance in SMoE occurs when only a small subset of experts are consistently selected [63, 77]. Numerous methods have been developed to counter this common phenomenon, such as introducing a buffer capacity for each expert and a load balancing loss [65, 18]. Orthogonal to these, in line with our GD framework for SMoEs, we examine the choice of experts determined by the size of the norm of their outputs, $|| f_i(x)||$.\n\nFrom a multi-objective optimization perspective, the optimal descent direction is one that minimizes the norm in the convex hull of the normalized gradients U (see Section 2.1). If the gradients are not normalized, the minimum norm direction is then expected to be mainly influenced by the gradients with the smallest norms [12]. From our GD analogy of SMoE in Section 2.3, the gradients correspond to the experts, whose outputs are not normalized. We then visualize the proportion of times the experts in a SMoE are chosen according to their norms during inference in Fig. 5(Left, A). We exactly observe the corresponding phenomenon in the SMoE, further empirically justifying our connection between SMoE and GD. The full plots for all layers and all models are provided in Appendix C.\n\nThe direction with the smallest norms are frequently related to the objectives that have already had a substantial degree of convergence and is insufficient for a balanced minimization of the multi-objective criteria. In this light, an ideal SMoE output would have a norm-based balanced choice of experts and should translate to improved model performance. Indeed, in Section 5.1, we established the superior performance of MomentumSMoE and AdamSMoE on large-scale WikiText-103 language modeling task, and in Figure 5(Left, B, C), this directly correlates with a significantly more balanced selection of experts with respect to their norms.\n\nAblation Study on Momentum \u00b5 and Step Size \u03b3. To better understand the effects of the momentum parameter and step size on the performance of the trained MomentumSMoE models, we do an ablation"}, {"title": "7 Related Work", "content": "Sparse Mixture of Experts. SMoE has been extensively studied to enhance the training efficiency of large language models (LLMs), with various stable routing strategies proposed, including (i) allowing tokens to select the top-k experts [35, 18, 79, 11], (ii) allowing experts to select the top-k tokens [77], and (iii) globally determining expert assignment [36, 9]. Recent works have also tried to enhance the robustness of SMOE. [55] study the robustness of SMOE for ViTs [15] while [75] investigates the robustness of SMoE for CNNs. Furthemore, [37] explores the potential of SMoE for domain generalization, and [22] employs SMoE for robust multi-task learning. Various works have also focused on addressing load imbalance in SMoE, including [63, 77, 7]. Our momentum-based SMOE can be easily incorporated into these methods above to further improve their performance.\n\nDeep Learning Models with Momentum. Momentum has been utilized in the design of deep neural network (DNN) architectures [72, 38]. [23] applies momentum to create large and consistent dictionaries for unsupervised learning using a contrastive loss, with a momentum-based moving average of the queue encoder at the core of this approach. Many DNN-based methods for sparse coding have been designed by unfolding classical optimization algorithms, such as FISTA [4], where momentum is used in the underlying optimizer [45]. In addition, [38] introduces momentum into ResNet and DenseNet, [73, 49] integrate momentum into neural differential equations, [52] incorporates momentum into transformers, and [50] designs RNNs using momentum-accelerated first-order optimization algorithms."}, {"title": "8 Concluding Remarks", "content": "In this paper, we propose MomentumSMoE, a new class of SMoE that utilizes heavy-ball momentum to stabilize and robustify SMoE. We theoretically justify the stability of our MomentumSMoE models compared to the SMoE baseline. Furthermore, we demonstrate that our momentum-based design framework for SMoE is universal and can incorporate advanced momentum-based optimization methods, including AdamW [33, 42] and Robust Momentum [10], into many existing SMoE models. We empirically validate the advantage of our momentum-based SMoE over the standard SMOE baseline on WikiText-103 and ImageNet-1K. As shown in Table 7 in Appendix E.2, momentum has no positive effect on the small SMoE of only 3 layers but attains an increasing improvement with the medium and large models of 6 and 12 layers, respectively. This is expected as each layer represents an iteration of GD. A limitation of MomentumSMoE is that while beneficial for larger models, for models that have few layers, MomentumSMoE has little effect. From a theoretical perspective, it would be intriguing to develop a theory to explain the enhanced robustness of MomentumSMoE. Furthermore, MomentumSMoE can be analyzed as a fixed-point iteration. We leave these theoretical developments as future work."}]}