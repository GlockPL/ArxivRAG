{"title": "PENDRAM: Enabling High-Performance and Energy-Efficient Processing of Deep Neural Networks through a Generalized DRAM Data Mapping Policy", "authors": ["Rachmad Vidya Wicaksana Putra", "Muhammad Abdullah Hanif", "Muhammad Shafique"], "abstract": "Convolutional Neural Networks (CNNs), a prominent type of Deep Neural Networks (DNNs), have emerged as a state-of-the-art solution for solving machine learning tasks. To improve the performance and energy efficiency of CNN inference, the employment of specialized hardware accelerators is prevalent. However, CNN accelerators still face performance- and energy-efficiency challenges due to high off-chip memory (DRAM) access latency and energy, which are especially crucial for latency- and energy-constrained embedded applications. Moreover, different DRAM architectures have different profiles of access latency and energy, thus making it challenging to optimize them for high performance and energy-efficient CNN accelerators. To address this, we present PENDRAM, a novel design space exploration methodology that enables high-performance and energy-efficient CNN acceleration through a generalized DRAM data mapping policy. Specifically, it explores the impact of different DRAM data mapping policies and DRAM architectures across different CNN partitioning and scheduling schemes on the DRAM access latency and energy, then identifies the pareto-optimal design choices. The experimental results show that our DRAM data mapping policy improves the energy-delay-product of DRAM accesses in the CNN accelerator over other mapping policies by up to 96%. In this manner, our PENDRAM methodology offers high-performance and energy-efficient CNN acceleration under any given DRAM architectures for diverse embedded AI applications.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the widespread use of Convolutional Neu-ral Networks (CNNs), a prominent type of Deep NeuralNetworks (DNNs), for organizing, analyzing, and inferringinformation from digital data is growing fast. The reasonis that, CNNs have achieved state-of-the-art performance insolving a wide range of Machine Learning (ML) applications,such as image classification, object segmentation, autonomousdriving, smart health-care assistance, and even financial anal-ysis [1]. Since the CNN algorithms are data- and compute-intensive, many specialized hardware (HW) accelerators havebeen proposed in the literature over the past few years toexpedite the inference process [2]\u2013[20]. These specialized HWaccelerators can achieve significantly higher performance andenergy efficiency as compared to general-purpose CPUs [21];see their typical hardware achitecture in Fig. 1. However, theystill face performance- and energy-efficiency challenges due"}, {"title": "A. The State-of-the-Art and Their Limitations", "content": "The existing works have proposed different techniques toreduce DRAM access energy by focusing on minimizing thenumber of DRAM accesses [3] [23] [26]\u2013[28]. They havesimilar key ideas: (1) defining data partitioning\u00b9 and thentransferring each partition from the off-chip DRAM to the on-chip memory/buffer in a defined schedule, and (2) maximallyreusing the data that are already in the on-chip memory/buffer.Furthermore, the state-of-the-art works [26] [23] also consideradaptive data partitioning and scheduling to minimize the num-ber of DRAM accesses, i.e., by adaptively switching the reusepriority among different data types: input activations/featuremaps (ifms), output activations/feature maps (ofms), and filterweights (wghs), across the layers of a network. Although thesestate-of-the-art works result in a reduced number of DRAMaccesses (that also leads to a reduced DRAM access energy),they have not studied the characteristics and potentials ofdifferent DRAM architectures for CNN accelerators, therebylimiting their performance and energy efficiency gains."}, {"title": "B. Motivational Case Study and Scientific Challenges", "content": "Overview: Although there are different types of commodityDRAMs (e.g., DDR3 and DDR4), they employ similar internalorganization and operations [29], thereby having similar trendsof latency-per-access and energy-per-access across differentaccess conditions (such as row buffer hit, miss, and conflict)2.In commodity DRAMs, each request is directed to a bankand it can only access a subarray at a time, despite thereare multiple subarrays-per-bank [24]. This limits the DRAMs\u2019capability to offer lower access latency and energy. To addressthis, new DRAM architectures have been proposed in theliterature. For instance, subarray-level parallelism (SALP) in abank is enabled through three variants of architectures, includ-ing SALP-1, SALP-2, and SALP-MASA\u00b3 [24]. Another workproposed tiered-latency DRAM (TL-DRAM) [25] which splitsthe long bitline in each subarray into two shorter segments(i.e., near and far segments from the sense amplifier) using anisolation transistor4.\nCase Study: To understand the characteristics of differ-ent DRAM architectures, we perform an experimental casestudy to observe the DRAM latency-per-access and energy-per-access considering different access conditions, and theresults are presented in Figure 2. Our observation results showthat SALP and TL-DRAM architectures have the potential tofurther reduce the DRAM latency-per-access and energy-per-access as compared to commodity DRAMs, since they can offerlower latency and/or energy consumption in certain conditions;\nas shown by 1, 2, and in Figure 2.\nChallenges: Results of our case study show that the DRAMlatency-per-access and energy-per-access can be further opti-mized considering the given DRAM architecture to improvethe energy efficiency of DRAM accesses for CNN accelerators.Hence, there is a need for a generalized DRAM mapping policythat can achieve maximum row buffer hits while exploiting theinternal organization of DRAM. Furthermore, to justify thatour DRAM mapping policy is applicable to different designchoices, a design space exploration (DSE) is required. ThisDSE should investigate the impact of different DRAM map-"}, {"title": "C. Our Novel Contributions", "content": "To address the above challenges, we propose PENDRAM, anovel methodology to enable high-Performance and Energy-efficient CNN accelerators through a generalized DRAM datamapping policy. It employs the following key techniques; seean overview in Figure 3.\n1) We propose a generalized DRAM data mapping policythat leads to minimum EDP of DRAM accesses, for anygiven combination of DRAM architecture, data partition-ing, and scheduling scheme in a CNN accelerator. Its keyidea is to orderly prioritizes maximizing row buffer hits,bank-level parallelism, and subarray-level parallelism in thenear segment of the subarray.\n2) We propose a DSE algorithm to find a DRAM mappingpolicy that offers minimum EDP of DRAM accesses, whileconsidering different DRAM architectures, different datapartitioning, and scheduling schemes.\n3) We propose an analytical model for estimating EDP ofDRAM mapping policies in the DSE. Here, the EDP foreacH DRAM mapping policy is estimated by multiplyingthenumber of DRAM accesses with the respective numberof cycles and energy consumption."}, {"title": "II. PRELIMINARIES", "content": "The full CNN model usually cannot be mapped at once onthe HW accelerator fabric because of the limited capacity ofon-chip buffers (i.e., about 100KB-500KB [22]), hence datapartitioning and scheduling are required to complete the CNNprocessing. A pseudo-code of a convolutional processing in aCNN accelerator is illustrated in Fig. 4. Here, the inner loops"}, {"title": "B. DRAM Fundamentals", "content": "Organization: From top-to-down hierarchy, the organiza-tion of a commodity DRAM comprises channel, rank, chip,bank, row, and column [29] [32], as shown in Fig. 5(a). Here,banks are the lowest hierarchy, which can be accessed inparallel (referred to as bank-level parallelism) [33]. Physically,\nA DRAM bank is not implemented in a monolithic designthrough a large array of cells with a single row buffer. Infact, a bank is implemented in multiple subarrays, and eachhaving its local row buffer; see Fig 5(b). Multiple subarrays ina bank share global bitlines, which connect local row buffersto a global row buffer, and a global row address decoder [24].\nOperations: A specific rank will respond to each DRAMrequest, and multiple chips in this rank can be accessed inparallel, contributing to a DRAM word. In each chip, therequest is directed to a specific bank and decoded into rowand column addresses. Here, the Activation (ACT) commandtriggers a row activation, and data from the requested row arecopied to the row buffer. Then, a read (RD) or write (WR)command can be executed to the requested column in theactivated row buffer. Here, a DRAM request may encounterdifferent possible access conditions. If the requested row is"}, {"title": "C. Subarray-Level Parallelism (SALP)-enabled DRAM", "content": "To reduce latency with minimum area and energy overheads,a recent work proposed three variants of DRAM architecturesand mechanisms that exploit SALP in a DRAM bank, calledSALP-1, SALP-2, and SALP-MASA [24], whose key ideasare the following.\n\u2022SALP-1 reduces the DRAM service time by overlapping theprecharging of one subarray with the activation of anothersubarray. To do this, re-interpretation of the existing timingconstraint for precharging, is needed.\n\u2022SALP-2 reduces the DRAM service time by overlappingthe write-recovery latency of an active subarray, with theactivation of another subarray. To do this, additional circuitsto activate two subarrays at the same time is needed.\n\u2022Multitude of Activated Subarrays (MASA) reduces theDRAM service time by activating multiple subarrays at thesame time. To do this, additional circuits to activate multiplesubarrays at the same time is needed.\nFor further details on the SALP-enabled DRAM architectures,we refer to the original studies in [24]."}, {"title": "D. Tiered-Latency DRAM (TL-DRAM)", "content": "In commodity DRAMs, many storage cells are connectedto a sense amplifier through a single bitline, for optimizingthe DRAM cost-per-bit [25]. In this manner, many cellscan be sensed with a relatively small number of sense amplifiers,but at the cost of long bitlines which lead to a high parasiticcapacitance (the dominant source of DRAM latency) [25].To achieve both low cost-per-bit and low access latency, arecent work proposed TL-DRAM architecture, which splits"}, {"title": "III. THE PENDRAM METHODOLOGY", "content": "We propose PENDRAM methodology to optimize DRAMenergy-per-access and latency-per-access for a given DRAMarchitecture, data partitioning, and scheduling scheme in CNNaccelerators. The key techniques in our PENDRAM are shownin Figure 7, and described in Section III-A to Section III-C."}, {"title": "A. The Generalized Data Mapping Policy", "content": "Results of our experimental case study in Fig. 2 show thatdifferent DRAM architectures have similar patterns in termsof latency-per-access and energy-per-access across differentaccess conditions (e.g., row buffer hits, misses, and conflicts).These patterns can be exploited to achieve efficient DRAM ac-cess latency and energy. Toward this, we propose a generalizedDRAM data mapping policy to achieve efficient DRAM accesslatency and energy for any given DRAM architecture as wellas data partitioning and scheduling scheme, thereby enablinghigh-performance and energy-efficient CNN accelerators. Itsmain idea is to orderly prioritize the data mapping that maxi-mizes DRAM row buffer hit, bank-parallelism, and subarray-level parallelism in the near segments of the subarrays over thefar segments. The pseudo-code and physical representation of"}, {"title": "B. DSE for Evaluating Different DRAM Mapping Policies", "content": "To evaluate the impact of different DRAM data mappingpolicies and the performance of our proposed mapping policyas compared to the others, we perform an extensive DSE. Anoverview of the DSE is shown in Fig. 9 and its pseudo-codealgorithm is presented in Alg. 1."}, {"title": "C. Analytical Model for EDP Estimation of DRAM Accesses", "content": "Based on our DSE, the optimization problem is formulatedto minimize the EDP of DRAM accesses for each layer of anetwork and can be stated as the following.\nObjective: \\(\\text{minimize} (EDP_l)\\)\n(1)\nThe EDP-per-layer (\\(EDP_l\\)) is obtained by multiplying thelatency-per-layer and energy-per-layer. The latency-per-layeris obtained by accumulating all latency values incurred fromthe DRAM accesses for all data tiles during the processing ofa network layer. Meanwhile, the energy-per-layer is obtainedby accumulating all energy consumption values incurred fromthe DRAM accesses for all data tiles during the processingof a network layer. Note, the DRAM access latency andenergy are calculated on the basis of accesses for each datatile as we consider the tile-based data partitioning approach.For each data tile, the number of cycles required for DRAMaccesses (\\(C_{tile}\\)) represents the DRAM access latency and canbe calculated using Eq. 2, while the DRAM access energy(\\(E_{tile}\\)) can be calculated using Eq. 3.\n\\(C_{tile} = acc_{column} C_{column} + acc_{row\\_near} C_{row\\_near}+\nacc_{row\\_far} C_{row\\_far} + acc_{subarray} C_{subarray}+ (2)\nacc_{bank} C_{bank}\\)\n\\(E_{tile} = acc_{column} E_{column} + acc_{row\\_near} E_{row\\_near}+\nacc_{row\\_far} E_{row\\_far} + acc_{subarray} E_{subarray}+ (3)\nacc_{bank} E_{bank}\\)\nHere, term \\(acc_x\\) represents the number of accesses to a dif-ferent DRAM-x. \\(C_x\\) represents the number of cycles incurredwhen accessing a different DRAM-x. \\(E_x\\) represents the accessenergy incurred when accessing a different DRAM-x. For allterms, x \u2208 {column, row_near, row_far, subarray, bank}.Note, row_near denotes row in the near segment, and row_fardenotes row in the far segment."}, {"title": "IV. EVALUATION METHODOLOGY", "content": "To evaluate our proposed PENDRAM methodology, webuilt an experimental setup as shown in Fig. 10. We em-ploy a state-of-the-art cycle-accurate DRAM simulator, Ra-mulator [30], to obtain the statistics of latency (i.e, DRAMcycle-per-access) for different DRAM access conditions (e.g.,row buffer hits, misses, conflicts, as well as subarray-level"}, {"title": "V. RESULTS AND DISCUSSION", "content": "We evaluate the impact of different DRAM mapping poli-cies on the performance of a CNN accelerator across dif-ferent DRAM architectures, data partitioning, and schedulingschemes. The experimental results are presented in Fig. 11-15"}, {"title": "A. Comparisons of Different DRAM Data Mapping Policies", "content": "Observation-1: Our proposed DRAM data mapping policy(i.e., Mapping-3) achieves the lowest EDP across differentlayers of the network, different DRAM architectures, differentscheduling schemes, and different networks. It indicates thatour proposed mapping is the most effective DRAM datamapping policy as it always achieves the smallest EDP foreach layer of networks across different possible design set-tings, thereby meeting the optimization objective describedin Section III-C. According to Table I, our mapping policy(Mapping-3) orderly prioritizes mapping the data to (1) dif-ferent columns in the same row, which leads to row bufferhits in DDR3, SALPs, and TL-DRAM; (2) different banks inthe same chip, which exploits bank-level parallelism in DDR3,SALPs, and TL-DRAM; (3) different subarrays in the samebank with priority mapping to the near segment rows, whichexploits subarray-level parallelism in SALPs and near segmentaccesses in TL-DRAM, but leads to row buffer conflicts inDDR3; and (4) different rows in the same subarray withpriority mapping to the near segment rows, which exploitsnear segment accesses in TL-DRAM but leads to row bufferconflicts in DDR3 and SALPs. Following are detailed EDPimprovements achieved by our DRAM data mapping policy(i.e., Mapping-3) as compared to other mapping policies.\n\u2022For the AlexNet, our mapping improves the EDP by up to96% in DDR3, 94% in SALP-1, 88% in SALP-2, 73% inSALP-MASA, and 96% in TL-DRAM.\n\u2022For the VGG-16, our mapping improves the EDP by up to96% in DDR3, 94% in SALP-1, 89% in SALP-2, 77% inSALP-MASA, and 96% in TL-DRAM.\n\u2022For the MobileNet, our mapping improves the EDP by upto 96% in DDR3, 94% in SALP-1, 89% in SALP-2, 79%in SALP-MASA, and 95% in TL-DRAM.\n\u2022In the SqueezeNet, our mapping improves the EDP by upto 95% in DDR3, 93% in SALP-1, 90% in SALP-2, 81%in SALP-MASA, and 95% in TL-DRAM.\n\u2022For the Sparse MobileNet, our mapping improves the EDPby up to 96% in DDR3, 94% in SALP-1, 89% in SALP-2,79% in SALP-MASA, and 95% in TL-DRAM.\nThese results prove that our proposed mapping policy is thegeneralized DRAM data mapping policy that offers the lowestEDP for different design settings. Moreover, different DRAMaccess scheduling schemes can also benefit from our DRAMdata mapping policy, so that the CNN accelerators that employdifferent scheduling schemes can optimize their DRAM accesslatency and energy.\nObservation-\u2461: Mapping-2 and Mapping-5 obtain worseEDP values across different layers of the network, differ-ent DRAM architectures, and different scheduling schemes,than other mapping policies. The reason is that, Mapping-2and Mapping-5 prioritize mapping the data across differentsubarrays in the same bank, hence exploiting subarray-levelparallelism in SALPs, but leading to row buffer conflicts inDDR3 and may lead to far segment accesses in TL-DRAM.Consequently, these mapping policies incur higher EDP valuesas compared to other mapping policies that mainly exploit row"}, {"title": "VI. CONCLUSION", "content": "We present the PENDRAM methodology which employs ageneralized DRAM data mapping policy that always offers thelowest EDP of DRAM accesses for enabling high-performanceand energy-efficient CNN accelerators. It is proven throughan extensive DSE that evaluates the EDP values of differentmapping policies across different DRAM architectures, aswell as different data partitioning and scheduling schemes.Therefore, this work enables energy-efficient CNN acceleratordesigns and improves the DRAM access latency and energyfor the existing CNN accelerators."}]}