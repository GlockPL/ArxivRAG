{"title": "PENDRAM: Enabling High-Performance and Energy-Efficient Processing of Deep Neural Networks through a Generalized DRAM Data Mapping Policy", "authors": ["Rachmad Vidya Wicaksana Putra", "Muhammad Abdullah Hanif", "Muhammad Shafique"], "abstract": "Convolutional Neural Networks (CNNs), a prominent type of Deep Neural Networks (DNNs), have emerged as a state-of-the-art solution for solving machine learning tasks. To improve the performance and energy efficiency of CNN inference, the employment of specialized hardware accelerators is prevalent. However, CNN accelerators still face performance- and energy-efficiency challenges due to high off-chip memory (DRAM) access latency and energy, which are especially crucial for latency- and energy-constrained embedded applications. Moreover, different DRAM architectures have different profiles of access latency and energy, thus making it challenging to optimize them for high performance and energy-efficient CNN accelerators. To address this, we present PENDRAM, a novel design space exploration methodology that enables high-performance and energy-efficient CNN acceleration through a generalized DRAM data mapping policy. Specifically, it explores the impact of different DRAM data mapping policies and DRAM architectures across different CNN partitioning and scheduling schemes on the DRAM access latency and energy, then identifies the pareto-optimal design choices. The experimental results show that our DRAM data mapping policy improves the energy-delay-product of DRAM accesses in the CNN accelerator over other mapping policies by up to 96%. In this manner, our PENDRAM methodology offers high-performance and energy-efficient CNN acceleration under any given DRAM architectures for diverse embedded AI applications.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the widespread use of Convolutional Neu-ral Networks (CNNs), a prominent type of Deep Neural Networks (DNNs), for organizing, analyzing, and inferring information from digital data is growing fast. The reason is that, CNNs have achieved state-of-the-art performance in solving a wide range of Machine Learning (ML) applications, such as image classification, object segmentation, autonomous driving, smart health-care assistance, and even financial anal-ysis [1]. Since the CNN algorithms are data- and compute-intensive, many specialized hardware (HW) accelerators have been proposed in the literature over the past few years to expedite the inference process [2]\u2013[20]. These specialized HW accelerators can achieve significantly higher performance and energy efficiency as compared to general-purpose CPUs [21]; see their typical hardware achitecture in Fig. 1. However, they still face performance- and energy-efficiency challenges due"}, {"title": "A. The State-of-the-Art and Their Limitations", "content": "The existing works have proposed different techniques to reduce DRAM access energy by focusing on minimizing the number of DRAM accesses [3] [23] [26]-[28]. They have similar key ideas: (1) defining data partitioning\u00b9 and then transferring each partition from the off-chip DRAM to the on-chip memory/buffer in a defined schedule, and (2) maximally reusing the data that are already in the on-chip memory/buffer. Furthermore, the state-of-the-art works [26] [23] also consider adaptive data partitioning and scheduling to minimize the num-ber of DRAM accesses, i.e., by adaptively switching the reuse priority among different data types: input activations/feature maps (ifms), output activations/feature maps (ofms), and filter weights (wghs), across the layers of a network. Although these state-of-the-art works result in a reduced number of DRAM accesses (that also leads to a reduced DRAM access energy), they have not studied the characteristics and potentials of different DRAM architectures for CNN accelerators, thereby limiting their performance and energy efficiency gains."}, {"title": "B. Motivational Case Study and Scientific Challenges", "content": "Overview: Although there are different types of commodity DRAMs (e.g., DDR3 and DDR4), they employ similar internal organization and operations [29], thereby having similar trends of latency-per-access and energy-per-access across different access conditions (such as row buffer hit, miss, and conflict)2. In commodity DRAMs, each request is directed to a bank and it can only access a subarray at a time, despite there are multiple subarrays-per-bank [24]. This limits the DRAMs' capability to offer lower access latency and energy. To address this, new DRAM architectures have been proposed in the literature. For instance, subarray-level parallelism (SALP) in a bank is enabled through three variants of architectures, includ-ing SALP-1, SALP-2, and SALP-MASA\u00b3 [24]. Another work proposed tiered-latency DRAM (TL-DRAM) [25] which splits the long bitline in each subarray into two shorter segments (i.e., near and far segments from the sense amplifier) using an isolation transistor4."}, {"title": "Challenges", "content": "Results of our case study show that the DRAM latency-per-access and energy-per-access can be further opti-mized considering the given DRAM architecture to improve the energy efficiency of DRAM accesses for CNN accelerators. Hence, there is a need for a generalized DRAM mapping policy that can achieve maximum row buffer hits while exploiting the internal organization of DRAM. Furthermore, to justify that our DRAM mapping policy is applicable to different design choices, a design space exploration (DSE) is required. This DSE should investigate the impact of different DRAM map-ping policies in different DRAM architectures with different data partitioning and scheduling schemes, to find the minimum energy-delay-product (EDP) of DRAM accesses. This EDP is a measure of the energy efficiency of a CNN accelerator. To enable this, an analytical model to estimate the EDP of different DRAM mapping policies in DSE is also needed."}, {"title": "Required", "content": "A design methodology that leverages a general-ized DRAM mapping policy to optimize DRAM energy-per-access and latency-per-access considering different DRAM architectures for CNN accelerators."}, {"title": "C. Our Novel Contributions", "content": "To address the above challenges, we propose PENDRAM, a novel methodology to enable high-Performance and Energy-efficient CNN accelerators through a generalized DRAM data mapping policy. It employs the following key techniques; see an overview in Figure 3.\n1) We propose a generalized DRAM data mapping policy that leads to minimum EDP of DRAM accesses, for any given combination of DRAM architecture, data partition-ing, and scheduling scheme in a CNN accelerator. Its key idea is to orderly prioritizes maximizing row buffer hits, bank-level parallelism, and subarray-level parallelism in the near segment of the subarray.\n2) We propose a DSE algorithm to find a DRAM mapping policy that offers minimum EDP of DRAM accesses, while considering different DRAM architectures, different data partitioning, and scheduling schemes.\n3) We propose an analytical model for estimating EDP of DRAM mapping policies in the DSE. Here, the EDP for each DRAM mapping policy is estimated by multiplying the number of DRAM accesses with the respective number of cycles and energy consumption."}, {"title": "II. PRELIMINARIES", "content": "The full CNN model usually cannot be mapped at once on the HW accelerator fabric because of the limited capacity of on-chip buffers (i.e., about 100KB-500KB [22]), hence data partitioning and scheduling are required to complete the CNN processing. A pseudo-code of a convolutional processing in a CNN accelerator is illustrated in Fig. 4. Here, the inner loops"}, {"title": "A. Data Partitioning and Scheduling for CNN Processing", "content": "The full CNN model usually cannot be mapped at once on the HW accelerator fabric because of the limited capacity of on-chip buffers (i.e., about 100KB-500KB [22]), hence data partitioning and scheduling are required to complete the CNN processing. A pseudo-code of a convolutional processing in a CNN accelerator is illustrated in Fig. 4. Here, the inner loops represent the on-chip processing. Meanwhile, the outer loops represent the scheduling of processing different partitions of data from all data types (i.e., ifms, wghs, and ofms). These data are partitioned in the form of tiles, whose sizes have to be less than or equal to the sizes of respective buffers (i.e., iB, wB, and oB). Furthermore, the sequence of the outer loops represents the order in which the tiles are accessed from DRAM to the on-chip buffer, thus reflecting the number of DRAM accesses required to process a layer of a network."}, {"title": "B. DRAM Fundamentals", "content": "Organization: From top-to-down hierarchy, the organiza-tion of a commodity DRAM comprises channel, rank, chip, bank, row, and column [29] [32], as shown in Fig. 5(a). Here, banks are the lowest hierarchy, which can be accessed in parallel (referred to as bank-level parallelism) [33]. Physically, A DRAM bank is not implemented in a monolithic design through a large array of cells with a single row buffer. In fact, a bank is implemented in multiple subarrays, and each having its local row buffer; see Fig 5(b). Multiple subarrays in a bank share global bitlines, which connect local row buffers to a global row buffer, and a global row address decoder [24].\nOperations: A specific rank will respond to each DRAM request, and multiple chips in this rank can be accessed in parallel, contributing to a DRAM word. In each chip, the request is directed to a specific bank and decoded into row and column addresses. Here, the Activation (ACT) command triggers a row activation, and data from the requested row are copied to the row buffer. Then, a read (RD) or write (WR) command can be executed to the requested column in the activated row buffer. Here, a DRAM request may encounter different possible access conditions. If the requested row is already activated, then it is a row buffer hit. If the requested row is not activated, then it is either a row buffer miss or conflict. In a row buffer miss, there is no activated row in the row buffer, hence this condition requires to activate the requested row. Meanwhile, in a row buffer conflict, there is an activated row in the row buffer, but it is not the requested row. Hence, this condition requires to close the activated row using the precharging (PRE) command, then activate the requested row using the activation (ACT) command.\nData Mapping: The default DRAM data mapping policy stores the data to different columns of the same row for maximizing row buffer hits, and to different banks of the same rank for maximizing bank-level parallelism. However, it does not exploit subarray-level parallelism and does not consider data partitioning and scheduling schemes in CNN processing, hence the default data mapping solution is sub-optimal."}, {"title": "C. Subarray-Level Parallelism (SALP)-enabled DRAM", "content": "To reduce latency with minimum area and energy overheads, a recent work proposed three variants of DRAM architectures and mechanisms that exploit SALP in a DRAM bank, called SALP-1, SALP-2, and SALP-MASA [24], whose key ideas are the following.\n\u2022 SALP-1 reduces the DRAM service time by overlapping the precharging of one subarray with the activation of another subarray. To do this, re-interpretation of the existing timing constraint for precharging, is needed.\n\u2022 SALP-2 reduces the DRAM service time by overlapping the write-recovery latency of an active subarray, with the activation of another subarray. To do this, additional circuits to activate two subarrays at the same time is needed.\n\u2022 Multitude of Activated Subarrays (MASA) reduces the DRAM service time by activating multiple subarrays at the same time. To do this, additional circuits to activate multiple subarrays at the same time is needed.\nFor further details on the SALP-enabled DRAM architectures, we refer to the original studies in [24]."}, {"title": "D. Tiered-Latency DRAM (TL-DRAM)", "content": "In commodity DRAMs, many storage cells are connected to a sense amplifier through a single bitline, for optimizing the DRAM cost-per-bit [25]. In this manner, many cells can be sensed with a relatively small number of sense amplifiers, but at the cost of long bitlines which lead to a high parasitic capacitance (the dominant source of DRAM latency) [25]. To achieve both low cost-per-bit and low access latency, a recent work proposed TL-DRAM architecture, which splits long bitline in each subarray into two shorter segments (i.e., near and far segments) using an isolation transistor [25]; see Fig. 6. Therefore, the near segment can be accessed with the latency of a short bitline, while incurring minimum area and cost-per-bit overheads. For further details on the TL-DRAM architecture, we refer to the original studies in [25]."}, {"title": "III. THE PENDRAM METHODOLOGY", "content": "We propose PENDRAM methodology to optimize DRAM energy-per-access and latency-per-access for a given DRAM architecture, data partitioning, and scheduling scheme in CNN accelerators. The key techniques in our PENDRAM are shown in Figure 7, and described in Section III-A to Section III-C."}, {"title": "A. The Generalized DRAM Data Mapping Policy", "content": "Results of our experimental case study in Fig. 2 show that different DRAM architectures have similar patterns in terms of latency-per-access and energy-per-access across different access conditions (e.g., row buffer hits, misses, and conflicts). These patterns can be exploited to achieve efficient DRAM ac-cess latency and energy. Toward this, we propose a generalized DRAM data mapping policy to achieve efficient DRAM access latency and energy for any given DRAM architecture as well as data partitioning and scheduling scheme, thereby enabling high-performance and energy-efficient CNN accelerators. Its main idea is to orderly prioritize the data mapping that maxi-mizes DRAM row buffer hit, bank-parallelism, and subarray-level parallelism in the near segments of the subarrays over the far segments. The pseudo-code and physical representation of the proposed DRAM mapping policy are shown in Fig. 8. Our DRAM data mapping policy considers tile-based partitioning, hence it can be performed for each data tile using the following steps (in each DRAM chip).\nStep-1:\n\u2022 We identify a target row that is available and closer to the sense amplifier in the target subarray and target bank, thus prioritizing rows in the near segment over the far segment.\n\u2022 Then, we map the data to different columns in the target row to maximize row buffer hits.\n\u2022 If multiple chips are available within a rank, this step can be performed in different chips in parallel for exploiting the chip-level parallelism.\n\u2022 If some data remain but the target row of the target subarray and the target bank is full, then we go to Step-2.\nStep-2:\n\u2022 We map the remaining data to a different target bank in the same chip to exploit bank-level parallelism.\n\u2022 If multiple chips are available, then this step can be per-formed in different chips in parallel.\n\u2022 Then, we follow the mapping mechanism in Step-1 to Step-2 again until all data are mapped in the target subarray across all banks.\n\u2022 If some data remain but the target row and target subarray across all banks are full, then we go to Step-3.\nStep-3:\n\u2022 We map the remaining data to a different target subarray in the target bank to exploit subarray-level parallelism.\n\u2022 If multiple chips are available, then this step can be per-formed in different chips in parallel.\n\u2022 Then, we follow Step-1 to Step-3 again until all data are mapped in the target subarray across all banks.\n\u2022 If some data remain but the target row across all subarray and all banks are full, then we go to Step-4.\nStep-4:\n\u2022 We select a different row index that is available and closer to sense amplifier from the target subarray and the target bank, as the new target row."}, {"title": "B. DSE for Evaluating Different DRAM Mapping Policies", "content": "To evaluate the impact of different DRAM data mapping policies and the performance of our proposed mapping policy as compared to the others, we perform an extensive DSE. An overview of the DSE is shown in Fig. 9 and its pseudo-code algorithm is presented in Alg. 1.\nFor each layer of a network, our DSE employs three key steps: (1) defining different sizes of data tiles and scheduling schemes, (2) defining different DRAM data mapping policies, and (3) performing exploration to find a mapping policy that offers minimum EDP. The operational flow is explained in the following points.\n\u2022 We define different sizes of data tiles for all data types (ifms, wghs, and ofms), and different scheduling schemes.\nTile sizes are defined by the step sizes in the outer loops of CNN processing in Fig. 4. The tile sizes of the ifms, wghs, and ofms have to fit in the corresponding buffers (iB, wB, and oB). Each combination of the tile sizes for all data types defines one possible partitioning, which will be considered in the DSE.\nThe scheduling schemes are defined by the sequence of the outer loops of CNN processing in Fig. 4. Here, we consider four scheduling schemes, based on the reuse pri-ority: ifms-, wghs-, ofms-, and adaptive-reuse scheduling.\n\u2713 The ifms-reuse scheduling means that ifms data type will be maximally reused when the data are available in the on-chip buffer. A similar definition is applied for wghs-reuse and ofms-reuse.\n\u2713 The adaptive-reuse scheduling means that the reuse priority changes across different layers of a network, according to which one among ifms-/wghs-/ofms-reuse that offers the minimum number of DRAM accesses.\n\u2022 We define different DRAM data mapping policies, by de-termining the different orders of mapping loops to different columns, rows (including near and far segments), subarrays, and banks in the same DRAM chip.\nFor commodity DRAMs, orders of mapping loops are the permutation of banks, rows, and columns.\nFor SALP-enabled DRAMs, orders of mapping loops are the permutation of banks, subarrays, rows, and columns.\nFor TL-DRAM, orders of mapping loops are the permu-tation of banks, subarrays, (near and far segment) rows, and columns.\n\u2022 We narrow down the design space by selecting the DRAM mapping policies that have the least frequent accesses to different rows, since it is the most expensive access in the same DRAM chip, for both latency and energy (as validated by Fig. 2). Therefore, there are six mapping policies to be explored in the DSE, as presented in Table I. Note, our DRAM data mapping policy is represented as Mapping-3.\n\u2022 We perform the DSE to find a DRAM data mapping policy that offers minimum EDP, across different combinations of DRAM architectures, data partitioning, and scheduling schemes. The outputs of DSE are the minimum EDP and the DRAM corresponding mapping policy.\nNote, the DSE incorporates the characteristics of DRAM latency and energy for determining the EDP in the final results. For each layer of a network, the EDP value is obtained by multiplying the DRAM access latency and energy consumed"}, {"title": "C. Analytical Model for EDP Estimation of DRAM Accesses", "content": "Based on our DSE, the optimization problem is formulated to minimize the EDP of DRAM accesses for each layer of a network and can be stated as the following.\nObjective: $minimize (EDP_l)$\n(1)\nThe EDP-per-layer ($EDP_l$) is obtained by multiplying the latency-per-layer and energy-per-layer. The latency-per-layer is obtained by accumulating all latency values incurred from the DRAM accesses for all data tiles during the processing of a network layer. Meanwhile, the energy-per-layer is obtained by accumulating all energy consumption values incurred from the DRAM accesses for all data tiles during the processing of a network layer. Note, the DRAM access latency and energy are calculated on the basis of accesses for each data tile as we consider the tile-based data partitioning approach. For each data tile, the number of cycles required for DRAM accesses ($C_{tile}$) represents the DRAM access latency and can be calculated using Eq. 2, while the DRAM access energy ($E_{tile}$) can be calculated using Eq. 3.\n$C_{tile} = acc_{column} C_{column} + acc_{row\\_near} C_{row\\_near} + acc_{row\\_far} C_{row\\_far} + acc_{subarray} C_{subarray} + acc_{bank} C_{bank}$\n(2)\n$E_{tile} = acc_{column} E_{column} + acc_{row\\_near} E_{row\\_near} + acc_{row\\_far} E_{row\\_far} + acc_{subarray} E_{subarray} + acc_{bank} E_{bank}$\n(3)\nHere, term $acc_x$ represents the number of accesses to a different DRAM-x. $C_x$ represents the number of cycles incurred when accessing a different DRAM-x. $E_x$ represents the access energy incurred when accessing a different DRAM-x. For all terms, x \u2208 {column, row_near, row_far, subarray, bank}. Note, row_near denotes row in the near segment, and row_far denotes row in the far segment."}, {"title": "IV. EVALUATION METHODOLOGY", "content": "To evaluate our proposed PENDRAM methodology, we built an experimental setup as shown in Fig. 10. We em-ploy a state-of-the-art cycle-accurate DRAM simulator, Ra-mulator [30], to obtain the statistics of latency (i.e, DRAM cycle-per-access) for different DRAM access conditions (e.g., row buffer hits, misses, conflicts, as well as subarray-level and bank-level parallelism) in different DRAM architectures. Meanwhile, to profile the DRAM energy-per-access, we em-ploy a real experiments-based DRAM energy simulator, VAM-PIRE [31]. Information of the DRAM energy-per-access and cycle-per-access are then leveraged for the DSE, which con-siders different DRAM mapping policies, different DRAM architectures, as well as different data partitioning and scheduling schemes to find the DRAM mapping policy that offers minimum EDP.\nIn the DSE process, we consider a state-of-the-art TPU [9]-like CNN accelerator, as illustrated in Fig. 1, with a reduced size of on-chip buffers and MAC array engine, as specified in Table II. We consider separate on-chip buffers for different data types (iB for ifms, wB for wghs, and oB for ofms). To represent different DRAM architectures, we consider DDR3, SALPS (SALP-1, SALP-2, and SALP-MASA), as well as TL-DRAM. For scheduling schemes, we use ifms-reuse, wghs-reuse, ofms-reuse, and adaptive-reuse scheduling schemes. For DRAM mapping policies, we evaluate six mapping policies presented in Table I. For the inputs, we consider AlexNet [34], VGG-16 [35], MobileNet [36], and SqueezeNet [37] for dense networks, as well as a the Sparse MobileNet (that is obtained through the AutoML for Model Compression [38] technique), while considering the ImageNet dataset."}, {"title": "V. RESULTS AND DISCUSSION", "content": "We evaluate the impact of different DRAM mapping poli-cies on the performance of a CNN accelerator across dif-ferent DRAM architectures, data partitioning, and scheduling schemes. The experimental results are presented in Fig. 11-15"}, {"title": "A. Comparisons of Different DRAM Data Mapping Policies", "content": "Observation-1: Our proposed DRAM data mapping policy (i.e., Mapping-3) achieves the lowest EDP across different layers of the network, different DRAM architectures, different scheduling schemes, and different networks. It indicates that our proposed mapping is the most effective DRAM data mapping policy as it always achieves the smallest EDP for each layer of networks across different possible design set-tings, thereby meeting the optimization objective described in Section III-C. According to Table I, our mapping policy (Mapping-3) orderly prioritizes mapping the data to (1) dif-ferent columns in the same row, which leads to row buffer hits in DDR3, SALPs, and TL-DRAM; (2) different banks in the same chip, which exploits bank-level parallelism in DDR3, SALPs, and TL-DRAM; (3) different subarrays in the same bank with priority mapping to the near segment rows, which exploits subarray-level parallelism in SALPs and near segment accesses in TL-DRAM, but leads to row buffer conflicts in DDR3; and (4) different rows in the same subarray with priority mapping to the near segment rows, which exploits near segment accesses in TL-DRAM but leads to row buffer conflicts in DDR3 and SALPs. Following are detailed EDP improvements achieved by our DRAM data mapping policy (i.e., Mapping-3) as compared to other mapping policies.\n\u2022 For the AlexNet, our mapping improves the EDP by up to 96% in DDR3, 94% in SALP-1, 88% in SALP-2, 73% in SALP-MASA, and 96% in TL-DRAM.\n\u2022 For the VGG-16, our mapping improves the EDP by up to 96% in DDR3, 94% in SALP-1, 89% in SALP-2, 77% in SALP-MASA, and 96% in TL-DRAM.\n\u2022 For the MobileNet, our mapping improves the EDP by up to 96% in DDR3, 94% in SALP-1, 89% in SALP-2, 79% in SALP-MASA, and 95% in TL-DRAM.\n\u2022 In the SqueezeNet, our mapping improves the EDP by up to 95% in DDR3, 93% in SALP-1, 90% in SALP-2, 81% in SALP-MASA, and 95% in TL-DRAM.\n\u2022 For the Sparse MobileNet, our mapping improves the EDP by up to 96% in DDR3, 94% in SALP-1, 89% in SALP-2, 79% in SALP-MASA, and 95% in TL-DRAM.\nThese results prove that our proposed mapping policy is the generalized DRAM data mapping policy that offers the lowest EDP for different design settings. Moreover, different DRAM access scheduling schemes can also benefit from our DRAM data mapping policy, so that the CNN accelerators that employ different scheduling schemes can optimize their DRAM access latency and energy.\nObservation-\u2461: Mapping-2 and Mapping-5 obtain worse EDP values across different layers of the network, differ-ent DRAM architectures, and different scheduling schemes, than other mapping policies. The reason is that, Mapping-2 and Mapping-5 prioritize mapping the data across different subarrays in the same bank, hence exploiting subarray-level parallelism in SALPs, but leading to row buffer conflicts in DDR3 and may lead to far segment accesses in TL-DRAM. Consequently, these mapping policies incur higher EDP values as compared to other mapping policies that mainly exploit row"}, {"title": "B. Comparisons of Employing Different DRAM Architectures", "content": "In general, we observe that employing the SALP architec-tures (i.e., SALP-1, SALP-2, or SALP-MASA) can improve the EDP as compared to DDR3, across different networks. For instance, if we consider an adaptive-reuse scheduling scheme, the SALP architectures achieve EDP improvements by up to 88% for AlexNet, by up to 87% for VGG-16, by up to 86% for MobileNet, by up to 81% for SqueezeNet, and by up to 85% for Sparse MobileNet. These EDP improvements are most no-table in Mapping-2 and Mapping-5 as these mapping policies prioritize mapping data across subarrays in the same bank, thus exploiting subarray-level parallelism in SALP architectures but leading to row buffer conflicts in DDR3. Likewise, employing the TL-DRAM can also improve the EDP as compared to DDR3 across different networks. For instance, if we consider an adaptive-reuse scheduling scheme, TL-DRAM achieves EDP improvements by up to 4% for all investigated networks (i.e., AlexNet, VGG-16, MobileNet, SqueezeNet, and Sparse MobileNet). These EDP improvements are most notable in Mapping-5 as this mapping policy prioritizes mapping data across subarrays in the same bank, thereby exploiting near-segment row accesses in TL-DRAM but leading to row buffer conflicts in DDR3.\nAlthough these mapping policies provide improvements in novel DRAM architectures (i.e., SALPs and TL-DRAM), their EDP values for DRAM accesses are still higher than the EDP values achieved by our DRAM data mapping policy (i.\u0435., Mapping-3) across different scheduling schemes. Therefore, these results lead to several observation points as described in the following.\n\u2022 The EDP of employing different DRAM architectures may be different because of different DRAM access latency and energy profiles."}, {"title": "VI. CONCLUSION", "content": "We present the PENDRAM methodology which employs a generalized DRAM data mapping policy that always offers the lowest EDP of DRAM accesses for enabling high-performance and energy-efficient CNN accelerators. It is proven through an extensive DSE that evaluates the EDP values of different mapping policies across different DRAM architectures, as well as different data partitioning and scheduling schemes. Therefore, this work enables energy-efficient CNN accelerator designs and improves the DRAM access latency and energy for the existing CNN accelerators."}]}