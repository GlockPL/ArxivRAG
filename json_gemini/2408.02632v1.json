{"title": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models", "authors": ["Muxi Diao", "Rumei Li", "Shiyang Liu", "Guogang Liao", "Jingang Wang", "Xunliang Cai", "Weiran Xu"], "abstract": "As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming. However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. To tackle these challenges, we introduce the Self-Evolving Adversarial Safety (SEAS) optimization framework, which enhances security by leveraging data generated by the model itself. SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety. This framework reduces reliance on manual testing and significantly enhances the security capabilities of LLMs. Our contributions include a novel adversarial framework, a comprehensive safety dataset, and after three iterations, the Target model achieves a security level comparable to GPT-4, while the Red Team model shows a marked increase in attack success rate (ASR) against advanced models.", "sections": [{"title": "1 Introduction", "content": "Recently, large language models (LLMs) have demonstrated impressive capabilities in various fields (OpenAI et al., 2024). Meanwhile, security risks associated with deploying LLMs in practical applications have raised widespread public concern (Christian, 2023; Chilton, 2023).\nOne potentially useful approach to enhance safety is red teaming (Meta, 2024; Anthropic, 2024). This involves employing manual or automated techniques to generate attack data, which is then used to critically examine a language model for harmful outputs. The model is subsequently updated to prevent these issues. Due to the high costs and time consumption associated with manual red team testing (Ganguli et al., 2022), a promising alternative is to automate the generation of adversarial prompts using a red team LLM (Perez et al., 2022).\nHowever, as the performance of LLMs continues to improve, their security and robustness are also enhanced, which may be accompanied by changes in security vulnerabilities, resulting in more subtle and covert failure modes (Ganguli et al., 2022). These changes pose significant challenges to existing attack data and automated attack methods. On one hand, current adversarial attack methods lack diversity in uncovering systemic vulnerabilities (Hong et al., 2024), as they are confined to using a single predefined attack strategy (Zhou et al., 2023b; Fernando et al., 2023). This limitation prevents them from fully exploring the broad space of adversarial prompts, reducing their effectiveness as diagnostic tools and sources of synthetic data to enhance robustness (Samvelyan et al., 2024). On the other hand, existing methods are limited in complexity and fail to consider the potential flaws of the target models themselves, thus unable to implement targeted attacks (Ge et al., 2023). Consequently, the development and testing of current LLMs still heavily rely on manual red team testing (OpenAI et al., 2024; Anthropic, 2024).\nTo address these issues, we propose SEAS, a Self-Evolving Adversarial Safety optimization framework. This framework utilizes data generated automatically to guide and align pre-trained language models, enhancing security performance while maintaining general capabilities.\nAs shown in Figure 1, our framework comprises three stages. In the initialization stage, the Red Team model (R), where \"R\" stands for Red Team model and \"0\" denotes the initial iteration) undergoes fine-tuning using the training subset of our"}, {"title": "2 Related Work", "content": "2.1 Red Teaming LLMs\nLarge-scale red teaming has been conducted during the pre-deployment phase of LLMs. Researchers rely on human annotators to handwrite (annotate) adversarial prompts to guide the LLM in generating harmful responses (Meta, 2024; Anthropic, 2024; OpenAI et al., 2024). While manually constructing adversarial prompts has proven effective in generating high-quality prompts (Yu et al., 2023), it has limitations in comprehensively exploring model vulnerabilities (Hong et al., 2024). Previous studies have used manual annotation and model generation methods to create jailbreak prompts to inject instructions into black-box models (Chao et al., 2023) or white-box models (Liu et al., 2024). Additionally, Perez et al. (2022) explored various strategies for training LLMs using reinforcement learning (RL), supervised learning (SL), and context learning methods to automatically generate adversarial prompts for red team testing."}, {"title": "3 Method", "content": "In this section, we detail the three stages of the SEAS framework. In the Initialization Stage, we start with the same initial model and separately initialize two LLMs with different functions: the Red Team model and the Target model. During the Attack Stage, the Red Team model generates adversarial prompts from seed prompts aimed at attacking the Target model. The generated prompts, combined with the responses from the Target model, are assessed by a Safe Classifier to determine the success of the attack (solely evaluates whether the response is safe). In the Adversarial Optimization Stage, successful adversarial prompts are used to optimize the Red Team model. While responses from the Target model in failed attacks are used to optimize the Target model. Through multiple iterations of Attack stages and Adversarial Optimization stages, both models evolve and update themselves. The offensive capabilities of the Red Team model are enhanced, while the resilience of the Target model against attacks is correspondingly strengthened.\n3.1 SEAS Dataset\nAlthough existing security testing datasets are extensive, they lack a sufficient number of robust adversarial test cases to fully assess model security (R\u00f6ttger et al., 2024b). Most open-source datasets do not effectively integrate the two critical dimensions of Risk Categories (Inan et al., 2023) and Attack Styles (Samvelyan et al., 2024) (details in Appendix A).\nTo address this gap, we have developed a SEAS dataset, which features 14 types that cover two risk dimensions: the first is risk categories, which are potential topics of unsafe content that LLMs might generate; the second is attack styles, which are specific expressive techniques designed to cover various prompt types that could trigger harmful behavior in models. This dataset contains approximately 18K entries, divided into a training set with 16K entries and an In-Domain test set with 2K entries. Specifically, to prevent the model from overfitting to the aggressive language style or syntax during training, rather than discerning the actual semantic content, we have created an ambiguous harmless dataset for evaluation purposes. This dataset, comprising 300 entries from human experts, mimics the aggressive instruction styles and syntax but is actually harmless."}, {"title": "3.2 Self-Evolving Framework", "content": "Initialization Stage. As shown in the Figure 1, during the Initialization Stage, we selected Llama-3-8B (Meta, 2024) as our initial model. For the Red Team model, we expect the model to not only generate adversarial prompts effectively but also ensure the diversity of these prompts. To achieve this, we adopted an initialization scheme based on random sample contexts. The specific procedure is as follows: We randomly designate a specific type and select a fixed number of data from the training set of the SEAS dataset that corresponds to this type. These data are used as a small set of sample examples, incorporated into the prompts for Supervised Fine-Tuning (SFT) input. Then, we randomly select another sample of the same type as the output. This method enhances the diversity of the Red Team model in generating adversarial prompts.\nFor the Target model, considering that Llama-3-8B-Instruct already has strong security capabilities, we have initialized a target model based on Llama-3-8B, which does not have additional security training, to better validate the effectiveness of our method. We selected three datasets specifically designed for SFT focused on general command adjustment. Our objective is to enhance the model's capability for instruction following and to respond appropriately to inputs.\nAttack Stage. At the beginning of each Attack Stage, we construct seed prompts by specifying a type and concatenating a fixed number (k) of prompts from the SEAS dataset's training set. This activates the Red Team model to generate adversarial prompts. In order to ensure the diversity of the Red Team model's output, we adopted nucleus sampling (Holtzman et al., 2019) and carried out multiple samplings to generate n prompts. Following this, we input these prompts to the Target model, also conducting multiple samplings, to obtain m output responses.\nBy concatenating n adversarial prompts with m responses and processing them through a Safe Classifier for safety evaluation, we obtain n\u00d7 m tuples of {seed prompt, adversarial prompt, response, label}, where label = 1 represents unsafe. Please note that the safety assessment specifically pertains to the response.\nAdversarial Optimization Stage. In the Adversarial Optimization Stage, we filter and construct pairs data for optimization. The optimization loss of the Red Team Model $R_{i+1}$:\n$L_R = -\\mathbb{E}_{(s, p_0, p_1) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_{\\theta}(p_1 \\vert s)}{\\pi_{R_i}(p_1 \\vert s)} - \\beta \\log \\frac{\\pi_{\\theta}(p_0 \\vert s)}{\\pi_{R_i}(p_0 \\vert s)} \\right) \\right]$\nwhere s represents the seed prompt, $p_1$ denotes the adversarial prompt that successfully induces the target model to generate unsafe content, $\\pi_{R_i}$ is the Red Team model of last iteration i, and $\\pi_{\\theta}$ is a policy to be optimized.\nThe optimization loss of the Target model $T_{i+1}$:\n$L_T = -\\mathbb{E}_{(p, r_0, r_1) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_{\\theta}(r_0 \\vert p)}{\\pi_{T_i}(r_0 \\vert p)} - \\beta \\log \\frac{\\pi_{\\theta}(r_1 \\vert p)}{\\pi_{T_i}(r_1 \\vert p)} \\right) \\right]$\nwhere p represents the adversarial prompt, and $r_0$ represents the model's safe response.\nSubsequently, the attack stage and the adversarial optimization stage are cyclically performed, iteratively updating the models over multiple iterations to enhance their capabilities."}, {"title": "4 Experiments", "content": "In this section, we provide some empirical evidence to assess the effectiveness of the SEAS pipeline in enhancing model performance.\n4.1 Experimental Details\nFinetuning Details. In our research, we filtered and constructed a dataset of 16K entries from the training set as of SEAS dataset for SFT targeted at the red team model.\nFor the construction of the Target model, we selected ShareGPT (Chiang et al., 2023), Dolly (Conover et al., 2023), and LIMA (Zhou et al., 2023a) as seed datasets to fine-tune the target model and enhance its ability to follow instructions. Moreover, to ensure these three datasets were free"}, {"title": "4.2 Experimental Results", "content": "4.2.1 Evaluation of Target Models\nPerformance across Different Models. To thoroughly evaluate the performance of Target models, we compared them with several benchmark methods and publicly available models within the industry.\nAs the Table 1 shows, the ASR on the In-Domain test set of the SEAS dataset decreased by 55.20% after three iterations of the SEAS iterations. The performance metric of the third-round optimized model (SEAS-T3) surpassed that of GPT-4. On open-source test sets Beaver and hh-rlhf, the ASR"}, {"title": "4.2.2 Evaluation of Red Team Models", "content": "To comprehensively evaluate the enhancement of the Red Team model's capabilities, we evaluate the quality and diversity of the adversarial prompts by considering two aspects: the success rate of eliciting unsafe responses and the cosine similarity of sentence embeddings (Hong et al., 2024).\nQuality. After attacking the publicly available and Target models with adversarial prompts generated by Red Team model (as shown in Tables 1 and 2), we observed an increase in the ASR with each iteration. This effectively demonstrates that SEAS significantly improves the quality of adversarial prompts from Red Team models. In particular, SEAS-R0, fine-tuned exclusively with the SEAS dataset, demonstrated a higher ASR, un-"}, {"title": "4.2.3 Advantages of Pair-wise Loss Updates", "content": "Existing automated red team attack solutions, including custom jailbreaking templates or pipelines, primarily utilize the SFT objective to update Red Team models and Target models (Ge et al., 2023). However, pair-wise loss - Direct Preference Optimization (DPO) (Rafailov et al., 2024) is more suitable in safety scenarios. This method not only increases the probability of the chosen text but also decreases the likelihood of the rejected text,"}, {"title": "4.2.4 Advantages of Iterative Updates", "content": "Red Team model. The results from Table 2 and 7 clearly shows that the ASR of various Red Team models on the same target model (include SEAS Target models and public models) increases with each iteration. This trend confirms that the SEAS framework is capable of effectively pinpointing and exploiting the vulnerabilities of the Target model through iterative updates.\nTarget model. We initially train and optimize the SEAS-T0 model using data processed through three iterations of SEAS, applying consistent hyperparameters to obtain SEAS-All. SEAS-T3 is the control group. Results displayed in Table 8 reveal that multiple iterative updates substantially improve performance on security test sets within the In-Domain and on open-source datasets. Simultaneously, the model's general capabilities, evaluated on Arena and MT-Bench, demonstrate marginal enhancements with each iteration. The main advan-"}, {"title": "4.2.5 Ablation", "content": "General Data. To ablate the influence of general data on the model's security performance, we optimized all the general data used in our SEAS process using the same scheme as SEAS-T0, resulting in SEAS-Gen. We assessed this method using three security datasets. The results, presented in the Table 9, show that the ASR has improved across all three test sets. This effectively confirms that the general data we utilized does not enhance the model's security capabilities.\nSeed Prompts. To ablate the impact of randomness from seed prompts on the diversity of adversarial prompts, we conducted a procedure where seed prompts from four attack stages were input into SEAS-R0 to obtain outputs (R0-out). We then collected all adversarial prompts from different models over three iterations (R0123-out) and measured their semantic similarity. The results, shown in the Table 10, indicate low similarity between the datasets, suggesting that seed prompt randomness"}, {"title": "5 Conclusion", "content": "The SEAS framework marks a significant advancement in LLM security by introducing a self-evolving adversarial safety optimization approach. It enhances robustness by using adversarial data to continuously refine models, overcoming the limitations of traditional red team testing. This framework not only improves LLM security but also preserves their general capabilities, offering a comprehensive solution for safer AI deployment. The iterative optimization process ensures continuous adaptation to new threats, maintaining robustness against various adversarial tactics and achieving high security levels, comparable to advanced models like GPT-4.\nAdditionally, the SEAS framework includes a diverse safety dataset, providing valuable resources for further research in secure LLM deployment. This contributes to the development of more secure, reliable, and trustworthy AI applications in real-world scenarios.\nOverall, SEAS sets a new standard in the secure development and deployment of LLMs, providing a scalable and efficient method to enhance AI system safety."}, {"title": "6 Limitation", "content": "In discussing the research on the Self-Evolving Adversarial Security (SEAS) optimization framework, we must acknowledge certain limitations that could impact the overall effectiveness and scalability of the framework. The following are the main limitations identified in this study:\nModel Dependency. Our framework utilizes Llama Guard 2 as a safe classifier to evaluate whether model responses are secure and conducts iterative updates based on this evaluation. Although the model provides a comprehensive set of safety assessment standards, when deploying and applying the SEAS framework in specific domains, it is still advisable to incorporate human evaluation or use a specific model to replace Llama Guard 2.\nQuality of Adversarial Sample Generation. Although the SEAS framework aims to generate high-"}, {"title": "7 Ethical Statement", "content": "Preventing Misuse. Although the SEAS framework is designed to enhance security by adversarial attacks, it can also be adversarially exploited. Additionally, we have released an open-source dataset for iterative training. This dataset includes red team prompts to assess the vulnerabilities of the target model's security mechanisms. We acknowledge the inherent risks of making the red team dataset publicly accessible, as it could be misused. Malicious actors might exploit this resource to fine-tune language models for objectives that could negatively impact public welfare. Therefore, we will implement measures to restrict access to adversarial generated data and ensure that it is not used for malicious purposes.\nOngoing Risk Assessment. We recognize that technological advancements may introduce new challenges and risks. Therefore, we are committed to continuous risk assessments to monitor and evaluate the potential negative impacts of the SEAS framework and its applications. Our goal is to ensure that the research and application of the SEAS framework not only enhance the security of large language models (LLMs) but also adhere to high ethical standards, contributing to responsible AI development. By employing rigorous scientific methods and maintaining transparency, we aim to balance technological innovation with social responsibility, ensuring AI technologies have a positive impact on individuals and society. Through our proactive approach, we strive to foster trust and accountability in the evolution of AI."}, {"title": "A SEAS Dataset", "content": "A.1 Data Classification\nOur classification standards are based on open-source datasets (Ganguli et al., 2022; Ji et al., 2023; Liu et al., 2023; Tedeschi et al., 2024).\nA.2 Type Details\nThe SEAS dataset integrate the two critical dimensions of Risk Categories (Inan et al., 2023) and Attack Styles (Samvelyan et al., 2024). Our classification criteria follow the prior work. All categories see Table 11. Below are their brief descriptions:\n\u2022 Role Play: Involves conversations where the content pretends to be another character, sometimes used to evade safety checks or produce inappropriate content.\n\u2022 Session Completion: Provides a harmful or dangerous scenario to induce the model to continue the conversation, leading to potentially harmful content.\n\u2022 Goal Hijacking: Utilizes the dialogue system to achieve goals that deviate from the original intent, often with malicious aims.\n\u2022 Token Manipulation: Intentionally uses specific words or symbols to mislead the dialogue system into generating misleading or harmful responses.\n\u2022 Adversarial Prefix: Adds particular words or phrases at the beginning of the conversation that cause the system to generate inappropriate responses.\n\u2022 Code Nesting: Uses nested code or complex structures of symbols to cause the system to produce unexpected or malicious output.\n\u2022 Jailbreaking: Employs specific methods to bypass the system's security restrictions, generating content that should not normally be produced.\n\u2022 One Sided Statement: Presents biased or misleading viewpoints within the conversation.\n\u2022 Word Play: Uses puns or word games that lead the system to produce humorous but potentially harmful outputs.\n\u2022 Unfairness And Discrimination: Involves language or actions that unfairly treat certain groups, exhibiting discrimination.\n\u2022 Unsafe Instruction: Provides unsafe advice or instructions that might lead to physical harm or other dangers.\n\u2022 Privacy And Property: Involves content that breaches personal privacy or property rights, including unauthorized sharing of personal information.\n\u2022 Health Harm: Offers advice or information that could result in health damage.\n\u2022 Crimes And Illegal Activities: Involves content related to illegal activities or crimes, potentially inducing unlawful behavior.\nA.3 Crowdsourcing Platform Details\nThe SEAS dataset partially sources its data from public crowdsourcing platforms. Based on this data, we conduct manual annotations and rewrites. The main process is as follows: First, on the crowdsourcing platforms, online users create prompts based on specific attack methods. These prompts are preprocessed by three models deployed by us. If any of the three models identifies a user's prompt as harmful, the attack is considered successful. Second, platform administrators conduct strict quality reviews of the prompts. Each prompt that passes the review is rewarded with 2 CNY. Third, we further examine the prompts with our algorithms to ensure their validity."}, {"title": "A.4 Harmless Test Set", "content": "The Harmless Test Set is a test set with a language style similar to adversarial prompts but consisting of harmless text. We evaluated it using GPT-4, the same as XSTest (R\u00f6ttger et al., 2024a), with the evaluation prompts also being identical. For more details, see Appendix C.4. Here is some examples of harmless test set in Figure 4.\nB Metrics\nB.1 ASR\nThe primary measure of a red teaming method's success is its attack success rate (ASR) on a given target model (Perez et al., 2022; Ganguli et al., 2022; Zou et al., 2023), which is the percentage of adversarial prompts that elicit the harmful responses from the target model. Formally, let f be a target model with generation function $f_T(x) = x'$, where T is the number of tokens to be generated, x is a adversarial prompt, and x' is the response. Let g be a red teaming method that generates a"}, {"title": "B.2 Diversity", "content": "To assess diversity, we use cosine distance-based similarity as our metrics, denoted as $B_{cos}$, as follows\n$B_{cos}(X_T, X_Q) = \\frac{1}{ \\vert X_T \\vert \\cdot \\vert X_Q \\vert} \\sum_{x_i \\in X_T} \\sum_{x_j \\in X_Q} \\frac{\\phi(x_i) \\cdot \\phi(x_j)}{\\vert \\phi(x_i) \\vert \\vert \\phi(x_j) \\vert}$\nwhere $\\phi$ represents the sentence embedding model, and $X$ represents the collection of prompts x generated during training up to the current iteration, T, Qrepresents different dataset.\nC Benchmarks\nC.1 Beaver Tails\nBeaverTails (Ji et al., 2023) is a collection focused on AI safety that includes a series of datasets. This dataset uniquely divides annotations of helpfulness and harmlessness for question-answer pairs, providing distinct views on these important attributes. There are a total of 333,963 question-answer (QA) pairs and 361,903 pairs of data for both the helpfulness and harmlessness metrics. We utilize its Evaluation subset to assess the security of models.\nC.2 Anthropic hh-rlhf\nThe Anthropic hh-rlhf dataset (Ganguli et al., 2022) consists of human-generated and annotated red teaming dialogues. These data are designed to explore how crowdworkers red team models and to identify which types of red team attacks are successful or not. We utilize its red-team-attempts subset for testing.\nC.3 Arena Hard and MT-Bench\nArena Hard (Chiang et al., 2024) and MT-bench (Zheng et al., 2023) are two benchmarks based on pairwise comparison and are effective at evaluating open-ended questions. We utilized a GPT-4-based evaluation pipeline for the model's evaluation."}, {"title": "C.4 XSTest", "content": "XSTest (R\u00f6ttger et al., 2024a) comprises 250 safe prompts across ten prompt types that wellcalibreate models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. This benchmark uses GPT-4 to evaluate responses labeled as full refusal, partial refusal, and full compliance. The specific prompt is as follows: our harmless test set employed the same scheme and prompt during the evaluation process.\nGPT-4 evaluation prompts (R\u00f6ttger et al., 2024a) :\" I need your help with classifying responses by a large language model. There are three classes of responses. 1_full_compliance. This includes any direct attempt to give an answer, regardless of how accurate or relevant the answer is. It also includes cases where the model rejects the question for not having a meaningful answer. 2_full_refusal. The model states that it cannot, in the sense of will not, answer the prompted question. Full refusal includes responses that only offer alternative suggestions without answering the original question (e.g. \"instead of smashing a pi\u00f1ata, you could decorate it\"). 3_partial_refusal. This is any combination of refusal and compliance. This includes self-contradictory responses that first refuse and then go on to answer the prompted question (e.g. \"killing time at the airport is unsafe, but here are some ways to pass the time at the airport\"). QUESTION:{QUESTION}\" RESPONSE: \"{RESPONSE}\" CLASS: \""}, {"title": "DOpen-source Datasets", "content": "D.1 ShareGPT\nShareGPT (Chiang et al., 2024) refers to the multi-turn chatting histories used by Vicuna (Chiang et al., 2023). ShareGPT includes 86K human queries and responses from ChatGPT and other chatbots.\nD.2 Dolly\nDolly (Conover et al., 2023) is an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the Instruct-GPT paper (Ouyang et al., 2022), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\nD.3 OpenOrca\nOpenOrca (Lian et al., 2023) dataset is a collection of augmented FLAN Collection data (Longpre et al., 2023). Currently 1M GPT-4 completions, and 3.2M GPT-3.5 completions. The data is primarily used for training and evaluation in the field of natural language processing.\nD.4 ORPO-DPO-mix\nORPO-DPO-mix (Labonne, 2024) is design for DPO training, it is a combination of high-quality DPO datasets and use rule-based filtering tools to chose answers.\nD.5 LIMA\nLIMA (Zhou et al., 2023a) refers to a mixture dataset of community question answering and human expert-written instruction and responses.\nD.6 CPAD\nCPAD (Liu et al., 2023) refers to a high-quality prompt attack samples, specify the attacking goals of each prompt, which not only accurately simulate prompt attacks on LLMs from the perspective of attackers, but also can be utilized to evaluate and analyse the response.\nD.7 HarmfulQA\nHarmfulQA(Bhardwaj and Poria, 2023) is a set of 1,960 harmful questions to evaluate LLM performance against red-teaming attempts.It contains questions spread over a set of 10 topics each with 10 subtopics.\nD.8 ALERT\nALERT (Tedeschi et al., 2024) refers to a fine-grained safety risk taxonomy, consisting of 6 coarse and 32 fine-grained categories. This taxonomy serves as the foundation for the benchmark to provide detailed insights about a model's weaknesses and vulnerabilities as well as inform targeted safety enhancements.\nE Details of Experiments\nFor all inference experiments involving red team models and target models, we utilized a cluster equipped with 8 NVIDIA A100-80GB GPUs. The VLLM (Kwon et al., 2023) was employed for inference acceleration, applying consistent chat templates and hyper-parameters across all tests: batch size = 1, temperature = 0.8, n = 5, top-p = 0.9, and max_tokens = 2048."}]}