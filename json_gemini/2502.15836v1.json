{"title": "Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models", "authors": ["Haokun Chen", "Sebastian Szyller", "Weilin Xu", "Nageen Himayat"], "abstract": "Large language models (LLMs) have become\nincreasingly popular. Their emergent capabili-\nties can be attributed to their massive training\ndatasets. However, these datasets often con-\ntain undesirable or inappropriate content, e.g.,\nharmful texts, personal information, and copy-\nrighted material. This has promoted research\ninto machine unlearning that aims to remove\ninformation from trained models. In particu-\nlar, approximate unlearning seeks to achieve\ninformation removal by strategically editing the\nmodel rather than complete model retraining.\nRecent work has shown that soft token attacks\n(STA) can successfully extract purportedly un-\nlearned information from LLMs, thereby expos-\ning limitations in current unlearning method-\nologies. In this work, we reveal that STAS\nare an inadequate tool for auditing unlearning.\nThrough systematic evaluation on common un-\nlearning benchmarks (Who Is Harry Potter?\nand TOFU), we demonstrate that such attacks\ncan elicit any information from the LLM, re-\ngardless of (1) the deployed unlearning algo-\nrithm, and (2) whether the queried content was\noriginally present in the training corpus. Fur-\nthermore, we show that STA with just a few\nsoft tokens (1 \u2013 10) can elicit random strings\nover 400-characters long. Thus showing that\nSTAs are too powerful, and misrepresent the\neffectiveness of the unlearning methods.\nOur work highlights the need for better evalua-\ntion baselines, and more appropriate auditing\ntools for assessing the effectiveness of unlearn-\ning in LLMs.", "sections": [{"title": "Introduction", "content": "In recent years, large language models (LLMs)\nhave undergone substantial advancements, leading\nto enhanced performance and widespread adop-\ntion. LLMs have demonstrated exceptional perfor-\nmance in various downstream tasks, such as ma-"}, {"title": "Background & Related work", "content": "Large language models (LLMs) process input text\nthrough an auto-regressive framework. Given an\ninput sequence of tokens x1:t, the model computes\nthe conditional probability distribution p(xt+1|21:t)\nover the vocabulary at each time-step. The likeli-\nhood of the sequence is given by:\n$\\log p(x_{t+1}|x_{1:t}) = \\sum_{t=1}^T \\log p(x_t|x_{1:t-1})$ (1)\nAt inference time, the tokens is generated itera-\ntively by sampling the next token Xt+1 from this\ndistribution (e.g., via greedy decoding or nucleus\nsampling (Holtzman et al., 2019)), then appending\nit to the context x1:t for the subsequent step.\nMachine unlearning is a tool for removing infor-\nmation from models. Consider a machine learning\nmodel f optimized over a training dataset Dtrain."}, {"title": "Auditing with Soft Token Attacks", "content": "An adversarial prompt xa, is an input prompt to\nthe LLM, obtained by applying the transform T(\u00b7)\nto the base prompt xp, xa = T(xp, aux) in order to\nelicit a desired completion c. T can be any function\nthat swaps, removes or adds tokens; aux denotes\nany additional needed information. However, such\narbitrary attacks are expensive to optimize\u00b9, and\ndifficult to reason about. In practice, T optimizes\nan adversarial suffix xs that is appended to xp to\nelicit c (Zou et al., 2023). Specifically, we optimize\nthe probability:\nProb = P(c|xp\u2295 Xs). (3)\nAn adversary with white-box access to the LLM,\ncan instead mount the attack in the embedding\nspace i.e. modify the soft tokens:\nProb = P(c|embed(xp) \u2295 embed(xs)). (4)\nIn this case, T uses the gradient from the LLM to\nupdate Xs.\nAn oracle auditor A, takes an unlearned model fu\nand the candidate sentences xc \u2208 Xc and outputs a\nground truth, binary decision a = {0,1} indicating\nwhether the given records was part of Dtrain of:\na = A\uff61(fu, Xc = Dforget, aux) (5)\nA is unrealistic in many scenarios; however, it\ncan be easily instantiated for exact unlearning\nwhere Ao knows the training data associated with\nf: aux = {Dretain}."}, {"title": "Evaluation", "content": "To attack unlearning and evaluate its\neffectiveness, we use two popular benchmark\ndatasets: 1) Who Is Harry Potter? (Eldan and Russi-\nnovich, 2023a), a benchmark that intends to re-\nmove information about the world of Harry Pot-\nter and the associated characters; WHP hereafter.\n2) TOFU (Maini et al., 2024a), a dataset of fic-\ntional writers that are guaranteed to be absent in\nthe LLM's training data2.\nWHP does not publish a complete dataset. For\nthat reason we use the passages included in the\nassociated Hugging Face page (Eldan and Russi-\nnovich, 2023b). Additionally, we augment it with\n20 (xp \u2192 c) pairs generated with Llama2-7b-chat-hf. These contain general trivia about the Harry\nPotter universe.\nFor TOFU, we use the 10% forget to 90% retain\nsplit provided by the authors (Maini et al., 2024b).\nFor all experiments,\nwe use Llama-2-7b-chat-hf (Touvron et al., 2023)\n(Llama2), and Llama-3-8b-instruct (Meta, 2024)\n(Llama3) downloaded from Hugging Face. We"}, {"title": "Eliciting random strings", "content": "To further show the power of STAs, we use them\nto elicit random strings. Unlike natural text, the\nchance of a random string appearing in the training\nset is negligible. Also, preceding tokens do not\ninform the selection of the next token. In the fol-\nlowing experiments, we pick characters uniformly\nat random in the range 33-126 of the ASCII ta-\nble (asciitable.com).\nTo elicit a random string, we initialize the soft\nprompt using randomly selected tokens. Unlike in\nthe WHP and TOFU experiments, there is only the\nsoft prompt. We then train the soft prompt using\nAdamW for up to 3000 iterations per soft token;\nusing learning rate 0.005, and \u1e9es = (0.9, 0.999).\nIn Figure 2, we report the longest elicited string\nfor a given number of soft tokens. We repeat the ex-\nperiment five times \u2013 e.g., the first marker implies\nthat for each of the five tested random strings of\nlength 150, we found an effective soft prompt. We\nobserve that not all initializations and seed config-\nurations succeed, in which case a run needs to be\nrestarted with a different seed. If the loss plateaus\naround 25% of the iterations, we restart the run.\nHowever, no single string was restarted more than\nten times. We experimented with learning rate\nschedulers but they did not improve the search.\nOur results show that STAs can be used to elicit\ncompletely random strings, thus undermining their\napplication for auditing unlearning. Due to lim-\nited computational resources and long run-times,\nour evaluation is limited to 10 soft tokens, and\n400-character long random strings. This does not\nprovide a bound on the longest string that can be\nelicited.\nNext, we aim to answer why eliciting strings\nis possible. Prompt-tuning (Lester et al., 2021)\nis a performance efficient fine-tuning technique in\nwhich instead of training all weights, one trains\nonly a soft prompt added to the input. STAs can be\nviewed as an extreme case of prompt-tuning, where\ninstead of training over many prompts, one trains\nan attack per each prompt. Hence, an LLM that\noutputs a completion that it was trained on is an\nexpected behavior. We urge against misinterpreting\nthe results and declaring techniques ineffective."}, {"title": "Discussion & Conclusion", "content": "Attacks such as\ngreedy coordinate gradient (Zou et al., 2023) op-\ntimize the attack prompt in the hard token space\ninstead of the soft token space. Hence, they are\nweaker at eliciting completions. On one hand, this\nmight make them more suitable for auditing un-\nlearning. On the other hand, due to their compu-\ntational requirements, they are often used to force\nonly the beginning of a harmful completion (e.g.\nSure, here's how to build a bomb...) with the hope\nthat the LLM follows. It is unclear whether this\nwould be sufficient to produce specific unlearned\npassages. We see it as an interesting direction for\nfuture work.\nPrior work (Zhang et al., 2024d) hinted that un-\nlearning and preventing harmful outputs can be\nviewed as the same task \u2013 removing or suppress-\ning particular information. STAs and fine-tuning\nattacks (Hu et al., 2024) are useful tools for eval-\nuating LLMs in powerful threat models. It was\nshown that fine-tuning on benign data, or data un-\nrelated to the unlearning records (for jail-breaking\nand unlearning respectively) can restore undesir-\nable behavior (\u0141ucki et al., 2024)."}, {"title": "Conclusion", "content": "In this work, we show that soft token attacks\n(STAs) cannot reliably distinguish between base,\nfine-tuned, and unlearned models. In all cases, the\nauditor can elicit all unlearned information by ap-\npending optimized soft prompts to the base prompt.\nAdditionally, we show that STA with a single soft\ntoken can elicit 150 random characters, and over\n400 with soft tokens.\nOur work demonstrates that machine unlearn-\ning in LLMs needs better evaluation frameworks.\nWhile many unlearning methods can be broken by\nsimple paraphrasing of original prompts, or by fine-\ntuning on partial unlearned data or even unrelated\ndata, STA misrepresents their efficacy."}, {"title": "Limitations & ethical considerations", "content": "Due to computational constraints our\nwork is limited to 7-8 billion parameter models.\nNevertheless, given that LLMs' expressive power\nincreases with size (Kaplan et al., 2020), our re-\nsults should hold for larger LLMs. Our evaluation\nwith random strings could be extended to verify if"}]}