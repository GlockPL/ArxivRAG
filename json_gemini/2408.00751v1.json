{"title": "A Policy-Gradient Approach to Solving Imperfect-Information Games with Iterate Convergence", "authors": ["Mingyang Liu", "Gabriele Farina", "Asuman Ozdaglar"], "abstract": "Policy gradient methods have become a staple of any single-agent reinforcement learning toolbox, due to their combination of desirable properties: iterate convergence, efficient use of stochastic trajectory feedback, and theoretically-sound avoidance of importance sampling corrections. In multi-agent imperfect-information settings (extensive-form games), however, it is still unknown whether the same desiderata can be guaranteed while retaining theoretical guarantees. Instead, sound methods for extensive-form games rely on approximating counterfactual values (as opposed to Q values), which are incompatible with policy gradient methodologies. In this paper, we investigate whether policy gradient can be safely used in two-player zero-sum imperfect-information extensive-form games (EFGs). We establish positive results, showing for the first time that a policy gradient method leads to provable best-iterate convergence to a regularized Nash equilibrium in self-play.", "sections": [{"title": "Introduction", "content": "In recent years, deep reinforcement learning (DRL) has succeeded tremendously in many applications with large and complex environments, such as games (Mnih et al., 2013; Silver et al., 2017), autonomous driving (Kiran et al., 2021), robotics (Ibarz et al., 2021), and large language models (e.g. Ouyang et al. (2022) and ChatGPT). Much of these successes are due to the applicability of scalable algorithms, such as proximal policy optimization (PPO) (Schulman et al., 2017) and soft actor-critic (SAC) (Haarnoja et al., 2018). The success of these algorithms hinges on a few critical properties\u2014these algorithms (I) only require value estimates obtained from repeated random rollouts which can be implemented efficiently; (II) converge in iterates (as opposed to in averages), removing the need for either training an average policy approximator or storing snapshots of past policies; and (III) soundly avoid importance sampling corrections, which can be detrimental in practice as they often lead to outsized reward estimates.\nHowever, DRL is not applicable in multi-agent imperfect-information settings, such as Texas Hold'em poker, where they tend to end up trapped in cycles without making progress (Balduzzi et al., 2019). Constructing policy gradient algorithms that enjoy the same wide applicability as their DRL counterparts and yet retain theoretical guarantees in tabular settings even in imperfect-information games is a challenging, open direction of research. Current sound algorithms for competitive games typically see their scalability limited by two major obstacles: their lack of last- (or even best-) iterate convergence, and their reliance on counterfactual values. In what follows we illustrate both of these issues separately.\nAverage vs iterate convergence. In the last decade, most scalable techniques to solve Nash equilibrium strategies in two-player zero-sum imperfect-information extensive-form games (EFGs) have been based"}, {"title": "Related Work", "content": "This section compares our paper to prior literature on three aspects: convergence guarantees, notion of values used by the algorithm, and support of value estimation via rollouts. We provide a visual comparison of the most relevant algorithms in Table 1.\nConvergence Guarantee. Most CFR-based algorithms (Zinkevich et al., 2007; Tammelin et al., 2015; Steinberger et al., 2020) only guarantee that the average strategy converges to a NE, though empirically some variants of CFR exhibit last-iterate convergence (Bowling et al., 2015; Tammelin et al., 2015). Motivated by the success of Optimistic Mirror Descent (OMD) achieving last-iterate convergence in normal-form games (NFGs) (Mertikopoulos et al., 2019; Wei et al., 2021; Cai et al., 2022), Farina et al. (2019b) first empirically showed that OMD also enjoys last-iterate convergence in EFGs. Then, Lee et al. (2021) theoretically proved that Optimistic Multiplicative Weights Update (OMWU), an instance of OMD, converges in EFGs with unique NE assumption.\nUse of Q-Values. To achieve last-iterate convergence in EFGs, additional regularization and optimism are widely used. Perolat et al. (2021) used that approach in continuous time, using counterfactual values under full-information (i.e., non-sampled) feedback. Lee et al. (2021); Liu et al. (2023) achieved last-iterate convergence in EFGs using discrete-time updates, but both of their convergence results are based on counterfactual values in the non-sampled case. Sokota et al. (2023)'s MMD algorithm empirically observed convergence by using sampled (trajectory) Q-values in conjunction with entropic regularization, without theoretical guarantees. In this paper, we combine regularization and optimism, and obtain a theoretically sound algorithm (QFR) for solving two-player zero-sum EFGs using sampled Q-values / trajectory Q-values.\nRollout-based estimation. Lanctot et al. (2009) proposed Outcome-Sampling Monte-Carlo CFR (OS-MCCFR), a variant of CFR which uses random rollouts to estimate counterfactual values. Later, Farina and Sandholm (2021); Farina et al. (2021b); Bai et al. (2022), and Fiegel et al. (2023) proposed algorithms that converge in EFGs with trajectories at each iteration. However, those algorithms rely on importance sampling, which causes numerical instability due to the large range of feedback. ESCHER (McAleer et al., 2023) samples trajectories off-policy. This is usually undesirable as it favors exploring parts of the game tree according to uniform random probability, rather on focusing on those that are more likely given the policy. ARMAC (Gruslys et al., 2020) and ACH (Fu et al., 2021) support both Q-values and approximately-on-policy estimation, but like ESCHER it does not guarantee convergence of the iterates."}, {"title": "Preliminaries", "content": "For any vector $x$, we use $x_i$ as element $i$ of vector $x$ and $||x||_p$ as the $p$-norm. We let $||x||$ denote the Euclidean norm $||x||_2$. We use $$\\Delta^n$ to denote the $(n - 1)$-dimensional probability simplex $\\{x \\in [0, 1]^n : \\sum_{i=1}^n x_i = 1\\}$. We also define the Bregman divergence $D_\\psi(x, y) := \\psi(x) - \\psi(y) - (\\nabla \\psi(y), x - y)$ with respect to the $c$-strongly convex function $\\psi$. The $c$-strong convexity of $\\psi$ implies the bound $D_\\psi(x, y) \\geq \\frac{c}{2} ||x - y||^2$. For any integer $n \\geq 0$, we use $[n] := \\{1, 2, ..., n - 1, n\\}$. For any set $S$, we denote with $|S|$ as its cardinality.\nExtensive-Form Games. EFGs are played on a rooted game tree, with the set of all histories (nodes) denoted as $H$. In this paper we focus on two-player zero-sum EFGs; hence, each node belongs to exactly one player out of the set $\\{1, 2\\} \\cup \\{c\\}$. The special player $c$ is called the chance player, and models stochastic events (for example: a roll of the dice or dealing a card from a shuffled deck) sampled from a known distribution. We use $H_1, H_2, H_c$ to denote the set of nodes belonging to each of the players. Terminal nodes (nodes without children) have an associated payoff for each player (the sum across player is 0 since the game is zero-sum).\nTo model imperfect information, the set of nodes $H_i$ of each player $i \\in [2]$ is partitioned into information sets (or infosets for short) $s_1, s_2, ..., s_m$. Nodes in the same infoset are indistinguishable for the acting player of that infoset. For example, in poker player 1 cannot distinguish two nodes in the game tree that only differ on the private cards of player 2, since player 1 does not observe the hand of the opponent. We use $S_i := \\{s_1, s_2, ..., s_m\\}$ to denote the collection of all infosets of player $i$. Let $H := H_1 \\cup H_2$ and $S := S_1 \\cup S_2$ be the joint set of nodes and infosets of player 1,2 for convenience. Because nodes in the same infoset are indistinguishable from the acting player, they must all have the same action set, which we denote with $A_s$ as the action set of infoset $s \\in S$. Furthermore, $\\rho: S \\rightarrow \\{1, 2\\}$ denotes the player that an infoset $s$ belongs to.\nWe make the assumption that each player remembers all their past observations and actions; this assumption is standard and goes under the name of perfect recall. A direct corollary of this assumption is that nodes in the same infoset $s \\in S_i$ have the same past observation along the path from the root to the node in the view of player $i$. For any two nodes $h, h' \\in H$, we write $h \\subseteq h'$ if $h$ is on the path from the root of the game tree to $h'$. Suppose $h \\in s$ and for any $a \\in A_s$, we write $(h, a) \\subseteq h'$ if the path from the root of the game tree to node $h'$ includes the edge corresponding to taking action $a$ at $s$. For any two infosets $s, s' \\in S_i$ that belong to the same player $i \\in [2]$, whenever there exist two nodes $h \\in s, h' \\in s'$ such that $h \\subseteq h'$, we write $s \\subseteq s'$. Similarly, we write $(s, a) \\subseteq s'$ where $a \\in A_s$ to mean that there exist nodes $h \\in s, h' \\in s'$ such that $(h, a) \\subseteq h'$. Moreover, we can define $(s, a) \\subseteq (s', a')$ for $s, s' \\in S_i$ for some $i \\in [2]$ and $a \\in A_s, a' \\in A_{s'}$ similarly. Furthermore, for any player $i \\in [2]$, we define the parent sequence $\\omicron(s)$ of an infoset $s \\in S_i$ as the last infoset-action pair $(s', a')$, where $s' \\in S_i, a' \\in A_{s'}$, encountered along the path from root to any nodes in $s$ (the choice of node in $s$ is irrelevant and $\\omicron(s)$ is either unique or non-existing due to the perfect-recall assumption ). If there does not exist such infoset-action pair, we let $\\omicron(s) = \\emptyset$. For any node $h \\in H$, we define $\\omicron_i(h)$ as the last infoset-action pair $(s, a)$, where $s \\in S_i$,"}, {"title": "Q-Function based Regret Minimization (QFR)", "content": "In this section, we propose our policy gradient algorithm for EFGs, which we coin Q-Function based Regret Minimization (QFR). In QFR, for each player $i \\in [2]$ and state $s \\in S_i$, we enforce the strategy $\\pi_i^{(\\tau)}(\\cdot | s)$ to explore with probability $\\gamma s$ using the exploration strategy $\\nu_s$, in order to ensure that each infoset will be reached with a positive probability $\\gamma > 0$.\nThen, we show that QFR converges in best iterate to the regularized Nash equilibrium. Specifically, QFR will converge to the solution $\\mu^{(\\tau, \\gamma),*} = (\\mu_1^{(\\tau, \\gamma),*}, \\mu_2^{(\\tau, \\gamma),*})$ of the original bilinear minimax objective plus additional regularization term $\\psi^{\\Pi_1}(\\mu_1, \\mu_2)$ and $\\psi^{\\Pi_2}(\\mu_1, \\mu_2)$. \\psi^{\\Pi_1}(\\mu_1, \\mu_2)$ is strongly convex with respect to $\\mu_1$ and convex with respect to $\\mu_2$. Conversely, $\\psi^{\\Pi_2}(\\mu_1, \\mu_2)$ is strongly convex with respect to $\\mu_2$ and convex with respect to $\\mu_1$. In contrast to the original bilinear objective, optimizing the regularized objective will stabilize the training process, and result in better convergence results. Formally, the regularized and perturbed (perturb refers to the exploration) game is,\n$\\max_{\\begin{subarray}{l} \\pi_1 \\\\ \\mu_1 \\in \\Pi_1: \\\\ \\forall s \\in S_1, \\pi_1(\\cdot | s) \\in \\Delta^{|A_s|}_{\\gamma_s \\nu_s} \\end{subarray}} \\min_{\\begin{subarray}{l} \\pi_2 \\\\ \\mu_2 \\in \\Pi_2: \\\\ \\forall s \\in S_2, \\pi_2(\\cdot | s) \\in \\Delta^{|A_s|}_{\\gamma_s \\nu_s} \\end{subarray}} \\mu_1^{\\top} A \\mu_2 - \\tau \\psi^{\\Pi_1}(\\mu_1, \\mu_2) + \\tau \\psi^{\\Pi_2}(\\mu_1, \\mu_2)$$\\qquad (4.1)\nwhere $\\Delta^{|A_s|}_{\\gamma_s \\nu_s} := \\{\\mu \\in \\Delta^{|A_s|} : \\forall a \\in A_s, \\mu_a \\geq \\gamma_s \\nu_{s, a} \\}$, and $\\tau \\geq 0$ controls the magnitude of the regularizer. Note that the NE of the non-regularized game can be computed by annealing the regularization coefficient $\\tau$ by using a standard technique (Liu et al., 2023).\nTo decompose the regularizer $\\psi^{\\Pi_i}$ to each infoset for efficient update, we resort to the concept dilated regularizer (Hoda et al., 2010). Take Euclidean norm for example, unlike naively choosing $||\\ \\cdot \\||_2$ as the regularizer, dilated regularizer weights the regularizer $\\frac{1}{2} ||\\pi_i(\\cdot | s)||^2$ at each infoset $s \\in S$ by the reach probability of player $i$ to $s$, i.e. $\\sum_{s \\in S_i} \\mu_i(\\sigma(s)) ||\\pi_i(\\cdot | s)||^2$. It is shown in Hoda et al. (2010) that the dilated regularizer is strongly convex with respect to $\\mu_i$. However, dilated regularizer $\\psi_i$ only weights the reach probability of player $i$, neglecting that of the opponents, of which the asymmetry"}, {"title": "Analysis", "content": "In this section, we provide the proof sketch of Theorem 4.1. Section 5.1 introduces some necessary notions and properties for the analysis. Section 5.2 shows the convergence of QFR under full-information feedback (traversing all infosets at each iteration), and Section 5.3 generalizes to the stochastic setting in Section 4.\nPreliminaries and Basic Properties\nIn order to keep the presentation modular between Q-values and trajectory Q-values, we will assume that, at each iteration $t$, the local strategy at each infoset will be updated by taking a step in the direction"}, {"title": "Experiments", "content": "In the experiments, we apply QFR in 4-Sided Liar's Dice, Leduc Poker (Southey et al., 2005), Kuhn Poker (Kuhn, 1950), and 2 \u00d7 2 Abrupt Dark Hex. The learning rate is the same in all infosets, unlike what the theorem requires, which shows that QFR is easier to implement than what the theory suggests. Note that for MMD (Sokota et al., 2023), there is no theory for convergence when using trajectory Q-value and Q-value as feedback, while QFR has.\nIn order to pick hyperparameters, we performed a grid-search for QFR and MMD on learning rate \u03b7, regularization \u03c4, perturbation \u03b3, and the regularizer is either negative entropy or Euclidean distance."}, {"title": "Conclusions and Future Work", "content": "In this paper, we propose QFR, the first policy gradient algorithm for two-player zero-sum EFGs with iterate convergence. This is the first algorithm that can learn in EFGs with stochastic feedback while maintaining iterate-convergence. Moreover, QFR also first achieves learning in EFG without importance sampling. Albeit empirically we show that QFR works with uniform learning rates across all infosets, an interesting future direction is to prove that uniform learning rate also works for QFR. Further, whether it is possible to achieve polynomial, instead of pseudo-polynomial, dependence on the game size is unknown. We propose Lazy QFR in Appendix H with polynomial dependence on game size for Q-value and counterfactual feedback, but it requires lazy-update, which is impractical in non-tabular games. Moreover, an interesting future direction is to scale up QFR for large games."}]}