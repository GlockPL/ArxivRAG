{"title": "A Policy-Gradient Approach to Solving Imperfect-Information Games with Iterate Convergence", "authors": ["Mingyang Liu", "Gabriele Farina", "Asuman Ozdaglar"], "abstract": "Policy gradient methods have become a staple of any single-agent reinforcement learning toolbox, due to their combination of desirable properties: iterate convergence, efficient use of stochastic trajectory feedback, and theoretically-sound avoidance of importance sampling corrections. In multi-agent imperfect-information settings (extensive-form games), however, it is still unknown whether the same desiderata can be guaranteed while retaining theoretical guarantees. Instead, sound methods for extensive-form games rely on approximating counterfactual values (as opposed to Q values), which are incompatible with policy gradient methodologies. In this paper, we investigate whether policy gradient can be safely used in two-player zero-sum imperfect-information extensive-form games (EFGs). We establish positive results, showing for the first time that a policy gradient method leads to provable best-iterate convergence to a regularized Nash equilibrium in self-play.", "sections": [{"title": "Introduction", "content": "In recent years, deep reinforcement learning (DRL) has succeeded tremendously in many applications with large and complex environments, such as games (Mnih et al., 2013; Silver et al., 2017), autonomous driving (Kiran et al., 2021), robotics (Ibarz et al., 2021), and large language models (e.g. Ouyang et al. (2022) and ChatGPT). Much of these successes are due to the applicability of scalable algorithms, such as proximal policy optimization (PPO) (Schulman et al., 2017) and soft actor-critic (SAC) (Haarnoja et al., 2018). The success of these algorithms hinges on a few critical properties\u2014these algorithms (I) only require value estimates obtained from repeated random rollouts which can be implemented efficiently; (II) converge in iterates (as opposed to in averages), removing the need for either training an average policy approximator or storing snapshots of past policies; and (III) soundly avoid importance sampling corrections, which can be detrimental in practice as they often lead to outsized reward estimates.\nHowever, DRL is not applicable in multi-agent imperfect-information settings, such as Texas Hold'em poker, where they tend to end up trapped in cycles without making progress (Balduzzi et al., 2019). Constructing policy gradient algorithms that enjoy the same wide applicability as their DRL counterparts and yet retain theoretical guarantees in tabular settings even in imperfect-information games is a challenging, open direction of research. Current sound algorithms for competitive games typically see their scalability limited by two major obstacles: their lack of last- (or even best-) iterate convergence, and their reliance on counterfactual values. In what follows we illustrate both of these issues separately."}, {"title": "Related Work", "content": "This section compares our paper to prior literature on three aspects: convergence guarantees, notion of values used by the algorithm, and support of value estimation via rollouts. We provide a visual comparison of the most relevant algorithms in Table 1."}, {"title": "Preliminaries", "content": "For any vector \u00e6, we use x\u012b as element i of vector \u00e6 and $||x||_p$ as the p-norm. We let $||x||$ denote the Euclidean norm $||x||_2$. We use $\u0394^n$ to denote the (n \u2013 1)-dimensional probability simplex {$x \u2208 [0,1]^n : \u2211_{i=1}^n x_i = 1$}. We also define the Bregman divergence $D_\u03c8(x, y) := \u03c8(x) \u2013 \u03c8(y) \u2013 \\langle \u2207\u03c8(y), x - y\\rangle$ with respect to the c-strongly convex function \u03c8. The c-strong convexity of \u03c8 implies the bound $D_\u03c8(x, y) \u2265 \\frac{c}{2} ||x \u2212 y ||^2$. For any integer n \u2265 0, we use $[n] := {1, 2, ..., n \u2212 1, n}$. For any set S, we denote with |S| as its cardinality.\nEFGs are played on a rooted game tree, with the set of all histories (nodes) denoted as H. In this paper we focus on two-player zero-sum EFGs; hence, each node belongs to exactly one player out of the set {1,2} \u222a {c}. The special player c is called the chance player, and models stochastic events (for example: a roll of the dice or dealing a card from a shuffled deck) sampled from a known distribution. We use $H_1$, $H_2$, $H_c$ to denote the set of nodes belonging to each of the players. Terminal nodes (nodes without children) have an associated payoff for each player (the sum across player is 0 since the game is zero-sum).\nTo model imperfect information, the set of nodes $H_i$ of each player $i \u2208 [2]$ is partitioned into information sets (or infosets for short) $s_1, s_2, ..., s_m$. Nodes in the same infoset are indistinguishable for the acting player of that infoset. For example, in poker player 1 cannot distinguish two nodes in the game tree that only differ on the private cards of player 2, since player 1 does not observe the hand of the opponent. We use $S_i := {s_1, s_2, ..., s_m}$ to denote the collection of all infosets of player i. Let $H := H_1 \u222a H_2$ and $S := S_1 \u222a S_2$ be the joint set of nodes and infosets of player 1,2 for convenience. Because nodes in the same infoset are indistinguishable from the acting player, they must all have the same action set, which we denote with $A_s$ as the action set of infoset $s \u2208 S$. Furthermore, $\u03c1 : S \u2192 {1,2}$ denotes the player that an infoset s belongs to.\nWe make the assumption that each player remembers all their past observations and actions; this assumption is standard and goes under the name of perfect recall. A direct corollary of this assumption is that nodes in the same infoset $s \u2208 S_i$ have the same past observation along the path from the root to the node in the view of player i. For any two nodes $h,h' \u2208 H$, we write $h \u2286 h'$ if h is on the path from the root of the game tree to h'. Suppose $h \u2208 s$ and for any $a \u2208 A_s$, we write $(h, a) \u2286 h'$ if the path from the root of the game tree to node h' includes the edge corresponding to taking action a at s. For any two infosets $s,s' \u2208 S_i$ that belong to the same player $i \u2208 [2]$, whenever there exist two nodes $h \u2208 s,h' \u2208 s'$ such that $h \u2286 h'$, we write $s \u2286 s'$. Similarly, we write $(s,a) \u2286 s'$ where $a \u2208 A_s$ to mean that there exist nodes $h \u2208 s,h' \u2208 s'$ such that $(h,a) \u2286 h'$. Moreover, we can define $(s,a) \u2286 (s', a')$ for $s,s' \u2208 S_i$ for some $i \u2208 [2]$ and $a \u2208 A_s,a' \u2208 A_{s'}$ similarly. Furthermore, for any player $i \u2208 [2]$, we define the parent sequence $o(s)$ of an infoset $s \u2208 S_i$ as the last infoset-action pair $(s', a')$, where $s' \u2208 S_i, a' \u2208 A_{s'}$, encountered along the path from root to any nodes in s (the choice of node in s is irrelevant and o(s) is either unique or non-existing due to the perfect-recall assumption ). If there does not exist such infoset-action pair, we let $o(s) = \u00d8$. For any node $h \u2208 H$, we define $o_i(h)$ as the last infoset-action pair $(s,a)$, where $s \u2208 S_i$,"}, {"title": "Q-Function based Regret Minimization (QFR)", "content": "In this section, we propose our policy gradient algorithm for EFGs, which we coin Q-Function based Regret Minimization (QFR). In QFR, for each player $i \u2208 [2]$ and state $s \u2208 S_i$, we enforce the strategy $\u03c0_1^{(()}(\u00b7|s)$ to explore with probability $\u03b3\u03bd_s$ using the exploration strategy $\u03bd_s$, in order to ensure that each infoset will be reached with a positive probability $\u03b3 > 0$.\nThen, we show that QFR converges in best iterate to the regularized Nash equilibrium. Specifically, QFR will converge to the solution $\u03bc^{(\u03c4.),*}= (\u03bc^{\u03c4,\u03b3),*}, \u03bc^{(\u03c4,\u03b3),*})$ of the original bilinear minimax objective plus ad ditional regularization term $\u03c8^1(\u03bc_1, \u03bc_2)$ and $\u03c8^2(\u03bc_1, \u03bc_2)$.  $\u03c8^1(\u03bc_1, \u03bc_2)$ is strongly convex with respect to $\u03bc_1$ and convex with respect to $\u03bc_2$. Conversely, $\u03c8^2(\u03bc_1, \u03bc_2)$ is strongly convex with respect to $\u03bc_2$ and convex with respect to $\u03bc_1$. In contrast to the original bilinear objective, optimizing the regularized objective will stabilize the training process, and result in better convergence results. Formally, the regularized and perturbed (perturb refers to the exploration) game is,\n$\\max_{\\substack{\u03c0_1\\\\ \u03bc^{\\pi_1} \u2208\u03a0_1 :\\\\ \\forall s \u2208S_1, \u03c0_1(\u00b7|s) \u2208 \u0394_{A_s}}} \\min_{\\substack{\u03c0_2\\\\ \u03bc^{\\pi_2} \u2208\u03a0_2 :\\\\ \\forall s \u2208S_2, \u03c0_2(\u00b7|s) \u2208 \u0394_{A_s}}} \\mu^{\\pi_1}^T A \\mu^{\\pi_2} - \u03c4 \u03c8^1(\u03bc_1, \u03bc_2) + \u03c4 \u03c8^2(\u03bc_1, \u03bc_2)$                                              (4.1)\nwhere $A := {\u03bc \u2208 \u0394^{[A_s]} : \u2200a \u2208 A_s, \u03bc_a \u2265 \u03b3_s\u03bd_{s,a}} \u2200_{s,a}}$, and $\u03c4 \u2265 0$ controls the magnitude of the regularizer. Note that the NE of the non-regularized game can be computed by annealing the regularization coefficient \u03c4 by using a standard technique (Liu et al., 2023).\nTo decompose the regularizer $\u03bc^{\u03c0_i}$ in each infoset for efficient update, we resort to the concept dilated regularizer (Hoda et al., 2010). Take Euclidean norm for example, unlike naively choosing $|| \u03c0_i(\u00b7|s) ||^2$ as the regularizer, dilated regularizer weights the regularizer $\u03c0_i^{pi} (\u03c3(s)) ||\u03c0_i(\u00b7|s)||^2$ at each infoset $s \u2208 S$ by the reach probability of player i to s, i.e. $\\Sigma_{s\u2208S_i} \u03bc_i(\u03c3(s)) ||\u03c0_i(\u00b7|s)||^2$. It is shown in Hoda et al. (2010) that the dilated regularizer is strongly convex with respect to $\u03bc^{\u03c0_i}$. However, dilated regularizer $\u03c8^i$ only weights the reach probability of player i, neglecting that of the opponents, of which the asymmetry"}, {"title": "Analysis", "content": "In this section, we provide the proof sketch of Theorem 4.1. Section 5.1 introduces some necessary notions and properties for the analysis. Section 5.2 shows the convergence of QFR under full-information feedback (traversing all infosets at each iteration), and Section 5.3 generalizes to the stochastic setting in Section 4."}, {"title": "Preliminaries and Basic Properties", "content": "In order to keep the presentation modular between Q-values and trajectory Q-values, we will assume that, at each iteration t, the local strategy at each infoset will be updated by taking a step in the direction"}, {"title": "Convergence with Full Information Feedback", "content": "QFR runs a variant of Regularized Optimistic Mirror Descent (Reg-OMD) (Liu et al., 2023) algorithm to update the strategy in each infoset. For notational simplicity, we define the reach probability of opponents to an infoset $s \u2208 S$ as $\u03bc_{-p(s)}^{(t)}(s) := \\Sigma_{h\u2208s} \u03bc_c(h)\u03bc_{3-p(s)}^{(t)}(o_{3-p(s)}(h))$. The update rule is\n$\u03c0^{(t+1)}(\u00b7|s) = \\argmin_{\u03c0^{\u03c1(s)}(\u00b7|s) \u2208\u0394_{Us}} (\\eta \\langle \u03c0_\u03c1(s) (\u00b7 |s), - q^{(t-1)} (s,.) \\rangle + \\frac{\u03c4 \u03bc_{\u2212p(s)}^{(t-1)} (s) }{m_s^{\u22121)}} \u03c8(\u03c0_\u03c1(s) (\u00b7|s)) + \\frac{1}{\u03b7_s} D_\u03c8 (\u03c0_\u03c1(s) (\u00b7 |s), \u03c0^{(t)}_\u03c1(. |s))$\\            (5.2)\nwhere $A := {\u03bc \u2208 \u0394^{[A_s]} : \u2200a \u2208 A_s, \u03bc_a \u2265 \u03b3_s\u03bd_{s,a}} \u2200_{s,a}}$ and y is the regularizer chosen for infoset s. Here $q^{(t)}(s,.)$ can be trajectory Q-value, Q-value, and counterfactual value associated with $\u03c0^{(t)}$. When $q^{(t)}(s,.)$ is the trajectory Q-value, (5.2) is the full-information version of (4.2).\nBy analyzing the update rule (5.2), we can get the following inequality. For any $s \u2208 S$ and strategy"}, {"title": "Convergence with Stochastic Feedback", "content": "We complement the results of Section 5.2 by showing that the best-iterate convergence guaranteed by QFR is still guaranteed when only visiting a trajectory at each iteration. The proof utilizes standard concentration inequalities, incurring an additional sublinear cost caused by the noise incurred from sampling, as recalled in the following lemma."}, {"title": "Experiments", "content": "In the experiments, we apply QFR in 4-Sided Liar's Dice, Leduc Poker (Southey et al., 2005), Kuhn Poker (Kuhn, 1950), and 2 \u00d7 2 Abrupt Dark Hex. The learning rate is the same in all infosets, unlike what the theorem requires, which shows that QFR is easier to implement than what the theory suggests. Note that for MMD (Sokota et al., 2023), there is no theory for convergence when using trajectory Q-value and Q-value as feedback, while QFR has.\nIn order to pick hyperparameters, we performed a grid-search for QFR and MMD on learning rate \u03b7, regularization \u03c4, perturbation \u03b3, and the regularizer is either negative entropy or Euclidean distance."}, {"title": "Conclusions and Future Work", "content": "In this paper, we propose QFR, the first policy gradient algorithm for two-player zero-sum EFGs with iterate convergence. This is the first algorithm that can learn in EFGs with stochastic feedback while maintaining iterate-convergence. Moreover, QFR also first achieves learning in EFG without importance sampling. Albeit empirically we show that QFR works with uniform learning rates across all infosets, an interesting future direction is to prove that uniform learning rate also works for QFR. Further, whether it is possible to achieve polynomial, instead of pseudo-polynomial, dependence on the game size is unknown. We propose Lazy QFR in Appendix H with polynomial dependence on game size for Q-value and counterfactual feedback, but it requires lazy-update, which is impractical in non-tabular games. Moreover, an interesting future direction is to scale up QFR for large games."}]}