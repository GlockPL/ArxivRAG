{"title": "Random Token Fusion for Multi-View Medical Diagnosis", "authors": ["Jingyu Guo", "Christos Matsoukas", "Fredrik Strand", "Kevin Smith"], "abstract": "In multi-view medical diagnosis, deep learning-based models often fuse information from different imaging perspectives to improve diagnostic performance. However, existing approaches are prone to overfitting and rely heavily on view-specific features, which can lead to trivial solutions. In this work, we introduce Random Token Fusion (RTF), a novel technique designed to enhance multi-view medical image analysis using vision transformers. By integrating randomness into the feature fusion process during training, RTF addresses the issue of overfitting and enhances the robustness and accuracy of diagnostic models without incurring any additional cost at inference. We validate our approach on standard mammography and chest X-ray benchmark datasets. Through extensive experiments, we demonstrate that RTF consistently improves the performance of existing fusion methods, paving the way for a new generation of multi-view medical foundation models.", "sections": [{"title": "Introduction", "content": "Physicians routinely employ multi-view analysis in diagnostic procedures. Images gathered at various angles can unveil details that may be obscured in a single view, enhancing the precision of the diagnosis. It has been shown in clinical trials that a large percentage of breast cancers can only be detected when both craniocaudal (CC) and mediolateral oblique (MLO) views are analyzed [14]. Similarly, the frontal and lateral views of chest X-rays can provide unique information that is valuable to the accurate diagnosis of diseases [12, 15, 31, 18]. This is because complementary information from different views helps physicians tackle the challenges posed by superimposed tissues and complex anatomy. Each additional view provides context and offers unique insights, enriching the overall understanding of a patient's condition [18].\nGiven the clear benefits of multi-view analysis in clinical practice, it stands to reason that foundation models for medical image analysis could similarly improve their diagnostic accuracy by integrating information from multiple views. Information from different views of anatomical structures may reduce ambiguity, better explain spatial relationships, and provide additional context. Although only a handful of works have addressed the topic of multi-view fusion in neural networks, evidence suggests that there is a tangible benefit from fusing information from multiple views [5, 2, 42, 48, 38].\nThe question of how and where to mix information from different views is an open area of research, with multiple strategies having been proposed in recent years. This is not our main focus. Rather,"}, {"title": "Related Work", "content": ""}, {"title": "Multi-view Fusion in Medical Imaging", "content": "Multi-view fusion is a technique for integrating multiple input signals that typically represent the same object or class [43]. It has been used in 3D scene comprehension in the natural domain [34, 30, 11, 13], and has recently gained attention in the medical field for improving diagnostics by exploiting complementary information from multiple views taken from the same exam [26, 42].\nFor chest X-rays [15, 38] and mammography screenings [39, 36, 28, 42, 5], studies have demonstrated the utility of merging different views. Although it has been established that registration is not necessary for fusing multiple views [3, 5], simply combining modalities does not guarantee enhanced performance [15, 42]. It is observed that in some cases, one view may overshadow the other in terms of relevance, dominating the learning signal [15, 42]. As such, various approaches to multi-view fusion have been investigated.\nWang et al. [39] adopt an attention mechanism coupled with a recurrent network to merge the two views. Similarly, Iftikhar et al. [17] propose a sensitivity-based weighting mechanism to fuse the predictions on individual views. Lopez et al. [24] treat the two views as separate channels, constructing a channel-wise input, and employ convolutions to learn the correlations between them."}, {"title": "Information Mixing as Regularization", "content": "Information occlusion and mixing have been used as a regularization technique in various other contexts. Techniques such as CutOut [8] and PatchDropout [22] have shown their efficacy as regularizers by denying partial information. Methods such as [46, 45, 10] have demonstrated useful regularization effects when mixing information of different inputs. Mixup [46] and CutMix [45] mix samples at image level during training, while PatchUp [10] works in the hidden space of CNNs. For ViTs, Liu et al. [21] adapt Mixup by blending two images at the token level, guided by contextual activation maps. Similarly, TokenMixup [6] employs self-attention-based saliency to guide the mixing process towards the most significant tokens. Zhao et al. [47] further advance this approach by optimizing both token and label spaces for ViTs.\nThe aforementioned methods provide effective strategies for increasing the diversity of data. They have been applied to random image pairs of different objects and labels, although the pairs are intentionally constructed and not intended to be mixed for the specific tasks, e.g.. classification. Naturally, we believe that the idea could benefit multi-view medical diagnosis where image pairs exist naturally: They originate from the same object and serve the exact same diagnostic goals, and we want to encourage models to fully utilize both inputs. To this end, the proposed RTF randomly selects features from both views before fusion, effectively functioning as a regularizer. By augmenting the feature space, RTF encourages models to analyze cases more comprehensively, leading to improved performance."}, {"title": "Methods", "content": "Combining information from multiple views in medical image analysis enhances diagnostic accuracy, but existing methods often overfit by relying too much on the most informative view [42, 40]. \u03a4\u03bf address this, we introduce Random Token Fusion (RTF). RTF is designed to augment feature learning in multi-view vision transformers by randomly selecting tokens from both views during the fusion phase of training, forcing the model to utilize information from both views effectively (See Figure 2).\nGiven two views $X_{1,2} \\in \\mathbb{R}^{H\\times W\\times C}$, where $H \\times W$ is the spatial dimension and $C$ the number of image channels, a local encoder $f_{local}$ processes each input independently, and generates representations for both views:\n$Z_{1,2} = f_{local}(X_{1,2})$\n(1)\nwhere $Z_{1,2} \\in \\mathbb{R}^{(N+1)\\times D}$, are the local representations of $X_1$ and $X_2$, consisting of $N$ patch tokens and a class (CLS) token [9] of size $D$. Then, $Z_{1,2}$ are processed by the fusion module that divides them into two different fusion branches: the random token fusion module $RTF_{F}$ (described in 3.1) and the global fusion module $Global_F$ (described in 3.2).\nSubsequently, the fused outputs are independently forwarded to a global encoder $f_{eglobal}$ that produces the final predictions for both the global and RTF tokens. The overall process for the two branches is described as follows:\n$\\hat{y} = f_{eglobal}(Global_F(f_{local}(X_{1,2})))$\n(2)\nfor the global branch, and\n$\\hat{y}_{RTF} = f_{eglobal}(RTF_{F}(f_{local}(X_{1,2})))$\n(3)\nfor the RTF branch, where $\\hat{y}$ and $\\hat{y}_{RTF}$ represent the model's predictions for the global and RTF branch, respectively. In this work, we construct the local and the global encoder from the original ViT"}, {"title": "Random Token Fusion", "content": "To better instruct the model to utilize information from both views, we introduce the Random Token Fusion (RTF). RTF maximizes the mutual information $I(Z; Y)$ between the fused representation $Z$ and the target $Y$, mathematically defined as:\n$I(Z;Y) = H(Z) - H(Z|Y)$\n(5)\nwhere $H(Z)$ is the entropy of the fused representation $Z$, and $H(Z|Y)$ is the conditional entropy given the target $Y$. By incorporating randomness into the token fusion process, RTF encourages the model to learn robust and generalized features from all views, ensuring that the fused representation captures the most informative features.\nRTF randomly selects part of the tokens from each view prior to fusion (see Figure 3 Right). Specifically, given two features $Z_{1,2} \\in \\mathbb{R}^{(N+1)\\times D}$, RTF randomly selects tokens from either view to form a mixed representation\n$Z_{spatial} = l_M Z_{1, spatial} + (1 - l_M) Z_{2,spatial}$,\n(6)\nfor the spatial tokens, where $l_M \\in \\mathbb{R}^N$ is a binary mask, whose elements take the value of 1 with a probability that follows a uniform distribution $p \\sim U(0, 1)$. To preserve high-level features used for classification from both views, we average the CLS tokens $z_{cls} = \\frac{Z_{1,cls}+Z_{2,cls}}{2}$. We concatenate $z_{cls}$ and $Z_{spatial}$ to form the final fused feature $z = \\{z_{cls}, Z_{spatial}\\}$. By doing so, it compels the network to capture dependencies between patches originating from different views, preventing the model from overfitting to view-specific features."}, {"title": "Fusion Strategies", "content": "RTF is designed to enhance common fusion strategies without bringing additional cost to inference. There are a number of ways to mix representations of different views. In this work, we consider the fusion strategies from the literature.\nGiven the representations of two views $Z_1$ and $Z_2$, where $z = \\{Z_{cls}, Z_{spatial_1},\u00b7\u00b7\u00b7, Z_{spatial_N} \\}$, and $Z_{cls}$ represents the class token, $spatial$ represents the i-th spatial token, and i ranges from 1 to N, one strategy is to perform some operation on the features to produce a fused feature $z$. The operation could be token-wise, such as concatenation or selection, or element-wise, such as addition, subtraction, multiplication, max, etc.\nIn this work, we consider token-wise fusion with concatenation, a choice widely adopted in current research [5, 38], implemented for ViTs by concatenating all tokens from two views. As such, $z$ is defined as $z = \\{Z_{1,cls}, Z_{1,spatial_1},\u00b7\u00b7\u00b7, Z_{1,spatial_N}, Z_{2,cls}, Z_{2,spatial_1},\u00b7\u00b7\u00b7, Z_{2,spatial_N} \\}$, as depicted in Figure 3. Since ViTs are able to process an arbitrary number of tokens [38, 5], this is a trivial operation. While this approach has the benefit of preserving information, it adds a considerable cost to the memory and compute. A compromised solution would be keeping and fusing only the CLS tokens $z = \\{Z_{1cls}, Z_{2cls}\\}$ for efficiency.\nAn alternative approach is element-wise fusion by averaging [42], implemented by averaging all tokens from two views $z = \\frac{Z_1+Z_2}{2}$ or by using only the CLS tokens, $z = \\frac{Z_{1,cls}+Z_{2,cls}}{2}$ (see Figure 3). While this approach is straightforward and computationally efficient, it may lead to loss of spatial information, particularly when input images are not registered [38]."}, {"title": "Experimental Setup", "content": "By design, RTF can be applied to most transformer-based multi-view fusion models. In this work, we employ the standard ViT family [9] for both the local and global encoders. Using ViT Tiny, Small, and Base, we conduct experiments on two standard benchmark medical datasets, CBIS-DDSM [20, 33] and CheXpert [18] to evaluate RTF performance on top of different fusion strategies, described in Section 3.2. We also compare against other multi-view fusion methods designed for ViTs and, for reference, against CNN-based multi-view fusion methods. For both datasets, we use the area under the receiver operating characteristic curve (AUC) to evaluate model performance. All experiments are repeated 4 times, and we report the mean value and standard deviation."}, {"title": "Datasets", "content": "CBIS-DDSM [20, 33] is a well-known public mammography dataset, containing 10, 239 samples from 1, 566 unique patients, including both craniocaudal (CC) and mediolateral oblique (MLO) views."}, {"title": "Implementation Details", "content": "We resize all images to 384 \u00d7 384 and initialize all models with weights pretrained on ImageNet-21K [32, 41], as randomly initialized ViTs cannot effectively be trained on small medical datasets [27]. By default, we use ViT Small [9] as the backbone, with concatenation as the fusion strategy. 75% of the total encoder blocks are used as the local encoder $f_{local}$, and the remaining 25% are applied to the fused feature. This partitioning is used by default throughout the rest of our experiments. We use the AdamW optimizer [25] to train the models for 300 epochs on CBIS-DDSM and for 60 epochs on CheXpert. The learning rate is selected based on a grid search. For both datasets, we use spatial scaling, flipping, rotation, and color jittering for data augmentation. After training, the checkpoint with the highest AUC validation score is selected for testing. We employ the same train/validation/test split as in [38, 2] for both datasets. All experiments are conducted on NVIDIA GeForce RTX 4090 Ti GPUs with 24 GB of memory."}, {"title": "Results and Discussion", "content": ""}, {"title": "Ablation Study", "content": "We begin by confirming the advantages of multi-view over single-view ViTs across various fusion strategies. We then show the benefits of incorporating RTF with standard fusion techniques. We then delve into identifying the most compatible fusion strategy for RTF, and conduct ablation studies for different model sizes.\nAre there benefits to using multiple views compared to a single view? To assess the benefits of multi-view ViTs, we conducted experiments training models on single views and with late fusions and report results in Table 1. Specifically, single-view models only consider one view throughout the task, while late-fusion models combine representations of different views at the end of the model [3, 2, 38]. We verify that, for both CBIS-DDSM and CheXpert, using two views results in improved"}, {"title": "Further Discussion", "content": "Can RTF work as a standalone, efficient training method? In this study, we propose RTF as a solution to enhance existing multi-view fusion strategies. RTF is designed to function with an additional branch in the network and loss function, making it compatible with various manipulations of intermediate features (tokens). Some approaches corrupt spatial information, while others retain all tokens as-is or completely discard the spatial ones. It would be intriguing to explore whether RTF alone could serve as an effective fusion strategy, potentially reducing computational requirements through random token selection compared to simple concatenation, and further improving perfor-mance. The potential positive outcomes could contribute to both efficient training and robust medical diagnosis. We plan to investigate this further in future work."}, {"title": "Conclusion", "content": "In this work, we focus on multi-view vision transformers for medical image analysis. We address shortcomings of current fusion methods that tend to overfit on view-specific features, not fully leveraging information from all views. Random token fusion (RTF) randomly drops tokens from both views during the fusion phase of training, encouraging the model to learn more robust representations across all views. RTF directly impacts the model's attention and enhances its performance without any additional cost at inference. Our experiments show that RTF exceeds the performance of other fusion methods and seamlessly boosts performance when combined with them \u2013 the degree of improvement is influenced by the dataset and model size. While our work focuses on multi-view ViTs for medical diagnosis with chest X-rays and mammograms, we believe that our findings can be extended to multiple medical modalities, as well as other tasks and domains, which we leave for future work."}, {"title": "Appendix A Extended Results", "content": "We report additional experimental results included in the main paper. Table 7 and Table 8 extend\nTable 2 of the main paper. We investigate how many blocks of a standard ViT are used as the local\nencoder and the remaining as the global encoder for the fused tokens with all mentioned fusion\nstrategies. Two key findings emerge from the results:\nFirst, concatenation proves more robust to the choice of local/global ratio compared to the other\nfusion strategies. This robustness is expected, as concatenation preserves the information to the\nmost extent and can fully utilize the global transformer blocks. Based on these results, we select\nconcatenation as the default fusion strategy. Second, RTF generally enhances performance across all\nsettings. The only exception occurs when using 25% blocks as the local encoder with CLS\nscenario, all spatial tokens are discarded at a very early stage, and only the two CLS tokens are sent to\nthe global encoder, resulting in extremely low model capacity. Applying RTF in this situation harms\nperformance, similar to the effects of aggressive regularization techniques on an already under-fitting\nmodel."}, {"title": "Appendix B Additional Saliency Results", "content": "Extended results on CBIS-DDSM (top) and CheXpert (bottom), showing the model's\nattention maps within the last block of the global encoder. RTF seems to address the issue of attention\nbeing allocated to uninformative areas, a common phenomenon observed in ViTs. It also encourages\nthe model to focus on both views in many cases."}]}