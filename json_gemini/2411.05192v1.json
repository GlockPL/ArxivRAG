{"title": "Explaining Mixtures of Sources in News Articles", "authors": ["Alexander Spangher", "James Youn", "Matt DeButts", "Nanyun Peng", "Emilio Ferrara", "Jonathan May"], "abstract": "Human writers plan, then write (Yao et al., 2019). For large language models (LLMs) to play a role in longer-form article generation, we must understand the planning steps humans make before writing. We explore one kind of planning, source-selection in news, as a case-study for evaluating plans in long-form generation. We ask: why do specific stories call for specific kinds of sources? We imagine a generative process for story writing where a source-selection schema is first selected by a journalist, and then sources are chosen based on categories in that schema. Learning the article's plan means predicting the schema initially chosen by the journalist. Working with professional journalists, we adapt five existing schemata and introduce three new ones to describe journalistic plans for the inclusion of sources in documents. Then, inspired by Bayesian latent-variable modeling, we develop metrics to select the most likely plan, or schema, underlying a story, which we use to compare schemata. We find that two schemata: stance (Hardalov et al., 2021) and social affiliation best explain source plans in most documents. However, other schemata like textual entailment explain source plans in factually rich topics like \"Science\". Finally, we find we can predict the most suitable schema given just the article's headline with reasonable accuracy. We see this as an important case-study for human planning, and provides a framework and approach for evaluating other kinds of plans. We release a corpora, NewsSources, with annotations for 4M articles.", "sections": [{"title": "1 Introduction", "content": "As language models (LMs) become more proficient at long-form text generation and incorporate resources (Lewis et al., 2020) and tools (Schick et al., 2023) to support their writing, recent work has shown that planning before writing is essential (LeCun, 2022; Spangher et al., 2023a; Park et al., 2023). However, supervised datasets to support learning and studying plans are few: they are difficult or expensive to collect, synthetic, or narrowly tailored to specific domains (Zhou et al., 2023).\nOne approach to collecting diverse planning data is to observe natural scenarios in which planning has already occurred. In this work, we consider one such real-world scenario: source selection by human journalists. Consider the article shown in Table 1. The author shares her plan\u00b9:\nNJ schools are teaching climate change\nin elementary school. We wanted to un-\nderstand: how are teachers educating\nchildren? How do parents and kids feel?\nIs there pushback?"}, {"title": "2 Source Categorization", "content": "Our central question is: why did the writer select sources 81, 82, 83... for document d? Intuitively, let's say we read an article on a controversial topic. Let's suppose we observe that it contains many opposing viewpoints: some sources in the article \"agree\" with the main topic and others \"disagree\u201d. We can conclude that the writer probably chose sources on the basis of their stance (Hardalov et al., 2021) (or their opinion-based support) rather than another explanation, like their discourse role (which describes their narrative function).\nMore abstractly, we describe source-selection as a generative process: first, journalists plan how they will choose sources (i.e. the set of k categories sources will fall into), then they choose sources, each falling into 1-of-k categories. Different plans, or categorizations, are possible (e.g. see Figure 1): the \"right\" plan is the one that best predicts the final document.\nEach plan, or categorizations, is specified by a schema. For the 8 schema used in this work, see Figure 2. To apply a schema to a document, we frame an approach consisting of two components: (1) an attribution function, a:"}, {"title": "2.1 Problem Statement", "content": "Our central question is: why did the writer select sources 81, 82, 83... for document d? Intuitively, let's say we read an article on a controversial topic. Let's suppose we observe that it contains many opposing viewpoints: some sources in the article \"agree\" with the main topic and others \"disagree\u201d. We can conclude that the writer probably chose sources on the basis of their stance (Hardalov et al., 2021) (or their opinion-based support) rather than another explanation, like their discourse role (which describes their narrative function).\nMore abstractly, we describe source-selection as a generative process: first, journalists plan how they will choose sources (i.e. the set of k categories sources will fall into), then they choose sources, each falling into 1-of-k categories. Different plans, or categorizations, are possible (e.g. see Figure 1): the \"right\" plan is the one that best predicts the final document.\nEach plan, or categorizations, is specified by a schema. For the 8 schema used in this work, see Figure 2. To apply a schema to a document, we frame an approach consisting of two components: (1) an attribution function, a:"}, {"title": "2.2 Comparing Plans, or Schemata", "content": "We can compare plans in two ways: (1) how well do they explain each observed document? and (2) how structurally consistent are they?\nExplainability A primary criterion for a plan is for it to explain the observed data well. To measure this, we use conditional perplexity\u00b3\np(x|z)\nwhich measures the uncertainty of observed data, x, given a latent structure, z. Measuring p(x|z)\nfor different z (fixing x) allows us to compare z.\nConditional perplexity is a novel metric we introduce, inspired by metrics to evaluate latent unsupervised models, like the \u201cleft-to-right\" algorithm introduced by (Airoldi and Bischof, 2016). 4\nStructural Likelihood: A second basic criterion for a latent structure to be useful is for it be consistent, which is a predicate for learnability. We assess the consistency of a set of assignments, z, by calculating the posterior predictive:\np(z|z,x)"}, {"title": "2.3 Source Schemata", "content": "Our schemata, or plans, are shown in Figure 2. We collect 8 schemata to compare, including three we introduce: Identity, Affiliation and Role. Each schema provides a set of labels, which each describe sources used in a news article. Again, our hypothesis is that the schema which best predicts the observed text of the article is the one the journalist most likely adhered to while planning the article (Section 4). See Appendix D for more details and definitions for each schema.\nWe note that none of these schemata are complete and that real-world plans likely have elements outside of any one schema (or are combinations of multiple schema). However, this demonstration is important, we argue, to prove that we can differentiate between purely latent plans in long-form text. We now introduce each schema:\nDebate-Oriented Schemata Both the Stance and NLI schemata are debate-oriented schemata. They each capture the relation between the information a source provides and the main idea of the article. NLI (Dagan et al., 2005) captures factual relations between text, while Stance (Hardalov et al., 2021) captures opinion-based relations . A text pair may be factually consistent and thus be classified as \"Entailment\" under a NLI schema, but express different opinions and be classified as \u201cRefute\u201d under Stance. In our setting, we relate article's headline with the source's attributable information. These schemata say a writer uses sources for the purpose of expanding or rebutting information in the narrative, offering different perspectives and broadening the main idea."}, {"title": "3 Building a Silver-Standard Dataset of Different Possible Plans", "content": "The schemata described in the previous section give us theoretical frameworks for identifying writers' plans. To compare schemata and select the schema that best describes a document, we must first create a dataset where informational sources are labeled according to each schema. We describe that process in this section."}, {"title": "3.1 Dataset Construction and Annotation", "content": "We obtain the NewsEdits dataset (Spangher et al., 2022), which consists of 4 million news articles, and extract sources using a methodology developed by Spangher et al. (2023b), which authors established was state-of-the-art for this task. This dataset spans 12 different news sources (e.g. BBC, NYTimes, etc.) over a period of 15 years (2006-2021). For our experiments, we sample 90,000 news articles that are long and contain more than 3 sources (on average, the articles contain ~ 7.5 sources). Then, we annotate to collect training data and build classifiers to categorize these sources.\nWe described those processes now.\nWe recruited two annotators, one an undergraduate and the other a former journalist. The former journalist trained the undergraduate for 1 month to identify and label sources, then, they independently labeled 425 sources in 50 articles with each schema to calculate agreement, scoring \u043a = .63, .76, .84 on Affiliation, Role and Identity labels. They then labeled 4,922 sources in 600 articles with each schema, labeling roughly equal amounts. Finally, they jointly labeled 100 sources in 25 documents with the other schemata for evaluation data over 1 month, with \u043a \u2265 .54, all in the range of moderate to substantial agreement (Landis and Koch, 1977)."}, {"title": "3.2 Training Classifiers to Label Sources", "content": "We train classifiers to label sources under each schema. Unless specified, we use a sequence classifier using RoBERTa-base with self-attention pooling, as in Spangher et al. (2021a). We deliberately chose smaller models to scale to large amounts of articles. We will open-source all of the classifiers trained in this paper.\nWe use our annotations to train classifiers which take as input all sentences attributable to source q and output a label in each schema, or p(t/s(q)... s)).\nWe use datasets, without modification, that were directly released by the authors. Each is labeled on a sentence-level, on news and opinion datasets. We train classifiers to label each sentence of the news article, s. Then, for each source q, we assign a single label, y,"}, {"title": "3.3 Classification Results", "content": "As shown in Table 2, we model schemata within a range of f1-scores \u2208 (53.3, 67.2), showing moderate success in learning each schema. These scores are middle-range and likely not useful on their own; we would certainly have achieved higher scores with more state-of-the-art methods. However, we note these classifiers are being used for comparative, explanatory purposes, so their efficacy lies in how well they help us compare plans, as we will explore in the next section."}, {"title": "4 Comparing Schemata", "content": "We are now ready to explore how well these schemata explain source selection in documents. We start by describing our experiments, then baselines, and finally results. All experiments in this section are based on the 90, 000 news articles filtered from NewsEdits, labeled as described in the previous section. We split 80,000/10,000 train/eval."}, {"title": "4.1 Implementing Planning Metrics", "content": "We now describe how we implement the metrics introduced in Section 2.2: (1) conditional perplexity and (2) posterior predictive.\nTo measure conditional perplexity, p(x|z), we fine-tune GPT2-base models (Radford et al., 2019) to take in it's prompt a sequence of latent variables, each for a different source, and then assess likelihood of the observed article text. 10 .10 This is similar to measuring vanilla perplexity on observed text, except: (1) we provide latent variables as conditioning (2) by fixing the model used and varying the labels, we are measuring the signal given by each set of different labels. Our template for GPT2 is:"}, {"title": "4.2 Baselines", "content": "Vanilla perplexity does not always provide accurate model comparisons (Meister and Cotterell, 2021; Oh et al., 2022) because it can be affected by irrelevant factors, like tokenization scheme. We hypothesized that the dimensionality of each schema's latent space might also have an effect (Lu et al., 2017); larger latent spaces tend to assign lower probabilities to each point. Thus, we benchmark each schema against baselines with similar latent dimensions.\nWe generate k unique identifiers12, and randomly assign one to"}, {"title": "4.3 Results and Discussion", "content": "As shown in Table 3, the supervised schemata mostly have have lower conditional perplexity than their random and unsupervised kmeans baselines. However, only the Stance, Affiliation and Role schemata improve significantly (at p < .001), and the Role schema's performance increase is minor. Retrieval has a statistically significant less explainability relative to it's baselines.\nThere is a simple reason for why some schemata have either the same or more conditional perplexity compared to their baselines: they lack explainability over the text of the document, but are not random and thus might lead to overfitting. We examine examples and find that Retrieval does not impact wording as expected: writers make efforts to convey information similarly whether it was obtained via a quote, document or a statement.\nWe face a dilemma: in generating these schemata, we chose Retrieval because we assumed it was an important planning criterion. However, our results indicate that it holds little explanatory power. Is it possible that some plans do not get reflected in the text of the document?\nTo address this question, we assign 2 = arg minz p(x|z), the schema for each datapoint with the lowest perplexity, using scores calculated in the prior section14, we calculate the lowest-perplexity schema. Table 5 shows the distribution of such articles. We then task 2 expert journalists with assigning their own guess about which schema best describes the planning for the particular article, for 120 articles. We observe an F1-score of 74, indicating a high degree of agreement.\nInterestingly, we also observe statistically significant improvements of kmeans over random base-"}, {"title": "5 Predicting Schemata", "content": "Taken together, our observations from (1) Section 3.3) indicate that schemata are largely unrelated and (2) Section 4.3 indicate that Stance and Affiliation both have similar explanatory power (although Stance is less predictable). We next ask: which kinds of articles are better explained by one schema, and which are better explained by the other? If we can answer this question, we take steps towards being able to plan source-selection via different schemata. Such a step could lead us towards better multi-document retrieval techniques, by giving us axes to combine different documents into a retriever."}, {"title": "6 Related Work", "content": "This work focuses on informational sources in news articles and is part of a broader field of character-based analysis in text."}, {"title": "6.1 Latent Variable Persona Modeling", "content": "Our work is inspired by earlier work in persona-type latent variable modeling (Bamman et al., 2013; Card et al., 2016; Spangher et al., 2021b). Authors model characters in text as mixtures of topics. We both seek to learn and reason about about latent character-types, but their line of work takes an unsupervised approach. We show that supervised schemata outperform unsupervised."}, {"title": "6.2 Multi-Document Retrieval", "content": "In multiple settings \u2013 e.g. multi-document QA (Pereira et al., 2023), multi-document summarization (Shapira et al., 2021), retrieval-augmented generation (Lewis et al., 2020) \u2013 information from a single source is assumed to be insufficient to meet a user's needs. In typical information retrieval settings, the goal is to retrieve a single document closest to the query (Page et al., 1998). Despite earlier work in multi-document retrieval (Zhai et al., 2015; Yu et al., 2023), in settings where multiple sources are needed, on the other hand, retrieval goals are not clearly understood16. Our work attempts to clarify this, and can be seen as a step towards better retrieval planning."}, {"title": "6.3 Planning in Language Models", "content": "Along the line of the previous point, chain-of-thought reasoning (Wei et al., 2022) and in context learning (ICL), summarized in (Sanchez et al., 2023), can be seen as latent-variable processes. Indeed, work in this vein is exploring latent-variable modeling for ICL example selection (Wang et al., 2024). Our work, in particular the conditional perplexity formulation and it's implementation, can be seen as a way of comparing different chain-of-thought plans as they relate to document planning."}, {"title": "6.4 Computational Journalism", "content": "Computational journalism seeks to apply computational techniques to assist journalists in reporting. Researchers have sought to improve detection of incongruent information (Chesney et al., 2017), detect misinformation (Pisarevskaya, 2017) and false claims made in news articles (Adair et al., 2017). Such work can improve readers' trust in news. Our work takes steps towards understanding plans, or schemata, in news articles. As such, further work in this direction might yield deeper, more latent critiques for identifying untrustworthy articles."}, {"title": "7 Conclusions", "content": "In conclusion, we explore ways of thinking about sourcing in human writing. We compare 8 schemata of source categorization, and adapt novel ways of comparing them. We find, overall, that affiliation and stance schemata help explain sourcing the best, and we can predict which is most useful with moderate accuracy. Our work lays the ground work for a larger discussion of discovering plans made by humans in naturally generated documents. It also takes us steps towards tools that might be useful to journalists. Naturally, our work is a simplification of the real human processes guiding source selection; these categories are non-exclusive and inexhaustive. We hope by framing these problems we can spur further research in this area."}, {"title": "8 Limitations", "content": "A central limitation to our work is that the datasets we used to train our models are all in English. As mentioned previously, we used English language"}, {"title": "9 Ethics Statement", "content": ""}, {"title": "9.1 Risks", "content": "Since we constructed our datasets on well-trusted news outlets, we assumed that every informational sentence was factual, to the best of the journalist's ability, and honestly constructed. We have no guarantees that our classification systems would work in a setting where a journalist was acting adversarially.\nThere is a risk that, if planning works and natural language generation works advance, it could fuel actors that wish to use it to plan misinformation and propaganda. Any step towards making generated news article more human-like risks us being less able to detect and stop them. Misinformation is not new to our media ecosystem, (Boyd et al.,"}, {"title": "9.2 Licensing", "content": "The dataset we used, NewsEdits (Spangher et al., 2022), is released academically. Authors claim that they received permission from the publishers to release their dataset, and it was published as a dataset resource in NAACL 2023. We have had lawyers at a major media company ascertain that this dataset was low risk for copyright infringement."}, {"title": "9.3 Computational Resources", "content": "The experiments in our paper required computational resources. We used 64 12GB NVIDIA 2080 GPUs. We designed all our models to run on 1 GPU, so they did not need to utilize model or data-parallelism. However, we still need to recognize that not all researchers have access to this type of equipment.\nWe used Huggingface models for our predictive tasks, and will release the code of all the custom architectures that we constructed. Our models do not exceed 300 million parameters."}, {"title": "9.4 Annotators", "content": "We recruited annotators from our educational institutions. They consented to the experiment in exchange for mentoring and acknowledgement in the final paper. One is an undergraduate student, and the other is a former journalist. Both annotators are male. Both identify as cis-gender. The annotation conducted for this work was deemed exempt from review by our Institutional Review Board."}, {"title": "Appendix", "content": "In Appendix A, we include more, precise detail about our experimental methods. Then, Appendix B, we present more exploratory analysis to support our experiments, including comparisons between schemata. In Appendix D, we give a more complete set of definitions for the labels in each schema. In Appendix G, we define the unsupervised latent variable models we use as baselines, including providing details on their implementation."}, {"title": "A Additional Methodological Details", "content": ""}, {"title": "A.1 Source Extraction", "content": "Before classifying sources, we first need to learn an attribution function (Equation 1) to identify the set of sources in news articles. Spangher et al. (2023b) introduced a large source attribution dataset, but their models are either closed (i.e. GPT-based) or underperforming. So, we train a high-performing open-source model using their dataset. We fine-tune GPT3.5-turbo 17, achieving a prediction accuracy of 74.5% on their test data18. Then, we label a large silver-standard dataset of 30,000 news articles and distill a BERT-base span-labeling model, described in (Vaucher et al., 2021), with an accuracy of 74.0%.19 We use this model to score a large corpus of 90, 000 news articles from the NewsEdits corpus (Spangher et al., 2022). We find that 47% of sentences in our documents can be attributed to sources, and documents each contain an average of 7.5 +-/5 sources. These statistics are comparable to those reported by Spangher et al. (2023b)."}, {"title": "B Exploratory Data Analysis", "content": "We explore more nuances of our schemata, including comparative analyses. We start by showing a view of 2, the conditions under which a schema best explains the observed results. In Tables 6 and 7, we show an extension of Table 4 in the main body: we show favored keywords across all schemata. (Note that in contrast to Table 4, we restrict the keywords we consider to a tighter range). When topics require a mixture of different information"}, {"title": "C Article Example", "content": "Here is an article example, annotated with different schemata definitions, along with a description by the journalist of why they pursued the sources they did.\nWe mined state and federal court paper-work. We went looking for [previous] stories. We called police and fire commu-nications people to determine [events]. We found families for interviews about"}, {"title": "D Further Schemata Definitions", "content": "Here we provide a deeper overview of each of the schemata that we used in our work, as well as definitions that we presented to the annotators during annotation.\n\u2022 Affiliation: Which group the source belongs to.\n\u2013 Institutional: The source belongs to a larger institution.\n1. Government: Any source who executes the functions of or represents a government entity. (E.g. a politician, regulator, judge, political spokesman etc.)\n2. Corporate: Any source who belongs to an organization in the private sector. (E.g. a corporate executive, worker, etc.)\n3. Non-Governmental Organization (NGO): If the source belongs to a nonprofit organization that operates independently of a government. (E.g. a charity, think tank, non-academic research group.)\n4. Academic: If the source belongs to an academic institution. Typically, these are professors or students and they serve an informational role, but they can be university administrators, provosts etc. if the story is specifically about academia."}, {"title": "E Example GPT Prompts", "content": "We give more examples for prompts."}, {"title": "E.1 Source Attribution Prompts", "content": "In Section A.1, we discuss training a GPT3.5-Turbo model with Spangher et al. (2023b)'s source attribution dataset to create more labeled datapoints, which we then distil into a BERT model. We train a batched model to save on costs. The prompt takes the following form:\nInput:\n1. \n2. \n3. ...\nResponse:\n1. \n2. \nHere is an example:\nSystem Prompt:\nYou are a journalist's\nfact-checker who identifies"}, {"title": "E.2 Stance-Based Prompts", "content": "In Section 3.2 we discuss the prompts we formulated to do appropriate transfer learning from the stance datasets others have annotated to our news setting. Because in Stance detection, there are usually many claims made for each hypothesis, we used batched prompts to save costs, in the following form:\nPremise: \nClaim:\n1. \n2. \nResponse: 1. \n2. \nHere is an example:\nSystem Prompt: You are a journalist's assistant who spots opposing claims. The user will give you a premise and 5 claims. Respond to each one, in numbered order from 1 to 5, with a choice from: ['Neutral', 'Affirm', 'Discuss', 'Refute\u2019]. Don't say anything else, and be sure to answer each one."}, {"title": "E.3 GPT-2 Conditional Perplexity Prompts", "content": "In Section 4.1, we discuss crafting prompts for GPT2-base models in order to calculate conditional perplexity. We give the outline of our prompt. Here is an example:\nRevelations from the artist's\nautobiography threaten to cloud\nher new show at the San Francisco\nMuseum of Modern Art."}, {"title": "F Combining Different Schemata", "content": "We show how two schemata, Role and Affiliation may be naturally combined. One function of journalism is to interrogate the organizations powering our society. Thus, many sources are from Affiliations: Government, Corporations, Universities, Non-Governmental Organizations (NGOs). And, they have different Roles in these places. Journalists first seek to quote decision-makers or participants: presidents, CEOs, or senators. Sometimes decision-makers only comment though Representatives: advisors, lawyers or spokespeople. These sources all typically provide knowledge of the inner-workings of an organization. Broader views are often sought from Informational sources: experts in government or analysts in corporations; scholars in academia or researchers in NGOs. These sources usually provide broader perspectives on topics."}, {"title": "G Latent Variable Models", "content": "As shown in Figure 8, our model observes a switching variable, y and the words, w, in each document. The switching variable, y is inferred and takes one of two values: \u201csource word\" for words that are associated with a source \u201cbackground\", for words"}, {"title": "G.1 Inference", "content": "We construct the joint probability and collapse out the Dirichlet variables: Pw, Pz, Ps, Pr to solve a Gibbs sampler. Next, we discuss the document-type, source-type, and word-topic inferences."}, {"title": "G.1.1 Document-Type inference", "content": "First, we sample a document-type Td \u2208 1, ..., T for each document:\np(T_d|T_{\u2212d}, s, z, \u03b3, H_\u03c4, H_s, H_z) \u221d  \\frac{\\Gamma (H_{\u03c4\u03c4} + \\sum_{S_d} C^{T*}_T)}{\\Gamma_{TT}} x\\frac{\u03a0 (H_{s} + C_{T_ds,*,*,*}}{\\Gamma_{TS}} x\\frac{\u03a0^{(s)} (H_{z} + C_{zk,T_d,*}}{\\Gamma_{TZS}} x\\frac{\u03a0{^(3)} (H_{w} + C_{zij,w_{ij},*})}{\\Gamma_{TZSW}}\nwhere the first term in the product is the probability attributed to document-type: CT,* cd) is the count of all documents with type Td, not considering the current document d's assignment. The second term is the probability attributed to source-type in a document: the product is over all sources in document d. Whereas CTd,s,*,* is the count of all sources of type s in documents of type Td, and CT\u300f,*,*,* is the count of all sources of any time in documents of type Td. The third term is the probability attributed to word-topics associated with the background word: the"}, {"title": "G.1.2 Source-Type Inference", "content": "Next, having assigned each document a type, Ta, we sample a source-type S(d,n) \u2208 1, ..., S for each source.\np(S_{(d,n)}|S_{-(d,n)}, T, z, H_{T}, H_{s}, H_{z}) \u221d  \\frac{\\Gamma (H_{SS} + C^{TS_({d,n}),*,*})}{\\Gamma_{TS}} x\\frac{\u03a0^{(s)} (H_{z} + C_{zj,*,S_{(d,n)},*,*}}{\\Gamma_{TZS}} x\\frac{\u03a0^{s}}{\u0393(TS)}\nWhere first term in the product is the probability attributed to the source-type: CT,S(d,n),*,* is the count of all sources of type S(d,n) in documents of type Td, not considering the current source's source-type assignment. The second term in the product is the probability attributed to word-topics of words assigned to the source: the product is over all words associated with source n in document d. Here, Czj,*,S(d,n),*,* is the count of all words with topic zj and source-type S(d,n), and c*,*,S(d,n),*,* is the count of all words associated with source-type S(d,n)."}, {"title": "G.1.3 Word-topic Inference", "content": "Finally, having assigned each document a document-type and source a source-type, we sample word-topics. For word i, j, if it is associated with sources (i,j = Source Word), we sample:\np(z_{(i,j)}|z_{\u2212(i,j)}, S, T, \u03c9,\u03b3,  H_\u03c9, H_s, H_\u03c4, H_z) \u221d \\frac{(C_{z{ij},*,S{d,*},*} + H_z)\\(i,j)}{C{z{ij},*,*,*} + V H_w} x \\frac{(C_{z{ij},wi,j,*}} + Hw)}{\\\u0393{TSz}}"}]}