{"title": "LLMs can Schedule", "authors": ["Henrik Abgaryan", "Ararat Harutyunyan", "Tristan Cazenave"], "abstract": "The job shop scheduling problem (JSSP) remains a significant hurdle in optimizing production processes. This challenge involves efficiently allocating jobs to a limited number of machines while minimizing factors like total processing time or job delays. While recent advancements in artificial intelligence have yielded promising solutions, such as reinforcement learning and graph neural networks, this paper explores the potential of Large Language Models (LLMs) for JSSP. We introduce the very first supervised 120k dataset specifically designed to train LLMs for JSSP. Surprisingly, our findings demonstrate that LLM-based scheduling can achieve performance comparable to other neural approaches. Furthermore, we propose a sampling method that enhances the effectiveness of LLMs in tackling JSSP.", "sections": [{"title": "1 Introduction", "content": "The job shop scheduling problem (JSSP) remains a well-studied and computationally challenging problem in the field of production scheduling and optimization. It entails the efficient allocation of a set of N jobs, each with heterogeneous processing times, to a limited number of M machines. The primary objective is to optimize a performance metric, such as minimizing the total completion time (makespan, denoted by Cmax) or reducing the flow time (average completion time) of individual jobs. JSSP finds application in diverse manufacturing and service environments, impacting factors like production throughput, resource utilization, and ultimately, customer service levels. Traditional approaches to JSSP have primarily relied on mathematical programming techniques and heuristic algorithms Chaudhry and Khan [2015]. However, these methods often exhibit limitations in scalability and effectiveness, particularly for large-scale problems, or those with complex job-machine precedence relationships. This has motivated the exploration of alternative approaches, particularly with the recent advancements in artificial intelligence (AI). Techniques like reinforcement learning and graph neural networks have shown promise in addressing JSSP, offering data-driven solutions to this problemZhang et al. [2020]Corsini et al. [2024]."}, {"title": "2 Related Work", "content": "The job shop scheduling problem (JSSP) with more than two machines is proven to be NP-hard Garey et al. [1976]. As a result, finding exact solutions for JSSP is generally infeasible, leading to the widespread use of heuristic and approximate methods for practical efficiency Cebi et al. [2020]. Traditional approaches to solving JSSP have primarily relied on search and inference techniques developed by the constraint programming community Beck et al. [2010]. These techniques effectively leverage constraints to define the relationships and limitations between jobs and resources, enabling efficient exploration of feasible solution spaces and the identification of optimal or near-optimal schedules Nowicki and Smutnicki [2005]. A widely used heuristic method in real-world scheduling systems is the Priority Dispatching Rule (PDR) Zahmani et al. [2015]. PDRs are effective, although designing an efficient PDR is time-consuming and requires extensive domain knowledge.\nRecently, approaches utilizing Deep Learning and Neural Networks have gained attention for finding promising solutions to the Job Shop Scheduling Problem (JSSP) Bonetta et al. [2023], Zhang et al. [2020], Corsini et al. [2024]. These methods can be broadly categorized into supervised learning and reinforcement learning (RL). Current research in deep reinforcement learning (DRL) is actively focused on developing advanced methods to tackle JSSP. Existing DRL methods typically represent JSSP as a Markov Decision Process (MDP) and learn a policy network based on DRL techniquesZhang et al. [2020].\nWhile there are currently no papers that directly address the scheduling of Job Shop Scheduling Problems (JSSP) using LLMs, some notable works explore the potential of LLMs in mathematical reasoning and programming Chen et al. [2023], Wei et al. [2022], Ahn et al. [2024], Yang et al. [2023]. Optimization using large language models (LLMs) has gained significant interest in recent"}, {"title": "3 Preliminary", "content": "The Job-Shop Scheduling Problem (JSSP) is formally defined as a problem involving a set of jobs J and a set of machines M. The size of the JSSP problem instance is described as $N_J \\times N_M$, where $N_J$ represents the number of jobs and $N_M$ the number of machines. For each job $J_i \\in J$, it must be processed through $n_i$ machines (where $n_i$ is the number of operations for job $J_i$) in a specified order $O_{i1} \\rightarrow \\cdots \\rightarrow O_{in_i}$, where each $O_{ij}$ (for $1 \\leq j \\leq n_i$) represents an operation of $J_i$ with a processing time $p_{ij} \\in \\mathbb{N}$. This sequence also includes a precedence constraint. Each machine can process only one job at a time, and switching jobs mid-operation is not allowed. The objective of solving a JSSP is to determine a schedule, that is, a start time $S_{ij}$ for each operation $O_{ij}$, to minimize the makespan $C_{\\text{max}} = \\max_{i,j}\\{C_{ij} = S_{ij} + p_{ij} \\}$ while meeting all constraints. The complexity of a JSSP instance is given by $N_J \\times N_M$."}, {"title": "4 Dataset Generation", "content": "In order to try to solve the JSSP with LLM, we first need to represent the problem in natural language. To do that, we have to transform the matrix-based representation in standard JSSP format to a human-readable format."}, {"title": "4.1 Converting JSSP problem instance to Natural Language: Feature Generation", "content": "We use two methods to convert a Job Shop Scheduling Problem (JSSP) from a matrix representation into a human-readable format. Each method presents the information differently."}, {"title": "4.2 Approach 1: Job-Centric", "content": "This approach describes the tasks organized by jobs, providing a job-centric view of the scheduling problem.\n\u2022 Initialization: Begins by introducing the problem, detailing the number of jobs and machines involved.\n\u2022 Task Organization: Enumerates operations for each job, specifying the sequence of operations, the corresponding machines, and their respective durations.\n\u2022 Description Generation: Provides a detailed description of each machine's tasks, including the job number, operation number, and duration, ensuring clarity and completeness."}, {"title": "4.3 Approach 2: Machine-Centric", "content": "This approach describes the operations organized by machines, providing a machine-centric view of the scheduling problem.\n\u2022 Initialization: Begins by introducing the problem, detailing the number of jobs and machines involved.\n\u2022 Task Organization: Enumerates operations for each machine, specifying the sequence of operations, the corresponding jobs, and their respective durations.\n\u2022 Description Generation: Provides a detailed description of each machine's tasks, including the job number, operation number, and duration, ensuring clarity and completeness."}, {"title": "4.4 Zero-shot inference and Label generation", "content": "Our choice of LLM is relatively small Phi-3-Mini-128K-Instruct open-source model with 128K context size. Later we will refer this model as Phi3. The model is one of the open-source AI models developed by Microsoft. It is a 3.8 billion-parameter, lightweight, state-of-the-art model trained using the Phi-3 datasets. It shows good performance across a variety of language, reasoning, coding, and math benchmarks Abdin et al. [2024].\nInitially, we considered performing zero-shot inference with the Phi3 to solve the JSSP. However, the model consistently produced general descriptions of how to solve the problem instead of actual solutions. Occasionally, it provided partial solutions, but these were mostly infeasible.\nBecause the zero-shot inference results were not satisfactory, we decided to finetune the large language model (LLM) using a supervised approach. This required creating a supervised dataset, which included not only the problem formulations in natural language as described in Section 4 but also the solutions.\nTo generate feasible solutions, we employed Google's OR-Tools. The configuration for the Google's OR-Tools solver was set as follows:"}, {"title": "5 Training", "content": "To prepare the dataset for fine-tuning we randomly select an initial user prompt variation from a predefined set to prevent overfitting on a single instruction:\n\u2022 \"Instruct: Provide a solution schedule for the JSSP problem below, also indicate the makespan.\"\n\u2022 \"Task: Provide the steps of a solution for the JSSP problem and determine the makespan.\"\n\u2022 \"Command: Give a detailed solution to tackle the JSSP problem, focusing on optimizing the makespan.\"\nThe context is established by defining the assistant as an expert in JSSP ({\"role\": \"system\", \"content\": \"You are an expert in Job Shop Scheduling Problem\" }). Then the function constructs a message sequence with roles for the system, user, and assistant, incorporating the selected user prompt and corresponding natural language JSSP problem and solution. This sequence is then formatted using a chat template from the tokenizerFace.\nThe tokenizer is initialized with the following settings: padding_side is set to 'right', ensuring that padding tokens are added to the end of the sequence; pad_token is set to the tokenizer's end-of-sequence (eos_token) token; model_max_length is set to 40,000, defining the maximum length of the sequences to avoid memory issues; truncation is enabled to truncate sequences longer than the maximum length; and padding is enabled to pad sequences shorter than the maximum length. These settings ensure that the sequences are appropriately padded and truncated for model input. In order to fine-tune the model we utilized the LoRA: Low-Rank Adaptation method Hu et al. [2022]. The"}, {"title": "6 Evaluation", "content": "To evaluate the model's performance, we created a separate random evaluation dataset distinct from the training data. This dataset included problems of various sizes, ranging from 2x2 to 9x9, with durations of operations ranging from 1 to 199. around 1000 examples in total. We fix the initial user prompt to \"Instruct: Provide a solution schedule for the JSSP problem below, also indicate the makespan.\" We observed that performing inference in float8 or float4 data types resulted in solutions that deviated from the training data format during generation. Therefore, we opted for float16 format for testing. Given the memory constraints of a single NVIDIA A6000 GPU with 48GB, we could only test instances with size $N_J \\times N_M < 100$ using float16 data type and sampling. The inference process itself consumes approximately 43GB of memory on the NVIDIA A6000 GPU with float16 data type."}, {"title": "6.1 Overview of JSSP Solution Parsing and Validation", "content": "Following inference, we employ regular expressions to parse the output string generated by the LLM. This process extracts job number, operation number, machine number, start time, duration, end time for each operation, and the makespan value (if present)."}, {"title": "6.1.1 Validating JSSP Solution", "content": "We then use the extracted information from the JSSP solution to perform the following checks:\n\u2022 Operation Validation: Confirms that each operation's machine and duration in the LLM output match the expected values from the problem data."}, {"title": "6.2 Hyperparameter Tuning", "content": "We employed specific sampling hyper-parameters during the inference. To determine these hyper-parameters, we conducted a grid search on a small dataset of 7x8 and 8x8 problems, encompassing 200 instances in total. hyperparameters:\n\u2022 top_k_values = [10, 20, 50]\n\u2022 temperature_values = [0.2, 0.5, 0.7, 1.0]\n\u2022 top_p_values = [0.8, 0.9, 0.95]\nWe fixed num_return_sequences=10 in all combinations. We found the following configuration to be the best in providing the lowest makespan:\n\u2022 sample=True for sampling-based generation.\n\u2022 num_return_sequences=10 to generate ten sequences.\n\u2022 temperature=1.0 to control randomness.\n\u2022 top_k=50 to limit sampling to the top 50 tokens.\n\u2022 top_p=0.95 to apply nucleus sampling.\nThe tokenizer is configured with a maximum length of 40,000 tokens, uses eos_token as the padding token, and pads on the left."}, {"title": "6.3 Comparative Analysis with Other Neural Approaches", "content": "Due to the lack of other works utilizing LLMs end-to-end for scheduling, we compared our results with other neural approaches. We assessed the average gap between the optimal makespan and the makespan of our fine-tuned Phi3 model on our dataset.\nWe compared our results with \"Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning\" (L2D) Zhang et al. [2020], which uses a Graph Neural Network (GNN) and Proximal Policy Optimization (PPO). L2D's method employs a size-agnostic policy network for generalization. The original paper uses greedy first choice over the policy network's probability distribution. For fair comparison, we sampled the policy network's probability distribution (s=10) and selected the solution with the minimum makespan. We used the network trained on instances with $N_J = 20$ and $N_M = 20$.\nAnother paper used for comparison is \"Self-Labeling the Job Shop Scheduling Problem\" (SLJ) Corsini et al. [2024], which introduces a novel self-supervised training strategy for the JSSP. Their method leverages a generative model based on the Pointer Network, training it by generating multiple solutions (Beta parameter) and using the best one as a pseudo-label, thus eliminating the need for costly optimal solutions. During the comparison, we used three different networks trained with Beta values of 32, 128, and 256, namely slj_32, slj_128 and slj_256. During testing, the method also utilizes sampling, selecting the best solution from several sampled solutions. We used a sample size of s = 10 for fair comparison."}, {"title": "7 Conclusion", "content": "This paper demonstrates the potential of Large Language Models (LLMs) in addressing the Job Shop Scheduling Problem (JSSP). We introduced a novel supervised dataset with natural language descriptions for JSSP tailored for LLM training. Our results on small-scale JSSP problems indicate that with minimal fine-tuning using the LoRA methodHu et al. [2022], Phi-3 can effectively schedule, sometimes matching or surpassing traditional neural network approaches.\nThe comparative analysis shows that our fine-tuned Phi-3 model performs competitively, achieving an average gap of 8.92 compared to 13.01 for the original L2D method. While the SLJ models (with \u1e9e values of 32, 128, and 256) achieve lower gaps, the Phi-3 model still demonstrates comparable"}, {"title": "8 Limitations and Future Work", "content": "Our investigation of the potential of using LLMs for the JSSP shows promising results but also highlights several limitations and future research directions.\nA key limitation is the computational overhead of fine-tuning LLMs, which remains resource-intensive. Additionally, due to lack of computational resources the generalizability of our results across larger JSSP instances is uncertain. Further research is needed to test LLMs on larger problem sizes. Comparing the performance of larger LLMs and different fine-tuning methods against the relatively small Phi-3 model is also necessary.\nThe interpretability of LLM-generated schedules is another challenge, due to their black-box nature. Additionally, while we used a sampling method to improve performance, exploring different sampling strategies could further enhance LLM-generated schedules.\nFuture research should also explore integrating LLMs with other AI techniques, such as reinforcement learning and graph neural networks, to combine their strengths."}]}