{"title": "Online Collision Risk Estimation via Monocular Depth-Aware Object Detectors and Fuzzy Inference", "authors": ["Brian Hsuan-Cheng Liao", "Yingjie Xu", "Chih-Hong Cheng", "Hasan Esen", "Alois Knoll"], "abstract": "This paper presents a monitoring framework that infers the level of autonomous vehicle (AV) collision risk based on its object detector's performance using only monocular camera images. Essentially, the framework takes two sets of predictions produced by different algorithms and associates their inconsistencies with the collision risk via fuzzy inference. The first set of predictions is obtained through retrieving safety-critical 2.5D objects from a depth map, and the second set comes from the AV's 3D object detector. We experimentally validate that, based on Intersection-over-Union (IoU) and a depth discrepancy measure, the inconsistencies between the two sets of predictions strongly correlate to the safety-related error of the 3D object detector against ground truths. This correlation allows us to construct a fuzzy inference system and map the inconsistency measures to an existing collision risk indicator. In particular, we apply various knowledge- and data-driven techniques and find using particle swarm optimization that learns general fuzzy rules gives the best mapping result. Lastly, we validate our monitor's capability to produce relevant risk estimates with the large-scale nuScenes dataset and show it can safeguard an AV in closed-loop simulations.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past decade, autonomous vehicles (AVs) have attained great development and can be seen on public roads nowadays. However, it is still possible to hear AV accidents, especially in corner cases such as severe weather conditions or the emergence of rare objects [1]. To ensure the safety of AVs and allow their wider deployment, it is important to have run-time monitors that can identify such performance- hindering situations. Correspondingly, regulations and indus- trial standards such as EU AI Act [2] and ISO 21448 [3] also demand the inclusion of monitoring mechanisms in safety-critical autonomous systems.\nIn the literature, in fact, one can find various run-time monitoring techniques. Focusing on planning and control, many work derive collision risks based on ego and traffic information, e.g., driving path deviation or time-to-collision to other agents [4]. These approaches, nonetheless, often assume perfect perception, which is generally not the case. In this work, we relax such an assumption and attempt to identify hazards as early as possible in an AV software stack, such that the controller can trigger a safety maneuver in time. Holding a similar mindset, several studies have suggested to monitor the object detection function. For instance, some propose algorithms that check the spatial or temporal consistency of the set of detected objects [5], [6]. Despite effective, one crucial limitation of the existing work is the relevance between the identified anomalies and the actual safety risk of the AV. For example, these monitors may raise a warning for an object that is far from the AV, which actually poses a low risk. Likewise, they only examine the set of detected objects and ignore potential misses (i.e., false negatives of the object detector), which are more likely risk-relevant.\nHence, in this work, we aim to find safety-related errors of the object detector and use them to characterize an AV's collision risks. Fig. 1 depicts the overall framework. In particular, our previous work presented a design-time safety- focused metric, called uncompromising spatial constraints (USC), which highly correlates with AV collision rates [7]. This work's objective, therefore, is to reproduce it and generate a relevant risk level during operation. To achieve this, there are two challenges:\n\u2022 The first and main challenge lies in the lack of ground- truth labels at run time. To overcome it, we ask the critical question whether employing a separate object re- trieval pipeline and measuring the inconsistencies from the original object detector's predictions can serve as a proxy to the ground truths. Specifically, considering cost factors and the recent breakthrough in monocular depth estimation, we employ the state-of-the-art ZoeDepth [8] and implement image processing techniques to retrieve safety-critical objects. Our key insight, thereby, is an experimental validation that confirms the inconsisten- cies between the two sets of predictions, measured by Intersection-over-Union (IoU) and depth discrepancy, are closely linked to safety-related errors in the 3D object detector when compared to ground-truth data.\n\u2022 With the confirmed correlation, the second challenge is how to associate them with the desired risk quantifier, USC."}, {"title": "II. RELATED WORK", "content": "Monitoring of AV functions and operations has been a long-standing research field [4]. Early work focused on esti- mating collision risks from system dynamics, with some fur- ther demonstrating the estimated risk information can be used to adapt the behavior of the AV [12], [13]. Recently, thanks to the advent of deep learning and computer vision techniques, there are results predicting collision risks in driving scenes based on camera images directly [14], [15]. However, these approaches do not consider the performance of an underlying perception function. In this regard, Grimmett et al. proposed the concept of introspective perception [16], which broadly encompasses the fields of uncertainty quantification, out-of- distribution detection, as well as performance monitoring. We briefly overview these fields in the following.\nUncertainty quantification aims at estimating the confi- dence level of a model in its predictions. Beyond classical methods such as the softmax output [17], Monte Carlo dropout [18], or ensemble models [19], recent studies attempt to formulate uncertainty-aware loss functions and directly estimate the variance of the object detector's predictions [20], [21]. Still, the main challenge in uncertainty quantification is its correlation with the actual model errors or system risks during operation. While research continues to investigate related uncertainty calibration techniques, we take a distinct yet analogous approach to infer system risks from model errors and achieve calibrated output via the adaptable FIS.\nSimilar in not explicitly modelling uncertainty, out-of-distribution (OoD) detection studies attempt to find other possibility to flag a data. For example, Du et al. models class-specific features with Gaussian distributions and generate OoD data to train an auxiliary OoD detector [22]. Recently, Wu et al. find it more effective to store features of training data into hyper-rectangles and use them as permissible ranges for new data [23]. However, it is hard to derive semantic information such as system risks from these approaches. Our usage of a depth estimator can naturally inform the monitor about such risks based on distance.\nAccordingly, performance monitoring approaches check- ing object detector output lie the closest to our work. Existing approaches can be categorized into two groups. The first group makes use of learning-based models. As an example, Rahman et al. [24] train a neural network (NN) aside the original object detector to estimate the typical mean Aver- age Precision (mAP) metric. Although they achieve great regression results, mAP might not be the suitable metric for safety or collision risk indication at each frame, as pointed out by a later study [25]. In addition, there are still concerns about NN robustness and interpretability. Our approach to regress a risk-relevant metric with fuzzy logic helps alleivate these concerns. Lastly, the second group of monitoring approaches uses formal methods and specifies rules such as the maintenance of class labels and the consistency of object locations across image frames [5], [6]. Their main constraint, as introduced, is the scope focusing on the set of predictions from an object detector only. This loses the possibility to check for detection misses. By contrast, our framework handles two sets of predictions, with one focusing on the safety-critical objects to check on the other.\nFinally, our approach is related to data fusion using multiple sensing/perception channels. Readers may refer to a recent survey for more comprehensive understanding [26]. Compared to the mass literature, the crux of our work is the orientation towards risk indication with perception processes using only monocular camera images."}, {"title": "III. THE RISK ESTIMATION FRAMEWORK", "content": "To iterate, we attempt to indicate relevant collision risks from object detection results during run time. In the follow- ing, Sec. III-A introduces how to attain 2.5D objects from a depth map and validates that comparing them with 3D object detection predictions gives valid proxies to ground- truth evaluation. Later, Sec. III-B presents the construction and optimization of the FIS that associate the comparison measures to the output risk level."}, {"title": "A. Depth-based object retrieval and consistency check", "content": "As mentioned, observing its potential to hint the where- abouts of objects, we employ a depth estimator that provides absolute depth maps [8]. Based on the depth maps, a typical object retrieval process consists of two main steps: (i) Apply the Canny edge detecting algorithm to mark the pixels that have large gradient magnitudes [27]; (ii) Find the objects by linking adjacent highlighted pixels into contours [28].\n1) Preprocessing techniques: In our work, we exploit the depth information in driving contexts and implement the following two techniques before applying the typical process to facilitate better object retrieving results. First, we take the inverse of the raw depth map so as to obtain stronger gradients for pixels closer to the ego vehicle. To explain, with $I_1(x,y)$ and $I_2(x, y)$ being two intensity images lin- early normalized from the raw depth map $D(x, y)$ and its inverse $\\frac{1}{D(x,y)}$ respectively, their gradient magnitudes can be written with:\n$\\begin{array}{c}||\\nabla I_{1}(x, y)|| \\propto||\\nabla D(x, y)|| \\frac{1}{D^{2}(x, y)} \\\\||\\nabla I_{2}(x, y)|| \\propto||\\nabla D(x, y)||\\end{array}$\n(1)\n$\\begin{array}{c}\\end{array}$\n(2)\nwhere (x, y) denotes a pixel of the image of size W \u00d7 H. In other words, using the inverse depth map, the same depth value change $||\\nabla D(x, y)||$ will be scaled into a much smaller gradient magnitude when it is distant from the ego vehicle. This naturally allows the Canny edge detection algorithm to focus on the closer field. The property does not exist with the raw depth map. Fig. 2(a) and Fig. 2(b) visualizes the comparison.\nOne may notice that the foreground close to the ego vehi- cle is also highlighted after taking the inverse. Fortunately, as an AV always keeps a similar driving perspective, we can remove it by computing the average of all inverse depth map in the training data and subtract the average from each map. Together with the first technique, this results in a strong emphasis on the close objects, as shown in Fig. 2(c). Lastly, following the mentioned typical process, we can effectively retrieve the most safety-critical objects in a driving scene. Fig. 2(d) shows an example result with bounding boxes fitted.\n2) Measuring alignment with object detector predictions: Using the retrieved safety-critical objects, we now compare them with the predictions of the underlying object detector. Specifically, we focus on the localization attributes and mea- sure how spatially aligned the object detector's predictions are with the safety-critical objects.\nMore formally, we first represent the safety-critical objects with 2.5D information from the retrieved bounding boxes and the original depth map. That is, we denote the set of M safety-critical objects as $S := {\\hat{S}_{m}|m = 1,...,M}$, where each $\\hat{S} \\stackrel{\\text{def}}{=} (x_s, y_s, w_s, h_s, d_s)$, with $(x_s, y_s)$ being the center of the bounding box, $w_s$ the width in the x-axis, $h_s$ the height in the y-axis, and $d_s$ the smallest depth value in the range of the bounding box.\nCorrespondingly, we represent the set of N predicted objects as $P := {P_n|n = 1,..., N}$, where each $P \\stackrel{\\text{def}}{=} (x_P, y_P, w_P, h_P, d_P)$. In practice, a 2D bounding box $P$ without the depth attribute can be obtained from a 3D coun- terpart via perspective projection, and $d_P$ can be concretized as the closest distance from the 3D bounding box to the AV's position.\nNow, to measure the spa- tial alignment between the two sets P and S, we con- sider two factors, namely the overlap of bounding boxes in the image plane and the depth discrepancy in the driving direction. Vi- sualized in the side figure, these two factors have been shown highly related to safety in a recent analysis [29]. In practice, we first run the standard Hungarian algorithm to find the closest pairs in terms of their center distances and, for each pair, compute the commonly used Intersection-over-Union (IoU) as well as a simple relative depth discrepancy (RDD) measure:\n$\\text{RDD}(d_P, d_S) \\stackrel{\\text{def}}{=} \\min(1, \\frac{|d_P - d_S|}{d_S}) \\in [0,1].$\n(3)\nWe additionally apply matching thresholds on the two mea- sures, IoU > a and RDD < \u03b2, so as to avoid actually distant pairs. For any unmatched safety-critical objects, the worst scores are assigned, i.e., IoU = 0 and RDD = 1. Lastly, we aggregate the information in one frame by taking the average of the two measures for all safety-critical objects, denoted as loU and RDD.\n3) Interim validation: To show the computed mea- sures, loU and RDD, are potential for risk inference at the next stage, we now replace S with ground-truth anno- tations G in the above alignment measuring process and computes the correlation coefficient between the measures obtained with the safety-critical objects S and those with the ground truths G."}, {"title": "B. Fuzzy logic-based risk inference", "content": "We first introduce the desired risk quantifier, USC, and then illustrate the approaches to construct an optimal FIS for risk estimation.\n1) A risk-correlating metric as the output target: Tech- nically, one can use any reasonable risk quantifier as the target. For instance, Wang et al. manually assigns discrete risk levels to driving scenes based on the number of nearby traffic agents [14]. Alternatively, Feth et al. calculates time- to-collision to other vehicles as a proxy [15]. In our work, we make use of a safety-oriented object detection metric that has been shown strongly correlated with actual AV collision rates in simulations [7].\nTo briefly introduce, the USC metric reflects how well the object detector's predictions spatially cover the ground- truth objects when seen from the ego vehicle. Using the representation scheme in Sec. III-A, it is computed for a prediction P and a ground truth G as:\n$\\text{USC}(P, G) \\stackrel{\\text{def}}{=} \\text{loG}(P, G) \\times \\text{DR}(d_P, d_G),$\n(4)\nwhere $\\text{loG}(P, G) \\stackrel{\\text{def}}{=} \\text{Area}(P \\cap G) / \\text{Area}(G)$\n(5)\nand $\\text{DR}(d_P, d_G) := \\min(1, d_G/d_P)$.\n(6)\nFor an aggregated indication on an image frame, we similarly find the average of the USC scores for all ground-truth objects, resulting in USC. As USC is a scoring function for safety characterization (i.e., the higher, the better), we shall use the deficit from the full mark (i.e., 1 USC) as the regression target for risk indication.\n2) Constructing the FIS: Having formulated the targeted risk quantifier, we describe how a FIS can be constructed to map the alignment measures, IoU and RDD, to the tar- get, 1 \u2013 USC. Notably, as the direction of depth discrepancy typically matters when estimating driving risks, we take out the absolute value operation from Eq. (3), allowing RDD to range across [-1,1].\nNow, to construct the FIS, we make use of MATLAB'S implementation of Mamdani system [30], [31] and formulate three different approaches. The first one is based on pure knowledge, specifying (i) high-level fuzzy rules and (ii) membership functions that map real values to linguistic terms for the input/output variables. The second one extends on the first one by having the membership functions optimized with a set of training data via local pattern search. Contrarily, the third one uses the training data to learn a rule set based on the crafted membership functions via global particle swarm optimization. More details about data preparation will be given in Sec. IV"}, {"title": "IV. VALIDATION AND DEMONSTRATION", "content": "In this section, we first conduct an open-loop validation with image sequences and then demonstrate our monitor in a closed-loop simulation."}, {"title": "A. Object detector monitoring", "content": "We continue to use the nuScenes dataset [11], the PDG object detector [10], and the ZoeDepth estimator [8] for validation. Specifically, we manually selected 110 scenes that involve dense traffic. In addition, to comprehensively validate our monitor, the 110 scenes are diversified into six different conditions, as shown in Tab. III. The rare- object subset is built from the annotations in a corner case dataset, CODA [32], and the image perturbation algorithms are employed from a comprehensive benchmark [33]. For FIS testing mentioned in Sec. III-B, four scenes are randomly picked from each condition type; The remaining are the training data.\nWe first do an evaluation comparing the objects retrieved from the depth-based pipeline against ground truths. As a benchmark, we take the ordinary 3D object detector. It can be seen in Tab. II that in nominal cases, the 3D object detector performs quite better than the depth-based pipeline, but the margin decreases once we allow for the more imprecise predictions. Notably, the depth-based pipeline appears to be more robust than the object detector in the case of sensor noises and snowy conditions, corresponding to recent findings showing the generalization and robustness potential"}, {"title": "B. AV safeguarding", "content": "Finally, we demonstrate our monitor's capability to safe- guard an AV in unusual situations. To 1 briefly illustrate, a baseline AV has been designed for an automated valet parking (AVP) use case in the Prescan simulator [35]. In each mission, it has to drive from a hand-off zone to a parking slot while navigating through mixed traffic of other vehicles and pedestrians. In addition, to mimic real-world operating conditions, different effects such as low illumination, weather changes, and rare objects are added to the simulations. For instance, Fig. 5 shows a scenario with a cow that has not been seen by the baseline object detector, likely ending up as a false negative.\nNow, to protect the AV against such challenging scenarios, we implement our perception monitor alongside a controller shield on top of the baseline AV stack. Whenever our monitor raises the risk level beyond 0.5, the controller shield starts to decelerate the AV. Fig. 5 depicts that our monitor successfully triggers the shield to decelerate the AV, which would have crashed into the cow. In general, we observe a successful collision avoidance rate beyond 80%. For a more lively demonstration with another scenario, readers can check a full use case video (where our run-time monitor is showcased from 9:16 minute) [36]."}, {"title": "V. CONCLUSION", "content": "In this work, by observing and validating that the align- ment measures between an ordinary object detector's predic- tions and the retrieved objects from a depth-based pipeline can be correlated to the error measures of the object detector against ground truths, we implemented a monitoring frame- work to estimate ego vehicle's collision risks based on the alignment measures. Specifically, we used a risk-correlating metric to optimize a fuzzy inference system for the run-time collision risk estimation. As a result, we are able to infer a relevant collision risk level using low-cost monocular camera images only. Experiments with the nuScenes dataset and a closed-loop simulator demonstrated the effectiveness of our approach.\nOur work opens several interesting directions to explore. For instance, one may use additional signals such as scene flow estimates to find objects that move fast towards the ego vehicle. In addition, intuitive rules regarding object classification results can also be considered, e.g., a car object should normally occupy x pixels at distance y. Lastly, how the monitoring results can be extended with active learning techniques to improve the perception results and hence the AV's performance is under investigation."}]}