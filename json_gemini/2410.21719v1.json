{"title": "On the Statistical Complexity of Estimating VENDI Scores from Empirical Data", "authors": ["Azim Ospanov", "Farzan Farnia"], "abstract": "Reference-free evaluation metrics for generative models have recently been studied in the machine\nlearning community. As a reference-free metric, the VENDI score [1] quantifies the diversity of generative\nmodels using matrix-based entropy from information theory. The VENDI score is usually computed\nthrough the eigendecomposition of an n\u00d7n kernel matrix for n generated samples. However, due to the\nhigh computational cost of eigendecomposition for large n, the score is often computed on sample sizes\nlimited to a few tens of thousands. In this paper, we explore the statistical convergence of the VENDI\nscore and demonstrate that for kernel functions with an infinite feature map dimension, the evaluated\nscore for a limited sample size may not converge to the matrix-based entropy statistic. We introduce an\nalternative statistic called the t-truncated VENDI statistic. We show that the existing Nystr\u00f6m method\nand the FKEA approximation method for the VENDI score will both converge to the defined truncated\nVENDI statistic given a moderate sample size. We perform several numerical experiments to illustrate\nthe concentration of the empirical VENDI score around the truncated VENDI statistic and discuss how\nthis statistic correlates with the visual diversity of image data.", "sections": [{"title": "Introduction", "content": "The widespread use of generative artificial intelligence (AI) has underscored the need for accurate evaluation\nof generative models based on the quality and diversity of their generated samples. Users of generative AI\noften have access to multiple models trained using different algorithms and datasets, necessitating efficient\nevaluation mechanisms to choose the best model from the available options. The applicability of a model\nevaluation approach depends on factors such as the required generated sample size, computational cost, and\nthe availability of reference data. Several recent studies on evaluating generative models have introduced\nassessment methods that relax the requirements for data and computational resources.\nSpecifically, to enable the evaluation of generative models in settings without reference data, the recent\nliterature has focused on reference-free evaluation scores, which remain applicable in the absence of a reference\ndataset. The VENDI score [1] is one such reference-free metric that quantifies the diversity of generated data\nusing the entropy of a kernel similarity matrix formulated for the generated samples. As analyzed by [1] and\n[2], the reference-free assessment of the VENDI score can be interpreted as an unsupervised identification of\nclusters within the generated data, followed by the calculation of the entropy of the detected cluster variable.\nDue to its flexibility and adaptability to various domains, the VENDI score has been applied to measure the\ndiversity of samples across different modalities, including image, text, and video data.\nWhile the VENDI score does not require reference samples, its computational cost increases rapidly with\nthe number of generated samples n. In practice, calculating the entropy of the eigenvalues of the n\u00d7n\nkernel matrix involves performing an eigendecomposition, which requires O(n\u00b3) computations. As a result,\nthe computational load becomes substantial for a large sample size n, and the VENDI score is typically\nevaluated for sample sizes limited to a few tens of thousands. Consequently, the exact VENDI score, as\ndefined in its original formulation, is usually not computed for sample sizes exceeding 20,000. A key question"}, {"title": "Preliminaries", "content": "Consider a generative model G that generates samples from a probability distribution Px. To conduct a\nreference-free evaluation of the model, we suppose the evaluator has access to n independently generated\nsamples from Px, denoted by x1,...,xn \u2208 X. The assessment task is to estimate the diversity of generative\nmodel G by measuring the variety of the observed generated data, x1,... xn. In the following subsections, we\nwill discuss kernel functions and their application to define the VENDI diversity score."}, {"title": "Kernel Functions and Matrices", "content": "Following the standard definition, k : X \u00d7 X \u2192 R is called a kernel function if for every integer n and\ninputs x1,...,xn \u2208 X, the kernel similarity matrix K = [k(xi, xj)] 1\u2264i,j\u2264n is positive semi-definite. It can be\nseen that this definition of kernel function is equivalent to the existence of a feature map \u00a2 : X \u2192 Rd such\nthat for every x, x' \u2208 X we have the following where (,) denotes the standard inner product in the Rd space:\n$k(x,x') = (\u03c6(x), \u03c6(x'))$ \nIn this work, we study the evaluation using two types of kernel functions: 1) finite-dimension kernels where\ndimension d is finite, 2) infinite-dimension kernels where there is no feature map satisfying (1) with a\nfinitie d value. A standard example of a finite-dimension kernel is the cosine similarity function where\n$cosine(x) = x/||x||_2$. Also, a widely-used infinite-dimension kernel is the Gaussian (RBF) kernel with\nbandwidth parameter \u03c3 > 0 defined as\n$k_{Gaussian (\u03c3)}(x, x') := exp(-||x - x'||^2 / (2\u03c3^2))$ \nBoth the mentioned kernel examples belong to normalized kernels which require k(x,x) = 1 for every x,\ni.e. the feature map (x) has unit Euclidean norm for every x. Given a normalized kernel function, the\nnon-negative eigenvalues of the normalized kernel matrix K for n points x1,... xn will sum up to 1, which\nmeans that they form a probability model."}, {"title": "Matrix-based Entropy Functions and VENDI Score", "content": "Given a PSD matrix A \u2208 Rdxd with a unit trace Tr(A) = 1, A's eigenvalues form a probability model.\nThe order-a Renyi entropy of matrix A is defined using the order-a entropy of its eigenvalues as\n$H_\u03b1(A) := \\frac{1}{1-\u03b1} log(\\sum_{i=1}^d \u03bb_i^\u03b1)$ \nSpecifically, in the case of a = 1, the above definition reduces to the Shannon entropy of the eigenvalues as\n$H_1(A) := \\sum_{i=1}^d \u03bb_i log(1/\u03bb_i)$. Applying the above standard definitions to the normalized kernel matrix K,\n[10] define the order-a VENDI score for samples x1,...,xn as\n$VENDI_\u03b1(x_1,...,x_n) := exp(H_\u03b1(K))$ \nWe note that in the case of a = 2, the definition of VENDI2 is identical to the RKE score proposed by [2].\nIn this particular case, the score can be formulated using the Frobenius norm of the kernel matrix, which we\ndenote by || ||F,\n$RKE(x_1,..., x_n) = VENDI_2(x_1,...,x_n) = ||K||_F^{-2}$"}, {"title": "Statistical Interpretation of VENDI Score", "content": "To derive the VENDI statistic that is supposed to be estimated by the VENDI score, we review the\nfollowing discussion from [23, 1, 2]. First, note that the normalized kernel matrix K, whose eigenvalues are\nused in the definition of VENDI score, can be written as:\n$\\frac{1}{n} K = \\frac{1}{n} \u03a6\u03a6^T$ \nwhere \u03a6 \u2208 Rn\u00d7d is an n \u00d7 d matrix whose rows are the feature presentations of samples, i.e., \u03c6(x1), ..., \u03a6(xn).\nTherefore, the normalized kernel matrix K shares the same non-zero eigenvalues with Cx, in which the\nmultiplication order is flipped. Note that Cx is defined as the empirical kernel covariance matrix Cx:\n$C_x := \\frac{1}{n}\\sum_{i=1}^n \u03c6(x_i) \u03c6(x_i)^T = \\frac{1}{n} \u03a6^T\u03a6$ \nAs a result, the empirical covariance matrix Cx and kernel matrix K have the same non-zero eigenvalues\nand therefore share the same matrix-based entropy value for any order a: Ha(K) = Ha(Cx). Therefore, if\nwe consider the underlying kernel covariance matrix Cx = Ex~Px [\u03c6(x)\u03c6(x)], we can define the VENDI\nstatistic as follows.\nDefinition 1. Given data distribution Px, we define the order-a VENDI statistic, VENDI(Px), using the\nmatrix-based entropy of the underlying kernel covariance matrix Cx = Ex~Px [\u03c6(x)\u03c6(x)] as\n$VENDI(P_X) := exp(H_\u03b1(C_X))$ \nIn the next sections, we study the complexity of estimating the above VENDI statistic from a limited\nnumber of samples."}, {"title": "Statistical Convergence of Vendi Scores in Finite-Dimension Ker- nels", "content": "Given the definitions of the VENDI score and the VENDI statistic, a relevant question is how many\nsamples are required to accurately estimate the VENDI statistic using the VENDI score. To address this\nquestion, we first prove the following concentration bound on the vector of ordered eigenvalues [\u03bb1,..., \u03bb\u03b7]\nof the kernel matrix for a normalized kernel function. We defer the proof of the theoretical results to the\nAppendix.\nTheorem 1. Consider a normalized kernel function k satisfying k(x,x) = 1 for every x \u2208 X. Let \u00c2n be the\nvector of sorted eigenvalues of the normalized kernel matrix K for n independent samples x1,...,xn ~ Px.\nIf we define x as the vector of sorted eigenvalues of underlying covariance matrix Cx, then for every\n\u03b4 \u2265 exp((2 \u2013 n)/8), the following inequality holds with probability at least 1 \u2013 \u03b4:\n$|| \\hat{\u03bb}_n - \\tilde{\u03bb} ||_2 \u2264 \\sqrt{\\frac{32 log(2/\u03b4)}{n}}$\nNote that in calculating the subtraction \u00c2n \u2013 \u00c3, we add |d \u2013 n| zero entries to the lower-dimension vector, if\nthe dimension of vectors \u00c2n and \u0101 do not match.\nThe above theorem implies the following corollary on a dimension-free convergence guarantee for order-a\nVENDI score with a > 2."}, {"title": "Truncated Vendi Statistic and its Estimation via Proxy Kernels", "content": "Corollaries 1,2 demonstrate that if the VENDI score order a is greater than 2 or the kernel feature map\ndimension d is finite, then the VENDI score can converge to the underlying VENDI statistic with n = O(d)\nsamples. However, the theoretical results do not apply to an order 1 < a < 2 when the kernel map dimension\nis infinite, e.g. the original order-1 VENDI score [1] with a Gaussian kernel. Our numerical observations\nindicate that a standard sample size below 20000 could be insufficient for the convergence of order-1 VENDI\nscore (Figure 1). To address this gap, here we define the truncated VENDI statistic and then show the\nexisting kernel approximation algorithms for VENDI score concentrate around this modified statistic.\nDefinition 2. Consider data distribution Px and its underlying kernel covariance matrix Cx = Ex~Px [\u03c6(x)\u03c6(x)].\nThen, for parameter t > 1, consider the top-t eigenvalues of Cx: \u03bb1 > \u03bb2 > \u2026 > \u03bbt. Define St = \u03a3i=1 \u03bbi,\nand consider the probability sequence Xi = \u03bbi / St + 1 - St /t for i = 1,...,t. Then, we define the order-a t-truncated\nVENDI statistic as\n$VENDI_\u03b1^{(t)}(P_X) := exp(\\frac{1}{1-\u03b1} log(\\sum_{i=1}^t \\tilde{\u03bb}_i^\u03b1))$ \nAccording to the above definition, we find the probability model with the minimum l2-norm difference\nfrom the t-dimensional vector [\u03bb1,..., \u03bbt] including only the top-t eigenvalues. Then, we use the order-a\nentropy of the probability model to define the order-a t-truncated VENDI statistic. Our next result shows\nthat this statistic can be generally estimated using n = O(t) samples.\nTheorem 2. Consider the setting in Theorem 1. Then, for every d > exp((2 \u2013 n)/8), the difference between\nthe t-truncated VENDI statistic and its empirical estimation from samples x1,...,xn is bounded as follows\nwith probability at least 1 \u2013 \u03b4:\n$|VENDI_\u03b1^{(t)}(x_1,...,x_n) - VENDI_\u03b1^{(t)}(P_X)| \u2264 \\sqrt{\\frac{32 max{1, t^{2-\u03b1} } log(2/\u03b4)}{n}}$\nAs implied by Theorem 2, the t-truncated VENDI statistic can be estimated using O(t) samples, i.e. the\ntruncation parameter t plays the role of the bounded dimension of a finite-dimension kernel map. Our next"}, {"title": "Numerical Results", "content": "We evaluated the convergence of the VENDI score, the truncated VENDI score, and the proxy VENDI\nscores using the Nystr\u00f6m method and FKEA in our numerical experiments. We provide a comparative analysis\nof these scores across different data types and models, including image, text, and video. In our experiments,\nwe considered the cosine similarity kernel as a standard kernel function with a finite-dimension map and\nthe Gaussian (RBF) kernel as a kernel function with an infinite-dimension feature map. In the experiments\nwith Gaussian kernels, we matched the kernel bandwidth parameter with those chosen by [2, 4] for the same\ndatasets. We used 20,000 number of samples per score computation, consistent with standard practice in the\nliterature. To investigate how computation-cutting methods compare to each other, in the experiments we\nmatched the truncation parameter t of our defined t-truncated VENDI score with the Nystr\u00f6m method's\nhyperparameter on the number of randomly selected rows of kernel matrix and the FKEA's hyperparameter\nof the number of random Fourier features. The VENDI and FKEA implementations were adopted from the\ncorresponding references' GitHub webpages, while the Nystr\u00f6m method was adopted from the scikit-learn\nPython package."}, {"title": "Convergence Analysis of VENDI scores", "content": "To assess the convergence of the discussed VENDI scores, we conducted experiments on four datasets\nincluding ImageNet and FFHQ[24] image datasets, a synthetic text dataset with 400k paragraphs generated\nby GPT-4 about 100 randomly selected countries, and the Kinetics video dataset [25]. Our results, presented\nin Figure 2, show that for the finite-dimension cosine similarity kernel the VENDI score converges rapidly to\nthe underlying value and the proxy versions including truncated and Nystr\u00f6m VENDI scores were almost\nidentical to the original VENDI score. This observation is consistent with our theoretical results on the\nconvergence of VENDI scores under finite-dimension kernel maps. On the other hand, in the case of infinite\ndimension Gaussian kernel, we observed that the VENDI\u2081 score did not converge using 20k samples and the\nscore value kept growing with a considerable rate. However, the t-truncated VENDI score with t = 10000\nconverged to its underlying statistic shortly after 10000 samples were used. Consistent with our theoretical\nresult, the proxy Nystr\u00f6m and FKEA estimated scores with their rank hyperparameter matched with t also\nconverged to the limit of the truncated VENDI scores. The numerical results show the connection between\nthe truncated VENDI statistic and the existing kernel-based algorithms for approximating the VENDI score."}, {"title": "Correlation between the truncated VENDI statistic and diversity of data", "content": "We performed experiments to test the correlation between the truncated VENDI statistic and the ground-\ntruth diversity of data. To do this, we applied the truncation technique to the FFHQ-based StyleGAN3[26]\nmodel and the ImageNet-based StyleGAN-XL[27] model and simulated generative models with different\nunderlying diversity by varying the truncation technique. Considering the Gaussian kernel, we estimated\nthe t-truncated VENDI statistic with t = 10000 by averaging the estimated t-truncated VENDI scores over\n5 independent datasets of size 20k where the score seemed to converge to its underlying value. Figure 3\nshows how the estimated statistic correlates with the truncation parameter for order-a Vendi scores with\na = 1, 1.5, 2. In all these experiments, the truncated VENDI statistic correlated with the underlying diversity\nof the models. In addition, we plot the proxy Nystr\u00f6m and FKEA proxy VENDI values computed using\n20000 samples which remain close to the estimated t-truncated statistic. These empirical results suggest that\nthe t-truncated VENDI statistic with Gaussian kernel can be used to evaluate the diversity of generated\ndata. Also, the Nystr\u00f6m and FKEA methods were both computationally efficient in estimating the truncated\nVENDI statistics from limited generated data. We defer the presentation of the additional numerical results\non the convergence of VENDI scores with different orders and kernel functions to the Appendix."}, {"title": "Conclusion", "content": "In this work, we investigated the statistical convergence properties of VENDI diversity scores estimated\nfrom empirical samples. We highlighted that, due to the high computational complexity of the score for\ndatasets larger than a few tens of thousands of generated data points, the score is often calculated using\nsample sizes below 20,000. We demonstrated that such limited sample sizes do not pose a problem for\nstatistical convergence as long as the kernel feature dimension is bounded. Conversely, our numerical results\nshowed a lack of convergence to the VENDI statistic when using an infinite-dimensional kernel map, such as\nthe Gaussian kernel. To address this issue, we introduced the truncated VENDI statistic as an alternative\ntarget statistic. We further showed that existing methods, such as Nystr\u00f6m and FKEA, for approximating\nVENDI scores concentrate well around this truncated VENDI statistic. A promising direction for future\nresearch is to explore the relationship between other kernel approximation techniques and the truncated\nVENDI statistic. Moreover, a comprehensive analysis of the computational-statistical trade-offs involved in\nestimating the VENDI score remains an open question for future work."}, {"title": "Proofs", "content": "To prove the theorem, we will use the following lemma followed from [28, 29].\nLemma 1 (Vector Bernstein Inequality [28, 29]). Suppose that 21,..., Zn are independent and identically\ndistributed random vectors with zero mean E[zi] = 0 and bounded l2-norm ||zi||2 \u2264 c. Then, for every\n0 < \u20ac < c, the following holds\n$P(| \\sum_{i=1}^n Zi || \u2265 \u20ac) \u2264 exp(\\frac{-ne^2}{ \\frac{2}{c^2} + \\frac{8}{c^2}})$$\nWe apply the above Vector Bernstein Inequality to the random vectors (x1) \u25ca \u00a2(x1),...,\u00a2(x1) \u25ca (x1)\nwhere \u25ca denotes the Kronecker product. To do this, we define vector vi = $(xi) & \u00a2(xi) \u2013 Ex~P [(x) & \u03c6(x)]$\nfor every i. Note that vi is, by definition, a zero-mean vector and also for every x we have the following for\nthe normalized kernel function k:\n$||\u03c6(x) & \u03c6(x)||_2 = ||\u03c6(x)||_2 . ||\u03c6(x)||_2 = k(x,x) \u2022 k(x, x) = 1$\nThen, the triangle inequality implies that\n$||Vi||_2 \u2264 ||\u03c6(xi) & \u03a6(xi)||_2 + ||Ex~P [(x) & \u03c6(x)] ||_2 \u2264 ||\u03c6(xi) & (xi)||_2 + Ex\u223cP[||\u00a2(x) & \u03c6(x)||_2] = 2$\nAs a result, the Vector Bernstein Inequality leads to the following for every 0 < \u20ac <\u2264 2:\n$P(| \\sum_{i=1}^n \u03c6(xi) & \u03c6(xi) \u2013 Exp[\u03c6(x) \u03c6(x)]||_2 \u2265 \u20ac) \u2264 exp(\\frac{8- ne^2}{32})$\nOn the other hand, note that f(x) (x) is the vectorized version of rank-1 matrix (x)(x), which shows\nthat the above inequality is equivalent to\n$P([\\sum_{i=1}^n \u03c6(x)\u03c6(x)] \u2013 Exp[\u03c6(x)\u03c6(x)] ||_F \u2265 \u20ac) \u2264 exp(\\frac{8- ne^2}{32})$\n$P(||C_x - Cx ||_F \u2265 \u20ac) \u2264 exp(\\frac{8- ne^2}{32})$$\nSubsequently, we can apply the Hoffman-Wielandt inequality which shows that for the sorted eigenvalue\nvectors of Cx (denoted by An in the theorem) and Cx (denoted by A in the theorem) we will have\n$|| \u00c2n - \u00c3||_2 \u2264 ||Cx - CX||_F$, which together with the previous inequality leads to\n$P(|| \u00c2n - \u00c3||_2 \u2265 \u20ac) \u2264 exp(\\frac{8- ne^2}{32})$\nIf we define d = exp((8 \u2013 ne\u00b2)/32) that implies \u0454 \u2264 $\\sqrt{\\frac{32 log(2/\u03b4)}{n}}$, we obtain the following for every d >\nexp((2 - n)/8) (since we suppose 0 \u2264 \u20ac \u2264 2)\n$P(|| \u00c2n - \u00c3||_2 \u2265 \\sqrt{\\frac{32 log(2/\u03b4)}{n}}) < \u03b4$\n$P(|| \u00c2n - \u00c3||_2 \u2265 \\sqrt{\\frac{32 log(2/\u03b4)}{n}}) \u2265 1 - \u03b4$\nwhich completes the proof."}]}