{"title": "Scaling Laws with Hidden Structure", "authors": ["Charles Arnal", "Clement Berenfeld", "Simon Rosenberg", "Vivien Cabannes"], "abstract": "Statistical learning in high-dimensional spaces is challenging without a strong underlying data structure. Recent advances with foundational models suggest that text and image data contain such hidden structures, which help mitigate the curse of dimensionality. Inspired by results from nonparametric statistics, we hypothesize that this phenomenon can be partially explained in terms of decomposition of complex tasks into simpler subtasks. In this paper, we present a controlled experimental framework to test whether neural networks can indeed exploit such \u201chidden factorial structures.\" We find that they do leverage these latent patterns to learn discrete distributions more efficiently, and derive scaling laws linking model sizes, hidden factorizations, and accuracy. We also study the interplay between our structural assumptions and the models' capacity for generalization.", "sections": [{"title": "Introduction", "content": "Context and motivations The ability of artificial intelligence systems to solve highly complex tasks by extracting information from a vast amount of data remains poorly understood. On the one hand, the curse of dimensionality states that learning becomes virtually impossible as the data dimension grows large. On the other hand, it has been now long observed that these systems can develop a fine understanding of even high-dimensional data. This is for instance the case for Large Languege Models (LLMs), which process lists of tokens whose different combinations can easily exceed $10^{80}$ in number (Brown et al., 2020).\nIt is well-known that the curse of dimensionality can be overcome when the data exhibits a lower-dimensional underlying structure, and if the learning procedure can adapt to this intrinsic dimension. Many structural assumptions have been proposed by statisticians, such as the existence of a low-dimensional manifold on which the data lies (Fefferman et al., 2016), smoothness of the functional to learn (Caponnetto and De Vito, 2006) or sparsity of the later (Hristache et al., 2001). While these assumptions allow for the derivation of theorems, they are arguably too simplistic to explain the current successes of machine learning (Cabannes and Vigogna, 2023). A recent line of research has argued that the factorization of a complex task into simpler sub-tasks could explain the ability to learn such a complex task efficiently (Parascandolo et al., 2018; Arora and Goyal, 2023; Ahuja and Mansouri, 2024; Cagnetta et al., 2024). We approach this work from this perspective.\nContributions We adopt a discrete data framework, which is a reasonable choice given the two following considerations: first, the frequently discrete nature of data (in particular text data); and second, the operational basis of LLMs, which rely on tokenized inputs and are of particular interest in contemporary research, and one of the main motivations behind this work.\nWithin this framework, we propose a factorization-based model where both the input and output spaces are decomposed into a products of small unknown factors, and where the tasks happen factor-wise. This data structure is motivated by examples detailed further below. Our goal is to test whether neural networks can leverage hidden factorial structure to learn more efficiently, both in terms of computational and statistical complexity. This approach departs from continuous models and their usual structural assumptions and gives insights into the impressive performances of current state-of-the-art models on complex discrete data.\nDue to the lack of theoretical tools at hand (see, e.g., Allen-Zhu and Li, 2024, for discussion on the matter), we focus on an empirical approach through a controlled exploratory study that connects the data structure and the performance a Multilayer Perceptron (MLP). The reason behind the choice of MLPs is that they serve as the essential building blocks of many modern machine learning techniques, and in particular of LLMs, where they represent the majority of the model's parameters. This way, we hope to shed new light on some of the core principles behind LLMs learning. We formulate our findings in term of scaling laws (Kaplan et al., 2020), by extensively studying how the performances of our model evolve with changes in train data size, number of parameters, compute resources, and complexity of the hidden data structure."}, {"title": "Setting", "content": "In this paper, we make Neural Networks (NNs) learn discrete conditional distributions $p(y|x)$ and we analyze both theoretically and experimentally how specific structural assumptions about these distributions can make the task easier.\nMore precisely, we consider a set of inputs $X$ of cardinality $N \\in \\mathbb{N}$ and a set of outputs $y$ of cardinality $M \\in \\mathbb{N}$. We assume that the input/output pairs are generated from a joint distribution $(X, Y) \\sim p$ on $X \\times Y$, and we task a NN with learning the conditional distribution $p(y|x)$ for each input $x \\in X$. The quality of a learned estimator $\\hat{p}$ is measured through the Kullback-Leibler divergence, defined as\n$\\mathcal{L}(\\hat{p}) = \\mathbb{E}_{(X,Y) \\sim p} \\left[ \\log \\frac{p(Y|X)}{\\hat{p}(Y|X)} \\right]$\nFrom an optimization point of view, this loss is equivalent to the cross-entropy loss used by practitioners. Indeed, the loss $\\mathcal{L}$ is the excess risk of the cross-entropy loss, i.e. the cross-entropy loss minus its minimizer.\nIn order to learn from discrete data, we embed our problem in a continuous space $\\mathbb{R}^d$ with two (learnable) embeddings:\n$e : X \\rightarrow \\mathbb{R}^d, \\qquad u: Y \\rightarrow \\mathbb{R}^d$.\nIn addition to the embeddings, the NN learns a transformation $F: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ of the embedding space. The final estimator is parameterized through the softmax function\n$\\hat{P}(y|x) = \\frac{\\exp(u_y^T F(e_x))}{\\sum_{y'\\in Y} \\exp(u_{y'}^T F(e_x))}$,\nwhere we use the abbreviations $e_x := e(x)$ and $u_y := u(y)$.\nThis setting, and in particular the fact that $e_x$ and $u_y$ belong to the same embedding space, was designed to closely match current state-of-the-art architectures, where residual connections are commonly used (He et al., 2015).\nAmong all possible conditional density functions $p(y|x)$, we are particularly interested in those that satisfy certain structural conditions, which we call and which we define and motivate thereafter. Our goal is to establish whether NNs can efficiently leverage those assumptions to better learn and represent $p(y|x)$ in two slightly different settings: one in which the embeddings $e_x$ are learned, and one in which fixed, factorization-compatible embeddings are used."}, {"title": "The Factorization Hypothesis", "content": "Our factorization hypothesis assumes that both the input space $X$ and the output space $y$ can be decomposed into $k \\in \\mathbb{N}$ and $l \\in \\mathbb{N}$ unknown factors respectively. In other terms,\n$x \\simeq \\prod_{i \\in [k]} X_i \\qquad \\text{and} \\qquad y \\simeq \\prod_{j \\in [l]} Y_j$,\nwhere each $X_i$ and $Y_j$ is a discrete space containing $p_i = |X_i|$, respectively $q_j = |Y_j|$, elements. By \u201cunknown\", we mean that the mappings $X \\simeq \\prod_{i \\in [k]} X_i$ and $y \\simeq \\prod_{j \\in [l]} Y_j$ are not given to the agent (the NN) trying to learn the conditional distribution $p(y|x)$. We further assume that for all index $j \\in [l]$, there exists a subset $I_j \\subseteq [k]$ such that the conditional probability distribution $p(y|x)$ factors into\n$p(y|x) = \\prod_{j \\in [l]} p(y_j | pa_j)$,\nwhere\n$pa_j = (x_i)_{i \\in I_j} \\in \\prod_{i \\in I_j} X_i$.\nHere, $x_i \\in X_j$ denotes the i-th coordinate of x in its factor decomposition (respectively $y_j \\in Y_j$ is the j-th coordinate of y), hence $pa_j$ is the set of coordinates that influence the factor $y_j$. In other words, the Factorization Hypothesis (FH) states that the coordinates $y_j$ of y in the decomposition are independent given the input x, and that each $y_j$ only depends on the subset $pa_j$ of the input coordinates. This model allows for structures where only a (potentially small)"}, {"title": "Input Embeddings", "content": "This subsection discusses two possible embeddings of our discrete problem in a continuous space, which are then used as inputs to the transform $F$: a learned encoding, and a fixed factorization-compatible embedding. Both are defined further below.\nLearned embedding In this classic setting, used e.g. in most LLMs, (see Brown et al., 2020; Touvron et al., 2023), each discrete element $x \\in X$ is mapped to a learned vector $e_x \\in \\mathbb{R}^d$. The embeddings are typically initialized as random Gaussian vectors, and learned during training jointly with the embedding transform $F$ and the output embedding u. Though very natural, this setting lacks a crucial property: as no structure is enforced on the embeddings, a trained NN has no hope of correctly generalizing to an input x that was not part of its training set, and whose embedding would simply be equal to its random initialization. This stands in contrast to the factorization-compatible embeddings described below.\nFactorization-compatible embedding In this setting, we assume that an oracle or a previously trained NN provides us with an embedding $e_x$ that is adapted to the (unknown) factorization of $X$ in the following sense:\n$e_x = \\sum_{i \\in [k]} e^i(x_i)$,\nwhere $E_i \\in \\mathbb{R}^{d \\times p_i}$ is some fixed matrix, $1_{x_i} \\in \\mathbb{R}^{p_i}$ is the one-hot encoding of $x_i \\in X_i$ and $e^i(x_i) = E_i 1_{x_i} \\in \\mathbb{R}^d$. In other words, each discrete element $x_i \\in X_i$ is mapped to some fixed, arbitrary embedding $e^i(x_i)$. The total embedding $e_x$ of x is then the sum of the embeddings of its factors.\nNote that when $d > \\sum_{i \\in [k]} p_i$, any matrix $[E_1 \\dots E_k]$ with coefficients sampled from continuous distributions will almost surely be of rank $\\sum_{i \\in [k]} p_i$. As a consequence, the"}, {"title": "Theoretical Analysis", "content": "We consider both the computational and statistical difficulty of learning a factorizable data distribution \u2013that is to say both the minimal theoretical number of parameters needed in an MLP and the minimal theoretical number of training points needed for this task."}, {"title": "Approximation Complexity", "content": "We aim to assess the impact of our factorization hypothesis on the computational complexity involved in approximating the conditional distributions $p(y|x)$ for our models.\nConsider first the learned embeddings defined in the previous subsection. Let $E \\in \\mathbb{R}^{d \\times N}$ and $U \\in \\mathbb{R}^{d \\times M}$ be the learned embedding matrices that map the one-hot encoding of x to $e_x$, respectively the one-hot encoding of y to $u_y$, and define the matrix $G := F(E) \\in \\mathbb{R}^{d \\times N}$, where the transform $F$ is applied column-wise (i.e. to each $e_x$). Note that as each token x can be mapped to any vector $e_x \\in \\mathbb{R}^d$ by the embedding process, the transform $F$ adds nothing to the expressivity of the overall model (though it might impact the training dynamics). Constrained by the functional form of our estimator stated in (3), we want to represent $p(y|x)$ as proportional to $\\exp(u_y^T F(e_x))$. This is equivalent to representing the matrix $A := (\\log p(y|x))_{y,x} \\in \\mathbb{R}^{M \\times N}$ as a matrix product\n$A = U^T G + \\text{column-wise constant}$,\nwhere the column-wise constant accounts for the renormalizing factor from (3). Under (FH), an exact solution is given by\n$G =\\left[\\begin{array}{c}(log\\, p(y_1|x))_{y_1 \\in Y_1, x\\in X}\\\\\n\\dots\\\\(log\\, p(y_l|x))_{y_l \\in Y_l, x\\in X}\\end{array}\\right] \\in \\mathbb{R}^{Q\\times N}$\nand\n$U =\\left[\\begin{array}{c}(1_{y_1})_{y \\in Y}\\\\\n\\dots\\\\(1_{y_l})_{y \\in Y}\\end{array}\\right] \\in \\mathbb{R}^{Q\\times M}$"}, {"title": "Statistical Complexity", "content": "The sample complexity of learning a given distribution $p(y|x)$ (for the KL / cross entropy loss) without any structural assumption at accuracy $\\epsilon > 0$ is given by (Canonne, 2020):\n$\\frac{1}{\\epsilon} \\times |X|\\times |Y|$.\nBecause the (hidden) factorization assumption transforms the learning task into $l$ independent (but unknown) learning tasks, we conjecture the optimistic bound $\\chi/\\epsilon$ on the sample complexity, where\n$\\chi = \\sum_{j=1}^l q_j \\times |pa_j|$,\nwhich is always smaller than (6) (and often exponentially smaller). The bound is optimistic because it is computed as if the factorization was known in advance; this can be heuristically justified by the fact that for most classical tasks in nonparametric statistics, the rate of estimation depends directly on structural assumptions (i.e. multi-index models, manifold hypothesis, etc) even when one does not know the precise instance of these assumptions (i.e. indices in the multi-index models, location of the manifold, etc)."}, {"title": "Experiments", "content": "We explore the practical implications of our discrete factorized structure (FH) on the performance of MLPs. We link these performances to the parameters of the factorization and to the various hyperparameters of our models, highlighting that MLPs perform better under (FH) while requiring fewer parameters and at a lower computational cost. Our code is available at https://github.com/facebookresearch/pal/."}, {"title": "Experimental Design", "content": "We describe in this section the data generation process, namely our procedure for generating distributions $p(y, x)$"}, {"title": "Approximation Complexity", "content": "where\n$Q = \\sum_{j \\in [l]} q_j$\nand $(1_{y_j})_{y \\in Y} \\in \\mathbb{R}^{|Y_j|\\times M}$ is the matrix such that its column indexed by $y \\in Y$ is the one-hot encoding of the factor $y_j$ of y in $Y_j$. This shows that in the learned embedding setting, the distribution can be perfectly approximated using as embedding dimension $d = Q$ and $(M + N) \\cdot d$ parameters. In fact, another exact solution to (5), which mixes the first one with a slightly altered version of it (see the Supplementary Materials for details), shows that the same property holds for $d = \\chi$, where\n$\\chi = \\sum_{j \\in [l]} \\min{|pa_j|, q_j},$\nand\n$|pa_j| = \\prod_{i \\in I_j} |X_i|$.\nThis showcases how the factorization hypothesis may enable an exponential gain regarding the computational difficulty of the task at hand, going from $d = \\min(M, N)$ when no structure is assumed on the data, to $d$ potentially as small as $\\min{\\{ \\log_2 M, \\log_2 N\\}}$.\nNote that while the quantity $\\sum_{j \\in [l]} \\min{\\{|pa_j|, q_j\\}}$ bounds the rank of A, it is easy to show that A having a small rank does not imply the existence of a factorization. Simple counter-examples are provided in the Supplementary Materials.\nLet us now consider the case of factorization-compatible embeddings. As before, A must be expressed as a product $UTF(E)$ (up to some column-wise additive constant), but now we have the constraint that E is of the form (4). To ease notations, let us set\n$P = \\sum_{i \\in [k]} P_i, \\qquad P = \\sum_{j \\in [l]} |pa_j|$.\nAssume that the matrix $[E_1 \\dots E_k]$ is invertible, which is generically the case as soon as $d > P$ (as noted in Subsection 2.2), and let T be its inverse, which maps $e_x$ to the product $(1_{x_1}, \\dots, 1_{x_k}) \\in \\mathbb{R}^P$ of the one-hot encodings of the factors $x_i$ of x. Let also\n$\\Gamma: \\mathbb{R}^P \\rightarrow \\mathbb{R}^P$\n$\\Gamma: (1_{x_1}, \\dots, 1_{x_l}) \\mapsto (1_{pa_1}, \\dots, 1_{pa_l})$\nmaps $(1_{x_1}, \\dots, 1_{x_l})$ to the corresponding product $(1_{pa_1}, \\dots, 1_{pa_l})$ of one-hot encoding of the parents. The map $\\Gamma$ is non-linear, but it can be represented by a single feedforward layer (whose number of parameters is roughly $P \\times P$). Then an exact solution can be expressed as\n$F: T \\circ e \\mapsto \\Gamma\\circ T\\circ e$"}, {"title": "Statistical Complexity", "content": "(where some padding can be added to ensure that the ambient dimension remains constant at each step) and\n$U =\\left[\\begin{array}{c}(log\\, p(y_1|z_1))_{z_1 \\in pa_1, y\\in Y}\\\\\n\\dots\\\\(log\\, p(y_l|z_l))_{z_l \\in pa_l, y\\in Y}\\end{array}\\right] \\in \\mathbb{R}^{P\\times M}$,\nwhere the rows of the submatrix $(log\\, p(y_j|z_j))_{z_j \\in pa_j, y\\in Y} \\in \\mathbb{R}^{|pa_j|\\times M}$ are indexed by the elements $z_j$ of $pa_j$. This shows that while the situation is slightly more complex than in the case of learned embeddings, we can still bound the number of parameters and the embedding dimension needed to approximate the conditional distribution in term of factorization characteristics."}, {"title": "Experimental Design", "content": "satisfying (FH), and then describe our chosen MLP architecture. The precise values of the parameters of our data generation process, of the model's hyperparameters and of their associated default values can be found in Table 1 in the Supplementary.\nData specification Unless otherwise specified, we let the token spaces be $X = y = [4096]$; as N and M are equal to $2^{12}$, this choice allows for multiple possible factorizations. Our data model depends on four parameters.\n(P1) Input factors: $(p_i)_{i \\in [k]} \\in \\mathbb{N}^k$.\nBy default, we let $k = 12$ and $(p_i)_i = (2)_{i \\in [12]}$.\n(P2) Output factors: $(q_i)_{i \\in [l]} \\in \\mathbb{N}^k$\nBy default, we let $l = 4$ and $(q_j)_j = (8)_{j \\in [4]}$.\n(P3) Number of parents: $|I_j| \\in \\mathbb{N}$.\nAlthough this results in slightly less general factorizations (as all the y-factors have the same number of parents), fixing $|I_j|$ reduces randomness in the graph generation process. Notably, it turns $\\chi$ (SC) and $\\Chi$ (AC) into deterministic quantities.\nAlternatively, we may want to consider:\n(P3') Connectivity parameter: $\\beta \\in [0,1]$.\nFor each tuple of integers $(i, j) \\in [k] \\times [l]$, we draw a random variable $Z_{ij}$ uniformly on $[0, 1]$. We let i belong to $I_j$, which is equivalent to the coordinates $x_i$ appearing in $pa_j$, when $Z_{ij} < \\beta$. In other terms, $\\beta$ controls the probability of activation of each edge in the bipartite graph linking the x-factors to the y-factors.\nUnless otherwise specified, we set ourselves in (P3) (rather than (P3')) with $|I_j| = 2$.\n(P4) Concentration parameter: $\\alpha \\in \\mathbb{R}^+$.\nWe let the marginal distribution $p(x)$ be the uniform law on $X$ for all experiments. We generate conditional distributions as follows: for each $j \\in [k]$, and each point $pa_j \\in \\prod_{i \\in I_j} X_i$, we sample the vector $(p(y_j | pa_j))_{y_j \\in Y_j}$ according to a Dirichlet distribution of parameter $(\\alpha, \\dots, \\alpha) \\in \\mathbb{R}^{q_j}$. When $\\alpha = 10^{-3}$, this leads to $(p(y_j | pa_j))$ being very close in distribution to a Dirac, while $\\alpha = 1$ leads to $(p(y_j | pa_j))$ being more uniformly sampled across the simplex. We let $\\alpha = 10^{-1}$ be our default value to achieve a balance between the deterministic and uniform cases, to reflect the kind of probability distributions that we expect to encounter in real-world use cases such as next-word prediction.\nModel specification To keep our experimental design simple and in line with LLMs being one of our main sources of inspiration, we use the same MLP architecture as used for the feedforward layers of Mistral's open-source implementation of transformers at the time of writing.\u00b9 It has three degrees of freedom:\n(P5) The embedding dimension: $d \\in \\mathbb{N}$;\n(P6) The hidden dimension: $h \\in \\mathbb{N}$;\n(P7) The number of layers: $L \\in \\mathbb{N}$.\nThe full MLP corresponds to the composition of L functions $F_i$ of the form:\n$F_i : z \\in \\mathbb{R}^d \\rightarrow z + W_{i,1} \\sigma\\left( W_{i,2}^T z \\right) \\odot \\frac{W_{i,3} z}{\\|W_{i,3} z\\|}$,\nwhere $W_{i,1}, W_{i,2}, W_{i,3} \\in \\mathbb{R}^{h \\times d}$, $\\sigma$ is the logistic function, and $\\odot$ is the element-wise product. This architecture was found to be more efficient in practice than purely vanilla MLP (He et al., 2015; Ba et al., 2016; Shazeer, 2020). Unless otherwise specified, we set $d = 32, h = 64$, and $L = 1$. These choices are motivated by compute-optimal design experiments, to be found in the Supplementary Materials (Figures 11 to 15).\nOptimizer specification To keep the optimization simple and mitigate the number of hyperparameters, we use the Adam optimizer as implemented in PyTorch and with default values for $\\beta_1, \\beta_2$. Similarly, we initialize the NN's weights with PyTorch's default scheme. We distinguish between two settings regarding the number of epochs.\nIn the first setting, we focus on quantifying the speed of learning based on different underlying factorizations. For this setting, we consider small numbers of epochs and use a cosine annealing learning rate schedule, defined as\n$\\lambda_t = \\lambda_T \\cdot \\left( \\frac{\\cos(\\pi t/T) + 1}{2} \\right) \\in [0, 1]$, where $\\lambda_T = \\eta$, which leads to two additional hyperparameters:\n(P8) The initial learning rate: $\\eta > 0$;\n(P9) The number of epochs: $T \\in \\mathbb{N}$.\nWe set the initial learning rate to $\\eta = 3\\cdot10^{-2}$ and the number of epochs T to be $10^3$. These choices are motivated by ablation studies to be found in the Supplementary Materials (Figure 16). In this setting, our figures represent averages over 100 independent runs.\nIn our second setting, we wait until the training procedure converges to study the optimal point reached by the networks. This requires a larger number of epochs, which we set to $T = 10^6$. For these experiments, we only average our figures over 10 runs. Ablation studies in the Supplementary Materials (Figure 17) show that the cosine learning rate schedule is not optimal in this setting. On the one hand, choosing a large initial learning rate, such as $\\eta = 3\\cdot10^{-2}$, allows for fast learning at the beginning but leads to instability towards the end of training. On the other hand, choosing"}, {"title": "Single Pass Study", "content": "In our first experiment, we consider the typical setting of LLMs training: a single pass over the data. In this setting, each update is done by sampling a batch of i.i.d. samples $(x^{(t)}, y^{(t)}) \\sim p$ drawn according to a data distribution generated as described in Subsection 4.1. We use batches of size $n = 8096$, we set all parameters and hyperparameters to default values except for the input factorization $(p_i)_{i \\in [l]}$, and we study how the test loss evolves with respect to the number of epochs.\u00b2 We do not vary output factorization $(q_j)$ to avoid confounding factors caused by changes in the distribution of the entropy of $p(y|x)$.\u00b3 In this setting, the embedding $e: X \\rightarrow \\mathbb{R}^d$ is learned.\nWe find that the learning speed is correlated with the statistical complexity $\\chi$ (SC), as highlighted in Figure 3. This suggests that the MLP is able to leverage the implicit product form of the target distribution to improve its learning."}, {"title": "Compression Study", "content": "In this setting, we assume that the batch size n approaches to infinity, so that we directly observe the conditional probabilities $p(y|x)$ for all values of x, and can optimize directly on the population loss $\\mathcal{L}$. Once again, the embedding $e: X \\rightarrow \\mathbb{R}^d$ is learned. We know from the theoretical"}, {"title": "Generalization Study", "content": "In this setting, we study NNs' capacity for generalizing their learning of $p(y|x)$ to yet unseen inputs by leveraging the factorial nature of the task at hand. More concretely, we assume that we only have access to a subset $X_{obs} \\subset X$ of the data at training time. This results in one additional experimental parameter:\n(P10) The data split: $\\gamma = \\frac{|X_{obs}|}{|X|}$.\nBy default we let $\\gamma = 0.9 = 90\\%$. As in the compression setting, we set ourselves in the ideal scenario where we observe $p(y|x)$ for $x \\in X_{obs}$, as if we were provided with very large batches. We are interested in understanding how the network can generalize to unseen values of $x \\in X \\setminus X_{obs}$. This setting falls into the scope of covariate shift (Sugiyama et al., 2007) where the distribution of x changes but the conditional distribution of $y|x$ remains the same, with the additional challenge here that the support of the shifted covariate is disjoint from the support of the observed one. To allow for generalization, the embedding is set to a fixed factorization-compatible embedding $\\tilde{e} : X \\rightarrow \\mathbb{R}^d$ (as defined in Subsection 2.2), with the embedding $e^i$ of each factor sampled as a centered isotropic Gaussian vector.\nWe observe in Figure 6 the effect of the connectivity of the factorization graph (see Figure 1), driven by the connectivity"}, {"title": "Discussion", "content": "In this work, we introduced a new set of structural assumptions on discrete data. Through our theoretical analysis, we provided new insights into how these assumptions can simplify the learning process, and we showed through extensive experiments that NNs can exploit these hidden structures to enhance their performances.\nOur key takeaway is the presence of implicit scaling laws of the form\n$\\mathcal{L} \\propto \\Lambda(\\xi, G)$,\nwhere $\\mathcal{L}$ is the loss, $\\xi$ is either the compute (e.g., Figure 3), the model capacity (e.g., Figure 4), or the number of data (e.g., Figure 8); and G represents the hidden \"graphical\" factorization underlying the data. The function $\\Lambda$ is typically a decreasing function of $\\xi$ and an increasing function of complexity parameters of G, such as the number of factors (Figures 3, 4, and 7), the connectivity of the graph (Figures 6 and 8), or $\\Chi$ and $\\chi$ defined in (AC) and (SC).\nWhile our findings align with the idea that neural network performance scales with structural complexity, they do not conform to the explicit power-law patterns that are usually reported in other scaling law studies Kaplan et al. (2020). We hypothesize that this is due to the scale of our experiments, leading to some alternative scaling regime. As a side note, a close look at some experiments (e.g., Figure 6 or 7) suggests the existence of a power-law regime, which posits a functional $\\omega(G)$ such that $\\mathcal{L} \\propto \\xi^{-\\omega(G)}$, where $\\omega(G)$ captures how simple the underlying factorization is, and is thus expected to be a decreasing function of the complexity parameters of G. We leave the precise identification of such a functional for future work.\nOn a final note, and as hinted at in Subsection 2.2, we previse a possible strong link between our data model and transfer learning. Transfer learning focuses on transferring knowledge learned from one task to improve learning in another, often related but distinct task or domain (Pan and Yang, 2009). Our framework could give a theoretical ground to explain the performance of transfer learning procedures on discrete data when compatible factorizations arise in different tasks, in the sense that a learned embedding adapted to a given task could become a factorization-compatible embedding for another subsequent task."}, {"title": "Theory Supplement", "content": "In Section 3", "l": ""}, "q_j$. Another exact solution, which works as a dual of sorts to the first one, is given by\n$G' =\\left[\\begin{array}{c}(1_{pa_1})_{x \\in X}\\\\\n\\dots\\\\(1_{pa_l})_{x \\in X}\\end{array}\\right"], "solution": "for each $j \\in [l]$, let $G_j$ be the matrix $(1_{pa_j})_{z \\in X} \\in \\mathbb{R}^{|pa_j| \\times N}$ if $|pa_j| < q_j$, and let it be the matrix $(log\\, p(y_j|x))_{y_j \\in Y_j, x \\in X} \\in \\mathbb{R}^{q_j \\times N}$ otherwise. Likewise, let $U_j$ be the matrix $(log\\, p(y_j|z_j))_{z_j \\in pa_j, y \\in Y} \\in \\mathbb{R}^{|pa_j| \\times M}$ if $|pa_j| < q_j$, and let it be the matrix $(1_{y_j})_{y \\in Y} \\in \\mathbb{R}^{q_j \\times M}$ otherwise. Then the matrices\n$G'' =\\left[\\begin{array}{c}G_1\\\\\n\\dots\\\\G_l\\end{array}\\right] \\in \\mathbb{R}^{\\sum_{j \\in [l]} \\min{\\{|pa_j|, q_j\\}} \\times N}$\nand\n$U'' =\\left[\\begin{array}{c}U_1\\\\\n\\dots\\\\U_l\\end{array}\\right] \\in \\mathbb{R}^{\\sum_{j \\in [l]} \\min{\\{|pa_j|, q_j\\}} \\times M}$\nare solutions to (5), which shows that the embedding dimension d can in fact be as small as $\\sum_{j \\in [l]} \\min{\\{|pa_j|, q_j\\}}$ and still allow for a perfect representation of the conditional distribution.\nAs alluded to in Section 3, the matrix $A = (\\log p(y|x))_{y,x}$ having a"}