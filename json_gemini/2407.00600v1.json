{"title": "GenderBias-VL: Benchmarking Gender Bias in Vision Language Models via Counterfactual Probing", "authors": ["Yisong Xiao", "Aishan Liu", "QianJia Cheng", "Zhenfei Yin", "Siyuan Liang", "Jiapeng Li", "Jing Shao", "Xianglong Liu", "Dacheng Tao"], "abstract": "Large Vision-Language Models (LVLMs) have been widely adopted in various applications; however, they exhibit significant gender biases. Existing benchmarks primarily evaluate gender bias at the demographic group level, neglecting individual fairness, which emphasizes equal treatment of similar individuals. This research gap limits the detection of discriminatory behaviors, as individual fairness offers a more granular examination of biases that group fairness may overlook. For the first time, this paper introduces the GenderBias-VL benchmark to evaluate occupation-related gender bias in LVLMs using counterfactual visual questions under individual fairness criteria. To construct this benchmark, we first utilize text-to-image diffusion models to generate occupation images and their gender counterfactuals. Subsequently, we generate corresponding textual occupation options by identifying stereotyped occupation pairs with high semantic similarity but opposite gender proportions in real-world statistics. This method enables the creation of large-scale visual question counterfactuals to expose biases in LVLMs, applicable in both multimodal and unimodal contexts through modifying gender attributes in specific modalities. Overall, our GenderBias-VL benchmark comprises 34,581 visual question counterfactual pairs, covering 177 occupations. Using our benchmark, we extensively evaluate 15 commonly used open-source LVLMs (e.g., LLaVA) and state-of-the-art commercial APIs, including GPT-4V and Gemini-Pro. Our findings reveal widespread gender biases in existing LVLMs. Our benchmark offers: (1) a comprehensive dataset for occupation-related gender bias evaluation; (2) an up-to-date leaderboard on LVLM biases; and (3) a nuanced understanding of the biases presented by these models.", "sections": [{"title": "1 Introduction", "content": "LVLMs have witnessed rapid development [1, 2, 3], which expands the capabilities of large language models (LLMs) [4, 5, 6] by incorporating additional modalities such as images, showcasing remarkable performance in perceiving and reasoning (e.g., GPT-4V [7], Gemini [8]). Despite the advancements, there remains a lingering social concern regarding the potential social biases caused by LVLMs [9, 10, 11]. For instance, research has revealed that ChatGPT associates certain occupations with genders [12], depicting doctors as male and nurses as female. Such bias discriminates against affected population groups and can significantly harm society as these models are extensively deployed in the real world. Therefore, it is crucial to probe and benchmark the social biases exhibited by LVLMs, serving as a necessary initial step to mitigating the risk of discriminatory outcomes.\nWhile considerable benchmarks [13, 14, 15, 16] have been proposed to measure social biases in LVLMs, they primarily focus on group fairness [17, 18], which involve comparing the model"}, {"title": "2 Related Works", "content": "Large Vision Language Models. The remarkable success of Large Language Models (LLMs) has sparked Large Vision Language Models (LVLMs) to the research hotspot, such as GPT-4V [7] and Gemini [8]. Typically, these LVLMs [1, 2, 32, 33, 3, 34, 35, 36, 37, 38] consist of three components: a CLIP image encoder [29, 39, 40] for handling visual input, a pre-trained LLM [4, 41, 5, 6] serving as the system's brain, and a modality interface (e.g., MLP [1, 30] and Q-Former [2, 32]) aligning different modalities. Recently, Qwen-VL [3] and InternLM-XComposer [38] have further enhanced multimodal comprehension by leveraging diverse task datasets. Despite their impressive performance in downstream applications, LVLMs often exhibit undesirable behaviors concerning robustness, privacy, and other trustworthiness issues [42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]. In particular, there is a significant lack of research dedicated to probing their social biases. Our work aims to fill this gap by providing a dataset for evaluating occupation-related gender bias in LVLMs.\nMeasuring Bias in LVLMs. Previous research has revealed that vision-language models can absorb social biases inherent in the training data that consists of stereotyped internet-scraped image-text pairs, potentially leading to the propagation and amplification of bias during content generation tasks such as image captioning [56, 57, 58] and retrieval [59, 60]. Several image-text datasets [61, 14, 62, 15, 63, 64] have been proposed to measure social bias in vision-language models. SocialCounterfactuals [15] probes intersectional bias in CLIP models via 171k generated counterfactual image-text pairs under retrieval task. Leveraging SocialCounterfactuals, Howard et al. [65] evaluated the influence of social attributes (e.g., gender and race) on the toxicity and competency-associated words within content generated by LVLMs. PAIRS [66] comprises 40 images depicting 10 visually ambiguous yet stereotyped working scenarios to examine gender and racial biases in four LVLMs. However, its conclusions are constrained by the small dataset size and unrigorous evaluation methods. Moreover, these works measure bias by comparing performance disparities across groups, overlooking granular"}, {"title": "3 GenderBias-VL Benchmark", "content": "In this section, we first introduce the terminology used in our paper. We then describe the construction pipeline of our GenderBias-VL benchmark, detailing the processes of occupation image generation, stereotyped occupation pairs identification, and visual question counterfactuals creation. Finally, we outline the evaluation protocol of GenderBias-VL. Fig. 1 provides an overview of our benchmark."}, {"title": "3.1 Terminology", "content": "In this paper, we employ a binary gender framework (i.e., female and male) to facilitate analysis. We refrain from making claims about gender identification or assignment, recognizing these as deeply personal experiences regardless of appearance or traits [67, 68]. Like previous work [14, 15, 25], our study is grounded in the concept of perceived gender presentation, which refers to the inference of gender by an external human annotator or model. We acknowledge that the labels used in this study may not fully align with an individual's gender identity. Moreover, we recognize that gender and gender identity are multidimensional notions spanning a spectrum, and acknowledge that misinterpretations within a binary paradigm may occur."}, {"title": "3.2 Counterfactual Visual Question Pairs Construction Pipeline", "content": "Occupation-related gender bias refers to how changes in gender within inputs affect the LVLM's inference on occupations. Thus, we construct visual questions and their gender counterfactual pairs to evaluate the LVLMs' occupation cognitive ability and bias, respectively. Specifically, LVLMs are tasked with inferring the depicted occupation in the input image from two stereotyped occupation options. Here, a visual question consists of three components: image, question, and option. We now illustrate our counterfactual visual question pairs construction pipeline by explaining each component; with more detailed information (e.g., occupation list) presented in the Appendix.\nOccupation Image Generation (image). We utilize text-to-image diffusion models to generate occupation images and their counterfactuals, following three steps: prompt creation, counterfactual image pairs generation, and image filtering."}, {"title": "Step 1: Prompt Creation", "content": "Given the sets of gender attributes G and occupation names O, we denote a subject as $x = \\text{``a {gender} {occ}''}$ (e.g., \u201ca female Chief executive\u201d), where gender \u2208 G and occ \u2208 O. For gender attributes, we employ the terms \u201cfemale\u201d and \u201cmale\u201d; for occupation names, we use a list of 248 occupations from the BLS. Following the guidelines in [69, 70], we leverage ChatGPT to generate high-quality prompts for text-to-image diffusion models. We iterate through all combinations of gender and occupation values to obtain different subjects, prompting ChatGPT to generate five diverse prompts for each subject."}, {"title": "Step 2: Counterfactual Image Pairs Generation", "content": "We employ Stable Diffusion XL [26], renowned for its superior photorealism, to generate base occupation images guided by the above prompts. In the realm of image editing, InstructPix2Pix [27] achieves strong image consistency pre- and post-editing by training diffusion models on meticulously curated paired images and captions. Consequently, we prompt InstructPix2Pix with a base image (\u201cfemale\u201d/\u201cmale\u201d) and corresponding edit instruction (\u201cturn her into a male\u201d/\u201cturn him into a female\u201d), enabling the creation of counterfactuals that preserve much of the original details while differing in gender. For each subject, we over-generate 100 base images (20 images for each prompt) and their counterfactuals. Additionally, we incorporate a universal negative prompt to steer the images away from undesired artifacts."}, {"title": "Step 3: Image Filtering", "content": "To ensure the validity of images for gender bias evaluation, we exclude not-safe-for-work contents [71] and apply filters based on person counts and gender attributes. For person count validation, we use the Grounding DINO [72] object detector to detect bounding boxes of individuals and determine the number of people in each image. Images with person counts other than one are excluded: multiple people may impact model evaluation, while images with no people violate the criterion for human-centric bias evaluation. For gender attributes, we use the CLIP ViT-L-14 [29] to classify images as female or male. Both base and counterfactual image pairs must align with the prompted gender during generation to be included; otherwise, they are filtered out."}, {"title": "Stereotyped Occupation Pairs Identification (option)", "content": "To expose gender bias in LVLMs, we introduce stereotyped occupation pairs, which share high semantic similarity yet exhibit opposite gender dominance. First, occupations are categorized as male- or female-dominated based on BLS [28] gender proportions, then combined pairwise to generate occupation pairs. Then, given $occ_m$ (male-dominated) and $occ_f$ (female-dominated), their base images $V_{occ_m}$ and $V_{occ_f}$ (100 over-generated images of each occupation), names $t_{occ_m}$ and $t_{occ_f}$, we utilize CLIP model's vision encoder $E_{img}$ and text encoder $E_{text}$ to calculate visual and textual similarities as follows:\n$sim_{img}(OCC_m, OCC_f) = (I_{occ_m}, I_{occ_f}), sim_{text}(OCC_m, OCC_f) = (E_{text}(t_{occ_m}), E_{text}(t_{occ_f})),$ (1)\nwhere $\\bar{I} = \\frac{1}{|V|} \\sum_{j=1}^{|V|} E_{img}(V_j)$ represents the averaged image embeddings of occupation concept. For all occupation pairs, we collect their similarities, apply min-max normalization within each modality, and compute the final semantic similarity scores as: $sim = 0.5 \\times [norm(sim_{img}) + norm(sim_{text})]$. To select pairs with high semantic similarity, we rank sim scores and retain scores higher than $\\epsilon$. Empirically, $\\epsilon$ is set as the lowest score among five commonly identified stereotyped occupation pairs [66]. Note that an occupation may share high similarities with many others, we further restrict each occupation to ten pairings to prioritize the most relevant matches. Finally, we identify 486 stereotyped occupation pairs (covering 177 unique occupations)."}, {"title": "Visual Question Counterfactuals Creation", "content": "Based on the above image and option components, we can devise question to create large-scale counterfactual visual question pairs. Since gender counterfactuals can be introduced to either the image or question components, we can control the gender counterfactual modality to support bias evaluation in both multimodal and unimodal contexts.\nMultimodal bias encompasses gender modifications across image and text. Given occ with its counterfactual image pairs, alongside its stereotyped occupation $occ_s$, the question is designed to be straightforward: \u201cWhat is the {gender}\u2019s occupation in this image?\\n\", where {gender} is the perceived gender of image. The option is structured as: \u201cOptions: (A) {occ} (B) {occs}\\n\u201d. We repeat this process for all identified stereotyped occupation pairs to develop GenderBias-VL.\nVisual unimodal bias exclusively involves gender modifications within the image. Thus, we merely repeat the construction process in Multimodal bias, substituting the question template with \u201cWhat is the person\u2019s occupation in this image?\\n\u201d, while keeping all other steps unchanged.\nLanguage unimodal bias exclusively involves gender modifications in the text, yet gender presen- tation in images could impede this evaluation. We utilize gender-bleached [73] images to address\""}, {"title": "3.3 Evaluation Protocol", "content": "Database. To achieve a trade-off between test significance and accessibility, we limit the number of counterfactual visual question pairs to 20 for each subject \u201c{gender} {occ}\u201d. Therefore, we develop GenderBias-VL, comprising 34,581 visual question counterfactuals across 486 occupation pairs, covering 177 occupations (85 male-dominated and 92 female-dominated) and 5,924 counterfactual image pairs, supporting bias evaluation in both multimodal and unimodal contexts.\nLVLMs. Based on GenderBias-VL, we benchmark 15 commonly used open-source LVLMs: InstructBLIP [2], LLaVA series (LLaVA1.5-7B, LLaVA1.5-13B, LLaVA1.6-13B) [30], Minigpt-v2 [75], mPLUG-Owl2 [76], Qwen-VL [3], LLaMA-Adapter-v2 (LAv2) [77], LAMM [33], Otter [78], Kosmos-2 [79], Shikra [37], InternLM-Xcomposer2 (InternLM-XC2) [80], LLaVA-RLHF [36], and RLHF-V [35]. We evaluate these LVLMs using the single-turn Perplexity (PPL) [81] inferencer provided in [82, 83], which confines their output to options and computes the probability for each option. We also evaluate state-of-the-art commercial LVLMs (i.e., GPT-4V [31] and Gemini-Pro [8]), obtaining answers through their official APIs.\nMetrics. An ideal LVLM should excel in cognitive abilities while avoiding biases. Thus, given a stereotyped occupation pair (occm, occf) and its two visual question counterfactuals sets Qm (images are occm) and Qf, we define the metrics for accuracy, bias, and an idealized score for this stereotyped pair. Additionally, we explain how to calculate these metrics for the entire dataset.\nAccuracy. We define the accuracy Acc of an occupation pair as the average percentage of correctly inferred base visual questions in Qm and Qf. Overall Acc of a dataset is the average Acc of all pairs.\nBias. GenderBias-VL supports measuring bias in two ways: probability difference (under counterfactual fairness [22]) and outcome difference (under causal fairness [21]). The former examines changes in prediction probabilities of open-source LVLMs, which we detail here. The latter considers only outcomes, making it applicable to APIs, and is detailed in the Appendix. For bias based on probability difference, we first define the bias of occm in an occupation pair:\n$bias(occ_m) = E_{(q_g, q_{\\bar{g}})\\sim Q_m}[1_{g=g_0}(P(occ_m|q_g)-P(occ_m|q_{\\bar{g}}))-1_{g=g_1}(P(occ_m|q_g)-P(occ_m|q_{\\bar{g}}))],$ (2)\nwhere $q_g$ and $q_{\\bar{g}}$ are the base visual question and its counterfactual respectively, g is the gender of base visual question $q_g$, $P(occ_m|q_g)$ is the probability of choosing $occ_m$ under $q_g$, $g_0$ and $g_1$ are predefined gender groups, and 1 is the indicator function (1 A = 1 if and only if the event \u201cA\u201d is true). Here We set $g_0$ as male and $g_1$ as female; thus, a positive value indicates a male bias for $occ_m$, while a negative value indicates a female bias. Then, we can define the bias $B_{pair}$ of an occupation pair as:\n$B_{pair}(OCC_m, OCC_f) = 0.5 \\times [bias(occ_m) - bias(occ_f)].$ (3)\nThis represents the average probability change introduced by gender counterfactuals, where a positive value means LVLMs perceive $occ_m$ as more masculine than $occ_f$, and a negative value means LVLMs perceive $occ_m$ as more feminine than $occ_f$. For the entire dataset, we define overall bias $B_{ovl}$ as the average absolute $|B_{pair}|$ of all pairs, and $B_{max}$ as the maximum absolute $|B_{pair}|$ among all pairs. Specifically, we define micro bias $B_{micro}$ to facilitate analysis on occupation level:\n$B_{micro}(occ_m) = \\frac{1}{|S_{occ_m}|} \\sum_{occ_f \\in S_{occ_m}} Bpair(occ_m, occ_f)$, where $S_{occ_m}$ is the set of pairs containing $occ_m$. For bias based on outcome difference, we similarly define $B_{pair}^o$, $B_{ovl}^o$, and $B_{max}^o$; details in Appendix.\nIdealized score. To combine accuracy and bias results, we introduce an idealized paired stereotype bias test score of an occupation pair as $Ipss = Acc \\times (1 - |B_{pair}|)$. We define the overall Ipss of the entire dataset as the average Ipss of all pairs. Similarly, we denote the idealized score for bias based on outcome difference as $Ipss^o$.\nUnder the above definitions, an ideal LVLM is characterized by high accuracy and low absolute bias, thereby achieving a high idealized score. Besides, we conduct an option-swapping test across the dataset as LVLMs may favor certain option orders [84]. We utilize the absolute accuracy difference ($\\Delta Acc$) to measure susceptibility to option order and report the average values for these metrics."}, {"title": "4 Benchmark Evaluation Results", "content": "In this section, we report the primary evaluation results of LVLMs within our GenderBias-VL benchmark and summarize the key findings to understand the biases presented by the LVLMs."}, {"title": "4.1 Bias Evaluation on Open-source LVLMS", "content": "We first report the overall bias evaluation results on 15 open-source LVLMs, as detailed in Tab. 1. Besides 15 LVLMs, We define a RANDOM model baseline that randomly selects options, resulting in 50 Acc, 0 Boul, and 50 Ipss. Fig. 2 presents a violin plot of the $B_{pair}$ distribution for 15 LVLMs under VL-Bias evaluation. From the results, we can identify:\nBias is widespread among the evaluated LVLMs. LVLMs show an average Boul of 3.09% across different evaluation settings, indicating that the predicted probability of the correct occupation changes by this amount when the gender in the visual question is counterfactually altered. Among the Top-50 most biased occupation pairs for each LVLM, this value increases to 9.25%. Specifically, InternLM-XC2 and RLHF-V showcase the most severe social bias, with Bmax exceeding 30% on average across evaluation contexts. Interestingly, we observe commonalities in highly biased occupation pairs across LVLMs, such as aircraft pilot and flight attendant.\nAs LVLMs become powerful (i.e., higher Acc), they also become increasingly biased. This trend is likely inevitable since higher performance requires better fitting to real-world corpus, which inherently contains social biases. Comparing LLaVA series and LLaVA-RLHF, we conjecture three factors that increase LVLM bias: larger LLM models themselves may embed more bias (LLaVA1.5-7B vs. LLaVA1.5-13B); \u2461 a larger training corpus may contain more human stereotypes (LLaVA1.5-13B vs. LLaVA1.6-13B); \u2462 Reinforcement Learning from Human Feedback [85] may inadvertently introduce human preferences (LLaVA1.5-13B vs. LLaVA-RLHF). While these factors boost performance, they also heighten bias risk, necessitating a balanced approach for responsible LVLMs.\nBesides the primary observations, we also notice that For Ipss, almost all LVLMs (except Kosmos- 2) surpass the RANDOM model, with InstructBLIP achieving the highest score (73.72% across contexts) by effectively balancing accuracy and bias. Regarding occupation cognitive ability, LVLMs exhibit inconsistently. Kosmos-2 performs worse than the RANDOM model, primarily because its specialized training on detection datasets hinders its ability to understand the provided op- tions. Additionally, all LVLMs are susceptible to option order, with LLaVA1.5 and LAv2 experiencing the most severe effects, averaging 96.24% and 89.33% \u2206Acc across contexts, respectively."}, {"title": "4.2 Case Studies on Commercial LVLMs", "content": "Besides open-source LVLMs, we evaluate the gender bias of state-of-the-art black-box commercial LVLM APIs, including GPT-4V [31] and Gemini-Pro [8]. Due to query limitations and costs, we restrict the evaluation dataset to the Top-10 biased occupation pairs listed in Tab. 2. Fig. 3 presents the average results (bias based on outcome difference) across three evaluation contexts. Despite"}, {"title": "4.3 Bias Characteristics Analysis", "content": "After reporting the overall bias performance, we then further study where and how these biases emerge. In particular, we focus our discussion on 14 of the 15 LVLMs (excluding Kosmos-2 as it performs worse than random choosing) evaluated under VL-Bias, with similar results for other bias contexts in the Appendix.\nTop Biased Pairs. From the main evaluation results, we identify the Top-10 biased occupation pairs (excluding duplicated occupations). As shown in Tab. 2, in addition to the commonly examined occupation pairs that embody gender stereotypes, such as CEO and executive secretary, we also find rarely mentioned pairs like refractory machinery mechanic and filling machine operator, as well as computer system manager and receptionist, which also present severe gender bias. It suggests that bias extends across a broader range of occupations than previously documented, demonstrating the effectiveness of our benchmark in detecting bias in the workplace.\nBias Patterns. Among occupation pairs with high levels of bias, we identify the following patterns that are more likely to reveal bias in LVLMs. Male-dominated occupations: management, business, finance, professional, or physically demanding categories; female-dominated occupations: service, office and administrative support, or patient-requiring categories. These combination patterns reflect societal stereotypes about gender roles in occupations, and the resulting biased behavior in LVLMs, in turn, indicates their absorption of such stereotypes during the training process. Interestingly, at the intersection of finance and management categories, LVLMs may encode stronger male stereotypes for financial roles. For example, InstructBLIP consistently shows positive biases towards pairs consisting"}, {"title": "Bias Direction", "content": "Here, we further examine the gender bias direction (i.e., the sign of $B_{pair}$). A positive $B_{pair}$ indicates that LVLMs perceive male-dominated occupations as more masculine and female- dominated occupations as more feminine, while a negative $B_{pair}$ indicates the opposite. As shown in Fig. 2, LVLMs exhibit positive biases for most occupation pairs, indicating that their biases mirror real- world stereotypes. However, we also observe that a small subset of occupation pairs exhibit negative bias. For example, among 10 occupation pairs involving female-dominated financial manager, InstructBLIP presents negative biases in eight pairs, indicating that financial manager is generally perceived as more masculine. The remaining two positive pairs involve industry/engineering roles, suggesting these are seen as even more masculine."}, {"title": "Observed Bias vs. Real-world Labor Statistics", "content": "To further understand the observed bias and its connection to real-world statistics, we compare occupation-level biases ($B_{micro}$) with U.S. Labor Force Statistics [28], as depicted in Fig. 4(a) for InstructBLIP (others shown in Appendix). The scatter plot is divided into four colored quadrants, where LVLMs\u2019 bias aligns with labor force data in quadrants one and three, but opposes it in quadrants two and four. We observe that bias presented by LVLMs generally aligns with labor statistics, as most occupations fall into quadrants one and three. Additionally, we measure the correlation between LVLM bias and occupation proportions using the Pearson Correlation (\u03c1) and Kendall-Tau Rank Correlation (\u03c4). All LVLMs exhibit strong correlations, with average \u03c1 and \u03c4 coefficients of 0.68 and 0.56, respectively, underscoring that LVLM biases inherit real-world skews."}, {"title": "4.4 Relationship between Visual and Language Bias", "content": "V-Bias vs. L-Bias. LVLM's inference involves complex cross-modal interactions between vision and language. To characterize their bias relationship, we use scatter plots comparing occupation-level bias ($B_{micro}$) in visual modal (V-Bias, x-axis) and language modal (L-Bias, y-axis), as shown in Fig. 4(b) for InstructBLIP (others shown in Appendix). Among the LVLMs (excluding Kosmos-2), we observe strong alignment between the direction of visual modal bias and language modal bias, with occupations clustering in quadrants one and three. Besides, there is a strong correlation in the extent of these biases, with average \u03c1 and \u03c4 coefficients of 0.77 and 0.60, respectively. The consistency of bias across modalities in LVLMs likely stems from their LLM-centric nature. LVLMs\u2019 modality interfaces are trained via LLM-centric alignment to project multimodal information into an LLM-understandable space. Then, LLM conducts the reasoning process based on the fused information, leading to consistent biases in the two modalities."}, {"title": "V-Bias correlation between CLIP and LVLMs", "content": "Moreover, we evaluate CLIP models involved in LVLMs under V-Bias to see whether visual biases in CLIP transfer to LVLMs. Tab. 3 reports the average results (bias based on outcome difference), revealing that CLIP models exhibit even greater biases than LVLMs. However, biases at the occupation pair level between CLIP and LVLMs show relatively weak cor- relations (0.26 and 0.19 for \u03c1 and \u03c4 coefficients on average). Further analysis of each counterfactual vi- sual question pair reveals subtle differences in the biases of CLIP and LVLMs. For instance, there are only 874 (11%) overlapping biased samples between EVA-G [40] and InstructBLIP, indicating that LVLMs display distinct behaviors from CLIP models, despite both sharing the same image encoder. The result also indirectly supports our analysis that the consistent bias across visual and language modalities stems from the LLM-centric nature."}, {"title": "5 Conclusion and Future Work", "content": "This paper introduces GenderBias-VL benchmark, the first to evaluate occupation-related gender bias in LVLMs under individual fairness criteria. GenderBias-VL comprises 34,581 visual question counterfactual pairs covering 177 occupations, supporting bias probing in both multimodal and unimodal contexts. The extensive evaluation of 15 open-source LVLMs and the most advanced commercial LVLMs (GPT-4V and Gemini-Pro) reveals pervasive biases within these models. As LVLM development races forward, GenderBias-VL offers a vital benchmark to assess social bias risks before their real-world deployment.\nLimitation. Despite our best efforts, we acknowledge three major limitations in our research. First, the generation pipeline may contain latent biases, which may contribute to implicit biases exhibited by LVLMs. Therefore, the findings we present to enhance the understanding of bias in LVLMs should be interpreted within the context of our experiments. Second, our benchmark is confined to a binary gender framework, whereas gender is inherently fluid, as discussed in Sec. 3.1. Future research should incorporate a broader range of gender identities, including those from the LGBTQIA+ community, to provide a more comprehensive scope of bias assessment in LVLMs. Third, since our benchmark is based on U.S. Labor Force Statistics, the bias analysis primarily focuses on the Western world. It is crucial to extend this analysis to encompass diverse cultures and countries in future work.\nEthical statement and broader impact. Our paper aims to probe and benchmark the occupation- related gender bias in LVLMs. While evaluation results may raise ethical concerns and potentially harm readers, our intention is not to cause harm. Rather, our work seeks to facilitate bias evaluation for LVLMs, serving as a crucial initial step toward mitigating discriminatory outcomes."}]}