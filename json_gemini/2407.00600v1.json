{"title": "GenderBias-VL: Benchmarking Gender Bias in Vision Language Models via Counterfactual Probing", "authors": ["Yisong Xiao", "Aishan Liu", "QianJia Cheng", "Zhenfei Yin", "Siyuan Liang", "Jiapeng Li", "Jing Shao", "Xianglong Liu", "Dacheng Tao"], "abstract": "Large Vision-Language Models (LVLMs) have been widely adopted in various applications; however, they exhibit significant gender biases. Existing benchmarks primarily evaluate gender bias at the demographic group level, neglecting individual fairness, which emphasizes equal treatment of similar individuals. This research gap limits the detection of discriminatory behaviors, as individual fairness offers a more granular examination of biases that group fairness may overlook. For the first time, this paper introduces the GenderBias-VL benchmark to evaluate occupation-related gender bias in LVLMs using counterfactual visual questions under individual fairness criteria. To construct this benchmark, we first utilize text-to-image diffusion models to generate occupation images and their gender counterfactuals. Subsequently, we generate corresponding textual occupation options by identifying stereotyped occupation pairs with high semantic similarity but opposite gender proportions in real-world statistics. This method enables the creation of large-scale visual question counterfactuals to expose biases in LVLMs, applicable in both multimodal and unimodal contexts through modifying gender attributes in specific modalities. Overall, our GenderBias-VL benchmark comprises 34,581 visual question counterfactual pairs, covering 177 occupations. Using our benchmark, we extensively evaluate 15 commonly used open-source LVLMs (e.g., LLaVA) and state-of-the-art commercial APIs, including GPT-4V and Gemini-Pro. Our findings reveal widespread gender biases in existing LVLMs. Our benchmark offers: (1) a comprehensive dataset for occupation-related gender bias evaluation; (2) an up-to-date leaderboard on LVLM biases; and (3) a nuanced understanding of the biases presented by these models.", "sections": [{"title": "1 Introduction", "content": "LVLMs have witnessed rapid development [1, 2, 3], which expands the capabilities of large language models (LLMs) [4, 5, 6] by incorporating additional modalities such as images, showcasing remarkable performance in perceiving and reasoning (e.g., GPT-4V [7], Gemini [8]). Despite the advancements, there remains a lingering social concern regarding the potential social biases caused by LVLMs [9, 10, 11]. For instance, research has revealed that ChatGPT associates certain occupations with genders [12], depicting doctors as male and nurses as female. Such bias discriminates against affected population groups and can significantly harm society as these models are extensively deployed in the real world. Therefore, it is crucial to probe and benchmark the social biases exhibited by LVLMs, serving as a necessary initial step to mitigating the risk of discriminatory outcomes.\nWhile considerable benchmarks [13, 14, 15, 16] have been proposed to measure social biases in LVLMs, they primarily focus on group fairness [17, 18], which involve comparing the model"}, {"title": "2 Related Works", "content": "Large Vision Language Models. The remarkable success of Large Language Models (LLMs) has sparked Large Vision Language Models (LVLMs) to the research hotspot, such as GPT-4V [7] and Gemini [8]. Typically, these LVLMs [1, 2, 32, 33, 3, 34, 35, 36, 37, 38] consist of three components: a CLIP image encoder [29, 39, 40] for handling visual input, a pre-trained LLM [4, 41, 5, 6] serving as the system's brain, and a modality interface (e.g., MLP [1, 30] and Q-Former [2, 32]) aligning different modalities. Recently, Qwen-VL [3] and InternLM-XComposer [38] have further enhanced multimodal comprehension by leveraging diverse task datasets. Despite their impressive performance in downstream applications, LVLMs often exhibit undesirable behaviors concerning robustness, privacy, and other trustworthiness issues [42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]. In particular, there is a significant lack of research dedicated to probing their social biases. Our work aims to fill this gap by providing a dataset for evaluating occupation-related gender bias in LVLMs.\nMeasuring Bias in LVLMs. Previous research has revealed that vision-language models can absorb social biases inherent in the training data that consists of stereotyped internet-scraped image-text pairs, potentially leading to the propagation and amplification of bias during content generation tasks such as image captioning [56, 57, 58] and retrieval [59, 60]. Several image-text datasets [61, 14, 62, 15, 63, 64] have been proposed to measure social bias in vision-language models. SocialCounterfactuals [15] probes intersectional bias in CLIP models via 171k generated counterfactual image-text pairs under retrieval task. Leveraging SocialCounterfactuals, Howard et al. [65] evaluated the influence of social attributes (e.g., gender and race) on the toxicity and competency-associated words within content generated by LVLMs. PAIRS [66] comprises 40 images depicting 10 visually ambiguous yet stereotyped working scenarios to examine gender and racial biases in four LVLMs. However, its conclusions are constrained by the small dataset size and unrigorous evaluation methods. Moreover, these works measure bias by comparing performance disparities across groups, overlooking granular"}, {"title": "3 GenderBias-VL Benchmark", "content": "In this section, we first introduce the terminology used in our paper. We then describe the construction pipeline of our GenderBias-VL benchmark, detailing the processes of occupation image generation, stereotyped occupation pairs identification, and visual question counterfactuals creation. Finally, we outline the evaluation protocol of GenderBias-VL. Fig. 1 provides an overview of our benchmark.\nIn this paper, we employ a binary gender framework (i.e., female and male) to facilitate analysis. We refrain from making claims about gender identification or assignment, recognizing these as deeply personal experiences regardless of appearance or traits [67, 68]. Like previous work [14, 15, 25], our study is grounded in the concept of perceived gender presentation, which refers to the inference of gender by an external human annotator or model. We acknowledge that the labels used in this study may not fully align with an individual's gender identity. Moreover, we recognize that gender and gender identity are multidimensional notions spanning a spectrum, and acknowledge that misinterpretations within a binary paradigm may occur.\nOccupation-related gender bias refers to how changes in gender within inputs affect the LVLM's inference on occupations. Thus, we construct visual questions and their gender counterfactual pairs to evaluate the LVLMs' occupation cognitive ability and bias, respectively. Specifically, LVLMs are tasked with inferring the depicted occupation in the input image from two stereotyped occupation options. Here, a visual question consists of three components: image, question, and option. We now illustrate our counterfactual visual question pairs construction pipeline by explaining each component; with more detailed information (e.g., occupation list) presented in the Appendix."}, {"title": "3.1 Terminology", "content": "In this paper, we employ a binary gender framework (i.e., female and male) to facilitate analysis. We refrain from making claims about gender identification or assignment, recognizing these as deeply personal experiences regardless of appearance or traits [67, 68]. Like previous work [14, 15, 25], our study is grounded in the concept of perceived gender presentation, which refers to the inference of gender by an external human annotator or model. We acknowledge that the labels used in this study may not fully align with an individual's gender identity. Moreover, we recognize that gender and gender identity are multidimensional notions spanning a spectrum, and acknowledge that misinterpretations within a binary paradigm may occur."}, {"title": "3.2 Counterfactual Visual Question Pairs Construction Pipeline", "content": "Occupation-related gender bias refers to how changes in gender within inputs affect the LVLM's inference on occupations. Thus, we construct visual questions and their gender counterfactual pairs to evaluate the LVLMs' occupation cognitive ability and bias, respectively. Specifically, LVLMs are tasked with inferring the depicted occupation in the input image from two stereotyped occupation options. Here, a visual question consists of three components: image, question, and option. We now illustrate our counterfactual visual question pairs construction pipeline by explaining each component; with more detailed information (e.g., occupation list) presented in the Appendix."}, {"title": "3.3 Evaluation Protocol", "content": "Database. To achieve a trade-off between test significance and accessibility, we limit the number of counterfactual visual question pairs to 20 for each subject \u201c{gender} {occ}\u201d. Therefore, we develop GenderBias-VL, comprising 34,581 visual question counterfactuals across 486 occupation pairs, covering 177 occupations (85 male-dominated and 92 female-dominated) and 5,924 counterfactual image pairs, supporting bias evaluation in both multimodal and unimodal contexts.\nLVLMs. Based on GenderBias-VL, we benchmark 15 commonly used open-source LVLMs: InstructBLIP [2], LLaVA series (LLaVA1.5-7B, LLaVA1.5-13B, LLaVA1.6-13B) [30], Minigpt-v2 [75], mPLUG-Owl2 [76], Qwen-VL [3], LLaMA-Adapter-v2 (LAv2) [77], LAMM [33], Otter [78], Kosmos-2 [79], Shikra [37], InternLM-Xcomposer2 (InternLM-XC2) [80], LLaVA-RLHF [36], and RLHF-V [35]. We evaluate these LVLMs using the single-turn Perplexity (PPL) [81] inferencer provided in [82, 83], which confines their output to options and computes the probability for each option. We also evaluate state-of-the-art commercial LVLMs (i.e., GPT-4V [31] and Gemini-Pro [8]), obtaining answers through their official APIs.\nMetrics. An ideal LVLM should excel in cognitive abilities while avoiding biases. Thus, given a stereotyped occupation pair $(occ_m, occ_f)$ and its two visual question counterfactuals sets $Q_m$ (images are $occ_m$) and $Q_f$, we define the metrics for accuracy, bias, and an idealized score for this stereotyped pair. Additionally, we explain how to calculate these metrics for the entire dataset.\nAccuracy. We define the accuracy $Acc$ of an occupation pair as the average percentage of correctly inferred base visual questions in $Q_m$ and $Q_f$. Overall $Acc$ of a dataset is the average $Acc$ of all pairs.\nBias. GenderBias-VL supports measuring bias in two ways: probability difference (under counterfactual fairness [22]) and outcome difference (under causal fairness [21]). The former examines changes in prediction probabilities of open-source LVLMs, which we detail here. The latter considers only outcomes, making it applicable to APIs, and is detailed in the Appendix. For bias based on probability difference, we first define the bias of $occ_m$ in an occupation pair:\n$bias(occ_m) = E_{(q_g, q_{\\bar{g}}) \\sim Q_m}[1_{g = g_0}(P(occ_m | q_g) - P(occ_m | q_{\\bar{g}})) - 1_{g = g_1}(P(occ_m | q_g) - P(occ_m | q_{\\bar{g}}))]$,\nwhere $q_g$ and $q_{\\bar{g}}$ are the base visual question and its counterfactual respectively, $g$ is the gender of base visual question $q_g$, $P(occ_m | q_g)$ is the probability of choosing $occ_m$ under $q_g$, $g_0$ and $g_1$ are predefined gender groups, and $1$ is the indicator function ($1_A = 1$ if and only if the event \"A\" is true). Here We set $g_0$ as male and $g_1$ as female; thus, a positive value indicates a male bias for $occ_m$, while a negative value indicates a female bias. Then, we can define the bias $B_{pair}$ of an occupation pair as:\n$B_{pair}(occ_m, occ_f) = 0.5 \\times [bias(occ_m) - bias(occ_f)]$.\nThis represents the average probability change introduced by gender counterfactuals, where a positive value means LVLMs perceive $occ_m$ as more masculine than $occ_f$, and a negative value means LVLMs perceive $occ_m$ as more feminine than $occ_f$. For the entire dataset, we define overall bias $B_{ovl}$ as the average absolute $|B_{pair}|$ of all pairs, and $B_{max}$ as the maximum absolute $|B_{pair}|$ among all pairs. Specifically, we define micro bias $B_{micro}$ to facilitate analysis on occupation level:\n$B_{micro}(occ_m) = \\frac{1}{|S_{occ_m}|} \\sum_{occ_f \\in S_{occ_m}} Bpair(occ_m, occ_f)$, where $S_{occ_m}$ is the set of pairs containing $occ_m$. For bias based on outcome difference, we similarly define $B'_{pair}$, $B'_{ovl}$, and $B'_{max}$; details in Appendix.\nIdealized score. To combine accuracy and bias results, we introduce an idealized paired stereotype bias test score of an occupation pair as $I_{pss} = Acc \\times (1 - |B_{pair}|)$. We define the overall $I_{pss}$ of the entire dataset as the average $I_{pss}$ of all pairs. Similarly, we denote the idealized score for bias based on outcome difference as $I'_{pss}$.\nUnder the above definitions, an ideal LVLM is characterized by high accuracy and low absolute bias, thereby achieving a high idealized score. Besides, we conduct an option-swapping test across the dataset as LVLMs may favor certain option orders [84]. We utilize the absolute accuracy difference $(\\Delta Acc)$ to measure susceptibility to option order and report the average values for these metrics."}, {"title": "4 Benchmark Evaluation Results", "content": "In this section, we report the primary evaluation results of LVLMs within our GenderBias-VL benchmark and summarize the key findings to understand the biases presented by the LVLMs."}, {"title": "4.1 Bias Evaluation on Open-source LVLMs", "content": "We first report the overall bias evaluation results on 15 open-source LVLMs, as detailed in Tab. 1. Besides 15 LVLMs, We define a RANDOM model baseline that randomly selects options, resulting in 50 Acc, 0 Boul, and 50 Ipss. Fig. 2 presents a violin plot of the Bpair distribution for 15 LVLMs under VL-Bias evaluation. From the results, we can identify:\nBias is widespread among the evaluated LVLMs. LVLMs show an average Boul of 3.09% across different evaluation settings, indicating that the predicted probability of the correct occupation changes by this amount when the gender in the visual question is counterfactually altered. Among the Top-50 most biased occupation pairs for each LVLM, this value increases to 9.25%. Specifically, InternLM-XC2 and RLHF-V showcase the most severe social bias, with Bmax exceeding 30% on average across evaluation contexts. Interestingly, we observe commonalities in highly biased occupation pairs across LVLMs, such as aircraft pilot and flight attendant.\nAs LVLMs become powerful (i.e., higher Acc), they also become increasingly biased. This trend is likely inevitable since higher performance requires better fitting to real-world corpus, which inherently contains social biases. Comparing LLaVA series and LLaVA-RLHF, we conjecture three factors that increase LVLM bias: larger LLM models themselves may embed more bias (LLaVA1.5-7B vs. LLaVA1.5-13B); \u2461 a larger training corpus may contain more human stereotypes (LLaVA1.5-13B vs. LLaVA1.6-13B); \u2462 Reinforcement Learning from Human Feedback [85] may inadvertently introduce human preferences (LLaVA1.5-13B vs. LLaVA-RLHF). While these factors boost performance, they also heighten bias risk, necessitating a balanced approach for responsible LVLMs.\nBesides the primary observations, we also notice that For Ipss, almost all LVLMs (except Kosmos-2) surpass the RANDOM model, with InstructBLIP achieving the highest score (73.72% across contexts) by effectively balancing accuracy and bias. Regarding occupation cognitive ability, LVLMs exhibit inconsistently. Kosmos-2 performs worse than the RANDOM model, primarily because its specialized training on detection datasets hinders its ability to understand the provided options. Additionally, all LVLMs are susceptible to option order, with LLaVA1.5 and LAv2 experiencing the most severe effects, averaging 96.24% and 89.33% \u2206Acc across contexts, respectively."}, {"title": "4.2 Case Studies on Commercial LVLMs", "content": "Besides open-source LVLMs, we evaluate the gender bias of state-of-the-art black-box commercial LVLM APIs, including GPT-4V [31] and Gemini-Pro [8]. Due to query limitations and costs, we restrict the evaluation dataset to the Top-10 biased occupation pairs listed in Tab. 2. Fig. 3 presents the average results (bias based on outcome difference) across three evaluation contexts. Despite"}, {"title": "4.3 Bias Characteristics Analysis", "content": "After reporting the overall bias performance, we then further study where and how these biases emerge. In particular, we focus our discussion on 14 of the 15 LVLMs (excluding Kosmos-2 as it performs worse than random choosing) evaluated under VL-Bias, with similar results for other bias contexts in the Appendix.\nTop Biased Pairs. From the main evaluation results, we identify the Top-10 biased occupation pairs (excluding duplicated occupations). As shown in Tab. 2, in addition to the commonly examined occupation pairs that embody gender stereotypes, such as CEO and executive secretary, we also find rarely mentioned pairs like refractory machinery mechanic and filling machine operator, as well as computer system manager and receptionist, which also present severe gender bias. It suggests that bias extends across a broader range of occupations than previously documented, demonstrating the effectiveness of our benchmark in detecting bias in the workplace.\nBias Patterns. Among occupation pairs with high levels of bias, we identify the following patterns that are more likely to reveal bias in LVLMs. Male-dominated occupations: management, business, finance, professional, or physically demanding categories; female-dominated occupations: service, office and administrative support, or patient-requiring categories. These combination patterns reflect societal stereotypes about gender roles in occupations, and the resulting biased behavior in LVLMs, in turn, indicates their absorption of such stereotypes during the training process. Interestingly, at the intersection of finance and management categories, LVLMs may encode stronger male stereotypes for financial roles. For example, InstructBLIP consistently shows positive biases towards pairs consisting"}, {"title": "4.4 Relationship between Visual and Language Bias", "content": "V-Bias vs. L-Bias. LVLM's inference involves complex cross-modal interactions between vision and language. To characterize their bias relationship, we use scatter plots comparing occupation-level bias (Bmicro) in visual modal (V-Bias, x-axis) and language modal (L-Bias, y-axis), as shown in Fig. 4(b) for InstructBLIP (others shown in Appendix). Among the LVLMs (excluding Kosmos-2), we observe strong alignment between the direction of visual modal bias and language modal bias, with occupations clustering in quadrants one and three. Besides, there is a strong correlation in the extent of these biases, with average \u03c1 and \u03c4 coefficients of 0.77 and 0.60, respectively. The consistency of bias across modalities in LVLMs likely stems from their LLM-centric nature. LVLMs' modality interfaces are trained via LLM-centric alignment to project multimodal information into an LLM-understandable space. Then, LLM conducts the reasoning process based on the fused information, leading to consistent biases in the two modalities."}, {"title": "V-Bias correlation between CLIP and LVLMs.", "content": "Moreover, we evaluate CLIP models involved in LVLMs under V-Bias to see whether visual biases in CLIP transfer to LVLMs. Tab. 3 reports the average results (bias based on outcome difference), revealing that CLIP models exhibit even greater biases than LVLMs. However, biases at the occupation pair level between CLIP and LVLMs show relatively weak correlations (0.26 and 0.19 for \u03c1 and \u03c4 coefficients on average). Further analysis of each counterfactual visual question pair reveals subtle differences in the biases of CLIP and LVLMs. For instance, there are only 874 (11%) overlapping biased samples between EVA-G [40] and InstructBLIP, indicating that LVLMs display distinct behaviors from CLIP models, despite both sharing the same image encoder. The result also indirectly supports our analysis that the consistent bias across visual and language modalities stems from the LLM-centric nature."}, {"title": "5 Conclusion and Future Work", "content": "This paper introduces GenderBias-VL benchmark, the first to evaluate occupation-related gender bias in LVLMs under individual fairness criteria. GenderBias-VL comprises 34,581 visual question counterfactual pairs covering 177 occupations, supporting bias probing in both multimodal and unimodal contexts. The extensive evaluation of 15 open-source LVLMs and the most advanced commercial LVLMs (GPT-4V and Gemini-Pro) reveals pervasive biases within these models. As LVLM development races forward, GenderBias-VL offers a vital benchmark to assess social bias risks before their real-world deployment.\nLimitation. Despite our best efforts, we acknowledge three major limitations in our research. First, the generation pipeline may contain latent biases, which may contribute to implicit biases exhibited by LVLMs. Therefore, the findings we present to enhance the understanding of bias in LVLMs should be interpreted within the context of our experiments. Second, our benchmark is confined to a binary gender framework, whereas gender is inherently fluid, as discussed in Sec. 3.1. Future research should incorporate a broader range of gender identities, including those from the LGBTQIA+ community, to provide a more comprehensive scope of bias assessment in LVLMs. Third, since our benchmark is based on U.S. Labor Force Statistics, the bias analysis primarily focuses on the Western world. It is crucial to extend this analysis to encompass diverse cultures and countries in future work.\nEthical statement and broader impact. Our paper aims to probe and benchmark the occupation-related gender bias in LVLMs. While evaluation results may raise ethical concerns and potentially harm readers, our intention is not to cause harm. Rather, our work seeks to facilitate bias evaluation for LVLMs, serving as a crucial initial step toward mitigating discriminatory outcomes."}]}