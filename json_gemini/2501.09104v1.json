{"title": "A Non-autoregressive Model for Joint STT and TTS", "authors": ["Vishal Sunder", "Brian Kingsbury", "George Saon", "Samuel Thomas", "Slava Shechtman", "Hagai Aronowitz", "Eric Fosler-Lussier", "Luis Lastras"], "abstract": "In this paper, we take a step towards jointly modeling automatic speech recognition (STT) and speech synthesis (TTS) in a fully non-autoregressive way. We develop a novel multimodal framework capable of handling the speech and text modalities as input either individually or together. The proposed model can also be trained with unpaired speech or text data owing to its multimodal nature. We further propose an iterative refinement strategy to improve the STT and TTS performance of our model such that the partial hypothesis at the output can be fed back to the input of our model, thus iteratively improving both STT and TTS predictions. We show that our joint model can effectively perform both STT and TTS tasks, outperforming the STT-specific baseline in all tasks and performing competitively with the TTS-specific baseline across a wide range of evaluation metrics.", "sections": [{"title": "I. INTRODUCTION", "content": "In the recent past, there has been a lot of work in developing large multimodal models which can handle both speech and text modalities [1]\u2013[7]. The underlying philosophy of this line of research is the development of models that can perform a variety of speech-to-text and text-to-speech tasks like automatic speech recognition (ASR), speech synthesis (TTS), speech-to-speech translation (S2ST), speech-to-text translation (S2TT), and spoken language understanding (SLU). There are two ways of accomplishing this. First, we can take a modular approach where individual models are built for speech and text processing tasks using large amounts of data and then combined in a way that the above multimodal tasks are performed by utilizing a specialized combination of the modality-specific modules. SeamlessM4T [4] and FunAudioLLM [7] take this approach. The second approach aims at training a single model that can handle all tasks with weight sharing across modalities [2], [3], [6].\nThe benefit of the first set of approaches is that models for individual tasks are easier to train, but the disadvantage is that combining them leads to an increase in model size and the tasks may not be aligned with each other. This disadvantage can be overcome using the second approach; yet, it can be challenging to train a model that can take both speech and text as input. Another feature of all the above models is that they are autoregressive; thus, their inference speed can be slow and they have a tendency to hallucinate [8]. Non-autoregressive (NAR) approaches for ASR [9] and TTS [10] are significantly faster during inference and do not hallucinate on account of being frame synchronous. Indeed, large NAR models have been introduced in Peng et al. [8], but they only perform speech-to-text tasks like ASR and S2TT. In this work, we explore whether we can perform speech recognition (STT) and speech synthesis (TTS) using a single multimodal model with a NAR approach.\nOur contributions: To the best of our knowledge, this is the first work that explores the possibility of performing STT and TTS jointly in a fully NAR manner. We utilize a CTC-based approach for STT and a FastSpeech-based approach for TTS. Our proposed multimodal model can take as input a sequence of text as a CTC alignment, a sequence of log-mel filterbank features, or a combination of both. The TTS task is done using a CTC alignment as input, which is another novelty of this work. We also train the model not only to perform STT and TTS, but also a set of tasks which use unpaired speech or text data for self-supervised learning and combined speech-text input for additional supervised learning tasks. Furthermore, we also propose an iterative refinement procedure during inference which effectively utilizes the multimodal nature of our model to refine STT and TTS predictions using a mask-predict approach.\nRelation with previous work: As mentioned before, many models have been proposed in the literature which perform STT and TTS tasks jointly, but all of them do so autoregressively. The SpeechT5 [2] model comes closest to our proposed model in that it performs STT, TTS and self-supervised tasks. However, this model is autoregressive and after pretraining, needs to be adapted for individual tasks separately. We train our model jointly for all the tasks and do not require separate adaptation. Regarding the use of text-only unpaired data for STT improvement, two important techniques were proposed by Sainath et al. [11] and Thomas et al. [12] which use token repetition and a so-called textogram, which is also repetition based, respectively, as the text modality input. Our model utilizes a masked CTC alignment for text-only data augmentation, which is a consistent input for the TTS task, and is completely non-autoregressive contrary to previous approaches. Semi-autoregressive STT has also been proposed recently to improve streaming STT [13], but our proposed iterative refinement approach is related more with recent work by Chi et al. [14] and Higuchi et al., [15]\u2013[18], for improving STT performance. However, our model is multimodal, i.e. it can take both speech and text as input, which allows the speech-text combined representation to be updated through iterations. In previous approaches, the speech representation from the encoder is fixed while the text from a decoder is updated and refined. Furthermore, our proposed iterative refinement can also work for TTS, which is completely novel to the best of our knowledge.\nWe show that it is possible to train a joint STT and TTS model in a NAR fashion. Although we find that the two tasks are not complementary to each other, using unpaired data during training and iterative refinement during inference leads either to parity with or better performance than unimodal baselines."}, {"title": "II. MODEL DESIGN", "content": "Our proposed model design is such that it can take speech, text, or a combination of both features as input. Let X be a sequence of log-mel filterbank (LFB) features and Y be the corresponding transcript. The multimodal model consists of four major components as shown in Figure 1 and outlined below.\nA. Components\nDuration model: The duration model converts the text Y to a C\u0422\u0421 alignment representation EA. To do this, each character in Y is interleaved with blanks (e.g. \"CAT\" is converted to \"_C_A_T_\") and fed to a bidirectional text encoder to get character embeddings. These embeddings are repeated R times\u00b9, based on the ground truth alignment at train time. At test time, we utilize the repeat predictions (R) from a duration predictor. Formally, this process is defined below,\n$E_y = TextEncoder(Embedding(addBlank(Y)))$\n$E_A = Repeat(E_y, R)$\n$R = DurationPredictor(E_y)$\n$L_{dur} = CrossEntropy(R, \\hat{R})$\nHere, TextEncoder(.) and DurationPredictor(.) are both modeled as 2-layer conformers; however, DurationPredictor(.) has an additional prediction layer on top. Ldur is the loss used to train the model to predict the number of repetitions. Note that EA is just an upsampled version of Ey. This part of the model is very similar to FastSpeech [10], except that we use character-based CTC alignments instead of a phoneme alignment. The composite model (left side of Figure 1) is noted as DurMod(.) in the remainder of the paper.\nMasking: We need a masking schedule for both speech and text modalities so that we can perform self-supervised learning with unpaired data and train the model for iterative refinement at inference. To mask text, we randomly replace p% of the characters with a <mask> token. When passing this through the duration model, we replace a blank token with <mask> if the prior character is also masked. Thus, we get noisy upsampled CTC alignment embeddings, EA. We do not mask the first blank token. For example, if the CTC alignment of \"CAT\" is \"_CCA_T_\" and its masked version is \"C<mask>T\", EA will correspond to \u201c_CC<mask><mask>T_\u201d. This module is represented as Masky (., p).\nWhen masking speech, we use two types of schedules. First, we use a wav2vec2.0 [19] masking schedule where we randomly sample p% of log-mel frames as the starting frames without replacement and mask M subsequent frames for each of the starting frames sampled. Masking a log-mel frame simply corresponds to replacing it with a zero-vector. This masking module is represented as Maskx1 (., p, \u041c).\nThe second masking schedule for speech is useful for iterative refinement of TTS predictions. If the number of time steps is T and the number of filters is F, we choose p% of T and F to get to and fo. Then, we mask all elements in X along the time axis at t = [to, T] and along the frequency axis at f = [fo, F]. This masking module is represented as Maskx2(., p).\nMultimodal encoder: The multimodal encoder forms the main component of our model design and contains the majority of the parameters in the system. The multimodal encoder takes as input (possibly masked) text embeddings and (possibly masked) log mel features; this enables the model to variously handle speech-only, text-only, or combined input. The two streams are added together as input to the model.\n$X = Mask_{X1}(X,p_X, M)$\n$E_A = DurMod(Mask_Y(Y, p_Y))$\n$Z = MMEncoder(X + E_A)$\nIf px = 1.0, it implies that the speech modality is absent and X is a sequence of zero-vectors per the definition of Maskx1(.). Similarly, if the text modality is absent, py = 1.0 and we get a sequence of <mask>s resulting in EA being a sequence of mask embeddings.\nTwo adjustments must be made to allow for the addition of modalities: first, the modalities must have the same sequence length; if a modality is absent, its masked version is upsampled to the length of the other modality. Second, in order to match embedding sizes, each modality is passed through a learnable linear transform and layer normalization [20] to match embedding sizes. This normalization is omitted from the above equation for brevity.\nTask-specific heads: The final component of our model converts Z into a model prediction. For text prediction, Z needs to be converted into a sequence of logits used to compute the CTC loss. For speech prediction tasks, Z is used to predict the log-mel features which are used to compute the L1 loss against the ground-truth. Formally,\n$O_X = W_X SpeechHead(Z) + b_X$\n$O_Y = W_Y TextHead(Z) + b_Y$\nHere, Ox and Oy are speech and text predictions respectively. SpeechHead(.) and TextHead(.) are modality specific non-autoregressive decoders modeled as 2-layer conformers. Wx, WY, bx and by are weights and biases of the prediction layer.\nB. Tasks\nWe use our proposed model to perform six different tasks which include both supervised tasks with paired data and self-supervised tasks with unpaired data. Thus, given task-specific data (Xtask, Ytask), the general forward propagation through our model is given below,\n$\\hat{E}_A = DurMod(Mask_Y(Y_{task}, p_Y))$\n$\\hat{X} = Mask_{X1/2}(X_{task}, p_X, M)$\n$Z = MMEncoder(\\hat{X} + \\hat{E}_A)$\n$O_X = W_X SpeechHead(Z) + b_X$\n$O_Y = W_Y TextHead(Z) + b_Y$\nBy controlling the task-specific data, the masking schedule and which task-specific head to use, we can define different modality-to-modality conversion tasks as explained below.\nSpeech-to-text (STT): The STT task uses paired data, (XSTT, YSTT). The input is speech, XSTT and the desired output is the corresponding transcript, YSTT. Thus, the text modality on the input side is completely masked, i.e. py = 1.0 in Equation 1. The speech modality is completely unmasked, i.e. px = 0.0. Finally, only the TextHead(.) is used to compute the prediction logits, OSTT from which we compute the STT loss as, LsTT = CTC-loss(OSTT, YSTT).\nText-to-speech (TTS): TTS also uses paired data, (XTTS, YTTs). Here, the input and output is text, YTTS and speech, XTTS respectively. This time, the speech modality on the input side is"}, {"title": "III. EXPERIMENTS", "content": "A. Data\n1) LJSpeech [24]: This is a single speaker dataset with audio amounting to roughly 24 hours. It contains 13,100 audio clips from which we randomly take 12,500 samples for training, 300 samples for validation and 300 samples for testing. Note that for this data, we use the same set of 12,500 samples for the supervised and self-supervised tasks, i.e. we do not have a separate set of unpaired data.\n2) LibriTTS [25]: This dataset contains speech from multiple speakers (around 2000) and is a subset of Librispeech [26]. The total data amounts to around 550 hours. We use the 50 hour \"clean-50\" subset as the paired data, 200 hour \"clean-200\" as the unpaired speech data and 300 hour \"other-300\" as the unpaired text data. We used the restored version of this dataset [27]. STT results are reported on the dev-clean (dc), test-clean (tc), dev-other (do) and test-other (to) subsets and TTS results are reported on the test-clean subset.\n3) Librispeech [26]: We also use the original Librispeech corpus. The train-clean-100 subset served as the paired data, train-clean-360 as the unpaired speech and train-other-500 as the unpaired text.\nB. Training details\nAll modules in the model are conformer blocks [28] with either 256 hidden units and 4 attention heads (for LJSpeech) or 384 hidden units and 6 attention heads (for LibriTTS and Librispeech). The MMEncoder has 12 layers and the rest have 2 layers each.\nFor multi-speaker TTS, we make use of pretrained speaker embeddings from the ECAPA-TDNN model [29]. These are added to the alignment embeddings, EA in the duration model for multispeaker data. For TTS, the model predicts 80-dimensional LFB features which are converted to the speech waveform using the HiFi-GAN vocoder [30]. When using text-only data, we do not have true CTC alignments to use for the T2T task. We train a standalone duration model using the paired data which is used to generate pseudo-CTC alignments.\nFor STT, we use characters at the output vocabulary and 80-dimensional LFB features as input. Only when using (XSTT, YSTT), we apply data augmentation in the form of SpecAugment [31] and speed and tempo augmentation [32] with factors of 0.9 and 1.1. The augmented data is only used for the STT task.\nAll joint models are trained for 100 epochs with a OneCycleLR annealing policy [33] of the learning rate whose peak value is 5e-4. A linear annealing happens after 30 epochs of warmup. We scale the peak learning rate with batch size as $lr = 0.0005 \\times \\sqrt{\\frac{bsize}{32}}$"}, {"title": "IV. LIMITATIONS AND CONCLUSION", "content": "Although our proposed model can perform both STT and TTS, we believe that the performance on these tasks can be improved further. For STT, we can use techniques like label smoothing [34], DropConnect [35], sequence noise injection [36] and self-conditioned CTC [37]. For TTS, we may use FastSpeech2's [38] approach of predicting pitch and energy as intermediate features. We can also utilize the entire 550 hours of paired data from LibriTTS instead of just the 50 hours. In addition, using a phoneme-based alignment as input instead of a character-level alignment can also help TTS.\nOur proposed model does open numerous possibilities for modeling various tasks like STT, TTS, S2ST, S2TT and SLU in a non-autoregressive manner. The fact that STT and TTS can indeed be modeled together as shown by our work in addition to the proposed improvements using unpaired data and iterative refinement is an encouraging step in building larger multimodal models."}]}