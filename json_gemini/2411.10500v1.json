{"title": "Edge-Only Universal Adversarial Attacks in Distributed Learning", "authors": ["Giulio Rossolini", "Tommaso Baldi", "Alessandro Biondi", "Giorgio Buttazzo"], "abstract": "Distributed learning frameworks, which partition neural network models across multiple computing nodes, enhance efficiency in collaborative edge-cloud systems but may also introduce new vulnerabilities. In this work, we explore the feasibility of generating universal adversarial attacks when an attacker has access to the edge part of the model only, which consists in the first network layers. Unlike traditional universal adversarial perturbations (UAPs) that require full model knowledge, our approach shows that adversaries can induce effective mispredictions in the unknown cloud part by leveraging key features on the edge side. Specifically, we train lightweight classifiers from intermediate features available at the edge, i.e., before the split point, and use them in a novel targeted optimization to craft effective UAPs. Our results on ImageNet demonstrate strong attack transferability to the unknown cloud part. Additionally, we analyze the capability of an attacker to achieve targeted adversarial effect with edge-only knowledge, revealing intriguing behaviors. By introducing the first adversarial attacks with edge-only knowledge in split inference, this work underscores the importance of addressing partial model access in adversarial robustness, encouraging further research in this area.", "sections": [{"title": "1. Introduction", "content": "In the last decade, adversarial attacks have posed significant challenges in deploying secure and robust deep learning models [6, 7, 34]. These attacks involve carefully crafted perturbations to input data that can effectively mislead model predictions, originating serious concerns about the safety and security of machine learning algorithms in various application domains, such as autonomous driving [30, 43] and healthcare [13].\nFrom a practical perspective, the severity of these attacks depends on the form of perturbation used by the attacker and on the threat model under investigation. Regarding the former, different types of attacks exist, including classic input-specific perturbations [6, 17, 34], universal adversarial perturbations (UAPs) [20, 39, 42], and physical attacks [4, 29, 36]. Additionally, an attack can either be crafted to follow a targeted objective, where the model is forced to predict a specific class, or an untargeted objective, which instead aims at a misclassification irrespective of the predicted class.\nIn terms of the threat model, adversarial attacks and the robustness of deep neural networks have been mostly studied in centralized environments, where the model is either fully accessible to the attacker (white-box) [34] or entirely inaccessible in a black-box setting [3]. However, with the rise of distributed learning systems [12, 18], where inference is split across multiple nodes, such as edge and cloud ones, new threat scenarios are emerging. As in this context attackers may gain access to a portion of the model only, a deeper investigation of the threats to which distributed learning is exposed becomes fundamental [1, 10].\nPrevious work on robustness of distributed learning has primarily focused on threats related to the training phase, where models can be susceptible to attacks mounted at training time [5, 37] and the attacker controls (a part of) the cloud node [9, 15, 28]. In contrast, motivated by the many hardware and system-level attacks that allow compromising edge nodes (even by means of physical access) [26, 27], this work addresses a different but yet practically-relevant threat model: the attacker can control the edge part only at the deployment (i.e., post-training) stage. Designing attacks in this context presents unique challenges as the attacker lacks access to the cloud part of the model, the final outputs, and even full knowledge of the class distribution, complicating the generation of effective adversarial attacks.\nContribution. To address these challenges, we propose a novel approach for generating universal adversarial attacks with knowledge limited to the initial layers of the model, which are commonly deployed at the edge side. The main idea, illustrated in Figure 1, relies on exploiting a small set of new samples to learn a binary separation of class-specific features from the available edge layers between a reference target class and all other classes. Even"}, {"title": "2. Related Work", "content": "In the following, we draw attention to our work by discussing related studies from two perspectives: universal adversarial attacks and the security threats posed within distributed learning scenarios.\nUniversal Adversarial Attacks. In recent years, adversarial perturbations have gained significant attention as a way to reveal critical weaknesses in the robustness of DNNs [6, 17, 34]. Among these, universal adversarial attacks are particularly important from a security standpoint, as they maintain their effectiveness across different inputs [19, 20, 32, 42]. Moosavi-Dezfooli et al. [20] were the first to demonstrate the existence of UAPs by introducing an adversarial optimization method that extends the DeepFool attack[21] to extract features that generalize across multiple images. Since then, various strategies have been developed to enhance UAPs' effectiveness [22, 24]. In these studies, also targeted formulations of UAPs were explored, which"}, {"title": "3. Background and Modeling", "content": "In this section, we first recap key terminologies related to split inference in distributed learning, and the threat model we consider. We then formalize universal perturbations, with a specific focus on the targeted formulation that drives the approach proposed in Section 4.\nSplit Inference. In split inference [12, 18], a DNN $f$ is divided into distinct portions. For simplicity, we consider DNNs split into an edge part $f_{edge}$ and a cloud part $f_{cloud}$, such that $f(x) = f_{cloud}(f_{edge}(x))$. In this work, $f$ is an image classifier, as commonly addressed in the UAP literature. Nevertheless, the whole approach and terminology can be seamlessly extended to other tasks. The edge part is composed by the first $e$ layers of the model, i.e., $L_{edge} = \\{l_1,l_2, ..., l_e\\}$, and processes an input $x$ by returning intermediate features $h^{l_e} = f_{edge}(x)$. These features $h^{l_e}$ are then transmitted to the cloud part, which completes the remaining inference to generate the final model output $f(x) = f_{cloud}(h^{l_e})$. When needed, we also denote by $h^{l_i} = f^{l_i}(x)$ the output of the model up to layer $l_i$.\nThreat Model. We study a secure setting in which the model output is not returned to the edge part [28]. We assume that: (i) the attacker can collect a small set of samples that differ from the training set; (ii) the attacker has white-box access to the edge portion of the model only, i.e., $f_{edge}$, allowing to study its activations and perform gradient backpropagation; (iii) the complete set of output classes $\\{1,..., N_c\\}$ is unknown to the attackers, while they can focus on a specific class (target class) to analyze the feature space at the edge.\nThe practical relevance of these assumptions is motivated as follows: (i)-(ii) the attacker can gain access to the edge node, even by means of physical access, hence being able to leak both the edge portion of the model and input samples; (iii) the attacker can classify a small set of samples independently of the model $f$ under attack.\nUniversal Adversarial Perturbations UAPs are particularly interesting from a security perspective due to their generalizability, as they maintain effectiveness across different samples without needing to be re-computed for each input. Formally, the objective is to find a perturbation $\\delta$ that induces mispredictions on any input $x$ of a distribution $X$. This problem can be formulated as follows:\n$\\max_{\\delta} E_{x \\sim X} [L(f(x + \\delta), y)] \\quad s.t. \\quad ||\\delta||_p \\leq \\epsilon, $ (1)\nwhere $y$ is the ground-truth output related to $x$, $L$ is a loss function (e.g., cross-entropy loss), and $\\epsilon$ is the maximum magnitude of $\\delta$ under a $p$-norm constraint. In this work, we focus on the $l_\\infty$ norm, commonly adopted in the UAP-related literature [41].\nParticularly relevant to this work is the targeted variant of UAPs, where the goal is to induce predictions towards a specific target class $y_t$ selected by the attacker. Optimizing targeted attacks involves minimizing a loss function with respect to $y_t$:\n$\\min_{\\delta} E_{x \\sim X} [L(f(x + \\delta), y_t)] \\quad s.t. \\quad ||\\delta||_p \\leq \\epsilon,$ (2)"}, {"title": "4. Edge-Only Universal Attack", "content": "In this section, we detail the edge-only universal attack following the pipeline shown in Figure 1. The attack consists of three main stages: (1) learning feature separations between target and non-target samples in the edge layers; (2) crafting the perturbation by exploiting these learned separations; and (3) applying an adversarial perturbation to influence the final model prediction.\n4.1. Learning Class-Feature Separations\nIn this stage, we aim to identify which intermediate features best represent the target class distribution. To this end, we assume that the attacker has access to a small set of samples linked to the target class, denoted by $D_t$ (e.g., 50 samples), and a complementary set of samples from other classes, denoted by $D_o$, with no particular restrictions on its distribution. We refer to the combined dataset as $D = \\{D_t \\cup D_o\\}$. Note that the attacker does not require detailed information about non-target samples: only binary labels indicating whether a sample belongs to the target class or not are required.\nGiven the access to the edge part of the model, $f_{edge}$, the attacker can extract features $h^l$ from any intermediate layer $l$ within $f_{edge}$. For each such layer $l \\in L_{edge}$, the attacker trains a binary classifier $g^l : R^{H^l} \\rightarrow [0,1]$, where $H^l$ is the dimension of the feature space at layer $l$, to discriminate the most representative features $h^l$ between the target and non-target classes. Formally, $g^l$ is trained as follows:\n$\\min E_{x \\sim D} [L_b (g^l(f^l(x)), \\hat{y}_t)],$ (4)\nwhere $L_b$ denotes the binary cross-entropy loss, $\\theta_{g^l}$ are the weights of $g^l$, and $\\hat{y}_t$ is an auxiliary label set to 1 for target samples and to 0 for non-target samples. By this binary classifier the attacker learns which features most strongly represent the target class and use them to guide the optimization of targeted universal perturbations.\nIt is also important to note that the depth of the edge part of the model, defined as the number of layers $e$ deployed at"}, {"title": "4.2. Adversarial Optimization", "content": "In the adversarial optimization stage, we aim to craft a universal perturbation that pushes an arbitrary sample $x$ to be classified in the target class $y_t$ by the binary classifiers $\\{g^{l_1},...,g^{l_e}\\}$ introduced above, so that $g^l(f^l(x + \\delta)) = 1$ for $l \\in L_{edge}$. This problem can be framed as a targeted universal optimization, where the objective is to craft malicious input patterns so that the perturbed features match those of the target class, thereby influencing the processing performed by the cloud part of the model. Extending the optimization problem of Eq. (2), the simplest approach to address this problem consists in adopting gradient ascent to each classifier $g^l$ as follows:\n$\\delta_{t+1} = \\delta_t + \\alpha \\cdot sign \\left( \\sum_{l \\in L_{edge}} \\nabla_{\\delta} g^l(f^l(x + \\delta_t)) \\right), $ (5)\nwhere $|\\delta||_\\infty < \\epsilon$. Note that we omit the loss function in this formula, as the gradient itself points directly towards the target label $\\hat{y}_t = 1$. However, we observed that the impact of the gradients is not properly balanced, as they are computed at different stages of the model. To address this issue, we balance the influence of multiple binary classifiers $g^l$ in the adversarial optimization by normalizing each respective gradient and combining them in the function $\\Phi$:\n$\\Phi(x, \\delta) = \\sum_{l \\in L_{edge}} \\frac{\\nabla_{\\delta} g^l(f^l(x + \\delta_t))}{||\\nabla_{\\delta} g^l(f^l(x + \\delta_t))||_2},$ (6)\nwhere each gradient $\\nabla_{\\delta} g^l(\\cdot)$ of classifier $g^l$ is normalized by its $l_2$-norm to ensure balanced contributions across layers. Both the usage of multiple gradients and the normalization allow improving the transferability of the attack to the cloud part, as shown in studies reported in Section 5.4.\nThe optimization is performed by iteratively updating the perturbation $\\delta$ in the direction of $\\Phi$, which pushes the intermediate features to resemble those of the target class across all edge layers, that is\n$\\delta_{t+1} = \\delta_t + \\alpha \\cdot sign \\left(\\sum_{x \\in B} \\Phi(x, \\delta_t) \\right),$ (7)\nwhere $|\\delta||_\\infty < \\epsilon$. The resulting perturbed samples can eventually be used to induce mispredictions, keeping their representation in [0, 1], i.e., $x = clip(x + \\delta, 0, 1)$."}, {"title": "5. Experimental Evaluation", "content": "After introducing the used settings, the section addresses the following research questions: \u2460 Are edge-only UAPs capable of propagating a strong misprediction effect to the cloud part, and how do they compare with a full-knowledge UAP formulation [20, 39]?; and 2 How the targeted objective in edge-layers propagates to the unknown cloud part? Ablations studies were also conducted, aimed at validating the effectiveness of the attacks under different settings.\n5.1. Settings\nAll the experiments were conducted on the ImageNet dataset [14]. We used class-balanced subsets of the Ima-geNet validation set: 5000 images as a test set for evaluating all attacks, 1000 images for $D_o$, and other 1000 images for $D_{opt}$. While $D_t$ was composed of only 50 samples for each selected target class, these also extracted from the validation set. For the attacked models, we selected CNNs architectures commonly used in distributed scenarios, including ResNet50 [11], Wide-ResNet101 [38], MobileNetV2 [31], and VGG16 [33]\u00b9.\nTo avoid an overly fine-grained analysis of internal layers, where feature representations between adjacent layers may exhibit minimal variation, we selected four key layers in each network, representing different depths within the backbone of each model. Specifically, these layers are chosen at depths of 1/4, 2/4, 3/4, and 4/4 of the backbone. Each of these layers serves as a potential split point, defining four configurations of the edge and cloud components, $f_{edge}$ and $f_{cloud}$, corresponding to different edge depths. For clarity, we denote a configuration of the edge component by the depth of the selected key layer that serves as the split point. For example, an edge depth of 1/4 means that the edge component includes all layers up to the 1/4 depth of the backbone (e.g., the first residual block in the ResNet architecture). Conversely, an edge depth of 4/4 indicates that the edge part covers the entire backbone of the model.\nTo simplify the notation, $l_i$ is used in the following to denote key layers only. As such, given a depth of the edge part, the set $L_{edge} = \\{l_1, ..., l_e\\}$ defined in Section 4 is formed by the key layer $l_e$ acting as a split point and the preceding key layers. For instance, for depth 2/4, the key layers are $L_{edge} = \\{l_1,l_2\\}$, where $l_1$ and $l_2$ correspond, for example, to the 3rd and 7th layer of the network, respectively.\nThe binary classifiers $g^l$ consist of a simple and lightweight module (avoiding overfitting) that includes: a conv block with kernel size 3, followed by an AvgPool and two fully connected layers with hidden size 128, separated by a ReLU activation. More details about the classifiers and the split points are in the supplementary material.\nRegarding the attack settings, we focused on $l_\\infty$ norm attacks, using $\\alpha = 2/255$ and the maximum number of epochs for the UAP optimization to 20, with a batch size of 100. In the following, we provide comparisons of edge-only"}, {"title": "5.2. Untargeted Effectiveness", "content": "To address research question \u2466, Figure 2 shows the effect on the test set accuracy when applying edge-only UAPs with magnitudes of $\\epsilon = \\frac{10}{255}$ and $\\epsilon = \\frac{16}{255}$. In particular, we selected five random target classes from ImageNet and then compared edge-only attacks under different depths of the edge part. For comparison, we also included classic, batch-level UAP attacks (both targeted and untargeted) [39] implemented through a cross-entropy loss. Please note that classic UAP assumes full model knowledge, whereas our edge-only attacks requires knowledge of the edge part only. Furthermore, UAP untargeted attacks assume full knowledge of the original labels of the tested samples, which are not available in edge-only attacks by definition (thus limiting their applicability in a targeted setting).\nAs reasonably expected given the nature of the attack, attacking a shallow edge part, hence basing on limited model knowledge, generally reduces the attack effectiveness (i.e., higher prediction accuracy) compared to cases with access to more layers of the distributed model. This trend is more pronounced for models like ResNet50 and Wide-ResNet101, while different observations arise for MobileNetV2 and VGG16, where edge-only UAPs, even when targeting the initial layers only, achieve similar effectiveness to full-knowledge attacks.\nAdditionally, analogously as found in previous work[20] that suggests the existence of dominant labels that occupy large regions in the image space, we observed that the choice of target class significantly impacts the generation of strong UAPs. This phenomenon is especially pronounced in edge-only attacks (e.g., see classes 'warthog' and 'crab' in Figure 2), where less-dominant classes result in even lower transferability."}, {"title": "5.3. Understanding Target Features Across Layers", "content": "While the proposed attacks may produce effective mispredictions, we also aim to assess the extent to which they produce cloud outputs aligned with the intended targeted objective 2 and how much steering the edge features towards a target may impact the remaining decision process of the model.\nTo directly evaluate whether the target objective is propagated to the cloud output, Figure 3 reports the target success rate (TSR) on the test set (i.e., the percentage of predictions matching the target class) when using edge-only UAPs crafted for various depths of the edge part. From the results, it is clear that (a) low-depth attacks do not induce mispredictions toward the target class. In fact, only attacks applied to the last layers of the backbone achieve a higher"}, {"title": "5.4. Ablation Studies", "content": "On the Attack Optimization Steps. We conducted ablation studies on the proposed attacks to separately assess the steps involved in optimizing edge-only perturbations (Section 4.2). Specifically, in Table 1 we reported the model accuracy under edge-only UAPs obtained under different settings. In particular, we evaluated the impact of combining the gradient of multiple edge layers during optimization ($\\sum_{l \\in L_{edge}}$ in Eq. (6)) and the effect of gradient normal-"}, {"title": "6. Conclusion and Future Work", "content": "In this work, we proposed, to the best of our knowledge, the first edge-only adversarial attack aimed at crafting universal adversarial perturbations without any knowledge of the cloud part model or the entire class distribution. This is accomplished by first training simple classifiers to identify target features in edge layers, and then performing adversarial optimization towards these features.\nExperimental results demonstrated that our attacks can produce a strong adversarial effect, even when the accessible edge part consists of a few initial model layers only, causing a high number of mispredictions comparable to standard UAPs crafted with full knowledge of the model and output class distribution.\nIn our experiments, particular attention was also devoted to understanding the propagation of the crafted per-"}]}