{"title": "MaxGlaViT: A novel lightweight vision transformer-based approach for early diagnosis of glaucoma stages from fundus images", "authors": ["Mustafa Yurdakul", "K\u00fcbra Uyar", "\u015eakir Ta\u015fdemir"], "abstract": "Background and objective: Glaucoma is a prevalent eye disease that progresses silently without symptoms. If it is not detected and treated at early stages, it can lead to permanent vision loss. Computer-assisted diagnosis (CAD) systems are crucial in timely and efficient eye disease identification. Deep learning-based CAD systems have become valuable tools in early detection and treatment.\nMethods: In this study, a novel lightweight model based on the restructured Multi-Axis Vision Transformer (MaxViT), MaxGlaViT, was designed for early detection of glaucoma stages. Firstly, MaxViT was scaled to optimize the number of blocks and channels of the model, resulting in a lighter architecture. Secondly, the stem in the MaxViT was improved by adding various attention mechanisms (CBAM, ECA, SE) after the convolution layers. As a result, a model was obtained that learns complex features more efficiently. In the third stage, the MBConv structures in the MaxViT blocks were replaced by advanced DL blocks (ConvNeXt, ConvNeXtV2, InceptionNeXt). The model was evaluated using the Harvard Dataverse V1 (HDV1) dataset that contains fundus images belonging to different glaucoma stages. In the experimental studies, state-of-the-art 40 Convolutional Neural Networks (CNN) and 40 ViT models were also evaluated on the HDV1 dataset to prove the efficiency of the proposed MaxGlaViT model.\nResults: Among CNN models, EfficientB6 outperformed all other CNN models with an accuracy of 84.91%. On the other hand, among ViT models, MaxViT-Tiny performed the best with an accuracy of 86.42%. Then, the scaled MaxViT achieved an accuracy of 87.93%. With the addition of ECA to the stem block, the accuracy increased to 89.01%. Another improvement was achieved by replacing the MBConv structure in the MaxViT block with ConvNeXtV2, with an accuracy of 89.87%. In line with all these results, scaled MaxViT was reconstructed using ECA in the stem block and ConvNeXtV2 in MaxViT block achieved an accuracy of 92.03%.\nConclusions: By testing 80 DL models for diagnosing glaucoma stages from fundus images, the proposed study is further expanded as the most comprehensive and comparative attempt in the current literature. In comparison with experimental and state-of-the-art methods, MaxGlaViT demonstrates notable performance, achieving 92.03% accuracy, 92.33% precision, 92.03% recall, 92.13% fl-score, and 87.12% Cohen's kappa score.", "sections": [{"title": "1. Introduction", "content": "The eye is a complex and unique organ that enables humans and many other forms of living beings\nto see. It helps us to understand the world around us by detecting light. The eye forms images by\nrefracting and focusing light, a process that takes place in retina located at the back of the eye.\nThanks to light cells (rod and cone cells), the retina converts images into electrical signals. Then\nthese signals are transmitted via the optic nerve to the brain, where they are interpreted and\ntransformed into an image. Eye health is crucial for keeping this process flowing smoothly, and\nany problems can affect our ability to see. Glaucoma is a chronic eye disease that occurs due to\nincreased intraocular pressure and causes blindness by damaging the optic nerve head. In the early\nstages of the disease, patients do not exhibit symptoms of vision loss, while in its advanced stages,\nvision loss becomes more apparent. Glaucoma known as silent theft of eyesight is an incurable\ndisease; however, with early diagnosis and medication, its progression can be prevented.\nOphthalmologists require a detailed digital image of the eye to diagnose glaucoma. Therefore, the\nstructural changes in the optic disc, nerve loss and atrophy in the peripapillary region are examined\nfrom the fundus image which is a medical imaging technique that displays the structure of the eye\nin color [1-3]. However, the number of ophthalmologists worldwide is insufficient, and the existing\nspecialists are working under heavy workloads. Moreover, the correct interpretation of the details\nand making the correct diagnosis requires experience. For all these reasons, there is a need for a\nCAD system that leverages advanced algorithms to analyze complex medical imaging data and\nsupports specialists. There are numerous studies and approaches aimed to diagnose glaucoma by\nanalyzing fundus images. One of the CAD methods is the classification of medical images based\non feature extraction and machine learning (ML) algorithms. Nayak et al. [4] extracted the cup-to-\ndisc ratio, the ratio of the distance between the center of the optic disc and the optic nerve head to\nthe diameter of the optic disc, and the ratio of the area of blood vessels from fundus images labeled\nas normal and glaucoma. They classified the features using an artificial neural network (ANN) and\nachieved a recall and specificity of 100% and 80%, respectively. Bock et al. [5] extracted a number\nof key features from fundus images using raw intensities, Fourier analysis, and spline interpolation\nto determine the Glaucoma Risk Index (GRI). Principal Component Analysis (PCA) was used to\ndimensionally reduce the features and classify them using support vector machines (SVM) with an\naccuracy of 80%, recall of 73%, and a specificity of 85%. Acharya et al. [6] used a combination of\ntexture and higher order spectral (HOS) features of the fundus image and the Random Forest (RF)\nalgorithm to determine the GRI and achieved a classification accuracy of 91.7%. Acharya et al. [7]\nused Gabor transformation and PCA approaches for feature extraction to classify normal and\nglaucoma images. Naive Bayes (NB) and SVM were tested and the most successful result was\nachieved with SVM with an accuracy of 93.10%, recall of 89.75%, and specificity of 96.20%. In\nparticular, studies mentioned in the literature review above (4-7) work on glaucoma disease\ndatasets as the assessment indicator."}, {"title": "2. Material and Methods", "content": "The basic concepts of CNN and ViT, attention modules, advanced DL blocks, the dataset\ndescription procedure, and various performance measurement metrics are included in the following\nsubsections."}, {"title": "2.1. CNN", "content": "CNN is a DL algorithm widely used in image analysis studies such as image classification,\ndetection, and segmentation. CNNs are designed to capture spatial hierarchies in data, using\nconvolutional layers to extract local patterns and pooling layers to reduce dimensionality, allowing\nthe network to learn features increasingly through each layer. CNNs are structured with layers that\napply filters to input images, progressively detecting more complex features through deeper layers.\nIn the literature, there are various CNN models that contain different topologies such as dense\nblock, ghost module, inception block, residual block, and separable convolution. Various CNN\narchitectures have been employed utilizing transfer learning to improve model performance for\nglaucoma stage detection. DenseNet models that contain dense blocks; EfficientNet models that\ninclude inverted residual blocks; GhostNet and variants that use ghost modules; Inception models\nwith various versions that include inception modules; ResNet, MobileNet, and NASNetMobile\nmodels that contain residual blocks; VGG models that contain stacked blocks (basic CNN layers);\nand Xception model that contains separable convolution were all assessed as CNN models."}, {"title": "2.2. ViT", "content": "Transformer is a recent DL algorithm that derives its power from self-attention and was first used\nin natural language processing (NLP). Vaswani et al. [18] used transformers for machine\ntranslation, Devlin et al. [19] proposed the BERT model, a bidirectional language representation\nthat takes context into account. Brown et al. [20] proposed the generative pre-trained transformer"}, {"title": "2.3. Attention Modules", "content": "In recent years, the attention mechanism has gained popularity and has been commonly used to\nimprove the accuracy of DL models [31,32]. The attention mechanism, in simple terms, detects\nwhich areas in the feature map are more important and so the model focuses on these areas.\nInjection of attention into convolution blocks is one of the implementation techniques, showing\ngreat potential for performance improvement in many studies [33,34]. Within the scope of this\nstudy, the popular attention mechanisms (CBAM, ECA, and SE) were used in the stem block of\nthe MaxViT model to improve performance. The general attention mechanisms are explained in\nthe following subsections."}, {"title": "2.3.1. SE", "content": "SENet which uses the SE module is a DL model that aims to improve the performance of models\nby using a customized attention mechanism to determine the importance of channels in feature\nmaps [35]. SE module, transforms the input feature map, compressing its dimensions into a\ncompressed format while maintaining the number of channels. In the process of compressing the\nfeature map, global average pooling (GAP) is applied to obtain a vector representing each channel.\nThe vector is of size 1x1xC and reflects the intensity of each channel. Then, the vector is processed\nwith fully connected layers and activation functions to generate attention scores that represent the\nimportance of the channels. As a result of that process, scaling coefficients of size 1x1xC are\nobtained. In the final stage, these coefficients are applied to the initial feature map on a channel-\nby-channel basis. Each channel is rescaled according to its attention coefficient so that the model\nmakes the important channels more salient and the unimportant ones weaker. The schematic\ndiagram of the SE module is shown in Fig. 1."}, {"title": "2.3.2. ECA", "content": "Inspired and in pursuit of improving SENet, Wang et al. found that dimensionality reduction has a\nside effect on channel attention based on empirical studies [36]. They proposed ECA, which avoids\ndimensionality reduction and captures cross-channel interaction in an efficient way. The ECA first\napplies the GAP to the input tensor. With GAP, average values are calculated for each channel to\nreduce the spatial dimensionality and the tensor is transformed into 1x1xC. Then, ECA uses one-\ndimensional convolution to learn the channel relationships. The resulting channel attention map is\nmultiplied by the input feature map on a channel-by-channel basis to scale the importance of each\nchannel so that the model emphasizes the important channels. The schematic diagram of the ECA\nmodule is shown in Fig. 2."}, {"title": "2.3.3. \u0421\u0412\u0410\u041c", "content": "CBAM [37] generates attention maps in two stages by analyzing the feature maps. In the first stage,\nthe channel attention module compresses the spatial dimension of the input feature map to\ndetermine how important each channel is. An average and maximum pooling method is used to\ncreate two different context descriptors. Then, the descriptors are processed through a shared neural\nnetwork, resulting in a channel attention map."}, {"title": "2.4. Advanced DL Blocks", "content": "Advanced DL blocks are specialized architectural components in DL models designed to enhance\nlearning efficiency, scalability, and model performance. Advanced blocks refine feature extraction\nwhile reducing the computational cost. These blocks are modular and can be stacked or combined\nwith other architectures, making them versatile tools for creating novel powerful models. The\ngeneral advanced DL blocks are explained in the following subsections."}, {"title": "2.4.1. ConvNeXt and ConvNeXtV2", "content": "ConvNeXt [38] is a modern CNN architecture that modernizes classical convolutional designs to\nachieve competitive performance on image classification tasks, competing with ViTs. Developed\nwith insights from both CNNs and ViTs, ConvNeXt refines traditional convolutional layers to\nimprove efficiency, scalability, and accuracy. Simplified convolutional blocks, large kernels for\nthe expanded receptive field, layer normalization and gaussian error linear unit (GELU) activation,\ninverted bottleneck design, and hierarchical feature representation are a breakdown of the key\nfeatures and innovations in ConvNeXt. It uses large kernel sizes of 7x7 to scan a larger area and\nprovide efficient information capture. It uses up-to-date techniques such as layer normalization and\nGELU activation function. It offers a simple yet effective structure with depthwise convolution for\ndownsampling followed by 1x1 convolution layers. In the ConNeXtV2 [39] model, which is an\nevolution of ConvNeXt, the LayerScale layer is removed from the block and the GRN module is\nadded to handle feature changes and avoid feature collapse in the learning process. The GRN layer\nincreases the contrast between channels, effectively improving the performance of the model."}, {"title": "2.4.2. InceptionNeXt", "content": "Yu et al. [40] proposed InceptionNeXt with the considering baseline as ConvNeXt. Compared to\nConvNeXt, InceptionNeXt is an architecture used to make large-kernel convolutions more\nefficient. In the InceptionNeXt architecture, the feature map is provided as input to an inception\nblock so that comprehensive feature extraction can be performed with filters of different kernel\nsizes. The extracted features are then combined and normalized. Finally, the feature map provided\nas input is merged with the final feature map with the residual block to obtain the resultant features."}, {"title": "2.5. MaxViT", "content": "MaxViT is a state-of-the-art DL architecture that effectively merges the strength of CNN and ViT.\nIntroduced by Tu et al. [30], MaxViT employs a hybrid approach that integrates both convolutional\nlayers and self-attention mechanisms. This design allows the model to capture local features\nthrough convolutional operations while also leveraging the global context provided by attention\nlayers. The MaxViT model achieved 86.5% top-1 accuracy on ImageNet-1K and 88.7% top-1\naccuracy on ImageNet-21K."}, {"title": "2.6. Description of the Dataset", "content": "All the DL models mentioned in this paper have been evaluated using the HDV1 dataset which is\nproposed in the study [7]. The dataset comprises three distinct classes: advanced glaucoma, early\nglaucoma, and normal (healthy). Early glaucoma class accounts for 289 samples, followed closely\nby advanced glaucoma with 467 samples. The normal class included 786 images. All images are\n224x224 in size and colorful fundus images. The classes of the data were labeled by means of a\nconsensus decision made by two experts and the dataset was divided into training, validation, and\ntest. The distribution of the data is shown in Table 3. Considering the class imbalance, online data\naugmentation techniques were applied during training."}, {"title": "2.7. Performance Measurement Metrics", "content": "Evaluating the classification success of DL models objectively is an important task. The confusion\nmatrix provides a tabulation of the predictions of model compared to the actual labels. It gives an\noverview of the performance of the model by distinguishing between correct and incorrect\npredictions for each class. The table contains values for true positive (TP: samples correctly\nclassified as positive), true negative (TN: samples correctly classified as negative), false positive\n(FP: samples incorrectly classified as positive) and false negative (FN: samples incorrectly\nclassified as negative).\nAccuracy indicates the overall success of the model, precision indicates the effect of false positives,\nand recall indicates the effect of false negatives. The fl-score is the harmonic mean of precision\nand recall and better evaluates performance in imbalanced datasets. Cohen's kappa measures the\nclassification success of the model by taking into account the chance factor. With these metrics, it\nis possible to evaluate the model from different perspectives and to better understand its strengths\nand weaknesses. The mathematical formula of the performance metrics is given in Eq. 1-5.\n$\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$ (1)\n$\text{Precision} = \\frac{TP}{TP + FP}$ (2)\n$\text{Recall} = \\frac{TP}{TP + FN}$ (3)\n$F1 \\; score = \\frac{2 * \\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ (4)\nCohen's kappa = $\\frac{P_o - P_e}{1-P_e}$ (5)\nIn Eq. 5, $p_o$ is defined as the observed proportion of agreement, which represents the relative\nfrequency of cases where the raters agree. $P_e$ is the expected proportion of agreement, calculated\non the basis of the frequency of each category, and represents the probability of random agreement\nbetween the raters."}, {"title": "3. Proposed MaxGlaViT Model", "content": "Fig. 8 shows the graphical representation of the proposed MaxGlaViT architecture with detailed\nlayers. The proposed MaxGlaViT comprises three main phases: scaling blocks and channels in\nMaxViT, improving stem block, and enhancing MaxViT block. The first phase optimizes the\nnumber of blocks and channels in the MaxViT architecture, which has a direct impact on the\ncomputational complexity of the model and the number of parameters. In this way, a lightweight\narchitecture is obtained without compromising the performance of the model and reducing the\ncomputational cost. In the second stage, various attention modules were added after the"}, {"title": "3.1. Scaling MaxViT", "content": "The scalability of ViTs makes them flexible, enabling them to be used effectively on datasets of\ndifferent sizes and for a variety of visual tasks. Furthermore, the ability to increase or decrease the\nnumber of parameters with scaling helps the model to adapt to different hardware capacities.\nRecent studies show that the performance of MaxViT can be improved by carefully optimizing the\nmodel parameters [41-43]. In particular, the number of blocks and channels has a direct impact on\nthe classification performance and computational complexity of the model. While the number of\nblocks increases the depth of features that the model can learn at each level, excessive block usage\nincreases the computational load and memory consumption. However, an excessive number of\nblocks and channels not only increases hardware burden but can also lead to model overfitting,\nwhich can negatively impact classification performance. In addition, if there is an imbalance\nbetween data and high-capacity models, there is a tendency for the model to overlearn the data,\nresulting in a less generalized and poorer-performing model.\nFor the purposes of this study, the number of blocks and channels of the stem block, while\nremaining in its original form, were set to block 2 and channel 32 for stage 1, block 2 and channel\n64 for stage 2, block 2 and channel 128 for stage 3, and block 2 and channel 256 for stage 4. These\nvalues have been chosen as optimal considering the amount and structure of the data. With the scaled model, a model with 6.2M parameters was\nobtained with 80% fewer parameters than the Tiny version (31M)."}, {"title": "3.2. Improving Stem and MaxViT Block", "content": "In MaxViT, the stem block is used to extract features from the image for the first time, and the\nextracted features are processed by MaxViT blocks. Since the features extracted in the stem block\nare used as input to other blocks, it is crucial to improve the stem block. To increase the feature"}, {"title": "4. Experimental Results and Discussion", "content": "This section provides a comprehensive assessment of the advanced DL models for glaucoma stage\ndetection on test data. The performance metrics and outcomes demonstrated in the confusion matrix\nassociated with the proposed model were discussed.\nThe experiments were performed on a computer with 128 GB of RAM, two Nvidia RTX 3090\n24GB GPUs combined with an NVLink bridge, and an Intel i9 processor. Python was utilized as\nthe programming language and Keras, a sub-library of Tensorflow, was used to perform CNN,\nViT, and MaxGlaViT models.\nAll DL models were trained with a transfer learning method based on ImageNet data weights.\nThroughout the experiments, a standardized configuration was consistently applied to all models.\nEach model was trained with categorical cross-entropy loss and the Adam optimizer with a learning\nrate of le-3 and a weight reduction value of 0.8. The batch size was set to 16 and the number of\ntraining epochs to 50 in all experiments. Furthermore, data augmentation techniques such as\nscaling, rotation, and vertical-horizontal shifting were used to reduce overfitting. These\nhyperparameters were selected in the same way as in recent work [14] for a fair comparison."}, {"title": "4.1. Results of the CNN Models", "content": "This section discusses the performance metrics of the various CNN models after the test process\nand the outcomes depicted in the confusion matrix. presents the experimental results of\nvarious CNN models for glaucoma stage detection on test data.\nAs listed in Table 5, the accuracy values of CNN models vary between 67.24% and 84.91%.\nAmong the GhostNet series models, GhostNet100 showed the highest performance with 81.43%\naccuracy and 81.42% fl-score. GhostNet-130 was the most successful model after GhostNet100"}, {"title": "4.2. Results of the ViT Models", "content": "The performance of the ViT-based models was also analyzed based on various measurement\nmetrics and results on test data were listed in Table 6.\nIn the DaViT series, the best result was obtained by the DaViT-Base model, with an accuracy of\n83.55% and an fl-score of 83.57%. The DaViT-Tiny model performed similarly, achieving 83.41%"}, {"title": "4.3. Results of the Proposed Model", "content": "The performance results of MaxViT series and the scaled version of the MaxViT are listed in Table\n7. Compared to MaxViT-Base and MaxViT-Large, MaxViT-Small, the MaxViT-Tiny model\nobtains the best result with 86.42% accuracy and 86.53% fl-score despite having fewer parameters\n(31M). It shows that smaller and optimized models can learn efficiently and generalize better when\nthey are of lower complexity."}, {"title": "4.3.1. Scaling The MaxViT", "content": ""}, {"title": "4.3.2. Improved Stem", "content": "The performance results of the MaxViT-Scaled model with the stem block enhanced with different\nattention mechanisms are given in Table 8. The original MaxViT-Scaled model shows a superior\nperformance with an accuracy of 87.93% and an fl-score of 87.96%. Moreover, the performance\nof the model is further improved by adding various attention mechanisms."}, {"title": "4.3.2. Improved MaxViT Block", "content": "The experimental results obtained by replacing MBConv in the MaxViT block with state-of-the\nart convolution modules are given in Table 9. The ConvNeXt module achieved better results than\nthe MaxViT-Scaled model with an accuracy of 88.15% and an fl-score of 88.16%. The advanced\nstructure of the ConvNeXt block provided a small but significant improvement to the model. The\nConvNeXtV2 module is the highest performing model with an accuracy of 89.87% and fl-score\nof 89.93%. ConvNeXtV2 block significantly improved the performance of the MaxViT-Scaled\nmodel and extracted features from the data more effectively. The optimized structure of the"}, {"title": "4.3.3. Improved Stem and MaxViT Block", "content": "At this stage of the study, three different experiments were conducted. In the first experiment,\nscaling the MaxViT model reduced its size (block and channel) and improved its performance. In\nthe second experiment, the stem block was enhanced with ECA, which resulted in a more robust\nfeature extraction and improved performance. In the last experiment, the performance was\nimproved by adding different convolutional blocks to the MaxViT block. Considering the results\nof the previous experiments, the power of the ECA attention module and ConvNeXtV2\nconvolutional block integrated to the scaled MaxViT model as a best combination. We also\nexperimented ECA with other convolutional blocks to prove the consistency of the proposed\napproach. The experimental results show the effects of combinations on the model and the results\nare presented in detail in Table 10."}, {"title": "5. Conclusion", "content": "Glaucoma is a chronic eye disease and it leads to irreversible vision loss if diagnosed at an early\nstage. This paper presents a CAD system to assist ophthalmologists in the diagnostic process of\nglaucoma stages. Experiments were performed on three main DL models, namely CNNs, ViTs and\nMaxGlaViT. Among 40 CNN models, EfficientB6 was the most successful model with an accuracy\nof 84.91%. On the other hand, among the ViT models, MaxViT-Tiny achieved the highest\nperformance with an accuracy rate of 86.42%. We then scaled the number of blocks and channels\nof the MaxViT-Tiny, resulting in a lightweight model with 6.2M parameters and an accuracy rate\nof 87.93%. After adding ECA to the stem block, the accuracy rate increased to 89.01%. Another\nimprovement was made by replacing the MBConv structure in the MaxViT block with\nConvNeXtV2 and an accuracy of 89.87% was obtained. In the last stage, the MaxGlaViT model\nwas obtained by using ECA and ConvNeXtV2 block on the stem and MaxViT block, respectively,\nand the accuracy was increased to 92.03%. Experimental results prove that the proposed\nlightweight MaxGlaViT model is among the most advanced models in this field by showing\nsuperior performance in glaucoma diagnosis. In future work, potential improvements and\nmechanisms that can be applied to the block, grid attention and head parts of the MaxViT model\nwill be investigated and experiments will be conducted."}, {"title": "Declaration of competing interest", "content": "The authors declare that they have no known competing financial interests or personal relationships\nthat could have appeared to influence the work reported in this paper."}, {"title": "CRediT authorship contribution statement", "content": ""}]}