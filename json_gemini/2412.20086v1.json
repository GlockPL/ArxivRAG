{"title": "MAFT: Efficient Model-Agnostic Fairness Testing for Deep Neural Networks via Zero-Order Gradient Search", "authors": ["Zhaohui Wang", "Min Zhang*", "Jingran Yang", "Bojie Shao", "Min Zhang"], "abstract": "Deep neural networks (DNNs) have shown powerful performance in various applications and are increasingly being used in decision-making systems. However, concerns about fairness in DNNs always persist. Some efficient white-box fairness testing methods about individual fairness have been proposed. Nevertheless, the development of black-box methods has stagnated, and the performance of existing methods is far behind that of white-box methods. In this paper, we propose a novel black-box individual fairness testing method called Model-Agnostic Fairness Testing (MAFT). By leveraging MAFT, practitioners can effectively identify and address discrimination in DL models, regardless of the specific algorithm or architecture employed. Our approach adopts lightweight procedures such as gradient estimation and attribute perturbation rather than non-trivial procedures like symbol execution, rendering it significantly more scalable and applicable than existing methods. We demonstrate that MAFT achieves the same effectiveness as state-of-the-art white-box methods whilst improving the applicability to large-scale networks. Compared to existing black-box approaches, our approach demonstrates distinguished performance in discovering fairness violations w.r.t effectiveness (~ 14.69\u00d7) and efficiency (~ 32.58\u00d7).", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning (DL) has become an indispensable tool in various domains, including healthcare, finance, weather prediction, image recognition and so on [1-4]. However, as DL models increasingly influence critical aspects of human lives, it has been found that they are vulnerable to slight perturbations [5-8]. In addition to robustness risks, Deep Neural Networks (DNNs) also face fairness threats. DL fairness is a growing area of concern that aims to address bias and unfairness in DNNs.\nDiscrimination in DNNs may arise from multiple sources, such as biased training data, inadequate features, or algorithmic discrepancies [9]. To detect and evaluate software bias, a growing number of studies have been targeted at group fairness and individual fairness. Individual fairness[10, 11]emphasizes treating similar individuals similarly, regardless of what a protected group they belong to. It evaluates the model on the level of individual instances, which is more fine-grained and can capture subtle biases that may be ignored by the former[11]. Thus we focus on individual fairness to develop methods that can effectively identify and address biases in DNNs at the level of individual instances by generating a large number of individual discriminatory instances. Those instances can be used to retrain models to ease the discrimination of DNNs. There have been several relevant attempts [11-13] on the problem.\nIn the traditional field of machine learning, existing black-box fairness testing methods perform well. Galhotra et al. proposed THEMIS which randomly samples each attribute in the neighborhood and identifies biased instances to measure the frequency of discrimination [11]. Udeshi et al. first proposed a two-stage global probabilistic search method called AEQUITAS to search discriminatory instances [13]. Agawal et al. proposed a method called Symbolic Generation(SG), which combines two well-established techniques: local interpretability and symbolic execution [14].\nHowever, these methods often shows low efficiency when they struggle to apply to DNNs, given the architectural and algorithmic differences between machine learning and deep learning. The unique challenges posed by DNNs, such as the complexity and non-linearity of their decision boundaries, demand novel efficient approaches to assess and mitigate biases.\nIn this case, Zhang et al. proposed a fairness testing approach ADF for DNNs. This approach still combines two-phase generation to search individual discriminatory instances based on input-specific probability distribution which depends on model internal information [15]. Then, Zhang et al. confirmed that ADF is far from efficient, and further proposed EIDIG to systematically generate instances that violate individual fairness. EIDIG inherits and further improves ADF to make it more effective and efficient [16]. However, white-box approaches such as ADF and EIDIG are not applicable in model-agnostic scenarios.\nTo this end, we propose a novel black-box individual fairness testing method called Model-Agnostic Fairness Testing (MAFT) which can be used in DNN fairness testing without requiring access to their internal workings. MAFT inherits the workflow from EIDIG, so it is almost the same efficient as EIDIG. By converting the use of a real gradient into an estimated gradient, MAFT removes EIDIG's dependency on model, making it a model-independent black-box method for fairness testing of DL. Despite the growing interests in DL fairness, there are only a few black-box fairness methods available that work effectively on deep models, making MAFT a valuable addition to the field.\nWe have implemented our framework MAFT and compared it with both advanced white-box methods and black-box methods. The overal improvement of MAFT over ADF is 7.92% and 70.77% in effectiveness and seperately, consistent with state-of-the-art EIDIG, outperform black-box methods AEQUITAS and SG. MAFT demonstrates a substantial improvement over AEQUITAS and SG, achieving an increase of 1369.42% in effectiveness over AEQUITAS and a 3158.43% enhancement in efficiency compared to SG. These results highlight MAFT's significant advancements in black-box fairness testing domain.\nOverall, we make the following main contributions:\n\u2022 We propose a model-agnostic approach MAFT to do fairness testing for different models without knowing the inner information of models. This versatility allows for broader applications across different DNN systems.\n\u2022 We evaluate MAFT against white-box methods ADF and EIDIG, along with black-box methods AEQUITAS and SG with 14 benchmarks on seven real-word datasets. The experimental results show that MAFT is almost the same as the state-of-the-art white-box method EIDIG in performance and far outperforms current black-box methods."}, {"title": "2 BACKGROUND", "content": "A deep neural network usually contains an input layer for receiving input data, multiple hidden layers for learning, and one output layer for formatting the outputs.\nUsually, we can view a trained DNN as a composite function F(x) and compute its gradient using the chain rule implemented as backpropagation easily. The Jacobian matrix of F(x) w.r.t a specific x can be expressed as [16]:\n$\\displaystyle J_{F}(x)={\\frac {\\partial F(x)}{\\partial x}}={\\begin{bmatrix}{\\frac {\\partial F_{1}(x)}{\\partial x_{1}}}&\\cdots &{\\frac {\\partial F_{1}(x)}{\\partial x_{n}}}\\\\\\cdots &\\cdots &\\cdots \\\\{\\frac {\\partial F_{m}(x)}{\\partial x_{1}}}&\\cdots &{\\frac {\\partial F_{m}(x)}{\\partial x_{n}}}\\end{bmatrix}}_{n\\times m}$ (1)\nwhere the m-th line is the gradient vector of the m-th output element respect to input data x.\nIn many cases, the information within hidden layers of a neural network remains unknown except input data and output confidence, making it intractable for white-box methods such as ADF and EIDIG. However, our proposed approach MAFT solely relies on knowledge of input data and output probabilities to find individual discriminatory instances, yielding excellent performance."}, {"title": "2.2 Individual Discrimination", "content": "We denote X as a dataset and its attributes set A = {A1, A2, ..., An}. The input domain is denoted as I and I = I\u2081 \u00d7 12 \u00d7 ... \u00d7 In if each attribute Ai has a value domain Ii. Then we use P to denote protected attributes of dataset X like age, race, or gener, and obviously that P \u2282 A and use NP or A \\ P to denote non-protected attributes. As for a DNN model trained on X that may include discrimination, we use D(x) to denote its output label on x.\nDEFINITION 1. Let x = [x1, x2,...,xn], where xi is the value of attribute Ai, is an arbitrary instance in I. We say that x is an individual discriminatory instance of a model D if there exists another instance x' \u2208 I which satisfies the following conditions:\n(1) \u2200q \u2208 NP, xq = xq;\n(2) \u2203p \u2208 P, xp \u2260 xp;\n(3) D(x) \u2260 D(x').\nFurther, (x, x') is called an individual discriminatory instance pair. Both x and x' are individual discriminatory instances.\nEXAMPLE 1. Let's consider a dataset about person information with 10 attributes, where the gender of a person is chosen as the protected attribute. We have the following pair (x, x') from the dataset as an example:\nx: [0, 1, 30, 1, 2, 0, 0, 1, 1, 0]\nx': [0, 0, 30, 1, 2, 0, 0, 1, 1, 0]\nWe highlight the gender attribute in red for clarity. Except gender, x and x' have the same feature. If the decision-making system provides different prediction labels for them, it would be thought as making decisions based solely on gender and ignoring any other attributes. As a result, x would be considered as an individual discriminatory instance with respect to gender."}, {"title": "2.3 Adversarial Attack", "content": "Recently, researchers have discovered that DNNs are vulnerable to robustness issues. Even state-of-the-art models can be easily deceived when attackers tamper the original input with minor distortion that are unrecognizable to humans. In light of this, various adversarial attack methods have been proposed to enhance model robustness. These methods have also been found useful in other domains for generating adversarial samples to meet specific requirements, such as fairness testing[15, 16].\nGradient-based adversarial attacks leverage gradients to minimize changes to the input while maximizing changes to the output of the sample. In [5], FGSM(Fast Gradient Sign Method) is first proposed as a one-step attack that perturbs the input data by adding small perturbation in the direction of the gradient of the loss function with respect to input according to the following Equation:.\n$\\displaystyle x^{adv}=x+\\epsilon \\cdot sign(\\nabla _{x}L(x,y))$ (2)\nwhere L is loss function of D, y is predicted class given by D(x) and \u2207xL(x, y) is gradient of loss function L on x w.r.t label y. The perturbation is scaled by a small value noted by \u20ac which controls the magnitude of the perturbation and real direction is only determined by the sign of gradient. FGSM is known for its simplicity and effectiveness in generating adversarial examples. Some iterative versions extending FGSM [6, 17] were later proposed. Different from these methods, JSMA(Jacobian-based Saliency Map Attack) [7] adopts the gradient of the model output instead of the loss function, which omits the backpropagation through loss function at each iteration. Inspired by this, a precise mapping between inputs and outputs of DNNs can be established with less time.\nIn [18], the author proposed a simple black-box adversarial attack. By randomly sampling a vector from a predefined orthonomal basis and then either adding or subtracting it to the target image, the DNN output could be changed. Similar to the setup for training alternative models, the author of [19] proposed a novel black-box attack that also only has access to the input (image) and output (confidence level) of the target DNN. Inspired by this, an efficient vectored gradient estimation method is proposed in our work to guide the generation of discriminatory individual instances."}, {"title": "2.4 Fairness Testing Problem Definition", "content": "Before introducing a fairness testing problem, we would like to give a formal definition of perturbation on non-sensitive attributes.\nDEFINITION 2. We use x to denote the seed input and p(x) to denote the instance generated from x by perturbation. We define the perturbation function as follow:\np: I \u00d7 (A\\P) \u00d7 \u0393 \u2192 I\nwhere I is the set of possible directions for perturbation, e.g., \u0393 is defined as {-1, 1} for a single discrete attribute.\nA system tends to make discriminatory decision when the system encodes individual discrimination. Based on this, we define fairness testing problem as follow:\nDEFINITION 3. Given a dataset X and a DNN model D, we attempt to generate as many diverse individual discriminatory instances which violates fairness principle in D (they can be used to mitigate discrimination) by perturbing the seed inputs in the dataset."}, {"title": "3 METHODOLOGY", "content": "In this section, we present our algorithm called Model Agnostic Fairness Testing (MAFT) for generating individual discriminatory instances. We first describe how to estimate gradient from the black-box model for further gradient-based two-phase algorithm. Then we introduce total workflow of MAFT shown in Fig 1."}, {"title": "3.1 Zero-Order Gradient", "content": "To leverage the gradient of any general black-box model f for fairness testing, we employ the asymmetric difference quotient to estimate the gradient.\nWe first approximate the derivative of function f with a small constant h:\n$\\displaystyle f'(x)=\\lim _{h\\to 0}{\\frac {f(x+h)-f(x)}{h}}\\approx {\\frac {f(x+h)-f(x)}{h}}$ (3)\nFurthermore, we can extend this expression to the predicted probability vector f of DNNs whose inputs consist of multi-dimensional features [20]:\n$\\displaystyle g_{i}:={\\frac {\\partial f(x)}{\\partial x_{i}}}\\approx {\\frac {f(x+he_{i})-f(x)}{h}}$ (4)\nwhere gi is the gradient $\\frac{\\partial f(x)}{\\partial x_{i}}$ and ei is a standard basis vector with h only the i-th component set to 1.\nSo we can get estimated gradient \u011fi:\n$\\displaystyle {\\tilde {g}}_{i}:={\\frac {f(x+he_{i})-f(x)}{h}}$ (5)\nExample 2. As an illustration of the asymmetric difference formula: $f'(x) \\approx (f(x + h) - f(x))/h$, we consider a simple one-dimensional function $f(x) = x^2$ and choose the perturbation size h = 0.001 to compute the estimated gradient of f at x = 2. By substituting x = 2 into the difference formula, we can get the estimated gradient value step by step: $f' (2) \\approx (f(2 +0.001) \u2013 f(2))/0.001 = ((2 +0.001)^2-2^2)/0.001 = 4.001$. We can see that the estimate is very close to the real gradient 4 of f at x = 2."}, {"title": "3.1.1 Estimation Error Analysis", "content": "It is important to note that the estimation error is of order O(h). We can expand f(x + hei) using the first-order Taylor series around x:\n$\\displaystyle f(x+he_{i})=f(x)+f'(x)\\cdot he_{i}+O(h^{2})$ (6)\nwhere f'(x) is the first derivative of f(x), and O(h\u00b2) represents all higher-order terms that are of order h\u00b2 or higher. Now, substituting this expansion into the formula of \u011fi:\n$\\displaystyle {\\tilde {g}}_{i}={\\frac {f(x)+f'(x)\\cdot he_{i}+O(h^{2})-f(x)}{h}}=f'(x)\\cdot e_{i}+O(h)$ (7)\nThe estimation error on dimension i is given by the difference between the true gradient and the estimated gradient:\n$\\displaystyle Error=f'(x)\\cdot e_{i}-{\\tilde {g}}_{i}=f'(x)\\cdot e_{i}-(f'(x)\\cdot e_{i}+O(h))=O(h)$ (8)\nWe can see that the estimation error of the asymmetric zero-order gradient on dimension i is proportional to h. This means that the error decreases linearly with the step size h.\nHowever, the perturbation size h must be chosen carefully to ensure that the perturbation is not rounded down too much by finite-precision numerical computations in practical phases. Despite concerns about numerical precision, obtaining an accurate gradient estimate is often unnecessary for successful searching. For instance, the FGSM [5] only requires the sign of the gradient, not its exact value, to discover adversarial examples. Thus, even if our initial approximations may lack accuracy, they can achieve high success rates, as demonstrated by our experiments in Section 4."}, {"title": "3.1.2 Original Zero-Order Gradient Algorithm", "content": "Based on zero-order gradients, we propose a naive original Non-Vectored Zero-Order Gradient Algorithm 1. The algorithm aims to estimate the gradient of a given black-box model f at a given input instance x. It perturbs each attribute of x with a specified perturbation step size and then obtains model output corresponding to the perturbed instance (lines 7,8). The finite difference is then used to calculate the gradient in one dimension (line 9). It proceeds iteratively until gradient values are obtained in all dimensions (lines 5-9).\nIn this version, gradient estimation for each attribute is calculated in an explicit loop which has obvious drawbacks. Its main disadvantage is computational inefficiency, especially for large models and high-dimensional data, where the computational time required increases linearly with the number of input attributes. If the input instance has n attributes, the process involves one forward propagation for the original input, followed by n loops to perform forward propagation for each of the n perturbed variants. Additionally, differential calculations are performed separately for each variant to obtain the complete gradient, resulting in increased computational overhead. This results in a total of n + 1 forward propagation steps and n differential calculations. Therefore, it is too slow for real-world applications, especially when dealing with high-dimensional data.\nExample 3. This time, suppose we have a two-dimensional function: $f(x_1,x_2) = x_1^2 + x_2^2$ and we still use h = 0.001. If we have an input instance x = [x1, x2] = [2,3] which has two attributes, we should compute the asymmetric finite difference for each dimension to get the complete estimated gradient: First, we compute $g_1 \\approx (f(x_1+h, x_2) - f(x_1, x_2))/h$ and then $g_2 \\approx (f(x_1, x_2 + h) \u2013 f(x_1,x_2))/h$. Thus, the zero-order gradient for x = [2, 3] is [4.001, 6.001].\nIn this example, we only need to compute twice to get the estimated gradient since the function f has only two attributes (x1 and x2). Note that it consumes much time to get the value of f (x1+h, x2) or f(x1, x2 + h) which is a complete forward propagation in deep learning. However, in most scenarios, the number of attributes can be larger, often exceeding ten or even more, which means the entire model must propagate an input ten or more times to estimate the gradient. The efficiency of the original non-vectored algorithm strongly depends on the number of attributes, making it less desirable for high-dimensional input instances. We aim to overcome this limitation by proposing a more efficient vectored zero-order gradient algorithm."}, {"title": "3.1.3 Vectored Zero-Order Gradient Algorithm", "content": "To overcome the limitations of the original version algorithm and improve computational efficiency, we subsequently introduce more efficient Vectored Zero-Order Gradient in Algorithm 2.\nThis algorithm takes instance x, a black-box model and a hyperparameter perturbation_size as its input arguments and returns the estimated gradient. We first initialize h with specified perturbation size and denote the feature numbers of the input instance x as n (lines 1,2). If we want to complete the calculation of the entire estimated gradient with only a fixed few times of forward propagation, we'd better obtain the gradient of model output confidence on right class p, denoted as \u2207Fp(x), to a set of variant inputs with different features perturbed at the same time. Note that every element of \u2207Fp(x) on different features should be independent. To achieve this goal, we first should construct an input square matrix X, each row of which is a variant of vector x which has been perturbed on the i-th feature (lines 3,4). For the model, each row in X is an independent input instance, so the perturbation of each one has no impact on the results of others. We first construct a diagonal matrix of n x n, where the diagonal element value is h (line 3). It then adds x row by row to obtain targeted square matrix X consisting of n perturbed instances (line 4). In addition to the necessary forward propagation to obtain the black-box model output confidence corresponding to the original input x (line 6), we only need to perform forward propagation once to obtain the confidence corresponding to each perturbed instance without knowing any internal structure of this model(line 5). Then we use the differential calculation to get a column vector, where each component is the derivative to the corresponding perturbation and reshape it to get the zero-order gradient we need (lines 7,8). Finally, we calibrate the sign of the estimated gradient according to whether the confidence is greater than 0.5 (lines 9-12).\nThe calculation in the above process does not need to rely on any internal information of the model or internal calculation process, it only needs to obtain the confidence level of the model output Fp(x) in the correct classification p. Therefore, the above process can be widely used in different scenarios to estimate gradient.\nOn the other hand, by performing perturbations on entire vectors simultaneously, the vectored version streamlines the computation, leading to better performance. It enables us to estimate the entire gradient with just one or a fixed number of forward propagation, regardless of the number of attributes, which is crucial for practical applications, as it empowers us to efficiently conduct fairness testing and model analysis on large-scale datasets and complex models, making the fairness testing process much more feasible and time-effi cient. This efficiency improvement greatly enhances the effectiveness and practicality of our proposed algorithm for fairness testing in real-world deep scenarios.\nIn essence, the efficiency achieved by the vectored approach brings it close to the computational efficiency of directly utilizing the computational graph for backpropagation, as demonstrated in our experiments as shown in 5(d). This efficiency enhancement is particularly significant for adversarial searching tasks involving black-box models, where computational speed is crucial for effective and timely exploration of potential adversarial examples."}, {"title": "3.2 Two-Phase Generation", "content": "In this section, we will introduce the workflow of MAFT, which consists of two sequential phases shown as Figure 1 and discuss the improvement of MAFT by transferring it to the black-box fairness testing domain."}, {"title": "3.2.1 Global Generation", "content": "Algorithm 3 shows the procedure of global generation. We adopt the global generation phase to accelerate and diversify discrimination generation. The intuition of it is shown in Fig 2(a). We first cluster the original input data set X with clustering algorithms such as K-means [21] with the goal of discovering diverse instances (line 2). We sample a seed x from the clusters in a round-robin fashion (line 4). For each selected seed, we perform max_iter iterations to find global discriminatory instances (lines 6-21).\nAccording to Definition 1, we need to find an individual discriminatory instance pair to identify an individual discriminatory instance. So we get all instances that differ only in protected attributes from x as a set similar_x(lines 7,8), whose size is the number of all possible combinations of the selected protected attributes in I except x. Then we check whether x violates individual fairness by identifying whether existing individuals in the similar_x have different labels with x. If so, x can be added to global_id as a global discriminatory instance and the iteration stops(lines 9-11). Otherwise, we iteratively perturb x until a new discriminatory instance is generated or max_iter is reached(lines 12-21). We have to find a new discriminatory pair (x, x'). To solve this problem, EIDIG chooses to traverse similar_x to select an instance x' that has the biggest difference in model output with x(lines 12,13) because the intuition is that x and x' are more likely to be separated by the decision boundary of the model after perturbation if the Euclidean distance between their model predictions is maximized. Then, we perturb x and x' simultaneously on non-protected attributes to make one of them cross the decision boundary in the opposite direction of the gradient by decreasing the model prediction confidence on their original label (lines 16-20). Momentum [22] is used as an optimization method for speeding up the procedure by accumulating local gradient and increasing efficiency, because it can help stabilize update and escape from local minimum or maximum[23](lines 14,15).\nEIDIG constructs a direct and precise mapping from input feature perturbation to output variation, which should be done by internal backpropagation of model for gradient calculation. However, we replace this computation of \u2207xFp(x) with our estimated gradient shown as ComputeGrad(x) to make this method independent of the model itself.\nAt last, we clip the generated instance x to keep it within the input domain I (line 21). Finally, this algorithm returns all generated discriminatory instances (i.e., global_id) which will be used as the seed inputs for local generation phase."}, {"title": "3.2.2 Local Generation", "content": "Algorithm 4 shows the contents of local generation phase, which quickly generates as many discriminatory instances as possible around the seeds global_id generated by the global generation phase. The intuition of it is shown in Fig 2(b).\nUnlike the global phase, which maximizes output variation to discover potential individual discriminatory instance pairs at each iteration, the local phase focuses on changing model variation to maintain the original predictions from the model. In doing so, we could get more similar discriminatory instances, which is motivated by DNN robustness that similar inputs lead to similar or the same outputs [8, 24]. For this, we just need slight perturbation on the local phase.\nFor each global seed from global_id, there must be a similar instance x' which has different label(lines 6-8). Then we also use gradient information to guide attribute selection and perturbation(lines 9-11, 14-17). To keep the prediction, we prefer to select an attribute that has less impact on the model, and thus the instance can keep the same label after perturbation. We calculate normalized probabilities of non-protected attributes as attribute contribution according to reciprocals of their gradients(line 11). Then we use this probability to choose an attribute to perturb on random direction s (lines 14-17), because we tend to choose an attribute that has less impact on the model after perturbation. If x becomes a new discriminatory instance after perturbation, we add it to local_id set (lines 21,22). Otherwise, we reset x and corresponding probability to next iteration. EIDIG shows that the information guiding attribute selection and perturbation is likely to be highly correlated at each iteration due to small perturbation in the local generation phase, so it chooses to calculate the attribute contributions every few iterations(lines 5-12).\nDuring this phase, EIDIG still use \u2207xFp(x) to establish a direct and precise mapping from input perturbation to output variation on original right label p to compute normalized probability. However, we replace this computation of \u2207xFp(x) with our estimated gradient shown as ComputeGrad(x) again to make this method independent of the model itself(lines 9,10).\nAt this time, we have generated a large number of individual discriminatory instances, which can be used for retraining to remove bias from the original model."}, {"title": "4 IMPLEMENTATION AND EVALUATION", "content": "In this section, we present experiments designed to evaluate the performance of MAFT and explore why estimated gradient is effective and efficient. The experiments can be structured into three primary research questions (RQs):\nRQ1: How does the choice of hyperparameter affect the performance of MAFT?\nRQ2: Given the selected hyperparameter, how does MAFT compare with AEQUITAS, ADF and EIDIG in terms of effectiveness and efficiency?\nRQ3: To what extent does the estimated gradient in MAFT match the real gradient in EIDIG in terms of effectiveness and efficiency?"}, {"title": "4.1 Experimental Setup", "content": "Baselines. According to the features of different methods that are shown in Table 1 (THE and AEQ are the abbreviation of THEMIS and AEQUITAS separately), we can know that model-agnostic methods such as AEQUITAS and SG are ineffective and inefficient. To answer these questions, we select to utilize AEQUITAS, SG, ADF and EIDIG as baseline comparison techniques. As THEMIS is shown to be significantly less effective [14] and thus is omitted. Through comparison with black-box methods, we demonstrate the superior performance of MAFT. Additionally, by contrasting it with white-box methods, we highlight why it have distinct advantages and strengths.\nIn addition to this, we have to highlight that MAFT differs from ADF and EIDIG by operating as a black-box approach, so retraining is unnecessary to demonstrate the effectiveness of the discriminatory instances generated. If you wish to confirm the effectiveness of these instances in retraining to alleviate the model's bias, you are encouraged to refer to ADF and EIDIG.\nDatasets and Models. To evaluate MAFT, we select seven benchmark datasets that have been used in previous studies [11, 13-16]. The details are as Table 2 (#Ins means number of instances, #Fea expresents the number of features and Protected Attrs are protected features).\nThe predictive tasks based on these datasets center around determining whether an individual meets certain conditions. Owing to the simplicity of these datasets, we employs the six-layer fully connected networks that was adopted by EIDIG. The details of the modes's accuracy are shown in Talbe 2. Prior to the tasks' beginning, it was necessary to preprocess the data particularly in regard to the conversion of continuous attributes into categorical ones. For instance, we discrete age based on human lifecyle. For ease of denotation, each benchmark was denoted as \"B-a\", where \"B\" represents the initial uppercase letter of the dataset, and \"a\" refers to the initial lowercase letter of the sensitive attribute.\nConfigurations. Both ADF and EIDIG are configured according to the best performance setting reported in the respective papers. For global, max iteration max_iter is set to 10 because less than 5 iterations need to be taken for most situations to find an individual discriminatory instance around the seed if there exists and both global step size global_step and local step size local_step are set to 1.0, i.e., the minimum step for the categorical attributes. During global phase, we set cluster count cluster_num to 4 as ADF and EIDIG did to cluster the training set using K-Means [21]. As EIDIG achieves best performance when past gradient information decays away to half its origin after each iteration, we set decay factor of momentum decay to 0.5. During local phase, we set prior information life cycle update_interval to 5 to make a balance of prior information effectiveness and update frequency. As for perturbation size perturbation_size, we will discuss it in 4.2.1.\nWe re-implemented SG and AEQUITAS using TensorFlow 2 and make slight adjustments to make them having the same maximum search iterations with white-box methods under same parameters. We opt for a fixed random seed to generate initial input for AE-QUITAS in input domain to keep stability. Other settings are also kept best.\nAll experiments were run on a personal computer with 32 GB RAM, Intel i5-11400F 2.66GHz CPU and NVIDIA GTX 3060 GPU in Ubuntu22.04"}, {"title": "4.2 Results and Discussion", "content": "Notice that besides AEQUITAS, all of ADF, EIDIG and MAFT share a similar gradient-based testing framework with two sequential phases. To this end, we compare them phase by phase to answer these research questions."}, {"title": "4.2.1 Hyperparameter Study: Perturbation Size (RQ1)", "content": "The perturbation size h is a critical hyperparameter in MAFT. In this experiment, our goal is to provide valuable insights for the effects of perturbation size and determine the optimal perturbation size for the MAFT method across various benchmarks in the settings of tabular data. In these experiments, g_num and l_num were set to 100 and 100 separately (in the formal experiments, they were set to 1000 and 1000). We performed experiments on multiple benchmarks using five different methods: MAFT with various perturbation size values ranging from 1e - 10 to 10, AEQUITAS, SG, ADF and EI-DIG, each with their optimal parameters. Specifically, the methods included MAFT, varying across 12 parameter configurations, thus yielding a total of 16 methods of distinct settings. Each unique combination underwent rigorous evaluation through 10 rounds. Subsequently, we aggregated the resultant data points for each method and plotted them with a boxplot shown in Fig 3.\nIn Fig 3, vertical axes in different subfigures represent instance generation number, instance generation speed, and success rate for generating discriminatory instances in effective attempts separately. Notice that higher values are better for all of the three metrics alone. The horizontal axis represents AEQUITAS, SG, ADF, EIDIG, and MAFT with different perturbation size values. We get several observations from the results. AEQUITAS adopts a global adaptive search strategy, which continuously reinforces past successful choices during search. In doing so, AEQUITAS is more inclined to exploit than to explore, and thus it stucks in duplicate search paths, resulting in finding much less fairness violations, even if it achieves good performance in terms of generation speed and success rate due to it heuristic nature. SG is both ineffective and inefficient, because it heavily relys on the existing techniques about local explanation and symbolic execution, which are both time-consuming. Specifically, MAFT with h = 1 generates 292.15% more biased samples than SG at 3158.43% higher speed on average. EIDIG significantly outperforms ADF especially in speed. Moreover, the success rate of EIDIG is slightly lower than ADF, since EIDIG doesn't utilizes completely accurate gradients in most iterations during local search, but EIDIG explores much more search space owing to its momentum integration during global search. For MAFT, when h is less than or equal to 1e - 6, the results are even significantly inferior to ADF. However, the results improve as h increases within the range of (1e-8, 1e-5] and keep increasing until the metrics remain relatively stable within the range of (1e - 5, 10] with the number and speed metrics comparable with EIDIG. We apply ANOVA[25] to verify that the experimental results of MAFT with h in (1e-5, 10] do not exhibit statistically significant differences. At h = 1 and h = 10, we get the highest average number and speed, but the success rate at h = 10 obviously drops. These results demonstrates that when h is too small, the perturbation is so subtle that it fails to estimate an useful gradient. Conversely, when h is too large, the estimated gradient deviates significantly. When h is kept at an appropriate value, the experimental effects are relatively stable and efficient.\nOverall, we recommand using the perturbation size of 1, and it is also the minimum granularity of the preprocessed attributes of the subject tabular datasets after discretization. We also encourage users to try to choose parameters that are more suitable for their specific datasets."}, {"title": "4.2.2 Comprehensive Results: Effectiveness and Efficiency (RQ2)", "content": "After selecting the hyperparameter, we compare the overall performance of MAFT to the baselines such as AEQUITAS, ADF and EIDIG, considering both the quantity of generated test cases which means effectiveness and the generation speed which means efficiency. Note that we exclude SG because it is too time-consuming.\nFollowing the setting established by"}]}