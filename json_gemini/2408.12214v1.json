{"title": "UNCO: Towards Unifying Neural Combinatorial Optimization through Large Language Model", "authors": ["Xia Jiang", "Yaoxin Wu", "Yuan Wang", "Yingqian Zhang"], "abstract": "Recently, applying neural networks to address combinatorial optimization problems (COPs) has attracted considerable research attention. The prevailing methods always train deep models independently on specific problems, lacking a unified framework for concurrently tackling various COPs. To this end, we propose a unified neural combinatorial optimization (UNCO) framework to solve different types of COPs by a single model. Specifically, we use natural language to formulate text-attributed instances for different COPs and encode them in the same embedding space by the large language model (LLM). The obtained embeddings are further advanced by an encoder-decoder model without any problem-specific modules, thereby facilitating a unified process of solution construction. We further adopt the conflict gradients erasing reinforcement learning (CGERL) algorithm to train the UNCO model, delivering better performance across different COPs than vanilla multi-objective learning. Experiments show that the UNCO model can solve multiple COPs after a single-session training, and achieves satisfactory performance that is comparable to several traditional or learning-based baselines. Instead of pursuing the best performance for each COP, we explore the synergy between tasks and few-shot generalization based on LLM to inspire future work.", "sections": [{"title": "I. INTRODUCTION", "content": "COMBINATORIAL optimization problems (COPs), such as the traveling salesman problem (TSP) and knapsack problem (KP), are a class of classical mathematical problems that aim at finding an optimal solution from a finite set of solutions. They are broadly studied by researchers from operations research and computer science, and have recently been introduced into the machine learning community [1], [2]. Compared to traditional exact and heuristic approaches, the learning-based methods can automate the design of heuristics, reducing the prerequisite for domain knowledge and striking a favorable balance between efficiency and optima for solving COPS.\nThe neural combinatorial optimization (NCO), as the most effective learning-based paradigm, approximates the optimal solutions of COPs by training a neural network. The mainstream NCO methods automatically learn constructive heuristics by capturing dependencies between objects from input features, e.g., node coordinates of a TSP instance. After being trained by supervised or reinforcement learning (RL), the neural network can construct solutions of a COP in an end-to-end manner. NCO has shown promising outcomes in solving several NP-hard COPs in a short bounded time.\nExistent NCO methods are typically centered on developing neural networks for a limited scope of COPs, such as the S2V-DQL [3] for COPs over graphs, the attention model (AM) [4] for vehicle routing problems (VRPs), and the matrix encoding network (MatNet) [5] for COPs with a bipartite graph structure. They redesign/adapt a neural network to be trained on each similar COP and acquire respective neural heuristics exclusively. However, training on each COP triggers substantial training costs and reduces practicality when multiple disparate COPs are to be tackled.\nRecent literature is witnessing the dawn of creating unified models for COPs, with the objective of escalating cross-problem generalization [6]. The effort has been put into unifying the process of solving multiple VRPs [7], [8], [9], which share similar underlying problem structures and resultantly, can be encoded by a common neural network. However, the problem formations and descriptions would be fundamentally different for disparate COPs, and in this regard, it is difficult to encode them simultaneously. Hence unifying the solving processes of significantly diverse COPs into a single model remains a formidable challenge.\nBased on the belief that solving processes of different types of COPs may share common knowledge, we aim to provide a novel paradigm for training a unified model by drawing upon the expressive power of large language models (LLMs). We propose a unified neural combinatorial optimization (UNCO) framework, which employs a single neural network to solve more general COPs than existing models. Specifically, different COPs are formulated into text-attributed instances (TAIs), which are then encoded by an LLM and a series of attention blocks. We further establish a problem-agnostic decoder to construct solutions for various COPs. Treating each COP as a learning task, the encoder (excluding the LLM) and decoder are trained through a conflict gradients erasing reinforcement learning (CGERL) algorithm, which aligns the gradients of different tasks and balances the performance improvement on respective COPs. In experiments, the COPs from different domains are taken as examples and solved simultaneously by the UNCO. The trained unified model demonstrates satisfactory performance on par with some traditional or learning-based baselines. The main contributions of this work are threefold:\n\u2022 We propose a generic UNCO framework to solve diverse COPs, which are formulated into TAIs by the natural language, in an end-to-end manner."}, {"title": "II. RELATED WORK", "content": "The constructive NCO methods aim to learn policies for constructing solutions in an autoregressive way. The early attempts are based on pointer networks [10], [11], a class of recurrent neural networks (RNNs) that process the input and generate the solution in a sequence-to-sequence fashion. Inspired by the Transformer [12], the attention model (AM) is put forward to solve simple VRPs, respectively, showing the advantage over conventional heuristics. Afterward, a series of strategies are proposed to enhance Transformer-based NCO models by leveraging the symmetricity of COPs [13], [14], [15], efficient active search [16], [17], [18], and curriculum learning [19], [1]. Despite the good performance, the constructive NCO methods usually redesign/adapt neural networks to be trained on similar COPs, respectively, making them less efficient and practical. In addition, the improvement NCO methods enhance stochastic search algorithms, such as local search [20], [21], neighborhood search [22], [23] and evolutionary algorithms [24], by employing neural networks to iteratively improve an initial solution. Some researchers improve specialized solvers in specific domains, e.g. Lin-Kernighan-Helsgaun (LKH) [25], [26] and Hybrid Genetic Search (HGS) [27]."}, {"title": "A. Neural combinatorial optimization", "content": "\u2022 We present an encoder-decoder based model (i.e., a neural network), which draws upon the expressive power of an LLM, to unify the processes of constructing solutions for different COPs.\n\u2022 We introduce the CGERL algorithm for reinforcement learning, which erases the conflict gradients and enhances the performance on respective COPs."}, {"title": "B. Unified neural network solver", "content": "Recent literature has started exploring the unified model for solving multiple COPs. [7] views VRP variants as combinations of attributes (node coordinates, demands, etc.), and train a Transformer-based backbone as the unified model. A group of similar VRPs such as the capacitated VRP (CVRP), VRP with time windows (VRPTW), and open VRP (OVRP) are simultaneously solved. Subsequently, [8] increases the capacity of the unified model with mixture-of-experts (MoE), which facilitates the zero-shot and few-shot generalization capability to some extent. However, these unified models are still confined to similar VRPs due to their analogous problem structures, i.e., problem-specific attributes can be simply addressed by supplementary sub-networks in the unified model. They do not suffice to encompass completely different attributes (e.g., edge information in graphs cannot be included in the two unified models), and are inapplicable to other COPs.\nAdditionally, [28], [29] implemented unified models for different types of COPs, but they rely on training different encoding blocks for each COP and compromise the degree of universality. Therefore, solving disparate COPs using a totally shared model is still an interesting and urgent research task."}, {"title": "C. LLMs for opimization", "content": "LLMs have demonstrated remarkable efficacy across a range of fields, such as natural language understanding [30], [31], code generation [32], and mathematical reasoning [33]. Recent advances showcase that LLMs are capable of generating code of heuristic algorithms to solve COPs [34]. Given an initial template of the code, LLMs update the heuristics following an evolutionary procedure. In addition, LLMs can serve as black-box optimizers to improve an initial solution of a simple COP [35], [36]. The COP description, in-context samples, and expected outputs are arranged in the prompts, which instruct LLMs to explore the solution space. Nevertheless, the heuristics generated by LLMs are too specific to be widely applied. The effectiveness of LLM optimizers is only validated by simple COPs, e.g., small-sized TSP instances [35], [37], and it is still challenging for them to solve medium-sized COPs and more general COPs. In this paper, we endeavor to address multiple COPs with the expressive power of LLMS and establish a unified model, fostering the integration of LLM into NCO."}, {"title": "III. PRELIMINARIES", "content": "Solving a COP often means searching an object from a finite (or countably infinite) discrete set. The object can be an integer number, a subset, or a permutation [38]. Most COPs are represented on graphs, in which the objects are denoted by nodes and edges in COPs. More formally, a COP P is formulated as:\n$\\\\min f(x, P) \\\\text{ s.t. } c_i(x, P) \\leq 0, i = 0, 1, ..., I$\nwhere x = {x1, ..., Xn} is a set of discrete decision variables, belonging to the domains {Di}i=1; f(x, P) denotes an objective function to be minimized and {ci(x, P)}i=1 denotes a set of problem-specific constraints for x. The set of all feasible solutions in the search space is denoted as:\n$S = {s = {(x_1, d_1), ..., (x_n, d_n)}|d_i \\in D_i, c(x, P) \\leq 0}$\nwhere we omit the subscript of c(x, P) for clarity. An optimal solution of a COP instance s* \u2208 S is supposed to satisfy f(s*, P) \u2264 f(s, P), \u2200s \u2208 S. Despite that the formations and descriptions of various COPs would be fundamentally different, all COPs can be universally described by natural language. Concretely, we regard each COP as a learning task for a neural network. As exhibited in Figure 1, we propose the task and instance descriptions to formulate COP instances into TAIs. The task description specifies the formation of a COP, the form of decision variables, the general constraints, and the objective function, while the instance description specifies the detailed feature of nodes or edges."}, {"title": "A. Combinatorial optimization problems", "content": "\u2022 We present an encoder-decoder based model (i.e., a neural network), which draws upon the expressive power of an LLM, to unify the processes of constructing solutions for different COPs.\n\u2022 We introduce the CGERL algorithm for reinforcement learning, which erases the conflict gradients and enhances the performance on respective COPs."}, {"title": "B. Transformer-based NCO", "content": "The constructive NCO models are typically based on Transformer-based neural networks. Formally, the numerical features of a COP instance GP are encoded by an embedding"}, {"title": "IV. METHODOLOGY", "content": "Unifying the solving processes of different COPs is chal- lenging since their formations might be distinct. For instance, the features in the Euclidean TSP are node coordinates, while in a graph-based COP, e.g., Minimum Vertex Cover Problem (MVCP), are adjacency matrices. The inherent differences bring about the difficulty in encoding COPs universally. Previous work attempts to incorporate separate modules in the neural network for encoding problem-specific features of different COPs[28], [29], which limits their applicability to only similar problems (e.g., only a class of VRPs), and compromises the universality due to the respective parameters for each COP.\nInspired by the recent LLM-based graph foundation models [41], [42], we articulate a wide range of COPs using the natural language. The universality of the natural language can address the challenge posed by diverse formations of COPs. Formally, we propose TAI T(GP) = {\u03baP, vP }, which encapsulates both the task description \u03baP and the instance description vP, and hence enables the recognition of instances across COPs. The task description \u03baP prompts the LLM with the definition and background of the target COP, while the instance description vP gives the details of an instance, e.g., the node attributes.\nTaking the instance descriptions in Figure 1 as the example, the sentences vi \u2208 vP, i \u2208 [1,n] delineates the numerical attributes on the nodes of an instance, indicating the basic features such as the node coordinates in TSP or the weight-profit pairs in KP. Additionally, we also incorporate heuristic information. By doing so, we expect to prompt the LLM with the knowledge of general and conventional heuristics. For example, inspired by greedy policy, we delineate the top-k (k = 3) nearest nodes with corresponding distances in the instance description of TSP, and the value-to-weight ratios with their ranks for KP instances. The inclusion of heuristic information enhances the LLMs' ability to comprehend the instances, which will be shown in the ablation study.\nThe task and instance descriptions are individually embedded by an LLM, which captures representations of instances and COPs. Following the protocol, the TAIs of various COPs can be transformed into fixed-length vectors by the LLM. In specific, both the instance description and task description are embedded by the same LLM, that is, xi = LLM(vi) and \u03baP = LLM(\u03baP). The embedded vectors {x\u0303i}i=1 contain the information pertaining to the instance, while \u03baP denotes the domain-specific information of the COP P. Note that the task description is encoded only once and then shared among instances of the same COP, thereby improving the computational efficiency. After embedding {x\u0303i}i=1 by the LLM, we further employ attention blocks in the encoder to extract interdependencies between nodes, and then propose a decoder to construct solutions according to the embedded task information in \u03baP."}, {"title": "A. Unifying descriptions of COPs by TAIs", "content": "Unifying the solving processes of different COPs is challenging since their formations might be distinct. For instance, the features in the Euclidean TSP are node coordinates, while in a graph-based COP, e.g., Minimum Vertex Cover Problem (MVCP), are adjacency matrices. The inherent differences bring about the difficulty in encoding COPs universally. Previous work attempts to incorporate separate modules in the neural network for encoding problem-specific features of different COPs[28], [29], which limits their applicability to only similar problems (e.g., only a class of VRPs), and compromises the universality due to the respective parameters for each COP.\nInspired by the recent LLM-based graph foundation models [41], [42], we articulate a wide range of COPs using the natural language. The universality of the natural language can address the challenge posed by diverse formations of COPs. Formally, we propose TAI T(GP) = {\u03baP, vP }, which encapsulates both the task description \u03baP and the instance description vP, and hence enables the recognition of instances across COPs. The task description \u03baP prompts the LLM with the definition and background of the target COP, while the instance description vP gives the details of an instance, e.g., the node attributes.\nTaking the instance descriptions in Figure 1 as the example, the sentences v \u2208 vP, i \u2208 [1,n] delineates the numerical attributes on the nodes of an instance, indicating the basic features such as the node coordinates in TSP or the weight-profit pairs in KP. Additionally, we also incorporate heuristic information. By doing so, we expect to prompt the LLM with the knowledge of general and conventional heuristics. For example, inspired by greedy policy, we delineate the top-k (k = 3) nearest nodes with corresponding distances in the instance description of TSP, and the value-to-weight ratios with their ranks for KP instances. The inclusion of heuristic information enhances the LLMs' ability to comprehend the instances, which will be shown in the ablation study.\nThe task and instance descriptions are individually embedded by an LLM, which captures representations of instances and COPs. Following the protocol, the TAIs of various COPs can be transformed into fixed-length vectors by the LLM. In specific, both the instance description and task description are embedded by the same LLM, that is, x = LLM(v) and kP = LLM(kP). The embedded vectors {x}1 contain the information pertaining to the instance, while kP denotes the domain-specific information of the COP P. Note that the task description is encoded only once and then shared among instances of the same COP, thereby improving the computational efficiency. After embedding {x}1 by the LLM, we further employ attention blocks in the encoder to extract interdependencies between nodes, and then propose a decoder to construct solutions according to the embedded task information in kP."}, {"title": "B. Unifying the encoder-decoder model", "content": "Inspired by the Transformer [12] and AM [4], we develop an encoder-decoder-based network in our UNCO model, as displayed in Figure 1. The LLM is used for initial embedding of TAIs in the encoder, making the model take as input different types of COPs. Both encoder and decoder do not rely on any additional problem-specific modules, and thus different COPs can be processed by one single model.\nEncoder. Given the embedded vectors of TAIs, we use a feed-forward (FF) layer and a series of attention blocks to extract common knowledge among multiple COPs. As the dimension do of embedding from an LLM is generally large, the FF layer with parameters We \u2208 Rdo\u00d7dn and be \u2208 Rdn is used for dimensionality reduction. As such, the concatenated embedded vectors xP = {x}i=1 are linearly projected by h(0) = WexP + be with h(0) \u2208 Rn\u00d7dn. Afterward, the embedding h(0) is processed by successive attention blocks, each of which consists of a multi-head attention (MHA) layer [12], a node-wise FF layer, a skip-connection layer [43] and a batch normalization (BN) layer [44], that is,\n$\\hat{h}^{(l)} = BN_1(h^{(l-1)} + MHA(\\lbrace h_i^{(l-1)}, ..., h_n^{(l-1)}\\rbrace))$\nh(l) = BN1(\\hat{h}^{(l)} + FF(\\hat{h}^{(l)}))\nwhere l \u2208 [1, N] represents the index of the attention block. Details of MHA, FF, and BN are similar to the AM [4] and elaborated in Appendix A. As a result, the instance description is processed by the encoder (the LLM and attention blocks), by which the instance embedding h(N) is obtained as shown in Figure 1.\nDecoder. The decoding is sequentially unfolded and at each step t \u2208 1, ..., n, a node is chosen to be joined with the partial solution until a complete feasible solution is constructed. Since the encoder has already extracted common knowledge of different COPs, the decoder should construct the solution by 1) discriminating different tasks, and 2) constructing solutions of different COPs by a unified process. To this end, we first define the context vector to incorporate problem-specific information, which is then used to intervene in the decoder to output the probabilities of selecting every node.\nOne feasible way for the decoder to distinguish between different COPs is by providing it with the task description and problem-specific constraints. Given that some COPs entail dynamic constraints, such as the constraints defined by the varied vehicle load and knapsack capacity in the CVRP and KP, we first incorporate these dynamic attributes into the decoding context. Concretely, we use a vector cf to monitor these dynamic attributes, indicating the remaining vehicle capacity C or knapsack capacity Ch at time t, as follows:\n$c_f=\\begin{cases}C_r, \\text{ if the COP is CVRP,} \\\\C_h, \\text{ if the COP is KP,} \\\\0, \\text{ otherwise}\\end{cases}$\nNote that we only take CVRP and KP as examples in the equation. One can easily extend it to encompass dynamic attributes of constraints in other COPs."}, {"title": "C. CGERL algorithm for reinforcement learning", "content": "We resort to RL to train the unified model. Since most NCO models are trained on every single COP, we adapt the vanilla RL algorithm to enable effective training on multiple COPs simultaneously. Taking a COP as a task, one approach to train a model on multiple tasks is simply averaging the losses of different tasks. For instance, the averaged multi-task REIN- FORCE is employed for training a multi-task solver of VRPs, with the same objective of minimizing the length of routes [7]. However, the simultaneous learning for solving clearly different types of COPs presents a challenging optimization problem, due to the involvement of diverse objectives [45]. The averaging scheme may yield undesirable performance for two reasons: Firstly, the objectives of different COPs may exhibit varied scales, making the larger gradients dominate the update of the model. Secondly, the updated directions of parameters may conflict under different objectives, potentially leading to a"}, {"title": "V. EXPERIMENTS", "content": "The effectiveness of the proposed framework is evaluated on five representative COPs, including TSP, CVRP, KP, MVCP, and the single-machine total weighted tardiness problem (SMTWTP). Meanwhile, we fine-tune the trained UNCO model on two new tasks, including the vehicle routing problem with backhauls (VRPB) and the maximum independent set problem (MISP). The problem description, instance generation, and the description for TAIs are provided in Appendix C. Note that in addition to the above tasks, many other COPs can also be incorporated and solved."}, {"title": "A. Benchmarks", "content": "The effectiveness of the proposed framework is evaluated on five representative COPs, including TSP, CVRP, KP, MVCP, and the single-machine total weighted tardiness problem (SMTWTP). Meanwhile, we fine-tune the trained UNCO model on two new tasks, including the vehicle routing problem with backhauls (VRPB) and the maximum independent set problem (MISP). The problem description, instance generation, and the description for TAIs are provided in Appendix C. Note that in addition to the above tasks, many other COPs can also be incorporated and solved."}, {"title": "B. Experimental settings", "content": "We mainly use the Llama2-7b model [47] as the LLM to evaluate the proposed method. We also provide an ablation study to test the performance of other language models. The UNCO models are trained on instances with sizes n = 20 and 50. It undergoes training in 100 epochs for n = 20 and 200 epochs for n = 50, with 2000 instances allocated for each task within each epoch. We fine-tune the model trained on n = 50 to n = 100 for 20 epochs and find it achieves better performance than directly training on COPs with n = 100. The comparison between fine-tuning and learning from scratch is provided in Appendix D. Policy gradients based on CGERL are computed across all tasks utilizing a batch size of 16 instances. Training is facilitated through the utilization of the Adam optimizer [48], integrated with the CGERL algorithm. The learning rate is configured to 10\u22124, accompanied by a weight decay of 10\u22126. The encoder is composed of N = 6 attention blocks. Given the variability in embedding dimensionality do across different LLMs, a linear layer with dimensionality d\u2081 = 512 is employed to align the LLMs and the attention model. During evaluation, we also introduce \u00d78 augmentation [13] for routing problems. The average result is reported based on the test set, consisting of 1000 random instances for each COP. All experiments are carried out on a machine with an AMD EPYC 7F72 CPU at 3.2 GHz and an NVIDIA H100 GPU."}, {"title": "C. Baselines", "content": "Traditional solvers: We use the Gurobi solver to determine the optimal solution of TSP, KP, MVCP, and SMTWTP. We use the Hybrid Genetic Search (HGS) algorithm [49] to solve CVRP. We also employ the LKH3 solver [50] to solve TSP and CVRP instances. Moreover, OR Tools is introduced to solve TSP, CVRP, and KP.\nThe proposed method is also compared with conventional heuristics or metaheuristics, such as the nearest neighbor and the farthest insertion algorithm for TSP, sweep heuristic and parallel saving algorithm for CVRP [51], the greedy policy for KP, the MVCApprox method (i.e., greedily reduces the costs over edges and iteratively building a cover) for MVCP [52], and the ant colony optimization (ACO) for SMTWTP [24]. The heuristics for solving TSP and CVRP are based on the implementation of pyCombinatorial\u00b9 and VeRyPy2, respectively, where the heuristics of TSP are enhanced by the 2-opt local search strategy.\nNCO solvers: We compare our UNCO model with other NCO approaches, including AM [4] and POMO [13] for TSP, CVRP and KP, CombOpt for MVCP [53], and DeepACO (T = 50) for SMTWTP [24].\nLLM-based constructive heuristics: For the TSP task, we also introduce the baseline heuristics generated by LLM through evolutionary strategies, including Algorithm Evolution Using Large Language Model (AEL) [54] and Reflective Evolution (ReEvo) [55]."}, {"title": "Ablation study on CGERL algorithm", "content": "Ablation study on CGERL algorithm. We train the UNCO model by both the CGERL algorithm and the vanilla REIN- FORCE algorithm, and the normalized results are compared in Figure 4 (b). Reinforcement learning that has undergone"}, {"title": "VI. CONCLUSION", "content": "This paper presents a framework to solve different COPs by a unified model. Given the textual description of the instances, the model utilizes the LLM to embed instances and construct solutions by a unified model. To implement simultaneous learning of multiple tasks, we introduce a multi-task RL approach to cope with conflict gradients during training. The model shows satisfactory performance on different COPs by sharing common knowledge across tasks. Therefore, this work provides a new unified paradigm for solving COPs from their textual descriptions. In the future, more efficient inference strategies can be implemented for the LLM and accelerate the solving process. Meanwhile, the LLM can be tuned to get better performance in the domain of NCO."}]}