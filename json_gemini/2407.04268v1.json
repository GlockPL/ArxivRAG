{"title": "NeuFair: Neural Network Fairness Repair with Dropout", "authors": ["Vishnu Asutosh Dasu", "Saeid Tizpaz-Niari", "Ashish Kumar", "Gang Tan"], "abstract": "This paper investigates the neural dropout method as a post-processing bias mitigation for deep neural networks (DNNs). Neural-driven software solutions are increasingly applied in socially critical domains with significant fairness implications. While neural networks are exceptionally good at finding statistical patterns from data, they are notorious for over-fitting the training datasets that may encode and amplify existing biases from the historical data. Existing bias mitigation algorithms often require either modifying the input dataset or modifying the learning algorithms. We posit that the prevalent dropout methods that prevent over-fitting during training by randomly dropping neurons may be an effective and less intrusive approach to improve fairness of pre-trained DNNs. However, finding the ideal set of neurons to drop is a combinatorial problem.\nWe propose NEUFAIR, a family of post-processing randomized algorithms that mitigate unfairness in pre-trained DNNs via dropouts during inference after training. Our randomized search is guided by an objective to minimize discrimination while maintaining the model's utility. We show that our design of randomized algorithms are effective and efficient in improving fairness (up to 69%) with minimal or no model performance degradation. We provide intuitive explanations of these phenomena and carefully examine the influence of various hyperparameters of search algorithms on the results. Finally, we empirically and conceptually compare NEUFAIR to different state-of-the-art bias mitigators.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial intelligence (AI), increasingly deployed with deep neu-ral network (DNN) components, has become an integral part of modern software solutions that assist in socio-economic and legal-critical decision-making processes such as releasing patients [30], identifying loan defaults [22], and detecting tax evasion [42].\nDespite many advances made possible by AI, there are challenges that require understanding the dimensions and implications of deploying Al-driven software solutions. One such concern about the trustworthiness of AI is discrimination. Unfortunately, there are plenty of fairness defects in real systems. Parole decision-making software was found to harm black and Hispanic defendants by falsely predicting a higher risk of recidivism than for non-Hispanic white defendants [27]; Amazon's hiring algorithm disproportion-ately rejected more female applicants than male applicants [26]; and data-driven auditing algorithm selected black taxpayers with earned income tax credit claims (EITC) at much higher rates than other racial groups for an audit [42]. As evidenced by these exam-ples, resulting software may particularly disadvantage minorities and protected groups and be found non-compliant with law such as the US Civil Rights Act [6]. Hence, helping programmers and users to mitigate unfairness in social-critical data-driven software systems is crucial to ensure inclusion in our modern, increasingly digital society.\nThe software engineering (SE) community has spent significant efforts to address discrimination in the automated data-driven soft-ware solutions [2, 10, 18, 47, 56]. Fairness has been treated as a critical meta-property that requires analysis beyond functional correctness and measurements beyond prediction accuracy [7]. Thus, the community presents various testing [2, 18, 57], debug-ging [21, 35, 50], and mitigation [5, 53] techniques to address fair-ness defects in data-driven software.\nBroadly, fairness mitigation can be applied in pre-processing stage (e.g., increasing the representations of an under-represented group by generating more data samples for them), in-processing step (e.g., changing the loss function to include fairness constraints during training), or post-processing stage (e.g., changing the logic of a pre-trained model). However, when the decision logic of AI is encoded via deep neural networks, it becomes increasingly chal-lenging to mitigate unfairness due to its black-box uninterpretable nature."}, {"title": null, "content": "Since the DNN-based software development is derived by prin-ciple of information bottleneck [45], which provides a trade-off be-tween over-fitting and parsimonious signals; we posit that over-fitted DNN models may contribute and amplify historical biases from the training datasets. Therefore, prevalent techniques such as dropout methods [39, 40]-a process of randomly dropping neurons during training-might be effective in mitigating unfairness in the pre-trained DNN models. While the dropout strategy has been sig-nificantly used to prevent over-fitting during the training of DNNs, to the best of our knowledge, this is the first work to systematically leverage dropout methods for mitigating unfairness in pre-trained DNN models, as a post-processing mitigation method.\nHowever, finding the optimal subset of dropout neurons is a com-binatorial problem that requires an exhaustive search in the subset of all combinations of neuron dropouts, and hence is intractable. To overcome the computational challenges, we explore the class of randomized algorithms with Markov chain Monte Carlo (MCMC) strategies to efficiently explore the search space with statistical guarantees. In doing so, we pose the following research questions:\nRQ1. How successful are randomized algorithms in fairness repairing of DNNs via dropouts?\nRQ2. Are there dropout strategies that improve model fairness and utility together?\nRQ3. What are the design considerations of search algorithms for efficient and effective dropouts?\nRQ4. How do dropout strategies compare to the state-of-the-art post-processing (bias) mitigators?\nTo answer these research questions, we present NEUFAIR (Neural Network Fairness Repair): a set of randomized search algorithms to improve the fairness of DNNs via inference-time neuron dropouts. We design and implement simulated annealing (SA) and random walk (RW) strategies that efficiently explore the state-space of neu-ron dropouts where we encode the frontiers of fairness and utility in a cost function.\nWe evaluate NEUFAIR over 7 deep neural network benchmarks as trained over 5 socially critical applications with significant fair-ness implications. We found that randomized strategies of neuron drops can successfully improve fairness (up to 69%) with a minimal utility degradation in the vast majority of cases. We also report a pathological case and reasons behind a failure of NEUFAIR. We also observe that NEUFAIR can improve fairness and utility together and provide intuitive explanations of such phenomena. Further-more, we examine different hyperparameter configuration options of randomized algorithms. While some hyperparameters always influence fairness with positive or negative impacts; we detect a hyperparameter that defines a trade-off between explorations and exploitation that should be tuned as a constant variable in the cost function for each benchmark. Finally, we show the effectiveness of the SA algorithm, compared to the RW and the state-of-the-art post-processing (bias) mitigator [35]. In summary, the key contributions of this paper are:"}, {"title": null, "content": "(1) Inference-Time dropout for fairness. To the best of our knowledge, we present the first dropout method of bias mit-igation over pre-trained deep neural networks,\n(2) Randomized algorithms for fairness. We create a well-defined framework to formulate the combinatorial inference-time dropout problem in DNNs using randomized algorithms,\n(3) Experimental evaluations. We implement the randomized algorithms in NEUFAIR and evaluate its effectiveness and efficiency vis-a-vis the state-of-the-art techniques."}, {"title": "2 BACKGROUND", "content": "We provide a background on the model utility and fairness measures. Then, we briefly discuss various bias mitigation methods from the Al fairness literature."}, {"title": "2.1 Notions of Model Utility", "content": "Given a binary classifier h, a set of features X, and predictions h(X) = {TP, TN, FP, FN}, we can define the following notions of model utility. In h(X), TP, TN, FP, and FN denote the set of True Positives, True Negatives, False Positives, and False Negatives. We drop the cardinality operator || in the following definitions for brevity.\n$\\begin{aligned} \\text { Accuracy }_h & =\\frac{\\text { TP }+\\text { TN }}{\\text { TP }+\\text { TN+FP }+\\text { FN }} \\\\ F 1_h & =\\frac{2 * \\text { Precision } * \\text { Recall }}{\\text { Precision }+\\text { Recall }}=\\frac{2 * \\text { TP }}{2 * \\text { TP }+\\text { FP }+\\text { FN }} \\end{aligned}$\nAccuracy can be used to gauge the overall performance of a classifier. However, for imbalanced datasets used in fairness evalua-tions, accuracy is a poor metric as the number of negative samples far outweighs the positive samples. For example, predicting all sam-ples as negative (0 true positives) in the Bank [13] dataset yields an accuracy of 88% but the F1 score would be 0/undefined. Since F1 is defined as the harmonic mean of precision and recall, rather than the arithmetic mean, it penalizes performance significantly if either precision or recall is low."}, {"title": "2.2 Notions of Model Fairness", "content": "Consider a machine learning classifier h, a set of features X, sensi-tive features A \u2286 X, and a set of labels Y. We can then define the following notions of fairness.\nDEFINITION 2.1 (DEMOGRAPHIC PARITY [1]). The classifier h satis-fies Demographic Parity under a distribution over (X, A, Y) if its pre-diction h(X) is statistically independent of the sensitive feature A i.e. P[h(X) = \u0177|A = a] = P[h(X) = \u0177] for all a, y. For binary classifica-tion with \u0177 = {0, 1}, this is equivalent to E[h(X)|A = a] = E[h(X)] for all a.\nDEFINITION 2.2 (EQUALIZED ODDS [1, 24]). The classifier h satis-fies Equalized Odds under a distribution over (X, A, Y) if its prediction h(X) is conditionally independent of the sensitive feature A given the label Y i.e. P[h(X) = \u0177|A = a, Y = y] = P[h(X) = \u0177|Y = y] for all a, y, and \u0177. For binary classification with \u0177 = {0, 1}, this is equivalent to E[h(X)|A = a, Y = y] = E[h(X)|Y = y] for all a, y.\nEqual Opportunity is a relaxed variant of Equalized Odds with Y = 1 [24]. In other words, Equalized Odds require the true positive and false positive rates to be equal across all sensitive groups. In contrast, Equal Opportunity only requires the true positive rate to be equal across all sensitive groups."}, {"title": null, "content": "Demographic Parity is the weakest notion of fairness, and Equal-ized Odds is the strongest. In our work, we use Equalized Odds as the fairness criterion. The disparity or unfairness for Equalized Odds is the Equalized Odds Difference (EOD), defined as the maxi-mum absolute difference between the true and false positive rates across the sensitive groups. Mathematically, this is represented as\n$\\text{EOD} = \\max\\begin{bmatrix} |P[h(X) = 1 | A = 0, Y = 1] - P[h(X) = 1 | A = 1, Y = 1]|, \\\\ |P[h(X) = 1 | A = 0, Y = 0] - P[h(X) = 1 | A = 1, Y = 0]| \\end{bmatrix}$\nwith sensitive features A = {0, 1}."}, {"title": "3 PROBLEM STATEMENT", "content": "We consider pre-trained deep neural network (DNN) classifiers with the set of input variables X partitioned into a protected set of vari-ables (such as race, sex, and age) and non-protected variables (such as profession, income, and education). We further assume the out-put is a binary classifier that gives either favorable or unfavorable outcomes."}, {"title": "3.1 Syntax and Semantics of DNN", "content": "A deep neural network (DNN) encodes a function D : X \u2192 [0, 1]^2 where X consists of the set of protected attributes X_1 \u00d7 X_2 \uff65\uff65\uff65\u00d7X_m and non-protected attributes X_{m+1} \u00d7 Y_{m+r}. The DNN model is parameterized by the input dimension m+r, the output dimension 2, the depth of hidden layers n + 1, and the weights of its hidden layers W_0, W_1, ..., W_n. We describe the hidden layers with M\u2190 [L_1, L_0, . . ., L_n, L_o], where L_1 and L_0 are the input and out-put layers, respectively, and L_i, \u2200i \u2208 [0, n], is the hidden layers. We assume that there exists a subset of neurons N\u2208 L_i, \u2200i \u2208 [0, n] in the hidden layers that disparately contribute to unfairness.\nLet L_i be the output of layer i that implements an affine mapping from the output of previous layer L_{i-1} and its weights W_{i-1} for 1 \u2264 i \u2264 n followed by a fixed non-linear activation unit (e.g., ReLU defined as L_{i-1} \u2192 max {W_{i-1}.L_{i-1},0}) for 1 \u2264 i \u2264 n. Let L_j^i be the output of neuron j at layer i that is $L_j^i(x) = ReLU(\\sum_{j=1} W_{ij}^i L_i)$. The output is the likelihood of favorable and unfavorable outcomes. The predicted label is the index of the maximum likelihood, $D(x) = \\max_i L_o(x) (i)$."}, {"title": "3.2 Inference Time Dropout for Fairness", "content": "Dropout [40] is a technique proposed to improve the generalization of neural networks by preventing overfitting. The key idea is that randomly dropping out neurons in a layer during training can help prevent overfitting of the neural network. Dropout randomly sets all w_i to 0 for each neuron in the hidden layers with probability p during training. Once training is complete, dropout is not used, and all the neurons in the network are utilized to make predictions. While dropout has been traditionally used to prevent overfitting during training, we hypothesize that dropping neurons of the model during inference after training can significantly improve fairness with a minimal impact on model performance. However, unlike traditional dropout, where a set of neurons are randomly dropped during training, we aim to identify a subset of neurons at the Pareto optimal curve of fairness-performance during inference."}, {"title": null, "content": "We consider a binary vector as the neuron state with s = {0, 1}^N, where $N = \\Sigma_{i=0}^n |L_i|$, $s_i$ indicates whether the neuron i is dropped or not and n is the number of layers. A pre-trained DNN model D does not include any dropouts, and hence all the indicators are 0.\nDEFINITION 3.1 (DESIRABLE DROPOUT OF FAIRNESS VS. UTILITY). Given a DNN model D trained over a dataset X; the search problem is to infer a repaired DNN model D' by dropping a subset of neurons I in the binary neuron state, i.e., s_i = 1 for any i \u2208 I, such that (1) the model bias (e.g., EOD) is maximally reduced, (2) the model performance (e.g., F1-score) is minimally degraded, and (3) the model structure in terms of the numbers of inputs, outputs, and hidden layers remains the same as compared to the original pre-trained model D.\nA brute-force search would be exponential in the size of DNN as we have $2^{|N|}$ possible subsets. The running time becomes pro-hibitively expensive, even for small neural networks. We define de-sirable states as those states that have a good fairness-performance tradeoff. To find the desirable subset of neurons, we explore differ-ent types of randomized algorithms to improve fairness via model inference dropout."}, {"title": "4 APPROACH", "content": "Our approach comprises two randomized search algorithms, namely Simulated Annealing (SA) and Random Walk (RW)."}, {"title": "4.1 Simulated Annealing Search", "content": "We formulate the problem of finding the desirable subset of neurons as a discrete optimization problem and solve it using Simulated An-nealing. Simulated Annealing (SA) [4] is a probabilistic algorithm that finds the global minima (w.r.t some cost function) in a large search space with high probability. Algorithm 1 presents a generic template to apply SA to a search space optimization problem. Figure 1 overviews the steps in our bias mitigation approach with the simulated annealing search. We now define the core concepts used in our SA algorithm."}, {"title": null, "content": "Search Space. For a DNN M \u2190 [L_1, L_0, . . ., L_n, L_o] we define a state of our search space S as a binary sequence s \u2208 {0, 1}^N, where $N = \\Sigma_{i=0}^n |L_i|^1$. The ith element of s, denoted s_i is 1 if neuron i is dropped and 0 otherwise^2. For example, consider a DNN M \u2190 [L_I, L_0, L_1, L_O], where |L_0| = 3 and |L_1| = 3. Then, the state s given by:\ns = (0, 1, 0, 0, 0, 1)\n$\\underbrace{}_{L_o} ~~~~ \\underbrace{}_{L_1}$\ndrops the second neuron in the first layer and the third neuron in the second layer.\nInstead of allowing our search space S to drop every possible subset of neurons possible (which would ensure S has size $2^N$), we restrict the size of S by fixing an upper and lower bound on the number of neurons that can be dropped from the DNN. Let $n_l$ and $n_u (n_l \u2264 n_u)$ denote the minimum and maximum number of neurons allowed by our DNN, respectively. We can then formally define S as:\n$S := \\{s \u2208 \\{0,1\\}^N | n_l \u2264 HW(s) \u2264 n_u\\}$                                                      (1)\nwhere HW(s) denotes the hamming weight of s. Restricting our search space with a conservative estimate of the lower and upper bound would have a minimal impact on our ability to find the desirable subset, as dropping too many neurons will reduce the model utility to less than acceptable levels, whereas dropping too few neurons will improve fairness by only a marginal amount. However, asymptotically, a conservative estimate of the lower and upper bound still generates prohibitively large search spaces. With the bounds $n_l$ and $n_u$, the cardinality of the search space |S| = $\\Sigma_{l=n_l}^{n_u} {N \\choose n}$ = $\\Omega(\\frac{N}{VN})$ = $\\Omega(\\frac{N}{VN})$ if $n_l < \\frac{N}{VN} < n_u$, which rules out brute-force as a viable option.\nNeighborhood Relation and \u2018Generate\u2019 subroutine. The neighbor-hood of any state s \u2208 S, denoted \u0393(s), is defined as the set of all"}, {"title": null, "content": "states that are at a hamming distance of 1. Mathematically, this is defined as\n\u0393(s) := {s' \u2208 S | HD(s, s') = 1}\nwhere HD(s, s') denotes the hamming distance between s and s'. With our definition of the search space and neighborhood of a state, the entire search space graph can be viewed as a subset of the N-dimensional hypercube, where the vertices of the hypercube rep-resent the states of our search space, and the edges of the hypercube represent the neighborhood relation. The Generate subroutine on input s \u2208 S uniformly samples a neighbor s' from \u0393(s) and returns s'. This is equivalent to uniformly sampling an index position i from [1, n] and subsequently uniformly flipping bit si in the binary sequence s to get s'. We make the following observation on our underlying search space graph\u00b3:\nLEMMA 4.1. If $n_l$ < $n_u$, then our search space graph is connected and has a diameter less than N. Moreover, the distance between any two states s, s' \u2208 S is given by HD(s, s').\nFor example, Figure 2 shows the search space graph for a neu-ral network with 1 hidden layer with 3 neurons, along with the neighbors and state transitions.\nCost Function. Our primary goal is to find a state s \u2208 S which minimizes its unfairness score EODs. However, we do not wish to consider those states with a significant loss in model performance (measured using the F1 score of the model) compared to the original model; thus, we try to find an acceptable balance between the improvement in unfairness and the loss in model performance. To realize this balance, we penalize our cost function with an additional term to artificially increase the cost if the state has a lower F1 score. We now formally define our cost function, cost() as follows:\ncost(s) := EODs + p. EODs0 1(F1s < tF1s0)       (2)\nIn the above equation, s0 is the original state (the DNN with no dropout), p \u2208 R\u22650 is called the penalty multiplier, t \u2208 (0, 1) is called the threshold multiplier, EODs and EODs0 are the unfairness scores of states s and s0 respectively, F1s and F1s0 are the F1 scores of the s and s0 respectively, and 1(.) denotes the indicator function."}, {"title": "4.2 Random Walk Search", "content": "The Random Walk (RW) strategy samples a random state s \u2208 S, and recursively samples states from the neighborhood (\u0393(s)) to explore the search space. The RW strategy then records the best state dis-covered throughout the walk. We use the same cost function as SA in RW to determine the desirable state. A key difference between RW and SA search is that in RW, we always transition to a new state, regardless of cost. In contrast, in SA, we always transition to a new state with a lower cost and transition with some probability p\u2208 [0, 1) if it has a higher cost. This is highlighted in Line 14 in"}, {"title": "5 EXPERIMENTS", "content": "We pose the following research questions:\nRQ1 How successful are randomized algorithms in fairness re-pairing of DNNs via dropouts?\nRQ2 Are there dropout strategies that improve fairness and utility together?\nRQ3 What are the design considerations of search algorithms for efficient and effective fairness repair of DNNs via dropout?\nRQ4 How do dropout strategies compare to the state-of-the-art post-processing (bias) mitigators?"}, {"title": "5.1 Datasets and Models", "content": "We evaluate NEUFAIR with five different datasets from fairness liter-ature. For two of the datasets, we consider two different protected groups, which effectively results in a total of 7 different benchmarks. The Adult Census Income [14], Bank Marketing [13], Compas Soft-ware [38], Default Credit [15], and Medical Expenditure (MEPS16) [17] are binary classification tasks to predict whether an individual has income over 50K, is likely to subscribe, has a low reoffending risk, is likely to default on the credit card payment, and is likely to utilize medical benefits, respectively.\nWe used DNNs as the machine learning model, trained over the dataset benchmarks. We use ReLU as the activation function after the linear layers. For the Bank, Default, and Compas datasets we use a dropout of 0.2 during training. For the Adult and MEPS16 datasets, we set the dropout to 0.1. For data preprocessing, we utilize standard techniques such as one-hot encoding for categorical features followed by min-max or standard scaling for numerical features."}, {"title": "5.2 Technical Details", "content": "We run all our experiments on a desktop running Ubuntu 22.04.3 LTS with an Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz processor, 32GB RAM, and a 1 TB HDD. The neural networks and repair algorithms were implemented using python3.8, torch==2.0.1, numpy==1.24.3, and scikit-learn==1.3.1."}, {"title": "5.3 Experimental Setup", "content": "For each dataset, we evaluate the performance of the NEUFAIR al-gorithms using 10 different random seeds. Specifically, the seed determines the randomness of the training, validation, and test splits and the randomness of model training. The NEUFAIR algorithms themselves are not seeded. Each SA and RW run is unique.\nDuring training, the training dataset was used for gradient de-scent, and the validation dataset was used to monitor the F1 score and determine the hyperparameters. Unless otherwise specified, all runs reported in the paper have a time-out limit of 1 hour with the following cost function:\nC(s) = EODs + 3.0 EODs0 1 (F1s < (0.98. F1s0)).\nThe F1 threshold is 98% of the validation F1 score of the unfair neural network, i.e., we tolerate a 2% degradation in the F1 score to improve fairness. For all datasets, we set the minimum number of neurons to nl = 2 and vary the maximum number of neurons nu between 20% - 40% of the number of hidden layer neurons depending on the overall size of the network.\n5.3.1 Effectiveness of randomized algorithms on mitigating unfair-ness using dropout (RQ1). Figure 3 and Figure 4 highlight the cost of the best state found for the SA and RW over time, respectively, for 10 different seeds per data subject using the logarithmic scale. In all cases, we see a positive trend where the cost improves over time, and the search identifies better subsets of neurons to drop. The progress graphs of the RW strategy are \"steep\" initially and rapidly improve as the initial states are unfair and also heavily penalized by the penalty multiplier as RW generates intermediate states with an F1 score less than the threshold. "}, {"title": null, "content": "Answer RQ1: The randomized algorithms effectively mitigate unfairness via dropouts by de-activating a desirable subset of neurons. On the test set, Fairness improves by up to 69%. The SA algorithm performs better than the RW algorithm.\n5.3.2 Dropout strategies improving both fairness and utility (RQ2). The second research question is answered by observing the F1 score and accuracy (model utility metrics), besides the EOD score (fair-ness). For all datasets, the F1 score decreases for the validation and test sets. The decrease in F1 score is accompa-nied by an improvement in fairness which highlights the tradeoff between the model utility and fairness.\nAnswer RQ2: The overall accuracy, as a model utility metric, may improve along with fairness using dropout strategies, as ac-curacy does not account for false positives and false negatives in imbalanced datasets. However, the F1 score model utility metric decreases as fairness increases."}, {"title": "5.3.3 Hyperparameters of randomized algorithms and fairness (RQ3)", "content": "The NEUFAIR algorithms have 4 hyperparameters: F1 threshold mul-tiplier, the minimum and maximum number of neurons to drop, time-out limit, and F1 penalty multiplier.\n\u2022 The F1 threshold multiplier is inversely proportional to fairness improvement. \u2022 The time-out positively affects fairness as the NEUFAIR algo-rithms have more time to explore the search space.\n\u2022 The F1 penalty multiplier has a more nuanced effect on the un-fairness as it controls the exploration vs. exploitation trade-offs. A low penalty multiplier increases the probability of state tran-sitions when the F1 score is less than our threshold, increasing the SA run's randomness to explore more of the search space outside of the current best state. A high penalty multiplier would decrease the probability of state transitions, thereby encouraging the SA algorithm to exploit the search space near the current best state.\nAnswer RQ3: The fairness of the repaired model can improve as the F1 threshold decreases, the number of neurons to drop increases, and the time-out limit increases for both NEUFAIR-SA and NEUFAIR-RW algorithms. The F1 penalty multiplier controls a trade-off between explorations vs. exploitation, which requires tuning for each specific benchmark. A higher F1 penalty mul-tiplier encourages SA to exploit the local solutions, whereas a lower penalty encourages exploring the global space."}, {"title": "5.3.4 Comparing to the state-of-the-art (RQ4)", "content": "We compare the efficacy of NEUFAIR against a state-of-the-art post-processing bias mitigator. DICE [35] used a fault localization technique via do logic to pinpoint a single neuron that significantly influences fairness. Table 6 highlights the test fairness after dropping a single neuron that most impacts fairness.\nAnswer RQ4: NEUFAIR outperforms the state-of-the-art unfair-ness mitigation algorithm DICE [35] across all benchmarks. The relative improvement of NEUFAIR is 56% higher than DICE."}, {"title": "6 DISCUSSION", "content": "Effectiveness of Randomized Algorithms. To understand why random-ized algorithms are effective in mitigating unfairness, we compare them to brute-force strategies. We create a small DNN for the Adult (Sex) dataset with 2 hidden layers, each with 16 neurons, to allow the brute force search to explore the entire space. We use a restricted search space with $n_l$ = 4 and $n_u$ = 9 that has a total of 43, 076, 484 states. SA finds the global optimal solution for two seeds in the search space.\nLimitations. Since we used randomized algorithms, our results might be sub-optimal. While simulated annealing provides sta-tistical guarantees on the confidence and running time to reach the optimal state, in practice such guarantees might require mul-tiple runs of algorithms over a long period of time.Threat to Validity. To address the internal validity and ensure our finding does not lead to an invalid conclusion, we follow the estab-lished SE guidelines and repeat the experiments 10 times, reporting both the average and 95% confidence intervals."}, {"title": "7 RELATED WORK", "content": "Since the main focus of this paper is unfairness mitigation, we primarily focus on the prior work on mitigation.\nA) Pre-processing Techniques exploit the space of input data to mit-igate fairness defects [16, 19, 28, 51]. Reweighting [28] augments the dataset where the datapoints in each group-label combina-tion are weighted differently to ensure fairness. Our work is tangential as it focuses on post-processing mitigation that does not modify or access the training dataset and works on pre-trained unfair DNNs.\nB) In-processing techniques aim to improve fairness during train-ing. One method, known as adversarial debiasing [52], employs adversarial learning to develop a classifier that not only improves prediction accuracy but also hinders the ability of adversaries to determine sensitive attributes from ML predictions. C) Post-processing techniques aim to modify the prediction outcomes of ML models to reduce discrimination [24, 29, 37, 43]. Compared to FAIRE, NEUFAIR does not retrain the model and does not separately use the protected attribute to iden-tify unfair neurons.\nFinally, FAIRNEURON utilizes dropouts to improve fairness, it fundamen-tally differs from NEUFAIR."}, {"title": "8 CONCLUSION", "content": "In this paper, we tackle the problem of mitigating unfairness in pre-trained deep neural networks using the dropout method. We showed that the neural dropout problem over the DNN models is computationally hard and presented NEUFAIR, a family of random-ized algorithms to efficiently and effectively improve the fairness of DNNs. Our experiments showed that NEUFAIR can identify an ideal subset of neurons to drop that disparately contribute to unfair-ness (leading to up to 69% fairness improvement) and outperform a state-of-the-art post-processing bias mitigator."}]}