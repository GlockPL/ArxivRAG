{"title": "NeuFair: Neural Network Fairness Repair with Dropout", "authors": ["Vishnu Asutosh Dasu", "Saeid Tizpaz-Niari", "Ashish Kumar", "Gang Tan"], "abstract": "This paper investigates the neural dropout method as a post-processing bias mitigation for deep neural networks (DNNs). Neural-driven software solutions are increasingly applied in socially critical domains with significant fairness implications. While neural networks are exceptionally good at finding statistical patterns from data, they are notorious for over-fitting the training datasets that may encode and amplify existing biases from the historical data. Existing bias mitigation algorithms often require either modifying the input dataset or modifying the learning algorithms. We posit that the prevalent dropout methods that prevent over-fitting during training by randomly dropping neurons may be an effective and less intrusive approach to improve fairness of pre-trained DNNs. However, finding the ideal set of neurons to drop is a combinatorial problem.\nWe propose NEUFAIR, a family of post-processing randomized algorithms that mitigate unfairness in pre-trained DNNs via dropouts during inference after training. Our randomized search is guided by an objective to minimize discrimination while maintaining the model's utility. We show that our design of randomized algorithms are effective and efficient in improving fairness (up to 69%) with minimal or no model performance degradation. We provide intuitive explanations of these phenomena and carefully examine the influence of various hyperparameters of search algorithms on the results. Finally, we empirically and conceptually compare NEUFAIR to different state-of-the-art bias mitigators.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial intelligence (AI), increasingly deployed with deep neural network (DNN) components, has become an integral part of modern software solutions that assist in socio-economic and legal-critical decision-making processes such as releasing patients [30], identifying loan defaults [22], and detecting tax evasion [42].\nDespite many advances made possible by AI, there are challenges that require understanding the dimensions and implications of deploying Al-driven software solutions. One such concern about the trustworthiness of AI is discrimination. Unfortunately, there are plenty of fairness defects in real systems. Parole decision-making software was found to harm black and Hispanic defendants by falsely predicting a higher risk of recidivism than for non-Hispanic white defendants [27]; Amazon's hiring algorithm disproportionately rejected more female applicants than male applicants [26]; and data-driven auditing algorithm selected black taxpayers with earned income tax credit claims (EITC) at much higher rates than other racial groups for an audit [42]. As evidenced by these examples, resulting software may particularly disadvantage minorities and protected groups and be found non-compliant with law such as the US Civil Rights Act [6]. Hence, helping programmers and users to mitigate unfairness in social-critical data-driven software systems is crucial to ensure inclusion in our modern, increasingly digital society.\nThe software engineering (SE) community has spent significant efforts to address discrimination in the automated data-driven software solutions [2, 10, 18, 47, 56]. Fairness has been treated as a critical meta-property that requires analysis beyond functional correctness and measurements beyond prediction accuracy [7]. Thus, the community presents various testing [2, 18, 57], debugging [21, 35, 50], and mitigation [5, 53] techniques to address fairness defects in data-driven software.\nBroadly, fairness mitigation can be applied in pre-processing stage (e.g., increasing the representations of an under-represented group by generating more data samples for them), in-processing step (e.g., changing the loss function to include fairness constraints during training), or post-processing stage (e.g., changing the logic of a pre-trained model). However, when the decision logic of AI is encoded via deep neural networks, it becomes increasingly challenging to mitigate unfairness due to its black-box uninterpretable nature.\nSince the DNN-based software development is derived by principle of information bottleneck [45], which provides a trade-off between over-fitting and parsimonious signals; we posit that over-fitted DNN models may contribute and amplify historical biases from the training datasets. Therefore, prevalent techniques such as dropout methods [39, 40]-a process of randomly dropping neurons during training-might be effective in mitigating unfairness in the pre-trained DNN models. While the dropout strategy has been significantly used to prevent over-fitting during the training of DNNs, to the best of our knowledge, this is the first work to systematically leverage dropout methods for mitigating unfairness in pre-trained DNN models, as a post-processing mitigation method.\nHowever, finding the optimal subset of dropout neurons is a combinatorial problem that requires an exhaustive search in the subset of all combinations of neuron dropouts, and hence is intractable. To overcome the computational challenges, we explore the class of randomized algorithms with Markov chain Monte Carlo (MCMC) strategies to efficiently explore the search space with statistical guarantees. In doing so, we pose the following research questions:\nRQ1. How successful are randomized algorithms in fairness repairing of DNNs via dropouts?\nRQ2. Are there dropout strategies that improve model fairness and utility together?\nRQ3. What are the design considerations of search algorithms for efficient and effective dropouts?\nRQ4. How do dropout strategies compare to the state-of-the-art post-processing (bias) mitigators?\nTo answer these research questions, we present NEUFAIR (Neural Network Fairness Repair): a set of randomized search algorithms to improve the fairness of DNNs via inference-time neuron dropouts. We design and implement simulated annealing (SA) and random walk (RW) strategies that efficiently explore the state-space of neuron dropouts where we encode the frontiers of fairness and utility in a cost function.\nWe evaluate NEUFAIR over 7 deep neural network benchmarks as trained over 5 socially critical applications with significant fairness implications. We found that randomized strategies of neuron drops can successfully improve fairness (up to 69%) with a minimal utility degradation in the vast majority of cases. We also report a pathological case and reasons behind a failure of NEUFAIR. We also observe that NEUFAIR can improve fairness and utility together and provide intuitive explanations of such phenomena. Furthermore, we examine different hyperparameter configuration options of randomized algorithms. While some hyperparameters always influence fairness with positive or negative impacts; we detect a hyperparameter that defines a trade-off between explorations and exploitation that should be tuned as a constant variable in the cost function for each benchmark. Finally, we show the effectiveness of the SA algorithm, compared to the RW and the state-of-the-art post-processing (bias) mitigator [35]. In summary, the key contributions of this paper are:\n(1) Inference-Time dropout for fairness. To the best of our knowledge, we present the first dropout method of bias mitigation over pre-trained deep neural networks,\n(2) Randomized algorithms for fairness. We create a well-defined framework to formulate the combinatorial inference-time dropout problem in DNNs using randomized algorithms,\n(3) Experimental evaluations. We implement the randomized algorithms in NEUFAIR and evaluate its effectiveness and efficiency vis-a-vis the state-of-the-art techniques."}, {"title": "2 BACKGROUND", "content": "We provide a background on the model utility and fairness measures. Then, we briefly discuss various bias mitigation methods from the Al fairness literature."}, {"title": "2.1 Notions of Model Utility", "content": "Given a binary classifier h, a set of features X, and predictions h(X) = {TP, TN, FP, FN}, we can define the following notions of model utility. In h(X), TP, TN, FP, and FN denote the set of True Positives, True Negatives, False Positives, and False Negatives. We drop the cardinality operator || in the following definitions for brevity.\n$\\begin{aligned} &\\text { Accuracy }_h=\\frac{\\text { TP + TN }}{\\text { TP + TN+FP + FN }} \\\\ &F 1_h=\\frac{2 * \\text { Precision } * \\text { Recall }}{\\text { Precision + Recall }}=\\frac{2 * \\text { TP }}{2 * \\text { TP + FP + FN }} \\end{aligned}$\nAccuracy can be used to gauge the overall performance of a classifier. However, for imbalanced datasets used in fairness evaluations, accuracy is a poor metric as the number of negative samples far outweighs the positive samples. For example, predicting all samples as negative (0 true positives) in the Bank [13] dataset yields an accuracy of 88% but the F1 score would be 0/undefined. Since F1 is defined as the harmonic mean of precision and recall, rather than the arithmetic mean, it penalizes performance significantly if either precision or recall is low."}, {"title": "2.2 Notions of Model Fairness", "content": "Consider a machine learning classifier h, a set of features X, sensitive features $A \\subset X$, and a set of labels Y. We can then define the following notions of fairness.\nDefinition 2.1 (DEMOGRAPHIC PARITY [1]). The classifier h satisfies Demographic Parity under a distribution over (X, A, Y) if its prediction h(X) is statistically independent of the sensitive feature A i.e. P[h(X) = \u0177|A = a] = P[h(X) = \u0177] for all a, y. For binary classification with \u0177 = {0, 1}, this is equivalent to E[h(X)|A = a] = E[h(X)] for all a.\nDefinition 2.2 (EQUALIZED ODDS [1, 24]). The classifier h satisfies Equalized Odds under a distribution over (X, A, Y) if its prediction h(X) is conditionally independent of the sensitive feature A given the label Y i.e. P[h(X) = \u0177|A = a, Y = y] = P[h(X) = \u0177|Y = y] for all a, y, and \u0177. For binary classification with \u0177 = {0, 1}, this is equivalent to E[h(X)|A = a, Y = y] = E[h(X)|Y = y] for all a, y.\nEqual Opportunity is a relaxed variant of Equalized Odds with Y = 1 [24]. In other words, Equalized Odds require the true positive and false positive rates to be equal across all sensitive groups. In contrast, Equal Opportunity only requires the true positive rate to be equal across all sensitive groups.\nDemographic Parity is the weakest notion of fairness, and Equalized Odds is the strongest. In our work, we use Equalized Odds as the fairness criterion. The disparity or unfairness for Equalized Odds is the Equalized Odds Difference (EOD), defined as the maximum absolute difference between the true and false positive rates across the sensitive groups. Mathematically, this is represented as\n$\\text{EOD} = \\max\\begin{aligned} &\\left[\\left|\\mathbb{P}[h(X) = 1 \\mid A = 0, Y = 1] - \\mathbb{P}[h(X) = 1 \\mid A = 1, Y = 1]\\right|, \\right.\\\\ &\\left|\\mathbb{P}[h(X) = 1 \\mid A = 0, Y = 0] - \\mathbb{P}[h(X) = 1 \\mid A = 1, Y = 0]\\right|\\Bigg] \\end{aligned}$\nwith sensitive features A = {0, 1}."}, {"title": "3 PROBLEM STATEMENT", "content": "We consider pre-trained deep neural network (DNN) classifiers with the set of input variables X partitioned into a protected set of variables (such as race, sex, and age) and non-protected variables (such as profession, income, and education). We further assume the output is a binary classifier that gives either favorable or unfavorable outcomes."}, {"title": "3.1 Syntax and Semantics of DNN", "content": "A deep neural network (DNN) encodes a function $D: X \\rightarrow [0,1]^2$ where X consists of the set of protected attributes $X_1 \\times X_2 \\cdots \\times X_m$ and non-protected attributes $X_{m+1} \\cdots Y_{m+r}$. The DNN model is parameterized by the input dimension m+r, the output dimension 2, the depth of hidden layers n + 1, and the weights of its hidden layers $W_0, W_1, ..., W_n$. We describe the hidden layers with $M\\leftarrow [L_1, L_0, . . ., L_n, L_o]$, where $L_1$ and $L_o$ are the input and output layers, respectively, and $L_i, \\forall i \\in [0, n]$, is the hidden layers. We assume that there exists a subset of neurons $N\\in L_i, \\forall i \\in [0, n]$ in the hidden layers that disparately contribute to unfairness.\nLet $L_i$ be the output of layer i that implements an affine mapping from the output of previous layer $L_{i-1}$ and its weights $W_{i-1}$ for $1 \\leq i \\leq n$ followed by a fixed non-linear activation unit (e.g., ReLU defined as $L_{i-1} \\rightarrow \\max \\{W_{i-1}.L_{i-1}, 0\\}$) for $1 \\leq i \\leq n$. Let $L_{i}^{j}$ be the output of neuron j at layer i that is $L_{i}^{j}(x) = \\text{ReLU}\\left( \\sum_{l=1}^{|L_{i-1}|} W_{il} L_{i-1}^{l} \\right)$. The output is the likelihood of favorable and unfavorable outcomes. The predicted label is the index of the maximum likelihood, $D(x) = \\text{max}_i L_o(x)(i)$."}, {"title": "3.2 Inference Time Dropout for Fairness", "content": "Dropout [40] is a technique proposed to improve the generalization of neural networks by preventing overfitting. The key idea is that randomly dropping out neurons in a layer during training can help prevent overfitting of the neural network. Dropout randomly sets all $w_i$ to 0 for each neuron in the hidden layers with probability p during training. Once training is complete, dropout is not used, and all the neurons in the network are utilized to make predictions. While dropout has been traditionally used to prevent overfitting during training, we hypothesize that dropping neurons of the model during inference after training can significantly improve fairness with a minimal impact on model performance. However, unlike traditional dropout, where a set of neurons are randomly dropped during training, we aim to identify a subset of neurons at the Pareto optimal curve of fairness-performance during inference.\nWe consider a binary vector as the neuron state with $s = \\{0, 1\\}^N$, where $N = \\Sigma_{i=0}^{n} |L_i|$, $s_i$ indicates whether the neuron i is dropped or not and n is the number of layers. A pre-trained DNN model D does not include any dropouts, and hence all the indicators are 0.\nDefinition 3.1 (DESIRABLE DROPOUT OF FAIRNESS VS. UTILITY). Given a DNN model D trained over a dataset X; the search problem is to infer a repaired DNN model $D'$ by dropping a subset of neurons I in the binary neuron state, i.e., $s_i = 1$ for any $i \\in I$, such that (1) the model bias (e.g., EOD) is maximally reduced, (2) the model performance (e.g., F1-score) is minimally degraded, and (3) the model structure in terms of the numbers of inputs, outputs, and hidden layers remains the same as compared to the original pre-trained model D.\nA brute-force search would be exponential in the size of DNN as we have $2^{|N|}$ possible subsets. The running time becomes prohibitively expensive, even for small neural networks. We define desirable states as those states that have a good fairness-performance tradeoff. To find the desirable subset of neurons, we explore different types of randomized algorithms to improve fairness via model inference dropout."}, {"title": "4 APPROACH", "content": "Our approach comprises two randomized search algorithms, namely Simulated Annealing (SA) and Random Walk (RW)."}, {"title": "4.1 Simulated Annealing Search", "content": "We formulate the problem of finding the desirable subset of neurons as a discrete optimization problem and solve it using Simulated Annealing. Simulated Annealing (SA) [4] is a probabilistic algorithm that finds the global minima (w.r.t some cost function) in a large search space with high probability. Algorithm 1 presents a generic template to apply SA to a search space optimization problem. Figure 1 overviews the steps in our bias mitigation approach with the simulated annealing search. We now define the core concepts used in our SA algorithm.\nFor a DNN $M \\leftarrow [L_1, L_0, . . ., L_n, L_o]$ we define a state of our search space S as a binary sequence $s \\in \\{0, 1\\}^N$, where $N = \\Sigma_{i=0}^{n} |L_i|^1$. The ith element of s, denoted $s_i$ is 1 if neuron i is dropped and 0 otherwise$^2$. For example, consider a DNN $M \\leftarrow [L_I, L_0, L_1, L_O]$, where $|L_0| = 3$ and $|L_1| = 3$. Then, the state s given by:\n$\\mathbf{s = (0, 1, 0, 0, 0, 1)}$\n$L_o\\qquad L_1$\ndrops the second neuron in the first layer and the third neuron in the second layer.\nInstead of allowing our search space S to drop every possible subset of neurons possible (which would ensure S has size $2^N$), we restrict the size of S by fixing an upper and lower bound on the number of neurons that can be dropped from the DNN. Let $n_l$ and $n_u$ ($n_l \\leq n_u$) denote the minimum and maximum number of neurons allowed by our DNN, respectively. We can then formally define S as:\n$\\mathcal{S}:=\\{\\mathbf{s} \\in \\{0,1\\}^N \\mid n_l \\leq HW(\\mathbf{s}) \\leq n_u\\}$  (1)\nwhere HW(s) denotes the hamming weight of s. Restricting our search space with a conservative estimate of the lower and upper bound would have a minimal impact on our ability to find the desirable subset, as dropping too many neurons will reduce the model utility to less than acceptable levels, whereas dropping too few neurons will improve fairness by only a marginal amount. However, asymptotically, a conservative estimate of the lower and upper bound still generates prohibitively large search spaces. With the bounds $n_l$ and $n_u$, the cardinality of the search space $|\\mathcal{S}| = \\sum_{i=n_l}^{n_u} \\binom{N}{i} = \\Omega(\\binom{N}{VN} \\\\text{if } n_l < \\frac{N}{VN} < n_u)$, which rules out brute-force as a viable option.\nNeighborhood Relation and \u2018Generate\u2019 subroutine. The neighborhood of any state s \u2208 S, denoted \u0393(s), is defined as the set of all\nstates that are at a hamming distance of 1. Mathematically, this is defined as\n$\\Gamma(\\mathbf{s}) :=\\{\\mathbf{s}^{\\prime} \\in \\mathcal{S} \\mid HD(\\mathbf{s}, \\mathbf{s}^{\\prime})=1\\}$\nwhere HD(s, s') denotes the hamming distance between s and s'. With our definition of the search space and neighborhood of a state, the entire search space graph can be viewed as a subset of the N-dimensional hypercube, where the vertices of the hypercube represent the states of our search space, and the edges of the hypercube represent the neighborhood relation. The Generate subroutine on input s \u2208 S uniformly samples a neighbor s' from I(s) and returns s'. This is equivalent to uniformly sampling an index position i from [1, n] and subsequently uniformly flipping bit $s_i$ in the binary sequence s to get s'. We make the following observation on our underlying search space graph\u00b3:\nLemma 4.1. If $n_l < n_u$, then our search space graph is connected and has a diameter less than N. Moreover, the distance between any two states s, s' \u2208 S is given by HD(s, s').\nFor example, Figure 2 shows the search space graph for a neural network with 1 hidden layer with 3 neurons, along with the neighbors and state transitions.\nCost Function. Our primary goal is to find a state s \u2208 S which minimizes its unfairness score EODs. However, we do not wish to consider those states with a significant loss in model performance (measured using the F1 score of the model) compared to the original model; thus, we try to find an acceptable balance between the improvement in unfairness and the loss in model performance. To realize this balance, we penalize our cost function with an additional term to artificially increase the cost if the state has a lower F1 score. We now formally define our cost function, cost() as follows:\n$\\text{cost}(s):=\\text{EOD}_s + p\\cdot\\text{EOD}_{s_o}\\cdot\\mathbb{1}(F1_s< tF1_{s_o})\\qquad$(2)\nIn the above equation, so is the original state (the DNN with no dropout), $p \\in R^{\\geq 0}$ is called the penalty multiplier, t \u2208 (0, 1) is called the threshold multiplier, EODs and $EOD_{s_o}$ are the unfairness scores of states s and so respectively, $F1_s$ and $F1_{s_o}$ are the F1 scores of the s and so respectively, and $\\mathbb{1}(.)$ denotes the indicator function.\nThe threshold multiplier t determines the percentage loss in F1 score we will tolerate to improve fairness. The penalty multiplier p discourages states with an F1 score less than the threshold by penalizing them with a multiple of the unfairness of the initial state. Our cost function formulation allows us to find the state with the minimum unfairness while maintaining a significantly higher F1.\nInitial Temperature, Cooling Schedule and \u2018Update\u2019 subroutine. The temperature T of the SA procedure determines the probability with which we accept a positive transition (a transition where the cost is increased); the higher the temperature, the more likely the algorithm accepts such a transition. The cooling schedule refers to the function we use to update our temperature after each iteration. We adopt the logarithmic cooling schedule [23] which has proven convergence guarantees [34]. According to the logarithmic cooling schedule, the temperature T(i) at iteration m is defined as\n$\\text{T}_{m}=\\frac{T_{0}}{\\log(2 + m)}, \\quad \\forall m \\in \\mathbb{Z}^{\\geq 0}$  (3)\nThe Update subroutine updates the temperature using the above formula. Using Mitra et al. [34] convergence results for SA with a logarithmic cooling schedule, we get the following result for our SA runs:\nLemma 4.2. Let p > 1. If we set $T_0 \\geq (1 + pEOD_{so})^2\\frac{(n_u-n_l)}{4}$, then the probability that SA finds a global minima within $k > \\left[Muni\\right]$ iterations is greater than $1 - \\frac{A}{(k)^c}$ where $c:=\\min\\left(\\frac{1}{(n_u - n_l) \\sqrt{N[1] T_0 S^2}},\\frac{4}{(n_u - n_l)^2}\\right)$, where S is the size of the validation dataset and A > 0 is a constant.\nWhile the above lemma provides a worst-case bound on the number of iterations required to find an optimal solution, it is practically infeasible to achieve the large bound. Ben-Ameur [3] proposes an algorithm for computing To, with which they show SA performs well experimentally with fewer iterations than estimated by [34]. We use the temperature initialization algorithm from [3] to estimate the initial temperature To. At a high level, this algorithm backcomputes an initial value for the temperature for which the expected value of acceptance probabilities for positive transitions from a random initial distribution is greater than some predefined threshold$^4$.\nStopping Criterion. We use time as a stopping criterion for our SA runs i.e., we run the SA search for time less than some hyperparameter time_limit."}, {"title": "4.2 Random Walk Search", "content": "The Random Walk (RW) strategy samples a random state s \u2208 S, and recursively samples states from the neighborhood () to explore the search space. The RW strategy then records the best state discovered throughout the walk. We use the same cost function as SA in RW to determine the desirable state. A key difference between RW and SA search is that in RW, we always transition to a new state, regardless of cost. In contrast, in SA, we always transition to a new state with a lower cost and transition with some probability p\u2208 [0, 1) if it has a higher cost. This is highlighted in Line 14 in Algorithm 2. A RW can also be considered an SA run with infinite temperature, i.e., the transition probability is always 1."}, {"title": "5 EXPERIMENTS", "content": "We pose the following research questions:\nRQ1 How successful are randomized algorithms in fairness repairing of DNNs via dropouts?\nRQ2 Are there dropout strategies that improve fairness and utility together?\nRQ3 What are the design considerations of search algorithms for efficient and effective fairness repair of DNNs via dropout?\nRQ4 How do dropout strategies compare to the state-of-the-art post-processing (bias) mitigators?"}, {"title": "5.1 Datasets and Models", "content": "We evaluate NEUFAIR with five different datasets from fairness literature. For two of the datasets, we consider two different protected groups, which effectively results in a total of 7 different benchmarks. The Adult Census Income [14], Bank Marketing [13], Compas Software [38], Default Credit [15], and Medical Expenditure (MEPS16) [17] are binary classification tasks to predict whether an individual has income over 50K, is likely to subscribe, has a low reoffending risk, is likely to default on the credit card payment, and is likely to utilize medical benefits, respectively.\nWe used DNNs as the machine learning model, trained over the dataset benchmarks. We use ReLU as the activation function after the linear layers. For the Bank, Default, and Compas datasets we use a dropout of 0.2 during training. For the Adult and MEPS16 datasets, we set the dropout to 0.1. For the Compas dataset, we use Adam as the optimizer with a learning rate of 0.001. For the other datasets, we use SGD with a learning rate of 0.01. For data preprocessing, we utilize standard techniques such as one-hot encoding for categorical features followed by min-max or standard scaling for numerical features. For the Compas dataset, we use a version of the dataset that has 12 features after feature selection that was used in [46]. The modified Compas dataset is available online5."}, {"title": "5.2 Technical Details", "content": "We run all our experiments on a desktop running Ubuntu 22.04.3 LTS with an Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz processor, 32GB RAM, and a 1 TB HDD. The neural networks and repair algorithms were implemented using python3.8, torch==2.0.1, numpy==1.24.3, and scikit-learn==1.3.1."}, {"title": "5.3 Experimental Setup", "content": "For each dataset, we evaluate the performance of the NEUFAIR algorithms using 10 different random seeds. The seeds are set for torch, numpy, and scikit-learn libraries before DNN training. Specifically, the seed determines the randomness of the training, validation, and test splits and the randomness of model training, such as sampling batches and initializing the weights of the DNN. The NEUFAIR algorithms themselves are not seeded. Each SA and RW run is unique.\nDuring training, the training dataset was used for gradient descent, and the validation dataset was used to monitor the F1 score and determine the hyperparameters. We use the model from the epoch with the highest validation dataset. The same validation dataset is used by NEUFAIR to identify the desirable set of neurons to drop. Finally, we evaluate the fairness of the original and fixed (repaired) models using the test dataset.\nUnless otherwise specified, all runs reported in the paper have a time-out limit of 1 hour with the following cost function:\nC(s) = EODs + 3.0 \u00b7 EODso \u00b7 1(F1s < (0.98 \u00b7 F1so)).\nThe F1 threshold is 98% of the validation F1 score of the unfair neural network, i.e., we tolerate a 2% degradation in the F1 score to improve fairness. A penalty of 3\u00d7 the baseline unfairness is added to the fairness of the current state if its F1 score is less than the threshold. For all datasets, we set the minimum number of neurons to $n_l$ = 2 and vary the maximum number of neurons $n_u$ between 20% - 40% of the number of hidden layer neurons depending on the overall size of the network. The $n_u$ values are 50, 24, 24, 20, and 135 for Adult, Compas, Bank, Default, and MEPS16 respectively. For all datasets, we use a train/validation/test split of 60%/20%/20%."}, {"title": "5.3.1 Effectiveness of randomized algorithms on mitigating unfair-ness using dropout (RQ1)", "content": "The first research question is answered by analyzing the data in Figure 3, Figure 4, Table 3, and Table 4. Figure 3 and Figure 4 highlight the cost of the best state found for the SA and RW over time, respectively, for 10 different seeds per data subject using the logarithmic scale. In all cases, we see a positive trend where the cost improves over time, and the search identifies better subsets of neurons to drop. We note the seemingly flat plots correspond to those cases where a good state was found in the beginning. The exception is the black line in the sub-plot for the Default dataset in both figures as it corresponds to a failed run; i.e., the best state found has an F1 score lesser than 0.98\u00d7 the F1 score of the initial state. Out of 70 runs (10 per benchmark), the SA and RW found a desirable state with 0.98\u00d7 the initial F1 score in 69 cases. In addition, we re-run the experiment with an F1 threshold multiplier of 0.96 and identify a desirable state with a test set fairness of 6.26%, an improvement from 13.2%. The progress graphs of the RW strategy are \"steep\" initially and rapidly improve as the initial states are unfair and also heavily penalized by the penalty multiplier as RW generates intermediate states with an F1 score less than the threshold. However, due to the cost function"}, {"title": "Pathological Case", "content": "Table 5 highlights the EOD scores for COMPAS (Sex) with the original and repaired networks. Here, we observe that SA improves the EOD in 4 cases, does not improve in 1 case, and has a worse EOD in 5 cases. RW improves the EOD in 5 cases and has a worse EOD in the other 5 cases. There are two reasons for the different trends in COMPAS (Sex). First, there is a discrepancy in the original model statistics between the validation and test splits. The difference arises because the DNN tends to overfit slightly on the training/validation sets. This is evident from model metrics in Table 3. Second, the COMPAS datasets by default are quite fair (according to EOD) compared to other datasets as their unfairness is around 2%. During the search, the SA algorithm correctly identifies a desirable state according to the validation dataset and performs better than RW. However, given the low initial unfairness and the model's tendency to perform better on the validation/training sets, we observe minor discrepancies in the test set. The average unfairness, however, increases by 0.399% (2.522% to 2.921%), which is negligible. For both NEUFAIR algorithms, the validation fairness improves in the COMPAS (Sex) experiment.\nAnswer RQ1: The randomized algorithms effectively mitigate unfairness via dropouts by de-activating a desirable subset of neurons. On the test set, Fairness improves by up to 69%. The SA algorithm performs better than the RW algorithm."}, {"title": "5.3.2 Dropout strategies improving both fairness and utility (RQ2)", "content": "The second research question is answered by observing the F1 score and accuracy (model utility metrics), besides the EOD score (fairness) in Table 3 vs. Table 4. For all datasets, the F1 score decreases for the validation and test sets. The decrease in F1 score is accompanied by an improvement in fairness which highlights the tradeoff between the model utility and fairness. However, we observe that the accuracy increases for the Bank, Default, and MEPS16 datasets. The biggest improvement in accuracy is observed in MEPS16, where the Validation and Test accuracy increase from 0.79 and 0.788 to 0.86 and 0.853 for SA and to 0.856 to 0.851 for RW. For Default, the validation and test accuracies increase from 0.774 and 0.769 to 0.794 and 0.79 for both SA and RW. For Bank, the validation and test accuracies increase from 0.842 and 0.84 to 0.892 and 0.88 for SA and 0.882 and 0.881 for RW.\nThe opposing trends of F1 score and accuracy can be attributed to the increase in negative predictions (0 is negative class and 1 is positive) as more and more neurons are dropped out from the DNN. The increase in negative predictions favors the true negatives as the datasets are higly imbalanced with more negative samples. The Default, Bank, and MEPS16 datasets have 78%, 88%, and 83% of the data belonging to the negative class. The trained DNN models were optimized for F1 score on the validation dataset to improve performance on the underrepresented positive class. The accuracy increases as the increase in true negatives outweighs the drop in true positives. The F1 score, however, always decreases as we lose precision and recall when the true positives decrease and false negatives increase.\nAnswer RQ2: The overall accuracy, as a model utility metric, may improve along with fairness using dropout strategies, as accuracy does not account for false positives and false negatives in imbalanced datasets. However, the F1 score model utility metric decreases as fairness increases."}, {"title": "5.3.3 Hyperparameters of randomized algorithms and fairness (RQ3)", "content": "The NEUFAIR algorithms have 4 hyperparameters: F1 threshold multiplier"}, {"title": "NeuFair: Neural Network Fairness Repair with Dropout", "authors": ["Vishnu Asutosh Dasu", "Saeid Tizpaz-Niari", "Ashish Kumar", "Gang Tan"], "abstract": "This paper investigates the neural dropout method as a post-processing bias mitigation for deep neural networks (DNNs). Neural-driven software solutions are increasingly applied in socially critical domains with significant fairness implications. While neural networks are exceptionally good at finding statistical patterns from data, they are notorious for over-fitting the training datasets that may encode and amplify existing biases from the historical data. Existing bias mitigation algorithms often require either modifying the input dataset or modifying the learning algorithms. We posit that the prevalent dropout methods that prevent over-fitting during training by randomly dropping neurons may be an effective and less intrusive approach to improve fairness of pre-trained DNNs. However, finding the ideal set of neurons to drop is a combinatorial problem.\nWe propose NEUFAIR, a family of post-processing randomized algorithms that mitigate unfairness in pre-trained DNNs via dropouts during inference after training. Our randomized search is guided by an objective to minimize discrimination while maintaining the model's utility. We show that our design of randomized algorithms are effective and efficient in improving fairness (up to 69%) with minimal or no model performance degradation. We provide intuitive explanations of these phenomena and carefully examine the influence of various hyperparameters of search algorithms on the results. Finally, we empirically and conceptually compare NEUFAIR to different state-of-the-art bias mitigators.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial intelligence (AI), increasingly deployed with deep neural network (DNN) components, has become an integral part of modern software solutions that assist in socio-economic and legal-critical decision-making processes such as releasing patients [30], identifying loan defaults [22], and detecting tax evasion [42].\nDespite many advances made possible by AI, there are challenges that require understanding the dimensions and implications of deploying Al-driven software solutions. One such concern about the trustworthiness of AI is discrimination. Unfortunately, there are plenty of fairness defects in real systems. Parole decision-making software was found to harm black and Hispanic defendants by falsely predicting a higher risk of recidivism than for non-Hispanic white defendants [27]; Amazon's hiring algorithm disproportionately rejected more female applicants than male applicants [26]; and data-driven auditing algorithm selected black taxpayers with earned income tax credit claims (EITC) at much higher rates than other racial groups for an audit [42]. As evidenced by these examples, resulting software may particularly disadvantage minorities and protected groups and be found non-compliant with law such as the US Civil Rights Act [6]. Hence, helping programmers and users to mitigate unfairness in social-critical data-driven software systems is crucial to ensure inclusion in our modern, increasingly digital society.\nThe software engineering (SE) community has spent significant efforts to address discrimination in the automated data-driven software solutions [2, 10, 18, 47, 56]. Fairness has been treated as a critical meta-property that requires analysis beyond functional correctness and measurements beyond prediction accuracy [7]. Thus, the community presents various testing [2, 18, 57], debugging [21, 35, 50], and mitigation [5, 53] techniques to address fairness defects in data-driven software.\nBroadly, fairness mitigation can be applied in pre-processing stage (e.g., increasing the representations of an under-represented group by generating more data samples for them), in-processing step (e.g., changing the loss function to include fairness constraints during training), or post-processing stage (e.g., changing the logic of a pre-trained model). However, when the decision logic of AI is encoded via deep neural networks, it becomes increasingly challenging to mitigate unfairness due to its black-box uninterpretable nature.\nSince the DNN-based software development is derived by principle of information bottleneck [45], which provides a trade-off between over-fitting and parsimonious signals; we posit that over-fitted DNN models may contribute and amplify historical biases from the training datasets. Therefore, prevalent techniques such as dropout methods [39, 40]-a process of randomly dropping neurons during training-might be effective in mitigating unfairness in the pre-trained DNN models. While the dropout strategy has been significantly used to prevent over-fitting during the training of DNNs, to the best of our knowledge, this is the first work to systematically leverage dropout methods for mitigating unfairness in pre-trained DNN models, as a post-processing mitigation method.\nHowever, finding the optimal subset of dropout neurons is a combinatorial problem that requires an exhaustive search in the subset of all combinations of neuron dropouts, and hence is intractable. To overcome the computational challenges, we explore the class of randomized algorithms with Markov chain Monte Carlo (MCMC) strategies to efficiently explore the search space with statistical guarantees. In doing so, we pose the following research questions:\nRQ1. How successful are randomized algorithms in fairness repairing of DNNs via dropouts?\nRQ2. Are there dropout strategies that improve model fairness and utility together?\nRQ3. What are the design considerations of search algorithms for efficient and effective dropouts?\nRQ4. How do dropout strategies compare to the state-of-the-art post-processing (bias) mitigators?\nTo answer these research questions, we present NEUFAIR (Neural Network Fairness Repair): a set of randomized search algorithms to improve the fairness of DNNs via inference-time neuron dropouts. We design and implement simulated annealing (SA) and random walk (RW) strategies that efficiently explore the state-space of neuron dropouts where we encode the frontiers of fairness and utility in a cost function.\nWe evaluate NEUFAIR over 7 deep neural network benchmarks as trained over 5 socially critical applications with significant fairness implications. We found that randomized strategies of neuron drops can successfully improve fairness (up to 69%) with a minimal utility degradation in the vast majority of cases. We also report a pathological case and reasons behind a failure of NEUFAIR. We also observe that NEUFAIR can improve fairness and utility together and provide intuitive explanations of such phenomena. Furthermore, we examine different hyperparameter configuration options of randomized algorithms. While some hyperparameters always influence fairness with positive or negative impacts; we detect a hyperparameter that defines a trade-off between explorations and exploitation that should be tuned as a constant variable in the cost function for each benchmark. Finally, we show the effectiveness of the SA algorithm, compared to the RW and the state-of-the-art post-processing (bias) mitigator [35]. In summary, the key contributions of this paper are:\n(1) Inference-Time dropout for fairness. To the best of our knowledge, we present the first dropout method of bias mitigation over pre-trained deep neural networks,\n(2) Randomized algorithms for fairness. We create a well-defined framework to formulate the combinatorial inference-time dropout problem in DNNs using randomized algorithms,\n(3) Experimental evaluations. We implement the randomized algorithms in NEUFAIR and evaluate its effectiveness and efficiency vis-a-vis the state-of-the-art techniques."}, {"title": "2 BACKGROUND", "content": "We provide a background on the model utility and fairness measures. Then, we briefly discuss various bias mitigation methods from the Al fairness literature."}, {"title": "2.1 Notions of Model Utility", "content": "Given a binary classifier h, a set of features X, and predictions h(X) = {TP, TN, FP, FN}, we can define the following notions of model utility. In h(X), TP, TN, FP, and FN denote the set of True Positives, True Negatives, False Positives, and False Negatives. We drop the cardinality operator || in the following definitions for brevity.\n$\\begin{aligned} &\\text { Accuracy }_h=\\frac{\\text { TP + TN }}{\\text { TP + TN+FP + FN }} \\\\ &F 1_h=\\frac{2 * \\text { Precision } * \\text { Recall }}{\\text { Precision + Recall }}=\\frac{2 * \\text { TP }}{2 * \\text { TP + FP + FN }} \\end{aligned}$\nAccuracy can be used to gauge the overall performance of a classifier. However, for imbalanced datasets used in fairness evaluations, accuracy is a poor metric as the number of negative samples far outweighs the positive samples. For example, predicting all samples as negative (0 true positives) in the Bank [13] dataset yields an accuracy of 88% but the F1 score would be 0/undefined. Since F1 is defined as the harmonic mean of precision and recall, rather than the arithmetic mean, it penalizes performance significantly if either precision or recall is low."}, {"title": "2.2 Notions of Model Fairness", "content": "Consider a machine learning classifier h, a set of features X, sensitive features $A \\subset X$, and a set of labels Y. We can then define the following notions of fairness.\nDefinition 2.1 (DEMOGRAPHIC PARITY [1]). The classifier h satisfies Demographic Parity under a distribution over (X, A, Y) if its prediction h(X) is statistically independent of the sensitive feature A i.e. P[h(X) = \u0177|A = a] = P[h(X) = \u0177] for all a, y. For binary classification with \u0177 = {0, 1}, this is equivalent to E[h(X)|A = a] = E[h(X)] for all a.\nDefinition 2.2 (EQUALIZED ODDS [1, 24]). The classifier h satisfies Equalized Odds under a distribution over (X, A, Y) if its prediction h(X) is conditionally independent of the sensitive feature A given the label Y i.e. P[h(X) = \u0177|A = a, Y = y] = P[h(X) = \u0177|Y = y] for all a, y, and \u0177. For binary classification with \u0177 = {0, 1}, this is equivalent to E[h(X)|A = a, Y = y] = E[h(X)|Y = y] for all a, y.\nEqual Opportunity is a relaxed variant of Equalized Odds with Y = 1 [24]. In other words, Equalized Odds require the true positive and false positive rates to be equal across all sensitive groups. In contrast, Equal Opportunity only requires the true positive rate to be equal across all sensitive groups.\nDemographic Parity is the weakest notion of fairness, and Equalized Odds is the strongest. In our work, we use Equalized Odds as the fairness criterion. The disparity or unfairness for Equalized Odds is the Equalized Odds Difference (EOD), defined as the maximum absolute difference between the true and false positive rates across the sensitive groups. Mathematically, this is represented as\n$\\text{EOD} = \\max\\begin{aligned} &\\left[\\left|\\mathbb{P}[h(X) = 1 \\mid A = 0, Y = 1] - \\mathbb{P}[h(X) = 1 \\mid A = 1, Y = 1]\\right|, \\right.\\\\ &\\left|\\mathbb{P}[h(X) = 1 \\mid A = 0, Y = 0] - \\mathbb{P}[h(X) = 1 \\mid A = 1, Y = 0]\\right|\\Bigg] \\end{aligned}$\nwith sensitive features A = {0, 1}."}, {"title": "3 PROBLEM STATEMENT", "content": "We consider pre-trained deep neural network (DNN) classifiers with the set of input variables X partitioned into a protected set of variables (such as race, sex, and age) and non-protected variables (such as profession, income, and education). We further assume the output is a binary classifier that gives either favorable or unfavorable outcomes."}, {"title": "3.1 Syntax and Semantics of DNN", "content": "A deep neural network (DNN) encodes a function $D: X \\rightarrow [0,1]^2$ where X consists of the set of protected attributes $X_1 \\times X_2 \\cdots \\times X_m$ and non-protected attributes $X_{m+1} \\cdots Y_{m+r}$. The DNN model is parameterized by the input dimension m+r, the output dimension 2, the depth of hidden layers n + 1, and the weights of its hidden layers $W_0, W_1, ..., W_n$. We describe the hidden layers with $M\\leftarrow [L_1, L_0, . . ., L_n, L_o]$, where $L_1$ and $L_o$ are the input and output layers, respectively, and $L_i, \\forall i \\in [0, n]$, is the hidden layers. We assume that there exists a subset of neurons $N\\in L_i, \\forall i \\in [0, n]$ in the hidden layers that disparately contribute to unfairness.\nLet $L_i$ be the output of layer i that implements an affine mapping from the output of previous layer $L_{i-1}$ and its weights $W_{i-1}$ for $1 \\leq i \\leq n$ followed by a fixed non-linear activation unit (e.g., ReLU defined as $L_{i-1} \\rightarrow \\max \\{W_{i-1}.L_{i-1}, 0\\}$) for $1 \\leq i \\leq n$. Let $L_{i}^{j}$ be the output of neuron j at layer i that is $L_{i}^{j}(x) = \\text{ReLU}\\left( \\sum_{l=1}^{|L_{i-1}|} W_{il} L_{i-1}^{l} \\right)$. The output is the likelihood of favorable and unfavorable outcomes. The predicted label is the index of the maximum likelihood, $D(x) = \\text{max}_i L_o(x)(i)$."}, {"title": "3.2 Inference Time Dropout for Fairness", "content": "Dropout [40] is a technique proposed to improve the generalization of neural networks by preventing overfitting. The key idea is that randomly dropping out neurons in a layer during training can help prevent overfitting of the neural network. Dropout randomly sets all $w_i$ to 0 for each neuron in the hidden layers with probability p during training. Once training is complete, dropout is not used, and all the neurons in the network are utilized to make predictions. While dropout has been traditionally used to prevent overfitting during training, we hypothesize that dropping neurons of the model during inference after training can significantly improve fairness with a minimal impact on model performance. However, unlike traditional dropout, where a set of neurons are randomly dropped during training, we aim to identify a subset of neurons at the Pareto optimal curve of fairness-performance during inference.\nWe consider a binary vector as the neuron state with $s = \\{0, 1\\}^N$, where $N = \\Sigma_{i=0}^{n} |L_i|$, $s_i$ indicates whether the neuron i is dropped or not and n is the number of layers. A pre-trained DNN model D does not include any dropouts, and hence all the indicators are 0.\nDefinition 3.1 (DESIRABLE DROPOUT OF FAIRNESS VS. UTILITY). Given a DNN model D trained over a dataset X; the search problem is to infer a repaired DNN model $D'$ by dropping a subset of neurons I in the binary neuron state, i.e., $s_i = 1$ for any $i \\in I$, such that (1) the model bias (e.g., EOD) is maximally reduced, (2) the model performance (e.g., F1-score) is minimally degraded, and (3) the model structure in terms of the numbers of inputs, outputs, and hidden layers remains the same as compared to the original pre-trained model D.\nA brute-force search would be exponential in the size of DNN as we have $2^{|N|}$ possible subsets. The running time becomes prohibitively expensive, even for small neural networks. We define desirable states as those states that have a good fairness-performance tradeoff. To find the desirable subset of neurons, we explore different types of randomized algorithms to improve fairness via model inference dropout."}, {"title": "4 APPROACH", "content": "Our approach comprises two randomized search algorithms, namely Simulated Annealing (SA) and Random Walk (RW)."}, {"title": "4.1 Simulated Annealing Search", "content": "We formulate the problem of finding the desirable subset of neurons as a discrete optimization problem and solve it using Simulated Annealing. Simulated Annealing (SA) [4] is a probabilistic algorithm that finds the global minima (w.r.t some cost function) in a large search space with high probability. Algorithm 1 presents a generic template to apply SA to a search space optimization problem. Figure 1 overviews the steps in our bias mitigation approach with the simulated annealing search. We now define the core concepts used in our SA algorithm.\nFor a DNN $M \\leftarrow [L_1, L_0, . . ., L_n, L_o]$ we define a state of our search space S as a binary sequence $s \\in \\{0, 1\\}^N$, where $N = \\Sigma_{i=0}^{n} |L_i|^1$. The ith element of s, denoted $s_i$ is 1 if neuron i is dropped and 0 otherwise$^2$. For example, consider a DNN $M \\leftarrow [L_I, L_0, L_1, L_O]$, where $|L_0| = 3$ and $|L_1| = 3$. Then, the state s given by:\n$\\mathbf{s = (0, 1, 0, 0, 0, 1)}$\n$L_o\\qquad L_1$\ndrops the second neuron in the first layer and the third neuron in the second layer.\nInstead of allowing our search space S to drop every possible subset of neurons possible (which would ensure S has size $2^N$), we restrict the size of S by fixing an upper and lower bound on the number of neurons that can be dropped from the DNN. Let $n_l$ and $n_u$ ($n_l \\leq n_u$) denote the minimum and maximum number of neurons allowed by our DNN, respectively. We can then formally define S as:\n$\\mathcal{S}:=\\{\\mathbf{s} \\in \\{0,1\\}^N \\mid n_l \\leq HW(\\mathbf{s}) \\leq n_u\\}$  (1)\nwhere HW(s) denotes the hamming weight of s. Restricting our search space with a conservative estimate of the lower and upper bound would have a minimal impact on our ability to find the desirable subset, as dropping too many neurons will reduce the model utility to less than acceptable levels, whereas dropping too few neurons will improve fairness by only a marginal amount. However, asymptotically, a conservative estimate of the lower and upper bound still generates prohibitively large search spaces. With the bounds $n_l$ and $n_u$, the cardinality of the search space $|\\mathcal{S}| = \\sum_{i=n_l}^{n_u} \\binom{N}{i} = \\Omega(\\binom{N}{VN} \\\\text{if } n_l < \\frac{N}{VN} < n_u)$, which rules out brute-force as a viable option.\nNeighborhood Relation and \u2018Generate\u2019 subroutine. The neighborhood of any state s \u2208 S, denoted \u0393(s), is defined as the set of all\nstates that are at a hamming distance of 1. Mathematically, this is defined as\n$\\Gamma(\\mathbf{s}) :=\\{\\mathbf{s}^{\\prime} \\in \\mathcal{S} \\mid HD(\\mathbf{s}, \\mathbf{s}^{\\prime})=1\\}$\nwhere HD(s, s') denotes the hamming distance between s and s'. With our definition of the search space and neighborhood of a state, the entire search space graph can be viewed as a subset of the N-dimensional hypercube, where the vertices of the hypercube represent the states of our search space, and the edges of the hypercube represent the neighborhood relation. The Generate subroutine on input s \u2208 S uniformly samples a neighbor s' from I(s) and returns s'. This is equivalent to uniformly sampling an index position i from [1, n] and subsequently uniformly flipping bit $s_i$ in the binary sequence s to get s'. We make the following observation on our underlying search space graph\u00b3:\nLemma 4.1. If $n_l < n_u$, then our search space graph is connected and has a diameter less than N. Moreover, the distance between any two states s, s' \u2208 S is given by HD(s, s').\nFor example, Figure 2 shows the search space graph for a neural network with 1 hidden layer with 3 neurons, along with the neighbors and state transitions.\nCost Function. Our primary goal is to find a state s \u2208 S which minimizes its unfairness score EODs. However, we do not wish to consider those states with a significant loss in model performance (measured using the F1 score of the model) compared to the original model; thus, we try to find an acceptable balance between the improvement in unfairness and the loss in model performance. To realize this balance, we penalize our cost function with an additional term to artificially increase the cost if the state has a lower F1 score. We now formally define our cost function, cost() as follows:\n$\\text{cost}(s):=\\text{EOD}_s + p\\cdot\\text{EOD}_{s_o}\\cdot\\mathbb{1}(F1_s< tF1_{s_o})\\qquad$(2)\nIn the above equation, so is the original state (the DNN with no dropout), $p \\in R^{\\geq 0}$ is called the penalty multiplier, t \u2208 (0, 1) is called the threshold multiplier, EODs and $EOD_{s_o}$ are the unfairness scores of states s and so respectively, $F1_s$ and $F1_{s_o}$ are the F1 scores of the s and so respectively, and $\\mathbb{1}(.)$ denotes the indicator function.\nThe threshold multiplier t determines the percentage loss in F1 score we will tolerate to improve fairness. The penalty multiplier p discourages states with an F1 score less than the threshold by penalizing them with a multiple of the unfairness of the initial state. Our cost function formulation allows us to find the state with the minimum unfairness while maintaining a significantly higher F1.\nInitial Temperature, Cooling Schedule and \u2018Update\u2019 subroutine. The temperature T of the SA procedure determines the probability with which we accept a positive transition (a transition where the cost is increased); the higher the temperature, the more likely the algorithm accepts such a transition. The cooling schedule refers to the function we use to update our temperature after each iteration. We adopt the logarithmic cooling schedule [23] which has proven convergence guarantees [34]. According to the logarithmic cooling schedule, the temperature T(i) at iteration m is defined as\n$\\text{T}_{m}=\\frac{T_{0}}{\\log(2 + m)}, \\quad \\forall m \\in \\mathbb{Z}^{\\geq 0}$  (3)\nThe Update subroutine updates the temperature using the above formula. Using Mitra et al. [34] convergence results for SA with a logarithmic cooling schedule, we get the following result for our SA runs:\nLemma 4.2. Let p > 1. If we set $T_0 \\geq (1 + pEOD_{so})^2\\frac{(n_u-n_l)}{4}$, then the probability that SA finds a global minima within $k > \\left[Muni\\right]$ iterations is greater than $1 - \\frac{A}{(k)^c}$ where $c:=\\min\\left(\\frac{1}{(n_u - n_l) \\sqrt{N[1] T_0 S^2}},\\frac{4}{(n_u - n_l)^2}\\right)$, where S is the size of the validation dataset and A > 0 is a constant.\nWhile the above lemma provides a worst-case bound on the number of iterations required to find an optimal solution, it is practically infeasible to achieve the large bound. Ben-Ameur [3] proposes an algorithm for computing To, with which they show SA performs well experimentally with fewer iterations than estimated by [34]. We use the temperature initialization algorithm from [3] to estimate the initial temperature To. At a high level, this algorithm backcomputes an initial value for the temperature for which the expected value of acceptance probabilities for positive transitions from a random initial distribution is greater than some predefined threshold$^4$.\nStopping Criterion. We use time as a stopping criterion for our SA runs i.e., we run the SA search for time less than some hyperparameter time_limit."}, {"title": "4.2 Random Walk Search", "content": "The Random Walk (RW) strategy samples a random state s \u2208 S, and recursively samples states from the neighborhood () to explore the search space. The RW strategy then records the best state discovered throughout the walk. We use the same cost function as SA in RW to determine the desirable state. A key difference between RW and SA search is that in RW, we always transition to a new state, regardless of cost. In contrast, in SA, we always transition to a new state with a lower cost and transition with some probability p\u2208 [0, 1) if it has a higher cost. This is highlighted in Line 14 in Algorithm 2. A RW can also be considered an SA run with infinite temperature, i.e., the transition probability is always 1."}, {"title": "5 EXPERIMENTS", "content": "We pose the following research questions:\nRQ1 How successful are randomized algorithms in fairness repairing of DNNs via dropouts?\nRQ2 Are there dropout strategies that improve fairness and utility together?\nRQ3 What are the design considerations of search algorithms for efficient and effective fairness repair of DNNs via dropout?\nRQ4 How do dropout strategies compare to the state-of-the-art post-processing (bias) mitigators?"}, {"title": "5.1 Datasets and Models", "content": "We evaluate NEUFAIR with five different datasets from fairness literature. For two of the datasets, we consider two different protected groups, which effectively results in a total of 7 different benchmarks. The Adult Census Income [14], Bank Marketing [13], Compas Software [38], Default Credit [15], and Medical Expenditure (MEPS16) [17] are binary classification tasks to predict whether an individual has income over 50K, is likely to subscribe, has a low reoffending risk, is likely to default on the credit card payment, and is likely to utilize medical benefits, respectively.\nWe used DNNs as the machine learning model, trained over the dataset benchmarks. We use ReLU as the activation function after the linear layers. For the Bank, Default, and Compas datasets we use a dropout of 0.2 during training. For the Adult and MEPS16 datasets, we set the dropout to 0.1. For the Compas dataset, we use Adam as the optimizer with a learning rate of 0.001. For the other datasets, we use SGD with a learning rate of 0.01. For data preprocessing, we utilize standard techniques such as one-hot encoding for categorical features followed by min-max or standard scaling for numerical features. For the Compas dataset, we use a version of the dataset that has 12 features after feature selection that was used in [46]. The modified Compas dataset is available online5."}, {"title": "5.2 Technical Details", "content": "We run all our experiments on a desktop running Ubuntu 22.04.3 LTS with an Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz processor, 32GB RAM, and a 1 TB HDD. The neural networks and repair algorithms were implemented using python3.8, torch==2.0.1, numpy==1.24.3, and scikit-learn==1.3.1."}, {"title": "5.3 Experimental Setup", "content": "For each dataset, we evaluate the performance of the NEUFAIR algorithms using 10 different random seeds. The seeds are set for torch, numpy, and scikit-learn libraries before DNN training. Specifically, the seed determines the randomness of the training, validation, and test splits and the randomness of model training, such as sampling batches and initializing the weights of the DNN. The NEUFAIR algorithms themselves are not seeded. Each SA and RW run is unique.\nDuring training, the training dataset was used for gradient descent, and the validation dataset was used to monitor the F1 score and determine the hyperparameters. We use the model from the epoch with the highest validation dataset. The same validation dataset is used by NEUFAIR to identify the desirable set of neurons to drop. Finally, we evaluate the fairness of the original and fixed (repaired) models using the test dataset.\nUnless otherwise specified, all runs reported in the paper have a time-out limit of 1 hour with the following cost function:\nC(s) = EODs + 3.0 \u00b7 EODso \u00b7 1(F1s < (0.98 \u00b7 F1so)).\nThe F1 threshold is 98% of the validation F1 score of the unfair neural network, i.e., we tolerate a 2% degradation in the F1 score to improve fairness. A penalty of 3\u00d7 the baseline unfairness is added to the fairness of the current state if its F1 score is less than the threshold. For all datasets, we set the minimum number of neurons to $n_l$ = 2 and vary the maximum number of neurons $n_u$ between 20% - 40% of the number of hidden layer neurons depending on the overall size of the network. The $n_u$ values are 50, 24, 24, 20, and 135 for Adult, Compas, Bank, Default, and MEPS16 respectively. For all datasets, we use a train/validation/test split of 60%/20%/20%."}, {"title": "5.3.1 Effectiveness of randomized algorithms on mitigating unfair-ness using dropout (RQ1)", "content": "The first research question is answered by analyzing the data in Figure 3, Figure 4, Table 3, and Table 4. Figure 3 and Figure 4 highlight the cost of the best state found for the SA and RW over time, respectively, for 10 different seeds per data subject using the logarithmic scale. In all cases, we see a positive trend where the cost improves over time, and the search identifies better subsets of neurons to drop. We note the seemingly flat plots correspond to those cases where a good state was found in the beginning. The exception is the black line in the sub-plot for the Default dataset in both figures as it corresponds to a failed run; i.e., the best state found has an F1 score lesser than 0.98\u00d7 the F1 score of the initial state. Out of 70 runs (10 per benchmark), the SA and RW found a desirable state with 0.98\u00d7 the initial F1 score in 69 cases. In addition, we re-run the experiment with an F1 threshold multiplier of 0.96 and identify a desirable state with a test set fairness of 6.26%, an improvement from 13.2%. The progress graphs of the RW strategy are \"steep\" initially and rapidly improve as the initial states are unfair and also heavily penalized by the penalty multiplier as RW generates intermediate states with an F1 score less than the threshold. However, due to the cost function"}, {"title": "Pathological Case", "content": "Table 5 highlights the EOD scores for COMPAS (Sex) with the original and repaired networks. Here, we observe that SA improves the EOD in 4 cases, does not improve in 1 case, and has a worse EOD in 5 cases. RW improves the EOD in 5 cases and has a worse EOD in the other 5 cases. There are two reasons for the different trends in COMPAS (Sex). First, there is a discrepancy in the original model statistics between the validation and test splits. The difference arises because the DNN tends to overfit slightly on the training/validation sets. This is evident from model metrics in Table 3. Second, the COMPAS datasets by default are quite fair (according to EOD) compared to other datasets as their unfairness is around 2%. During the search, the SA algorithm correctly identifies a desirable state according to the validation dataset and performs better than RW. However, given the low initial unfairness and the model's tendency to perform better on the validation/training sets, we observe minor discrepancies in the test set. The average unfairness, however, increases by 0.399% (2.522% to 2.921%), which is negligible. For both NEUFAIR algorithms, the validation fairness improves in the COMPAS (Sex) experiment.\nAnswer RQ1: The randomized algorithms effectively mitigate unfairness via dropouts by de-activating a desirable subset of neurons. On the test set, Fairness improves by up to 69%. The SA algorithm performs better than the RW algorithm."}, {"title": "5.3.2 Dropout strategies improving both fairness and utility (RQ2)", "content": "The second research question is answered by observing the F1 score and accuracy (model utility metrics), besides the EOD score (fairness) in Table 3 vs. Table 4. For all datasets, the F1 score decreases for the validation and test sets. The decrease in F1 score is accompanied by an improvement in fairness which highlights the tradeoff between the model utility and fairness. However, we observe that the accuracy increases for the Bank, Default, and MEPS16 datasets. The biggest improvement in accuracy is observed in MEPS16, where the Validation and Test accuracy increase from 0.79 and 0.788 to 0.86 and 0.853 for SA and to 0.856 to 0.851 for RW. For Default, the validation and test accuracies increase from 0.774 and 0.769 to 0.794 and 0.79 for both SA and RW. For Bank, the validation and test accuracies increase from 0.842 and 0.84 to 0.892 and 0.88 for SA and 0.882 and 0.881 for RW.\nThe opposing trends of F1 score and accuracy can be attributed to the increase in negative predictions (0 is negative class and 1 is positive) as more and more neurons are dropped out from the DNN. The increase in negative predictions favors the true negatives as the datasets are higly imbalanced with more negative samples. The Default, Bank, and MEPS16 datasets have 78%, 88%, and 83% of the data belonging to the negative class. The trained DNN models were optimized for F1 score on the validation dataset to improve performance on the underrepresented positive class. The accuracy increases as the increase in true negatives outweighs the drop in true positives. The F1 score, however, always decreases as we lose precision and recall when the true positives decrease and false negatives increase.\nAnswer RQ2: The overall accuracy, as a model utility metric, may improve along with fairness using dropout strategies, as accuracy does not account for false positives and false negatives in imbalanced datasets. However, the F1 score model utility metric decreases as fairness increases."}, {"title": "5.3.3 Hyperparameters of randomized algorithms and fairness (RQ3)", "content": "The NEUFAIR algorithms have 4 hyperparameters: F1 threshold multiplier, the minimum and maximum number of neurons to drop, time-out limit, and F1 penalty multiplier.\n\u2022 The F1 threshold multiplier is inversely proportional to fairness improvement. For example, as highlighted in RQ1, the fairness of the Default experiment improves when the F1 threshold is reduced from 0.98 to 0.96.\n\u2022 Decreasing the minimum number of neurons and increasing the maximum number of neurons can have a positive effect on fairness, provided the time-out limit increases. Consider two ranges [$n_{l1}$, $n_{u1}$", "n_{u2}$": "such that $n_{l1}$ < $n_{l2}$ < $n_{u2}$ < $n_{u1}$. The search space of"}]}]}