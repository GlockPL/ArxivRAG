{"title": "Effective Monitoring of Online Decision-Making Algorithms in Digital Intervention Implementation", "authors": ["Anna L. Trella", "Susobhan Ghosh", "Erin E. Bonar", "Lara Coughlin", "Finale Doshi-Velez", "Yongyi Guo", "Pei-Yao Hung", "Inbal Nahum-Shani", "Vivek Shetty", "Maureen Walton", "Iris Yan", "Kelly W. Zhang", "Susan A. Murphy"], "abstract": "Objective: Online AI decision-making algorithms are increasingly used by digital interventions to dynamically personalize treatment to individuals. These algorithms determine, in real-time, the delivery of treatment based on accruing data. The objective of this paper is to provide guidelines for enabling effective monitoring of online decision-making algorithms with the goal of (1) safeguarding individuals and (2) ensuring data quality.\nMaterials and Methods: We elucidate guidelines and discuss our experience in monitoring online decision-making algorithms in two digital intervention clinical trials (Oralytics and MiWaves). Our guidelines include (1) developing fallback methods, pre-specified procedures executed when an issue occurs, and (2) identifying potential issues categorizing them by severity (red, yellow, and green).\nResults: Across both trials, the monitoring systems detected real-time issues such as out-of-memory issues, database timeout, and failed communication with an external source. Fallback methods prevented participants from not receiving any treatment during the trial and also prevented the use of incorrect data in statistical analyses.\nDiscussion: These trials provide case studies for how health scientists can build monitoring systems for their digital intervention. Without these algorithm monitoring systems, critical issues would have gone undetected and unresolved. Instead, these monitoring systems safeguarded participants and ensured the quality of the resulting data for updating the intervention and facilitating scientific discovery.\nConclusion: These monitoring guidelines and findings give digital intervention teams the confidence to include online decision-making algorithms in digital interventions.", "sections": [{"title": "1 Background and Significance", "content": "There is increasing interest in digital interventions that include online decision-making algorithms to personalize sequences of treatments to individuals. In digital interventions, treatments involve engagement prompts containing motivational or informational messages to engage individuals in healthy behaviors. Here personalized treatment means delivering the most beneficial treatment to an individual given their current context. Example contexts include the individual's current location, mood, and stress level. Personalization is intended to enhance the overall effectiveness of the digital intervention (1; 2; 3). \u03a4o enhance effectiveness, online, real-time decision-making algorithms are included as an intervention component in the digital intervention (4; 5; 6; 7; 8; 9; 10). These algorithms assign treatment by identifying patterns in past data to continuously learn and update which treatments are most effective in which contexts.\nBefore deploying the digital intervention, monitoring systems must be in place to ensure safety, data quality, and implementation fidelity. These systems are well-established in the field of clinical trials. Clinical trial monitoring traditionally involves monitoring by staff and can involve automated monitoring by online algorithms. Examples of monitoring systems include Data and Safety Monitoring Boards (DSMB) to oversee participant safety and data quality (11), Source Data Verification (12) and statistical monitoring (13) to ensure data quality in clinical trials, and risk-based monitoring (14) tools to ensure participant safety. Frameworks like FRAME (15), RE-AIM (16), PRISM (17) and RAPICE (18) provide structured approaches for monitoring implementation fidelity (19; 20), the degree at which an intervention is delivered as intended. These approaches ensure that any modifications or exceptions, along with their impact, are documented: playing a crucial role in accurately interpreting findings regarding the intervention's effectiveness.\nWhile many of these guidelines are useful for monitoring digital interventions, additional monitoring is required when the digital intervention includes an online decision-making algorithm. This additional monitoring is required because the online decision-making algorithm adjusts future treatments based on an individual's past treatment responses. If the decision-making algorithm functions incorrectly, it detracts from the intended implementation of the digital intervention, and could compromise individuals' experience and the quality of data for use in statistical analyses (21). Minimally, such failures can jeopardize the future development and use of these promising algorithms.\nThis paper provides (1) guidelines for monitoring online decision-making algorithms and (2) discusses two case studies in which monitoring systems were deployed. These case studies offer in-depth examples and findings gained from implementing real-time monitoring systems built for digital interventions."}, {"title": "2 Case Studies", "content": "To illustrate the guidelines, we draw upon insights from two digital intervention clinical trials that both employed online decision-making algorithms to personalize treatment assignment. Case study 1 describes a clinical trial of the Oralytics intervention (22; 1; 23; 24; 25) which focuses on improving toothbrushing behaviors for participants at risk for dental disease. Case study 2 describes a clinical trial of the MiWaves intervention (26; 27) that aims to reduce cannabis use amongst emerging adults (ages 18-25). In both of these digital interventions, the treatment involves the delivery of prompts to support engagement in healthy behaviors.\nThe Oralytics and MiWaves interventions each include an online decision-making algorithm to govern the delivery of prompts or treatments. Both decision-making algorithms use reinforcement learning (RL) (28), a branch of machine learning focused on optimizing sequences of decisions to promote some outcome. In Oralytics and MiWaves, the RL algorithm utilizes a model of the proximal outcome or reward (e.g., brushing quality, an individual's app engagement) that predicts how the reward will change based on the individual's current context (e.g., time of day, recent brushing quality, recent cannabis use) and the selected treatment or action (e.g., delivering a prompt vs not delivering a prompt). At pre-specified update times (e.g. nightly, weekly), the RL algorithm updates model parameters and measures of confidence in the estimates of the parameters, using prior interaction history (i.e., participants' context over time, the sequence of assigned treatments, and the observed rewards following each treatment assignment). Using the updated model, the RL algorithm forms a policy to assign treatment. The RL algorithm uses this policy at decision points (e.g., one hour before an individual brushes their teeth) to assign treatment according to the participant's current context. In this paper, we refer to the online decision-making algorithm as the RL algorithm."}, {"title": "2.1 General Software Framework", "content": "Multiple components form the software system used in both Oralytics and MiWaves (Figure 1). The back-"}, {"title": "2.2 Case Study 1: Oralytics", "content": "The Oralytics digital intervention is designed to improve the proximal outcome of oral self-care behaviors (OSCB) for individuals at risk of dental disease. This intervention uses a commercially-available electric toothbrush with Bluetooth connectivity and integrated sensors as well as the Oralytics mobile app. The intervention facilitates high-quality OSCB through self-monitoring and prompts delivered via a push notification from the Oralytics app. These prompts contain content from the field of health psychology known to influence engagement in behavior change (reciprocity, reciprocity by proxy and curiosity) (24).\nOralytics uses an RL algorithm (25; 23) to govern whether or not to deliver a prompt at two decision points per day, one hour prior to an individual's usual morning and evening brushing windows. The proximal outcome for Oralytics is OSCB measured in seconds. For the individual's context, the RL algorithm uses whether it was morning or evening, recent past brushing, recent number of prompts delivered, and recent app engagement (see Appendix B.2 for exact definition of context). The RL algorithm updates its policy on a weekly cadence. See Appendix B for more information on the Oralytics RL algorithm.\nThe Oralytics RL algorithm uses app analytics and brushing data. However, for the Oralytics trial, both types of data came from external systems (i.e., Oralytics app and proprietary toothbrush sensor data provider) that were developed and maintained by a separate third-party. Thus, the RL algorithm lacked control over much of the data processing and storage. We discuss later (Section 4.1) the impact of this constraint in the development of the monitoring system.\nThe Oralytics clinical trial was a one-arm trial where all participants were provided the Oralytics digital intervention. The trial ran from September 2023 to July 2024, with N = 79 participants. Each participant was in the trial for 70 days. Please refer to the trial design (22; 24) for a detailed description of the Oralytics clinical trial."}, {"title": "2.3 Case Study 2: MiWaves", "content": "The MiWaves digital intervention is designed to help reduce cannabis use among emerging adults (EAs). The MiWaves intervention app includes support for self-monitoring and prompts containing behavior change strategies. The behavior change strategies include strategies to reduce negative and enhance positive affect, increase cannabis-free activities, and enhance future/goal-directed thinking. (29)\nMiWaves uses an RL algorithm (27) to personalize, twice daily, whether or not to deliver the prompt containing a behavior change strategy. The proximal outcome is engagement with the app and the suggested behavior change strategies. The RL algorithm utilizes an individual's time-of-day, recently reported cannabis use behavior and recent app and intervention engagement as context (see Appendix C.1 for exact definition of context). The RL algorithm autonomously updates its policy on a daily cadence. See Appendix C for more information on the MiWaves RL algorithm.\nThe MiWaves RL algorithm utilizes data coming directly from the self-monitoring and mobile app usage of a participating individual, and not through an external system. Therefore, the RL algorithm has direct access and control over the data processing and storage.\nThe MiWaves clinical trial was a one-arm trial in which all participants were provided the MiWaves digital intervention. The trial ran from March 2024 to May 2024, with N = 122 EAs. Each participant was in the trial for 30 days. Please refer to the MiWaves trial design (26) for more information."}, {"title": "3 Monitoring Online Decision-Making Algorithms", "content": ""}, {"title": "3.1 Why Monitor?", "content": "Online RL algorithms generally include two processes, (1) learning: modeling how individuals' outcomes vary with treatment and updating the model's parameters (repeatedly estimated) as data accrues, and (2) treatment assignment: at each decision point, the algorithm uses the most recent model and the individual's current context to assign treatment. To ensure these two processes work correctly, it's essential to accurately collect and transmit current context and outcome data to the RL algorithm to personalize treatments. If this data is inaccurate or missing, these two processes might cause issues that could compromise individuals' experience and the utility of data for subsequent statistical analyses. For example, communication issues between the RL algorithm and the backend controller can lead to individuals receiving too many or too few treatments, or treatments at the incorrect times. Or the software system may fail to save data needed to conduct high-quality statistical analysis.\nAutonomously monitoring the RL algorithm is important for detecting, alerting, and preventing issues such as those presented above. A monitoring system prevents such issues and detects issues as soon as they arise. This allows digital intervention teams to identify, triage, and resolve issues which (1) minimizes negative impacts on individuals and (2) reduces the burden on the digital intervention staff who would otherwise need to manually monitor the algorithm in real time."}, {"title": "3.2 Elements of the Algorithm Monitoring System", "content": "We offer guidelines for building a real-time monitoring system for an RL algorithm (Table 1). These guidelines consists of: (1) issues and severity and (2) fallback methods. Both categories of guidelines work with each other to avoid issues that otherwise would have occurred. In the Section 4, we provide concrete examples from Oralytics and MiWaves.\nIssues and Severity Development teams should begin by identifying and assigning severity to potential issues that could occur. This approach allows development teams to prioritize addressing issues based on their severity. Once these issues are identified, an automated notification system (e.g., automated email) can be utilized to alert the development teams when an issue occurs. We recommend categorizing issues into three severity levels:\n\u2022 Red Severity (Safeguarding Individuals and Utility of Data) Red severity issues compromise individuals' experiences or the scientific utility of the data, requiring immediate attention. These issues trigger an automated email to the algorithm development team and are prominently displayed on the monitoring dashboard for the digital intervention staff. Individuals' experiences can be compromised if they receive an unreasonable amount of prompts. This could result from communication errors between software components, invalid treatment assignment by the RL algorithm, or in the case of a clinical trial, failing to recognize that a participant has ended the trial. The scientific utility of the data could be compromised if the database fails to store necessary values because it timed-out, closed connections, or exceeded its storage capacity.\n\u2022 Yellow Severity (Compromises Algorithm Functionality) Yellow severity issues compromise the ability of the online RL algorithm to learn and personalize treatment. These issues also immediately trigger an automated email and appear on the monitoring dashboard, but the urgency to resolve them is lower, allowing the RL development team more time to address them. The RL algorithm's ability to learn and personalize can be compromised in multiple ways, including (1) in the case of treatment assignment, not obtaining the necessary context data in a timely manner, and (2) in the case of learning, failing to acquire necessary context and outcome data from the algorithm's internal database.\n\u2022 Green Severity (Impacts Resulting Statistical Analyses) Green severity issues are problems that need to be documented to properly adjust statistical analyses of the resulting data. Not documenting these issues compromises the validity of the statistical analyses and/or hinders improving the digital intervention. While a majority of green issues involve properly documenting red and yellow issues, one could also document issues involving components outside of the RL algorithm (e.g., mobile app or backend controller), maintain a list of items to investigate, and check the consistency of data saved by all software components. Since managing these issues involve documentation by other components, we recommend establishing agreements between all software development teams prior to the deployment of the intervention. These agreements clarify responsibilities for documenting specific green issues that may arise.\nFallback Methods Fallback methods are pre-specified procedures executed when an issue occurs, serving as a backup to treatment assignment and learning procedures. These methods ensure that even when red or yellow issues occur, the system defaults to baseline functionality agreed upon before deployment. Specifically, when treatment assignment by the RL algorithm fails, individuals should still be assigned a treatment randomized with a pre-specified probability deemed reasonable in the individual's cur-"}, {"title": "4 Implementation of Algorithm Monitoring", "content": ""}, {"title": "4.1 Setting and Constraints", "content": "Constraints can impact the structure of the monitoring system. For example, the constraints in Oralytics were: (1) a lack of control over data processing and storage due to dependence on an external system and (2) a short time frame for developing the monitoring system. In contrast, MiWaves had greater control over data processing and storage and fewer time constraints. These differing settings and constraints led to the development of different monitoring systems for Oralytics and MiWaves.\nDue to different time constraints, the two monitoring systems made different trade-offs between coverage of issues and efficiency in implementation. Since Oralytics had limited time to develop the monitoring system, the system specified broad categories of issues (Table 2). MiWaves had more time to develop the monitoring system and specified more detailed issues (Table 3). For example, while Oralytics had one broad check for failed communication with the backend controller, MiWaves had granular checks such as checking if requests from the backend controller had the correct authorization.\nSince the Oralytics system only had access to sensor data (e.g., brushing data from the electronic toothbrush) from an external system after this data was pre-processed and stored in a data base maintained by the backend controller, the Oralytics monitoring system mostly focused on monitoring the validity of this data. In contrast, MiWaves only had to monitor communication issues with the backend controller because MiWaves had direct access to participant data (i.e., data was stored and processed on the MiWaves software system and not in an external system). The differing circumstances between Oralytics and MiWaves show how different constraints influence the design of monitoring systems in real-world digital intervention applications."}, {"title": "4.2 Fallback Methods", "content": "To clarify the Oralytics fallback methods presented later, we first discuss the backup treatment schedule. A critical Oralytics design decision was to construct a backup treatment schedule for 70 days every morning starting with the current decision point. This means treatment was selected not only for that day's decision points but also for decision points for the subsequent 69 days. See Appendix B.4 for full details on the backup treatment schedule. MiWaves did not employ a backup treatment schedule.\nOralytics employed a two-layer fallback method for treatment assignment and a separate method for updating: (i) if there's any communication issue between the backend controller and the app, treatment from the most recent backup treatment schedule saved on the Oralytics app is delivered; and (ii) if there are any issues in constructing the backup treatment schedule, then treatment for each decision point in the schedule is assigned with 0.5 probability rather than using the current policy. (iii) for updating, if issues arise (e.g., malformed data or recent data is not available), then the algorithm stores the data point, but does not update parameters with that data point.\nWe now describe the fallback methods for MiWaves. If the RL algorithm could not assign treatment, the MiWaves backend controller assigned treatment to participants instead (i.e. prompts were sent with 0.5 probability). If the RL algorithm could not update its policy, the algorithm continued using the last policy to assign treatments. This ensured that even when the algorithm update failed, the algorithm did not use malformed policies to assign treatments.\nIn the Oralytics clinical trial, all three fallback methods were triggered in response to yellow severity issues such as failing to obtain necessary data or the RL algorithm crashing. In MiWaves, fallback methods were triggered in response to three yellow severity issues when the RL algorithm was unresponsive or had crashed. Fallback methods triggered in both trials highlight the importance of planning ahead for issues so that the digital health intervention system functions appropriately."}, {"title": "4.3 Issues and Severity", "content": "In this section, we highlight key examples and insights of issues that occurred for each severity level. For a comprehensive list of potential issues that were checked from each case study, please refer to Tables 2"}, {"title": "4.3.1 Red Severity", "content": "Both Oralytics and MiWaves checked if (i) the RL algorithm assigned an unreasonable number of engagement prompts (less than the minimum amount or more than the maximum amount) and (ii) treatment assignment probabilities returned by the RL algorithm for any participant at any given decision point stayed within a bounded limit (see Appendix B.3). Due to time constraints on software development, Oralytics trial staff manually checked for red severity issues, whereas in the MiWaves trial, this check was automated in the software and an automated email would be sent to staff and development teams if such issues were detected. The automation in MiWaves allowed for more comprehensive checks. For example, MiWaves included checks for an unreasonable number of assigned prompts (i) at any decision point as well as for individual participants over seven days. Although these types of checks were implemented, this specific category of issues was never detected in either Oralytics nor MiWaves.\nDuring the MiWaves trial, development teams categorized an additional set of red severity issue to detect if the cloud computer had low available memory. This precaution was taken after two incidents occurred during the trial where the available memory was too low, causing the backend controller to mal-function, and RL algorithm along with its database to crash. Low available memory on the cloud computer is a red severity issue because it can cause the backend controller to crash, preventing interventions from being sent to participants. In order to preemptively mitigate the impact of this issue, the cloud computer's memory was increased, and automated email checks were designed to notify the team when the cloud computer exceeded a specified memory usage threshold. After implementation, these checks were triggered once subsequently in the MiWaves trial, which prompted the development teams to increase the available memory once again."}, {"title": "4.3.2 Yellow Severity", "content": "Oralytics implemented checks such as ensuring requests for sensor data (originally from external source but processed and made available to the RL algorithm by the backend controller) were successful, verifying that the sensor data was correctly formatted to be processed, and ensuring that the RL algorithm can properly read and write data to its own database.\nOne example of a yellow issue encountered during the Oralytics trial is that on two occasions, the RL algorithm crashed and was unavailable. However, because of fallback method (i), participants were assigned treatment from the most recent backup treatment schedule saved on their Oralytics app, but no new schedules were formed for that day. The RL development team restarted the system and the RL algorithm became available again the next day. For additional yellow issues that occurred during the trial, see Appendix A.1.\nIn MiWaves, the majority of the yellow severity issues were aimed at verifying the functionality of the RL algorithm and it's components, and checking for any associated issues. For example, an yellow severity issue was encountered at the start of the MiWaves clinical trial. On the first day of the MiWaves clinical trial, certain participants were removed after the digital intervention team discovered that they were fraudulent (see Appendix H). However, this information was not communicated to the RL algorithm. As a result, there was a discrepancy between the actual number of active participants in the trial and the number perceived to be active by the RL algorithm. This mismatch caused the RL algorithm to incorrectly assign treatments to the participants (participant A was incorrectly assigned treatment for participant B). Within one day, the development teams addressed the issue by implementing a fix (in both the backend controller and the RL algorithm) and restarting the RL algorithm to prevent similar scenarios in the future. The fix involved adding a check to communicate about active participants, and henceforth the RL algorithm maintaining the correct set of active participants in the trial."}, {"title": "4.3.3 Green Severity", "content": "Green severity issues need to be documented to adjust for statistical analyses. In both Oralytics and MiWaves, a majority of green severity incidents involved documenting red and yellow issues and information on crashes, resets, or reboots of the RL algorithm. However, information about issues with other system components (e.g., backend controller) outside of the RL algorithm were also documented. The documentation details the timestamps, the number of decision points and participants impacted, what fallback method was used for yellow issues, and how the development team fixed the issue.\nDuring the Oralytics trial, intervention staff noticed that only a few participants had prompts scheduled on their apps. Upon investigation, the RL algorithm was functioning correctly, but the backend controller failed to wait long enough to gather the backup treatment schedules for all participants, resulting in"}, {"title": "5 Discussion", "content": "Algorithm monitoring systems are essential to support the use of online decision-making algorithms in digital interventions. We provided guidelines for monitoring an online decision-making algorithm and shared detailed findings from two case studies where online RL algorithms dynamically adjusted treatment based on incoming data. As demonstrated in Section 4, the absence of a monitoring system could have resulted in critical issues, such as individuals not receiving any prompts or the use of incorrect data in statistical analyses. Although our paper focused on decision-making algorithms, we expect many findings are applicable to a variety of online learning algorithms, such as those used for fine-tuning prediction algorithms (e.g., predicting stress) in digital interventions. We hope these monitoring guidelines empower digital intervention teams to confidently incorporate the innovative benefits that online decision-making algorithms offer in digital intervention trials."}, {"title": "A Additional Yellow Severity Issues", "content": "Here we list and describe all the yellow severity issues that occured during each respective trial."}, {"title": "A.1 Oralytics", "content": "There were three types of yellow severity issues encountered during the Oralytics trial:\n1. The RL algorithm was unable to obtain current context (i.e., app analytics data) from the backend controller which affected the data used to update parameters. This happened on two separate occasions for two different participants: (a) the first occasion was because too many requests were made as more participants had joined the trial and the backend controller communicating between the external system and the RL algorithm to provide this data started ignoring the RL algorithm's requests after a certain threshold was exceeded and (b) the second occasion was because the raw participant's data given by the backend controller was malformed and could not be processed. In both cases, fallback method (iii) was executed and the RL algorithm saved the affected data points from those days for the participants affected, but did not add those data points to the batch data used for updating parameters. To solve (a), the RL development team worked with the backend controller team to create a more efficient request strategy that would not exceed the request threshold imposed by the external source and to solve (b), the RL development team worked with the backend controller team to ensure the raw participant's data was no longer malformed.\n2. On two occasions, the RL algorithm crashed and was unavailable. However, because of fallback method (i), participants were assigned treatments from the most recent backup treatment schedule saved on their Oralytics app, however, no new schedules were formed for that day. The RL development team restarted the system and the RL algorithm became available again the next day.\n3. During treatment assignment, the RL algorithm lost connection to its internal database and was unable to obtain context data from its internal database on three separate occasions. At each occasion, fallback method (ii) was executed and the RL algorithm provided non-personalized schedules for all affected participants. After being notified of each occasion, the RL development team restarted the RL algorithm's internal database and monitored the next week. The exact cause of this issue is still unknown and the RL team is currently investigating to prevent this issue in the next implementation."}, {"title": "A.2 MiWaves", "content": "In MiWaves, the majority of the yellow severity issues were aimed at verifying the functionality of the RL algorithm and it's components, and checking for any associated issues. Three broad instances of yellow severity issues encountered during the MiWaves clinical trial are specified below:\n1. The RL algorithm utilized consistent and accurate participant data for algorithm update. However, the RL algorithm encountered a problem prior to an update where it received duplicate or empty responses for participant data from the backend controller after the end of a given decision time. The development teams discovered that when a participant traveled to a different time zone and missed their self-monitoring period, the backend controller would skip the scheduled tasks for that decision time. As a result, no data was available for the RL algorithm. Since this issue occurred infrequently (in less than 3% of decision points), the teams decided not to fix it immediately during the trial. However, the RL algorithm excluded the data associated with these decision points to update the algorithm (this was the fallback method).\n2. If the RL algorithm received an empty response concerning a particular participant from the backend controller (as seen in scenario 1), the RL algorithm could not update the current time of day for that participant. This issue also affected all subsequent time-of-day values (necessary for the RL algorithm's context) for that participant. The app and RL development team quickly implemented a solution within two business days to fix the issue. They added a label to specify the time of day when the backend controller communicated with the RL algorithm, which the RL algorithm would then use to correctly determine the time for each participant in the trial."}, {"title": "B Oralytics RL Algorithm", "content": "For completeness and to clarify ideas in the main paper, we offer an overview of key components of the Oralytics RL algorithm. See the Oralytics RL design manuscripts (25; 24) for full details on the Oralytics RL algorithm."}, {"title": "B.1 Definitions", "content": "We use $i \\in [1 : N]$ to denote participants and $t \\in [1 : T]$ to denote decision points. $S_{i,t} \\in \\mathbb{R}^d$ denotes the vector of covariate features of dimension $d$ (num. of features) used for participant i's current context at decision point t. $A_{i,t}$ denotes the treatment assigned to participant i's current context at decision point t. Recall that in our case, assigning treatment means selecting between sending an engagement prompt or not. $Q_{i,t}$ denotes the proximal health outcome of oral self-care behaviors (OSCB) measured in seconds, observed after treatment $A_{i,t}$ is executed."}, {"title": "B.2 Reinforcement Learning Framework", "content": "\u2022 Context: Let $S_{i,t} \\in \\mathbb{R}^d$ represent the vector of covariate features of dimension $d = 5$ (num. of features) used as participant i's current context at decision point t by the RL algorithm:\n1. Time of Day (Morning/Evening) $ \\in \\{0,1\\}$\n2. B: Exponential Average of Brushing Quality Over Past 7 Days (Normalized) $ \\in [-1,1]$\n3. A: Exponential Average of Prompts Sent Over Past 7 Days (Normalized) $ \\in [-1,1]$\n4. Prior Day App Engagement (Opened App / Not Opened App) $ \\in \\{0,1\\}$\n5. Intercept Term = 1\nFeature 1 is 0 for morning and 1 for evening. Features 2 and 3 are $B_{i,t} = c_{\\gamma}\\sum_{j=1}^{14} \\gamma^{j-1} Q_{i,t-j}$ and $A_{i,t} = c_{\\gamma}\\sum_{j=1}^{14} \\gamma^{j-1} A_{i,t-j}$ respectively, where $\\gamma = 13/14$ and $c_{\\gamma} = \\frac{1}{1 - \\gamma}$. Recall that $Q_{i,t}$ is the proximal outcome of OSCB and $A_{i,t}$ is the treatment indicator. Feature 4 is 1 if the participant has opened the app in focus (i.e., not in the background) the prior day and O otherwise. Feature 5 is the intercept which is always 1. Please refer to Section 2.7 in (25) for full details on the design of the context.\n\u2022 Actions: Binary actions, i.e. $A = \\{0,1\\}$ to not send an intervention message (0) or to send an intervention message (1).\n\u2022 Decision Points: T = 140 decision points per participant. For each participant, the Oralytics clinical trial ran for 70 days, and each day had 2 decision points (i.e., morning and evening).\n\u2022 Reward: Let $R_{i,t} \\in \\mathbb{R}$ denote the reward for participant i after taking action $A_{i,t}$ at decision point t. We defined the reward $R_{i,t}$ given to the algorithm to be a function of brushing quality $Q_{i,t}$ (i.e., proximal health outcome) in order to improve the algorithm's learning (23). The reward $R_{i,t}$ for the ith participant at decision point t is:\n$R_{i,t}:= Q_{i,t}-C_{i,t}$\nThe cost term $C_{i,t}$ allows the RL algorithm to optimize for immediate healthy brushing behavior, while also considering the delayed effects of the current action on the effectiveness of future actions. The"}, {"title": "B.3 Treatment Assignment and Parameter Updating", "content": "The algorithm assigns treatment by deciding between sending a prompt $A_{i,t} = 1$ or not $A_{i,t} = 0$. If $A_{i,t} = 1$, the backend controller (described in Section 2.1) randomly selected content for each prompt. There are 3 different content categories: (1) participant winning a gift (direct reciprocity), (2) participant winning a gift for their favorite charity (reciprocity by proxy), and (3) Q&A for the morning decision point or feedback on prior brushing for the evening decision point. To decide on the exact content, the main controller first samples a category with equal probability and then samples with replacement prompt content from that category. Due to the large number of prompt content in each category, it is highly unlikely that a participant received the same content twice.\nWe now discuss how the algorithm learns and assigns treatment. Since the RL algorithm in Oralytics is a generalized contextual bandit, the model of the participant environment is a model of the conditional mean reward (reward given current context and selected treatment). To model the conditional mean reward, the Oralytics RL algorithm uses a Bayesian Linear Regression with action centering model:\n$R_{i,t} = S_{i,t} \\alpha_0 + \\pi_{i,t} S_{i,t} \\alpha_1 + (A_{i,t} - \\pi_{i,t})S_{i,t} \\beta + \\epsilon_{i,t}$\nwhere $S_{i,t}$ is the algorithm's context (defined in Appendix B.2), $\\pi_{i,t}$ is the probability that the RL algorithm assigns treatment $A_{i,t} = 1$ in context $S_{i,t}$. $\\epsilon_{i,t} \\sim N(0,\\sigma_0^2)$ and there are priors on $\\alpha_0 \\sim N(\\mu_{\\alpha_0}, \\Sigma_{\\alpha_0})$, $\\alpha_1 \\sim N(\\mu_{\\beta}, \\Sigma_{\\beta})$, $\\beta \\sim N(\\mu_{\\beta}, \\Sigma_{\\beta})$.\nParameter Updating To learn throughout the trial, the algorithm updates its parameters. Let $\\tau(i, t)$ be the update time for participant i that included the current decision point t. We needed an additional index because update times $\\tau$ and decision points t are not on the same cadence. At parameter update time, the reward model's posterior parameters are updated with all history of context, treatments, and rewards up to that point. Since the RL algorithm uses Thompson sampling, the algorithm updates parameters $\\theta_{\\text{post}}^{(\\tau(i,t))}$ and $\\Sigma_{\\text{post}}^{(\\tau(i,t))}$ of the posterior distribution.\nTreatment Assignment The RL algorithm for Oralytics is a modified posterior sampling algorithm called the smooth posterior sampling algorithm. The algorithm selects treatment $A_{i,t} \\sim Bern(\\pi_{i,t})$:\n$\\pi_{i,t} = E_{\\beta \\sim N(\\theta_{\\text{post,i}}^{(\\tau(i,t)-1)}, \\Sigma_{\\text{post,i}}^{(\\tau(i,t)-1)})} [p(S_{i,t} \\beta) | H_{1:n,\\tau(i,t)-1}, S_{i,t} = s]$\n$\\tau(i, t) - 1$ is the last update time for the posterior parameters. Notice that the last expectation above is only over the draw of $\\beta$ from the posterior distribution.\nIn smooth posterior sampling, p is a smooth function chosen to enhance the replicability of the randomization probabilities if the study is repeated (30). The Oralytics RL algorithm set p to be a generalized logistic function:\n$\\rho(x) = L_{\\text{min}} + \\frac{L_{\\text{max}} - L_{\\text{min}}}{[1 + c \\exp(-bx)]}$\nwhere asymptotes $L_{\\text{min}} = 0.2$, $L_{\\text{max}} = 0.8$ are clipping values (bounded away from 0 and 1) to enhance the ability to answer scientific questions with sufficient power (31). This way $\\pi_{i,t}$ can only attain values within [$L_{\\text{min}} = 0.2$, $L_{\\text{max}} = 0.8$]."}, {"title": "B.4 Backup Treatment Schedule", "content": "A critical design decision made for Oralytics was to construct a backup treatment schedule (for the full 70 days duration of the intervention) every morning rather than only assigning treatment for the current day. For Oralytics, the backend controller and mobile app were designed and developed separately by different teams. Therefore, there's a higher likelihood of integration and communication issues that could arise that impact treatment delivery. This decision mitigates possible communication issues between the backend controller and mobile app which could lead to participants failing to obtain treatment for the current decision point.\nRecall that after the"}]}