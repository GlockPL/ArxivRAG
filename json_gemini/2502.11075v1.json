{"title": "Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models", "authors": ["Haoyang LI", "Xuejia CHEN", "Zhanchao XU", "Darian LI", "Nicole HU", "Fei TENG", "Yiming LI", "Luyu QIU", "Chen Jason ZHANG", "Qing LI", "Lei CHEN"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in natural language processing tasks, such as text generation and semantic understanding. However, their performance on numerical reasoning tasks, such as basic arithmetic, numerical retrieval, and magnitude comparison, remains surprisingly poor. This gap arises from their reliance on surface-level statistical patterns rather than understanding numbers as continuous magnitudes. Existing benchmarks primarily focus on either linguistic competence or structured mathematical problem-solving, neglecting fundamental numerical reasoning required in real-world scenarios. To bridge this gap, we propose NumericBench, a comprehensive benchmark to evaluate six fundamental numerical capabilities: number recognition, arithmetic operations, contextual retrieval, comparison, summary, and logical reasoning. NumericBench includes datasets ranging from synthetic number lists to the crawled real-world data, addressing challenges like long contexts, noise, and multi-step reasoning. Extensive experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal persistent weaknesses in numerical reasoning, highlighting the urgent need to improve numerically-aware language modeling.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Zhao et al., 2024) have demonstrated remarkable capabilities in text generation, semantic understanding, and task adaptation across diverse domains (Ling et al., 2024). Their success is largely attributed to pretraining on vast text corpora using next-token prediction objectives (He and Su, 2024), which enables generalization to tasks requiring linguistic creativity, commonsense reasoning, and domain-specific knowledge (Ye, 2024). However, while LLMs perform well in text understanding tasks and Olympic mathematics questions (Team et al., 2024), they surprisingly struggle with simple numerical tasks such as basic multiplication, comparison, and retrieval. For example, in Figure 1, current LLMs fails on simple numerical tasks, even on number comparisons.\nUnlike tasks that rely primarily on semantic coherence and linguistic structures, numerical reasoning requires a deeper understanding of numbers as continuous magnitudes rather than discrete tokens. Current LLMs tend to prioritize surface-level statistical patterns, such as lexical co-occurrences and syntactic regularities, which limits their ability to process numerical operations (Ahn et al., 2024; Feng et al., 2024; Zhou et al.). As a result, LLMs frequently struggle with tasks involving numeric retrieval, arithmetic operations, and magnitude comparisons. These shortcomings highlight an urgent need to systematically evaluate and improve the numerical reasoning capabilities of LLMs.\nCurrent evaluation frameworks for LLMs prioritize either linguistic competence or formal mathematical problem-solving. For instance, semantic-oriented benchmarks (Vuli\u0107 et al., 2020), such as GLUE (Wang, 2018), SuperGLUE (Wang et al., 2019), and SimpleQA (Wei et al., 2024), primarily assess linguistic competence and semantic understanding, while math-oriented benchmarks, such as MathQA (Amini et al., 2019), GSM8K (Cobbe et al., 2021), and MathBench (Liu et al., 2024b), focus on structured algebraic or geometric tasks."}, {"title": "2 Preliminary and Related Works", "content": "In this section, we first introduce large language models and then present existing benchmarks."}, {"title": "2.1 Large Language Models", "content": "Large language models (LLMs), such as GPT-4 (Achiam et al., 2023), DeepSeek (Liu et al., 2024a), PaLM (Anil et al., 2023), and LLaMA (Touvron et al., 2023), have revolutionized natural language processing (NLP) through their ability to generate coherent text (Cho et al., 2019), answer questions (Chen et al., 2024), and adapt to diverse tasks (Wang et al., 2025; Jiang et al., 2024). Their success stems from pretraining on vast text corpora using next-token prediction objectives, which enable generalization on tasks requiring semantic understanding, commonsense reasoning, and linguistic creativity. However, this training paradigm encourages LLMs to prioritize surface-level statistical patterns (e.g., lexical co-occurrences, syntactic regularities) rather than numerically grounded reasoning (Bachmann and Nagarajan, 2024). Consequently, LLMs treat numbers as discrete tokens rather than continuous magnitudes, inherently limiting their ability to understand exact numerical semantics. This leads to errors in numeric retrieval, arithmetic operations, and magnitude comparisons (Qiu et al., 2024)."}, {"title": "2.2 Benchmarks on Large Language Models", "content": "Existing benchmarks (Li et al., 2024a; Chang et al., 2024; Zhao et al., 2023) for evaluating LLMs primarily fall into two categories, i.e., semantic-oriented and math-oriented benchmarks. Specifically, semantic-oriented benchmarks, such as GLUE (Wang, 2018), SuperGLUE (Wang et al., 2019), SimpleQA (Wei et al., 2024), and Long-Bench (Bai et al., 2023), focus on semantic understanding and linguistic competence, testing skills like textual entailment, commonsense reasoning, and domain-specific knowledge (e.g., science and law). While effective for assessing linguistic proficiency, these benchmarks largely overlook numerical reasoning. On the other hand, math-oriented benchmarks (Gao et al., 2025; Li et al., 2024b; Cobbe et al., 2021), such as MathQA (Amini et al., 2019), GSM8K (Cobbe et al., 2021), and Math-Bench (Liu et al., 2024b), target mathematical problem-solving (e.g., algebra, calculus) or extractive question-answering with numerical answers. However, these datasets emphasize well-formed mathematical problems in controlled and clean settings. Consequently, math-oriented benchmarks poorly evaluate numerical retrieval and reasoning in real-world conditions, where noise, and contextual complexity (e.g., multi-step financial workflows or long stock sequences) are common.\nConsidering that numeric retrieval and reasoning are critical for real-world applications, such as finance (Islam et al., 2023) and weather forecasting (Zhang et al., 2024), we propose NumericBench to systematically evaluate the fundamental numerical abilities of LLMs on intensive tasks, such as precise value retrieval, dynamic comparisons, and arithmetic-logical reasoning."}, {"title": "3 NumericBench Generation", "content": "In this section, we present our created NumericBench, which is specifically designed to evaluate fundamental numerical capabilities of LLMs. NumericBench consists of diverse datasets and tasks, enabling a systematic and comprehensive evaluation. We discuss the datasets included in NumericBench, the key abilities it evaluates, and the methodology for benchmark generation."}, {"title": "3.1 Numeric Dataset Collection", "content": "NumericBench offers a diverse collection of numerical datasets and questions designed to reflect real-world scenarios. This variety ensures that LLMs are thoroughly tested on their fundamental abilities on numerical data.\nNumber List Dataset. The synthetic number list dataset consists of simple collections of numerical values (integer and floats) presented as ordered or unordered lists. Numbers in lists are one of the simplest and most fundamental data representations encountered in real-world scenarios. Despite their simplicity, retrieving, indexing, comparison, and summary on numbers can verify the fundamental numerical ability of LLMs. This dataset serves as a fundamental dataset of how well LLMs understand numerical values as discrete entities.\nStock Dataset. The time-series stock dataset is crawled from Eastmoney website (Eastmoney Website, 2025), which has eighteen attributes, such as stock close prices, open price, trading volumes, and price-earnings ratio, over time. Stock data reflects dynamic, real-world numerical reasoning challenges that involve trend analysis, comparison, and decision-making under uncertainty, representing real-world financial workflows.\nWeather Dataset. The weather dataset is crawled from Open-Meteo python API (Open-Meteo), which includes data related to weather metrics, such as temperature, precipitation, humidity, and wind speed. The data is presented across various longitude and latitude.\nNumeric Sequence Dataset. The synthetic numeric sequence dataset comprises sequences of numbers generated by arithmetic or geometric progression, complex patterns, or noisy inputs. Tasks require identifying patterns, predicting the next number, or reasoning about relationships between numbers. Numerical sequences test the logical reasoning capabilities of LLMs, requiring pattern recognition and multi-step reasoning. This dataset introduces structured challenges that are common in mathematics and algorithmic reasoning.\nArithmetic Operation Dataset. The dataset comprises 12,000 pairs of simple numbers, each undergoing addition, subtraction, multiplication, and division operations. Each pair of numbers, a and b, consists of k-digit integers with three decimal places, where $k \\in \\{1, 2, \\dots, 6\\}$. For each value of k, there are 2,000 pairs, evenly distributed across the four basic operations (i.e, +, -, *, /), with 500 pairs per operation. This dataset is to evaluate the fundamental mathematical operation capabilities of LLMs, simulating the majority of mathematical calculation requirements in real-world scenarios.\nMixed-number-string Sequence Dataset. The"}, {"title": "3.2 Fundamental Numerical Ability", "content": "NumericBench is designed to comprehensively evaluate six fundamental numerical reasoning abilities of LLMs, which is essential for solving real-world numeric-related tasks.\nContextual Retrieval Ability. Contextual retrieval ability evaluates how well LLMs can locate, extract, and identify specific numerical values or their positions within structured or unstructured data. This includes tasks like finding a specific number in a list, retrieving values, and indexing numbers based on their order. For example, as shown in Table 1, it evaluates LLMs on tasks such as retrieving stock prices and identifying key values within numerical lists or domain-specific data (e.g., stock market and weather-related information). This ability is fundamental to numerical reasoning because it forms the foundation for higher-order tasks, such as comparison, aggregation, and logical reasoning.\nComparison Ability. Comparison ability evaluates how well LLMs can compare numerical values to determine relationships such as greater than, less than, or equal to, and identify trends or differences in datasets. Comparison is vital for logical reasoning and decision-making, as many real-world tasks depend on accurate numerical evaluation. For instance, as shown in Table 1, comparing prices is essential in stock for assessing performance, while weather forecasting requires analysis of temperature or precipitation trends over time.\nSummary Ability. Summary ability assesses the LLM's capacity to aggregate numerical data into concise insights, such as calculating totals, averages, or other statistical metrics. Summarization is critical for condensing large datasets into actionable information, enabling decision-making based on aggregated insights rather than raw data. This ability is indispensable in domains like electricity usage analysis, where summarizing hourly or daily consumption helps forecast bills, in business reporting for aggregating sales and revenue data to evaluate performance, and in healthcare analytics to monitor trends in patient metrics over time.\nLogic Reasoning Ability. Logical Reasoning Ability measures the LLM's ability to perform multi-step operations involving numerical data, such as recognizing patterns, inferring rules, and applying arithmetic or geometric reasoning to solve complex problems. Logical reasoning extends beyond simple numerical tasks and reflects the LLM's capacity for deeper, structured thinking. This ability is crucial for algorithm design, where solving problems involving numeric sequences or patterns is essential, in scientific research for identifying relationships and correlations in data.\nArithmetic Operation Ability. It reflects the LLM's capacity to perform mathematical calculations accurately. Such ability is essential for tasks involving numerical computations, such as automated machine learning through LLMs.\nNumber Recognition Ability. This measures the LLM's proficiency in identifying and interpreting numerical information within a given context. It represents a fundamental requirement for handling numeric-based tasks effectively."}, {"title": "3.3 NumericBench Generation", "content": "We use the number list, stock, and weather datasets to evaluate the contextual retrieval, comparison, and summary abilities of LLMs. Specifically, for each ability and each dataset, we prepare a set of questions designed to assess the corresponding target ability. As shown in Table 4, Table 5, and Table 6 in Appendix, there are nine question sets in total, covering three abilities across three datasets. When evaluating a specific ability (e.g., contextual retrieval) on a specific dataset (e.g., stock data), we randomly select one question from the corresponding question set for each data instance (e.g., a stock instance) and manually label the answer. This approach enables us to generate question-answer pairs for each ability on the number list, stock, and weather datasets.\nFor arithmetic operations and number counting in the strings dataset, the question format is straightforward, as illustrated in Table 1. These questions are designed to evaluate the basic arithmetic operation and number recognition abilities of LLMs."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment Setting", "content": "Benchmarks and Evaluated Protocols. The statistic of NumericBench is provided in Table 1. Also, we set the exact answer for mixed-number-string dataset, set the computed answer to two decimal places for arithmetic datasets, and set the answer of each question as a single choice (e.g., A, B, or C) for other datasets to reliable evaluate LLMs (Bai et al., 2024). The evaluation metric is accuracy.\nEvaluated Models. To comprehensively evaluate the retrieval and reasoning abilities of state-of-the-art and widely-used LLMs on numeric data, we benchmark over 10 popular LLMs with our constructed NumericBench, as follows.\n\u2022 The Llama Series (Grattafiori et al., 2024). include Llama-3.1-8B Instruct, Llama-3.1-70B Instruct, Llama-3.3-70B-Instruct, Llama-3.1-405B Instruct and Llama-3.1-Nemotron-70B-Instruct.\n\u2022 The Qwen Series (Qwen et al., 2025). include the effective Qwen2.5-7B-Instruct and Qwen2.5-72B-Instruct.\n\u2022 The GLM Series (GLM et al., 2024). We use GLM4-Long to run the benchmark, since it is the commonly used in GLM series.\n\u2022 The Deepseek Series (Liu et al., 2024a) (DeepSeek-AI et al., 2025). We currently use Deepseek V3 to run the benchmark. Deepseek R1 will be evaluated in the future, since its API is down and unavailable now.\n\u2022 The GPT Series (Achiam et al., 2023). We use GPT-40 to run the benchmark.\nWe attempted to conduct experiments on various math-oriented LLMs, such as Metamath-Llemma-7B (Yu et al., 2023), Deepseek-Math-7B-instruct (Shao et al., 2024), InternLM2-Math-7B (Ying et al., 2024) and MAmmoTH-7B (Xiang Yue, 2023). However, these models fail during experiments for various reasons such as overly long output sequence length and limited input sequence length. Fail cases are demonstrated in the Figure 11, Figure 12, Figure 13, and Figure 14 in Appendix."}, {"title": "4.2 Main Experiments", "content": "Evaluation on Contextual Retrieval, Comparison, Summary, and Logic Reasoning Abilities. As shown in Table 2, current popular and effective LLMs perform poorly on basic numerical tasks, including retrieval, comparison, summarization, and logical reasoning. The random baseline for each task is 12.5%, as there are 8 choices, and the probability of randomly selecting the correct answer is 1/8. Human evaluation was conducted by three undergraduate students.\nFirstly, LLMs particularly struggle with accurately retrieving numerical data. This limitation arises from LLMs treating numbers as discrete tokens rather than continuous ones, coupled with insufficient exposure to structured numerical datasets during training, which restricts their ability to handle simple numeric retrieval tasks. Secondly, LLMs demonstrate weaknesses in recognizing numerical relationships, such as greater-than or less-than comparisons, due to a lack of numerical semantics and underdeveloped arithmetic reasoning capabilities. Thirdly, LLMs also perform poorly in summarizing numerical data (e.g., calculating sums or means), reflecting their inability to execute multi-step numerical operations. Similarly, logical reasoning tasks, especially those involving patterns or sequences, are particularly challenging, with all models scoring below 20%. These tasks require multi-step reasoning, pattern recognition, and arithmetic operations, which expose the architectural limitations of current LLMs.\nEvaluation on Different Context Length via Stock and Weather Datasets. We evaluate LLMS on varying context lengths. Specifically, we categorize the contexts of number lists, stock data, and weather data into short and long contexts. The average token numbers for the short and long contexts across the three datasets are listed in Table 7. As illustrated in Figure 2, Figure 8, and Figure 9, LLMs generally achieve lower accuracy on long contexts compared to short contexts. This is because long contexts require the model to have a stronger ability to capture long-range dependencies. Furthermore, if an LLM fails to perform well on short contexts, it is unlikely to achieve good results on long contexts."}, {"title": "Evaluation on Noisy Context via Stock and Weather Datasets", "content": "To evaluate the numerical abilities of LLMs in noisy contexts, we add $k \\in \\{2, 4, 6\\}$ irrelevant attributes to each instance in the stock and weather. These irrelevant attributes are not used in the user queries. As shown in Figure 3 and Figure 10 in Appendix, as k increases, most LLMs exhibit degraded performance. This indicates that irrelevant context can affect the LLM's numerical retrieval and reasoning abilities."}, {"title": "Evaluation on Arithmetic Operations", "content": "Similarly, we evaluate five strong LLMs on arithmetic operations. Specifically, as illustrated in Figure 4 (a), even for simple arithmetic operations involving two numbers, LLMs fail to achieve 100% accuracy. Moreover, as the number of digits increases shown in Figure 4 (b), the accuracy of LLMs decreases, highlighting their limited ability to handle arithmetic tasks effectively, which is also observed in (Qiu et al., 2024). This poor performance stems from how LLMs generate responses. LLMs predict the highest-order digit before the lower-order digit (Zhang-Li et al., 2024), contradicting the standard arithmetic logic of progressing from lower- to higher-order digits. In particular, Figure 4 (a) and (c) shows that LLMs perform similarly on addition, subtraction, and division operations but achieve extremely low accuracy on multiplication tasks."}, {"title": "Evaluation on Number Recognition via Mixed-number-string Dataset", "content": "We evaluate the number recognition ability of effective LLMs by identifying numbers from mixed-number-string sequences. For this evaluation, we select five effective LLMs based on Table 2, including DeepSeek-v3, GLM-4-Long, LLaMA3.1-405B, and Qwen2.5-72B. As shown in Table 3, all LLMs achieve extremely low accuracy in counting numbers within strings. Moreover, as the length of the string increases from 50 to 100, the accuracy of the LLMs decreases further. These results highlight that LLMs are significantly weak at distinguishing numbers from strings. The underlying reason is that current LLMs treat numbers as strings during training. This training paradigm inherently limits their ability to understand and process numbers effectively. Also, the tokenizer can split a single number into multiple tokens, which can negatively affect the numeric meaning of each number."}, {"title": "4.3 Discussions on Numeracy Gaps of LLMs", "content": "In summary, extensive experimental results show that current state-of-the-art LLMs perform poorly on six fundamental numerical abilities. Here we discuss five potential reasons behind their poor performance on numerical tasks."}, {"title": "Tokenizer Limitation", "content": "LLMs use tokenizers to split input text into smaller units (tokens). Thus, Numbers are split into chunks as strings, based on statistical patterns in the training data. For example, 10000 is split into 100 and 00 tokens\u00b9. These tokenizers do not considering the real meaning of numbers and continuous magnitude of numbers. Thus, LLMs do not perform well on simple number retrieval and comparison tasks."}, {"title": "Training Corpora Limitation", "content": "LLMs are trained on extensive corpora, which also limits their ability to understand numerical-related symbols, such as *. For example, the multiplication of 246 and 369 can be denoted as 246*369. However, 246*369 may be interpreted as a password or encrypted text, since * in text strings is often associated with encryption. As a result, enabling LLMs to accurately interpret arithmetic symbols remains an open problem."}, {"title": "Training Paradigm Limitation", "content": "The training of LLMs relies on the next-token prediction paradigm, which is inherently misaligned with the logic of numerical computation. For example, when solving 16 + 56 with the result being 72, an LLM will first predict the highest-order digit of the answer (i.e., 7) before predicting the lower-order digit (i.e., 2). This approach contradicts the fundamental logic of arithmetic computation, which typically proceeds from the lower-order digit to the higher-order digit. This discrepancy implies that LLMs effectively need to know the entire result upfront to generate digits sequentially in the correct order. As a result, LLMs struggle to perform well even on simple arithmetic operations."}, {"title": "Positional Embedding Limitation", "content": "Note that LLMs incorporate positional embeddings for tokens in sequence inputs. In arithmetic operations like 12 + 26 and 26 + 12, the order of the numbers does not affect the result. However, LLMs assign different positional embeddings to the number 12 in each equation, as its position in the sequence differs. This lack of invariance in positional embeddings for numbers can influence the results. Therefore, how to design the positional embedding that improves numerical ability of LLMs without affecting the text understanding of LLMs is critical (McLeish et al., 2024; Golovneva et al., 2024)."}, {"title": "Transformer Architecture Limitation", "content": "LLMs use Transformer to process input sequence, which rely on pattern recognition rather than explicit algorithmic reasoning. The computational power of transformers has upper bounds (Merrill and Sabharwal, 2023). Considering the complexity of arithmetic operations in real-world applications, it still needs to be theoretically investigated whether transformers can perform well on numerical operations."}, {"title": "5 Conclusion and Future Directions", "content": "In this paper, we propose a comprehensive benchmark NumericBench to evaluate the six fundamental numerical abilities of LLMs, including number recognition, arithmetic operations, contextual retrieval, comparison, summary, and logical reasoning. Our experiments reveal significant gaps in LLMs' numerical reasoning, as even state-of-the-art models like GPT-4 and DeepSeek struggle with simple arithmetic, number retrieval, and multi-step reasoning. These shortcomings are from tokenization issues, training paradigms, and architectural limitations, highlighting the need for more numerically-aware modeling approaches.\nTo address these gaps, there are several future directions worth exploring. Firstly, numerically-aware tokenizers can be developed to treat numbers as continuous magnitudes, which fundamentally enables large language models to better understand numerical concepts. Secondly, instead of the next-token prediction objective, pretraining objectives specifically tailored to numerical reasoning can be designed. This could help models become more adept at solving numerical problems. Thirdly, incorporating structured numerical datasets can enhance real-world applicability by grounding models in more accurate and practical numerical contexts. Lastly, research into suitable positional embeddings and hybrid symbolic-numeric model architectures for numeric data is promising to improve the numerical abilities of LLMs. Together, these advancements have the potential to bridge the gap between large language models' linguistic and numerical reasoning capabilities."}, {"title": "Limitations", "content": "There are two main limitations of this paper. Firstly, the numerical tasks encountered in real-world scenarios are often far more complex and diverse compared to the six datasets proposed in NumericBench. Expanding the scope to include a broader range of numerical reasoning categories, such as traffic, would provide a more comprehensive assessment. Nevertheless, our work can serves as a meaningful point, highlighting the current limitations of LLMs in numerical tasks. We also analyze the potential reasons why LLMs struggle with numerical reasoning tasks, which can be attributed to the inherent limitations of transformer architectures and the next-token prediction objective. We hope it inspires further efforts to address these challenges and develop more advanced LLMs with enhanced numerical capabilities.\nSecondly, although we evaluate ten state-of-the-art LLMs, several newer LLMs and their variants, such as Claude and GPT-01 from major companies, are not included in our experiments. The reason for this exclusion is the expensive cost of accessing these model APIs. In brief, evaluating additional LLM variants across Claude, OpenAI, Mistral and GLM, typically requires a minimum budget of $15,000 US dollars. Specifically, experiments on the datasets in Table 2 require approximately 180 million tokens as inputs, while all left experiments (e.g., noisy contexts) require about 84 million tokens as inputs. For 1 million input tokens, Claude 3 Opus costs $152, Claude 3.5 Sonnet costs $33, OpenAI-01 costs $154, Gemini 1.5 Pro costs $12.55, GLM4-Plus costs $6.896, Mistral Large 24.11 costs $27, Mixtral 8x22B costs $28, and OpenAI 03-mini costs $1.19.\nIf we conduct experiments above with these top-tier models from major companies, it would cost at least 3960 dollars for Claude 3 Opus, 3960 dollars for OpenAI-01, 3300 dollars for Gemini 1.5 Pro, 1819 dollars for GLM4-Plus, 792 dollars for Claude Sonnet 3.5, 528 dollars for Mistral Large 24.11, 528 dollars for Mixtral 8x22B and 290 dollars for OpenAI 03-mini, which is beyond our expected total experiment cost. Meantime, there are too many LLM variants in each series.\nAlso, for models such as OpenAI-01, which require generating really longer outputs for reasoning purposes, the output length is often unpredictable, while the model charges for $60 per million output tokens, making the experiments even more expensive and difficult to control. Particularly, The reason for not using DeepSeek-R1 is that its official API is currently down and unavailable. We plan to include it in our comparisons once the API is restored. Considering that GPT-40 and DeepSeek-V3 represent the most state-of-the-art LLM models, we believe our evaluation can reflects the current numerical abilities of leading-edge LLMs. Therefore, our evaluation highlights the weaknesses of LLMs in numerical abilities and serves as a bridge to inspire further research focused on improving the numerical capabilities of these models."}, {"title": "A Appendix", "content": "In this appendix, we provide additional details about the design of NumericBench, along with supplementary experimental results and case studies. The organization of the supplementary materials in this appendix is as follows:\n1. Question formats for contextual retrieval, comparison, and summary abilities. As shown in Table 4, Table 5, and Table 6, we designed diverse question types tailored to each dataset to evaluate the three fundamental numerical abilities of LLMs: contextual retrieval, comparison, and summary. contextual retrieval assesses the model's capacity to accurately extract relevant numerical information from complex contexts; comparison tests the ability to analyze and compare numerical values; Summary evaluates the synthesis of numerical information into concise and meaningful insights for tasks like reporting or trend analysis.\nBy designing tailored questions for each dataset, we ensure a comprehensive evaluation of LLMs\u2019 numerical reasoning abilities across varying scenarios and complexities.\n2. Basic numerical questions answered incorrectly by GPT-40. As illustrated in Figure 5, Figure 6, and Figure 7, GPT-40 failed to answer three basic numerical questions correctly. This result is surprising, considering GPT-40's impressive performance in real-world applications. However, these findings highlight the weak fundamental numerical abilities of LLMs.\n3. Token counts for short and long contexts. As shown in Table 7, the token counts of long and short contexts differ significantly. This distinction enables a more thorough evaluation of LLM performance across scenarios involving varying context lengths. Short contexts are designed to test the model's ability to process and understand concise information, focusing on immediate comprehension and reasoning. In contrast, long contexts present a more complex challenge, requiring the model to handle extended sequences of information, maintain coherence over a larger context window, and retrieve relevant details from earlier parts of the input. Such two type length can more comprehensively evaluate LLMs.\n4. Additional experimental results on noisy and varying-length contexts. As shown in Figure 8 and Figure 9, existing LLMs perform poorly on the stock and weather datasets, although they achieve better performance compared to their results on short contexts. Similarly, as shown in Figure 10, LLMs perform poorly on noisy weather data.\n5. Real failure cases of math-oriented LLMs. In this paper, we do not compare existing math-oriented LLMs, such as Metamath-Llemma-7B (Yu et al., 2023), Deepseek-Math-7B-Instruct (Shao et al., 2024), InternLM2-Math-7B (Ying et al., 2024), and MAmmoTH-7B (Xiang Yue, 2023). This is primarily because these math-oriented LLMs are designed for specialized geometric and structured mathematical problems. They are unable to understand the tasks in NumericBench, fail to follow a correct reasoning process, and directly produce meaningless outputs. These failure cases are illustrated in Figure 11, Figure 12, Figure 13, and Figure 14.\nThe Use of AI Tools. When writing this paper, we use Grammarly for automated spell checking and use GPT-40 to refine several sentences."}]}