{"title": "AGENTLESS:\nDemystifying LLM-based Software Engineering Agents", "authors": ["Chunqiu Steven Xia", "Yinlin Deng", "Soren Dunn", "Lingming Zhang"], "abstract": "Recent advancements in large language models (LLMs) have significantly\nadvanced the automation of software development tasks, including code\nsynthesis, program repair, and test generation. More recently, researchers\nand industry practitioners have developed various autonomous LLM\nagents to perform end-to-end software development tasks. These agents are\nequipped with the ability to use tools, run commands, observe feedback\nfrom the environment, and plan for future actions. However, the complexity\nof these agent-based approaches, together with the limited abilities of cur-\nrent LLMs, raises the following question: Do we really have to employ complex\nautonomous software agents? To attempt to answer this question, we build\nAGENTLESS \u2013 an agentless approach to automatically solve software devel-\nopment problems. Compared to the verbose and complex setup of agent-\nbased approaches, AGENTLESS employs a simplistic two-phase process\nof localization followed by repair, without letting the LLM decide future ac-\ntions or operate with complex tools. Our results on the popular SWE-bench\nLite benchmark show that surprisingly the simplistic AGENTLESS is able\nto achieve both the highest performance (27.33%) and lowest cost ($0.34)\ncompared with all existing open-source software agents! Furthermore, we\nmanually classified the problems in SWE-bench Lite and found problems\nwith exact ground truth patch or insufficient/misleading issue descriptions.\nAs such, we construct SWE-bench Lite-S by excluding such problematic\nissues to perform more rigorous evaluation and comparison. Our work\nhighlights the current overlooked potential of a simple, interpretable\ntechnique in autonomous software development. We hope AGENTLESS will\nhelp reset the baseline, starting point, and horizon for autonomous software\nagents, and inspire future work along this crucial direction.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have become the go-to default choice for code gener-\nation [18, 14, 34, 54]. State-of-the-art LLMs like GPT-4 [44] and Claude-3.5 [13] have\ndemonstrated their prowess in being able to synthesize code snippets based on given user\ndescription. However, compared to the main evaluation setting of simple, self-contained\nproblems, applying LLMs on repository-level software engineering tasks has been\nunderstudied. Software engineering tasks like feature addition, program repair, and test\ngeneration require an in-depth understanding of not only information within files, which\ncan contain thousands of lines of code, but also repository-level dependencies across files.\nRecently, to address the gap and evaluate the ability of tools to automatically solve\nreal-world software engineering problems, the popular SWE-bench [28] benchmark has\nbeen developed. In SWE-bench, each problem consists of a real-world GitHub issue"}, {"title": "2 AGENTLESS Approach", "content": "Figure 1 shows the overview of AGENTLESS, consisting of two phases: localization and\nrepair. We first take in the issue description and the existing project codebase as input.\nThen, we begin our hierarchical localization process by turning the project codebase into\na tree-like structure format that demonstrates the relative location of each file in the project.\nNext, 2 using this repository structure format along with the original issue description, we\nask the LLM to localize and rank the top N most suspicious files that need editing to solve\nthe issue. However, not all contents in each file need to be modified. As such, we provide\na skeleton for each file (i.e., a list of declaration headers of the classes and functions) and\nask the LLM to output a specific list of classes and functions that we should examine more\nclosely to fix the bug. We then provide the complete code content of the previous locations\nand ask the LLM to finalize a smaller set of edit locations (i.e., classes, functions, or even"}, {"title": "2.1 Localization", "content": "To fix or implement a new feature, the first step is to obtain the locations in the source code,\nas without the correct locations, it can be impossible to provide the right edits. The difficulty\nlies in the fact that there could be hundreds of files with thousands of lines of code each in\na repository, whereas the correct locations to edit are only a few selected lines or functions.\nAGENTLESS addresses this by using a simple three-step hierarchical localization process:\n1) localize to selected files; 2) localize each selected files into relevant classes, functions, and\nvariables; 3) localize to code edit locations.\nLocalize to suspicious files. First, AGENTLESS localizes the possible locations to specific\nsuspicious files. Instead of providing the complete code snippet for each file in the repository,\nAGENTLESS constructs a succinct representation of the repository's file and directory struc-\nture. We refer to this as the repository structure format, which begins with the root folder of\nthe repository and organizes code files or folder names. Files and folders at the same direc-\ntory level are aligned vertically, and files/folders in sub-directories are indented. We recur-\nsively traverse the entire repository to obtain the repository structure format, which will be\nused as input for the LLM. The repository structure format provides the necessary file paths\nalongside the neighboring file names to maintain organizational information in the original\ncodebase. AGENTLESS then inputs the processed repository structure format along with the\noriginal issue description to an LLM and requests it to identify a list of the top N suspicious\nfiles in the repository that need further inspection or modification to resolve the issue.\nLocalize to related elements. After obtaining the list\nof suspicious files to edit to solve the issue, AGENT-\nLESS then moves on to the second part of the local-\nization process: localize to related elements within\nthe suspicious files. Directly providing the complete\ncontext of all files can be large. As such, AGENTLESS\nbuilds a compressed format of each file that contains\nthe list of class, function, or variable declarations. We\nrefer to this format as skeleton format, with an ex-"}, {"title": "2.2 Repair", "content": "In the repair stage, the goal is to produce\nthe correct patch to solve the issue. Follow-\ning existing work on LLM-based program\nrepair [31, 48, 27], we first utilize the iden-\ntified edit locations and construct a con-\ntext window of code snippets to provide\nto the LLM for repair. For example, if the\nidentified location was a class from line 40\nto 78, we would produce a context win-\ndow of [40 \u2212 x, 78 + x] where x denotes\nthe context window size. The intuition be-\nhind adding the additional code before and\nafter the identified location is to provide the\nLLM with relevant contextual information\nfor better program repair [57]. If multiple"}, {"title": "3 Experimental Setup", "content": "Datasets. We evaluate AGENTLESS and baselines using the popular SWE-bench dataset\nto test the ability to solve real-world software engineering issues. Each problem in SWE-\nbench requires submitting a patch to solve the underlying issue described in the input issue\ndescription. In particular, we focus on the filtered subset SWE-bench Lite, containing 300\nproblems with tests to evaluate the functional correctness of submitted patch. Furthermore,\nwe also conduct a detailed study (Section 5.1) on the SWE-bench Lite benchmark to not\nonly demonstrate potential issues and biases but also produce a more rigorous filtered set\nof problems for better evaluation.\nImplementation. We implement AGENTLESS using GPT-40 (gpt-40-2024-05-13) [45]. By\ndefault, we query the LLM with greedy decoding. During sampling, we use a sampling\ntemperature of 0.8. For each issue, we first localize to the top three suspicious files, and then\nlocalize to an unrestricted number of suspicious classes and functions within these files, all\nusing greedy decoding. Next, to maximize the chances of finding the correct edit locations,\nwe draw four samples of edit locations per issue (i.e., the third step in the localization phase),\nand combine two sampling runs together to provide more context for repair. This gives us\ntwo separate sets of edit locations per issue. For each set, we adopt a context window of \u00b1\n10 lines around each edit location, and generate 21 patches (1 greedy and 20 samples). This\nresults in a total of 42\u00b9 patches per bug. We adopt the same Search/Replace edit format from\nprior work [21], and use the built-in Python ast library [2] to perform parsing in our patch\nnormalization step. Due to issues with the original SWE-bench evaluation script at the time\nof writing, we adopt the SWE-bench-docker [68] evaluation setup used by prior tools [21].\nBaselines. We compare AGENTLESS against 13 agent-based approaches. These baseline\ntools represent the state-of-the-art performance on SWE-bench. We include state-of-the-art\nopen-source as well as commercial or closed-source baselines (indicated via a). We note\nhere that the majority of the closed-source baselines do not provide any trajectories, just\nthe submission patches. Therefore, we cannot verify the steps taken to arrive at the final\npatches. Moreover, we also include a simple agentless baseline using retrieval-augmented\ngeneration (RAG) proposed as part of SWE-bench [28] for comparison. In this case, the\nagentless baseline uses the LLM to directly generate a patch file by providing it with the file\ncontent of the most relevant files, retrieved using BM25 [49]. Additionally, we also list the\nunderlying LLM used by each tool whenever possible.\nMetrics. Following prior work [65], we report 1) % Resolved: the percentage of resolved\nproblems in the benchmark, 2) Avg. $ Cost: average inference cost of running the tool,\nand 3) Avg. # Tokens: average number of input and output tokens used to query to LLM.\nAdditionally, we also report the % Correct Location: the percent of problems where the\npatch produced by the tool matches with the edit location of the ground truth developer\npatch. We compute this metric over three granularities: file, function, and line. We report\nthat a patch contains the correct location if it edits a superset of all locations in the ground\ntruth patch. For baseline tools, we directly use the reported results either from the official\nleaderboard [29] or from the tool's official paper/repository."}, {"title": "4 Evaluation", "content": "Repair performance. Table 1 shows the main evaluation result of AGENTLESS and prior\nagent-based approaches on SWE-bench Lite. We observe that AGENTLESS is able to solve 82\nout of 300 problems (27.33%). While this is not the highest percentage of problems solved\non SWE-bench Lite, AGENTLESS is extremely competitive compared with prior agent-based\napproaches while using a much simpler design and overall technique. It is important to note\nhere that many of the top techniques are closed-source/commercial and did not release any\nsource code to reproduce experiments or even trajectories for further verification. Compared\nwith open-source approaches, AGENTLESS is able to achieve the highest performance of"}, {"title": "5 Additional Analysis on SWE-bench Lite", "content": "5.1 Problem Classification\nWe now take a closer look at the problems in SWE-bench Lite. We first classify the existing\nproblems to gain better understanding and additional insights on exactly what types of\nproblems AGENTLESS and prior approaches can solve. Specifically, we perform manual\nclassification based on the issue description and ground truth developer patch of each"}, {"title": "5.2 SWE-bench Lite-S", "content": "Building on the above problem classifications, in the following evaluation section, we will\nmore rigorously compare and contrast AGENTLESS and existing work. Specifically, we focus\non a subset of the problems in SWE-bench Lite after removing the problems that contain\nthe exact patch in the problem description, misleading solutions, or do not provide enough\ninformation in the original issue description. This eliminates the less reasonable problems\nand normalizes the difficulty level of the benchmark. For future work, we hope to work\nwith the maintainers and contribute to the SWE-bench Lite benchmark by fixing these\nunreasonable problems to add additional information as well as removing exact ground\ntruth patches in the problem descriptions. However, as we are not able to run commercial\ntools ourselves on the modified problems, we simply exclude the problematic problems in\nthe below evaluation. We refer to our subset of 252 problems as SWE-bench Lite-S.\nTable 4 shows the results on the SWE-bench Lite-S benchmark and the corresponding\nranking of each approach. We also included the results on the original 300 problems in\nSWE-bench Lite for comparison. While the general ranking of all approaches stay roughly\nthe same, we do observe some small ranking changes. Compared to the original SWE-bench\nLite, our filtered benchmark of SWE-bench Lite-S provides a more accurate reflection of the\ntrue capability of autonomous software development tools."}, {"title": "6 Related Work", "content": "LLMs for code. LLMs have become the default choice for various coding tasks, due to\nthe impressive results achieved by LLMs in both code generation and understanding [18].\nDevelopers and researchers have applied on software engineering tasks, such as program\nsynthesis [47, 18, 35, 25], code translation [46, 50, 51], program repair [59, 58, 42, 31, 15], and\ntest generation [19, 60, 20, 33, 30]. Apart from using general-purpose LLMs, code-specific\nLLMs have been built by further training LLMs using large amounts of open-source code\nsnippets. Examples of code LLMs include CODEX [18], CodeLlama [52], StarCoder [34,\n39], DeepSeek-Coder [22], etc. Furthermore, researchers have also developed instruction-\nfollowing code-specific LLMs using instruction-tuning methods. Examples of such LLMs\ninclude CodeLlama-Inst [52], DeepSeek-Coder-Inst [22], WizardCoder [40], Magicoder [54],\nand OpenCodeInterpreter [67].\nBenchmarking for LLM-based coding tasks. To evaluate the capability of LLMs on code,\nvarious benchmark has been proposed. HUMANEVAL [18] and MBPP [14] are two of the most\nwidely-used handcrafted code generation benchmarks complete with test cases to check for"}, {"title": "7 Conclusion", "content": "We propose AGENTLESS\u2013 an agentless approach to automatically tackle software develop-\nment problems. AGENTLESS uses a simple two phase approach of localization followed by\nrepair. Compared to prior agent-based approaches, AGENTLESS deliberately disallows the\nLLM for autonomous tool usage or planning. Our evaluation on the popular SWE-bench Lite\nbenchmark demonstrates that AGENTLESS can achieve the highest performance compared\nwith other open-source techniques while at the same time minimizing the cost. Furthermore,\nwe perform a detailed classification of problems in SWE-bench Lite to not only offer new\ninsights but to construct a more rigorous benchmark of SWE-bench Lite-S after removing\nproblematic problems."}]}