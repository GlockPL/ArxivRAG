{"title": "AGENTLESS:\nDemystifying LLM-based Software Engineering Agents", "authors": ["Chunqiu Steven Xia", "Yinlin Deng", "Soren Dunn", "Lingming Zhang"], "abstract": "Recent advancements in large language models (LLMs) have significantly\nadvanced the automation of software development tasks, including code\nsynthesis, program repair, and test generation. More recently, researchers\nand industry practitioners have developed various autonomous LLM\nagents to perform end-to-end software development tasks. These agents are\nequipped with the ability to use tools, run commands, observe feedback\nfrom the environment, and plan for future actions. However, the complexity\nof these agent-based approaches, together with the limited abilities of cur-\nrent LLMs, raises the following question: Do we really have to employ complex\nautonomous software agents? To attempt to answer this question, we build\nAGENTLESS \u2013 an agentless approach to automatically solve software devel-\nopment problems. Compared to the verbose and complex setup of agent-\nbased approaches, AGENTLESS employs a simplistic two-phase process\nof localization followed by repair, without letting the LLM decide future ac-\ntions or operate with complex tools. Our results on the popular SWE-bench\nLite benchmark show that surprisingly the simplistic AGENTLESS is able\nto achieve both the highest performance (27.33%) and lowest cost ($0.34)\ncompared with all existing open-source software agents! Furthermore, we\nmanually classified the problems in SWE-bench Lite and found problems\nwith exact ground truth patch or insufficient/misleading issue descriptions.\nAs such, we construct SWE-bench Lite-S by excluding such problematic\nissues to perform more rigorous evaluation and comparison. Our work\nhighlights the current overlooked potential of a simple, interpretable\ntechnique in autonomous software development. We hope AGENTLESS will\nhelp reset the baseline, starting point, and horizon for autonomous software\nagents, and inspire future work along this crucial direction. We have open-\nsourced AGENTLESS at: https://github.com/OpenAutoCoder/Agentless", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have become the go-to default choice for code gener-\nation [18, 14, 34, 54]. State-of-the-art LLMs like GPT-4 [44] and Claude-3.5 [13] have\ndemonstrated their prowess in being able to synthesize code snippets based on given user\ndescription. However, compared to the main evaluation setting of simple, self-contained\nproblems, applying LLMs on repository-level software engineering tasks has been\nunderstudied. Software engineering tasks like feature addition, program repair, and test\ngeneration require an in-depth understanding of not only information within files, which\ncan contain thousands of lines of code, but also repository-level dependencies across files.\nRecently, to address the gap and evaluate the ability of tools to automatically solve\nreal-world software engineering problems, the popular SWE-bench [28] benchmark has\nbeen developed. In SWE-bench, each problem consists of a real-world GitHub issue"}, {"title": "2 AGENTLESS Approach", "content": "Figure 1 shows the overview of AGENTLESS, consisting of two phases: localization and\nrepair. We first take in the issue description and the existing project codebase as input.\nThen, we begin our hierarchical localization process by turning the project codebase into\na tree-like structure format that demonstrates the relative location of each file in the project.\nNext, 2 using this repository structure format along with the original issue description, we\nask the LLM to localize and rank the top N most suspicious files that need editing to solve\nthe issue. However, not all contents in each file need to be modified. As such, we provide\na skeleton for each file (i.e., a list of declaration headers of the classes and functions) and\nask the LLM to output a specific list of classes and functions that we should examine more\nclosely to fix the bug. We then provide the complete code content of the previous locations\nand ask the LLM to finalize a smaller set of edit locations (i.e., classes, functions, or even"}, {"title": "2.1 Localization", "content": "To fix or implement a new feature, the first step is to obtain the locations in the source code,\nas without the correct locations, it can be impossible to provide the right edits. The difficulty\nlies in the fact that there could be hundreds of files with thousands of lines of code each in\na repository, whereas the correct locations to edit are only a few selected lines or functions.\nAGENTLESS addresses this by using a simple three-step hierarchical localization process:\n1) localize to selected files; 2) localize each selected files into relevant classes, functions, and\nvariables; 3) localize to code edit locations.\nLocalize to suspicious files. First, AGENTLESS localizes the possible locations to specific\nsuspicious files. Instead of providing the complete code snippet for each file in the repository,\nAGENTLESS constructs a succinct representation of the repository's file and directory struc-\nture. We refer to this as the repository structure format, which begins with the root folder of\nthe repository and organizes code files or folder names. Files and folders at the same direc-\ntory level are aligned vertically, and files/folders in sub-directories are indented. We recur-\nsively traverse the entire repository to obtain the repository structure format, which will be\nused as input for the LLM. The repository structure format provides the necessary file paths\nalongside the neighboring file names to maintain organizational information in the original\ncodebase. AGENTLESS then inputs the processed repository structure format along with the\noriginal issue description to an LLM and requests it to identify a list of the top N suspicious\nfiles in the repository that need further inspection or modification to resolve the issue.\nLocalize to related elements. After obtaining the list\nof suspicious files to edit to solve the issue, AGENT-\nLESS then moves on to the second part of the local-\nization process: localize to related elements within\nthe suspicious files. Directly providing the complete\ncontext of all files can be large. As such, AGENTLESS\nbuilds a compressed format of each file that contains\nthe list of class, function, or variable declarations. We\nrefer to this format as skeleton format"}, {"title": "2.2 Repair", "content": "In the repair stage, the goal is to produce\nthe correct patch to solve the issue. Follow-\ning existing work on LLM-based program\nrepair [31, 48, 27], we first utilize the iden-\ntified edit locations and construct a con-\ntext window of code snippets to provide\nto the LLM for repair. For example, if the\nidentified location was a class from line 40\nto 78, we would produce a context win-\ndow of [40 x, 78 + x] where x denotes\nthe context window size. The intuition be-\nhind adding the additional code before and\nafter the identified location is to provide the\nLLM with relevant contextual information\nfor better program repair [57]. If multiple\nedit locations are identified, we would concatenate these context windows together sepa-\nrated with \"...\" to indicate missing context in the middle.\nPatch format. Using the code snippets, we then ask the LLM to generate patches to solve\nthe issue. However, instead of directly producing the entire code snippet to replace the\nentire given context, AGENTLESS asks the LLM to generate a Search/Replace edit [21]:\na simple diff format to efficiently create each patch. Figure 3 shows an example of the\nSearch/Replace format containing two main parts: 1) search: the original code snippet we\nwant to replace and 2) replace: the replacement code snippet we want to replace with. To\napply the generated Search/Replace diff to the original file, we can simply match the search\ncode snippet and replace it with the replacement. This simple diff format avoids generating\nthe complete code and instead focuses on producing small edits, which are not only more\ncost-efficient, but also more reliable and accurate (less chances for hallucination).\nFiltering and patch selection. For each issue, AGENTLESS uses the LLM to generate multiple\npotential patches (starting with greedy and then sample multiple patches with higher tem-\nperature). We also apply traditional software engineering technique of regression testing [55]\nto run the existing tests in the repository on all the generated patches. Any patches which\nfailed the existing tests can be filtered out as they incorrectly change the correct behavior of\nprevious code. Note, our implementation of this regression test filtering step follows prior\nwork also evaluated on the same benchmark [21, 17]. For the remaining patches, AGENT-\nLESS applies a re-ranking approach using majority voting: We first normalize each patch to\nignore surface-level differences (e.g., extra spaces, newlines, and comments), and then select\nthe patch with the highest number of occurrences as the final patch for submission. More\nspecifically, to standardize the patch, we begin by parsing both the old and new code (after\napplying the patch) into abstract syntax trees. Next, we unparse the trees into a canonical\nsource code format with docstrings removed. Finally, we compute the textual diff between\nthe standardized old and new code to get the normalized patch.\nAGENTLESS solves repository-level issues using a simple step-by-step procedure. We note\nhere that none of the techniques used by AGENTLESS in isolation are revolutionary, but in-\nstead AGENTLESS smartly combines existing techniques to construct an easy-to-understand\napproach. Different from prior autonomous agent-based tools that involve complex interac-\ntions with the environment, AGENTLESS uses a simple two phase approach to first localize\nand then repair the bug without relying on any agents for decision-making. By conducting\nlocalization in a hierarchical manner, AGENTLESS can efficiently and effectively compute the\nfine-grained locations for editing. AGENTLESS then performs repair by sampling multiple\npatches using a simple diff format. We filter out any patches with syntax and regression\ntests errors, and finally select the patch for submission using classic majority voting."}, {"title": "3 Experimental Setup", "content": "Datasets. We evaluate AGENTLESS and baselines using the popular SWE-bench dataset\nto test the ability to solve real-world software engineering issues. Each problem in SWE-\nbench requires submitting a patch to solve the underlying issue described in the input issue\ndescription. In particular, we focus on the filtered subset SWE-bench Lite, containing 300\nproblems with tests to evaluate the functional correctness of submitted patch. Furthermore,\nwe also conduct a detailed study (Section 5.1) on the SWE-bench Lite benchmark to not\nonly demonstrate potential issues and biases but also produce a more rigorous filtered set\nof problems for better evaluation.\nImplementation. We implement AGENTLESS using GPT-40 (gpt-40-2024-05-13) [45]. By\ndefault, we query the LLM with greedy decoding. During sampling, we use a sampling\ntemperature of 0.8. For each issue, we first localize to the top three suspicious files, and then\nlocalize to an unrestricted number of suspicious classes and functions within these files, all\nusing greedy decoding. Next, to maximize the chances of finding the correct edit locations,\nwe draw four samples of edit locations per issue (i.e., the third step in the localization phase),\nand combine two sampling runs together to provide more context for repair. This gives us\ntwo separate sets of edit locations per issue. For each set, we adopt a context window of \u00b1\n10 lines around each edit location, and generate 21 patches (1 greedy and 20 samples). This\nresults in a total of 42\u00b9 patches per bug. We adopt the same Search/Replace edit format from\nprior work [21], and use the built-in Python ast library [2] to perform parsing in our patch\nnormalization step. Due to issues with the original SWE-bench evaluation script at the time\nof writing, we adopt the SWE-bench-docker [68] evaluation setup used by prior tools [21].\nBaselines. We compare AGENTLESS against 13 agent-based approaches. These baseline\ntools represent the state-of-the-art performance on SWE-bench. We include state-of-the-art\nopen-source as well as commercial or closed-source baselines (indicated via a). We note\nhere that the majority of the closed-source baselines do not provide any trajectories, just\nthe submission patches. Therefore, we cannot verify the steps taken to arrive at the final\npatches. Moreover, we also include a simple agentless baseline using retrieval-augmented\ngeneration (RAG) proposed as part of SWE-bench [28] for comparison. In this case, the\nagentless baseline uses the LLM to directly generate a patch file by providing it with the file\ncontent of the most relevant files, retrieved using BM25 [49]. Additionally, we also list the\nunderlying LLM used by each tool whenever possible.\nMetrics. Following prior work [65], we report 1) % Resolved: the percentage of resolved\nproblems in the benchmark, 2) Avg. $ Cost: average inference cost of running the tool,\nand 3) Avg. # Tokens: average number of input and output tokens used to query to LLM.\nAdditionally, we also report the % Correct Location: the percent of problems where the\npatch produced by the tool matches with the edit location of the ground truth developer\npatch. We compute this metric over three granularities: file, function, and line. We report\nthat a patch contains the correct location if it edits a superset of all locations in the ground\ntruth patch. For baseline tools, we directly use the reported results either from the official\nleaderboard [29] or from the tool's official paper/repository."}, {"title": "4 Evaluation", "content": "Repair performance. Table 1 shows the main evaluation result of AGENTLESS and prior\nagent-based approaches on SWE-bench Lite. We observe that AGENTLESS is able to solve 82\nout of 300 problems (27.33%). While this is not the highest percentage of problems solved\non SWE-bench Lite, AGENTLESS is extremely competitive compared with prior agent-based\napproaches while using a much simpler design and overall technique. It is important to note\nhere that many of the top techniques are closed-source/commercial and did not release any\nsource code to reproduce experiments or even trajectories for further verification. Compared\nwith open-source approaches, AGENTLESS is able to achieve the highest performance of\n1 the answer to the ultimate question of life, the universe, and everything [12]"}, {"title": "5 Additional Analysis on SWE-bench Lite", "content": "We now take a closer look at the problems in SWE-bench Lite. We first classify the existing\nproblems to gain better understanding and additional insights on exactly what types of\nproblems AGENTLESS and prior approaches can solve. Specifically, we perform manual\nclassification based on the issue description and ground truth developer patch of each"}, {"title": "5.1 Problem Classification", "content": "problem. Below describes each of classification dimensions and their categories in more\ndetail:\n1) Description quality. We first inspect whether each issue description contains sufficient\ninformation to perform the desired task. Figure 5a shows the distribution of each category: (i)\ncontains enough information in natural language, (ii) contains reproducible failure example,\n(iii) contains partially reproducible example, and (iv) does not contain enough informa-\ntion.We observe that while a majority of the tasks in SWE-bench Lite contains sufficient\ninformation, with many having some small failure examples to showcase the bug, there\nis a non-trivial percentage (9.3%) of problems which do not contain enough information.\nSuch problems include those that require implementing a new function with a specific name\nor adding an error message with a specific string that was not provided in the problem\ndescription.2 This means the test will fail if the function name or error message string does\nnot match exactly, even if the underlying functionality is correctly implemented. Another\nexample of insufficient information are problems that have multiple different interpretations\non how to solve the issue, and only a subset of them can pass the ground truth test. For\ninstance, the issue description will outline two possible solutions suggestions with only one\nof them aligned well with developer intention. Implementing the other proposed solution\nsuggestion will lead to test failure. This highlights the necessity to further sanitize/improve\nSWE-bench Lite where these problems with uninformative descriptions shall be further\nexcluded.\n2) Solution in description. We also examine whether the solution or steps to solve the\nproblem are already provided in the issue description. Figure 5b shows the breakdown of\nour categories: (i) no solution or steps provided, (ii) partial solution provided (e.g., some\nsteps in natural language), (iii) complete solution provided (e.g., complete steps in natural\nlanguage), (iv) exact patch provided, and (v) misleading solution or steps. Interestingly, we\nobserve that 4.3% of issues contain the exact ground truth patch in the issue description,\nwhile an additional 10.0% of issues describe the exact steps required to come up with the\ncorrect solution. This shows that certain problems in SWE-bench Lite can be much easier\nto solve since they provide the solution either in exact code snippets or natural language.\nFurthermore, we also observe 4.3% of issues contain proposed solution or steps in the issue\ndescription that do not reflect the ground truth patch introduced by the developers. This\nfurther highlights potential issues with the benchmark, as these discrepancies can mislead\ntools to generate incorrect solutions simply by following the issue description.\n3) Location information. We further check if the issues description contains the correct\nlocation information. We divide the granularity into line, function, and file level locations.\nOur categories are: (i) exact locations in natural language, (ii) exact locations provided in\nfailure stack traces, iii) related keywords in the issue description that can be used to search\nfor the location, and (iv) not provided. We first observe that only in very few cases (<10%),\nthe issue provides the exact lines needed to fix the bug. However, this number increases\n2These types of problems still exist in the benchmark despite claims that they have been completely\nremoved by the filtering process according to SWE-bench Lite."}, {"title": "5.2 SWE-bench Lite-S", "content": "Building on the above problem classifications, in the following evaluation section, we will\nmore rigorously compare and contrast AGENTLESS and existing work. Specifically, we focus\non a subset of the problems in SWE-bench Lite after removing the problems that contain\nthe exact patch in the problem description, misleading solutions, or do not provide enough\ninformation in the original issue description. This eliminates the less reasonable problems\nand normalizes the difficulty level of the benchmark. For future work, we hope to work\nwith the maintainers and contribute to the SWE-bench Lite benchmark by fixing these\nunreasonable problems to add additional information as well as removing exact ground\ntruth patches in the problem descriptions. However, as we are not able to run commercial\ntools ourselves on the modified problems, we simply exclude the problematic problems in\nthe below evaluation. We refer to our subset of 252 problems as SWE-bench Lite-S.\nTable 4 shows the results on the SWE-bench Lite-S benchmark and the corresponding\nranking of each approach. We also included the results on the original 300 problems in\nSWE-bench Lite for comparison. While the general ranking of all approaches stay roughly\nthe same, we do observe some small ranking changes. Compared to the original SWE-bench\nLite, our filtered benchmark of SWE-bench Lite-S provides a more accurate reflection of the\ntrue capability of autonomous software development tools."}, {"title": "6 Related Work", "content": "LLMs for code. LLMs have become the default choice for various coding tasks, due to\nthe impressive results achieved by LLMs in both code generation and understanding [18].\nDevelopers and researchers have applied on software engineering tasks, such as program\nsynthesis [47, 18, 35, 25], code translation [46, 50, 51], program repair [59, 58, 42, 31, 15], and\ntest generation [19, 60, 20, 33, 30]. Apart from using general-purpose LLMs, code-specific\nLLMs have been built by further training LLMs using large amounts of open-source code\nsnippets. Examples of code LLMs include CODEX [18], CodeLlama [52], StarCoder [34,\n39], DeepSeek-Coder [22], etc. Furthermore, researchers have also developed instruction-\nfollowing code-specific LLMs using instruction-tuning methods. Examples of such LLMs\ninclude CodeLlama-Inst [52], DeepSeek-Coder-Inst [22], WizardCoder [40], Magicoder [54],\nand OpenCodeInterpreter [67].\nBenchmarking for LLM-based coding tasks. To evaluate the capability of LLMs on code,\nvarious benchmark has been proposed. HUMANEVAL [18] and MBPP [14] are two of the most\nwidely-used handcrafted code generation benchmarks complete with test cases to check for"}, {"title": "7 Conclusion", "content": "We propose AGENTLESS\u2013 an agentless approach to automatically tackle software develop-\nment problems. AGENTLESS uses a simple two phase approach of localization followed by\nrepair. Compared to prior agent-based approaches, AGENTLESS deliberately disallows the\nLLM for autonomous tool usage or planning. Our evaluation on the popular SWE-bench Lite\nbenchmark demonstrates that AGENTLESS can achieve the highest performance compared\nwith other open-source techniques while at the same time minimizing the cost. Furthermore,\nwe perform a detailed classification of problems in SWE-bench Lite to not only offer new\ninsights but to construct a more rigorous benchmark of SWE-bench Lite-S after removing\nproblematic problems."}]}