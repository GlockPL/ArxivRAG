{"title": "Rare Disease Differential Diagnosis with Large Language Models at Scale: From Abdominal Actinomycosis to Wilson's Disease", "authors": ["Elliot Schumacher", "Dhruv Naik", "Anitha Kannan"], "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in disease diagnosis. However, their effectiveness in identifying rarer diseases, which are inherently more challenging to diagnose, remains an open question. Rare disease performance is critical with the increasing use of LLMs in healthcare settings. This is especially true if a primary care physician needs to make a rarer prognosis from only a patient conversation so that they can take the appropriate next step. To that end, several clinical decision support systems are designed to support providers in rare disease identification. Yet their utility is limited due to their lack of knowledge of common disorders and difficulty of use.\nIn this paper, we propose RareScale to combine the knowledge LLMs with expert systems. We use jointly use an expert system and LLM to simulate rare disease chats. This data is used to train a rare disease candidate predictor model. Candidates from this smaller model are then used as additional inputs to black-box LLM to make the final differential diagnosis. Thus, RareScale allows for a balance between rare and common diagnoses. We present results on over 575 rare diseases, beginning with Abdominal Actinomycosis and ending with Wilson's Disease. Our approach significantly improves the baseline performance of black-box LLMS by over 17% in Top-5 accuracy. We also find that our candidate generation performance is high (e.g. 88.8% on gpt-4o generated chats).", "sections": [{"title": "Introduction", "content": "Rare diseases, often overlooked in the medical landscape, impact approximately 5.9% of the global population. For those affected, the journey to diagnosis is fraught with challenges, with an average wait of four to five years. Patients typically endure three misdiagnoses and consultations with at least five doctors during this process. This prolonged diagnostic odyssey is often due to limited provider knowledge, implicit biases, or symptom overlap with common conditions (Kliegman and Brett J Bordini, 2017; Office, 2021). Only in a few instances does the absence of specific tests or the extreme rarity of the disease present the challenge.\nClinical decision support systems focusing on rare disease diagnoses emerged in the 1980s as a tool to address this challenge Miller et al. (1982); Buchanan and Shortliffe (1985); Barnett et al. (1987); Lauritzen and Spiegelhalter (1988); Jackson (1998). Practical use of these systems, however, has been constrained by several factors (Miller, 1994), including lack of integration with physician workflows. Additionally, they are often custom-built explicitly for rare diseases and exclude common conditions.\nRecent advances in large language models (LLMs; OpenAI (2024); Anthropic (2024); Meta (2024); Jiang et al. (2023)) achieve state-of-the-art performance on a variety of tasks (Zheng et al., 2023; Wang et al., 2019; Hendrycks et al., 2020), including in high-stakes healthcare applications (Chen et al., 2023; Thawakar et al., 2024; Nair et al., 2024). Studies on rare disease diagnosis using LLMs show similar promise but have been on smaller, curated datasets of clinical vignettes that often include laboratory and imaging information to diagnose (Hu et al., 2023; Shyr et al., 2024; Sandmann et al., 2024; Chen et al., 2024; Yang et al., 2024b,a; Mehnen et al., 2023; Shyr et al., 2024; do Olmo et al., 2024).\nFurthermore, the challenge with these studies is that the first point of contact for the patient is the primary care physician. For an untrained physician in rare diseases, the overlap of symptoms (e.g., pulmonary legionellosis usually manifests with symptoms similar to flu or pneumonia) makes it easy to overlook without specialized knowledge. While rare diagnoses often require additional information beyond the initial symptom presentation, they cannot be diagnosed if not considered in the first place. This illustrates the criticality of including relevant rare diseases in the differential diagnosis, even before any lab work, directly based on patient-provider history-taking conversations. We found that applying black-box LLMs to the task of rare disease diagnosis shows only moderate performance (e.g. 56.8% Top-5 accuracy using gpt-4o). This indicates that while LLMs have some knowledge of rare diseases, they also may struggle in certain cases.\nIn this paper, we improve the diagnostic capability of LLMs in identifying rare diseases based on two key insights. First, given LLMs have some knowledge of rare diseases, can we more explicitly target this knowledge? Second, can knowledge from rare expert systems be used to better evaluate and inform LLMs?\nWe propose RareScale, which is designed to improve differential diagnoses of rare diseases directly from history-taking dialogues. Figure 1 provides an overview of our approach.\nWe utilize a rare disease expert system and a medical case simulator to generate a broad set of structured clinical cases with closed-world assumptions within the expert system. These cases consist of structured findings, that a patient with the given rare disease may exhibit. These cases are then used as inputs to the history-taking conversation simulator that generates free-form provider questions and patient answers that conform to those cases. This approach covers 575 rare diseases, beginning with Abdominal Actinomycosis and ending with Wilson's Disease.\nGiven that the expert system excludes common diseases, we develop a rare disease candidate generation model instead of a full diagnosis system. For each case, we generate a list of candidate rare diseases to prompt a larger, black-box LLM, which combines its general diagnostic capabilities with the specialized knowledge of the smaller model.\nOur results show statistically significant performance improvements compared to relying solely on LLMs like GPT-40. For example, we find that the Top-5 accuracy improves from 56.7% to 74.1% on gpt-40 generated chats. We believe this illustrates the potential of integrating curated expert knowledge sources and the power of LLMs."}, {"title": "Problem Setup", "content": "When a patient visits a medical provider with a new, unknown health condition, the first step is understanding the issue deeply. This process, known as history-taking, consists of collecting all relevant information about the patient's current health issue, including positive and negative findings (or symptoms), via a series of questions. These details are used to make treatment decisions if a confident diagnosis can be made. Alternatively, they inform the selection of tests or imaging to help further diagnose.\nWhat if a possible diagnosis is not even considered because it's so rare, and the medical provider is unaware of it? This paper aims at proposing a method RareScale as a step towards removing that educational barrier. Given the history-taking conversation between the provider and the patient, the goal is to identify possible rare diseases that need to be considered. When doing this, we must balance the fact that the disease is rare and that the patient is more likely to have a common disease."}, {"title": "Overview of the approach", "content": "Figure 1 provides the overview of RareScale: It trains a smaller expert LLM \u00a7 4 to generate candidate rare diseases using a simulated labeled conversational dataset using a combination of expert systems and LLMs (\u00a7 3). The candidates generated from this model are used as additional inputs to the black-box LLM to enable better weighing rare and common diseases. This leads to improved efficacy over using only black-box LLMs."}, {"title": "Corpus Simulation", "content": "The use of synthetic data from large language models has been widely studied (Li et al., 2023; Yu et al., 2023; Liu et al., 2024). However, our task requires generating history-taking conversational data with labeled differential diagnosis. Since identifying differential diagnosis is the task we are trying to solve with RareScale, we took a different approach to generate labeled history taking chats to avoid simply evaluating an LLM based on it's existing knowledge.\nAs shown in Figure, 2, we first simulate an example using expert system knowledge. We start with a seed disease and generate a structured patient case (represented as a list of findings). Such a generation can have ambiguity about the final diagnosis. Therefore, we use the same expert system to assign a full differential diagnosis. The structured case is then used to simulate a history-taking conversation as detailed in \u00a7 3.2 so that LLM needs to only generate a patient-facing question with the provided finding, while maintaining the conversational flow. We repeat this process independently for all diseases. The resulting dataset of history-taking conversation and differential diagnosis pairs is used to train rare diseases candidate generation model (\u00a7 4)."}, {"title": "Generation of structured cases", "content": "We use a rare disease expert system, evolved from Internist-1 (Miller et al., 1982) and QMR (Miller and Masarie, 1990), to generate structured cases using expert-curated diagnostic rules based on a knowledge base of diseases, findings, and their relationships. Findings include symptoms, signs, lab results, demographics, or medical history. In this study, we focus on patient-answerable findings and only focus on symptoms. Each finding-disease link is defined by evoking strength and frequency, scored from 1 to 5 by a team of medical experts based on the studies for that disease. Evoking strength measures the association between a finding and a disease, while frequency indicates how often a finding occurs in patients with the disease. Each finding also has a disease-independent import variable, indicating its global importance.\nOur simulation algorithm, adapted from medical case simulator Parker and Miller (1989); Ravuri et al. (2018), starts by sampling a seed disease and sequentially constructing a set of findings. First, demographic variables are sampled, followed by predisposing factors, and then other findings are made to decrease frequency relative to the disease. Each finding is randomly determined to be present or absent, with impossible findings excluded and high co-occurrence findings prioritized. The simulation concludes once all findings in the knowledge base for that disease are considered, operating under a closed-world assumption that limits diseases and findings to those in the knowledge base. Unlike previous approaches to simulation, we also use the expert system to compute differential diagnosis after the first six findings are sampled. We use this to consider findings from other diagnoses in the differential diagnosis to prioritize sampling additional negative findings that overlap with the seed disease. This allows the sample to be slightly more targeted at the seed disease. In turn, we widen the gap between the seed disease and the remainder of the differential diagnosis.\nEach simulation includes a differential diagnosis of up to 5 diseases as ranked by expert system scoring. A DDx may consist of multiple diseases in cases where a final diagnosis may not be ascertained without laboratory testing. We maintain this differential diagnosis list for training in \u00a74. We iterate through 630 rare diseases in the expert system, using each as a seed disease. We keep the generated structured case if the seed disease is top-scoring in the differential to reduce the effect of the noise in the simulation process. We set a minimum of 50 valid simulations out of 200 attempts. Any diseases falling below that are excluded (e.g. Friedreich's ataxia), as this typically indicates that the disease is unidentifiable from symptoms alone. This results in 575 diseases in our dataset. We include an example in Appendix \u00a73."}, {"title": "Generation of chats from structured cases", "content": "For each (structured case, DDx) pair, we start with creating a complete demographic profile. This includes name, gender, age, race, education, and location by first selecting from the structured case. If not present in the case, we randomly generate from the LLM. We incorporate location and education to introduce additional variability in patient language. This diversifies the profiles and reduces sensitivity to names and locations across different simulations, which could lead to misleading patterns.\nWe use the structured case along with the expanded demographic profile to anchor LLM-powered history-taking chat simulation. We prompt an LLM to create a conversation that closely adheres to that case. The LLM-based chat simulator has access to the disease name, finding set, and demographic profile. For each finding, we include a definition if the expert system provides one so the LLM's understanding of the term aligns with the expert system. For each disease, we persist a database of messages that correspond to each finding (e.g. \"I feel hot\u201d corresponds to fever (present)). We include the previous message in the prompt for each subsequent generation and ask the LLM to generate a different one so it does not repeat.\nWe enforce a format for the chat, which begins with a \"system\" message with the patient's demographic information. The provider simulator starts the conversation open-ended, and the patient simulator reports the symptoms drawn from the findings set. For each patient utterance, the LLM outputs both the message and which findings are included. This is to encourage the message to be consistent with the findings. We prompt LLM for the patient simulator to report only at most, three findings in a single message and also prompt the LLM for the provider simulator not to generate leading questions.\nFor the simulation, we use two different LLMs to generate our chats \u2013 OpenAI's Gpt-40 and Anthropic's Claude-3.5-sonnet \u2013 and use the same approach unless noted. All components of the generation pipeline use only one of the LLMs. For chats generated by GPT-4o, we found that doing this process in a single prompt is sufficient (see Appendix Prompt 1). For Claude, we found that this generated chats which were very terse. Therefore, we prompt Claude with the same information and ask it to generate one patient-provider turn at a time (see Appendix Prompt 2). The findings provided to Claude are separated into findings already included and those that need to be added.\nFor chats generated by both LLMs, we finally run a checker prompt that ensures that the resulting chats include all symptoms in the finding set and edits the chat if not all are included. If the first edit fails, we try two more times. If all other attempts fail, we exclude this chat from our dataset. This results in a corpus of rare disease chats \u2013 28,589 using GPT-40 and 14,573 using Claude (further statistics are included in Appendix Table 5). We include a full example chat using GPT-40 in Appendix Figure 4 and using Claude in Figure 5."}, {"title": "Rare Disease Differential Diagnoses", "content": "The RareScale scalable synthetic generation pipeline provides us with a sizable dataset of 575 rare disease history-taking conversations. However, our expert system doesn't provide information on a variety of common diseases, such as the Common Cold. A real-world diagnostic system will need to account for these diseases. Given the high diagnostic performance of many LLMs (Liu et al., 2025; Rutledge, 2024; Zhou et al., 2024; Tu et al., 2024), it would be challenging to meet their performance even with a corpus of similar non-rare chats.\nWe therefore combine existing LLM diagnostic performance with a specialized rare-disease model. To do this, we train a rare disease candidate generation system which proposes a set of at most 5 rare diseases. This list is recall-oriented, to allow several possibilities to be considered. These rare disease candidates are then passed through a prompt of a larger LLM to produce a final diagnostic list. The combination can leverage the niche expertise of an expert system while also leveraging the broader knowledge of general LLMs."}, {"title": "Candidate Generation", "content": "To generate rare disease candidates for a given history-taking conversation, we train a smaller LLM to generate a list of at most 5 rare diseases. We train using Llama 3.1 7b as our base model (Meta, 2024) using the chat text as input (training details in Appendix \u00a7A.1). We exclude the structured findings and all other information used in the previous section. Note that these history-taking chats do not include a final diagnosis from the provider, so the model must make inferences from the chat alone. As the target output, we use the differential diagnosis list produced by our expert systems which contains up to five diseases. We only include the disease names ordered by likelihood per the expert system score but do not include the scores themselves."}, {"title": "Diagnosis Generation", "content": "For the final step of generating the differential diagnosis for the conversation, we use a larger black-box general LLMs\u00b9 (gpt-40, claude, llama 3.3 70b) to fuse common and rare diseases seamlessly. We leave the role of identifying the relevant common diseases to the LLM, but infuse the candidate rare diseases through inferencing on the smaller model we trained according to \u00a7 4.1\nThe prompt (Appendix Prompt 3) uses multi-stage instructions, instructing the LLM to generate a list of 5 diagnoses without considering the rare\nWe explored using the publicly-available MEDITRON-70B(Chen et al., 2023), but it could not follow the DDx instructions in the prompt, instead outputting unrelated text.\nlist. Then, it is instructed to consider the rare disease candidate list. Finally, it selects whichever from the two sections is most appropriate, including discarding all rare candidates if best. The prompt is also flexible, so we can remove instructions for incorporating candidate rare diseases or include other means of candidate rare diseases when available. We study these variations in \u00a7 5.1."}, {"title": "Evaluation and Results", "content": "To understand the performance of RareScale, we frame our evaluation around three questions. First, does the RareScale end-to-end approach improve rare disease diagnoses over black-box LLMs (\u00a75.1)? Second, how does our expert candidate generation model perform at producing an accurate list of rare diseases for each case (\u00a75.2)? Finally, how accurate are the simulated chats as judged by experts (\u00a75.3)? We include details on dataset creation in Appendix \u00a7A.2."}, {"title": "DDx Improvements with RareScale", "content": "We compare the end-to-end RareScale to two baselines. First, in base gpt-40, we generate a DDx with no external candidate list. gpt-40 rare candidates uses the RareScale prompt described in \u00a7 4.2 without any candidate list. Similar to RareScale, gpt-40 rare candidates uses rare disease candidate list, but obtained using a separate LLM prompt. In contrast, RareScale uses a smaller trained model to generate the candidate list. For all three methods, we use the same prompt setup (\u00a7 4.2) except for the additional candidate list and instructions.\nTo be able to compare the generated DDx to the ground seed disease, we use judge prompts adopted from Tu et al. (2024) (Appendix Prompts 4 and 5), with gpt-40 as judge model. Previously, these prompts were shown to be well-calibrated with the human expert as a judge. The first judge prompt returns a binary judgment, iterating through the DDx list and taking the first match present, if any. The second judge prompt compares the full DDx list to the seed disease and returns a degree of similarity (unrelated to exact match) We use Top-1 accuracy, Top-5 accuracy, and mean reciprocal rank (MRR) as the metrics.\nResults Table 1 shows the performance of the combination of our best-performing candidate generation model (gpt-40 + claude, see \u00a75.2) with gpt-40 serving as a DDx generator. We find that RareScale produces the best performance across the board, including statistically significant gains over the no candidate baseline. Notably, we also find that performance is worse when using gpt-40 as the rare disease candidate generator, reinforcing the utility of a specialized model.\nWe believe the most crucial improvement is seen in the Top-5 performance, as adding the rare disease candidate in the DDx would allow a provider to consider that disease. The degree of improvement \u2013 17.42% \u2013 illustrates this impact. While we also see improvements in MRR performance, we also observe that the rare disease candidate is not very likely to be in the first position.\nIncluding the rare disease candidate but not over-relying on the candidate list is likely the preferred behavior (see \u00a7 5.2). In many cases, a common disease should be considered first, but the inclusion of a rare possibility can help a provider to best identify next steps. Using only our smaller rare disease candidate generation model would likely overproduce rare diagnoses, whereas providing a black box LLM with options allows it to weigh the entire picture. Alternatively, the LLM may not be able to properly diagnose a specific rare condition because it has no knowledge of it to begin with (see the analysis for further discussion).\nIn Table 2, we compare LLM performance on the differential diagnosis task with and without RareScale broken down by similarity level. We combine the gpt-40 and claude chats for this analysis. Since this prompt uses varying levels of granularity and compares the entire DDx, the distribution is different than with the binary prompt, which only compares diseases one-to-one.\nIn the case of gpt-4o, we see broad improvements when adding RareScale. This includes a 33% improvement in the number of Exact Matches. Yet we see even larger performance improvements when applying RareScale to claude-sonnet's DDx capabilities. We see a 25.2% reduction in unrelated diagnoses and a 37% increase in exact matches. Surprisingly, we also see similar performance gains on Llama 3.3 70b, including a 27% increase in exact matches and an 11.7% reduction in unrelated matches. While there is a gap between the closed- and open-weight models, the margin is small.\nAnalysis We break down the results of the RareScale gpt-40 dataset results in Table 1 into performance by disease hierarchical category as taken from our expert system. A disease can align with one or more categories. The full results are shown in Appendix Table 4. The group with the largest number of diseases \u2013 Infectious diseases \u2013 performs slightly less than the average. This could potentially be due to the overall broader number of diseases in this category, many of them closely related. Other large groups, such as Neoplastic disease, Impaired cardiovascular function, and non-infectious inflammatory diseases have roughly average performance. On the lower end, as expected, Congenital disorders due to abnormal fetal development performs worst at 53.3% Top-5 accuracy, whereas Disorders of smooth muscle contraction and/or relaxation performs best at 94.4%, although both are small categories."}, {"title": "RareScale Candidate Generation", "content": "We measure the performance of RareScale candidate generation by comparing it to the seed disease. The candidate generator outputs a list of at most 5 candidates, and we compare to the seed disease using exact match. As before, we use MRR and Top-1 and Top-5 accuracy. Since GPT-40 and Claude required different approaches to generate the chat data, we considered different dataset variations to understand differences in the performance.\nResults Table 3 reports the performance of RareScale on candidate generation. Using the combined training set during training leads to the highest performance \u2013 88.8% Top-5 on gpt-4o, and 77.82% on claude on the test sets. When only trained on one of the data sources, performance is best when applied to data from the same source. In our initial experiments, we found that including roughly 40 to 50 training examples per disease would achieve the same performance as more samples (e.g. 100), but fewer would result in worse performance.\nWhile the gpt-40 test set performs similarly on gpt-40 trained models and combined training data, the claude chat test set sees a sizable performance bump when using all training data. Since our claude dataset is smaller in size, we hypothesize that the additional signal from the gpt-40 training examples helps improve performance on the claude test set. To illustrate this, we train a model on only gpt-40 data but randomly (per-seed disease basis) downsampled to roughly the same amount of data in claude's training set. We find a similar pattern \u2013 the training data reduction (40.5% of the original size) results in a 17% drop in Top-5 accuracy, and similar reductions for other metrics. This suggests the lower claude performance is due to dataset size instead of worse corpus simulation performance.\nWhile we only consider exact matches in Table 3, we also run the similarity judge prompt (Prompt 5) on the gpt-40 + claude trained model, gpt-40 test set results. In only 5.2% of cases, the model returns a DDx list unrelated to the seed diagnosis. In the remaining cases, non-exact matches have some degree of similarity. We also include validation results in Appendix Table 7"}, {"title": "Expert Evaluation of Chats", "content": "We evaluate whether our synthetically generated chats are possible manifestations of the seed rare disease. For this, we use an external reviewing service that provides medical annotators and use third-year medical students for this task as they have been recently trained in similar tasks. They can also use additional resources (e.g. medical references) during annotation. We also included hard negative examples during annotations to serve as distractors. We include details in Appendix \u00a7A.2.1.\nResults On our annotation set of 651 cases (with 73 annotation-specific negatives), we found the overall agreement rate to be 88.6% with a Cohen's kappa of 0.53. For the expected positive cases, i.e. the disease was indicated for that patient case during simulation, the agreement rate was 90.48%. For the negative cases, the agreement rate was 73.97%. The high agreement rate of the positive examples used for training illustrates the performance of our synthetic generation pipeline. We include additional discussion in Appendix Section A.3."}, {"title": "Deployment Considerations", "content": "We show that RareScale significantly improves the ability to identify potential rare diseases over baseline LLM performance. We also know that most patients at a primary care clinic are likely to have a common disease. RareScale aims at balancing common diseases with rare diseases so that the primary care physician does not oversubscribe to the possibility of a rare disease. Such rare diseases are hard to rule out without expensive testing and added mental toll.\nWhen deploying in practice, there are several possible approaches. For instance, a system could only include rare diseases in cases where common diseases are ruled out first. For example, pulmonary legionellosis presents similarly to the flu or pneumonia, and a provider would likely treat one of the common conditions first. But when flu or pneumonia management fails, they can use RareScale to consider rarer conditions. Alternatively, RareScale could be used to gather more information from the patient and exclude rarer conditions more quickly. We believe that this is an open problem that may require physician training and clinical testing."}, {"title": "Related Work", "content": "Using LLMs for diagnosis is increasingly powerful (Liu et al., 2025; Rutledge, 2024; Zhou et al., 2024; Tu et al., 2024). Previous rare disease investigations (Hu et al., 2023; Shyr et al., 2024; Sandmann et al., 2024; Chen et al., 2024; Yang et al., 2024a; Mehnen et al., 2023; Shyr et al., 2024; do Olmo et al., 2024) focus on generating diagnoses from smaller sets of vignettes, which are concise summaries of a patient's health issue. Vignettes are commonly used in narrow settings. Comparatively, using the history-taking chat replicates a primary care setting where confirmatory testing has not been performed. Previous chat-related work focused on a much smaller set (Yang et al., 2024b).\nPrompting methods such as chain of thought (Wei et al., 2022) may improve rare disease performance to a degree but cannot enable the integration of external knowledge. Other techniques such as retrieval augmented generation (Lewis et al., 2020) are unlikely to capture the complex nuances of symptoms as stated by patients. LLM performance on emerging data has also been studied (Mitchell et al., 2021; Gu et al., 2024). A related approach is Yao et al. (2024), which uses a smaller expert model, but their approach seeks to update the original model. (Gupta et al., 2024) illustrates that model editing can also lead to the model losing knowledge. Other work (Zhao et al., 2024) studies LLM performance in other rare data situations, and studies the related issue of hallucinations(Fabbri et al., 2022; Min et al., 2023)."}, {"title": "Conclusion", "content": "We propose RareScale as a multi-component approach to improve rare disease diagnosis. Based on curated rare disease expert systems, we use a medical case simulator to generate structured cases and differential diagnoses for each rare disease. In turn, we prompt black-box LLMs to generate history-taking conversations of these cases. This allows us to benchmark the performance of LLMs on rare disease history-taking conversations. We show that explicitly training an LLM for disease candidate generation, which can then provide suggestions to a black-box LLM, achieves statistically significant performance improvements compared to baselines.\nWhile this paper focused on the rare disease diagnosis, we hope RareScale can transfer to other settings. For example, one could leverage a location-specific or population-specific expert system instead of using a rare-specific expert system. This could especially be useful in settings where the real-world disease distribution differs significantly and at the tail of the data distribution, which black-box training data can not corroborate.\nAlthough our approach scales the number of rare diseases to 575, many rare diseases (NORD, 2025) are left unaddressed. An inherent limitation of our approach is that expert systems rely on the labor of highly-trained medical experts who review medical literature and curate knowledge. This allows us to use the expert system as a surrogate source of knowledge instead of directly integrating with rare disease literature. Yet expanding an expert system to all rare diseases with human experts alone is likely impossible. Future work should focus on using medical literature with techniques beyond retrieval augmented generation, which likely struggles to capture the subtlety and relative importance of the various studies. Alternatively, explorations that rely on electronic health record systems could remove the need for the expert system while still using a smaller model tuned for the rare disease candidate generation to be used as in RareScale."}, {"title": "Limitations", "content": "One limitation of our work is that we are limited to the rare diseases and symptoms provided in our rare disease expert system. While our approach does cover a broader set of rare diseases than existing work, it does not cover the vast set of diseases present in the world. For example, (NORD, 2025) lists over 10,000. Yet rare diseases highly vary in prevalence, with only about 100 rare diseases accounting for 80% of diagnoses(Evans and Rafi, 2016). Others are so rare that only a few cases have ever been identified. Expanding to these extremely rare diseases is a remaining challenge.\nWe are also limited by the number of expert annotations that are feasible for this task. While the results were positive, we couldn't annotate a larger amount without spending excessively more time and money. Additionally, we are unable to release the datasets due to legal requirements. While this is a common issue in the medical domain, we hope that future work can provide open datasets for broader use."}, {"title": "Appendix", "content": "Training Details\nWe train for 10 epochs using supervised fine-tuning only on 8 Nvidia A100s. Each training run, using the huggingface transformers library (v4.43.3), took 2-3 hours depending on the setup. We used the AdamW 8-bit optimizer (Loshchilov and Hutter, 2017), a learning rate of $2e-05$, and a batch size of 4. We selected these parameters based on early experiments on subsets of our larger disease set.\nEvaluation Details\nFollowing the approach in Section 3, we generate chats using gpt-40 and claude-sonnet 3.5. For each, we create train, validation, and test splits as detailed in Appendix Table 5. We also provide the average and standard deviation of the number of findings and the number of messages. To ensure that both the validation and test sets are distinct from the training set, we removed all examples from the training set that have the exact same structured case in the validation or test sets. We generated a smaller dataset for claude due to the additional cost and time of running multiple prompts per chat.\nExpert Evaluation\nWe provide the annotators with the chat, list of findings, and the seed disease. They are asked \"Assume the patient has a rare disease and more common conditions have been ruled out. Is this specific rare disease a possible diagnosis for the patient? This rare disease doesn't need to be the most likely diagnosis, but just a possible diagnosis.\". They are prompted to respond yes or no, and provide a 1-2 sentence explanation. They are allowed to use any external reference material they choose.\nOne challenge in this annotation task is that our dataset does not include negative examples as they are not required for training. However, only annotating positive examples may lead to annotators blindly answering \u201cyes\u201d. Therefore, we generate negative training examples solely for expert evaluation and ask annotators to review datasets that are 90% positives and 10% negatives.\nGenerating a chat where a rare disease is not possible is a challenging task. Providing annotators with cases that are obviously negative would not be informative. Generating cases where a disease is negative but close requires conclusively ruling out a disease, which is challenging in a history taking setting where lab work isn't performed. To achieve a close approximation, we regenerate examples from our expert system and take diagnoses that were present in earlier simulation phases but removed later when additional findings were added. These \"discarded diagnoses\" are likely to be negative diagnoses for the findings.\nWe separately generate chats for these findings and discarded diagnosis using the process described in Section 3. As a second filtering step, we provide the chat and discarded diagnosis to a gpt-40 prompt, and prompt it to provide the same binary judgment and explanation as we do with the annotators. Finally, we manually filter out chats where the explanation rests mostly on likelihood (i.e. disease A is unlikely because disease B is more likely), and retain cases where there is a clear separation. However, we want to emphasize that even in these cases, it is hard to completely rule out a rare disease. All cases were reviewed by at least one annotator cases where the original annotator disagreed with the initial label was reviewed by a second labeler.\nExpert Evaluation Results\nWhile the agreement rate for negative cases is lower than the agreement for positive cases, it is inherently challenging to create believably negative cases. It also does not impact RareScale's performance given that we only add negatives to ensure that annotators do not blindly choose yes. We include a selection of annotator reasoning in Appendix Table 6. In many cases where the annotator disagrees with our baseline label, they note that the full symptom set isn't present for the disease. While that may make it unlikely, that does not mean it can be ruled out entirely because an atypical presentation could occur or further symptoms may arise. This tension is inherent when dealing with rare diseases."}]}