{"title": "Adversarial Detection with a Dynamically Stable System", "authors": ["Xiaowei Long", "Jie Lin", "Xiangyuan Yang"], "abstract": "Adversarial detection is designed to identify and reject maliciously crafted adversarial examples(AEs) which are generated to disrupt the classification of target models. Presently, various input transformation-based methods have been developed on adversarial example detection, which typically rely on empirical experience and lead to unreliability against new attacks. To address this issue, we propose and conduct a Dynamically Stable System (DSS), which can effectively detect the adversarial examples from normal examples according to the stability of input examples. Particularly, in our paper, the generation of adversarial examples is considered as the perturbation process of a Lyapunov dynamic system, and we propose an example stability mechanism, in which a novel control term is added in adversarial example generation to ensure that the normal examples can achieve dynamic stability while the adversarial examples cannot achieve the stability. Then, based on the proposed example stability mechanism, a Dynamically Stable System (DSS) is proposed, which can utilize the disruption and restoration actions to determine the stability of input examples and detect the adversarial examples through changes in the stability of the input examples. In comparison with existing methods in three benchmark datasets(MNIST, CIFAR10, and CIFAR100), our evaluation results show that our proposed DSS can achieve ROC-AUC values of 99.83%, 97.81% and 94.47%, surpassing the state-of-the-art(SOTA) values of 97.35%, 91.10% and 93.49% in the other 7 methods.", "sections": [{"title": "1 Introduction", "content": "Adversarial examples (AEs) are intentionally perturbed examples containing artificial noises, inducing misclassification in deep neural networks(DNNs). This susceptibility of DNNs to AEs has raised extensive concerns [Fawzi et al., 2018] due to the diverse security applications of DNNs, such as face recognition [Sharif et al., 2016], autonomous driving [Liu et al., 2023], and the medical domain [Uwimana and Senanayake, 2021], etc. Consequently, an advanced defense method is necessary to mitigate the risk from adversarial examples.\nA widely accepted hypothesis [Szegedy et al., 2014] suggests that the success of AEs is attributed to their capability to shift the data flow from high-probability regions to low-probability regions outside the normal training domain of classifiers. Two types of defense methods: adversarial training and adversarial detection have been proposed based on the hypothesis of data flow patterns. Initially, the adversarial training [Goodfellow et al., 2015] is introduced, in which researchers incorporate AEs into the training data to broaden the training data flow, thereby enhancing the robustness to AEs. Subsequently, adversarial detection emerges, aiming to extract manifold features from inputs to differentiate between benign and malicious inputs and subsequently reject the latter. For the reason that adversarial training leads to a notable decrease in classification accuracy [Tsipras et al., 2019] and generalization ability [Laidlaw and Feizi, 2019], adversarial detection is widely adopted, as it does not weaken the original performance of models.\nCurrently, many detection methods have been proposed and are mainly divided into mathematical statistical and variational analytical categories. Mathematical statistics methods, like Kernel Density and Bayesian Uncertainty (KDBU) [Feinman et al., 2017] and Local Intrinsic Dimensionality (LID) [Ma et al., 2018], calculate density information based on the intermediate layer distribution of input data. Furthermore, [Lee et al., 2018] introduces the Mahalanobis distance to improve the assessment of high-dimensional data, while Joint statistical Testing across DNN Layers for Anomalies (JTLA) [Raghuram et al., 2021] incorporates a meta-learning system to combine several existing methods. However, these mathematical-statistical methods, which are static and lacking considering dynamic characteristics of inputs over the input transformations, perform weakly in the generalization domain.\nVariational analytical methods calculate the variations of the outputs in the target model to differentiate AEs and NEs when the input is transformed by rotation, flip, shift, etc. Feature Squeezing (FS) [Xu et al., 2018] involves bit reduction and shifting inputs, while Lightweight Bayesian Refinement (LIBRE) [Deng et al., 2021] utilizes Bayesian network structures to extract the uncertainty of inputs and Expected Perturbation Score (EPS) [Zhang et al., 2023] introduces a dif-"}, {"title": "fusion model to augment input diversities. However, these methods only empirically demonstrate the difference of output variations on both AEs and NEs in the target model, lacking theoretical support, resulting in unreliability to new attack strategies such as Square [Andriushchenko et al., 2020].\nTo solve these issues, we construct the Dynamically Stable System(DSS) based on the Lyapunov stability theory. Our system falls under the variational analytical category, as the stability module of the system involves disruption and restoration actions. In the stability module, we iteratively disrupt and restore the inputs to obtain dynamic stability features. Normal and noisy examples, when subjected to the disruption and restoration process, tend to maintain stability, as shown in Fig.1 (a) and (b). On the contrary, AEs exhibit a tendency to diverge as the system progresses and will be detected based on this tendency, as shown in Fig.1 (c). In the monitor module, we distinguish AEs from NEs based on different stability features provided by the stability module.", "content": null}, {"title": "Our main contribution can be summarized as below:", "content": "Firstly, we prove that the process of adversarial examples generation can be considered as a Lyapunov dynamic stable system, and we propose an example stability mechanism, in which a novel control term is proposed to be involved in the generation. The proposed control term is proven to ensure the normal examples achieve the original examples stability. That is, with the control term, the stability of outputs of normal examples will be few changes to that of the original examples, while the stability of outputs of adversarial examples will achieve high alternations to that of the original examples.\nThen, based on the proposed example stability mechanism, we propose a Dynamically Stable System, which can effectively detect the adversarial examples according to the changes in the stability of our original examples. The proposed DSS consists of a stability module and a monitor module. In the stability module, the input original example is disturbed by the malicious gradient information with the proposed control term and then we introduce an inpainting model to repair the disturbed examples. In this way, the original normal examples will achieve similar stability to the repaired examples, while the adversarial examples will achieve much different stability to the repaired examples. Hence, the monitor module can detect the adversarial examples according to the"}, {"title": "changes in the stability of input original examples.", "content": "Lastly, extensive experiments are conducted to demonstrate the effectiveness of our proposed DSS in three benchmark datasets(MNIST, CIFAR10, and CIFAR100). The evaluation results show that our DSS outperforms the best results, i.e., achieving the average AUC of 99.83%, 97.81% and 94.47% respectively, in comparison with 7 existing adversarial detection methods. Furthermore, the evaluation in terms of generalization, intensity, ablation and sensitivity has been conducted as well and the results demonstrate the outstanding performance of our DSS compared to other methods."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Adversarial Attacks", "content": "Adversarial attacks add artificial malicious perturbations invisible to human eyes on inputs, causing misclassifications of models. [Goodfellow et al., 2015] firstly proposed the FGSM method by loss gradient descents according to the linear ability assumption of neural networks. Based on the linear assumption, BIM [Kurakin et al., 2017] utilized an iterative loop strategy with a smaller stepsize based on FGSM; PGD [Madry et al., 2018] optimized the BIM by adding random initializations at each loop; APGD and AutoAttack [Croce and Hein, 2020] utilized the adaptive learning rate and a linear combination of several attacks, respectively. Besides the gradient-based attacks, DeepFool [Moosavi-Dezfooli et al., 2016] calculated the classification boundaries to decide the attacking directions; CW [Carlini and Wagner, 2017] optimized the loss function and input space domains. Square [Andriushchenko et al., 2020] accomplished the black-box attack with thousands of queries. These attacks span various attack domains and are subsequently utilized to generate AEs."}, {"title": "2.2 Adversarial Defenses", "content": "Existing defense mechanisms primarily fall into two categories: adversarial training and adversarial detection. The prevailing strategy in adversarial training involves incorporating AEs into the model training dataset. For instance, [Goodfellow et al., 2015] initially introduced AEs into the training set, aiming to enhance the robustness of the model by learning the characteristics of AEs. Additionally, both [Pinot et al., 2020] and [Dong et al., 2022] employed random normalization layer modules to improve the model robustness. Meanwhile, [Li et al., 2022] conducted adversarial training with constrained gradients to avoid overfitting. However, the model generalization ability is quite limited due to the addition of AEs into the training set.\nIn the detection domain, [Feinman et al., 2017] firstly utilized intermediate layer embeddings to construct probability density statistics based on training data. [Ma et al., 2018] extracted the local distance features and [Lee et al., 2018] imported Mahalanobis distance into the detection process. Furthermore, [Papernot and McDaniel, 2018; Dubey et al., 2019; Abusnaina et al., 2021] utilized the latent data neighbors to defend the attacks, while [Raghuram et al., 2021] incorporated feature statistics between layers to detection. However, these statistics-based methods lack consideration of"}, {"title": "the dynamic features of inputs. While [Roth et al., 2019; W\u00f3jcik et al., 2021] utilized noise addition to accomplish the detection task. [Tian et al., 2021] applied wavelet transformation, converting features from the spatial domain into the frequency domain. And recently, [Zhang et al., 2023] utilized a diffusion model to augment the features of AEs. These variational analytical methods considered the dynamic features based on empirical experience, resulting in unreliable performance in unknown attacks.\nOur proposed method(DSS) falls into the variational analytical category, as the constructed dynamically stable system involves disrupting and restoring inputs. With the system theoretically showing the differences between AEs and NEs, our method exhibits good performance in AEs detection.", "content": null}, {"title": "3 Preliminary", "content": null}, {"title": "AEs Generation Process:", "content": "For a deeper comprehension of the AEs, we take the PGD [Madry et al., 2018] attack as an example to show the AEs generation process, as follows:\n$Xt+1 = Clipx,\\epsilon(Xt+\\alpha\\cdot sign(\\nabla_xJ(\\theta, xt, yo)))$  (1)\nIn the equation (1), x0 = x is the initial condition while x' = xn is the ending result where n is the total attacking times, \u03b8 is the parameter of the target model f(.), and yo is the true label of x. As Eq.(1) shows, the input xt at t loop is modified by the gradient descents of loss function J(\u00b7) with a step size."}, {"title": "The Lyapunov Stable Theory:", "content": "For a system to be Lyapunov stable, it should be able to maintain stability at xo under various initial conditions. The stability of the system is represented by a function V(x), and if V(x) > 0, V(x) \u2264 0, the system is considered Lyapunov stable while V(x) represents the derivative of V(x) with respect to perturbations."}, {"title": "4 Proposed Method", "content": "In Section 4, we initially present our motivation. Then, the example stability mechanism is proposed and proven, and the dynamic stable system is proposed and conducted based on the proposed example stability mechanism. Finally, the running process of the proposed DSS to detect AEs is presented."}, {"title": "4.1 Motivation", "content": "Currently, detection methods can be broadly categorized into two types: those based on data statistics, which primarily analyze features from intermediate and logit layers of the model; and those based on variational analysis, which induce transformations to the inputs. Statistics methods calculate the distance between inputs and prior data, easily leading to overfitting to the prior data and resulting in weak performance in generalization. Additionally, existing variational analytical methods leverage the dynamic features of inputs, but these methods are conducted based on empirical experience and lack theoretical evidence, resulting in unreliable performance in unknown attacks.\nTherefore, leveraging the Lyapunov stability theory, we establish a dynamically stable system to generate the stability differences between NEs and AEs. Furthermore, we monitor the dynamical stability features of inputs through our monitor module to accomplish adversarial detection, providing a"}, {"title": "complement to previous methods which are based on empirical experience.", "content": null}, {"title": "4.2 The Example Stability Mechanism", "content": "In this subsection, the example stability mechanism is proposed and proven based on the Lyapunov stable theory.\nInspired by the adversarial examples (AEs) generation process shown in Eq.(1), it can be viewed as a perturbation process of a dynamic system, described as follows:\n$\\begin{cases} x = \\alpha\\nabla_xJ(\\theta, x, Yo)\\\\ Yp = f(x + x \\cdot \\triangle t) \\end{cases}$   (2)\nIn Eq.(2), \u00e0 represents the derivative of x with respect to the iterations, and At represents the iteration interval. With iteratively adding malicious gradient information, the outputs lead to yp \u2260 yo, in which yo and yp represent the true label and the predicted class.\nBased on the dynamic system of adversarial examples generation, the example stability mechanism can be proposed, in which the malicious gradient information VJ(0, x, yp) is used to perturb the inputs and a novel control term u(t) is proposed to maintain the stability of normal original examples, which can be represented as:\n$\\begin{cases} x = \\alpha\\nabla_xJ(\\theta,x,yp) + u(t)\\\\ yp = f(x + x \\cdot \\triangle t) \\end{cases}$  (3)\nAdditionally, in our example stability mechanism, the quadratic Lyapunov function (i.e., V(x) = (x \u2212 xo)2) is introduced to represent the stability status of input examples. Obviously, when x is equal to xo (i.e., x = x0), the stability status of example x will be 0 (i.e., V(x) = 0), and when x is not equal to xo (i.e., x \u2260 xo), the stability status of example x will be larger than 0 (i.e., V(x) > 0) satisfying the initial Lyapunov stable condition V(x) > 0. What's more, to ensure the stability of x at xo, there should also be V(x) \u2264 0, i.e.,\n$\\begin{aligned} V(x) &\\leq 0 \\\\ &\\Rightarrow 2(x - xo) x \\leq 0 \\\\ &\\Rightarrow 2(x - xo) \\cdot [\\alpha\\nabla_xJ(\\theta,x,yp) + u(t)] \\leq 0 \\end{aligned}$ (4)\nDue to the uncertainty of 2(x - xo) as it represents the image disturbance, it is necessary to satisfy $\\alpha\\nabla_xJ(\\theta,x,y) + u(t) = 0$ to ensure V(x) \u2264 0. Hence, we set u(t) = -$\\alpha\\nabla_xJ(\\theta, x, yp)$ to satisify V(x) = 0.\nTherefore, in our example stability mechanism, the proposed control term u(t) is set as -$\\alpha\\nabla_xJ(\\theta,x, yp)$, i.e., u(t) = -$\\alpha\\nabla_xJ(\\theta,x,yp)$. By doing this, the control term u(t) can make a normal example x stable at the original example xo, i.e., x = $\\alpha\\nabla_xJ(\\theta,x,yp)$ \u2013 $\\alpha\\nabla_xJ(\\theta, x, yp)$ = 0. However, due to the adversarial noises are analogous to Gaussian noises for u(t), the u(t) will see the label of x' as the original label yo while the perturbation is based on the malicious label yp \u2260 yo, resulting in x' not stable at the initial status, i.e., x' = $\\alpha\\cdot\\nabla_{x'}J(\\theta,x', yp)$ \u2013 $\\alpha\\nabla_{x'} J(\\theta, x', yo)$ \u2260 0. Hence, adversarial examples will diverge, thereby deviating from the initial point."}, {"title": "4.3 The Dynamically Stable System (DSS)", "content": "In this section, we introduce our Dynamically Stable System(DSS), which comprises the stability and monitor modules. In the stability module, the repeated disruption and restoration actions are performed to present the stability of the inputs, while the monitor module distinguishes between AEs and NEs based on the different stability, as shown in Fig.2."}, {"title": "The Stability Module", "content": "The stability module comprises two actions: disruption and restoration. In the disruption action, we disrupt critical information of inputs based on the gradient descents. In the restoration action, we repair the disrupted parts using the remaining information from the inputs.\nIn the disruption action, considering that the success of AEs is attributed to the addition of harmful noises on NEs, we contemplate disrupting these types of noises. Previous works [Wang et al., 2021; Zhang et al., 2022] have provided that the saliency map of inputs could be calculated by the gradient descents of the loss. Moreover, considering that AEs alter the predicted class and the loss is calculated based on the predicted class, we focus on disrupting the elements that contribute to the minimum loss. Assuming the system is at the tth loop, we calculate the gradient Gt of the loss as follows:\n$Gt = \\nabla_xL_{CE}(Xt-1, f(xt-1))$  (5)\nIn Eq.(5), f(xt-1) represents logits of inputs by the classifier f(), and LCE represents the cross entrophy loss function. With the gradient Gt, we sort the values of Gt and select the value located at the disrupting ratio r 3% position as the disrupting threshold 7. Furthermore, values in Gt greater than 7 are retained, while values less than 7 are disrupted. Then the disrupting matrix Mt can be calculated as follows:\n$Mt,i,j = \\begin{cases} 1, if Gt,i,j > \\tau \\\\ 0, if Gt,i,j \\leq \\tau \\end{cases}$  (6)\nIn Eq.(6), the notations i and j denote the row and column coordinates of the disrupted pixel, respectively. With the disrupting matrix Mt, we get the disrupted data Mtxt-1.\nIn the restoration action, we employ an inpainting model to repair the disrupted data Mtxt\u22121. The inpainting model g(\u00b7) takes two inputs: the disrupted data Mtxt-1 and the disrupting matrix Mt. The disrupted data Mtxt-1 provides initial information for the model, while the disrupting matrix Mt identifies information positions to take. The outputs of the model are generated examples \u00eet with the same shape as inputs, and the process is as follows:\n$xt = g(Mtxt-1, Mt)$ (7)"}, {"title": "Algorithm 1 Dynamically stable system", "content": "Input: Clean set Xclean, noisy set Xnoisy, adversarial set Xadv\nParameter: Classifier f(\u00b7), inpainting model g(\u00b7), length of set N, loop times n\nOutput: Dynamically stable feature matrices SP, \u015cP in pixel-wise, SL, SL in logit-wise\nUsing the generated examples \u00eet, we can calculate the outputs at the tth loop as follows:\n$xt = (1 - M_t) \\cdot Xt + M_t\\hat{xt\u22121}$  (8)\nWith the composed examples xt, we can proceed to the (t + 1)th loop to calculate xt+1.\nOur stability module is built on the disruption and restoration actions described above. The state-space equation of our module is as follows:\n$\\begin{cases} \\triangle Xt = Xt - Xt\u22121 = (1 \u2212 Mt)[g(Mtxt\u22121, Mt) \u2013 Xt\u22121] \\\\ Lt = L_{CE}(xt, f(xt)) \u2013 L_{CE}(xt\u22121, f (xt-1)) \\end{cases}$  (9)\nIn Eq.(9), the variables at the tth loop of the dynamically stable system are presented, in which Lt represents the state variable of the system. Taking the initial inputs xo = x, we iterate the disrupting and restoring actions total of n times, with the outputs of each iteration serving as the inputs for the next one, getting the module outputs {X1,X2, ..., Xn} and {21, 22, ..., n}."}, {"title": "The Monitor Module", "content": "Through the stability module, we obtain the states {X1, X2, ..., Xn} and {1,2,..., \u00cen} of the input after repeated disruption and restoration. In the monitor module, the system consists of two parts: the stability extractor and the detector. In the stability extractor, it extracts the stability as follows:\n$\\begin{cases} lt,1,p = ||Xt - Xo||p\\\\ \\hat{lt,1,p} = ||\\hat{It} - Xo||p \\end{cases}$ (10)\nIn the formula above, p denotes the chosen norm type. lt,1,p and \u00cet,1,p denote detecting features at tth loop in pixel-wise,"}, {"title": "while lt,2,p and It,2,p denote detecting features in logit-wise which could be seen as below:", "content": "$\\begin{cases} fl_{t,2,p} = ||f(xt) \u2212 f(x0)||p\\\\ (\\hat{I}_{t,2,p} = || f(\\hat{xt}) \u2212 f(x0)||p \\end{cases}$  (11)\nWhat's more, with stability features lt,1,p, \u00cet,1,p, lt,2,p, It,2,p extracted, we utilize a detector based on logistic regression to accomplish the detection. The detector will output a score, where the score closer to 1 indicates a higher confidence that the input is an adversarial example, while the score closer to 0 suggests a higher likelihood that the input is a normal example."}, {"title": "4.4 The Detection Process of our DSS", "content": "With our DSS introduced, we can get the whole detection process as Algorithm 1 shows. Through the system, we obtain the stability features lt at different iterations and then accomplish detection tasks based on logistic regression.\nIn the algorithm 1, line 1 sets i = 1 to record the input number. Line 2 selects one example x as the input for the system from the sets of clean, noisy and adversarial examples. Line 3 sets t = 1 as the initial iteration number, with XO = x as the input of the first iteration. Line 4 specifies the total iterations of the system as n. Line 5 inputs xt-1 to the classifier f(.) and backpropagates the classifier logits to calculate the disrupting matrix Mt by equation (6). Line 6 inputs the disrupted data Mtxt-1 and mask Mt into the inpainting model g(\u00b7) to generate the generated example \u00eet by equation (7). Line 7 calculates the composed example xt with the equation(8). Line 8 extracts the stability features lt,1,p, \u00cet,1,p, lt,2,p, and \u00cet,2,p by the equations (10) and (11). Line 9 records the stability features into the matrices SP, \u015cP, SL, and \u015cL. Lines 10-13 end the loop of the system and the inputs traversal, respectively. Using these features, we train a logistic regressor to perform detection on the inputs."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Experiment Setup", "content": null}, {"title": "Datasets", "content": "In this paper, three benchmarks are utilized to verify the effectiveness of our proposed method, namely MNIST [LeCun et al., 1998], CIFAR10 [Krizhevsky et al., 2009] and CIFAR100."}, {"title": "Classifier Models", "content": "For CIFAR10 and CIFAR100, we select VGG19 and ResNet50 networks as validation models. For MNIST, the LeNet network is chosen."}, {"title": "Inpainting Models", "content": "Due to the irregular positions that need restoring, existing inpainting models primarily restore information at fixed locations. Therefore, we use the partial convolutional layers [Liu et al., 2018b; Liu et al., 2018a] due to their capability to restore irregular positions."}, {"title": "Attacks Setup", "content": "We employ six types of attacks for evaluation, namely FGSM, PGD, CW, DeepFool, Square, and AutoAttack. The attack Square is a black-box attack while the others are gray-box attacks. Additionally, they can be divided into gradient-based(FGSM, PGD, AutoAttack) and optimization-based(the others). In terms of norm type, we categorize them as l2(CW and DeepFool) and l\u221e (other four attacks). For parameter settings, we choose attack intensity \u20ac = 8/255 for CIFAR10 & CIFAR100, \u20ac = 0.3 for MNIST, step size a = 1, CW optimization steps = 500, with the remaining parameters set to the default by the torchattacks library."}, {"title": "Metric", "content": "Like the majority of existing works, we utilize the area under roc curve(AUC) [Davis and Goadrich, 2006] as the performance metric to compare these methods."}, {"title": "Baseline", "content": "In our experiment, the examples conclude three parts: clean examples, noisy examples, and adversarial examples. The inclusion of noisy examples is due to the consideration that noisy data is commonly present in the natural environment and typically does not pose misclassification to classifier models. In the experiment, we select examples that the classifier successfully classify but fail in corresponding AEs. For noisy examples, random noises are added to clean examples, with the same intensity of AEs. Then, the detection features extracted by existing detection methods are split into an 80-20 ratio, with 80% of the features used to train a logistic regressor and 20% used for testing the detection performance.\nWe compare our proposed method with seven other detection methods, namely KDBU [Feinman et al., 2017], FS [Xu et al., 2018], LID [Ma et al., 2018], MD [Lee et al., 2018], JTLA [Raghuram et al., 2021], LIBRE [Deng et al., 2021], and EPS [Zhang et al., 2023]. The parameters of these methods are either taken from the respective papers or source codes. And for the EPS detection method, it is only compared on CIFAR10 since the method requires a diffusion model, but the authors only provided the diffusion model for CIFAR10."}, {"title": "5.2 Detection Performance Results", "content": "In this subsection, we compare our method with seven other detection approaches, quantifying the detection performance by the AUC metric. As shown in Table.1, we present the detection results of the eight methods on CIFAR10 and CIFAR100, with both VGG19 and ResNet50 as target models, respectively. In Table.1, the bold values represent the best detection performance; the underlined ones represent the second-best detection performance; and subscript values denote the detection performance combined with our method.\nFrom Table.1, it is evident that our DSS method outperforms other seven detection approaches. Our approach achieves AUC values of 97.81% and 94.47% on CIFAR10 and CIFAR100, respectively, surpassing the SOTA of other methods with AUC values of 91.10%(MD) and 93.49%(MD). Additionally, to demonstrate the compatibility of the features extracted by our method, we combine our features with those of others. The results of this combined manner are presented as the subscript values in Table.1. It shows that our method exhibits good compatibility with other approaches after feature combination, demonstrating a general upward trend in AUC values."}, {"title": "5.3 Detection Performance Comparison Under Different Attack intensities", "content": "In this subsection, we demonstrate the detection capabilities of our method and other approaches under different attack intensities. We take PGD(gradient-based) and CW(optimization-based) attacks as examples, conducted under the CIFAR10 & VGG19 condition. For the PGD(l\u221e norm) attack, the attack strength \u20ac ranges from [1/255, 2/255, 4/255, 6/255, /8/255, 12/255, 16/255], and the step size a = ; for the CW(l2 norm) attack, the optimization steps ranges from [10, 20, 50, 100, 500, 3000, 10000]."}, {"title": "From Fig.4, we observe that under PGD(l\u221e norm) attack, our method performs slightly worse than FS at \u20ac = 1/255, but it outperforms the other methods; when \u20ac \u2208 [2/255,4/255,6/255], our method exhibits the best detection performance; when \u20ac \u2208 [8/255, 12/255, 16/255], our method, along with FS, KDBU, LID, and MD, demonstrate outstanding detection performance. Under CW(l2 norm) attack, due to the large span of our x-axis, we use a logarithmic scale. LID and MD methods decrease notably with increasing optimization steps, while FS, KDBU, JTLA, and LIBRE exhibit weak detection performance at lower optimization steps. In contrast, our method's detection performance remains relatively stable and consistently high as the optimization steps change, indicating that the dynamic features introduced in our method exhibit more stable detection", "content": null}, {"title": "performance.", "content": null}, {"title": "5.4 Generalization Study", "content": "Considering that we often face the challenges of unknown attacks in the real world, therefore, we assess the generalization performance of our method by using a detector trained on FGSM attack to detect other attacks. Besides, to validate the effectiveness of our method features in pixel-wise and logit-wise, we conduct the generalization study on these two types of features labeled as P(pixel-wise) and L(logit-wise), respectively."}, {"title": "As shown in Table.3, under the CIFAR10&VGG19 condition, our proposed method achieves a generalization accuracy of 91.07% at the logit level, slightly lower than the generalization performance of FS 91.14%. In contrast, the remaining methods, KDBU, MD, JTLA, LIBRE, and EPS, exhibit a generalization AUC of less than 0.5. Additionally, combining our method with FS and JTLA, which involves the fusion of static and dynamic features, results in an average generalization AUC of 95.45%.", "content": null}, {"title": "5.5 Ablation Study", "content": "We utilized logit-wise and pixel-wise features in the previous subsection to accomplish adversarial detection tasks. To validate their effectiveness, in this subsection, we conduct adversarial detection at pixel-wise and logit-wise levels, separately. Similar to the experiments in Table.1, we perform detection on VGG19 and ResNet50 architectures on CIFAR10 and CIFAR100. As shown in Fig.5, features at the pixel-wise level demonstrate better detection performance against FGSM attack, while features at the logit-wise level exhibit superior detection performance against PGD, CW, DeepFool, Square, and AutoAttack attacks."}, {"title": "5.6 Sensitivity Study", "content": "For the reason that our DSS method involves a disruption phase, the disrupting ratio r is crucial in our method. In this subsection, under the CIFAR10 & CIFAR100 of ResNet50 condition, we validate the detection performance of our experiments under different disrupting ratios to further affirm the effectiveness of our approach."}, {"title": "As shown in Fig.6, with the increase of the disrupting ratio, AUC exhibits an initial rise and then a decreasing trend for FGSM, CW, DeepFool, and Square, while it shows a tiny changes in PGD and AutoAttack. Considering the performance of our dynamic features across these six attacks, we choose the disrupting ratio r = 3% in our system.", "content": null}, {"title": "6 Conclusion", "content": "To tackle the generalization issues in existing methods, we construct a dynamically stable system from the perspective of Lyapunov stable theory. By continuously introducing artificial disruption, we extract the inputs stability features to accomplish the detection task. We compare our method with seven different detection approaches, and across various model structures and datasets, our method demonstrates optimal detection performance, up to 99.83%, 97.81% and 94.47% average AUC across MNIST, CIFAR10, and CIFAR100. Additionally, intensity, generalization, ablation, and sensitivity studies have been conducted, demonstrating the SOTA performance of our method."}]}