{"title": "Development of CNN Architectures using Transfer Learning Methods for Medical\nImage Classification", "authors": ["Ganga Prasad BASYAL", "David ZENG", "Bhaskar P. RIMAL"], "abstract": "The application of deep learning-based architecture has seen a\ntremendous rise in recent years. For example, medical image\nclassification using deep Learning achieved breakthrough\nresults. Convolutional Neural Networks (CNNs) are\nimplemented predominantly in medical image classification and\nsegmentation. On the other hand, transfer learning has emerged\nas a prominent supporting tool for enhancing the efficiency and\naccuracy of deep learning models. This paper investigates the\ndevelopment of CNN architectures using transfer learning\ntechniques in the field of medical image classification using a\ntimeline mapping model for key image classification\nchallenges. Our findings help make an informed decision while\nselecting the optimum and state of the art CNN architectures.", "sections": [{"title": "INTRODUCTION", "content": "Computer-aided diagnosis and detection (CAD) have\nbeen predominantly applied and researched in the\nbiomedical and medical informatics domain for a long\ntime. Historically the CAD systems were used by\npathologists and radiologists in the diagnosis of various\ndiseases through the analysis of medical images [1].\nHaving huge dependence over the CAD system for a\nlonger period of time allowed CAD systems to set\nmilestones in the field of medical imaging. The\nresearchers have a mixed review on the technology front.\nSome suggest the methods are efficient enough, whereas\nfew questioned the accuracy of it [1]. Deep learning is\nanother field that successfully emerged in medical image\nanalysis and is widely implemented through its methods\nand techniques [2]. Deep learning has seen a tremendous\nincrease in its adaptation and application into the medical\ninformatics domain. Having a widespread presence and\nshowing its potential in the last two decades, deep\nlearning has been highly researched and widely applied in\nthe industry. Medical data generated in the form of\nclinical reports, patient charts, and diagnosis reports\nresult in an exponential surge in medical data. Deep\nlearning can process big data and analyze them efficiently\nwhich proved to be an advantage for its rapid and\niterative implementation. Deep learning has three key\nmethodologies, namely, Artificial Neural Network\n(ANN), Recurrent Neural Network (RNN), and\nConvolutional Neural Network (CNN). ANN and RNN\nhave been applied majorly in text and number data\nwhereas, CNN is applied for vision or image data [3].\nCNN gained its attention right from its inception by\nwinning one of the most prestigious challenges for image\nclassification ILSVRC in 2012. It managed to win the\ncompetition by a huge margin as compared to the other\ncompetitors [2]. Later entering into the biggest medical\nimage classification challenge Multimodal Brain Tumor\nSegmentation Challenge (BraTS) laid the foundation of\nCNN into the machine vision domain. Afterward, many\nother CNN models came into the field (many of which\nsets the industry standards for others) over time. Medical\nimaging is a very intriguing field where models are\ntrained to read and analyze images.\nMedical image analysis is one of the many application\nareas of CNN where it is applied for various tasks such as\nclassification, segmentation, detection, and analysis [4].\nThere are many types of medical images known as image\nmodalities. These include Magnetic Imaging Resonance\n(MRI) [4], Computed Tomography (CT) [5], X-Ray [6].\nTransfer learning on the other had can be defined as the\ntransfer of knowledge gained by one architecture\ntransferred to another domain architecture [7]. Transfer\nlearning provides the advantage of acquiring the domain\nknowledge for achieving the greater generalization of the\nmodel, as the training of the model is done with different\nsets of data this enables the model to better generalize the\ndata being tested with a different domain.\nFurthermore, it seems very difficult for a researcher to\ndecide which CNN architecture to adopt when it comes to\nmedical image classification and the task becomes even\nmore tedious when we have several imaging modalities."}, {"title": "DEVELOPMENT OF CONVOLUTIONAL NEURAL\nNETWORKS", "content": "Application of various (CNN) architecture is specific to\nthe task. For example, the study in [6] is a retrospective\nstudy where the author compared the results of the CNN\nmodel with and without transfer learning and then\ncompared it with a radiologist. The author argues that the\nresults generated by the implementation of VGG-16 CNN\narchitecture provided excellent results and match the\nhuman-level performance. Another study in [8] describes\nCNN architectures using transfer learning with ensemble\nclassifier, the author claims that ensemble is a very\nefficient method in multi-categorical classification and\ndetection on color fundus images, where images have\nclassified on 16 different species level. A similar method\nadopted in research in [5] instead of ensemble author\nintegrated two CNN architecture making a pair together,\nnamely, Inception-ResNet and AlexNet-GoogleNet,\nResNet with transfer learning performed best among the\nrest. ResNet and Inception V3 were applied in [9] for bird\nspecies classification with the implementation of a two-\nstage training model achieved an accuracy of 55.67%,\nwhich was one of the best results during the bird species\nchallenge of 2018.\nTransfer learning is applied in various CNN architectures,\nand almost every case, it enhanced the results as\ncompared to the model trained without transfer learning\n[9], [10], and [11]. Studies in [6] and [12] emphasize that\ntransfer learning increased the accuracy of model\nperformance even having the issue of low sample size.\nThe U-Net architecture applied in [13] and [12] for the\nsegmentation of brain CT images performed very well\nover the 3D images. The U-Net is efficient in both 2D\nand 3D medical image segmentation."}, {"title": "MAPPING THE DEVELOPMENT OF CNN\nARCHITECTURE THROUGH IMAGENET\nCHALLENGE", "content": "ImageNet Large Scale Visual Recognition Challenge\n(ILSVRC) is considered one of the top-ranked image\nclassification challenge. The Challenge consist of 10\nmillion images for training with over ten thousand\nannotated image (labeled data) categories for the\nclassification and validation and the test dataset consists\nof over 150 thousand images without labeling [14]. The\nobjective of the challenge is to classify the unannotated\nnatural images into one of the categories of image labels\nprovided at the time of the training dataset. There is three\nkey classification task is given at the challenge: 1)\nClassification \u2013 The output of the model should provide\ntop 5 object categories based on the confidence of the\nclassification, 2) Classification with Localization \u2013 Along\nwith classification as in task one the model should also\nproduce five class labels for each image has the bounding\nbox for each image the result should be matched with the\nactual ground truth, and 3) Fine-grained Classification\nThis is even a step further in classification, which means\nsubcategory classification i.e., classifying the breed of the\ndog inside the overall dog category.\nIn the year 2012, research by Krizhevsky et al., [15]\nreserved the top spot by introducing the groundbreaking\nCNN model architecture in the image recognition field,\nnamed after the author's first name, i.e., AlexNet. The\narchitecture consists of five convolutional layer and three\nconnected layers, whereby the input size of the images is\n224x224x3, which then divided into two different layers\neach representing one GPU (graphics processing unit).\nThe processing of these layers is independent of each\nother throughout the convolution process and merge in\nthe final dense layer. To overcome the issue of overfitting\nauthor utilized two methods 1) Data Augmentation and 2)\nDropout with (0.5) rate. The author advocated due to the\ninception of GPU the model was able to hold the memory\nfor 60 million parameter and 650 thousand neurons, and\nwith this speed, the model was able to achieve the top\nrank with the lowest error rate of 16% winning by a huge\nmargin over the second-place holder with 26.2% error\nrate. In 2013, a team of researchers from New York\nUniversity proposed ZFNet inspired by the first name of\nthe author. It was an alteration of the CNN proposed by\n[15]. The performance of the proposed model was\nslightly better than the last year winner and winning over\nby 4% less error rate with 12%. Another notable CNN\narchitecture proposed by [16] deconvoluted the layer\nresulting in each layer decomposing the image\nhierarchically, and reconstructing it again with sparsity\nconstraints."}, {"title": "MAPPING THE DEVELOPMENT OF CNN\nARCHITECTURE THROUGH BRATS IMAGE\nCLASSIFICATION", "content": "The brain tumor image segmentation (BraTs) challenge is\na renowned name in the field of medical image\nclassification used Magnetic Resonance Imagining (MRI)\nscan, which is publicly available for high-quality research\n[24]. Starting from the year 2012 in association with the\nmedical image computing and computer-assisted\nintervention society (MICCAI) conference where a\nresearcher from all around the world has been given a\nreal-world medical image dataset with a problem\nstatement of brain MRI segmentation. Classification or\nsegmentation of brain tumor tissues broadly classified\ninto malignant considered as high-grade tumor and\nbenign tumor considered as low-grade tumor [25]. That\ncan be grouped into three key categories of segmentation:\na) manual segmentation, b) semi-automatic segmentation,\nand c) automatic segmentation methods [26]. CNN\narchitecture excels in the latter two methods as neither it\nrequires large training data nor completely manually\nannotated data.\nIn the year 2012, the challenge took place the first time\nand had received several submissions ranging from\nresearchers from prestigious universities to medical\nhospitals and healthcare organizations. There was no live\nconference submission using CNN methods at the time\ndue to the focus of the challenge on multi-atlas labeling\nrather than the actual segmentation as a whole [27]. The\ntechniques such as Random Forest (RF) and Logistic\nregression were used those requiring the manual\nannotation. But after the challenge attempts had been\nmade to analyze the dataset using CNN methods on the\n2013 and 2014 challenge [28],[29], where the very initial\nCNN model was used. The initial models used were\nshallow such as research by [28] implemented 5 layers in\nwhich the first and last layers were the input and output\nlayers, and the middle three-layer included two filter\nlayers and one max-pool layer. The later version of CNN\nwas much advanced, and deeper having many hidden\nlayers. Due to the excellent performance of CNN-based\narchitecture in 2014, the adoption of CNN based model\nincreased in 2015 and major participates have used the\nCNN model along with traditional manual annotation\nmethods such as RF and logistic regression (LR). The\ntop-ranked dice score was achieved by Havaei et. al [30].\nThe author applied Input Cascade CNN based on 2D\nCNN model architecture with entails two pathways\ncovering the small details of the images along with the\nlarger context. The performance of CNN architecture was\nbetter than the winner of last year.\nWith 18 submissions for the BraTs 2016 competition, the\nbar had been rising and the number of participations\nmade during the year had also reached more than 150%\ncompared to previous years' submission. Though having\na higher participation rate still there are participations\nhaving traditional methods such as RF and support vector\nmachines (SVM) [31]. With more than half of the\nsubmission having some CNN, the application makes it\nthe best choice for the researcher and participants. CNN\narchitecture introduced by Kamnitsas et al. [32] had\nperformed exceptionally well and this is the first time\nwhen a 3D CNN architecture had been introduced in\ncompetition and won it. Another notable contribution was\nfrom Randhawa et al in which they presented a deeper\nversion of 2D CNN having the advantage of the choice\nover the loss function which uplifted the model\nperformance from earlier versions [33].\nThe year 2017 had seen exponential growth in the\nnumber of participants researching up to over 50\nsubmissions [34]. The winner of the competition,\nKamistsas et al. [35], introduced an ensemble model\nEMMA, comprises of three separates model (U-Net,\nDeepMedic and Fully Convolutional Neural Network\n(FCN), using transfer learning method and trained on\ncompletely different data the model achieved best results\nthrough averaging the output of all three models to get a\ngeneralized output. EMMA performed very well yielding\na dice score of .90 enhanced, .82 Whole, .75 Core.\nRunner up for the competition was the Cascaded\nAnisotropic CNN model introduced by [36] achieving the\nDice score of 0.764 enhanced, 0.897 whole, and 0.825\ntumor core. The CNN models used in the challenge were\nvery deep having layers as deep as 26 [34].\nFurthermore, the succession of the challenge attracts\nmore participants in the year 2018 with numbers\nexceeding 60, participants from all over the world shown\ntheir enthusiasm, and 50 plus submission had utilized\nCNN architecture as their choice for the classification and\nsegmentation, looking at the model adoption U-Net\nseems to be the unanimous choice for the researcher as\nmore than half of the researcher used U-Net either\nstandalone or in combination with other CNN\narchitectures [37]. The highest Dice scores were achieved\nby Myronenko et al. [38]. The author introduced a\nsemantic segmentation network where the input image is\nreconstructed with the help of auto-encoder which results\nin the regularization of decoder resulting in efficient\nmodel requiring less training samples. The Dice scores\nreported as the model output were 0.823 enhanced\ntumors, 0.91 whole tumors, and 0.866 tumor core.\nInclusion of modified version of CNN architecture such\nas ResNet [38], [39] and DenseNet [20], [40]. The\nrunner-up for the challenge utilized a modified version of\nthe U-Net model, author added region-based training\nalong with minor tweaks in postprocessing and loss\nfunction computation through which they were able to\nachieve a dice score of 77.88 enhanced, 87.81 whole, and\n80.62 tumor core.\nFig.1 below represents the average Dice Score with the\nEnhanced tumor, wide tumor, core tumor respectively.\nLooking at the graph we can ascertain that the graph has\nan upward trend after the year 2013 which means the\nscore has improved every year as compared to the\nprevious year. The difference in performance between the\nyear 2012 and 2013 is due to the newly introduced 2D\nCNN which got improved year after year resulting in the\nbest choice for medical image classification."}, {"title": "DISCUSSION AND FUTURE WORK", "content": "ImageNet Challenge being there for a longer period had\nan edge over BraTs for the early introduction of the initial\nCNN model. Looking at the empirical analysis and our\nliterature review we identified that CNN architecture\nsuch as ResNet and U-Net was first introduced to BraTs\nand later their improved version introduced to ImageNet\nChallenge [41],[18]. Furthermore, CNN architecture such\nas AlexNet being introduced to ImageNet first, and later\nbeing applied to medical image challenge [41]. But one\npattern is common despite the platform. These CNN\nmodels got better and better as their newer, and improved\nversions had been introduced.\nState-of-the-Art improved versions of prior CNN models\nsuch as EfficientNet [42], NAS U-Net [43], and NAS V-\nNet [44] are the latest models. Deploying these models to\nthe prior datasets results in achieving astonishing results,\nsurpassing all of the previous models. EfficientNet is the\nimproved version of ResNet and MobileNet, it is trained\non low parameters and yielded very good results [42]. It\ntends to be one of the best models for natural image\nclassification in the current scenario. The model has been\nimplemented only for natural image classification but has\nnot been applied to medical image classification yet.\nNatural Architecture Search (NAS) models such as NAS\nU-Net and V-NAS had achieved promising results in\nmedical image classification surpassing all the previous\nstandards in medical image classification. Based on the\nprior results generated by different ensemble learning\nmodels such EMMA and (CRF+ RF) lead them to win\nthe competitions in past. These ensemble models used\ncontemporary CNN architectures. Our future research\nwork will be focused on deploying the three latest and\nstate-of-the art CNN models i.e. (NAS U-NET, V-NAS\nand Efficient Net). Implementing them in the form of\nmulti-stages followed by averaging the results in the form\nof ensemble learning for the medical image classification."}, {"title": "CONCLUSION", "content": "In our review research, we discussed the fundamentals of\ndeep learning models and transfer learning techniques. To\nthe best of our knowledge, this is a unique study\nattempted to provide a comprehensive review of CNN\narchitectures and of their performance through\nvisualization, encapsulating over a half-decade time\nframe. We structured them in the form of timeline\nmapping development using two of the biggest image\nchallenge competitions. We reviewed some of the\nfoundational CNN models, such as AlexNet, VGG,\nInception (GoogleNet), GBDNet, and ResNet, and\ncompared their performance with their counterparts. Our\nstudy suggests that ensembles of CNN models had\nperformed exceptionally well in both the source and\ntarget domains of image recognition. The inclusion of the\ntransfer learning method along with mechanisms such as\nnormalization and augmentation, can substantially\nincrease the training efficiency and the model\nperformance."}]}