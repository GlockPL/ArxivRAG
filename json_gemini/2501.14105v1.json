{"title": "MedSlice: Fine-Tuned Large Language Models\nfor Secure Clinical Note Sectioning", "authors": ["Joshua Davis", "Thomas Sounack", "Kate Sciacca", "Jessie M Brain", "Brigitte N Durieux", "Nicole D Agaronnik", "Charlotta Lindvall"], "abstract": "Objective Extracting sections from clinical\nnotes is crucial for downstream analysis but\nis challenging due to variability in formatting\nand labor-intensive nature of manual section-\ning. While proprietary large language models\n(LLMs) have shown promise, privacy concerns\nlimit their accessibility. This study develops a\npipeline for automated note sectioning using\nopen-source LLMs, focusing on three sections:\nHistory of Present Illness, Interval History, and\nAssessment and Plan.\nMaterials and Methods We fine-tuned three\nopen-source LLMs to extract sections using\na curated dataset of 487 progress notes, com-\nparing results relative to proprietary models\n(GPT-40, GPT-40 mini). Internal and external\nvalidity were assessed via precision, recall and\nF1 score.\nResults Fine-tuned Llama 3.1 8B outper-\nformed GPT-40 (F1 = 0.92). On the external\nvalidity test set, performance remained high\n(F1 = 0.85).\nDiscussion and Conclusion Fine-tuned open-\nsource LLMs can surpass proprietary models\nin clinical note sectioning, offering advantages\nin cost, performance, and accessibility.", "sections": [{"title": "1 Background And Significance", "content": "Clinical documentation is critical for patient\ncare, facilitating communication across clinicians\nand providing a comprehensive record of patient\nprogress from inpatient to outpatient settings.\nWhile clinical notes often follow semi-structured\nformats, such as SOAP or sectioned templates\n(e.g., History of Present Illness, Family History,\nReview of Systems, Physical Exam, Assessment,\nand Plan), they also contain rich, unstructured\nfree-text narratives documenting a clinician's\ndirect observations and assessments (Podder et al.,\n2023). Though unstructured/semi-structured\nfree text contains valuable clinical information,\nthe variability in formatting between individual\ndocumenting clinicians presents a challenge in\nthe research setting. Manually \u201csectioning\" of\nnotes to find current information is labor-intensive,\nerror-prone, and unsuitable for large-scale data\nanalysis (Sheikhalishahi et al., 2019). Prior efforts\nto automate this process have included rule-based\nheuristics and machine learning models (Denny\net al., 2009; Eyre et al., 2022; Pomares-Quimbaya\net al., 2019); however, these approaches have\nlimited generalizability across diverse note types,\nhospital systems, and clinical domains.\nThe emergence of large language models\n(LLMs) presents a transformative opportunity for\nsection segmentation in clinical documentation\n(Zhou and Miller, 2024). Unlike earlier approaches,\nLLMs are trained on diverse datasets, enhancing\ntheir adaptability to varied formats and institutions\n(Grabar et al., 2020). Successful implementation\nof these methods could enable streamlined work-\nflows, focusing on extracting and analyzing specific\nsections of interest from clinical notes. A previ-\nous study found that proprietary LLMs, such as\nOpenAI's GPT-4, achieved an average F1 score"}, {"title": "2 Objective", "content": "This study aims to develop an automated method to\nextract clinically relevant sections of notes essential\nfor downstream analysis, using a scalable pipeline\ncompatible with local and cloud hardware."}, {"title": "3 Materials and Methods", "content": null}, {"title": "3.1 Dataset", "content": "Clinical notes from three oncology groups (breast,\ngastrointestinal, neurological) were annotated\nby two nurse practitioners (KS and JB). The\nfirst 25 notes from the gastrointestinal group\nwere independently coded to facilitate initial\ndata familiarization and the development of a\ncodebook. Using this preliminary codebook, KS\nand JB independently coded a total of 653 notes,\nidentifying spans related to the history of present\nillness, interval history, and assessment & plan\n(A&P). Due to variability in documentation, the\nhistory of present illness and interval history were\ncombined into a single label, recent clinical history\n(RCH).\nInter-rater reliability was calculated using Jac-\ncard Index (JI) (Grabar et al., 2020). For sections\nwhere the JI between the two annotations exceeded\n80%, the union of the annotations was adopted as\nthe final label. A total of 125 notes did not meet\nthis threshold and were re-coded through group dis-\ncussion involving all annotators and a third-party\nadjudicator (JD). This process resulted in the final-\nized codebook (Appendix A). An additional 494\nnotes were single coded by KS using the finalized\ncodebook, culminating in a dataset of 1,147 clinical\nnotes"}, {"title": "3.2 Baseline", "content": "For baseline evaluation, we tested two rule-based\napproaches: SecTag and the sectioner module from\nMedSpaCy (Eyre et al., 2022; Denny et al., 2009).\nSecTag employs terminology-based rules and\nnaive Bayesian scoring to identify section headers\nin clinical notes, while MedSpaCy, an updated\nversion of SecTag used by the VA in multiple\nstudies (Chapman et al., 2020, 2021), builds upon\nthis methodology. Both tools were adapted for\ncompatibility with our processing pipeline.\nIn addition to these baselines, we utilized a\nClinical-Longformer with a 4096-token context\nwindow (Li et al., 2023), trained with a custom"}, {"title": "3.3 Models", "content": "Five LLMS (GPT-40, GPT-40 mini (OpenAI et al.,\n2024), Llama 3.2 instruct (1B), Llama 3.2 instruct\n(3B), Llama 3.1 instruct (8B) (Grattafiori and et al.,\n2024)) were evaluated for section identification.\nOpenAI models ran on a HIPAA-compliant end-\npoint (Umeton and et al., 2023), while Meta models\nwere run on a virtual machine with a context win-\ndow of 8192 tokens. All used a unified prompt\n(Appendix B); OpenAI models applied function-\ncalling, and Meta models were tested pre and post\nsupervised fine-tuning (SFT) (Wei et al., 2022). Pre\nSFT inference was done with grammar to enforce\noutput structure. Llama models were selected for\nSFT because of their accessibility and widespread\nadoption in clinical informatics research (Nowak\net al., 2025). All fine-tuning and inference was\nperformed on a HIPAA-secure virtual machine\nequipped with an A100 40GB GPU."}, {"title": "3.4 Fine-Tuning", "content": "We performed supervised fine-tuning of the\nLLMs using the Unsloth library (Daniel Han\nand team, 2023). The models were trained us-\ning rank-stabilized LoRA (Kalajdzievski, 2023),\na parameter-efficient fine-tuning method that im-\nproves on the popular LoRA algorithm (Hu et al.,\n2021) and showed better performance in our ex-\nperiments. The training parameters were found\nthrough initial exploration: rsLoRA rank and alpha\nof 16, 5 epochs, batch size of 2 and learning rate\nof 2e-4. The fine-tuning dataset corresponded to\nthe notes from the breast cancer center (n = 487),\nwith no patient overlap with our test set. The fine-\ntuning process took one hour with the largest model\n(Llama 3.1 8B) and twenty minutes with the small-\nest model (Llama 3.2 1B)."}, {"title": "3.5 Postprocessing", "content": "An evaluation pipeline was implemented to process\nmodel outputs for each section of interest. Using\nVLLM (Kwon et al., 2023) to perform inference,\nthe model was prompted to generate the first five\nwords and the last five words of each predicted\nspan (Zhou and Miller, 2024). These 5-grams were\ncompared to the source text to identify matches. If\na match was found, the segment from the identified\nstarting position to the identified ending position\nwas extracted and labeled as the 'predicted output'\nDue to the generative nature of LLMs, achiev-\ning an exact 5-gram match was uncommon, as ob-\nserved in prior studies and in our experience (Zhou\nand Miller, 2024). To address this, fuzzy matching\nwas employed to align the predicted start and end\nstrings with the source text. This process used a\nsliding window of 5-grams derived from the source\ntext and assessed similarity using the Levenshtein\ndistance (Levenshtein, 1966), which measures the\nminimal number of edits required to transform one\nstring into another. Matches with a similarity score\nexceeding 80% were considered valid, ensuring ro-\nbust identification of spans in the generated output\nthat closely align with the source text."}, {"title": "3.6 Evaluation", "content": "The predicted outputs were compared to ground\ntruth annotations , and precision, recall,\nand F1 score were calculated. To assess model per-\nformance, we first ran inference three times on each\nmodel, then bootstrapped (n = 1,000) each run to\nobtain 3,000 sets of metrics for evaluation. Statisti-\ncal significance was assessed using a Friedman test\n(\u03b1 = 0.05) (Zimmerman and Zumbo, 1993), with\npost-hoc pairwise comparisons via the Wilcoxon\nsigned-rank test and a Bonferroni adjusted alpha of\n0.01 (Woolson, 2005; Bland and Altman, 1995)."}, {"title": "3.6.1 Internal Validity", "content": "Outputs were evaluated on notes from two cancer\ncenters (gastrointestinal and neurological), distinct\nfrom the cancer center used for training (breast), to\nassess performance across different patient popula-\ntions at one institution."}, {"title": "3.6.2 External Validity", "content": "To evaluate the external validity of this method,\nthe best-performing model was used to section 50\nprogress notes from breast cancer patients at UCSF\n(Sushil et al., 2024). To ensure label consistency\neach note was annotated by KS using the validated\ncodebook, and F1 scores were calculated."}, {"title": "4 Results", "content": "SecTag achieved an F1 score of 0.30 on the A&P\nsection but was unable to generate a valid output\nfor the RCH section. The average F1 scores\nacross both labels for MedSpaCy and Clinical-\nLongformer were 0.19 and 0.62, respectively.\nWe found that using SFT, the open source LLMs\ngenerated higher quality outputs relative to their\nbase counterpart without the need for enforced\nstructure (base model performance can be found\nLlama 3.1 8B had F1 scores of\n0.89 and 0.94 for RCH and A&P respectively\nThe difference in model performance\nwas statistically significant (p<0.01). Notably,\nLlama 3.1 8B scored 9-16 points higher than\nGPT-40.\nError analysis was conducted on the top-\nperforming model, Llama 3.1 8B, focusing on\ninstances where the F1 score for a section fell\nbelow 0.8 (Gastrointestinal n = 96, Neurological\nn = 24). The most common error was over/under-\nprediction of target section; detailed error analysis\ncan be found in Appendix E.\nOn the 50 external progress notes, the F1 scores\nfor RCH and A&P using Llama 3.1 8B were 0.82\nand 0.87 respectively."}, {"title": "5 Discussion", "content": "This study demonstrates that small, fine-tuned\nlanguage models can outperform proprietary\nmodels in clinical section segmentation, offering\nsignificant advantages in cost, accuracy, and\naccessibility. Unlike proprietary models requiring\ninstitutional agreements and high computational\ncosts (Umeton and et al., 2023), our approach\nenables deployment on local or cloud-based\nsystems, making it usable by researchers operating (Chandler et al., 2022).\nunder resource constraints. This adaptability is\ncrucial for downstream tasks such as symptom\nanalysis and cohort discovery, where high-quality,\nactionable insights are critical.\nOur findings demonstrate the potential of fine-\ntuning models with small datasets (fewer than 500\nnotes) to effectively perform note sectioning, even\nin the face of variability across clinical notes from\ndifferent patient populations, offering a robust and\nadaptable solution for institutional use. Testing on\nnotes from two distinct cancer populations and the\nprogress notes of another institution highlights this\napproach's internal and external validity. While\nour study focused on progress notes, the strong\nperformance demonstrates that fine-tuned models\nmay effectively adapt to variations in note structure\nand content across institutions.\nBy integrating note sectioning with a small\nlanguage model as a preprocessing step, the input\nsize for larger, more resource-intensive language\nmodels in downstream tasks is significantly\nreduced. This reduction in input size decreases\ncomputational demands, leading to lower energy\nconsumption and, consequently, a reduced carbon\nfootprint (Stojkovic et al., 2024). This approach un-\nderscores the potential for sustainable AI practices\nin clinical data processing by optimizing resource\nusage without compromising performance.\nBy providing a cost-effective and privacy-\nconscious solution, this work reduces reliance on\nproprietary systems. The affordability and acces-\nsibility of our approach ensures that high-quality\nresearch is no longer limited to large institutions,\nfostering innovation across diverse settings.\nLimitations\nWhile the model demonstrated strong perfor-\nmance overall, error analysis revealed patterns of\noverprediction and underprediction, particularly\nin sections with ambiguous or inconsistent\nboundaries. These errors highlight challenges\nposed by variability in clinical note structures\nand suggest areas for improvement, such as\nincorporating additional section labels to enhance\ndiscriminatory power. A potential mitigation\nstrategy is incorporating a human-in-the-loop step\nto ensure sectioning aligns with study standards\nThis study focused exclusively on notes authored\nby physicians, nurse practitioners, and physician\nassistants, without evaluating notes written by other\nclinical staff, such as physical therapists, occupa-\ntional therapists, or nutritionists. Furthermore, all\nanalyzed notes originated from academic medical\ncenters, limiting the assessment of variability in\nnote styles across different types of hospital sys-\ntems, such as community hospitals."}, {"title": "6 Conclusion", "content": "Our method demonstrates a robust, institution-\nagnostic solution for segmentation of clinical notes.\nBy leveraging fine-tuned models that are cost-\neffective and adaptable, this approach offers a\nscalable and accessible methodology for improv-\ning clinical documentation analysis across diverse\nhealthcare settings."}, {"title": "Conflicts of interest", "content": "The authors have no competing interest to share."}, {"title": "Data availability", "content": "The code used for this project as well as\nsample annotations based on the CORAL\ndataset are available in the following repository:"}, {"title": "A Annotation Codebook", "content": "General Guidelines\nIn general, stick with annotating in big chunks rather than separated sections. It's not possible if HPI and\nInterval History are separated by a large chunk of the oncology history, and that's okay.\nRecent Clinical History (HPI / Interval History)\nInclude:\n\u2022 Anything in the following section heads/content:\nInterval History, interval treatment\nSubjective\nHPI, even if there is a lot of past Onc info in it, unless there is a separate section labeled Onc hx\n(then can omit that).\n\u2022 Free-hand documentation (e.g., unstructured communication notes with a patient at the bedside/clinic)\nthat appear to be written without template.\n\u2022 Text which looks like past interval history or past HPI but is not clearly demarcated by either a title\n(\"HPI\"), phrases, or another indication.\n\u2022 Talk of a list of current symptoms that is outside the standard ROS and can clearly be seen as free-text\ndocumentation from the encounter.\nExclude:\nExample: \"no nausea or itching.\"\n\u2022 The following sections: (even if they have something that might look important as it will be discussed\nagain later on)\nChief complaint\nPatient ID\nReason for visit \u2013 UNLESS the words in there are the HPI!!\nOncology history\nReview of systems\nCurrent treatment/therapy\nTemplated lists of ESYM responses\nPatient instructions\nReferral orders\n\u2022 Information that is clearly copied forward, typically starts with or is followed by one of these\nsentences: Copied from, Above is for reference only, For reference, Carried through for continuity,\nAbove history is for clinical reference only, Oncology history overview, OncHx has been copied\nforward and edited/updated from prior documentation for the purpose of clinical reference only,\nOncology History, PMH, FH, and SH copied forward from previous notes and updated, included for\nclinical reference only."}, {"title": "Assessment and Plan", "content": "Include:\n\u2022 Beginning at assessment and ending at the end of the follow-up instructions.\n\u2022 Attending attestations (continue the same block of labeled text even if you include some things you\nnormally would not).\n\u2022 Statements about follow-up timing if it seems to be free text or there are clinical implications or\ninformation present.\n\u2022 \"IMP\" = impression\n\u2022 \"Impression and recommendations\"\nExclude:\n\u2022 Information that is copied forward: \u201cLast assessment and plan.\"\n\u2022 Billing statements.\n\u2022 \u201cVerbalized understanding, all questions answered, will call...\" unless it has non-templated writing\nlike \"for worsening pain.\"\n\u2022 Attestations if there is no free-written text, and it is just templated language, e.g., \u201cI agree with\nassessment and plan with PA above.\""}, {"title": "B Prompt used for all LLMs", "content": "Prompt: Your task is to find the parts of a clinical note corresponding to the sections -History\nof Present Illness and Interval History-, and -Assessment and Plan-. You should organize this\ninformation in a JSON output that extracts the first and last five words for each of these sections.\nIf the sections HPI_Interval_Hx or A&P are not in the medical note, return an empty string for\nthe corresponding section's start and end. Below is the medical note:"}, {"title": "C Evaluation of sectioning approaches found in the litterature", "content": "Average performance"}, {"title": "D Performance of base models", "content": "Average performance of LLMs with 95% confidence intervals"}, {"title": "E Error Analysis of Llama 3.1 8B Instruct on Gastrointestinal and Neurological Notes", "content": "Review of errors in the Neurological center\nReview of errors in the Gastrointestinal center"}]}