{"title": "Hierarchical Reinforcement Learning for Optimal Agent Grouping in Cooperative Systems", "authors": ["Liyuan Hu"], "abstract": "This paper presents a hierarchical reinforcement learning (RL) approach to address the agent grouping or pairing problem in cooperative multi-agent systems. The goal is to simultaneously learn the optimal grouping and agent policy. By employing a hierarchical RL framework, we distinguish between high-level decisions of grouping and low-level agents' actions. Our approach utilizes the CTDE (Centralized Training with Decentralized Execution) paradigm, ensuring efficient learning and scalable execution. We incorporate permutation-invariant neural networks to handle the homogeneity and cooperation among agents, enabling effective coordination. The option-critic algorithm is adapted to manage the hierarchical decision-making process, allowing for dynamic and optimal policy adjustments.", "sections": [{"title": "Introduction", "content": "Sequential grouping or pairing problems are ubiquitous in many domains, from resource allocation and scheduling to team formation and matchmaking. These problems are challenging due to their inherent combinatorial complexity and the need for efficient decision-making under constraints.\nIn studies of natural [Jeanson et al., 2005, Wittemyer and Getz, 2007] and multi-agent systems [Phan et al., 2021], grouping has been recognized as a means to enhance efficient collaboration. However, devising a general criterion for grouping agents without specific domain knowledge remains a significant challenge for the research community. Previous works often focus on well-structured tasks with predefined roles or task decompositions [Cossentino et al., 2014, Lhaksmana et al., 2018, Phan et al., 2021, Jiang et al., 2021], which may not be practical in real-world settings and can restrict the transferability of these methods. Existing approaches to grouping and pairing policies in RL have limitations. For example, user pairing in the NOMO system [Wang et al., 2021, Lee and So, 2020] suffers from exponential growth in the state-action space when modeling Q(s, a) directly with actions as inputs. Other RL-based methods [Zhou et al., 2016] focus on grouping problems but do not address sequential decision-making. Grouping problems typically involve partitioning a set into mutually disjoint subsets according to specific criteria and constraints, yet these approaches lack scalability and flexibility for sequential applications. Recently, Saul et al. [2005] proposed an automatic agent clustering method based on the contribution weight of each agent to specific groups. Our method differ with their work in a sense that the policy of agent grouping is learnt under RL framework with the goal of maximizing the cumulative reward while they learn the grouping by a more indirect sparsity regulation.\nThe application of RL to multi-agent matching or pairing problems presents unique computational and modeling challenges, particularly when scaling to larger problem sizes. Traditional Q-learning or option-critic approaches face critical limitations in this context:\n1.  Challenges in Critic Network Design and Learning:\n\u2022 Expansive Option Space: The number of ways to pair n items (teams) is given by the double factorial (n - 1)!!, leading to a massive growth in the option space. For example, when n = 20, the number of pairings exceeds 3.7 billion.\n\u2022 Inefficient Modeling: Pairings such as (1,2) and (3,4) versus (3,4) and (1,2) represent identical team matchings but may be treated differently in conventional Q-function models, leading to redundancy and inefficiency.\n2.  Challenges in Policy Network Design and Learning:"}, {"title": null, "content": "\u2022 Expansive Action Space: As the number of teams and team sizes increase, the action space grows exponentially, complicating policy learning.\n\u2022 Scalability Issues: Traditional option-critic architectures struggle to scale due to the exponential growth in policy parameter space with increasing option complexity.\nTo address these challenges, we propose the following contributions:\n1.  Solutions to Challenges in Critic Network Design:\n\u2022 We redefine the critic Q-function by embedding pairing information directly within the network architecture rather than treating it as a separate input. This ensures permutation invariance of team pairings, enhancing modeling efficiency and reducing computational redundancy (see Figure 2).\n\u2022 We decompose the joint Q-function into combinations of pair Q-values, transforming the problem of finding optimal team pairings into a linear optimization problem. This significantly reduces computational complexity by avoiding exhaustive enumeration (see Section 3.2).\n2.  Solutions to Challenges in Policy Network Design:\n\u2022 We implement a Permutation Invariant (PI) network architecture that leverages the homogeneity of agents for effective parameter sharing (see Figure 1).\n\u2022 Instead of using independent policy networks for each option, we incorporate pairing information directly within the policy network architecture (see Figure 1).\nOur critic and policy networks employ a shared-parameter embedding for individual states, which are aggregated to form team character embeddings. These embeddings are transformed to generate PI matching embeddings for team pairs, which are summed to produce a final Q-value through a single fully connected layer. This architecture avoids the curse of dimensionality by circumventing direct engagement with the expansive action space, ensuring scalability as the number of teams or individuals increases without a proportional rise in parameter count."}, {"title": "Related Work", "content": "Hierarchical RL addresses the challenge of large state and action spaces in long-horizon tasks by decomposing the problem into a hierarchy of subtasks. A higher-level policy selects optimal subtasks, while lower-level policies solve them, improving exploration and scalability.\nHRL Foundations. Pateria et al. [2021] highlight how HRL improves exploration through structured subtasks or subgoals. The Semi-Markov Decision Process (SMDP) framework [Sutton et al., 1999] underpins many HRL methods by allowing variable-length subtasks. The Option-Critic architecture [Bacon et al., 2016, Chunduru and Precup, 2022] automates the learning of options (subtasks) and their termination conditions, facilitating adaptive and autonomous decision-making. Extensions to multi-agent systems [Chakravorty et al., 2019] allow decentralized execution and cooperative behavior.\nPolicy over Options. An option is defined as a tuple $(I, \\pi, \\beta)$, where I specifies the initiation set, $\\pi$ is the intra-option policy, and $\\beta$ is the termination function. Our work builds on this foundation by considering recursive optimality, where sub-behaviors are individually optimal, and policy learning over large option spaces, where permutation invariance and parameter efficiency are key."}, {"title": "Multi-Agent Reinforcement Learning (MARL)", "content": "Multi-agent RL involves multiple interacting agents learning policies to optimize collective or individual objectives. Hierarchical approaches [Pateria et al., 2021] integrate HRL concepts to decompose complex team-based decision problems, enabling more efficient coordination and scalability."}, {"title": "Matching Problems", "content": "Matching problems, particularly in reinforcement learning, span static and dynamic contexts, bipartite and monopartite structures, and stochastic settings.\nBipartite vs. Monopartite Matching. Bipartite matching involves two distinct sets (e.g., tasks and workers), while monopartite matching operates within a single homogeneous set. Dynamic bipartite graph matching (DBGM) [Wang et al., 2019] addresses scenarios where tasks and workers arrive and leave dynamically. The proposed adaptive batch-based framework leverages RL for batch splitting, enabling theoretical guarantees and efficient real-time decision-making.\nOrder Dispatch and Markov Matching Markets. Order dispatch systems [Xu et al., 2018] apply policy iteration and Q-learning to optimize driver-task assignments, handling stochastic action spaces [Cohen et al., 2022]. Markov matching markets [Min et al., 2022] introduce stochastic agent sets and planner actions that affect environmental transitions, integrating RL to optimize matching policies.\nCompeting Matching with Complementary Preferences. Huang [2022] tackle many-to-one matching using Thompson Sampling for preference exploration and double matching for stability, demonstrating RL's applicability in balancing exploration and exploitation."}, {"title": "The Option-Critic Architecture", "content": "The Option-Critic framework [Bacon et al., 2016] provides a foundation for hierarchical policy learning by automating the discovery of options and their termination conditions. However, traditional architectures face challenges in scaling to large option spaces.\nDifferences in Proposed Architecture.\n1. Option Space: Our problem involves an extremely large, known option space, necessitating parameter-efficient critic networks and advanced optimization methods to avoid exhaustive enumeration.\n2. Termination Function: Unlike traditional approaches requiring learned termination functions, our framework assumes predefined termination conditions, simplifying the learning process.\nComparison to Multi-Agent Cooperative Option-Critic. Chakravorty et al. [2019] address partially observed state spaces and decentralized execution in multi-agent settings. Our work contrasts by leveraging fully observed state spaces, homogeneity assumptions, and parameter-efficient critic designs to manage large option spaces effectively.\nIn summary, our approach integrates and extends hierarchical RL, multi-agent RL, and matching theory to address challenges in large-scale, dynamic decision-making tasks. By leveraging permutation-invariant architectures and efficient policy optimization, we provide scalable solutions for complex pairing and grouping problems."}, {"title": "Hierarchical Multi-Agent Reinforcement Learning for Agent Grouping/Pairing", "content": "We first introduce the motivating example. In the Intern Health Study [Wang et al., 2023], medical interns from diverse backgrounds-different specialties and universities are weekly grouped into teams to compete on health metrics including daily step counts, mood scores, and sleep hours, all managed through a dedicated app. This app is responsible for determining the optimal team pairings each week and deciding whether to send health-promoting messages to individuals daily, with the goal of maximizing interns' health throughout their internship period. This setup presents a challenging multi-agent problem, where high-level decisions on team formations and low-level decisions on individual interventions must be dynamically optimized. The hierarchical nature of these decisions where team pairings influence daily interactions and messaging-motivates the application of a hierarchical multi-agent reinforcement learning approach, capable of efficiently coordinating both levels to achieve the best health outcomes.\nThis problem is addressed within a hierarchical Reinforcement Learning (RL) framework, involving homogeneous and fully cooperative agents. This approach is well-suited for scenarios where agents need to dynamically form groups or pairs to optimize collective performance.\n\u2022 Homogeneous and Fully Cooperative Agents. In this setting, agents are homogeneous, meaning they share the same structure, skills, and learning models. This homogeneity allows for efficient"}, {"title": null, "content": "learning and scalability, as agents can share experiences and model parameters. Despite sharing the same policy network, agents can exhibit diverse behaviors due to their unique observations at any given time. Additionally, agents can leverage centralized value functions, incorporating mutual information and joint actions, to enhance learning efficiency [Gronauer and Diepold, 2021]. We adopt the CTDE (Centralized Training with Decentralized Execution) paradigm, which is state-of-the-art for multi-agent learning [Kraemer and Banerjee, 2016, Oliehoek et al., 2008].\n\u2022 Hierarchical Reinforcement Learning The hierarchical RL framework involves two levels of decision-making:\n1. Option Policy: This higher-level policy determines the groupings or pairings of agents (e.g., weekly team pairings), effectively selecting \"options\" that define the structure of interactions.\n2. Intra-Option Policy: Within the context of a selected grouping (option), this lower-level policy governs the day-to-day interactions (e.g., daily messaging) aimed at optimizing health outcomes.\nGiven the availability of a centralized controller in our application (e.g., an IHS APP), option decisions can be based on the full state space, unlike scenarios where options are based on partial observations [Chakravorty et al., 2019]. The homogeneity of agents suggests the use of permutation-invariant neural networks to reduce the parameter space of the joint Q-function or policy network."}, {"title": "The Options Framework", "content": "Define the state for subject j of team i observed at time t as $s_{i,j,t}$, the action as $a_{i,j,t}$ and the reward as $r_{i,j,t}$. We consider a discrete and finite individual action space $A = \\{a_1,...,a_{|A|}\\}$. Define the number of teams as M, the number of days requiring individual action decision each week as H and the weeks are indicated by w. We model the underlying system as an MDP and employ the options framework to structure hierarchical learning. An option $\\omega$ is defined as a triple $(\\Z_\\omega, \\pi_\\omega, \\beta)$:\n\u2022 Initiation Set $\\Z_\\omega$: Specifies the states where option $\\omega$ can be initiated.\n\u2022 Intra-Option Policy\n$\\pi_{\\omega}(a_t | S_t; \\theta) = \\prod_{k=1}^{M/2} \\prod_{i \\in m_k(\\omega)} \\prod_{j} \\pi(a_{i,j,t} | S_{m_k(\\omega),t}; \\theta)$\nwhere $S_{m_k(\\omega)} = \\{S_{i,j,t}\\} i \\in m_k(\\omega), j$. Define the set of team matching pairs induced by $\\omega$ as $S_w = \\{(m_1(\\omega)^1, m_1(\\omega)^2), (m_2(W)^1, m_2(\\omega)^2),..., (m_{M/2}(\\omega)^1,m_{M/2}(\\omega)^2) : where \\cup (m_i(\\omega)^1, m_i(\\omega)^2) = [M] \\text{ and } (m_i(\\omega)^1, m_i(\\omega)^2)\\cap(m_j(\\omega)^1, m_j(\\omega)^2) = \\O, \\forall i \\neq j\\}$. Define $m_i(\\omega) = (m_i(\\omega)^1, m_i(\\omega)^2)$. The parameter $\\theta$ is shared across all teams and options to ensure consistency and reduce complexity.\n\u2022 Termination Condition $\\beta$: Determines the probability of terminating the option in a given state:\n$\\beta(S_{.,.,t}) = \\begin{cases} 0 & \\text{if } wH < t < (w + 1)H \\\\ 1 & \\text{otherwise} \\end{cases}$\nAgents select an option $\\omega$ according to the policy over options $\\pi_{\\eta}(\\omega | s)$, execute the intra-option policy $\\pi_{\\omega}$ until termination (as dictated by $\\beta$), and then repeat the process. This setup transforms the MDP into a Semi-Markov Decision Process (SMDP), enabling the use of optimal value functions over options $V_\\theta(s)$ and $Q_{\\eta}(s,\\omega)$\nWe adopt the option-critic frame work to learn the joint Q network and intra-option policy. We define several important functions in the option-critic framework under our agent grouping settings. The option-value function $Q_{\\eta,\\psi}(s,\\omega)$ is defined as:\n$Q_{\\eta,\\psi}(s, \\omega) = \\sum_{a} \\pi_{\\omega,\\theta}(a | s)Q_{U, \\psi}(s,\\omega, a)$\nwhere $Q_U$ represents the value of executing an action in the context of a state-option pair:\n$Q_{U, \\psi}(s,\\omega, a) = \\sum_i r(s_i, a_i) + \\gamma \\sum_{s'} P(s' | s, a, \\omega)U(\\omega,s')$"}, {"title": null, "content": "The option-value function upon arrival $U_{\\psi}(,\\omega,s')$ is:\n$U_{\\psi}(\\omega, s') = (1 \u2013 \\beta(s'))Q_{\\eta,\\psi}(s', \\omega) + \\beta(s')V_{\\theta,\\psi}(s')$\nwith $V_{\\theta,\\psi}(s) = \\sum_{\\omega} \\pi_{\\Omega}(\\omega)Q_{\\eta,\\psi}(S, \\omega)$.\nThe gradient of the expected discounted return with respect to $\\theta$ and initial condition $(s_0, \\omega_0)$ is:\n$\\sum_{\\varsigma,\\omega} \\mu_{\\eta}(\\varsigma, \\omega | s_0, \\omega_0) \\sum_{a} \\frac{\\delta \\pi_{\\omega,\\theta}(a | s)}{\\delta \\theta} Q_U(\\varsigma, \\omega, a)$\nwhere $\\mu_{\\eta}(\\varsigma, \\omega | s_0, \\omega_0)$ is the discounted weighting of state-option pairs along trajectories starting from $(s_0, \\omega_0)$.\nThe gradient of the expected discounted return with respect to $\\theta$ and initial condition $(s_0, \\omega_0)$ is:\n$\\sum_{\\varsigma,\\omega} \\mu_{\\eta}(\\varsigma, \\omega | s_0, \\omega_0) \\sum_{a} \\frac{\\delta \\pi_{\\omega,\\theta}(a | s)}{\\delta \\theta} Q_U(\\varsigma, \\omega, a)$\nwhere $\\mu_{\\xi}(\\varsigma, \\omega | s_0, \\omega_0)$ is the discounted weighting of state-option pairs along trajectories starting from $(s_0, \\omega_0)$.\nWith those definitions, the option-critic framework in Bacon et al. [2016] can be adopted to solve the hierarchical policy learning in theory. However, the large option/action space involved in the Q functions above make it practically challenging to solve the optimization."}, {"title": "Network Architecture for Dimension Reduction", "content": "To handle the permutation invariance and scalability required for our multi-agent setting, we employ the Deep Set architecture [Zaheer et al., 2017] for both the policy and critic networks. This architecture ensures that the network is invariant to the order of input agents and can handle varying numbers of agents. The permutation invariant network enjoys several advantages:\n\u2022 Permutation Invariance: The network's output remains unchanged regardless of the order of input agents.\n\u2022 Permutation Invariance: The network's output remains unchanged regardless of the order of input agents.\n\u2022 Input Size Irrelevance: The network can handle any number of agents without modification\n\u2022 Privacy Consideration: Individual agent information is encoded into embeddings before being aggregated, ensuring privacy.\n\u2022 Generalization: The trained network can be applied to new cohorts of agents with different sizes.\nWithout loss of geneority, assume team $i = m_1(\\omega)^1$ at time $t$. The decomposed individual policy $\\pi(a_{i,j,t}|S_{m_1(\\omega),t}) = \\pi(a_{i,j,t}|S_{i,j,t}, S_{m_1(\\omega)1} | S_{i,j,t}, S_{m1(\\omega)2})$ is designed in this way. First the individual's state $s_{i,j,t}$, the state of each of the rest individual from team i, and the state of each of the individuals from team $m_i(\\omega)^2$ are separatly passed through three encoders. Then, the embeddings of the remaining individuals from team i and team $m_1(\\omega)^2$ are pooled together separately by some permutation invariant operators such as averaging to obtain the team member's characters and the rival characters. Finally, the individual embedding, the team member's characters and the rival characters are passed through some layers to output the final action for this individual. The policy network designed is illustrated in Figure 1.\nThe joint Q-network is designed to avoid explicit enumeration of the action space when combined with a greedy policy. It uses a shared encoder to generate embeddings for individual agents, which are then aggregated using permutation-invariant operators to form team and then group embeddings. This design significantly reduces the number of parameters and accelerates learning. The joint Q-network is illustrated in Figure 2. Given the Q-network architecture in Figure 2, the greedy policy over options can"}, {"title": "Simulation Study", "content": "We design a simulated environment similar to the Intern Health Study. The state variable for each individual is the daily the square root of step count and the reward is the square root of next day's step count. The action is a binary variable indicating whether to send a text message or not. We also allow some teams not enter competition in some weeks. We would view this case as a pair $(m_k(\\omega)^1, m_k(\\omega)^2)$ where $m_k(\\omega)^1 = m_k(\\omega)^2$. The transition function is defined as:\n$S_{m_k(\\omega)^q,j,t+1} = \\begin{cases} 0.0005 + 0.3289S_{m_k(\\omega)^q, j,t} + 0.0672 \\sum_{j'} S_{m_k(\\omega)^{q'} [j'],j,t} + 0.0103\\mathbb{1}(q = q') & \\text{if } A_{i,j,t} = 0; \\\\ -0.0009 + 0.32450S_{m_k(\\omega)^q, j,t} + 0.0746 \\sum_{j'} S_{m_k(\\omega)^{q'} [j'],j,t} + 0.0136\\mathbb{1}(q = q') & \\text{if } A_{i,j,t} = 1; \\end{cases}$\nAll encoders in the critic model and the policy network is a two-layer perceptron with each layer containing two nodes followed by a ReLu activation function. We initialize 10 teams each containing 10 subjects with initial state sampled from a standard normal distribution. We train the policy with 10000 iterations and a learning rate of 0.001. After training, the obtained policy is evaluated in the same simulated environment by executing it for 1000 steps and calculated the cumulative reward. The gamma discounted factor is set ot be 0.9. We compare the policy obtained by the proposed method with two"}]}