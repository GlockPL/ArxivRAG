{"title": "CURLORA: STABLE LLM CONTINUAL FINE-TUNING AND CATASTROPHIC FORGETTING MITIGATION", "authors": ["Muhammad Fawi"], "abstract": "This paper introduces CURLORA, a novel approach to fine-tuning large language models (LLMs) that leverages CUR matrix decomposition in the context of Low-Rank Adaptation (LoRA). Our method addresses two critical challenges in LLM fine-tuning: mitigating catastrophic forgetting during continual learning and reducing the number of trainable parameters. We propose a unique modification to the CUR decomposition process, utilizing inverted probabilities for column and row selection which acts as an implicit regularization, and initializing the U matrix as a zero matrix, and only fine-tuning it. We demonstrate through experiments on multiple datasets that CURLORA out-performs standard LoRA in mitigating catastrophic forgetting. It maintains model stability and performance across tasks while significantly reducing the number of trainable parameters. Our results show that CURLORA achieves very good and stable task accuracy while maintaining base model's perplexity scores fixed compared to LoRA upon continual fine-tuning, particularly in scenarios with limited data.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have revolutionized natural language processing, demonstrating remarkable capabilities across a wide range of tasks [1]. However, fine-tuning these large models for specific tasks requires a lot of computational resources making it challenging to adapt these models efficiently, especially when working with limited datasets and in resource-constrained environments. [2]. Parameter-Efficient Fine-Tuning (PEFT) Methods have gained a lot of attention because they make fine-tuning large models accessible and possible. [3]\nLow-Rank Adaptation (LoRA) [4] has emerged as an efficient PEFT method, enabling fine-tuning large language models on custom tasks while decreasing the number of trainable parameters hence requiring less resources. LoRA works by decomposing pre-trained weight matrices into low-rank matrices and fine-tune these ones instead of the original matrix. Although LoRA has proven to be very excellent and promising, it still faces challenges with catastrophic forgetting. Catastrophic forgetting in LLMs is a critical issue where the model loses previously acquired knowledge when fine-tuned on new tasks [5]. It occurs due to the overwriting of previously learned (pre-trained) weights during the fine-tuning process. In LoRA, this often happens as the adapted output can significantly deviate from the original:\n\\(y = xW + xWadapted = x(W + AB)\\)\nwhere \\(W\u2208 R^{m\u00d7n}\\) is the original weight matrix, and \\(AB\\) is the low-rank update from multiplying \\(A\u2208 R^{mxr}\\) by \\(B\u2208 R^{rxn}\\) where r < n.\nThis work introduces CURLORA, a novel approach that applies low-rank adaptation (LoRA) to pre-trained weight matrices using CUR matrix decomposition [6] instead of random initiation of the low-rank A or B matrices. We propose a unique modification to the CUR decomposition process and demonstrate its effectiveness in mitigating catastrophic forgetting while also reducing the number of trainable parameters. While LoRA successfully reduces computational costs by decomposing weight updates into low-rank matrices, it still suffers from catastrophic forgetting. CURLORA leverages CUR decomposition with inverted probabilities and initiating U matrix as zero to further mitigate this issue."}, {"title": "Related Work", "content": "Catastrophic forgetting is a big challenge in machine learning, particularly in the context of continual learning [5]. Various approaches have been proposed to address this issue:\n\u2022 Elastic Weight Consolidation (EWC) [7] uses Fisher information to measure the importance of parameters and selectively slow down learning on important parameters.\n\u2022 Progressive Neural Networks [8] propose to freeze the network trained on previous tasks and add lateral connections to new columns for new tasks.\n\u2022 Memory-based approaches like Experience Replay [9] store and replay examples from previous tasks during training on new tasks."}, {"title": "Efficient Fine-tuning of Large Language Models", "content": "As LLMs have grown in size, efficient fine-tuning methods have become crucial:\n\u2022 Adapter layers [10] introduce small trainable modules between layers of a pre-trained model.\n\u2022 Low-Rank Adaptation (LoRA) [4] decomposes weight updates into low-rank matrices, significantly reducing the number of trainable parameters.\n\u2022 Prefix-tuning [11] prepends trainable continuous prompts to the input, allowing for task-specific adaptations."}, {"title": "CUR Matrix Decomposition", "content": "CUR decomposition has been applied in various domains for its interpretability and efficiency:\n\u2022 In data analysis, CUR has been used for feature selection and dimensionality reduction [6].\n\u2022 In scientific computing, CUR has been applied to accelerate large-scale matrix computations [12].\n\u2022 In machine learning, CUR has been explored for model compression and interpretation [13].\nHowever, to the best of our knowledge, CUR decomposition has not been previously applied to the problem of fine-tuning large language models or addressing catastrophic forgetting in this context."}, {"title": "Background on CUR Decomposition", "content": "CUR decomposition is a matrix factorization technique that approximates a matrix A as the product of three matrices: C, U, and R. Unlike Singular Value Decomposition (SVD), CUR decomposition uses actual columns and rows from the original matrix, making it more interpretable.[6].\nGiven a matrix \\(A \u2208 R^{m\u00d7n}\\), CUR decomposition approximates A as:\n\\(A\u2248 CUR\\)\nwhere:\n\u2022 \\(C\u2208 R^{m\u00d7c}\\) consists of c columns of A\n\u2022 \\(R\u2208 R^{r\u00d7n}\\) consists of r rows of A\n\u2022 \\(U\u2208 R^{c\u00d7r}\\) is a small matrix that ensures CUR is close to A\nThe columns and rows are typically chosen based on their statistical leverage scores. [12] Leverage scores indicate the importance of columns and rows in representing the original matrix. High leverage scores identify influential columns and rows, while low scores identify less critical ones."}, {"title": "This Work", "content": "In this section, we present CURLORA, our novel approach to fine-tuning large language models that leverages a modified CUR matrix decomposition to mitigate catastrophic forgetting. We provide a detailed mathematical formulation of the approach, analyze it theoretically, and explain how it addresses the challenge of catastrophic forgetting upon continual learning."}, {"title": "CURLORA", "content": "The core idea is to decompose the pre-trained weight matrices using a modified CUR approach and then fine-tune only the U matrix. This approach constrains the parameter space of possible adaptations keeping the fine-tuned parameters as small as possible to keep \\(||W_{adapted} - W ||_F\\) close to the original weight matrix frobenius norm (\\(||W||_F\\)) i.e. \\(W + W_{adapted}\\) is so close to W to avoid the deviation of the adapted output."}, {"title": "Mathematical Formulation", "content": "Given a weight matrix \\(W \u2208 R^{m\u00d7n}\\), we first compute the probability of each column:\n\\(p_j = \\frac{||W_{:j}||^2}{||W||_F}\\)\nwhere \\(W_{:j}\\) is the j-th column of W, while \\(|| \u00b7 ||_2\\) denotes the square of the L2 norm of the column and \\(|| \u00b7 ||_F\\) denotes the square of the Frobenius norm of W. This will give us the probability of each column. For instance, if W has three columns with norms 2, 3, and 5, the probabilities are 4/38, 9/38, and 25/38 respectively.\nWe then invert these probabilities:\n\\(p'_j = \\frac{1/p_j}{\\sum_{i=1}^n 1/p_i}\\)\nwhere \\(p'_j\\) is the inverted probability of the j-th column of W. The same steps are followed for rows. Inverted probabilities are used to sample columns and rows with lower leverage scores, which implicitly regularize the model and limit the magnitude of fine-tuning adjustments.\nThen, we sample r columns and rows, where r < n, according to these inverted probabilities to construct C and R, which will always be fixed, with columns and rows with lower original probabilities. This trick plays a major role in the approach as it serves two purposes:\n\u2022 It acts as a form of regularization, preventing the model from overfitting or moving too much towards the task and limiting the adaptation of the U matrix stopping it from growing so big in magnitude.\n\u2022 It preserves the model's original behavior by focusing adaptations on less influential parts of the weight matrix. In addition, since C and R contain actual columns and rows from the original matrix, they contribute to the stability of the fine-tuning process.\nCURLORA's approach differs significantly from other initialization methods. Unlike LoRA's random initialization using Kaiming-uniform or Gaussian for weight A and zeros for weight B [4], or the SVD-based initialization [14], CURLORA offers more controlled adaptation. While these other methods ensure starting from the base model, they don't inherently limit the growth of the adaptation matrix (AB in LoRA), potentially leading to significant deviations during training. In contrast, CURLORA initializes the U matrix as zeros, and importantly, constructs C and R matrices using columns and rows with lower original probabilities (i.e., lower values). This unique combination ensures that the fine-tuning process not only starts from the base configuration but also remains constrained throughout training. The low-value C and R matrices act as natural limiters on the growth of U, thereby preventing large deviations and contributing to enhanced model stability during the fine-tuning process.\n\\(C = SampleColumns(W, r, p)\\)\n\\(R = SampleRows(W, r, p)\\)\n\\(U_{init} = 0\\)"}, {"title": "Theoretical Analysis of Catastrophic Forgetting Mitigation", "content": "To understand how CURLORA helps mitigate catastrophic forgetting, we analyze its properties mathematically:"}, {"title": "Parameter Space Constraint", "content": "In CURLORA, we decompose the original weight matrix W as:\n\\(W\u2248 CUR\\)\nDuring fine-tuning, we're optimizing:\n\\(W_{adapted} = C(U + \u2206U)R\\)\nwhere AU represents the changes made to U during fine-tuning. By constraining the updates to the subspace defined by C and R, CURLORA limits drastic changes, thereby preserving the model's original knowledge."}, {"title": "Implicit Regularization", "content": "By initializing U as a zero matrix, and C and R with columns and rows of low weight values, the ones with lower probabilities, we provide an implicit regularization where C and R will always limit the unnecessary increase of U. This can be seen as adding a regularization term to the loss function quantified by the norm of the matrix U that is aimed to be kept small:\n\\(L_{CURLORA}(\\theta) = L_{task}(\\theta) + ||U||_F\\)\nwhere \\(||U||_F\\) is the Frobenius norm of the U matrix that is being fine-tuned. This implicit regularization term encourages the model to keep the changes small. For instance, if U is initially zero, this term will push the fine-tuning process to make only necessary adjustments, preventing overfitting and excessive reliance on the fine-tuned parameters."}, {"title": "Reduced Interference", "content": "During fine-tuning, W is fixed, so the variable gradient flows through \\(W_{adapted}\\), which is itself updated through U as C and R are fixed. Considering the gradients of the loss L with respect to the parameters, we can, in a simple way, express the gradient of the loss with respect to \\(W_{adapted}\\) as follows:\n\\(\\frac{\\partial L}{\\partial W_{adapted}} = C(\\frac{\\partial L}{\\partial U})R\\)\nThis means that the gradient of the loss with respect to \\(W_{adapted}\\) is dependent on the gradients with respect to U scaled by the fixed matrices C and R. By projecting the gradients onto the subspace defined by C and R, the updates to \\(W_{adapted}\\) are constrained. This means that changes during fine-tuning are less likely to interfere with the model's ability to perform the original task, potentially reducing interference with directions important for the original task."}, {"title": "Reduced Degree of Freedom", "content": "If \\(W \u2208 R^{m\u00d7n}\\) and we use a rank-k adaptation, then:\n\u2022 Full fine-tuning has mn degrees of freedom\n\u2022 LoRA has k(m + n) degrees of freedom"}, {"title": "Stability Analysis", "content": "We can analyze the stability of the adapted and fine-tuned weights and how its change is bounded using the fact that the change that happens to original W is \\(W_{adapted}\\):\n\\(\u2206W = W_{fine-tuned} - W = W + W_{adapted} - W = W_{adapted}\\)\nTo quantify this change, we can use the Frobenius norm, \\(|| W_{adapted} ||_F\\). By utilizing the submultiplicativity property of the Frobenius norm, we can say that the growth of \\(W_{adapted}\\) is controlled through the norms of C, U, and R:\n\\(||W_{adapted} ||_F = ||CUR||_F \u2264 ||C||_F||U||_F||R||_F\\)\nThis equation ensures that the Frobenius norm of the adapted weight matrix \\(W_{adapted}\\) has an upper bound. Since C and R are fixed and U starts at zero, the fine-tuning process focuses on minimizing \\(W_{adapted}\\). As a result, the adaptation remains stable and the model preserves its original knowledge while allowing for necessary adjustments.\nEmpirical results (see Section 7) demonstrate that the Frobenius norm of \\(W_{adapted}\\) remains bounded across multiple tasts, validating the theoretical stability analysis."}, {"title": "Theoretical Analysis of Output Shift", "content": "To understand why CURLORA is expected to perform better than standard LoRA in terms of catastrophic forgetting, we can analyze the shift in the output during fine-tuning.\nFor a given input x, the original output is y = xW. After fine-tuning:\nFor LORA: \\(Y_{adapted} = x(W + AB)\\)\nFor CURLORA: \\(Y_{adapted} = x(W + CUR)\\)\nWe can quantify the shift using the Frobenius norm of the difference:\n\\(||Y - Y_{adapted} ||_F = ||xW - x(W + W_{adapted})||_F = ||xW \u2013 xW \u2013 xW_{adapted}||_F = ||XW_{adapted} ||_F\\)\nFor LoRA: \\(||x(AB)||_F\\)\nFor CURLORA: \\(||x(CUR) ||_F\\)\nThis equation measures the shift in the model's output after fine-tuning. y is the original output, and \\(Y_{adapted}\\) is the output after fine-tuning. After fine-tuning for a different task, the adapted output \\(Y_{adapted}\\) might shift. We use the Frobenius norm to quantify this shift. If the shift is small, it means that the model's predictions haven't changed much, indicating that the model has retained its original knowledge. As shown, the shift depends on \\(W_{adapted}\\) i.e. to make sure the shift isn't so big, we need to keep \\(W_{adapted}\\) as small (in magnitude or size) as possible.\nCURLORA's main aim is to minimize \\(W_{adapted}\\) while ensuring that the difference \\(||W \u2013 W_{adapted}||_F\\) remains close to \\(||W||_F\\). By focusing on minimizing \\(W_{adapted}\\), CURLORA effectively controls the shift in the output, thereby preserving the model's original behavior and mitigating catastrophic forgetting.\nTheoretically, CURLORA should result in a smaller shift because:\n1. The C and R matrices are directly sampled from W, maintaining some structure of the original matrix.\n2. The C and R matrices are sampled from columns and rows with lower values.\n3. Only U is trained, which is constrained by C and R.\n4. The initialization of U as a zero matrix.\nThis constrained adaptation in CURLORA is expected to lead to better preservation of the model's original knowledge, thereby reducing catastrophic forgetting."}, {"title": "Memory Efficiency", "content": "CURLORA offers significant memory savings compared to full fine-tuning and even LoRA. For a weight matrix \\(W \u2208 R^{m\u00d7n}\\), the number of trainable parameters for each method, considering rank r where r < n, is:\n\u2022 Full fine-tuning: mn\n\u2022 LORA (rank r): mr + nr\n\u2022 CURLORA (rank r): r2\nThe memory savings can be substantial, especially for large matrices. In our Mistral experiment, with rank 16, the trainable parameters were:\n\u2022 Full fine-tuning: 7,248,023,552 parameters\n\u2022 LoRA: 9,437,184 parameters\n\u2022 CURLORA: 24,576 parameters\nThis reduction in trainable parameters not only saves memory but also potentially leads to faster training and inference times.\nIn conclusion, CURLORA provides multiple mathematical mechanisms that can help mitigate catastrophic forgetting:\n\u2022 It constrains the parameter space of possible adaptations.\n\u2022 It provides implicit regularization towards the original weights.\n\u2022 It preserves important directions from the original weight matrix.\n\u2022 It reduces the degrees of freedom in adaptation, limiting potential deviation.\n\u2022 It allows for direct control and analysis of weight stability through the U matrix.\nThese properties suggest that CURLORA can indeed help in reducing catastrophic forgetting while still allowing for meaningful and good adaptation to new tasks. The effectiveness of these theoretical mechanisms are validated through our experiments on various tasks and datasets, as detailed in the following sections."}, {"title": "Methodology", "content": ""}, {"title": "CURLORA Implementation", "content": "Our CURLORA implementation consists of the following steps:\nFor each weight matrix W in the layers we want to apply CURLORA to, we perform the following:\n\u2022 Compute column probabilities: \\(p_j = \\frac{||W_{:j}||^2}{||W||_F}\\)\n\u2022 Invert probabilities: \\(p'_j = \\frac{1/p_j}{\\sum_{i=1}^n 1/p_i}\\)\n\u2022 Sample columns and rows according to \\(p'_j\\) to construct C and R\n\u2022 Initialize U as a zero matrix\n\u2022 Objective:\nThe primary objective of the experiment is to evaluate catastrophic forgetting during continual learn-ing, rather than to optimize accuracy for each individual task.\n\u2022 Model Specific Adjustments:\nFor GPT-2 and Mistral, the model's \"lm_head\" is replaced with a task-specific output layer. During training, only the U matrix is continually updated, while C and R remain fixed.\nReplacing the \"lm_head\" ensures that each task has its own task-specific output layer that remains untouched when the model is being fine-tuned on a different task, contributing to the mitigation of task knowledge degradation."}, {"title": "Experiment Setup", "content": ""}, {"title": "Datasets", "content": "We used the following datasets for our experiments:\n\u2022 GLUE-MRPC: Microsoft Research Paraphrase Corpus for paraphrase detection [16]\n\u2022 GLUE-SST-2: Stanford Sentiment Treebank for binary sentiment classification [17]\nThese datasets are part of the General Language Understanding Evaluation (GLUE) benchmark [18], which includes a diverse set of tasks for evaluating natural language understanding systems.\n\u2022 Sentiment140: A large-scale sentiment analysis dataset [19]\n\u2022 WikiText-2: A dataset that we use to measure language model perplexity [20]\nThe datasets were selected for their diverse task requirements and common use in benchmarking."}, {"title": "Model and Hyperparameters", "content": "We used Mistral 7B (v0.3) [21] and GPT-2 Large [22] as our base models. For both LoRA and CURLORA, we used the following hyperparameters:\n\u2022 Ranks: [8, 16, 24]\n\u2022 Alpha: 1\n\u2022 Optimizer: AdamW\n\u2022 Learning rate: 2.5e-4\n\u2022 Scheduler: Cosine with 500 warmup steps\n\u2022 Training epochs: 3\n\u2022 Batch size:\nMistral: 8\nGPT-2: 32\n\u2022 Max length:\nMistral: 512\nGPT-2: 256"}, {"title": "Notes on hyperparemeters and architecture", "content": "CURLORA's performance was evaluated across different ranks, demonstrating robustness to moderate changes. Optimal results can be achieved by fine-tuning other hyperparameters, such as the learning rate. Dropout was not utilized, as the objective was to observe the implicit regularization effects of CURLORA without the influence of explicit regularization."}, {"title": "Experimental Procedure", "content": "Our experimental procedure was as follows:\n1. Measure initial perplexity of the base model on WikiText-2 concatenating the whole dataset into a single string.\n2. Fine-tune on MRPC and evaluate.\n3. Fine-tune on SST-2 and evaluate, then re-evaluate on MRPC.\n4. Fine-tune on Sentiment140 and evaluate, then re-evaluate on MRPC and SST-2.\n5. Re-calculate perplexity on WikiText-2.\nThis procedure was carried out for both LoRA and CURLORA independently."}, {"title": "Results and Discussion", "content": "Tables 1 and 2 present the results of our experiments comparing LoRA and CURLORA across multiple tasks and evaluation metrics."}, {"title": "Performance Analysis", "content": ""}, {"title": "Task-Specific Performance", "content": "CURLORA consistently performed well on different tasks, showing high accuracy even after fine-tuning on subsequent tasks. This suggests that CURLORA is more effective at preserving task-specific knowledge.\nBased on the experiments, CURLORA may require a slightly higher learning rate than LoRA to achieve comparable accuracy. This is due to the implicit regularization introduced by the C and R matrices, which constrain the adaptation space of the U matrix. However, this same property makes CURLORA more robust against overfitting, even at"}, {"title": "Catastrophic Forgetting and Stability", "content": "The stability of CURLORA's performance across tasks is particularly noteworthy. While (Mistra) LoRA-16's accuracy, for example, on MRPC dropped from 0.6495 to 0.32 after fine-tuning on other tasks, CURLoRA-16 (Mistral) maintained its accuracy at 0.66. This demonstrates CURLORA's superior ability to mitigate catastrophic forgetting."}, {"title": "General Language Modeling Capability", "content": "The final perplexity scores on WikiText-2 provide strong evidence for CURLORA's effectiveness in preserving general language modeling capabilities. While all LoRA's perplexity, in both Mistral and GPT2, increased dramatically, all CURLORA models maintained the original perplexity, indicating no degradation in general language understanding."}, {"title": "Theoretical Insights", "content": "The experimental results align with our theoretical analysis:\n\u2022 Parameter Space Constraint: The stability of CURLoRA's performance across tasks supports our hypothesis that constraining adaptations to the subspace spanned by C and R helps preserve original knowledge.\n\u2022 Implicit Regularization: The maintained perplexity on WikiText-2 suggests that CURLORA's implicit regularization effectively prevents overfitting to specific tasks.\n\u2022 Reduced Interference: The consistent performance across tasks indicates that CURLORA successfully reduces interference between task-specific adaptations."}, {"title": "Limitations and Future Work", "content": "While CURLORA shows promising results, there are several areas for future research:\n\u2022 Scalability: While CURLORA shows promising results, its scalability to larger models needs further investigation. Further studies are needed to assess CURLORA's performance on larger models and more diverse tasks like instruction tuning and datasets.\n\u2022 Computational Complexity: Conducting detailed analysis of time and space complexity compared to full fine-tuning and LoRA.\n\u2022 Implicit Regularization Limitation: Implicit regularization via zero initialization of U has to be further studied especially in highly dynamic environments where more flexible adaptations are needed.\n\u2022 Optimal Rank and Alpha Selection: Investigating methods for automatically selecting the optimal rank and alpha for CURLORA could further improve performance.\n\u2022 Combination with Other Techniques: Exploring the integration of CURLORA with other continual learning techniques could yield even better results."}, {"title": "Conclusion", "content": "This paper introduced CURLORA, a novel approach to fine-tuning large language models that leverages CUR matrix decomposition to mitigate catastrophic forgetting and improve computational efficiency. Through theoretical analysis and empirical experiments, we demonstrated that CURLORA outperforms standard LoRA in maintaining model stability and performance across tasks while significantly reducing the number of trainable parameters.\nKey contributions of this work include:\n\u2022 A novel modification to CUR decomposition using inverted probabilities for column and row selection and initiating U matrix as zeros. Sampling columns and rows based on inverted probabilities distinguishes CURLORA from traditional CUR, offering better stability and performance.\n\u2022 Theoretical analysis of how CURLORA addresses catastrophic forgetting.\n\u2022 Empirical evidence of CURLORA's effectiveness across multiple tasks and evaluation metrics with multiple models.\nOur results suggest that CURLORA is a promising approach for efficient and stable fine-tuning of large language models, particularly in scenarios with limited fine-tuning data. CURLORA's approach to mitigating catastrophic forgetting has broad implications for continual learning in NLP and beyond. Future research could explore its integration with other adaptation techniques to enhance model robustness"}]}