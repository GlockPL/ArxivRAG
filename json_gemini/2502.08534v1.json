{"title": "Input convex neural networks: universal approximation theorem and implementation for isotropic polyconvex hyperelastic energies", "authors": ["Gian-Luca Geuken", "Patrick Kurzeja", "David Wiedemann", "J\u00f6rn Mosler"], "abstract": "This paper presents a novel framework of neural networks for isotropic hyperelasticity that enforces necessary physical and mathematical constraints while simultaneously satisfying the universal approximation theorem. The two key ingredients are an input convex network architecture and a formulation in the elemen- tary polynomials of the signed singular values of the deformation gradient. In line with previously published networks, it can rigorously capture frame-indifference and polyconvexity - as well as further constraints like balance of angular momentum and growth conditions. However and in contrast to previous networks, a universal approximation theorem for the proposed approach is proven. To be more explicit, the proposed network can approximate any frame-indifferent, isotropic polyconvex energy (provided the network is large enough). This is possible by working with a sufficient and necessary criterion for frame-indifferent, isotropic polyconvex functions. Comparative studies with existing approaches identify the advantages of the proposed method, particularly in approximating non-polyconvex energies as well as computing polyconvex hulls.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Motivation for physics-based neural networks in constitutive modeling", "content": "Recent developments have shown that data-driven and machine learning methods such as neural networks are indeed useful alternatives to classical formulations of physical models [Kirchdoerfer & Ortiz, 2016; Linden et al., 2023; Bartel et al., 2023]. They pave the way for automatic model generation and offer enormous flexibility in terms of functional relationships [Thakolkaran et al., 2022]. A potential for the improvement of such approaches was found in embedding physical knowledge and principles into them [Kumar & Kochmann, 2022; Peng et al., 2021; Moseley, 2022; Geuken et al., 2024].\nThe present work specifically aims at the incorporation of physical principles into neural networks, because this coupling can improve the predictions significantly. One of the first combinations of neural networks and constitutive modeling of solids are the works by Shen et al. [2004] and Liang & Chandrashekhara [2008]. Since then many different approaches were published, e.g. physics-informed neural networks (PINNs) [Lagaris et al., 1998; Raissi et al., 2019] with many extensions and variations, e.g. [Kashefi & Mukerji, 2022; Jagtap & Karniadakis, 2020; Henkes et al., 2022]. See also [Hu et al., 2024] for a recent review on developments of PINNs. Linka et al. [2021] improved data-driven constitutive modeling by continuum-mechanics knowledge in a unified network approach referred to as constitutive artificical neural networks (CANNs). In [Linka & Kuhl, 2023] this concept was enhanced to autonomously discover constitutive models and learn an associated set of physically meaningful parameters. The variety of this research field furthermore covers physics-augmented neural networks [Liu et al., 2021; Klein et al., 2023; Fuhg et al., 2024; Benady et al., 2024], mechanics-informed ones [As'ad et al., 2022; Li et al., 2023] and many more neural network approaches incorporating physics [Zlatic & Canadija, 2024; St. Pierre et al., 2023; Canadija et al., 2024; Chen & Guilleminot, 2022; Li et al., 2023; Meyer & Ekre, 2023; Settgast et al., 2020]. Two contributions, [Linden et al., 2023] and [Klein et al., 2022], will later serve as valuable references in Section 2 because of their role for polyconvex hyperelastic models. Linden et al. [2023] proposed a comprehensive guide to enforce physics in neural networks for hyperelasticity and Klein et al. [2022] provided two neural network frameworks for polyconvex anisotropic hyperelasticity. The present study will precisely add a new focus towards a more general incorporation of polyconvexity with a universal approximation property in isotropic hyperelasticity."}, {"title": "1.2 Challenges of physical constraints for hyperelastic modeling", "content": "The mathematical formulation of material models is subject to various principles and restrictions, of which this work focuses specifically on isotropic hyperelastic materials. From a physical viewpoint, a hyperelastic model should [Truesdell & Noll, 1965]:\n1. Be path-independent. Accordingly the stresses derive from a potential \u03a8, e.g. the first Piola-Kirchhoff stress tensor\n$$P = \\frac{\\partial \\Psi}{\\partial F}$$\n2. Fulfill the second law of thermodynamics (which trivially yields zero dissipation by the existence of a potential \u03a8)\n$$P : \\dot{F} = 0.$$\n3. Be objective with respect to the orthogonal group of rotations SO(3) applied to the current configuration (frame-indifference)\n$$\\Psi(F) = \\Psi(Q \\cdot F) \\quad \\forall Q \\in SO(3)$$"}, {"title": "1.3 Open challenge of a universal, frame-indifferent, isotropic polyconvex ANN approximation for hyperelasticity", "content": "The current aim is to develop a neural network framework for isotropic hyperelasticity that fulfills all desired physical and mathematical principles (1)-(7) in an exact way and still possesses a universal approximation capability. To achieve an integration of the physical and mathematical constraints without undesired restrictions of the solution space, the work develops along the following highlights:"}, {"title": "2 Designing frame-indifferent and polyconvex functions", "content": "The design of material models or their neural network surrogates becomes even more challenging when the aforementioned physical and mathematical constraints need to be considered. Especially the restrictions associated with frame-indifference, isotropy and polyconvexity are difficult to reconcile since naive approaches often lead to contradictions as will also be highlighted later. Several modeling frameworks were hence developed for this task. In the following, we recap two frameworks that are also used in the context of polyconvex material modeling with neural networks. Delivering a valuable basis, their different focus yet misses the exact fulfillment of frame-indifference or uses an overly strict implementation, respectively. While these two approaches will be briefly recapped as references, a third approach is discussed in more detail to derive an improved strategy for neural network modeling. Its advantage lies in a sufficient and necessary (and thus equivalent) condition for polyconvexity under the assumption of frame-indifference and isotropy."}, {"title": "2.1 Approach 1: Anisotropic polyconvex hyperelasticity", "content": "The first approach is for the more general, anisotropic case. Klein et al. [2022] proposed to work directly with the definition of polyconvexity leading to parametrization\n$$\\Psi(F, cof F, det F),$$\nwhere convexity of \u03a8 is implemented by the neural network design. While the energy in (8) is by design polyconvex, it is not a priori frame-indifferent. Certainly, one could alternatively start directly from an energy in terms of C = FT.F. In this case, frame-indifference would be a priori fulfilled. However, enforcing polyconvexity for a representation in terms of C remains unclear. For instance, the St. Venant-Kirchhoff model is convex in C (and thus polyconvex in C), but it is known to be not polyconvex in F, cf. [Raoult, 1986]. As a consequence, a rigorous a priori enforcement of frame-indifference and polyconvexity at the same time is not possible for the most general anisotropic case at least, not at the present time. For this reason, frame-indifference of energy (8) is only approximated in [Klein et al., 2022] on a discrete set of rotations. Similarly, invariance with respect to a given symmetry group can be enforced or approximated. This is implemented by data augmentation for the training of a neural network."}, {"title": "2.2 Approach 2: Isotropic polyconvex hyperelasticity based on invariants", "content": "Linden et al. [2023] worked directly with invariants I\u2081 := tr C = F : F, I2 := tr(cof C) = cof F : cof F and I3:= det C = det F of the right Cauchy-Green tensor C, a priori fulfilling frame-indifference and isotropy, i.e.\n$$\\Psi(F) = \\tilde{\\Psi}(I_1, I_2, I_3).$$\nSince the invariants I1, I2 and 13 are evidently convex in F, cof F and det F, a sufficient criterion for to be polyconvex is that \\tilde{\\Psi} is convex and non-decreasing in the first two invariants and convex in the third. The non-decreasing constraint originates from the fact that the composition of a non-decreasing convex function with a convex function is convex. Since this is a sufficient, but not a necessary condition, some polyconvex energies cannot be designed by means of Eq. (9). To be more precise, for instance, terms that are linear in F can not be approximated since that requires the application of the square root, which is not convexity preserving. Thus, working with invariants of the right Cauchy-Green tensor is not ideal."}, {"title": "2.3 Approach 3: Isotropic polyconvex hyperelasticity based on signed singular values", "content": "Wiedemann & Peter [2023] proposed to work with the signed singular values (v1, v2 and v3) of F in order to obtain a sufficient and necessary condition for the polyconvexity of \u03a8; see also [Dacorogna & Koshigoe, 1993] for the 2D case. Starting from the spectral decomposition of the right Cauchy-Green deformation tensor\n$$C = F^T \\cdot F = \\sum_{i=1}^3 \\lambda_i N_i \\otimes N_i,$$\nthe singular values \u03c3i of F (principal stretches) are simply the square root of the eigenvalues of C, i.e. \u03c3i =\n\u221a\u03bbi > 0. The signed singular values vi have the same absolute value, |vi| = \u03c3i, but can become negative\nby rotation. The choice of their sign, however, does not affect the determinant (orientation preserving),\nyielding v1 v2 v3 = \u03c31 \u03c32 \u03c33. Apparently, frame-indifference can be a priori guaranteed by using (signed)\nsingular values, as they relate to the frame-indifferent eigenvalues.\nFor every frame-indifferent and isotropic (energy) function \u03a8(F) there exists a unique function \\tilde{\\Psi}(v1, v2, v3)\nthat characterizes \u03a8 in terms of the signed singular values of F; see also [Dacorogna, 2008, Proposition 5.31].\nThis description will help to achieve polyconvexity. In line with [Wiedemann & Peter, 2023], the function \u03a8\nis referred to as singular value polyconvex if the associated \\tilde{\\Psi} is polyconvex, i.e.\n$$\\tilde{\\Psi} singular value polyconvex :\\Leftrightarrow \\Psi polyconvex$$\nAs an intermediate step, we consider frame-indifference and isotropy, or mathematically speaking SO(3) \u00d7\nSO(3)-invariance of \u03a8. This implies that \\tilde{\\Psi} is I3-invariant with\n$$I_3 := {B \\cdot diag(\\epsilon) | B \\in Perm(3), \\epsilon \\in {-1,1}^3, \\epsilon = \\epsilon_1 \\epsilon_2 \\epsilon_3 = 1},$$\nwhere Perm (3) denotes the set of permutation matrices. To give a simplified interpretation, the I3-invariance includes the four symmetries\n$$\\tilde{\\Psi}(v_1, v_2, v_3) = \\tilde{\\Psi}(-v_1, v_2, v_3) = \\tilde{\\Psi}(-v_1, v_2, -v_3) = \\tilde{\\Psi}(v_1, -v_2, -v_3)$$"}, {"title": "3 Physics-based convex signed singular value neural network for isotropic hyperelasticity", "content": "Using Theorem 1 (Eq. (14)), we present a new neural network framework that is able to approximate any frame-indifferent, isotropic polyconvex function. We achieve this by an input convex neural network (ICNN) [Amos et al., 2017] that works with the elementary polynomials of the signed singular values as input. In order to account for the I3-invariance of the potential, we propose\n$$\\Psi^{INN} = \\frac{1}{24} \\sum_{j=1}^{24} NN(x^{(j)}),$$\nwhere x(i) \u2208 six permutations times four reflections of x(1), cf. Tab. 1\nSee Tab. 1 for all combinations of x(i). It should be emphasized that the same NN and therefore the same weights have to be used for every input permutation in order to guarantee \u03a03-invariance. We define the neural network over the generalized input x using the architecture\n$$z_{i+1} = g_i \\left( W^{(z)}_i z_i + W^{(x)}_i x + b_i \\right) \\quad \\text{for } i = 0, ..., k-1 \\text{ and } NN(x; \\Theta) = z_k,$$\nwhere zi are the layer activations (z0 = 0, W(z)0 = 0), \u0398 = {W(z)1:k\u22121, W(x)1:k\u22121, b0:k\u22121} are the weights, gi are non-linear activation functions and k is the number of layers. The overall architecture is sketched in Fig. 1.\nThen NN is convex in x if all W(z)1:k\u22121 are non-negative and all activation functions gi are convex and non- decreasing [Amos et al., 2017]. Huang et al. [2020] and Chen et al. [2019] provided a universal approximation theorem for convex functions for an architecture of type (20) when ReLu or Softplus activation functions are used, proving the representation power of ICNNs, see also Lemma 2. A universal approximation theorem for the underlying neural network is proven in Section 4.\nTo summarize, \u03a8NN represents a hyperelastic potential, is frame-indifferent, isotropic, polyconvex, guaran- tees a symmetric Cauchy stress and fulfills the Clausius-Duhem inequality. Hence, the framework fulfills all desired constraints 1\u20135 and 7 exactly. Further constraints, such as the growth conditions (6a) and (6b) or energy and stress normalization, can be easily enforced by adding respective terms to the potential\n$$\\Psi = \\Psi^{NN} + \\Psi^{energy} + \\Psi^{stress} + \\Psi^{growth}.$$\nSince those constraints are not important for the underlying work and sometimes even not desired, they are not considered further at this point and the interested reader is referred to [Linden et al., 2023] for details on how to design such terms. Within this work those properties will be learned by the neural network from respective training data.\nFinally, the first Piola-Kirchhoff stress tensor defined by the network potential reads\n$$P^{NN} = \\frac{\\partial \\Psi^{NN}}{\\partial F} = \\frac{1}{24} \\sum_{j=1}^{24} \\sum_{k=1}^{3} \\frac{\\partial NN(x^{(j)})}{\\partial x^{(j)}} \\frac{\\partial x^{(j)}}{\\partial v_k} \\frac{\\partial v_k}{\\partial F}.$$"}, {"title": "4 Universal approximation theorem for frame-indifferent, isotropic polyconvex functions", "content": "In this section, we provide a universal approximation theorem for the frame-indifferent, isotropic polyconvex CSSV-NN, i.e. we show that one can approximate any frame-indifferent, isotropic polyconvex function \u03a8 up to arbitrary accuracy by a frame-indifferent, isotropic polyconvex CSSV-NN. First, we recall the universal approximation of convex functions by ICNNs. Second, we transfer this property to singular value poly- convex functions. The proof for frame-indifferent, isotropic polyconvex functions is completed by using the equivalence in (17).\nLet \u03a9\u2282 Rn be a compact set and f: \u03a9 \u2192 R be convex and, in particular, continuous. Then, for every \u025b > 0 there exists an ICNN such that\n$$sup |NN(x) \u2013 f(x)| < \\epsilon.$$\nThe proposed network is an ICNN by design and a proof for the universal approximation theorem is provided in [Huang et al., 2020] for ReLU and Softplus activation functions for \u03a9 \u2286 [0,1]n. Their result can be easily extended to an arbitrary compact set \u03a9 \u2282 Rn.\nLet \u03a8: R7 \u2192 R\u221e be lower semicontinuous and convex such that \u03a8 \u25e6 m is I3-invariant. Let V \u2282 R3 be a compact set where \u03a8 attains only finite values. Then, for every \u025b > 0, there exists a neural network NN : R7 \u2192 R\u221e of the form (18) such that\n$$sup |\\Psi^{NN}(m(v)) \u2013 \\Psi(m(v))| < \\epsilon.$$\nSince  we approximate \u03a8 on m(V) with NN by accuracy \u025b. The I3-invariance of \u03a8 implies \\tilde{\\Psi}(m(v)) = \\tilde{\\Psi}(Pj(m(v))) for x(i) = Pj(x(1)) according to Tab. 1. Due to the group structure of \u03a03 the equality holds also for Pj\u22121. We compute with the \u25b3-inequality, I3-invariance and the approximation"}, {"title": "5 Algorithmic implementation", "content": "The proposed neural network model was implemented using Python and TensorFlow."}, {"title": "5.1 Network architecture", "content": "The Softplus function gSP(x) = ln(1 + exp(x)) is employed as activation, except for the last layer, which has linear activation. Both functions are convex, non-decreasing and C\u221e and thus fulfill the requirements for ICNNs used in the present polyconvex framework. Furthermore, this choice also guarantees universal approximation (see previous section). We directly set W(x)v1 = 0, because the input arguments would cancel each other out due the employed symmetry anyway (except for the determinant for which the weight is also set to zero for implementational convenience). This does not affect the representation power or the universal approximation in any means.\nEight different network sizes were investigated in terms of number of layers and neurons per layer. We denote them by \"size of input vector - number of neurons of hidden layer 1 - ... - number of neurons of hidden layer k-1 - size of final layer k\":"}, {"title": "5.2 Sobolev training", "content": "The different networks are trained in the Sobolev space by strain-stress tuples\n$$D = \\{(F^{(1)}, p^{(1)}), (F^{(2)}, p^{(2)}), ..., (F^{(n)}, p^{(n)})\\}$$\nbased on the mean squared error\n$$MSE = \\frac{1}{n} \\sum_{i=1}^n || P^{(i)} - P^{NN}(F^{(i)};\\Theta) ||^2$$\nwith the Froebenius norm || \u2022 ||. The MSE represents deviations in the stress space and therefore even has a physical interpretation. Other choices for designing the loss function are of course also possible, e.g. the networks can also be trained on strain-energy tuples or a combination of strain-energy and strain-stress tuples. However, training on strain-stress tuples only, yielded the best results for preliminary test cases and often is a natural choice when using experimental training data.\nThe following load cases have been used for training with analytical reference models (load direction Fij,"}, {"title": "5.3 Approximation of polyconvex hulls", "content": "The presented framework can also be used for the approximation of a polyconvex hull defined as\n$$PC(\\Psi)(F) := sup \\{V(F) | V \\text{ polyconvex, } V < \\Psi\\}$$\nfor a (potentially non-polyconvex) energy \u03a8. Again, there exist multiple options on how to design a loss function for that case. The idea here is to enhance the loss function by a penalty term, which penalizes energies larger than the true energy to ensure\n$$\\Psi^{INN}(F^{(i)}; \\Theta) \u2264 \\Psi(i) \\quad \\forall i$$\nfor all data points in the set of strain-energy tuples\n$$D = \\{(F^{(1)}, \\Psi^{(1)}), (F^{(2)}, \\Psi^{(2)}), ..., (F^{(n)}, \\Psi^{(n)})\\}.$$\nThis is realized by the penalty function\n$$PEN = \\alpha \\sum_{i=1}^n max (\\Psi^{INN}(F^{(i)}; \\Theta) - \\Psi(i), 0),$$\nwhere \u03b1 is a penalty factor. For the numerical example in Section 6.3 a value of \u03b1 = 1 was already sufficient to ensure constraint (28). The network is then trained based on the strain-energy tupels together with the penalty term, leading to the loss function\n$$L = \\frac{1}{n} \\sum_{i=1}^n || \\Psi^{(i)} - \\Psi^{NN}(F^{(i)}; \\Theta) ||^2 + PEN.$$"}, {"title": "6 Numerical results", "content": "To validate the framework from Section 5, three different numerical examples are conducted. The first two examples focus on the identification of a hyperelastic energy for a given set of data following the implementation presented in Sections 5.1 and 5.2, while the third one illustrates the ability to approximate polyconvex hulls according to Section 5.3.\nAll neural network energy predictions were normalized for the first two examples such that \\tilde{\\Psi}(F = I) = 0 holds. As a helpful reference to explore the potentials and limitations of neural network modeling, we compare the results of the proposed CSSV-NN approach with predictions obtained from the neural networks of Linden et al. [2023], which are referred to as PANNs. The hyperparameter settings and the training procedure from Section 5.2 are equal for both approaches for comparison reasons."}, {"title": "6.1 CSSV-NN model of a polyconvex signed singular value energy", "content": "The first example is a polyconvex energy directly based on signed singular values\n$$\\Psi^{ssve}(F) = |v_1(F)| + |v_2(F)| + |v_3(F)|+ \\frac{1}{10} |(det F)|^{10}$$\nA hyperparameter study showed that architecture size 7 performs best in terms of the lowest overall MSE (as defined in Eq. (26)) for the training data set with a value of 0.0146, see Fig. 2 left. While the other large architectures (4, 5, 6 and 8) fall into approximately the same low regime of MSE values, the smaller models (1, 2 and 3) do not approximate the energy as accurately and show approximately 50 times higher errors for their best MSE. In contrast, none of the PANN models can capture the energy accurately. For every architecture the PANN framework performs worse than the CSSV-NNs as compared in Fig. 2. The best PANN model obtained for architecture 3 has a 140 times higher MSE than the best CSSV-NN. These differences can be explained by a closer look at the energy and stress responses.\nConsidering the energy and stress predictions, we compare the best CSSV-NN architecture (size 7), the analytical reference and the best PANN architecture (size 3) for different load cases, see Fig. 3. The CSSV- NN fits the reference solution visually perfectly for volumetric compression/extension (middle of Fig. 3) and simple shear (right). Even the extrapolation outside of the volumetric training stretches 0.9 < \u03bb < 1.1 and trained shear 0 \u2264 X \u2264 0.1 works extremely well. Under uniaxial compression/tension (left), the prediction is perfect for smaller deformations, while small deviations occur for large tension. If better approximations are desired for specific deformations ranges, these can be accomplished by heavier weights in the loss function or more training data points in that region. We refrain from such a posteriori modifications, though, for better comparison and interpretation of the frameworks themselves. However, it should be emphasized once more that the minor differences can indeed be completely eliminated - thanks to the universal approximation property of the neural network."}, {"title": "6.2 CSSV-NN model of a non-polyconvex Hencky energy", "content": "The second example is a non-polyconvex Hencky energy\n$$\\Psi^{\\text{Hencky}}(F) = \\frac{1}{2} (ln(\u03bb_1) + ln(\u03bb_2) + ln(\u03bb_3))^2 + ln(\u03bb_1)^2 + ln(\u03bb_2)^2 + ln(\u03bb_3)^2$$\nwith \u03bbi being the eigenvalues of the right stretch tensor U that are equal to the absolute values of the signed singular values of F (principal stretches). The material parameters in terms of the Lam\u00e9 constants are set to \u03bb = 1MPa and \u00b5 = 1MPa corresponding to a Young's Modulus of E = 2.5 MPa and a Poisson ratio of v = 0.25. According to [Bruhns et al., 2001], this energy is not polyconvex. However, it is clear that both approaches under investigation, CSSV-NNs and PANNs, are inherently polyconvex and thus can not fit the non-polyconvex Hencky energy perfectly. However, it is often of great importance to find the best polyconvex approximation to non-polyconvex (training) data.\nThe best CSSV-NN architecture (size 8) performs with a training data MSE of 0.203 (see Fig. 2 right). Again, it shows lower MSEs when compared to PANNs throughout all network sizes. This is due to the fact that the polyconvexification of an energy requires linear energy growth in terms of the stretches of F"}, {"title": "6.3 Approximation of a polyconvex hull", "content": "In order to analyze the CSSV-NN's capability to approximate polyconvex hulls, we consider a model for the macroscopic response of nematic elastomers [Bladon et al., 1993]. It is examined here, because DeSimone & Dolzmann [2002] derived an analytic polyconvex hull for the proposed energy class that can be used as a reference solution. The (non-polyconvex) energy then reads\n$$\\Psi^{nematic}(F) = \\begin{cases}\n \\frac{1}{8} \\left( \\frac{1}{\u03bb_1^p} + \\frac{1}{\u03bb_2^p} + \\frac{1}{\u03bb_3^p} \\right) + \\frac{p}{2} \\ln{\u03bb_1 \u03bb_2 \u03bb_3} - 3 & \\text{if } det F = 1, \\\\\n \\infty & \\text{else},\n\\end{cases}$$\nwith the already introduced eigenvalues \u03bb1 \u2264 \u03bb2 \u2264 \u03bb3 of the right stretch tensor U being equal to the absolute values of the signed singular values of F (principal stretches), the exponent p \u2208 (2,\u221e) and the parameters 0 < \u03b31 \u2264 \u03b32 \u2264 3 satisfying \u03b31 \u03b32 \u03b33 = 1. Here, we choose p = 2, \u03b31 = 0.5, \u03b32 = 1 and \u03b33 = 2. The infinite energy for deformations with det F \u2260 1 accounts for the incompressibility of nematic elastomers. We however do not have to consider this for the underlying application, since we only train and evaluate the neural network on data points of isochoric deformations. As a direct result, the energy only depends on two independent eigenvalues and the third follows from the constraint det F = 1. The first two eigenvalues are discretized as \u03bb1, \u03bb2 \u2208 \u03a9\u03b1 = [0.4, 0.5, ..., 1, 1.4, ..., 5]2 for training data generation. The energy of the training data is furthermore min-max normalized and shifted by +0.5 to improve training performance. Again, 30 randomized weight initializations were investigated and every network was trained with TensorFlow's SGD optimizer for 1000 epochs with a learning rate of 0.1 and a batch size of 2 based on the penalty enhanced loss function given by Eq. (31). The lowest overall loss for the training data domain is obtained for architecture 4 with a value of 0.00123. Note that it can never reach zero for a non-polyconvex energy due to the enforcement of polyconvexity.\nThe (non-polyconvex) nematic energy, the analytic polyconvex hull and the CSSV-NN convex hull pre- diction are depicted in Fig. 5 for qualitative comparison. While providing a polyconvex approximation that performs especially well at larger stretches, it tends to only slightly undershoot at three corners of the domain. For a quantitative comparison, though, the domain of the polyconvexified hull must be considered. At first, one might expect that the network converges against the analytic polyconvex hull, but this is not the case. The reason is that the network only approximates the hull within the given domain of the training data, while the analytic polyconvexification considers the full domain (0,\u221e]2. Thus, the neural network solution is domain-dependent and the analytic polyconvex hull is domain-independent by construction. Depending on the intended application, this can be a major advantage if a polyconvex hull is only of interest for a limited range of operational loads. A domain-limited polyconvexification can then yield improved performance and solutions. At the same time, this can be a limitation of data-based frameworks operating on a bounded domain and requires a clear problem definition in practice. For the present training data domain, the error of the CSSV-NN network is smaller than the analytic approach, cf. Fig. 5. For a finer discretization such as \u03a9test = [0.4, 0.41, 0.42, ..., 5]2, for instance, the neural network achieves a loss value (according to Eq. (31)"}, {"title": "7 Conclusion", "content": "A novel Convex Signed Singular Value Neural Network (CSSV-NN) suitable for approximating isotropic hyperelastic energies was presented. It a priori fulfills frame-indifference, isotropy and polyconvexity while still satisfying the universal approximation theorem. Accordingly and in contrast to previous networks, any frame-indifferent, isotropic polyconvex hyperelastic energy can be approximated up to a desired accuracy. Its construction is based on the coupling of an input convex neural network with the elementary polynomials of the signed singular values as input. Respective invariances are ensured by a summation over network evaluations. A relevant, underlying property is the sufficient and necessary criterion for frame-indifferent, isotropic polyconvex functions. Practicability of the ANN implementation is guaranteed by building upon standard methods and constraints from Tensorflow.\nCompared to previous networks and due to the universal approximation theorem, the CSSV-NN showed superior performance for polyconvex and non-polyconvex energies. A particular advantage is the coverage of regimes where the energy shows linear growth with respect to the stretches. This property is especially important for approximating non-polyconvex energies and polyconvex hulls. Within the implementation for polyconvex hulls, the network only approximates the hull within the given domain of the training data, while the analytic polyconvexification considers the full domain. Thus, the neural network solution is domain- dependent and the analytic polyconvex hull is domain-independent by construction. Depending on the intended application, this can be a major advantage of the CSSV-NN. To be more precise, a domain-limited polyconvexification can yield an improved approximation within the considered data interval.\nEmploying signed singular values into an artificial neural network is indeed very powerful for approximat- ing polyconvex energies. In future works, it shall be extended on the one hand to other material classes such as incompressible and anisotropic models. On the other hand, a hyperparameter guideline for experimental data is to be developed."}]}