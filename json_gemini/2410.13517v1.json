{"title": "Bias in the Mirror : Are LLMs opinions robust to their own adversarial attacks ?", "authors": ["Virgile Rennard", "Christos Xypolopoulos", "Michalis Vazirgiannis"], "abstract": "Large language models (LLMs) inherit biases\nfrom their training data and alignment pro-\ncesses, influencing their responses in subtle\nways. While many studies have examined these\nbiases, little work has explored their robustness\nduring interactions. In this paper, we introduce\na novel approach where two instances of an\nLLM engage in self-debate, arguing opposing\nviewpoints to persuade a neutral version of the\nmodel. Through this, we evaluate how firmly\nbiases hold and whether models are suscepti-\nble to reinforcing misinformation or shifting\nto harmful viewpoints. Our experiments span\nmultiple LLMs of varying sizes, origins, and\nlanguages, providing deeper insights into bias\npersistence and flexibility across linguistic and\ncultural contexts.", "sections": [{"title": "1 Introduction", "content": "Similar to humans, it is widely recognized that\nlanguage models inherit biases through both their\ntraining and alignment processes (Feng et al., 2023;\nScherrer et al., 2024; Motoki et al., 2024). Identify-\ning the opinions and values that LLMs possess has\nbeen a particularly intriguing area of research, as it\ncarries significant sociological and quantitative im-\nplications for real-world applications (Naous et al.,\n2023). Understanding the biases embedded in these\npowerful tools is crucial, given their widespread\nuse and the potential influence they may exert on\nusers, often in unintended ways (Hartmann et al.,\n2023) or in downstream tasks, such as content mod-\neration. While the biases of media outlets are gen-\nerally apparent through their political leanings, lan-\nguage models, that appear as neutral tools, can\ninfluence users in more subtle ways.\nOn the other hand, while it is essential to as-\nsess the biases inherent to language models, any\nconclusions drawn on sociological issues must be\napproached with great caution. Expressing that\na model holds harmful opinions without conduct-ing thorough robustness testing can have negative\nconsequences.R\u00f6ttger et al. (2024) highlights that\nmuch of the prior research on biases in large lan-\nguage models lacks robustness, often forcing mod-\nels into binary choices, disregarding subtler change\nin opinions due to question paraphrasing, and fail-\ning to simulate realistic use cases. This undermines\nthe validity of such findings and calls for more\nnuanced and comprehensive evaluations.\nExisting work has primarily focused on prompt-\ning models to display alternative biases by directly\ninjecting them or fine-tuning them to adopt new bi-\nases. In this paper, our objective is to evaluate the\nextent and persistence of biases when confronted\nwith contradictory prompts, without introducing ad-\nditional bias through training or background knowl-\nedge, but instead by trying to let the model con-\nvince itself through debating. We assess the robust-\nness of both initial biases and post-contradiction bi-\nases across different languages of prompting. Eval-\nuating biases across multiple languages is critical as\nLLMs trained in one linguistic and cultural context\nmay not generalize fairly or accurately to others,\nleading to culturally inappropriate or biased out-\nputs when used globally. Our multilingual experi-\nments further reveal that models exhibit different\nbiases in their secondary languages, such as Arabic\nand Chinese, which underscores the importance of\ncross-linguistic evaluations in understanding bias\nresilience. Furthermore, we introduce a compre-\nhensive human evaluation to compare how humans\nrespond to contradictions on a range of topics, con-\ntrasting these results with those of the LLMs.\nIn summary, the contributions of this paper are\nthe following:\n\u2022 Comprehensive Evaluation of Bias in\nLLMs: We conduct an extensive analysis\nacross a diverse set of large language models,\nvarying in parameter size, accessibility (both\nproprietary and open-source), and trained on"}, {"title": "2 Related work", "content": "different native datasets reflecting their geo-\ngraphical origins. This broad evaluation en-\nhances our understanding of bias across dif-\nferent models. Additionally, we propose a\nnovel approach to assess bias by prompting\nthe model to engage in self-debate, where two\ndifferent instances of the same model are in-\nstructed to argue opposing viewpoints in an\nattempt to persuade a neutral, unmodified ver-\nsion of the model, thus evaluating whether a\nmodel's stance can be shifted without intro-\nducing artificial bias from additional training\ndata or personalities.\n\u2022 Language of Prompting: We investigate the\nimpact of language on bias detection for one\nsame LLM, examining how language varia-\ntions affect the expression of biases. This pro-\nvides valuable insights into the multilingual\nand cross-linguistic behavior of LLMs.\n\u2022 Human vs. LLM Comparison: We conduct\ncomprehensive human evaluations, comparing\nhow humans and LLMs respond to contradic-\ntions on a range of topics. This comparison\noffers important insights into the alignment\n(or divergence) between LLM reasoning and\nhuman reasoning in the face of contradictions,\nshedding light on the models potential use in\nreal-world decision-making contexts."}, {"title": "2.1 Surveying LLMs", "content": "The study of biases in large language models\n(LLMs) has been extensively explored, particularly\nfrom political and cultural perspectives. Tests like\nthe Political Compass (Feng et al., 2023; Rozado,\n2023; Rutinowski et al., 2024), the Political Coor-\ndinates test (Motoki et al., 2024; Rozado, 2024),\nand the Pew American Trends Panel (Santurkar\net al., 2023) have been used to measure political\nbiases. In cultural settings, approaches like the\nCultural Alignment test (Cao et al., 2023; Masoud\net al., 2023) assess how closely models align with\ncultural norms. A limitation of these methods is\ntheir tendency to force models to take a stance,\noften by using multiple-choice options, which pre-\nvents neutral or nuanced responses. This design\ncan exaggerate biases, as the models are not given\nthe option to provide more balanced or uncertain\nanswers."}, {"title": "2.2 LLMs and Debates", "content": "Moreover, most studies test model bias with lim-\nited robustness checks, typically repeating exper-\niments only a few times. This lack of repetition\ncan overemphasize the detected biases. Alterna-\ntive methods have been used to evaluate political\nor cultural bias, notably Bang et al. (2024), who\npropose assessing bias in models on specific topics\nby using positive and negative news article titles\nas anchors and measuring distances to naturally\ngenerated titles. Similarly, Naous et al. (2023) cre-\nate a benchmark dataset to measure cultural biases\nin LLMs using masked prompts based on Arabic\ncultural entities, showing that LLMs favor Western\nentities even in Arab contexts.\nIn addition to evaluating biases, other research\nhas demonstrated how easily models can adopt\nharmful behavior through specific conditioning or\nfine-tuning (Taubenfeld et al., 2024; Feng et al.,\n2023). This research suggests that further pre-\ntraining can cause a language model to acquire\nnew biases, or that models exposed to biased de-\nscriptions may initially shift but eventually revert\nto their original viewpoints after continued interac-\ntion.\nIn all cases, the consistency of models plays a\nkey role in bias assessment and understanding how\nthey process information over time. Elements like\nquestion phrasing, the sequence of discussions, and\nthe criteria for evaluating responses all significantly\nimpact the evaluation outcomes.\nDebate as a framework for eliciting more truthful\nand accurate responses from large language models\n(LLMs) has gained attention recently. Khan et al.\n(2024) investigate how structured debates between\nexpert and non-expert models can improve accu-\nracy in reading comprehension tasks by utilizing ad-\nversarial critiques generated during the debate pro-\ncess. This research demonstrates that even weaker\njudges, including LLMs, can achieve high accuracy\n(76% with debate) when assessing stronger models'\narguments, significantly outperforming non-debate\nbaselines such as consultancy, where only a sin-\ngle model presents an argument (Michael et al.,\n2023). Taubenfeld et al. (2024) explored political\ndebates between LLM agents to examine how bias\nand identity impact attitude change during discus-\nsions, revealing persistent model biases affecting\noutcomes even in simulated multi-party debates.\nLastly, Liu et al. (2024)highlight various biases in\nLLM evaluations of debates, such as positional and"}, {"title": "3 Experimental setting", "content": "In this section, we describe the questions employed\nfor model evaluation, outline the models under con-\nsideration, and detail the experimental setup, in-\ncluding the prompting strategies utilized. Addition-\nally, we provide a clear explanation of the human\nevaluation methodology applied."}, {"title": "3.1 Question selection for bias evaluation", "content": "Much of the prior research on political bias de-\ntection approaches the problem through a binary\nframework, often focusing on partisan affiliations\n(e.g., \"Answer as a Democrat, your views\nare....\"). However, political ideology is more\naccurately represented as a spectrum rather than\na binary choice (Rokeach, 1973; Gindler, 2022).\nTo capture this complexity, many studies have\nadopted the widely recognized Political Compass\ntest, which evaluates individuals' political posi-\ntions based on their responses to 62 statements\nacross various topics, such as economics, soci-\nety, and religion. Respondents express their level\nof agreement with statements using a limited set\nof options, ranging from strong disagreement to\nstrong agreement. The Political Compass test then\nmaps responses onto a two-dimensional space, with\none axis representing economic views (left-right)\nand the other representing social views (libertarian-\nauthoritarian). This mapping provides a visual rep-\nresentation of a respondent's political orientation\nacross two key dimensions. While this method of-\nfers a structured metric for understanding biases, it\nhas notable limitations (R\u00f6ttger et al., 2024) such\nas the absence of neutral options or the oversimpli-\nfication of complex ideologies. To overcome these\nissues, we retain the questions from the Political\nCompass but allow models to respond using a scale\nfrom -10 to 10. This gives them the option to re-\nmain neutral, and enables us to evaluate small shifts\nin biases more precisely, capturing subtler move-\nments in ideology. Nevertheless, the compass lacks\ncomponents that assess attitudes towards misinfor-\nmation and conspiracy theories, which we propose\nto integrate into the evaluation framework, we also\npropose to add nonsensical question as a baseline.\nNonsensical questions such as \"It is much health-\nier to draw circles rather than triangles\" serve as\na baseline to measure how easily models can be\ninfluenced by illogical or implausible claims, pro-\nviding insights into the model's tendency to shift\nits bias even when faced with invalid arguments."}, {"title": "3.2 Models tested", "content": "In this study, we evaluated a wide range of lan-\nguage models, incorporating both open-source and\nproprietary architectures from diverse regions and\nof varying sizes, to ensure a comprehensive and\nrobust comparison. For open-source models, we\nincluded Llama 3.1 (Dubey et al., 2024) in its chat-\noptimized variants, specifically the 7B and 70B\nversions. We further incorporated models from the"}, {"title": "3.3 Experimental framework", "content": "Mistral (Jiang et al., 2023) family, namely Mis-\ntral Large and Mistral 7B, to represent a broader\nspectrum of publicly available architectures. Addi-\ntionally, we evaluated GPT-4 (Achiam et al., 2023).\nTo ensure geographical diversity in our model se-\nlection, we tested Jais 13B (Sengupta et al., 2023),\na model developed within the Arabic-speaking AI\ncommunity, as well as Qwen1.5 110B and Qwen2\n72b (Yang et al., 2024), a large-scale model orig-\ninating from the Chinese AI research ecosystem.\nThis diverse selection of models allowed us to ex-\nplore both regional and architectural differences,\nparticularly with respect to the impact of model\nsize, which ranged from 7B to 110B and more pa-\nrameters.\nTo establish a preliminary assessment of the values\nand potential biases of the large language models\n(LLMs) under evaluation, we propose specific mea-\nsurement methods. Each model is presented with\neach question twenty times, with independent eval-\nuations conducted to gauge the variance in bias ex-\nhibited by the model. We ask the models to respond\nusing numerical values between -10 and 10 to quan-\ntify the degree of agreement. The decision to use a\nscale passing through 0 (rather than 0 to 10) allows\nfor neutral responses and ensures a balanced repre-\nsentation of sentiment. This open-ended approach\ngenerally yields less extreme responses, providing\na more muted measure of bias than by asking the\nmodel to answer with a categorical option.\nFurther, two different methods are applied to\nevaluate bias robustness"}, {"title": "3.3.1 Debate robustness", "content": "Our framework, illustrated in Figure 1, operates as\nfollows: starting from a given question, we facili-\ntate a structured debate spanning four turns. The\ndebate is conducted between two speakers, each\nexplicitly assigned a particular point of view with-\nout personality bias introduction. The debate be-\ngins with the first speaker presenting an opening\nstatement. In response, the second speaker deliv-\ners their own opening statement, which also in-\ncludes a rebuttal to the first speaker's argument.\nFollowing this, the first speaker offers a rebuttal\nand concludes their argument. Finally, the second\nspeaker provides a concluding rebuttal to complete\nthe debate. This framework allows for the con-\ntrolled comparison of differing perspectives, with\neach speaker having opportunities to defend and re-fine their stance through structured dialogue. Each\nLLM will participate in five debates per question.\nFor each instance, an independent version of the\nmodel is prompted to express its stance on the de-\nbate question, before and after being exposed to\nthe debate. This setup allows us to measure how\na language model's opinion might be influenced\nthrough exposure to structured discussions and user\ninputs, simulating typical conversational dynamics.\nIn addition to the standard debates, we make\neach models undergo \"biased\" debates, in which\nthe instance of the model holding the original opin-\nion is prompted to perform as a bad debater, giving\narguments of lower quality and stammering. Exam-\nple abridged debates are available in Appendix A.\nFollowing each of the biased debates, the model is\nagain asked to assess its stance on the question. By\ncomparing the pre- and post-debate opinions across\nboth fair and biased conditions, we aim to mea-\nsure the impact of debate quality on the model's\nbiases. Specifically, we hypothesize that for ques-\ntions where the model's stance remains unchanged\nafter exposure to a biased debate, the underlying\nbias is likely stronger. This allows us to quantify\nthe model's susceptibility to external influence and\nidentify areas where biases are more deeply en-\ntrenched, and how easily a human can convince\nhimself of harmful ideas by having a confirmation\nbias through the models change of mind."}, {"title": "3.3.2 Multilingual Bias Evaluation", "content": "To further assess bias robustness and capture cross-\ncultural variations, we extend our framework by\nconducting multilingual experiments. This ap-\nproach not only introduces prompt variations across\ndifferent languages but also highlights the influ-\nence of cultural contexts embedded in the mod-\nels responses through bias in the original training\ndata. By replicating our debate framework across\nmultiple languages, we explore how linguistic and\ncultural diversity impacts bias expression and re-\nsilience in the largest models. Specifically, we con-\nduct debates in Arabic and Chinese for both GPT-4\nand Mistral Large, in Arabic for JAIS, and in Chi-\nnese for Qwen. These experiments are tailored\nto the native languages of the respective models,\nallowing us to evaluate their responses in linguisti-\ncally and culturally relevant contexts. By compar-\ning how models perform across English, Arabic,\nand Chinese, we can identify shifts in biases influ-\nenced by the language of prompting, and uncover\nany discrepancies in how models trained on dif-"}, {"title": "3.3.3 Human Response to Debates", "content": "ferent linguistic datasets internalize and express\nbiases. This multilingual setup provides a more\ncomprehensive view of bias, revealing how linguis-\ntic diversity in training data affects model robust-\nness and bias persistence across various social and\npolitical contexts.\nFor a smaller subset of questions, we involve 20\nhuman annotators to evaluate the shifts in their\nopinions before and after exposure to debates. We\nfocus on 16 specific questions the LLMs have seen\nacross eight distinct topics: Religion, Economy,\nRace, Misinformation, Nonsense, Culture, Femi-\nnism, and Sexuality. To ensure clarity annotators\nwere provided with the context of the task by be-\ning shown how the models handle the debate pro-\ncess. Annotators were drawn from diverse cul-\ntural backgrounds and gender identities to capture\na broad range of perspectives and mitigate cultural\nor gender-specific biases. We excluded biased de-\nbates in these experiments, as we have found them\nto have a lesser impact on human participants. By\ncomparing the shifts in opinion between humans\nand LLMs across these topics, we gain insight into\nthe strength of the held biases. Human response\nshifts will be analyzed, allowing for a comparison\nof the magnitude of change in both human and\nmodel responses across the eight topics."}, {"title": "4 Results", "content": "In this section, we aim to evaluate the robustness\nof the biases exhibited by the different LLMs."}, {"title": "4.1 Quantifying the Impact of Debate Strategies on Response Shifts", "content": "The results in Table 1 provide a summary of the\nstandard deviation of outputs for each model, the\naverage impact of paraphrasing, and the shift in val-\nues before and after debates under both fair and bi-\nased conditions. For each model, we report the av-\nerage standard deviation and the paraphrasing shift,where paraphrasing was performed using GPT-4\nto assess the expected variation due to model \"ran-\ndomness.\" Additionally, we present the bias shifts\nafter both fair and biased debates, indicating which\nmodels are more likely to change their responses.\nOur findings reveal that models from the Qwen\nsuite show minimal shifts in opinion compared to\nothers, along with GPT-4, with moderate shifts,\nwhile Mistral and Llama models appear less biased,\nexhibiting greater shifts between standard deviation\nand debate responses. This suggests that Mistral\nand Llama models are more flexible and less en-\ntrenched in their biases than the Qwen and GPT-4\nmodels.\nIn the following subsections, we focus on the\nspecific biases of the models across different sub-\nject areas. Instead of evaluating overall bias, we\nanalyze the extent of bias shifts within individual\nsubjects to identify where each model demonstrates\nstrong or weak bias tendencies. This approach al-\nlows us to pinpoint the specific domains in which\nthe average shift is highest or lowest, helping to\ntarget the areas where each model may be more\nbiased."}, {"title": "4.2 Studying categorical biases", "content": "We separate our questions in six distinct categories\nPolitical, Economical, Societal, Morality, Sexuality\nand Secularity.\nWe present our results in figure 2. All the mod-\nels are run with their default parameters. In this\nfigure, higher values show a more \"Progressive\"\nbias, while negative ones show a \"Conservative\"\nbias.\nProgressiveness vs. Conservativeness: All mod-\nels except for Mistral 7b and JAIS, demonstrate a\nstrong tendency toward progressive values across\nthe board, with an especially strong bias on topics\ntouching sexuality. Llama 70b and GPT-4 are the\nmost consistently progressive, showing high initial\nvalues across a wide range of political and societal"}, {"title": "4.3 Multilinguality and Bias Shifts in Prompting", "content": "categories, suggesting a firm alignment with liberal\nideologies. Qwen and Mistral Large, while still pro-\ngressive in certain areas, take a more moderate or\ncentrist approach, reflecting more flexibility and a\nless fixed ideological position than Llama 70b and\nGPT-4. Mistral 7b stands out as more conservative,\ndisplaying lower values across several categories,\nparticularly in areas like morality, where its outlook\nis more restrained or neutral compared to the other\nmodels.\nEffects of Biased vs Fair Debating: Fair de-\nbates tend to cause smaller shifts in the models'\nstances, while reinforcing their initial biases, when\nbiased debates typically introduce larger fluctua-\ntions in perspective. In many instances, fair debates\nstrengthen the models original views by provid-\ning reasoned arguments that align with their initial\nleanings, especially in cases where the leanings are\nmoderate in on direction. However, there are no-\ntable cases where fair debates manage to convince\nmodels to change their views, sometimes more ef-\nfectively than biased debates, that is particularly\ntrue when the model holds stronger biases. For ex-\nample, GPT-4 exhibited greater shifts in its stance\non societal issues after fair debates compared to\nbiased ones. This suggests that well-structured,\nbalanced debates are not only capable of reinforc-\ning existing biases but can also be more persuasive\nin prompting models to reconsider their positions\nin some cases."}, {"title": "4.4 Comparison to human annotators", "content": "Our multilingual experiments reveal that language\nsignificantly impacts bias expression. For instance,\nGPT-4, when prompted in Chinese, demonstrated\nmore conservative stances on societal issues com-\npared to its English responses, likely due to the cul-\ntural context embedded in its training data. Qwen,\na model primarily trained in Chinese, showed min-\nimal bias shifts across all categories, reflecting\nstronger entrenchment in culturally specific views.\nInterestingly, societal related topics exhibited the"}, {"title": "5 Conclusion", "content": "Our final set of experiments seeks to evaluate the\nstrength of bias in large language models by com-\nparing it to the strength of human bias on similar\nquestions. We focus on eight distinct topics: Secu-\nlarity, Economy, Race, Misinformation, Nonsense,\nCulture, Feminism, and Sexuality. For each topic,\nwe present two different questions. Additionally,\n20 human annotators, similar to the amount of time\neach models see a question, were asked to share\ntheir opinions before and after engaging in a debate\non these topics. As a baseline, we included \"non-\nsensical\" questions to measure how easily humans\ncan be influenced on topics where they hold no\nprior opinion. This setup helps us understand how\ndebates shift opinions and how model biases com-\npare to human tendencies across a range of complex\nsocial and cultural issues. Figure 4 shows that hu-\nman biases tend to remain stronger on most topics,\nindicating they are less easily influenced than mod-\nels. However, humans are more persuadable ontopics related to misinformation and nonsensical\nquestions.\nThis paper provided a detailed analysis of biases\nin various language models, comparing them to\nhuman biases. By testing bias robustness through\nmultilingual evaluations and model debates, we\nidentified areas where models are most vulnerable\nto bias and highlighted questions that prompt more\nnuanced or neutral responses. Our findings offer\nimportant insights into the impact of cultural and\ncontextual factors on language model behavior and\nbias expression."}, {"title": "Limitations", "content": "One significant limitation in assessing biases in\nNLP systems arises from the nature of human re-\nsponses to questions about sensitive topics. Re-\nsponses can be influenced by various factors, in-\ncluding dishonesty and a lack of knowledge, which\nmay not accurately reflect individuals' true beliefs.\nFor instance, while many people may assert that\nthey are not racist, underlying biases can still per-\nsist for these reasons. Moreover, models like large\nlanguage models (LLMs) are often designed to\navoid expressing controversial opinions on race\nand related issues. However, this does not guaran-\ntee that biases are absent in their outputs. Research\nsuch as the CAML paper illustrates this challenge;\nfor example, stories generated with Arabic names\nmay disproportionately associate these characters\nwith poverty, highlighting the subtlety and com-\nplexity of measuring bias in NLP systems. Thus,\nthere is a pressing need for future work focused\non developing metrics that effectively capture and\nassess the impact of these biases on downstream\ntasks. Addressing this issue is essential for en-\nsuring the equitable performance of NLP models\nacross diverse applications.\nAnother important limitation lies in the na-\nture of the debates themselves. While these de-\nbates are crucial for evaluating biases on societal\ntopics\u2014especially when comparing machine re-\nsponses to human perspectives\u2014it would be valu-\nable to explore biases on more specific political\nquestions. Additionally, it raises the question of\nwhether humans are more influenced by debatesconducted by other humans, which often rely less\non structured argumentation and factual accuracy,\nand more on rhetorical devices and emotional ap-\npeal. Given the vast amount of data available from\nhuman debates (Chalkidis and Brandl, 2024; Ren-\nnard et al., 2023; Mirkin et al., 2018), future re-\nsearch could investigate bias in less grounded top-\nics, both for humans and models. Moreover, exam-\nining the impact of the speech's nature\u2014whether\nfact-based or emotionally driven\u2014on bias measure-\nment could provide deeper insights.\nEthical concerns\nThe comparison between human and model biases,\nas conducted in our study, brings to light the ethi-\ncal complexity of human biases themselves. While\nhuman evaluators are included to provide a bench-\nmark for assessing model bias, it is important to\nrecognize that human opinions and judgments are\nalso influenced by individual biases, cultural norms,\nand subjective experiences. This raises the question"}]}