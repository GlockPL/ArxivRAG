{"title": "Fairness in Survival Analysis: A Novel Conditional Mutual Information Augmentation Approach", "authors": ["Tianyang Xie", "Yong Ge"], "abstract": "Survival analysis, a vital tool for predicting the time to event, has been used in many domains such as healthcare, criminal justice, and finance. Like classification tasks, survival analysis can exhibit bias against disadvantaged groups, often due to biases inherent in data or algorithms. Several studies in both the IS and CS communities have attempted to address fairness in survival analysis. However, existing methods often overlook the importance of prediction fairness at pre-defined evaluation time points, which is crucial in real-world applications where decision making often hinges on specific time frames. To address this critical research gap, we introduce a new fairness concept: equalized odds (EO) in survival analysis, which emphasizes prediction fairness at pre-defined time points. To achieve the EO fairness in survival analysis, we propose a Conditional Mutual Information Augmentation (CMIA) approach, which features a novel fairness regularization term based on conditional mutual information and an innovative censored data augmentation technique. Our CMIA approach can effectively balance prediction accuracy and fairness, and it is applicable to various survival models. We evaluate the CMIA approach against several state-of-the-art methods within three different application domains, and the results demonstrate that CMIA consistently reduces prediction disparity while maintaining good accuracy and significantly outperforms the other competing methods across multiple datasets and survival models (e.g., linear COX, deep AFT).", "sections": [{"title": "1. INTRODUCTION", "content": "Survival analysis is a set of statistical methods designed to model data where the outcome of interest is the time to the occurrence of a particular event (P. Wang et al., 2017). It is widely applied across many domains, such as healthcare (Khuri et al., 2005; Reddy et al., 2015), education (Ameri et al., 2016), business intelligence (Li et al., 2016; Rakesh et al., 2016), etc. In these applications, survival analysis provides likelihood estimation for the occurrence of events over time, which is useful for a lot of crucial decision making. For example, in healthcare, survival analysis models predict the risk of patient's death and the prediction help providers with strategizing treatment and distribution of medical resources (Keya et al., 2021; Maryland Department of Health, 2019); in criminal justice, survival models are used to estimate the likelihood of recidivism (Angwin et al., 2022; Mattu, 2016), and the estimation is then used for parole decisions and allocation of social resources; in finance, survival models are applied to assess default risks, and loan and investment decisions are further made based on the risk assessment (Stepanova & Thomas, 2002).\nHowever, like other prediction tasks (e.g., classification), survival analysis can exhibit bias against disadvantaged groups, often due to biases inherent in the data or algorithms (Mehrabi et al., 2021, Keya et al., 2021). Such bias can lead to discriminatory decisions, disproportionately affecting individuals in different groups. For instance, in healthcare, when historical records reflect systematic inequalities---such as unequal access to medical care or differences in treatment based on demographic factors---a survival model trained on such data may yield poorer predictions for minority groups, thereby exacerbating existing inequalities (Barocas &\nSelbst, 2016; Keya et al., 2021); in criminal justice system, if historical data is biased against\ncertain racial or socioeconomic groups, survival models built with the data might overestimate recidivism risks for disadvantaged groups, leading to unjust decision-making (Angwin et al., 2022). Therefore, it is very important to investigate and address potential bias or disparity in\nsurvival analysis across diverse application domains.\nThere are a few recently developed studies on addressing fairness of survival analysis, which form two groups. The first group defines fairness based on the disparity of predicted outcome (i.e., the predicted likelihood of event occurrence) across different subject groups. To achieve the defined fairness, Keya et al. (2021) proposed a fairness regularization term to minimize the absolute difference of predicted outcome across demographic groups and Do et al. (2023) designed a fairness regularization term based on mutual information. Besides, Rahman &\nPurushotham (2022) adopted Keya et al.'s (2021) fairness regularization design to achieve similar predicted outcomes between the censored and uncensored groups. The second category of work considers fairness as the disparity of prediction accuracy across subject groups, which takes into account both prediction outcome and ground truth information (e.g., actual event occurrence). For example, Zhang & Weiss (2022) introduced a fairness metric, concordance imparity, defined as the worst-case difference in concordance index (i.e., an accuracy metric for survival analysis (Harrell et al., 1982)) between groups, and they further developed a modified random forest algorithm to improve concordance imparity-based fairness. With the same fairness metric, Hu & Chen (2024) incorporated a distributionally robust optimization (DRO) framework for training survival models to improve survival analysis fairness.\nHowever, both lines of research present limitations in many practical applications of survival analysis. As noted by Hu & Chen (2024), the fairness definition in the first category of work is constrained by the assumption that the underlying event occurrence likelihood is the same across\ndifferent subject groups, which is often unrealistic. For example, in healthcare, studies have found that men have a higher mortality rate than women following a breast cancer diagnosis (F. Wang et al., 2019); when those methods in the first category are applied to achieve fair survival analysis in this scenario, it will misalign predictions with real-world patterns, thus significantly compromising overall predictive performance. While the second category of work aims to achieve fair prediction accuracy, these approaches still fall short of practical needs in many domains, as they measure fairness over the entire evaluation time period. For instance, the aforementioned concordance imparity, a specific fairness metric proposed in (Zhang & Weiss 2022), quantifies the difference of prediction accuracy between groups across each possible time point within the evaluation period. In other words, the objective of these methods is to achieve fair prediction accuracy along continuous time (i.e., all time points) within the evaluation period. However, in many application domains, survival analysis is used to predict event occurrence across pre-defined and discrete time points. For example, in healthcare, practitioners use survival analysis to forecast mortality risk over a fixed period (e.g., 30 days), rather than across all time points (Khuri et al., 2005); in crowdfunding, survival models are employed to estimate the likelihood of project success within a pre-defined timeframe such as 60 days (Li et al., 2016); in\nrecidivism prediction, survival analysis is applied to predict the likelihood of recidivism over three years, which aligns with the average duration of a federal supervision term (Just the Facts, 2018). Therefore, achieving survival analysis fairness across such pre-defined time points is critical. Unfortunately, fairness metrics (e.g., concordance imparity) proposed in the second category of work do not meet this need because they place undue emphasis on less important time points, instead of those pre-defined ones (e.g., 30 days), and those methods developed based\non these metrics cannot yield desired prediction fairness across the pre-defined time intervals."}, {"title": "2. RELATED WORKS", "content": "In this section, we review the related works on survival models and fairness in survival analysis."}, {"title": "2.1. Survival Analysis Models", "content": "The extant survival analysis models can be categorized into three groups (P. Wang et al., 2017):\nnon-parametric, parametric, and semi-parametric. Below, we elaborate on each category.\nNon-parametric models make no assumptions about the underlying data distribution. The most widely used one is the Kaplan-Meier (KM) estimator (Kaplan & Meier, 1958), which calculates survival probability up to a specific time as a cumulative product of prior conditional survival probabilities. On the other hand, the Nelson-Aalen estimator (NA) (Andersen et al., 2012) employs counting process techniques to estimate survival probabilities. Another well-known method is Life Table (LT) analysis (Cutler & Ederer, 1958), which organizes survival data into tables and provides visual analysis, making it especially useful for grouped data.\nParametric models, on the other hand, assume that the survival time -- the time to the event occurrence follows a specific distribution, such as Weibull and Normal distributions. Maximum likelihood estimation (MLE) (Lee, 2003) is used to estimate parameters and predict survival time under these assumptions. When the logarithm of survival time is expressed as a linear function of covariates, the model (i.e., the distribution assumption) is known as an\nAccelerated Failure Time (AFT) model (Kalbfleisch & Prentice, 2011), which allows for the use of generalized linear regression techniques for efficient estimation.\nFinally, semi-parametric models combine elements of both parametric and non-parametric approaches. The cornerstone semi-parametric model is the Cox Proportional Hazards (COX) model (Cox, 1972). Like non-parametric models, it makes no assumption about the distribution\nof survival times. At the same time, it incorporates a parametric loss function to explain the risk\nof the event occurrence. Building on the original COX model, (Katzman et al., 2018) extended the linear relationship between covariates and the risk of the event occurrence to a deep model framework, allowing for more complex and nonlinear relationship modeling.\nIn this study, we consider Cox and AFT models as they have been widely used in many\nsurvival analysis applications. In Section 3.1, we will review their technical details, based on which we propose our EO fairness measure and CMIA approach."}, {"title": "2.2. Fairness in Survival Analysis", "content": "Prior studies on fairness of survival analysis can be grouped into two categories.\nThe first strategy aims to reduce disparities in predicted outcomes, specifically the\nlikelihood of an event occurring, across different groups. As discussed in Section 1, the works of Keya et al. (2021), Do et al. (2023), and Rahman & Purushotham (2022) align with this approach. The key distinction lies in their methods: Keya et al. (2021) proposed a fairness regularization term based on minimizing the absolute differences in predicted outcomes between groups, while Do et al. (2023) designed their regularization using mutual information. In contrast, Rahman & Purushotham (2022) shifted the focus to addressing disparities caused by\ndata censorship rather than demographic differences, adopting Keya et al.\u2019s (2021) design.\nThe second strategy takes a different route by emphasizing fairness in predictive accuracy\nrather than just predicted outcomes. This approach considers not only the model's predictions but also the actual observed data. For instance, Zhang & Weiss (2022) developed a metric called concordance imparity, which quantifies the largest discrepancy in concordance index, and further designed a modified random forest algorithm to improve fairness according to this metric. Later, Hu & Chen (2024) introduced a distributionally robust optimization (DRO) framework, also\naiming to address fairness in predictive accuracy.\nHowever, none of these prior studies have addressed the fairness of survival analysis predictions that are performed at pre-define time points (e.g., every 30 days), which is very needed in many real-world applications such as recidivism prediction. This paper aims to address this critical research gap, and our proposed EO fairness measure for survival analysis and designed CMIA approach are fundamentally different from the prior studies."}, {"title": "3. METHODOLOGY", "content": "In this section, we introduce our proposed methodology. First, we outline preliminaries, including the problem formulation and survival analysis models. Next, we formally define equalized odds fairness within the context of survival analysis. Following this, we introduce our proposed Conditional Mutual Information Augmentation method, which consists of two core\ncomponents, i.e., Fairness Regularization and Censor Data Augmentation, and present the formalized learning objective."}, {"title": "3.1. Preliminaries", "content": "In this subsection, we provide the background on survival analysis relevant to this study. We first formalize the survival analysis problem and then briefly introduce several survival models."}, {"title": "3.1.1. Survival Analysis Problem", "content": "In survival analysis, each observation (e.g., a patient's medical record) is represented as a four-tuple {(X, T, 8, Z)}{=1, where n is the number of observations. Here, X \\in R\u00ba denotes the feature vector, which may include attributes like age, income, and social status. The binary indicator \u03b4 \u2208\n{0,1} specifies whether the event of interest (e.g., death) occurs during the study period. T E R represents the time duration: when 8 = 1, T_is the time-to-event, i.e., the time from the study's start to the occurrence of the event; when d = 0 (i.e., there is no occurrence of event during the\nstudy period), T is the time until the study concludes, marking the observation as \"censored\"\nwhich means the event may still happen in the future and its timing remains unknown. Finally,\nZEC is a categorical variable representing a sensitive attribute, such as ethnicity or gender of patient. Given a set of observations, survival analysis problem is to train a model to predict the likelihood that one event occurs after time duration t for each single record."}, {"title": "3.1.2. Survival Analysis Models", "content": "To illustrate the concept of survival analysis models, we begin with a foundational example: the Cox Proportional Hazards (COX) model (Cox, 1972). We then expand our discussion to other models by modifying key components of the COX model.\nThe COX model has two fundamental functions: (1) S(t|X) = P(T > t|X), which is the survival function, denoting the probability of event occurrence after time duration t, or in other words, the subject has \"survived\u201d in time duration t without the event occurrence. The estimation\nof survival function is the ultimate target for survival analysis because once we successfully estimate it, we can use it to predict survival probability for any time duration. (2) \u03bb(t]X) =\n\\lim_{\\eta \\to 0} \\frac{P(t<T<t+\\eta|T>t,X)}{\\eta}, which is the hazard function, denoting the probability that an individual will not survive an extra infinitesimal amount of time n, given that she has already survived for time during t. These two functions have the following relationship:\nS(t|X) = exp(-\\int_0^t \\lambda(t'|X)dt') (1)\nThe COX model further assumes that the hazard function can be decomposed into a time\ndependent component \\lambda_0(t) and a feature dependent component g_0(X):\n\\lambda(t|X) = \\lambda_0(t) \\cdot exp(g_0(X)), (2)\nwhere \u03b8 \u0395 \u0398 are model parameters. Additionally, in the original COX model, the ge(X) function\nis assumed to be linear: ge(X) = X0. The estimation of ge (X) is performed by optimizing:\n\\min_{g_\\theta} L(g_\\theta) = \\sum_{i=1}^{n} \\delta_i[-g_\\theta(X_i) + log \\sum_{j=1}^{n} \\eta_i exp(g_\\theta(X_i))]. (3)\nAfterward, \u03bb\u03bf(t) can be estimated by a non-parametric method, such as the Kaplan-Meier (KM)\nestimator (Kaplan & Meier, 1958), allowing S(t|X) to be calculated using Eq. (1).\nThe form of ge(X) is flexible. For example, Katzman et al. (2018) explored deep COX models by redefining ge(X) as: ge(X) = MLP(X), where MLP represents a Multi-layer\nPerceptron (MLP) (Goodfellow et al., 2016) function. The training and prediction workflow remains unchanged. Furthermore, the objective function in Eq. (3) can also be modified. For\nexample, when the objective function is redefined as:\n\\min_{g_{\\theta},\\sigma} L(g_\\theta) = \\sum_{i=1}^{n} [\\delta_i(log(\\sigma) \u2013 w_i) + exp(w_i)], (4)\nwhere Wi = \\frac{log(T_i)-g_{\\theta}(X_i)}{\\sigma}, the formulation becomes the well-known AFT model, where \u03c3\u2208 R+\nis an additional parameter that is jointly learned alongside the model parameters \u03b8. Accordingly,\nthe survival function in the AFT model is updated as:\nS(t|X) = exp(-(\\frac{t}{exp(g_{\\theta}(X_i))})^\\frac{1}{\\sigma}). (5)\nThis difference between both formulated objective functions (i.e., Eq. (3), (4)) indeed reflects a different assumption about the underlying data pattern.\nIn this study, we consider four survival model scenarios, formed by combining two different\nformulations of go (X) (i.e., Linear vs Deep models) with two different learning objective functions (COX vs AFT objectives). As our proposed CMIA approach is model agnostic, we will marry it with each of the four survival model scenarios and evaluate its performance. Next, we\nintroduce our proposed new fairness measure, equalized odds (EO), for survival analysis."}, {"title": "3.2. Equalized Odds Fairness in Survival Analysis", "content": "As introduced in Section 1, our proposed EO fairness requires conditionally independence between predicted outcome and a sensitive attribute, and it aims to achieve equitable prediction accuracy between groups across each pre-defined time interval (e.g., 30 days). Mathematically,\nthe EO fairness requires a survival model to satisfy the following conditional independence:\n\\hat{Y}_t \\perp Z | Y_t, \\forall t \\in Q, (6)\nwhere Yt := I(T < t, \u03b4 = 1) is a binary variable indicating whether the event occurs before time t for a given observation; Yt represents the predicted binary outcome inferred by the survival function S(t\u2758X). It is usually assigned a value of 1 when S(t|X) is less than 0.5, and otherwise it\nis assigned a value of 0; The set Q refers to a collection of pre-specified evaluation time points, which is often determined based on the practical needs in different applications.\nLike EO in classification, EO for survival analysis has the following necessary and sufficient condition: the equality of true positive rate (TPR) and false positive rate (FPR) across all sensitive-attribute groups: TPRc\u2081\u2081t = = TPRc\\c\\,t; FPRC1,t = \u2026 = FPRc\\c\\,t, t EQ,\nwhere TPRciut = P(\u00ce\u00bf = 1|Y\u2081 = 1,Z = ci) and FPRc\u2081\u2081t = P(\u0176t = 1|Yt = 0, Z = ci),\nrespectively. |C| denotes the number of sensitive-attribute groups. The proof of this condition is\nprovided in Appendix A. Satisfying this condition essentially implies that the survival analysis yields equitable predictive accuracy (quantified by TPR and FPR) between different sensitive-attribute groups across all the pre-specified evaluation time points. This necessary and sufficient condition will be later used for evaluating the EO fairness of survival models in Section 4.3.\nWe would like to remark that unlike the existing two categories of fairness measures reviewed in Section 2.2, our proposed EO fairness aims to achieve fair prediction of event occurrence across the pre-defined time points (e.g., 30 days), which is very needed in many real-"}, {"title": "3.3. Conditional Mutual Information Augmentation (CMIA) Approach", "content": "Our proposed CMIA approach consists of two core components: a novel fairness regularization term and a new censored data augmentation module. The fairness regularization term is designed\nto ensure the survival model meets the EO fairness criteria, and the censored data augmentation module is developed to further enhance the balance between accuracy and fairness of survival analysis prediction. Both components are fused through a joint-learning objective function."}, {"title": "3.3.1. Fairness Regularization", "content": "The fairness regularization term, which will be added to the original learning objectives (i.e., Eq. (3) and (4)), is designed to regulate the survival model to meet the EO fairness criterion.\nTherefore, the fairness regularization term needs to capture how well the survival model satisfies the EO fairness criterion. More specifically, it should be formalized to quantify the degree of\nconditional independence \u0176 1 Z|Y, \u2200t \u2208 Q (i.e., Eq. (6)).\nTo achieve this, we adapt the conditional mutual information (CMI) statistics (Wyner, 1978) from information theory to develop our fairness regularization term. It measures the shared\ninformation between two variables, conditioned on a third one. In our context, it is defined as:\nCMIt := \\sum_{\\Upsilon_\\tau \\in {0,1}} \\sum_{\\Upsilon_\\tau \\in {0,1}} \\sum_{Z\\in C} P_{yt,z,y} log \\frac{P_{Y_t Z,Y_t}}{P_{Y_t|Y_t}}, \\forall t \\in Q (7)\nwhere P_{y,z,y} is the joint probability mass function of Y, Z, Yt; P_{Y\\\u2081\u0131z,yt} is the probability mass\nfunction of Y conditional on Z, Yt; P_{y\u0131lYt} is the probability mass function of Y conditional on Yt.\nWith this definition, we identify and prove an important mathematical property of the CMI:\nCMI \u2265 0, and CMIt = 0 \\Rightarrow \u0176 1 Z|Yt. (8)"}, {"title": "3.3.2. Censored Data Augmentation", "content": "While our developed fairness regularization term imposes the survival model to satisfy the defined EO fairness, it remains a challenge how to strike a balance between accuracy and fairness in survival analysis. Such a contradiction between accuracy and fairness exists in many fairness-aware prediction tasks because optimizing fairness often does not align with optimization of accuracy during the learning process (Fu et al., 2021; Sener & Koltun, 2018). \u03a4\u03bf mitigate this challenge, we propose a novel censored data augmentation approach to improve the balance between accuracy and fairness. Intuitively, the augmented data serves as a \"mediation zone,\" where the objectives of optimizing accuracy and fairness find common ground. In this zone, both objectives tend to compromise slightly throughout the optimization process, allowing the parameter estimation to eventually achieve a state that benefits both perspectives.\nOur data augmentation approach draws inspiration from the uniqueness of survival analysis data, which is that some event occurrences in the observations are \"censored.\" In other words, for observations with d = 0, the time of event occurrence is unknown. During survival model training (via minimizing the learning objective), these censored observations contribute by signaling that the event has not occurred within the study period, thus encouraging the survival\nmodel to assign lower likelihood of event occurrence to similar data instances; compared with\nuncensored observations that offer precise time information on event occurrence, these censored\ninstances are less informative in guiding the training process because the time to event is missing. Such missing information indeed provides us with a unique opportunity to balance prediction accuracy and fairness through data imputation (Shorten & Khoshgoftaar, 2019), which could alleviate the intricate data bias issue that inherently causes the prediction unfairness\n(Mehrabi et al., 2021). In fact, extant literature has explored such data imputation approach for\nimproving the balance between recommendation accuracy and fairness (Rastegarpanah et al.,\n2019), where the imputed data are unobserved user-item ratings. In the context of survival analysis, we propose to impute the time of event occurrence for the censored observations.\nFor each censored observation j, we design the data augmentation as follows. Let \u2206j denote\nan additional time duration corresponding to the censored observation j. When \u2206j> 0, the module synthesizes an event time T; = T; + \u2206; for the censored observation j and updates the\nevent occurrence indicator as j = 1, where Tj is its original time duration before imputation.\nWhen \u2206;= 0, there will be no imputation and the observation j remains unchanged. Assuming there are n8=0 censored observations in the original dataset, we use \u0394\u0395 R+18=0, a non-negative\nvector, to denote the to-be-optimized additional time durations for all the censored observations.\nAfter the data imputation, the augmented dataset can be denoted as:\n{(Xi, Ti, di, Zi)}8\u2081=1 \u222a {(Xj, Tj, 8j, Zj)}8;=0 (12)\nwhere T; = T; + Aj and \u03b4; = I(\u0394;> 0). In other words, the augmented dataset consists of two\nparts: the original uncensored records and the possibly updated ones from the original censored observations. For the second part, both the time to event T; and the event occurrence indicator \u03b4;\nfor each original censored observation are determined by the data augmentation."}, {"title": "3.3.3. Joint Learning Objective", "content": "With the above design, the next challenge is how to optimize the values of \u2206, which essentially determines the augmented data. To solve this challenge, we include A as a set of parameters into the learning objective and the values of \u2206 will be jointly learned alongside the survival model parameters 0.\nThe joint learning objective of our CMIA approach is finally formalized as follows:\n\\min_{g_{\\theta},\\Delta}L(g_{\\theta}, \\Delta) + \\lambda_1 R_{EO} (g_{\\theta}, \\Delta) + \\lambda_2||\\theta||_2^2, (13)\nwhere L(ge, \u2206) is the foundational objective function defined in Section 3.1 and REO (ge, \u2206) is the regularization term introduced in Section 3.3.1. As introduced in Section 3.3.1, minimizing\nREO (ge, \u2206) through the learning process will compel the survival model to meet our defined EO fairness. Both terms (i.e., L(ge, \u2206) and Reo (g\u04e9, \u2206)) have an additional input A because the\nimputed data (i.e., synthetic uncensored observations) depend on the values of \u0394. \u03bb\u2081 and 22 are\nthe scale parameters for fairness regularization and the L2-penalty. As the formalization suggests,\nA will be jointly learned with the survival model parameters @ by minimizing this objective.\nOptimizing the joint objective with \u2206 as additional parameters enables the survival model to\nstrike a good balance between prediction accuracy and fairness. The optimization of the joint learning objective proceeds similarly to the standard process for training other machine learning models. Specifically, we compute the gradient of the joint learning objective with respect to\n0 and A, and apply the Adam algorithm (Kingma & Ba, 2017) to perform the optimization."}, {"title": "4. EVALUATION", "content": "In this section, we evaluate our proposed CMIA approach in comparison to state-of-the-art\n(SOTA) benchmark methods with three datasets collected in different domains. After introducing\nthe datasets, we present the experiment settings, the SOTA methods, and evaluation metrics. We\nthen present the main results and discuss the findings. Finally, we use visualization to illustrate\nhow the data augmentation improves the balance between accuracy and fairness."}, {"title": "4.1. Data", "content": "We use prominent datasets collected from healthcare, social justice, and consumer loans to\nevaluate our CMIA approach. The details of each dataset are outlined below.\nFLC Dataset: This widely used healthcare dataset was collected from a study that evaluated the ability of the serum immunoglobulin free light chain (FLC) assay to predict overall survival related to immune dysregulation (Dispenzieri et al., 2012; Kyle et al., 2006). It includes 6,524 patients, each described by six features. The goal is to predict the risk of death. During the study, 30% of the patients passed away, while the remaining 70% were marked as censored. The sensitive attribute in this dataset is gender. The minimum, maximum, and average values of the time duration (i.e., T) are 1, 5166, and 3583 days. Accordingly, we predict the risk of death across the time points of 2190, 2920, 3650, and 4380 days.\nConsumer Loan Dataset: This is a well-known credit scoring dataset used to analyze default rate of consumer loans in the German market (Hofmann, 1994). We use the version suggested by the PySurvival package (Fotso, 2019) to study the speed of loan repayment. The objective of the survival models is to predict the likelihood of full loan repayment during the study period. The dataset contains 1,000 loans, each described by 18 demographic features. In this dataset, 70% of the loans are fully repaid, while the remaining 30% are marked as censored. The sensitive attribute is also gender. The minimum, maximum, and average values of the time duration are 4, 72, and 21 days. We predict the outcome (i.e., full repayment) at 21 and 35 days.\nCOMPAS &COMPAS Multi Dataset: The COMPAS dataset pertains to a system used to predict criminal recidivism, which has faced criticism for potential bias (Angwin et al., 2022). It\nis primarily employed in bail and sentencing decisions but could also be used to allocate social work resources. The dataset includes 10,314 offenders and six demographic attributes. During the study, 27% of subjects reoffended, with a median event time of 173 days, while the remaining 73% are marked as censored. The sensitive attributes are ethnicity and gender. For\nsimplicity, ethnicity is discretized into a binary variable: Caucasian and Non-Caucasian.\nWe consider two versions of the COMPAS dataset: COMPAS and COMPAS Multi. The COMPAS version uses binary ethnicity (Caucasian or Non-Caucasian) as the sensitive attribute, while COMPAS Multi includes both ethnicity and gender as sensitive attributes. This results in four groups, which allows us to evaluate our approach on a dataset with multiple sensitive attributes. To align with the COMPAS system's definition of recidivism \"a new misdemeanor\nor felony offense within two years of the COMPAS administration date\u201d (Mattu, 2016)\u2014 we apply the survival models to predict recidivism at 730 days (2 years). Additionally, we consider the time point of 1095 days (3 years), which corresponds to the average length of a federal\nsupervision term (Just the Facts, 2018). Therefore, the survival models are used to predict the outcome at two time points, 730 and 1095 days, for both the COMPAS and COMPAS Multi."}, {"title": "4.2. Experiment Settings and Benchmark Methods", "content": "Each dataset is then randomly split into training, validation, and test sets with ratios of 0.8, 0.1, and 0.1, respectively. Survival models are trained on the training set, hyperparameters are fine-tuned on the validation set, and performance is evaluated on the test set. We do not show the\ndetailed hyperparameter selection here due to space constraints. For each dataset, as outlined in Section 3.1, we consider four survival model scenarios, which are combinations of two go(X)\nfunctions\u2014Linear and Deep\u2014and two foundational objective functions: AFT and COX. In the\nfour scenarios, we evaluate our proposed CMIA approach against three different benchmark methods. Below, we briefly introduce each benchmark method.\ng-Difference (GD) (Keya et al., 2021): This method introduces a regularization term based on\nthe absolute difference of groupwise ge(X) function values to achieve group fairness in terms of statistical parity. While the original implementation (Keya et al., 2021) focuses on the COX\nobjective function, we generalize the idea to the AFT objective function as well.\nDistributionally Robust Optimization (DRO) (Hu & Chen, 2024): This benchmark method\nuses distributionally robust optimization to address survival analysis fairness by ensuring equal\naccuracy across groups defined by the sensitive attribute. Specifically, it minimizes the worst-case error across all subpopulations that exceed a predetermined threshold. As in the original\npaper, DRO is only applied to the COX objective function.\nVanilla: In addition, for each of the four scenarios, we consider the vanilla survival model (e.g.,\nlinear ge(X) with COX objective) as an additional benchmark method."}, {"title": "4.3. Evaluation Metrics", "content": "We employ two widely used metrics to evaluate the accuracy of survival models: the averaged\narea under the curve (aAUC) and the averaged Brier score (aBrier). The aAUC is defined as:\naAUC = \\frac{1}{|Q|} \\sum_{t \\in Q} AUC_t, which represents the average AUC value across all evaluation time\npoints. The averaged Brier score is defined as:\naBrier = \\frac{1}{|Q|} \\sum_{t \\in Q} brier_t = \\frac{1}{|Q|} \\sum_{t \\in Q} [\\frac{1}{n} \\sum_{i=1}^n (I(T_i > t) - P(T_i \\le t))^2], (14)\nwhich averages the Brier score (Brier, 1950) over all evaluation time points. P(T\u2081 \u2264 t) represents\nthe predicted probability of the event occurrence before time t. While a higher value of aAUC indicates better performance, a lower value of aBrier reflects higher accuracy."}, {"title": "4.4. Main Results", "content": "We present the main evaluation results in Tables 1-4, from which we have the following observations. First, the vanilla survival models consistently show notable unfairness in terms of\nEqualized Odds across all datasets and scenarios. For instance, on the FLC dataset, the vanilla models result in an adTPR exceeding 0.0241 and an adFPR higher than 0.0119 across all survival\nmodel scenarios. Second, while both GD and DRO cannot consistently mitigate this disparity,\nour proposed CMIA approach significantly reduces unfairness across all datasets and survival model scenarios. Notably, on the COMPAS dataset, CMIA achieves a mitigation of up to 93.78%\nin adTPR and 92.75% in adFPR for the deep survival model trained with the Cox objective function. Third, CMIA successfully balances accuracy and fairness. On the FLC dataset, it even improves accuracy while reducing unfairness. For instance, in the scenario of the linear ge(X)\ntrained with the AFT objective function, CMIA improves the aAUC by 4.83% and reduces the\naBrier, adTPR, and adFPR by 13.48%, 41.25%, and 60.96%, respectively."}, {"title": "To evaluate the EO fairness of survival models, we use the averaged maximum difference of", "content": "the true positive rate (adTPR) and the false positive rate (adFPR)", "as": "nadTPR = \\frac{1}{|Q|} \\sum_{t \\in Q} dTPR_t = \\frac{1}{|Q|} \\sum_{t \\in Q} [\\max_{c\\in C} TPR_{c,t} - \\min_{c\\in C} TPR_{c,t}"}]}