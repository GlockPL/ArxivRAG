{"title": "Dance of the ADS: Orchestrating Failures through Historically-Informed Scenario Fuzzing", "authors": ["Tong Wang", "Taotao Gu", "Huan Deng", "Hu Li", "Xiaohui Kuang", "Gang Zhao"], "abstract": "As autonomous driving systems (ADS) advance towards higher levels of autonomy, orchestrating their safety verification becomes increasingly intricate. This paper unveils ScenarioFuzz, a pioneering scenario-based fuzz testing methodology. Designed like a choreographer who understands the past performances, it uncovers vulnerabilities in ADS without the crutch of predefined scenarios. Leveraging map road networks, such as OPENDRIVE, we extract essential data to form a foundational scenario seed corpus. This corpus, enriched with pertinent information, provides the necessary boundaries for fuzz testing in the absence of starting scenarios. Our approach integrates specialized mutators and mutation techniques, combined with a graph neural network model, to predict and filter out high-risk scenario seeds, optimizing the fuzzing process using historical test data. Compared to other methods, our approach reduces the time cost by an average of 60.3%, while the number of error scenarios discovered per unit of time increases by 103%. Furthermore, we propose a self-supervised collision trajectory clustering method, which aids in identifying and summarizing 54 high-risk scenario categories prone to inducing ADS faults. Our experiments have successfully uncovered 58 bugs across six tested systems, emphasizing the critical safety concerns of ADS.", "sections": [{"title": "1 INTRODUCTION", "content": "Rapid advancements in autonomous driving systems are driving the shift from Level 1 to Level 5 autonomy, closely tied with integrating Al technologies like deep neural networks. As ADS evolves, the complexity of their models, the multiplicity of modules, the probabilistic nature of deep learning, extensive parameter counts, and training data volume collectively heighten safety risks and pose challenges in ADS verification and testing. The variety of external environments further complicates these challenges. The National Highway Traffic Safety Administration (NHTSA) reports numerous ADS-related accidents annually, highlighting the urgent need for rigorous safety verification in industry and academia [27].\nGiven the complexity of ADS, verifying the safety of individual modules and their interactions is crucial. Scenario verification offers a practical solution, validating the overall system and module interactions. However, constructing diverse real-world scenarios is impractical and costly, leading to the use of simulation engines for customizable scenario creation. While many manufacturers focus on developing scenario libraries for testing, identifying and constructing scenarios that effectively induce ADS errors remains a pressing research issue [9, 29, 42].\nIn this context, we draw an analogy between the ADS and a lead actor in a play, aiming to create a \"script\" that tests the ADS under various challenging conditions and exposes system vulnerabilities. Script: The key challenge is the script's availability, particularly scenario sources. These originate from traffic videos, accident data, and mandated scenarios, with some studies converting these into simulation scenarios [13, 34, 37, 39]. Others use scenario description languages to translate scenarios into simulations [5, 8, 17, 18, 20-24, 28, 41]. However, organizing and acquiring these sources is labor-intensive and often limited in scope.\nStage Scope: The range of scenario layouts is another critical aspect. An expansive scope incurs long execution times and computational costs. Randomly selecting points in a city map to find ADS flaws is not only computationally intensive but also adds uncertainty to testing.\nStage Elements: Key elements like roads, signs, and natural features are often fixed in reality, but modifying these in simulations deviates from scenario generation goals and is technically challenging.\nStage Lighting: Lighting, representing weather conditions, is crucial. Simulation platforms like CARLA [10] and LGSVL [25] effectively emulate diverse weather conditions, essential for testing ADS's visual perception."}, {"title": "2 BACKGROUND", "content": "2.1 Autonomous Driving Systems (ADS)\nFigure 1 illustrates the architecture of contemporary ADS, comprised of four main modules: sensing, perception, planning, and control. An alternate version, end-to-end autonomous driving systems, combines the last three modules as a whole.\nThe sensing module encompasses various sensors, such as radar, cameras, GPS, and IMU, to collect environmental and vehicle data. Perception involves processing sensor data for vehicle localization, object detection, and behavior prediction. Planning formulates driving routes based on perception results and driving tasks, involving both global and local planners. The control module generates vehicle control signals using various algorithms and functions. While deep learning techniques find use in perception and end-to-end modules, other modules typically employ logic-based or corresponding computational methodologies.\n2.2 Scenarios in the Context of ADS\nIn ADS, scenario and scene are two concepts. As defined by S.Ulbrich et al. [32], a scenario is a temporal sequence of scenes, each representing a snapshot of the environment, including entities and their relationships. As shown on the left of Figure 1, the continuous process displayed in this image portrays a typical scenario. The tested vehicle (Ego-car) is driving straight through an intersection, with a potential for collisions with other vehicles moving in the perpendicular direction and pedestrians crossing the road. Simultaneously, traffic lights, speed limit signs, and other traffic infrastructure are present, and the current weather is sunny. A scene, in contrast, is the instantaneous state depicted in the image.\nBagschik et al. [2] proposed a scenario layer model in Figure 2. Layer 1 comprises the road layer, which includes road materials and types. Layer 2 constitutes the traffic infrastructure, encompassing traffic lights, signs, etc. Layer 3 involves operations on Layers 1 and 2, such as puddles on the road. Layer 4 consists of objects in the scenario, such as pedestrians and vehicles, each with a series of attributes like motion state, appearance type, and path. Layer 5 represents weather, including sunlight conditions (intensity and angle), rain conditions (precipitation intensity), and other factors.\nIn our work using the CARLA simulator, we cannot adjust the Layer 1 setting, but we can identify and extract viable waypoints and paths for various road types and traffic infrastructure, using the map crawling technique (see \u00a73.2). This process does not alter the road shapes or infrastructure but allows us to obtain detailed"}, {"title": "3 APPROACH", "content": "3.1 Overview of the ScenarioFuzz Method\nOur approach leverages map crawling technology to generate an initial scenario seed corpus for fuzzing. The selected seeds then undergo a two-stage mutation strategy using a variety of mutators for the mutations. A scenario evaluation model filters these mutated seeds and selects the one projected to pose the highest risk as the test case. The test case is utilized to build the simulator scenario and integrate it into the ADS. (See Figure 3 and Algorithm 1).\nIn this workflow, the function SELECT_SEED() filters seeds based on the map name, road type, and traffic infrastructure, and randomly selects a seed from this set. Over Nc mutation cycles, Nm mutated seeds are generated in each cycle using the two-stage MUTATION() process. From the second round, informed random neighbor mutations are conducted. The scenario is mutated by several mutators, including weather, object, puddle, and ADS mission.\nSubsequently, the scenario evaluation model (SEM, detailed in \u00a73.5) filters Ne seeds most likely to cause ADS error. This model also undergoes real-time training TRAIN() once the test data T (all systems under test) reaches a certain size. These seeds are used to construct the scenario within the simulator and connect to the ego car. The DETECT_MISBEHAVIOR() function monitors ADS behavior in real-time and identifies error scenarios. Once an error scenario is identified, the fuzzing operation is stopped and the case is retained. The selection frequency of the corresponding scenario seed is evaluated using the CHECK_FREQUENCY() function, with a roulette-based approach determining the seed's reinsertion into the queue. If a seed stops yielding error scenarios, an alternative seed is chosen for the following rounds. Seeds with high selection frequencies are excluded to prevent local optima and ensure diversity.\n3.2 Building Corpus with Map Crawling\nThe corpus serves as the cornerstone of our scenario construction process. Utilizing the map crawling method with the map road networks as input, we construct topological graphs, and then segment and classify roads by their types based on geographical location and topological structure. Furthermore, we extract key data from these segments, thereby constructing an essential scenario seed corpus for our study."}, {"title": "3.3 Scenario Seed Selection", "content": "Initial seeds can be further selected from the corpus based on testing requirements. This selection is based on various data, such as city names, road types, and proximity to specific traffic signs. When only a subset of this information is available, the selection is filtered accordingly. The future mutations of these seeds are then guided by the waypoint positions n, the paths p, and the central location c within the seed scenario."}, {"title": "3.4 Scenario Seed Mutation", "content": "The mutation process for a scenario seed consists of two essential components: mutators and mutation strategy. Various types of mutators perform mutation operations on the corresponding elements of the scenario, and the configuration of the associated attribute values is dictated by the mutation strategy.\n3.4.1 Mutators. In the context of scenario mutation, we use several mutator types to adjust different scenario aspects. These alterations aim to examine the ADS's resilience and adaptability under various challenging circumstances.\nMission Mutator. This mutator modifies the ego car's mission path. It first selects a location from the waypoint information n in the scenario seed s as the starting point for the test vehicle. It then randomly selects a path from p, ensuring the chosen path includes specified starting and ending points based on the initial location. If a specific driving direction, such as left, straight, or right, is required, p is filtered accordingly.\nPuddle Mutator. This mutator generates puddles within the scenario. It begins by establishing the number of puddles. For each puddle, it generates the corresponding size, location, and friction coefficient. The puddles' presence tests the robustness of the ADS control module since irregular ground friction can significantly influence the control signals' effect on the vehicle state.\nObject Mutator. This module generates objects within the scenario, spanning pedestrians to various types of vehicles. It first determines the number of objects to be generated within the scenario. Each object is associated with a specific type, i.e., pedestrian or vehicle. In terms of appearance, vehicle objects can represent buses, passenger cars, police cars, and other common models. Similarly, pedestrian objects can depict individuals with diverse clothing and skin tones, with a total of 26 types available. Certain special models, like bicycle or motorcycle riders, are also included in the vehicle category. For some vehicle models, exterior colors can be specified.\nObject action types include:\n\u2022 Immobile: Static objects remain at their starting positions.\n\u2022 Linear: Objects follow pre-planned paths, moving at a consistent speed\u00b9 set during their generation."}, {"title": "3.4.2 Mutation Strategy", "content": "Our mutators require mutating multiple attribute parameters, which are further subdivided into discrete and continuous categories. Discrete attributes typically cover integers or specific choices, such as quantity or type. In contrast, continuous attributes typically represent decimal values and necessitate a defined range and precision to generate corresponding values. The mutation strategy guides the selection of these attribute values. Our method utilizes a two-stage mutation strategy.\nRandom: This strategy randomly selects a value from a predetermined range to serve as the corresponding attribute value. This approach helps to ensure a broad coverage of the input space and potentially discover more anomalies.\nRandom Neighbor: This strategy generates a new attribute value randomly within a range that extends five steps above and below the current value of each mutator attribute. For discrete values, one 'step' equates to one choice from the set of possible values. For continuous values, one 'step' corresponds to a specific precision, defined by the attribute's measurement scale. After each mutation cycle, the scenario seed with the lowest driving score is chosen. A random neighbor sampling technique based on this seed is then used to draw new inputs from the neighborhood of the existing reference mutation attribute values, thus creating new test cases. This strategy may facilitate a more intensive exploration of the local region of the input space, thereby identifying potential issues.\nThe incorporation of these two sampling methods allows the implementation of diverse mutation strategies, striking a balance between extensive coverage and localized exploration as required."}, {"title": "3.5 Mutated Seeds Filtering", "content": "Fuzzing often produces numerous mutated seeds, with many not triggering ADS incidents. Each seed requires substantial resources for scenario construction in the simulator and imposes time and computational overheads for testing. With the average execution time per scenario being approximately several minutes, generating error scenarios amidst the wide range of scenario types and parameter space after numerous mutations becomes both costly and challenging. To counter this, we introduce the scenario evaluation model that estimates the likelihood of each mutated scenario seed evolving into an error scenario and assigns a confidence level. This confidence level is used to filter the mutated seeds, selecting those with the highest probability of becoming error scenarios, which helps save on overhead. Furthermore, this model utilizes historical test data for training, effectively repurposing previously collected data, as illustrated on the right side of Figure 3.\nInitially, mutated scenario seeds are converted into graph data, as depicted in Figure 4. Each node $n_i$ encapsulates the waypoint information n from the seed (if a traffic light is present, its position information is also stored in the graph data as a node). Each node's features include the relative position of the node with the ego vehicle initial position $p_{n_i}=(x_{n_i}-x_{ego},y_{n_i}-y_{ego},z_{n_i}-z_{ego})$, the node type $t_{n_i}$, the traffic sign type $m_{n_i}$, and the object appearance type $a_{n_i}$ located at that point. The last three types are encoded as integers. The node type encompasses the default case, traffic light, start and end points of the ego vehicle, other vehicles and pedestrians. The traffic sign type and the object appearance type at the point are assigned integer values from corresponding ID sets. If none exist, the value corresponding to the default case is used.\nThe edges between nodes in the graph are established using the path information p from the seed. Each edge $e_{ij}$ represents a path from node $n_i$ to $n_j$, featuring distance $d_{e_{ij}}$, type $t_{e_{ij}}$, and direction $w_{e_{ij}}$. The distance $d_{e_{ij}}$ is calculated as the 2-norm distance between the nodes. Both the latter two features are integer-encoded. The edge type $t_{e_{ij}}$ includes the default case, the path of the ego vehicle's task, the driving paths of other vehicles, and the movement paths of pedestrians. The direction $w_{e_{ij}}$ encompasses left turn, right turn, straight, and an unknown case for scenarios where accurately classifying the direction of longer paths is challenging.\nCorresponding scenario weather parameters w are incorporated into this graph data as separate information (see \u00a73.4.1).\nBefore inputting into the model, the converted graph data are preprocessed by standardization and normalization. This step involves standardizing node positions pn, and weather features w within the batch, and normalizing the edge distance feature to facilitate the model's fitting during training and computation during prediction. Leveraging the Graph Attention Transformer (GAT), we integrate dropout and batch normalization to mitigate overfitting. The prediction confidence for each seed evolving into an error scenario is derived via a sigmoid layer.\nThe model is trained with historical data increments of tr = 1000, splitting 80% for training and 20% for validation over 1000 epochs. During the prediction phase, the batch size fed into the model is equivalent to Nm, which represents the count of mutated seeds. We take into seeds with prediction confidence exceeding 0.5 (i.e., seeds anticipated to be potential error scenarios). These seeds are"}, {"title": "3.6 Execution and Result Monitoring", "content": "The chosen Ne scenario seeds are used to set up the simulation environment and the system under test is integrated. The system's state data during the test within this scenario is also collected.\nMisbehavior detector. This module is responsible for dynamically identifying improper behaviors exhibited by the ADS during testing. Error scenarios are defined as those in which the system under test exhibits one or more of the following erroneous behaviors:\n\u2022 Crash: Collisions are detected when the vehicle's physical model contacts another entity. This process utilizes CARLA's collision detector APIs.\n\u2022 Red: Running Red Lights is identified when the vehicle crosses an intersection against a red traffic signal, employing positional checks relative to traffic lights.\n\u2022 Speeding: Speeding occurs when the vehicle's speed exceeds the CARLA-defined speed limit for a given road segment, with this limit serving as a fixed threshold.\n\u2022 Lane invasion: Improper Lane Changes are detected through CARLA's lane invasion APIs, which trigger upon the vehicle crossing road lane markers.\n\u2022 Stuck: Stagnation is defined when the vehicle remains stationary for a set duration, typically over five minutes, indicating a deadlock or system failure.\nThe misbehavior detector halts the simulation and the current seed fuzzing process immediately if any of these behaviors occur, classifying the currently executed scenario seed as an error scenario. Each error type's detection mechanism is parameterized to efficiently flag deviations from normal operational parameters and is designed to be indicative of potential ADS failures.\nDriving score calculator. If ADS does not display improper behavior throughout the simulation, state information is collected from the back-end upon completion. This data, encompassing vehicle speed, braking signals, and acceleration, serves as the basis for calculating the ADS driving score, reflecting the vehicle's stability during operation, and indirectly indicating the extent to which the current scenario impacts this stability. We utilize the formula suggested by Drivefuzz for this calculation, with further details available in the supplementary material."}, {"title": "3.7 Seed Scheduling Strategy", "content": "After each fuzz generation round, we evaluate seeds by driving scores. If no error scenarios arise, the seed with the lowest score progresses to the next fuzzing round. Mutations then apply the random nearest neighbor strategy based on the seed's status to enhance error scenario discovery. Upon identifying an error scenario, the fuzz phase stops, and the original seed may re-enter the queue, subject to roulette-based frequency checks to prevent local optima and ensure diversity. If no errors occur, fuzzing proceeds with subsequent seeds until the queue depletes, then replenishes from the seed pool to maintain diversity.\nThis approach maintains a steady flow of new seeds for fuzzing, prevents stagnation at local optima, and ensures scenario variety."}, {"title": "3.8 Replay and Analysis of Test Results", "content": "In the error scenarios, we preserve the frame-by-frame driving trajectory coordinates of the ego car and surrounding objects throughout the testing phase, along with details such as weather parameters and object style types. Furthermore, we chronicle a gamut of data produced by the ADS in operation, including prediction and log records, ensuring the reproducibility of each scenario.\nConsidering the multitude of error scenarios unveiled, categorizing and organizing them is crucial. For collision incidents, we implement a feature extraction technique to reduce manual sorting efforts. This approach leverages features from the SEM and a self-supervised extractor based on the trajectory shared between the ego car and the object involved in the collision. These features are then amalgamated for clustering, aiming to distill characteristic vehicle interaction patterns, as illustrated in Figure 5. The manual review follows the clustering to confirm the uniqueness and significance of each pattern, especially since automated clusters can vary in utility and coherence. This step involves manually verifying and de-duplicating data, ensuring that the distilled patterns accurately reflect real-world interactions.\nWhen dealing with incidents like speeding, traffic light violations, unlawful lane shifts, and stagnation, we examine the coordinates pinpointing these errors and classify them, extracting representative scenarios. Upon identifying these scenarios, we trace back system bugs by replaying the test sequence and meticulously analyzing the logs."}, {"title": "4 EVALUATION", "content": "To evaluate the performance of our framework, we conducted experiments to answer the following research questions:\n\u2022 RQ1: How do components like mutation strategy and seed filtering impact the testing framework's performance?\n\u2022 RQ2: What is the accuracy and generalization capability of the SEM, and does it improve with more data?\n\u2022 RQ3: How does our method's capability in uncovering erroneous scenarios on a single seed compare to other methods?\n\u2022 RQ4: Does our method demonstrate improved efficiency in error scenario discovery and code coverage compared to Drivefuzz, and what role does the corpus play?\n\u2022 RQ5: Can the method detect bugs in ADS, and what are the common errors and collision-prone scenarios it reveals?\nWe address these questions through a series of detailed experiments, designed to explore the core aspects of our testing framework. The results and analyses of these experiments are presented in the following subsections, setting the stage for a comprehensive evaluation of our framework's performance and capabilities."}, {"title": "4.1 Experimental Setup", "content": "Experiment Configuration: We configured the experiment parameters as follows: Nc = 3 for the maximum number of cycles, Nm = 100 as the mutation seed count (defaulting to Nm = 3 if the scenario evaluation model is unset due to omission of the filtering process), and Ne = 3 for the execution seed number. Details regarding the distribution of each type of road in the scenario seed corpus (from \u00a73.2) across the map can be found in the supplementary material. For our experimental evaluation, we randomly selected an initial seed from the scenario seed corpus spanning maps Town01 to Town05 within the CARLA. This selection strategy facilitates a comprehensive comparison across a wide range of scenarios, without the need for specific constraints on road types or traffic signs.\nADS Types Studied: Our study encompasses a range of ADS types: Autoware\u00b3 [11], LAV4 [4], Transfuser5 [7], and NEAT6 [6] .\nAdditionally, we evaluate a distinct ADS kind, termed Autopilots, which relies solely on planning and control modules, interfacing directly with the world state and map data in CARLA. While this type is rare in real-world applications, it offers valuable insights for validating our proposed testing methodologies. We include two logic-based Autopilot systems Basic agent and Behavior agent, both formerly examined in the Drivefuzz project. Our study introduces three academic ADSS - LAV, Transfuser, and NEAT - all rigorously assessed via the CARLA Leaderboard.\nBaseline Comparisons: Considering the interdependencies among CARLA, ADS, and the test method framework, Drivefuzz [19] exclusively meets our needs for fuzzing from scratch and complete scenario construction where there are no predefined scenario sources. For approaches with predefined scenario sources, we have"}, {"title": "4.2 Results", "content": "4.2.1 Component Impact on Performance. To compare the roles of the two-stage mutation strategy and SEM in the entire framework, we conducted relevant ablation experiments. These are divided into: 1) RMS: using random mutation; 2) 2SMS: using a two-stage mutation strategy; 3) RMS+SEM: using random mutation followed by SEM filtering; 4) 2SMS+SEM: using a two-stage mutation strategy followed by SEM filtering, which is the complete method. We tested the Autoware for 6 hours using these various combinations. As Autoware is a mature ADS already applied in the industry, the testing efficiency performance evaluated is relatively objective.\nFrom Figure 6a, it can be observed that as the components are progressively refined, the efficiency of the framework in uncovering error scenarios also gradually improves. In the comparison of whether to adopt a two-stage mutation strategy, it is evident that the random mutation strategy tends to stagnate in terms of the number of scenarios uncovered during the exploration process (e.g., at the 3-hour mark). With the guidance of the driving score, the two-stage mutation strategy can alleviate such issues, with this adaptive strategy better delving into and further exploring the vicinity of error scenarios. In the comparison of whether to use SEM, it's clear that the efficiency is higher with SEM. SEM filters mutated seeds based on historical data, effectively selecting high-quality seeds for execution, thereby enhancing efficiency.\n4.2.2 SEM's Accuracy and Growth. In the previous section, we discussed the impact of having or not having SEM on the testing framework. Next, we need to evaluate the growth performance of SEM under different amounts of historical data, as well as the accuracy and generalization of seed filtering capabilities across different testing systems. As described in \u00a73.5, we train SEM whenever the historical test data reaches a certain quantity and retain 20% of the data for model evaluation. In our experiments, we updated the model three times when the historical data quantities reached 1k, 2k, and 3k, respectively.\nWe integrated the SEM from these three different stages into the testing framework and conducted a 6-hour test on Autoware. The efficiency of error scenario discovery is shown in Figure 6b. As the volume of training historical data increased, SEM's filtering capability improved, driving an enhancement in discovery efficiency. The difference in efficiency between 1k and 2k data volumes was not particularly significant, but when the data volume reached 3k, there was a substantial increase in efficiency. This indicates that"}, {"title": "4.2.3 Single-Seed Discovery Performance", "content": "We conducted a six-hour Autoware system test across three types of road scenarios at the same geographical location, assessing each method's ability to generate erroneous scenarios.\nIn which, AV-Fuzzer determines the initial positions of all vehicles in a given scenario and mutates the maneuvers of other vehicles. SAMOTA optimizes scenario parameter vectors, which are used to select from three predefined ego car tasks and determine whether to position other vehicles in one of three types of locations. Our method, leveraging automatically generated corpus information, allows the mutator to select appropriate positions and paths for entities, creating scenarios with various interactions, as"}, {"title": "Scenario Trajectory Distribution and Method Analysis", "content": "To assess the influence of different scenario generation processes, we visualized the trajectories of both the ego car and other objects for all three methods, pinpointing collision sites and other errors as demonstrated in Figure 9.\nAV-Fuzzer and SAMOTA similar to many existing methods, predefine the positions and paths for the ego car and other objects, our approach diverges significantly. Specifically, AV-Fuzzer focuses on controlling the maneuver of other objects, while SAMOTA emphasizes combinations of ego car tasks with other vehicles' presences within the same scenario. In contrast, our method employs an automated map crawling technique to extract all potential placement points and paths from the road network, thereby furnishing both the ego car and other objects with comprehensive path and placement information from our initial seed, as depicted in Figure 8.\nThe initial seed setup of our approach addresses the limitations associated with manual predefined settings. Whereas AV-Fuzzer and SAMOTA require manually setting within the map, getting the limited quantity of initial seeds, our method automates the extraction of segment information across different geographical locations and road types, efficiently using the resources of the map. This automation expands the testing breadth and allows for a richer set of scenario settings, enabling thorough exploration of"}, {"title": "4.2.4 Drivefuzz Comparison and Corpus Role", "content": "In assessing generation efficiency, we conducted tests on six ADS systems using three different approaches: Drivefuzz, Drivefuzz complemented with our corpus, and our proprietary method, ScenarioFuzz. Each system underwent five testing iterations. We then computed the average quantity of erroneous scenarios generated (depicted as lines) and their standard deviation (illustrated as areas) during these tests7.\nEfficiency: As can be seen from Figure 10, our method consistently outperforms Drivefuzz in efficiency across all systems, and the stability across the five tests is also notably superior. The magnitude of difference between the two methods on different systems is also related to the error patterns inherent in the systems. On Autoware, although the overall number advantage isn't pronounced, it's mainly because the Autoware system struggles to effectively recognize traffic lights in CARLA. Drivefuzz, which randomly selects start and end points on the map, predominantly captures red-light running scenarios. NEAT, due to performance issues, frequently stagnates during testing, which severely hampers the potential to unearth other types of errors.\nCode Coverage: In our testing process, we specifically measured the code line coverage for the perception, planning, and control modules of the Autoware system, and for the planning and control modules of the Basic and Behavior systems, utilizing lcov and the Python library 'coverage', respectively. By treating each generated scenario as a unique test case, we aimed to trigger diverse operational conditions within these modules. This approach allows us to assess how thoroughly different scenarios exercise the ADS's code, highlighting areas that remain untested. High coverage indicates that the system has been activated and executed across a diverse range of environments.\nAs shown in Figure 12, the code coverage for Basic and Behavior was relatively low in the initial phase. This was due to scenarios generated by DriveFuzz, similar to those depicted in Figure 13, where ineffective scenarios reduced vehicle operation time, preventing the effective triggering of the corresponding code. During testing, DriveFuzz's entirely random mutation strategy led to it being stuck in local scenarios, continuously mutating on ineffective scenario seeds. However, as seen from the code line coverage results, our method triggers planning and control modules more frequently. This can be attributed to the corpus providing all appropriate placement points and path settings within scenarios, enhancing behavioral interactions, and thus enabling better triggering and coverage"}, {"title": "4.2.5 The Identified Bugs and Typical Scenarios", "content": "In our framework, we define a 'bug' as any deviation in the system's operation from its expected performance or safety standards. These deviations could manifest as reduced system functionality, potential safety hazards, or incorrect responses to environmental stimuli. We continuously monitored the operational status of various systems under generated error scenarios and identified 58 bugs, as shown in the supplementary material. This revealed serious safety issues in components across different modules of each system. We highlight a few concerning typical cases:"}, {"title": "5 RELATED WORK", "content": "Current approaches in ADS testing often rely on predefined scenario sources, necessitating exact scenario layouts, starting and ending points for vehicles and pedestrians, and constraints on their behaviors. This method typically requires choreographing a starting scenario with designated constraints, which can be time-consuming and limits the variety of scenarios to those pre-designed. To achieve comprehensive testing, multiple scenarios must be pre-set, but this approach is not exhaustive.\nAV-Fuzzer was an early method based on this approach, utilizing a genetic algorithm to generate object trajectories within a starting scenario [21]. SAMOTA adopts a manual approach, annotating road sections for scenario locations and testing through parameter combinations and search [16]. In contrast, DriveFuzz departs from relying on a predefined scenario source. It utilizes CARLA API's placement waypoints for randomizing the starting and ending points of the ego car and nearby objects, driving seed mutation and generation through driving score feedback [19].\nOur method, in response, employs map crawling technology to extract placement positions and reasonable driving paths based on road networks, proposing a novel corpus construction method. The mutator settings cover all scenario levels within CARLA's interface range, offering diversified object appearances in mutations. A two-stage mutation strategy effectively explores global and local areas, and with the addition of SEM, historical test data is used to enhance seed filtering, improving the overall efficiency of the method."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced ScenarioFuzz, a fuzz testing generation framework based on autonomous driving scenarios. This framework aims to identify error scenarios in autonomous driving systems without an initial scenario source and reveal the underlying issues. Our work includes: (1) Automatically building a scenario seed corpus using map crawling technology, providing an effective information foundation for subsequent mutations; (2) Training a scenario evaluation model using historical test data, effectively filtering mutated scenario seeds, thereby enhancing the accuracy and efficiency of the fuzz testing generation process; (3) Designing a comprehensive fuzz testing framework to test autonomous driving systems from a scenario perspective. Our work can effectively mine error scenarios of ADS and locate a series of bugs."}]}