{"title": "Discovering Chunks in Neural Embeddings for Interpretability", "authors": ["Shuchen Wu", "Stephan Alaniz", "Eric Schulz", "Zeynep Akata"], "abstract": "Understanding neural networks is challenging due to their high-dimensional, interacting components. Inspired by human cognition, which processes complex sensory data by chunking it into recurring entities, we propose leveraging this principle to interpret artificial neural population activities. Biological and artificial intelligence share the challenge of learning from structured, naturalistic data, and we hypothesize that the cognitive mechanism of chunking can provide insights into artificial systems. We first demonstrate this concept in recurrent neural networks (RNNs) trained on artificial sequences with imposed regularities, observing that their hidden states reflect these patterns, which can be extracted as a dictionary of chunks that influence network responses. Extending this to large language models (LLMs) like LLaMA, we identify similar recurring embedding states corresponding to concepts in the input, with perturbations to these states activating or inhibiting the associated concepts. By exploring methods to extract dictionaries of identifiable chunks across neural embeddings of varying complexity, our findings introduce a new framework for interpreting neural networks, framing their population activity as structured reflections of the data they process.", "sections": [{"title": "1. Introduction", "content": "Neural networks are known as \u201cblack box systems\" (Templeton, 2021; Petsiuk et al., 2018; Ribeiro et al., 2016; Zhang & Zhu, 2018; Marks et al., 2024b; Elhage et al., 2022): as their computation are performed by millions and billions of interacting components a number far exceeding any conventional models such as physical laws describing the interaction between bodies, or linear regression functions. Many approaches have attempted to bridge the gap between simple and complex models by replacing complex computations with symbolic substitutes. However, this is inherently difficult and inevitably involves a trade-off between computational efficiency and the number of symbolic components used (Adadi & Berrada, 2018; Lipton, 2017; Belinkov, 2022).\nA substantial part of the interpretability question is cognitive (Adadi & Berrada, 2018; Miller, 2018) and demands understanding what makes high dimensional data meaningful for people. We present a novel perspective on interpreting artificial neural networks by referencing how cognition interprets sensory data. Similar to the dimensionality of neural networks' activities, perceptual data that flood into our sensory stream is also high-dimensional. We make sense of this \"booming, buzzing confusion\u201d by instantly segmenting recurring patterns in perceptual sequences as chunks (Graybiel, 1998; Gobet et al., 2001; Egan & Schwartz, 1979; Ellis, 1996; Koch & Hoffmann, 2000; Chase & Simon, 1973). In this way, cognition perceives a structured representation of the overwhelming flow of the sensory stream as the appearance and disappearance of chunks over time (Wu et al., 2023; 2025). Inspired by this principle of cognition, we explore whether the cognitive mechanism of chunking can be leveraged to interpret high-dimensional neural population activities.\nWe hypothesize that this approach could work if the neural network's hidden activity reflects the regularities in the data it learns to predict. To test this \u201creflection hypothesis\", we first engineer datasets by injecting known regularities and analyze the neural population activity of a simple RNN trained to predict these sequences as a proof of concept.\nWe then extend our investigation to large language models predicting sequences. Throughout, we explore several chunk extraction methods from low to high dimensions of neural population activities, highlighting their strengths and limitations. Our findings suggest a novel interpretability framework that leverages the cognitive principle of chunking to identify meaningful entities within artificial neural activities."}, {"title": "2. Related Work", "content": "Most interpretability approaches hold a salient agreement on \"what is interpretable\" and \"what to interpret\". \"What is interpretable\" is influenced by models in physics and mathematics, where operations and derivations are framed around the manipulation of a small set of well-defined symbols. Hence, interpretable concepts are confined to word-level or token-level description, and approaches try to learn a mapping between the neural activities and the target interpretable concept to understand (Geva et al., 2022; Zou et al., 2023; Belinkov, 2022; Belrose et al., 2023; Pal et al., 2023; Din et al., 2023).\nThe current approaches on \"what to interpret\" to understand the computations inside a neural network is heavily influenced by neuroscience: either on the level of neurons as a computation unit or in a low-dimensional neural activity descriptions. The earliest interpretability approaches, inspired by neuroscience discoveries such as \u201cgrandmother cells\" and \"Jennifer Aniston neurons\", focused on understanding the semantic meanings that drive the activity of individual neurons. Similarly, studies in artificial neural networks, from BERT to GPT, have identified specific neurons and attention heads whose activations correlate with semantic meanings in the data (Olah et al., 2020b; Elhage et al., 2022; Wang et al., 2022; Marks et al., 2024a; Bau et al., 2020; Goh et al., 2021; Nguyen et al., 2016; Mu & Andreas, 2021; Radford et al., 2017). The sparse autoencoders (SAEs) approach can be seen as an intermediate step that encourage the hidden neurons to be more monosemantic (Bricken et al., 2023; Braun et al., 2024; Cunningham et al., 2023; Chaudhary & Geiger, 2024; Karvonen et al., 2024). Thereby, one trains an autoencoder to map neural activities of a hidden unit layer to a much larger number of intermediate hidden units while encouraging a sparse number of them to be active. In this way, the target hidden layer activity can be represented by a superposition of several individual neurons inside the SAE.\nOther approaches reduces and interprets neural population activities in lower dimensions: representation engineering captures the distinct neural activity corresponding to the target concept or function, such as bias or truthfulness (Zou et al., 2023). Then, it uses a linear model to identify the neural activity direction that predicts the concept under question or for interference with the network behavior.\nThe current interpretability approach that studies language-based descriptions as conceptual entities and their implications for individual/low-dimensional neurons suffers from limitations on both ends: meanings are finite, and individual neurons are limited in their expressiveness and may not map nicely to these predefined conceptual meanings.\nJust like physics models lose their failure to have a closed-form description of motion beyond two interacting bodies (Tao, 2012), confined, symbolic definitions of interpretation have inherent limitations in precision. This cognitive constraint-our reliance on well-defined symbolic entities for understanding-has made deciphering the complexity of billions of neural activities an especially daunting task. It underscores a fundamental trade-off between the expressiveness of a model and its interpretability (Wang et al., 2024).\nFocusing solely on individual neurons is also is insufficient to capture the broader mechanisms underlying neural activity across a network. \u201cmonosemantic\u201d neurons, which respond to a single concept, make up only a small fraction of the overall neural population (Radford et al., 2017; Elhage et al., 2022; Wang et al., 2022; Dai et al., 2022; Voita et al., 2023; Miller & Neo, 2023). Empirically, especially for transformer models (Elhage et al., 2022), neurons are often observed to be \u201cpolysemantic\u201d,, i.e., associated with multiple, unrelated concepts (Mu & Andreas, 2021; Elhage et al., 2022; Olah et al., 2020a), which complicates the task of understanding how neural population activity evolves across layers (Elhage et al., 2022; Gurnee et al., 2023). This highlights the need for more holistic approaches that account for the complex, distributed nature of neural representations."}, {"title": "3. Chunks from Neural Embeddings (CNE)", "content": "We effortlessly perceive high-dimensional perceptual signals by segmenting them into recurring, meaningful patterns. The recurring patterns reside in a subpopulation of our perceptual dimensions as cohesive wholes (Miller, 1956; Laird et al., 1984; Graybiel, 1998; Gobet et al., 2001).\nHumans segment perceptual data into chunks as a strategy to reduce the complexity of naturalistic data. As naturalistic data exerts a compositional structure and contains rich regularities - isolating out the recurring entity as a concept, is an effective way for an agent to compress their observation into entities and their relations (Wu et al., 2023; 2022). For instance, as shown in Figure 1 (top-left), natural language contains recurring alphabets, words, phrases, grammatical rules, and morphological and sentence structures on a concrete and abstract level.\nSimilar to humans, artificial intelligence systems are tasked with this problem of understanding reality from naturalistic data. The regularities in naturalistic data may drive converging representations in diverse AI models. Past research indicates that neural networks of different architectures, scales, and sizes learn representations that are remarkably similar (Balestriero & richard baraniuk, 2018; Moschella et al., 2023), even when trained on different data sources (Lenc & Vedaldi, 2015). This phenomenon is particularly pronounced in larger, more robust models(Bansal et al., 2021; Dravid et al., 2023; Kornblith et al., 2019; Roeder et al., 2021). Such patterns have been observed across multiple data modalities (Lenc & Vedaldi, 2015; Moschella et al., 2023; Kornblith et al., 2019; Roeder et al., 2021). Recently, it has been hypothesized that such convergences reflect an underlying statistical model of reality, aligning with a Platonic conceptual framework (Huh et al., 2024).\nThe Reflection Hypothesis: We propose that artificial neural networks, like human cognition, may reflect the regularities and redundancies present in the data they are trained on by encoding these patterns within their neural computations. Specifically, we hypothesize that a well-trained neural network should exhibit trajectories of neural computation that mirror the structure of its training data.\nIf this hypothesis holds, it could allow us to leverage the human cognitive ability to \u201cchunk\u201d high-dimensional perceptual data into meaningful units. Just as human perception breaks down complex sensory inputs into recurring, segregated patterns, neural network activities might also be decomposable into recurring \"chunks\". These chunks, residing within subsets of all neurons of the network, could represent distinct patterns of computation and serve as interpretable entities. By identifying and analyzing these chunks, we could reduce the complexity of neural activities into interactions between these interpretable units."}, {"title": "3.1. Extracting Invariances", "content": "We explore multiple methods to extract recurring chunks in neural population activities, which can be applied to different dimensionalities of data. Formally, denote the training sequence as $S = (s_1, s_2, ..., s_n)$, indexed by $I = {1,2,3,\u2026\u2026, n}$, the sequence of neural network activations of the input sequence as $S_h = (h_1,h_2,\u2026\u2026, h_n)$, which we also refer to as neural population activity. Each neural population vector has embedding dimension d: $h_i \\in R^d$. Depending on the dimensionality d and the nature of the problem, we develop three chunk extraction methods: discrete sequence chunking for small d, neural population averaging for large d and when there is an identifiable pattern in S, and unsupervised chunk discovery when no supervising pattern is available.\nDiscrete Sequence Chunking When d is relatively small, we can use a cognitive-inspired method to extract recurring patterns (Wu et al., 2022), which is also used for text compression (Gage, 1994; Zaki, 2000; Agrawal & Srikant,"}, {"title": "4. Results", "content": "4.1. Proof of concept in simple RNNs\nFirst, we assessed the reflection hypothesis using a simple recurrent neural network (Figure 2a, details in Appendix A) trained on artificially generated sequences with a known chunk, ABCD. We begin with sequences featuring periodic occurrences of ABCD (Figure 2b), then gradually increase complexity and noise: first, by embedding ABCD sparsely within a default sequence of E (Figure 2c), and then, by presenting ABCD as a cohesive chunk amid background noise composed of random occurrences of E, F, and G (Figure 2d). For each sequence type, an RNN was trained to predict the next element in the sequence. Visually, the hidden states of the RNN exhibited identifiably recurring patterns aligned with the ABCD chunks in the input sequence.\nUnderstanding the neural state and its mapping to the input We then applied Discrete Sequence Chunking (Figure 1 bottom) to convert hidden unit activity from a continuous time scale to the symbolic and discretized description. Thereby, we can denote population activities by the alternation of indices that marks the belonging cluster centroid and visualize the population activity by the indices of the assigned nearest cluster. This symbolic description of the neural population trajectory allows decoding the trajectory of the hidden state and its corresponding input regularity via a look-up table (illustrated in Appendix C), and reaches a perfect decoding accuracy on the test set.\nFrom this the symbolic description of neural population trajectories, we can then apply chunk discovery methods to learn a dictionary of symbolized chunks. This dictionary contains the maximally recurring patterns inside the network, reflecting the patterns in the data consistently and in an interpretable way. This suggests that RNN's hidden unit activities when trained on sequences with repeating inputs, follow a highly regularized trajectory, and conventional chunking methods can extract the neural activity corresponding to processing these sequence regularities.\nPopulation activity chunks causally alter network's behavior We test the causal role of these extracted recurring population states by grafting the embedding state to the identified chunk states. As shown in Figure 3 (left), we start with the current state of the RNN that predicts B from the input character A, and the memory state that resulted from the"}, {"title": null, "content": "$\nhc(s) = \\frac{\\sum_{j \\in V(s)} h_{c(s),j}}{|V(s)|} \\text{ and } \\lim_{|V(s)| \\rightarrow \\infty} \\overline{h_{c(s)}} = \\mu \\text{ (a.s.)}\n$"}, {"title": null, "content": "$\\ C(s) = {i \\in W : |h_{i,j} - \\overline{h_i}| < tol \\forall j \\in V(s)}$\n$"}, {"title": null, "content": "$\\ \\Delta = \\max_{j \\in V(s)} \\frac{||h_{c(s),j}-\\overline{h_{c(s)}}||}{d} $"}, {"title": null, "content": "$\nf_{chunk}(\\overline{h_{c(s)}}, h_{c(s)}, \\Delta_{(s)}) = \\begin{cases}\n1, & \\text{if } \\frac{||h_{c(s)} - \\overline{h_{c(s)}}||_2}{d} < \\Delta_{(s)},\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$"}, {"title": null, "content": "$\\L = \\frac{1}{M} \\sum_{m \\in {1,\\ldots M}} \\max_{k \\in {1,...,K}} SIM(D_k, X_m)$"}, {"title": null, "content": "$\\SIM(d, x) = \\frac{d \\cdot x}{||d||_2 ||X||_2}$"}, {"title": "5. Discussion", "content": "We propose concept-guided interpretability methods that leverage the cognitive tendency for chunking to distill high-dimensional neural activities into interactions of recurring entities. As an initial proof of concept, our approach has limitations. We encourage future work to further test the reflection hypothesis across different models, establish its theoretical foundations, refine chunk extraction methods-such as capturing multi-token chunks in unsupervised discovery-and extend this framework toward nonparametric chunk discovery."}, {"title": "6. Conclusion", "content": "We identified recurring chunks within the neural dynamics of RNNs and LLMs, suggesting that the cognitive tendency for chunking can be leveraged to segment high-dimensional activity into neural trajectory chunks\u2014revealing a structured reflection of the world within artificial minds."}, {"title": "Software and Data", "content": "Software will be published conjunctively with paper."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of interpretability. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}