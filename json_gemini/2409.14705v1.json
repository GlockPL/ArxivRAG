{"title": "Target-Aware Language Modeling via Granular Data Sampling", "authors": ["Ernie Chang", "Pin-Jie Lin", "Yang Li", "Changsheng Zhao", "Daeil Kim", "Rastislav Rabatin", "Zechun Liu", "Yangyang Shi", "Vikas Chandra"], "abstract": "Language model pretraining generally targets a broad range of use cases and incorporates data from diverse sources. However, there are instances where we desire a model that excels in specific areas without markedly compromising performance in other areas. A cost-effective and straightforward approach is sampling with low-dimensional data features, which allows to select large-scale pretraining data for domain-specific use cases. In this work, we revisit importance sampling with n-gram features consisting of multi-granular tokens, which strikes a good balance between sentence compression and representation capabilities. We observed the sampled data to have a high correlation with the target downstream task performance while preserving its effectiveness on other tasks. This leads to the proposed data sampling paradigm where language models can be pretrained more efficiently on selected documents. On eight benchmarks we demonstrate with ~1% of the data, pretrained models perform on par with the full RefinedWeb data and outperform randomly selected samples for model sizes ranging from 125M to 1.5B.", "sections": [{"title": "1 Introduction", "content": "Language model pretraining is the cornerstone of universal language models (LMs), creating general-purpose representations to excel across a variety of NLP downstream tasks (John and Draper, 1975; Murphy, 2012). This process often involves the use of vast amounts of text, sometimes measured in billions or even trillions of tokens from webpages (Abnar et al., 2022; Kaplan et al., 2020). However, there are instances where a model needs to perform well in specific domains while not compromising performance in others. This necessitates the use of data selection methods to determine which potential data points should be included in the training dataset and how to effectively sample from these selected points (Albalak et al., 2024)."}, {"title": "2 The Approach", "content": "Selecting samples from large-scale datasets such as RefinedWeb (Penedo et al., 2023) is slow and expensive. A tractable solution is to encode each document as a vector using n-gram features that can be computed easily. Here we assume in our settings a small number of target text examples \\u201cDtask from a target distribution p and a large raw dataset Draw drawn from a distribution q with N examples, we aim to select k examples (k \\u00ab N) from the raw dataset that are similar to the target.\nWe adopted the importance sampling technique as in Xie et al. (2023) which selects examples that\nalign with target distribution. The technique provides a tractable importance estimate of each text, and applies importance sampling on a feature space Z that provides the necessary structure. The feature extractor h: X \\u2192 Z is used to transform the input x into features z = h(x). The resulting raw and target feature distributions are qfeat and Pfeat, respectively. Our objective is to select examples whose features align with the target feature distribution Pfeat. To do so, features qfeat and Pfeat are extracted (Figure 1) using n-grams extracted from each tokenized document using an adapted tokenizer. Each n-gram is mapped to a key in the hash table where the ids of the table define a fixed-size embedding, and each key maps to the n-gram count. Then, the importance weights is computed for each featurized example zi = h(xi) from the N raw examples, with the weight wi =\n$\\frac{P_{feat}(z_i)}{\u00ce_{feat}(z_i)}$. The final step involves sampling, where we select k examples without replacement from a categorical distribution, the probabilities of which are given by $\\frac{w_i}{\\sum_{i=1}^N w_i}$.\nTokenizer Adaptation. Here we adapt the vocabulary to the target data. To derive target vocabulary V(t), we use Llama-3 tokenizer's vocabulary Vstart as the starting point and merge Vstart with Vtask which is learned from task data Dtask. In constructing Vtask, we make sure to include multi-granular tokens (i.e. words and multi-words), where Vtask is then merged with Vstart to form v(t \\u2212 1). Next, we incrementally remove tokens from v(t - 1) to obtain v(t), where we minimize the distance from the original vocabulary set such that a less biased document feature can be extracted as n-gram vectors. We first define a metric to measure the quality of vocabulary set on a corpus, following Xu et al. (2021), which proposed to learn optimal vocabulary by maximizing the vocabulary utility metric (H) computed as:\nHt = -$\\sum_{j \\in V} P(j) log P(j)$,\nwhere P(j) is the relative frequency of token j from the target data and l is the average length of tokens in vocabulary v. For any vocabulary, its entropy score Hv can be calculated based on a vocabulary from its previous step. The optimization problem can be formulated as:\narg min [Hv(t) \\u2212 Hv(t \\u2212 1)].\nv(t-1),v(t)"}, {"title": "3 Experimental Setup", "content": "Network, training details and evaluation. We pretrain the decoder-only transformer using causal language modeling objectives on selected datasets, averaging over three initialization runs for each configuration, where model weights were randomly initialized. The language models varied in size, with 125M, 350M, 500M, and 1.5B parameters. This range allowed us to explore how model complexity impacts the final results. Pretraining was conducted on a distributed computing setup with 32 GPUs across 4 nodes, each equipped with an H100 graphics card. We evaluated our proposed Multi-granular selection approach against random selection (Random) and compared it with the same sampling algorithm using word-based N-gram features. Importance sampling (Xie et al., 2023) was employed for all feature types.\nDatasets. We evaluate the models on eight common sense reasoning tasks in a zero-shot fashion, including ARC-easy, ARC-challenge (Clark et al., 2018), BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), OBQA (Mihaylov"}, {"title": "4 Main Results", "content": "Overall, language models varied in four sizes display marked improvements when trained on sampled coresets selected using multi-granular features, achieving a 6.94% improvement in average benchmark scores (AVG.). The proposed method with multi-granularity consistently outperforms importance sampling that uses only n-gram features on the average of all benchmarks. Moreover, our result also shows that despite sampling based upon a target dataset, the model performance does not degrade on non-target benchmarks (See Figure 4). Figure 3 shows the sharp metric improvements of averaged performance on the eight tasks, starting at a model of size 125M to 1.5B, which indicates the potential of the technique to scale up the capabilities of small language models."}, {"title": "5 Further Discussion", "content": "Finer-grained Features Reduces Task Biases. Based on our ablation, we observe marked improvement by simply using subword n-grams. Moreover, we show in Figure 4 that selecting from a single task introduces task data biases that degrades the performance. This is mitigated through the use of finer-grained n-gram features where we introduce multi-granular tokens containing subwords and multi-words, which gives an additional 5.78% improvement over word-based n-grams. We postulate that this improvement has to do with the reduction of hash collisions in the hashed n-gram features, where the joint use of subword and multi-word capture beyond the boundaries of a word while preserving parts of a word in tokens so that"}, {"title": "6 Conclusion and Future Works", "content": "In this study, we revisited importance sampling of text corpus in language modeling by exploring multi-granular n-grams as features. This led us to explore a pretraining paradigm where we can obtain more targeted data for more efficient language modeling. Our findings, validated across eight benchmarks, allow us to put forward multi-granular n-grams features as viable document representations used in importance sampling. For future work, we will aim to extend this approach to larger language models and datasets."}, {"title": "Limitations", "content": "While the method of targeted data sampling using low-dimensional features is efficient and enhances specific performance areas, it is not without its challenges. Further exploration is needed to refine the process of selecting optimal data features that balance domain specificity with general applicability. Importantly, we have not taken explicit steps to ensure that the sampled data does not contain biases in the data. Moreover, we believe a more solid conclusions can be drawn when an even larger pretraining data is experimented, and other model-based approaches are also taken into account. All in all, this study highlights the importance of fine-tuning data selection in pretraining smaller language models to avoid overfitting while maintaining robust performance across diverse tasks."}, {"title": "Ethics Statement", "content": "The practice of selective data sampling in language model pretraining has shown promising results in enhancing model performance in targeted tasks. Our experiments are conducted using datasets that are widely recognized and utilized within the research community, ensuring the reproducibility and reliability of our results. However, the application of this method to sensitive or private datasets necessitates stringent adherence to ethical standards. Furthermore, the increased efficiency in training specialized smaller models could potentially lead to escalated computational demands, which must be considered when scaling these methods."}]}