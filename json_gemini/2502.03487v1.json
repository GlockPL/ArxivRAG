{"title": "Artificial Intelligence and Legal Analysis: Implications for Legal Education and the Profession", "authors": ["Lee F. Peoples"], "abstract": "This article reports the results of a study examining the ability of legal and non-legal Large Language Models (LLMs) to perform legal analysis using the Issue-Rule-Application-Conclusion (IRAC) framework. LLMs were tested on legal reasoning tasks involving rule analysis and analogical reasoning. The results show that LLMs can conduct basic IRAC analysis, but are limited by brief responses lacking detail, an inability to commit to answers, false confidence, and hallucinations. The study compares legal and non-legal LLMs, identifies shortcomings, and explores traits that may hinder their ability to \u201cthink like a lawyer.\" It also discusses the implications for legal education and practice, highlighting the need for critical thinking skills in future lawyers and the potential pitfalls of over-reliance on artificial intelligence (AI) resulting in a loss of logic, reasoning, and critical thinking skills.", "sections": [{"title": "Introduction", "content": "1 Law and society at large are in the midst of a fourth industrial revolution brought about by artificial intelligence and related technologies.\u00b9 These developments could potentially usher in a \"golden age for the [legal] profession and society\u201d\u201d but also have the potential to create \u201ceconomic inefficiency, social disfunction, and a declining legal system.\u201d\u00b3 Goldman Sachs predicted that AI could be more disruptive in law than other industries and could potentially replace 44 percent of legal jobs.4\n2 Previous studies have concluded that law students using generative AI tools could \"substantially improve the efficiency with which they complete a broad array of legal tasks without adversely affecting (or even slightly improving) the quality of that work product.\"5 Lawyers will also benefit from AI, and it will be a vital tool for law practice both in the future and in the long term.6\n3 However, the gains to be realized from using AI in legal education and the legal profession are not without potential perils. Al research tools have notoriously hallucinated facts and laws, fabricated case law that does not exist, falsely attributed a Supreme Court dissenting opinion to a justice who actually joined the majority, falsely accused a law professor of sexual assault, and even temporarily going insane.10 Lawyers have misused AI to create fictional cases cited in court filings\u00b9\u00b9 and to estimate attorney's fees, leading to criticism, sanctions from judges, and disciplinary actions.12\n14 This study examines the abilities of legal and non-legal LLMs to think like a lawyer by performing legal analysis and reasoning on legal factual scenarios using the Issue-Rule- Application-Conclusion (IRAC) framework. LLMs tested include Lexis+ AI, Claude,"}, {"title": "Using IRAC to Test Legal Analysis Abilities of LLMs", "content": "15 This study examines the abilities of legal and non-legal LLMs to think like a lawyer by performing legal analysis and reasoning. A common framework taught in American law schools is IRAC. The IRAC framework breaks legal analysis down into four discrete steps. The IRAC framework can be used to analyze legal rules, statutes, and to reason analogically using caselaw. A helpful explanation is found in the Legal Bench study.\nFirst, lawyers identify the legal issue in a given set of facts (issue-spotting). An issue is often either (1) a specific unanswered legal question posed by the facts, or (2) an area of law implicated in the facts. Depending on the setting, a lawyer may be told the issue or be required to infer a possible issue.\nSecond, lawyers identify the relevant legal rules for this issue (rule-recall). A rule is a statement of law that dictates the conditions that are necessary (or sufficient) for some legal outcome to be achieved. In the United States, rules can come from a variety of sources: the Constitution, federal and state statutes, regulations, and court opinions (case law). Importantly, rules often differ between jurisdictions. Hence, the relevant rule in California might be different than the relevant rule in New York.\nThird, lawyers apply these rules to the facts at hand (rule-application). Application, or the analysis of rule applicability, consists of identifying those facts which are most relevant to the rule, and determining how those facts influence the outcome under the rule. Application can also involve referencing prior cases involving similar rules (i.e., precedent) and using the similarities or differences to those cases to determine the outcome of the current dispute, once pretraining is complete. Finally, lawyers reach a conclusion with regards to their application of law to determine what the legal outcome of those facts are (rule-conclusion).25"}, {"title": "Testing Methodology", "content": "16 Factual scenarios were adapted from a text utilized in introductory law school courses on legal research, writing, and legal analysis.26 Factual scenarios were anonymized by removing specific names to keep from exposing content-specific identifiers to LLMs' corpora. Additionally, LLMs were instructed to not train on the prompts.\n17 LLMs were tested on a total of seven scenarios. Scenarios requiring beginning and skilled IRAC abilities were selected for testing LLMs abilities to analyze legal rules and perform analogical reasoning.27 Scenarios requiring beginning, intermediate, and skilled IRAC abilities were selected for testing LLMs' abilities to analyze statutes.28 LLMs' statutory reasoning abilities were explored in more detail because previous studies have demonstrated"}, {"title": "How LLMS Work", "content": "12 Large language models are neural networks with billions of parameters trained on vast datasets of text. At their core, they represent words as high-dimensional numerical vectors. These word vectors are fed into layers of artificial neurons that perform mathematical operations to capture linguistic patterns and contexts.15 This process is an \u201calgorithmic approach to machine learning known as a \u2018neural network\u201d16 The neural network is an \u201cextremely flexible pattern detector.\u201d17 LLMs using neural networks have been described as \u201cadvanced Al word-prediction systems.\"18\n13 When a user enters a prompt into an LLM, it is processed within the LLM's \"context window,\" which functions as its \u201cshort-term memory,\u201d allowing it to make an \u201ceducated guess\" about what word should come next.19 The context window expands as the LLM guesses and generates more words. The words come from the corpus of data that the LLM has been \"pretrained\u201d20 on. Pretraining involves \u201cteaching an AI model how to understand and generate human-like text by exposing the model to \nbillions of webpages, books, contracts, legal opinions, and other text documents.\u201d21 Pretraining gives an AI system \u201cbillions of parameters\" ... \"appropriately adjusted to reliably predict the next words, given nearly any selection of prompting words. \"22\n14 LLMs use the additional step of \u201cdeep learning\u201d to scale a neural network to include \u201cbillions of neurons or parameters with many deep layers.\u201d23 Innovations in transformer architecture have improved the accuracy of LLMs by allowing them \u201cto look at the entire context of the user input, even words that were far away, and determine which contextual words were most helpful in figuring out the more accurate next word.\u201d24"}, {"title": "Legal Analysis Results", "content": "27 The beginning rules analysis problem involved the Americans with Disabilities Act55 and an unhoused person seeking to keep his animal with him in a homeless shelter.56 The facts include a description of the unhoused person's afflictions of anxiety and depression and an explanation of how the animal provides comfort to the unhoused person, including potentially preventing him from overdosing by removing pills from his hand.\n28 The problem set out the following rules to be applied to the factual situation:"}, {"title": "LLMs' Responses \u2013 Beginning Rule Analysis", "content": "30 Copilot outperformed other LLMs at analyzing the beginning rule analysis question. Copilot's answer contained significantly more relevant details than some of the other models. The answer correctly identified the key issue of the animal's lack of training as required by the ADA. Claude performed well but its responses were shorter and lacked the detail provided by Copilot's response.\n31 GPT and Gemini both hallucinated the facts provided and made the incorrect assumption that the animal's act of potentially preventing its owner from overdosing by removing pills from his hand could meet the ADA requirement of having special training or abilities as a service animal. Both hedged their bets on the final conclusion and qualified their answers that more investigation is needed to determine if the animal qualifies as a service animal under the ADA.\n32 Lexis+ AI did not respond in a meaningful way to the iterative prompts to answer thinking step by step or to respond in IRAC format. Lexis+ AI ignored or was not able to process the instructions for any of the six discrete legal analysis tasks. Lexis+ AI included citations to relevant ADA regulations found in the Code of Federal Regulations (CFR). Lexis+ Al failed to identify the issue that the animal does not qualify as a service animal because it has not received the training required under the ADA. Ultimately, Lexis+ AI's conclusion was to name the animal in question and then re-state the ADA language defining a service animal as \u201cany guide dog, signal dog, or other animal individually trained to provide assistance to an individual with a disability.\u201d Lexis+ AI ignored the premise in the facts that the dog had not received any special training. Lexis+ AI's reply included a hallucination of the facts provided by responding with information about miniature horses and non-human primates as service animals under the ADA."}, {"title": "Skilled Rule Analysis", "content": "33 The skilled rules analysis problem involves questions of criminal law. Law enforcement is seeking a 30-day search warrant for continuous and detailed location data on a suspect from the suspect's cellular phone service provider.59 The search warrant is sought to aid law enforcement in executing an arrest warrant on the suspect. No claim has been made that the location data will provide evidence of a crime. The problem sets out the following rules to be applied to the factual situation:\n\u2022\n\u2022\n\u2022\nUnder the Fourth Amendment, subjects of an arrest warrant have reasonable expectations of privacy in their movements and location.\nSearch warrants can be justified under the Fourth Amendment by showing the evidence to be obtained has some nexus with criminal activity.\nThe correct answer to this problem should evaluate whether an arrest warrant alone provides a legal basis to obtain continuous and detailed location data on a suspect for 30 days."}, {"title": "LLMs' Responses - Skilled Rule Analysis", "content": "34 Claude's answer was the most complete and thorough of the group, its conclusion was stated with certainty, and citations to relevant federal criminal law cases were included with brief comments explaining the relevance of the cases to the fact pattern.\n35 Lexis+ AI provided a solid answer that included links to relevant case law within Lexis. However, its ultimate answer lacked the certainty found in answers returned by other LLMs.\n36 Copilot's performance was the worst of all LLMs evaluated at skilled rule analysis. Copilot failed to clearly and distinctly identify the two rules stated in the problem. Copilot's initial answers failed to include the key phrase that a search warrant may not be justified if the evidence to be obtained does not have a nexus to a crime. Following up with the iterative prompt to answer the problem thinking step by step improved the clarity of Copilot's response and the requirement of location data being related to a crime appeared in the response.\n37 GPT and Gemini returned much shorter and less detailed responses than the other LLMs. Their answers lacked factual details and nuances found in the responses of the other LLMs. Gemini responded positively to the iterative prompt to think step by step by providing more details and citing some case law."}, {"title": "Beginning Analogical Reasoning", "content": "38 The beginning analogical reasoning problem presents a hypothetical tort of battery. 60 The problem includes two case citations to be used in analyzing the problem.61 The question presented is whether the facts constitute a prima facia case of battery. The relevant rules found in the cases include the definition of battery as \"the infliction of a harmful or offensive contact upon another with the intent to cause such contact or the apprehension that such contact is imminent\u201d62 and offensiveness is an essential element of the tort of battery. 63 The factual situation involves two friends playing golf. One attempts to give the other a high-five but instead hits the intended recipient in the ear. The recipient of the high-five suffered a ruptured ear drum requiring medical treatment. The correct answer to the problem will address whether a meritorious claim of battery can be brought based on the facts."}, {"title": "LLMs' Responses \u2013 Beginning Analogical Reasoning", "content": "39 Claude, GPT, and Copilot performed similarly, responding with brief but succinct answers. All three discussed the two provided cases and pulled out relevant rules from the cases. These rules were correctly applied to the facts and all three LLMs reached the correct conclusions that no tort of battery could be established because the intention to cause harm and the required element of offensiveness were both lacking.\n40 Gemini reached the correct conclusion, but it provided less detail and analysis than the other LLMs and did not explain the two cases with the level of detail provided by the other LLMs.\n41 Lexis+ Al provided only a single paragraph response and did not improve when asked to answer by reasoning step by step. Lexis+ AI cited only one of the two provided cases in its' response but mentioned the offensiveness element from Gatto v. Publix Supermarket, Inc., without citing the case.64"}, {"title": "Skilled Analogical Reasoning", "content": "142 The skilled analogical reasoning exercise involves a detailed factual scenario of a potential robbery from a store.65 The defendant (D) is alleged to have committed the robbery in the Commonwealth of Virginia by concealing 10 bottles of perfume in her purse and attempting to leave the store. A customer interrupted D while she was in the process of concealing the perfume by asking D what she was doing and asking to see a bottle of perfume. The store's security guard observed D placing the bottles into her bag from a location where D could not see him. D attempted to exit the store and the security guard ran to stop D. D slammed the guard to the ground and disposed of her bag containing the perfume while exiting the store. The problem's instructions include three Virginia cases addressing robbery to be applied to the facts."}, {"title": "LLMs' Responses - Skilled Analogical Reasoning", "content": "44 Copilot performed the best of all LLMs at the skilled analogical reasoning exercise. Copilot's initial response was to provide a single paragraph containing arguments for the Commonwealth and for D. Once iteratively prompted to reason step by step, Copilot expanded its answer to a much lengthier and in-depth response with detailed explanations of the elements described in the cited cases. Analysis was provided for both the Commonwealth and D. Ultimately, Copilot reached the correct conclusion that D would likely be convicted for robbery.\n45 Lexis+ AI cited the cases but also several Virginia statutes that it was not instructed to cite. The exercise is a \u201cclosed universe\" problem with specific instructions to only consult the cases cited. However, in real life outside of the bounds of the exercise any law student or lawyer would certainly not answer this question by only referring to three cases. Lexis+ AI provided a brief answer and did not fully develop arguments for the Commonwealth and the defendant as instructed.\n46 Claude's response was narrowly focused on arguments for the Commonwealth and the defendant as instructed. Claude correctly identified nuances in the facts and applied concepts discussed in the cases citing those facts. For example, Claude explained how the defendant's use of violence against the guard after taking the perfume was similar to the force used in Green v. Commonwealth.74 The defendant's act of ignoring the customer was analogized to the facts of Mason v. Commonwealth,75 where the court found that intimidation can be directed at someone other than the owner of the property to establish the element of"}, {"title": "Beginning Statutory Analysis", "content": "48 The beginning statutory analysis exercise requires the evaluation of federal and Indiana state constitutional provisions and case law holding that the U.S. Constitution's guarantee of a civil jury trial does not apply in state court trials.78 The hypothetical states that neither party requested a jury trial within the time period required under Indiana law. At the start of the bench trial in this matter the plaintiff makes an oral motion for a jury trial. The plaintiff admits that they forgot to request a jury. The plaintiff bases their request on the Seventh Amendment to the United States Constitution and Article 1, Section 20 of the Indiana Constitution. The exercise instructs the student that they represent the defendant who does not want a jury trial and to develop a response against the plaintiff's oral motion for a jury trial."}, {"title": "LLMs' Responses - Beginning Statutory Analysis", "content": "49 All LLMs were prompted to draft a legal argument in opposition to the plaintiff's oral request for a jury trial. Most of the LLMs replied with a written script that could be the basis of an appropriate oral argument presented to the court. However, GPT concluded its response with a salutation typically found in business letters and not in legal arguments \"thank you for your attention to this matter.\u201d79\n50 An interesting wrinkle in LLMs' responses to the problem was the identification of a relevant rule found in the civil Indiana Rules of Trial Procedure that was not mentioned in the exercise. Rule 38 contains the court rule setting out the procedural requirements to demand a jury trial in a civil case.80 The rule includes a deadline and states that failure to demand a jury trial as set out in this rule constitutes a waiver of trial by jury.81 The exercise mentions Hayworth v. Bromwell, a 1959 Indiana Supreme Court case holding that a state court rule requiring a request for jury trial to be made at a specific time does not violate the right to a jury trial.82 Rule 38 is clearly an efficient and effective way for the defendant to prevail at the motion hearing on the plaintiff's request for a jury trial."}, {"title": "Intermediate Statutory Analysis", "content": "52 The intermediate statutory analysis exercise involves the application of a Good Samaritan statute to a physician who rendered emergency care to a visitor injured while visiting a relative who was a patient at a hospital.85 The visitor was not at the hospital for the purposes of receiving medical care and was injured by accidentally hitting his head on hospital equipment. The physician discovered the injured visitor and began administering medical assistance. The physician accidentally injured the visitor's neck while moving him. The physician's specialty is oncology, not emergency medicine. The visitor's injuries required months of therapy and he brought suit against the physician for negligence. The physician claimed immunity from suit under the state's Good Samaritan statute. This scenario presents a case of first impression in Florida.\n53 The exercise includes two cases and a Florida statute to be applied to the facts.86 The Florida Good Samaritan statute extends immunity from civil damages to healthcare practitioners who voluntarily provide care to someone requiring immediate medical care as long as there is no existing relationship between the practitioner and the recipient of the care.87 Care found to be willful and wanton conduct likely to result in injury is excluded from immunity under the statute.88\n154 Burciaga v. St. John's Hosp. is a California case holding that Good Samaritan laws apply to protect physicians who have no prior relationship with individuals they provide emergency care to inside hospitals.89 Burciaga applied Good Samaritan immunity to physicians who rendered care in their specialty regardless of location inside the medical facility.90"}, {"title": "LLMs' Responses - Intermediate Statutory Analysis", "content": "57 All LLMs struggled with this exercise. Factors causing the models difficulty included the length and complexity of the facts, a statute that included several conditional elements and was incorrectly cited, and cases from jurisdictions other than Florida. All models based their answers on the correct version of the Florida statute despite the outdated citation provided to them in the prompt. Lexis+ AI included the correct citation to the statute in its answers. All other models cited the statute incorrectly using the old citation provided in the exercise and used in the prompt.\n58 Claude marginally outperformed the other models. It provided a detailed analysis of arguments for and against application of Good Samaritan immunity. The analysis included all the relevant details. Claude waffled and refused to commit to a final answer, but this is understandable given this matter of first impression involving precedents from other jurisdictions.\n59 Initially Lexis+ AI and Gemini were unable to analyze the exercise in a meaningful way until receiving the iterative prompt to solve the exercise thinking step by step.94 After receiving the iterative command to address the exercises by reasoning step by step, the responses of both models improved somewhat. Lexis+ AI hallucinated an assumption not supported by the facts, speculating that it was not clear whether the visitor objected to the physician's assistance. Nothing in the facts indicate that the visitor objected to receiving care from the physician. Additionally, Lexis+ AI's response included citations to the two cases, but they were not applied to the facts or included in the substance of its answer."}, {"title": "Skilled Statutory Analysis", "content": "61 The skilled statutory analysis exercise involves a statute of limitations question and potential tolling of the statute based on the discovery of an injury that was unknown during the two-year statutory period.95 Additionally, the plaintiff (P) may have initially filed suit against the wrong party. The facts involve P who trades in exotic pets. P purchased an exotic pet. The pet's medical records indicate it had received a vaccination against a potentially fatal virus. The pet subsequently became ill and died from the virus despite having been vaccinated against it. P brought suit against the veterinarian's office listed on the pet's vaccine records. The veterinary office argues that it does not vaccinate pets and that it merely noted on the pet's record that a third party provided the vaccine. The lawsuit against the veterinary office is dismissed. P is seeking advice about suing the third party for not administering the vaccine. The applicable statute of limitations expired at least two and a half years ago."}, {"title": "LLMs' Responses - Skilled Statutory Analysis", "content": "63 The answers provided to the skilled statutory exercise demonstrate the outer limits of the LLMs' abilities to perform legal analysis. All LLMs struggled significantly with this exercise. Lexis+ AI's initial response included the hallucination of analyzing the prompt under the National Childhood Vaccine Injury Act with a citation included to the federal statute of limitations for suing the federal government.100 Following the prompt to think step by step, Lexis+ Al's response included the cases cited in the exercise plus additional Ohio cases. Lexis+ AI mentioned a case supporting the application of the discovery rule to the animal's missed vaccination but ignored the issue of tolling the statute when the incorrect party is sued and concluded with a vague answer."}, {"title": "Comparisons with Previous Studies of LLMs' Legal Reasoning and Analysis Abilities", "content": "167 Several previous studies have evaluated the abilities of legal and non-legal LLMs to perform legal reasoning and analysis.101 The results of this study specific to ChatGPT's and other LLMs' legal reasoning and analysis abilities confirms the findings of a previous study examining ChatGPT's abilities to answer law school examination questions, \u201cChatGPT Goes to Law School.", "scored at or near the bottom of each class. \\\"103\n68 Some findings described in \u201cChatGPT Goes to Law School": "ere present in this study. ChatGPT struggled to focus and often veered off topic. In one example from a previous study, \"ChatGPT failed to discuss the primary issue\u201d104 and instead focused on causes of action not relevant to the facts. 105 Similarly, this study found that Lexis+ AI exhibited similar behavior when answering a skilled statutory exercise involving vaccinations administered to a pet with a hallucination analyzing the prompt under the National Childhood Vaccine Injury Act and included a citation on the statute of limitations for suing the federal government. 106"}, {"title": "General Comparisons", "content": "69 Another common issue identified in the \u201cChatGPT Goes to Law School\u201d study was ChatGPT providing very brief answers, not going into sufficient detail when applying rules to facts, or not explaining how a cited case was relevant.107 In this study, ChatGPT and Gemini often replied to prompts with very brief answers in bullet point style, while their answers lacked citations to sources, omitted nuances, and failed to address arguments for both sides of the issues. 108 As a result, ChatGPT and Gemini tied for the second lowest overall scores for applying rules to facts. 109\n170 Finally, \"ChatGPT Goes to Law School\u201d demonstrated how ChatGPT responded with answers that were \u201cexcessively cagey\u201d or \u201crefused to make an argument about the most plausible interpretation of the relevant facts when those facts potentially pointed in competing directions."}, {"title": "Prompting to Think Step by Step", "content": "71 A study testing ChatGPT-3's abilities to perform statutory reasoning concluded that prompting it to \"think step by step was found, out of a number of prompts to maximize performance.\u201d112 This finding is opposite to the results of this study where the prompt \u201cthink step by step\" only improved ChatGPTs' performance on one out of the three statutory reasoning exercises.113 When looking at ChatGPTs' performance on all seven reasoning exercises, the prompt to think step by step resulted in only a comparatively small increase in performance compared with the other LLMs.\n72 Only Lexis+ AI performed worse than ChatGPT when provided with the prompt to think step by step. These findings are similar to a 2023 study demonstrating that the performance of seven out of eight LLMs in solving tax law problems was not improved by prompting them to think step by step.114 However, the 2023 study found GPT-4's performance improved when asked to think step by step.115\n73 Claude, Copilot, and Gemini all outperformed ChatGPT when instructed to think step by step. The prompt to think step by step particularly improved these LLMs' ability to analyze the more complex \"skilled\" exercises. Relevant examples are discussed below. The full text of each LLMs' responses pre- and post-prompting to think step by step are available in Appendices 3, 4, and 5.\n74 Claude's initial response to the intermediate statutory exercise does a decent job of analyzing the Florida Good Samaritan statute, but only superficially discusses the two cases provided in the prompt without any detailed analysis on how they might be used to make arguments for or against the application of the statute in a case of first impression.116 Claude's response to the iterative prompt to think step by step includes important details not found in its initial response. Specifically, the relevant holdings of the two cases cited in the prompt were explained and used in the context of arguments in support of finding good Samaritan immunity as instructed in the prompt. .117 Claude's response pointed out that the cases provided are from jurisdictions other than Florida where the exercise is set. A novice law student or legal researcher might appreciate this detail when considering what precedential value a Florida court might attribute to these cases.\n75 Copilot's initial response to the skilled analogical reasoning exercise provided the text of possible arguments for the Commonwealth, the defendant, and a conclusion predicting the outcome of the case. .118 Copilot's response to the initial prompt only mentioned one of the three cases cited in the prompt. Copilot's response to the iterative prompt to think step by step greatly improved its answer. The response discussed \"robbery elements and arguments in Defendant's case.\u201d119 The response discussed important takeaways from each of the three cases cited in the initial prompt, a summary of arguments for the Commonwealth and the defendant, and ended with the correct conclusion stated with certainty. 120"}, {"title": "Legal vs. Non-Legal LLMs", "content": "76 The study \u201cLawBench: Benchmarking Legal Knowledge of Large Language Models\" compared the abilities of legal and non-legal LLMs to memorize legal knowledge, understand legal concepts, and apply legal knowledge.121 LawBench found that \u201clegal specific LLMs do not necessarily outperform general large language models.\u201d122 This is similar to the findings of this study where non-legal LLMs routinely outperformed Lexis+ AI at legal analysis and reasoning. In fairness, Lexis+ Al's corpus is much more limited than the other LLMs evaluated. Lexis+ AI's comparatively limited corpus may account for its performance relative to the other LLMs evaluated."}, {"title": "Hallucinations", "content": "78 This study identified two types of hallucinations. The first is a hallucination of the law or facts provided that might \"produce a response that is unfaithful to or in conflict with the input prompt. .\u201d123 The second type of hallucination is a response that is not consistent \u201cwith the facts of the world\u201d including hallucinated statements of law. 124\n79 Claude and Copilot both received perfect scores in the hallucination category for responding with zero hallucinations to all seven factual scenarios. ChatGPT and Gemini each hallucinated once when responding to the beginning rule exercise. This constitutes a hallucination rate of 14 percent. They both hallucinated that the animal's act of removing pills from the unhoused person's hand met the ADA requirements for having special training or abilities as a service animal.\n80 Lexis+ AI performed the worst at hallucinating out of all LLMs evaluated, hallucinating when answering four out of the seven exercises constituting a hallucination rate of 57 percent. Lexis+ AI hallucinated the facts and law provided in the beginning rule exercises by stating that the dog received specialized training and by providing legal information about miniature horses and non-human primates as services animals under the ADA. Lexis+ AI hallucinated on the law provided in the beginning statutory exercise by citing a rule of criminal procedure despite the exercise's focus exclusively on a civil case. Lexis+ AI hallucinated the facts provided in the intermediate statute exercise when responding that it was unclear if the visitor objected to the physician's assistance. Nothing in the facts indicated any objections made by the visitor. Finally, Lexis+ AI hallucinated the law provided in the skilled statutory exercise by analyzing the prompt under the National Childhood Vaccine Injury Act when the exercise specifies the vaccine in question was to be administered to an animal and not a human.\n81 This study's findings that Lexis+ AI hallucinated more than any other LLM evaluated differs from a recently published study comparing hallucination rates between Westlaw AI Assisted Research, Westlaw Ask Practical Law AI, Lexis+ AI, and GPT-4.125 The study, \"Hallucination Free? Assessing the Reliability of Leading AI Legal Research Tools,\" looked at hallucinations that are unfaithful to the facts of the world.126 The study found that LLMs hallucinated between 17 percent and 33 percent of the time.127 Lexis+ AI performed the best of the four LLMs tested \u201canswering 65 percent of queries accurately"}, {"title": "False Confidence \u2013 When You Know, You Know", "content": "182 All the LLM's tested demonstrated some degree of false confidence in responses by stating a conclusion with certainty despite the conclusion being objectively wrong. 129 This false confidence is the other side of the coin of the cagey, overqualified responses discussed above.\n83 This phenomenon is not a recent development in electronic legal research. Unjustified overconfidence in research conducted electronically dates to the CD-ROM days of the mid-1990s.130 Studies \u201chave clearly documented a false sense of security on the part of computer researchers.\u201d131 In one previous study, students who incorrectly answered questions using electronic research tools \"refused to give [the electronic tools] a lower effectiveness rating\"132 than the rating given to print research tools. A study of Westlaw's early AI product, Westlaw Next, demonstrated that the majority of researchers using the product and expressing a high degree of confidence in their answers \u201cdid not come anywhere close to identifying the correct answer to the question.\u201d133\n184 Recent research reveals troubling trends with LLMs that are likely to amplify researchers' sense of false confidence in incorrect results. The studies find that LLMs demonstrate \"certainty in their responses, i.e., their self-awareness of their propensity to hallucinate\u201d134 and that \"LLMs often provide seemingly legitimate but incorrect answers to contra-factual legal questions\"135 and \u201cstruggle to accurately gauge their own level of certainty without post- hoc recalibration.\u201d136 Some research suggests that when LLMs engage in this behavior they \u201cmight be mimicking human expressions when verbalizing confidence.\u201d137 In other words, they learned it by watching you! 138\n85 Copilot, GPT, and Geminis' responses to the skilled statutory exercise all demonstrate false confidence in their incorrect responses to the prompt. The skilled statutory exercise tests LLMs' abilities to analyze a statute of limitations question with a possible tolling of the statute based on a previously unknown but relevant factor. Copilot's response to the prompt was brief and certain stating that \"since four years and six months have passed since the missed vaccination, it appears that the two-year statute of limitations would bar the\""}, {"title": "Comparisons with Previous Studies of LLMs' Legal Reasoning and Analysis Abilities", "content": "86 GPT's response to the skilled statutory exercise contains a contra-factual error. In its response, GPT states that the statute of limitations began running \u201cwhen the injury is discovered or should have been discovered.\u201d142 GPT's response indicates the statute started to run when the animal's illness was discovered.143 GPT then veers off course stating that \u201cFrom the information provided, it appears that more than two years have elapsed since that time.\u201d144 This response is contrary to the prompt stating that the statute began to run when the illness was discovered and that the animal died two weeks after the illness was discovered.\n187 Gemini's response was brief. It refused to consider the cases cited in the initial prompt even after receiving an iterative prompt asking it to apply the cases to the facts. Gemini only analyzed the prompt using the two-year statute of limitations discussed in the initial prompt. Gemini concluded, \"Based on the scenario and the two-year statute of limitations in Ohio, it is likely that the statute of limitations will bar Pet Owner's lawsuit.\u201d145 Gemini responded that the three cases cited in the prompt \u201care not directly applicable as they don't address the specific situation of exceeding the statute of limitations.\u201d146 This response is erroneous as all three cases specifically address the tolling of statute of limitations because of late discovery of an alerting event. .147 In response to the iterative prompt to apply the cases to the facts, Gemini responded but did not mention the cases."}, {"title": "Can LLMs Think Like a Lawyer?", "content": "188 The results of this study, as explained above, clearly demonstrate that LLMs can \"think like a lawyer\" in a limited sense by analyzing a factual scenario using the IRAC framework. Table 2 shows the performance of each LLM at the discrete IRAC tasks measured in this study. The results mirror the overall performance of each LLM tested on all tasks. Claude narrowly outperformed other LLMs with Copilot, GPT, Gemini, and Lexis+ AI finishing closely behind. If the LLMs were graded on their performance at IRAC analysis their scores would range from Lexis+ AI's 73 percent on the low end to Claude's 90 percent at the high end."}, {"title": "Challenges - Stability and Transparency", "content": "91 The instability of answers created by LLMs complicates their usefulness for legal work and ability to think like a lawyer. Researchers who repeatedly input identical prompts to generative Al will never receive the same responses. One study exploring ChatGPT's rate of nondeterminism (being unpredictable or returning different results when presented with identical queries) concluded that ChatGPT \u201cis very unstable\u201d and responded with \u201chigh degrees of non-determination.\u201d152 The authors posit that such a high degree of non- determinism is \u201ca potential menace to scientific conclusion validity\" because it makes scientific conclusions unrepeatable and therefore unreliable. 153\n92 Lawyers who use LLMs for legal research should be concerned about nondeterministic results. Paul Callister eloquently summarized how this inconsistency is problematic for lawyers and legal researchers.\n[O"}]}