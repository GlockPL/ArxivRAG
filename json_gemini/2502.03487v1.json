{"title": "Artificial Intelligence and Legal Analysis: Implications for Legal Education and the Profession", "authors": ["Lee F. Peoples"], "abstract": "This article reports the results of a study examining the ability of legal and non-legal Large Language Models (LLMs) to perform legal analysis using the Issue-Rule-Application-Conclusion (IRAC) framework. LLMs were tested on legal reasoning tasks involving rule analysis and analogical reasoning. The results show that LLMs can conduct basic IRAC analysis, but are limited by brief responses lacking detail, an inability to commit to answers, false confidence, and hallucinations. The study compares legal and non-legal LLMs, identifies shortcomings, and explores traits that may hinder their ability to \u201cthink like a lawyer.\" It also discusses the implications for legal education and practice, highlighting the need for critical thinking skills in future lawyers and the potential pitfalls of over-reliance on artificial intelligence (AI) resulting in a loss of logic, reasoning, and critical thinking skills.", "sections": [{"title": "Introduction", "content": "Law and society at large are in the midst of a fourth industrial revolution brought about by artificial intelligence and related technologies. These developments could potentially usher in a \"golden age for the [legal] profession and society\u201d but also have the potential to create \u201ceconomic inefficiency, social disfunction, and a declining legal system.\u201d Goldman Sachs predicted that AI could be more disruptive in law than other industries and could potentially replace 44 percent of legal jobs.\nPrevious studies have concluded that law students using generative AI tools could \"substantially improve the efficiency with which they complete a broad array of legal tasks without adversely affecting (or even slightly improving) the quality of that work product.\" Lawyers will also benefit from AI, and it will be a vital tool for law practice both in the future and in the long term.\nHowever, the gains to be realized from using AI in legal education and the legal profession are not without potential perils. Al research tools have notoriously hallucinated facts and laws, fabricated case law that does not exist, falsely attributed a Supreme Court dissenting opinion to a justice who actually joined the majority, falsely accused a law professor of sexual assault, and even temporarily going insane. Lawyers have misused AI to create fictional cases cited in court filings and to estimate attorney's fees, leading to criticism, sanctions from judges, and disciplinary actions.\nThis study examines the abilities of legal and non-legal LLMs to think like a lawyer by performing legal analysis and reasoning on legal factual scenarios using the Issue-Rule-Application-Conclusion (IRAC) framework. LLMs tested include Lexis+ AI, Claude,"}, {"title": "How LLMS Work", "content": "Large language models are neural networks with billions of parameters trained on vast datasets of text. At their core, they represent words as high-dimensional numerical vectors. These word vectors are fed into layers of artificial neurons that perform mathematical operations to capture linguistic patterns and contexts. This process is an \u201calgorithmic approach to machine learning known as a \u2018neural network\u201d The neural network is an \u201cextremely flexible pattern detector.\u201d LLMs using neural networks have been described as \u201cadvanced Al word-prediction systems.\"\nWhen a user enters a prompt into an LLM, it is processed within the LLM's \"context window,\" which functions as its \u201cshort-term memory,\u201d allowing it to make an \u201ceducated guess\" about what word should come next. The context window expands as the LLM guesses and generates more words. The words come from the corpus of data that the LLM has been \"pretrained\u201d on. Pretraining involves \u201cteaching an AI model how to understand and generate human-like text by exposing the model to billions of webpages, books, contracts, legal opinions, and other text documents.\u201d Pretraining gives an AI system \u201cbillions of parameters"}, {"title": "Using IRAC to Test Legal Analysis Abilities of LLMs", "content": "This study examines the abilities of legal and non-legal LLMs to think like a lawyer by performing legal analysis and reasoning. A common framework taught in American law schools is IRAC. The IRAC framework breaks legal analysis down into four discrete steps. The IRAC framework can be used to analyze legal rules, statutes, and to reason analogically using caselaw. A helpful explanation is found in the Legal Bench study.\nFirst, lawyers identify the legal issue in a given set of facts (issue-spotting). An issue is often either (1) a specific unanswered legal question posed by the facts, or (2) an area of law implicated in the facts. Depending on the setting, a lawyer may be told the issue or be required to infer a possible issue.\nSecond, lawyers identify the relevant legal rules for this issue (rule-recall). A rule is a statement of law that dictates the conditions that are necessary (or sufficient) for some legal outcome to be achieved. In the United States, rules can come from a variety of sources: the Constitution, federal and state statutes, regulations, and court opinions (case law). Importantly, rules often differ between jurisdictions. Hence, the relevant rule in California might be different than the relevant rule in New York.\nThird, lawyers apply these rules to the facts at hand (rule-application). Application, or the analysis of rule applicability, consists of identifying those facts which are most relevant to the rule, and determining how those facts influence the outcome under the rule. Application can also involve referencing prior cases involving similar rules (i.e., precedent) and using the similarities or differences to those cases to determine the outcome of the current dispute, once pretraining is complete. Finally, lawyers reach a conclusion with regards to their application of law to determine what the legal outcome of those facts are (rule-conclusion).\nTesting Methodology\nFactual scenarios were adapted from a text utilized in introductory law school courses on legal research, writing, and legal analysis. Factual scenarios were anonymized by removing specific names to keep from exposing content-specific identifiers to LLMs' corpora. Additionally, LLMs were instructed to not train on the prompts.\nLLMs were tested on a total of seven scenarios. Scenarios requiring beginning and skilled IRAC abilities were selected for testing LLMs abilities to analyze legal rules and perform analogical reasoning. Scenarios requiring beginning, intermediate, and skilled IRAC abilities were selected for testing LLMs' abilities to analyze statutes. LLMs' statutory reasoning abilities were explored in more detail because previous studies have demonstrated"}, {"title": "Legal Analysis Results", "content": "Beginning Rule Analysis\nThe beginning rules analysis problem involved the Americans with Disabilities Act and an unhoused person seeking to keep his animal with him in a homeless shelter. The facts include a description of the unhoused person's afflictions of anxiety and depression and an explanation of how the animal provides comfort to the unhoused person, including potentially preventing him from overdosing by removing pills from his hand.\nThe problem set out the following rules to be applied to the factual situation:\nThe ADA requires public entities to make reasonable modifications to operations or policies to avoid discrimination based on disabilities. Upon a showing that a modification would \u201cfundamentally alter the nature of the services, program, or activity,\" a public entity may not be required to make the requested modification. Animals providing comfort to a disabled person are not automatically classified as service animals. The ADA defines service animals as an animal \u201cindividually trained to do work or perform similar tasks for the benefit of an individual with a disability.\u201d The problem does not state that the animal in question has had any training or special abilities as required under the ADA. Service animals are required to possess abilities beyond those of dogs in general or typical to their breed.\nThe correct answer to this problem should conclude that the ADA would not recognize the animal as a \u201cservice animal\u201d because it lacks the specific training required for classification as a service animal. Ultimately, the unhoused individual will not be able to require the shelter to accommodate his animal according to the ADA.\nLLMs' Responses \u2013 Beginning Rule Analysis\nCopilot outperformed other LLMs at analyzing the beginning rule analysis question. Copilot's answer contained significantly more relevant details than some of the other models. The answer correctly identified the key issue of the animal's lack of training as required by the ADA. Claude performed well but its responses were shorter and lacked the detail provided by Copilot's response.\nGPT and Gemini both hallucinated the facts provided and made the incorrect assumption that the animal's act of potentially preventing its owner from overdosing by removing pills from his hand could meet the ADA requirement of having special training or abilities as a service animal. Both hedged their bets on the final conclusion and qualified their answers that more investigation is needed to determine if the animal qualifies as a service animal under the ADA.\nLexis+ AI did not respond in a meaningful way to the iterative prompts to answer thinking step by step or to respond in IRAC format. Lexis+ AI ignored or was not able to process the instructions for any of the six discrete legal analysis tasks. Lexis+ AI included citations to relevant ADA regulations found in the Code of Federal Regulations (CFR). Lexis+ Al failed to identify the issue that the animal does not qualify as a service animal because it has not received the training required under the ADA. Ultimately, Lexis+ AI's conclusion was to name the animal in question and then re-state the ADA language defining a service animal as \u201cany guide dog, signal dog, or other animal individually trained to provide assistance to an individual with a disability.\u201d Lexis+ AI ignored the premise in the facts that the dog had not received any special training. Lexis+ AI's reply included a hallucination of the facts provided by responding with information about miniature horses and non-human primates as service animals under the ADA."}, {"title": "Skilled Rule Analysis", "content": "The skilled rules analysis problem involves questions of criminal law. Law enforcement is seeking a 30-day search warrant for continuous and detailed location data on a suspect from the suspect's cellular phone service provider. The search warrant is sought to aid law enforcement in executing an arrest warrant on the suspect. No claim has been made that the location data will provide evidence of a crime. The problem sets out the following rules to be applied to the factual situation:\n\u2022\tUnder the Fourth Amendment, subjects of an arrest warrant have reasonable expectations of privacy in their movements and location.\n\u2022\tSearch warrants can be justified under the Fourth Amendment by showing the evidence to be obtained has some nexus with criminal activity.\n\u2022\tThe correct answer to this problem should evaluate whether an arrest warrant alone provides a legal basis to obtain continuous and detailed location data on a suspect for 30 days.\nLLMs' Responses - Skilled Rule Analysis\nClaude's answer was the most complete and thorough of the group, its conclusion was stated with certainty, and citations to relevant federal criminal law cases were included with brief comments explaining the relevance of the cases to the fact pattern.\nLexis+ AI provided a solid answer that included links to relevant case law within Lexis. However, its ultimate answer lacked the certainty found in answers returned by other LLMs.\nCopilot's performance was the worst of all LLMs evaluated at skilled rule analysis. Copilot failed to clearly and distinctly identify the two rules stated in the problem. Copilot's initial answers failed to include the key phrase that a search warrant may not be justified if the evidence to be obtained does not have a nexus to a crime. Following up with the iterative prompt to answer the problem thinking step by step improved the clarity of Copilot's response and the requirement of location data being related to a crime appeared in the response.\nGPT and Gemini returned much shorter and less detailed responses than the other LLMs. Their answers lacked factual details and nuances found in the responses of the other LLMs. Gemini responded positively to the iterative prompt to think step by step by providing more details and citing some case law.\nBeginning Analogical Reasoning\nThe beginning analogical reasoning problem presents a hypothetical tort of battery. The problem includes two case citations to be used in analyzing the problem. The question presented is whether the facts constitute a prima facia case of battery. The relevant rules"}, {"title": "Skilled Analogical Reasoning", "content": "The skilled analogical reasoning exercise involves a detailed factual scenario of a potential robbery from a store. The defendant (D) is alleged to have committed the robbery in the Commonwealth of Virginia by concealing 10 bottles of perfume in her purse and attempting to leave the store. A customer interrupted D while she was in the process of concealing the perfume by asking D what she was doing and asking to see a bottle of perfume. The store's security guard observed D placing the bottles into her bag from a location where D could not see him. D attempted to exit the store and the security guard ran to stop D. D slammed the guard to the ground and disposed of her bag containing the perfume while exiting the store. The problem's instructions include three Virginia cases addressing robbery to be applied to the facts. The cases set out the following important elements and requirements for the crime of robbery in Virginia:\nProperty must be taken by violence to the possessor or by putting the possessor in fear of immediate injury to his person. The violence or intimidation used in a robbery \"need not precede but must be concomitant with the taking.\u201d\n\"The predicate element of robbery is the actual taking by caption and asportation of the personal property of the victim.\u201d Property of the victim has been construed broadly to include property in the victim's custody when the victim has a superior right of possession to that of the defendant.\nAbsolute control is required to establish taking. Only slight asportation is required and is assessed on a case-by-case basis.\nThe correct answer to this problem should apply the elements of the crime of robbery as articulated in the cases cited above to the facts and should provide arguments for the Commonwealth of Virginia and the defendant. Ultimately, a court would likely conclude that the D's actions constituted the crime of robbery.\nLLMs' Responses - Skilled Analogical Reasoning\nCopilot performed the best of all LLMs at the skilled analogical reasoning exercise. Copilot's initial response was to provide a single paragraph containing arguments for the Commonwealth and for D. Once iteratively prompted to reason step by step, Copilot expanded its answer to a much lengthier and in-depth response with detailed explanations of the elements described in the cited cases. Analysis was provided for both the Commonwealth and D. Ultimately, Copilot reached the correct conclusion that D would likely be convicted for robbery.\nLexis+ AI cited the cases but also several Virginia statutes that it was not instructed to cite. The exercise is a \u201cclosed universe\" problem with specific instructions to only consult the cases cited. However, in real life outside of the bounds of the exercise any law student or lawyer would certainly not answer this question by only referring to three cases. Lexis+ AI provided a brief answer and did not fully develop arguments for the Commonwealth and the defendant as instructed.\nClaude's response was narrowly focused on arguments for the Commonwealth and the defendant as instructed. Claude correctly identified nuances in the facts and applied concepts discussed in the cases citing those facts. For example, Claude explained how the defendant's use of violence against the guard after taking the perfume was similar to the force used in Green v. Commonwealth. The defendant's act of ignoring the customer was analogized to the facts of Mason v. Commonwealth, where the court found that intimidation can be directed at someone other than the owner of the property to establish the element of"}, {"title": "Beginning Statutory Analysis", "content": "The beginning statutory analysis exercise requires the evaluation of federal and Indiana state constitutional provisions and case law holding that the U.S. Constitution's guarantee of a civil jury trial does not apply in state court trials. The hypothetical states that neither party requested a jury trial within the time period required under Indiana law. At the start of the bench trial in this matter the plaintiff makes an oral motion for a jury trial. The plaintiff admits that they forgot to request a jury. The plaintiff bases their request on the Seventh Amendment to the United States Constitution and Article 1, Section 20 of the Indiana Constitution. The exercise instructs the student that they represent the defendant who does not want a jury trial and to develop a response against the plaintiff's oral motion for a jury trial.\nLLMs' Responses - Beginning Statutory Analysis\nAll LLMs were prompted to draft a legal argument in opposition to the plaintiff's oral request for a jury trial. Most of the LLMs replied with a written script that could be the basis of an appropriate oral argument presented to the court. However, GPT concluded its response with a salutation typically found in business letters and not in legal arguments \"thank you for your attention to this matter.\u201d\nAn interesting wrinkle in LLMs' responses to the problem was the identification of a relevant rule found in the civil Indiana Rules of Trial Procedure that was not mentioned in the exercise. Rule 38 contains the court rule setting out the procedural requirements to demand a jury trial in a civil case. The rule includes a deadline and states that failure to demand a jury trial as set out in this rule constitutes a waiver of trial by jury. The exercise mentions Hayworth v. Bromwell, a 1959 Indiana Supreme Court case holding that a state court rule requiring a request for jury trial to be made at a specific time does not violate the right to a jury trial. Rule 38 is clearly an efficient and effective way for the defendant to prevail at the motion hearing on the plaintiff's request for a jury trial."}, {"title": "Intermediate Statutory Analysis", "content": "The intermediate statutory analysis exercise involves the application of a Good Samaritan statute to a physician who rendered emergency care to a visitor injured while visiting a relative who was a patient at a hospital. The visitor was not at the hospital for the purposes of receiving medical care and was injured by accidentally hitting his head on hospital equipment. The physician discovered the injured visitor and began administering medical assistance. The physician accidentally injured the visitor's neck while moving him. The physician's specialty is oncology, not emergency medicine. The visitor's injuries required months of therapy and he brought suit against the physician for negligence. The physician claimed immunity from suit under the state's Good Samaritan statute. This scenario presents a case of first impression in Florida.\nThe exercise includes two cases and a Florida statute to be applied to the facts. The Florida Good Samaritan statute extends immunity from civil damages to healthcare practitioners who voluntarily provide care to someone requiring immediate medical care as long as there is no existing relationship between the practitioner and the recipient of the care. Care found to be willful and wanton conduct likely to result in injury is excluded from immunity under the statute.\nBurciaga v. St. John's Hosp. is a California case holding that Good Samaritan laws apply to protect physicians who have no prior relationship with individuals they provide emergency care to inside hospitals. Burciaga applied Good Samaritan immunity to physicians who rendered care in their specialty regardless of location inside the medical facility.\nIn contrast, the New Jersey Supreme Court refused to extend Good Samaritan immunity in Velazquez v. Jiminez to an obstetrician with no prior relationship to a patient in labor inside a hospital. Important factors for the New Jersey court included the patient's location inside a hospital for the purposes of receiving care.\nThe correct answer should conclude that the Florida Good Samaritan statute shields the physician from liability for treating the visitor because no pre-existing relationship existed between the two. The location in which care is provided is not relevant to the Florida statute when there is no pre-existing relationship between caregiver and injured party. Because the exercise indicates this is a case of first impression in Florida, the citation of out-of-state cases may be persuasive. Velazquez would most likely not apply to limit the Good Samaritan statute's application because the visitor in the exercise was not a hospital patient, unlike the facts in Velazquez.\nLLMs' Responses - Intermediate Statutory Analysis\nAll LLMs struggled with this exercise. Factors causing the models difficulty included the length and complexity of the facts, a statute that included several conditional elements and was incorrectly cited, and cases from jurisdictions other than Florida. All models based their answers on the correct version of the Florida statute despite the outdated citation provided to them in the prompt. Lexis+ AI included the correct citation to the statute in its answers. All other models cited the statute incorrectly using the old citation provided in the exercise and used in the prompt.\nClaude marginally outperformed the other models. It provided a detailed analysis of arguments for and against application of Good Samaritan immunity. The analysis included all the relevant details. Claude waffled and refused to commit to a final answer, but this is understandable given this matter of first impression involving precedents from other jurisdictions.\nInitially Lexis+ AI and Gemini were unable to analyze the exercise in a meaningful way until receiving the iterative prompt to solve the exercise thinking step by step. After receiving the iterative command to address the exercises by reasoning step by step, the responses of both models improved somewhat. Lexis+ AI hallucinated an assumption not supported by the facts, speculating that it was not clear whether the visitor objected to the physician's assistance. Nothing in the facts indicate that the visitor objected to receiving care from the physician. Additionally, Lexis+ AI's response included citations to the two cases, but they were not applied to the facts or included in the substance of its answer."}, {"title": "Skilled Statutory Analysis", "content": "The skilled statutory analysis exercise involves a statute of limitations question and potential tolling of the statute based on the discovery of an injury that was unknown during the two-year statutory period. Additionally, the plaintiff (P) may have initially filed suit against the wrong party. The facts involve P who trades in exotic pets. P purchased an exotic pet. The pet's medical records indicate it had received a vaccination against a potentially fatal virus. The pet subsequently became ill and died from the virus despite having been vaccinated against it. P brought suit against the veterinarian's office listed on the pet's vaccine records. The veterinary office argues that it does not vaccinate pets and that it merely noted on the pet's record that a third party provided the vaccine. The lawsuit against the veterinary office is dismissed. P is seeking advice about suing the third party for not administering the vaccine. The applicable statute of limitations expired at least two and a half years ago.\nThe exercise situates the case in Ohio and provides the citation to the relevant statute of limitations and three cases. The statute limits recovery for injury to pets at two years from the cause of the injury. Case law cited in the exercise provides an exception to the two-year limitation period when the injury is discovered after the time period has passed if P could not have known the facts necessary to make a claim within the two-year time period. Additional case law refused to apply the discovery rule when the proper party to sue is not identified until after the two-year statute of limitations has expired. However, courts may refuse to apply this precedent in the future because its reasoning was based on a case that did not involve a party identified only after the statute of limitations had expired.\nLLMs' Responses - Skilled Statutory Analysis\nThe answers provided to the skilled statutory exercise demonstrate the outer limits of the LLMs' abilities to perform legal analysis. All LLMs struggled significantly with this exercise. Lexis+ AI's initial response included the hallucination of analyzing the prompt under the National Childhood Vaccine Injury Act with a citation included to the federal statute of limitations for suing the federal government. Following the prompt to think step by step, Lexis+ Al's response included the cases cited in the exercise plus additional Ohio cases. Lexis+ AI mentioned a case supporting the application of the discovery rule to the animal's missed vaccination but ignored the issue of tolling the statute when the incorrect party is sued and concluded with a vague answer."}, {"title": "Comparisons with Previous Studies of LLMs' Legal Reasoning and Analysis Abilities", "content": "General Comparisons\nSeveral previous studies have evaluated the abilities of legal and non-legal LLMs to perform legal reasoning and analysis. The results of this study specific to ChatGPT's and other LLMs' legal reasoning and analysis abilities confirms the findings of a previous study examining ChatGPT's abilities to answer law school examination questions, \u201cChatGPT Goes to Law School.\u201d ChatGPT passed all the exams but \u201cscored at or near the bottom of each class.\u201d\nSome findings described in \u201cChatGPT Goes to Law School\u201d were present in this study. ChatGPT struggled to focus and often veered off topic. In one example from a previous study, \"ChatGPT failed to discuss the primary issue\u201d and instead focused on causes of action not relevant to the facts. Similarly, this study found that Lexis+ AI exhibited similar behavior when answering a skilled statutory exercise involving vaccinations administered to a pet with a hallucination analyzing the prompt under the National Childhood Vaccine Injury Act and included a citation on the statute of limitations for suing the federal government.\nAnother common issue identified in the \u201cChatGPT Goes to Law School\u201d study was ChatGPT providing very brief answers, not going into sufficient detail when applying rules to facts, or not explaining how a cited case was relevant. In this study, ChatGPT and Gemini often replied to prompts with very brief answers in bullet point style, while their answers lacked citations to sources, omitted nuances, and failed to address arguments for both sides of the issues. As a result, ChatGPT and Gemini tied for the second lowest overall scores for applying rules to facts.\nFinally, \"ChatGPT Goes to Law School\u201d demonstrated how ChatGPT responded with answers that were \u201cexcessively cagey\u201d or \u201crefused to make an argument about the most plausible interpretation of the relevant facts when those facts potentially pointed in competing directions.\u201d This study discovered all LLMs evaluated exhibited this behavior to some extent. This element of legal reasoning and analysis was measured with the criteria", "think step by step was found, out of a number of prompts to maximize performance.\u201d This finding is opposite to the results of this study where the prompt \u201cthink step by step": "nly improved ChatGPTs' performance on one out of the three statutory reasoning exercises. When looking at ChatGPTs' performance on all seven reasoning exercises, the prompt to think step by step resulted in only a comparatively small increase in performance compared with the other LLMs.\nOnly Lexis+ AI performed worse than ChatGPT when provided with the prompt to think step by step. These findings are similar to a 2023 study demonstrating that the performance of seven out of eight LLMs in solving tax law problems was not improved by prompting them to think step by step. However, the 2023 study found GPT-4's performance improved when asked to think step by step."}, {"title": "Legal vs. Non-Legal LLMs", "content": "The study \u201cLawBench: Benchmarking Legal Knowledge of Large Language Models\" compared the abilities of legal and non-legal LLMs to memorize legal knowledge, understand legal concepts, and apply legal knowledge. LawBench found that \u201clegal specific LLMs do not necessarily outperform general large language models.\u201d This is similar to the findings of this study where non-legal LLMs routinely outperformed Lexis+ AI at legal analysis and reasoning. In fairness, Lexis+ Al's corpus is much more limited than the other LLMs evaluated. Lexis+ AI's comparatively limited corpus may account for its performance relative to the other LLMs evaluated."}, {"title": "Hallucinations", "content": "This study identified two types of hallucinations. The first is a hallucination of the law or facts provided that might \"produce a response that is unfaithful to or in conflict with the input prompt. .\u201d The second type of hallucination is a response that is not consistent \u201cwith the facts of the world\u201d including hallucinated statements of law.\nClaude and Copilot both received perfect scores in the hallucination category for responding with zero hallucinations to all seven factual scenarios. ChatGPT and Gemini each hallucinated once when responding to the beginning rule exercise. This constitutes a hallucination rate of 14 percent. They both hallucinated that the animal's act of removing pills from the unhoused person's hand met the ADA requirements for having special training or abilities as a service animal.\nLexis+ AI performed the worst at hallucinating out of all LLMs evaluated, hallucinating when answering four out of the seven exercises constituting a hallucination rate of 57 percent. Lexis+ AI hallucinated the facts and law provided in the beginning rule exercises by stating that the dog received specialized training and by providing legal information about miniature horses and non-human primates as services animals under the ADA. Lexis+ AI hallucinated on the law provided in the beginning statutory exercise by citing a rule of criminal procedure despite the exercise's focus exclusively on a civil case. Lexis+ AI hallucinated the facts provided in the intermediate statute exercise when responding that it was unclear if the visitor objected to the physician's assistance. Nothing in the facts indicated any objections made by the visitor. Finally, Lexis+ AI hallucinated the law provided in the skilled statutory exercise by analyzing the prompt under the National Childhood Vaccine Injury Act when the exercise specifies the vaccine in question was to be administered to an animal and not a human.\nThis study's findings that Lexis+ AI hallucinated more than any other LLM evaluated differs from a recently published study comparing hallucination rates between Westlaw AI Assisted Research, Westlaw Ask Practical Law AI, Lexis+ AI, and GPT-4. The study, \"Hallucination Free? Assessing the Reliability of Leading AI Legal Research Tools,\" looked at hallucinations that are unfaithful to the facts of the world. The study found that LLMs hallucinated between 17 percent and 33 percent of the time. Lexis+ AI performed the best of the four LLMs tested \u201canswering 65 percent of queries accurately\" and only hallucinating 35 percent of the time. Differences between testing methodologies make it difficult to make any direct comparisons between Lexis+ AI's 57 percent hallucination rate found in this study and Lexis+ AI's 35 percent hallucination rate found in the \"Hallucination Free\" study."}, {"title": "False Confidence \u2013 When You Know, You Know", "content": "All the LLM's tested demonstrated some degree of false confidence in responses by stating a conclusion with certainty despite the conclusion being objectively wrong. This false confidence is the other side of the coin of the cagey, overqualified responses discussed above.\nThis phenomenon is not a recent development in electronic legal research. Unjustified overconfidence in research conducted electronically dates to the CD-ROM days of the mid-1990s. Studies \u201chave clearly documented a false sense of security on the part of computer researchers.\u201d In one previous study, students who incorrectly answered questions using electronic research tools \"refused to give [the electronic tools] a lower effectiveness rating\" than the rating given to print research tools. A study of Westlaw's early AI product, Westlaw Next, demonstrated that the majority of researchers using the product and expressing a high degree of confidence in their answers \u201cdid not come anywhere close to identifying the correct answer to the question.\u201d\nRecent research reveals troubling trends with LLMs that are likely to amplify researchers' sense of false confidence in incorrect results. The studies find that LLMs demonstrate \"certainty in their responses, i.e., their self-awareness of their propensity to hallucinate\u201d and that \"LLMs often provide seemingly legitimate but incorrect answers to contra-factual legal questions\u201d and \u201cstruggle to accurately gauge their own level of certainty without post-hoc recalibration.\u201d Some research suggests that when LLMs engage in this behavior they \u201cmight be mimicking human expressions when verbalizing confidence.\u201d In other words, they learned it by watching you!\nCopilot, GPT, and Geminis' responses to the skilled statutory exercise all demonstrate false confidence in their incorrect responses to the prompt. The skilled statutory exercise tests LLMs' abilities to analyze a statute of limitations question with a possible tolling of the statute based on a previously unknown but relevant factor. Copilot's response to the prompt was brief and certain stating that \"since four years and six months have passed since the missed vaccination, it appears that the two-year statute of limitations would bar the"}, {"title": "Can LLMs Think Like a Lawyer?", "content": "The results of this study, as explained above, clearly demonstrate that LLMs can \"think like a lawyer\" in a limited sense by analyzing a factual scenario using the IRAC framework. Table 2 shows the performance of each LLM at the discrete IRAC tasks measured in this study. The results mirror the overall performance of each LLM tested on all tasks. Claude narrowly outperformed other LLMs with Copilot, GPT, Gemini, and Lexis+ AI finishing closely behind. If the LLMs were graded on their performance at IRAC analysis their scores would range from Lexis+ AI's 73 percent on the low end to Claude's 90 percent at the high end."}, {"title": "Challenges - Stability and Transparency", "content": "The instability of answers created by LLMs complicates their usefulness for legal work and ability to think like a lawyer. Researchers who repeatedly input identical prompts to generative Al will never receive the same responses. One study exploring ChatGPT's rate of nondeterminism (being unpredictable or returning different results when presented with identical queries) concluded that ChatGPT \u201cis very unstable\u201d and responded with \u201chigh degrees of non-determination.\u201d The authors posit that such a high degree of non- determinism is \u201ca potential menace to scientific conclusion validity\" because it makes scientific conclusions unrepeatable and therefore unreliable.\nLawyers who use LLMs for legal research should be concerned about nondeterministic results. Paul Callister eloquently summarized how this inconsistency is problematic for lawyers and legal researchers."}, {"title": "Challenges - Creativity", "content": "Creativity is a component of thinking like a lawyer. Lawyers often advocate for their clients by urging a court to adopt \u201cnew or innovative applications of existing laws.\u201d Lawyers must be able to reliably find the law including \"arcane doctrines and infrequently utilized cases or statutes\u201d \u201cbefore they can think creatively and develop novel arguments. This work often occurs in the gray areas and fringes of existing law.\u201d Writing several decades ago, Barbara Bintliff described how computerized legal research can make it difficult to discover legal rules and potentially hinder creative legal thought. \u201cUrging a change in the law's application, pushing the envelope, is difficult when you haven't even found the envelope.\"\nLLMs are known for their abilities to produce creative works of music, poetry, and visual art to name a few examples. The creative power of LLMs could be extremely useful to lawyers when thinking creatively. Unfortunately, the highly-secretive stances adopted by the corporate interests controlling LLMs are hindering the full potential of LLMs to think like a lawyer and making it more difficult for creative lawyers to \u201cfind the envelope.\u201d"}, {"title": "Theoretical Challenges and Examples of LLMs Limitations", "content": "Legal and linguistic theorists question the reasoning abilities of LLMs. Cass Sunstein, writing several decades ago in the dawn of AI, critiqued the legal reasoning abilities of it. Sunstein questioned the abilities of AI to perform analogical legal reasoning. Sunstein agreed with Ronald Dworkin's view that \u201clegal reasoning often consists of an effort to make best constructive sense out of past legal events.\u201d Sunstein criticizes \u201cextravagant claims on behalf of artificial intelligence in law based on a crude picture of legal reasoning, one that disregards the need to root judgments of analogousness, or disanalogous, in judgements of principle and policy.\u201d Sunstein admits that in the future, computers may \u201cbe able to both generate competing principles for analogical reasoning and to give grounds for thinking that one or another principle is best.\""}, {"title": "Challenges - Thinking (and Acting) Like a Lawyer Includes Complying with Ethical Rules", "content": "The conduct of lawyers is governed by ethical rules that are specific to the jurisdictions where they are admitted to practice law. Other rules governing lawyers conduct include federal and state court rules, local court rules, and even chamber rules that are specific to individual judges.\nThe use of AI and LLMs by lawyers and those who work for lawyers implicates a number of the ABA's Model Rules of Professional Conduct, including Rule 1.1 (Competence), Rule 1.6 (Confidentiality), Rule 3.3 (Candor to Tribunals and Fairness to\nLessons from the Infamous Case of Matta v. Avianca\nThe case of Matta v. Avianca serves as a cautionary tale, showcasing the dangers of relying solely on LLMs for legal research and ignoring lawyers' basic ethical duties of competence and candor. On March 1, 2023, a New York lawyer filed an Affirmation in Opposition to a Motion to Dismiss. The filing \"cited and quoted from purported judicial decisions that were said to [be"}]}