[{"title": "Distributed Conformal Prediction via Message Passing", "authors": ["Haifeng Wen", "Hong Xing", "Osvaldo Simeone"], "abstract": "Post-hoc calibration of pre-trained models is critical for ensuring reliable inference, especially in safety-critical domains such as healthcare. Conformal Prediction (CP) offers a robust post-hoc calibration framework, providing distribution-free statistical coverage guarantees for prediction sets by leveraging held-out datasets. In this work, we address a decentralized setting where each device has limited calibration data and can communicate only with its neighbors over an arbitrary graph topology. We propose two message-passing-based approaches for achieving reliable inference via CP: quantile-based distributed conformal prediction (Q-DCP) and histogram-based distributed conformal prediction (H-DCP). Q-DCP employs distributed quantile regression enhanced with tailored smoothing and regularization terms to accelerate convergence, while H-DCP uses a consensus-based histogram estimation approach. Through extensive experiments, we investigate the trade-offs between hyperparameter tuning requirements, communication overhead, coverage guarantees, and prediction set sizes across different network topologies.", "sections": [{"title": "1. Introduction", "content": null}, {"title": "1.1. Context and Motivation", "content": "The post-hoc calibration of pre-trained artificial intelligence (AI) models has become increasingly important as a means to ensure reliable inference and decision-making in safety-critical domains such as healthcare (Kompa et al., 2021), engineering (Cohen et al., 2023) and large language models (LLMs) (Ji et al., 2023; Huang et al., 2023). Conformal prediction (CP) is a model-agnostic post-hoc calibration framework that provides distribution-free statistical coverage guarantees. This is done by augmenting an AI model's decisions with prediction sets evaluated on the basis of held-out data (Vovk et al., 2005; Angelopoulos et al., 2024b).\nCP uses held-out calibration data to infer statistical information about the distribution of the errors made by the pre-trained AI model. Based on this analysis, CP associates to each decision of the AI model a prediction set that includes all the outputs that are consistent with the inferred error statistics. Specifically, CP calculates a quantile of performance scores attained by the pre-trained model on the calibration data (Vovk et al., 2005; Lei et al., 2018; Barber et al., 2021). The prediction set provably meets marginal coverage guarantees for a user-defined target. Accordingly, the correct output is included in the prediction set with high probability with respect to the distribution of calibration and test data.\nIn light of its simplicity and of its strong theoretical properties, CP has been recently developed in several directions, including to address more general risk functions (Angelopoulos et al., 2024c), to provide localized statistical guarantees (Gibbs et al., 2023), and to operate via an online feedback-based mechanism (Gibbs & Candes, 2021). Furthermore, it has been applied in safety-critical areas such as medical diagnostics (Lu et al., 2022) and LLMs (Quach et al., 2024; Mohri & Hashimoto, 2024).\nAs mentioned, CP requires access to a data set of calibration data points. However, in practice, a decision maker may not have sufficient calibration data stored locally. This is an important issue, since the size of the prediction set depends on the number of available calibration data points, and thus a small calibration data set would yield uninformative prediction sets (Zecchin et al., 2024). However, calibration data may be available in a decentralized fashion across multiple devices (Xu et al., 2023). Examples include diagnostic healthcare models at different hospitals, Internet-of-Things (IoT) systems, and autonomous vehicle networks. In many of these scenarios, the distributed devices are privacy-conscious, preventing a direct exchange of the local calibration data sets.\nWith this motivation, prior art has studied settings in which multiple data-holding devices are connected to the decision-"}, {"title": "1.2. Main Contributions", "content": "The star topology assumed in the prior art does not reflect many important scenarios of interest in which communication is inherently local, being limited to the neighbors of each device. As illustrated in Fig. 1(a), this paper studies CP in a fully decentralized architecture in which the data-holding devices can only communicate via message passing on a connectivity graph. In this setting, all devices are decision-makers that have access to limited local calibration data and share a common pre-trained model. As shown in Fig. 1(b), given a common input, the devices aim at producing a prediction set that includes the true label with a user-defined probability.\nThis paper proposes two distributed CP (DCP) approaches: quantile-based DCP (Q-DCP) and histogram-based DCP (H-DCP). Q-DCP employs distributed quantile regression enhanced with tailored smoothing and regularization terms to accelerate convergence via message passing, while H-DCP uses a consensus-based histogram estimation approach inspired by (Zhu et al., 2024b).\nSpecifically, the main contributions of this work are as follows:\n1. We introduce Q-DCP, a message-passing CP protocol based on quantile regression. Q-DCP addresses a decentralized quantile regression problem by means of the alternating direction method of multipliers (ADMM) (Boyd et al., 2011) with tailored smoothing and regularization to accelerate the convergence speed. Q-DCP guarantees marginal coverage as centralized CP, as long as hyperparameters related to the network topology and to the initialization are properly selected.\n2. We also introduce H-DCP, a decentralized CP protocol that attains hyperparameter-free coverage guarantees. H-DCP estimates the global histogram of quantized local scores via a consensus algorithm.\n3. Experiments explore the trade-offs between hyperparameter tuning requirements, communication overhead, coverage guarantees, and prediction set sizes for Q-DCP and H-DCP across different network topologies. While H-DCP requires a larger communication load than Q-DCP, its coverage guarantees are not tied to hyperparameter selection."}, {"title": "1.3. Notations", "content": "We use lower-case letter x to denote scalars; upper-case letter X to denote random variables; bold lower-case letter x to denote vectors; bold upper-case letter X to denote matrices; script letter \\( \\mathcal{X} \\) to denote sets. We denote \\( ||\\mathbf{x}|| \\) as the \\( l_2 \\)-norm of a vector \\( \\mathbf{x} \\); \\( \\mathbb{I}\\{\u00b7\\} \\) as the indicator function;"}, {"title": "2. Problem Description", "content": "We study a network of K devices in which each device k has access to a local calibration data set \\( \\mathcal{D}_k = \\{(\\mathbf{x}_{i,k}, y_{i,k})\\}_{i=1}^{n_k} \\) with data points \\( (\\mathbf{x}_{i,k}, y_{i,k}) \\) drawn i.i.d. from a distribution \\( P_k \\). We define the global data set as the union \\( \\cup_{k=1}^{K} \\mathcal{D}_k = \\mathcal{D} \\) and \\( n = \\sum_{k=1}^K n_k \\) as the total number of data points. Using the local data sets \\( \\{\\mathcal{D}_k\\}_{k=1}^{K} \\), and message-passing-based communication, the devices aim to collaboratively calibrate the decision of a shared pre-trained model \\( f : \\mathcal{X} \\to \\mathcal{Y} \\).\nTo elaborate, assume that a test input \\( \\mathbf{x}_{test} \\) is available at all devices. This may represent, e.g., a common observation in a sensor network or a user query distributed to multiple servers. Given a target coverage level \\( (1 - \\alpha) \\) for \\( \\alpha \\in [0, 1] \\), the goal of the system is to determine a set-valued function \\( C(\\cdot|\\mathcal{D}) : \\mathcal{X} \\to 2^{\\mathcal{Y}} \\) with marginal coverage guarantees. Specifically, following (Lu et al., 2023), given a test data point \\( (\\mathbf{x}_{test}, y_{test}) \\), drawn from the mixture distribution\n\\[\nP = \\sum_{k=1}^{K} w_k P_k\n\\]\n\\[\n\\sum_{k=1}^{K} w_k = 1,\n\\]\nfor some weight \\( w_k > 0 \\) and satisfying the coverage requirement\n\\[\nP(\\mathbf{y}_{test} \\in C(\\mathbf{x}_{test}|\\mathcal{D})) \\geq 1 - \\alpha.\n\\]\nThis condition requires that the test label \\( \\mathbf{y}_{test} \\) belongs to the prediction set \\( C(\\mathbf{x}_{test}|\\mathcal{D}) \\) with probability at least \\( 1 - \\alpha \\). The probability in (2) is evaluated over the calibration data in \\( \\mathcal{D} \\) and the test data \\( (\\mathbf{x}_{test}, \\mathbf{y}_{test}) \\). As in (Lu et al., 2023), we will specifically concentrate on the choice\n\\[\nw_k \\propto n_k + 1,\n\\]\nin which the weight \\( w_k \\) for each device k is proportional to the size of the local data set \\( \\mathcal{D}_k \\).\nThe average size of the output of \\( C(\\cdot|\\mathcal{D}) \\), referred to as the inefficiency, is defined as the expectation \\( E[|C(\\mathbf{x}_{test}|\\mathcal{D})|] \\), where the average is evaluated over \\( \\mathbf{x}_{test} \\) and on the distribution of the data \\( \\mathcal{D} \\). Ideally, the prediction set \\( C(\\mathbf{x}_{test}|\\mathcal{D}) \\) minimizes the inefficiency while satisfying the constraint (2).\nAs shown in Fig. 1(a), in order to produce the prediction set \\( C(\\mathbf{x}_{test}|\\mathcal{D}) \\), devices can communicate over a connectivity graph. The connectivity graph \\( \\mathcal{G}(\\mathcal{V}, \\mathcal{E}) \\) is undirected,"}, {"title": "3. Background", "content": null}, {"title": "3.1. Split Conformal Prediction", "content": "Split CP provides a general framework for the design of post-hoc calibration strategies satisfying the coverage requirement (2) in centralized settings. CP constructs the prediction set \\( C(\\cdot|\\mathcal{D}) \\) based on a calibration data set \\( \\mathcal{D} \\). This is done by evaluating a quantile, dependent on the largest mis-coverage level \\( \\alpha \\), of the scores assigned by the pre-trained model f to the calibration data points in set \\( \\mathcal{D} \\).\nTo elaborate, define as \\( s(\\mathbf{x}, y) \\) a negatively oriented score derived from model f, such as the absolute error \\( s(\\mathbf{x}, y) = |y - f(\\mathbf{x})| \\) for regression or the log-loss \\( s(\\mathbf{x},y) = -\\log f_y(\\mathbf{x}) \\) for classification. Write as \\( S_i = s(\\mathbf{x}_i, y_i) \\) the score assigned by the model f to the i-th calibration data point \\( (\\mathbf{x}_i, y_i) \\) in the data set \\( \\mathcal{D} \\). The prediction set is evaluated by including all the labels \\( y \\in \\mathcal{Y} \\) with a score no larger than a fraction, approximately \\( 1 - \\alpha \\), of the calibration scores \\( \\{S_i\\}_{i=1}^n \\). Specifically, CP produces the set\n\\[\nC(\\mathbf{x}|\\mathcal{D}) = \\{y \\in \\mathcal{Y} : s(\\mathbf{x}, y) \\leq Q((1 - \\alpha)(1 + 1/n); \\{S_i\\}_{i=1}^n)\\},\n\\]\nwhere \\( Q(\\gamma; \\{S_i\\}_{i=1}^n) \\) is the \\( \\lceil\\gamma n\\rceil \\)-th smallest value of the set \\( \\{S_i\\}_{i=1}^n \\), which is set as the score threshold.\nThe empirical \\( \\gamma \\)-quantile \\( Q(\\gamma; \\{S_i\\}_{i=1}^n) \\) in (4) can be obtained by solving the following quantile regression problem (Koenker & Bassett Jr, 1978)\n\\[\nQ(\\gamma; \\{S_i\\}_{i=1}^n) = \\arg \\min_{s \\in \\mathbb{R}} \\rho_{\\gamma}(s |\\{S_i\\}_{i=1}^n),\n\\]\nwhere \\( \\rho_{\\gamma}(s |\\{S_i\\}_{i=1}^n) \\) is the pinball loss function defined as\n\\[\n\\rho_{\\gamma}(s|\\{S_i\\}_{i=1}^n) = \\frac{1}{n}\\sum_{i=1}^n \\gamma \\text{ReLU}(S_i - s) + (1 - \\gamma)\\sum_{i=1}^n \\text{ReLU}(s - S_i),\n\\]\nwith \\( \\text{ReLU}(x) = \\max(x, 0) \\)."}, {"title": "3.2. Distributed Conformal Prediction on a Star Topology", "content": "The calculation of the empirical quantile in the prediction set (4) requires full knowledge of the scores of all calibration data points in set \\( \\mathcal{D} \\). When the data points are distributed across privacy-sensitive and communication-constrained devices as in the setting under study in this paper, evaluating the empirical quantile is not possible unless communication is enabled. Prior work has studied a federated setting in which all devices are connected to a centralized server (Humbert et al., 2023; Lu et al., 2023; Plassier et al., 2023; Zhu et al., 2024b;a; Kang et al., 2024). For reference, we briefly introduce FedCP-QQ (Humbert et al., 2023), FCP (Lu et al., 2023) and WFCP (Zhu et al., 2024b) next.\nFedCP-QQ: Denote as \\( \\{S_{i,k} = s(\\mathbf{x}_{i,k}, y_{i,k})\\}_{i=1}^{n_k} \\) the scores evaluated using the shared model f on the local data set \\( \\mathcal{D}_k \\) for device k. In FedCP-QQ (Humbert et al., 2023), each device k first calculates the empirical \\( (1 - \\alpha') \\)-quantile of its local scores, which is transmitted to the centralized server for some probability \\( \\alpha' \\). After collecting K local quantiles, the central server calculates the empirical \\( (1-\\beta) \\)-quantile of the received quantiles, thus obtaining quantile-of-quantiles. By optimizing the probabilities \\( \\alpha' \\) and \\( \\beta \\), the prediction set (4) is constructed using quantile-of-quantiles as the threshold. This approach satisfies the coverage condition (2) if the data sets \\( \\mathcal{D}_k \\) are i.i.d. across devices.\nFCP: While FedCP-QQ assumes i.i.d. data sets across devices, FCP (Lu et al., 2023) adopts the model described in Sec. 2 allowing data points from different local data sets to be drawn from different distributions \\( \\{P_k\\}_{k=1}^K \\), while the test data point \\( (\\mathbf{x}_{test}, y_{test}) \\) is drawn from a mixture \\( P = \\sum_{k=1}^{K} w_kP_k \\) of the local distributions. In FCP, the central server collects the local scores from the K devices, and then calculates the empirical \\( (1-\\alpha)(1+K/n) \\)-quantile. This value is used as the threshold to construct prediction set \\( C(\\mathbf{x}|\\mathcal{D}) \\) in (4). FCP satisfies coverage condition (2) by choosing the weight \\( w_k \\) to be proportional to \\( n_k + 1 \\), i.e., \\( w_k \\propto n_k + 1 \\).\nWFCP: Unlike FedCP-QQ and FCP, which communicate quantiles between devices and server, Zhu et al. (2024b) proposed to exchange information about the local histograms of the quantized scores. Specifically, the scores \\( \\{S_{i,k}\\}_{i=1}^{n_k} \\) are first quantized to M levels by each device k, which then evaluates the histogram vector \\( \\mathbf{p}_k = [p_{1,k}, p_{2,k}, ..., p_{M,k}]^T \\in \\mathbb{R}^M \\) of the scores. The vectors \\( \\mathbf{p}_k \\), with \\( k = 1, 2, ..., K \\), are synchronously transmitted to the centralized server on an additive multi-access channel, and the server estimates the average histogram \\( \\mathbf{p} = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{p}_k \\) based on the received signal. The empirical \\( (1 - \\alpha') \\)-quantile of all the scores is then estimated from the estimated average histogram \\( \\mathbf{p} \\) for some optimized value \\( \\alpha' \\). Under the non-i.i.d. setting described in Sec. 2, WFCP can guarantee the coverage condition (2)."}, {"title": "4. Quantile-based Distributed CP", "content": "In this section, we present the first fully decentralized protocol proposed in this work, which is referred to as quantile-based distributed CP (Q-DCP)."}, {"title": "4.1. Decentralized Quantile Regression via ADMM", "content": "Q-DCP addresses the quantile regression problem (5) in the fully decentralized setting described in Sec. 2 using ADMM (Boyd et al., 2011). This strategy obtains an approximation of the empirical quantile \\( Q((1 - \\alpha)(1 + K/n); \\{S_i\\}_{i=1}^n) \\) with a controlled error, which requires a single scalar broadcast to the neighbors of each device at each optimization iteration. As will be discussed, on the flip side, Q-DCP requires careful hyperparameter tuning in order to satisfy the coverage condition (2). The alternative strategy proposed in the next section alleviates such requirements on hyperparameter tuning at the cost of a larger per-iteration communication cost.\nFormally, in Q-DCP, the K devices collaboratively solve the quantile regression problem\n\\[\n\\min_{s \\in \\mathbb{R}} \\sum_{k=1}^{K} \\rho_{(1-\\alpha)(1+K/n)}(s |\\{S_{i,k}\\}_{i=1}^{n_k}),\n\\]\nwhere target coverage \\( (1 - \\alpha)(1 + K/n) \\) follows FCP (Lu et al., 2023) (see previous section). The objective function (7) is convex, but it is not smooth or strongly convex. For such an objective function, a direct application of ADMM would exhibit a sub-linear convergence rate, causing a large optimality gap when the number of communication rounds is limited (He & Yuan, 2012).\nTo ensure a linear convergence rate, we propose to replace the ReLU operation in (6) with the smooth function \\( \\bar{g}(x) = x + (1/\\kappa)\\log(1+e^{-\\kappa x}) \\), where smaller \\( \\kappa \\) leads to greater smoothness at the cost of larger approximation error. In practice, the function \\( \\bar{g}(x) \\) coincides with \\( \\text{ReLU}(x) \\) as \\( \\lim_{\\kappa \\to \\infty} \\bar{g}(x) = \\text{ReLU}(x) \\) (Chen & Mangasarian, 1996). Furthermore, to guarantee strong convexity, we add a regularization term to the pinball loss function (6) as\n\\[\n\\rho_{\\gamma}(s|\\{S_i\\}_{i=1}^n) = \\frac{1}{n}\\sum_{i=1}^n \\left(\\gamma \\bar{g}(S_i - s) + (1 - \\gamma)\\bar{g}(s - S_i)\\right) + \\frac{\\mu}{2}(s - s_0)^2,\n\\]\nwhere \\( \\mu > 0 \\) and \\( s_0 \\) are hyperparameters. The modified pinball loss function \\( \\rho_{\\gamma}(s|\\{S_i\\}_{i=1}^n) \\) is L-smooth and \\( \\mu \\)-strongly convex, with \\( L = (n\\kappa)/4 + \\mu \\). Using the smooth"}, {"title": "4.2. Description of Q-DCP", "content": "Following ADMM (Boyd et al., 2011), Q-DCP solves problem (10) by considering the augmented Lagrangian\n\\[\n\\mathcal{L}_c(\\mathbf{s}, \\mathbf{z}, \\mathbf{\\lambda}) = f(\\mathbf{s}) + \\langle \\mathbf{\\lambda}, A\\mathbf{s} + B\\mathbf{z} \\rangle + \\frac{c}{2}||A\\mathbf{s} + B\\mathbf{z}||_2^2,\n\\]\nwhere \\( \\mathbf{\\lambda} \\in \\mathbb{R}^{2|\\mathcal{E}|} \\) is the Lagrange multiplier associated with the 2E equality constraints in (10), and \\( c > 0 \\) is a hyperparameter. The updates of the local estimated quantiles s, the consensus variables z and the dual variables at iteration t + 1 are given by (Boyd et al., 2011)\n\\[\n\\mathbf{s}^{(t+1)} = \\arg \\min_{\\mathbf{s} \\in \\mathbb{R}^K} \\mathcal{L}_c(\\mathbf{s}, \\mathbf{z}^{(t)}, \\mathbf{\\lambda}^{(t)}),\n\\]\n\\[\n\\mathbf{z}^{(t+1)} = \\arg \\min_{\\mathbf{z} \\in \\mathbb{R}^{2|\\mathcal{E}|}} \\mathcal{L}_c(\\mathbf{s}^{(t+1)}, \\mathbf{z}, \\mathbf{\\lambda}^{(t)}),\n\\]\n\\[\n\\mathbf{\\lambda}^{(t+1)} = \\mathbf{\\lambda}^{(t)} + c(A\\mathbf{s}^{(t+1)} + B\\mathbf{z}^{(t+1)}).\n\\]\nAfter T iterations, ADMM produces the quantile estimate \\( s_k^{(T)} \\) for each device \\( k \\in \\mathcal{V} \\). Then, the devices run the fast distributed averaging algorithm (Xiao & Boyd, 2004) to obtain the average \\( \\bar{s}^{(T)} = \\frac{1}{K} \\sum_{k \\in \\mathcal{V}} s_k^{(T)} \\) with negligible error.\nFinally, the prediction set is constructed as\n\\[\nC^{Q-\\text{DCP}}(\\mathbf{x}|\\mathcal{D}) = \\{y \\in \\mathcal{Y} : s(\\mathbf{x}, y) < \\bar{s}^{Q-\\text{DCP}}\\},\n\\]\nwith\n\\[\n\\bar{s}^{Q-\\text{DCP}} = \\bar{s}^{(T)} + \\epsilon^{Q-\\text{DCP}} ,\n\\]\nwhere \\( \\epsilon^{Q-\\text{DCP}} \\) is an upper bound on the error \\( |\\bar{s}^{(T)} - s^*| \\) of the quantile estimate \\( \\bar{s}^{(T)} \\) to be derived in the next subsection.\nAn algorithmic table for Q-DCP can be found in the supplementary material (see Appendix C)."}, {"title": "4.3. Theoretical Guarantees", "content": "In this section, we first derive the \\( \\epsilon^{Q-\\text{DCP}} \\) upper bound on the estimation error of Q-DCP used in constructing the prediction set (13). Then we prove that Q-DCP attains the coverage guarantee (2).\nBy the triangle inequality, the estimation error \\( |\\bar{s}^{(T)} - s^*| \\) can be upper bounded as\n\\[\n|\\bar{s}^{(T)} - s^*| \\leq |\\bar{s}^{(T)} - \\hat{s}^*| + |\\hat{s}^* - s^*|,\n\\]\nwhere the first term accounts for the convergence error for problem (10), while the second term quantifies the bias caused by the use of the smooth and strongly convex approximation (8).\nThe bias term can be bounded as follows. We start with the following assumption.\nAssumption 4.1. The regularization parameter \\( s_0 \\) and the optimal solution \\( s^* \\) in (5) differ by at most \\( \\epsilon_0 \\geq 0 \\), i.e., \\( |s_0 - s^*| \\leq \\epsilon_0 \\).\nProposition 4.2. Under Assumption 4.1, the bias \\( |\\hat{s} - s^*| \\) is upper bounded as\n\\[\n|\\hat{s} - s^*| \\leq \\sqrt{\\frac{2n \\log(2)}{n\\kappa}} + \\epsilon_0 \\stackrel{\\triangle}{=} \\tilde{\\epsilon}_0.\n\\]\nProof: See supplementary material (see Appendix A.1).\nNext, the convergence error \\( |\\bar{s}^{(T)} - \\hat{s}^*| \\) is bounded by leveraging on existing results on the convergence of ADMM.\nProposition 4.3 (Theorem 1, Shi et al., 2014). The convergence error \\( |\\bar{s}^{(T)} - \\hat{s}^*| \\) is upper bounded as\n\\[\n|\\bar{s}^{(T)} - \\hat{s}^*| \\leq \\frac{1}{T-1} \\sqrt{\\frac{8LK}{K \\mu} \\left( 1 + \\frac{L}{\\kappa \\mu} \\right)} ||u^{(0)} - u^*||_G \\stackrel{\\triangle}{=} \\epsilon^{(T)},\n\\]\nwhere the parameter \\( \\delta > 0 \\) is defined as\n\\[\n\\delta = \\min \\left\\{ \\frac{(b - 1)\\lambda_{\\min}(M_-)}{b \\lambda_{\\max}(M_+)}, \\frac{\\mu}{(c/4)\\lambda_{\\max}(M_+) + (b/c)L^2 (\\lambda_{\\min}(M_-)) } \\right\\},\n\\]\nwith any \\( b > 1 \\), \\( M_- = A_1^T A_1 - A_2^T A_2 \\), \\( M_+ = A_1^T A_1 + A_2^T A_2 \\), and \\( G = \\begin{bmatrix} (\\frac{\\eta}{K})I_E & 0_E \\\\ 0_E & (1/c)I_E \\end{bmatrix} \\). Moreover, \\( u^{(0)} = [(\\mathbf{z}^{(0)})^T, ((\\mathbf{\\lambda}^{(0)})^T]^T \\) with \\( (\\mathbf{\\lambda}^{(0)}) \\in \\mathbb{R}^{|\\mathcal{E}|} \\) containing the first E entries of the vector \\( \\mathbf{\\lambda}^{(0)} \\), and we also write as \\( u^* ="}, {"title": "5. Histogram-based Distributed CP", "content": "As demonstrated by Theorem 4.4, Q-DCP requires knowledge of a parameter \\( \\epsilon_0 \\) satisfying Assumption 4.1, as well as of the parameter \\( \\delta \\) in (17), which depends on the connectivity properties of the graph through the incidence matrix A. In this section, we present an alternative fully decentralized protocol, referred to as histogram-based DCP (H-DCP), which provides rigorous coverage guarantees without the need for hyperparameter tuning as in Q-DCP, but at the cost of larger communication overhead.\nIn H-DCP, devices exchange histograms of quantized calibration scores in a manner similar to WFCP (Zhu et al., 2024b). As a result of a consensus algorithm, the devices evaluate the average histogram in order to yield an estimate of the mixture distribution P in (1) with weights (3). From this estimate, a suitable bound is derived on the \\( (1-\\alpha)(1 + K/n) \\)-quantile of the distribution P to evaluate the prediction set of the form (4)."}, {"title": "5.1. Description of H-DCP", "content": "To start", "Gamma(\\cdot)": [0, 1], "frac{1}{M}": "frac{m"}, {"frac{m}{M}": "text{ for"}, "m = 2,..., M. \\end{cases}\n\\"], "mathcal{Y}": "Gamma(s(\\mathbf{x}, y)) \\leq \\frac{M_{\\alpha, k}^{H-\\text{DCP}"}, {}]