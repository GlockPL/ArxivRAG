{"title": "GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMS", "authors": ["Maxim Zhelnin", "Viktor Moskvoretskii", "Egor Shvetsov", "Egor Venediktov", "Mariya Krylova", "Aleksandr Zuev", "Evgeny Burnaev"], "abstract": "Parameter Efficient Fine-Tuning (PEFT) methods have gained popularity and democratized the usage of Large Language Models (LLMs). Recent studies have shown that a small subset of weights significantly impacts performance. Based on this observation, we introduce a novel PEFT method, called Gaussian noise Injected Fine Tuning of Salient Weights (GIFT-SW). Our method updates only salient columns, while injecting Gaussian noise into non-salient ones. To identify these columns, we developed a generalized sensitivity metric that extends and unifies metrics from previous studies. Experiments with LLaMA models demonstrate that GIFT-SW outperforms full fine-tuning and modern PEFT methods under the same computational budget. Moreover, GIFT-SW offers practical advantages to recover performance of models subjected to mixed-precision quantization with keeping salient weights in full precision. Code is available in our repository.", "sections": [{"title": "Introduction", "content": "Modern LLMs demonstrate remarkable generalization capabilities on unseen tasks. However, fine-tuning remains crucial to enhance these models performance or to restore the performance after compression techniques like quantization (Dettmers et al., 2024; Moskvoretskii et al., 2024), pruning (Frantar and Alistarh, 2023; Kim et al., 2023), or tensor decomposition have been applied. Given the large scale of modern LLMs, fine-tuning all parameters can be computationally and memory-intensive. To overcome this challenge, Parameter Efficient Fine-Tuning schemes have been developed, aimed to improve model performance while using limited computational and memory resources.\nTo date, PEFT methods have not matched the accuracy of full fine-tuning (Nikdan et al., 2024), highlighting the need for new approaches that can close this gap while still minimizing resource use. Additionally, most PEFT methods involve adding extra parameters, which increases computational demands.\nTo address those issues and enhance the performance of efficiently trained LLMs, we introduce a novel PEFT method, GIFT-SW. This approach focuses on updating a small subset of salient weights while injecting noise into the non-salient weights. The development of this method is grounded in observations from previous studies and the related questions they raise, which we aim to answer:\nPrevious research has shown that there is a small subset of salient weights which can significantly affect the effectiveness of post-training quantization (PTQ) (Dettmers et al., 2022, 2023; Kim et al., 2023) and pruning techniques (Yin et al., 2023; Frantar and Alistarh, 2023; Sun et al., 2023). Moreover, Gurnee et al. identified a group of \"universal neurons\" that are critical to a model's functionality, emphasizing the importance of selecting and updating these salient weights. Question 1: Does updating a small subset of salient weights is sufficient to adjust the model?\nRecent studies have demonstrated that Perturbed Gradient Descent (PGD), with noise injections applied both before and after the gradient step, can stabilize convergence and help prevent overfitting (Poole et al., 2014; Zhu et al., 2018; Jin et al., 2021). Question 2: Does Injecting Noise helps convergence?\nPGD is commonly employed to enhance model robustness by approximating the quantization process (Shvetsov et al., 2022; Shin et al., 2023; D\u00e9fossez et al., 2021). This increased robustness can aid in maintaining the quality of the quantized model. Question 3: Does injecting noise helps robustness?\nSelecting salient weights is a significant challenge, particularly in quantization and pruning, and it is central to our method. In our paper, we derive a general formulation for all previously established saliency metrics and present experiments to compare their effectiveness.\nThe main contributions of our work can be summarized as follows:\n\u2022 We introduce a novel PEFT method for pre-trained and quantized LLMs, called GIFT-SW. It is designed to fine-tune weights in salient columns while injecting Gaussian noise into non-salient weights, which are kept frozen during training.\n\u2022 We generalize sensitivity metrics for identifying salient columns in pre-trained LLMs. We compare various novel and existing instances of the proposed general form and identify a new metric, which on average outperform previously studied in the literature metrics(Xiao et al., 2023; Lee et al., 2024).\n\u2022 Experiments demonstrate that GIFT-SW outperforms modern PEFT methods and full fine-tuning baselines across most zero-shot tasks. GIFT-SW for LLAMA models achieve comparable accuracy to the corresponding state-of-the-art T\u00dcLU2 models, despite fine-tuning only 3% of the parameters and utilizing ten times less computational resources.\n\u2022 We demonstrate that GIFT-SW is more stable with respect to a size of training set compared with low-rank adapters."}, {"title": "Related Work", "content": "One of the most popular method with high efficiency is LORA (Hu et al., 2021), which trains the low-rank adapters. Recent modifications to the method aim to improve the initialization of the adapters (Liu et al., 2024) and enhance the low-rank representation of pre-trained weights by adding sparse adapters (Nikdan et al., 2024). Another improvement of the learning capacity of LoRA is given by DORA (Liu et al., 2024), which fine-tunes magnitude and direction components of the pre-trained weights. This method achieves considerable performance across various fine-tuning tasks."}, {"title": "Salient Weights in LLMS", "content": "The identification of salient weights\u00b9 is one of the main problems in weight pruning. Recently, several approaches have been proposed to identify such weights in LLMs, including SparseGPT (Frantar and Alistarh, 2023), Wanda (Sun et al., 2023), and OWL (Yin et al., 2023).\nDettmers et al.'s (2022) demonstrated that a small subset of outliers in input activations has a substantial impact on LLM performance, highlighting the relationship between the activation outliers and the salient weights. Many subsequent Post-Training Quantization (PTQ) methods used similar or identical pruning metrics to identify these salient weights (Dettmers et al., 2023; Xiao et al., 2023; Lee et al., 2024)."}, {"title": "Structured and Non-structured Salient Weights selection", "content": "Since salient weights account for only a few percent of all the weights, a straightforward approach to preserve them would be to store unstructured salient weights in a sparse matrix. (Dettmers et al., 2023) demonstrated that this approach is computationally reasonable and leads to performance improvement. On the other hand, Xiao et al.'s (2023) revealed that outliers in activations are confined to a small fraction of weight channels, which was incorporated into SmoothQuant, where outlier columns are identified using a small calibration dataset. This concept is further developed in QUIK (Ashkboos et al., 2023), where outlier columns are retained in full precision, while other columns are quantized using GPTQ (Frantar et al., 2022). A similar procedure is used in OWQ (Lee et al., 2024), but with an OBD-based metric (LeCun et al., 1989).\nDue to the lack of results in the literature on which approach brings better results, structured or unstructured salient weight selection, and motivated by computational efficiency mentioned in (Ashkboos et al., 2023), in our work we follow the second line of work with structured column-wise salient weight selection."}, {"title": "Noise Injections", "content": "In this section, we briefly describe Gaussian Noise Injections (GNI) and its benefits. Then, we show that the approximation of quantization noise and GNI are identical. Therefore, GNI can also benefit further model quantization. Therefor, to examine our third question, we sample noise relative to quantization levels, leaving other sampling options for future work.\nPerturbed Gradient Descent (PGD) is a family of methods that involve adding or multiplying weights with samples from some random distribution, during an optimization procedure. Gaussian noise injection (GNI) after the gradient step helps model to escape saddle points efficiently in non-convex optimization (Jin et al., 2021). However, when Gaussian noise is injected before the gradient step, it helps model to escape from the spurious local optimum (Zhu et al., 2018).\n\\begin{equation}\n0_{t+1} \\leftarrow \\theta_t - \\tau(\\nabla f(0_t) + \\S)\n\\end{equation}\n\\begin{equation}\n0_{t+1} \\leftarrow \\theta_t - \\tau(\\nabla f(\\theta_t + \\S))\n\\end{equation}\n\\begin{equation}\n\\xi ~ N(\\mu, \\sigma^2)\n\\end{equation}\nMoreover, practical benefits of noise injections are well documented in the literature and often can be discussed as regularization techniques (Bishop, 1995; Srivastava et al., 2014; Camuto et al., 2020), methods to prompt adversarial robustenss (Panda and Roy, 2021) and to be used for data agumentation (Moreno-Barea et al., 2018).\nIn our work we use GNI before evaluating the gradient. For this scenario, Orvieto et al. (2023) proposed to add noise only to one layer at training iteration to avoid variance explosion. It was empirically and theoretically demonstrated that GNI serves as a regularization. Liu et al. (2023) study fine-tuning of pre-trained Language Models with GNI. Authors propose first to learn layer-wise variance parameters for noise distributions and then to fine-tune the model by adding noise to all the weights. The obtained results showed that the approach is superior to independent layer-wise noise injections.\nQuantization aware training (QAT) of networks is applied to mitigate their accuracy degradation after quantization. However, uniform quantization \u00b2 Q is a non-differentiable operation. For simplicity, it can be expressed as a composition of scaling and rounding operations, Q(W) = \u2206[W]. In terms of QAT operation Q can be efficiently approximated with quantization noise \u03a9 such that \u03a9 = Q(W) \u2013 W (D\u00e9fossez et al., 2021; Shvetsov et al., 2022; Shin et al., 2023). Thus, training models with QNI is exactly the same as employing PGD with GNI before evaluating the gradient.\nUnder some assumptions the noise \u03a9 induced by uniform quantization can often be modeled by an additive noise that is uniformly distributed, uncorrelated with the input signal, and has a white spectrum (Widrow et al., 1996). However in practice, the conditions are often not satisfied. Therefore employing Gaussian distribution N(\u03bc, \u03c3\u00b2) for \u03a9 typically yields improved outcomes (D\u00e9fossez et al., 2021; Shvetsov et al., 2022).\nAlthough GNI is beneficial for model training there is no clear answer on how to choose noise"}, {"title": "Straight Through Estimator", "content": "The most popular QAT technique incorporating quantization operation into the traning process is Straight Through Estimation (STE)\u00b3 (Bengio et al., 2013; Shang et al., 2023), which basically re-parameterizes gradients. However, D\u00e9fossez et al.'s (2021) demonstrated that STE has some disadvantages compared with QNI\u2074, as STE is biased and may cause weight oscillation between quantization steps. Shin et al.'s (2023) demonstrated that pre-training models for the following quantization with QNI instead of STE results in better performance. More technical details are provided in Section C."}, {"title": "Method", "content": "GIFT-SW consists of the following steps:\n(1) Identify a fixed number of salient columns using a chosen sensitive metric, based on a small calibration set. This number remains consistent across all layers.\n(2) Split columns of the matrices into subsets of salient columns and regular ones.\n(3) During training, add noise to the weights in non-salient columns and update weights only in the salient columns.\nThus, the method depends on two main design choices: 1) how to choose salient columns and 2) the parameters of noise injections. We cover the choice of metrics in Section 3.1. Noise injection details are provided in Section 3.2."}, {"title": "Generalizing parameter sensitivity metrics", "content": "Several approaches have been proposed recently to identify weights sensitive to quantization (Dettmers et al., 2023) or pruning (Sun et al., 2023). We generalize them as metrics for sensitivity to perturbations, and by applying these metrics, we determine which columns are more susceptible to degradation. Therefore, we avoid adding noise to such columns and use them to fine-tune the model.\nThe proposed sensitivity metric is written for a column j of weight matrix W as\n\\begin{equation}\nS_j = ||D_j||\\gamma ||X_j ||,\\rho\n\\end{equation}\nwhere Dj is a measure of weights perturbation, s\u2c7c denotes sensitivity of the column to perturbations, X is the input feature, and \u03b3 takes on one of the following values 1/2, 1, 2. As discussed in Section 2.4 we could apply GNI as a source of perturbations, then we would compute Dj = W:,j+\u03be. However, sampling noise \u03be is not deterministic. To approximate an influence of the noise \u03be we utilize perturbations caused by quantization. That would lead to Dj = W:,j \u2013 Q(W:,j), where Q(W:,j) corresponds to the weights subjected to uniform symmetric quantization (see Appendix A).\nThe input feature X for each layer is computed using a number of random sentences from a calibration dataset. After that, sensitivity values sj are estimated for individual columns. Columns with the highest values are identified as the salient columns. Some details about the calibration dataset is described in Section 4.1."}, {"title": "Quantization Noise Injection", "content": "To improve our fine-tuning procedure with QNI, we avoid applying perturbations to sensitive weights. Therefore, after identifying columns that are sensitive to perturbations or salient during the fine-tuning stage, we inject quantization noise only into non-salient columns across all layers, as shown in Figure 2.\nThe scale parameters of the Gaussian noise are determined by the quantization step sizes, which are computed for each layer prior to the training process.\nFor the weight matrix W of a given layer in the model, the process of noise injection can be described as follows. During each forward pass in the training phase, we first sample elements of noise matrix \u03a9 from standard normal distribution N(0,1). Subsequently, the matrix \u03a9 is scaled with the quantization step size \u2206. Finally, we add scaled noise to weights of non-salient columns W[:,non-salient]. The operation of the noise injection is given as\n\\begin{equation}\n\u0e0a(W)\n=\\begin{cases}\nW [:,salient],\\\\\nW[:,non-salient] + diag(\\triangle)\\Omega\\\\\n\\end{cases}\n\\end{equation}\nwhere diag(\u2206) is the diagonal matrix with elements of the vector \u2206.\nOnly weights of the salient columns W[:,salient] are updated during training, whereas weights of other columns W[:,non-salient] are frozen. We do not inject noise to salient weights since small perturbations in them can cause high model degradation.\nThe quantization step size \u2206 is determined only for weights in non-salient columns W[:,non-salient]. To closer match the initial distribution of the weights, quantization scale factors including in \u2206 are estimated for each row individually. For i-s row the scale factor \u2206\u1d62 is computed as:\n\\begin{equation}\n\\Delta_i = \\frac{a_i}{2^{b-1}-1},\n\\end{equation}\nwhere b is the bit-width and a\u1d62 is the quantization parameter. As in quantization methods, smaller bit-width b corresponds to higher quantization noise. The parameter a\u1d62 is estimated by optimizing weight error through linear search as discussed in Appendix A.\nBased on Equations 5 and 6, the variance of the injected noise is determined by the distribution of non-salient weights across rows. We exclude salient columns from this distribution, as the salient weights may induce large quantization error and distort row-wise scale factors. This approach helps us to minimize the noise variance, which, in turn, leads to a reduction in the deviation of the non-salient weights during training.\nBy sampling noise in such way we can use it for quantization pre-training experiments discussed in Section 6.3."}, {"title": "Experiments", "content": "In this section, we describe the experimental procedure used to test the performance of GIFT-SW compared to others."}, {"title": "Data", "content": "Following previous studies (Nikdan et al., 2024; Hu et al., 2021; Liu et al., 2024), we focus on the instruction tuning task. For this purpose, we use the TULU-V2-Mix as the main source of data (Ivison et al., 2023), as it encompasses a wide range of instructions from different sources. This dataset has been filtered, contains a substantial amount of data without being too large, and models tuned to this set show superior performance. Additionally, we utilize the OpenOrca dataset (Mukherjee et al., 2023) to demonstrate that our method does not depend on a specific set of instructions.\nThe sensitivity metrics to find salient columns are estimated based on 512 random sentences from the Pile validation dataset (Xiao et al., 2023)."}, {"title": "Baselines", "content": "We consider several baselines for both full precision and quantized experiments. All baselines are applied to LLaMA2-7b, LLaMA2-13b and LLaMA3-8b.\nFull precision version includes the choice of baselines, following recent studies (Liu et al., 2024; Nikdan et al., 2024). We employ:\n\u2022 LoRA is a widely used adapter-based method (Hu et al., 2021)\n\u2022 DORA is modification of LoRA outperforming all current PEFT methods (Liu et al., 2024)\n\u2022 FT is full fine-tuning of all parameters\nWe do not include PEFT methods connected with prompt tuning, as they show worse performance compared to adapter-based methods (Xu et al., 2023)."}, {"title": "Evaluation and Datasets", "content": "We perform a comprehensive evaluation measuring zero-shot performance on HellaSwag (Zellers et al., 2019), BoolQ (Clark et al., 2019), WinoGrande (Sakaguchi et al., 2021), PiQA (Tata and Patel, 2003), ARC-easy, and ARC-challenge (Clark et al., 2018) using the LM Eval Harness (Gao et al., 2023). The choice of baselines is similar to those in previous studies (Egiazarian et al., 2024; Frantar et al., 2022; van Baalen et al., 2024).\nWe demonstrate average accuracy across all the datasets, detailed per-dataset comparison can be found in Section D."}, {"title": "Compute Budget", "content": "In all experiments, the number of salient columns in the models is fixed at 128. Furthermore, we fix our training budget at 500 training iterations, unless specified otherwise. According to a recent study (Komatsuzaki, 2019), it is more effective to train for one epoch with a larger dataset rather than multiple epochs with less data. Therefore, all 500 iterations are performed within one epoch with no instruction repetitions."}, {"title": "Training Details", "content": "The training was performed with 4 GPUs (40 GB each) for 500 iterations. The batch size is 128 for 7b models and 64 for 13b models. For baseline methods, the learning rate was set to 3 \u00d7 10-5 for LLaMA2 models and to 1 \u00d7 10-5 for the LLaMA3 model. We experimented with different learning rates and found these to be the most beneficial for baseline methods. We used a cosine annealing scheduler with the warmup ratio of 0.03. The LORA and DORA alpha and dropout values were as specified in the original papers, and the rank was set to 64 to match the number of trainable parameters in our method. Thus, the number of trainable parameters is 160M for LLaMA2-7b, 250M for LLaMA2-13b, 167M for LLaMA3-8b.\nFor our method, the learning rate was set to 1 \u00d7 10-4 for salient columns of LLaMA2 models and to 1 \u00d7 10-5 of the LLaMA3 model. We fixed the number of salient columns at 128, such that the number of trainable parameters is 174M for LLaMA2-7b, 272M for LLaMA2-13b, and 176M for LLaMA3-8b.\nIn the case of full fune-tuning with the noise injection, the learning rate was set to 3 \u00d7 10-5 and 1 \u00d7 10-5 for LLaMA2 & 3 models, correspondingly."}, {"title": "Results", "content": "In this section, we present the results of our computational experiments and answer the questions posed in Section 1. In short, our results are as follows:\nQ1: The results confirm that fine-tuning a subset of salient weights produces results comparable to those obtained using low-rank adapters.\nQ2: Noise injections lead to improved model performance.\nQ3: We could not confirm that models trained with noise injections are more robust to further degradation."}, {"title": "Full Precision", "content": "The average performance across evaluation benchmarks for full precision models is presented in Table 1. GIFT-SW generally shows superior metrics across most models and instruction sets. However, we observe slight underperformance in LLaMA3 on the OpenOrca subset, where full training proves superior. This issue likely stems from the choice of learning rate and schedule, which can impact the tuning of outliers."}, {"title": "Quantized Models", "content": "We present the averaged performance of models quantized with different precision (4, 3, 2) in Table 2. For 4 and 3 bits GIFT-SW achieves comparable quality with STE, however, latter one requires significantly more compute. In the 2-bit setting, GIFT-SW shows a substantial quality improvement, surpassing the second-ranked model by over 5 points."}, {"title": "Comparison with T\u00dcLU2", "content": "We compare GIFT-SW with T\u00dcLU2 models (Ivison et al., 2023), which are LLaMA2 models fine-tuned using a combination of instructions and DPO (Rafailov et al., 2023). These models are among the top-performing LLaMA2 modifications but demand significant computational resources.\nIn Table 3, we show that by applying GIFT-SW with significantly lower computational budget (a smaller number of parameters and iterations) we achieve comparable results for LLaMA2-7b and outperform T\u00dcLU2 for 13b."}, {"title": "Scaling Properties", "content": "We perform experiments to explore the performance of GIFT-SW and baselines with scaling data using LLaMA2 and LLaMA3 models. The results reported in Figure 1 show that while LoRA and DORA exhibit unstable performance with scaling data, our method and full fine-tuning are more stable. Moreover, our method consistently ranks first across nearly all data budgets."}, {"title": "Ablation", "content": "We study sensitivity metrics with respect to different noise levels (various perturbation magnitudes), which translate into varying quantization precision. In this experiment, the non-salient weights of LLaMA2 and T\u00dcLU2 with 7B and 13B parameters. Models are quantized with QUIK, the salient weights are not updated. We select 128 columns of salient weights.\nMean results for zero-shot tasks in Table 5 show that for most precisions, the best performance is achieved with salient columns identified by Equation 4 with \u03b3 = 1, \u03c1 = \u221e, \u03c4 = \u221e (second column). Columns identified by the squared l2 norm of the input feature (the OWQ metric) show better performance only for T\u00dcLU2 quantized to 3 and 2 bits. Choosing salient columns solely by the input features (the QUIK metric) leads to underperformance, especially for 2 bit. Therefore, identifying salient columns sensitive to quantization noise requires considering both the weight quantization error and the maximum values of input activation. Based on the results, we chose the best-performing sensitivity metric with \u03b3 = 1, \u03c1 = \u221e, \u03c4 = \u221e. However, the results do not reveal a clear rule for selecting the optimal sensitivity metric, as performance varies across different bit-widths and models with no discernible pattern. This remains an area for future research."}, {"title": "Noise Injection Impact", "content": "To ablate the importance of QNI in the full-precision setting, we measure the mean performance of LLaMA2 models with and without noise injections for both salient columns fine-tuning and full fine-tuning. In the latter case, the noise is applied to the entire weight matrix.\nThe results in Table 6 show that QNI consistently enhances the performance of outlier fine-tuning. Although QNI can reduce performance when applied to the entire network, it still benefits LLaMA3-8b. Notably, outlier fine-tuning outperforms full fine-tuning, but only when QNI is used."}, {"title": "Quantization Before and After Training", "content": "From studies related to QAT, it is known that pre-training a model with noise injection enables to improve its predictive capabilities after quantization (D\u00e9fossez et al., 2021; Shvetsov et al., 2022). Based on those observations, in this section we examine the performance of the quantized LLaMA2-7b after fine-tuning full precision salient columns in several settings:\n\u2022 Pre-GIFT-SW. Applying GIFT-SW prior to the quantization.\n\u2022 Post-GIFT-SW. Applying GIFT-SW after the quantization.\n\u2022 Salient FT. Fine-tuning salient columns after quantization with no noise injected\nIn the case of the pre-training, the bit-width for the model quantization corresponds to the noise level injected during the training. For the post-training, the noise injection is always performed at 4 bit.\nTable 4 presents the average scores achieved by the models across evaluation benchmark. In the case of 4 bit quantization the Pre-GIFT-SW model considerable outperforms other models. But in the case of 3 and 2 bits, fine-tuning salient columns after quantization enables to achieve quantized models better generative capabilities.\nIt can be explained by significant deviation of the quantized weights from their original values that is induced by the extremely low-bit quantization. As a result, the interrelations between the salient weights and the quantized weights are disrupted, and the positive effect of pre-training disappears. However, post-training of the salient weight enables to form them new relations with other weights, so the model partially recovers its generative capabilities.\nAlso it can be observed that application of Post-GIFT-SW and Salient FT to model quantized in 3 bit gives the similar scores. But in the case of 2 bit quantization, the noise injection improves the fine-tuning of the quantized model."}, {"title": "Conclusion", "content": "In this paper, we introduce GIFT-SW, a parameter-efficient fine-tuning method that trains only weights in a small subset of salient columns while injecting quantization noise into the frozen weights. GIFT-SW proves to be superior to previous fine-tuning strategies in both full precision and quantized settings, requiring less compute budget. In data scaling experiments, GIFT-SW demonstrates greater stability than previous PEFT methods and outperforms both PEFT and full fine-tuning across nearly all data budgets. Our ablation studies show that QNI is beneficial but only with salient weights. Although GIFT-SW outperforms previous methods, further research is needed to determine how to maximize its performance in quantized settings.\nWe generalize the criterion for selecting salient columns from previous studies and empirically compare various parameters. Our experiments show that while some criteria perform better than others, none emerge as a clear dominant choice. This significant finding underscores the need for further research to refine these criteria."}, {"title": "Limitations", "content": "We find the main limitations of our work as follows:\n1. We report results of GIFT-SW exclusively for LLaMA models. Currently, numerous open-source pre-trained LLMs with high generative capabilities are available. However, LLaMA models are the most commonly chosen for studying the efficiency of modern PEFT and quantization methods. Despite the architectural similarities among most LLMs, future experiments with different models are necessary.\n2. For quantizing models, we use only the GPTQ method, which is widely used for mixed-precision quantization of LLMs. This method improves the performance of quantized models by aggregating quantization error into columns stored in full precision. However, GIFT-SW can be easily integrated with other methods, such as conventional RTN or QuantEase.\n3. Experiments with GIFT-SW report results for salient columns selected using the sensitivity metric (4) with \u03b3 = 1. Our proposed metric, based on our analysis, shows high sensitivity of the salient columns to quantization in most LLaMA2 cases. However, other sensitivity metrics may yield better performance for GIFT-SW and mixed-precision quantization in different LLMs.\n4. Noise parameters for fine-tuning the salient weights are determined using the QNI approach. However, other noise distributions may also enhance the fine-tuning process. Identifying the optimal noise distribution is beyond the scope of this paper.\n5. In this study, we focus on developing the GIFT-SW algorithm for effective fine-tuning of LLMs, but we do not provide computationally efficient implementations of CUDA kernels for the algorithm. In the future, CUDA kernels for GIFT-SW can be developed based on the code from QUIK (Ashkboos et al., 2023) and OWQ (Lee et al., 2024).\n6. We train GIFT-SW with only a few fine-tuning instruction sets, selected for their size and high benchmark results in previous studies. However, expanding the number of fine-tuning sets could make the experiments more comprehensive.\n7. We evaluate our method using six distinct benchmarks inherited from various previous studies. In future research, it would be beneficial to include more benchmarks to gain additional insights."}, {"title": "Potential Risks", "content": "The GIFT-SW method poses risks similar to those of any PEFT method. For example, it omits explicit safety training measures, so could be applied to fine-tune LLMs for generating harmful content. Also it can be applied to tailor LLMs to tailor highly specific and potentially dangerous outputs."}]}