{"title": "GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMS", "authors": ["Maxim Zhelnin", "Viktor Moskvoretskii", "Egor Shvetsov", "Egor Venediktov", "Mariya Krylova", "Aleksandr Zuev", "Evgeny Burnaev"], "abstract": "Parameter Efficient Fine-Tuning (PEFT) methods have gained popularity and democratized the usage of Large Language Models (LLMs). Recent studies have shown that a small subset of weights significantly impacts performance. Based on this observation, we introduce a novel PEFT method, called Gaussian noise Injected Fine Tuning of Salient Weights (GIFT-SW). Our method updates only salient columns, while injecting Gaussian noise into non-salient ones. To identify these columns, we developed a generalized sensitivity metric that extends and unifies metrics from previous studies. Experiments with LLaMA models demonstrate that GIFT-SW outperforms full fine-tuning and modern PEFT methods under the same computational budget. Moreover, GIFT-SW offers practical advantages to recover performance of models subjected to mixed-precision quantization with keeping salient weights in full precision. Code is available in our repository.", "sections": [{"title": "1 Introduction", "content": "Modern LLMs demonstrate remarkable generalization capabilities on unseen tasks. However, fine-tuning remains crucial to enhance these models performance or to restore the performance after compression techniques like quantization (Dettmers et al., 2024; Moskvoretskii et al., 2024), pruning (Frantar and Alistarh, 2023; Kim et al., 2023), or tensor decomposition have been applied. Given the large scale of modern LLMs, fine-tuning all parameters can be computationally and memory-intensive. To overcome this challenge, Parameter Efficient Fine-Tuning schemes have been developed, aimed to improve model performance while using limited computational and memory resources.\nTo date, PEFT methods have not matched the accuracy of full fine-tuning (Nikdan et al., 2024), highlighting the need for new approaches that can close this gap while still minimizing resource use. Additionally, most PEFT methods involve adding extra parameters, which increases computational demands.\nTo address those issues and enhance the performance of efficiently trained LLMs, we introduce a novel PEFT method, GIFT-SW. This approach focuses on updating a small subset of salient weights while injecting noise into the non-salient weights. The development of this method is grounded in observations from previous studies and the related questions they raise, which we aim to answer:\nPrevious research has shown that there is a small subset of salient weights which can significantly affect the effectiveness of post-training quantization (PTQ) (Dettmers et al., 2022, 2023; Kim et al., 2023) and pruning techniques (Yin et al., 2023; Frantar and Alistarh, 2023; Sun et al., 2023). Moreover, Gurnee et al. identified a group of \"universal neurons\" that are critical to a model's functionality, emphasizing the importance of selecting and updating these salient weights. Question 1: Does updating a small subset of salient weights is sufficient to adjust the model?\nRecent studies have demonstrated that Perturbed Gradient Descent (PGD), with noise injections applied both before and after the gradient step, can stabilize convergence and help prevent overfitting (Poole et al., 2014; Zhu et al., 2018; Jin et al., 2021). Question 2: Does Injecting Noise helps convergence?\nPGD is commonly employed to enhance model robustness by approximating the quantization process (Shvetsov et al., 2022; Shin et al., 2023; D\u00e9fossez et al., 2021). This increased robustness can aid in maintaining the quality of the quantized model. Question 3: Does injecting noise helps robustness?\nSelecting salient weights is a significant challenge, particularly in quantization and pruning, and"}, {"title": "2 Related Work", "content": "2.1\nParameter efficient fine-tuning of LLM\nOne of the most popular method with high efficiency is LORA (Hu et al., 2021), which trains the low-rank adapters. Recent modifications to the method aim to improve the initialization of the adapters (Liu et al., 2024) and enhance the low-rank representation of pre-trained weights by adding sparse adapters (Nikdan et al., 2024). Another improvement of the learning capacity of LoRA is given by DORA (Liu et al., 2024), which fine-tunes magnitude and direction components of the pre-trained weights. This method achieves considerable performance across various fine-tuning tasks.\n2.2 Salient Weights in LLMS\nThe identification of salient weights\u00b9 is one of the main problems in weight pruning. Recently, several approaches have been proposed to identify such weights in LLMs, including SparseGPT (Frantar and Alistarh, 2023), Wanda (Sun et al., 2023), and OWL (Yin et al., 2023).\nDettmers et al.'s (2022) demonstrated that a small subset of outliers in input activations has a substantial impact on LLM performance, highlighting the relationship between the activation outliers and the salient weights. Many subsequent Post-Training Quantization (PTQ) methods used similar or identical pruning metrics to identify these salient weights (Dettmers et al., 2023; Xiao et al., 2023; Lee et al., 2024)."}, {"title": "2.3 Structured and Non-structured Salient Weights selection", "content": "Since salient weights account for only a few percent of all the weights, a straightforward approach to preserve them would be to store unstructured salient weights in a sparse matrix. (Dettmers et al., 2023) demonstrated that this approach is computationally reasonable and leads to performance improvement. On the other hand, Xiao et al.'s (2023) revealed that outliers in activations are confined to a small fraction of weight channels, which was incorporated into SmoothQuant, where outlier columns are identified using a small calibration dataset. This concept is further developed in QUIK (Ashkboos et al., 2023), where outlier columns are retained in full precision, while other columns are quantized using GPTQ (Frantar et al., 2022). A similar procedure is used in OWQ (Lee et al., 2024), but with an OBD-based metric (LeCun et al., 1989).\nDue to the lack of results in the literature on which approach brings better results, structured or unstructured salient weight selection, and motivated by computational efficiency mentioned in (Ashkboos et al., 2023), in our work we follow the second line of work with structured column-wise salient weight selection."}, {"title": "2.4 Noise Injections", "content": "In this section, we briefly describe Gaussian Noise Injections (GNI) and its benefits. Then, we show that the approximation of quantization noise and GNI are identical. Therefore, GNI can also benefit further model quantization. Therefor, to examine our third question, we sample noise relative to quantization levels, leaving other sampling options for future work.\nGaussian Noise Injections (GNI). Perturbed Gradient Descent (PGD) is a family of methods that involve adding or multiplying weights with samples from some random distribution, during an optimization procedure. Gaussian noise injection (GNI) after the gradient step helps model to escape saddle points efficiently in non-convex optimization (Jin et al., 2021). However, when Gaussian noise is injected before the gradient step, it helps model to escape from the spurious local optimum (Zhu et al., 2018).\n$\\theta_{t+1} \\leftarrow \\theta_t - \\tau(\\nabla f(\\theta_t) + \\xi)$ (1)\n$\\theta_{t+1} \\leftarrow \\theta_t - \\tau(\\nabla f(\\theta_t + \\xi))$ (2)\n$\\xi \\sim \\mathcal{N}(\\mu, \\sigma^2)$ (3)\nMoreover, practical benefits of noise injections are well documented in the literature and often can be discussed as regularization techniques (Bishop, 1995; Srivastava et al., 2014; Camuto et al., 2020), methods to prompt adversarial robustenss (Panda and Roy, 2021) and to be used for data agumentation (Moreno-Barea et al., 2018).\nIn our work we use GNI before evaluating the gradient. For this scenario, Orvieto et al. (2023) proposed to add noise only to one layer at training iteration to avoid variance explosion. It was empirically and theoretically demonstrated that GNI serves as a regularization. Liu et al. (2023) study fine-tuning of pre-trained Language Models with GNI. Authors propose first to learn layer-wise variance parameters for noise distributions and then to fine-tune the model by adding noise to all the weights. The obtained results showed that the approach is superior to independent layer-wise noise injections.\nQuantization Noise Injections (QNI). Quantization aware training (QAT) of networks is applied to mitigate their accuracy degradation after quantization. However, uniform quantization\u00b2 Q is a non-differentiable operation. For simplicity, it can be expressed as a composition of scaling and rounding operations, Q(W) = \u0394[W]. In terms of QAT operation Q can be efficiently approximated with quantization noise \u03a9 such that \u03a9 = Q(W) \u2013 W (D\u00e9fossez et al., 2021; Shvetsov et al., 2022; Shin et al., 2023). Thus, training models with QNI is exactly the same as employing PGD with GNI before evaluating the gradient.\nUnder some assumptions the noise \u03a9 induced by uniform quantization can often be modeled by an additive noise that is uniformly distributed, uncorrelated with the input signal, and has a white spectrum (Widrow et al., 1996). However in practice, the conditions are often not satisfied. Therefore employing Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ for \u03a9 typically yields improved outcomes (D\u00e9fossez et al., 2021; Shvetsov et al., 2022).\nAlthough GNI is beneficial for model training there is no clear answer on how to choose noise"}, {"title": "2.5 Straight Through Estimator", "content": "The most popular QAT technique incorporating quantization operation into the traning process is Straight Through Estimation (STE)\u00b3 (Bengio et al., 2013; Shang et al., 2023), which basically re-parameterizes gradients. However, D\u00e9fossez et al.'s (2021) demonstrated that STE has some disadvantages compared with QNI\u2074, as STE is biased and may cause weight oscillation between quantization steps. Shin et al.'s (2023) demonstrated that pre-training models for the following quantization with QNI instead of STE results in better performance. More technical details are provided in Section C."}, {"title": "3 Method", "content": "GIFT-SW consists of the following steps:\n(1) Identify a fixed number of salient columns using a chosen sensitive metric, based on a small calibration set. This number remains consistent across all layers.\n(2) Split columns of the matrices into subsets of salient columns and regular ones.\n(3) During training, add noise to the weights in non-salient columns and update weights only in the salient columns.\nThus, the method depends on two main design choices: 1) how to choose salient columns and 2) the parameters of noise injections. We cover the choice of metrics in Section 3.1. Noise injection details are provided in Section 3.2."}, {"title": "3.1 Generalizing parameter sensitivity metrics", "content": "Several approaches have been proposed recently to identify weights sensitive to quantization (Dettmers et al., 2023) or pruning (Sun et al., 2023). We generalize them as metrics for sensitivity to perturbations, and by applying these metrics, we determine which columns are more susceptible to degradation. Therefore, we avoid adding noise to such columns and use them to fine-tune the model.\n$S_j = ||D_j ||_\\gamma ||X_j ||_\\rho,$\n(4)\nwhere $D_j$ is a measure of weights perturbation, $S_j$ denotes sensitivity of the column to perturbations, $X$ is the input feature, and $y$ takes on one of the following values 1/2, 1, 2. As discussed in Section 2.4 we could apply GNI as a source of perturbations, then we would compute $D_j = W_{:,j} + \\xi$. However, sampling noise $\\xi$ is not deterministic. To approximate an influence of the noise $\\xi$ we utilize perturbations caused by quantization. That would lead to $D_j = W_{:,j} \u2013 Q(W_{:,j})$, where $Q(W_{:,j})$ corresponds to the weights subjected to uniform symmetric quantization (see Appendix A).\nThe input feature $X$ for each layer is computed using a number of random sentences from a calibration dataset. After that, sensitivity values $s_j$ are estimated for individual columns. Columns with the highest values are identified as the salient columns. Some details about the calibration dataset is described in Section 4.1."}, {"title": "3.2 Quantization Noise Injection", "content": "To improve our fine-tuning procedure with QNI, we avoid applying perturbations to sensitive weights. Therefore, after identifying columns that are sensitive to perturbations or salient during the fine-tuning stage, we inject quantization noise only into non-salient columns across all layers, as shown in Figure 2.\nThe scale parameters of the Gaussian noise are determined by the quantization step sizes, which are computed for each layer prior to the training process.\nFor the weight matrix $W$ of a given layer in the model, the process of noise injection can be described as follows. During each forward pass in the training phase, we first sample elements of noise matrix \u03a9 from standard normal distribution $\\mathcal{N}(0,1)$. Subsequently, the matrix \u03a9 is scaled with the quantization step size \u0394. Finally, we add scaled noise to weights of non-salient columns $W[:,non-salient]$. The operation of the noise injection U is given as\n$\\mho(W) = \\begin{cases} W [:,salient], \\\\ W[:,non-salient] + diag(\\triangle)\\Omega, \\end{cases}$,\n(5)\nwhere $diag(\\triangle)$ is the diagonal matrix with elements of the vector \u0394.\nOnly weights of the salient columns $W[:,salient]$ are updated during training, whereas weights of other columns $W[:,non-salient]$ are frozen. We do not inject noise to salient weights since small perturbations in them can cause high model degradation.\nThe quantization step size \u0394 is determined only for weights in non-salient columns $W[:,non-salient]$. To closer match the initial distribution of the weights, quantization scale factors including in A are estimated for each row individually. For i-s row the scale factor \u0394\u2081 is computed as:\n$\\triangle_i = \\frac{a_i}{2^{b-1}-1}$,\n(6)\nwhere b is the bit-width and $a_i$ is the quantization parameter. As in quantization methods, smaller bit-width b corresponds to higher quantization noise. The parameter a\u017e is estimated by optimizing weight error through linear search as discussed in Appendix A.\nBased on Equations 5 and 6, the variance of the injected noise is determined by the distribution of non-salient weights across rows. We exclude salient columns from this distribution, as the salient weights may induce large quantization error and distort row-wise scale factors. This approach helps us to minimize the noise variance, which, in turn, leads to a reduction in the deviation of the non-salient weights during training.\nBy sampling noise in such way we can use it for quantization pre-training experiments discussed in Section 6.3."}, {"title": "4 Experiments", "content": "In this section, we describe the experimental procedure used to test the performance of GIFT-SW compared to others.\n4.1 Data\nFollowing previous studies (Nikdan et al., 2024; Hu et al., 2021; Liu et al., 2024), we focus on the instruction tuning task. For this purpose, we use the TULU-V2-Mix as the main source of data (Ivison et al., 2023), as it encompasses a wide range of instructions from different sources. This dataset has been filtered, contains a substantial amount of"}, {"title": "4.2 Baselines", "content": "We consider several baselines for both full precision and quantized experiments. All baselines are applied to LLaMA2-7b, LLaMA2-13b and LLaMA3-8b.\nFull precision version includes the choice of baselines, following recent studies (Liu et al., 2024; Nikdan et al., 2024). We employ:\n\u2022 LoRA is a widely used adapter-based method (Hu et al., 2021)\n\u2022 DORA is modification of LoRA outperforming all current PEFT methods (Liu et al., 2024)\n\u2022 FT is full fine-tuning of all parameters\nWe do not include PEFT methods connected with prompt tuning, as they show worse performance compared to adapter-based methods (Xu et al., 2023)."}, {"title": "4.3 Evaluation and Datasets", "content": "We perform a comprehensive evaluation measuring zero-shot performance on HellaSwag (Zellers et al., 2019), BoolQ (Clark et al., 2019), Wino-Grande (Sakaguchi et al., 2021), PiQA (Tata and Patel, 2003), ARC-easy, and ARC-challenge (Clark et al., 2018) using the LM Eval Harness (Gao et al.,"}, {"title": "4.4 Compute Budget", "content": "In all experiments, the number of salient columns in the models is fixed at 128. Furthermore, we fix our training budget at 500 training iterations, unless specified otherwise. According to a recent study (Komatsuzaki, 2019), it is more effective to train for one epoch with a larger dataset rather than multiple epochs with less data. Therefore, all 500 iterations are performed within one epoch with no instruction repetitions."}, {"title": "4.5 Training Details", "content": "The training was performed with 4 GPUs (40 GB each) for 500 iterations. The batch size is 128 for 7b models and 64 for 13b models. For baseline methods, the learning rate was set to 3 \u00d7 10-5 for LLaMA2 models and to 1 \u00d7 10-5 for the LLaMA3 model. We experimented with different learning rates and found these to be the most beneficial for baseline methods. We used a cosine annealing scheduler with the warmup ratio of 0.03. The LORA and DORA alpha and dropout values were as specified in the original papers, and the rank was set to 64 to match the number of trainable parameters in our method. Thus, the number of trainable parameters is 160M for LLaMA2-7b, 250M for LLaMA2-13b, 167M for LLaMA3-8b.\nFor our method, the learning rate was set to 1 \u00d7 10-4 for salient columns of LLaMA2 models and to 1 x 10-5 of the LLaMA3 model. We fixed the number of salient columns at 128, such that the number of trainable parameters is 174M for LLaMA2-7b, 272M for LLaMA2-13b, and 176M for LLaMA3-8b.\nIn the case of full fune-tuning with the noise injection, the learning rate was set to 3 \u00d7 10-5 and 1 \u00d7 10-5 for LLaMA2 & 3 models, correspondingly."}, {"title": "5 Results", "content": "In this section, we present the results of our computational experiments and answer the questions posed in Section 1. In short, our results are as follows:\nQ1: The results confirm that fine-tuning a subset of salient weights produces results comparable to those obtained using low-rank adapters.\nQ2: Noise injections lead to improved model performance.\nQ3: We could not confirm that models trained with noise injections are more robust to further degradation."}, {"title": "5.1 Full Precision", "content": "The average performance across evaluation benchmarks for full precision models is presented in Table 1. GIFT-SW generally shows superior metrics across most models and instruction sets. However, we observe slight underperformance in LLaMA3 on the OpenOrca subset, where full training proves superior. This issue likely stems from the choice of learning rate and schedule, which can impact the tuning of outliers."}, {"title": "5.2 Quantized Models", "content": "We present the averaged performance of models quantized with different precision (4, 3, 2) in Table 2. For 4 and 3 bits GIFT-SW achieves comparable quality with STE, however, latter one requires significantly more compute. In the 2-bit setting, GIFT-SW shows a substantial quality improvement, surpassing the second-ranked model by over 5 points."}, {"title": "5.3 Comparison with T\u00dcLU2", "content": "We compare GIFT-SW with T\u00dcLU2 models (Ivison et al., 2023), which are LLaMA2 models fine-tuned using a combination of instructions and DPO (Rafailov et al., 2023). These models are among the top-performing LLaMA2 modifications but demand significant computational resources.\nIn Table 3, we show that by applying GIFT-SW with significantly lower computational budget (a smaller number of parameters and iterations) we"}, {"title": "5.4 Scaling Properties", "content": "We perform experiments to explore the performance of GIFT-SW and baselines with scaling data using LLaMA2 and LLaMA3 models. The results reported in Figure 1 show that while LoRA and DORA exhibit unstable performance with scaling data, our method and full fine-tuning are more stable. Moreover, our method consistently ranks first across nearly all data budgets."}, {"title": "6 Ablation", "content": "6.1 Comparison sensitivity metrics\nWe study sensitivity metrics with respect to different noise levels (various perturbation magnitudes), which translate into varying quantization precision. In this experiment, the non-salient weights of LLaMA2 and T\u00dcLU2 with 7B and 13B parameters. Models are quantized with QUIK, the salient weights are not updated. We select 128 columns of salient weights.\nMean results for zero-shot tasks in Table 5 show that for most precisions, the best performance is achieved with salient columns identified by Equation 4 with \u03b3 = 1, \u03c1 = \u221e,\u03c4 = \u221e (second column). Columns identified by the squared 12 norm of the input feature (the OWQ metric) show better performance only for T\u00dcLU2 quantized to 3 and 2 bits. Choosing salient columns solely by the input features (the QUIK metric) leads to underperformance, especially for 2 bit. Therefore, identifying salient columns sensitive to quantization noise requires considering both the weight quantization error and the maximum values of input activation.\nBased on the results, we chose the best-performing sensitivity metric with \u03b3 = 1, \u03c1 = \u221e,\u03c4 = \u221e. However, the results do not reveal a clear rule for selecting the optimal sensitivity metric, as performance varies across different bit-widths and models with no discernible pattern. This remains an area for future research."}, {"title": "6.2 Noise Injection Impact", "content": "To ablate the importance of QNI in the full-precision setting, we measure the mean performance of LLaMA2 models with and without noise injections for both salient columns fine-tuning and full fine-tuning. In the latter case, the noise is applied to the entire weight matrix.\nThe results in Table 6 show that QNI consistently enhances the performance of outlier fine-tuning. Although QNI can reduce performance when applied to the entire network, it still benefits LLaMA3-8b. Notably, outlier fine-tuning outperforms full fine-tuning, but only when QNI is used."}, {"title": "6.3 Quantization Before and After Training", "content": "From studies related to QAT, it is known that pre-training a model with noise injection enables to improve its predictive capabilities after quantization (D\u00e9fossez et al., 2021; Shvetsov et al., 2022). Based on those observations, in this section we examine the performance of the quantized LLaMA2-7b after fine-tuning full precision salient columns in several settings:\n\u2022 Pre-GIFT-SW. Applying GIFT-SW prior to the quantization.\n\u2022 Post-GIFT-SW. Applying GIFT-SW after the quantization.\n\u2022 Salient FT. Fine-tuning salient columns after quantization with no noise injected\nIn the case of the pre-training, the bit-width for the model quantization corresponds to the noise level injected during the training. For the post-training, the noise injection is always performed at 4 bit.\nTable 4 presents the average scores achieved by the models across evaluation benchmark. In the case of 4 bit quantization the Pre-GIFT-SW model considerable outperforms other models. But in the case of 3 and 2 bits, fine-tuning salient columns after quantization enables to achieve quantized models better generative capabilities.\nIt can be explained by significant deviation of the quantized weights from their original values that is induced by the extremely low-bit quantization. As a result, the interrelations between the salient weights and the quantized weights are disrupted, and the positive effect of pre-training"}, {"title": "7 Conclusion", "content": "In this paper, we introduce GIFT-SW, a parameter-efficient fine-tuning method that trains only weights in a small subset of salient columns while injecting quantization noise into the frozen weights. GIFT-SW proves to be superior to previous fine-tuning strategies in both full precision and quantized settings, requiring less compute budget. In data scaling experiments, GIFT-SW demonstrates greater stability than previous PEFT methods and outperforms both PEFT and full fine-tuning across nearly all data budgets. Our ablation studies show that QNI is beneficial but only with salient weights. Although GIFT-SW outperforms previous methods, further research is needed to determine how to maximize its performance in quantized settings.\nWe generalize the criterion for selecting salient columns from previous studies and empirically compare various parameters. Our experiments show that while some criteria perform better than others, none emerge as a clear dominant choice. This significant finding underscores the need for further research to refine these criteria."}, {"title": "8 Limitations", "content": "We find the main limitations of our work as follows:\n1. We report results of GIFT-SW exclusively for LLaMA models. Currently, numerous open-source pre-trained LLMs with high generative capabilities are available. However, LLaMA models are the most commonly chosen for studying the efficiency of modern PEFT and quantization methods. Despite the architectural similarities among most LLMs, future experiments with different models are necessary.\n2. For quantizing models, we use only the GPTQ method, which is widely used for mixed-precision quantization of LLMs. This method improves the performance of quantized models by aggregating quantization error into columns stored in full precision. However, GIFT-SW can be easily integrated with other methods, such as conventional RTN or QuantEase."}, {"title": "9 Potential Risks", "content": "The GIFT-SW method poses risks similar to those of any PEFT method. For example, it omits explicit safety training measures, so could be applied to fine-tune LLMs for generating harmful content. Also it can be applied to tailor LLMs to tailor highly specific and potentially dangerous outputs."}, {"title": "10 Acknowledgment", "content": "The work was supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021)."}, {"title": "A Uniform quantization", "content": "While non-uniform quantization may lead to higher compression rates, in our work we focus on uniform quantization since it widely used in efficient PTQ methods such as GPTQ, QUIK, OWQ (Frantar et al., 2022; Ashkboos et al., 2023; Lee et al., 2024). Quantization is a mapping that converts a range of full-precision values into a discrete range of values allowing usage of integer arithmetic and reduced memory consumption. For example, Fig. 3 depicts a mapping with the quantization scale size \u0394 of float values from the interval (0, 1) into integer values.\nIn our work we apply uniform symmetric quantization with the row-wise quantization step size \u0394. In this case, computations of quantization, dequantization and estimation of \u0394 are performed for the bit-width b as below\n$q_{min} = -2^{b-1}, q_{max} = 2^{b-1} - 1$ (7)\n$clamp(x; q_{min}, q_{max}) = max(q_{min}, min(x, q_{max}))$ (8)\n$\\triangle = (\\triangle_1, ..., \\triangle_n)^T, \\triangle_i = \\frac{Wi,:}{Imax}$ (9)\n$w^{int} = clamp (\\frac{Wi,:}{\\triangle_i}; min, max)$ (10)\n$W\\approx Q(W) = diag(\\triangle)W^{int}$ (11)\nwhere \u25b3\u2081 is the scale factor for i row $W_{i,:}$, $W^{int}$ denotes the matrix of the quantized weights, $diag(\\triangle)$ is the diagonal matrix with elements of the vector \u0394. For the given bit-width b, the parameter a\u017e is found for each row by performing linear grid search over the interval [0, max($W_{i,:}$)], where max($W_{i,:}$) is the maximum element of i row . The search is conducted to minimize layer-wise mean squared error between weights:\n$\\argmin_Q||W \u2013 Q(W)||^2_2$, (12)"}, {"title": "B Details of LLMs quantization", "content": "For only weight quantization of LLaMA and T\u00dcLU2 models models, we apply QUIK implementation of mixed-precision GPTQ method (Ashkboos et al., 2023; Frantar et al., 2022). We isolate 128 salient columns in full-precision. Non-salient columns are subjected to uniform symmetric quantization, as discussed in Appendix A. The salient columns are identified through sensitive metrics described in Section 3.1. The Hessian matrix for the GPTQ method is computed on 128 random samples of the Wikitext-2 dataset."}, {"title": "C Straight Through Estimator", "content": "STE can be described in two steps:\n\u2022 Obtain quantized weights Q(W) from the real-valued parameters W with some quantization function Q, which is usually is non differentiable.\n\u2022 Compute gradients at quantized weights Q(W) and update real valued weights $W_{t+1} \\leftarrow W_t - \\tau\\nabla f(Q(W))$\nSTE makes a particular choice of a quantization function to obtain the discrete weights from the real-valued weights. This approximation can be justified in some settings (Lin et al., 2017) but in general the reasons behind its effectiveness are unknown."}, {"title": "D Detailed Benchmark Results", "content": "In this section we report detailed benchmark results for LLaMA 2 & 3 after training with different methods. Tables 7, 8 present accuracy metrics which are achieved by the full-precision models after fine-tuning on the T\u00dcLU-V2-mix and OpenOrca subsets. Corresponding mean values are listed in Table 1. Tables present accuracy metrics which are achieved by quantized in 4, 3, 2 bits models after fine-tuning on the T\u00dcLU-V2-mix subset. Corresponding mean values are listed in Table 2."}, {"title": "E T\u00dcLU-V2-mix subset", "content": "Figure 4 shows number of examples in datasets included in the T\u00dcLU-V2-mix subset, which is used for fine-tuning experiments presented in this paper."}]}