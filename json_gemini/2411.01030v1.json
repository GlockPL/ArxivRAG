{"title": "Birdie: Advancing State Space Models with Reward-Driven Objectives and Curricula", "authors": ["Sam Blouir", "Jimmy Smith", "Antonios Anastasopoulos", "Amarda Shehu"], "abstract": "Efficient state space models (SSMs), such as linear recurrent neural networks and linear attention variants, offer computational advantages over Transformers but struggle with tasks requiring long-range in-context retrieval like text copying, associative recall, and question answering over long contexts. Previous efforts to address these challenges have focused on architectural modifications, often reintroducing computational inefficiencies. In this paper, we propose a novel training procedure that significantly enhances the in-context retrieval capabilities of SSMs without altering their architecture. Our approach combines bidirectional input processing with dynamic mixtures of specialized pre-training objectives, optimized via reinforcement learning. We introduce a new bidirectional SSM architecture that seamlessly transitions from bidirectional context processing to causal generation. Experimental evaluations demonstrate that our method, called Birdie, markedly improves performance on retrieval-intensive tasks where SSMs typically underperform, narrowing the performance gap with Transformers while retaining computational efficiency. Our findings highlight the importance of training procedures in leveraging the fixed-state capacity of SSMs, offering a new direction to advance their capabilities.", "sections": [{"title": "1 Introduction", "content": "Due to their scaling properties (Hoffmann et al., 2022) and in-context learning (Garg et al., 2023), large Transformer models using attention (Bahdanau, 2014; Vaswani et al., 2017) are now prominent in natural language processing (NLP) and achieve effective performance in natural language generation tasks (NLG), including language modeling, machine translation, and question and answering (Q&A) (Yue et al., 2022; Xie et al., 2022; Kumar et al., 2021). However, the softmax attention mechanism cost scales quadratically with sequence length during training, and its key-value (KV) cache grows linearly with sequence length during inference. This leads to increasing costs for training and deployment as model providers continue to increase the context length (Dubey et al., 2024; Reid et al., 2024). This trend in increasing context length has sparked a strong interest in developing efficient alternative sequence models. The goal is to maintain high performance while scaling effectively with longer sequences. Recent work has focused on recurrent models which offer two key advantages: subquadratic scaling for parallel processing and a fixed state size (in contrast to the growing KV"}, {"title": "2 Background and Related Work", "content": "This section relates background and prior work."}, {"title": "2.1 State Space Models", "content": "Given a length L sequence of inputs $X_{1:L} \\in \\mathbb{R}^{L \\times D}$, a general class of linear recurrences with hidden states $h_{1:L} \\in \\mathbb{R}^{L \\times N}$ and outputs $y_{1:L} \\in \\mathbb{R}^{L \\times D}$ can be computed as\n\n$h_k = A_k h_{k-1} + B_k x_k$\n$y_k = g(h_k, x_k)$\n\nwith state transition matrix $A_k \\in \\mathbb{R}^{N \\times N}$, input matrix $B_k \\in \\mathbb{R}^{N \\times U}$ and output function $g(\\cdot)$ to transform the hidden state into an output.\nMany recent recurrent models fall within this SSM framework. Some are time-invariant, such that the dynamics parameters are static across time, i.e. $A_k = A$ and $B_k = B \\forall k$. This includes state space layer/linear RNN variants such as S4 (Gu et al., 2022), S5 (Smith et al., 2023) and LRU (Orvieto et al., 2023) and as well as linear attention variants such as linear transformer (Katharopoulos et al., 2020) and RetNet (Sun et al., 2023). Other linear recurrent models have input-varying dynamics; these include state space layer/linear RNN variants such as Liquid-S4 (Hasani et al., 2022), HGRU (Qin et al., 2023), Mamba (Gu and Dao, 2023), Hawk (De et al., 2024), gated linear attention (Yang et al., 2024) methods, and prior work in linear RNNS (Balduzzi and Ghifary, 2016; Martin and Cundy, 2018; Bradbury et al., 2016; Lei et al., 2018). The linear (or conditionally linear) dependencies between time steps allow for efficient parallelization across the sequence via Fast Fourier Transforms (Gu et al., 2022; Fu et al., 2023b), parallel scans (Blelloch, 1990; Martin and Cundy, 2018; Smith et al., 2023) or other structured matrix operations (Yang et al., 2024) while also allowing for fast recurrences at inference.\nIn this work, we focus on input-varying SSMs, as they have provided better performance on language (Gu and Dao, 2023; De et al., 2024; Yang et al., 2024) compared to their time-invariant counterparts. This is generally attributed to their ability to ignore or forget contextually-irrelevant information. As an example, consider the Hawk model (De et al., 2024) which showed strong performance for attention-free methods on common max-likelihood evaluations. At its core, Hawk is powered by the Real-Gated LRU (RG-LRU), an input-dependent version of LRU. The mathematical formulation of the RG-LRU is:"}, {"title": "2.2 Weaknesses of Current SSMS", "content": "While the fixed state size allows for efficient deployment at inference time, this limited state capacity also creates a tradeoff in how much information can be stored and used for in-context retrieval. These limitations have been characterized both theoretically (Arora et al., 2023; Jelassi et al., 2024; Wen et al., 2024) for simple tasks and empirically on both synthetic and more realistic tasks.\nPark et al. (2024) and Arora et al. (2024a) show that recurrent models struggle to perform synthetic multi-query associative recall (MQAR) (Arora et al., 2023) even when trained directly on the task. Jelassi et al. (2024) compared Pythia Transformers (Biderman et al., 2023) to Mamba SSMs (Gu and Dao, 2023) pre-trained on the same dataset and found that Mamba models significantly underperformed the Transformer baselines on retrieval tasks, such as phone-book lookup and long paragraph question-answering. Similarly, De et al. (2024) show that Hawk can perform phone-book retrieval for short lengths but fails to recall the correct phone number as the length grows. In the same work, even the Griffin model, which adds sliding window attention to Hawk struggles to retrieve phone numbers when the task exceeds the sliding window length. This phenomenon is also observed for Based (Arora et al., 2024a), a hybrid of linear attention and sliding window attention on synthetic MQAR tasks.\nDespite their computational appeal, current SSMs display significant weaknesses on the important skill of in-context retrieval. This limits how useful these models can be for practical deployment. We note that these prior works all train models with a simple Next Token Prediction objective. These observations lead us in this work to question the standard training procedure and re-think it as a potential avenue for better utilization of the fixed state size and improved performance on in-context retrieval tasks."}, {"title": "2.3 Pre-training Objectives", "content": "Pre-training \u201cinstills\u201d general-purpose knowledge and abilities (Raffel et al., 2020). While the default choice in NLP for a pre-training objective is Next Token Prediction, several alternative objectives have been proposed that can improve model performance in general language tasks (Tay et al., 2022, 2023a; Anil et al., 2023), code generation (Bavarian et al., 2022; Rozi\u00e8re et al., 2024a), and multi-modal audio and vision Transformers (Chen et al., 2023).\nFor instance, masked language modeling (MLM) includes objectives where a limited number of tokens are replaced with a mask token, and the model must predict the original tokens. In its original conception with BERT (Devlin et al., 2019), each mask token represented one obfuscated input token. Span corruption extends the MLM objective to generative models (Guo et al., 2022). For a given input, several spans of tokens are replaced with unique sentinel tokens. The model then generates the masked tokens and their respective sentinel tokens. Prefix language modeling (PLM) does not calculate a loss on the prefix, and the model is allowed a bidirectional view of the context. During pre-training, input sequences are randomly split in two, with the prefix serving as context and the suffix as the target for the direct loss computation (Raffel et al., 2020). The UL2 (Tay et al., 2023a) objectives combine PLM and SC.\nIn this paper, we consider and build on the above representative pre-training objectives. As described in Section 3, we introduce new objectives and dynamic mixtures."}, {"title": "3 Methods", "content": "We propose two key methodological components to reduce the gap between SSMs and Transformers on in-context retrieval tasks: bidirectional processing of the input prompt or prefix and new mixtures of pre-training objectives designed to improve the ability of SSMs to perform retrieval. We then offer a new pre-training procedure that leverages reinforcement learning for dynamic sampling of the pre-training objectives to reduce the burden of pre-selecting the optimal mixture ahead of time. We combine these components to define the Birdie training procedure. In the final part of this section, we also describe a baseline Gated SSM that allows for a simple implementation to test our methods."}, {"title": "3.1 Bidirectional processing", "content": "Bidirectional processing has shown advantages in generative Transformers using prefix language modeling and span corruption objectives (Raffel et al., 2020; Tay et al., 2023b). There is also a rich history of encoder-decoder nonlinear RNNs that compress a prefix into a single vector before generating a suffixs (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). Bidirectionality may enable SSMs to better triage state capacity, which is crucial for retrieval-intensive tasks. Our results indicate that bidirectional SSMs outperform their unidirectional counterparts on several such tasks.\nWe adapt the bidirectional prefix-LM architecture to SSMs, addressing these challenges while matching a standard causal configuration in both parameter count and compute during pre-training. To do so, we divide the recurrent state into forward and reverse components. In the suffix region, we preserve causality by masking out the forget gate dimensions on reverse state components. This prevents the state from propagating information backwards on causal areas. Additionally, this approach enables bidirectional layers to send information from the prefix to the suffix using the forward state dimensions, in contrast to Encoder layers in Encoder-Decoder models that are constrained to operate in the prefix. We provide a mathematical description below and provide an example in appendix section A.4.\n$x^{forward}_t = x_t, D_{forward}$\n$h^{forward}_t = A h^{forward}_{t-1} + x^{forward}_t$\n$x^{rev}_t = x_t, D_{rev}$\n$h^{rev, prefix area}_t = A h^{rev}_{t-1} + x^{rev}_t$\n$h^{rev, causal area}_t = \\begin{cases}\n0, A h^{rev}_{t-1} + x^{rev}_t\n\\end{cases}$\n$h^{rev}_t = [ h^{rev, prefix area}, h^{rev, causal area} ]$\n$h_t = [ h^{forward} h^{rev}]$\nWe note that concurrent work, Arora et al. (2024b), also considers applying bidirectional processing to the prefix of linear attention models, similar to prior encoder-decoder works (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). These models use separate components and stages for processing the prefix and generating the target sequences. In contrast, our ap-"}, {"title": "3.2 Pre-training Objectives for SSMs", "content": "We hypothesize that Next Token Prediction does not strongly necessitate in-context retrieval capabilities within SSMs. For most of the pre-training corpus, much of this objective can be achieved by leveraging information from local tokens alone (Xiao et al., 2024). Additionally, common pre-training data preprocessing techniques eliminate repeated or duplicated data in individual training samples (Raffel et al., 2020; Xue et al., 2021; Groeneveld et al., 2024), further reducing the model's need to learn dense copying or long-range retrieval mechanisms. These factors collectively hinder the model's abilities to retrieve information over long distances. Although Next Token Prediction does not prevent Transformers from developing long-range retrieval capabilities, SSM architectures inherently possess different inductive biases.\nTo enhance the in-context retrieval abilities of SSMs in downstream tasks, we design novel objective mixtures that explicitly train models to learn long-range and high-density retrieval abilities throughout the pre-training process. We list these objectives and mixtures that we investigate in Table 1, and briefly describe several objectives that are core to our new methods:\nSelective Copying: We introduce a novel pre-training task we call Selective Copying, where the model is tasked with retrieving specified spans in a given context, found between provided beginning and end tokens. We provide an example in Figure 2 and a detailed explanation of this task's format in Appendix Section A.5.\nWe find that this objective enables SSMs to perform zero-shot retrieval of text after pre-training. In Figure 5 in the Appendix, we show performance on this objective using validation data from The Pile, which includes emails, Wikipedia articles, and more. The design of this pretraining objective is"}, {"title": "3.3 Optimal Mixtures with Objective Sampling via Reinforcement Learning", "content": "Although we observed promising results in pilot runs, we found it difficult to pre-select optimal task mixture ratios. We also observed that seemingly optimal ratios can change during training, and different model architectures benefit from specialized ratios. Similar challenges in optimally scheduling and adjusting mixtures rates has been noted in Tay et al. (2022).\nTo address this, we propose a dynamic, automated curriculum that adapts pre-training task mixtures according to the evolving needs of the model. Our approach utilizes a reward model, which we use to predict rewards for proposed actions, given previous actions and observed outcomes. We define actions as training objectives along with their probabilities of being sampled or applied to incoming training data during training. Overall, this forms a classic multi-armed bandit framework and is related to a recent Gaussian Process approach for dynamic masking rates in MLM (Urteaga et al., 2023), which we found unable to model our diverse objectives and needs. We adopt a four-layer Gated SSM model (See Section 3.4) to directly predict per-objective rewards based on historical training data. We generate random actions and average the top 8 actions with greatest predicted reward.\nWe visualize loss, greedy-decoding accuracy, and sampling probabilities for training objective categories in Figure 5 in Appendix A. We observe trends, such as the observation that training on the Autoencoding objective appears to boost both Copying and Deshuffling objectives to the extent that their sampling can be nearly shut-off. Other behaviors emerge, such as the selective copying ability continuing to form once the model sees sufficient amounts of these samples.\nWe combine this approach with the new objectives described in Section 3.2 and the bidirectional processing described above in Section 3.1, and refer to the resulting method as Birdie. We observe that Birdie consistently improves SSM performance on a variety of downstream tasks, as related"}, {"title": "3.4 Gated SSM baseline", "content": "We define a generic Gated SSM baseline to also test our methods on other general SSMs.\nThe recurrence equations are:\n\n$i_t = \\sigma(W^i x_t) \\in \\mathbb{R}^{N}$,\n$z_t = W^z x_t \\in \\mathbb{R}^{N}$,\n$o_t = GeLU(W^o x_t) \\in \\mathbb{R}^{N}$,\n$f_t = \\sigma(W^f x_t) \\in \\mathbb{R}^{N}$,\n$h_t = f_t h_{t-1} + i_t z_t$,\n$y_t = W^{out}(o_t h_t)$,\n\nwhere $o$ is the logistic sigmoid function, $x_t$ is the normalized input at time $t$, and $y_t$ is the output that feeds into a residual connection. The operator represents element-wise multiplication. We note that this generic Gated SSM is closely related to a parallelizable version of an LSTM (Hochreiter et al., 1997) with the state dependency removed.\nIn our basic Gated SSM above, we fuse the SSM and MLP blocks as done in Mamba (Gu and Dao, 2023). We find this simple baseline performs comparably with state-of-the-art SSMs, such as Hawk, on max-likelihood tasks, but does not perform as well when asked to retrieve multiple phone numbers simultaneously, or when generating responses to questions about Wikipedia excerpts in SQUAD-V2."}, {"title": "4 Experiments and Results", "content": "Here, we present our experimental setup and main findings."}, {"title": "4.1 Experimental Setup", "content": "We pre-train and instruction-tune a series of 1.4B parameter SSM and Transformer models to investigate the proposed methods. This size allows us to achieve non-trivial performance on popular public benchmarks, while making it feasible to ablate a number of design choices.\nPre-training: We train versions of Hawk, a state-of-the-art SSM, using either Next Token Prediction or the Birdie training procedure described in Section 3.3, with its bidirectional prefix processing and dynamic mixture selection. We also include a version without bidirectional prefix processing we"}, {"title": "4.2 Comparative Performance and Ablation Study on Maximum-Likelihood Tasks", "content": "We report the average accuracy across 21 unique tasks in Table 2, with specific task-level metrics provided in Appendix Table 9. Our findings indicate that models trained using the Birdie procedure perform comparably to those using the Next Token Prediction objective, demonstrating that Birdie"}, {"title": "4.3 Analysis on Phone Number Retrieval", "content": "Next, we explore the phone number retrieval task that previous works have found SSMs trained at various scales struggle on (Jelassi et al., 2024; De et al., 2024; Waleffe et al., 2024). In addition, to make it more challenging and further stress-test retrieval capacity, we also measure the simultaneous retrieval of up to 32 numbers from phone books containing 750-800 entries.\nAll models underwent minimal fine-tuning from their base configurations, primarily to extend the positional encodings of Transformers to handle longer sequence lengths\u2014from 2,048 to 16,384 tokens. The fine-tuning process spanned 1,000 steps with a batch size of 64, utilizing training samples containing uniformly sampled entries ranging from 200 to 800. For further details, please refer to Appendix Section A.9.\nSee Figure 1A for a summary of the main results. We first note that, as expected, the Transformer models trained with either the Birdie or Next Token Prediction procedure achieve high performance regardless of the number of phone numbers retrieved. In addition, in line with previous works, we also ob-"}, {"title": "4.4 Question-Answering", "content": "We next evaluate performance on the SQUAD V2 Q&A task (Rajpurkar et al., 2018). Using greedy decoding for up to 128 tokens on all answerable questions, we format inputs as \u2018\u2018{context}\\n\\n{question}\u2019\u2019 without including any few-shot examples. We report an \"Answer Contains Label\", where a question is considered correct if any of the labels are found in the response, as well as the classic F1 score."}, {"title": "4.5 Infilling Results", "content": "Finally, we introduce a new infilling task to assess models' capabilities in copying, retrieval, and context comprehension. Models are presented with a story containing 3-7 ordered story entries, one of which is made blank. Models then predict the most appropriate option to fill this blank. As on other tasks, we observe that the Birdie procedure allows the SSM models to perform more closely to the Transformer baselines. The Transformer trained with Birdie also improves its performance. Table 4 relates the main results. More results, details, and an example can be found in Appendix A.16."}, {"title": "5 Conclusion", "content": "In this work, we investigated the significant impact of the training procedure on the downstream capabilities of State Space Models (SSMs). While prior research highlighted major weaknesses of SSMs on in-context retrieval tasks, we demonstrated that refining the training process can enhance their performance in these areas. Specifically, we proposed a novel combination of bidirectional processing of the prefix with mixtures of specialized pre-training objectives designed to improve infilling, copying, and handling of long-range dependencies. Additionally, we introduced an RL-based dynamic sampling procedure that adaptively selects optimal objective mixtures throughout training. As a result, the Birdie training procedure strongly improves a model's ability to tackle retrieval-heavy tasks where previous SSM methods have struggled. This finding suggests that, despite the simplicity of Next Token Prediction, this objective may not align optimally with the inductive biases inherent in SSM architectures.\nOur work posits that SSMs can achieve enhanced performance through careful selection and design of training objectives, offering a novel pathway for improvement beyond architectural modifications. By showcasing substantial performance gains achievable through this approach, we advocate for a broader reconsideration of how SSMS are developed and optimized. The introduction of Birdie exemplifies the benefits this methodology can bring, pointing toward new directions for future research. We hope that our findings will inspire further exploration of pre-training objectives as a critical factor in advancing SSMs and their application to complex NLP challenges."}, {"title": "6 Limitations", "content": "Our experiments were limited by an academic budget. While the 1.4B models we trained for 32B tokens are large enough to perform certain tasks, it is important to question of how results scale with larger models and more data. Although 8B Mamba and Mamba-2 models trained for 3.5 trillion tokens trained using Next Token Prediction are unable to perform the phonebook lookup task (Waleffe et al., 2024), we did not evaluate if those models could be finetuned to perform the task.\nIn terms of implementation, it is hard to beat the simplicity of the Next Token Prediction objective. The training setup required for the mixture of objectives approach requires more care to implement correctly.\nWe also note that the availability of long context evaluations of LLMs is challenging. It is often difficult to find tasks that separate out the use of parametric knowledge from true in-context reasoning abilities (Hsieh et al., 2024). This can be particularly true in tasks using realistic data, since the knowledge required to solve the task may have been present in the training data. It is possible our long paragraph question-answering and infilling tasks slightly suffer from this issue. On the other hand, synthetic tasks, such as the phone book retrieval task, can make it easier to ensure the task requires true in-context reasoning, albeit it is easy to question the usefulness and applicability of such synthetic tasks. Continued innovation in long context evaluations is crucial to stronger long context abilities in language models (agnostic of architecture).\nFinally, we note that in our experiments we observe the performance of SSMs on retrieval tasks still starts to degrade more quickly than the Transformer baselines. We do not claim to have completely solved the retrieval problem, and there may well be other weaknesses of SSMs that are not captured in the tasks considered in our work."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Reinforcement Learning for Objective Sampling", "content": "We propose Birdie, a reinforcement learning-based approach to dynamically adjust the sampling ratios and configurations (parameters) of multiple training objectives during model training. Our goal is to maximize the overall reduction in loss across various objective classes, under the assumption that each class is equally important to minimize. This method, described below, enables the model to learn which objectives and configurations are most beneficial at different stages of training. Critically, Birdie takes into account the interactions between training objectives."}, {"title": "A.1.1 Objective Classes", "content": "We train our models using a variety of objectives, which each have several configurations or parameterizations. These objective encourage the model to learn various aspects of language, such as next token prediction, span corruption, and sequence reordering, or tasks, like deshuffling and selective copying. The objectives we consider are:\n\nNext Token Prediction\nPrefix Language Modeling\nSelective Copying\nCopying\nDeshuffling\nInfilling (Span Corruption)\nAutoencoding (Deshuffling is included)\nExamples of these can be found in Table 1, and the configurations are shown in Section A.2."}, {"title": "A.1.2 Actions and Configurations", "content": "In Birdie's framework, an action is a probability vector representing the sampling frequency for each training objective and configuration. To give Birdie additional control over the training process, we create multiple configurations for each objective class by varying parameters such as context sequence length and masking percentage. Each unique configuration is treated as a separate objective in the action space.\nFor instance, the Deshuffling objective includes configurations with varying percentages of shuffled"}, {"title": "A.1.3 Reward Function", "content": "A reward vector, with elements corresponding to each possible objective parameterization, is calculated based on the change in loss achieved by that configuration. The reward function is designed to:\nReduce noise: Small, insignificant changes in loss are scaled down.\nMaintain scale: Rewards are normalized to the range [-1,1] to stabilize learning. Negative rewards provide an intuitive interface for discouraging undesirable actions.\nFocus on improvement areas: Diminishing rewards for already low losses prevent over-focusing on well-performing objectives.\nThe reward for each objective configuration is calculated as:\n$\\Delta L = \\frac{loss_{old} - loss_{new}}{loss_{old}}$, $S = \\sqrt{loss_{old} \\cdot loss_{new}}$,\n$Reward = -r\\cdot100\\cdot tanh(r \\cdot S \\cdot (\\Delta L)^3)$\n$Reward = clip(Reward, -1, 1)$,\nwhere:\n$\\Delta L$ is the normalized change in loss.\nS is the geometric mean of the old and new losses.\nr is a sensitivity hyperparameter (by default, we use Euler's number e).\nThis function balances improvements across different loss scales. For example, reducing a loss from 4.5 to 4.2 yields a similar reward to reducing a loss from 0.6 to 0.5207, despite the difference in percentage changes (-7% vs -13%).\nTo normalize for certain objective classes having greater numbers of configurations than others, we compute the Class Reward $R_c$ for each objective class c by averaging the rewards of all configurations within that class:"}, {"title": "A.1.4 Choosing an Action", "content": "At each evaluation iteration, the Birdie's action selection procedure involves the following steps. Please see the pseudocode in Appendix Figure 4 for added clarity.\nPrepare historical data: losses and action: At each time step, we have two vectors. One represents the loss across objective configurations, and the other represents the action at that time, or sampling probability for each objective configuration. We stack and concatenate these, creating an input of shape (number of time steps, number of objective configurations * 2)\nGenerate candidate actions: Since we have prepared our historical data, we now pick an action for our current state. We randomly sample 2,048 probability vectors representing potential actions, and concatenate them with the current loss vector. This creates an input of shape (2048, number of objective configurations * 2).\nPredict rewards: After concatenating the proposed actions with the current loss, we repeat the 2D array containing our historical losses and actions 2048 times. We concatenate these once more to creates an input of shape (2048, number of time steps+1, number of objective configurations * 2). We then process this using the reward model to obtain our predicted rewards per action.\nSelect top actions: We choose the 8 actions with the highest predicted rewards.\nAverage actions: Average these top actions and take this final action until the next evaluation.\nBirdie is trained on historical loss, action, and uses observed reward data as labels, enabling it to"}, {"title": "A.1.5 Reward Model Architecture", "content": "The reward model utilizes a Gated SSM architecture (Section 3.4) with four layers, each with a hidden size of 256. This model takes as input a sequence of historical losses and actions and predicts the future reward for a given action and current losses. We apply independent RMSNorm layers for the loss and action input dimensions."}, {"title": "A.2 Birdie's controls", "content": "Here, we describe the parameters, or configurations, for our objectives. For a given objective, its configurations are applied as the superset of available settings. We then allow Birdie to adaptively set the sampling probabilities for each configuration independently. When calculating the total reward, we normalize the reward by scaling each configuration's reward by the number of configurations for that objective class. A plot of the average performance per class is shown in Appendix Figure 5.\nInfilling (Span Corruption):\nThe length of the sequence (between 128-256, 256-512, 512-1024, or 1024-2048 tokens long).\nThe total percentage of the sequence to be masked (5%, 15%, or 50%).\nThe mean length of spans (3, 8, or 32 tokens long).\nNext Token Prediction: Due to an implementation limitation, we allow for no controls other than how often this objective is sampled.\nPrefix Language Modeling: Due to an implementation limitation, we allow for no controls other than how often this objective is sampled."}, {"title": "A.5 Selective Copying", "content": ""}, {"title": "A.5.1 Example Illustration", "content": "Consider the input sequence \u201cABCDEF\u201d. We use the following variables with randomly selected values:\n\nSelected Span: \u201cDE\u201d\nStart Delimiter Length: \u201c2\u201d\nEnd Delimiter Length: \u201c1\u201d\nThese arguments result in the following selected inputs and labels for the model:\nProcessed Input:\n[CONTEXT] ABCDEF [COPY] [START] BC [END] F\nProcessed Label:\nDE [DONE]"}, {"title": "A.5.2 Detailed Instructions", "content": "To construct a Selective Copying instance involving a single span, follow the procedure outlined below:\nSample Loading: Load an input string from the dataset and tokenize it. This tokenized string is referred to as the \"context.\" The model will extract one or more spans from this context.\nSpan Selection: Randomly select at least one contiguous span from the context, with a length between 4 to 32 tokens. If multiple spans are selected, ensure they do not overlap.\nDelimiter Identification: For each selected span, randomly determine the lengths of the start and end delimiters (ranging from 1 to 8 tokens). Extract the specified number of tokens immediately before the span as the start delimiter and the specified number of tokens immediately after the span as the end delimiter.\nFormatting the Span and Delimiters: Concatenate the delimiters with the following tokens:\n[START]  [END] \nPrepend this sequence with the [COPY] token to indicate a copying task:\n[COPY]  [END]"}, {"title": "A.5.3 Detailed Example", "content": "Let's revisit the sequence \"ABCDEF\u201d with the span \"DE\" selected:\nStart Delimiter Length: 2 tokens (BC)\nEnd Delimiter Length: 1 token (F)\nFormatted Delimiters:\n[COPY]  [END] F\nFinal Concatenated Input (Prepend Example):\n[COPY]  [END] F [CONTEXT] ABCDEF\nFinal Concatenated Input (Append Example):\n[CONTEXT] ABCDEF [COPY]  [END] F"}, {"title": "A.6 UL2", "content": "UL2 is a mixture-of-denoisers found via ablations on Transformer models (Tay et al., 2023a).\nIt uses the following mixture of infilling and prefix-language modeling objectives:"}, {"title": "A.7 Denoising Objectives", "content": "We summarize the denoising objectives used in UL2.\nThis table organizes the denoisers by type, indicating specific parameters used in each denoising task. \"S\" denoisers use a prefix language modeling approach where input samples are randomly split, typically at a 50% rate, for context generation.\nUL2 prepends data samples using a \"paradigm\" token that mark which objective a sample is noised with. We take a similar approach in Birdie, but, rather than always prepend the paradigm token to the context region, we also append the token 50% of the time.\nUL2's mixtures out-performed training solely on Next Token Prediction strongly on the majority of downstream tasks considered by its authors. On tasks where UL2 did not out-perform Next Token prediction, it was usually only by a small amount. PaLM-2's procedure was not disclosed.\nWith Birdie, we aim to address the following limitations with UL2:\nUL2's training objective selection and ratios was ablated for Transformers, not recurrent models.\nUL2's objective selections do not appear to induce robust copying or retrieval abilities in SSMs. Our Gated SSM pre-trained using UL2 was not able to retrieve phone numbers any better than the Next Token Prediction model. However, the UL2 model was able to retrieve answers better during SQUAD V2.\nUL2 uses fixed sampling ratios of objectives throughout the training process. The SSMs struggle on Span Corruption relative to the Transformers, they may need a greater sampling ratio for similar performance. Adjusting these ratios to be optimal for each model likely require costly ablations. Instead, we avoid ablations on our end by approximating optimal ratios using Birdie."}, {"title": "A.11 Pretraining", "content": "We train all models on the same data pipeline using The Pile (Gao et al., 2020)6. The Pile is a collection of several datasets, and includes books, code, web scrapes, emails, and question-answer instruction formatted examples.\nDuring all training and fine-tuning, we always use sequence packing and proper masking for all models, preventing samples from interfering with each other. For Hawk, we add spacing between samples to prevent the Conv1D layer from leaking information between samples. All models use"}]}