{"title": "An Eye for an Al: Evaluating GPT-4o's Visual Perception Skills and Geometric Reasoning Skills Using Computer Graphics Questions", "authors": ["Tony Haoran Feng", "Paul Denny", "Burkhard C. W\u00fcnsche", "Andrew Luxton-Reilly", "Jacqueline Whalley"], "abstract": "CG (Computer Graphics) is a popular field of CS (Computer Science), but many students find this topic difficult due to it requiring a large number of skills, such as mathematics, programming, geometric reasoning, and creativity. Over the past few years, researchers have investigated ways to harness the power of GenAI (Generative Artificial Intelligence) to improve teaching. In CS, much of the research has focused on introductory computing. A recent study evaluating the performance of an LLM (Large Language Model), GPT-4 (text-only), on CG questions, indicated poor performance and reliance on detailed descriptions of image content, which often required considerable insight from the user to return reasonable results. So far, no studies have investigated the abilities of LMMs (Large Multimodal Models), or multimodal LLMs, to solve CG questions and how these abilities can be used to improve teaching.\nIn this study, we construct two datasets of CG questions requiring varying degrees of visual perception skills and geometric reasoning skills, and evaluate the current state-of-the-art LMM, GPT-4o, on the two datasets. We find that although GPT-4o exhibits great potential in solving questions with visual information independently, major limitations still exist to the accuracy and quality of the generated results. We propose several novel approaches for CG educators to incorporate GenAI into CG teaching despite these limitations. We hope that our guidelines further encourage learning and engagement in CG classrooms.", "sections": [{"title": "INTRODUCTION", "content": "The advancement of GenAI (Generative Artificial Intelligence), especially LLMs (Large Language Models), has garnered global attention from the Computing Education research community [Denny et al. 2024b]. LLMs excel at generating solutions that are typical of many programming-focused computing courses [Denny et al. 2023; Finnie-Ansley et al. 2022, 2023; Savelka et al. 2023]. However, since LLMs can only process textual inputs, they perform poorly in tasks requiring image inputs and/or visual and geometric processing skills [Feng et al. 2024a], which are essential in solving questions in CG (Computer Graphics) [Rodrigues et al. 2021; Suselo et al. 2017]. LMMs (Large Multimodal Models), or VLMs (Visual Language Models), are extensions of LLMs that allow users to provide information in non-textual formats, such as images. An example is GPT-4o (GPT-4 Omni) [OpenAI 2024b], an LMM developed by OpenAI that allows image inputs. The release of LMMs opened many new opportunities. With image inputs, users can provide visual context to the GenAI model, making human-AI interactions easier, and questions requiring visual context can now be asked effortlessly. This also possibly allows LMMs to solve questions requiring visual and geometric reasoning skills, such as those in CG.\nInvestigating the performance of LMMs, such as GPT-4o, on CG questions can provide insight into decisions and opportunities related to teaching CG. Past research suggests that the poor performance of GPT-4 in CG questions limits students' ability to misuse it [Feng et al. 2024a], but also makes it harder for CG instructors to use GenAI for teaching (e.g., by providing formative feedback, creating practice questions [Feng et al. 2024b], generating explanations). Evaluating the capabilities and limitations of GPT-4o in the context of CG enables educators to make more informed decisions about integrating GenAI into their teaching.\nIn this work, we investigate the visual perception and geometric reasoning capabilities of GPT-4o by using two datasets of CG-related questions. We compare the visual processing capabilities of GPT-4o to its textual processing capabilities and outline implications and recommendations for CG educators. Our study aims to answer the following Research Question:\nHow well can GPT-4o solve Computer Graphics questions requiring visual perception and geometric reasoning skills?"}, {"title": "RELATED WORK", "content": "The is an area of active research. We evaluated the performance of GPT-4 (text-only) on assessment questions used in an undergraduate introductory CG course and found that GPT-4 produced correct solutions to only 42.1% of the questions [Feng et al. 2024a]. Another study assessed GPT-4's ability to generate code for a Ray Tracing application, and the results demonstrated a similar performance compared to the previous study (42% accuracy) [Feng et al. 2024b]."}, {"title": "Evaluations of LMMs", "content": "We theorize that the low performance of LLMs for CG questions is due to CG requiring extensive visual-based reasoning skills, and LLMs struggle with these tasks due to their textual nature and lack of visual training data [Feng et al. 2024a; Singla 2023]. LMMs allow users to provide visual context to the model directly. However, since LMMs are still relatively new, few studies have been conducted to measure their capabilities in various tasks.\nTwo early evaluation reports on GPT-4V (GPT-4 Vision [OpenAI 2024a], the predecessor of GPT-4o) showcased its capabilities on queries requiring visual contexts in a wide variety of settings, such as visual math reasoning and code generation [Wu et al. 2023b; Yang et al. 2023]. The results showed impressive visual-based reasoning skills of LMMs. Nevertheless, they often produce errors. Similar evaluation studies on more specialized areas showed that LMMs are somewhat capable of assisting in medical diagnoses [Wu et al. 2023a], map analysis [Xu and Tao 2024], and autonomous driving [Driessen et al. 2024; Wen et al. 2024]. However, the consensus remains that there are significant limitations to the capabilities of LMMs, and more development is needed before they can reliably support real-world applications.\nIn the context of education, GPT-4V has been compared with its text-only counterpart, GPT-4 Turbo, on a specialized medical examination, and no statistically significant differences between the results were found between the two models [Hirano et al. 2024], indicating that LMMs do not necessarily outperform LLMs. In a study more relevant to CS, the ability of GPT-4V to generate code based on UML diagrams was evaluated, and it was observed to perform well for simpler, single-class UML diagrams, but it failed to consistently generate correct code for more complex, multi-class UML diagrams [Antal et al. 2024].\nDespite the mediocre performance of LMMs on some educational tasks, the use of LMMs or similar applications can increase student performance and interest [Zain et al. 2023]. Effectively leveraging this in educational settings may lead to similar positive impacts."}, {"title": "METHODS", "content": "We investigate the current capabilities of GPT-4o on CG questions by 1) collecting and creating CG questions; 2) converting them into the JSON format accepted by GPT-4o; 3) fetching responses from"}, {"title": "Collecting Questions", "content": "Our first dataset derives from a previous study using GPT-4 [Feng et al. 2024a]. It contains 101 assessment questions used in a third-year introductory CG course, 68 of which are MCQs and 33 are programming questions. The questions are taken from the mid-semester tests and final exams of the 2022 and 2023 iterations of the course (i.e., four assessments in total). We refer to this dataset as CG_TEST in this paper. The topics covered in the questions include but are not limited to introductory Linear Algebra, introductory OpenGL, Colors and Lighting, Illumination and Shading, Texture Mapping, Ray Tracing, 3D Modelling, Parametric Curves and Surfaces, and Image Processing.\nEach assessment is split into Theory and Programming parts. Theory parts consist of MCQs of four or more options. Programming parts consist of programming questions that often require students to write code snippets, which are then executed against pre-written test cases. If all test cases are passed, then the student is awarded all marks allocated for the question. Otherwise, no marks are awarded. No partial marks are given.\nOf all 101 questions, 67 contain no images, and 34 contain images. Although many of the questions contain no images, almost all questions require visual perception and geometric reasoning intelligence as the course focuses heavily on developing these skills and contains highly visual concepts. Several example questions are listed throughout the paper.\nSince the assessment questions are quite technical and specialized, we also want to investigate GPT-4o's ability to process visual information without using specialized knowledge and whether this makes a difference in performance. Hence, we also created a small dataset containing 10 basic image-based CG-related short-answer questions, which we refer to as CG_EASY in this paper. However, little to no CG background is required to answer these questions, and only common sense and a moderate amount of visual and geometric reasoning skills are needed. The questions involve identifying and counting geometric objects in a scene, light-surface interactions, basic 3D geometry, and basic 3D transformations (translations and rotations). The two datasets used in this study are publicly available through the link provided in Section 7."}, {"title": "Converting to JSON", "content": "GPT-4o allows for inputs in various formats, such as images from publicly accessible URLs, and combinations of multimodal content as single inputs, such as interweaving texts and images. Multimodal inputs follow the JSON format shown in Figure 2.\nThe questions collected from CG assessments contain texts, mathematical formulas, and images, but they are not in the format accepted for multimodal inputs. Hence, some preprocessing needs to occur before the questions can be processed by GPT-4o."}, {"title": "Fetching Responses", "content": "After the questions are converted to JSON objects, the data is sent to GPT-4o via the OpenAI API, to which the model responds with its answers. Each JSON object is sent 10 times, and 10 responses are received, which are treated as 10 independent attempts. For every question containing images, two separate JSON objects are sent (text-only version and multimodal version), and 20 responses are received for that question, 10 for each version. The model's temperature is set to 0.75, which is reported to perform well on previous, similar studies [Feng et al. 2024a; Pursnani et al. 2023]. The system message we use in this study is \"You are a helpful assistant, and you are knowledgeable in Computer Graphics. When you answer a multiple-choice question, you state your selected option explicitly while providing a concise and accurate explanation.\""}, {"title": "Evaluating Correctness", "content": "The responses from the GPT-4o are then evaluated for correctness. No partial marks are given (responses are categorized as correct or incorrect).\nFor MCQs, responses are marked as correct if they state the correct option or the letter associated with the correct option. The responses usually contain explanations of their solutions, but they are not required to be considered correct.\nFor programming questions, responses are marked as correct if they contain the correct solution code that can be copied and pasted into the AAT (Automated Assessment Tool) used in the assessments and pass all test cases [W\u00fcnsche et al. 2018; W\u00fcnsche et al. 2019]. We allow for some deletions from the generated code solutions, such as boilerplate code which is often present in outputs, as boilerplate code is already supplied by the AAT."}, {"title": "RESULTS", "content": "For CG_TEST, out of all 1350 responses received from GPT-4o (67 text-only questions, 34 image-based questions converted to text-only using textual descriptions, 34 image-based questions using multimodal input, 10 responses each), 676 responses are marked as correct, which is 50.1% of all responses. Out of 800 responses to MCQs (56 text-only MCQs, two versions of 12 image-based questions), 501 responses are correct (62.6%). Of 550 responses to programming questions (11 text-only questions, two versions of 22 image-based questions), 175 responses are correct (31.8%). There are 55 groups of 10 responses to programming questions, each corresponding to one programming question, and 28 out of the 55 groups contained at least one correct response (50.9%). Of 670 responses to text-only questions (67 text-only questions), 455 are correct (67.9%). Of 340 responses to image-based questions using textual descriptions (34 image-based questions using textual descriptions), 121 are correct (35.6%). Of 340 responses to image-based questions using real images (34 image-based questions using real images), 100 are correct (29.4%). For CG_EASY, out of 100 responses from GPT-4o (10 image-based questions using real images), 62 are correct (62.0%). These results are summarized in Table 1."}, {"title": "DISCUSSION", "content": "Overall, GPT-4o answers around half of the queries from CG assessments correctly, i.e., it is not a reliable source of answers for CG assessments or specialized CG questions. A slightly higher accuracy is achieved for questions in the CG_EASY dataset. However,"}, {"title": "Overall Results", "content": "Overall, GPT-4o answers around half of the queries from CG assessments correctly, i.e., it is not a reliable source of answers for CG assessments or specialized CG questions. A slightly higher accuracy is achieved for questions in the CG_EASY dataset. However,"}, {"title": "Common Characteristics of Responses", "content": "Please note that the behavior of the responses is dependent on the system message used with the query, and queries using different system messages may not show the following characteristics."}, {"title": "Varying lengths of explanations", "content": "The responses to the CG_TEST dataset are generally lengthy and detailed, with most explanations to questions reaching more than 10 lines of text and some even reaching 30 lines. Additionally, GPT-4o would often make mistakes but continue to elaborate along incorrect lines of thinking, which can confuse students and reduce learning. Common errors made by GPT-4o for this dataset are conceptual errors (e.g., using incorrect concepts, hallucinating false facts), mathematical errors (e.g., incorrectly substituting values into formulas, incorrectly expanding expressions, calculation errors), and logical errors (e.g., stating fallacious causal relationships).\nThe responses to CG_EASY are much shorter, typically only around 1-10 lines long, since solutions are often straightforward and do not require complex explanations (although the responses can still be incorrect)."}, {"title": "Unnecessary code", "content": "For programming questions in the CG_TEST dataset, the generated code snippets can be unnecessarily long and contain large amounts of boilerplate code. For example, the solution to a 3D programming question (shown in Figure 8) is simply 3 lines of OpenGL code, calling the functions glRotatef(), glScalef(), glTranslatef() once each. However, most generated solutions for this question exceed 20 lines of code and include boilerplate code supplied by the AAT, such as a main() function. There are also many instances where the questions state that some classes and functions are already provided, but GPT-4o still includes those classes and functions with their full implementations, which leads to redefinition errors when the unmodified code solutions are executed, and these extra code snippets have to be manually removed when testing for correctness."}, {"title": "Failure to follow specific instructions", "content": "In addition to disregarding statements specifying the existing classes and functions, GPT-4o often fails to follow the correct syntax to call existing functions. For example, a common function supplied by the debugger is \"dot(Vector v1, Vector v2)\" for calculating the dot product of two vectors. The syntax for calling this function is always specified in the questions, but GPT-4o often fails to follow the syntax and instead writes \"v1.dot(v2)\". More than 40 responses out of the total 550 for programming questions include errors of this kind."}, {"title": "Specific Observations", "content": ""}, {"title": "Breakthroughs in difficult questions", "content": "The CG_TEST dataset contains several questions GPT-4 could not answer correctly in previous studies [Feng et al. 2024a,b]. One example is a programming question for ray tracing a cut sphere (shown in Figure 6), which GPT-4 answered incorrectly for all 30 attempts across two studies. In this study, GPT-4o answered this question correctly (with complete working code) in 1 out of the 10 responses for the textual description version and also 1 out of 10 for the image version. This is impressive since fewer than 5% of students could answer this question with unlimited attempts in an exam.\nThe CG_TEST dataset also contains a texture mapping programming question (shown in Figure 7), GPT-4 (text-only) could consistently solve this question, but this could be due to the textual descriptions of the images, and the visual processing that the human did by extracting the coordinates of the faces when describing"}, {"title": "Challenges in answering questions in CG_EASY", "content": "GPT-4o can successfully solve 62.0% of the image-based questions in CG_EASY without any human assistance or textual descriptions for the images, which is certainly an impressive performance. However, there are still some questions that GPT-4o struggles with, such as the two questions shown in Figure 1.\nFor the question shown on the left of Figure 1, GPT-4o answers correctly in only 2 out of the 10 responses. In the 8 other attempts, GPT-4o states that there are either 8 or 9 objects in the image, and in most cases, it identifies 4 cylinders. However, when directly asking about the number of cylinders in the image, GPT-4o answers correctly 9 out of 10 times. We theorize that the added complexity of the question may have confused GPT-4o, and this reduction in performance may not be related to its visual perception skills.\nFor the question on the right of Figure 1, GPT-4o incorrectly answers \"orange\" in all 10 attempts. Although it sometimes states that the blue vector is also coplanar with the gray vectors, it always perceives the orange vector as coplanar, hence they cannot be marked as correct. From the results of this question, we suggest that although GPT-4o has modest visual perception skills, it still lacks geometric reasoning skills."}, {"title": "Challenges in answering 3D transformation questions", "content": "A question type that GPT-4 and GPT-4o struggle with is programming questions related to 3D transformations, one of which is shown in Figure 8. All 40 attempts from this study and the previous study provide incorrect code solutions for this question, despite the correct solution only being 3 lines of code. This is further evidence that GPT-4 and GPT-4o lack geometric reasoning skills, which are essential in solving this question."}, {"title": "Implications", "content": ""}, {"title": "GenAl models are unreliable in visual question-answering", "content": "The results of our study suggest that GPT-4o, or LMMs in general, may not reliably answer CG questions requiring visual perception skills and especially geometric reasoning skills. However, this does not mean that GenAI models cannot be used to improve learning. For example, CG educators can write or generate image descriptions for CG problems and ask students to evaluate the quality of the descriptions and/or improve the descriptions to enable GenAI models to solve the original problems. Additionally, LMMs are also useful for improving self-reflective practice [Kumar et al. 2024].\nConversely, CG educators should also raise student awareness of the limitations of GenAI for CG questions and the importance of critically evaluating the generated solutions. For CG educators who are opposed to the use of GenAI for teaching and learning purposes, since GPT-4o performs more poorly on image-based questions than textual questions, greater use of image-based questions may discourage students from using GenAI and encourage independent thinking and learning."}, {"title": "A new exercise: Spot the error", "content": "An exercise for CG educators is to use incorrect AI-generated solutions to CG questions and ask students to find the errors in these solutions. This can simultaneously encourage students to reflect critically on their understanding of the topics and also raise awareness of the limitations of GenAI."}, {"title": "Prompt engineering for more accurate and helpful responses", "content": "In this study, we directly used the question texts (after formatting) as prompts for the GenAI model. Recent research suggests that GenAI can achieve higher performance through better prompting strategies, such as splitting each question into smaller subquestions or asking GenAI to explain step by step [Denny et al. 2023; Kojima et al. 2022]. Different system messages can also be used to achieve different performances and characteristics. This could also be a good learning task for students, i.e., develop prompting strategies to solve complex CG questions."}, {"title": "CONCLUSION", "content": "In this study, we constructed two datasets of CG assessment and basic visual CG-related questions requiring varying degrees of visual perception skills and geometric reasoning skills. We evaluated the performance of GPT-4o on these two datasets. Although GPT-4o has improved in performance on visual questions compared to predecessor models, it still lacks the visual processing power to provide reliable academic support to CG students and, in general, real-world applications requiring visual understanding. We also described several common characteristics exhibited by GPT-4o in its responses and outlined various specific questions on which GPT-4o performed well or poorly. Finally, we suggested some implications for CG education and provided recommendations to CG educators on utilizing LMMs to improve CG teaching."}, {"title": "RESOURCES", "content": "All images, textual descriptions, and JSON objects can be accessed through this link: https://github.com/TFPlusPlus/GPT-4V-vs.-CG."}]}