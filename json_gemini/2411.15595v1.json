{"title": "An adversarial feature learning based semantic communication method for Human 3D Reconstruction", "authors": ["Shaojiang Liu", "Jiajun Zou", "Zhendan Liu", "Meixia Dong", "Zhiping Wan"], "abstract": "With the widespread application of human body 3D reconstruction technology across various fields, the demands for data transmission and processing efficiency continue to rise, particularly in scenarios where network bandwidth is limited and low latency is required. This paper introduces an Adversarial Feature Learning-based Semantic Communication method (AFLSC) for human body 3D reconstruction, which focuses on extracting and transmitting semantic information crucial for the 3D reconstruction task, thereby significantly optimizing data flow and alleviating bandwidth pressure. At the sender's end, we propose a multitask learning-based feature extraction method to capture the spatial layout, keypoints, posture, and depth information from 2D human images, and design a semantic encoding technique based on adversarial feature learning to encode these feature information into semantic data. We also develop a dynamic compression technique to efficiently transmit this semantic data, greatly enhancing transmission efficiency and reducing latency. At the receiver's end, we design an efficient multi-level semantic feature decoding method to convert semantic data back into key image features. Finally, an improved ViT-diffusion model is employed for 3D reconstruction, producing human body 3D mesh models. Experimental results validate the advantages of our method in terms of data transmission efficiency and reconstruction quality, demonstrating its excellent potential for application in bandwidth-limited environments.", "sections": [{"title": "0 Introduction", "content": "With the rapid development of Virtual Reality (VR), Augmented Reality (AR) and Autonomous Driving technology, 3D reconstruction technology has gradually become the focus of scientific research and industrial applications. These technologies have a wide range of application prospects in the fields of digital media, online education, remote sensing monitoring, medical imaging, etc., and can provide a more immersive and interactive user experience [1-2]. For example, in the field of medical imaging, 3D reconstruction can help doctors analyze and diagnose more accurately; in urban planning and disaster assessment, 3D map reconstruction through remote sensing technology can provide more effective decision support. However, 3D reconstruction usually requires processing and transmitting large amounts of data, which is particularly difficult in environments with limited bandwidth or high network latency. In addition, high-quality 3D reconstruction often relies on powerful computational resources, which limits its usefulness in mobile devices or real-time application scenarios [3-4].\nIn order to overcome these limitations, this paper proposes a semantic communication approach for 3D reconstruction of the human body. Semantic communication is an innovative communication paradigm [5-7] that focuses on transmitting information that is critical for accomplishing a specific task, rather than simply transmitting large amounts of raw data. By transmitting only the most critical semantic information, the method can significantly reduce the amount of data required, thereby increasing transmission efficiency and reducing bandwidth dependency [8-9]. The use of semantic communication for human 3D reconstruction not only guarantees the quality of the reconstruction, but also significantly reduces the time required for data transmission and processing, which is particularly important for application environments that have restricted network conditions or limited computational resources. Potential applications of this technology include online gaming, telemedicine diagnosis, and holographic projection technology, which all require processing and transmitting large amounts of 3D data while ensuring real-time and high interactivity. By optimizing the data flow, semantic communication brings new development opportunities to these fields, making high-quality 3D reconstruction possible in a wider range of application scenarios.\nHow to accurately recover detailed human 3D data from limited semantic information and how to design efficient algorithms to achieve stability and reliability under different network and device conditions. In addition, how to balance the efficiency and accuracy between semantic information extraction, transmission and reconstruction is also a key issue to be addressed in current research [10-12]. The method in this paper accurately extracts the key semantic information that can be used for human 3D reconstruction at the sending end and efficiently transmits it to the receiving end through an adaptive compression algorithm, and then accurately reconstructs the human 3D model at the receiving end by utilizing semantic decoding and 3D reconstruction methods. This strategy not only improves the speed of data processing, but also ensures the accuracy and low latency of the reconstruction process. The method in this paper can provide a new solution for real-time human 3D reconstruction technology, especially in application environments where bandwidth is limited and real-time response is required, demonstrating its unique advantages and potential prospects for a wide range of applications.\nHuman 3D reconstruction typically requires processing and transmitting large amounts of data, which is particularly challenging in environments with limited bandwidth or high network latency. In addition, huge computational resources are often required to achieve high-quality reconstruction results, which limits its wide application in mobile devices or application scenarios that require real-time responses. To address the above challenges, our proposed AFLSC method not only optimizes the data flow, but also ensures the integrity of the information and the high quality of the reconstruction by accurately extracting the semantic information that is crucial for human 3D reconstruction and efficiently transmitting it through adaptive compression techniques, and the main contributions are summarized as follows:\n(1) Innovative semantic encoding mechanism: This study develops a semantic encoding strategy optimized for 3D reconstruction of the human body, which is able to convert 2D images into key semantic data oriented to 3D reconstruction at the sending end. This approach reduces the transmission of irrelevant data and ensures efficient encoding and transmission of key information.\n(2) Dynamic compression rate adjustment: this method adaptively adjusts the data compression rate according to the importance of the semantic data and the current network status (bandwidth and latency) by means of dynamic compression technology, which optimizes the data transmission process and significantly reduces the latency, while ensuring the integrity of the data and the reconstruction quality."}, {"title": "1 Related Work", "content": "In recent years, human 3D reconstruction techniques have become a hotspot in computer vision and graphics research, especially in the reconstruction of high-resolution and complex scenes, and significant progress has been made. NeRF [13] (Neural Radiance Fields for View Synthesis) represents an emerging technique for reconstructing high-quality continuous 3D scenes from sparse views by means of a deep learning framework, which drastically improves the realism of visual effects by optimizing the view synthesis process. PIFuHD [14] (Multi-level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization) achieves a high quality 3D scene through a pixel-level Aligned Implicit Function for High-Resolution 3D Human Digitization) achieves a direct conversion from a single image to a high-resolution 3D human model, which significantly improves the detail quality of the reconstruction. ECON [15] (Explicit Clothed Humans Optimized via Normal Integration) method is used to optimize the reconstruction process by combining the displayed human body and the clothing model, and utilizing the Normal Integration ECON [15] (Explicit Clothed Humans Optimized via Normal Integration) method optimizes the reconstruction process by combining the displayed human body and garment models and utilizes normal integration techniques to effectively improve the reconstruction quality of the garment and body details. One-stage 3D Whole-Body Mesh Recovery [16] employs the Component Aware Transformer, which directly reconstructs the entire 3D human mesh from a single image, simplifying the reconstruction process and improving the processing speed. In addition, SIFU [17] and GauHuman [18] introduced implicit functions under side-view conditions and monocular video-based Gaussian projection techniques, respectively, and these methods have demonstrated great utility and flexibility in real-world application scenarios."}, {"title": "1.2 Image Semantic Communication", "content": "Image semantic communication is an emerging cross-cutting area of information and communication technology, focusing on the efficient transmission of image content. Research in this field focuses on reducing the amount of data transmission required while ensuring transmission efficiency and semantic accuracy of images. Huang et al. [19] demonstrated a deep learning based image semantic coding method, which optimizes the coding and transmission process by extracting the semantic features of an image and significantly improves the transmission efficiency. Wu et al. [20] improved the semantic accuracy of image transmission by using semantic segmentation techniques. This method reduces the required bandwidth by transmitting only the critical semantic information. Xie et al. [21] demonstrated a deep learning based image semantic encoding method, which optimizes the coding and transmission processes by extracting semantic features of the image, which significantly enhances transmission efficiency. semantic accuracy, this approach reduces the required bandwidth by transmitting only the critical semantic information. Zhang et al. [22] proposed a semantic communication framework for image transfer that presents a multimodal metric called image-to-graph Semantic Similarity (ISS), which"}, {"title": "2 System Modeling", "content": "In this study, we design a semantic communication system for 3D reconstruction of the human body to realize the process starting from the input of the original 2D image I at the sender's end to the final generation of the 3D model at the receiver's end. The system covers feature extraction and semantic coding of images, compression and transmission of semantic data, and 3D reconstruction based on the received semantic data.\nAt the sender side, 2D human body images I are used as input and these images are first processed through Vision Transformer (ViT). At this stage, the images are divided into multiple blocks (patches), and each block is processed by ViT to extract key features related to 3D reconstruction of human body, such as shape, spatial layout, pose and depth information, to obtain a high-dimensional feature vector X. These features are further semantically encoded by Conditional Variational Auto-Encoders (CVAEs) [24], which, in combination with an adversarial learning approach, generates a labeled dataset y' with rich semantic information. In order to be transmitted from the sender to the receiver, this semantic data y' needs to be compressed. We employ a dynamic compression strategy that dynamically adjusts the compression rate based on the current network conditions (e.g., bandwidth and latency) and the importance of the data. This strategy ensures efficient and stable transmission of semantic data in various network environments.\nAt the receiver side, the system first decodes the received semantic data y' to obtain potential features z', and then converts these potential features back to image features si through a multi-level decoding process. This process involves multiple decoding layers, each of which gradually reconstructs the detailed features of the image by utilizing the output of the previous layer and the potential state of the current layer. After obtaining the image features, we use a modified ViT-diffusion model to generate a 3D point cloud, a process that progressively refines the point cloud data through a diffusion and inverse diffusion process, and finally converts the point cloud into a fine 3D mesh model using the Marching Cubes algorithm."}, {"title": "3. Feature extraction and semantic encoding", "content": "In order to enable 3D reconstruction of the human body at the receiver side, it is crucial to efficiently extract and encode the 2D human image features at the transmitter side. These features need to contain not only information about the shape and spatial layout of the object, but also capture task-specific details such as the pose and relative depth of the object. Therefore, this chapter aims to explore how to extract key features from complex data and transform these features into useful semantic information through advanced deep learning models. We first parse how the Vision Transformer model can be adapted and optimized for different visual tasks based on multi-task learning feature extraction techniques. Next, we delve into semantic encoding methods based on adversarial feature learning."}, {"title": "3.1 Feature extraction based on multi-task learning", "content": "In the traditional ViT model [25], the image is divided into patches, and each patch is transformed into a D-dimensional feature vector before entering the self-attention mechanism. In order to adapt to multi-task learning, we modify the Transformer structure as follows:\n(1) Task-related positional coding: to better adapt ViT to keypoint detection and depth estimation tasks, we introduce task-related positional coding increments. The purpose of this design is to enhance the model's ability to understand spatial information when processing different visual tasks, which is implemented as follows:\n$E_{pos} E_{task} = E_{pos} + \\Delta E_{task}$ (1)\n$E_{pos}$ is the original position encoding and \u0394Etask is the incremental position encoding adjusted to the type of task, which allows the model to better interpret the spatial relationships in the image when dealing with different visual tasks.\n(2) Integrated Multi-Task Learning Heads: we leverage the capabilities of the ViT model by integrating specialized heads for performing multi-task learning in its architecture. These heads are designed to handle keypoint detection, pose estimation, and depth estimation tasks, respectively. By introducing task-specific branches in the middle layer of the Transformer network, we make it possible for each head to optimize the processing path specifically for its particular task while receiving a shared feature representation.\nWe introduce three branches on the L-th layer of the Transformer's output XL, each of which further processes features through a combination of layer normalization and a task-specific network layer. The specific design of each task header is as follows:\nKeypoint Detection Header. For keypoint detection, we use a multi-scale feature fusion strategy which enhances the localization accuracy of keypoints by combining feature responses at different scales as follows:\n$Y_{k} = \\sigma (\\sum_{i=1}^{3}BN( C_{3*3} ( Re(C_{1*1} (LN(X^{k}_{L}) LN(X^{k}_{L}))))))$ (2)\nWhere \u03c3 is the sigmoid function, C3*3 denotes the 3\u00d73 convolutional layer applied to the i-th scale feature, LN(\u00b7) denotes the layer normalization operation, and $X^{k}_{L}$ stands for the i-th set of features extracted from the output of the L-th layer when dealing with the keypoint detection task.\nAttitude estimation head. In pose estimation, an adaptive feature fusion technique can be defined as the use of a dynamically adjusted weighting system that adapts the network focus according to the dynamic range and spatial distribution of the input features:\n$Y_{pose} = MLP (\\sum_{i} \\frac{exp(FC(stat(X^{pose}_{L,i})))}{\\sum_{j}exp(FC(stat(X^{pose}_{L,j})))} LN (X^{pose}_{L,i}) )$ (3)\nStat denotes a statistical function, such as mean and variance computation, used to evaluate the activity level of the features. FC is the fully connected layer used to convert the statistics into weights. MLP is the multilayer perceptron used for the final prediction of the pose parameters. $X^{pose}_{L}$ stands for the ith set of features extracted from the output of the L-th layer while processing the pose estimation task.\nDepth estimation header. In the scheme of this paper, the context-aware network for depth estimation can be realized by a hybrid feature extraction network that combines local features and global context information:\n$Y_{depth} = ReLU (De ( \\sum_{k} \\beta_{k} C_{3*3}(LN (X^{depth}_{L,k})))))$ (4)\n$\\beta_{k} = \\frac{exp(GC(x^{depth}_{L,m}))}{\\sum_{m} exp(GC(x^{depth}_{L,m}))}$ (5)\nWhere $\u03b2_{k}$ is the weight generated based on the global context module, which evaluates the importance of different scale features for the task at hand. De denotes the inverse convolutional layer, and GC is a network module designed to analyze and integrate the global information from the output of the different layers in order to determine the weight of each feature. $X^{depth}_{L,k}$ stands for processing the depth estimation task by extracting from the output of the L-th layer the of the k-th set of features.\nWhile the introduction of task-dependent positional coding and an integrated multi-task learning head can enhance the Vision Transformer model's ability to handle keypoint detection, pose estimation, and depth estimation tasks, in order to further improve the adaptability to the specific needs of 3D reconstruction, especially in dealing with spatial relations and depth perception, we have made the necessary customizations to the self-attention mechanism customization. The self-attention layer is customized to handle spatial relations and depth perception more efficiently for the specific needs of human 3D reconstruction:\n$AT_{task} (Q, K, V) = softmax (\\frac{QKT + M_{task}}{\\sqrt{d_{k}}}) V$ (6)\nWhere Mtask is a mask adjusted based on the task requirements to enhance or suppress the strength of association between certain features, which facilitates the model to focus more on the key features corresponding to the task, e.g., to strengthen the distinction between near and far objects in depth estimation.\nIn order to optimize all the tasks in the multitask learning model simultaneously, the overall loss function is designed as a weighted sum of the losses of each task, and each task loss corresponds to a weight coefficient, which reflect the importance of each task in the training process. The loss function for multitask optimization is defined as follows:\n$L_{total} = \\sum_{task}a_{task}\\cdot L_{task} (Y^{task}, \\hat{Y^{task}})$ (7)\n$A_{task}$ is the weight coefficients for each task, these coefficients are used to balance the importance of each task in the training process and to ensure that the model achieves good performance on all the tasks. $Y^{task}$ denotes the predicted outputs of the key point detection task, the pose estimation task, and the depth estimation task, and $\\hat{Y^{task}}$ is the corresponding true label. $L_{task}$ is the loss function for a specific task. The loss for the keypoint detection task mainly evaluates the difference between the predicted heatmap and the true heatmap, and thus the binary cross-entropy loss is chosen. Posture estimation involves predicting a series of continuous values (e.g., 3D coordinates of joints or rotation angles of body parts), and therefore uses a mean square error loss. The depth estimation task takes into account that depth values are often unevenly distributed, and uses the Structural Similarity Index (SSIM) to more accurately assess visual and structural differences."}, {"title": "3.2 Semantic encoding based on adversarial feature learning", "content": "For the feature X extracted by our multi-task based learning approach, in order to efficiently encode deep semantic information from X, we propose an adversarial feature learning approach for semantic encoding that integrates a Conditional Variational Auto-Encoder (CVAE) and Generative Adversarial Networks (GANs), where the CVAE part is responsible for extracting meaningful latent representations from the input features and the GAN part is used to optimize these representations through adversarial learning to generate more realistic and semantically consistent outputs.\nThe approach consists of two main components: the CVAE and an auxiliary adversarial discriminator, working together to accurately generate and validate semantic labels. The encoder is designed to capture the joint probability distribution between the input features X and the target semantic labels y (used only in the training phase) and efficiently maps this information into a latent space z. The process learns the mean $\\mu_{\\phi}(X,y)$ and the variance $\\sigma_{\\phi}(X, y)^2$ of the input data over the network and is characterized as:\n$q_{\\phi}(z|X,y) = N(z; \\mu_{\\phi}(X,y), \\sigma_{\\phi}(X,y)^{2}I)$ (8)\nWhere $\\mu_{\\phi}(X,y), \\sigma_{\\phi}(X,y)$ represent the centrality and dispersion of potential features, respectively, thus describing the conditional distribution of the latent variable z.\nThe task of the decoder is to reconstruct the input features X from the latent variables z and predict as accurately as possible the semantic labels y. The decoder combines continuous data reconstruction with discrete label prediction in the expression:\n$p_{\\theta}(X,y|z) = N(X; \\mu_{\\theta}(z), \\sigma_{\\theta}(z)^{2}I) \\times Cat(y; \\pi_{\\rho}(z))$ (9)\n$N(X; \\mu_{\\theta}(z), \\sigma_{\\theta}(z)^{2}I)$ is a Gaussian distribution that describes the process of reconstructing the input feature X from the latent variable z. $\\mu_{\\theta}(z)$ and $\\sigma_{\\theta}(z)$ define the distribution of the reconstructed data, while $\\pi_{\\rho}(z)$ provides the probability of generating a specific semantic label from the latent variable z. Cat(y; \u03c0e(z)) is a categorical distribution for predicting discrete semantic labels y based on the latent variable z's current state. z's current state to predict the discrete semantic label y.\nThe role of the discriminator is to distinguish the differences between the samples generated by the decoder and the real samples, and to evaluate whether these generated samples are consistent in content with the input features X. The discriminator is a tool that can be used to evaluate the content of the samples generated by the decoder and the real samples. The operation of the discriminator is based on the following equation.\nD(\\tilde{X}, \\tilde{y}) = sigmoid(f(X, \\tilde{X}, y, \\tilde{y})) (10)\nWhere $\\tilde{X}$ and $\\tilde{y}$ are the samples and labels generated by the decoder, respectively, and f is the discriminative network.\nThe training objective of this semantic coding model for adversarial feature learning is optimized through the composition of two main loss functions, a CVAE loss and an adversarial loss, and we combine the CVAE loss and the adversarial loss into a single comprehensive loss function, $L_{com}$, which can optimize the model's reconstruction ability and the realism of the generated samples in a more effective and simultaneous way. This loss function $L_{com}$ aims to optimize the performance of the entire model by balancing the reconstruction error, KL scatter, and adversarial evaluation with a single optimization objective. The adversarial loss consists of two parts, one that encourages the discriminator to accurately recognize real samples, and the other that motivates the discriminator to recognize the samples generated by the generator, driving the authenticity and semantic consistency of the generated samples. The integrative loss function is defined as follows:\n$L_{com} = MSE(X, \\mu_{\\theta}(z)) + \\beta KL(q_{\\phi}(z|X,y)||p(z)) + \\gamma (logD(X, y) + log(1 \u2013 D(\\mu_{\\theta}(z), \\pi_{\\rho}(z))))$ (11)\nHere MSE is used to quantify the accuracy of the reconstruction, while KL scatter helps to regularize the latent space. \u03b2 and \u03b3 are moderating terms used to balance the influence of the different parts of the loss function to suit specific training needs. \u03b2 controls the influence of KL scatter, which is typically used to regulate the encoding constraints in the latent space. \u03b3 adjusts"}, {"title": "4. Semantic transfer based on dynamic compression rate adjustment", "content": "Once the semantically labeled data y' is extracted from the 2D human body image at the sender, it is particularly important to transmit the data efficiently and reliably to the receiver. Considering the dynamic nature of network environments, especially in mobile or international communication scenarios, transmission efficiency and stability are often challenged by bandwidth fluctuations and delay variations. In order to optimize the transmission of semantic data, this paper proposes a dynamic compression rate adjustment method, which is specifically adapted and optimized for semantically tagged data y' to ensure that high efficiency and data integrity are maintained under different network conditions.\nConsidering that the semantic label y' may be closely related to the criticality of a particular task, we define a formula to quantify the importance of each label, with attributes of the label containing type, frequency, and contextual relevance.\nSuppose that each tag is associated with an importance weight w\u2081 that reflects its importance in a particular task. We define this weight using the following formula:\n$\\omega_{i} = \\tau\\cdot exp(-\\frac{d(y_{i}, y_{contex})}{\\sigma})$ (14)\nWheret is a task-dependent adjustment factor with respect to the type of the label. d(yi, ycontex) denotes the average distance between the label $y_{i}$ and its contextual label set $Y_{contex}$, and \u03c3 is a normalization factor to adjust for the effect of distance.\nWe can get instant feedback on current network conditions by periodically sending control packets and measuring their response times. This data helps us evaluate the network congestion level and transmission speed in real time, which provides the basis for the next compression rate adjustment. Based on the real-time monitored bandwidth and latency data, we optimize the compression rate of semantically tagged data y' using a dynamic tuning method that uses an exponentially weighted moving average (EWMA) approach to smooth the bandwidth and latency measurements, thereby reducing the impact of abrupt changes on the compression rate tuning. The expressions for bandwidth and delay monitored by the method are given below:\n$B(t) = \\varphi B(t - 1) + (1 - \\varphi) (\\frac{S_{i}}{A_{ti}})$ (15)\n$D(t) = \\frac{\\sum_{i=1}^{m} W_{it} RTT_{i}}{\\sum_{i=1}^{m} W_{i}}$ (16)\nWhere $S_{i}$ and $A_{ti}$ represent the packet size and time interval of the i-th transmission within the time window, respectively, o is the attenuation factor of the bandwidth measurement, and $w_{i}$ is the weight of the i-th response time in the delay measurement. In this way, we are able to obtain smoother and more reliable estimates of the network state, providing accurate inputs for compression rate tuning. Using the importance weights wi obtained above, we design a dynamic compression rate adjustment formula that is not only based on the importance of labels but also considers the real-time network condition:\n$\\rho(y_{i}) = \\rho_{min} + (\\rho_{max} - \\rho_{min})\\cdot (1-\\frac{w_{i}}{max(w)})\\cdot f(B(t), D(t))$ (17)\nWhere w\u2081 is the importance weight of semantic label y' and W is the set of importance weights of all labels. And f(B(t),D(t)) is a network bandwidth and latency based adjustment function defined as follows:\n$f(B(t), D(t)) = a (\\frac{B_{max}-B(t)}{B_{max}} ) +b(\\frac{D(t)-D_{min}}{D_{max}})$ (18)\na and b are coefficients to balance the effects of bandwidth and delay on the compression rate. \u0412\u0442\u0430\u0445, \u0412min, Dmax, Dmin denote the maximum and minimum values of bandwidth and delay, respectively, which are used to normalize these measurements.\nThis dynamic adjustment strategy ensures the flexibility and adaptability of semantic data transmission, which can intelligently optimize the data flow according to the changing network conditions and the importance of the data. With this dynamic compression technique, the transmission of semantically labeled data y' is more efficient and stable."}, {"title": "5 3D reconstruction based on semantic data", "content": "With semantic transfer, we can utilize the received semantic data at the receiver side for 3D human reconstruction. This process involves two key techniques: firstly, multilevel semantic feature decoding, and secondly, improved ViT-diffusion modeling for 3D reconstruction. We will describe in this chapter how to recover image features from semantic tags and utilize these features to guide the generation of 3D models."}, {"title": "5.1 Multi-level semantic feature decoding", "content": "In the multilevel semantic feature decoding section, we are concerned with accurately reconstructing the complete set of image features from the semantic labels y'. This process involves extracting the necessary information from a highly parameterized latent space and progressively reconstructing the detailed features of the image through a series of complex decoding layers.\nThe encoder at the receiver side first maps the received semantic signal y' to a parameterized potential space. This mapping is realized by a Conditional Variational Auto Encoder (CVAE) that employs a hybrid Gaussian Model (GMM) [26] to express this potential space, thus capturing the complex distribution and intrinsic variability of the input data. The process is represented as:\n$\\mu_{\\varphi}, \\Sigma_{\\varphi} = Encoder(y', \\$)$ (19)\n$q_{\\phi}(z'|y') = \\sum_{k=1}^{K} \\pi_{k} N(z';\\mu_{\\varphi,k}, \\Sigma_{\\varphi,k})$ (20)\nWhere the encoder Encoder is still implemented using CVAE, outputting the mean and covariance of the latent variable to provide the required parameters for the subsequent sampling steps. \u03c0k is the mixing weight of the k-th Gaussian distribution, which indicates that different regions in the latent space correspond to different semantic features. K is the number of Gaussian distributions. $\\mu_{\\varphi}, \\Sigma_{\\varphi}$ are respectively the mean and covariance matrix.\nOnce the distribution of the latent space has been defined, the next step is to sample from this distribution to generate the latent feature vector z' that can be used for further decoding. Sampling using the Cholesky decomposition ensures that samples are drawn stably from each component of the Gaussian mixture model by:\nz' = $\\mu_{\\varphi,k} + chol(\\Sigma_{\\varphi,k})\\cdot\\epsilon,\\epsilon~~(0,1)$ (21)\nWhere chol denotes the Cholesky decomposition, which decomposes the covariance matrix into a product of a lower triangular matrix and its transpose, thus allowing us to generate the desired samples from the standard normal distribution by a simple transformation.\nAfter the potential feature vectors z' are generated, the next task is to reconstruct the original image features by decoding these features layer by layer, with each layer combining the output from the previous layer and the current potential state. During decoding at each level, not only the current potential features z' are used, but also the information hi-1 passed from the previous layer is utilized. This integration of information is achieved through recurrent neural networks [27], which can process and maintain the flow of information across multiple decoding stages:\n$s_{i} = f_{i}(h_{i-1}, z'; \\theta_{i}), h_{i} = RNN(h_{i-1}, s_{i}; \\Psi_{i})$ (22)\nWhere si denotes the output of the decoded layer i, which is the local features of the image, and hi is the hidden state of the RNN, which is used to maintain and transfer the dependencies and information between layers. Where $h_{i}$ denotes the hidden state of the RNN and feedback is used to handle the complex dependencies between layers. The loss function is designed to balance the accuracy of reconstruction and regularization of the latent space, as well as the adaptability of the model to new semantic information, denoted as follows:\n$L = \\sum_{i=1}^{L} g_{k} ||s_{i} - \\hat{s_{i}}||^{2} + g_{KL} KL(q(z'|y')||p(z')) + g_{reg}\\cdot R(z')$ (23)\nWhere $s_{i}$ denotes the final output features of layer i, i.e., the features that are integrated and ready to be passed on to the next layer, and R(z') is an additional regularization term designed to encourage the diversity of latent variables and to improve the model's generalization ability and adaptability to new semantic information."}, {"title": "5.2 Improved ViT-diffusion model for 3D reconstruction", "content": "In this section we explore how to utilize improved and diffused models based on the visual transformer (ViT) in order to achieve 3D human reconstruction based on image features si. This process involves not only adapting the structure of the ViT to accommodate multi-scale feature extraction, but also how these features can be effectively applied to the diffusion and inverse diffusion processes, ultimately generating accurate 3D models.\nFor traditional ViT, the input image is usually divided into blocks (patches) of uniform size. In our improved model, we introduce multiple parallel ViT paths, each processing image chunks of different scales to accommodate multi-level feature capture from local details to global structures, denoted as:\n$u_{m} = \\sum_{k} \\omega_{k}\\cdot ViT_{k}(Rescale(s_{i}, r_{k}); \\theta_{ViT,k})$ (24)\nWhere k denotes different scale paths, wk is a weight that is dynamically adjusted according to the importance of features at each scale, Rescale(si, rk) is a function that adjusts the input image si to the scale rk, and $O_{vit,k}$ is the ViT model parameter of the corresponding scale.\nIn order to better adapt ViT to the needs of transitioning from images to 3D models, we introduce a dynamic attention adjustment mechanism based on the input feature si. This allows ViT to adjust its internal attention allocation according to different regions of the image content.\nAttention(Q, K, V, si) = softmax (\\frac{QKT}{\\sqrt{d_{k}}} + B(s_{i})) V (25)\nWhere B(si) is an adjustment matrix dynamically generated based on the input features to adjust the attention weights.\nWe then incorporate the ViT-extracted feature um to guide the diffusion process, an approach that allows us to accurately control the process of generating a 3D point cloud based on the high-dimensional information extracted by deep learning, thus enhancing the representativeness of the model and the accuracy of the generated 3D structure:\n$X_{t} = \\sqrt{C_{k}}X_{t-1}+\\sqrt{1 \u2013 c_{k}}(u_{m} \\odot \\epsilon), \\epsilon ~N(0,1)$ (26)\nWhere Ck is the noise control coefficient, which is used to adjust the proportion of noise. This noise is not a simple random perturbation, but is regulated by the feature um extracted by ViT. The purpose of this is to ensure that the addition of noise reflects the structure and content characteristics of the image itself, so that the generated point cloud is closer to the real 3D structure of the original image.\nThen the noise is gradually reduced and structured 3D data is recovered through an inverse diffusion process using the same conditionalization strategy. The inverse diffusion formula is:\n$X_{t-1} = \\frac{1}{\\sqrt{c_{k}}}(X_{t} - \\sqrt{1 \u2013 C_{k}}u_{m})$ (27)\nA 3D mesh is generated from the final optimized point cloud Xo using the Marching Cubes algorithm:\nMesh = MarchingCubes (X) (28)"}, {"title": "Algorithm 2 3D Reconstruction Based on Semantic Data", "content": "1: Input: Semantic labels y'\n2: Output: 3D human model\n3: procedure MULTI-LEVEL SEMANTIC FEATURE DECODING(y')\n4: Encode y' into latent space using CVAE:\n5: $\\mu_{\\varphi"}, "Sigma_{\\varphi}$\u2190 Encoder(y', $)\n6: 9$('y') \u2190 $\\sum_{k=1}^{K} \\pi_{k}N (2';$\\mu_{\\varphi,k}, $\\Sigma_{\\varphi,k})$\n7: Sample z' from q(z'y') using Cholesky decomposition:\n8: z' \u2190 $\\mu_{\\varphi,k}$+ chol ($\\Sigma_{\\varphi,k})\\cdot$\u20ac,\u20ac~N(0, I)\n9: Decode features from z' at each layer:\n10: for each layer i do\n11: $s_{i}$ \u2190 $f_{i}(h_{i-1},2'; \\theta_{i})$\n12: $h_{i}$ \u2190 RNN($h_{i-1}, s_{i}; \\Psi_{i})$\n13: end for\n14: Optimize loss:\n15: L\u2190$\\sum_{i=1}$9k||$s_{i} - \\hat{s_{i}}$||2+9KLKL(q$(z'y')$||p(z')) + $g_{reg}. R(2')\n16: end procedure\n17: procedure 3D RECONSTRUCTION WITH VIT-DIFFUSION MODEL($s_{i}$)\n18: Adapt ViT to multiple scales and integrate features:\n19: $u_{m}$ \u2190 $\\sum_{k}w_{k}$. ViT$_{k}$ (Rescale($s_{i}, r_{k}$); $(\\theta_{ViT,k})$\n20: Apply diffusion process guided by $u_{m}$:\n21: Xt\u2190 \u221a$C_{k}$Xt-1 + \u221a$1 - C_{k}$ ($u_{m}$ \u20ac), \u20ac ~ N (0, 1)\n22: Reverse diffusion to recover structured 3D data:\n23: X$_{t-1}$\u2190$\\{\\sqrt{c_{k}}}(X_{t} - \\sqrt{1 \u2013 C_{k}}u_{m})$\n24: Generate 3D mesh using Marching Cubes:\n25: Mesh \u2190 MarchingCubes("]}