{"title": "GROUNDING BY TRYING: LLMS WITH REINFORCE-MENT LEARNING-ENHANCED RETRIEVAL", "authors": ["Sheryl Hsu", "Omar Khattab", "Chelsea Finn", "Archit Sharma"], "abstract": "The hallucinations of large language models (LLMs) are increasingly mitigated by allowing LLMs to search for information and to ground their answers in real sources. Unfortunately, LLMs often struggle with posing the right search queries, especially when dealing with complex or otherwise indirect topics. Observing that LLMs can learn to search for relevant facts by trying different queries and learning to up-weight queries that successfully produce relevant results, we introduce Learning to Retrieve by Trying (LeReT), a reinforcement learning framework that explores search queries and uses preference-based optimization to improve their quality. LeReT can improve the absolute retrieval accuracy by up to 29% and the downstream generator evaluations by 17%. The simplicity and flexibility of LeReT allows it to be applied to arbitrary off-the-shelf retrievers and makes it a promising technique for improving general LLM pipelines. Project website: http://sherylhsu.com/LeReT/.", "sections": [{"title": "INTRODUCTION", "content": "Despite tremendous progress, large language models (LLMs) still often hallucinate, motivating significant interest in grounding LLM answers in verified sources (Guu et al., 2020; Komeili et al., 2022; PerplexityAI, 2024; Google, 2024; OpenAI, 2024). Unfortunately, simply retrieving semantically similar documents to the user question, as is prevalent in retrieval-augmented generation (RAG; Lewis et al. 2020) pipelines, tends to fail for complex information needs not answered directly by any individual document. To tackle this, multi-hop retrieval pipelines gather information incrementally over multiple steps of search. For example, if a user asks What is a good dinner place driving from the Bay Area to Lake Tahoe on Friday night to avoid traffic?, a grounded system might need to learn about towns en route Lake Tahoe from the Bay Area, followed by traffic forecast on I-80 and finally, restaurants in Auburn (and other towns).\nHowever, doing this successfully is hard as off-the-shelf LLM performance is often unsatisfactory, and producing supervision for the best search queries to generate in a sequence of \u201chops\u201d is nontrivial and expensive. Recent work tackles this via prompt optimization and rejection fine-tuning given a downstream signal. For example, Khattab et al. (2023) \"bootstrap\" trajectories of reasoning and search queries and collect trajectories that lead to a high downstream answer accuracy, using them either to search for effective few-shot prompting examples or to finetune the LLM responsible for query generation. We observe that the problem of teaching a LLM to generate effective search queries is inherently a reinforcement learning (RL) problem and ask can RL improve the grounding of answers generated by LLMs when wielding open-ended tools like search engines?\nIf LLMs can observe the retrieved documents for different search queries, and receive rewards for finding relevant documents, they could learn which queries lead to better outcomes. Such learning from trial-and-error naturally lends itself to RL formalism, going beyond imitation-based methods in prior works. Indeed, we find that na\u00efve sampling from LLMs with high temperature and using the observed data for RL is not effective. Instead, our proposed framework, Learning to Retrieve by Trying, or LeReT, induces diverse search queries for each question by diversifying the few-shot examples in the prompts of the system. After this diversified sampling of search queries and the resulting retrieval, LeReT applies context distillation (Snell et al., 2022) followed by optimizing the"}, {"title": "RELATED WORK", "content": "Retrieval-Augmented Generation (RAG). Over the past few years, interest has been growing in conditioning LLM outputs on retrieved information (Chen et al., 2017; Lee et al., 2019; Guu et al., 2020; Lewis et al., 2020; Lazaridou et al., 2022; Asai et al., 2024). This strategy seeks to make LLM systems more efficient, updatable, and transparent by decoupling the system's knowledge from the model parameters. This makes it easy to update the knowledge corpus and also makes it possible to inspect the sources relied upon when LLMs produce factual statements.\nMulti-Hop Retrieval. The standard RAG formulation is best suited for \u201csimple\" questions, where a direct search can find all the information required for producing responses. Beyond these, benchmarks such as HotPotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022), and HoVer (Jiang et al., 2020) assess systems on gathering and synthesizing information from several independent documents within a massive corpus like Wikipedia. To tackle retrieval in this setting, early systems like MDR (Xiong et al.) and Baleen (Khattab et al., 2021) introduced bespoke strategies for finetuning the retrieval models that produce representations of queries and documents, adapting them directly for compositional search queries. Unfortunately, fine-tuning the retriever representations is a data hungry approach that is also challenging to scale to massive corpora like the Web, as re-training the retriever often requires re-indexing the corpus. Increasingly, research in this space (Trivedi et al., 2023),"}, {"title": "PRELIMINARIES", "content": "Multi-Hop Retrieval Setup. An overview of retrieval is shown in Figure 2. We assume access to a retriever that maps a search query q to a set of N most similar documents D = {di}i=1^N , where\ndi denotes an individual document. A user asks a question u, a LLM \u03c0r generates a query q1 for\nthe retriever, which results in an ordered set of document D1. In the next hop, the LLM \u03c0r takes u\nand D1 as input and outputs query q2. This repeats for H hops. The final ordered set of retrieved\ndocuments DR = D1 \u222a D2 \u222a . . . DH is given as input to the LLM generator \u03c0g along with the\nquestion u, which generates the final answer for the user query. In this work, we restrict ourselves\nto fine-tuning the LLM \u03c0r and treat the retriever and LLM generator \u03c0g as blackbox models.\nLanguage Models and Reinforcement Learning. Reinforcement learning has become the de facto\ntool for aligning large language models and has inspired considerable work at the intersection of\nlanguage models and RL (Stiennon et al., 2020; Ouyang et al., 2022; Zhao et al., 2023; Rafailov\net al., 2024b). We briefly review direct alignment methods (Rafailov et al., 2024b). Given a dataset\nof preferences Dp = {(xi, yw, yj )}, where xi denotes the dialogue history, yw denotes the preferred\nresponse and yj denotes the dispreferred response. Bradley-Terry (Bradley & Terry, 1952) offers a\nmodel that connects choice to an implicit goodness score, useful for learning a reward model r\u03d5:\nLBT = \u2212E(x,yw,y1)\u223cDp [logP\u03d5(yw > y1)] = \u2212E(x,yw,y1)\u223cDp [logo (r\u03d5(x, yw) \u2212 r\u03d5(x,yj))], (1)"}, {"title": "LERET: LEARNING TO RETRIEVE BY TRYING", "content": "We introduce Learning to Retrieve by Trying, or LeReT, a novel framework for improving the\ngrounding of LLM generations by training the search query LLM \u03c0r using preference optimization.\nIn hop i, we sample a query qi from the LLM based on the user question and documents seen in\nhops < i, and observe a reward signal for the retrieval quality. Both sampling from the LLM and\nretrieval make it hard to backpropagate directly from the reward signal, making RL a more suitable\noptimization framework for this setting. We first discuss how to generate a dataset of queries and\nretrieved documents that is suitable for RL optimization in Section 4.1. We then discuss how to\nconvert the reward-annotated dataset into a dataset of preferred and dispreferred queries and use that\ndataset to optimize \u03c0r with IPO in Section 4.2. We also briefly discuss an iterative version of LeReT\nthat alternates between sampling and optimization. Finally, we combine the elements and give a\npractical overview in Section 4.3."}, {"title": "PROMPT DRIVEN DIVERSE QUERY GENERATION", "content": "Given a dataset of questions, we want to \"try\" a set of search queries and observe the retrieved\ndocuments. What queries would be good to observe the retrieved documents for? This roughly cor-\nresponds to the exploration problem in RL. As our experiments later in Section 5.4 also indicate, a\ngood distribution of queries would result in diverse outcomes (for better exploration), but it is impor-\ntant that some queries produce high quality retrievals. To sample such diverse and effective queries,\nLeReT moves beyond high-temperature sampling and uses a diverse set of examples to few-shot\nprompt the LLM \u03c0r. We use DSPy (Khattab et al., 2022; 2023)'s prompt optimizers, specifically\na simple BootstrapFewShotWithRandomSearch (BFRS), to self-generate a number of in-\ndependently optimized few-shot, chain-of-thought prompts P = {P1,...,Pp} for LLM \u03c0r. The\nindependently optimized prompts would naturally lead to diverse samples from \u03c0r, and DSPy's op-\ntimization ensures that the prompts are still resulting in relevant retrievals. Note that we can reuse\nthe same set of prompts across all questions throughout the dataset.\nFor a hop h, LeReT does the following: For every question u \u2208 U, LeReT samples search\nqueries conditioned on each of the prompts p\u2208 P, resulting in a set of search queries\nQh = {\u03c0r(\u00b7 | Pi, u, Ch\u22121)) | Pi \u2208 P}, where Ch\u22121 denotes the context from previous hops. For\nevery query qi \u2208 Qh, we retrieve a set of documents denoted Chi and compute the reward\ncorresponding to each query by evaluating the retrieved documents, that is rret(u, Ch\u22121,qi) =\nR(Chi) (more on retrieval reward R in Section 4.3). The final dataset for this hop consists of\nDh = {(u, Ch\u22121, qi, r(u, Ch\u22121, qi)) | u \u2208 U, qi \u2208 Qh}, where every entry consists of the user ques-\ntion, the context from the previous hop, the sampled query, and the reward computed by running the\nretriever. Though the prompts we generate are used to sample diverse high quality search queries\nat training time, we leverage context distillation to remove the need for optimized prompting at\ntest-time.\nWhen training in a multi-hop setting like this, we must select which contexts to use for the next hop.\nNa\u00efvely generating queries for every possible context leads to an exponentially growing dataset with\nrespect to the number of hops, which becomes computationally infeasible quickly. At the end of"}, {"title": "MODEL OPTIMIZATION", "content": "Given the training dataset Dtr, we want to update the LLM \u03c0r. First, we make a simplification\nby optimizing every hop greedily, that is, based on the reward obtained in that hop alone. Ideally,\nupdating \u03c0r on data in hop h accounts for rewards obtained in all future hops. Retrieving relevant\ndocuments in earlier hops is likely always better, and intuitively, retrieving irrelevant documents in\nearlier hops will rarely lead to better overall retrieval. We verify this assumption empirically: For\ntwo sets of retrieved documents, a low reward and high reward set, a low reward retrieval leads\nto a higher total reward in only 0.026% of the cases (Appendix B.1). Thus, the greedy approach\nconsiderably simplifies the optimization, without any evident theoretical or empirical sacrifice.\nGiven the dataset Dtr, we can optimize \u03c0r using any RL algorithm. In this work, we consider\npreference-based RL approaches, specifically IPO described in Section 3 because of its simplicity\nand effectiveness. To optimize \u03c0r using IPO, we need to transform Dtr into a preference dataset.\nThis can be done straightforwardly by comparing two search queries qi and qj for the same user\nquestion and context and choosing the query with higher reward as the preferred response and the\nother query as the dispreferred response. However, before we can optimize \u03c0r using IPO, we must\naccount for the fact that the search queries were sampled using few-shot prompting, and at test-time\nwe will not be using the prompt. To do so, we leverage context distillation (Snell et al., 2022)\nby fine-tuning on the search queries without the context. We observe in our experiments that this\nroughly matches the performance of few-shot prompting. After context distillation and converting\nthe training dataset into a preference dataset, we optimize \u03c0r using IPO.\nIterative-LeReT. Thus far, we have assumed that sampling search queries and model training are\ndone as two separate steps. However, we can alternate between the two steps, leveraging the im-\nprovements in previous iterations to sample better data for the next iterations. Iterative-LeReT\nclosely follows iterative-DPO (Xu et al., 2024). We partition the dataset of user questions U and\nrun LeReT on each partition, sampling from the model fine-tuned on the previous partitions. Specif-\nically, we have I data partitions U1, . . ., UI. We start with the LLM \u03c00 and apply LeReT on U1 to\nobtain the fine-tuned LLM \u03c01, so we have LeReT (\u03c00, U1) \u2192 \u03c01. We use LeReT on U2, using \u03c01 to\nsample the training data and continue fine-tuning from, that is, LeReT (\u03c01, U2) \u2192 \u03c02. We can repeat\nthis for all I partitions until we get the final model \u03c0I. We find that iterative sampling and training\ncan be effective as models may not achieve accurate and relevant retrievals in the initial iterations,\nand later models may be able to generate better exploration data."}, {"title": "REWARD LABELING FOR RETRIEVED DOCUMENTS", "content": "In order to construct the required preference datasets, we need a reward signal R to score the doc-\numents retrieved by a search query. How do we compute this reward signal? There are broadly\ntwo ways to get such supervision: direct supervision, where a human provides oracle documents to\nground the answers in the training dataset or explicitly reviews the relevance of documents retrieved\nby the search query and indirect supervision, where the supervision comes from evaluations of the\ndownstream generator such as preference feedback on the final answers or some answer verification.\nThe latter is indirect because we do not obtain any explicit supervision for retrieval, but only receive\ninformation about how the generator performed after being conditioned on the retrieved documents.\nWe run a short study in Section B.5 comparing the two forms of supervision, and find that direct\nsupervision results in better performing models. However, a full study comparing direct and indirect\nforms of supervision and their trade-offs is beyond the scope of this paper, and potentially requires\nnovel algorithmic considerations. For the majority of the paper, we assume some form of direct\nsupervision, as allowed by commonly used datasets and benchmarks."}, {"title": "EXPERIMENTAL EVALUATION", "content": "We now evaluate how LeReT impacts the quality of retrieval and of downstream generation. We\nfirst test LeReT on two multi-hop question answering datasets, finding that LeReT significantly\noutperforms baselines such as few-shot prompting and supervised fine-tuning. We also find that\napplying LeReT iteratively leads to further improvement over iterations. We analyze prompt driven\ndiverse sampling in contrast with sampling using high temperature and also discuss different reward\nfunctions for the retrieval step. Finally, we evaluate LeReT's adaptability for various pipelines by\ntesting it against retrievers.\nDatasets. We test LeReT on HotpotQA (Yang et al., 2018) and HoVer (Jiang et al., 2020). Both\ndatasets are based on a Wikipedia knowledge base and are multi-hop, meaning that models must\nreason across multiple articles to arrive at the correct answer. The datasets provide both the correct\nanswer and supporting articles. HotpotQA is a question-answering dataset that requires up to 2 hops,\nwhile HoVer is a fact verification dataset that requires up to 4 hops. Both datasets are relatively large,\nallowing us to train on over 10,000 questions.\nEvaluation metrics. We measure retrieval performance using recall and average precision. Recall\nis the number of correctly retrieved documents over the total number of correct documents. Average\nprecision (Eq. 3 in the appendix) takes into account the ordering of the documents, i.e. if the correct\n3 documents are ranked as the last 3 out of 6 the score will be lower. For generation, we measure\nboth exact match on the entire answer and F1 at the word level."}, {"title": "RESULTS ON HOTPOTQA & HOVER", "content": "In terms of retrieval recall, LeReT improves recall by 9-22% on HotPotQA and 27-29% on HoVer\nrelative to the Llama and Gemma unadapted instruct models (\u201cbase\u201d). This substantially exceeds the\ngains achieved via few-shot prompting alone, showing that sampling from multiple few shot prompt\nensembles and training the model with RL is crucial. The gains also compounds over hops, possibly\nbecause lower quality search queries at a given hop distract future steps. We feed the improved\nretrievals into Llama 3.1 70b, asking it to generate a response to the question using the provided\ncontext. We find that improving retrieval produces a corresponding improvement in generations,\nwith the generator exact match increasing at approximately half the rate of recall."}, {"title": "ITERATIVE-LERET", "content": "We evaluate the performance of applying LeReT for two iterations. Training with only half the data\n(iteration 1) results in slightly worse performance compared to standard non-iterative LeReT, but"}, {"title": "FACTUALITY WITH DIFFERENT GENERATORS", "content": "Improving retrieval seeks to improve LLM grounding. Intuitively, stronger models with better reasoning capabilities should benefit more from having the correct documents to reason with than weaker models that may not be able to generate the correct answer even with the right documents. To assess this, we take the retrieval output by various Llama 3 8b models for HotpotQA and condition various generator models with them. As seen in Table 3, stronger generators deliver higher quality and larger gains when supplied with LeReT-trained retrieval contexts. We note that although GPT4 has the largest improvement, it does not have the highest score. Examining the generations, we see that GPT4 very closely followed the instructions to \u201cbase your answers only on the provided context\" and would output statements such as \"answer cannot be found in the context\" instead of trying to answer it anyway the way weaker models did."}, {"title": "DIVERSE FEW-SHOT PROMPTING VERSUS HIGH-TEMPERATURE SAMPLING", "content": "While prior work typically uses high temperature sampling to generate diversity, LeReT leverages\nfew-shot prompting to generate diverse exploration data. To evaluate the effectiveness of few-shot\nprompt diversification, we consider alternate sampling strategies, particularly sampling from \u03c0r at\ndifferent temperatures with no few-shot prompting, a fixed few-shot prompt (fixed), or multiple few\nshot prompts (diverse).\nFirst, we compute some statistics about the rewards generated by the sampled search queries: we\ncompute the average number of unique rewards per question and the standard deviation of the re-\nwards as a proxy for diversity, and we measure the percentage of questions where at least one query\n(gold star answer) achieves maximal reward as a proxy for quality of sampled data. We find in\nTable 4 that while sampling with higher temperature improves diversity in search queries, few-shot\nprompting leads to significantly higher quality data and using multiple few-shot prompts provides\ncomparable diversity.\nWe train on four of the sampled datasets: (1) queries sampled with diverse few-shot prompting at\nstandard temperature (0.7), (2) queries sampled at a high temperature (2.0), (3) queries sampled with"}, {"title": "DIFFERENT RETRIEVERS", "content": "Finally, we test whether LeReT is applicable to general RAG systems by swapping our retriever from ColBERT over Wikipedia to Azure AI Search, applied with the default configuration for full text search. We observe that the base Llama model performs very poorly compared to its retrievals with ColBERT. This is likely because Azure is not specialized to the Wikipedia index which is helpful for our multi-hop tasks. However, the query generating LLM can adapt to compensate for this weaker retriever, as we see significant improvement with few-shot prompting and LeReT. This demonstrates the power of LeReT to adapt to general blackbox tools in the pipeline. Given the poor performance of the base model, few-shot prompting based exploration (Section 5.4) is found to be necessary, versus simply sampling with high temperature."}, {"title": "DISCUSSION", "content": "In this work, we introduced LeReT, a framework for improving the quality of multi-hop retrieval via reinforcement learning and thus enabling better grounding for LLM systems. Beyond retrieval specifically, this can be extended to learning for agentic systems or LLM programs that use other tools. LeReT conducts diversified exploration of search queries in each hop by sampling using varied optimized few-shot prompts. It then uses this to construct a preference dataset for every hop"}, {"title": "SAMPLING AND TRAINING DETAILS", "content": "We sample with P = 4 for HotpotQA and P = 3 for HoVer. We implement our sampling pipeline on top of DSPy (Khattab et al., 2023), specifically defining a single hop as a program and sampling data using the evaluate functions. We also use chain-of-thought prompting when generating queries.\nWe use a learning rate of 1e-7 for SFT/context distillation in all our experiments, and use a \u03b3 = 0.05 and learning rate of 1e-7. We train SFT for 1 epoch, and we only distill the best performing prompt. We train IPO for 2 epochs."}, {"title": "DATA SCALING ANALYSIS", "content": "We conduct data scaling experiments for LeReT. We evaluated a training run of Llama 3 8b on the full HotpotQA training set (90,447 questions), which resulted in 494,208 preference pairs after prompt driven diverse sampling. We find that the majority of the improvement occurs relatively quickly. Based on this, we only use a quarter of the HotpotQA training set for subsequent experiments. However, data scaling likely depends on a host of factors, including the task complexity and the base model, and we conducted the data scaling experiment to reduce the computational cost of our experiments."}, {"title": "JUSTIFYING GREEDY OPTIMIZATION", "content": "We run LeReT for two hops on all 90,447 questions in HotpotQA. After sampling the first hop, we chose two sets of documents that have different rewards. We then run the second hop with no few-shot prompting and evaluate the reward after the second hop. We find that the lower reward set of documents resulted in a higher reward after the second hop in only 0.026% of cases."}, {"title": "AVERAGE PRECISION", "content": "Average precision is defined according to Eq. 3 where R is the total number of relevant documents, P(k) is the precision of the first k documents, and rel(k) is 1 if the kth document is relevant and 0 otherwise:\nAP = 1/R * \u2211k=1^N P(k) * rel(k) (3)"}, {"title": "MULTI-HOP SAMPLING", "content": "Can we get away without training on data from all hops? We run an ablation to determine the necessity of sampling across multiple hops. Sampling across multiple hops requires is computationally and less parallelizable. Specifically, we train Llama 3 8b on Hotpot and HoVer, sampling only the data from the first half of the hops. For Hotpot, this amounts to sampling from just the first hop and for HoVer, this amounts to sampling data from the first two hops."}, {"title": "LONG FORM GENERATIONS", "content": "We present a preliminary attempt at long form generation. The majority of LLM use cases are for long form generation, and as such we want to test LeReT's ability to improve long form generations. In addition, it is more to difficult to evaluate the factuality of long form answers, meaning that evaluating the relevance of the documents it conditioned its answer on as LeReT does may be simpler. Current retrieval datasets focus on short question answering, leading us to generate our own long form dataset.\nTo create a dataset with open ended questions that still had correct retrievals, we prompted GPT for 20 broad topics. From each of those 20 broad topics, we prompted GPT for 500 topics, giving us a total of 10,000 topics such as \"Injuries in American football\" or \"Effects of mobile radiation on human health\". We then fed those topics into Colbert and retrieved the top 10 wikipedia abstracts. We then prompt GPT with the 10 wikiepdia abstracts and asked it to come up with a question that required students to use exactly 3 of the 10 articles. This gave it the freedom to choose articles that were closely related and led to more natural questions than forcing it to use a given 3 articles.\nWe then train Llama 3 8b on this dataset using LeReT. We find an approximately 8.47% improvement in document retrieval. We create long form generations by feeding the retrieved documents into Llama 3.1 70B. We find that the LeReT-generations are superior to few shot prompting with a 55.56% win rate."}, {"title": "POSSIBLE REWARD FUNCTIONS", "content": "Do we need direct supervision in LeReT for computing the reward function that outputs a reward for a given question and set of retrieved documents or can we get away with indirect methods for supervising retrieval quality? We currently use average precision of retrieved documents, which provides more direct supervision for retrieval but requires knowing a correct set of documents in advance, as is available in Hotpot/HoVer. But, there are settings where the optimal retrieved documents may be hard to specify in advance. In such cases, indirect supervision may be easier to provide, where the final generated answer conditioned on the retrieved documents is reviewed, and a reward is generated based on the verification of the final answer. Such supervision can be quite weak and have a high amount of noise, as the generator may answer correctly even when conditioned on incorrect documents (because of internal knowledge) or provide incorrect answers even when conditioned on the right documents.\nTo explore these different settings, we apply LeReT using the F1 score of the final generated answer as the reward. We condition the LLM on retrieved documents along with the actual question to generate an answer, and compare that to the correct answer. Formally, instead of using AP(Chi) as the reward, we use IR F1(\u03c0g (d, Chi)).\nWe find that the F1 score of the generator does not provide a very strong signal. For the preference dataset that we constructed using the generator, over 50% of the preference pairs are wrong. We split these incorrect pairs into two categories: hard and soft disagree. Hard disagree means that according to the F1 generation reward, qi > qj but according to the AP retrieval reward, qi < qj. Soft disagree means that according to the F1 generation reward, qi > qj but according to the AP retrieval reward, qi = qj. To reduce the number of soft disagrees, we experiment with adding a threshold for the difference in F1 score to form a preference pair, but find that over half the questions in HotpotQA and all the questions in HoVer have one word answers so this is not effective."}]}