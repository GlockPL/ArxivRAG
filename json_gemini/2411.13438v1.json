{"title": "Robust Monocular Visual Odometry using Curriculum Learning", "authors": ["Assaf Lahiany", "Oren Gal"], "abstract": "Curriculum Learning (CL), drawing inspiration from natural learning patterns observed in humans and animals, employs a systematic approach of gradually introducing increasingly complex training data during model development. Our work applies innovative CL methodologies to address the challenging geometric problem of monocular Visual Odometry (VO) estimation, which is essential for robot navigation in constrained environments. The primary objective of our research is to push the boundaries of current state-of-the-art (SOTA) benchmarks in monocular VO by investigating various curriculum learning strategies. We enhance the end-to-end Deep-Patch-Visual Odometry (DPVO) framework through the integration of novel CL approaches, with the goal of developing more resilient models capable of maintaining high performance across challenging environments and complex motion scenarios. Our research encompasses several distinctive CL strategies. We develop methods to evaluate sample difficulty based on trajectory motion characteristics, implement sophisticated adaptive scheduling through self-paced weighted loss mechanisms, and utilize reinforcement learning agents for dynamic adjustment of training emphasis. Through comprehensive evaluation on the real-world TartanAir dataset, our Curriculum Learning-based Deep-Patch-Visual Odometry (CL-DPVO) demonstrates superior performance compared to existing SOTA methods, including both feature-based and learning-based VO approaches. The results validate the effectiveness of integrating curriculum learning principles into visual odometry systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual Odometry (VO) is a crucial technique in robotics and computer vision that estimates an agent's egomotion, specifically, its position and orientation, based on visual input. While VO has shown promising results in controlled environments, its application in critical real-world scenarios, especially when sensors like GPS, LiDAR, and Inertial Measurement Units (IMUs) cannot be used, presents significant challenges that can compromise accuracy or lead to system failure. The performance of VO systems is particularly susceptible to dynamic motion patterns. High-frequency movements, abrupt camera tilts, and rapid maneuvers can introduce noise and discontinuities in the visual stream, complicating the extraction of reliable motion estimates. These challenges are further exacerbated in less-than-ideal, and often adverse, environmental conditions. A robust VO model must demonstrate resilience across a spectrum of visual contexts. Low-light scenarios, for instance, reduce the visibility of salient features necessary for accurate tracking. Motion blur, resulting from relative movement between the camera and the environment, introduces additional complexity to feature detection and matching algorithms. These multifaceted challenges underscore the need for advanced VO algorithms and training techniques capable of maintaining accuracy and reliability across a wide range of operational conditions. As such, addressing these issues remains a critical focus in the ongoing development of robust visual odometry systems for autonomous navigation and localization.\nTo address these challenges, we propose novel curriculum learning strategies integrated into the Deep-Patch-Visual-Odometry (DPVO) framework. Our approach prioritizes intelligent training methodologies over complex multi-modal architectures, aiming to enhance model robustness and reduce training resources while preserving inference computational efficiency."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Visual odometry (VO) has been a focal point of research in robotics and computer vision, with monocular VO gaining particular attention due to its cost-effectiveness and simplicity. Traditional monocular VO methods primarily relied on handcrafted features and geometric techniques, as demonstrated by Nist\u00e9r et al. (2004) and Scaramuzza and Fraundorfer (2011). While effective in controlled environments, these methods often face challenges in complex real-world scenarios due to scale ambiguity and sensitivity to environmental changes. The emergence of deep learning has revolutionized VO research, including monocular approaches. Kendall et al. (2015) introduced PoseNet [20], marking one of the first deep learning methods for camera relocalization, which laid the groundwork for end-to-end learning of pose estimation directly from images. Building on this foundation, Wang et al. (2017) developed DeepVO [9], showcasing the potential of recurrent neural networks to capture temporal dependencies in monocular VO tasks.\nRecent advancements have focused on enhancing the robustness of deep learning-based monocular VO systems. Zhan et al. (2019) introduced Unsupervised VO with Geometric Constraints (UnDeepVO) [21], which leverages unsupervised learning to address the limitations of supervised methods in real-world settings. Saputra et al. (2019) [5] proposed a novel approach that combines geometric and learning-based techniques to improve performance in challenging environments. Additionally, researchers like Zachary et al. (2023) [3] and Klenk et al. (2024) [16] have employed deep patch selection mechanisms to further enhance model accuracy and efficiency, including an advanced event-based variation. However, augmenting a model's proficiency in adapting to varied motion dynamics and visual degradation remains a significant challenge. Several methodologies have been proposed to mitigate input distortions and variability, yet each harbors inherent limitations:\nPreprocessing Enhancement: Utilizing techniques such as image deblurring [22] and super-resolution [23] before model inference. Although beneficial in certain contexts, this strategy results in information loss stemming from the enhancement models' presupposed priors on \"clean\" data statistics.\nSingle Model with Diverse Training: This involves training one model across a broad spectrum of input qualities and distortions. This method often necessitates extensive datasets and more sophisticated models for effective generalization [24],[25][26][27].\nEnsemble Methods: Training multiple models, each tailored to specific distortion ranges [7], [36]. While this method can be effective, it does not facilitate information exchange among models, thus limiting its ability to generalize across all input quality variations.\nData & Sensor Fusion: By integrating traditional camera imagery with event-based camera data, learning-based and model-based approaches have shown promise in improving accuracy under difficult conditions. Nonetheless, the management and integration of these diverse data sources significantly increase computational demands and system complexity [35]."}, {"title": "B. Curriculum learning", "content": "Curriculum learning, introduced by Bengio et al. in 2009 [2], offers a potential solution to these challenges. This approach involves designing a \"training curriculum\" that progressively introduces more difficult examples during the training process. Recent applications of curriculum learning in computer vision have shown promising results: Jiang et al. [8] presented a curriculum-based CNN for scene classification, where the training curriculum was based on image difficulty defined by the source of the image. In the domain of image segmentation, Wei et al. [19] proposed a curriculum learning approach where an initial model is trained on simple images using saliency maps, followed by the progressive inclusion of more complex samples. Weinshall et al. [3] investigated the robustness of curriculum learning across various computer vision tasks, highlighting its superiority in convergence compared to standard training methods."}, {"title": "C. Curriculum Learning in Visual Odometry", "content": "A critical aspect of curriculum learning is the requirement for explicit labels of task complexity for each training instance. In the context of visual odometry, this can be achieved by applying synthetic augmentations with controlled parameters (e.g., noise levels, blur, resolution degradation) to clean inputs and the use of diverse dynamic motion scenarios (e.g., maximum translation and rotation speed in recorded trajectories). Hacohen and Weinshall (2019) introduced a method for automatically determining the difficulty of training examples by combining transfer learning from teacher network, which could be adapted for VO tasks [6]. Another notable work includes Saputra et al. (2019) [5], which presented a novel CL strategy for learning the geometry of monocular VO by gradually making the learning objective more difficult during training using geometry-aware objective function."}, {"title": "III. METHODOLOGY", "content": "We propose a comprehensive curriculum learning framework for training Deep Visual Odometry systems that adaptively controls the learning progression across multiple components of the visual odometry task, enhancing model performance in real-world scenarios with different motion complexities and environmental conditions. Our approach implements both trajectory based difficulty assignment and dynamic progression strategies to optimize the training trajectory. The curriculum learning system manages three critical aspects of the visual odometry problem: optical flow estimation, pose prediction, and rotation estimation. Each component's difficulty is independently controlled through interpolation weights between initial (simpler) and final (more challenging) configurations. The framework incorporates three methodological approaches: (1) a trajectory-based approach where difficulties are pre-calculated based on camera motion characteristics and scene complexity metrics, (3) a self-paced learning strategy that dynamically adjusts the curriculum based on current loss values, and (4) an adaptive Deep Deterministic Policy Gradient (DDPG) based scheduler that learns to optimize the curriculum through reinforcement learning. While the first approache rely on predefined, per-trajectory progression scheme, the latter two dynamically adapt the difficulty levels in response to the model's performance, with self-paced learning using direct loss feedback and DDPG learning a more complex policy through experience."}, {"title": "A. Dataset", "content": "We train and evaluate our curriculum learning strategies on TartanAir dataset. TartanAir has become a widely adopted benchmark in monocular VO research since its introduction in 2020. One of its primary benefits lies in its synthetic nature, which allows for the creation of highly controlled precise and customizable environments using Unreal Engine's photorealistic capabilities. The dataset provides diverse sequences with varying motion patterns and scene complexities, which is crucial for developing robust monocular VO systems that can handle scale ambiguity and drift - common issues in single-camera setups. Its synthetic nature ensures perfect ground truth for both camera poses and optical flow, enabling precise evaluation of monocular performance without the uncertainties typically present in real-world datasets."}, {"title": "B. Baseline Model", "content": "As our baseline model architecture, we use the Deep Patch Visual Odometry (DPVO) model, introduced in [3]. DPVO represents a state-of-the-art approach to monocular visual odometry, demonstrating competitive performance across standard benchmarks through its patch-based deep learning framework. A key strength of DPVO lies in its end-to-end trainable nature and its inference computational efficiency allowing high FPS with minimal memory requirements. The learning-based end-to-end nature allows the model to learn more robust patch representations and matching strategies directly from data, while the patch-based approach provides better handling of local image structures. Upon its publication, DPVO demonstrated superior performance compared to existing monocular VO methods across standard evaluation metrics, including those utilizing comprehensive SLAM frameworks like DROID-SLAM [29]. Subsequently, DPVO has established itself as a fundamental baseline for numerous learning-based architectural enhancements, notably Deep-Event-Visual-Odometry (DEVO) [16], which leverages simulated event data to enhance system robustness. Given its proven capabilities, DPVO serves as an ideal baseline for investigating the impact of curriculum learning strategies, where we maintain its architecture and hyperparameters while modifying the training procedure and objective function through our proposed curriculum learning framework."}, {"title": "C. Model Supervision", "content": "At a high level, The DPVO approach works similarly to a classical VO system: it samples a set of patches for each video frame, estimates the 2D motion (optical flow) of each patch against each of its connected frames in patch graph, and solves for depth and camera poses that are consistent with the 2D motions. This approach differs from a classical system in that these steps are done through a recurrent neural network (update operator) and a differentiable optimization layer. DPVO apply supervision to poses and induced optical flow (i.e. trajectory updates), supervising each intermediate output of the update operator and detach the poses and patches from the gradient tape prior to each update. In its core, the DPVO define two types of supervisions:\nPose Supervision: By scaling the predicted trajectory to match the ground truth using the Umeyama alignment algorithm [33], every pair of poses (i, j), is supervise on the error:\n$\\sum_{(i,j) i\u2260 j}|| LogSE (3) [(G_i^{-1}G_j) (T_i^{-1}T_j)]||$\nwhere G is the ground truth and T are the predicted poses.\nFlow Supervision: Employs supervision based on the difference between predicted and ground truth optical flow fields within a \u00b12-frame temporal window. Both supervisions are incorporated into the overall loss through weighted summation:\n$L_{total} = 10L_{pose} + 0.1L_{flow}$"}, {"title": "D. Loss Structure", "content": "Our curriculum learning framework modifies the DPVO training objective by introducing weighted loss components that adapt throughout the training process. The total loss is structured as a nested weighted combination of flow, translation, and rotation components.\n$L_{pose} = (L_{translation} + W_rL_{rotation})$\n$L_{total} = W_fS_fL_{flow} + W_pS_pL_{pose}$\nThis hierarchical weighting scheme implements curriculum learning at multiple levels:\nBase Weights (Sf, Sp): Fixed weights that balance the fundamental trade-off between flow and pose estimation tasks. Ensuring correct magnitude scaling of the flow and pose losses. During are experiments we use sf = 0.1, sp = 10.\nCurriculum Weights (wf,wp,wr): Dynamic weights controlled by the curriculum learning scheduler that adjust the learning emphasis throughout training. Wf controls the importance of optical flow estimation, wp modulates the overall pose learning (affecting both translation and rotation) and wr specifically adjusts the rotation component within pose estimation. The hierarchical weighting scheme enables independent control over the learning progression of each component while preserving their natural relationships. The rotation weight wr operates within the broader pose estimation weight wp, allowing fine-grained control over rotation learning while maintaining the overall pose learning trajectory. Translation learning is implicitly controlled through the pose weight without requiring a separate translation weight, as it represents the fundamental aspect of pose estimation. This simplified structure reduces complexity by respecting the natural hierarchy of the visual odometry task, where translation serves as the base component of pose estimation, while rotation requires additional fine-tuning through its dedicated weight."}, {"title": "E. Curriculum-Learning Strategies", "content": "Trajectory-Based\nThe trajectory-based curriculum learning strategy begins with a preprocessing phase to quantify motion complexity across the dataset. We compute the maximum translational and rotational magnitudes for each sequence by analyzing frame-to-frame pose differences. We than normalizes the maximum translation and rotation values to ensure comparable scale and produce a combined difficulty score as a weighted average of the normalized values for each sequence. In this strategy we use the original DPVO objective function (2). \nSelf-paced Progression\nThe self-paced learning strategy implements a dynamic curriculum that adapts based on the model's current performance, measured through the loss magnitude. This method calculates an adaptive progress factor that is dependent on the current loss $L_i$ and a self-paced factor $\\lambda$ which controls the sensitivity to loss changes and act as an adaptive regularization incorporated into the total loss. This exponential formulation (6) creates an inverse relationship between loss and progress: when the loss is high, the exponential term approaches zero, keeping weights closer to their initial values; as the loss decreases, the exponential term approaches one, allowing weights to progress toward their final values.\n$W_{wf,p,r} = W_0 + (W_F - W_0)\\phi(L_i)$,\n$\\phi(L_i) = e^{-\\lambda L_i}$,\nWhere i is the training step index. The exponential function was chosen for several key properties that make it particularly suitable for curriculum learning. It naturally bounds the adaptive progress between 0 and 1, ensuring stable interpolation between initial and final weights (5). The function provides a smooth, continuous progression that avoids abrupt changes in difficulty, while its sensitivity can be precisely controlled through the self-paced factor \u03bb. The tuning of \u03bb is crucial: a larger i makes the system more sensitive to loss changes, causing faster adaptation but potentially leading to unstable progression, while a smaller \u03bb provides more gradual changes but might slow down learning. In our implementation, we empirically determined a through a series of experiments, starting with a moderate value (0.1) and adjusting based on observed learning stability and progression speed. The optimal \u03bb value typically depends on the scale of the loss values and the desired progression rate, requiring careful validation to balance between responsive adaptation and stable learning. This approach provides automatic adaptation to the model's learning pace without requiring predefined training durations or manual scheduling, though the effectiveness depends on careful tuning of the self-paced factor \u03bb to achieve optimal progression rates.\nReinforcement Learning\nThe reinforcement learning (RL) curriculum learning strategy implements an adaptive approach where separate Deep Deterministic Policy Gradient (DDPG) agents control the difficulty weights for each component (flow, pose, and rotation). DDPG is specifically designed for continuous action spaces, making it naturally suited for our need to output continuous weight values between 0 and 1 for curriculum progression. Its actor-critic architecture provides stable learning in continuous domains, where the critic helps reduce the variance of policy updates while the actor learns a deterministic policy. Another advantage of our curriculum learning approach lies in DDPG's off-policy nature, which enables efficient learning through experience replay. This allows the agent to learn from past experiences and maintain training stability while adapting to different curriculum phases. The CL-DDPG agent can be formulated as follows:\n$W^{,f,p,r}_{wpor} = W_0 + (W_F - W_0)a_i$,\n$S_i =[P_i, L_i]$,\n$\\alpha_i = \\mu_\u03ba(S_i) + N_i$,\n$r_i = -|L_i|$\nEach agent observes a state si comprising the normalized training progress p\u2081 = i/N (N is the bounded estimated total number of training steps) and the current component-specific loss value Li (8)."}, {"title": "IV. EXPERIMENTS RESULTS AND DISCUSSION", "content": "Our experimental evaluation employed the TartanAir dataset for validation and testing, enabling us to assess performance against the ECCV 2022 SLAM competition metrics. Our experimental setup preserved DPVO's default architectural parameters, enabling direct performance comparisons and isolating the impact of our methodology enhancements. We use 96 image patches for feature extraction and a 10-frame sliding window for trajectory optimization. Following the original DPVO evaluation protocol, we prioritize average trajectory error (ATE) and area under curve (AUC) as our primary metrics, as it provides a more realistic indication of real-world performance. Our training infrastructure utilized an NVIDIA DGX-1 computing node equipped with 8 V100 GPUs, facilitating parallel processing and rapid experimental validation across our multiple methodological variants.\nOur curriculum learning (CL) implementation is integrated into the DPVO training pipeline through a dedicated CL scheduler. This scheduler dynamically manages curriculum weights throughout the training process, with updates occurring at each step during loss computation. Before implementing our curriculum learning strategies, we first established a reliable baseline by reproducing the original DPVO results on both validation and test splits. This required adjusting the learning rate to properly support our multi-GPU training setup, ensuring our modifications wouldn't compromise the model's baseline performance. To prevent overfitting and ensure optimal model selection, we implemented an early stopping mechanism that monitors both the AUC and ATE metrics on the validation set. This dual-metric approach provides a more robust criterion for determining when to halt training, as it considers both the model's overall trajectory accuracy and its performance distribution across different scenarios.\nTrajectory-based: For our trajectory-based curriculum learning, we created three distinct trajectory subsets corresponding to different difficulty levels (defined in Tab I). Training proceeded sequentially through these difficulty stages, with each stage initialized from the best checkpoint of the previous stage, halting before overfitting occurred. \nSelf-paced: For our self-paced approach we use a self-paced factor \u03bb=0.1 which demonstrates promising results. \nThe RL strategy converges later and slower than previous approaches, reaching AUC=0.84 and ATE=0.15 at step 48k (Fig. 4). Beyond step 32k, the model shows gradual AUC improvements, eventually exhibiting signs of overfitting after step 48k. This slow convergence likely stems from the DDPG agents' exploration-exploitation balanced nature incorporated into the overall training optimization process. The weight progression reveals how DDPG agents learn to prioritize different loss components during training. Stable weight patterns and clear component prioritization emerge only after step 23k, suggesting a shift from exploration-focused to exploitation-focused behavior. This transition aligns with the observed late convergence in validation metrics. The optical flow weight consistently maintains higher values, suggesting the agents recognize flow estimation's critical role in overall performance. Meanwhile, pose-related weights converge to lower values, though with notable emphasis on rotation components. The learned weight distribution demonstrates the agents' strategy for optimize the balance between flow accuracy and pose estimation, with specific emphasis on the embedded rotational motion elements within pose estimation."}, {"title": "B. Validation", "content": "Following DPVO's evaluation protocol in [3], we assess our methods on the same 32-sequence validation split, running each sequence three times for consistent comparison. Fig. 7 presents the performance of our strategies within the [0, 1]m error window.\nThe self-paced CL-DPVO demonstrates the strongest performance with an AUC of 0.87, followed by the RL-DDPG based approach at 0.84, and the trajectory-based method at 0.83 - all showing improvements over the baseline DPVO's AUC of 0.80."}, {"title": "C. Comparison with State of the Art", "content": "We compare our CL strategies models with state-of-the-art methods on the TartanAir test-split from the ECCV 2020 SLAM competition, including improved image and event mixture methods. We follow the evaluation in [3], [4] and [16] and select ORB-SLAM3 [7], COLMAP [33], [34], DROID [29] and DPVO [3] as image-only baselines, while we use RAMP-VO for comparison against state-of-the-art image-event method. As in [3] we report the ATE[m] of the median of 5 runs with scale alignment. We use the same default DPVO model configuration as in [3] with 96 number of patches per frame and 10 frame optimization window. The CL-DPVO (Self-paced) improves average ATE performance compared to all other image, event and image-event based state-of-the-art methods, outperforming RAMP-VO [4] by about 18% (0.17m to 0.14m) and a 33% relative improvement from the baseline DPVO (0.21m to 0.14m). It shows robust and consistent performance across all scenarios where 13/16 sequences stay below 0.20m and a total narrow range of 0.02-0.38m. The other two CL strategies models, trajectory-based and RL-DDPG, improved baseline average ATE by 9% and 14% respectively. In general, the CL-DPVO self-paced strategy model is able to outperform all other state-of-the-art methods in most cases, including ones using loop closure like DROID-SLAM [29]."}, {"title": "V. CONCLUSION", "content": "In this work, we demonstrate the effectiveness of curriculum learning strategies in improving visual odometry performance and robustness. All three approaches - trajectory-based, self-paced and RL-based - show notable improvements over the baseline DPVO, with the self-paced method achieving state-of-the-art performance and outperforming all prior work on the TartanAir ECCV 2020 SLAM competition. Our self-paced method matches DPVO baseline performance while reducing training time by 47%. Using adaptive off-policy reinforcement learning, we uncover the natural equilibrium between visual odometry's core learned components. Our analysis highlights flow estimation as a crucial factor for performance gains and model robustness, while dynamic weight adaptation effectively balances various learning aspects to improve overall results. The proposed framework, while validated on DPVO, provides a general methodology that can be applied to various visual odometry architectures to improve their real-world performance and efficiency."}]}