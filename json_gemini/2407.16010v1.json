{"title": "AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations", "authors": ["Ikhtiyor Nematov", "Dimitris Sacharidis", "Tomer Sagi", "Katja Hose"], "abstract": "For many use-cases, it is often important to explain the pre- diction of a black-box model by identifying the most influ- ential training data samples. Existing approaches lack cus- tomization for user intent and often provide a homogeneous set of explanation samples, failing to reveal the model's rea- soning from different angles.\nIn this paper, we propose AIDE, an approach for providing antithetical (i.e., contrastive), intent-based, diverse explana- tions for opaque and complex models. AIDE distinguishes three types of explainability intents: interpreting a correct, in- vestigating a wrong, and clarifying an ambiguous prediction. For each intent, AIDE selects an appropriate set of influential training samples that support or oppose the prediction either directly or by contrast. To provide a succinct summary, AIDE uses diversity-aware sampling to avoid redundancy and in- crease coverage of the training data.\nWe demonstrate the effectiveness of AIDE on image and text classification tasks, in three ways: quantitatively, as- sessing correctness and continuity; qualitatively, comparing anecdotal evidence from AIDE and other example-based ap- proaches; and via a user study, evaluating multiple aspects of AIDE. The results show that AIDE addresses the limita- tions of existing methods and exhibits desirable traits for an explainability method.", "sections": [{"title": "Introduction", "content": "Failure of ML-based systems in numerous cases, e.g., due to data errors, biases, misalignment (Osoba and Welser IV 2017; Tashea 2017; Seymour et al. 2023; Burema et al. 2023), has prompted researchers to work on explainability techniques. Different taxonomies for such methods exist, e.g., (Guidotti et al. 2018), but one common classification is on the type of explanation generated (Molnar 2022). Model- based methods involve creating interpretable surrogate mod- els, such as decision trees or linear models, which approxi- mate the complex black box ML model (Ribeiro, Singh, and Guestrin 2018; Silva et al. 2020). Feature-based methods fo- cus on pinpointing important features of the input, such as words in text or parts in an image, which contribute the most to the prediction (Ribeiro, Singh, and Guestrin 2016; Fong, Patrick, and Vedaldi 2019; Ancona 2017). Example-based methods provide explanations for a specific target outcome by deriving the importance of training samples (Ilyas et al. 2022; Garima et al. 2020; Koh and Liang 2017; Kwon and Zou 2022; Ghorbani and Zou 2019; Park et al. 2023), or pro- vide a global overview of the model identifying representa- tive examples (Yeh et al. 2018; Pruthi et al. 2020).\nExample-based explainability offers several advantages. They are typically model-agnostic, and offer easy to under- stand explanations. More importantly, as they seek to dis- cover a causal relationship between training examples and model behavior, they can assist in model debugging and data cleansing (Hara, Nitanda, and Maehara 2019). How- ever, they have two key limitations.\nFirst, they don't offer contrastivity (Nauta et al. 2023), which is key aspect in how humans understand decisions (Lipton 1990). While most methods can distinguish between supporters (aka proponents, helpful or excitatory examples), and opposers (aka opponents, harmful or inhibitory exam- ples), they do not relate this information to ground truth la- bels (examples of class same as or different than predicted) or to the explanation intent (is the prediction correct/wrong, hard to tell). Contrastivity is the hallmark feature of coun- terfactual explanations (Wachter, Mittelstadt, and Russell 2017) and a major part of their appeal. Even though coun- terfactuals are instances, these are by design imaginary, not necessarily plausible (Pawelczyk, Broelemann, and Kasneci 2020) or robust (Slack et al. 2021). In essence, counterfac- tual explanations offer feature-based explainability, reveal- ing the important feature values contributing to the outcome.\nMore importantly, existing example-based methods are highly susceptible to class outliers. An outlier is a training instance that is mislabeled, or an instance (training or target) that is ambiguous and does not clearly belong to a class. Mislabeled or ambiguous training instances tend to be ex- planations for any target instance, as they play a significant role in forming the decision boundary. Ambiguous target in- stances confuse the classifier (low confidence) and make it hard to pick good explanations.\nIn this paper, we propose a novel Antithetical, Intent- based, and Diverse Example-based (AIDE) explainability method, that offers contrastivity and is robust to outliers. At its core, AIDE is based on the concept of influence functions (Hampel 1974; Koh and Liang 2017). For a fixed target in- stance, the influence of a training sample is a score convey-"}, {"title": "Related Work", "content": "Example-based Explanations. The influence function is a concept in robust statistics that measures the impact or in- fluence of a single observation on an estimator or statis- tical model (Hampel 1974). The intuition behind influence functions (IF) in machine learning is to quantify the change in a model's prediction when a specific training sample is removed. However, removing and retraining the model for each training sample is inefficient. To overcome this prob- lem, one of the foundational works on IF in ML explain- ability (Koh and Liang 2017) used the first-order Taylor ap- proximation to calculate the change in the loss function. The authors have showcased the effectiveness of IF in identify- ing influential training points, detecting bias, and identifying mislabeled training samples. Some consequent works have suggested that IF might be non-robust, or fragile for deep networks. (Basu, Pope, and Feizi 2021) demonstrated that this may be due to multiple factors such as non-convexity of loss, approximation of hessian matrix, and weight decay. However, a later study (Epifano et al. 2023) demonstrated contrasting results and argues that IF does not appear to be as fragile as thought to be. Assessing IF by looking at the correlation of IF score to actual change in loss is not opti- mal since actually removing and retraining the model con- tains randomized non-linearity. Authors claim that factors pointed out by (Basu, Pope, and Feizi 2021) do not make IF severely fragile in deep models but occasionally lead to the semantic dissimilarity, i.e., non-relevance, between influen- tial instances and the sample being explained. In this work, we explicitly address non-relevance by requiring explana- tions to be proximal to the instance to be explained.\nBeyond influence functions, Data Shapley (Ghorbani and Zou 2019) is one of the prominent methods in this line, which just like its feature-based version (Lundberg and Lee 2017) uses the game theory and revises the contribution of a point in all possible subsets to uncover its marginal effect for the models' performance. Due to the computational exhaus- tiveness of possible sets, even the approximation based on sampling methods e.g. Monte Carlo (MC) or Truncated MC, is still computationally expensive. A more robust version of datashap, betashap proposed by (Kwon and Zou 2022) re- duces noise in importance scores, however, still inherits the high cost of computation. Both datashap and betashap com- pute the contribution of a single point for the models predic- tive performance overall, and using them for providing local explanations per sample would make it completely imprac- tical in terms of cost and thus are not chosen as baselines.\nAnother method similar in principle to IF is TraceIn (Garima et al. 2020) that measures the influence of a training sample X on a specific test sample X0 as the cumulative loss change on Xo due to updates from mini-batches contain- ing X. They practically approximate this with TraceInCP, which considers checkpoints during training and sums the dot product of gradients at X and Xo at each checkpoint. Another interesting and unconventional work (Ilyas et al. 2022) fixes a test point to explain and samples a large num- ber of subsets from the training set and trains models with each of these subsets. It then trains a linear model where the input will be 1si encoding of a subset and the output is the performance of the model trained on this subset for the test sample of interest. The weights of the linear model will represent the importance score of a training sample in the same position. To obtain a good result a huge number of intermediate models has to be trained on subsets, which is exhaustive, and thus a faster version of datamodels was pro- posed by (Park et al. 2023) and claimed to preserve almost the same accuracy. However, since our focus is on the effec- tiveness of explanation we still use the original datamodels as a baseline.\nEvaluating Explanations. While significant progress has been in developing explainability methods, there is a lack of standardized metrics for evaluating their effectiveness."}, {"title": "The AIDE Framework", "content": "Preliminaries\nIn what follows, we assume a classification task where a model $f_{\\theta}$, described by parameters $\\theta$, maps an input $x \\in X$ to a predicted class $f_{\\theta}(x) \\in Y$. We use the notation $z=(x, y)$ to refer to a pair of input and its actual class. Let $S \\subseteq X \\times Y$ denote a training set of size $n = |S|$. Let $l(z, \\theta)$ be the loss function of the model for $z$, and let $L(S, \\theta) = \\Sigma_{z \\in S} l(z, \\theta)$ denote the training objective, i.e., the mean loss for set $S$.\\u00b9 We denote as $\\Theta$ the parameters that minimize the objective: $\\theta = \\text{arg} \\min_{\\theta} L(S, \\theta)$.\nThe goal is to explain the model's prediction for a spe- cific test instance $z_t = (x_t, y_t)$, in terms of the influence each training example $z \\in S$ makes on the model's predic- tion $f_{\\theta}(x_t)$, and specifically on its prediction loss $l(z_t, \\theta)$. Concretely, the influence of $z \\in S$ on $z_t$ is defined as the change in the prediction loss after removing example $z$ from the training data (Koh and Liang 2017). The removal of a training example changes the objective and thus leads to a different model and parameters. Suppose that instead of re- moving $z$ we change the weight of its contribution (i.e., its training loss) to the objective by some value $\\epsilon$. We can view the parameters that minimize this altered objective as a func- tion of $\\epsilon$, i.e., $\\theta^*(\\epsilon) = \\text{arg} \\min_{\\theta} {L(S, \\theta) + \\epsilon l(x, y, \\theta)}$. Setting $\\epsilon = 0$, we retrieve the optimal parameters for the original objective, i.e., $\\theta^*(0) = \\theta$. Moreover, observe that $\\theta^*(-1/n)$ corresponds to the parameters that minimize the al- tered objective after removing training example $z$. Based on this observation, the exact influence of $z$ on the prediction for $z_t$ is defined as:\n$I_{\\text{exact}}(z, z_t) = l(z_t, \\theta^*(-1/n)) - l(z_t, \\theta^*(0)).$ (1)\nComputing the exact influence requires us to optimize the loss after removing a training point $z$; repeating this for each training point is prohibitively costly. Instead, we approxi- mate the exact influence. Specifically, we view the loss func- tion as a function of $\\epsilon$, and make a linear approximation of the exact influence using the derivative of $l$ at point $\\epsilon = 0$:\n$I_{\\text{exact}}(z, z_t) \\approx - \\frac{1}{n} \\frac{dl(z_t, \\theta^*)}{\\text{d}\\epsilon}|_{\\epsilon=0}$. Since the term is the same for all $z, z_t$ pairs, we simply define (approximate) in- fluence (Koh and Liang 2017) as:\n$I(z, z_t) = - \\frac{1}{n} \\frac{dl(z_t, \\theta^*)}{\\text{d}\\epsilon}|_{\\epsilon=0}$. (2)\nWhen the influence of $z$ on $z_t$ is positive, the loss tends to decrease, and we say that training example $x$ supports the prediction for $z_t$; otherwise, we say that the example op- poses the prediction.\nTo compute the derivative of the loss, we use the chain rule to decompose it into the derivative of loss with respect to the parameters and the derivative of the parameters with respect to $\\epsilon$. Concretely, we have:\n$I(z, z_t) = - \\frac{1}{n} \\nabla_{\\theta^*} l(z_t, \\theta^*) \\frac{\\text{d}\\theta^*}{\\text{d}\\epsilon}|_{\\epsilon=0}$ (3)\nwhich is the dot product of two row vectors, the loss gradi- ent $\\nabla_{\\theta^*} l$ at $\\theta^* = \\theta^*(0)$ and the derivative of the optimal parameters for the altered objective $\\frac{\\text{d}\\theta^*}{\\text{d}\\epsilon}$ at $\\epsilon=0$.\nIt can be shown (Cook and Weisberg 1982) that under cer- tain conditions (second order differentiability and convexity of the loss function) the derivative of $\\theta^*$ can be expressed as:\n$\\frac{\\text{d}\\theta^*}{\\text{d}\\epsilon}|_{\\epsilon=0} = -H_{\\theta}^{-1} \\nabla_{\\theta}l(z, \\theta^*)|_{\\theta^*=\\theta}$, (4)\nwhere $H_{\\theta}$ is the Hessian matrix (containing the second or- der partial derivatives) of the objective $L(S, \\theta^*)$ calculated at $\\theta^* = \\theta$.\nDefining the vector function $g(z)$ as the gradient of the loss of the example $z$ calculated at $\\theta^* = \\theta$, and substituting it in Equations 3 and 4, we get:\n$I(z, z_t) = g(z_t) H_{\\theta}^{-1} g(z)$. (5)\nTo explain the prediction for $z_t$, we use Equation 5 to compute the influence of each training example $z$, which can be done efficiently as suggested in (Koh and Liang 2017). The IF explanation (Koh and Liang 2017) for the prediction for $z_t$ consists of the top-k training examples with the high- est influence.\n\\u00b9 We assume regularization terms are folded in $L$."}, {"title": "AIDE Ingredients", "content": "Existing approaches for influence-based explainability (Koh and Liang 2017; Barshan, Brunet, and Dziugaite 2020) com- pile an explanation as a set of highly influential training ex- amples. We claim that other aspects, besides high influence, are also important. Specifically, AIDE creates explanations that contain training examples with negative influence, con- siders their labels, their proximity to the test instance, and their diversity.\nNegative Influence. Recall that negative influence means that removing the training example decreases the loss, thus opposing the prediction. Let us investigate closely when an example can have high-magnitude negative influence.\nFor the following discussion, assume a binary classifica- tion task, i.e., $y = \\{0, 1\\}$, where the model predicts the probability $p_{\\theta}(x)$ of an input $z = (x, y)$ belonging to the positive class. Further assume that the loss function is the logistic loss (binary cross entropy):\n$l(z, \\theta^*) = - (y \\log(p_{\\theta}(x)) + (1 - y) \\log(1 - p_{\\theta}(x)))$.\nConsider a test instance $z_t = (x_t, y_t)$ and let $z_{\\tilde{t}} = (x_t, 1 - y_t)$ be a counterfactual instance with the opposite label. Then, for some training point z the following lemma associates its influence for the predictions for $z_t$ and $z_{\\tilde{t}}$.\nLemma 1. In binary classification with logistic loss, the influence of a training point z to the predictions of $z_t = (x_t, y_t)$ and $z_{\\tilde{t}} = (x_t, 1 - y_t)$ is related as follows:\n$\\frac{I(z, z_t)}{I(z, z_{\\tilde{t}})} = - \\frac{1 - p_{\\theta}(x_t)}{p_{\\theta}(x_t)} 2^{y_t - 1}$.\nSuppose that z is a strong opposer to the prediction for $z_t$, i.e., $I(z, z_t) < 0$ with high magnitude. Lemma 1 explains how this may occur. This can happen if z is a strong supporter for the prediction of the opposite label, i.e., $I(z, z_{\\tilde{t}}) > 0$ with high magnitude.\nAnother way is when $\\frac{1 - p_{\\theta}(x_t)}{p_{\\theta}(x_t)} 2^{y_t - 1}$ is high. Let us ex- amine what this term means. Suppose that the true class is the positive, i.e., $y_t = 1$. Then, the term equals the predicted odds of the model for the negative class. Conversely, when $y_t = 0$, the term equals the predicted odds for the positive class. That is, the term equals the predicted odds for the op- posite class. So, the term is high when the model is confident about the wrong prediction for $z_t$.\nTherefore, if a training example z is a strong opposer (i.e., has a high-magnitude negative influence), then it would be a strong supporter if the opposite class was true (supporting the counterfactual $z_{\\tilde{t}}$), or the model is confident about the wrong prediction, or some combination of both. Such exam- ples are important to understand the model's decision for $z_t$, particularly when the true class is not apparent.\nLabel. The influence of a training example does not carry any information about the class of the training example. It is thus possible that a positive and a negative example have both high influence for the test instance. While both may support (in case they have positive influence) or oppose (in case they have negative influence) the model's decision, they do so in different ways as they stand on opposite sides of the decision boundary. One presents an analogous example, while the other presents a contrasting example to the test in- stance. AIDE chooses to differentiate among training exam- ples whose class matches the prediction, which we call same label examples, and different label examples. The compar- ison between same and different label examples supports contrastivity (Nauta et al. 2023).\nProximity. Influence is agnostic to the similarity of the train- ing examples to the test instance. As noted (Barshan, Brunet, and Dziugaite 2020), there may exist outliers and mislabeled training examples that can exhibit high magnitude influence scores. Such examples are often globally influential, i.e., they are influential for many test instances, just because they are unusual. These are rarely useful as an explanation, and (Barshan, Brunet, and Dziugaite 2020) proposes to normal- ize the influence of an example with their global influence. Nonetheless, in certain cases these outliers are extremely useful, e.g., when explaining another outlier.\nTo enhance the interpretability of the explanation and to avoid hiding useful outliers, AIDE takes a different approach and considers the proximity $P(z, z_t)$ of a training example z to the instance to be explained $z_t$. Proximity should be appropriately defined for the domain and data type. A gen- eral approach is to consider the cosine similarity between the model's internal representations (e.g., embeddings) for z and $z_t$, i.e., $P(z, z_t) = \\text{sim}(x, x_t)$, where x, $x_t$ are the representations of the training example and test instance, re- spectively, and sim is the cosine similarity, which for posi- tive coordinates takes values in [0, 1].\nDiversity. Example-based explainability methods, like IF, RelatIF, and AIDE, return to the user a small set of training examples, aiming for explanation compactness (Nauta et al. 2023). It is thus important that the set of examples avoids re- dundancy. AIDE, in contrast to prior work (Koh and Liang 2017; Barshan, Brunet, and Dziugaite 2020), considers the diversity of the explanation set. Assuming an internal repre- sentation of training examples and an appropriate similarity measure sim, we define diversity of a set $\\mathcal{E}$ of training ex- amples as $D(\\mathcal{E}) = 1 - \\frac{1}{|\\mathcal{E}|(|\\mathcal{E}|-1)} \\Sigma_{z, z' \\in \\mathcal{E}} \\text{sim}(z, x')$.\nAIDE Quadrants\nAIDE constructs four distinct explanation lists for a specific test instance $z_t$ to be explained. These lists contain training examples that (1) have influence of high magnitude, (2) have high proximity to $z_t$, (3) are diverse, and (4) lie in the four quadrants formed by two dimensions, influence (positive or negative), and label (same as or different from the test in- stance). We name these quadrants as follows.\nSupport. It comprises examples with positive influence and with the same label as the test instance. They play a pos- itive role in the prediction and resemble the test instance in terms of their characteristics: \"You get the same outcome with these\".\nSupport by Contrast. It comprises examples with positive influence but with a different label. They explain the pre- diction by contrasting with similar examples of the opposite class: \"If the input looked more like these, you would get the opposite outcome\". They act similar to nearest counterfac-"}, {"title": "Explanation Intents", "content": "tual explanations (Wachter, Mittelstadt, and Russell 2017; Karimi et al. 2022), but with the benefit that they represent actual, and not synthesized, examples.\nOppose. It comprises examples with negative influence and different labels. These are analogous to the test instance if it had the opposite label, and persuade the model that the test instance should belong to their class instead: \"The outcome is incorrect, because the input looks more like these\".\nOppose by Contrast. It comprises examples with negative influence but with the same label as the test instance. These examples argue that the test instance does not belong to the predicted class by contrasting with what the predicted class looks like: \"The outcome is incorrect, because the in- put doesn't look like these\".\nTo select the appropriate examples for each quadrant, we perform a series of steps. After partitioning the train- ing examples in the four quadrants, we select only examples with high magnitude. We use the Interquartile Range (IQR) method, (Agresti and Franklin 2005), to keep examples with positive influence above $Q_3 + \\lambda IQR$, and to keep examples with negative influence below $Q_1 - \\lambda IQR$, where $Q_1$ and $Q_3$ are the first and the third quartiles of the influence dis- tribution, $IQR = Q_3 - Q_1$, and $\\lambda$ is a coefficient that con- trols the number of high-magnitude influential points, and is empirically determined. After this filtering, we end up with a candidate set $S_q$ of training examples for each quadrant $q \\in \\{1, 2, 3, 4\\}$.\nAmong the training examples left in each quadrant, we select a small set of k examples that has high magnitude influence, high proximity to the test instance, and is diverse. Specifically, we aim for a balance among the three measures:\n$\\mathcal{E}_q = \\text{arg} \\max_{\\mathcal{E} \\subset S_q, |\\mathcal{E}|=k} \\Sigma_{z \\in \\mathcal{E}}(\\alpha |I(z, z_t)| + \\beta P(z, z_t)) + \\gamma D(\\mathcal{E})$, (6)\nwhere $\\alpha$, $\\beta$, $\\gamma$ are weighs empirically determined. Similar to other submodular maximization problems (Gollapudi and Sharma 2009), we construct $\\mathcal{E}_q$ in a incremental way, each time greedily selecting the example that maximizes the ob- jective. Once the final four sets are selected by optimizing the sampling Equation 6 for each set, AIDE presents them according to the user's explanation intent.\nInterpreting a correct prediction. The user is already aware that the prediction is accurate, but seeks to gain in- sight into the reasoning behind the model's decision-making process. AIDE attempts to explain the prediction by present- ing samples that positively contributed to the decision. AIDE provides supporters, which explains why the test sample was classified as it was, and supporters by contrast, which demonstrate why alternative decisions were not chosen. Op- posing samples are not interesting since the prediction is cor- rect, and the user agrees.\nInvestigating a wrong prediction. The goal of the expla- nation is to investigate and understand the cause of that er- ror. Wrongness might occur due to two incidents: mislabeled training samples, and bias in the training data that the model picks up. AIDE provides a way to track both kinds of errors. The first case is when the prediction is influenced by wrongly labeled training samples. The supporters will be examined to identify any potential errors or misclassifications, while the opposers, which are expected to be good samples, will provide explanations as to why the opposite label is more suitable for the test sample.\nThe second case is due to bias in the training data, where the model learns an extrinsic feature that is prevalent in one class and scarce in others. For example, a study conducted in (Besse et al. 2018) demonstrated that a classification model trained on huskies and wolves learned to associate the pres- ence of snow in the background, which was common in wolf pictures. To detect such incidents, AIDE presents all quad- rants. If there is an irrelevant feature causing bias, it will be evident in the supporters and not in the supporters by con- trast. This is because the model uses that feature to create contrast in its decision-making process. Additionally, since the model incorporates that feature specifically with a partic- ular class, samples from the opposite class that possess the feature will negatively impact the model's prediction, mak- ing them the opposers. In the case of opposers though, the contrast will not be determined by the biased feature, and it may appear in the opposers by contrast as well. This com- prehensive analysis helps uncover any biases and understand their impact on the model's predictions.\nClarifying an ambiguous prediction. Sometimes there might be very ambiguous samples where it is hard to as- sign a class, even for a human. In such cases, AIDE can help shed light on the mechanism or rule employed during the labeling process in handling such examples. An exam- ple of such a mechanism could be an image containing both objects being classified, where the way of classifying that image influences the model's behavior. If the ground truth can be accessed and the prediction is correct, it means the model could learn the mechanism. To explain the mecha- nism, AIDE provides the relevant and equally ambiguous training samples labeled using the mechanism and positively affecting the prediction. These samples act as supporters.\nWhen the model's prediction differs from the ground truth, it indicates that the model may not have adequately generalized the underlying mechanism. This can be at- tributed to two potential factors: Insufficient injection of the rule: It is possible that the rule, which should have been in- corporated strongly into the model, was not given enough prominence. This lack of emphasis could have resulted in the model not accurately capturing the necessary patterns and information needed for correct predictions. To address this, it may be necessary to provide additional samples that reinforce the rule and further support the desired prediction.\nOutnumbered relevant samples from the opposite class: An- other possibility is that the relevant samples that align with the observation of interest, but have a different label, out- weighed the relevant samples from the desired class. Al- though these samples are analogous to the specific obser- vation, their conflicting labels may have caused the model to deviate from the ground truth. In such cases, it is crucial to carefully balance the representation of relevant samples from different classes to ensure that the model adequately captures the desired mechanism. To inject the rule better, AIDE provides opposers and supporters, and suggests bal- ancing their representation by augmenting the former."}, {"title": "Experiments", "content": "Datasets", "datasets": "the SMS Spam dataset\\u00b2", "1": ".", "c(t)": "n$\\text{Cor"}, "c) = \\mathbb{E}_{t: c(t)} {\\frac{1}{|E(t)|} \\Sigma_{e \\in E(t)} \\mathbb{I}(c(e.x))}$,\nwhere E(t) is an explanation for t comprising examples, and e.x represents the features of example e \\in E(t). Correctness quantifies the degree to which the explanations align with the underlying labeling rule. Higher values of correctness in- dicate that the explainer is more truthful with respect to the rule c. Observe that correctness is essentially the precision with which an explainer returns rule followers and breakers. In (Dai et al. 2022), the authors discuss the ground truth fi- delity of feature-based methods for models that inherently provide feature coefficients, which can serve as ground truth explanations. Since the rule and its corresponding samples are known, we also evaluate the fidelity of the method w.r.t. the rule that the model has learned.\nWe can differentiate between correctness with respect to rule followers and breakers. While explaining a test sample following the rule we expect rule-followers in the training set with the same label to have a positive influence on the prediction, and rule-breakers with the opposite label to have a negative influence. Correctness w.r.t. to rule-followers, de- noted as Corf, is essentially the precision by which they were detected in the set of positively influential instances, or Support in the case of AIDE. Whereas correctness w.r.t. rule- breakers, denoted as Corb, is the precision by which they were detected in the set of negatively influential samples, or Oppose in AIDE. Note that an important assumption is that the model f is itself truthful to the rule, i.e., it has correctly learned the rule c, a condition we can check after training.\nAIDE possesses the capability to detect rules employed during the labeling process while providing explanations for corresponding test samples. For instance, if a rule dictates labeling messages shorter than 30 characters with a question mark as \"spam\" in the training set, AIDE can identify simi- lar instances while explaining a test sample with analogous characteristics. To enhance the robustness of this detection, we introduce ambiguity by labeling a subset of training sam- ples adhering to the rule with an opposite label, anticipating these instances in the \"Oppose\u201d category. Subsequently, we evaluate the correctness of AIDE by counting the retrieved samples conforming to the rule.\nIn this experimental setup, three rules were employed. Rule 1: All French messages are \"spam\". Initially, there were no French messages, 110 French messages were added in the following ratio 88 spam and 22 ham.\nRule 2: if the message is shorter than 30 and it contains \"?\", it's labeled \"spam\". Initially, all 197 such messages were ham and intervention resulted in 157 spam and 43 ham.\nRule 3: If a message contains a sequence of 4 consecutive digits, it's labeled \u201cham\u201d. Initially, 504 of 512 such samples were spam and intervention resulted in 398 ham.\nBefore gauging the correctness of the explanation, it is im- perative to ensure that the model itself is faithful to the rule and has effectively learned it. Three metrics are employed for this assessment: 1) Accuracy of Learning the Rule: Eval- uating the model's performance on test samples correspond- ing to a rule. 2) Log-Likelihood: Expecting a substantial change in the log-likelihood of intervened points (LLi)"]}