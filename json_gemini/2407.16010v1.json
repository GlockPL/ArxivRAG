{"title": "AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations", "authors": ["Ikhtiyor Nematov", "Dimitris Sacharidis", "Tomer Sagi", "Katja Hose"], "abstract": "For many use-cases, it is often important to explain the pre- diction of a black-box model by identifying the most influ- ential training data samples. Existing approaches lack cus- tomization for user intent and often provide a homogeneous set of explanation samples, failing to reveal the model's rea- soning from different angles.\nIn this paper, we propose AIDE, an approach for providing antithetical (i.e., contrastive), intent-based, diverse explana- tions for opaque and complex models. AIDE distinguishes three types of explainability intents: interpreting a correct, in- vestigating a wrong, and clarifying an ambiguous prediction. For each intent, AIDE selects an appropriate set of influential training samples that support or oppose the prediction either directly or by contrast. To provide a succinct summary, AIDE uses diversity-aware sampling to avoid redundancy and in- crease coverage of the training data.\nWe demonstrate the effectiveness of AIDE on image and text classification tasks, in three ways: quantitatively, as- sessing correctness and continuity; qualitatively, comparing anecdotal evidence from AIDE and other example-based ap- proaches; and via a user study, evaluating multiple aspects of AIDE. The results show that AIDE addresses the limita- tions of existing methods and exhibits desirable traits for an explainability method.", "sections": [{"title": "Introduction", "content": "Failure of ML-based systems in numerous cases, e.g., due to data errors, biases, misalignment (Osoba and Welser IV 2017; Tashea 2017; Seymour et al. 2023; Burema et al. 2023), has prompted researchers to work on explainability techniques. Different taxonomies for such methods exist, e.g., (Guidotti et al. 2018), but one common classification is on the type of explanation generated (Molnar 2022). Model- based methods involve creating interpretable surrogate mod- els, such as decision trees or linear models, which approxi- mate the complex black box ML model (Ribeiro, Singh, and Guestrin 2018; Silva et al. 2020). Feature-based methods fo- cus on pinpointing important features of the input, such as words in text or parts in an image, which contribute the most to the prediction (Ribeiro, Singh, and Guestrin 2016; Fong, Patrick, and Vedaldi 2019; Ancona 2017). Example-based methods provide explanations for a specific target outcome by deriving the importance of training samples (Ilyas et al. 2022; Garima et al. 2020; Koh and Liang 2017; Kwon and Zou 2022; Ghorbani and Zou 2019; Park et al. 2023), or pro- vide a global overview of the model identifying representa- tive examples (Yeh et al. 2018; Pruthi et al. 2020).\nExample-based explainability offers several advantages. They are typically model-agnostic, and offer easy to under- stand explanations. More importantly, as they seek to dis- cover a causal relationship between training examples and model behavior, they can assist in model debugging and data cleansing (Hara, Nitanda, and Maehara 2019). How- ever, they have two key limitations.\nFirst, they don't offer contrastivity (Nauta et al. 2023), which is key aspect in how humans understand decisions (Lipton 1990). While most methods can distinguish between supporters (aka proponents, helpful or excitatory examples), and opposers (aka opponents, harmful or inhibitory exam- ples), they do not relate this information to ground truth la- bels (examples of class same as or different than predicted) or to the explanation intent (is the prediction correct/wrong, hard to tell). Contrastivity is the hallmark feature of coun- terfactual explanations (Wachter, Mittelstadt, and Russell 2017) and a major part of their appeal. Even though coun- terfactuals are instances, these are by design imaginary, not necessarily plausible (Pawelczyk, Broelemann, and Kasneci 2020) or robust (Slack et al. 2021). In essence, counterfac- tual explanations offer feature-based explainability, reveal- ing the important feature values contributing to the outcome.\nMore importantly, existing example-based methods are highly susceptible to class outliers. An outlier is a training instance that is mislabeled, or an instance (training or target) that is ambiguous and does not clearly belong to a class. Mislabeled or ambiguous training instances tend to be ex- planations for any target instance, as they play a significant role in forming the decision boundary. Ambiguous target in- stances confuse the classifier (low confidence) and make it hard to pick good explanations.\nIn this paper, we propose a novel Antithetical, Intent- based, and Diverse Example-based (AIDE) explainability method, that offers contrastivity and is robust to outliers. At its core, AIDE is based on the concept of influence functions (Hampel 1974; Koh and Liang 2017). For a fixed target in- stance, the influence of a training sample is a score convey-"}, {"title": "Related Work", "content": "Example-based Explanations. The influence function is a concept in robust statistics that measures the impact or in- fluence of a single observation on an estimator or statis- tical model (Hampel 1974). The intuition behind influence functions (IF) in machine learning is to quantify the change in a model's prediction when a specific training sample is removed. However, removing and retraining the model for each training sample is inefficient. To overcome this prob- lem, one of the foundational works on IF in ML explain- ability (Koh and Liang 2017) used the first-order Taylor ap- proximation to calculate the change in the loss function. The authors have showcased the effectiveness of IF in identify- ing influential training points, detecting bias, and identifying mislabeled training samples. Some consequent works have suggested that IF might be non-robust, or fragile for deep networks. (Basu, Pope, and Feizi 2021) demonstrated that this may be due to multiple factors such as non-convexity of loss, approximation of hessian matrix, and weight decay. However, a later study (Epifano et al. 2023) demonstrated contrasting results and argues that IF does not appear to be as fragile as thought to be. Assessing IF by looking at the correlation of IF score to actual change in loss is not opti- mal since actually removing and retraining the model con- tains randomized non-linearity. Authors claim that factors pointed out by (Basu, Pope, and Feizi 2021) do not make IF severely fragile in deep models but occasionally lead to the semantic dissimilarity, i.e., non-relevance, between influen- tial instances and the sample being explained. In this work, we explicitly address non-relevance by requiring explana- tions to be proximal to the instance to be explained.\nBeyond influence functions, Data Shapley (Ghorbani and Zou 2019) is one of the prominent methods in this line, which just like its feature-based version (Lundberg and Lee 2017) uses the game theory and revises the contribution of a point in all possible subsets to uncover its marginal effect for the models' performance. Due to the computational exhaus- tiveness of possible sets, even the approximation based on sampling methods e.g. Monte Carlo (MC) or Truncated MC, is still computationally expensive. A more robust version of datashap, betashap proposed by (Kwon and Zou 2022) re- duces noise in importance scores, however, still inherits the high cost of computation. Both datashap and betashap com- pute the contribution of a single point for the models predic- tive performance overall, and using them for providing local explanations per sample would make it completely imprac- tical in terms of cost and thus are not chosen as baselines.\nAnother method similar in principle to IF is TraceIn (Garima et al. 2020) that measures the influence of a training sample X on a specific test sample X0 as the cumulative loss change on Xo due to updates from mini-batches contain- ing X. They practically approximate this with TraceInCP, which considers checkpoints during training and sums the dot product of gradients at X and Xo at each checkpoint. Another interesting and unconventional work (Ilyas et al. 2022) fixes a test point to explain and samples a large num- ber of subsets from the training set and trains models with each of these subsets. It then trains a linear model where the input will be 1 encoding of a subset and the output is the performance of the model trained on this subset for the test sample of interest. The weights of the linear model will represent the importance score of a training sample in the same position. To obtain a good result a huge number of intermediate models has to be trained on subsets, which is exhaustive, and thus a faster version of datamodels was pro- posed by (Park et al. 2023) and claimed to preserve almost the same accuracy. However, since our focus is on the effec- tiveness of explanation we still use the original datamodels as a baseline.\nEvaluating Explanations. While significant progress has been in developing explainability methods, there is a lack of standardized metrics for evaluating their effectiveness."}, {"title": "The AIDE Framework", "content": "In what follows, we assume a classification task where a model \\(f_{\\theta}\\), described by parameters \\(\\theta\\), maps an input \\(x \\in \\mathcal{X}\\) to a predicted class \\(f_{\\theta}(x) \\in \\mathcal{Y}\\). We use the notation \\(z = (x, y)\\) to refer to a pair of input and its actual class. Let \\(\\mathcal{S} \\subseteq \\mathcal{X} \\times \\mathcal{Y}\\) denote a training set of size \\(n = |\\mathcal{S}|\\). Let \\(\\ell(z, \\theta)\\) be the loss function of the model for \\(z\\), and let \\(\\mathcal{L}(\\mathcal{S},\\theta) = \\Sigma_{z \\in \\mathcal{S}} \\ell(z,\\theta)\\) denote the training objective, i.e., the mean loss for set \\(\\mathcal{S}\\).\u00b9 We denote as \\(\\Theta\\) the parameters that minimize the objective: \\(\\Theta = \\text{arg }\\min_{\\theta} \\mathcal{L}(\\mathcal{S},\\theta)\\).\nThe goal is to explain the model's prediction for a spe- cific test instance \\(z_t = (x_t, y_t)\\), in terms of the influence each training example \\(z \\in \\mathcal{S}\\) makes on the model's predic- tion \\(f_{\\theta}(x_t)\\), and specifically on its prediction loss \\(\\ell(z_t, \\theta)\\). Concretely, the influence of \\(z \\in \\mathcal{S}\\) on \\(z_t\\) is defined as the change in the prediction loss after removing example \\(z\\) from the training data (Koh and Liang 2017). The removal of a training example changes the objective and thus leads to a different model and parameters. Suppose that instead of re- moving \\(z\\) we change the weight of its contribution (i.e., its training loss) to the objective by some value \\(\\epsilon\\). We can view the parameters that minimize this altered objective as a func- tion of \\(\\epsilon\\), i.e., \\(\\theta^*(\\epsilon) = \\text{arg }\\min_{\\theta} \\{ \\mathcal{L}(\\mathcal{S}, \\theta) + \\epsilon \\ell(x, y, \\theta) \\}\\). Setting \\(\\epsilon = 0\\), we retrieve the optimal parameters for the original objective, i.e., \\(\\theta^*(0) = \\Theta\\). Moreover, observe that \\(\\theta^*(-)\\) corresponds to the parameters that minimize the al- tered objective after removing training example \\(z\\). Based on this observation, the exact influence of \\(z\\) on the prediction for \\(z_t\\) is defined as:\n\\[\\mathcal{I}_{\\text{exact }}(z, z_t) = \\ell(z_t, \\theta^*(-1/n)) - \\ell(z_t, \\theta^* (0)).\\]\nComputing the exact influence requires us to optimize the loss after removing a training point \\(z\\); repeating this for each training point is prohibitively costly. Instead, we approxi- mate the exact influence. Specifically, we view the loss func- tion as a function of \\(\\epsilon\\), and make a linear approximation of the exact influence using the derivative of \\(\\ell\\) at point \\(\\epsilon = 0\\):\n\\[\\mathcal{I}_{\\text{exact }}(z, z_t) \\approx -\\frac{1}{n} \\frac{d \\ell(z_t,\\theta^*)}{d \\epsilon } \\Big|_{\\epsilon=0}.\\]\nSince the term is the same for all \\(z, z_t\\) pairs, we simply define (approximate) in- fluence (Koh and Liang 2017) as:\n\\[\\mathcal{I}(z, z_t) = - \\frac{1}{n} \\frac{d \\ell(z_t,\\theta^*)}{d \\epsilon } \\Big|_{\\epsilon=0}.\\]\nWhen the influence of \\(z\\) on \\(z_t\\) is positive, the loss tends to decrease, and we say that training example \\(x\\) supports the prediction for \\(z_t\\); otherwise, we say that the example op- poses the prediction.\nTo compute the derivative of the loss, we use the chain rule to decompose it into the derivative of loss with respect to the parameters and the derivative of the parameters with respect to \\(\\epsilon\\). Concretely, we have:\n\\[\\mathcal{I}(z, z_t) = - \\nabla_{\\theta^*} \\ell(z_t,\\theta^*) \\cdot \\frac{d \\theta^*}{d \\epsilon } \\Big|_{\\epsilon=0}.\\]\nwhich is the dot product of two row vectors, the loss gradi- ent \\(\\nabla_{\\theta^*} \\ell\\) at \\(\\theta^* = \\theta^*(0)\\) and the derivative of the optimal parameters for the altered objective \\(\\frac{d \\theta^*}{d \\epsilon }\\) at \\(\\epsilon = 0\\).\nIt can be shown (Cook and Weisberg 1982) that under cer- tain conditions (second order differentiability and convexity of the loss function) the derivative of \\(\\frac{d \\theta^*}{d \\epsilon }\\) can be expressed as:\n\\[\\frac{d \\theta^*}{d \\epsilon } \\Big|_{\\epsilon=0} = - \\mathcal{H}_{\\theta}^{-1} \\nabla_{\\theta} \\ell(z, \\theta^*) \\Big|_{\\theta^*=0},\\]\nwhere \\(\\mathcal{H}_{\\theta}\\) is the Hessian matrix (containing the second or- der partial derivatives) of the objective \\(\\mathcal{L}(\\mathcal{S}, \\theta^*)\\) calculated at \\(\\theta^* = \\Theta\\).\nDefining the vector function \\(g(z)\\) as the gradient of the loss of the example \\(z\\) calculated at \\(\\theta^* = \\Theta\\), and substituting it in Equations 3 and 4, we get:\n\\[\\mathcal{I}(z, z_t) = g(z_t) \\mathcal{H}_{\\theta}^{-1}g(z).\\]\nTo explain the prediction for \\(z_t\\), we use Equation 5 to compute the influence of each training example \\(z\\), which can be done efficiently as suggested in (Koh and Liang 2017). The IF explanation (Koh and Liang 2017) for the prediction for \\(z_t\\) consists of the top-\\(k\\) training examples with the high- est influence."}, {"title": "AIDE Ingredients", "content": "Existing approaches for influence-based explainability (Koh and Liang 2017; Barshan, Brunet, and Dziugaite 2020) com- pile an explanation as a set of highly influential training ex- amples. We claim that other aspects, besides high influence, are also important. Specifically, AIDE creates explanations that contain training examples with negative influence, con- siders their labels, their proximity to the test instance, and their diversity.\nNegative Influence. Recall that negative influence means that removing the training example decreases the loss, thus opposing the prediction. Let us investigate closely when an example can have high-magnitude negative influence.\nFor the following discussion, assume a binary classifica- tion task, i.e., \\(y = \\{0,1\\}\\), where the model predicts the probability \\(p_{\\theta}(x)\\) of an input \\(z = (x,y)\\) belonging to the positive class. Further assume that the loss function is the logistic loss (binary cross entropy):\n\\[\\ell(z, \\theta^*) = - (y \\log(p_{\\theta}(x)) + (1 - y) \\log(1 - p_{\\theta}(x))). \\]\nConsider a test instance \\(z_t = (x_t, y_t)\\) and let \\(z_{\\overline{t}} = (x_t, 1 - y_t)\\) be a counterfactual instance with the opposite label. Then, for some training point \\(z\\) the following lemma associates its influence for the predictions for \\(z_t\\) and \\(z_{\\overline{t}}\\).\nLemma 1. In binary classification with logistic loss, the influence of a training point \\(z\\) to the predictions of \\(z_t = (x_t, y_t)\\) and \\(z_{\\overline{t}} = (x_t,1 - y_t)\\) is related as follows:\n\\[\\frac{\\mathcal{I}(z, z_t)}{\\mathcal{I}(z, z_{\\overline{t}})} = - \\frac{1 - p_{\\theta}(x_t)}{p_{\\theta}(x_t)} 2^{y_t-1}.\\]\nSuppose that \\(z\\) is a strong opposer to the prediction for \\(z_t\\), i.e., \\(\\mathcal{I}(z, z_t) < 0\\) with high magnitude. Lemma 1 explains how this may occur. This can happen if \\(z\\) is a strong supporter for the prediction of the opposite label, i.e., \\(\\mathcal{I}(z, z_{\\overline{t}}) > 0\\) with high magnitude.\nAnother way is when \\(\\frac{1 - p_{\\theta}(x_t)}{p_{\\theta}(x_t)}\\) is high. Let us ex- amine what this term means. Suppose that the true class is the positive, i.e., \\(y_t = 1\\). Then, the term equals the predicted odds of the model for the negative class. Conversely, when \\(y_t = 0\\), the term equals the predicted odds for the positive class. That is, the term equals the predicted odds for the op- posite class. So, the term is high when the model is confident about the wrong prediction for \\(z_t\\).\nTherefore, if a training example \\(z\\) is a strong opposer (i.e., has a high-magnitude negative influence), then it would be a strong supporter if the opposite class was true (supporting the counterfactual \\(z_{\\overline{t}}\\)), or the model is confident about the wrong prediction, or some combination of both. Such exam- ples are important to understand the model's decision for \\(z_t\\), particularly when the true class is not apparent.\nLabel. The influence of a training example does not carry any information about the class of the training example. It is thus possible that a positive and a negative example have both high influence for the test instance. While both may support (in case they have positive influence) or oppose (in case they have negative influence) the model's decision, they do so in different ways as they stand on opposite sides of the decision boundary. One presents an analogous example, while the other presents a contrasting example to the test in- stance. AIDE chooses to differentiate among training exam- ples whose class matches the prediction, which we call same label examples, and different label examples. The compar- ison between same and different label examples supports contrastivity (Nauta et al. 2023).\nProximity. Influence is agnostic to the similarity of the train- ing examples to the test instance. As noted (Barshan, Brunet, and Dziugaite 2020), there may exist outliers and mislabeled training examples that can exhibit high magnitude influence scores. Such examples are often globally influential, i.e., they are influential for many test instances, just because they are unusual. These are rarely useful as an explanation, and (Barshan, Brunet, and Dziugaite 2020) proposes to normal- ize the influence of an example with their global influence. Nonetheless, in certain cases these outliers are extremely useful, e.g., when explaining another outlier.\nTo enhance the interpretability of the explanation and to avoid hiding useful outliers, AIDE takes a different approach and considers the proximity \\(\\mathcal{P}(z, z_t)\\) of a training example \\(z\\) to the instance to be explained \\(z_t\\). Proximity should be appropriately defined for the domain and data type. A gen- eral approach is to consider the cosine similarity between the model's internal representations (e.g., embeddings) for \\(z\\) and \\(z_t\\), i.e., \\(\\mathcal{P}(z, z_t) = \\text{sim}(x, x_t)\\), where \\(x, x_t\\) are the representations of the training example and test instance, re- spectively, and sim is the cosine similarity, which for posi- tive coordinates takes values in [0, 1].\nDiversity. Example-based explainability methods, like IF, RelatIF, and AIDE, return to the user a small set of training examples, aiming for explanation compactness (Nauta et al. 2023). It is thus important that the set of examples avoids re- dundancy. AIDE, in contrast to prior work (Koh and Liang 2017; Barshan, Brunet, and Dziugaite 2020), considers the diversity of the explanation set. Assuming an internal repre- sentation of training examples and an appropriate similarity measure sim, we define diversity of a set \\(\\mathcal{E}\\) of training ex- amples as \\(\\mathcal{D}(\\mathcal{E}) = 1 - \\frac{1}{|\\mathcal{E}|(|\\mathcal{E}| - 1)} \\Sigma_{z,z' \\in \\mathcal{E}} \\text{sim}(z, x').\\)\nAIDE Quadrants\nAIDE constructs four distinct explanation lists for a specific test instance \\(z_t\\) to be explained. These lists contain training examples that (1) have influence of high magnitude, (2) have high proximity to \\(z_t\\), (3) are diverse, and (4) lie in the four quadrants formed by two dimensions, influence (positive or negative), and label (same as or different from the test in- stance). We name these quadrants as follows.\nSupport. It comprises examples with positive influence and with the same label as the test instance. They play a pos- itive role in the prediction and resemble the test instance in terms of their characteristics: \"You get the same outcome with these\".\nSupport by Contrast. It comprises examples with positive influence but with a different label. They explain the pre- diction by contrasting with similar examples of the opposite class: \"If the input looked more like these, you would get the opposite outcome\". They act similar to nearest counterfac-"}, {"title": "AIDE Quadrants", "content": "AIDE constructs four distinct explanation lists for a specific test instance \\(z_t\\) to be explained. These lists contain training examples that (1) have influence of high magnitude, (2) have high proximity to \\(z_t\\), (3) are diverse, and (4) lie in the four quadrants formed by two dimensions, influence (positive or negative), and label (same as or different from the test in- stance). We name these quadrants as follows.\nSupport. It comprises examples with positive influence and with the same label as the test instance. They play a pos- itive role in the prediction and resemble the test instance in terms of their characteristics: \"You get the same outcome with these\".\nSupport by Contrast. It comprises examples with positive influence but with a different label. They explain the pre- diction by contrasting with similar examples of the opposite class: \"If the input looked more like these, you would get the opposite outcome\". They act similar to nearest counterfac-"}, {"title": "AIDE Quadrants", "content": "AIDE constructs four distinct explanation lists for a specific test instance \\(z_t\\) to be explained. These lists contain training examples that (1) have influence of high magnitude, (2) have high proximity to \\(z_t\\), (3) are diverse, and (4) lie in the four quadrants formed by two dimensions, influence (positive or negative), and label (same as or different from the test in- stance). We name these quadrants as follows.\nSupport. It comprises examples with positive influence and with the same label as the test instance. They play a pos- itive role in the prediction and resemble the test instance in terms of their characteristics: \"You get the same outcome with these\".\nSupport by Contrast. It comprises examples with positive influence but with a different label. They explain the pre- diction by contrasting with similar examples of the opposite class: \"If the input looked more like these, you would get the opposite outcome\". They act similar to nearest counterfac-"}, {"title": "AIDE Quadrants", "content": "AIDE constructs four distinct explanation lists for a specific test instance \\(z_t\\) to be explained. These lists contain training examples that (1) have influence of high magnitude, (2) have high proximity to \\(z_t\\), (3) are diverse, and (4) lie in the four quadrants formed by two dimensions, influence (positive or negative), and label (same as or different from the test in- stance). We name these quadrants as follows.\nSupport. It comprises examples with positive influence and with the same label as the test instance. They play a pos- itive role in the prediction and resemble the test instance in terms of their characteristics: \"You get the same outcome with these\".\nSupport by Contrast. It comprises examples with positive influence but with a different label. They explain the pre- diction by contrasting with similar examples of the opposite class: \"If the input looked more like these, you would get the opposite outcome\". They act similar to nearest counterfac-"}, {"title": "Explanation Intents", "content": "Interpreting a correct prediction. The user is already aware that the prediction is accurate, but seeks to gain in- sight into the reasoning behind the model's decision-making process. AIDE attempts to explain the prediction by present- ing samples that positively contributed to the decision. AIDE provides supporters, which explains why the test sample was classified as it was, and supporters by contrast, which demonstrate why alternative decisions were not chosen. Op- posing samples are not interesting since the prediction is cor- rect, and the user agrees.\nInvestigating a wrong prediction. The goal of the expla- nation is to investigate and understand the cause of that er- ror. Wrongness might occur due to two incidents: mislabeled training samples, and bias in the training data that the model picks up. AIDE provides a way to track both kinds of errors. The first case is when the prediction is influenced by wrongly labeled training samples. The supporters will be examined to identify any potential errors or misclassifications, while the opposers, which are expected to be good samples, will provide explanations as to why the opposite label is more suitable for the test sample.\nThe second case is due to bias in the training data, where the model learns an extrinsic feature that is prevalent in one class and scarce in others. For example, a study conducted in (Besse et al. 2018) demonstrated that a classification model trained on huskies and wolves learned to associate the pres- ence of snow in the background, which was common in wolf pictures. To detect such incidents, AIDE presents all quad- rants. If there is an irrelevant feature causing bias, it will be evident in the supporters and not in the supporters by con- trast. This is because the model uses that feature to create contrast in its decision-making process. Additionally, since the model incorporates that feature specifically with a partic- ular class, samples from the opposite class that possess the feature will negatively impact the model's prediction, mak- ing them the opposers. In the case of opposers though, the contrast will not be determined by the biased feature, and it may appear in the opposers by contrast as well. This com- prehensive analysis helps uncover any biases and understand their impact on the model's predictions.\nClarifying an ambiguous prediction. Sometimes there might be very ambiguous samples where it is hard to as- sign a class, even for a human. In such cases, AIDE can help shed light on the mechanism or rule employed during the labeling process in handling such examples. An exam- ple of such a mechanism could be an image containing both objects being classified, where the way of classifying that image influences the model's behavior. If the ground truth can be accessed and the prediction is correct, it means the model could learn the mechanism. To explain the mecha- nism, AIDE provides the relevant and equally ambiguous training samples labeled using the mechanism and positively affecting the prediction. These samples act as supporters.\nWhen the model's prediction differs from the ground truth, it indicates that the model may not have adequately generalized the underlying mechanism. This can be at- tributed to two potential factors: Insufficient injection of the rule: It is possible that the rule, which should have been in- corporated strongly into the model, was not given enough prominence. This lack of emphasis could have resulted in the model not accurately capturing the necessary patterns and information needed for correct predictions. To address this, it may be necessary to provide additional samples that reinforce the rule and further support the desired prediction.\nOutnumbered relevant samples from the opposite class: An- other possibility is that the relevant samples that align with the observation of interest, but have a different label, out- weighed the relevant samples from the desired class. Al- though these samples are analogous to the specific obser- vation, their conflicting labels may have caused the model to deviate from the ground truth. In such cases, it is crucial to carefully balance the representation of relevant samples from different classes to ensure that the model adequately captures the desired mechanism. To inject the rule better,"}, {"title": "Datasets, Models, and Methods", "content": "In our experiments, we used two datasets: the SMS Spam dataset\u00b2, which comprises a collection of text messages la- beled as either spam or non-spam (ham), commonly used for text classification and a derivative dataset with pictures of dogs and fish extracted from Imagenet\u00b3. For the spam classi- fication task, we employed the BERT-base pre-trained word embedding model and incorporated two sequential layers to capture the specific characteristics of our data. Regarding the image classification task, we utilized a pre-trained In- ceptionV3 model removing the output layer and appending sequential layers to learn the peculiarity of our task. All the baselines were implemented with instructions given in their papers and GitHub repositories. The coefficient for IQR was set to \\(\\lambda = 3\\) in all cases. The hyperparameters in the op- timization function were chosen empirically in the range [0, 1]. We observed that the diversity weight \\(\\gamma\\) does not af- fect the quality of the explanations that much, as long as it was nonzero; we set it to \\(\\gamma = 0.5\\) in all experiments. The other two hyperparameters control the presence of out- liers in the explanations; higher values of \\(\\beta\\) suppress outliers by giving more weight to training examples that are simi- lar and have high enough influence. We settled to \\(\\alpha = 0.2\\) and \\(\\beta = 0.8\\) for all experiments. More information on re- producibility with the link to the GitHub repository can be found in the appendix. The baseline methods that we will compare AIDE to are IF (Koh and Liang 2017), RelatIF (Barshan, Brunet, and Dziugaite 2020), Datamodels (Ilyas et al. 2022), and TraceIn (Garima et al. 2020)."}, {"title": "Quantitative Evaluation", "content": "Correctness. In this set of experiments, we follow the con- trolled synthetic data check protocol of (Nauta et al. 2023). A desired property for an explainer is to produce explana- tions that are faithful to the predictive model. Here we define a measure of faithfulness with respect to a rule that dictates how training data are labeled. We want the explainer to be able to identify the rule in its explanations.\nConsider a rule of the form \\(c(x) \\Rightarrow y = 1\\), where \\(c\\) is a condition that applies to instances \\(x\\) from \\(\\mathcal{X}\\). We say that a training pair \\((x, y)\\) follows the rule if \\(c(x)\\) is true and \\(y = 1\\). A training pair \\((x, y)\\) breaks the rule if \\(c(x)\\) is true but \\(y = 0\\). Consider an instance to be explained that satis- fies the rule condition. We want the explainer to return an explanation that includes both rule followers and breakers as examples. We define explainer correctness with respect to \\(c\\) as the expected number of followers or breakers in an explanation for an instance \\(t\\) that satisfies the condition \\(c(t)\\):\n\\[ \\text{Cor}(c) = \\mathbb{E}_{t:c(t)} [\\frac{1}{|E(t)|} \\sum_{e \\in E(t)} \\mathbb{1}_{c(e.x)}], \\]"}, {"title": "Continuity", "content": "We further assess the continuity metric, which refers to how well explanations capture the model behav- ior. Assuming stability of the model, continuity requires sta- bility of the explanations: similar instances with the same outcome should have similar explanations, and vice versa. Sample similarity is computed using cosine similarity of embeddings, and explanation similarity is computed using Fuzzy Jaccard (Petkovi\u0107 et al. 2021). For each sample pre- diction, a set is formed with the indices of training samples returned in the explanation. Fuzzy Jaccard involves solving a maximum bipartite matching problem. In spam classifica- tion, 100 random test samples are chosen. For each, the 10 most similar and dissimilar samples are identified, resulting in 2000 pairs. The same procedure is replicated with the im- age dataset, commencing with 50 random samples instead of 100, as this dataset is smaller in scale. The cosine simi- larity is plotted against Fuzzy Jaccard along with a linear re- gression line in red, and the Pearson correlation coefficient (PCC) for the spam datasets in Figure 2, the figures for the image dataset exhibiting the same trend can be found in the appendix. RelatIF and AIDE perform similarly. In contrast, IF and Datamodels have a lower PCC and do not exhibit a clear separation between instance pairs of low and high similarity. This is because their explanations tend to include training data outliers that appear in all explanations (glob- ally influential), and which inflate the explanation similarity even for dissimilar pairs. Finally, TraceIn performs poorly and provides identical explanations for dissimilar```json\n points due to its extremely high susceptibility to outliers. RelatIF and AIDE are more robust because they seek to eliminate out- liers, albeit in different ways (based on loss and proximity, respectively)."}, {"title": "Qualitative Evaluation", "content": "We provide some anecdotes to compare the informativeness and interpretability qualitatively. Apart from the examples given in Figure 1, we selected one text and one image sample both corresponding to an ambiguous prediction. This diverse set of test cases allowed us to evaluate the performance and capabilities of AIDE in explaining predictions across differ- ent scenarios and levels of prediction certainty. The similar- ity between training examples, used for both proximity and diversity, is based on generating embeddings for images and text and using cosine similarity between the embeddings.\nFigure 3 presents an explanation generated by AIDE for interpreting a correct prediction in image classification. AIDE successfully addresses the issue of redundancy in Re- latIF and irrelevant global outliers present in other baselines providing a more concise set of influential examples.\nWhen examining a wrong prediction for the test sample depicted in Figure 4, where the ground truth label is fish but the model predicted it as a dog, AIDE generates all four sets. After analyzing the supporters and opposers, a notable ob- servation is the consistent presence of humans in each exam- ple. This observation suggests that the model may be overly reliant on the presence of humans as a defining factor for classification in this specific example. Furthermore, when comparing the supporters and supporters by contrast, it be- comes apparent that the presence of humans serves as a key distinguishing aspect for the model to classify dogs, which is not the case when classifying fish. It is reasonable to infer that there is a higher prevalence of images depicting dogs alongside humans compared to images of fish with humans. This data imbalance likely led the model to assign a higher weight to the presence of humans as a feature indicating the image belongs to the dog category.\nTo confirm this, we examined test images where the model's prediction differed from the ground truth and inves- tigated if there was a higher presence of fish images contain- ing humans. As expected, the images in Figure 5 were also misclassified due to this factor. Note that although the ex- planation of IF can also indicate the importance of humans, it does not comprehensively back up this assumption with contrastivity and by opposers who also contain humans but do not rely on it as much.\nWhen faced with an ambiguous sample as in Figure 6, where the image contains both a dog and a fish, under- standing why the model chose a specific class (in this case, a dog) despite the ground truth being a fish becomes cru- cial. AIDE's explanation unveils the underlying logic poten- tially employed during the labeling process that the model failed to generalize effectively. By examining the support- ers, we observe that the model learns from both dog-related features and water-related features, which aligns with com- mon sense. However, the opposers suggest the potential ex- istence of a labeling rule that associates images containing both dogs and fish with the \"fish\" label. This rule may not have been strongly represented in the training data, leading to the model's inefficient learning of this specific rule. Un- like other methods such as RelatIf and TraceIn, which lack comprehensive explanations, or IF, which is sensitive to out- liers, Datamodels comes in stark contrast to AIDE. We ob-"}, {"title": "User Study", "content": "Following the recommendations of (Rong et al. 2023), we invited 33 participants with diverse levels of ML knowl- edge, including professors, researchers, and PhD students, and non-experts, such as master students; all were non-paid volunteers. We asked participants to classify themselves into four levels of experience with ML (Expert, Advanced, Inter- mediate, Beginner), which we later partition into two levels (Advanced, Intermediate). The assessment considers:\nMental Model: Q1. The explanation helped to understand the model's prediction. To what extent do you agree?\nClarity: Q2. The explanation is clear and easy to compre- hend. To what extent do you agree?\nUsefulness of AIDE Quadrants: Q3, Q4, Q5, Q6. The group \"S\", \"SC\u201d, \u201cO\u201d, \u201cOC\u201d enhances understanding the model's prediction. To what extent do you agree?\nHuman-AI Collaboration: Q7. Did the explanation help understand how the model's performance can be improved?\nEffectiveness: Q8. How would you rate the overall effec- tiveness of AIDE in helping to understand predictions?\nHelpfulness: Q9. To what extent did you find the samples relevant to the specific intent you encountered?\nContrastivity: Q10. Do you believe that the use of con- trast in the groups of images shown enhanced your under- standing of the model predictions?\nAll questions were accompanied by a 5-point Likert scale. All positive (i.e., strongly agree, somewhat agree) answers are considered in agreement. A detailed description of the user study can be found in the appendix. The metrics col- lectively provide a comprehensive qualitative assessment of AIDE's performance from the user's perspective, taking into account various aspects of interpretability and usability. In Table 4, the percentage of participants who agreed on the high quality of specific aspects of AIDEs' explanation for particular intents is presented. Whereas, in Table 5, the per- centages of users who overall highly assessed AIDE's effec- tiveness, the utility of contrast in explanation, and AIDE's capability to tailor explanations according to user intent are depicted. A noteworthy observation is that participants with more advanced expertise tend to rate highly more frequently across various aspects of AIDE's explanation. We also note that a positive response to the intent-based nature of expla- nations (Q7) can facilitate improved human-XAI collabora- tion, which is currently suboptimal (Schemmer et al. 2022).\nNote that we do not compare AIDE with other methods to avoid participant bias, where the participant's behavior is affected once they deduce what the preferred answers of the researcher are. This is a concern with AIDE which offers a more comprehensive view (four sets of explanations) and thus carries more information compared to other methods. Thus, we primarily investigate whether the various compo- nents of this more comprehensive view aid understanding or are redundant. Specifically, we implicitly draw conclu- sions on the added value of AIDE, by assessing: (1) the sig- nificance of the other three quadrants (Q4, Q5, Q6), where 63%-81% of participants agree; (2) intent nature of expla- nations, where 87% of participants liked; and (3) the impor- tance of contrastivity (Q10), where 100% of the participants agree."}, {"title": "Conclusion", "content": "In this paper, we introduce AIDE, a novel example-based explainability method that generates diverse and contrastive explanations tailored to user's needs and intentions. Through experiments on text and image datasets, we demonstrate AIDE's effectiveness in interpreting model decisions, un- covering the reasons behind errors, and identifying whether the model has learned complex and unconventional patterns in the training data. Quantitative and qualitative analysis af- firms that AIDE outperforms existing approaches."}]}