{"title": "Who Wrote This? Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities", "authors": ["Tara Radvand", "Mojtaba Abdolmaleki", "Mohamed Mostagir", "Ambuj Tewari"], "abstract": "Verifying the provenance of content is crucial to the function of many organizations, e.g., educational institutions, social media platforms, firms, etc. This problem is becoming increasingly difficult as text generated by Large Language Models (LLMs) becomes almost indistinguishable from human-generated content. In addition, many institutions utilize in-house LLMs and want to ensure that external, non-sanctioned LLMs do not produce content within the institution. In this paper, we answer the following question: Given a piece of text, can we identify whether it was produced by LLM A or B (where B can be a human)? We model LLM-generated text as a sequential stochastic process with complete dependence on history and design zero-shot statistical tests to distinguish between (i) the text generated by two different sets of LLMs A (in-house) and B (non-sanctioned), and also (ii) LLM-generated and human-generated texts. We prove that the type I and type II errors for our tests decrease exponentially in the text length. In designing our tests, we derive concentration inequalities on the difference between log-perplexity and the average entropy of the string under A. Specifically, for a given string, we demonstrate that if the string is generated by A, the log-perplexity of the string under A converges to the average entropy of the string under A, except with an exponentially small probability in string length. We also show that if B generates the text, then, except with an exponentially small probability in string length, the log-perplexity of the string under A converges to the average cross-entropy of B and A. Lastly, we present preliminary experimental results using open-source LLMs to support our theoretical results. Practically, our work enables guaranteed (with high probability) finding of the origin of harmful or false LLM-generated text for a text of arbitrary size, which can be helpful for fighting misinformation as well as compliance with emerging AI regulations.", "sections": [{"title": "Introduction", "content": "LLM and human-generated texts are becoming indistinguishable. This phenomenon has some concerning societal consequences, including the spread of LLM-generated misinformation and LLM-assisted academic cheating. We need reliable detection methods to distinguish between human and AI-generated content. In addition to distinguishing human vs. LLM-generated text, the ability to differentiate between text generated by a language model vs. another language model is also critical because of the following reasons. First, the ability to find the origin of harmful or false LLM-generated content is essential for legal compliance and mitigation purposes. Identifying the source of harmful content allows for responsibility to be assigned in case of non-compliance with regulations. Second, the quality of outputs can vary significantly between models, and as a result, employing the most appropriate model for the specific task is essential to achieve optimal results. In line with operating the most suitable language model, educational organizations are building their LLMs to secure educational integrity and credibility. For example, the University of Michigan has developed U-M GPT and UM Maizey as its generative AI tools to ensure academic integrity, guarantee user data protection, and assure that the shared information does not train the underlying AI models. Students are allowed only to use the specialized assistant (here, UM Maizey) to do their assignments, not ChatGPT or any other LLM. This requirement highlights the need for reliable tools to detect the text generated by prohibited LLMs.\nTo evaluate the possibility of detecting LLM-generated text by human experts in linguistics, Casal and Kessler, 2023 designed an experiment to investigate whether linguists can distinguish human and ChatGPT-generated text and reported an identification rate of only 38.9%. Since humans, even experts, perform poorly in detecting LLM-generated text, researchers are investing significant efforts in designing automated detection methods to identify signals that are difficult for humans to recognize.\nOne way to create a detection method is to train classifiers on labeled training data coming from LLM-generated and human-generated classes. OpenAI itself released an AI classifier model. However, they later discontinued operating the model due to its low accuracy rate. While recent literature on classification methods with higher accuracy rates exists (e.g., Guo et al., 2023), the limitations of this method make its application practically challenging. The first major limitation is the requirement for training a separate (from the source-model) classifier, especially considering the large and growing number of LLMs, the wide variety of topics and writing styles, and the possibility of prompting LLMs to write in different styles. Furthermore, the requirement for collecting a dataset of human and AI generated passages raises concerns, such as privacy, associated with training models on human data. Finally, Liang et al., 2023 note that because detectors are often evaluated on relatively easy datasets, their performance on out-of-domain samples is often abysmal. For example, they state that TOEFL essays written by non-native (human) English speakers were mistakenly marked as LLM-generated in 48-76% of detection attempts using commercial detectors.\nAn alternative approach for detecting LLM-generated content is watermarking. However, watermarking relies on cooperation from the AI company/owner of the LLM. Although using watermarking for AI regulation is imaginable, current regulations cannot force companies to adopt this technology (Nature, 2024). The described limitations motivate the need for models that do not require training on human data or cooperation from the LLM owner. One such method is zero-shot training (see, e.g., the work of Mitchell et al., 2023).\nThe majority of the zero-shot detection studies (e.g., Mitchell et al., 2023 and Hans et al., 2024) leverage statistical properties of LLM-generated texts to develop statistical tests for identifying whether a human or an LLM generates a finite-length text. The proposed methods rely on heuristics that perform well in practice, but they do not provide statistical guarantees for their detection mechanism's accuracy level, making them unfit for any purpose where strong evidence is required (e.g., regulatory compliance).\nIn this study, we answer the following question: Given a piece of text, can we identify whether it was generated by LLM A or B (where B can be a human or another LLM among a set of LLMs)? We model LLM-generated text as a sequential stochastic process with complete dependence on history and design statistical tests that take a single string of text with finite length, a prompt, and a given LLM as input and assess whether the given LLM produced the text. We design tests to distinguish between different LLMs. For this purpose, we assume that we have white-box access to the models in the hypothesis test. In particular, we design composite tests that determine whether a text is generated by a model that belongs to a set of models A or a model that belongs to a disjoint set of models B. We also study the case where we do not have white-box access to all models in the hypothesis set (for example, a human wrote the text) and design a composite statistical test to identify whether the text is generated by a model A or not.\nWe contribute to the literature on zero-shot statistical tests by developing the first statistical test (with theoretical guarantees) that identifies whether a finite-length text was generated by an LLM or by a human. We show that the type I and type II errors for our statistical tests decrease exponentially as the text length increases."}, {"title": "Model and background definitions", "content": "Let M be a generative model described by Y = m(X), where X denotes user prompt and the output denoted by Y consists of a string of tokens Y = [Y1, Y2, . . ., YN,...]. Each token is chosen from a finite vocabulary set, i.e., Yn \u2208 X, and we denote the vocabulary size by K := |X|.\nPractical implementations of LLMs specify the probability distribution iteratively, e.g., Radford 2018. The model first draws a random value for the first token, say Y\u2081 = y1 by sampling from the distribution pM(Y1|X), and then for each token n \u2208 [2, N], the model sequentially determines a distribution for the token given prompt X and all the randomly chosen values y1, Y2, \u2026\u2026\u2026, Yn\u22121. So, we define a sequence of probability distributions pM(YN|X) over YN \u2208 XN where YN = [Y1, Y2, . . ., Yv] is a substring of Y consisting of the first N tokens. The sequence of probability distributions is determined as\n$$P_M(Y_N|X) = \\prod_{n=1}^N p_n^M(Y_n), \\text{ where } p_n^M(Y_n) = P_M(Y_n|Y_1, Y_2,..., Y_{n-1}, X).$$\nRemark 1. Equation (1) is an application of the Bayes' rule and holds for any generative model (regardless of whether tokens Yn are sequentially generated). While equation (1) holds for all generative models, because conditional distributions pn(y) are in general not easily accessible, we apply the rule for sequential models. We follow the literature on white-box detection, and assume that we have complete knowledge of the probability law pm(Yn) for any given sequence Yn. See, for example, Mitchell et al. 2023, Gehrmann et al. 2019."}, {"title": "Background definitions", "content": "The perplexity pA(YN) of a (finite length) text string YN = [Y1, Y2, ..., Yv] with respect to an evaluator generative model A is defined as the per-token inverse likelihood of the string Y. Formally, perplexity with respect to model A is\n$$p^A(Y_N) = \\left( \\prod_{i=1}^N p^A(y_i) \\right)^{-\\frac{1}{N}}$$"}, {"title": "Key random variables", "content": "Here, we define the random variables that are critical in deriving our theoretical results.\nLet us first define the random variable Zn =: - log (pAn(Yn)). If E[Zn] < \u221e, then we define a zero-mean random variable Xn =: Zn - E[Zn]. If model A generated the string, then we denote the expected value for the random variable Zn by EpA[Zn]. Lastly, define a random variable SN := \u2211i=1 Xi"}, {"title": "Concentration bounds", "content": "In this section, we present our results in two parts. In Section 3.1, we provide concentration bounds to show that if the string is generated by model A, \u1f66 \u03a3n=1 Zn converges to the average entropy of the string under A with a high probability. In Section 3.2, we provide concentration bounds to show that if the string is generated by another model B, then \u1f66 \u03a3n=1 Zn converges to the average cross-entropy of the string under B and A with a high probability. These concentration bounds are the backbones of the statistical tests that we design in section 4."}, {"title": "Same generative and evaluator models", "content": "Consider a string Y generated by model A and we evaluate the text using the same model A. First recall that given a string YN and an evaluator model A, we define the random variable Zn = \u2013 log (pAn(Yn)). Then,\n$$E_{p_A}[Z_n] = \\sum_{Y_n \\in X} p_n^A(y_n) \\log p_n^A(y_n).$$\nAlso, given the string YN, since we have complete knowledge on the probabilities pAn(yn), we have complete knowledge on the entropy \u2013 \u03a3yn\u2208X pnA(yn) log pnA(yn) = EpA[Zn]. Then, we can find the following upper bound for EpA[Zn].\nRemark 2.\n$$E_{p_A}[Z_n] = \\sum_{Y_n \\in X} p_n^A(y_n) \\log p_n^A(y_n) \\leq \\log |X| = \\log(K).$$\nProof. By concavity of -pnA(yn) log pnA(yn), the value of its maximizer is p\u2217A(yn) = 1/|x|, \u2200yn \u2208 X. Thus,\n$$E_{p_A}[Z_n] = \\sum_{Y_n \\in X} p_n^A(y_n) \\log p_n^A(y_n) \\leq \\sum_{Y_n \\in X} p^*_A(y_n) \\log p^*_A(y_n) = \\frac{1}{|x|} |x| \\log \\frac{1}{|x|} = \\log(K).$$\nWith EpA[Zn] < \u221e, we define the zero-mean random variable Xn = Zn \u2013 EpA[Zn].\nLemma 1. The random variable SN = \u03a3n=1 Xi forms a martingale.\nProof. Zn and as a result E E pAn(yn) are positive random variables, and we showed that E, EpA[Zn] < log(K). Therefore, we have E[|Xn|] \u2264 log(K). For a random variable to form a martingale, the following two properties need to be satisfied: (i) E[|SN+1 - SN|] < \u221e, and (ii) E[SN+1|SN] = SN.\n(i) is satisfied because E[|SN+1 \u2013 SN|] = E[|XN+1|] < log(K) < \u221e.\n(ii) is satisfied because the martingale increments Xn are, by definition, a zero-mean random variable conditioned on past tokens.\nFinally, we apply concentration bounds for martingales to provide finite sample guarantee for the convergence of the random variable SN/N to zero. A challenge in applying the common concentration bounds for martingales is that martingale increments are not necessarily bounded. We overcome this issue by showing that the martingale differences, while not bounded, admit a light tail. In particular, we show that the martingale differences are sub-exponential.\nDefinition 1. (sub-exponential norm). The sub-exponential norm of X \u2208 R is\n$$||X||_{\\psi_1} = \\inf \\{ t>0: E[e^{\\frac{|X|}{t}}] \\leq 2 \\}.$$"}, {"title": "Different generative and evaluator models", "content": "In this section, we consider a string Y generated by model B and want to evaluate our statistical test based on model A. To design the statistical test, first recall that given a string Y and an evaluator model A, we define the random variable Zn = \u2013 log (pAn(Yn)). Note that\n$$E_{p_B}[Z_n] = \\sum_{Y_n \\in X} -p_n^B(y_n) \\log(p_n^A(y_n)) = H(p_n^B, p_n^A),$$\nwhere H(pnB, pnA) is the cross entropy between the two distributions pnB(.) and pnA(.).\nNote that, unlike the case analyzed in section 3.1, here, EpB [Zn] is not necessarily finite.\nFor EpB [Zn] to be infinite, as we can infer from equation 4, we must have that pAn(yn) = 0 and pnB(yn) > 0 for some yn \u2208 x. In this case, if the string includes such yn, then we realize that the string is not generated by model A with the probability of 1. This is a trivial case.\nYet, if the string does not include any such yn, then we can update the probability distributions as\n$$p^B_n = \\frac{p_n^B(y_n)}{\\sum_{y_k : p^A(y_k)>0} p_n^B(y_k)}.$$"}, {"title": "Statistical test", "content": "Now we design our statistical tests using the results in Theorems 1 and 2 and then evaluate type I (false positive) and type II (false negative) errors. In particular, we consider a finite-length text with length N generated by a model M. We first design tests for detection between different LLMs. In that, we have white-box access to all models in the hypothesis test. In Section 4.1, we design a simple statistical test that determines whether a text is generated by a model A or another model B. Then, in Section 4.2, we extend our results to composite tests that determine whether a text is generated by a model that belongs to a set of models A or a model that belongs to a disjoint set of models B. Finally, in Section 4.3, we study the case where we don't have white-box access to all models in the hypothesis set (for example, a human produced the text) and design a composite statistical test to identify whether the text is generated by a model A or not."}, {"title": "Simple statistical test for detection between two LLMS", "content": "Statistical test. Given a string YN with length N, we design a statistical test to detect whether model A or model B generated the text. The null hypothesis Ho is that the text YN is generated by B, and the alternative hypothesis H\u2081 is that YN is generated by A. We first calculate the random variables\n$$Z_n^A:= - \\log(p_n^A(Y_n)) \\text{ and } Z_n^B:= - \\log(p_n^B(Y_n)), \\text{ and then we calculate the sums } \\frac{1}{N} \\sum_{i=1}^N Z_i^A \\text{ and } \\frac{1}{N} \\sum_{i=1}^N Z_i^B. \\text{ Our test rejects the null hypothesis } H_0 \\text{ if } \\frac{1}{N} \\sum_{n=1}^N Z_n^A < \\frac{1}{N} \\sum_{n=1}^N Z_n^B.$$\nOtherwise, our test accepts the null hypothesis.\nType I and type II errors. Type I error occurs when the test incorrectly concludes that the text is generated by the model A when it is written by B, and Type II error happens when the test fails to identify that text is generated by the model A and incorrectly concludes that it is generated by B.\nTo quantify our model's type I and type II errors, we need to make the following (mild) assumption.\nAssumption 2. (minimum difference). We assume that if the generative and evaluator models are different, for an arbitrarily small positive \u03f51 > 0, we have\n$$\\frac{1}{N} \\sum_{n=1}^N D_{KL}(p_n^B||p_n^A) \\geq \\epsilon_1.$$\nAssumption 2 ensures that the two models satisfy a minimum distance in terms of their KL divergence over the generated text. Note that KL divergence, by definition, is a non-negative value that demonstrates the distance between the two distributions over the next word for the two models. Our results show that the type I and type II errors of our statistical test are approximately exp (O(-N\u03f51)), which indicates that even for small values of \u03f51 that can converge to zero with the length of text (for example, \u03f51 = O(N-1/2)), our statistical test provides exponentially small type I and type II errors in the length of the text. Hence, our theoretical bounds only require that the two models do not impose the same probability distribution on the string.\nProposition 1. If Assumptions 1 and 2 hold, then the type I and type II errors for our statistical test are upper bounded by\n$$2 \\exp \\left[ - \\frac{N(\\epsilon_1/2)^2}{-c_3 \\log(\\epsilon)} \\min \\left(1, \\frac{(\\epsilon_1/2)}{-c_3 \\log(\\epsilon)} \\right) \\right] + 2 \\exp \\left[ - \\frac{N\\epsilon_1/2}{c_1 \\log(K)} \\min \\left(1, \\frac{\\epsilon_1/2}{c_1 \\log(K)} \\right) \\right],$$\nwith constants c1, c3, and e as introduced in Theorems 1 and 2.\nInterpretation. Proposition 1 demonstrates that the type I and type II errors of our simple test decrease exponentially in the text length."}, {"title": "Statitical test for detection among multiple LLMs", "content": "Statistical test. Given a string YN with size N, we design a statistical test to detect whether the text is generated by one of the models A = {A1,..., Ap} or one of models B = {B1,..., Bq} generated the text. The null hypothesis Ho is that the text YN is generated by one of the models in B, and the alternative hypothesis H\u2081 is that it is generated by one of the models in A. We first calculate the random variables ZM =: \u2013 log (pMn(Yn)), and sum ZM =: \u2013 log (pMn(Yn)), for all models M\u2208 AUB. Our test rejects the null hypothesis Ho if for some Ai \u2208 A, we have\n$$\\frac{1}{N} \\sum_{n=1}^N Z_n^{A_i} < \\frac{1}{N} \\sum_{n=1}^N Z_n^{B_j}; \\forall B_j \\in B.$$\nOtherwise, our test accepts the null hypothesis.\nType I and type II errors. Type I error occurs when the test incorrectly concludes that the text is generated by one of the models in A when it is written by one of the models in B, and Type II error happens when the test fails to identify that text is generated by one of the models in A and incorrectly concludes that it is generated by one of the models in B. Similar to our test for the two model version, Assumption 2 must hold for us to quantify our model's type I and type II errors."}, {"title": "Statistical test for detection between an LLM and human", "content": "We need to make the following assumption to design our test.\nAssumption 3. (minimum tangible difference). We assume that if the generative and evaluator models are different, then\n$$\\frac{1}{N} \\sum_{n=3}^N E \\left[ D_{KL} (p_n^B||p_n^A) | Y_{n-2} \\right] \\geq 4 \\log^2(K).$$\nAssumption 3 ensures that the two models satisfy a minimum distance in terms of their expected KL divergence. Clearly, if models are the same (or very similar), then A and B impose the same (or almost the same) probability distributions pAn(yn) over YN, and hence KL divergence becomes zero. This makes differentiation impossible. Assumption 3 rules out cases where the two models impose very similar distributions over the text under evaluation.\nLemma 4. Under assumption 3, for any positive constant c4 \u2264 log2(K)/2, we have\n$$IP ( |h_N (B, A)(Y_N) \u2013 h_N (A,A)(Y_N)| \\leq c_4 ) \\leq 2 \\exp \\left[ - \\frac{Nc_4}{-c_3 \\log(\\epsilon)} \\min \\left(1, \\frac{c_4}{-c_3 \\log(\\epsilon)} \\right) \\right].$$\nRemark 3. We note that the only application of Assumption 3 for establishing our results is that this assumption ensures that the statement c4 < log2(K)/2 in Lemma 4 holds. Our results remain true ( even without Assumption 3) if there exists c4 > 0 such that if the generative and evaluator models are different, then\n$$P( |h_N (B, A)(Y_N) \u2013 h_N (A,A)(Y_N)| \\leq c_4 < 2 \\exp \\left[ - \\frac{Nc_4}{-c_3 \\log(\\epsilon)} \\min \\left(1, \\frac{c_4}{-c_3 \\log(\\epsilon)} \\right) \\right].$$\nStatistical test. Given a string YN with size N, for arbitrary constants t < c4 \u2264 log\u00b2(K)/2, we design a statistical test to detect whether the evaluator model A generated the text. The null hypothesis Ho is that the text YN is not generated by the evaluator model A (e.g., it is generated by another model B), and the alternative hypothesis H\u2081 is that YN is generated by the evaluator model A. We first calculate the random variable Zn =: log (pAn(Yn)), and then we calculate the sum \u03a3n=1 Zn. Our test rejects the null hypothesis Ho in favor of the alternative H\u2081 if\n$$\\sum_{n=1}^N Z_n > h_N(A, A)(Y_N) \\leq t.$$\nOtherwise, our test accepts the null hypothesis.\nType I and type II errors. Type I error occurs when the test incorrectly concludes that the text is generated by the evaluator model A when it is written by B, and Type II error happens when the test fails to identify that text is generated by the evaluator model A and incorrectly concludes that it is not written by A."}, {"title": "Preliminary experiments", "content": "We conduct our numerical analysis on the following pre-trained language models: GPT-2 small, GPT-2 medium, GPT-2 large, GPT-2 XL, and GPT-Neo. Through experiments with different generative and evaluator models, we examine whether the log-perplexity of a short portion of text converges to the average cross-entropy. Our experiments measure these values across generated text and analyze their performance over different configurations. Our setup includes generating tokens with pre-trained models and recording each token's selection probability and calculated metrics."}, {"title": "Same generative and evaluator model", "content": "In the first set of experiments, we employ GPT-2 to generate a series of 100 tokens, beginning with the fixed prompt \"Jack\". We use the model's conditional probability distribution for each token generation step to sample the next token. Note that for the white-box model of GPT-2, probability distributions are accessible. We calculate each generated token's empirical entropy and log-perplexity and repeat this process for comparisons. We use Softmax-normalized probabilities to select the next token and store the generated token and its probability distribution. For each sub-string of length N starting from the first token in the generated sequence, we compute the log-perplexity la(YN), and the empirical entropy hn (A, A)(YN). The results are shown in Figures (1a-1d). We consistently observe that the numerical results confirm Theorem 1 that the log-perplexity converges to the average entropy when the generative and evaluator models are the same."}, {"title": "Different generative and evaluator models", "content": "To extend our numerical analysis to the case with different generative and evaluator models, we generate a string using the following generative models: GPT-2 medium, GPT-2 large, and GPT-2 XL. Then, we calculate the log-perplexity of these strings using the evaluator model GPT2-small. We calculate the cross-entropy of the strings under each generative model and the evaluator model (GPT2-small). The results are shown in Figures (2a-2c). Results in these figures confirm Theorem 2. In particular, we observe that when the evaluator and generative models are different, the log-perplexity of the string converges to the average cross-entropy of the string under generative and evaluator models."}, {"title": "Conclusion", "content": "In this study, we establish first zero-shot statistical tests with theoretical guarantees for text with finite length to distinguish between (i) LLM-generated and human-generated texts in Proposition 3, and (ii) the text generated by two different LLMs A and B in Propositions 1 and 2. We prove that the type I and type II errors for our tests decrease exponentially in the text length. As a critical step in designing our tests, we derive concentration bounds in the difference between log-perplexity and the average entropy of the string under A. Specifically, for a given string, in Theorem 1, we demonstrate that if the string is generated by A, the log-perplexity of the string under A converges to the average entropy of the string under A, except with an exponentially small probability in string length. Furthermore, in Theorem 2, we show that if B (which can be either another model or human) generates the text, then, except with an exponentially small probability in string length, the log-perplexity of the string under A converges to the average cross-entropy of B and A. Our theoretical results rely on establishing concentration bounds for the difference between the log-likelihood of a sequence of discrete random variables and the negative entropy for non-independent random variables on a finite alphabet. Results in the literature (e.g., Zhao [2022]) derive concentration bounds for iid random variables, and one of our theoretical contributions is to extend the results to non-independent random variables by introducing random variables that form a martingale. We hope that our work inspires more research on zero-shot LLM-text detection with provable guarantees."}]}