{"title": "Prevailing Research Areas for Music AI in the Era of Foundation Models", "authors": ["Megan Wei", "Mateusz Modrzejewski", "Aswin Sivaraman", "Dorien Herremans"], "abstract": "In tandem with the recent advancements in foundation model research, there has been a surge of generative music AI applications within the past few years. As the idea of AI-generated or AI-augmented music becomes more mainstream, many researchers in the music AI community may be wondering what avenues of research are left. With regards to music generative models, we outline the current areas of research with significant room for exploration. Firstly, we pose the question of foundational representation of these generative models and investigate approaches towards explainability. Next, we discuss the current state of music datasets and their limitations. We then overview different generative models, forms of evaluating these models, and their computational constraints/limitations. Subsequently, we highlight applications of these generative models towards extensions to multiple modalities and integration with artists' workflow as well as music education systems. Finally, we survey the potential copyright implications of generative music and discuss strategies for protecting the rights of musicians. While it is not meant to be exhaustive, our survey calls to attention a variety of research directions enabled by music foundation models.", "sections": [{"title": "Introduction", "content": "Music information retrieval (MIR) is considered to be a mature, interdisciplinary research area, and rightfully so\u2014over the years, many tasks have been significantly improved on, and the community has become ever more inclusive with music of different cultures and artist-centered approaches. However, the emergence of a new wave of generative artificial intelligence (AI) models has made a significant impact on researchers and artists alike. Major news outlets are now covering AI-powered tools for music composition as well as AI-generated music (e.g., the \"audio deepfakes\" of high-profile artists) (Chow 2023). Moreover, numerous music AI startups have formed in a very short timespan, promising capabilities such as prompt-based music synthesis and acoustic style transfer; their initial results have already gained massive popularity on social media. At the core of this music AI boom is the rapid advancement of foundation models and natural language processing (NLP) technologies, particularly large language models (LLMs). In the context of these new technologies and the ongoing public conversation about their implications, researchers may be misled to believe that a majority of MIR and music AI tasks have already been solved.\nTo that end, this work investigates several avenues of MIR research, identifying the needs for greater validity and integrity, while also highlighting unsolved topics. We divide our discussion among three broad categories: fundamental, applied, and responsible music AI\u2014hoping to inspire potential answers to the age-old researcher's dilemma: what should I work on now?"}, {"title": "Fundamental Music AI", "content": ""}, {"title": "Model Architectures", "content": "In the adjacent research field of computer vision, convolutional neural network (CNN) models like ResNets (He et al. 2016) and vision transformer models (Dosovitskiy et al. 2020) can be seen as foundation models, having learned rich visual representations from extensive image datasets. In language processing, BERT (Devlin et al. 2018) is a famous example of large-scale language understanding, with the subsequent T5 transformer (Raffel et al. 2020) paving the way for a natural and unitary interface for language models leading up to modern LLMs.\nThe research in music foundation models, however, has only recently started to emerge (Ma et al. 2024). A music foundation model would learn to understand music in multiple dimensions, capturing elements like melody, harmony, rhythm and a variety of stylistic and cultural musical traits, leading to a universal representation verified by multiple downstream tasks. The Holistic Evaluation of Audio Representations (HEAR) challenge at NeurIPS 2021 (Turian et al. 2022) was one of the first larger initiatives to come to robust audio representations, by providing a benchmark of nineteen diverse tasks.\nGeneratively pre-trained latent representations from the VQ-VAE Jukebox (Dhariwal et al. 2020) have been shown to be useful for a variety of tasks, including typical music information retrieval (Castellon, Donahue, and Liang 2021a) and source separation (Manilow et al. 2022). Neural audio codecs such as Encodec (D\u00e9fossez et al. 2022) and SoundStream (Zeghidour et al. 2021) utilize discrete neural representation learning (Van Den Oord, Vinyals et al. 2017) to encode audio into discrete tokens, useful for generative tasks.\nFinally, (Won, Hung, and Le 2024) and (Gardner et al."}, {"title": "Explainability", "content": "Explainable artificial intelligence (XAI), a crucial area for the development of trustworthy and interpretable methods, has not yet seen widespread applications to MIR. (Choi, Fazekas, and Sandler 2016) and (Won, Chun, and Serra 2019) explain music tagging models by auralisation and visualization of spectrogram features, similar to saliency map approaches in image processing, where the goal is to attribute regions of interest and their influence on the model's decisions. (Foscarin et al. 2022) develop a concept-based method of explaining composer classification, attempting to provide explanations understandable by a musicologist, as opposed to being useful only to a computer scientist or AI expert. (Mishra et al. 2017) adapt a XAI tool called LIME (Local Interpretable Model-agnostic Explanations (Ribeiro, Singh, and Guestrin 2016)) to music content analysis. This technique is also further applied to singing voice detection in (Mishra et al. 2020). (Haunschmid, Manilow, and Widmer 2020) propose an extension called audioLIME, using source separation instead of image segmentation on spectrograms.\nHowever, adaptation of currently existing XAI tools, like SHAP (Shapley additive explanations (Lundberg and Lee 2017)), LRP (layer-wise relevance propagation (Bach et al. 2015)) or CRP (concept relevance propagation (Achtibat et al. 2023)) is still very sparse in the music space, and even more rarely seen beyond the task of music tagging."}, {"title": "Interpretability", "content": "Several works in the language modeling domain have assessed whether these models have internal representations that capture meaningful features applicable to various downstream tasks (Meng et al. 2022; Tenney et al. 2019), from color (Abdou et al. 2021) to world knowledge (Li et al. 2022; Yun et al. 2023) to auditory representations (Ngo and Kim 2024). Understanding the inner representations of these black-box models offers an alternative approach to MIR tagging models, mitigating the issue of the lack of detailed annotations in existing music datasets. Furthermore, it can offer a new way towards inference-time control, by editing particular internal representations. Recent work on training probe classifiers on the embeddings of music generative models has shown that these models encode internal representations from higher-level musical concepts, such as genre, emotion, and instrument type (Castellon, Donahue, and Liang 2021b; Koo et al. 2024) to lower-level, music theory concepts (Wei et al. 2024). In addition, several works have investigated how different architectures and self-supervised learning play a role towards music understanding tasks (Won, Hung, and Le 2023; Li et al. 2023). However, further work can investigate how to leverage these internal representations towards controllable generation as well as gaining new insights about these model architectures by investigating how these representations change layer-by-layer."}, {"title": "Generative Models", "content": "Although more complete overviews of generative models have been published (Herremans, Chuan, and Chew 2017; Ji, Yang, and Luo 2023; Le et al. 2024), here we briefly touch upon a few remaining challenges.\nLarge strides have been made in the last year, with the emergence of text-conditioned audio-based generative music models (Melechovsky et al. 2024; Agostinelli et al. 2023; Copet et al. 2024). There are a number of challenges remaining, however. Firstly, many of these systems are only able to generate short fragments, although recently some text-to-music models are able to generate longer fragments through cascading (Schneider et al. 2023) approaches. Secondly, many of the models that come out are not available open source. Rare examples of open source models include MusicGen (Copet et al. 2024) and Mustango (Melechovsky et al. 2024). While it may appear that we have reached impressive text-to-music performance with the emergence of companies such as Suno, none of these models are available for the research community to build upon. Furthermore, some of these models lie in a gray area with respect to training on copyrighted data.\nAnother challenge is real-time music generation. The main constraint here is hardware. Many of the better generative music systems have high GPU requirements. For instance, Mustango take about 40 seconds for inference of a 10s audio fragment on an NVIDIA A100 GPU. There is an opportunity to research technologies and models that are GPU-optimized and manage to reach real-time performance.\nIf we want composers and music producers to benefit from the generative music tools developed by the MIR community, we need to provide interfaces so that they can use these tools within their familiar digital audio workstations (DAWs). This would enable these systems to truly become tools for co-creation instead of merely replacing composers and musicians. Interfacing with DAWs is typically done through virtual studio technology (VST) plugins (Marrington et al. 2017). While there have been a few attempts at interfacing MIR tools with VSTs, the threshold remains high due to the fact that VSTs are coded in C++ and most machine learning models use Python. There have been limited attempts to bridge the gap between Python and VSTs (Braun 2021). These may inspire future researchers to enable easy MIR Python integration.\nAnother remaining challenge is generating singing voices, which currently has not been perfected. This may be due to the fact that until a few years ago the field has relied on MIDI files for generation (which do not contain singing voices). A few works have recently come out, indicating the potential of this task (Huang et al. 2022b). Further aspects of generative music are discussed in more detail in later sections."}, {"title": "Evaluation Metrics", "content": "Evaluating music and audio systems presents unique challenges, primarily due to the subjective nature of human auditory perception and the diverse preferences of listeners. Although a variety of metrics exist both for audio (like KL divergence or the Frechet Audio Distance (Kilgour et al."}, {"title": "Controllability", "content": "The aim of controllable generation is to guide powerful AI models towards generating outputs with user-specified attributes; this task has seen some progress in the field of music AI. In addition to the traditional MIDI-based models that take music features or emotion conditions as input (Makris, Agres, and Herremans 2021), a number of text-based controllable models have come out in the last year. Many of the text-prompt models, however, are only controllable on a very high level (e.g. genres, emotions). Only a handful of controllable models such as Mustango, provide dedicated mechanisms for applying music theory into the model prompts. This enables, for instance, generating music that use certain chord sequences, tempos, or keys. Furthermore, text control is more oriented towards manipulating global styles (mood, genre). However, to control music generative models on time-varying, low-level concepts (tempo, dynamics), we need to devise more intuitive interfaces and mediums of control to define these changes. Music ControlNet (Wu et al. 2023b) incorporates time-varying controls (melody, dynamics, rhythm) on top of global control (mood, genre). Several other works have adopted symbolic, compositional, and rule-based approaches towards controllable music generation (Huang et al. 2024; Thickstun et al. 2023). In the future, there is still much room for improvement to allow musicians and composers to truly co-create with AI through more descriptive, and temporal, theory-aware instructions, e.g. 'start with an ascending melody, include a key change and end on a C-note.'\nAnother approach for controllable music generation includes emotion steering. Some work has looked into MIDI-generation with emotion such as the lead sheet generation by (Makris, Agres, and Herremans 2021), and tension-steered classical music by MorpheuS (Herremans and Chew 2017). Being able to control the emotions of generated music would have widespread implications for using generative Al systems for healthcare and personalized music generation (Agres et al. 2021). This task remains very hard, as music emotion recognition systems have not yet reached high accuracy (Bogdanov et al. 2019), despite the availability of some larger datasets like MTG-Jamendo (Bogdanov et al. 2019). Furthermore, recent work has adopted reinforcement learning from human feedback (RLHF) in music generation (Cideron et al. 2024) to align towards human preferences.\nIn general, we see a trend towards multimodal control of generative systems. We have discussed the rise of text-to-music system in Section. Another emerging multimodal control is video-to-music. This task has only seen a handful of models emerge (Kang, Poria, and Herremans 2024; Su et al. 2024), leaving much opportunity for future work. In the same line, the task of real-time, adaptive game music generation offers promising avenues for conditioned music generation. The latter has a myriad of challenges, including the real-time aspect, as well as a lack of large datasets."}, {"title": "Multimodality", "content": "Multimodal methods operate by compressing data in different modalities (i.e., images and their corresponding text descriptions) into a joint embedding space. This is often done by the means of contrastive learning, as in the famous vision model CLIP (Contrastive Language-Image Pre-Training (Radford et al. 2021)). This technique is the backbone of the current wave of text-prompted generative models. Similar approaches have already been used for bridging audio and text, with notable examples being two separate models named CLAP (Contrastive Language-Audio Pre-Training (Elizalde et al. 2023; Wu et al. 2023c)) and a model called MuLAN (Huang et al. 2022a). OpenL3, proposed in (Cramer et al. 2019), is a model trained on an audio-video correspondence task, where the model is trained to determine whether a second of audio matches a frame of video. CLaMP, proposed in (Wu et al. 2023a), is a pioneering contrastive learning model which learns a joint embedding space for symbolic music and text descriptions for purposes of information retrieval and querying by compositional traits. However, multimodal representation learning of symbolic music is still an underexplored area. Further research on cross-modality in music may involve combining all the aforementioned modalities and allowing for transformations between them, as well as unlocking larger datasets leading to stronger foundational representations."}, {"title": "Efficiency", "content": "Efficiency is a big part of music AI models for multiple reasons. First, efficient models are accessible to a wider range of researchers, who may not have access to huge computational resources. Second, the music domain oftentimes has requirements of efficient generation and processing, for instance in all live music applications. Recent times have brought advancements such as mixed precision training and model quantization, facilitated by additional support in modern hardware. Nevertheless, smaller, efficient models are an active research area, often including a mix of signal processing techniques and deep learning paradigms such as self-supervised learning. PESTO (Riou et al. 2023) is a recent state of the art self-supervised fundamental frequency estimator with only around 29k parameters, leveraging the transposition qualities of the Constant-Q Transform (CQT). Basic Pitch (Bittner et al. 2022) is a pitch tracker with only around 16k parameters. It uses a harmonic CQT representation of audio. RAVE (Caillon and Esling 2021) is a generative model for style transfer and audio generation which is 20-80 times faster than real time and can be easily by run on a modern CPU. It is based on a convolutional architecture using a mix of representation learning and adversarial fine-tuning. In order to obtain its high efficiency, it also leverages multi-band decomposition and PQMF (Pseudo Quadrature Mirror Filters (Nguyen 1994)) as part of the audio processing.\nWe emphasize the fact that many of the recent highly efficient models rely on Digital Signal Processing (DSP) techniques, variants of the Fourier transform and audio decomposition methods. We consider these techniques to be a key factor in further research on efficient music AI models, equally as important as the advancements of hardware architectures and deep learning paradigms."}, {"title": "Applied Music AI", "content": ""}, {"title": "Music Discovery", "content": "The integration of recommender systems, deep learning, and music information retrieval has revolutionized the way users discover music, offering personalized suggestions based on listening habits and preferences. However, this innovation is not without its challenges. One significant issue is the \"echo chamber\" effect, where recommender algorithms tend to suggest music similar to the user's previous choices, potentially limiting exposure to a wider variety of genres, artists and cultures. Long-term user engagement is a non-trivial metric to grasp and analyze in recommender systems (Wang et al. 2022) - nevertheless, recent works like (Wang et al. 2023) emphasize the importance of fairness in recommender systems, while (Zangerle and Bauer 2022) specifically emphasize that a key desirable feature of a music recommender system is its ability to help users discover lesser-known or older tracks from its back catalog, which is sometimes referred to as long-tail recommendation. Furthermore, (Knees, Schedl, and Goto 2020) provides an analysis of discovery interfaces design principles and their varying emphasis on quality metadata along with artist and musical context, describing a shift in the process of music discovery from a listeners perspective. Further research is needed to ensure a diverse and inclusive listening experience while keeping an artist-centered perspective on discovery."}, {"title": "Music Editing & Production", "content": "A new task that has leveraged recent successes in generative music audio models, is true music editing with free text instructions. Models such as MusicMagus (Zhang et al. 2024b) and Instruct-MusicGen (Zhang et al. 2024a) are large multimodal models that allow the user to perform novel tasks such as changing the instrument of an audio file, the genre, mood, or even generating a new instrument on top of existing accompaniment. To train such models, researchers often build upon datasets for source separation, which can be augmented for additive/generative tasks of specific stems.\nOne of the largest benefits of MIR research for music editors and producers has also been the development of music source separation (MSS) software. In the last five years, neural networks have come to define the state-of-the-art performance in the MSS task (Stoller, Ewert, and Dixon 2018; D\u00e9fossez et al. 2019). Certain models such as Spleeter (Hennequin et al. 2020) have been open-sourced, while proprietary tools like LALAL.AI have seen widespread commercial usage. Recent research on MSS has assessed novel model architectures (e.g., band-split RNN (Luo and Yu 2023) or transformers (Rouard, Massa, and D\u00e9fossez 2023)) or new training paradigms (e.g., augmenting training data (Pons et al. 2024) or modifying the loss function (Sawata et al. 2023)). Overall, improving the separation of unique instruments, like distinct singing voices or synthesizers, continues to prove challenging. However, because some MSS models are packaged into ready-to-use web apps or DAWs, many musicians and DJs have gained wider access towards remixing and interpolating. The legal implications of MSS algorithms might be addressed by commercial systems, for example, by leveraging established audio fingerprinting methods to identify copyrighted works and ensuring proper royalties.\nEmerging AI-based methods for music mixing and mastering have been introduced (Steinmetz et al. 2021; Mart\u00ednez-Ram\u00edrez et al. 2022). But notably, with this automated music mastering (AMM) task, there is a lack of publicly accessible training data\u2014namely, recordings of dry (anechoic) real-world instruments together with their final mastered mix. Additionally, obtaining annotations for real song mixes from professional audio engineers could greatly improve the fully-supervised deep learning models for music mastering. A few researchers have noted the mutual objectives between MSS and AMM, suggesting that joint optimization may lead to a more tractable and explainable separation strategy (Yang et al. 2022).\nIn addition to audio mixing and mastering, the human-like rendering of MIDI files is also a remaining challenge. This includes both adding expressiveness to quantized MIDI (Cancino-Chac\u00f3n et al. 2018), as well as developing new neural sound fonts. The latter ranges from neural networks that mimic tube amplifiers (Damsk\u00e4gg et al. 2019) to models can directly synthesize MIDI instruments (Castellon, Donahue, and Liang 2020). This research has not been on the"}, {"title": "Music Performance", "content": "Using neural networks as live musical instruments introduces a new paradigm in musical creativity, where artificial intelligence collaborates with human musicians to generate novel sounds and compositions. RAVE (Caillon and Esling 2021) has been used to play drum rhythms through integration with Max\u00b9 and controls from a motion sensor, allowing to create rhythms by waving the sensor in the air in distinctive, expressive gestures. The sampling and looping capabilities of VampNet (Garcia et al. 2023) have been used for creating live experimental soundscapes with a live instrumentalist by means of interplay of the musician with the model. AI models can also adapt in real-time to inputs from human performers allowing otherwise physically impossible instruments. An example is the HITar, an augmented acoustic guitar played with a hybrid-percussive technique, morphed and extended in real-time by AI-generated sounds of the tabla (Martelloni, McPherson, and Barthet 2023).\nWe consider the usage of AI to create totally new, previously undiscovered means of musical expression an especially exciting and worthwhile endeavor."}, {"title": "Music Education", "content": "Although the applications of AI in music education are still under-explored, the most common ideas are either creating software that directly assists a student in musical practice or software that increases the accessibility and therapeutic qualities of learning music. These methods include providing additional feedback, creating personalized learning materials or providing new ways of interacting with music. (Gover and Zewi 2022) propose the task of conversion of difficulty level, aiming to generate easier or harder versions of piano arrangements. Pitch and beat tracking models can be used to create software assistants for vocalists and instrumentalists alike. (Morsi et al. 2023) propose a way of detecting conspicuous mistakes in piano performances, regardless of the presence of a score to compare with. (Balliauw et al. 2017) provide automatic generation of piano fingerings for students. (Volk et al. 2023) provide insight into the benefits of using music AI in music therapy, which has been proven to be beneficial when working with a variety of cases and conditions. Furthermore, works like Piano Genie (Donahue, Simon, and Dieleman 2019) have shown the potential of using machine learning to create novel hardware interfaces for existing instruments - these interfaces may not only enable new means of creativity, but may also be used as assistive tools for learning music and may provide greatly enhanced accessibility in music education."}, {"title": "Responsible Music AI", "content": ""}, {"title": "Datasets", "content": "Given the breadth of MIR tasks discussed in this work, there are ample opportunities for researchers to develop new datasets that can enrich the community. Historically, different approaches have been used for audio datasets: either small audio datasets were distributed, only the extracted features were distributed, or track IDs were distributed (Won, Spijkervet, and Choi 2021). A portion of available music datasets is provided in MIDI format, representing another one of the multiple modalities in music. Symbolic formats like MIDI in many cases require different approaches than audio and are useful for applications involving note sequences, timing, and structure.\nUsing MIDI data for MSS necessitates re-synthesis in order to obtain any raw audio signals (Manilow et al. 2019). Subsequently, using virtual instruments can introduce digital artifacts and limited expressivity. A very popular real instrument dataset for MSS is MUSDB18 (Rafii et al. 2017), which provides four instrument stems for 150 full lengths music tracks, or roughly 10 h. Because of its relatively short total duration, in recent years MUSDB18 has become an benchmark dataset more so than a training dataset. To cover a greater breadth of instruments, genres, and mixes, more researchers are looking out towards open-domain audio datasets like FMA (Defferrard et al. 2016) (which contains 106K or roughly 343 days of Creative Commons-licensed audio). In the case of open-domain downmixed recordings (i.e., without individual instrument stems), self-supervised or unsupervised methods for MSS would be needed.\nThe lack of legally obtainable training data affects the text-to-music generation problem as well. Looking at text captioned datasets, a necessity for training the ever popular text-to-music models, the choice is extremely limited. MusicCaps (Agostinelli et al. 2023), MusicCaps-LM (Doh et al. 2023), and MusicBench (Melechovsky et al. 2024) are the only copyright cleared datasets with captions and music. In terms of MIDI datasets, the recently released MidiCaps dataset is the only dataset that contains full length MIDI files with extensive music-descriptive text (Melechovsky, Roy, and Herremans 2024). The long absence of such datasets explains the lack of text-to-MIDI systems, except for a few very recent releases such as MuseCoco text-to-MIDI that circumvents the lack of a caption dataset by extracting features from the input text captions (Lu et al. 2023)."}, {"title": "Copyright", "content": "Over the decades, countless legal conflicts have come up between the creators of copyrighted works and disruptive technologies (affecting cable television, MP3 players, and even streaming services); most of these cases have failed in the courts, given that the technologies' capabilities were not at odds with the interests of copyright holders. However, generative AI models have progressed and proliferated at an unprecedented rate, vastly outpacing the evolution of laws and policies (Samuelson 2023). Most recent litigation has targeted the tendency of LLMs to reproduce portions of copyrighted text (e.g., song lyrics) which was present in its"}, {"title": "Conclusion", "content": "So what should I work on now? Although the storied history of AI and music extends as far back as 1950s, in the last few years alone, recent algorithms have achieved unparalleled capabilities, amplifying the now mainstream dialogue about AI. The technology and software discussed in this survey paper have already started to impact music educators, producers, performers, record labels, and artists of every level. However, from our brief discussion, we see that this rapid progress has introduced newfound challenges and opportunities unique to the field of MIR. From ethical datasets and copyright attribution, to multimodal controllable systems, real-time generative modeling, to novel methods for music production, remixing, and editing-there has never been a better time to become a music Al researcher."}, {"title": "Reproducibility checklist.", "content": "This paper:\nIncludes a conceptual outline and/or pseudocode description of Al methods introduced. NA\nClearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results. Yes\nProvides well marked pedagogical references for lessfamiliare readers to gain background necessary to replicate the paper Yes\nDoes this paper make theoretical contributions? No\nDoes this paper rely on one or more datasets? No\nDoes this paper include computational experiments? No"}]}