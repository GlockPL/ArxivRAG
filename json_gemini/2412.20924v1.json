{"title": "HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization", "authors": ["Zijie Fang", "Yifeng Wang", "Peizhang Xie", "Zhi Wang", "Yongbing Zhang"], "abstract": "Tissue semantic segmentation is one of the key tasks in computational pathology. To avoid the expensive and laborious acquisition of pixel-level annotations, a wide range of studies attempt to adopt the class activation map (CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue segmentation. However, CAM-based methods are prone to suffer from under-activation and over-activation issues, leading to poor segmentation performance. To address this problem, we propose a novel weakly-supervised semantic segmentation framework for histopathological images based on image-mixing synthesis and consistency regularization, dubbed HisynSeg. Specifically, synthesized histopathological images with pixel-level masks are generated for fully-supervised model training, where two synthesis strategies are proposed based on Mosaic transformation and B\u00e9zier mask generation. Besides, an image filtering module is developed to guarantee the authenticity of the synthesized images. In order to further avoid the model overfitting to the occasional synthesis artifacts, we additionally propose a novel self-supervised consistency regularization, which enables the real images without segmentation masks to supervise the training of the segmentation model. By integrating the proposed techniques, the HisynSeg framework successfully transforms the weakly-supervised semantic segmentation problem into a fully-supervised one, greatly improving the segmentation accuracy. Experimental results on three datasets prove that the proposed method achieves a state-of-the-art performance. Code is available at https://github.com/Vison307/HisynSeg.", "sections": [{"title": "I. INTRODUCTION", "content": "CANCER is one of the major leading causes of global population death [1]. Among all cancer diagnosis methods, pathological diagnosis is the gold standard. Since the U.S. Food and Drug Administration approved the use of whole slide image (WSI) scanners for the initial pathological diagnosis of cancer in 2017 [2], the digital observation and preservation of tissue slides have become a reality. Along with the rapid development of artificial intelligence, utilizing deep learning for automatic pathological diagnosis with digital WSIs has become a new trend, giving birth to a new cross-discipline named computational pathology [3]. In computational pathology, tissue semantic segmentation is one of the key tasks due to its ability to differentiate various types of tissues. Different tissue types, such as tumor, stroma, lymphocyte, and normal tissues, form the tumor microenvironment (TME), which plays a key role in the generation and development of cancer, and the therapeutic efficacy and prognosis of cancer patients are closely related to the TME [4].\nTo enable automated tissue semantic segmentation, a large number of researchers have designed a series of deep-learning models. For example, Ronneberger et al. [5] developed a U-shaped network called U-Net by proposing skip connections between encoders and decoders to enhance the segmentation quality of boundaries. After that, many U-Net-based segmentation networks have been proposed [6], [7]. Although these deep learning models have achieved satisfactory tissue segmentation results and provided great opportunities for automated TME analysis, all of them are based on fully-supervised learning strategies. In fully-supervised segmentation, pixel-level segmentation masks are required for model training. However, obtaining dense pixel-level masks is expensive and tedious. Additionally, due to the specialized nature of histopathological images, only trained clinicians or pathologists can perform the annotation task, which further increases the difficulty of acquiring fine annotations. A study [8] has shown that the average time required to perform pixel-level annotations for 100 histopathological images under a 224 x 224 size is more than 200 minutes. Besides, pixel-level fine annotations suffer from variations among different annotators due to subjectivity, leading to unavoidable noises in annotations.\nIn order to solve the above problems, some studies try to achieve fine-grained semantic segmentation based on weakly-supervised learning frameworks using weak labels such as classification labels [10], point annotations [11], and scribbles [12]. Among all forms of weak labels, classification labels are the easiest to acquire. Compared with pixel-level annotations, which take 200 minutes to annotate 100 images, assigning"}, {"title": "II. RELATED WORKS", "content": "1) WSSS for Natural Images: Due to the time-consuming and laborious process of obtaining pixel-level segmentation masks, as well as the risk of inaccurate annotation, many researchers have attempted to achieve semantic segmentation using weak labels. Among all types of weak labels, image-level classification labels have received extensive attention due to their ease of access. Most existing studies first train a classification network and then generate pseudo-masks for the training set based on the CAM [13] to enable segmentation model training. These studies mainly focus on designing novel CAM generation strategies or training constraints to amend the existing issues in CAM. For example, to solve the under-activation problem of CAM, SC-CAM [19] divides each classification category into subclasses by clustering. Ultimately, the CAMs of different subclasses are fused, improving the accuracy of segmentation boundaries. Considering the equivariant property of the segmentation task, Wang et al. [14] design a self-supervised equivariant attention mechanism (SEAM) to alleviate the under-activation and over-activation problems in CAM. However, the above studies are all designed for natural images. Compared to natural images, histopathological images have more homogeneous content and more uniform colors, which often makes the performance of methods designed for natural images unsatisfactory when directly applied to histopathological images.\n2) WSSS for Histopathological Images: In WSSS for histopathological images, most studies are also based on CAM. HistoSegNet [10] is one of the earliest works focusing on weakly-supervised histopathology tissue segmentation. HistoSegNet firstly infers pseudo-masks with GradCAM [20] and then designs a series of post-processing strategies to refine the pseudo-masks. However, the manually-defined post-processing methods are data-sensitive, thereby limiting the generalization ability of the model. Han et al. [8] propose a novel WSSS model named WSSS-Tissue for histopathological images with a progressive dropout attention strategy, forcing the model to focus on the boundary regions of the tissues. To tackle the low-resolution problem of CAMs, Zhong et al. [15] propose a WSSS framework named HAMIL. Li et al. [21] propose an online easy example mining mechanism named OEEM, where the segmentation loss is weighted by the confidence of the predicted probability map to avoid incorrect predictions in the pseudo-masks affecting the segmentation performance. Zhong et al. [22] introduce a CDMA+ framework by adopting a multi-task learning approach, where weakly-supervised segmentation is considered as an auxiliary task while semi-supervised segmentation is taken as the primary task. Specifically, CDMA+ employs CAM to constrain the learning process of the segmentation probability maps, which is similar to the proposed consistency regularization in this paper. However, CDMA+ utilizes upsampling to interpolate the CAMs to the same shape as the probability maps to compute consistency. In contrast, we use downsampling to reshape the probability maps instead of the CAMs. Considering that downsampling usually more accurately preserve the contour of the image content than interpolation, the consistency constraint proposed in this work may be more reasonable. Besides, all the abovementioned studies are still improvements to CAM and do not consider the characteristics like more uniform colors of histopathological images. In contrast, this paper fully uses the homogeneity property of the histopathological images to generate synthesized images with pixel-level masks, which provides new clues to WSSS for histopathological images.\nIn addition to CAM-based methods, some studies utilize multiple-instance learning (MIL) for weakly-supervised tissue segmentation. For example, Xu et al. [23] propose a CAMEL method, which takes the histopathological images as bags and the cropped small patches in the images as instances. Specifically, CAMEL first proposes an instance pseudo-labeling module by developing a cMIL strategy to obtain the pseudo-labels for the patches. Next, fine-grained pseudo-masks are obtained to train a segmentation model by assigning the pseudo-labels to all pixels in the patch. By taking histopathological images as bags and the pixels in the images as instances, Jia et al. [24] propose a framework named DWS-MIL for cancerous and non-cancerous tissue segmentation. However, the MIL-based WSSS methods only consider two tissue categories. On the contrary, in clinical, clinicians usually need to analyze multiple tissue types in the TME during cancer treatment, which limits the application of these MIL-based methods."}, {"title": "B. Image Synthesis in Computer Vision", "content": "Image synthesis is one of the most important tasks in computer vision. Since Goodfellow et al. [25] proposed the generative adversarial network (GAN), researchers have conducted a series of explorations in image synthesis based on GAN. For example, the Pix2Pix [26] is first proposed to accomplish the image-to-image synthesis task. The CycleGAN [27] is proposed to solve the problem that paired data required by Pix2Pix is difficult to obtain. In medical image synthesis, Guan et al. [28] design a GramGAN framework to synthesize histopathological images with different stainings from the hematoxylin-eosin (H&E) stained images. In recent years, diffusion models have attracted extensive attention due to their advantages of a more stable training process and more diverse generation styles [29]\u2013[31]. In the medical field, Moghadam et al. [32] achieve histopathological image synthesis based on a diffusion probability model using genotype labels as conditions. Oh et al. [33] propose a method called DiffMix, which uses pixel-level cell segmentation masks as conditions and synthesizes histopathological images corresponding to these masks. However, generative model-based methods cannot be directly applied to WSSS tasks, because they either only synthesize histopathological images without segmentation masks or require known segmentation masks as conditions to generate the corresponding images."}, {"title": "III. METHOD", "content": "To avoid the issues in CAMs, we propose a HisynSeg framework empowered by an image-mixing synthesis approach. An overview of HisynSeg is shown in Fig. 3. The framework consists of three modules, i.e., an image-mixing synthesis module, a synthesized image filtering module, and a histopathological image segmentation module. In the image-mixing synthesis module, we propose two synthesis strategies: Mosaic transformation and B\u00e9zier mask generation. The role of the synthesized image filtering module is to pick out authentic images from all the synthesized images. The histopathological image segmentation module is built to train a segmentation model in a fully-supervised manner with the synthesized images and corresponding pixel-level masks. Besides, to improve the robustness of the model to the occasional artifacts in the synthesized images, the real images in the training set are also fed into the segmentation model. Considering that real images do not have segmentation masks, this paper further proposes a self-supervised consistency regularization. The details of each module are explained in the following subsections."}, {"title": "A. Image-Mixing Synthesis Module", "content": "Unlike natural images, whose backgrounds often have semantic information (e.g., sky, grass, and water), the back-"}, {"title": "1) Image-Mixing Synthesis with Mosaic Transformation", "content": "The Mosaic transformation [35] commonly utilizes four images $\\{I_i\\}_{i=1}^{4}$ to generate one synthesized image. For a synthesized image $I_M$ shaped $H \\times W$, the Mosaic transformation first arbitrarily selects an anchor point $A = (H_A, W_A)$, where $\\alpha H < H_A < \\beta H$ and $\\alpha W < W_A < \\beta W$ (0 < \\alpha < \\beta < 1. We set $\\alpha$ = 0.2 and $\\beta$ = 0.8 in the experiments). Then, the four parts divided by the anchor point in the synthesized image are filled by the pixels in $\\{I_i\\}_{i=1}^{4}$, respectively. More specifically, the four images $\\{I_i\\}_{i=1}^{4}$ are firstly randomly cropped into four intermediate images $\\{I_i'\\}_{i=1}^{4}$, whose shapes are $H_A \\times W_A$, $(H - H_A) \\times W_A$, $H_A \\times (W - W_A)$, and $(H - H_A) \\times (W \u2013 W_A)$. Then, the four intermediate images are placed on the four corners of the synthesized image, i.e.,\n\n$I_M = \\begin{bmatrix}I_1'&I_2'\\\\I_3'&I_4'\\end{bmatrix}$"}, {"title": "2) Image-Mixing Synthesis with B\u00e9zier Mask Generation", "content": "Although the image-mixing synthesis strategy based on Mosaic transformation can provide histopathological images with multiple tissue types, introducing abundant tissue heterogeneity and semantic information about inter- and intra-tissue interactions into the synthesized images, the splicing operation in Mosaic transformation makes the segmentation boundary between different types of tissues a straight line, as shown in Fig. 4. However, the boundaries between different types of tissues in real histopathological images are mostly smooth curves. To this end, this paper further designs an image-mixing synthesis strategy by generating masks with smooth B\u00e9zier curves [36]. Benefiting from the smoothness of the B\u00e9zier curves, the synthesized images have similar appearances to the real images, thereby improving the segmentation performance.\nSpecifically, we achieve image-mixing synthesis by constructing multiple consecutive B\u00e9zier curves to form a closed shape, which is used as a B\u00e9zier mask. Formally, an n-order B\u00e9zier curve can be uniquely determined by n + 1 control points, denoted as $P_0, P_1, ..., P_n$. The curve equation $C(t)$ with parameter $t$ can be calculated by\n\n$C(t) = \\sum_{k=0}^{n} {n\\choose k}t^k (1 \u2013 t)^{n-k}P_k, t\\in [0,1]$,\n\nwhere $P_k = [x_k, y_k]$ is the coordinate of a control point, ${n\\choose k} = \\frac{n!}{k!(n-k)!}$ is the bionormial coefficient, and $0^0$ is defined as 1 [36].\nNext, multiple B\u00e9zier curves are connected end-to-end to construct the B\u00e9zier mask. Firstly, N points are randomly taken in a unit square following the anti-clockwise direction. Then, two consecutive points $P_i$ and $P_{(i+1)\\ mod\\ N}$ are taken in turn as the start and terminal control points of a 3-order B\u00e9zier curve $C_i(t)$ (i = 0,\u2026\u2026, N\u22121). Since the 3-order B\u00e9zier curve needs to be determined by four control points, the two other control points are generated between the two selected points to guarantee the smoothness between two successive B\u00e9zier curves. In other words, the first derivative at the endpoint of the former B\u00e9zier curve should be equal to the first derivative at the start point of the latter B\u00e9zier curve, i.e.,\n\n$\\frac{dC_i(t)}{dt}|_{t=1} = \\frac{dC_{(i+1)\\ mod\\ N}(t)}{dt}|_{t=0}, i = 0,..., N-1.$\n\nFinally, by scaling the coordinates of the curves to the image size, a smooth B\u00e9zier mask M composed of N B\u00e9zier curves can be generated. By assigning 1 to the inner part and 0 to the outer part of the mask, a synthesized image can be generated with two histopathological images with a single tissue type by\n\n$I = M\\odot I_1 + (1 \u2212 M)\\odot I_2$,\n\nwhere $\\odot$ means element-wise multiplication, and $I_1$ and $I_2$ represent the two selected histopathological images.\nSince the synthesized images are made up of images with a single tissue type, for each synthesized image, we can obtain the pixel-level segmentation mask, which is utilized as the ground truth for training the segmentation model."}, {"title": "B. Synthesized Image Filtering Module", "content": "In the synthesized images, artifacts usually exist due to variations in staining concentration and brightness. Besides, histopathological images with different subtypes, grades, or other clinical characteristics may be composed in a synthesized image, making the TME features in the synthesized"}, {"title": "C. Segmentation with Consistency Regularization", "content": "After obtaining the authentic synthesized images from the image filtering module, we can train a tissue segmentation model under a fully-supervised scheme. Denote a synthesized image as $I_{syn}$, whose segmentation mask is $M_{syn}$, the predicted segmentation probability map can be obtained by\n\n$M_{syn} = D(\\varepsilon(I_{syn}))$,\n\nwhere D and $\\varepsilon$ stand for the decoder and the encoder of the segmentation model, respectively. Then, the segmentation probability map can be supervised via the segmentation loss, which is defined by\n\n$\\mathcal{L}_{seg} = Dice(M_{syn}, M_{syn}),$\n\nwhere $Dice(x, y) = \\frac{2|x \\cap y|}{|x| + |y|}$ represents the Dice loss.\nAlthough the authenticity of the synthesized images has greatly improved with the filtering module, it is still difficult to guarantee that there are no artifacts in the filtered synthesized images. In contrast, real images in the WSSS dataset do not contain any synthesis artifacts and have true TME. Therefore, leveraging the real images for model training can further prevent the model from overfitting the occasional artifacts while facilitating the model to capture the heterogeneity of the TME. Inspired by this, we utilize the real histopathological images in the WSSS dataset to supervise the segmentation model. Denote a real image as $I_{real}$. By feeding it into the segmentation network, the predicted segmentation probability map can be yielded by\n\n$M_{real} = D(\\varepsilon(I_{real})).$\n\nHowever, the real images only have image-level classification labels. Therefore, no pixel-level segmentation masks can be utilized to supervise the predicted probability map $M_{real}$. To solve this problem, we adopt the idea of CAM, where the image features extracted by a classification network are utilized to locate the semantic information in an image. Specifically, an auxiliary classification task is built to obtain a tissue activation map to supervise the probability map $M_{real}$. We take the encoder $\\varepsilon$ as the backbone of a classification network (i.e., sharing the same network structure and weights with $\\varepsilon$) and add a specially designed classification head after the last feature extraction layer of the encoder. Specifically, we build a 1 \u00d7 1 convolution layer to project the feature map to an output category space, i.e.,\n\n$\\mathcal{F}_c = Conv_{1\\times1}(\\mathcal{F}).$\n\nHere, the feature map extracted by the last layer of the encoder is denoted as $\\mathcal{F} \\in \\mathbb{R}^{H'\\times W'\\times D}$, where H', W', and D stand for the height, width, and channel number of the feature map, respectively. The output $\\mathcal{F} \\in \\mathbb{R}^{H'\\times W'\\times C}$ has the same number of channels as the segmentation map (i.e., number of tissue categories). Therefore, it can be viewed as a tissue activation map, which should share a consistency with the segmentation probability map $M_{real}$. To this end, a consistency regularization is proposed to enable the supervision of $M_{real}$ with the $L_1$ loss by\n\n$\\mathcal{L}_{reg} = ||\\sigma(\\mathcal{F}_c) \u2013 R(M_{real}) ||_1,$\n\nwhere $\\sigma$ and R represent the softmax function and the down-sampling operation, respectively. Here, considering the distributions of $M_{real}$ and $\\mathcal{F}_c$ are different, we first project $\\mathcal{F}_c$ into a probability space using the softmax activation and then guarantee the pixel-wise consistency with the $L_1$ norm by down-sampling $M_{real}$ to make the tensors\u2019 shapes compatible. Because the synthesized images have more accurate masks"}, {"title": "A. Experiment Dataset", "content": "This paper uses three weakly-supervised tissue semantic segmentation datasets for experimental evaluation. The details are described as follows.\na) WSSS4LUAD [17]: The dataset is derived from two cohorts from the Cancer Genome Atlas and Guangdong Provincial People's Hospital, respectively. The training set contains 10,091 histopathological images with sizes ranging from (200 ~ 500) \u00d7 (200 ~ 500) pixels. For each image in the training set, the existence of three tissue types, i.e., tumor (TUM), stroma (STR), and normal (NOM) tissue, is annotated in a multi-label scheme. The validation set consists of 31 small images ranging from (200 ~ 500) \u00d7 (200 ~ 500) pixels and 9 large images ranging from (1500 ~ 5000) \u00d7 (1500 ~ 5000) pixels. The test set contains 66 small and 14 large images whose shape range is the same as the validation set.\nThe annotation process of WSSS4LUAD was completed by eight junior, two senior, and one expert pathologist. Specifically, the junior pathologists were responsible for annotation and labeling, while the senior and expert pathologists were required to check and verify the results. Deep learning models were utilized to accelerate the labeling and annotation process. A ResNet-38 model was first trained with around 500 patches with pathologist-annotated image-level labels. The trained ResNet-38 model was then utilized to infer pseudo-labels for other patches. Patches with low prediction confidence were reviewed and corrected by the senior pathologists, and the junior pathologists checked those with high confidence. For pixel-level annotation, the deep learning framework WSSS-Tissue [8] was utilized to generate initial pseudo-masks for each image. Then, PhotoShop was used to refine the pseudo-masks. A label review board confirmed all the image-level labels and pixel-level masks.\nb) BCSS-WSSS: This dataset is a WSSS version of the BCSS dataset [9], which was created by Han et al. [8]. The original BCSS dataset consists of 151 WSIs, each of which was annotated with a region of interest (RoI). The RoIs were selected and annotated with pixel-level masks by a study coordinator and a doctor. A senior pathologist was responsible for checking and approving the results. The original BCSS dataset was annotated with 21 different tissue types. The BCSS-WSSS dataset combined these 21 tissue types and background pixels into 5 classes, namely tumor (TUM), stroma (STR), lymphocytic infiltrate (LYM), necrosis (NEC), and background. Finally, by randomly cropping 224 \u00d7 224 patches from the RoIs, Han et al. [8] built the training, validation, and test sets which consist of 23,422, 3,418, and 4,986 histopathological images, respectively. The image-level labels were inferred from the pixel-level masks.\nc) LUAD-HistoSeg [8]: This dataset is derived from the H&E stained WSIs of lung adenocarcinoma patients from Guangdong Provincial People's Hospital. 224 \u00d7 224-shaped images are cropped from the WSIs, making the LUAD-HistoSeg dataset consist of a training set of 16,678, a validation set of 300, and a test set of 307 images. Four tissue types, tumor epithelial (TE), necrosis (NEC), lymphocyte (LYM), and tumor-associated stroma (TAS), are labeled in the dataset. The dataset was annotated by five junior clinicians and three professional pathologists. The junior clinicians were responsible for annotation with a two-stage process. In the first stage, the junior clinicians utilized the Labelme software to outline rough segmentation masks. Next, PhotoShop was used to refine the masks. After labeling and annotating all images, the three professional pathologists were asked to check the labels and annotations. Images with poor quality (e.g., blurred, dirty, and unevenly stained) or suffering from inconsistency among the pathologists' annotations were discarded.\nSince HisynSeg needs images with a single tissue type to synthesize images, we list the number of these images in Table I. Although the number of images with a single tissue type is not abundant for some datasets, e.g., LYM images in the LUAD-HistoSeg dataset, the following experimental results validate that the proposed HisynSeg can still have competitive performance compared with the SOTA frameworks."}, {"title": "C. Experimental Settings and Implementation Details", "content": "All experiments are conducted on a Ubuntu 18.04 LTS server with a single Nvidia RTX 3090 GPU. All codes are implemented using Pytorch 1.12.1 and Pytorch Lightning 1.7.1. For the WSSS4LUAD dataset and the LUAD-HistoSeg dataset, we separately synthesize 3,600 real images with the Mosaic transformation strategy or the B\u00e9zier mask generation strategy. For the BCSS-WSSS dataset, 7,200 real images are synthesized with each strategy. Since the WSSS4LUAD dataset focuses more on the segmentation of tumors and stroma, the foreground and background images used in the B\u00e9zier mask generation strategy are only selected from either tumor or stroma images. For other datasets, the images utilized for synthesizing are randomly selected from images with a single tissue type. For Mosaic transformation, the four gridded images are preprocessed by random flipping, shifting, scaling, rotation, and cropping. We set H = W = 224 in Mosaic transformation to ensure the synthesized images are 224 \u00d7 224. In contrast, there is no pre-processing for B\u00e9zier mask generation, except in the WSSS4LUAD dataset, where we first use bilinear interpolation to reshape the images into 224 \u00d7 224 to solve the problem of different image shapes. For the synthesized image filtering module, a ResNet-18 [37] is trained from scratch for each synthesized dataset with an Adam [38] optimizer and a 0.001 learning rate. For the BCSS-WSSS dataset, we train the discriminator for three epochs on the synthesized dataset with Mosaic transformation. For other synthesized datasets, we train the discriminator for five epochs. The reason for utilizing a small epoch number mainly comes from the fact that abundant number of real images (over thousands for each dataset) and synthesized images can be obtained to train the discriminator. Therefore, even with a small epoch number, the discriminator can still be trained well to learn the differences between the real and synthesized images, thereby improving the performance of the segmentation network.\nThe segmentation network is implemented by DeepLabV3+ [39] with EfficientNet-b6 [40] as the backbone. The network is trained for 60 epochs over the WSSS4LUAD dataset and 25 epochs over other datasets. The optimizer is AdamW [41] with a learning rate of 0.0001 and a weight decay of 0.05. The model with the best validation mIoU is selected for testing. During training, random resizing, cropping, flipping, shifting, scaling, rotation, and optical distortion are utilized as pre-processing to augment the training images. For the WSSS4LUAD dataset, we resize all training images to 224 x 224 by bilinear interpolation. Besides, we utilize a sliding window strategy with a window size of 224 \u00d7 224 in inference to handle the large-size validation and test images. Specifically, no overlapping is taken in validation. While in testing, we use multiscale images with 50% overlapping. The sliding window strategy is also utilized in all the competing methods for fairness. In addition, we adopt a test-time augmentation strategy with flips and 0, 90, 180, and 270-degree rotations. The final results are fused by averaging. We adopt the iterative training strategy over the BCSS-WSSS and the LUAD-HistoSeg dataset and retrain the model for one time. We do not utilize the iterative training strategy over the WSSS4LUAD dataset since we empirically find no performance improvement. All baselines are compared strictly following the published papers or open-sourced codes."}, {"title": "D. Statistical Analysis", "content": "In order to alleviate the impact of randomness on performance evaluation, we use five seeds for each experiment to train five models with the same training, validation, and test set, and report the average value with standard deviation for each metric in the form of Meanstd, where Mean and std represent the average of the metric and the standard deviation, respectively. In this paper, a two-tailed permutation test is utilized to calculate the p-value of the difference between the performance of our proposed HisynSeg and the competing methods. It is worth noting that the permutation test has been widely used to calculate p-values of different performance metrics in computational pathology [42], [43]. Unless otherwise stated, we use * in each reported metric of a competing method to represent that the metric of the competing method is significantly different from ours (the last line of a table) with a p-value less than 0.05."}, {"title": "E. Comparative Experiments", "content": "1) Quantitative Results: The quantitative performance of the proposed HisynSeg framework and other competing methods over the WSSS4LUAD dataset is shown in Table II. We also provide the performance of a baseline which shares the same architecture with our HisynSeg framework and is trained utilizing the CAMs generated by a ResNet-50 model with"}, {"title": "F. Ablation Studies", "content": "1) Comparison of Different Synthesis Strategies: To show the reasonableness of the image-mixing synthesis module, we study and compare different synthesis strategies, including directly using images with a single tissue type (OneLabel), gridding (Gridded), CutMix [34], or using Mosaic transformation without image splicing (Mosaic w/o splicing) over the LUAD-HistoSeg dataset. An example image synthesized by"}, {"title": "6) Ablation Studies on Image Filtering Module", "content": "To validate the synthesized image filtering module, we train a segmentation model with synthesized images that do not pass the filtering module while keeping other settings the same over both WSSS4LUAD and LUAD-HistoSeg datasets. Experimental results in Table XIII show that the segmentation model trained with the filtering strategy outperforms that without the module in both mIoU and fwIoU over the two datasets. Compared with the LUAD-HistoSeg dataset, the filtering module exhibits more pronounced performance improvements when applied to WSSS4LUAD. This disparity can be attributed to the homogeneous nature of LUAD-HistoSeg, where images originate from a single source and share a consistent original resolution. Therefore, the synthesized images remain relatively realistic even without filtering. Conversely, WSSS4LUAD encompasses images from two distinct cohorts with varying original resolutions. Directly resizing these diverse images for synthesis can result in distortion and unreality. Hence, the filtering module's ability to eliminate these unrealistic images significantly enhances the performance of the framework. This demonstrates the filtering module's capacity to improve the framework's robustness to data quality variations. The proposed HisynSeg can achieve impressive segmentation performance even when faced with substantial differences in image appearance."}, {"title": "7) Ablation Studies on Loss Combinations", "content": "One of this paper's major contributions is proposing an auxiliary classification task and consistency regularization to enable the images without pixel-level masks to participate in the training of the segmentation model. To validate the loss design, ablation studies are conducted with different loss combinations. The results are shown in Table XIV. As we can see from the table, when not applying the classification loss, the segmentation mIoU is only around 70%, which may result from the fact that the activation map (i.e., Fc) cannot accurately locate the semantic information in histopathological images without the"}, {"title": "8) Ablation Studies on Loss Weights", "content": "In HisynSeg, the total loss is calculated by summing up the segmentation loss, regularization loss, and classification loss, which can be viewed as utilizing a weight of 1 for each sub-loss. To validate whether changing the weights may affect the performance of the model, we conduct additional experiments in this subsection. Firstly, we plot the training curves for all the sub-losses by monitoring the value of each sub-loss every 50 training iterations. As shown in Fig. 11, all sub-losses share similar magnitudes during the training process, regardless of the specific dataset. This result supports our weight setting where we do not handcraft the loss weights to different values.\nTo further prove that our model is robust to different loss weights, we change the weight of one sub-loss in {0.1, 0.5, 1, 5, 10} while fixing the weights of the other two sub-losses over the LUAD-HistoSeg dataset. The results are listed in Table XV. The table shows that the default setting can achieve the best performance in all types of IoUs except the LYM IoU, where setting the weight for segmentation loss to 0.5 is the best. However, statistical analysis shows no significant difference between the default and best settings in LYM IoU. Statistical tests also show that the IoUs in several settings have no significant differences from our current setting, proving our framework is robust to different loss weights. Hence, we conveniently utilize a fixed weight of 1 for all the sub-losses."}, {"title": "9) Ablation Studies on Different Backbones", "content": "Because the proposed image-mixing synthesis module, filtering module, and consistency regularization do not rely on any specifically designed network structure, the HisynSeg framework can be deployed to arbitrary segmentation backbones. In the experiments, we utilize DeepLabV3+ [39] as the backbone to realize image segmentation because it has been widely employed for histopathological image segmentation [8], [15]. To validate this choice, we conduct ablation experiments where different backbones are applied to HisynSeg. The results are shown in Table XVI, indicating that the proposed HisynSeg can achieve an mIoU of more than 72% regardless of the backbone used. Besides, DeepLabV3+ shows the best performance than other backbones, and EfficientNet-b6 can beat ResNet-34 in all backbones. These results also prove that using DeepLabV3+ with EfficientNet-b6 can achieve the best performance in all types of IoUs, validating our setting."}, {"title": "10) Applying Synthesized Images to Existing WSSS Frameworks", "content": "Besides providing images with pixel-level masks, the two proposed image-mixing synthesis strategies can also be"}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "In recent years, WSSS for histopathological images has received extensive attention since there is no need to acquire expensive and laborious pixel-level annotations. Currently, mainstream WSSS methods use CAM to generate pseudo-masks and train a segmentation model. However, the CAM has long been criticized for its over-activation and under-activation issues. To this end, this paper proposes a novel WSSS framework named HisynSeg for histopathological images based on image-mixing synthesis and consistency regularization. Considering the artifacts in the synthesized images, this paper proposes a synthesized image filtering module to guarantee the authenticity of the synthesized images. Besides, since it is hard to ensure that there are no artifacts in the filtered synthesized images, this paper further feeds the real images into the segmentation model to avoid the model overfitting the artifacts. Considering there are no segmentation masks for real images to supervise the model, a self-supervised consistency regularization is further proposed to constrain the segmentation probability maps of the real images, enabling the real images without artifacts to take part in the segmentation model training process.\nThere are some limitations in this study. Since the image filtering module is independent of the synthesis module, knowledge about real and fake images learned by the filtering module cannot be fed back to the synthesis module, preventing the synthesis process from being guided. If a learnable image synthesis module can be developed to synthesize authentic images and pixel-level masks gradually, the computation resource wastage caused by generating fake images can be reduced. Furthermore, since the proposed framework relies on accurate image-level labels to synthesize images, all tissue types in the image should be known. However, in practice, there are some datasets like DigestPath [50"}]}