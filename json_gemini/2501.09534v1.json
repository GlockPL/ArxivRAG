{"title": "AI in Support of Diversity and Inclusion", "authors": ["\u00c7i\u00e7ek G\u00fcven", "Afra Alishahi", "Henry Brighton", "Gonzalo N\u00e1poles", "Juan Sebastian Olier", "Eric Postma", "Marie \u0160af\u00e1\u0159", "Dimitar Shterionov", "Mirella De Sisto", "Eva Vanmassenhove"], "abstract": "In this paper, we elaborate on how AI can support diversity and inclusion and exemplify research projects conducted in that direction. We start by looking at the challenges and progress in making large language models (LLMs) more transparent, inclusive, and aware of social biases. Even though LLMs like ChatGPT have impressive abilities, they struggle to understand different cultural contexts and engage in meaningful, human-like conversations. A key issue is that biases in language processing, especially in machine translation, can reinforce inequality. Tackling these biases requires a multidisciplinary approach to ensure AI promotes diversity, fairness, and inclusion. We also highlight AI's role in identifying biased content in media, which is important for improving representation. By detecting unequal portrayals of social groups, AI can help challenge stereotypes and create more inclusive technologies. Transparent AI algorithms, which clearly explain their decisions, are essential for building trust and reducing bias in Al systems. We also stress AI systems need diverse and inclusive training data. Projects like the Child Growth Monitor show how using a wide range of data can help address real-world problems like malnutrition and poverty. We present a project that demonstrates how AI can be applied to monitor the role of search engines in spreading disinformation about the LGBTQ+ community. Moreover, we discuss the SignON project as an example of how technology can bridge communication gaps between hearing and deaf people, emphasizing the importance of collaboration and mutual trust in developing inclusive AI. Overall, with this paper, we advocate for AI systems that are not only effective but also socially responsible, promoting fair and inclusive interactions between humans and machines.", "sections": [{"title": "AI's blessings and threats", "content": "The contemporary advancements in artificial intelligence (AI) have a substantial impact on society. The rise of deep-learning and generative algorithms"}, {"title": "Transparancy of AI", "content": "Within CSAI researchers try to enhance explainability by uncovering the nature of the internal representations in large language models."}, {"title": "Delving into the inner workings of Large Language Models", "content": "The aim of unravelling the opaque nature of large language models (LLMs) is a crucial challenge for their adoption in society. By peering into the proverbial \"black box,\" researchers try to obtain insights into the mechanisms governing LLM behavior, thereby facilitating the identification and rectification of biases inherent within these systems.\nRecent improvements in the performance of Large Language Models (LLMs) like Galactica and ChatGPT might seem impressive. For a growing number of enterprises and institutions, the efficacy gains made possible by these technologies have been reasons to already deploy them for text and speech processing applications. Yet in various societal contexts, these models still lack the capacity for truly meaningful human-like interactions. To facilitate this, it is necessary to make the social dynamics and characteristics of human communication an integral part of their development.\nThe responsible deployment of AI and Deep Learning models in general is hindered by the fact that their workings are inherently difficult to interpret and"}, {"title": "Identifying and resolving biases in AI", "content": "Biases are omnipresent in society and therefore also in AI systems. Three lines of CSAI research address these biases."}, {"title": "Scrutinizing gender biases in language Eva Vanmassenhove", "content": "A meticulous examination of gender biases prevalent within linguistic frameworks constitutes another focal point of CSAI research. Through rigorous analysis, researchers aim to unearth latent biases entrenched within language datasets, thereby paving the way for the development of more equitable and inclusive linguistic models\nLanguage technology pervades our daily lives more than ever, via large language models and virtual assistants to automated translations and voice recognition software. If these technologies are biased against certain genders or reinforce harmful stereotypes, they can perpetuate inequality and exclusion."}, {"title": "Mitigating implicit and explicit biases - Gonzalo Napoles", "content": "The development of transparent AI algorithms stands as an imperative in the quest for fostering trust and accountability within AI frameworks. By imbu-ing AI systems with transparency, researchers aim to engender greater clarity regarding the decision-making processes underlying AI-generated outcomes, thereby mitigating the proliferation of biased outputs.\nWhile the primary focus of AI research revolves around the accuracy of solutions, there has been a recent shift towards studying the impact of AI-based systems in society. These intelligent systems are as good as the data used to build them, which might contain historical biases reflecting arguably fair and ethical processes and practices. Detecting, mitigating and explaining these explicit or implicit biases is key to building responsible, inclusive AI-based systems that contribute positively to the values of our society. Moreover, the fact that recent scientific breakouts in the AI field have been attached to large private companies like Google, Meta or OpenAI has made fair machine learning research a priority for academicians and governments alike.\nOne of the main technical challenges of building fair machine learning mod-els is how to reuse existing historical data that might contain biased patterns."}, {"title": "Facilitating accessibility and empowering diversity", "content": "AI can be utilized to create more inclusive and accessible environments for people of all abilities."}, {"title": "Diversifying training datasets - \u00c7i\u00e7ek G\u00fcven", "content": "The composition of training datasets wields profound implications for the ef-ficacy and fairness of AI algorithms. In recognition of this salient fact, CSAI researchers advocate for the diversification of training datasets to encompass a broader spectrum of demographic, cultural, and experiential attributes, thereby fostering the development of more inclusive AI frameworks.\nAI's potential to enhance diversity and inclusion becomes evident when it actively identifies and addresses the needs of underprivileged groups, those who"}, {"title": "The dynamics of LGBTQ+ disinformation across Europe Henry Brighton", "content": "In 2021, the European Parliament published a 31-page briefing on disinformation campaigns targeting the European LGBTQ+ community [17]. The LGBTQ+ community includes individuals who are lesbian, gay, bisexual, trans-gender, queer, or questioning. An example of disinformation is the claim that the LGBTQ+ community was responsible for the spread of COVID-19. The overall goal of the actors behind anti-LGBTQ+ campaigns is to undermine European unity and destabilize democratic processes in various European countries by attacking \"progressive\" ideologies that champion diversity and inclusion. A key conclusion of the briefing was that the European Union lacks the infrastruc-ture needed to effectively monitor online disinformation. This is an alarming situation because the technologies underlying the propagation of disinformation,"}, {"title": "SignON project: Facilitating inclusivity", "content": "The objective of the recently concluded SignON project was to reduce the communication gap between hearing, deaf and hard-of-hearing people through mobile applications and services for automatic translation between sign and spoken languages [16]. But the \"communication gap\" is not a one-sided issue and we need to make a clarification here. Reducing the communication gap does not imply \"helping deaf people\". It goes in both directions and implies promoting"}, {"title": "Leveraging AI as a tool for bias detection - Juan Sebastian Olier", "content": "Harnessing the capabilities of AI as a discerning instrument for detecting biased visual and linguistic narratives represents a potent strategy in the pursuit of fostering diversity and inclusion. By deploying AI algorithms endowed with bias-detection capabilities, researchers endeavor to identify and mitigate instances of discriminatory content dissemination.\nThe way media represent specific social groups can reinforce prejudices and dominant views about these groups, and can perpetuate their exclusion and lack of power. We seek to illuminate the terminology and frameworks used to describe 'otherness', or how individuals and groups perceive those different from them. By uncovering the issues and logic behind societal categorizations, this research can inform more equitable and just systems for recognizing 'the other' that do not sustain exclusion.\nAI offers a powerful means to study and detect these issues on a large scale. Similarly, AI can uncover new societal issues, raising awareness of exclusionary issues. In a previous project for example we used Deep Learning techniques for an automatic visual content analysis of how migrants are portrayed by media in ten countries. We compared the general group 'migrants' with the specific groups 'refugees' and 'expats'. We found that portrayals predominantly focus on asylum seekers and associate them with poverty and risks for host societies. The results also showed that the demographics as portrayed in the images de-viate from official statistics to various degrees per country. In the portrayal of expats for instance 'white' faces are overrepresented, while 'Asian' faces are underrepresented. And confirming previous research, we found that migrant women are underrepresented and portrayed as younger than men [14].\nCurrently, we are working on understanding the predominant views of the other from a nationalist perspective. The 'other' is often defined by their na-tionality, and this definition shapes perspectives and gives rise to stereotypes and discriminatory behaviors. Recognizing and challenging these stereotypes is crucial to understanding broader phenomena associated with continuing spe-cific dynamics of, for example, power asymmetries that can affect individuals solely due to their nationality. In the future, the goal will focus on the auto-matic discovery of biases in the representation of social groups in media images. The objective is to better understand societal stereotypes and prejudices while developing novel bias discovery methods that can be applied to large image sets.\nSuch insights are crucial for developing more responsible and trustworthy AI approaches, auditing existing AI systems, or assessing their potential societal impacts. More responsible AI requires a better understanding of the phenomena associated with defining the other. Disregarding those phenomena in develop-ing AI systems - which happens because identity-related attributes are often considered insignificant, indisputable and apolitical - these systems can amplify societal issues and perpetuate exclusion. On the other hand, a clear understand-ing of the issues in question contributes to AI systems that can challenge the normative frameworks that sustain biased views of 'others' [15]. As such, AI"}, {"title": "The broader picture", "content": "CSAI research encompasses more than the examples described in this white paper. In particular, its research addresses other societal relevant themes, such as AI for biodiversity, AI for zero hunger, AI for zero poverty, AI for energy and water infrastructure, AI for cybersecurity, and AI for disaster management and defense.\nIn conclusion, this paper has explored how AI can support diversity and in-clusion by highlighting the importance of transparent, socially responsible, and context-aware approaches in AI research. While large language models show promise, they also have limitations especially in understanding cultural nu-ances and reducing social biases\u2014which calls for a multidisciplinary approach. Projects like Child Growth Monitor and SignON demonstrate how AI can ad-dress real-world issues like malnutrition and accessibility. This paper also ex-plores how AI models can be applied in collaboration with both the affected community and domain experts, as demonstrated by the Tilburg Algorithm Ob-servatory's work on disinformation targeting the LGBTQ+ community. Achiev-ing these goals will require inclusive datasets, trust-building, and ensuring that AI algorithms make fair, clear decisions. As AI develops, fairness and inclu-sivity must remain priorities to prevent reinforcing stereotypes and inequalities. Through these efforts, we aim to build AI systems that not only work effectively but also promote a fairer, more inclusive society."}]}