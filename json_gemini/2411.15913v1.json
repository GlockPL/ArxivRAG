{"title": "A Training-Free Approach for Music Style Transfer with Latent Diffusion Models", "authors": ["Sooyoung Kim", "Joonwoo Kwon", "Heehwan Wang", "Shinjae Yoo", "Yuewei Lin", "Jiook Cha"], "abstract": "Music style transfer, while offering exciting possibilities for personalized music generation, often requires extensive training or detailed textual descriptions. This paper introduces a novel training-free approach leveraging pre-trained Latent Diffusion Models (LDMs). By manipulating the self-attention features of the LDM, we effectively transfer the style of reference music onto content music without additional training. Our method achieves superior style transfer and melody preservation compared to existing methods. This work opens new creative avenues for personalized music generation.", "sections": [{"title": "Introduction", "content": "Music has long been a fundamental element of human culture, offering unique experiences that resonate with individual listeners (Barton 2018). As a universal language, music transcends boundaries by facilitating both communication of ideas (Miell, MacDonald, and Hargreaves 2005) and emotional expression through various forms of engagement, from composition to performance and listening (Robinson and Hatten 2012). Nonetheless, music creation has traditionally presented significant barriers to entry, requiring specialized knowledge of theory, instrumental mastery, and technical expertise in composition and production. These requirements have historically restricted music production to individuals with formal training or substantial resources.\nThe rise of artificial intelligence, with its goal of emulating human creativity, has sparked a significant increase in interest in music-related research. Early efforts focused on challenges such as representing music data (Wiggins 1995; Camurri et al. 1995; Balaban 1996) and generating individual notes (sounds) (Miranda 1995). Over time, the scope of research expanded to include more complex issues, such as the generation of longer musical segments, music classification (Weihs et al. 2007; Fern\u00e1ndez and Vico 2013; Kaliakatsos-Papakostas, Floros, and Vrahatis 2020; Ndou, Ajoodha, and Jadhav 2021), and music recommendation (Casey et al. 2008; Song, Dixon, and Pearce 2012). Building upon these advancements, recent developments in AI have led to the exploration of more sophisticated techniques, such as music style transfer. This innovation has opened up new possibilities for ordinary individuals, enabling them to create and experience personalized music in ways that were previously inaccessible.\nInspired by the success of neural style transfer in computer vision (Gatys 2015; Johnson, Alahi, and Fei-Fei 2016; Chandran et al. 2021; Kwon et al. 2024; Wang et al. 2023), music style transfer has emerged as a powerful technique for generating music by decomposing and recombining the content and style elements of different musical pieces (Dai, Zhang, and Xia 2018). Though the distinction between musical content and musical style is not formally defined and context-dependent, researchers have proposed various frameworks for decomposing musical elements into content and style components. Content typically encompasses structural elements such as melody, harmony, and rhythmic patterns, while style encompasses performance-specific attributes including timbre, articulation, dynamics, and genre-specific characteristics (Dai, Zhang, and Xia 2018; C\u00edfka, \u015eim\u015fekli, and Richard 2019). Several studies further demonstrated the possibility of style transfer using non-musical elements, such as environmental sound (Grinstein et al. 2018).\nRecent advances in large language models (LLMs) and diffusion models have simplified music generation and style transfer, enabling intuitive control even for those without detailed musical knowledge. Conventional methods for diffusion model-based music generation utilize the generative power of pre-trained diffusion models. Agostinelli et al. (2023) and Copet et al. (2024) introduced music generation using text conditioning (text-to-audio generation) and melody guidance by using a transformer-based autoregressive model. These works make style transfer more simple and controllable by using text descriptions as style specifications and audio clips as content. Most recently, drawing inspiration from textual inversion in the diffusion model (Gal et al. 2022), Li et al. (2024) utilized a pseudo-word representation for musical styles, enabling style transfer for arbitrary input music while maintaining structural information (e.g., melody or rhythm). However, to obtain satisfactory style-transfer results, it is necessary to provide detailed textual guidance on various musical attributes (e.g., timbre, pitch, performance style, composition style), which require specialized knowledge. Additionally, training or fine-tuning these models demands substantial computational power and time, which greatly limits their practical application in real-world scenarios.\nTo overcome these limitations, we propose a straightforward yet effective training-free approach for music style transfer using pre-trained latent diffusion models (LDM) (Rombach et al. 2022), which are commonly used for training text-to-image diffusion models. Building on the findings of (Chung, Hyun, and Heo 2024; Feng et al. 2022; Ma et al. 2024), attention maps are responsible for determining the spatial arrangement, while the key and value in cross-attention handle the content that fills the space. Additionally, (Chung, Hyun, and Heo 2024) showed that the self-attention layer in diffusion models is well-suited for style transfer, as it preserves the relationships between content image patches after the transfer and encourages style transfer based on the similarity of local textures between the content and the style. As a result, our method aims to transfer the style of reference music to the content music by explicitly manipulating the self-attention features of pre-trained large-scale diffusion models for text-to-image synthesis without any further training or optimization while leveraging the image characteristics of mel-spectograms. Specifically, we simply replace the content's key and value of self-attention with those of the style music, focusing particularly on the later layers of the decoder that capture relevant local textures of mel-spectograms. To further improve music stylization, we also incorporate additional techniques originally introduced in (Chung, Hyun, and Heo 2024), including query preservation, attention temperature scaling, and initial latent Adaptive Instance Normalization (AdaIN).\nOur key contributions include:\n\u2022 We propose a straightforward yet effective training-free approach for music style transfer, leveraging pre-trained LDM through direct manipulation of self-attention features.\n\u2022 Extensive experiments validate that our method can faithfully transfer the style of reference music to content music without the need for additional training or fine-tuning."}, {"title": "Related Works", "content": "Music Style Transfer. Music style transfer enables musical generation through element decomposition and recombination (Dai, Zhang, and Xia 2018). The field has evolved through various neural architectures, each addressing different aspects of musical transformation. Initial research focused on timbre transformation while preserving melodic content, with Engel et al. (2017) using WaveNet-based autoencoders and Grinstein et al. (2018) applying Convolutional Neural Networks (CNN) for timbre transfer between musical and non-musical sounds. These successes extended to genre transformation, where Brunner et al. (2018) demonstrated symbolic music manipulation using Recurrent Neural Networks (RNN). C\u00edfka, \u015eim\u015fekli, and Richard (2019) achieved more sophisticated genre transformation by separately processing melodic content and arrangement style by using the hybrid of CNN and RNN. Adversarial training approaches also produced significant results. WaveNet and CycleGAN architectures enabled style-preserved timbre transfer (Huang et al. 2018; Bonnici, Benning, and Saitis 2022), while Hung et al. (2019) achieved direct disentanglement of pitch and timbre. By leveraging adversarial approach, (Lee et al. 2020) demonstrated the potential of multi-media style transfer by injecting music as the style information for image style transfer.\nMusic style transfer has advanced significantly through both LLM and diffusion-based approaches. LLM-based methods, such as MusicLM (Agostinelli et al. 2023) and MusicGen (Copet et al. 2024), introduced transformer-based autoregressive models that enable text-conditioned and melody-guided generation, making style control more accessible through text descriptions and audio references. Huang et al. (2023) (Huang et al. 2023) first demonstrated diffusion models' potential in music generation, working with both waveforms and spectrograms. Building on these advances and drawing inspiration from textual inversion image diffusion models (Gal et al. 2022), Li et al. (2024) (Li et al. 2024) developed a pseudo-word representation approach, achieving structure-preserving style transfer that maintains melodic and rhythmic elements with flexible style manipulation for arbitrary input music. However, all of these algorithms require detailed guidance or extensive model training. In this paper, we faithfully transfer the style to the content music without any optimization process."}, {"title": "Diffusion Models", "content": "Diffusion models (DM) are a subclass of generative models based on likelihood estimation, with the foundational work being the Denoising Diffusion Probablistic Model (DDPM) (Ho, Jain, and Abbeel 2020). These models are grounded in the theoretical principles of Markov chains and Langevin dynamics. Due to their stable training process and scalability, diffusion models have surpassed Generative Adversarial Networks (GANs) (Dhariwal and Nichol 2021) in image generation tasks, yielding superior sample quality. However, the sampling process in diffusion models is typically slow, as it necessitates the generation of samples through a stepwise Markov chain process. To mitigate this issue, Denoising Diffusion Implicit Models (DDIM) (Song, Meng, and Ermon 2020) introduce a non-Markovian iterative sampling method that accelerates the process while preserving the training procedure. More recently, the LDM (Rombach et al. 2022) has been proposed for image synthesis. This approach compresses images into a lower-dimensional latent space before applying the diffusion process, significantly reducing computational complexity while maintaining high-quality image generation. However, the use of LDM for music generation remains a relatively underexplored area, primarily due to the challenges posed by the scarcity of relevant data and the significant computational cost associated with model training. To address these limitations, we propose a novel, training-free method for music style transfer that does not require any specific dataset or additional training. To the best of our knowledge, this is the first attempt to apply an attention-based manipulation method for style infusing within the context of diffusion models for music style transfer."}, {"title": "Method", "content": "We utilized Stable Diffusion (Rombach et al. 2022) as the backbone to achieve music stylization, as shown in Figure 1. Our work is conducted in the image domain, processing a mel-spectogram obtained from the input audio waveform using a Short Time Fourier Transform (STFT), based on the ideas of Riffusion (Forsgren and Martiros 2022) and MusicTI (Li et al. 2024).\nThe Self-Attention Block in LDM. The Latent Diffusion Model (LDM) (Rombach et al. 2022) is a type of diffusion model trained in a lower-dimensional latent space, which enables the model to focus on essential semantic features of data while reducing computational complexity. Given an image $x \\in R^{H \\times W \\times 3}$, the encoder $\\mathcal{E}$ encodes $x$ into the latent representation, $z \\in R^{h \\times w \\times c}$, and the decoder $\\mathcal{D}$ reconstructs the image from the latent space.\nLeveraging a pre-trained encoder, the entire images are encoded into latent space, and a diffusion model is trained on these latent representations, z. The model predicts the noise $\\epsilon$ added to the noised version of the latent variable $z_t$ at each time step t. The training objective for LDM is given by:\n$L_{LDM} = E_{z,\\epsilon,t}[||\\epsilon - \\epsilon_{\\theta} (z_t, t, y)||^2]$,\nwhere $\\epsilon \\in N(0, 1)$ represents noise, t is a time step uniformly sampled from {1, ..., T}, and y is a condition. $e_{\\theta}$ is a neural network that predicts the noise added to z. In our case, y is a text condition, and $e_{\\theta}$ is modeled using a U-Net architecture that includes a residual block, a self-attetion block, and a cross-attention block for each resolution in sequence.\nGiven a feature $\\phi$ after the residual block and a projection layer $f(.)$, the self-attention mechanism is computed as follows:\n$\\phi_{out} = Attn(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}}) \\cdot V$,\nwhere d represents the feature dimension of the projected query. Q, K, V represent query, key, and value, respectively. Notably, in our configuration, the condition y is an empty text prompt, meaning that no specific text conditions are applied."}, {"title": "Attention-based Style Manipulation", "content": "Following (Chung, Hyun, and Heo 2024), we treat the features in the self-attention layers similarly to cross-attention, using the style image $I_s$ as the conditioning input. Specifically, during the generation process, we replace the key and value features of the content music's mel-spectrogram with those from the style music's mel-spectrogram. To achieve this, we first obtain the latent representations for both the content and style mel-spectograms through DDIM inversion (Song, Meng, and Ermon 2020). We then capture the self-attention features of the style mel-spectrogram throughout the DDIM inversion process. For pre-defined timesteps t = 0,..., T, we invert the style and content mel-spectrograms, denoted as $z_{m_c}$ and $z_{m_s}$, from the image space (t = 0) to Gaussian noise (t = T). During this process, we also gather the query features of the content mel-spectrogram (Q) and the key and value features of the style mel-spectrogram ($K_{m_s}, V_{m_s}$) at each timestep. Next, we initialize the stylized output latent noise $z_{m_o}$ by directly copying the content latent noise $z_{m_c}$. The style transfer is carried out by replacing the original key $K_{m_o}$ and value $V_{m_o}$ in the self-attention layer with the key $K_{m_s}$ and value $V_{m_s}$ derived from the style mel-spectrogram, during the reverse process of generating the stylized output latent $z_{m_o}$. To preserve the content structure and avoid unwanted distortions, we implement a query preservation technique as follows:\n$Q_{out}^{mo} = \\alpha \\times Q_t^{mc} + (1 - \\alpha) \\times Q_t^{mo}$,\n$\\phi_{out}^{mo} = Attn(Q_t^{mo}, K_t^{ms}, V_t^{ms})$,\nwhere $\\alpha$ is a hyperparameter. These operations are applied to the later layers of the decoder (layers 7\u201312 in the SD model) that focus on capturing local texture features.\nAdditional Techniques. In line with the approach of (Chung, Hyun, and Heo 2024), we applied an attention temperature scaling to rectify the attention map sharper and employed AdaIN to modulate the initial latent for guiding the generation process to capture the structural features of a mel-spectogram from the content, as expressed in:\n$z_T^{mo} = \\sigma(z^{mc}_{mo}) (\\frac{z^{mo}_t - \\mu(z^{mc}_{mo})}{\\sigma(z_{mc_T})}) + \\mu(z^{mc}_{mo})$,\nwhere $\\mu(\\cdot), \\sigma(\\cdot)$ denote channel-wise mean and standard deviation, respectively. Our empirical results show that using AdaIN for modulating the initial latent provides optimal performance compared to other style transfer methods, such as AdaConv (Chandran et al. 2021) or EFDM (Zhang et al. 2022)."}, {"title": "Implementation Details", "content": "We experiment with the MusicTI Dataset (Li et al. 2024), which contains a total of 254 five-second clips, with 74 style clips and 179 content clips. Note that our approach does not require a training process; thus, we used all these data at inference only. For LDM, we used stable diffusion ver.1.5 (Rombach et al. 2022). In all our experiments, we fix the parameters of LDM and use the author-released codes using default configurations. All experiments were conducted using the PyTorch framework (Paszke et al. 2019) on a single NVIDIA A100(40G) GPU."}, {"title": "Experimental Results", "content": "In this section, the proposed model's validity is assessed both qualitatively and quantitatively.\nFigure 2 illustrates that our approach produces qualitatively satisfactory audio-transferred outputs throughout various instruments and musical elements. Analysis of mel-spectrograms shows that stylized outputs maintain content structure while incorporating characteristic elements of style sources. For instance, when applying accordion style to hip-hop audio, the stylized mel-spectrograms display distinctive horizontal energy bands in mid-frequencies, characteristic of accordion harmonics. Similarly, cornet-styled piano pieces exhibit concentrated mid-frequency energy and reduced high-frequency components, reflecting the cornet's distinctive timbral characteristics. It is worth noting that even though the stable diffusion model is originally trained for text-to-image generation, not for music generation, it showed remarkable performance on synthesizing style-transferred music without any optimization process and music datasets."}, {"title": "Conclusion & Future Works", "content": "We present a novel approach to music style transfer that leverages pre-trained LDM through attention manipulation, enabling musical style transfer without additional training. Our method modifies self-attention layers during content generation by replacing content's key and value components with those from style music, similar to cross-attention mechanisms. Our experimental results demonstrate successful style injection between mel-spectrograms without additional training. This extensibility of our approach suggests broad applications across various audio domains, including speech and environmental sounds, which we plan to explore in future work. Furthermore, we plan to comprehensively evaluate our approach with multiple evaluation metrics."}]}