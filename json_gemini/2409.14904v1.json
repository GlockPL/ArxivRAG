{"title": "DSG-KD: Knowledge Distillation from Domain-Specific to General Language Models", "authors": ["SANGYEON CHO", "JANGYEONG JEON", "DONGJOON LEE", "CHANGHEE LEE", "JUNYEONG KIM"], "abstract": "The use of pre-trained language models fine-tuned to address specific downstream tasks is a common approach in natural language processing (NLP). However, acquiring domain-specific knowledge via fine-tuning is challenging. Traditional methods involve pretraining language models using vast amounts of domain-specific data before fine-tuning for particular tasks. This study investigates emergency/non-emergency classification tasks based on electronic medical record (EMR) data obtained from pediatric emergency departments (PEDs) in Korea. Our findings reveal that existing domain-specific pre-trained language models underperform compared to general language models in handling N-lingual free-text data characteristics of non-English-speaking regions. To address these limitations, we propose a domain knowledge transfer methodology that leverages knowledge distillation to infuse general language models with domain-specific knowledge via fine-tuning. This study demonstrates the effective transfer of specialized knowledge between models by defining a general language model as the student model and a domain-specific pre-trained model as the teacher model. In particular, we address the complexities of EMR data obtained from PEDs in non-English-speaking regions, such as Korea, and demonstrate that the proposed method enhances classification performance in such contexts. The proposed methodology not only outperforms baseline models on Korean PED EMR data, but also promises broader applicability in various professional and technical domains. In future works, we intend to extend this methodology to include diverse non-English-speaking regions and address additional downstream tasks, with the aim of developing advanced model architectures using state-of-the-art KD techniques.", "sections": [{"title": "I. INTRODUCTION", "content": "With the proactive utilization of Electronic Medical Records (EMR) by healthcare institutions worldwide, a vast amount of medical information is being stored as data. [1]-[3] In particular, EMRs are documented in Pediatric Emergency Departments (PEDs) to capture patients' conditions at the time of visit, test results, and additional details in the form of free-text. These free-text entries are crucial components of EMRs and include essential clinical notes, such as the patient's gender, age, and vital signs. Given the importance of initial responses in PEDs, distinguishing between emergency and non-emergency patients based on EMRs recorded at the time of admission is critical. [4]-[6]\nHowever, owing to the nature of PED environments, clinical notes are often recorded in an unstructured and inconsistent manner, complicating the classification of emergency and non-emergency patients based on them. [7]\u2013[9] This reduces the utility of the data in supporting decision-making by healthcare professionals, such as physicians. [10], [11]\nIn particular, EMR data obtained from PEDs in Korea, a non-English-speaking country, are written in Korean. Non-English-speaking countries present an additional layer of complexity in classification tasks owing to the use of both English and local languages. [12], [13] In such cases, medical jargon and test results are often written in English or using English abbreviations, while the patient's condition and"}, {"title": "II. RELATED WORKS", "content": "EMR [22], [23] represents digitally formatted data that are systematically collected and electronically stored to document patients' health information. In recent years, EMR data obtained from PEDs have garnered significant attention as the effective processing of such data is likely to enhance decision-making in clinical environments. [4], [24], [25] Various related studies have focused on the utilization, analysis, and decision-support methods of EMR data obtained from PEDs from multiple perspectives. [26]\u2013[28]\nEarly research on EMR data processing primarily targeted English-based data [29], applying traditional NLP methods such as Bag of Words (BoW) [30]\u2013[32] and Term Frequency-Inverse Document Frequency (TF-IDF) [33], [34]. However, owing to the significant increase in the amount of acquired medical data in recent years, pre-trained LMs based on transformers have emerged [35], [36], and active research has been conducted on fine-tuning these models for various medical datasets. Notably, LMs such as KM-BERT and Clinical-BERT have been pre-trained on extensive medical data, exhibiting great potential in various NLP tasks in the medical domain. [16], [37]\nHowever, the aforementioned LMs exhibit certain vulnerabilities in the case of N-lingual, free-text-formatted EMR data obtained from PED environments of non-English-speaking countries (see Figure 1). Given the importance of initial responses in PEDs, such issues are critical. [38], [39] This study describes methodologies capable of effectively learning and processing such N-lingual and free-text formatted data."}, {"title": "B. N-LINGUAL FREE-TEXT DATA", "content": "The processing of N-lingual and free-text data presents various motivations and challenges for NLP and deep learn-ing. Early research on NLP focused primarily on English-based datasets. [29] However, data obtained from diverse countries has motivated the study of N-lingual processing [40]-[43], leading to extensive research. This has led to the development of robust models capable of understanding the nuances of mixed languages to enhance the efficiency and"}, {"title": "C. KNOWLEDGE DISTILLATION", "content": "In this study, domain knowledge is transferred from the teacher model to the student model using KD. [21] Traditionally, KD is used in the NLP field to transfer knowledge from a larger teacher model to a smaller student model, with a focus on model size and lightweighting. [51], [52] Previous research approached this by having the student model mimic the teacher model's predictions using relatively simple methods. Recently, KD between transformer-based LMs has evolved to mimic the representation vectors of hidden states and attention matrices. This study, however, does not focus on model size and lightweighting, but rather on \"extracting the unique knowledge possessed by the teacher model and transferring it to the student model.\" Although we use conventional distillation methods [51], [52], we first define the knowledge that the student model needs to learn and subsequently extract it from the teacher model for distillation. Detailed explanations are provided in [Section 3].\nBy examining previous studies that applied KD from a knowledge transfer perspective rather than a lightweighting perspective, active research has been conducted on transferring knowledge from image models to language models [53], [54]; however, little research has been conducted on"}, {"title": "III. PROPOSED METHOD", "content": "In this section, we formalize the central problem considered in this study and introduce the proposed methodology."}, {"title": "A. TEXT PREPROCESSING & LABELING", "content": "EMR data are obtained from Korean PEDs and then analyzed and preprocessed before model training. Even though South Korea is a non-English-speaking country, English is often used alongside local languages. This is reflected in the data obtained, which are recorded in the form of bilingual free-text clinical notes. The preprocessing and analysis methods, described later, are summarized in Table 1.\nFirst, the bilingual free-text clinical notes are preprocessed by performing language and symbol distinction processing and new character removal while retaining the meaning of the existing content as far as possible. Newline characters (\\r, \\n) generated during the process of recording and storing data in medical institutions are removed, and blanks are inserted between languages and symbols to separate them appropriately. This step is also performed for a method described below. Because we use a byte-pair encoding-based tokenizer, inserted blanks do not affect the input words severely [56].\nIn addition, because it is not possible to specify the features to be used as labels in the acquired data, active doctors are consulted to determine the label criteria using columns such as test status and medication history, as recorded in the data.\nAs per the preprocessed data, approximately 43% of the words are Korean, 23% are English, and 33% are symbols and other words. In addition, the number of English words in the data that are medical terms is evaluated by creating our own Korean medical dictionary by crawling the Korean Medical Search Engine and online medical dictionaries. According to Table 1, approximately 20% of the English words in the data are medical words, whereas only 5% of the Korean words are medical terms. This implies that almost all medical terms are more biased toward English than local languages and that"}, {"title": "B. METHOD", "content": "This section describes the domain knowledge transfer methodology proposed in this thesis in detail. The goal of this methodology is to transfer domain knowledge from a language model (teacher) pre-trained on a specific domain to a language model (student) pre-trained on general-purpose data. An overview of the framework is presented in Figure 2.\nWe define S as a generalized language model that categorizes urgent/non-urgent cases based on free-text clinical note data as input data. We train and validate the student model S. In addition, we define a domain-specific language model T that conveys domain knowledge and aids learning when S performs emergency or non-emergency classification tasks. The input free-text clinical notes are denoted by\n$X = {x_s,x_t}$.\nHere, $x_s$ denotes the input of the S model, and $x_t$ denotes the input of the T model. We also define\n$Y = {y}o, Y \u2208 {0,1}$, and label it as emergency/non-emergency. Finally, we construct a dataset Data = {X,Y} consisting of X and Y."}, {"title": "1) Definition of Domain Knowledge", "content": "To transfer domain knowledge, we must define the domain knowledge present in the input $x_i$. This is defined as English words in the PED's EMR data (because, as discussed previously, medical knowledge that the teacher needs to extract is almost always expressed in English, instead of the local language). To represent domain knowledge words (subwords) in the input sequence $x_i$, we represent the knowledge mask M as follows:\n$X_i = {t_1, t_2,..., t_{l-1}, t_l} \u2208 R^l$\n$M_i = {m_1, m_2,\u2026,m_{l-1}, m_l } \u2208 {0,1,\u2026,k}^l$     (1)\nwhere the knowledge mask Mis $M \u2208 R^l$, where each element $m_i$ of M represents the index of the word for which the token at that position expresses a particular piece of domain knowledge. If $m_i = 0$, the token does not contain domain knowledge; if $m_i > 0$, the input $x_i$ contains domain knowledge. In addition, k denotes the number of domain knowledge words. The appropriate domain knowledge token $d_i$ is extracted from the input sequence $x_i$ using M.\n$d_i = {x_i[j]|m_j > 0, \u2200j \u2208 {0,\u2026,l}}$          (2)\nwhere $d_i$ denotes the domain knowledge tokens corresponding to each token of the input $x_i$ with the set relation $d_i \u2208 X_i$.\nChoosing the scope of domain knowledge definition is directly linked to model performance in the proposed methodology. In the data considered in this study, knowledge words are not defined as exclusively medical terms because the boundaries between medical and non-medical terms in the language"}, {"title": "2) Problem Formulation", "content": "Let S and T contain P transformer layers. Knowledge transfer is achieved by performing KD on the encoder layers of the student and teacher models. S further solves the prediction task of classifying emergency/non-emergency cases, given X. Formally, the student receives domain knowledge from the teacher and solves the prediction task by minimizing the following objective function:\n$L_{total} = \u2211_{X\u2208X} L_{pred} (S(x_s), Y)$\n$+\\sum_{d\u2208X} \\sum_{p=0}^{P} L_{layer} (f_p(d_s),f_p(d_t), \u03bb)$                 (3)\nwhere $L_{pred}$ denotes the loss function that optimizes the classifier prediction of the student model performing the emergency/non-emergency classification task and $L_{layer}$ denotes the loss function applied to a given layer of the model. Here, $f_p(\u00b7)$ denotes the encoder block output in the p-th layer. The hyperparameter \u03bb comprises \u03bb = {\u03b1, \u03b2}, where \u03b1 and \u03b2 are applied to $L_{hidn}$ and $L_{attn}$, respectively."}, {"title": "3) Knowledge distillation", "content": "In this section, we mathematically express the process by which a student model receives domain knowledge from a teacher model. To this end, two types of KD are performed: hidden-state and attention-matrix distillation. [51], [52]\nHidden-state Loss is expressed as follows :\n$L_{hidn} = \u03b1MSE(\u0124^S,\u0124^T)$         (4)\nLet $H_S$ and $H_T$ be the hidden states of the student and teacher encoder layers, respectively. Let $H^S$ and $H^T$ be $H^S \u2208 R^{lxd}$ and $H^T \u2208 R^{l'xd'}$, respectively. (Note that d = d', l = l').\n$\u0124^S$ and $\u0124^T$ are expressed as follows:\n$\u0124^S = \u2211_{i=0}^k Avg.Pool(H_i^S, m_i^S) \u2208 R^{kxd}$                (5)\n$\u0124^T = \u2211_{i=0}^k Avg.Pool(H_i^T, m_i^T) \u2208 R^{kxd'}$"}, {"title": "Attention Matrices Loss is expressed as follows :", "content": "$L_{attn} = \u03b2MSE(\u00c2^S,\u00c2^T)$  (6)\nLet $A_S$ and $A_T$ be the hidden states of the encoder layer for the student and teacher models, respectively. Let $A^S$ and $A^T$ be $A^S \u2208 R^{h\u00d7l\u00d7l}$ and $A^T \u2208 R^{h\u00d7l'\u00d7l'}$, respectively (but l = l').\n$\u00c2^S$ and $\u00c2^T$ are expressed as follows:\n$\u00c2^S = \u2211_{i=0}^k Avg.Pool(A^S, m_i^S) \u2208 R^{k\u00d7l}$\n$\u00c2^T = \u2211_{i=0}^k Avg.Pool(A^T, m_i^T) \u2208 R^{k\u00d7l'}$               (7)\nFinally, for each encoder layer, the loss is calculated using the following formula:\n$L_{layer} = \\begin{cases}   \u03b1L_{hidn} & n = 0\\\\L_{hidn} + \u03b2L_{attn} & n\u22600\\end{cases}$            (8)\nwhere n = 0 denotes the embedding layer and $L_{attn}$ is not performed because no attention matrix is generated."}, {"title": "4) Prediction Loss", "content": "The student model continues to perform and optimize the classification task.\n$L_{pred} = CE(S(x_s), Y)$         (9)\nwhere $x_s$ denotes the input to the student model and S($x_s$) denotes the logits of the student model. CE(\u00b7) denotes the cross entropy loss and Y is the label."}, {"title": "IV. EXPERIMENTS", "content": "The proposed methodology is validated using EMR data obtained from Korean PEDs. The task considered in this study is to classify the data into emergencies and non-emergencies using binary classification."}, {"title": "B. IMPLEMENTATION DETAILS", "content": "In this section, we introduce the experimental environment and models on which the proposed methodology is applied. The A6000 GPU device is used, with a batch size of 32, 15 epochs of training with an early termination condition at 15 to choose the weight with the best valid performance. The learning rate is set to 1e-5 for the BERT body and 1e-2 for the classifier. For reproducibility, we set the random seed to 42.\nThe transformer-based language models Ko-BERT and BERT-base are used as baseline student models, and KM-BERT, Clinical-BERT (C-BERT), RoBerta, Bio-RoBerta (B-RoBerta), and Bio-Multilingual-BERT (Bio-M-BERT) are"}, {"title": "C. RESULTS", "content": "Table 2 presents a comparison of all methods in different groups in terms of accuracy, AUROC, AUPRC, recall, precision, and F1 Score. In addition, the average values are listed in the Average column to facilitate the overall performance comparison. In general, transformer-based NLP models are observed to outperform traditional NLP processes proposed in previous studies. Transformer-based language models can adapt quickly to the language patterns present in data when fine-tuned using language patterns during pretraining.\nIn particular, it is interesting to note from Table 2 that language models pre-trained with data from the medical field (KM-BERT, Clinical-BERT, etc.) perform worse than general language models (BERT-based, Ko-BERT) in terms of AUROC and AUPRC. This is because, as explained earlier, general language models perform favorably with respect to N-lingual and unstructured data while pretraining with a large amount of data, whereas language models pre-trained on the medical domain struggle on N-lingual or unstructured real data, even though they have learned medical domain information.\nThe bottom part of Table 2 presents the results obtained by training the existing language models using the proposed methodology. The student models are divided into Ko-BERT- and BERT-based cases and teacher models are taken to be pre-trained language models in the medical domain. The results are observed to be significantly better than those obtained by fine-tuning a simple language model. Moreover, the performance is even better than that of existing student and teacher models when trained alone. The aforementioned experimental results demonstrate the effectiveness of knowledge transfer between language models, leading to efficient interaction between knowledge of different language models."}, {"title": "1) Qualitative analysis", "content": "To verify the relationship between domain words and teacher embeddings before and after DSG-KD visually, we present a scatter plot in Figure 3. Random samples are obtained from the dataset and visualized. In the figure, Ko-BERT is used as the student model and KM-BERT is used as the teacher model."}, {"title": "2) Case analysis", "content": "Furthermore, to understand the impact of the proposed methodology on Domain words (in this study, medical words), we defined and measured the Medical Word Proportion Score (MWPS). The MWPS is defined as follows:\n$MWPS = \\frac{\\% \\text{ of medical terms in English words}}{\\% \\text{ of English words in all words}}$                 (10)\nThis can be expressed mathematically as:\n$MWPS = \\frac{m/E}{E/A} = \\frac{m \u00b7 A}{E^2}$     (11)\nwhere m is the number of medical terms in English words, E is the total number of English words, and A is the total number of words.\nWe measured the MWPS for correctly classified cases in the test set for each model. This allowed us to evaluate the effectiveness of the proposed methodology. The comparison of MWPS for each model is shown in Figure 4. From the figure, we can observe that our model achieves the highest MWPS, indicating that the DSG-KD methodology effectively learns Domain words, i.e., Domain Knowledge. Specifically, it demonstrates that medical knowledge can be transferred to a general language model and that our model shows a slightly higher understanding of medical terms than KM-BERT. In summary, this suggests that the proposed methodology is highly beneficial for applications in specific domains such as the medical field. Thus, it is proven that the DSG-KD methodology learns the medical knowledge embedded in medical terms and achieves superior classification performance."}, {"title": "D. ABLATION STUDY", "content": "The influence of $L_{hidn}$ and $L_{attn}$ on performance is investigated by considering Equation 8. The influences of \u03b1 and \u03b2 are also studied. Two additional experiments are performed with Ko-BERT as the student model and KM-BERT as the teacher model. The performances are evaluated in terms of three metrics: AUROC, AUPRC, and F1 Score."}, {"title": "1) Influence of $L_{hidn}$ & $L_{attn}$ on the objective function", "content": "Table 3 presents the effects of $L_{hidn}$ and $L_{attn}$ on $L_{total}$. The baseline model, which includes all loss components, exhibits an AUROC of 79.4\u00b10.4, an AUPRC of 84.1\u00b10.4, and an F1 Score of 78.6\u00b10.3. This is used as the baseline for comparison in the ablation study, which is performed by removing $L_{hidn}$ and $L_{attn}$ individually, or in combination. The results are as follows.\nCase 1: Removing both $L_{hidn}$ and $L_{attn}$: When $L_{hidn}$ and $L_{attn}$ are removed, all three performance metrics decrease, indicating that the two losses play significant roles in improving model performance. Case 2: Removing only $L_{hidn}$: The absence of $L_{hidn}$ also results in a decrease in performance, although to a lesser extent than in Case 1. This suggests that hidden losses contribute to model performance to a certain extent. Case 3: Removing $L_{attn}$: The absence of $L_{attn}$ has the largest effect on F1 Score, suggesting that this factor is particularly important for the accuracy and recall balance of the model. In contrast to Cases 2 and 3, removing both $L_{hidn}$ and $L_{attn}$ further degrades the performance, emphasizing the importance of synergistic functioning of these components to achieve high model performance.\nThis study demonstrates that each component plays an important role, and that prediction loss is the most important factor in maintaining the integrity of model performance. The cumulative degradation caused by removing multiple components suggests that the interactions among these components are complex and essential for high-quality model learning."}, {"title": "2) Effect of \u03b1 & \u03b2 on the objective function", "content": "Table 4 presents the impact of \u03b1 and \u03b2 on the performance, expressed in terms of performance. When \u03b1 = 0.6 and \u03b2 = 0.2, AUROC = 79.4\u00b10.4, AUPRC = 84.1\u00b10.4, and F1 Score = 78.6\u00b10.3 are observed. We systematically vary \u03b1 and \u03b2 to observe changes in the performance, and the results are presented below.\nCase 1: Impact of equal weights (\u03b1 = \u03b2 = 1.0): Assigning equal weights to both hyperparameters results in a slight decrease in all metrics, suggesting that the model may not need to emphasize the factors controlled by \u03b1 and \u03b2 equally. Case 2: Increasing the emphasis on \u03b2 (\u03b1 = 1.0, \u03b2 = 0.6): Increasing \u03b2 relative to \u03b1 slightly improves the AUROC to 79.0\u00b10.4, indicating that aspects of the model influenced by \u03b2 may be more important for predictive ability. Case 3: Gradual adjustment of \u03b1 and \u03b2: A pattern is observed when proportionally adjusting \u03b2 from 1.0 to 0.1 while gradually decreasing \u03b1 from 1.0 to 0.1, with the highest F1 Score achieved when \u03b1 = 0.8 and \u03b2 = 0.4, and the best AUROC and AUPRC at \u03b1 = 0.8 and \u03b2 = 0.4. This suggests an optimal range for the hyperparameters that balances the factors controlled by the model. Case 4: Significant reduction of \u03b1 (\u03b1 = 0.1, \u03b2 = 0.1): Significantly reducing both \u03b1 and \u03b2 to 0.1 results in the lowest AUROC, a slight increase in AUPRC, and a decrease in the F1 Score, which suggests that very low values of these hyperparameters can degrade the performance of the model, especially in terms of precision and recall balance.\nIn conclusion, the hyperparameters \u03b1 and \u03b2 play important roles in the performance of the Ko-BERT + KM-BERT model. For each data environment, an optimal parameter range yields the best performance. This emphasizes the importance of fine-tuning hyperparameters to satisfy specific performance goals during model training."}, {"title": "V. CONCLUSIONS", "content": "This study addresses the need for effective domain knowledge transfer in pre-trained language models and provides a promising approach for bridging the gap between general-purpose language understanding and domain-specific expertise. As the demand for domain expertise in NLP continues to increase, this study makes a timely and valuable contribution to improving the ability of language models to utilize specific and nuanced domain knowledge.\nTo this end, we introduce a novel methodology to extract domain knowledge from domain-specific pre-trained language models and transfer the knowledge to more generalized language models. We find that domain-specific pre-trained language models are vulnerable to bilingual data in non-English-speaking countries such as Korea, and demonstrate that by transferring domain knowledge to generalized language models, the proposed methodology is robust in N-lingual environments and exhibits effective classification performance in specific domains.\nIn this study, a highest average metric of 789\u00b10.4 is achieved by transferring medical knowledge from the teacher model (KM-BERT) to the student model (Ko-BERT) using Korean emergency room EMR data. This demonstrates that the proposed framework can be used as a tool to assist doctors and healthcare workers in decision-making by helping them overcome N-lingual challenges inherent in EMR data recorded in non-English speaking countries. Beyond the medical field, the methodology can also be applied in various professional and technical fields, opening up potential avenues for its use as a decision aid in many forms.\nIn future works, we intend to extend and improve this"}]}