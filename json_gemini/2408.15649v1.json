{"title": "HIERARCHICAL BLOCKMODELLING FOR KNOWLEDGE GRAPHS", "authors": ["Marcin Pietrasik", "Marek Reformat", "Anna Wilbik"], "abstract": "In this paper, we investigate the use of probabilistic graphical models, specifically stochastic block- models, for the purpose of hierarchical entity clustering on knowledge graphs. These models, seldom used in the Semantic Web community, decompose a graph into a set of probability distributions. The parameters of these distributions are then inferred allowing for their subsequent sampling to generate a random graph. In a non-parametric setting, this allows for the induction of hierarchical clusterings without prior constraints on the hierarchy's structure. Specifically, this is achieved by the integration of the Nested Chinese Restaurant Process and the Stick Breaking Process into the generative model. In this regard, we propose a model leveraging such integration and derive a collapsed Gibbs sampling scheme for its inference. To aid in understanding, we describe the steps in this derivation and provide an implementation for the sampler. We evaluate our model on synthetic and real-world datasets and quantitatively compare against benchmark models. We further evaluate our results qualitatively and find that our model is capable of inducing coherent cluster hierarchies in small scale settings. The work presented in this paper provides the first step for the further application of stochastic blockmodels for knowledge graphs on a larger scale. We conclude the paper with potential avenues for future work on more scalable inference schemes.", "sections": [{"title": "1 Introduction", "content": "In recent years, using graph structures to model and store data has been garnering an increasing amount of attention among practitioners in sectors ranging from academia to government to industry. Indeed by some measures [64, 29], graph database management systems are the fastest growing database type over the past decade. One of the more obvious manifestations of this rise is the recent growth of large scale public graph databases such as DBpedia [35], YAGO [48], and WikiData [68]. The last of these, for instance, contains just over 100 million entities as of 2024, a near seven fold increase over its count in 2014. The open access to such amounts of graph data has spurred on its use in research related to the Semantic Web, artificial intelligence, and computer science broadly. One field of research which has received considerable attention is that of mathematically modelling the underlying graph structure that emerges when a knowledge base is populated by information. The modelling of this structure \u2013 which we refer to as the knowledge graph \u2013 proves useful in its application to solve downstream problems such as link prediction, entity clustering, and hierarchy induction. The last two of these provided the impetus for our work."}, {"title": "2 Related Work", "content": "Our proposed model lies at the intersection of two areas in artificial intelligence which deal with modelling graph data: stochastic blockmodelling and hierarchy induction. Due to the limited overlap of these fields, we provide separate summaries of related works for each."}, {"title": "2.1 Stochastic Blockmodels", "content": "Stochastic blockmodels are a class of probabilistic graphical models used for generating random graphs with roots in the fields of social science and mathematics. First proposed in 1983 by Holland et al. [27] for modelling social networks, they have expanded their utility to fields such as biochemistry [72], education [65], and artificial intelligence [2, 25, 76] among others. In simplest terms, stochastic blockmodels are a type of Bayesian non-parametric graph partition model in that their approach relies on grouping graph entities together via partitions \u2013 often referred to as blocks \u2013 which share similar structural properties. The generative process by which this partitioning occurs is realized by sampling from a set of probability distributions, giving rise to the stochasticity of stochastic blockmodels. The learning process is then infer the parameters of these distributions using a Bayesian inference scheme. We provide a technical introduction to stochastic blockmodels in the subsequent section.\nThe seminal work in this area is the Stochastic Blockmodel [46] which partitions entities into a fixed number of communities and models the interactions between them as those of their communities. Community relations are"}, {"title": "2.2 Hierarchy Induction Models", "content": "In the context of our work, hierarchy induction refers to the discovery of hierarchical structures which are implicit and otherwise unexpressed in a knowledge graph. One concrete way this task is formulated is as that of learning subsumption axioms for classes in a knowledge graph, thereby discovering a hierarchical organization of a knowledge graph's entities. To this end, Statistical Schema Induction [67] uses association rule mining on a knowledge graph's transaction table to generate subsumption axioms with support and confidence values which are then used as the basis for a greedy algorithm for constructing an ontology. SMICT [49] transforms a knowledge graph into a tuple structure wherein entities are annotated by tags and applies a greedy algorithm to learn a taxonomy of classes. This method was extended to perform hierarchical clustering using the Jaccard coefficient [51]. In general, by transforming a knowledge graph to a tuple structure, various [24, 61, 71] methods in the area of tag hierarchy induction can be leveraged. In a related approach, Chen and Reformat [15] derive a similarity matrix from a knowledge graph's tuple structure which serves as the clustering metric for hierarchical agglomerative clustering. Mohamed [41] takes a similar approach wherein subjects which are described by the same tag pairs are assigned to the same groups. The similarity between these groups is then calculated to construct a hierarchy. In a method which bears similarity to our own, Zhang et al. [76] use a non-parametric Bayesian approach to induce a hierarchy of topic communities. Despite a similar statistic framework and inference scheme, the hierarchy induced by this work differs significantly from our own. For instance, relations between communities are not modelled and entities are never explicitly assigned to communities. Along similar lines is GMMSchema [12] which uses a Gaussian mixture model to generate a schema graph which can be viewed as a hierarchical abstraction of the original knowledge graph.\nAnother common approach to learning hierarchies from knowledge graphs is via an intermediate representation which lends itself well to existing hierarchy induction methods. To this end, knowledge graph embedding is oftentimes leveraged. This process involves learning a mapping from the discrete knowledge graph to a continuous vector space. The vector representation may then serve as the input to machine and deep learning methods for hierarchy learning. Translation based methods such as the seminal TransE [13] and its extensions [73, 36, 31] treat relations in a knowledge graph as translations between entities. Additive in nature, they operate on the intuition that embeddings of subjects and objects should be proximal when translated by the relation of a valid triple. These embeddings are learned by minimizing an objective function using an optimization method such as stochastic gradient descent. Bilinear methods [43, 75, 32, 6] operate on the binary adjacency tensor of the knowledge graph and factorize entities and relations into vectors and matrices. Triples are then modelled as their resulting product. These methods tend to perform well on"}, {"title": "3 Preliminaries", "content": "Before describing the details of our proposed model, we provide a basic overview of several concepts necessary for its understanding. These concepts are described only insofar as to provide readers with the foundation on which the explanation of our model can be built. We implore readers unfamiliar with knowledge graphs or Bayesian nonparametrics to follow the relevant citations provided in each of the subsequent subsections. To aid in readability we use the following conventions in our notation: lowercase italic Latin letters for iterators and indexers; uppercase italic Latin letters for scalar variables; lowercase boldface Latin letters for vectors; uppercase boldface Latin letters for matrices and tensors; uppercase stylized Latin letters for sets; lowercase Greek letters for hyperparameters; and uppercase Greek letters for functions."}, {"title": "3.1 Knowledge Graphs", "content": "We refer to Hogan et al. [26] for their definition of knowledge graphs as \"a graph of data intended to accumulate and convey knowledge of the real world, whose nodes represent entities of interest and whose edges represent potentially different relations between these entities.\u201d Concretely, information is stored as a collection of triples wherein each triple relates a subject entity, $e_i$, to an object entity, $e_j$, via a predicate, $r_p$. Formally, we define a knowledge graph, G, as a set such that $G = \\{\\langle e_i, r_p, e_j \\rangle \\in \\mathcal{E} \\times \\mathcal{R} \\times \\mathcal{E}\\}$ where $\\langle e_i, r_p, e_j \\rangle$ is a triple, $\\mathcal{E}$ is the set of entities in G, and $\\mathcal{R}$ is the set of predicates in G. When put together, the triples form a directed graph with nodes corresponding to entities and edges corresponding to predicates. Each triple in a knowledge graph describes one piece of information or fact. For instance, (Henry Ford, occupation, Engineer) relates the subject Henry Ford to the object Engineer through the predicate occupation and states, in plain English, that Henry Ford's occupation is an engineer. Notice that this definition of knowledge graphs allows for cycles and entity self-relations to exist. This is made clear when analyzing a knowledge graph's binary adjacency tensor which may be asymmetric and containing non-zero values in its main diagonal. Knowledge graphs are oftentimes represented in their tensor form as it allows for easier numerical operation and thus opens the door to various tools and methods in artificial intelligence. A binary adjacency tensor is obtained from a knowledge graph by ordering its entities and predicates along an $|\\mathcal{E}| \\times |\\mathcal{E}| \\times |\\mathcal{R}|$ tensor, G, that takes on values $g_{ijr} = 1$ if there exists a triple in G from entity $e_i$ to entity $e_j$ on predicate $r_p$ and $g_{ijr} = 0$ otherwise. This representation is used in stochastic blockmodelling and is the one we will use in this paper henceforth. The left half of Figure 1 depicts a simple knowledge graph along with its adjacency tensor representation. A comprehensive introduction to knowledge graphs is provided by Gutierrez and Sequeda [23]."}, {"title": "3.2 Stochastic Blockmodels", "content": "Stochastic blockmodels are a heterogeneous collection of generative models united in their adoption of two charac- teristics: stochasticity in the generative process and the partitioning of nodes into communities. Describing them by referring to a concrete instance is thus bound to include definitions which do not apply to all members of the class. With this in mind, our introduction to stochastic blockmodels draws on their key characteristics to motivate a toy stochastic blockmodel for generating a knowledge graph. All stochastic blockmodels are defined by a set of probability"}, {"title": "3.3 The Chinese Restaurant Process", "content": "The Chinese restaurant process (CRP) [3] is a discrete stochastic process that yields a probability distribution in accordance with the preferential attachment principle. In this view, it is both a Dirichlet process [21] as it generates a probability distribution and a preferential attachment process [7] as the distribution is generated such that probabilities are proportional to past draws. The process is explained through a metaphor of sitting patrons at a Chinese restaurant. Consider this restaurant as containing an infinite number of tables with each table having the capacity to seat an infinite number of patrons. Patrons are seated sequentially, such that the first patron is seated at the first table and every subsequent patron may be seated at an occupied table or the first unoccupied table. The probability of being seated at an occupied table is proportional to the number of patrons already seated at it. This process is illustrated through the toy example in Figure 2 which shows a potential state of the CRP after sitting six patrons along with the sample probabilities of sitting the seventh. Formally, the probability of seating patron $e_i$ at a table $t_m$ in a restaurant where $T_i$ is the set of occupied tables when patron $e_i$ arrives is:\n\n$P(e_i = t_m | C_0, C_1, ..., C_{i-1}, \\gamma) = \\begin{cases}  \\frac{\\#t_m}{i + \\gamma} & t_m \\in T_i \\\\  \\frac{\\gamma}{i + \\gamma} & t_m \\notin T_i  \\end{cases}$  (4)\nHere, $\\#t_m$ is the number of patrons seated at table $t_m$ when patron $e_i$ arrives and $\\gamma > 0$ is a hyperparameter of the CRP responsible for controlling the probability that an incoming patron is seated at an unoccupied table such that increasing $\\gamma$ increases this probability. Thus, increasing $\\gamma$ values will yield results with an increasing number of occupied tables. Specifically, the expected number of occupied tables grows logarithmically with respect to the number of seated patrons:\n$\\sum_{t_m \\in T_i} E[\\mathbb{I}(\\#t_m > 0) | \\gamma] = O(log i)$  (5)\nWhere $\\mathbb{I}$ is the indicator function which returns 1 if the condition is met and 0 otherwise. Big-O notation is leveraged with O to indicate the asymptotic upper bound of the expectation. This principle becomes relevant when controlling for the branching factor of the induced tree as we will see later on. The realization of the CRP yields a partition of patrons over the infinitely many tables in the restaurant. If we consider each table to be a community, we can leverage this process to obtain a probability distribution over an infinite number of communities. Indeed this is the main utility of the CRP, namely to serve as a conjugate prior to infinite non-parametric discrete distributions. While this approach allows for the modelling of flat communities, it does not account for hierarchical relations between them. To remedy this, the CRP must be extended to its nested variant."}, {"title": "3.4 The Nested Chinese Restaurant Process", "content": "The nested Chinese restaurant process (nCRP) [22, 10] is an extension of the CRP formulated to account for hierarchical relations between the generated communities. The realization of this process is an infinitely deep and infinitely branching tree of communities defined by a set of paths, $\\mathcal{P}$, taken from the root community to a leaf community. In principle, the tree is unbounded in depth, however, we limit our discussion to a nCRP bounded to a depth of L. As in the case of the CRP, the allocation of paths along the tree is consistent with the preferential attachment principle. The tree is generated stochastically by sampling a path at each level in the tree via the CRP such that drawing a table is analogous to taking a path at that level. To extend the metaphor of seating patrons at a Chinese restaurant, consider the scenario of an infinite number of restaurants with an infinite number of infinite seat tables. When patrons are seated at these restaurants they are not served food but rather a table specific reference to another restaurant to which they must go. One of these restaurants is designated a root restaurant with no reference and all other restaurants are referenced exactly once. The seating of patrons at these restaurants is performed as in the CRP. We can see how realizing this process yields a tree by examining the paths taken by patrons. They first arrive at the root restaurant before being sent off to one of the root restaurant's descendant restaurants. At this restaurant, the patron is sent off to another descendant restaurant and this process is repeated until L restaurants have been visited in the bounded case. The paths taken by patrons generate the tree as illustrated in the toy example in Figure 3. As before, we extend this analogy of patrons and tables to entities and communities, respectively. Thus, when drawing path $p_i$ for entity $e_i$, the process starts by initializing the path at the top level to the root community, namely $p^0_i = t_0$ where the superscript in path $p_i$ indexes into the path vector to obtain the community at the corresponding level and $t_0$ is the root community. The process then continues by drawing a descendant community according to the CRP. Recall that this draw results in a community which either has or hasn't been visited before by a previous entity. The latter case corresponds to branching out a new path in the tree at the descendant level. This process is repeated L times at which point the specified depth has been reached. We can formalize this process by extending the previously defined notation. Specifically, let $T_i$ be the set of communities in the tree before entity $e_i$ has its path sampled and $C^l_q$ be the set of children communities for community $t_q$ at this time as well. The sampling process is then expressed as follows: when entity $e_i$ arrives at community $t_q$ on the $(l - 1)$th level in the tree, the probability of selecting an existing community, $p \\in C^l_q$, or creating a new community, $p \\notin T_i$, is:\n\n$P(p^l_i = t_c | P_0, P_1, ..., P_{i-1}, P^{l-1}_i = t_q, \\gamma) = \\begin{cases}  \\frac{\\#^{C^l_q}_{t_c}}{#^{C^l_q}_{t_q} + \\gamma} & t_c \\in C^l_q \\\\  \\frac{\\gamma}{#^{C^l_q}_{t_q} + \\gamma} & t_c \\notin T_i  \\end{cases}$ (6)\nWhere $\\#^{C^l_q}_{t_c}$ and $\\#^{C^l_q}_{t_q}$ is the number of entities that have passed through communities $t_c$ and $t_q$ before entity $e_i$ started its path. The superscript in $p^{l-1}_i$ indicates that the probability distribution for sampling $p^l_i$ is conditioned on the path taken by entity $e_i$ up until level l. The hyperparameter $\\gamma$ serves a similar function as in the CRP, namely controlling the branching factor of the tree such that higher $\\gamma$ values yield trees with more branches. The use of the CRP in the path decision process ensures that probability mass will be pulled towards drawing paths which have been more frequently drawn before. The resulting distribution allows us to use the nCRP as a non-parametric prior over a tree structure in our model. In drawing paths, we not only generate a hierarchy but also define a subset of communities to which an entity can belong to, namely those along the path. This highlights an important difference between the CRP and the nCRP. While the CRP is sufficient for drawing a community for an entity, the nCRP must be used alongside another stochastic process to determine the level along the path that the entity belongs to. This provides a segue to one such process, specifically the stick breaking process."}, {"title": "3.5 The Stick Breaking Process", "content": "The stick breaking process [62] is \u2013 like the CRP and nCRP \u2013 a Dirichlet process that draws its name from a metaphor which describes it. The metaphor starts by breaking a stick of unit length into two fragments at a point in the interval from 0 to 1 as drawn from the Beta distribution. One of the two fragments is preserved and the other fragment is broken again, analogously to the initial stick. This process is repeated an infinite number of times to yield an infinite number of fragments whose combined length is that of the initial stick. These fragments may be viewed as a probability distribution over the infinite sequence of discrete time-steps used to generate them. In other words, the stick breaking process is an infinite extension of the Dirichlet distribution insofar as while the Dirichlet distribution yields a probability distribution over L categories, the stick breaking process yields a probability distribution over an infinite number of categories. Formally, let the draw from the Beta distribution at the $l$th iteration of the stick breaking process be denoted as $v^l \\sim Beta(\\mu\\sigma, (1 - \\mu)\\sigma)$. Thus, the lengths of the first fragment, denoted $\\alpha^1$, and its remainder are $v^1$ and $1 - v^1$, respectively. To obtain the length of the second fragment, $\\alpha^2$, draw $v^1$ and break off that fragment from what remains of the stick, namely $\\alpha^2 = v^1(1 - v^1)$. We define this process for an arbitrary $l$th time-step as follows:\n$\\alpha^l = v^l \\prod_{k=1}^{l-1} (1 - v^k)$  (7)\nThe stick breaking process is a generalization of the Griffiths-Engen-McCloskey distribution [10, 52] which may be seen as a special case where $\\mu\\sigma = 1$. The hyperparameters, $1 > \\mu > 0$ and $\\sigma > 0$, control the mean and variance of the distribution, respectively. Specifically, increasing $\\mu$ values will pull the mean towards fragments broken later in the process and increasing $\\sigma$ values will increase the variance of the distribution. The resulting distribution can be used in conjunction with the nCRP to obtain a community for an entity given its sampled path. This is because by sampling the stick breaking distribution an index is obtained which can correspond to the level on the path that the entity belongs to. This motivates the use of the stick breaking process in our model. Namely, we use the stick breaking process as a prior over the levels in the induced hierarchy. We explain this in detail in the subsequent section."}, {"title": "4 Proposed Model", "content": "In describing our proposed model, we will adopt the notations used in the previous section to indicate the connection with the ideas discussed in the preliminaries. To aid in understanding, we first provide a summary of the components of our model before defining the generative process. This is followed by a formalization of the Gibbs sampling procedure and derivation of sampling equations."}, {"title": "4.1 Model Description", "content": "Like all stochastic blockmodels, our model is defined as a set of probability distributions such that when these distributions are sampled from, they generate the adjacency tensor of the knowledge graph. The choice of these distributions makes assumptions about the underlying structure that governs the graph's interactions. In devising our model, we assume a hierarchy of entity communities which are captured in the form of a tree. The entities in these communities interact with one another as a function of their membership to a community. In other words, interactions are modelled at the community level and extended downwards to their constituent entities. Unlike most stochastic blockmodels, these community relations are modelled with respect to a predicate in the knowledge graph. This allows the model to capture structures extending beyond those implied by mere interaction density. Thus, in order to generate the knowledge graph's adjacency tensor, we need to know its hierarchical community structure, its entities' memberships to communities, and the interactions between its communities. The induction of these components, which may be seen"}, {"title": "4.1.1 Community Memberships", "content": "Entities are assigned to communities through the conjunction of two variables: entity paths and level indicators. Paths define the tree structure over the community hierarchy by sampling from the nCRP as described in the previous section. We thus denote an entity path as $p_i$ for entity $e_i$, such that $p_i := [p^1_i, p^2_i, ...,p^L_i]$ where $p^l_i$ represents the community at level l. We draw attention to the fact that this definition omits the root community from the path, namely $p^0_i$, since all entities must pass through it. It also allows a hierarchy with a depth of L to have entity path vectors of dimension L, simplifying the notation. Entity paths are drawn from the nCRP, denoted as $p_i \\sim nCRP(\\gamma)$. Thus, all the entity paths sampled in the model form a $|\\mathcal{E}| \\times L$ matrix which we denote as P. $\\gamma$ is the aforementioned hyperparameter of the nCRP and is responsible for controlling the probability of generating a new branch in the hierarchy as the path is being sampled. When a new branch is generated at level l such that l > L, L \u2013 l new communities are also generated and populated solely by the sampling entity. Furthermore, if a path is resampled such that its corresponding entity obtains a new path which leaves behind empty communities, those empty communities and removed from the hierarchy. As such, the number of communities in the hierarchy is subject to constant change throughout the sampling process. Having sampled entity paths, in order for entities to be assigned to communities, their levels must be obtained. Entity levels are modelled by two variables in our approach: level memberships and level indicators. Level memberships, denoted $a_i$ for entity $e_i$, capture the probability of the entity's belonging to each of the L levels. As such, all the level memberships in our model form a $|\\mathcal{E}| \\times L$ matrix, A. This is similar to the mixed-membership property of the Mixed Membership Stochastic Blockmodel wherein an entity has a membership distribution over all communities. The difference, as pointed out by Ho et al. [25], is that in hierarchical models this distribution is restricted to communities along the entity's sampled path, otherwise the process of obtaining paths, and indeed the hierarchy itself, would lose its meaning. Level memberships are drawn from the stick breaking process, $a_i \\sim Stick(\\mu, \\sigma)$ with hyperparameters $\\mu$ and $\\sigma$. Recall that this process yields an infinite distribution and must therefore be truncated to a dimension of L to correspond with the depth of the tree. The truncation is performed by removing all probabilities at levels greater than L and renormalizing. The distribution captured by an entity's level membership is used to sample its level indicator. The level indicator indicates the level to which an entity belongs and thus, in conjunction with its path, assigns it to a community. Level indicators are drawn in the context of an interaction between two entities. Specifically, when modelling the probability of an interaction from entity $e_i$ to entity $e_j$ we draw two level indicators, one for the sender entity and one for the receiver entity denoted as $z_{i\\rightarrow j}$ and $z_{i\\leftarrow j}$, respectively. The sender and receiver level indicators correspond to the levels of entities $e_i$ and $e_j$ in the context of their pairwise interaction. Thus, our model samples $|\\mathcal{E}|^2$ sender and receiver level indicators each leading to two $|\\mathcal{E}| \\times |\\mathcal{E}|$ matrices $Z_{\\rightarrow}$ and $Z_{\\leftarrow}$ for all the senders and receivers,"}, {"title": "4.1.2 Community Relations", "content": "Community relations describe the degree to which entities in any two communities are likely to interact with one another through a specific predicate. In other words, they model the probability of observing a value of one in the knowledge graph's adjacency tensor. These interactions are captured by a $\\mathcal{T}\\times \\mathcal{T} \\times \\mathcal{R}$ tensor, denoted C, where $\\mathcal{T}$ is the set of all communities in the hierarchy. We note that because the communities in $\\mathcal{T}$ are a result of sampling from the nCRP and are thus subject to change with each successive sample, the dimensionality of C is also subject to change in the sampling process. This presents a challenge to our sampling scheme since it is possible to sample communities via the nCRP for which there are no community relation values. We overcome this issue through the marginalization of community relations as discussed in the subsequent subsection. The community relation $C_{pqr}$ is an entry in C and captures the probability of interaction between entities in community $t_p$ with entities in community $t_q$ through predicate $r_r$. As such, the value of $C_{pqr}$ is bounded to $1 \\geq C_{pqr} \\geq 0$. In order to preserve the hierarchical structure that was induced by sampling paths and levels, the community relations must be limited to take on non-zero values only when interacting with communities which are proximal to them in the hierarchy. This restriction is vital as allowing for interaction between any two communities in the hierarchy would render it meaningless and our model would be reduced to a fixed size mixed membership stochastic blockmodel such as the ones described in Section 2.\nIn restricting the values of community relations we take an approach similar to that of the Multiscale Community Blockmodel. Specifically, we borrow the concept of a sibling group which refers to a set of communities that share the same parent in the hierarchy. Only the community relations between communities in the same sibling group are modelled in our approach. Thus, when obtaining the interaction degree of two entities whose communities have the same parent, it's sufficient to merely access the corresponding value in C. When their communities do not share the same parent, a coarsening procedure is applied to obtain an interaction degree. The coarsening procedure traverses the paths of the two entities to find the deepest pair of communities which are in the same sibling group. Formally, to obtain"}, {"title": "4.1.3 Generative Process", "content": "The generative process of our model refers to the sequential sampling of components which allows for the generation of the target knowledge graph. In other words, the goal is to draw a binary value for each $g_{ijr} \\in G$ such that it equals the knowledge graph's adjacency tensor. But before this can be done, it's necessary to sample the variables it is dependent on. The first components sampled in the generative process are the paths and level memberships for each entity in the knowledge graph from the nCRP and stick distributions, respectively. Having drawn the paths, we now have the set of communities in the hierarchy and can draw community relations from the Beta distribution. At this point in the generative process, entities are not yet assigned to communities. The community memberships for these entities have been drawn, however, allowing for the sampling of community levels for each pair of entities in the knowledge graph from the multinomial distribution. With the community levels drawn, all the components for generating the knowledge graph are in place. The binary value for the interaction from entity $e_i$ to entity $e_j$ on predicate $r_r$ is drawn from the Bernoulli distribution using each entity's respective community's interactions, namely $g_{ijr} \\sim Bernoulli(\\Psi(i, j, r))$. The plate diagram for this process is illustrated in Figure 7 and the formal definition is as follows:\n\u2022 For each entity in the knowledge graph; $e_i \\in \\mathcal{E}$\n$p_i \\sim nCRP(\\gamma)$\n$a_i \\sim Stick(\\mu, \\sigma)$\n\u2022 For each sender community in the hierarchy; $t_p \\in \\mathcal{T}$\nFor each receiver community in the hierarchy; $t_q \\in \\mathcal{T}$\n* For each predicate in the knowledge graph; $r_r \\in \\mathcal{R}$\n$C_{pqr} \\sim Beta(\\lambda, \\eta)$\n\u2022 For each sender entity in the knowledge graph; $e_i \\in \\mathcal{E}$\nFor each receiver entity in the knowledge graph; $e_j \\in \\mathcal{E}$\n* $Z_{ij} \\sim Multinomial(a_i)$\n* $Z_{ij} \\sim Multinomial(a_j)$\n* For each predicate in the knowledge graph; $r_r \\in \\mathcal{R}$\n$g_{ijr} \\sim Bernoulli(\\Psi(i, j, r))$\nWe note that this process is unsupervised and does not impose any assumptions about the partition of entities to communities or the structure of the hierarchy other than to limit its depth. In fact, the depth is the only constraint imposed on the generative process. The other hyperparameters which must be specified a priori \u2013 namely $\\gamma, \\mu, \\sigma, \\lambda$, and $\\eta$ \u2013 merely influence the prior distributions of our model. They may pull the latent variables in the assumed direction but only insofar as the data allows it. This, recall, is due to the sampling of latent variables from their posterior distribution which is conditioned on the data. As a result, with a strong enough likelihood, the effects of the hyperparameters and the prior relatively diminish. As with most stochastic blockmodels, the exact inference for our model is intractable and must be approximated using an inference scheme. For this we adopt collapsed Gibbs sampling, an extension of the aforementioned Gibbs sampling."}, {"title": "4.2 Collapsed Gibbs Sampling", "content": "Collapsed Gibbs sampling refers to an extension of Gibbs sampling in which a subset of model variables are marginalized over and therefore do not need to be sampled directly. These variables are said to be collapsed out of the Gibbs sampler. Collapsing of these variables is done analytically via integration and ensures a faster mixing process. This is because the calculation of probability distributions for sampling is generally computationally expensive. Having fewer variables then leads to a faster arrival at the desired stationary distribution. Furthermore, the calculation of probability distributions which have not been collapsed out of the sampling process is generally faster in collapsed Gibbs sampling. This is because in regular Gibbs sampling draws are made from the full conditionals of variables. In collapsed Gibbs sampling, collapsed variables have been integrated out of the process and the remaining variables are conditioned on a lower-dimensional space. Collapsing of variables is usually tractable when they are the conjugate prior of their dependant variables. In our model, community relations and level memberships are both conjugate priors of their dependant variables, namely level indicators and entity relations, respectively. We leverage these conjugacies to marginalize over these two variables in our sampling process. After marginalization, the sampling equations may be derived for the remaining variables."}, {"title": "4.2.1 Marginalizing Community Relations", "content": "In order to marginalize out community relations, it is necessary to find a closed form solution which allows for integration during path sampling. To this end, we can leverage the Bernoulli-Beta conjugacy which ensures that given a Bernoulli likelihood and Beta prior, the posterior will also be drawn from the Beta distribution. Employing this conjugacy is possible due to the formulation of our model in which entity relations are drawn from the Bernoulli distribution and community relations assume a Beta prior. We see this explicitly when applying Bayes' theorem to obtain the posterior as follows:\n$P(C_{pqr}|C_{-(pqr)}, G, P, Z, \\lambda, \\eta) = \\frac{P(G | C, P, Z, \\lambda, \\eta)P(C_{pqr} | C_{-(pqr)}, \\lambda, \\eta)}{\\int_{C_{pqr}}P(G | C, P, Z, \\lambda, \\eta)P(C_{pqr} | C_{-(pqr)}, \\lambda, \\eta) dC_{pqr}}$ (11)\nWhere $P(G | C, P, Z, \\lambda, \\eta)$ is the likelihood of generating entity relations and $P(C_{pqr} | C_{-(pqr)}, \\lambda, \\eta)$ is the prior placed on community relations. $C_{-(pqr)}$ indicates the community relations tensor C without $C_{pqr}$. Before proceeding we introduce helper variables $\\#C_{pqr=1}$ and $\\#C_{pqr=0}$ to indicate the number of existing and non-existing interactions between entities from community $t_p$ to community $t_q$ on predicate $r_r$, respectively:\n$\\#C_{pqr=1} = \\{g_{xyz} \\in G : \\Psi(x, y, z) = C_{pqr} \\, g_{xyz} = 1\\}|\n$\\#C_{pqr=0} = \\{g_{xyz} \\in G : \\Psi(x, y, z) = C_{pqr} \\, g_{xyz} = 0\\}|$  (12)"}, {"title": "4.2.2 Marginalizing Level Memberships", "content": "There are two ways in which to approach marginalizing level memberships in our model. Firstly", "62": "showed that the realization of the stick breaking process follows the Dirichlet distribution. We can leverage this because", "25": "this prior has the disadvantage of either being too expressive or not expressive enough depending on its parameterization. Regardless", "follows": "n\n$P(a_i | A_{-i}, Z, \\mu, \\sigma) = \\frac{P(Z | A, \\mu, \\sigma)P(a_i | A_{-i}, \\mu, \\sigma)}{\\int_{a_i} P(Z | A, \\mu, \\sigma)P(a_i | A_{-i}, \\mu, \\sigma) da_i} = \\frac"}]}