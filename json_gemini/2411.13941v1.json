{"title": "LLMs as Continuous Learners: Improving the Reproduction of Defective Code in Software Issues", "authors": ["YALAN LIN", "YINGWEI MA", "RONGYU CAO", "BINHUA LI", "FEI HUANG", "XIAODONG GU", "YONGBIN LI"], "abstract": "Reproducing buggy code is the first and crucially important step in issue resolving, as it aids in identifying the underlying problems and validating that generated patches resolve the problem. While numerous approaches have been proposed for this task, they primarily address common, widespread errors and struggle to adapt to unique, evolving errors specific to individual code repositories. To fill this gap, we propose EvoCoder, a multi-agent continuous learning framework for issue code reproduction. EvoCoder adopts a reflection mechanism that allows the LLM to continuously learn from previously resolved problems and dynamically refine its strategies to new emerging challenges. To prevent experience bloating, EvoCoder introduces a novel hierarchical experience pool that enables the model to adaptively update common and repo-specific experiences. Our experimental results show a 20% improvement in issue reproduction rates over existing SOTA methods. Furthermore, integrating our reproduction mechanism significantly boosts the overall accuracy of the existing issue-resolving pipeline.", "sections": [{"title": "1 Introduction", "content": "Issue resolving is a fundamental aspect of software development and maintenance, critical to preserving the quality and stability of software systems [26, 41, 49, 53]. Throughout a project's life-cycle, various issues inevitably arise. Automating the resolving of these issues not only accelerates error identification and correction but also boosts development efficiency, playing a pivotal role in sustaining the long-term health of the project [16, 28, 40, 50]. This process is especially vital in the early stages of development, where test cases are often incomplete, requiring continuous refinement as new bugs are reported by users [17].\nIn issue resolving, the initial and critical step is issue reproduction which involves automatically generating code to reproduce the reported problem based on the issue description. Specifically, given an issue from a code repository, which contains key steps to reproduce the problem (as shown in Figure 1), the goal is to generate executable code to replicate the issue. The output of this generated code should match the \u201cActual Result\" specified in the issue while applying an appropriate patch should modify the code's output to reflect the \u201cExpected Result\u201d. Successful issue reproduction not only accelerates the process of problem localization and resolving but also strengthens quality assurance processes in continuous integration and delivery, thereby improving the robustness and reliability of software systems.\nPrevious studies have extensively explored the role of reproducing code [8, 44, 50]. Some research integrates this process into code intelligent agents (e.g., CodeR [8] and SWE-Agent [44]). These methodologies design computer interfaces for viewing, searching, and editing files, thereby enabling"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Issue Resolving", "content": "Issue resolving is a critical component of software development and maintenance, encompassing the identification, diagnosis, and resolving of software defects or issues reported by users or developers. In this task, based on the provided problem descriptions and snapshots of the codebase, the model needs to autonomously identify the specific locations that need modification [14, 29, 38, 50]. This can be achieved through commands to search for certain files or by analyzing execution paths using existing passing and failing test cases to pinpoint the most likely points of failure. Once these locations are identified, the model generates the corresponding code snippets to address the issues [25, 39, 46, 52]. During the generation process, the model can also perform debugging and iterative revisions on its own [10, 12, 31, 35, 43]. For instance, syntax checkers can identify syntactical errors, and test cases or code snippets that reproduce the issue can be executed to verify whether the generated patches resolve the original errors. Any error messages or feedback from these processes can be fed back into the model to further refine and improve the generated code. In the final testing phase, the generated code patches must pass both the unit tests associated with the current modifications\u2014extracted from the pull requests submitted by programmers-and the existing unit tests. This ensures that the new changes do not adversely affect previously implemented functionalities, the generated code patches must pass both the unit tests associated with the current modifications\u2014extracted from the pull requests submitted by programmers\u2014and the existing unit tests. This ensures that the new changes do not adversely affect previously implemented functionalities."}, {"title": "2.2 Issue Code Reproduction", "content": "Issue Reproduction involves the creation of executable code designed to reproduce the issues reported by users. Historically, this challenge has been framed as a single-step code generation task, wherein the model processes the issue description provided by the user and outputs the correspond-ing code snippet intended to recreate the reported problem [17]. However, with advancements in LLMs, there has been a shift towards models capable of engaging in multi-turn dialogues. This evolution allows the models to not only generate initial code but also to interact with a simulated environment for debugging purposes. Prominent instances of this advanced approach can be found in systems like SWE-Agent [44] and CodeR [8], both of which utilize agent-based methodologies. In these frameworks, the interaction begins with the model receiving a system prompt that outlines the range of actions it can undertake, including edit, search, and other relevant tasks. Subsequently, the user inputs the specifics of the issue they wish to address. Operating in a ReAct paradigm [45], the model proceeds to analyze the current state, contemplate potential steps, and execute the most suitable action. This typically involves generating the necessary files, modifying them to integrate"}, {"title": "3 Motivation", "content": "In this section, we motivate our approach by analyzing the failure cases of CodeR [8], the state-of-the-art method for issue reproduction. We run CodeR on SWE-bench-lite [6] and extract reproduction code generated by CodeR from its resolving trajectories. Upon running the generated issue code, we manually inspect the execution output and assess its correctness based on the following criteria:\n(1) Completeness: The reproduced code must contain the core or complete code provided by\nthe users in the original issue.\n(2) Consistency: (i) The error messages upon running the code should be consistent with the\nprovided full error messages. If the input issue involves adding a new feature, the output\nshould contain the new feature. (ii) Execution commands involved in the code must match\nthose described in the problem statement exactly.\n(3) Authenticity: Error messages must be derived from actual runtime results, instead of mocked\noutputs.\nUltimately, we collect a total of 84 erroneous codes generated by CodeR. Through manual examination, we identified seven types of errors that can be categorized into two groups:\nInternal errors (58.55%) refers to intrinsic errors that arise from the reproduction code itself, including 1) Wrong Reproduction Targets (4.00%): The execution results from the reproduced code do not align with the expected error message described in the original issue. This is likely due to the model's difficulty in fully interpreting the natural language within the issue description-such as distinguishing between error outputs and correct outputs. 2) Wrong Function Call (10.97%): the reproduced code calls wrong functions or commands compared to the intended issue code. 3) Over-mocking (14.63%): Mocking is a commonly used technique in software testing that isolates test scripts from external dependencies by replacing them with mocked functions. However, when the behavior of these dependencies is overly predefined, the mocked functions may not accurately reflect the actual behavior of the original code. For instance, using print statements to output error messages instead of triggering real errors can cause tests to pass even if underlying defects exist. This limits the tests' effectiveness in detecting issues, especially when the true behavior of dependencies changes. 4) Missing Environment Requirements (6.10%): The reproduction only contains the core code while missing the environmental setup. 5) Inaccurate Execution Results (23.17%): The reproduced code contains logical errors during the reproduction process, causing inconsistent results to those described in the issue. This is probably due to the restriction of model capability and the task complexity.\nExternal errors (41.45%) refers to the errors from external environmnents or executions, including 1) Incorrect commands to run the code (14.63%): Some issues require specific commands for reproduction, which are usually mentioned in the issue description or pertain to particular usages of the repository. However, the previous methods uniformly used Python commands to run the code, which could result in the inability to reproduce certain problems and 2) Wrong environment setting (26.82%): Given that reproducing these issues often requires specific versions of libraries, particular operating systems, and interactive environments (such as Jupyter), it may happen that"}, {"title": "4 Metohod", "content": ""}, {"title": "4.1 Overview", "content": "In this paper, we propose a continuous learning pipeline that enables LLM agents to accumulate experience from previously encountered issues. Unlike conventional methods, our approach does not require fine-tuning; instead, it facilitates the model's ability to continuously update and optimize its stored experiences when addressing new challenges. The overall methodology is structured into three main components: (i) an Actor LM which reproduces the issue using instructions and previous experiences; (ii) a Reflection LM which extracts experience from the Actor's reproduction trajectories; and (iii) a hierarchical Experience Pool which stores general and repository-specific experiences, enabling the Reflection LM to continuously update and refine its accumulated knowledge."}, {"title": "4.2 Actor: Reproduction Trial", "content": "The Actor is an LLM prompted to perform issue reproduction. Adopted from SWE-agent [44] and CodeR [8], we formulate issue reproduction as a multi-turn conversation between agent and computer. In each turn, the Actor is presented with a task instruction, the current system's output, and experiences from past resolving. It is asked to output a \u201cThought\u201d about the current reproducing step, followed by an \u201cAction\u201d like searching, viewing, and editing files. Once the reproduction \u201cAction\u201d has been executed, the system responds with the new output, which is fed back to the Actor as input for the next interaction. The dialogue continues, yielding a trajectory of Thought-Action-Response sequences. To mitigate problems caused by improper environment configuration and instruction misunderstanding, we also introduce a standardized process, which asks the model to write instructions to run the code or install related packages in a sh script. The detailed prompt for the Actor is as follows:"}, {"title": "4.3 Reflection LM: Extracting Experience from Reproduction Trajectories", "content": "Having collected the trajectories, we distill them into experiences, guiding further issue repro-duction. An experience is defined as a rule guiding what the Agent must do to avoid a certain reproduction failure or follow unique coding styles that certain code repositories may have. While previous work [23, 37] proposed using experience pools for knowledge accumulation, their ap-proach faces two critical challenges: (1) experience bloat, where experiences become increasingly verbose and detailed over time, hindering accurate issue pattern matching and experience utlizling, and (2) experience rigidity, where experiences become static and fail to adapt to emerging issue types and repository-specific error patterns.\nTo enable effective experience refinement, we design an LLM-based reflection mechanism that analyzes task trajectories and golden test patches to continuously update the experience pool. Unlike previous approaches that simply accumulate experiences, our method actively manages experience quality through carefully designed prompts that instruct the reflection LLM to: (1) Analyze Trial. Examines the reproduction trajectory and compares it with the golden test patch to identify"}, {"title": "4.4 Hierarchical Experience Pool", "content": "Based on the findings in Section 3, some error patterns are consistent across repositories, while others are repository-specific. To better maintain and utilize the extracted experiences, we design a hierarchical experience pool structure: a general pool at the top level captures common experiences shared across all repositories, followed by a range of repository-specific pools that maintain experi-ences unique to individual repositories. The common experiences are visible and collaboratively maintained by all issues, whereas repository-specific experiences are only visible and maintained by issues related to the specific repository. This architecture enables the model to distinguish between common and repo-specific experiences more effectively, ensures that different types of experiences"}, {"title": "4.5 The Reproduction Process", "content": "Upon a new issue, the Actor LM selects both common and repo-specific experiences from the expe-rience pool. For each repository, at most 10 experiences can be selected. The selected experiences are ranked by their importance. Each experience has an initial importance of 2 and is increased by 1 if it is agreed upon. The selected experiences are incorporated into the prompt for resolving the current issue.\nThe Reflection LM summarizes experiences from the resolving history (i.e., trajectories of issue-resolving dialogues), determines the manipulation action, and updates the experience pool.\nThe reproduction process continues until the model concludes that it has successfully replicated the issue. To prevent the model from prematurely declaring the task complete, we incorporate"}, {"title": "5 Evaluation", "content": "In this section, we present a comprehensive evaluation to assess the effectiveness of our method. We address three primary research questions (RQs).\n\u2022 RQ1: How effective is EvoCoder in issue code reproduction? We conducted experiments\nto explore whether the continuous learning pipeline helps LLMs in generating reproduc-\ntion code. We further conduct an ablation study of each component, in particular, whether\nEvoCoder can better acquire general and repository-specific knowledge, as well as learning\nfrom previous debugging processes to fix errors."}, {"title": "5.1 RQ1: Effectiveness in Issue Code Reproduction", "content": ""}, {"title": "5.1.1 Dataset", "content": "We utilize the widely used SWE-bench benchmark [16], which is designed to test the capability to address practical software engineering challenges. For a fair comparison, we focus on a refined subset called SWE-bench Lite [6]. This subset comprises 300 instances from SWE-bench that have been sampled to be more self-contained and covers 11 out of the original 12 repositories, ensuring a comparable range and distribution of projects as the full dataset. Our goal is to have the model attempt to reproduce and solve the specific issues described in each dataset entry."}, {"title": "5.1.2 Metrics", "content": "We did not use automatic metrics because there is a lack of ground-truth answers for reproducing code in the selected dataset. In addition, other metrics, such as the failure of reproduced code when run on the original code, do not fully equate to successful reproduction. Therefore, we ask LLM and human to score the quality of the reproduced code, adhering to five criteria: 1) the reproduction precisely aligns with the issue description; 2) the code contains no syntax or logical errors; 3) the replication process must NOT involve any form of mocking, simulation, or re-implementation of core logic that substitutes real interactions when such interactions are necessary to reproduce the issue. 4) The reproduction code should correctly interact with the necessary systems or components to produce an authentic replication of the issue. 5) the execution result of the reproduction code captures and demonstrates the key aspect of the issue as described.\n1. LLM-as-a-judge scores. We leverage GPT-4 [3, 32] to judge the success of reproduced code. The input provided to the model includes the content of the issue, the reproduced code, and the execution result of the code. The LLM is asked to analyze whether the reproduced code meets the five criteria. Based on the results, the model concludes whether the code successfully reproduces the issue. The prompt used for the scoring is as follows:"}, {"title": "5.1.3 Baselines", "content": "We compared our method with three state-of-the-art approaches for issue code reproduction:\nSWE-agent [44]: a system that facilitates LM agents to autonomously use computers to solve\nsoftware engineering tasks, including issue resolving. SWE-agent provides an Agent-Computer\nInterface that includes actions such as opening and editing files and executing commands, allowing\nthe model to interact almost freely with the computer. In the paper, the system is employed to\naddress the full issue resolving process, with reproduction being one of its stages.\nCodeR [8]: an issue resolving approach built upon SWE-agent. The method adopts a multi-agent\narchitecture, designating the reproducer as a specialized agent. It also introduces format constraints\nfor the generated reproduction code.\nLIBRO [17]: an initial exploration into using large language models to accomplish the task of\nissue code reproduction. The primary method involves employing few-shot examples to prompt\nthe LLM to generate more effective reproduction code. To adapt this method to SWE-bench, we\nemployed a multi-round conversational approach similar to CodeR, and included two successful\nreproduction code snippets from the same library in the initial prompt to guide the model."}, {"title": "5.1.4 Results", "content": "As shown in Table 1, EvoCoder exhibits a significant improvement in accuracy, indicating that the continuous learning approach indeed enhances the model's performance on similar tasks. Although few-shot prompting can be somewhat effective, its impact is limited. One reason is that identifying the most relevant examples can be challenging and may not be solely based on character or semantic similarity. Another reason is that, without explicit guidance, the model may struggle to reflect and generalize useful insights from the provided examples for subsequent reproduction tasks."}, {"title": "5.1.5 Ablation Study", "content": "We also conducted an ablation study on our method. Specifically, we tested the removal of the action-based update mechanism, the exclusion of repository-specific experiences, and the elimination of general experiences. We found that all components are crucial for the final results. The action-based mode ensures that the model's experience is updated in a timely and flexible manner, preventing it from becoming overly bloated or rigid. Removing repo-specific experiences renders the learning process too generalized, hindering the acquisition of repository-specific knowledge. Conversely, removing general experiences isolates the experience pool within each repository, preventing the sharing of common knowledge, which can lead to a lack of experience when encountering a new repository with no existing experience."}, {"title": "5.2 RQ2: Effect on Issue Resolving", "content": "We investigate the impact of our issue reproduction method in the entire issue resolving pipeline. In other words, how EvoCoder enhances the overall process of identifying, diagnosing, and fixing defects, thereby improving the quality and maintainability of software."}, {"title": "5.2.1 Setup", "content": "We take bug fix as an instance of issue resolving, where the initially generated code fails to pass the test cases. To fix the bug, the model generates a patch, applies it to the reproduced code, and verifies whether the bug has been fixed. If the patch fails, the model iterates through debugging and patch regeneration, continuing this cycle until the code is corrected or a maximum of three attempts (as set in our experiments) are reached.\nWe apply EvoCoder to two state-of-the-art debugging methods, AutoCodeRover [50] and Agent-less [40], and compare the performance before and after applying EvoCoder. We measure the performance using the number of resolved issues and the rate of issue fixes. For the Agentless approach, we use the reproduction code to filter generated patches. We merge patches based on their occurrence frequency and select only those that produce different outputs before and after applying the patch, prioritizing the one with the highest frequency."}, {"title": "5.2.2 Result", "content": "As shown in Table 2, incorporating the reproduction code by EvoCoder has a signifi-cant impact on the model's accuracy. For example, by applying EvoCoder to AutoCodeRover, the number of resolved issues increases from 15 to 18, an approximation of 20% improvement. The results are consistent across two basic debuggers and metrics, suggesting the effect of EvoCoder in issue resolving."}, {"title": "5.3 RQ3: Error Type Transitions", "content": "To understand how EvoCoder facilitates issue resolving, we revisit the seven reproduction errors made by CodeR as discussed in Section 3. We perform a more in-depth analysis to examine how EvoCoder addresses or mitigates each of these errors."}, {"title": "6 Case Study", "content": "To further verify the effectiveness of EvoCoder in real-world scenarios, we analyze two cases from SWE-bench to demonstrate the experiences we have gathered and how these experiences assist in addressing subsequent issues. The results are shown in Figures 6.\nThe first example pertains to an issue with the \u2018pytest' library. The model merely included an assert statement within the 'test_reproduce()' function without performing any additional operations. The code is run by simply running the 'python' command when no prior experience exists. After incorporating the experience gained from previous tasks, the model did not explicitly call the defined functions within the reproduction script; instead, it allowed the 'pytest' command to automatically scan and check the tests. This approach is specific to 'pytest' and differs from the practices used with other libraries. Consequently, without specific experience with this library, the model would not be able to learn or adopt this method.\nThe second reproduction case involves a specific time formatting requirement within an error message. Our experiences specified that the model's output should align with the details provided in the issue, yet the model's output failed to capture the error message both before and after integrating these experiences. This situation highlights two key points: First, despite the incorpo-ration of prior experience, the model's inherent limitations may still prevent it from successfully reproducing the issue, underscoring the need for further enhancement of the model's natural language comprehension capabilities. Second, such challenges commonly occur when the issue description is limited to textual explanations without concrete elements, such as reproducible code, expected results, or detailed specifications. For large models to effectively assist with coding issues, it is crucial to provide them with clear, comprehensive input."}, {"title": "7 Limitations and Future Work", "content": "Despite achieving significant results, our work has certain limitations, pointing to potential research directions that warrant further exploration.\nFirst, the current research derives experiences directly from dialogue history, resulting in rela-tively generalized guidelines. Future work could benefit from a more granular approach-such as analyzing specific actions taken during the repair process-to generate more targeted and actionable recommendations.\nSecond, existing code generation [15, 27, 34, 42, 51] technologies lack comprehensive handling of boundary conditions in the way that unit tests do. As a result, even reproduced code may still exhibit issues. A promising direction for enhancement could involve integrating code generation with automated unit test generation. This combined approach could leverage the strengths of both methods while addressing their individual limitations, leading to more robust testing outcomes.\nWhile this study primarily addresses issue code reproduction, our method holds significant potential for broader application. In future research, our approach could be extended to a range of coding scenarios, such as code translation [13, 21, 33], code editing and refactoring [4, 7, 19, 36, 47, 48]. By collaboratively building and maintaining a comprehensive repository of best practices and insights for these tasks, we can improve development efficiency and promote knowledge sharing to better support the software development community."}, {"title": "8 Related Work", "content": ""}, {"title": "8.1 Repository-level Issue Resolving", "content": "With the introduction of Devin [11], the world's first AI programmer, researchers have begun to pursue the goal of enabling AI not only to assist in programming but also to independently complete software development and repair tasks [18, 22]. Towards this objective, SWE-bench [16]"}, {"title": "8.2 Issue Code Reproduction", "content": "Issue code reproduction is attracting increasing attention from researchers. LIBRO [17] was the first to propose using large models to accomplish the task of reproducing problematic code, prompting these models with a few examples to generate more effective reproduction code.\nWith the development of various agent strategies [11, 24, 28, 38, 40, 44, 50], it has been found that adopting a multi-round dialogue approach can more effectively activate the capabilities of models, thereby achieving better results. Many solutions addressing the full-chain issues of agents in SWE-Bench [16] have already incorporated this element [8, 44]. However, they have only added some fixed tips in a relatively simplistic manner, without implementing more refined designs.\nFurthermore, some research attempts to directly generate corresponding unit tests from issue reports [30]. The test functions produced by this method can integrate better into the overall content of the repository. However, the generation of unit tests may be a more challenging task, as it requires simultaneous consideration of code location and generation while ensuring that the addition of new tests does not disrupt the existing structure. Observations indicate that the accuracy of unit test generation is currently lower than that of the aforementioned code reproduction methods.\nIn this study, we further improve the success rate of code reproduction based on the Agent-Computer Interface provided by SWE-agent [44] and the mechanism of multi-round dialogues. Meanwhile, we view the automatic generation of unit tests as a more ambitious and challenging long-term goal, anticipating a transition towards this advanced form of reproduction when model capabilities significantly improve in the future."}, {"title": "9 Conclusion", "content": "In this paper, we propose a novel continuous learning framework for issue code reproduction. Our method maintains a hierarchical pool for both common and repo-specific experiences. By continuously resolving new issues, the model accumulates experiences specific to each repository and progressively updates its common knowledge base. Experimental results demonstrate that our method achieves a 20% improvement in issue code reproduction compared to the state-of-the-art methods. Additionally, our approach significantly enhances performance in the repository-level issue resolving tasks."}, {"title": "10 Data Availability Statement", "content": "We release our code to encourage further exploration in this direction. The artifact that supports the results discussed in this paper is available at https://anonymous.4open.science/r/EvoCoder-6433/"}]}