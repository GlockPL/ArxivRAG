{"title": "FADAS: Towards Federated Adaptive Asynchronous Optimization", "authors": ["Yujia Wang", "Shiqiang Wang", "Songtao Lu", "Jinghui Chen"], "abstract": "Federated learning (FL) has emerged as a widely adopted training paradigm for privacy-preserving machine learning. While the SGD-based FL algorithms have demonstrated considerable success in the past, there is a growing trend towards adopting adaptive federated optimization methods, particularly for training large-scale models. However, the conventional synchronous aggregation design poses a significant challenge to the practical deployment of those adaptive federated optimization methods, particularly in the presence of straggler clients. To fill this research gap, this paper introduces federated adaptive asynchronous optimization, named FADAS, a novel method that incorporates asynchronous updates into adaptive federated optimization with provable guarantees. To further enhance the efficiency and resilience of our proposed method in scenarios with significant asynchronous delays, we also extend FADAS with a delay-adaptive learning adjustment strategy. We rigorously establish the convergence rate of the proposed algorithms and empirical results demonstrate the superior performance of FADAS over other asynchronous FL baselines.", "sections": [{"title": "1. Introduction", "content": "In recent years, federated learning (FL) (McMahan et al., 2017) has drawn increasing attention as an efficient privacy-preserving distributed machine learning paradigm. An FL framework consists of a central server and numerous clients, where clients collaboratively train a global model without sharing their private data. FL entails each client conducting multiple local iterations, while the central server periodically aggregates these local updates into the global model. Following the original design of the FedAvg algorithm (McMahan et al., 2017), a large number of stochastic gradient descent (SGD)-based FL methods have emerged, aiming to improve the performance or efficiency of FedAvg (Karimireddy et al., 2020; Acar et al., 2021; Wang et al., 2020b). In addition to the successes of SGD-based algorithms in enhancing the efficiency of FL, the adoption of adaptive optimization techniques is becoming increasingly prevalent in FL. Adaptive optimization techniques such as Adam (Kingma & Ba, 2015) and AdamW (Loshchilov & Hutter, 2017) have proven their advantages over SGD in effectively training or fine-tuning large-scale models like BERT (Devlin et al., 2018), ViT (Dosovitskiy et al., 2021), and Llama (Touvron et al., 2023). This progress has encouraged the incorporation of adaptive optimization into the FL settings, taking advantage of their ability to navigate update directions and dynamically adjust learning rates. For example, FedAdam (Reddi et al., 2021) and FedAMS (Wang et al., 2022b) employ global adaptive optimization after the server aggregates local model updates. Moreover, strategies such as FedLALR (Sun et al., 2023a), FedLADA (Sun et al., 2023b), and FAFED (Wu et al., 2023) replace SGD with the Adam optimizer for the local training phase, exemplifying the utility of local adaptive optimizations in FL. However, existing methods in adaptive FL still rely on traditional synchronous aggregation approaches, where the server must wait for all participating clients to complete their local training before global updates. This reliance presents a significant challenge to the practical implementation of adaptive FL methodologies, as the server is required to wait until slower clients, which may have limited computation or communication capabilities. While asynchronous FL strategies such as FedBuff (Nguyen et al., 2022) and FedAsync (Xie et al., 2019) have been investigated to improve the scalability and to study the impact of client delays on the convergence of SGD-based FL algorithms, the specific implications of asynchronous delays on nonlinear adaptive gradient operations are not completely understood. This motivates us to explore the following question: Can we develop an asynchronous method for adaptive federated optimization (with provable guarantees) that enhances training efficiency and is resilient to asynchronous delays?"}, {"title": "2. Related Work", "content": "Federated learning. FL, as introduced by McMahan et al. (2017), has become a pivotal framework for collaboratively training machine learning models on edge devices while keeping local data private. Following the initial FedAvg algorithm, several works studied the theoretical analysis and empirical performance of it (Lin et al., 2018; Stich, 2018; Li et al., 2019a; Karimireddy et al., 2020; Wang & Joshi, 2021; Yang et al., 2021), and a range of works aim to improve FedAvg from different perspectives, such as reducing the impact of data heterogeneity (Karimireddy et al., 2020; Acar et al., 2021; Wang et al., 2020b), saving the communication overhead (Reisizadeh et al., 2020; Jhunjhunwala et al., 2021), and adjusting the parameter aggregation procedure (Tan et al., 2022; Wang & Ji, 2023). Adaptive FL optimizations and adaptive updates. Besides traditional SGD-based methods, there is a line of works focusing on adaptive updates in FL. A local adaptive FL method with momentum-based variance-reduced gradient was used in FAFED (Wu et al., 2023). Li et al. (2023) proposed a framework for local adaptive gradient methods in FedDA. FedLALR (Sun et al., 2023a) uses local adaptive optimization in FL with local historical gradients and periodically synchronized learning rates. FedLADA (Sun et al., 2023b) is an efficient local adaptive FL method with a locally amended technique. Jin et al. (2022) developed novel adaptive FL optimization methods from the perspective of dynamics of ordinary differential equations. Moreover, Reddi et al. (2021) introduced FedAdagrad, FedAdam and FedYogi, and Wang et al. (2022b) proposed FedAMS for global adaptive FL optimizations. Several works of global adaptive learning rate (Jhunjhunwala et al., 2023) and adaptation in aggregation weights (Tan et al., 2022; Wang & Ji, 2023) are also related to adaptive learning rate adjustment. Asynchronous SGD and asynchronous FL. There have been extensive studies over the years about asynchronous optimization techniques, including asynchronous SGD and its various adaptations. For example, Hogwild (Niu et al., 2011) includes an applicable lock-free, coordinate-wise asynchronous method and has been widely used in multi-thread computation. A body of works focuses on the theoretical analysis and explorations of asynchronous SGD (Mania et al., 2017; Nguyen et al., 2018; Stich et al., 2021; Leblond et al., 2018; Glasgow & Wootters, 2022) and discusses the gradient delay in the convergence rate (Avdiukhin & Kasiviswanathan, 2021; Mishchenko et al., 2022; Koloskova et al., 2022; Wu et al., 2022). Within federated learning, innovative asynchronous aggregation algorithms like FedAsync (Xie et al., 2019) allow the server to update the global model once a client finishes local training, and FedBuff (Nguyen et al., 2022) introduces a buffered aggregation approach. There are also many works focusing on algorithms based on FedBuff with theoretical and/or empirical analysis (Toghani & Uribe, 2022; Ortega & Jafarkhani, 2023; Wang et al., 2023), and other aspects of asynchronous FL (Chen et al., 2020b; Yang et al., 2022; Bornstein et al., 2023). Although adaptive FL and asynchronous FL have achieved the success of training large machine learning models with desirable numerical performance, the exploration of asynchronous updates in the context of adaptive FL has not been well-studied yet. In this paper, we start with the asynchronous update framework in adaptive FL and further integrate delay-adaptive learning rate scheduling into it."}, {"title": "3. Preliminaries", "content": "Federated learning. A general FL framework considers a distributed optimization problem across N clients:\n$\\min _{x \\in \\mathbb{R}^{d}} f(x):=\\frac{1}{N} F(x)=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}_{\\xi_{i} \\sim \\mathcal{D}_{i}}\\left[F_{i}\\left(x ; \\xi_{i}\\right)\\right],$(1)"}, {"title": "4. Proposed Method: FADAS", "content": "Although adaptive FL methods achieve promising convergence and generalization performance theoretically and empirically, the existing adaptive FL methods are restricted to synchronous settings, as the server needs to wait for all the assigned clients to finish their local updates for aggregation and then update the global model. However, those synchronous adaptive FL algorithms are susceptible to the presence of stragglers, where slower clients with insufficient computation or communication speed impede the progress of the global update. To improve the efficiency and resiliency of adaptive FL in the presence of stragglers, we introduce FADAS, a Federated Adaptive ASynchronous optimization method. Similar to FedAdam and FedAMS, the proposed FADAS algorithm takes the model update difference from clients as a pseudo-gradient and it updates the global model following an Adam-like update scheme. Algorithm 1 summarizes the details. FADAS keeps the local asynchronous training scheme as FedBuff and maintains the concept of concurrency and buffer size for flexible control of the number of active clients and the frequency of global model update. In FADAS, after the server aggregates to obtain model update difference $\\Delta_t$, it finds an adaptive update direction, whose components are computed based on the AMSGrad optimizer (Reddi et al., 2018) as follows:\n$\\begin{aligned} &m_{t}=\\beta_{1} m_{t-1}+\\left(1-\\beta_{1}\\right) \\Delta_{t}, \\\\ &v_{t}=\\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\\right) \\Delta_{t} \\odot \\Delta_{t}, \\\\ &v_{t}=\\max \\left(v_{t-1}, v_{t}\\right) . \\end{aligned}$(3) In general, FADAS enables clients to conduct local training in their own pace, and the server aggregates the asynchronous updates for global adaptive updates. It improves the training efficiency and scalability of over synchronous adaptive FL while inheriting the advantage of adaptive optimizer of reducing oscillations and stabilizing the optimization process. Although FADAS applies asynchronous local training for adaptive FL, the global adaptive optimizer adjusts the global update direction only based on local updates but without considering the impact of asynchronous delay. Intuitively, a large asynchronous delay from a client means that this model update is made based on an outdated global model. This may lead to a negative effect on the convergence, and later we also verify this intuition in the theoretical analysis. This inspires us to apply a delay-adaptive learning rate adjustment to improve the resiliency of FADAS to stragglers with large delays. Specifically, we let the server track the delay for every received model update and adopt a delay-adaptive learning rate. We highlight the delay-adaptive steps in Algorithm 1 and those steps are executed with almost no extra overhead. Delay tracking. In general, the server manages the delay record for each client through straightforward time-stamping. For example, the server records the global update round $t^{\\prime}$ when it broadcasts the current global model $x_{t^{\\prime}}$ to client $i$, the client conducts local training with $x_{t^{\\prime}}$. When the server receives the first $\\Delta$ from client $i$ at round $t>t^{\\prime}$, the gradient delay for $\\Delta$, which is $\\tau_{i}^{t}=t-t^{\\prime}$, is updated and recorded on the server. Delay-adaptive learning rate. Assume that for each global update round $t$, clients in the set $M_{t}\\left(\\left|M_{t}\\right|=M\\right)$ send updates to the server. The received model updates at global"}, {"title": "5. Theoretical Analysis", "content": "In this section, we delve into the theoretical analysis of our proposed FADAS algorithm. We first introduce some common assumptions required for the analysis. Subsequently, we present the analysis in two parts: one focusing on FADAS without delay adaptation, as discussed in Section 5.1, and the other on the delay-adaptive FADAS in Section 5.2. Assumption 5.1 (Smoothness). Each objective function on the i-th worker $F_i(x)$ is L-smooth, i.e., $\\forall x, y \\in \\mathbb{R}^{d}$,\n$\\|\\nabla F_{i}(x)-\\nabla F_{i}(y)\\|\\leq L\\|x-y\\| .$\nAssumption 5.2 (Bounded Variance). Each stochastic gradient is unbiased and has a bounded local variance, i.e., for all $x, i \\in[N]$, we have $\\mathbb{E}[\\left\\|\\nabla F_{i}(x ; \\xi)-\\nabla F_{i}(x)\\right\\|^{2}] \\leq \\sigma^{2}$, and the loss function on each worker has a global variance bound, $\\frac{1}{N} \\sum_{i=1}^{N}\\left\\|\\nabla F_{i}(x)-\\nabla f(x)\\right\\|^{2} \\leq \\sigma_{g}^{2}$. Assumption 5.1 and 5.2 are standard assumptions in federated non-convex optimization literature (Li et al., 2019b; Yang et al., 2021; Reddi et al., 2021; Wang et al., 2022b; Wang & Ji, 2023). The global variance upper bound of $\\sigma_g$ in Assumption 5.2 measures the data heterogeneity across clients, and a global variance of $\\sigma_g^2 = 0$ indicates a uniform data distribution across clients. Assumption 5.3 (Bounded Gradient). Each loss function on the i-th worker $F_i(x)$ has G-bounded stochastic gradient on $l_2$ norm, i.e., for all $\\xi$, we have $\\|\\nabla F_{i}(x ; \\xi)\\| \\leq G$. Assumption 5.3 is necessary for adaptive gradient algorithms for both general (Kingma & Ba, 2015; Chen et al., 2020a), distributed (Wang et al., 2022a) and federated adaptive optimization (Reddi et al., 2021; Wang et al., 2022b; Sun et al., 2023b). This is because the effective global learning rate for adaptive gradient methods is $\\frac{\\eta}{\\sqrt{v_t + \\epsilon}}$, and we need a lower bound for $\\sqrt{v_t + \\epsilon}$ to guarantee that the effective learning rate does not vanish to zero. Assumption 5.4 (Bounded Delay of Gradient Computation). Let $\\tau_{i}^{t}$ represent the delay for global round $t$ and client $i$ which is applied in Algorithm 1. The delay $\\tau_{i}^{t}$ is the difference between the current global round $t$ and the global round at which client $i$ started to compute the gradient. We assume that the maximum gradient delay (worst-case delay) is bounded, i.e., $\\tau_{\\max }=\\max _{t \\in[T], i \\in[N]}\\left{\\tau_{i}^{t}\\right\\} < \\infty$. Assumption 5.4 is common in analyzing asynchronous and anarchic FL algorithms which incorporate the gradient de-"}, {"title": "5.1. Convergence Rate of FADAS", "content": "For expository convenience, in the following, we provide the theoretical convergence analysis of FADAS under the case of $\\beta_{1}=0$. The theoretical analysis and the proof for the general case of $0<\\beta_{1}<1$ are provided in Appendix A. We define the average of the maximum delay over time as $\\tau_{\\text {avg }}=\\frac{1}{T} \\sum_{t=1}^{T} \\tau_{\\max }^{t}=\\frac{1}{T} \\sum_{t=1}^{T} \\max _{i \\in[N]}\\left{\\tau_{i}^{t}\\right\\}$ which is useful in our analysis. Theorem 5.6. Under Assumptions 5.1-5.5, let T represent the total number of global rounds, K be the number of local SGD training steps and M be the number of the accumulated updates (buffer size) in each round. If the learning rate $\\eta$ and $\\eta_l$ satisfies $\\eta \\eta_{l} \\leq \\min \\left{\\frac{\\epsilon^{3} M(N-1)}{360 C G N(N-M) \\bar{\\tau}_{\\max } K L}, \\frac{\\epsilon^{2} M(N-1)}{12 \\sqrt{C G} N(M-1) \\bar{\\tau}_{\\max } K L}\\right\\}, \\eta_{l} \\leq \\min \\left{\\sqrt{\\frac{\\epsilon}{K \\sigma_{g}^{2}}}, \\frac{\\epsilon}{K G}\\right\\}$, then the global iterates $\\left\\{x_{t}\\right\\}_{t=1}^{T}$ of Algorithm 1 satisfy\n$\\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E}\\left[\\left\\|\\nabla f\\left(x_{t}\\right)\\right\\|^{2}\\right] \\leq \\frac{4 C G}{T} \\frac{F}{\\eta \\eta_{l} K M}+\\left[8 C G \\eta \\eta_{l} \\bar{\\tau}_{\\text {avg }} \\bar{\\tau}_{\\max } K L^{2}+120 C G \\eta \\eta_{l} \\bar{\\tau}_{\\max } K L^{2}\\right] \\frac{\\sigma_{g}^{2}}{\\mathcal{M} \\epsilon^{3}}+\\frac{N-M}{N-1} \\frac{\\sigma^{2}}{M}+\\frac{N-M}{N-1}\\left[15 \\eta^{2} K^{2} L^{2}\\left(\\sigma^{2}+6 K \\sigma_{g}^{3}\\right)+3 K \\sigma_{g}^{3}\\right].$(5) where $F=f(x_1) - f_*, f_* = \\min_x f(x) > -\\infty$ and $C_G = \\eta_l K G + \\epsilon$. Corollary 5.7. If we choose the global learning rate $\\eta=\\Theta(\\frac{\\epsilon}{\\sqrt{M}})$ and $\\eta_l = \\Theta(\\sqrt{\\frac{\\epsilon}{T K(\\sigma^2 + K\\sigma_g^3)}})$ in Theorem 5.6, then for sufficiently large T, the global iterates $\\left\\{x_{t}\\right\\}_{t=1}^{T}$ of Algorithm 1 satisfy\n$\\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E}\\left[\\left\\|\\nabla f\\left(x_{t}\\right)\\right\\|^{2}\\right] \\leq 0\\left(\\frac{\\sqrt{F} G}{\\sqrt{T} K M}+\\frac{\\sqrt{F} \\sigma_{g}}{\\sqrt{T} M}+\\frac{F G}{\\mathcal{T} \\sqrt{M}}+\\frac{F \\bar{\\tau}_{\\max } \\bar{\\tau}_{\\text {avg }}}{\\mathcal{T} M}\\right)$.(6) Remark 5.8. Corollary 5.7 suggests that given sufficiently large T and relatively small worst-case delay $\\tau_{\\max }$, the proposed FADAS (without delay-adaptive learning rate) achieves a convergence rate of $O\\left(\\frac{1}{\\sqrt{T} \\mathcal{M}}\\right)$ w.r.t. T and M. Comparison to asynchronous FL methods. Compared with the analysis for FedBuff in Nguyen et al. (2022) and Toghani & Uribe (2022), our analysis for FADAS obtains a relaxed dependency on the worst-case gradient delay $\\tau_{\\max }$, and FADAS achieves a slightly better rate on non-dominant term than $O\\left(\\frac{\\bar{\\tau}_{\\text {avg }}}{\\sqrt{\\mathcal{T}}}+\\frac{1}{\\mathcal{T}} \\mathcal{M}\\right)$ obtained in Toghani & Uribe (2022). Moreover, Wang et al. (2023) also studied the convergence for FedBuff with relaxed requirements for $\\tau_{\\max }$, and our FADAS achieves a similar convergence of $O\\left(\\sqrt{\\frac{1}{\\mathcal{T}}}+\\frac{\\bar{\\tau}_{\\max } \\bar{\\tau}_{\\text {avg }}}{\\mathcal{T}}\\right)$ as in Wang et al. (2023). It is worthwhile to mention that recently CA2FL (Wang et al., 2023) improves the convergence of asynchronous FL under heterogeneous data distributions, while the improvement is obtained by using the cached variable on the server for global update calibration. Note that when $\\tau_{\\max }$ in Eq. (6) is large, particularly in cases where $\\tau_{\\max } > \\sqrt{\\mathcal{T}}$, then $\\bar{\\tau}_{\\max } \\bar{\\tau}_{\\text {avg }}$ becomes the dominant term in the convergence rate. This implies that a large worst-case delay $\\tau_{\\max }$ may lead to a worse convergence rate. In the next subsection, we demonstrate that the delay-adaptive learning rate strategy can relieve this problem and enhance FADAS with better resilience to large worst-case delays."}, {"title": "5.2. Convergence Rate of Delay-adaptive FADAS", "content": "In the following, we provide the convergence analysis for delay-adaptive FADAS with $\\beta_{1}=0$. To get started, we first define the median of the maximum delay over all communication rounds $[T]$:\n$\\bar{\\tau}_{\\text {median }}=\\operatorname{median}\\left{\\bar{\\tau}_{1}^{\\max }, \\bar{\\tau}_{2}^{\\max }, \\ldots, \\bar{\\tau}_{T}^{\\max }\\right\\}$.(7) The definition of $\\bar{\\tau}_{\\text {median }}$ implies that the number of global update rounds that have a maximum delay greater than $\\bar{\\tau}_{\\text {median }}$ is less than half of the total number of global updates $T$. With this definition, we present the following theorem characterizing the convergence rate of delay-adaptive FADAS. Theorem 5.9. Under Assumptions 5.1\u20135.5, let T be the total number of global rounds, K be the number of local SGD training steps and M be the number of the buffer size"}]}