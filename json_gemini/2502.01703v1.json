{"title": "QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning", "authors": ["Moses Ananta", "Muhammad Farid Adilazuarda", "Zayd Muhammad Kawakibi Zuhri", "Ayu Purwarianti", "Alham Fikri Aji"], "abstract": "Fine-tuning large language models (LLMs) is\noften constrained by the computational costs\nof processing massive datasets. We propose\nQLESS (Quantized Low-rank Gradient Simi-\nlarity Search), which integrates gradient quan-\ntization with the LESS framework to enable\nmemory-efficient data valuation and selection.\nQLESS employs a two-step compression process:\nfirst, it obtains low-dimensional gradient repre-\nsentations through LoRA-based random projec-\ntion; then, it quantizes these gradients to low-\nbitwidth representations. Experiments on multi-\nple LLM architectures (LLaMA, Mistral, Qwen)\nand benchmarks (MMLU, BBH, TyDiQA) show\nthat QLESS achieves comparable data selection\nperformance to LESS while reducing memory\nusage by up to 16x. Even 1-bit gradient quan-\ntization preserves data valuation quality. These\nfindings underscore QLESS as a practical, scal-\nable approach to identifying informative examples\nwithin strict memory constraints.", "sections": [{"title": "1. Introduction", "content": "As Large Language Models (LLMs) continue to scale in size\nand complexity, their training and fine-tuning demand effi-\ncient data management strategies to optimize performance\non specific tasks. A pivotal aspect of this optimization is\ndata valuation\u2014the process of identifying influential train-\ning data to improve learning outcomes. Gradient-based data\nvaluation methods"}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Gradient-Based Data Valuation", "content": "Data valuation methods aim to quantify each training sam-\nple's contribution to a model's performance. Among these,\ngradient-based approaches are particularly appealing for\nlarge-scale models, as they directly measure how training\nexamples influence parameter updates. A key representative\nis TracIn (Garima et al., 2020), which evaluates the impact\nof a training sample z on a validation sample z' by summing\nthe dot products of their gradients across k checkpoints:\nInfTracIn(z,z\u2032)=\u2211ki=1ni(\u2207l(z;\u03b8i),\u2207l(z\u2032;\u03b8i)),\nwhere \u2207l(z; \u03b8i) and \u2207l(z';0i) are the gradients of the\nmodel's loss at checkpoint i, and ni is the learning rate.\nThis dot-product summation captures how z \"pushes\" the\nparameters in directions that aid (or impede) performance\non z'.\nHowever, storing gradients for multiple checkpoints be-\ncomes prohibitive in large language models (LLMs). To\naddress this, approaches like LESS (Xia et al., 2025) apply\ncompression strategies that retain crucial gradient align-\nments while reducing the memory burden."}, {"title": "2.2. LESS: Low-Rank Gradient Similarity Search", "content": "LESS (Low-Rank Gradient Similarity Search) (Xia et al.,\n2025) extends gradient-based data valuation by leveraging\nthe parameter efficiency of LoRA (Hu et al., 2021). Instead\nof computing full-model gradients, LESS focuses on the\nLORA parameters, extracting Adam training gradient \u0393(z; 0)\nfor each training sample z. For validation samples z', it\nuses an SGD validation gradient \u2207l(z'; 0) to measure how\ntraining samples influence validation performance.\nTo handle the high dimensionality of these gradients, LESS\napplies a randomly initialized linear mapping R : Rd \u2192 Rk\nwhere k \u226a d. By the Johnson\u2013Lindenstrauss lemma (John-\nson & Lindenstrauss, 1984), this projection approximately\npreserves pairwise distances or inner products, enabling\nefficient similarity searches. Concretely, for each training\nsample z at checkpoint i:\ngz,i=R(\u0393(z;\u03b8i))\u2208Rk.(1)\nIn instruction tuning, each sample z often spans multiple\ntokens, so its gradient is an average of token-level gradients.\nAs reported in (Xia et al., 2025), this can disproportionately\nfavor shorter sequences by yielding larger gradient norms.\nTo counteract this bias, LESS normalizes each projected\ngradient:\n\u02c6gz,i=gz,i||gz,i||.(2)"}, {"title": "2.3. Quantization in Distributed and Federated\nLearning", "content": "Quantization-mapping high-precision numerical data to\nlower bit-width representations\u2014has been widely adopted\nin distributed and federated learning to mitigate commu-\nnication overhead. The core idea is to transform a\nfloating-point vector x \u2208 Rk into a more compact integer or\nfixed-point format, frequently reducing memory by factors\nof four, eight, or more.\nA straightforward technique is absmax-based quantization,\nwhich scales each vector by its largest absolute value to fit\ninto a specified bit budget b. Suppose\nS=max1<m<k|xm|,(4)"}, {"title": "2.4. Quantized Random Projection (QRP)", "content": "Quantized random projection (QRP) combines the dimen-\nsionality reduction benefits of random projection with\ndiscrete quantization to produce compact vector repre-\nsentations while preserving key geometric relationships.\nThe theoretical foundations of QRP lie in the Johnson-\nLindenstrauss lemma and\nits quantized variants, which provide guarantees on the ap-\nproximate preservation of pairwise distances or inner prod-\nucts under random projection and quantization.\nIn the conventional random projection pipeline, we map\nhigh-dimensional vectors {x}=1 CRd to a lower-\ndimensional space Rk via a randomly sampled matrix\nR\u2208 Rk\u00d7d drawn from a suitable distribution (e.g., Gaus-\nsian or Rademacher).\nOnce the vectors yi = Rx\u012f are projected into Rk, a quanti-\nzation function\nQ: Rk \u2192 Ck\nfurther compresses them into a set of finite levels C.\nFor instance, binary QRP restricts each dimension to"}, {"title": "3. QLESS: Integrating LESS with\nQuantization", "content": "Building upon the gradient-based data valuation strategies\nintroduced in the previous section, we now present QLESS,\na method that combines the memory-efficient random pro-\njection and normalization of LESS with an additional quanti-\nzation step. The key insight is that even after reducing gradi-\nent dimensionality, storing the resulting vectors in floating-\npoint format can be expensive for large datasets. By quan-\ntizing each projected gradient, QLESS substantially reduces\nmemory consumption while preserving enough directional\ninformation to maintain the effectiveness of influence-based\ndata selection. The full process is visualized in Figure 2."}, {"title": "3.1. Quantizing Randomly Projected Gradients", "content": "Building on the random projection and normalization strat-\negy in LESS (Xia et al., 2025), QLESS further leverages\nQuantized Random Projection (QRP; see Section 2.4) to\ndrastically reduce the memory footprint of stored gradi-\nents. Specifically, for each training sample z at checkpoint\ni, QLESS computes a LoRA-based gradient \u0393(z; \u03b8\u2081) and\napplies the same random projection\ngz,i=R(\u0393(z;\u03b8i))\u2208Rk\nas in LESS. However, rather than storing gz,i in floating-\npoint format, QLESS immediately quantizes it using a sim-\nple but effective absmax-based scheme, where vectors are\ncompressed after random projection to retain essential geo-\nmetric information.\nConcretely, let\nSz,i=max1<m<k|gz,i,m|,a=2b\u22121\u22121,\nfor a chosen bit budget b \u2208 {1,2,4,8}. Each component\ngz,i,m is mapped to an integer qz,i,m via\nqz,i,m=round(agz,i,mSz,i)."}, {"title": "3.2. Influence Computation with Quantized-Normalized\nGradients", "content": "During data valuation, QLESS normalizes the quantized\nrepresentation of gz,i directly. Each quantized component\nqiz,i,m is normalized as:\n\u02c6qz,i,m=qz,i,m||qz,i||,(6)\nwhere || qz,i || is the norm of the quantized vector.\nSimilarly, validation gradients gz',i are quantized and nor-\nmalized, yielding \u02c6qz',i. QLESS computes the influence of\na training sample z on a validation sample z' by summing\nthe learning-rate-weighted cosine similarities between the\nquantized-normalized gradients across N checkpoints:\nInfQLESS(z,z\u2032)=\u2211Ni=1ni(\u02c6qz\u2032,i,\u02c6qz,i).(7)\nBy utilizing QRP (\u00a73.1), QLESS retains the essence of\ngradient-based data valuation while substantially cutting\nstorage demands. As we will show in our experiments,\nthis approach enables efficient data selection across larger\ndatasets and a broader range of LLM architectures and sizes\nthan would be practical with higher-precision storage."}, {"title": "4. Experiments", "content": "This section describes our experimental setup for evaluating\nQLESS, focusing on datasets, models, baselines, and the\noverall pipeline. By adhering to the same framework as\nLESS, we ensure fair comparisons while isolating the effects\nof gradient quantization introduced in QLESS."}, {"title": "4.1. Experimental Setup", "content": "Datasets. We adopt the same datasets used in LESS to\nensure consistency. The training data comprises four major\ninstruction-tuning datasets: Flan v2 Chain-of-Thought , Dolly and OpenAssistant totaling approximately 270K training instances. These datasets\nspan diverse instruction types, from single-turn queries to\ncomplex reasoning tasks and multi-turn conversations. For\nvalidation and evaluation, we use the same benchmarks as\nLESS: MMLU which tests knowl-\nedge across 57 academic subjects, BBH for assessing complex reasoning capabilities, and"}, {"title": "4.2. Main Results", "content": "The results of our experiments, as summarized in Table 1\nand Table 4, highlight the efficacy of QLESS in balanc-\ning storage efficiency and downstream model performance\nacross various bit-width configurations. Our analysis fo-\ncuses on the trade-offs between performance and storage,\nthe generalizability of QLESS across models, and the sur-\nprising robustness of extreme quantization.\nOverall Performance. Our experiments demonstrate that\nQLESS achieves competitive performance across all evalu-\nated models and benchmarks while significantly reducing\nstorage requirements compared to LESS. On most bench-\nmarks, QLESS results in comparable or better performance"}, {"title": "5. Ablation Studies and Analysis", "content": "Effect of QLoRA on Gradient Extraction. Motivated by\nthe desire to further reduce memory usage during gradient\nextraction, we replace the standard LoRA with QLoRA\nin our QLESS pipeline. Specifically,\nfor model quantization, we employ the LLM int8 approach\nand bitsandbytes NF4 enabling more efficient parameter\nstorage while still preserving key model capabilities. Our\nprimary question is whether QLoRA\u2014when combined with\ngradient quantization\u2014meaningfully degrades the gradient\nfidelity required for accurate influence estimation.\nAs shown in Table 2 and Table 5, the memory footprint\ndrops from around 32-35 GB down to 22-26 GB for both\nLLaMA2-7B and Qwen2.5-7B, yet the average performance\non TyDiQA, MMLU, and BBH remains close to that of\nhigher-precision baselines. Although there is a minor drop\nin scores when combining QLoRA with 1-bit or 2-bit gra-\ndient quantization, the overall performance remains well\nabove random-selection levels. These results suggest that\nusing QLoRA in tandem with QLESS provides a viable\nsolution for resource-constrained environments, where ag-"}, {"title": "Analyzing Sparsity Effects of Absmax Quantization", "content": "While absmax-based quantization effectively compresses\ngradients, it can induce significant sparsity as bit-width de-\ncreases. In particular, most gradient values collapse into\nthe zero bin for 2-bit and 4-bit settings, leading to a heavily\nsparse representation that degrades dot-product operations\ncrucial for influence computation. This effect arises because\nthe Gaussian-like distribution of gradients under absmax\nscaling often places a large fraction of values near zero,\nexacerbating the loss of directional information.\nTo counteract this, we explore absmean-based quantization,\nwhich shifts the quantization scale to emphasize the mean\nabsolute value rather than the maximum. By pushing values\naway from zero and closer to the outer bins, absmean-based\nquantization yields a denser distribution of nonzero entries,\nthereby preserving more effective gradient directions for\nsimilarity calculations. Interestingly, 1-bit quantization does\nnot exhibit the extreme sparsity problem because its repre-\nsentation inherently omits a zero bin. Figure 3 provides vi-\nsualizations showing how absmax-based quantization leads\nto severe zero bin occupancy in low-bit regimes and how\nabsmean-based quantization alleviates this issue at the cost\nof reducing the fidelity of gradient information, especially\nin 8-bit and 4-bit precision.\nTable 3 compares absmean and absmax quantization for\nLLaMA2-7B under various bit-widths. At higher precisions\n(e.g., 16-bit), absmax slightly outperforms absmean on Ty-"}, {"title": "Empirical results on selected data percentage", "content": "Our\nmain experiments follow LESS (Xia et al., 2025) in select-\ning only 5% of data for fair comparisons, although there has\nyet been empirical results on whether or not this is the op-\ntimal percentage. We experiment on selecting 0.1%, 0.5%,\n1%, 2%, and 10% in addition to 5% of data, selected using\nQLESS specifically at 16-bit model precision and 1-bit gra-\ndient store quantization using Llama 2 7B and Qwen 2.5 7B\non the 3 benchmarks.\nThe results can be seen in Figure 4. Interestingly, we find\nthat the performance of the models are already obtainable\neven at 0.5%. Selecting only 0.1% may not be enough, but\nperformance on these benchmarks plateaus starting from\n0.5% up to 5% and only goes up slightly at 10%. These\nresults suggest that even with a tighter budget, it is still\npossible to gain performance with QLESS to some degree."}, {"title": "6. Related Work", "content": "Gradient-based data valuation and selection. Data val-\nuation techniques have evolved from computationally in-\ntensive leave-one-out approaches to gradient-based strate-"}, {"title": "7. Conclusion", "content": "We propose QLESS, a quantization-enhanced data selection\nalgorithm that extends LESS with gradient compression\ntechniques (\u00a73). QLESS maintains selection effectiveness\nwhile dramatically reducing storage requirements through\nabsmax-based quantization of gradient features (\u00a73). Our ex-\nperiments in \u00a74 demonstrate that even extreme quantization\n(1-bit) can achieve competitive performance compared to\nhalf-precision baselines across multiple model architectures.\nLooking forward, QLESS could be extended to support\ndynamic quantization schemes that adapt to gradient dis-\ntributions during training. The surprising effectiveness of\n1-bit representations also suggests investigating theoretical\nconnections between gradient quantization and influence\nestimation."}, {"title": "7.1. Limitations", "content": "Despite QLESS's strong performance, several limitations\nwarrant discussion. First, while our method substantially re-\nduces storage requirements, the initial gradient computation\nstill entails significant computational overhead. Although\nusing QLORA can help mitigate this cost, especially for\nlarge language models, further research is necessary to lower\nthe overall gradient extraction burden for extensive datasets.\nSecond, our sequential compression approach\u2014applying\nrandom projection followed by quantization\u2014may not opti-\nmally preserve influence relationships, as joint optimization\nof dimensionality reduction and precision reduction could\npotentially retain more information. Finally, the effective-\nness of different quantization schemes (e.g., absmax vs.\nabsmean) can vary by bit width, model architecture, and\ndataset characteristics; thus, practitioners must carefully se-\nlect and empirically validate suitable strategies for each new\nscenario."}, {"title": "8. Impact Statement", "content": "This paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here."}, {"title": "A. Experimental Details", "content": "All experiments were conducted using the parameter-efficient fine-tuning method LoRA . We employed a\nlearning rate scheduler with linear warm-up and cosine decay, reaching a peak learning rate of 2 \u00d7 10-5. Training was\ncarried out for 4 epochs across all selected datasets with a batch size of 128. For the LoRA module, we specified a rank of\n64, an a value of 256, a dropout rate of 0.1, and learned LoRA matrices for query, key, value, and output projection layers.\nWe performed three trials using distinct random seeds. For random selection approaches, this involved selecting three\ndifferent random subsets from the training dataset. In our approach (QLESS), this meant conducting warmup training with\nvarious subsets of the training data and subsequently selecting different subsets for each trial from each warmup-trained\nmodel. We maintained consistent optimization seeds across all experiments to ensure reproducibility.\nWe followed an evaluation procedure similar to , focusing on three target tasks. For MMLU, we reported\n5-shot accuracy on the test set, averaging results across 57 subtasks. For TyDiQA, we measured 1-shot macro-averaged F1\nover all 11 languages using the gold-passage setup. For BBH, we computed the 3-shot exact match score across all tasks,\nproviding chain-of-thought reasoning in each in-context learning example.\nAll experiments were conducted on 4 NVIDIA A100 GPUs with 40GB memory. We utilized BFloat16 precision and\nemployed Fully Sharded Data Parallel (FSDP) training with auto-wrapping to optimize memory and computational efficiency.\nThe per-device batch size was set to 1, with gradient accumulation steps adjusted to achieve an effective batch size of 128."}, {"title": "B. Additional Results", "content": "To further validate the effectiveness and generalizability of QLESS, we conducted experiments on two additional large\nlanguage models: Llama 3.2 3B and Mistral 7B. Table 4 presents the performance of these models, as well as the original\nLlama 2 7B, across different data selection methods and gradient storage configurations. Table 5 presents the performance\nof Llama 2 7B under different model and gradient quantization settings."}, {"title": "C. Qualitative Analysis", "content": "In addition to the quantitative metrics reported above, we examine which specific examples are chosen by QLESS under\nvarious quantization levels. Figure 5 shows the distribution of the top 5% selected examples (by influence score) across\nfour instructional datasets (Flan v2, CoT, Dolly, Oasst1) for three validation benchmarks (TyDiQA, MMLU, and BBH).\nMeanwhile, Tables 6, 7, and 8 present concrete examples selected for each benchmark, illustrating how the chosen samples\ndiffer across bit-precision settings."}, {"title": "Distribution of Data Sources", "content": "From Figure 5, we observe that the overall composition of the top 5% selected training\ndata remains relatively stable across 16-bit, 8-bit, and 4-bit, and even 1-bit quantizations, showing robustness over extreme\nquantization. The proportion of each dataset (Flan v2, CoT, Dolly, Oasst1) shift more noticeably at 2-bit, which may be\ncaused by the sparsity effect of quantization (Figure 3). For instance, in BBH (Figure 5 (C)), 2-bit selection draws a larger\nfraction of examples from Oasst1."}, {"title": "Granular Example Differences", "content": "For TyDiQA (Table 6), we observe that the 16-bit, 8-bit, and 4-bit variants of QLESS\nselect a highly relevant example that demonstrates the model's ability to extract factual information from a given passage to\nanswer a question. The 2-bit and 1-bit variants, on the other hand, select examples that test the model's capacity to combine\nfacts from different sources to answer more complex questions. While these examples are less directly aligned with the"}, {"title": "Interpretation", "content": "These observations indicate that QLESS largely preserves consistent types of top-ranked examples at\n16-bit, 8-bit, 4-bit, and even 1-bit. Only the 2-bit quantization stands out as notably different, likely because the jump\nin sparsity degrades data valuation precision enough to emphasize distinct prompts. Although our quantitative results in\nearlier sections show that 2-bit can still perform reasonably well, the shift in selected data highlights the delicate balance\nbetween extreme compression and influence fidelity. These qualitative findings reinforce the notion that QLESS is robust\nto quantization\u2014even down to 1-bit\u2014while hinting that 2-bit may require closer calibration to avoid undesired selection\nbiases."}]}