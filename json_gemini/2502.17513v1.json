{"title": "Int2Int: a framework for mathematics with transformers", "authors": ["Fran\u00e7ois Charton"], "abstract": "This paper documents Int2Int, an open source code base for using transformers on problems of mathematical research, with a focus on number theory and other problems involving integers. Int2Int is a complete PyTorch implementation of a transformer architecture, together with training and evaluation loops, and classes and functions to represent, generate and decode common mathematical objects. Ancillary code for data preparation, and Jupyter Notebooks for visualizing experimental results are also provided. This document presents the main features of Int2Int, serves as its user manual, and provides guidelines on how to extend it. Int2Int is released under the MIT licence, at https://github.com/FacebookResearch/Int2Int.", "sections": [{"title": "Introduction", "content": null}, {"title": "Int2Int: Mathematics as a translation task", "content": "The transformer architecture [19], introduced in 2017 for machine translation, can be applied to problems of mathematics by rewriting problems and their solutions as sentences sequences of words from a finite vocabulary and training transformers, from generated examples, to \u201ctranslate\" the sequence representing a problem into the sequence representing its solutions. For instance, a transformer can be trained to add integers, by learning to translate a pair of integers, say 12 and 23, represented as a sequence of digits in base 10, + , 1, 2, + , 2, 3, into the sequence +, 3, 5, which represents their sum.\nPrevious research suggests that this approach can be applied to a wide range of mathematical problems, from symbolic integration [15] to arithmetic [17, 16, 5] and computing the eigenvalues of real symmetric matrices [4]. More recently, such techniques were used to solve hard and open problems, like symbolic regression [3, 9], discovering the Lyapunov functions that govern the global stability of dynamical systems [1] and generating competitive candidate solutions of hard combinatorial problems [6]. These results illustrate the potential use of transformers to assist mathematical discovery.\nApplying transformers to a specific problem of mathematics can be a challenging task. While Python implementations of transformers can be found on open source repositories, such as HuggingFace [20], adapting these architectures to specific problems, generating training and test sets"}, {"title": "Supervised training in a nutshell", "content": "Deep learning models learn from examples. In the supervised setting, the model is trained from pairs of problems and solutions (x, y), represented as sequences of tokens over a finite vocabulary. The model implements a mapping from input to output sequences, \\(M : x \\rightarrow y\\), which depends on a large number (millions to billions) of parameters, the model weights w.\nDuring training, inputs \\(x_i\\) are grouped into batches and fed into the model, which produces as many predicted outputs \\(\\hat{y}_i = M(x_i)\\). These output are compared to the desired output \\(y_i\\), using a differentiable loss function \\(L(\\hat{y}_i, y_i)\\), and the gradient \\(\\nabla_wL(\\hat{y}_i, y_i)\\), the vector of partial derivatives of the loss with respect to the model weights, is automatically computed and averaged over the batch. The gradient is used to update the model weights, so as to reduce the training loss on the current batch. At this point, a new batch of input is created, and a new optimisation step begins.\nModels weight, initialized at random, are updated after every optimisation step, hopefully allowing the model to learn, i.e. make better prediction of the output. This is evaluated by calculating the model accuracy (i.e. the percentage of correct predictions), on a validation set of test examples that were not used during training. Evaluation happens once the model has seen a fixed number of training examples (300,000 by default), which we call an epoch. (Note: in other works, an epoch is defined as \"one pass over the whole training set\". This makes little sense in AI for Mathematics,"}, {"title": "Int2Int in a nutshell", "content": "Int2Int, a framework for supervised learning, is made of several components.\nThe model is a parametric function that maps input to output sequences. It uses a cross-entropy loss function to measure the discrepancy between its predictions and the correct solutions, and can calculate the gradient of the loss with respect to its parameters (i.e. model weights). Int2Int currently implements transformers, LSTM [14] and GRU [7]. The model code can be found in files src/model/transformer.py and src/model/lstm.py.\nThe optimizer is in charge of updating the model weights after a batch has been processed. It uses the current gradient (calculated by the model) and previous values of the gradient, to compute a direction for the update, and a learning rate, which quantifies the size of the \"step down\", along the direction. Different optimizers (SGC, Adam, AdamW) implement different techniques pour selecting the direction of update. The optimizer is also in charge of varying the learning rate as training proceeds, a process known as warm-up at the beginning of training, when the learning rate linearly grows from zero to a fixed value, and scheduling in later stages, when the learning rate is gradually reduced as the model achieves better accuracy. In Int2Int, optimizers are implemented in the file src/optim.py. Int2Int is compatible with (and heavily relies on) PyTorch optimizers.\nThe data loader is in charge of reading the training and validation data, assembling it into batches, and feeding it into the model. The data can be found in a text file, that the data loader reads, or be generated on the fly by the generator (see below). Specific data sampling strategies, like curriculum learning (presenting examples in a specific order), or repeating some examples [12], are implemented in the data loader. It is to be found in file src/dataset.py.\nThe trainer implements the main training logic. It loads and save the model, computes the gradient, calls the optimizer, and updates. It is implemented in src/trainer.py.\nThe evaluator implements the evaluation at the end of each epoch. It is a simplified version of the trainer, which computes the model predictions from the test data, but does not calculate the gradient, or run the optimizer. Instead, a number of metrics and statistics are calculated and reported. It is found in src/evaluator.py\nThe model, optimizer, data loader and trainer are independent of the problem to be solved (up to the choice of hyper-parameters, e.g. different problems may require different model parameters). So long one uses the same architectures (transformers), they can be re-used without modification. The evaluator may require small changes if one wants to calculate specific metrics (see also section 4.2). If the model is trained from files of pre-generated data (see section 3.4 for the required format),"}, {"title": "Getting started: two worked-out examples", "content": "To get started with Int2Int, please clone Int2Int from the repository at https://github.com/FacebookResearch/Int2Int (instructions in the Code button of GitHub). This should create a directory, named Int2Int (you may rename it) that contains (among other things) a file train.py, a README.md file that partially duplicates these instructions, and two subdirectories: src, which contains the source code, and data which contains the elliptic function dataset for our first worked-out example. You will also need to have installed Python (version 3.0 or better), PyTorch (pytorch.org), and NumPy."}, {"title": "Out of the box: learning the greatest common divisor of two integers", "content": "As a very first attempt running Int2Int, one may type, from the Int2Int directory on your computer:\npython train.py --dump_path /some_path_on_your_computer/ --exp_name\nmy_first_experiment --exp_id 1 --operation \"gcd\"\nNote: this assumes your computer has an NVIDIA GPU. If you do not, you can add --cpu\ntrue to the command line. The model will run much slower.\nThis will train a transformer to compute the greatest common divisor of two integers. The training and test data are generated on the fly. The parameters dump_path, exp_name and exp_id indicate where the experimental results, and the trained models, will be saved: here in /some_path_on_your_computer/my_first_experiment/1/."}, {"title": "Out of the box: predicting the rank of elliptic curves", "content": "Int2Int can also be run from a pre-computed dataset. The data files (one train set, one test set) should be text files, with one example per line, input and output, separated by a tabulation, written as tokens separated by spaces. See section 3.4 for more information about data file and how to produce them. Here we train a model on a database of elliptic curves and their ranks, created from the database the LFMDB database [?], by Barinder Banwait. The data files can be down loaded from I assume they are copied in a directory named /data.\nThe data sets include 3,810,000 elliptic curves in the training set, and 10,000 in the test set. The first four lines of the training set are:\n+ 1 + 0 + 1 + 5168 902 2\n+ 1 + 0 + 0 3 881 72 555 0\n+ 0 + 0 + 0 502 891 875 + 4 340 702 513 250 1\n+ 0 + 0 + 0 138 477 + 19 834 180 2\nThe input are the five parameters of the elliptic curve, written in base 1000, and the output is the rank: 1,0,1,516, -8902 and 2 for the first curve, 0, 0, 0, -502891875, +4340702513250 and 1 for the third.\nTo train a transformer to predict the rank of elliptic curves we can run, from the Int2Int directory:\npython train.py --operation data --dump_path /some_path_on_your_computer\n--exp_name my_second_experiment --exp_id 1\n--train_data/data/elliptic_rank.train --eval_data/data/elliptic_rank.test\nAs previously, the model reports a training loss that drops, then saturates around 0.5."}, {"title": "Using Int2Int: command-line parameters", "content": "This section serves as the user manual for Int2Int."}, {"title": "Base parameters", "content": "--dump_path, --exp_name and --exp_id define the directory where experimental results are saved:\ndump_path/exp_name/exp_id. Absent directories will be created. If --exp_id is not specified,\nInt2Int will generate a random sequence of 10 letters and digits (cf. function get_dump_path in src/utils.py). When training begins, Int2Int will create two files in this directory: params.pkl, which contains the command-line parameters of the model, and train.log, which contains the experimental log and results. At the end of every epoch, the model will be saved as checkpoint.pth.\nIf the folder dump_path/exp_name/exp_id already exists, Int2Int will try to continue an existing experiment. It will reload the last saved model (checkpoint. pth) if it exists (alternatively, a path to another checkpoint can be provided as parameter --reload_checkpoint), and append to the log file (train.log).\nAlternatively, the model can be initialized with a pre-trained model, by indicating its path in --reload_model. In this case, a new log file is created, and training starts afresh.\nSaved models take a large amount of disk space. For this reason, only the last epoch is saved by default. The parameter --save_periodic allows for more regular saves. If set to 100, it will cause an additional savefile to be created at epochs 100, 200, etc. (as checkpoint-100.pth, checkpoint-200.pth, etc.). The parameter --validation_metrics causes Int2Int to save the best models according to some evaluation metric. For instance, a validation metrics with valid_arithmetic_acc would cause Int2Int to save the models with the highest accuracy on the validation set. To save the lowest value of the metric, instead of the highest, prefix the name of the indicator with: _test_arithmetic_xe_loss saves the models with the lowest test loss. Several validation metrics can be used at the same time, by concatenating them, separated by commas: valid_arithmetic_acc_valid_arithmetic_xe_loss. All available metrics, are logged at the end of each epoch, into a python directory. For the elliptic curve example, they are: valid_arithmetic_xe_loss, valid_arithmetic acc, valid_arithmetic perfect, valid_arithmetic_correct, valid_arithmetic_acc_0; valid_arithmetic_acc_1, valid_arithmetic_acc_2, valid_arithmetic_acc_3, valid_arithmetic_acc_4. See section 4.2"}, {"title": "Technical parameters", "content": "By default, Int2Int assumes a NVIDIA GPU is present to train the transformer, and that its device_id is 0. On a model with no GPU, you can set --cpu true, the model will train, but much slower. Evaluation will not be slowed a lot when performed on CPU. Running on a CPU can also be useful when debugging your code, as GPU parallelization complicates everything. On a local GPU, the model assumes device_id=0, but you can change this by setting --local_gpu to a different integer. Int2Int can also be run on a cluster using Slurm (set --local_rank O to run on a single GPU). See src/slurm.py for more information on the use of Slurm.\nMulti-GPU is supported under Slurm. At present, it allows splitting data batches on several GPU, which share the same copy of the model. In other words, a job erun with batches of 200 on four GPU will use an effective batch size of 800. This can be useful when you train large models on long input or output sequence (e.g. 800 tokens or more), or when ou run opn machines with small GPU memory (16GB or less). To avoid out-of-memory errors (from which Int2Int cannot recover), you have to reduce the batch size, and can compensate by running on several GPU. In this setting, evaluation is performed on one GPU only. Model sharding, which allows a very large model to be split over several GPU, is not supported at the moment.\nWhen training from a GPU, speed and memory usage can be accelerated by setting --fp16 true and --amp 1. This will cause all model parameters and gradients to be encoded as 16-bit floats, resulting in a smaller memory footprint and faster execution speed."}, {"title": "Evaluation", "content": "At the end of each epoch, the model is evaluated, either on data from a file defined in --eval data, or on a generated sample of --eval_size examples. Generated test sets are recreated at each epoch. Evaluation is performed in batches of --batch_size_eval examples. Because evaluation does not require gradient estimation, it uses less memory thant traiinng, and you can set the evaluation batch size to a larger value than the training batch size. With encoder-decoder transformers, you can use beam search by setting --beam_search true and --beam_size to a value larger than 1. This will cause the model to generate several solutions, and keep the best.\nWith encoder-decoder and decoder-only transformers, evaluation tends to be slow, because the model must be called as many times as there are tokens in the output sequence. If you know the maximal length of the output, consider setting --max_output_len to this value plus 2 (for the beginning and end of sequence tokens). This will greatly accelerate evaluation.\nBy default, aggregated evaluation results are output in the train.log file. If you set --eval_verbose to 1 or 2, a file containing model predictions will also be saved at the end of every epoch. With eval_verbose_print true, model predictions will also be written in the log file. If you set --eval_verbose to 2, model beam predictions for \"perfect answers\" (i.e. when the top-1 prediction is the solution from the test file) are exported, if set to 1, the beam is not logged for perfect predictions.\nWhen problem solutions can only take a small number of positive values (e.g. modular arithmetic with small moduli, greatest common divisors), setting --export_pred true will cause Int2Int to report model predictions for all desired outputs up to --max_class. This can provide insights about what the transformer is doing [5].\nFinally, --eval_only, associated with --reload_model will cause the model to evaluate the model and return, without training. Alternatively, you can provide the path of the experiment you want to test in --eval_from_exp. Int2Int will then reload the model saved as best_validation.pth, where validation is the criterion passed as --validation metrics, or checkpoint.pth (i.e. the last checkpointed model) if it does not exist."}, {"title": "Reading from files", "content": "Int2Int can read its training and test data from a file. The path to the training file must be passed in --train_data. If a training file is used, the --reload_data_size first examples in the file will be used (all examples if --reload_size is set to -1). The path to the test data should be defined in --eval_data. it is possible to evaluate the model on several test sets, by adding several paths separated by a comma. The first file will be the validation file, the next ones the test files. Evaluation results will be computed on the first --eval_data_size examples of all test sets (all examples if --eval_data_size is -1), and the performance metrics will be prefixed by the name of the evaluation set (valid, test, test2, etc.). If no path is provided, training and/or test data will be generated on the fly.\nBy default, all training examples are loaded in memory when Int2Int initializes. On machines with limited memory, this may runtime errors, or a significant slow down (due to lack of memory and caching). Setting --batch_load true will cause Int2Int to load the entire training file in batches of --reload_size examples. The training examples will be used in order, until all training data is read, and a new batch is loaded.\nWhen --batch_load is set to false, the data loader creates batches by randomly picking examples from the dataset. By default, examples are uniformly sampled, which means every training example has the same probability of being used. In [12], we showed that increasing repetition on a random subset of training examples can greatly improve model performance. This can be enabled by setting --two_classes true. Examples from the first --first_class_size of the training set will then be selected with probability --first_class_prob. These two parameters allow one to adjust the repetition levels in the two samples.\nInt2Int file format. The data files read by Int2Int are plain text files. Each line contains one example, with tokenized input and output separated by a tabulation. Tokens are written as strings separated by spaces. For instance, the pair (10, 12) and its GCD 2, written in base 10, would be saved in trhe training file as + 10 + 12<TAB>+ 2. In base 1000 it would be represented as + 10 + 12<TAB>+ 2. For Int2Int to be able to process the file, all tokens used must be known to the tokenizer. By default, Int2Int knows all integers from 0 to B \u2013 1, where B is the --base (1000 by default), the signs + and -, separators <sep, (and), and some special tokens <SPECIAL_O> to <SPECIAL_9>, which you can redefine as you please. The list of symbols known to the system is defined in input_encoder.symbols and output_encoder.symbols, in src/envs/arithmetic.py and src/envs/encoder.py."}, {"title": "Data generation", "content": "When no training or evaluation datasets are specified, the model generates, and tokenizes, its train and test data. The code for data generation must be added to files src/envs/generator.py and src/envs/encoders.py (see sections 4.3 and 4.1). However, a few generators and tokenizers are provided as examples. The problem to be solved is defined by the parameters --operation. At present, 10 arithmetic operations are proposed. All integers are tokenized as sequence of digits in base --base, preceded by a sign token(+ or -) that also serves as a separator.\n\u2022 matrix_rank: compute the rank of an integer matrix, of dimension --dim1 \u00d7 --dim2, with entries in [- maxint, maxint] (defined by parameter --maxint.)"}, {"title": "Model architecture", "content": "The default architecture implemented in Int2Int is the sequence-to-sequence transformer [19]. It is configured by default by setting --architecture to encoder_decoder, and includes a bidirectional encoder, and an autoregressive decoder, both multi-layer transformers, connected by a cross-attention layer. In effect, the encoder transforms the input sequence into some latent representation, and the decoder outputs the prediction, token by token, as a function of the latent representation of the input (accessed via the cross-attention) and the previously decoded output."}, {"title": "Optimizers", "content": "During training, the gradient of the (cross-entropy) loss function is accumulated over all examples presented to the model before an optimization step is performed. By default, this is done on one batch of --batch_size examples. If the model runs on several GPU, the gradients from each GPU are accumulated, and one optimization step processes n times the batch size (n the number of GPU). For large models processing long sequences, the GPU memory needed to compute gradients may grow large, and cause the batch size to be very low. This can be mitigated by setting --accumulate gradients to a value larger than one. The gradient computation are then done"}, {"title": "Extending Int2Int: generators, verifiers and tokenizers", "content": "This section deals with adapting Int2Int to a new math problem (for which you do not have pre-calculated training data). This requires adding, or modifying, the following components.\n\u2022 The generator, a descendent of class Generator or Sequence, from src/envs/generators.py, which is responsible for generating pairs of problems and solutions (class function generate()) and verifying model predictions (class function evaluate()).\n\u2022 Two tokenizers, descendents of class Encoder in src/envs/encoders.py for the input and output, which are responsible for encoding problems and solutions into sequences of tokens (function encode()), and decoding sequences into problems and solutions (functionparse()).\nOnce a generator and two tokenizers are defined, all you have to do is to define them in the __init_() function of ArithmeticEnvironment (in src/env/arithmetic.py), by such code as:\nif params.operation == 'my_operation':\nself.generator = generators.MyGenerator(params)\nself.input_encoder = encoders.MyIEncoder(params)\nself.output_encoder = encoders. MyOEncoder(params)"}, {"title": "Tokenizers", "content": "The tokenizers already present in Int2Int will prove sufficient if your input and output are either integers (or elements of a finite set that you can map to integers), or arrays of integers (vectors, matrices or tensors, or anything that can be mapped to them). Integers can be encoded either as SymbolicInts or PositionalInts. SymbolicInts(min,max,prefix='') are a finite set of tokens, mapped to integers from min to max (inclusive), and potentially prefixed by a letter. For instance, binary tokens would be tokenized as SymbolicInts(0,1,''), integers between -10 and 10 as SymbolicInts(-10,10,''), and the nodes of a graph with 20 nodes could be encoded as SymbolicInts(1,20,\u2019N').PositionalInts (base) are integers represented as strings of digits in base base, and prefixed by a sign. In base 10, 1024 would be tokenized as + 102 4.\nArrays of integers (symbolic or positional) are tokenized as NumberArray(params, max_dim, dim_prefix, tensor_dim, code). code indicates how the array elements are encoded (pos_int or symbolic). tensor_dim is the dimension of the array (1 for vectors, 2 for matrices). The array will be prefixed by tensor dim tokens indicating its dimensions: a vector of 5 elements will be prefixed"}, {"title": "Verifiers and additional metrics", "content": "During the evaluation phase, test examples are first evaluated by comparing the predicted sequence with the target sequence. The number of such perfect matches is reported as valid_arithmetic_perfect. If the match is not perfect, the function src/envs/arithmetic.py/check_prediction() is called, which first verified the predicted sequence decodes as a valid mathematical object (i.e. an integer, or a sequence of integers, and not a random sequence such as + + 1 -). The number of well-formed prediction (correct or not), is reported as valid_arithmetic_correct. Finally, it calls src/envs/generator.py/evaluate(), which returns O is the prediction is incorrect, and 1 if it is correct. The perfect and correct predictions are reported as valid_arithmetic_acc. Note that when the output sequence associated to the solution to the problem is unique, correct solutions are perfect solution. in that case, evaluate() must always returns 0 (this is the default behavior).\nperfect, correct and acc are the three basic evaluation metrics in Int2Int. Additional problem-specific metrics can be introduced by calculating them in evaluate(). It returns 2 lists, the size of which are defined in--n_eval_metrics and --n_error metrics. All these metrics take values in [0, 1], are summed over all test examples, and reported as valid_arithmetic_acc_eval1,valid_arithmetic_acc_eval2, etc. and valid_arithmetic_acc_error1, valid_arithmetic_acc_error2, etc. For eval metrics, the number of perfect predictions are added to the indicator.\nThis allows the model to report \"weak successes\" such as close predictions, or correct predictions up to the sign, and specific error cases. Note that whereas eval metrics can be used with beam search (the best vaue over the beam is then reported), error metrics cannot.\nInt2Int can also report accuracy on subgroups of the test set, via the function src/envs/arithmetic.py/code_class( It computes, for each test example, an integer representing its class. During evaluation, an accuracy"}, {"title": "Generators", "content": "The generate() function, in src/envs/generator.py handles the creation of data examples, for training, evaluation, or data generation(i.e. when setting --export_data true). When working on a new problem, it will have to be rewritten, unless one learns and evaluate from an external data file. generate() uses an external random number generator - not the default Python or NumPy generator. This prevents different instances of the generator (iwhen the model runs on several GPU, or uses several worker CPU) to generate the exact same data. The parameter type can be set to the dataset you are generating train or valid, for training and evaluation. This allows test examples to be drawn from a different data distribution than training examples.\ngenerate() returns two mathematical objects, the problem and solution, to be fed into the tokenizers. The typical input is a list (or an array) of integers, the output could be an array, a list or a single integer. If generation fails, the function can return None, this (or any exception) will cause generate() to be called again."}, {"title": "Visualizing results", "content": "A Jupyter Notebook is provided in tools/ReadXP.ipynb. It provides basic functions to read output logs, build result tables and learning curves. You need to define the dump_path (as in Int2Int), the exp_name of the experiments you are investigating, and the sterr_path where the stderr output of your runs are being saved (assumed to be of the fors stderr_path/exp_name/*/. The notebook will the collect all train.log files in these directories, and print tables and learning curves."}]}