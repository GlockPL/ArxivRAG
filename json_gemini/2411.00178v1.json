{"title": "Clinical Evaluation of Medical Image Synthesis: A Case Study in Wireless Capsule Endoscopy", "authors": ["Panagiota Gatoula", "Dimitrios E. Diamantis", "Anastasios Koulaouzidis", "Cristina Carretero", "Stefania Chetcuti-Zammit", "Pablo Cortegoso Valdivia", "Bego\u00f1a Gonz\u00e1lez-Su\u00e1rez", "Alessandro Mussetto", "John Plevris", "Alexander Robertson", "Bruno Rosa", "Ervin Toth", "Dimitris K. Iakovidis"], "abstract": "Sharing retrospectively acquired data is essential for both clinical research and training. Synthetic Data Generation (SDG), using Artificial Intelligence (AI) models, can overcome privacy barriers in sharing clinical data, enabling advancements in medical diagnostics. This study focuses on the clinical evaluation of medical SDG, with a proof-of-concept investigation on diagnosing Inflammatory Bowel Disease (IBD) using Wireless Capsule Endoscopy (WCE) images. The paper contributes by a) presenting a protocol for the systematic evaluation of synthetic images by medical experts and b) applying it to assess TIDE-II, a novel variational autoencoder-based model for high-resolution WCE image synthesis, with a comprehensive qualitative evaluation conducted by 10 international WCE specialists, focusing on image quality, diversity, realism, and clinical decision-making. The results show that TIDE-II generates clinically relevant WCE images, helping to address data scarcity and enhance diagnostic tools. The proposed protocol serves as a reference for future research on medical image-generation techniques.", "sections": [{"title": "Introduction", "content": "Sharing retrospectively acquired data is crucial for clinical training and research, yet privacy regulations, such as the General Data Protection Regulation (GDPR)\u00b9, pose significant challenges. Synthetic Data Generation (SDG) solves these barriers, revolutionizing clinical diagnostics for improved patient safety. Currently, SDG research predominantly utilizes generative Artificial Intelligence (AI) models, such as Generative Adversarial Networks (GANs)\u00b2 and Variational Autoencoders (VAEs)\u00b3, to produce synthetic versions of original clinical data4\u20137. Application domains include Computed Tomography (CT), Magnetic Resonance Imaging (MRI) and endoscopy8\u201311. To date, SDG has mainly been considered as a technical means of enhancing the performance of AI-based Clinical Decision Support (CDS) systems. Such systems exploit knowledge that can be automatically extracted from large amounts of annotated retrospective data to assist clinicians in decision-making, e.g., detecting or identifying suspicious lesions in medical images.\nOnly a few studies have addressed synthetic image generation from a clinical viewpoint10,12\u201317, most involving clinicians performing Visual Turing Tests (VTTs) that focus on discriminating between real and synthetic images. Previously, VTT procedures have been used to assess the plausibility of generated X-ray15, CT16, MRI17 and retinal14 images compared to the real ones. Fewer studies have clinically evaluated the results of endoscopic image generation methods with more thorough protocols10,13. Yoon et al13 applied a GAN-based method18 for the generation of colonoscopic images containing sessile serrated lesions, and four experts evaluated the quality of synthetic images using a three-point Likert scale in addition to performing a typical VTT. The more recent study of Vats et al10 assessed the same generation methodology on a publicly available dataset, and then eight medical experts applied a threefold evaluation protocol to assess synthetic images produced. In addition to distinguishing synthetic images from real images, this protocol measured the difficulty medical experts have in performing the VTT, and compared the degree of realism of the images with a sorting procedure. Also, it assessed the plausibility of endoscopic findings in synthetic image sequences by employing a four-point Likert scale of likelihood ranging between \"very unlikely\u201d to \u201cvery likely\u201d. However, not all the experts participated in the evaluation tasks performed.\nCurrent evaluation protocols for synthetic image generation are limited to assessing only synthetic image quality or plausibility. More importantly, while existing protocols rely on the experience of medical experts to draw reliable results, they need to uncover the underlying factors influencing the clinicians' decision-making process. An additional aspect overlooked in related studies concerns the diagnostic accuracy of recognizing pathological conditions in the generated images from a clinical perspective. Although the concept of identifying pathological conditions in synthetic images is assessed through data-augmentation techniques applied in the training of AI classification algorithms, this aspect is not evaluated in the existing clinical evaluation protocols. It should be noted that the protocols used in previous studies are limited to evaluating only the methodologies proposed in these studies, and do not include any clinical comparisons with other image generation methodologies.\nThis study performs a proof-of-concept investigation on Inflammatory Bowel Disease (IBD) diagnosis based on evidence from Wireless Capsule Endoscopy (WCE) examinations. The selection of this domain is mainly motivated by the currently low diagnostic yield of WCE reported in clinical studies\u00b9\u2079. Synthetic WCE images can improve the diagnostic yield through lifelong clinical training and AI-based CDS systems' performance. WCE is a medical imaging modality that has become the prime choice for small-bowel examinations20. WCE uses a small swallowable capsule, with the size of a large vitamin pill, equipped with at least one colour camera, which, when swallowed, captures high-quality images of the gastrointestinal (GI) tract. Although WCE is effective, it requires significant time from experienced physicians to review the captured video, typically 60-90 minutes\u00b2\u00b9, which can lead to human error due to fatigue, lowering the overall accuracy of the examination19. To overcome that, CDSs have been used to alleviate this issue22,23. Yet, their performance is impacted by either the small number of images available for training or the class imbalance which characterizes WCE datasets. Most SDG methods used to battle this problem focus on abnormalities, such as polyps10, characterized by homogeneity in texture and colour. A few SDG methodologies 24,25,12 have been proposed to replicate more complex conditions, such as IBD. Manifestations of IBD include various lesions, such as erythema, erosions, and ulcers, characterized by variations in colour, texture and anatomical structure. Despite these advancements, in the context of IBD, the quality of generated images has yet to be assessed through systematic clinical studies and has only been evaluated using typical VTT procedures10,12,13.\nThis study addresses this gap by presenting a protocol for medical experts' systematic evaluation of synthetic images. The proposed protocol includes qualitative and quantitative aspects assessing the synthetic images' quality, variety, and plausibility regarding texture, anatomical structures, and diagnostic relevance. Furthermore, this study applies the proposed evaluation protocol to TIDE-II, which is the second version of our recently proposed deep learning architecture called \u2018This Intestine Does Not Exist' (TIDE)\u00b9\u00b2. This revised version of the TIDE architecture is an enhanced generative model based on a variational autoencoder (VAE). It significantly improves upon the original TIDE\u00b9\u00b2 by generating high-resolution images that are four-fold larger than those produced in our previous study. This advancement allows for a broader variety of plausible images, showcasing the enhanced capability of the model. Unlike prior studies, this research incorporates a comprehensive qualitative evaluation involving an international group of 10 medical experts. These experts provided detailed assessments of the synthetic images, comparing various SDG methodologies and evaluating the images based on specific criteria of the proposed protocol, including texture, anatomical accuracy, and diagnostic difficulty.\nIn summary, this paper contributes to the field of medical image synthesis by proposing:\n\u2022\tA protocol for the Clinical Evaluation of Medical Image Synthesis (CEMIS) by physicians. CEMIS can serve as a reference for future research in medical image generation, setting a first standard for assessing synthetic data quality and its applicability in clinical practice.\n\u2022\tA novel VAE architecture, TIDE-II, which draws inspiration from architectural concepts of Visual Transformers (ViTs)\u00b2\u2076 and can generate more realistic high-resolution images.\n\u2022\tA thorough clinical evaluation of the generated images by 10 experts with different levels of expertise demonstrates the utility of the proposed protocol and the effectiveness of TIDE-II over the relevant state-of-the-art architectures in the context of WCE for IBD lesion detection."}, {"title": "Results", "content": "Overview of Datasets\nThe clinical evaluation presented in this study was conducted on two publicly available WCE image datasets, namely, KID 2\u00b2\u2077 and Kvasir-Capsule datasets\u00b2\u2078. KID 2 includes 2,371 fully annotated colour images with a resolution of 360\u00d7360 pixels, captured from the entire GI tract using the MiroCam\u00ae (IntroMedic Co., Seoul, Korea) capsule. It has a subset of 728 images illustrating healthy tissue of the small bowel and 593 images with pathological findings, out of which 227 represent inflammatory lesions. The Kvasir-Capsule dataset includes 47,238 weakly annotated colour images with a resolution of 336\u00d7336 pixels, captured using the Olympus ENDOCAPSULE 10 System with Olympus EC-S10 capsule. It has a subset of 34,338 images illustrating healthy tissue of the small-bowel, and 4,266 images with pathological findings, out of which 1,519 represent inflammatory lesions. The two described datasets were used to train TIDE-II and state-of-the-art generative models to produce synthetic normal and abnormal (with inflammatory lesions) WCE images.\nMedical experts\nA group of 10 WCE specialists with 5-27 years of clinical experience reviewing WCE videos participated in clinically evaluating the synthetic images. Three had less than 10 years of experience, four had 10-20 years of experience, and three had over 20 years of experience. The CEMIS protocol involves 5 assessment procedures (A1-A5). In each procedure, the experts provided feedback while observing one or more images without any time limit. Once the experts had submitted their feedback, they were not able to reconsider or change their replies. All the experts participated in all assessment procedures.\nIndividual Assessment of Real and Artificially Synthesized Images (A1)\nA set of 50 WCE images (generated by TIDE-II) with a balanced distribution of samples in terms of type (real/synthetic), category (normal/abnormal), and origin (KID/Kvasir), was randomly selected and used for the first assessment procedure (A1) of the CEMIS protocol. In the first task (A1.TI), the experts were classified the images as real or synthetic. Considering real images as positive and synthetic images as negative, the results of this task, in terms of mean accuracy, sensitivity and specificity, are summarized in Table 1. Further analysis of these results based on the experts' years of experience in WCE reviewing is presented in Fig. 1a. Experts with 10-20 years of experience performed best in accuracy and sensitivity. Supplementary Fig. 1 illustrates images correctly classified by all experts of the same experience groups.\nThe experts' predictions were evaluated using a Chi-Square test for goodness-of-fit, assuming that any differences in the rates of correct and incorrect predictions between real and synthetic images could be attributed to a random variation rather than a systematic effect. The overall probability of correct predictions for real images was 0.65, with a 95% confidence interval ranging from 0.59 to 0.71, whereas for synthetic images, it was 0.70, with a 95% confidence interval ranging from 0.64 to 0.76. The Chi-Square test yielded a p-value of 5.76\u00d710-14, which is significantly less than 0.05. However, for three out of ten medical experts, one from each group based on the years of experience, the p-values were greater than 0.05, indicating no statistically significant differences in frequencies of correct and incorrect predictions between the real and synthetic images. This suggests that the images evaluated in this procedure were challenging for 30% of the experts.\nIn the second task (A1.T2), the clinicians rated the difficulty level regarding their choice in the previous task, i.e., if the image presented was real or synthetic. The assessments of the experts are summarized in Figs. 2a and 2b. The real images (Fig. 2a) were considered as difficult to classify by the least experienced and the most experienced experts, and as easy by the experts with 10-20 years of experience. Synthetic images were regarded as easy by experts with less than 10 years of experience, as difficult for those with 10-20 years, and as neutral by experts with over 20 years of experience.\nIn the third task (A1.T3), the experts explained the reasons behind their decision on whether the images evaluated were real or synthetic. Their assessments are summarized in Fig. 2c. Color and texture affected most of the experts' decisions. Furthermore, both real and synthetic images were associated with the existence of artifacts or luminal content and with the existence of unrealistic anatomical structures in their content.\nIn the fourth task (A1.T4), the experts classified the images into normal or abnormal (containing inflammatory lesions). Considering abnormal images as positive, and normal images as negative, the results of this task in terms of accuracy, sensitivity and specificity are summarized in Table 2. It can be noticed that the classification was more accurate in the case of the real images, and that the lower accuracy observed in the case of the synthetic images was mainly due to the higher false-negative classifications. The false negatives could be attributed to the normal synthetic images containing more conspicuous structures than the real ones. The classification performance of the experts on real and synthetic images, accordingly, with respect to their experience, is provided in Figs. 2d and 2e. These figures validate that the overall observations are consistent across the different experience groups. In the fifth task (A1.T5), the medical experts evaluated the displayed image quality. The quality of most of the real images (Fig. 2f) was rated as acceptable, and the quality of most of the synthetic images (Fig. 2g) was rated as slightly acceptable by all the experts regardless their experience. Supplementary Figs. 2 and 3 illustrate representative results from both real and synthetic images, rated as acceptable and slightly acceptable by the expert groups and the experts as a whole. Observing the synthetic images rated as acceptable and slightly acceptable, the experts converged to that both preserve the clinical features of real images, with ulcers (2b, 3a, and 3c), erosions (2a), and normal mucosa (2d, 3c, 3d) represented best."}, {"title": "Individual Assessment of Synthetic Images (A2)", "content": "In the second assessment procedure (A2) of the CEMIS protocol, a total of 50 synthetic WCE images (generated by TIDE-II) with a balanced distribution of samples in terms of category (normal/abnormal), and origin (KID/Kvasir), was randomly selected for evaluation by the medical experts. In the first task (A2.TI), the experts classified the images presented as real or synthetic. The mean accuracy of the predictions performed by the clinicians was 46.60% \u00b1 6.54% (Table 1). This indicates that 53.40% of the images generated by TIDE-II from both datasets were incorrectly perceived as real, whereas 46.60% were correctly identified as synthetic. An overview of the classification performance of the medical experts in procedure A2 in comparison with their performance in Al upon their experience is included in Fig. 1b, where it can be noticed that it is significantly lower. Supplementary Fig. 4 illustrates synthetic images misperceived as genuine by all the medical experts of the same experience groups.\nConsidering that the average accuracy obtained by the medical experts was below 50.00% in this assessment, a Binomial test was performed to determine whether the observed predictions were statistically significantly lower than expected under the assumption of no systematic advantage attributed to the artificial origin of synthetic images. For the 10 medical experts, the test yielded a p-value of 0.07 with a 95% confidence interval ranging from 0.42 to 0.51. Moreover, all the p-values of the clinicians were more significant than 0.05 (ranging between 0.84 and 0.06). These results suggest that there is no statistically significant evidence to indicate that the synthetic images produced by the TIDE-II model are biased towards being perceived as artificial.\nFigure 3a summarizes the experts' responses concerning the difficulty level in predicting if the images displayed were real or synthetic (task A2.T2). These results indicate that TIDE-II generates images that are challenging to be distinguished as synthetic. Synthetic images generated by TIDE-II were considered difficult to identify as synthetic by experts with less than 20 years of experience. The opinion of the most experienced clinicians (>20 years of experience) was more divergent, rating them as easy and difficult at similar rates. Figure 3b presents the reasons (task A2.T3) behind the medical experts' decision regarding the first task of this assessment. It can be noticed that in total, 17.80% of the synthetic images were associated with the unrealistic appearance of anatomical structures, and this was the main characteristic that drove the experts to correctly identify the synthetic images, since it appears in 38.20% of the correct predictions and 0.00% of the incorrect predictions.\nTable 2 summarizes the results of the experts in classifying the synthetic images as normal or abnormal (task A2.T4). It can be noticed that the mean sensitivity estimated for the images generated based on the KID dataset was significantly higher than that of the Kvasir dataset. In contrast, in the case of Kvasir-based synthetic images, the mean specificity was considerably higher than that of KID-based synthetic images. This implies that the KID-based abnormal synthetic images can be better recognized as abnormal, and the normal Kvasir-based synthetic images can be better recognized as normal. The comparison of the overall performance of the experts in relation to their years of experience is presented in Fig. 3c, where it can be noticed that the most experienced clinicians were more accurate in this task.\nThe results from the quality evaluation of the images generated by TIDE-II (task A2.T5) are presented in Fig. 3d. It can be observed that synthetic images were rated as acceptable by the experts with 5-10 years of experience and as moderately acceptable by the experts with 10-20 years of experience. However, the clinicians with over 20 years of experience rated the quality of synthetic images as slightly acceptable at a percentage of 30.00%. In contrast, the quality of most synthetic images (42.00%) was rated as acceptable and very acceptable."}, {"title": "Individual Assessment of Real Images (A3)", "content": "In the third assessment procedure (A3) of the CEMIS protocol, the medical experts evaluated solely real WCE images. A set of 50 real WCE images with a balanced distribution of samples in terms of category (normal/abnormal) and origin (KID/Kvasir) was randomly selected. As in the previous assessment procedures, in the first task (A3.TI), the clinicians decided whether the images presented were real or synthetic. The mean accuracy was 66.40 \u00b1 11.88% (Table 1). An overview of the classification performance of the medical experts on both datasets with respect to the years of experience is included in Fig. 1b.\nA two-sided Binomial test was applied to evaluate whether the predictions were statistically significant from what would be expected under the assumption of no systematic advantage attributed to the appearance of real images. The test yielded a p-value of 1.90\u00d710-13 for the ten experts, which is significantly lower than 0.05, with a 95% confidence interval ranging from 0.62 to 0.70. For four out of ten experts (one expert with 5-10 years of experience, two with 10-20 years of experience, and one with more than 20 years of experience), the test yielded a p-value greater than 0.05 These results suggest that there is statistically significant evidence that real images have a distinguishing advantage, but this effect varies among different experts based on their experience.\nFigure 4a shows the level of difficulty, based on the experts' opinions, in predicting whether the images displayed were real or synthetic (task A3.T2). In most cases, the experts with 5-10 years of experience characterized distinguishing real images as a task of neutral difficulty. It is shown that the overall opinion of the more experienced groups of experts tends to have a more balanced distribution with similar levels of difficulty for easy and difficult categories. These results indicate that real WCE images exhibit diverse perceptions for their difficulty level.\nFigure 4b analyzes the reasons behind the experts' decision regarding whether the images presented were real or synthetic (task A3.T3). Although all the images examined were real, there were images (11.40%) in which the experts characterized some anatomical structures as having an unrealistic appearance. The diagram shows that this was the main reason for the experts' incorrect classification of the real images (33.93%) as synthetic.\nTable 2 summarizes the results of the experts' classification of real images as normal or abnormal (task A3.T4), and Fig. 4c presents the performance of experts in relation to the years of their experience. It can be noticed that the experts performed better overall with the real images than with the synthetic ones. Figure 4d summarizes the results of the quality of the real images (task A3.T5), where it can be observed that it has been mainly considered as acceptable from the different groups of experts."}, {"title": "Paired Image Assessment (A4)", "content": "This assessment procedure involved pairwise evaluation of the synthetic images generated by TIDE-II with the real images. Two new subsets of 50 synthetic and 50 real images, with a balanced distribution of samples in terms of type (real/synthetic), category (normal/abnormal), and origin (KID/Kvasir), were randomly selected and provided for evaluation to the experts as pairs of images composed by one synthetic and one real image.\nIn the first task (A4.TI), the experts were identified which was the real image of each pair displayed. The mean accuracy obtained based on the clinicians' predictions was 66.82\u00b116.04%. Aiming to determine the statistical significance of this result, a two-sided Binomial test was performed. The results obtained show that, for all experts, the overall predictions were statistically significant (p-value<0.05). In addition, for 50% of the experts (two experts with 5-10 years of experience, two with 10-20 years of experience, and one with more than 20 years of experience), the discrimination against real images was quite challenging (p-value>0.05). This suggests that for half of the experts, the synthetic images generated by TIDE-II were difficult to distinguish from the real images when placed side-by-side.\nIn the second task (A4.T2), the clinicians specified the difficulty level concerning the assessments submitted in the previous task. Figure 5a presents the assessments collected. It can be noticed that, in total, the experts from all experience groups rated more than 65% of the image pairs examined as difficult and very difficult. This indicates that TIDE-II synthesizes images that are hard to distinguish next to the real WCE images. Supplementary Fig. 5 presents the image pairs that were characterized as the most challenging ones.\nIn the third task (A4.T3), the experts' motivation in deciding which was the real image in each pair was investigated. An overview of the assessments submitted is presented in Fig. 4b. The experts attributed their choice mainly to the texture and color properties. The rates associated with the realistic appearance of findings and anatomical structures to the content of the images were similar to each other. In contrast, the percentage of images related to the absence of artifacts was significantly lower. Furthermore, it is worth noting that in cases where the synthetic images were misperceived as real (incorrect predictions), it was attributed to the absence of artifacts and the existence of findings with a realistic appearance at rates of 10.27% and 32.88%, respectively, in contrary to the cases where the predictions where correct and the above reasons were rated at 8.84% and 20.07% respectively. This implies that the images synthesized by TIDE-II tend to be considered equivalent to the real ones for similar reasons concerning their clinical appearance.\nMoreover, the clinicians classified the images of each pair as normal or abnormal when representing pathological findings (tasks A4.T4a, A4.T4b). At the same time, the origin, i.e., real or synthetic, of the images remained undisclosed. Abnormal images were regarded as positive predictions, whereas normal images were considered as negative predictions. The respective results are included in Table 2, where it can be noticed that the experts' performance was comparable for the real and the synthetic images. A comparison of the total performance of the clinicians in relation to the years of their experience is provided in Figs. 5c (real images) and 5d (synthetic images). The classification results indicate that TIDE-II generates normal and abnormal images with distinct features that contribute to the discrimination of pathological and non-pathological conditions when compared side-by-side with real images.\nThe last tasks (A4.T4a, A4.T4b) of paired image assessment evaluated the quality of images included in each pair. Figures 5e and 5f summarize the results of the evaluation. The quality of real images (Fig. 5e) and synthetic images quality (Fig. 5f) were mainly considered acceptable by experts with 5-10 years and over 20 years of experience, respectively. The experts with 10-20 years of experience evaluated the quality of real and synthetic images as very acceptable in most cases."}, {"title": "Evaluation of Diversity and Realism (A5)", "content": "In the last part of the CEMIS protocol, the experts evaluated the realism and the diversity of both real and synthetic images that were provided to them in different image groups. This was the fifth assessment procedure (A5), which included 600 randomly selected images composed of two parts: a part with 300 synthetic images and a part with 300 real images. Each part had a balanced distribution of samples in terms of category (normal/abnormal) and origin (KID/Kvasir). The part with the synthetic images was composed of 6 different subsets of 50 images generated using state-of-the-art SDG methods, three GAN-based (StyleGANv2, CycleGAN, TS-GAN), and three VAE-based (EndoVAE, TIDE, and TIDE-II). The images were organized into 60 image groups, 30 groups with real images and 30 with synthetic ones, balanced in terms of category and origin. Each group was composed of 10 randomly selected images. The origin of the image groups presented to the experts remained undisclosed during this assessment.\nA detailed analysis of the realism between the synthetic images generated from various state-of-the-art generative models, including VAE-based and GAN-based methodologies, is presented in Fig. 7. Most synthetic normal images generated by EndoVAE and TIDE, were evaluated as slightly realistic (Fig. 7a). Moreover, only 7.50% of synthetic normal images from EndoVAE and 10.00% from TIDE were evaluated as realistic, while the corresponding percentage in TIDE-II exceeded 20.00%. More than 50% of the normal images generated by GANs, were considered as not realistic and slightly realistic (Fig. 7b). Regarding synthetic abnormal images, TIDE and EndoVAE were considered to synthesize slightly realistic images (Fig. 7c). Abnormal images from StyleGANv2 and CycleGAN were regarded as not realistic, whereas only 15.00% of the TS-GAN abnormal images were regarded as realistic (Fig. 7d). It can be observed that none of the CycleGAN normal and abnormal images were rated as realistic. Additionally, according to the opinion of the experts, only TIDE-II generates very realistic abnormal images among all state-of-the-art methodologies.\nFigure 8 presents a detailed analysis of diversity assessment between synthetic images generated by VAE-based and GAN-based models. It is shown that TIDE-II generates less normal images rated as not diverse compared to previous VAEs (Fig. 8a). Additionally, neither EndoVAE nor TIDE normal images were rated as very diverse. Normal images from TS-GAN and StyleGANv2 were considered as not diverse and slightly diverse, accordingly (Fig. 8b). Normal images from CycleGAN were considered as diverse.\nRegarding abnormal synthesis, images from EndoVAE and TIDE models were assessed as slightly diverse (Fig. 8c). In contrast, the images synthesized by TIDE-II were better rated as diverse, with a difference of 28.75% and 33.75% over EndoVAE and TIDE, respectively. Abnormal images from StyleGANv2 and CycleGAN were rated as not diverse, whereas those from TS-GAN were rated as slightly diverse (Fig. 8d). However, neither StyleGAN nor CycleGAN produced diverse abnormal images. In total, according to the opinion of medical experts, StyleGANv2 and TS-GAN generated neither normal nor abnormal diverse images."}, {"title": "Discussion", "content": "Medical image synthesis has stimulated significant research interest; however, the evaluation of such images has yet to be systematically studied, primarily due to the lack of domain-specific metrics that access both the quality and clinical relevance of the synthetic images. To this end, in this study, we introduced the CEMIS protocol for evaluating synthetic medical images, which is applied in the context of WCE image generation for IBD diagnosis. Furthermore, we introduced a novel VAE-based architecture, named TIDE-II, for synthetic WCE image generation. We evaluated its performance compared to state-of-the-art WCE image synthesis methodologies using the proposed CEMIS protocol.\nThe results of the proposed CEMIS protocol indicate that synthetic images produced by TIDE-II are hard to distinguish from real ones. This difficulty was particularly evident in the individual image assessment (A1), where all experts consistently described the task as challenging (Figs. 2a and 2b). This difficulty was further corroborated by the evaluations conducted in assessments A2 and A3, where TIDE-II images were consistently more challenging to identify as synthetic than real images. Furthermore, the paired assessment (A4), where real and synthetic images were presented side-by-side, was also very challenging, with experts across all groups finding it difficult to distinguish the synthetic images generated by TIDE-II from the real ones (Fig. 5a).\nAcross all assessments (A1-A4), we observed that experts primarily relied on low-level properties like color and texture, rather than higher-level features such as anatomical findings. Although in assessment Al, only the synthetic images were associated with unrealistic anatomical structures and artifacts (Fig. 2c), this was not validated in the individual assessment of solely real images (A3), which shows that real images also exhibited artifacts and unrealistic structures present in 17.00% and 11.00% of the cases, respectively. The paired assessment (A4) further revealed that both real and synthetic images exhibited similar levels of artifacts when compared side-by-side, with TIDE-II images occasionally displaying more realistic features (Fig. 5b). These findings indicate that even real datasets include images with inherent flaws or characteristics that can misperceived and affect the experts' judgement to characterize them as non-plausible. The image synthesis models subsequently replicate these, and since they are not so common in the real training datasets, their synthesis can be less accurate. Regarding the artifacts, Supplementary Fig. 8 illustrates representative cases of artifacts detected in both synthetic and real images. Observing these images, the experts noted that the representation of the non-mucosal areas, such as debris and bubbles, or the areas under them, is not sufficiently realistic, which makes them distinguishable from the real ones, e.g. the whitish area in the center of Fig. 8a, the mucosa behind the debris on the left part of Fig. 8b, and the debris in the lower right part of Fig. 8d.\nIn both real and TIDE-II synthetic images, experts were more accurate in the identification of normal than abnormal images. This can be attributed to the nature of IBD findings. Inflammatory lesions include a variety of findings of different size such as erosions, erythema, ulcers etc. Additionally, the appearance of these lesions may have colour gradations and vary in shape as inflammation conditions can be flat or excavated.\nAn additional aspect examined in the proposed clinical protocol was the quality of both real and synthetic images. The results demonstrated that the quality of real images was mainly evaluated as acceptable in the assessments of our protocol (A1, A3, A4). More specifically, in individual assessment of solely TIDE-II images (A2), most of the synthetic images were rated as acceptable and very acceptable (Fig. 3d), and in paired assessment (A4), TIDE-II images were considered as acceptable or very acceptable. Only in individual image assessment (A1), where both real and synthetic images were present, the quality of TIDE-II images was rated as slightly acceptable (Fig. 2g).\nUnlike relevant WCE image synthesis assessment protocols, CEMIS allowed a comprehensive comparative assessment of state-of-art GAN-based and VAE-based endoscopic synthesis methodologies by evaluating both the realism and diversity of the synthetic images in relation to real ones (A5). The results demonstrate that TIDE-II outperforms previous models, generating more realistic and diverse images (Figs. 7 and 8). Although the images generated by CycleGAN were noted for their higher diversity, their plausibility was significantly lower than that of the images generated by VAE-based methodologies. This discrepancy arises because GAN-based methods tend to reuse features from their training set when generating images, which, in the context of WCE image synthesis, affect the plausibility of the synthetic images (Fig. 7b) due to the limited meaningful combinations of these reused features. While GAN models have significantly contributed to the advances in medical image synthesis, especially in domains where the content is aligned within the images, such as in MRI and CT images, recent studies question their effectiveness in the context of endoscopy.\nIt can be observed that the synthetic images produced by TIDE-II are more diverse than the real ones, and there is a narrow margin for deep generative algorithms to reach the plausibility of real images (Fig. 6). To this end, future research in deep generative methodologies for endoscopic image synthesis could focus on rethinking the role of the low-level image features, such as the color and texture, which, according to this study, seem highly correlated with the clinical relevance in endoscopic images. This investigation could improve the quality of synthetic images without compromising their clinical relevance.\nIn summary, this study applied the proposed CEMIS protocol for a proof-of-concept investigation assessing TIDE-II as a generative model for WCE image synthesis. The study involved ten WCE specialists - more than in any previous study - who comprehensively evaluated endoscopy images in the context of IBD. The results indicate that TIDE-II outperforms previous GAN-based and relevant VAE-based synthesis methodologies. The findings of this study validate, from a medical perspective, the effectiveness of VAE-based methods towards generating clinically relevant endoscopy images. Considering the multilevel evaluation enabled by the proposed protocol, we believe it can serve as a reference point for future research in synthetic medical image assessment of other imaging modalities."}, {"title": "Methods", "content": "Clinical Protocol for Synthetic Image Assessment\nThis study presents a protocol that comprehensively assesses the plausibility and clinical relevance of synthetic medical images generated by AI algorithms based on clinicians' feedback. It extends previous relevant protocols12", "parts": "the first part (A1-A3) assesses individual images; the second part (A4) enables the pairwise comparison of images; the third part (A5) assesses the properties of groups composed of multiple images. Table 3 summarizes the tasks involved in the five assessment procedures, detailed in the following paragraphs.\nIndividual Image Assessment\nThe first assessment procedure (Al) considers an equal number of real and synthetic images randomly presented one by one to clinicians without giving them any information about the images. For each image, the clinicians must complete five tasks (A1.T1-5). In the first task (TI), clinicians they have to identify if the image is real or synthetic (T1.01-02). This is followed by a second task (T2) that assesses the difficulty level for their choice using a five-point Likert scale (T2.01-05). T2 aims to explore the degree to which the images are correctly classified as real or synthetic and to ascertain to what extent the images synthesized by deep generative models can be considered realistic.\nThe third task ("}]}