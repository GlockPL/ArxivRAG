{"title": "GENERALIZING WEISFEILER-LEHMAN KERNELS\nTO SUBGRAPHS", "authors": ["Dongkwan Kim", "Alice Oh"], "abstract": "Subgraph representation learning has been effective in solving various real-world\nproblems. However, current graph neural networks (GNNs) produce suboptimal\nresults for subgraph-level tasks due to their inability to capture complex interac-\ntions within and between subgraphs. To provide a more expressive and efficient\nalternative, we propose WLKS, a Weisfeiler-Lehman (WL) kernel generalized for\nsubgraphs by applying the WL algorithm on induced k-hop neighborhoods. We\ncombine kernels across different k-hop levels to capture richer structural infor-\nmation that is not fully encoded in existing models. Our approach can balance\nexpressiveness and efficiency by eliminating the need for neighborhood sampling.\nIn experiments on eight real-world and synthetic benchmarks, WLKS signifi-\ncantly outperforms leading approaches on five datasets while reducing training\ntime, ranging from 0.01x to 0.25x compared to the state-of-the-art.", "sections": [{"title": "INTRODUCTION", "content": "Subgraph representation learning has effectively tackled various real-world problems (Bordes et al.,\n2014; Luo, 2022; Hamidi Rad et al., 2022; Maheshwari et al., 2024). However, existing graph neural\nnetworks (GNNs) still produce suboptimal representations for subgraph-level tasks since they fail\nto capture arbitrary interactions between and within subgraph structures. These GNNs cannot cap-\nture high-order interactions beyond and even in their receptive fields. Thus, state-of-the-art models\nfor subgraphs have to employ hand-crafted channels (Alsentzer et al., 2020), node labeling (Wang\n& Zhang, 2022), and structure approximations (Kim & Oh, 2024) to encode subgraphs' complex\ninternal and border structures.\nAs an elegant and efficient alternative, we generalize graph kernels to subgraphs, which measure\nthe structural similarity between pairs of graphs. We propose WLKS, the Weisfeiler-Lehman (WL)\nKernel for Subgraphs based on WL graph kernel (Shervashidze & Borgwardt, 2009). Specifically,\nwe apply the WL algorithm (Leman & Weisfeiler, 1968) on induced k-hop subgraphs around the\ntarget subgraph for all possible ks. The WL algorithm's output (i.e., the color histogram) for each k\nencodes structures in the receptive field of the k-layer GNNs; thus, the corresponding kernel matrix\ncan represent the similarity of k-hop subgraph pairs. A classifier using this kernel can be trained\nwithout GPUs in a computationally efficient way compared to deep GNNs.\nTo enhance the expressive power, we linearly combine kernel matrices of different k-hops. The\nmotivation is that simply using larger hops for WL histograms does not necessarily lead to more\nexpressive representations. We theoretically demonstrate that WL histograms of the (k + 1)-hop\nare not strictly more expressive than those of k-hop in distinguishing isomorphic structures, while\n(k+1)-hop structures include entire k-hop structures. Therefore, combining kernel matrices across\nmultiple k-hop levels can capture richer structural information around subgraphs.\nHowever, sampling k-hop subgraphs can increase the time and space complexity, as the number of\nnodes in the k-hop neighborhoods grows exponentially (Hamilton et al., 2017). To mitigate this\nissue, we choose only two values of k: 0 and the diameter D of the global graph. No neighborhood\nsampling is required for the case where k = 0 since it only uses the internal structure. When k is set\nto the diameter D, the expanded subgraph encompasses the entire global graph, making the k-hop\nneighborhood identical for all subgraphs. Consequently, there is no need for explicit neighborhood"}, {"title": "RELATED WORK", "content": "WLKS is a 'graph kernel' method designed for 'subgraph representation learning.' This section\nexplains both of these areas and their relationship to our model.\nSubgraph Representation Learning Subgraph representation learning can address various real-\nworld challenges by capturing higher-order interactions that nodes, edges, or entire graphs cannot\nmodel. For example, subgraphs can formulate diseases and patients in gene networks (Luo, 2022),\nteams in collaboration networks (Hamidi Rad et al., 2022), and communities in mobile game user\nnetworks (Zhang et al., 2023). Existing methods are often domain-specific (Zhang et al., 2023; Li\net al., 2023; Tr\u00fcmper et al., 2023; Ouyang et al., 2024; Maheshwari et al., 2024) or rely on strong\nassumptions about the subgraph (Meng et al., 2018; Hamidi Rad et al., 2022; Kim et al., 2022; Luo,\n2022; Liu et al., 2023), limiting their generalizability.\nRecent deep graph neural networks designed for subgraph-level tasks can apply to any subgraph\ntype without specific assumptions. However, they often generate suboptimal representations due to\ntheir inability to capture arbitrary interactions between and within subgraph structures. They strug-\ngle to account for high-order interactions beyond their limited receptive fields; thus, they should\nincorporate additional techniques including hand-crafted channels (Alsentzer et al., 2020), node\nlabeling (Wang & Zhang, 2022), random-walk sampling (Jacob et al., 2023), and structural approx-\nimations (Kim & Oh, 2024). In contrast, we design kernels that can capture local and global inter-\nactions of subgraphs, respectively, to enable simple but strong subgraph prediction. We formally\ncompare our proposed WLKS with representative prior models in Appendix A.\nGraph Kernels Graph kernels are algorithms to measure the similarity between graphs to\nenable the kernel methods, such as Support Vector Machines (SVMs) to graph-structured\ndata (Vishwanathan et al., 2010). Early examples measure the graph similarity based on random\nwaks (Kashima et al., 2003) or shorted paths (Borgwardt & Kriegel, 2005). One of the most in-\nfluential graph kernels is the Weisfeiler-Lehman (WL) kernel (Shervashidze & Borgwardt, 2009),\nwhich leverages the WL isomorphism test to refine node labels iteratively, improving the expres-\nsiveness of the graph structure comparison. While the WL test is designed for graph isomorphism,\nWL kernels capture structural similarities using the WL test's outcomes even when graphs are not\nstrictly isomorphic (See Appendix B for detailed comparison). Kernels for graph-level prediction\nby counting, matching, and embedding subgraphs have been deeply explored (Shervashidze et al.,\n2009; Kriege & Mutzel, 2012; Yanardag & Vishwanathan, 2015; Narayanan et al., 2016). However,\nthere has been no research on kernels to solve subgraph-level tasks by computing the similarity of\nsubgraphs and their surroundings. To the best of our knowledge, our paper is the first to investigate\nthis approach."}, {"title": "WL GRAPH KERNELS FOR SUBGRAPH-LEVEL TASKS", "content": "This section introduces WLKS, the WL graph kernels generalized for subgraphs. We first describe\nthe original WL algorithm and its extension for subgraphs, which is a foundation of WLKS. Then,\nwe suggest WLKS and its enhancement of expressiveness and efficiency. Finally, we introduce how\nto integrate continuous features with WLKS models."}, {"title": "SUBGRAPH REPRESENTATION LEARNING", "content": "We first formalize subgraph representation learning as a classification task. Let $G = (V, A)$ represent\na global graph, where $V$ denotes a set of nodes (with $|V| = N$) and $A \\subset V \\times V$ represents a set of\nedges (with $|A| = E$). A subgraph $S = (V_{sub}, A_{sub})$ is a graph formed by subsets of nodes and edges\nin the global graph $G$ (with $|V_{sub}| = N_{sub}$ and $A_{sub} | = E_{sub}$). There exists a set of $M$ subgraphs,\nwith $M < N$, denoted as $\\mathcal{S} = \\{S_1, S_2, ..., S_M \\}$. In a subgraph classification task, the model learns\nrepresentation $h_i \\in \\mathbb{R}^{F}$ and the logit vector $y_i \\in \\mathbb{R}^{C}$ for $S_i$ where $F$ and $C$ are the dimension size\nand the number of classes, respectively."}, {"title": "1-WL ALGORITHM FOR K-HOP SUBGRAPHS", "content": "1-WL for Graphs We briefly introduce the 1-dimensional Weisfeiler-Lehman (1-WL) algorithm.\nAs illustrated in Algorithm 1, the 1-WL is an iterative node-color refinement by updating node\ncolors based on a multiset of neighboring node colors. This process produces a histogram of refined\ncoloring that captures graph structure, which can distinguish non-isomorphic graphs in the WL\nisomorphism test."}, {"title": "1-WL for Subgraphs (WLS)", "content": "We then propose the WLS, the 1-WL algorithm generalized for\nsubgraphs. Since surrounding structures are the core difference between graphs and subgraphs, the\nmain contribution of the WLS lies in encoding the k-hop neighborhoods of the subgraph. Here, k\nwill be denoted in superscript as $WLS^k$ if a specific k is given.\nFormally, for a subgraph $S = (V_{sub}, A_{sub})$ in a global graph $G = (V, A)$, the $WLS^k$'s goal is to get\nthe refined colors of nodes in $V_{sub}$, where each color represents a unique subtree in k-hop neigh-\nborhoods. As in the Algorithm 2, we first extract the k-hop subgraph $S_k$ of $S$, which contains all\nnodes in $S$ as well as any nodes in $G$ that are reachable from the nodes in $S$ within k hops. The\n1-WL algorithm is then run on this induced k-hop subgraph to generate the colors of the nodes in\n$S_k$. The WLS returns the node coloring belonging to the original $S$, not in $S_k$. In general, k-hop\nneighborhoods are much larger than the original subgraph, so using all the colors in $S_k$ will likely\nproduce a coloring irreverent to the target subgraph."}, {"title": "WL Kernels for Subgraphs (WLKS)", "content": "Now, we suggest WLKS, the corresponding kernel matrix\n$K_{WLS}^k \\in \\mathbb{R}^{M \\times M}$ of which is defined as as the number of common subtree patterns of two subgraphs in\ntheir k-hop neighborhoods. That is, each element can be formulated as an inner product of a pair\nof $\\phi_{WLS}^k$. This WLKS is a valid kernel since $K_{WLS}^k$ is positive semi-definite for all non-negative ks, as\ndemonstrated in Proposition 3.1."}, {"title": "EXPRESSIVENESS DIFFERENCE OF THE WLS BETWEEN k AND k + 1", "content": "How do we choose k? Intuitively, selecting one large k seems reasonable since the k-hop neighbor-\nhoods include the k'-hop structures of all smaller k's. Against this intuition, we present a theoretical\nanalysis that the $WLS^{k+1}$ histogram is not strictly more expressive than the $WLS^{k}$ histogram.\nIn Proposition 3.2, we show that non-equivalent colorings of two subgraphs in $WLS^{k+1}$ do not guar-\nantee non-equivalent colorings in $WLS^k$. This is also true for the inverse: equivalent colorings in\n$WLS^{k+1}$ do not guarantee equivalent colorings in $WLS^k$. We obtain the same conclusion as Proposi-\ntion 3.2 for GNNs as powerful as the WL test (e.g., Wang & Zhang (2022)), and some recent models\nare based on even less powerful GNNs than the WL test (e.g., Kim & Oh (2024))."}, {"title": "SELECTING k FOR MINIMAL COMPLEXITY", "content": "In WLKS, selecting appropriate values of k during the k-hop subgraph sampling is crucial for bal-\nancing expressive power and complexity. As the number of nodes in the k-hop neighborhood grows\nexponentially with increasing k (Hamilton et al., 2017), an unbounded increase in k can result in\nsubstantial computation and memory overhead. To mitigate this, we strategically limit the choice of\nk to two specific values: k = 0 and k = D, where D is the diameter of the global graph G.\nWhen k = 0, the WLKS consumes the least computation and memory by using only the internal\nstructure of the subgraph without neighborhood sampling. In contrast, when k is set to diameter D,\nevery subgraph has the same k-hop neighborhood, which is the global graph G; thus, the WLS is\nperformed just once on G without per-subgraph computations. By using 0 and D, WLKS-{0, D}\ncan capture both the local and the largest global structure of subgraphs. This approach offers a\npractical model that balances expressive power and efficiency, avoiding excessive computation and\nmemory consumption from intermediate k values."}, {"title": "COMPUTATIONAL COMPLEXITY", "content": "The original WL Kernel has a computational complexity of $O (T \\sum_{i} E_{sub}^i + M T \\sum_{i} N_{sub}^i)$ for M\nsubgraphs, T iterations, and the number of nodes $N_{sub}$ and edges $E_{sub}$ of the subgraph i (Sher-\nvashidze & Borgwardt, 2009). When k is 0, a set of subgraphs is identical to a set of individ-\nual graphs, so its complexity is the same as the original's. When k is D, after performing the\nWL algorithm on the global graph once (i.e., $O(TE)$), the coloring of each subgraph is aggre-\ngated to a histogram (i.e., $O(\\sum_{i} N_{sub}^i)$). Thus, the computational complexity of WLKS-{0, D} is\n$O (T(E + \\sum_{i} E_{sub}^i) + M T \\sum_{i} N_{sub}^i)$.\nWe note that WLKS-{0, D} do not perform k-hop neighborhood sampling, which adds a complexity\nof $O(N_{sub,k} + E_{sub,k})$ per subgraph from a breadth-first search from $V_{sub}$. Learning SVM with pre-\ncomputed kernels has a complexity of $O(M^2)$ dependent on the number of subgraphs M, but this\nstep is typically secondary to the WLS in practice."}, {"title": "INCORPORATING CONTINUOUS FEATURES FOR WLKS", "content": "WLKS is designed to capture structural information but can be simply integrated with continuous\nfeatures. This section introduces four methods to incorporate continuous features for WLKS."}, {"title": "COMBINATION WITH KERNELS ON CONTINUOUS FEATURES", "content": "WLKS can be linearly combined with kernel matrices $K_X$ derived from continuous features as\nin Equation 4. This combination enables the model to account for structure and feature similarities\nbetween subgraphs. One straightforward way is to directly compute a kernel on features, which mea-\nsures the similarity between subgraphs based on their feature vectors. Another approach involves\napplying the Continuous Weisfeiler-Lehman operator (Togninalli et al., 2019) to features, generating\na kernel matrix. This operator extends the original WL framework to continuously attributed graphs.\n$\\alpha_{structure} K_{WLS-K} + \\alpha_{feature} K_X$ where $\\alpha_* \\in \\mathbb{R}^+$.\nIn both cases, the kernel matrix from continuous features tends to be denser and has a different\nscale compared to those from the WL histogram. To address this, we standardize the features before\ncreating kernels and use the RBF and linear kernels."}, {"title": "GNNS WITH THE WLKS KERNEL MATRIX AS ADJACENCY MATRIX", "content": "Another way to integrate features with WLKS is to use the kernel matrix as an adjacency matrix.\nSpecifically, we consider the WLKS kernel matrix $K_{WLS}$ as the adjacency matrix of a weighted\ngraph where subgraphs $S$ serve as nodes. The rationale for this approach is that a kernel represents\nthe similarity between data points.\nIn this graph, the edge weight between subgraphs i and j corresponds to $K_{WLS}[i, j]$. By applying\ndeep GNNs to this graph, we can leverage the expressive power of WLKS for structural information\nand the capabilities of GNNs for feature representation. For this paper, we adopt state-of-the-art\nGNN-based models, S2N+0 and S2N+A (Kim & Oh, 2024), for the graph created by WLKS-{0, D}\nas an instantiation of this approach.\nGiven the original feature $X \\in \\mathbb{R}^{N \\times \\#features}$, in S2N+0, the hidden feature $H \\in \\mathbb{R}^{M \\times \\# features}$ is\na sum of original features in the subgraph, and then a GNN on $K_{WLS} \\in \\mathbb{R}^{M \\times M}$ is applied to get\nthe logit matrix $Y \\in \\mathbb{R}^{M \\times \\#classes}$ for the prediction. S2N+A first encodes each subgraph as an\nindividual graph with a GNN, readout its output to get the hidden feature $H$, then the other GNN\non $K_{WLS}$ is applied for the prediction. Formally,\nWLKS for S2N+0: $H[i, :] = \\mathbb{1}_{|V_{sub}^i|}^{\\top} X[V_{sub}^i, :), Y = \\text{GNN}(H, K_{WLS}),$\nWLKS for S2N+A: $H[i, :] = \\frac{1}{|V_{sub}^i|} \\text{GNN}_1 (X [V_{sub}^i, :), A_{sub}^i), Y = = \\text{GNN}_2(H, K_{WLS}),$\nwhere $\\mathbb{1}_n \\in \\mathbb{R}^{n \\times 1}$ is a vector of ones. Since the kernel matrix is dense for GPUs, we sparsify and\nnormalize it using the same method in the S2N's paper."}, {"title": "EXPERIMENTS", "content": "This section outlines the experimental setup, covering the datasets, training details, and baselines.\nDatasets We employ four real-world datasets (PPI-BP, HPO-Neuro, HPO-Metab, and EM-User) and\nfour synthetic datasets (Density, Cut-Ratio, Coreness, and Component) introduced by Alsentzer et al.\n(2020). Given the global graph G and subgraphs S, the goal of the real-world benchmark is subgraph\nclassification on various domains: protein-protein interactions (PPI-BP), medical knowledge graphs\n(HPO-Neuro and HPO-Metab), and social networks (EM-User). For synthetic benchmarks, the goal is\nto determine the structural properties (density, cut ratio, the average core number, and the number of\ncomponents) formulated as a classification. Note that WLKS does not need pretrained embeddings.\nWe summarize dataset statistics in Table 1.\nModels We experiment with five WLKS-K where K is {0}, {1}, {2}, {D}, {0, D}. Coefficients $\\alpha$\nis set to 1 when one k is selected, and $\\alpha_0+\\alpha_D = 1$ for WLKS-{0, D}. We do a grid search of five hy-\nperparameters: the number of iterations (\\{1, 2, 3, 4, 5\\}), whether to combine kernels of all iterations,\nwhether to normalize histograms, L2 regularization (\\{$\\frac{2^3}{100}, \\frac{2^4}{100}, ..., \\frac{2^{14}}{100}$\\}), and the coeffi-\ncient $\\alpha_D$(\\{0.999, 0.99, 0.9, 0.5, 0.1, 0.01, 0.001\\}). When combining with kernels on continuous fea-\ntures (Equation 4), we tune $\\alpha_{feature}$ from the space of \\{0.0001, 0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25\\}\nand set $\\alpha_{structure} = 1/(1 + \\alpha_{feature})$. For fusing WLKS-{0, D} to S2N, we follow the GCNII-\nbased (Chen et al., 2020) architecture and settings presented in Kim & Oh (2024).\nBaselines We use state-of-the-art GNN-based models for subgraph classification tasks as base-\nlines: Subgraph Neural Network (SubGNN; Alsentzer et al., 2020), GNN with LAbeling trickS\nfor Subgraph (GLASS; Wang & Zhang, 2022), Variational Subgraph Autoencoder (VSubGAE; Liu\net al., 2023), Stochastic Subgraph Neighborhood Pooling (SSNP; Jacob et al., 2023) and Subgraph-\nTo-Node Translation (S2N; Kim & Oh, 2024). Baseline results are taken from the corresponding\nresearch papers.\nEfficiency Measurement When measuring the complete training time, we run models of the best\nhyperparameters from each model's original code, including batch sizes and total epochs, using\nIntel(R) Xeon(R) CPU E5-2640 v4 and a single GeForce GTX 1080 Ti (for deep GNNs).\nImplementation All models are implemented with PyTorch (Paszke et al., 2019) and PyTorch\nGeometric (Fey & Lenssen, 2019). We use the implementation of Support Vector Machines (SVMs)\nin Scikit-learn (Pedregosa et al., 2011)."}, {"title": "RESULTS AND DISCUSSIONS", "content": "In this section, we compare the classification performance and efficiency of WLKS and baselines. In\naddition, the performance of WLKS according to K is demonstrated to exhibit the usefulness of the\nkernel combination. Finally, we investigate how integrating structures and features across subgraph\ndatasets affects downstream performance."}, {"title": "A FORMAL COMPARISON WITH REPRESENTATIVE RELATED WORK", "content": "In this section, we compare WLKS with highly related prior work including Subgraph Neural Net-\nwork (SubGNN; Alsentzer et al., 2020), GNN with LAbeling tricks for Subgraph (GLASS; Wang &\nZhang, 2022), Subgraph-To-Node Translation (S2N; Kim & Oh, 2024), and GNN As Kernel (GNN-AK; Zhao et al., 2022).\nWhile SubGNN employs message-passing within subgraphs, its reliance on ad hoc patch sampling\nand its separation of hand-crafted channels (e.g., position, neighborhood, structure) introduces com-\nplexity and potential sub-optimality in information aggregation. Without requiring hand-crafted\npatch designs or sampling strategies, WLKS captures a unified and expressive structural representa-\ntion based on the theoretical rationale that structures at multiple levels are important.\nGLASS uses separate message-passing for node labels that distinguish internal and global struc-\ntures. This can enhance expressiveness by mixing representations from local and global structures\nsimilar to WLKS. However, GLASS has a limited ability to handle multiple labels in batched sub-\ngraphs; thus, a small batch size is required for GLASS. WLKS provides a generalized framework to\nrepresent fine-grained levels of structures around subgraphs, which can process multiple subgraphs\nefficiently by leveraging kernel methods.\nS2N efficiently learns subgraph representations by compressing the global graph. However, this\ncompression results in a loss of structural information and expressiveness in tasks where the global\nstructure is important. In particular, since the approximation bound of S2N depends on how many\nsubgraphs are spread out in the global graph, we cannot always expect a robust approximation. In\ncontrast, WLKS does not rely on lossy compression and can yield informative representations using\nefficient kernel methods.\nGNN-AK generates a graph representation by aggregating the information of each node's locally in-\nduced encompassing subgraph. Although there are local and global interactions, there are fundamen-\ntal differences between WLKS. First, GNN-AK is designed for graph-level tasks, so the interactions\nbetween the graph itself and its local subgraphs are modeled. However, dealing with subgraph-level\ntasks is more challenging since modeling both the inside and the outside of the subgraph is required.\nWLKS encodes them by using multiple k-hop kernels. Second, GNN-AK has a large complexity\nthat depends on the total number of neighbors and the sum of edges between neighbors, so it cannot\nbe applied to a large global graph, unlike WLKS. In fact, the average number of nodes covered by\nthe GNN-AK paper is much smaller, ranging from 25 to 430. In these perspectives, our study takes\na complementary approach to GNN-AK, addressing aspects not covered in their work."}, {"title": "DISCUSSION ON RELATIONS BETWEEN WL ISOMORPHISM TEST AND WL\nKERNELS", "content": "In this section, we provide a detailed discussion to clarify the distinctions between Weisfeiler-\nLehman (WL) isomorphism test and WL kernels, elaborating on how our work builds upon these\nfoundational concepts.\nThe WL algorithm is recognized for testing graph isomorphism by iteratively refining node labels\nto capture the structural similarity between graphs. Its ability to distinguish non-isomorphic graphs\nis often considered a benchmark for evaluating the expressiveness of graph representation methods.\nWhile isomorphism distinguishability is theoretically significant, it can be overly restrictive in prac-\ntical applications where the exact topological equivalence of graphs is not the primary concern. For\nexample, many real-world tasks involve identifying structural similarities between graphs that may\nnot be strictly isomorphic but share functional or semantic similarities.\nOur work aligns more closely with the WL kernel framework (Shervashidze & Borgwardt, 2009;\nShervashidze et al., 2011), which extends the application of the WL algorithm beyond isomorphism\ntesting. WL kernels compute graph similarity based on histogram representations of subtree patterns\ngenerated by the WL algorithm. These histograms serve as compact summaries of graph structure,\nallowing for the comparison of graphs even when they are not isomorphic. In this context, WL\nkernels prioritize capturing similarities between graphs over distinguishing isomorphic structures."}, {"title": "STEP-BY-STEP VISUALIZATION OF WLSK ALGORITHM", "content": "In Figure 5, we visualize each iteration of the $WLS^k$ algorithm using an example of Figure 1."}, {"title": "USING KERNELS OF DISTANCE OR STRUCTURAL ENCODING", "content": "It is well-known that additional structural features (often called labeling tricks, distance encoding, or\nstructural encoding) can enhance the expressiveness of message-passing mechanisms under certain\nconditions (Zhang & Chen, 2018; Li et al., 2020; Zhang et al., 2021; Dwivedi et al., 2022; Wang &\nZhang, 2022).\nWe argue that these approaches can have different effectiveness for subgraph-level tasks and kernel-\nbased methods:\n\u2022 Zero-one labeling (Zhang et al., 2021; Wang & Zhang, 2022): This binary labeling (as-\nsigning 0 to internal nodes and 1 to external nodes) shows limited expressiveness when\naggregating labels to histograms as kernel inputs. Its histogram is represented as a length-2\nvector (0 or 1), which only counts the number of nodes inside and outside the subgraph,\nthereby omitting finer structural details.\n\u2022 SEAL's Double-radius node labeling (Zhang & Chen, 2018): SEAL computes distances\nwith respect to target structures (e.g., links) and can be applicable to k-hop neighborhoods"}, {"title": "", "content": "of subgraphs but computationally challenging. While efficient for link prediction tasks due\nto the smaller size of enclosing subgraphs, extending this approach to general subgraphs\nbecomes infeasible due to the computational overhead of calculating all pairwise distances.\n\u2022 Distance Encoding (DE) (Li et al., 2020) and Random Walk Structural Encoding\n(RWSE) (Dwivedi et al., 2022): DE uses landing probabilities of random walks from nodes\nin the node set to a given node, and RWSE uses diagonal elements of random walk matrices.\nIn this line of work, random walk matrices are shown to encode structures in an expressive\nway, even on large-scale graphs.\nWe linearly combine WLKS-{0, D} with an inner product kernel of RWSE (the walk length of 1 \u2013\n64) sum-aggregated per subgraph, yielding a significant performance boost on the Cut-Ratio dataset\n(from 60.0 to 96.0). However, this improvement is not shown across the other seven benchmarks we\ntested. We leave investigating which specific node labeling methods are effective, which aggrega-\ntions of node labels are effective, and which kernels (e.g., linear, polynomial, RBF) best complement\nthe specific node labeling as future work."}, {"title": "CONCLUSION", "content": "We proposed WLKS, a simple but powerful model for subgraph-level tasks that generalizes the\nWeisfeiler-Lehman (WL) kernel on induced k-hop neighborhoods. WLKS can enhance expressive-\nness by linearly combining kernel matrices from multiple k-hop levels, capturing richer structural\ninformation without redundant neighborhood sampling. Through extensive experiments on eight\nreal-world and synthetic benchmarks, WLKS outperformed state-of-the-art GNN-based models on\nfive datasets with reduced training times-ranging from \u00d70.01 to \u00d70.25 compared to existing mod-\nels. Furthermore, WLKS does not need pre-computation, pre-training, GPUs, or extensive hyperpa-\nrameter tuning.\nOur method offers a promising and accessible alternative to GNN-based approaches for subgraph\nrepresentation learning, but some tasks can still benefit from incorporating continuous features. We\nleave as future work the seamless integration of WLKS with Graph Neural Networks to leverage the\nexpressive power of both structures and features."}]}