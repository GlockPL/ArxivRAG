{"title": "Empowering Domain-Specific Language Models with Graph-Oriented Databases: A Paradigm Shift in Performance and Model Maintenance", "authors": ["Ricardo Di Pasquale", "Soledad Represa"], "abstract": "In an era dominated by data, the management and utilization of domain-specific language have emerged as critical challenges in various application domains, particularly those with industry-specific requirements. Our work is driven by the need to effectively manage and process large volumes of short text documents inherent in specific application domains. By leveraging domain-specific knowledge and expertise, our approach aims to shape factual data within these domains, thereby facilitating enhanced utilization and understanding by end-users. Central to our methodology is the integration of domain-specific language models with graph-oriented databases, facilitating seamless processing, analysis, and utilization of textual data within targeted domains. Our work underscores the transformative potential of the partnership of domain-specific language models and graph-oriented databases. This cooperation aims to assist researchers and engineers in metric usage, mitigation of latency issues, boosting explainability, enhancing debug and improving overall model performance. Moving forward, we envision our work as a guide AI engineers, providing valuable insights for the implementation of domain-specific language models in conjunction with graph-oriented databases, and additionally provide valuable experience in full-life cycle maintenance of this kind of products.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of artificial intelligence has witnessed a paradigm shift with the advent of Large Language Models (LLMs). These advanced models, capable of processing and generating human-like text at an unprecedented scale,"}, {"title": "2 Domain-specific LLM solutions for industry-specific document processing and querying", "content": "Generative AI and LLMs are supposed to have a huge impact transforming work across industries [1]. Domain-specific (DS) LLM solutions are well suited for industry-specific applications for many reasons. Among them, we can mention (1) specialized vocabulary: this kind of applications need to apply a specific jargon and technical terms for dealing with complex industry processes which were not included in original foundational model training. Usually text/audio/videos ingested in these solutions make extensive use of DS language. DS LLMS are trained or tuned on data from the particular industry, enabling them to better"}, {"title": "3 Knowledge Graph", "content": "The concept of KG has evolved over time [5], there isn't a single work in literature that definitively defined KG. Instead, the idea has emerged gradually through various contributions in the fields of AI, databases, semantic web, and information retrieval. In [7] authors provide a survey on potential definitions of KG. Since knowledge bases emerged more than a decade ago as fully func-tional products [14], we will take this source as the most important influence in"}, {"title": "Knowledge acquisition", "content": "Knowledge acquisition [13] is central for our case: the creation and updating of the KG. LLMs can efficiently assist on this task by easily extracting entities and labeled relations between them. Graph-oriented databases (GODB) are necessary in our case to provide a sustainable environ-ment for this large database. Additionally, most foundational models can han-dle specific GODB query languages, such as Cypher expressions for a Neo4j database. This process can be integrated into a data pipeline framework using data engineering techniques. There are key points AI engineer should take into account during this phase:\nPrompt engineering during ingestion Prompt engineering techniques enable both the extraction of knowledge from documents and the construction of the KG in the GODB. The fundamentals for the extraction and generation prompt can be summarized as follows: (1) Role: This sets a statement of the role the LLM agent should play, for example, \"You are an assistant in the data science team of a construction materials company aiming to extract information from the min-utes of meetings between its sales representatives and clients, with the goal of capturing that knowledge in knowledge graphs in Neo4j\". (2) Instruction: This provides specific instructions to build the KG, such as \"Could you construct a knowledge graph considering the following premises? (a) It's important to break down the data that can be extracted from the objectives, meeting summaries, and topics discussed. (b) [...]\". (3) Output format: It's crucial to define the specific output needed for the data pipeline processing workflow, for instance, \"Format the output only with the Cypher statements necessary to create the graph, con-sidering that several nodes may already exist in the graph\". In our experiments, we verified that this zero-shot/few-shot approach was able to achieve high per-formance, with error rates ranging from 3% to 4% in Cypher output. Although these results seem to be satisfactory, AI engineers should consider several areas for improvement:"}, {"title": "3.1 Agentic Workflow", "content": "One common issue in this process is the assumption that LLMs are capable of consistently producing excellent results in zero-shot mode. Similar to human reasoning, it's often necessary to iterate repeatedly to generate valuable insights. Fortunately, there is a growing trend to approach these solutions in an Agentic Workflow style [11], which outperforms any zero-shot approach by far. It's im-portant to keep in mind that many iterations for knowledge extraction and KG building will likely be needed, each potentially requiring a different approach or focus. Al engineers should carefully design this task."}, {"title": "3.2 KG Instruction-tuning", "content": "The instruction section of the prompts is crucial to ensuring success in this pro-cess. It's not easy to define these instructions a priori. Both domain knowledge and data analysis are required to accurately define this set of instructions. For example, in KG, it's crucial to label relations accurately. The language used in our case may vary from person to person. One important approach is to normal-ize relation names as early as possible. While it's possible to normalize them in another stage of the pipeline, the sooner this is done, the better. One common approach is the exploratory approach, where AI engineers run the extraction with production data and shape the KG accordingly. Shaping the KG involves normalizing entity and relation names, standardizing and unifying units (such as money, weight, distance, etc.), and refining the set of instructions for improved performance. It's important to find the right trade-off between a static graph schema and a free-form one. If a static graph is defined, there may be no differ-ence between using a GODB or another type of database. However, a completely free and denormalized graph schema may result in low performance or pose a risk of hallucinations."}, {"title": "3.3 Instruction meta-data", "content": "During KG instruction-tuning, the refined set of instructions may often expand significantly beyond token limits for a prompt. Maintaining a large instruction set within the prompt can result in issues during LLM execution, such as high costs, low accuracy, and potential hallucinations. In such cases, we recommend creating an instruction KG that embeds the necessary knowledge for feature extraction from documents and the creation of the data KG. A Retrieval-Augmented Gen-eration (RAG) [10] process is needed to be defined over instruction KG in order to let the LLM follow instructions on a particular document. needs to be defined over the instruction KG to enable the LLM to follow instructions on a particular document. The emergence of frameworks like langchain [3] has simplified RAG processes as well as GODB integration."}, {"title": "3.4 Quality metrics for the ingestion process", "content": "The framework we've established for the ingestion process allows for the in-corporation of quality metrics for KG construction. Once AI engineers define a specific use case, the KG can undergo validation with subject matter experts (SMEs), enabling the creation of automated test suites comprising input documents and KG outputs. These test suites facilitate regression testing, en-suring the preservation of functionality over time. Similar to quality assurance in software engineering, it's imperative to maintain these test suites consistently. GODB supports AI engineers in identifying discrepancies in the generated graphs when matches aren't perfect. Additionally, engineers should recognize that LLMs excel at explaining these graph differences and their semantics."}, {"title": "4 Retrieval-Augmented Generation with KG and GODB", "content": "Once a KG on data is created, numerous advanced analytics paths become avail-able: we can extract insights directly from the GODB itself, or we can construct intelligent agents with LLMs to leverage the KG, as in our case. The importance of applying RAG on GODB instead of building vector embeddings of minutes (in our case) is based on the fact that many user questions can result in a large number of vector matches after running cosine vector similarity match (or any other vector matching function). Just imagine questions like \"give me the vol-ume of cement or concrete sales lost due to humidity issues in 2023\": probably, a large number of vectors can emerge as candidates, so the standard solution is not suitable for our case. The Langchain framework has introduced a highly main-tainable approach to working with RAG [6], with a focus on the consumption side. We will explore three possible patterns:"}, {"title": "4.1 RAG with GODB", "content": "Prompt engineering patterns can be implemented to answer user questions using KG in GODB. The Langchain framework can assist AI engineers in implementing a chain of prompts (via chain, agent, or tooling) where the user prompt can be split into short pieces, each of which can be translated to Cypher. Each query can then be executed in Neo4j, and finally, the LLM can interpret the query responses, combine them, and elaborate a conclusive answer [6]. A good use case for this pattern consists of numerical questions, grouping logic, etc. These types of questions can be accurately answered from the perspective of a graph rather than through the elaboration of large amounts of text. Obviously, this approach outperforms RAG over plain text embeddings."}, {"title": "4.2 RAG with vector index and GODB", "content": "This approach, referred to as Hybrid Retrieval for RAG [6], combines the bene-fits of RAG with GODB and classic vector index embeddings. In this case, the"}, {"title": "4.3 RAG with embedded vectors in GODB", "content": "The most advanced technique is to integrate embeddings (vectors) into the graph. Neo4j supports vector embeddings in the graph, enabling not only simi-larity search but also graph operations. Chunks (vectors) become a specific type of nodes connected to the extended KG. Therefore, vector filtering is powered by graph operations, which may traverse paths inside the graph or compute com-plex algorithms using the GODB's high-performance engine to determine which vectors better match the user prompt, outperforming standard cosine similarity search. The possibilities of this approach extend beyond the scope of this work and represent an open research field."}, {"title": "5 Enhanced Explainability", "content": "LLMs have introduced an issue regarding explainability. Machine Learning ex-plainability traditionally addresses non-black box logic. However, complex neu-ral networks have introduced behaviors that are more difficult to explain. One potential approach to address explainability in LLMs is through the prompt en-gineering pattern called Chain of Thought (CoT). It constitutes a prompt engi-neering technique aimed at enhancing the reasoning capabilities of large language models (LLMs) by decomposing complex problems into intermediate steps. This strategy enables the model to allocate increased computational resources to each step, thereby enhancing the accuracy and efficiency of problem-solving.\nKnowledge graphs serve as structured representations of facts and their in-terconnections, allowing AI systems to retrieve information by traversing the graph to locate relevant nodes (entities or concepts) and edges (relationships) corresponding to the query. This approach offers a subset of the graph as a foun-dation for fact-checking in LLMs, rendering the process both explainable and grounded, and incorporating hierarchical relationships. GODB play a pivotal role in enabling this capability."}, {"title": "6 Memory and Context", "content": "In LLM solutions, memory is only introduced when implementing a chatbot. In such instances, the typical approach involves incorporating memory context"}, {"title": "7 Factuality for avoiding Hallucinations", "content": "Hallucinations pose a significant challenge for LLM applications, and consider-able efforts are underway in industry-specific domains [15]. A useful initial step in assessing the risk of hallucinations is to evaluate the factual adherence of our application's responses. When utilizing KGs supported by GODB, leveraging GODB to assess factual adherence is advisable.\nWhile this approach is effective for analytical purposes, it may incur a latency penalty if performed online. Consider a model output R, which is then split into chunks requiring verification with the assistance of an LLM. We have a sequence of short sentences $R_1, R_2, \u2026, R_n$. For each $R_i$ sentence, factual adherence can be checked against other $R_k$ chunks (where k < i) with the help of an LLM. The remaining R sentences can be analyzed against our KG with LLM assistance, which may provide support for these statements. Although this process requires exceptions to be added to the base metric model, it enables the calculation of a factual-adherence metric, serving as a Hallucination Risk indicator. Moreover, this process can be easily automated."}, {"title": "8 Conclusion and future directions", "content": "This study has highlighted the significance of the collaboration between GODB and LLMs in developing domain-specific solutions for industry-specific appli-cations. Drawing on our consulting experience in this field, we emphasize the importance of considering the partnership between GODB and LLMs as a pri-mary approach for implementing Generative AI solutions."}]}