{"title": "Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation", "authors": ["Juncheng Ma", "Peiwen Sun", "Yaoting Wang", "Di Hu"], "abstract": "Audio-Visual Segmentation (AVS) aims to achieve pixel-level localization of sound sources in videos, while Audio-Visual Semantic Segmentation (AVSS), as an extension of AVS, further pursues semantic understanding of audio-visual scenes. However, since the AVSS task requires the establishment of audio-visual correspondence and semantic understanding simultaneously, we observe that previous methods have struggled to handle this mashup of objectives in end-to-end training, resulting in insufficient learning and sub-optimization. Therefore, we propose a two-stage training strategy called Stepping Stones, which decomposes the AVSS task into two simple subtasks from localization to semantic understanding, which are fully optimized in each stage to achieve step-by-step global optimization. This training strategy has also proved its generalization and effectiveness on existing methods. To further improve the performance of AVS tasks, we propose a novel framework Adaptive Audio Visual Segmentation, in which we incorporate an adaptive audio query generator and integrate masked attention into the transformer decoder, facilitating the adaptive fusion of visual and audio features. Extensive experiments demonstrate that our methods achieve state-of-the-art results on all three AVS benchmarks. The project homepage can be accessed at https://gewu-lab.github.io/stepping_stones.", "sections": [{"title": "1 Introduction", "content": "In the real world, audio is consistently associated with its sources, enabling hu- mans to locate sound sources based on what they hear. In the past few years, this phenomenon has spurred significant research on Audio-Visual Localization"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Audio-Visual Localization", "content": "Audio-Visual Localization (AVL) aims to predict the location of sound sources within a video, with results typically presented as coarse heatmaps [1,11]. Prior investigations [26, 29] have employed Audio-Visual Correspondence as a self- supervised learning objective, leveraging the inherent alignment between au- dio and visual features along the temporal dimension. Further advancements by [2,30] have introduced contrastive learning, utilizing positive and negative in- stances to enhance sound source localization, emerging as a dominant approach in recent years. To tackle the complexities of multi-source localization, [27] pro- posed a two-stage training framework incorporating objectives ranging from image-level to class-level granularity. The method facilitates the progressive re- finement of multi-source localization, serving as a foundational approach in this field. These pioneering studies have significantly influenced our proposal of a two-stage framework, achieving a step-by-step optimization from localization to semantic understanding for Audio-Visual Semantic Segmentation."}, {"title": "2.2 Audio-Visual Segmentation", "content": "In contrast to AVL, Audio-Visual Segmentation (AVS) represents a more fine- grained task introduced by [38]. AVS offers pixel-level annotations for sound source localization, aiming to achieve a detailed understanding of audio-visual scenes, encompassing both single-source and multi-source scenarios. Further- more, Audio-Visual Semantic Segmentation emerges as a more challenging task [37], recently proposed as an extension of AVS, with the goal of attaining fine-grained sound source localization and semantic comprehension simultaneously. Existing methods primarily rely on the fusion of visual and audio features. AVS- Bench [38] is the first framework that employs multimodal feature fusion to achieve pixel-level dense predictions. Subsequent advancements [6,12] have in- troduced an audio-queried transformer structure to establish global contextual dependencies, thereby mitigating limitations of the receptive field inherent in convolutional approaches. Other methods include the incorporation of temporal contexts to capture audio-visual spatial-temporal correlations [15], the utiliza- tion of bi-directional generation to establish robust correlations [8], and the integration of generative models to facilitate audio-visual fusion [23]. Addition- ally, several works [20,31] leverage the power of large foundation models such as SAM [14] to implement audio-visual segmentation and enhance model general- ization. However, previous approaches have overlooked the intricacies inherent in the AVSS task, employing end-to-end models that face challenges in comprehen- sively mastering both audio-visual alignment and semantic comprehension. Con- sequently, we propose a two-stage strategy, aiming to decompose the AVSS task into two simple subtasks from localization to semantic understanding, thereby facilitating incremental progress toward enhanced model performance."}, {"title": "2.3 Semantic Segmentation", "content": "Semantic segmentation is a fundamental task in computer vision, involving the assignment of semantic labels to each pixel in an image. Early methodolo- gies [22, 28] employ an encoder-decoder architecture and adopt per-pixel dense prediction, which entails downsampling to aggregate global semantic features during encoding and upsampling to recover local detailed features during decod- ing. With the advent of vision transformers [5, 21], recent works [3, 13, 16] have strived for universal segmentation, including semantic segmentation, instance segmentation, and image segmentation, outperforming previous specialized mod- els. Among them, Mask2Former [3] proposes per-mask classification, which pre- dicts numerous masks and assigns semantic labels to each one. This approach not only unifies semantic segmentation with other image segmentation tasks but also achieves noteworthy performance. Motivated by these advancements, we propose a novel model using per-mask classification for audio-visual segmenta- tion designed to adaptively encode audio queries and dynamically capture visual features using masked attention to attain state-of-the-art performance."}, {"title": "3 Methods", "content": "In this section, we will first introduce our proposed AAVS framework in Sec. 3.1, including the Adaptive Audio Query Generator, which dynamically fuses audio features with object queries, and the transformer decoder with masked attention to flexibly adjust attention region of the visual feature maps for audio queries. In addition, we will illustrate our training strategy Stepping Stones in Sec. 3.2 and show methods to apply it efficiently to the AAVS model in detail."}, {"title": "Feature Extraction", "content": "Followed [15, 38], we employ pre-trained backbone as visual encoder to extract feature maps across different scales. After that, we utilize the Feature Pyramid Network [17] as a pixel decoder to aggregate feature maps of various resolutions. To integrate the global semantic and the fine-grained features, we iteratively aggregate neighboring resolution feature maps, resulting in merged multi-scale feature $F_i \\in R^{T \\times D \\times \\frac{H}{2^{i+1}} \\times \\frac{W}{2^{i+1}}}$, $i \\in \\{0,1,2,3\\}$. Here, T, D, H and W denotes the number of frames, embedding dimensions (default to 256) and the original size respectively.\nWe preprocess the audio input following previous work [31,38]. Then, we utilize VGGish [9] pre-trained on Audioset [7] and a linear projector to extract audio features as $F_A \\in R^{T \\times D}$."}, {"title": "3.1 Adaptive Audio Visual Segmentation", "content": "Adaptive Audio Query Generator. Most of existing methods [12,15,19] for generating audio queries lack discriminative features in relation to the audio, simply repeating the audio features $F_A$ and combining it with the learnable ob- ject queries to be fed into a transformer decoder then. A recent work [6] implicitly decomposes the audio features to obtain the audio queries using a transformer structure. However, due to the inherent complexity and ambiguity of audio, we believe that this method does not effectively decompose audio components and is parameter redundant.\nTherefore, we propose an Adaptive Audio Query Generator without intro- ducing extra parameters to encode adaptive audio queries, which can directly re- place the previous repeat method. Initially, we define $N_a$ learnable object queries $Q_{obj} \\in R^{N_q \\times D}$ and $N_a$ corresponding audio prototypes $P_{audio} \\in R^{N_q \\times D}$ to rep- resent $N_a$ potential sound source objects. Each $P_{audio}(i \\in [1, N_q])$ also serves as the positional embedding for $Q_{obj}$ in the transformer decoder enhancing their correlation during training, which also demonstrates that the module introduces no additional parameters.\nSubsequently, we dynamically weight $F_a$ based on the cosine similarity with each $P_{audio}$ and then integrate the results with $Q_{obj}$ to obtain audio-conditioned query $Q_a \\in R^{D}$,\n$Q_a = \\frac{<P_{audio}, F_A>}{||P_{audio}|| | | F_A | |} F_A + Q_{obj}$           (1)"}, {"title": "Transformer Decoder", "content": "In the transformer decoder, we integrate $Q_a$ and multi- scale visual features $[F_2, F_3, F_4]$ sufficiently to obtain refined $Q_{fuse}$. Specifically, the decoder consists of four stages, each comprising three transformer layers corresponding to distinct scales of feature maps. Within each layer, we first compute the cross-attention between $Q_a$ and $F_v$ using the Masked Mul- tihead Attention [3], which enables the model to focus more on the region to be predicted and thus adaptively adjust the attention range. Subsequently, we perform self-attention on $Q_a$ and feed it into the Feed Forward Network."}, {"title": "Loss", "content": "After decoding, we adopt per-mask classification [4] to generate the fi- nal segmentation predictions. Specifically, for each $Q_{fuse}$, the mask predictor combines mask feature $F_l$ to predict a binary mask, while the class predictor determines the category for this mask. Then, we use the Hungarian algorithm [3] based on the loss function described in Eq. 2 to identify the optimal set of queries that minimizes Eq. 2 and back-propagate the loss,\n$L_{main} = \\lambda_{cls} L_{cls} + \\lambda_{mask} L_{mask} + \\lambda_{dice} L_{dice}$.            (2)\nThe loss function comprises three components: $L_{els}$ represents the cross- entropy loss for mask classification, while $L_{mask}$ and $L_{dice}$ denote the cross- entropy loss and Dice loss [25] for mask prediction respectively. We introduce $\\lambda_{cls}, \\lambda_{mask}, and \\lambda_{dice}$ to weight these losses.\nAdditionally, we employ deep supervision to compute auxiliary losses for the output of each transformer decoder layer to improve performance. The total loss is presented in Eq. 3,\n$L = L_{main} + \\lambda_{aux} L_{aux}$,            (3)\nwhere $\\lambda_{aux}$ is used to weight the auxiliary loss.\nInference. For inference, $Q_{fuse}$ is fed to the mask predictor and class pre- dictor to generate mask prediction $O_{mask} \\in R^{T \\times N \\times \\frac{H}{32} \\times \\frac{W}{32}}$ and class prediction $O_{class} \\in R^{T \\times N \\times (C+1)}$, where C donates the total number of classes and 1 do- nates a extra class null. Then, $O_{mask}$ and $O_{class}$ are postprocessed to obtain $O_{pred} \\in R^{T \\times (C+1) \\times \\frac{H}{32} \\times \\frac{W}{32}}$ followed [3]. Finally, We apply bilinear interpolation to upsample $O_{pred}$ to its original size and take the argmax operation along the category dimension to obtain the final prediction mask $P \\in R^{T \\times H \\times W}$"}, {"title": "3.2 Stepping Stones Training Strategy", "content": "As mentioned above, the objective of AVSS can be viewed as the combination of AVS and SS, which necessitates the model to simultaneously learn audio-visual correspondence and semantic understanding. However, the integration of these objectives has led in insufficient learning and sub-optimal performance in previous end-to-end methods, prompting us to propose a two-stage training strategy, termed Stepping Stones, to tackle the intricate goal as depicted in Fig. 1. In the first stage, we train an AVS model supervised by binary labels to focus on sound source localization. Subsequently, in the second stage, we train an AVSS model supervised by semantic labels, leveraging the first-stage results M as stepping stones. Here, the model shifts its emphasis to semantic discrimination of sound source, with audio augmenting visual modal features to enhance performance. By decomposing the AVSS task into two stages of step- by-step learning, we further maximize the capabilities of the end-to-end model and facilitate its comprehensive training. To mitigate the impact of errors, we used ground truth labels M during training and $\\hat{M}$ exclusively during testing.\nNext, we will elaborate on how the second stage model effectively utilizes the first-stage results as stepping stones when applying the strategy to AAVS. The modified AVSS model is illustrated in Fig. 2."}, {"title": "Prior Knowledge Input as Stepping Stones", "content": "In contrast to semantic seg- mentation, the newly introduced audio modality in the AVSS task plays a crucial role in filtering out silent objects through modal alignment. Consequently, $\\hat{M}$ offers reliable prior knowledge and mitigates the need for audio-visual alignment within the second-stage model. In the second stage, the model can stand on the stepping stones and concentrate further on comprehending visual semantics.\nSpecifically, we integrate $\\hat{M}$ with the mask feature $F_l$ in the prediction head. This simple operation serves as a shortcut for the model, aiding in the easy recognition of the sounding region and enhancing its focus on the semantic discrimination of the sound source during training.\nAdditionally, we utilize $\\hat{M}$ as the initialization mask for masked attention in the transformer decoder. Previous initialization methods result in notably low accuracy of the original mask, where the initial attention mask is derived from the initial queries. An inaccurate initialization mask can hinder the effective confinement of attention to the foreground region in the cross-attention mod- ule, potentially leading to slow convergence or model degradation performance. Therefore, we naturally employ $\\hat{M}$ as prior information to initialize the attention mask, enabling queries to concentrate the region relevant to the sound source."}, {"title": "Robust Audio-aware Key Generator", "content": "To mitigate the inevitable error be- tween $\\hat{M}$ obtained from the first stage and ground truth labels M, we devised the Robust Audio-aware Key Generator to fuse audio information into feature maps $[F_2, F_3, F_4]$ as key in cross-modal cross attention to bolster the model's robustness and maximize its utilization of the stepping stones.\nThe module incorporates three learnable embeddings: $E_{silent}, E_{uncertain}, E_{sounding}$ and two thresholds $\\tau_1, \\tau_2 (\\tau_1 < \\tau_2)$. Initially, M is resized to match the feature map $F_v$. Subsequently, $\\hat{M}_{resized}$ is thresholded by $\\tau_1$ and $\\tau_2$ to produce a index mask. Then, an audio-aware embedding mask $M_{audio} \\in R^{T \\times \\frac{H}{2^{i+1}} \\times \\frac{W}{2^{i+1}} \\times D}$ is generated based on the index mask, which is then combined with the $F_v$ to derive audio-aware keys for cross attention. The entire process is illustrated in Eq. 4,\n$M_{t,i,j}^{audio} = \\begin{cases} E_{silent} & \\text{if } \\hat{M}_{t,i,j}^{resized} < \\tau_1 \\\\ E_{uncertain} & \\text{if } \\tau_1 < \\hat{M}_{t,i,j}^{resized} \\leq \\tau_2 \\\\ E_{sounding} & \\text{if } \\hat{M}_{t,i,j}^{resized} > \\tau_2 \\end{cases}$              (4)\nwhere t represents the current frame, and (i,j) denotes the coordinates of the current pixel. The module enables the queries $Q_a$ to dynamically adjust the fusion strategy with visual features based on audio-aware keys during decoding. For instance, in a guitar video scenario, while the original visual key may solely represent the corresponding region as a guitar feature, the audio-aware key can offer supplementary details regarding whether the guitar is silent, uncertain, or sounding, thereby enhancing the model's robustness and capacity."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "AVSBench-Object [38] is an audio-visual dataset for audio-visual segmenta- tion. The dataset consists of two subsets evaluating the S4 and MS3 subtasks respectively, providing pixel-level binary labels about sound sources in video. The S4 subset contains 4,932 videos, with 3,452 videos for training, 740 for vali- dation, and 740 for testing. Conversely, the MS3 subset includes 424 videos, with 286 videos for training, 64 videos for validation, and 64 videos for testing.\nAVSBench-Semantic [37] introduces the AVSS subtask as an extension of AVSBench-Object. In addition to providing semantic labels for S4 and MS3 subsets, it also expands the V2 subset. Overall, the AVSBench-Semantic dataset consists of 8,498 videos for training, 1,304 videos for validation, and 1,554 videos for testing. The target objects span 71 categories, encompassing humans, musical instruments, animals, and tools.\nEvaluation Metrics. Followed [6,38], we use the mean Intersection-over-Union (mIoU) and F-score as the evaluation metrics.\nImplementation Details. We choose the released Mask2Former [3] model with Swin-B [21] visual backbone pre-trained on ADE20k [36] as weight initialization. To effectively leverage the knowledge within the pre-trained models, we freeze most of the parameters and introduce adapters [10] for training to achieve faster convergence and improved performance. We utilize the AdamW optimizer with a learning rate of 10-4. The batch size is set to 1. We train the model on the S4 subset for 30 epochs, the MS3 subset for 50 epochs, and the AVSS subset for 30 epochs. To evaluate the effectiveness and generalization of the Stepping Stones strategy, we train and test AVSBench and AVSegformer model according to the setting of [6,38]."}, {"title": "4.2 Results and Analysis", "content": "We conduct experiments on all three subtasks (S4, MS3, AVSS) of the AVSBench dataset to evaluate the effectiveness of our method and compare it with previ- ous approaches. For fairness, we employ transformer-based backbone to extract visual features and the Audioset [7] pre-trained VGGish [9] to extract audio features for all methods.\nQuantitative Comparison. We conduct a comprehensive comparison between our AAVS model and existing methods on the AVSBench-Object dataset. The results are presented in Tab. 1. Our AAVS model outperforms previous methods in terms of mIoU and F-score for both the S4 and MS3 subtasks. Specifically, for"}, {"title": "4.3 Generalization of Stepping Stones Training Strategy", "content": "To assess the effectiveness and generalizability of the Stepping Stones training strategy, we also applied it to the AVSBench and AVSegformer models for the AVSS task with minor modifications. Without introducing any special design, we straightforwardly incorporated the sound source localization results with the mask feature in the prediction head, providing a shortcut for the model. Since the effectiveness of the Stepping Stones training strategy relies on the accuracy of the first stage results during inferring, we simulated three levels of accuracy of the first stage results as inputs to the second stage in this section. The re- sults are presented in Tab. 2, revealing that when the accuracy of the first stage results is low, the model's performance evens declines due to the dangerous stepping stones. Conversely, when the first stage results demonstrate high ac- curacy, serving as a reliable stepping stone, the performance of both methods improves. Experiments conducted above only apply the Stepping Stones strategy with minimal modifications to the original method. This underscores the pos- sibility of achieving further performance enhancements by exploring alternative approaches to leveraging the first stage results. The enhancement in performance"}, {"title": "4.4 Ablation Study", "content": "We conduct ablation experiments to validate the effectiveness of the proposed AAVS model and each key design in the Stepping Stones training strategy.\nAblation of adaptive audio queries. We initially assess the effectiveness of the Adaptive Audio Query Generator within the AAVS model. The experiments were conducted across all three subtasks, with the visual backbone frozen. The results are presented in Tab. 3. It is evident that the inclusion of this module enhances the model's performance compared to using the same audio query across all three subtasks."}, {"title": "Ablation of Stepping Stones training strategy", "content": "Finally, we conduct ex- periments to assess the effectiveness of key components in the Stepping Stones training strategy. Since the validity of several components in this section is highly correlated with the accuracy of the AVS results provided from the first stage, our experimental results shed light on the impact of actual pseudo labels and ground truth labels. The pseudo labels, inferred from the trained AAVS model, yield an IoU of 83.2% for S4 labels, 67.5% for MS3 labels, and 72.8% for V2 labels. The results of robustly encoding keys and fusing the sound source local- ization results with mask features are displayed in Tab. 4. It is evident that all components exhibit enhancements compared to the single-stage AAVS model, highlighting the significant improvement brought about by the Stepping Stones training strategy."}, {"title": "5 Conclusion", "content": "In this paper, we propose a simple yet effective progressive training strategy called Stepping Stones, where the sound source localization results obtained from the first-stage model serve as a stepping stone for the second-stage model, fa- cilitating a more seamless and comprehensive learning on AVSS subtask. The method is generalizable and directly applicable to preceding end-to-end meth- ods, thereby enhancing their efficacy in AVSS task performance. In the future, the effectiveness of our training strategy is expected to increase as sound source localization methods with greater accuracy emerge. Additionally, we propose a novel framework AAVS designed for dynamically integrating audio and visual features. Extensive experimental results substantiate the superior performance of AAVS and the Stepping Stones training strategy compared to existing state- of-the-art methods. For the future work, there is still a large gap between using sound source localization results obtained from the first stage and ground truth labels, as shown in Sec. 4. The potential of this training strategy remains ripe for exploration, with future endeavors aimed at delving deeper into enhancing the robustness of the model for stepping stones with noise."}]}