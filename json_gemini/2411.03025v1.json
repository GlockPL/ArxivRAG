{"title": "DA-MoE: Addressing Depth-Sensitivity in Graph-Level Analysis through Mixture of Experts", "authors": ["Zelin Yao", "Chuang Liu", "Xianke Meng", "Yibing Zhan", "Jia Wu", "Shirui Pan", "Wenbin Hu"], "abstract": "Graph neural networks (GNNs) are gaining popularity for processing graph-structured data. In real-world scenarios, graph data within the same dataset can vary significantly in scale. This variability leads to depth-sensitivity, where the optimal depth of GNN layers depends on the scale of the graph data. Empirically, fewer layers are sufficient for message passing in smaller graphs, while larger graphs typically require deeper networks to capture long-range dependencies and global features. However, existing methods generally use a fixed number of GNN layers to generate representations for all graphs, overlooking the depth-sensitivity issue in graph structure data. To address this challenge, we propose the depth adaptive mixture of expert (DA-MoE) method, which incorporates two main improvements to GNN backbone: 1) DA-MoE employs different GNN layers, each considered an expert with its own parameters. Such a design allows the model to flexibly aggregate information at different scales, effectively addressing the depth-sensitivity issue in graph data. 2) DA-MoE utilizes GNN to capture the structural information instead of the linear projections in the gating network. Thus, the gating network enables the model to capture complex patterns and dependencies within the data. By leveraging these improvements, each expert in DA-MoE specifically learns distinct graph patterns at different scales. Furthermore, comprehensive experiments on the TU dataset and open graph benchmark (OGB) have shown that DA-MOE consistently surpasses existing baselines on various tasks, including graph, node, and link-level analyses.", "sections": [{"title": "Introduction", "content": "Graph neural networks (GNNs), renowned for their efficacy in capturing relationships by aggregating neighborhood information, have become increasingly popular in the graph-structured data field. In recent years, GNNs have been successfully applied in various areas, such as recommendation systems (Wang et al. 2023b; Lin, Wang, and Li 2023; Luo, Liu, and Pan 2024), social networks (Zhang et al. 2022b,a), and molecular structures (Hu* et al. 2020; Yu et al. 2024a), achieving state-of-the-art performances in numerous tasks. Besides, GNNs can adjust their layers to aggregate information, making them adaptable to datasets with varying scales."}, {"title": "Related Work", "content": "Graph Neural Networks. GNNs are designed to handle graph-structured data, achieving promising performance using a message passing mechanism to aggregate information from neighbors and update node representations. Traditional GNNs that achieve state-of-the-art performance in various tasks, including GatedGCN (Li et al. 2016), GCN (Kipf and Welling 2017), GAT (Veli\u010dkovi\u0107 et al. 2018), GIN (Xu et al. 2019), and GraphSAGE (Hamilton, Ying, and Leskovec 2017). Presently, researchers are focusing on improving GNN architecture to better aggregate graph information. For example, SAGIN (Zeng et al. 2023) enhances GNNs by encoding subgraphs at different levels and inserting this information into nodes. Mao et al.(2024) proposed that the aggregation operation exhibits different effects on nodes with structural disparity. CF-GNN (Huang et al. 2024) extends conformal prediction to traditional GNNs. Despite these advancements, existing GNN models often overlook the variations in data scale within a dataset, relying on a fixed layer configuration for all graphs. This approach can hinder their performance and reduce their adaptability when dealing with graphs of diverse scales and complexities.\nMixture of Expert Models. The MoE concept can be initially traced back to the work (Jacobs et al. 1991; Jordan and Jacobs 1994), which involved training a set of independent experts that specialized in learning distinct data features. Subsequently, a series of enhancement of MoE have been proposed, such as Aljundi et al. (2017) introduced a model that determines the most pertinent expert network for a given task by employing an auto-encoder gate. Similarly, Noam Shazeer et al. (2017) presented a model that employs a sparse gating mechanism to activate only a subset of experts. Currently, the MoE module is widely applied in the computer vision (Dai et al. 2021; Yu et al. 2024b) and natural language processing (Fedus, Zoph, and Shazeer 2022; Du et al. 2022) domains. The integration of GNNs with MoE models is also advancing gradually. TopExpert (Kim et al. 2023) utilizes a clustering-based gating module to categorize input molecules. GMoE (Wang et al. 2023a) incorporates multiple experts at each layer, enabling the model to learn different hop information. G-FAME (Liu et al. 2023) introduces a MoE module that leverages an ensemble of specialized neural networks to capture diverse facets of knowledge within the realm of fairness. Link-MoE (Ma et al. 2024) utilize a range of existing linker predictors as experts, while GraphMETRO (Wu et al. 2023) introduces a novel mixture-of-aligned-experts architecture and training framework to address the distribution shift challenge."}, {"title": "Preliminaries", "content": "Notations. A graph G(A; X) consists an adjacency matrix A \u2208 {0,1}$^{n \\times n}$ and a node feature matrix X \u2208 R$^{n \\times d}$, where n denotes the number of nodes, d represents the node feature dimension, and A[i, j] = 1 indicates the presence of an edge between nodes v$_{i}$ and v$_{j}$, otherwise A[i, j] = 0.\nMixture of Experts. The MoE (Aljundi, Chakravarty, and Tuytelaars 2017; Wang et al. 2023a) comprises a collection of expert networks, denoted by E = {E$_{1}$, E$_{2}$, ..., E$_{n}$}, each with its own trainable parameters. Additionally, a gating network Q is employed to score and select the experts based on their outputs. Given the initial feature X, we denote E(X) = {E$_{i}$(X)}$_{i=1}^{s}$ as experts' output and Q(X) = {Q$_{i}$(X)}$_{i=1}^{s}$ as the gating network's, where s is the number of experts. In addition, we normalize the scores to ensure stable training of the gating network and maintain"}, {"title": "DA-MoE: Proposed Method", "content": "MoE on GNN layer\nIn graph-structured data, the GNN layer depth determines the scale of the neighborhood information that can be captured. Existing methods employ a fixed layer configuration to graphs of varying scales within a dataset. However, using a fixed number of GNN layers results in under-reaching for large graphs, while causing over-fitting for small ones, neglecting depth-sensitivity. Thus, we integrate the MoE module with the GNN layer, treating GNNs at different layers as experts. This approach enables adaptive learning and efficiently captures the varying scale of neighborhood information across different graphs. Given the i-th expert E(X) is a L-th layer GNN, we can formulate the following:\nH$_{i}^{(l)}$ = GNN$^{(l)}$(H$^{(l-1)}_{i}$, A),                                                                  (1)\nE(X)$_{i}$ = READOUT({H$_{i}^{(L)}$|v \u2208 V}),\nwhere GNN$^{(l)}$(.) is the l-th layer of the GNN backbone, with H$^{(0)}$ = X \u2208 R$^{d}$, READOUT(\u00b7) function aggregates node features from the final iteration to obtain the entire graph's representation, and E(X)$_{i}$ is the i-th expert output. By adopting distinct aggregation scales, each expert is capable of capturing distinct aspects of knowledge and patterns, thereby enhancing the overall performance of the model.\nIn graph-level tasks, the depth-sensitivity phenomenon refers to graph representations being easily influenced by the graph's scale. Similar to depth-sensitivity in graph-level tasks, node representations are also susceptible to the influence of node degrees during aggregation. High-degree nodes may receive excessive information, which introduces noise, while low-degree nodes might not gather enough information at the same scale. Due to the varying degrees within a graph, fixing the aggregation scale for a node can lead to suboptimal performance. To address this issue, DA-MoE allows each node to select a specific expert, enabling the model to dynamically adjust the aggregation scales. Unlike Eq. (1), the experts' outputs are directly obtained from the GNN layer without applying the READOUT function. Thus, these modifications enables DA-MoE to be easily applied to node and link-level tasks, which require node representation for classification or prediction.\nStructure-Based Gating Network\nThe gating network is a central component of the MoE module. By dynamically selecting a subset of experts, it enables the model to focus on the most relevant experts for a given task, thereby enhancing overall efficiency and performance. In the study (Aljundi, Chakravarty, and Tuytelaars 2017; Wang et al. 2023a) linear projection was employed to obtain decision scores. Specifically, they utilized the noise top-k gating strategy as the score function. This strategy introduces two key characteristics: sparsity and noise. Sparsity"}, {"title": "Balanced Loss Function", "content": "In the MoE module, we discovered a common issue known as mode collapse. This phenomenon is characterized by the gating network consistently selecting a few experts, leading to an imbalance during the training process. Consequently, the frequently chosen experts have high scores, leading to further preferential selection. To address these issues, we introduced two additional balanced loss functions following these works (Wang et al. 2023a; Aljundi, Chakravarty, and Tuytelaars 2017). The first loss function is designed to resolve experts' imbalanced scores, and the second the unequal probabilities problem that occurs during expert selection. The first loss function can be expressed as:\nCV(S)$^{2}$ = $\\frac{\\sum(S - \\mu)^{2}}{\\mu^{2} + \\epsilon}$,\n                                    (3)\nL$_{1}$(X) = CV ($\\frac{\\sum Q(x)}{\\Sigma(x)}$)$^{2}$                                                                 (4)\nwhere CV(.) denotes the coefficient of variation, which encourages a more uniform positive distribution, \u00b5 denotes the mean tensor of S, and \u03f5 represents a small positive constant added for numerical stability. Though the L$_{1}$ loss function can ensure that the scores given to each expert are equal, situations where a single expert is assigned scores to various graphs can still occur. Besides, the scores of experts are discrete, making it challenging to achieve a balanced scores among the experts. Thus, we implement a load balancing loss to ensure that the probability of each expert being selected is equal. We define P(X, i) as the probability"}, {"title": "Complexity Analysis", "content": "In this section, we analyze the DA-MoE model's computational complexity. As previously discussed, we replaced the traditional GNN layer with the DA-MoE one. The complexity of a l-th GNN layer can be represented as O(l \u00b7 (|E|d + nd\u00b2)), with |E|, n, and d referring to number of edges, number of nodes, and hidden dimension respectively. Since our DA-MoE model incorporates MoE modules into the GNN layers, let K represent the number of GNN layers of the selected experts. During the inference process, the complexity is O(\u2211k\u2208kk\u00b7 (|E|d + nd\u00b2))), which is related to the number of selected experts. Notably, when k = 1, the DA-MoE model's complexity is approximate to a typical fixed-layer GNN. The detailed experimental results are presented in the experiment section."}, {"title": "Experiment", "content": "In this section, we conduct the experiment on graph-level task to demonstrate that DA-MoE can effectively solve the depth-sensitivity issue. In the process of propagating node information, there is a similar issue that the degrees of the nodes can influence the nodes representation. Then, we extend this framework to node and link-level tasks. In addition, we conduct a series of experiments to validate the effectiveness of DA-MoE from various aspects.\nExperimental Settings\nBaselines. To demonstrate the effectiveness of our proposed method, we compared DA-MoE with three GNN backbone models, including GCN (Kipf and Welling 2017), GIN (Xu et al. 2019), and GatedGCN (Li et al. 2016). We also used popular MoE model like GMoE (Wang et al. 2023a). For all the datasets and tasks, we leveraged results from previous studies wherever possible. In addition, for datasets where baseline results are not provided, we conducted the experiments using the same parameters as our"}, {"title": "Graph-Level Tasks", "content": "Datasets and Metrics. For the graph-level task, we selected 13 real-world datasets from various sources, including seven datasets form the TU dataset (i.e., PROTEINS, NCI1, MUTAG, IMDB-B, IMDB-M, COLLAB, and REDDIT-B) and six from OGB (i.e., ogbg-molhiv, ogbg-moltox21, ogbg-moltoxcast, ogbg-molbbbp, ogbg-molesol, and ogbg-molfreesolv). Furthermore, these datasets comprise various domains ((i.e., biochemical, social network, and molecular) and tasks (i.e., classification and regression). For the aforementioned datasets, we strictly follow the evaluation metrics which are recommended by the given benchmarks. Specifically, for the TU dataset, we evaluated model performance based on the accuracy. For the OGB dataset, we used the area under the receiver operating characteristic curve (ROC-AUC) graph classification task, and the root mean squared error (RMSE) regression tasks.\nResults. Table 1 presents the experimental results obtained from the TU dataset. The table demonstrates that DA-MoE can be integrated with three distinct GNN backbones,"}, {"title": "Node-Level Task", "content": "Datasets and Metrics. We utilized two OGB datasets (i.e., ogbn-proteins and ogbn-arxiv) for the node classification task. Following the previous study (Hu et al. 2020; Wang et al. 2023a), we used ROC-AUC as the evaluation metric for ogbn-proteins, and accuracy for ogbn-arxiv.\nResults. Table 3 shows the experimental results from the node classification task. We observed that DA-MoE improved significantly across all datasets when compared with the single expert GIN and fixed GNN layer GMOE. Notably, DA-MoE enhanced the ROC-AUC metric by 2.30% and accuracy by 0.31% on the ogbn-proteins and ogbn-arxiv datasets. This indicates that DA-MoE allows the nodes to adaptively select aggregation scales based on their degree,"}, {"title": "Link-Level Task", "content": "Datasets and Metrics. We selected two datasets from OGB (i.e., ogbl-ppa and ogbl-ddi) for the link prediction task, and evaluated the model's performance based on HITS@N following the settings in OGB (Hu et al. 2020). This measures the ratio of positive samples ranked among the top N against negative samples. Specifically, the HITS@100 and HITS@20 metrics were used for the ogbl-ppa and ogbl-ddi datasets, respectively.\nResults. Table 4 summarizes the results, and we observed that the DA-MoE model's performance surpassed all baselines on these two datasets. Notably, DA-MoE outperformed the GIN backbone, delivering a substantial improvement of 14.94% on ogbl-ppa and a remarkable 22.96% on ogbl-ddi. Furthermore, when compared with other MoE models, DA-MoE's performance also improved significantly. The significant improvement over various baselines further demonstrate DA-MOE can effectively capture the nuances of different aggregation scales. The consistency of these enhancements across a variety of tasks and datasets underscores the model's excellent generalization capabilities."}, {"title": "Visualization", "content": "To further explore the DA-MoE model's depth-sensitivity capacity, we visualized the gating network's scores across varying graph scales. Figure 3 illustrates the mean scoring results for each expert, and we obtain the following observations: 1) Depth-Sensitivity: In the same dataset, smaller graphs tend to select shallow GNN layers. Meanwhile, large graphs prefer to use deep GNN layers to capture further neighbors' information. The experimental results further confirm depth-sensitivity characteristics in graph datasets. 2) Sparse Model: We discovered that the distribution of expert scores is uniform, and sparse experts help focus on specific data scales, thereby improving the feature capturing process. This approach improves the model's overall performance while reducing operational costs."}, {"title": "Impact of Structure-Based Gating Network", "content": "As detailed before, our gating network utilizes a GNN model which consisting structural information instead of linear projection. Figure 4 displays the study result to further evaluate the effectiveness of structure-based gating network. Based on the figure, we observe that combining the gating network with the GNN backbone consistently outperformed the model with linear projection. Specifically, the observed performance reduction in the absence of the structure-based gating network (i.e., 1.24% on PROTEINS, 3.92% on NCI1, and 0.47% on IMDB-M). In summary, this modification greatly enhances the model's ability to capture the relationships and patterns within the gating network."}, {"title": "Efficiency Analysis", "content": "Table 5 presents the running time and GPU memory usage during the inference process for the OGBG-moltoxcast and OGBG-molbbbp datasets, comparing them to two GNN backbone(i.e., GIN (Xu et al. 2019) and GCN (Kipf and Welling 2017)) and one MoE model (i.e., GMoE (Wang et al. 2023a)). From the results, we observe two important insights: 1) Time Consumption: DA-MoE displays an increase in time usage compared to each GNN backbone, but it is significantly lower than other MoE methods. This phenomenon can be explained by the fact that DA-MoE requires the computation of representations obtained from multiple GNN layers, which inevitably leads to higher time consumption. 2) Memory Usage: DA-MoE has a slightly higher GPU memory cost compared to the backbone model, but is slightly lower than GMoE. This increase is primarily due to the storage of embeddings from various GNN layers, which necessitates additional memory for storing and processing these diverse representations. In summary, although DA-MoE incurs increased time and memory usage com-"}, {"title": "Parameter Analysis", "content": "Number of Selected Experts. The varying number of top experts selection (i.e., k) enables the model to adaptively capture the patterns from various GNN layers. As shown in Table 6, we observed that the optimal performance on these datasets is attained when the selected experts are sparse compared to the total. Specifically, compare to a dense model, a notable increase in the performance metrics by 3.37%, 1.42%, and 5.64% was observed on the three datasets, respectively. This improvement underscores that sparse experts are capable of precisely capturing the aggregation information at different layers in GNNs, resulting in improved generalization.\nBalanced Loss Scale Factor. In this section, we comprehensively analyze the impact of scale factor in balanced loss. As illustrated in Figure 5, we observed that the metric initially increases as lambda rises, peaking around the scale factor at 0.001. Beyond this point, the model's performance gradually decreases. Notably, incorporating the balanced loss significantly impacted on the results, yielding a 3.69%, 2.68%, and 4.56% improvement on the three datasets' performance. Despite the adjustments to the parameter A having a certain impact on the model's performance, the results consistently outperform the scenario where x = 0. This further confirms the effectiveness of the balanced loss approach."}, {"title": "Conclusion", "content": "This study proposed DA-MoE, a novel MoE framework dedicated to addressing depth-sensitivity issue in graph-structured data. DA-MoE utilized different GNN layers as experts and allowed each individual graph to adaptively select experts. Additionally, this framework highlights two key modifications: the structure-based gating network and balanced loss function. Through comprehensive experiments"}]}