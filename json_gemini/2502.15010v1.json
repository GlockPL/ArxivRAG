{"title": "Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models", "authors": ["Mark Russinovich", "Ahmed Salem"], "abstract": "Recent copyright agreements between AI companies and content creators have highlighted the need for precise control over language models' ability to reproduce copyrighted content. While existing approaches rely on either complete concept removal through unlearning or simple output filtering, we propose Obliviate, a novel post-training technique that selectively prevents verbatim reproduction of specific text while preserving semantic understanding.\nObliviate operates by selecting tokens within memorized sequences and modifying the model's probability distribution to prevent exact reproduction while maintaining contextual understanding. We evaluate Obliviate on multiple large language models (LLaMA-3.1 8B, LLaMA-3.1-instruct 8B, Qwen-2.5-7B, and Yi-1.5 6B) across both synthetic memorization tasks and organic copyright content. Our results demonstrate that Obliviate achieves orders of magnitude reduction, e.g., 100x, in verbatim memorization while maintaining model performance within 1% of baseline on standard benchmarks (HellaSwag, MMLU, TruthfulQA, and Winogrande). This makes Obliviate particularly suitable for practical deployment scenarios where companies need to efficiently address copyright concerns in pretrained models without compromising their general capabilities.", "sections": [{"title": "1. Introduction", "content": "The rise of large language models (LLMs) has sparked intense debate about intellectual property rights in AI-generated content. Central to this debate is the fundamental question: When does an AI system's output constitute a derivative work versus an infringing reproduction? Recent legal frameworks and industry agreements have begun to converge on this distinction through specific policies targeting verbatim content reproduction(Bailey). For instance, publishers are discussing how large an excerpt would be acceptable to use(IngramSpark; Friedman). Similarly, major Al companies are starting to implement strict limits on exact text reproduction, with policies that will prevent generating copyrighted content and others that stop generating verbatim text after the generation exceeds a specific number of words.\nTraditional approaches to protecting intellectual property in LLMs have primarily relied on post-processing filters, alignment or aggressive unlearning techniques that attempt to remove copyrighted content entirely from the model. However, emerging copyright discussions/agreements seems to focus more narrowly on protecting specific expressions rather than underlying ideas or concepts. This shift suggests a more targeted technical challenge: instead of preventing models from learning concepts altogether, we need only prevent them from reproducing specific sequences verbatim-a distinction that enables more efficient solutions.\nIn this work, we propose Obliviate, a novel technique that enables selective unmemorization of exact sequences in pre-trained language models while maintaining their semantic knowledge of the text. Although recent work like the Gold-Fish loss (Hans et al., 2024) function has shown promise in preventing verbatim memorization during training, our approach uniquely addresses the critical challenge of modifying already trained models, where the vast majority of deployment scenarios lie.\nObliviate intuitively works by identifying specific tokens within the text that need to be unmemorized and applying targeted modifications. More concretely, we apply the KL (Kullback-Leibler) divergence loss to adjust the model's output distribution, reducing the probability that the target token will reproduce ($L_{forget}$) while maintaining fluency and coherence. Target tokens are selected using a sliding stride to ensure comprehensive coverage, and replacement candidates are carefully constrained to maintain the model's utility.\nFor non-target tokens -those we do not wish to unmemorize- we apply KL divergence to encourage the top-k token distribution to remain consistent ($L_{maintain}$), ensuring the model"}, {"title": "2. Background and Preliminaries", "content": "Large Language Models (LLMs) exhibit two distinct types of training data memorization:\nSemantic Memorization. The first type is semantic memorization, where models learn and internalize concepts from training data. This form of memorization is generally desirable as it enables models to understand and reason about learned concepts.\nVerbatim Memorization. The second type is verbatim memorization, where models store and can reproduce exact sequences from their training data. This form of memorization is particularly problematic for copyright protection and is the primary focus of our work. We formally define verbatim memorization in terms of a model's ability to complete sequences given partial contexts. Specifically, for a sequence $s = (s_1, ..., s_n)$, we say it is $(l, \u03b2)$-memorized if"}, {"title": "2.1. Types of Model Memorization", "content": "where i denotes the starting location of the prefix, formally defined as i ~ U[1, n \u2013 l] with U[1, n-l] denoting uniform sampling of prefix positions, and \u03b2\u2208 [0, 1] is a threshold parameter.\nFollowing Carlini et al. (2021), we simplify the memorization definition to consider greedy decoding: given a prefix $S_{i:i+l-1}$, we measure the longest sequence n that model M generates that matches the original text:\n$n^* = max \\{n : \u2200j \u2208 [i + l, i + n], s_j = arg max_{x \u2208 V} P_M(x|S_{i:j-1})\\}$\nwhere $arg max_{x\u2208y}$ represents greedy token selection from the vocabulary V. Furthermore, to ensure we thoroughly evaluate memorization, we also consider using the default temperature during generation"}, {"title": "2.2. Threat Model", "content": "We focus on copyright protection in non-adversarial settings, aiming to prevent models from unintentionally generating or being directly prompted to generate copyrighted content during normal usage. While a sufficiently motivated attacker might still be able to coerce the model into producing such content, our primary objective is to prevent routine copyright infringements."}, {"title": "3. Related Work", "content": "Our work builds upon and extends several lines of research in machine learning memorization, unlearning, and copyright protection. We organize related work into three main categories: (1) memorization in language models, (2) machine unlearning approaches, and (3) copyright protection techniques."}, {"title": "3.1. Memorization in Language Models", "content": "Understanding and measuring memorization in language models has been an active area of research. Carlini et al. (2023); Zhang et al. (2023) demonstrated that large language models can memorize and reproduce substantial portions of their training data. Kandpal et al. (2022) and Lee et al. (2022) showed that memorization is particularly prevalent in commonly used web-scraped datasets, with exact duplicates in training data significantly increasing the likelihood of memorization. These findings raise significant concerns about privacy and copyright protection in deployed language models."}, {"title": "3.2. Machine Unlearning", "content": "Machine unlearning approaches focus on removing specific data points or concepts from trained models, targeting both semantic and verbatim memorization. These approaches can be categorized into exact and approximate unlearning methods. Exact unlearning methods provide theoretical guarantees for complete removal of target sequences. Bourtoule et al. (2020) proposed an efficient retraining approach using data sharding. However, subsequent work(Chen et al., 2021) demonstrated that such methods remain vulnerable to membership inference attacks (Shokri et al., 2017; Salem et al., 2019) when adversaries have access to both the original and unlearned models. Approximate unlearning methods trade theoretical guarantees for computational efficiency. Graves et al. (2021) introduced Amnesiac Unlearning, which tracks model updates and their corresponding training batches, enabling selective removal by ignoring specific updates. Various other approximate techniques have been proposed (Guo et al., 2023; Golatkar et al., 2020; Wu et al., 2022; Mehta et al., 2022; Eldan & Russinovich, 2023), each offering different trade-offs between unlearning effectiveness and computational cost.\nWhile the recent GoldFish loss (Hans et al., 2024) similarly focuses on verbatim memorization, it operates during the training phase and cannot be directly applied to pretrained models. Our work adapts and extends these concepts for post-training modification, introducing novel techniques for maintaining model utility while targeting specific memorized sequences."}, {"title": "3.3. Copyright Protection", "content": "Recent work on copyright protection in language models has explored several approaches. Watermarking techniques (Li et al., 2023; Sablayrolles et al., 2020; Sander et al., 2024) enable detection of copyrighted content by embedding identifiable markers in training data-if a model generates text containing these watermarks, it indicates training on copyrighted content. However, these approaches focus on detection rather than prevention of copyright violations. In practice, companies primarily rely on filtering mechanisms to detect and block the generation of copyrighted content, such as Azure's content filters, and alignment techniques that teach models to not generate copyrighted content. Obliviate improves upon these approaches by modifying the model's parameters directly with a targeted loss function, making it inherently resistant to generating copyrighted content during normal usage without requiring additional computational overhead during inference from classifiers or blocklists.\nWe believe our work lies between complete unlearning and simple output filtering. While unlearning approaches aim for complete removal of concepts at significant computational cost, and filtering approaches operate only at inference time,"}, {"title": "4. Methodology", "content": "Consider a pretrained language model M with parameters 0 that has potentially memorized sensitive sequences/text T from its training data. Our goal is to modify M such that it cannot reproduce these sequences verbatim while maintaining its ability to generate contextually appropriate text, i.e., without affecting M's utility.\nThis approach differs from unlearning, where the model M is expected to completely forget the concept of T and not only avoid verbatim reconstruction of T. In fact, we aim for Obliviate to retain the concept of T, meaning the model still understands what \"Harry Potter\" is and can answer questions about it, but without reproducing verbatim text from the Harry Potter books.\nWe believe this definition of unmemorization is crucial as it simplifies the unlearning problem and aligns with how most Al companies are addressing copyright issues. For example, Anthropic is discussing making deals with copyright owners to prevent exact reconstruction of their content(Brittain)."}, {"title": "4.1. Problem Formulation", "content": "We take inspiration from the \u201cGoldFish\u201d loss (Hans et al., 2024) and adapt it to post-training, i.e., where the model is already trained and produces memorized -copyright-protected- content. Intuitively, Obliviate aims to perturb certain tokens of the memorized text T to divert the model M from outputting T, thereby generating entirely different text.\nTo achieve this, Obliviate operates by defining a stride s, which it sweeps over the complete memorized text T as a window. This stride s is used to identify the target tokens for unmemorization, essentially skipping s tokens after each target token to be unmemorized. For each of these selected tokens, we use the forget loss $L_{forget}$ to forget these tokens, while employing the maintain loss $L_{maintain}$ to preserve the behavior of the model. The top of shows an example of text from a Harry Potter book with the target tokens selected based on a stride of 10."}, {"title": "4.2. Obliviate", "content": "Forget Loss. $L_{forget}$ employs KL-divergence (KL-div) on the output distribution of the target model, but with the top predicted token (the target token to unmemorize) removed."}, {"title": "5. Evaluation", "content": "We evaluate Obliviate on four large language models of varying sizes and architectures:"}, {"title": "5.1. Experimental Setup", "content": "We evaluate Obliviate through two main experiments: (1) unmemorizing synthetically memorized data (simulating deeply memorized), and (2) unmemorizing organically memorized data present in the pre-trained models. For each scenario, we assess both the effectiveness of Obliviate memorization and its effect on model utility."}, {"title": "5.1.1. MODELS", "content": "- LLaMA-3.1 8B (Llama): Base model without instruction tuning (Grattafiori et al., 2024)\n- LLaMA-3.1-instruct 8B (Llama-Inst): Instruction-tuned variant (Grattafiori et al., 2024)"}, {"title": "5.1.2. DATASETS", "content": "- Qwen-2.5-7B (Qwen): Base model (Qwen et al., 2025)\n- Yi-1.5 6B (Yi): Base model (01.AI et al., 2024)\nWe evaluate unmemorization on two different types of datasets:"}, {"title": "5.2. Metrics", "content": "We evaluate Obliviate using three distinct metrics:\nLongest Common Subsequence (LCS). We begin by measuring the longest common subsequence of words between the generated data and the ground truth data, which the model is assumed to memorize. A higher LCS indicates a greater degree of memorization by the model.\nEdit Distance (ED). Next, we assess the edit distance using the Levenshtein distance, calculated based on words rather than characters. A lower ED suggests a higher level of memorization by the model. For both LCS and ED, we ignore differences in capitalization.\nROUGE-2 Score (ROUGE-2). Lastly, we compute the ROUGE-2 score, which evaluates the overlap between bigrams in the generated data and the ground truth data. This metric provides another perspective on the model's ability to replicate the training data."}, {"title": "5.3. Results", "content": "Synthetic Dataset. We create a synthetic dataset consisting of 10 articles, each approximately 380 words (around 2,000 characters) in length, covering diverse topics. These articles are specifically generated to test the effectiveness of unmemorization on deeply memorized content, where the probability of reconstructing the sequence exceeds 99%. Using synthetic data ensures that the content does not exist in the models' pretraining data, thereby providing a controlled evaluation environment.\nOrganic Dataset. We evaluate on excerpts from widely-read literary works, including \"Harry Potter and the Philosopher's Stone\u201d, \u201cMoby Dick\u201d, \u201cFrankenstein\u201d, \u201cAdventures of Huckleberry Finn\u201d, \u201cAlice in Wonderland\", \"Sherlock Holmes\", \"A Tale of Two Cities\", \"Treasure Island\", \"Les Mis\u00e9rables\", and \"Great Expectations\". These works are particularly relevant as their content has been identified in publicly available models.\nBenchmarks. To assess model utility preservation, we evaluate performance on four standard benchmarks, namely HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021b;a), TruthfulQA (Lin et al., 2022), and Winogrande (Sakaguchi et al., 2019).\nUnmemorizing Synthetic Data. We first finetune the target models on 10 samples using our synthetic dataset until deep memorization is achieved. Intuitively, we finetune the model till its ability to reproduce the complete article given a prefix. We refer to these as \"Memorized\" models.\nNext, we apply Obliviate with a token stride of 5, modifying the token following every set of five tokens, and a modification rate of one token per modification. For the KL-divergence, we consider the top-10 probabilities. We implement early stopping when the probability of the target token falls below a predetermined threshold.\nTo comprehensively evaluate memorization, we evaluate different testing settings that varies prefix offset positions, i.e., where does the prefix start, from 0 to 200 tokens at 50-token intervals, and prefix lengths ranging from 8 to 20 tokens. We examine both greedy decoding (Greedy) and temperature-based sampling (temperature = 0.6, the default for Llama). For each sample, we report the best performing metrics across all combinations, i.e., the maximum score for Longest Common subsequence (LCS) and ROUGE-2, and the minimum score for Edit Distance (ED). Finally, for the instruct version of the model, we precede the prefix with the following instruction: \u201cGenerate the entire rest of this text from the start, continuing until you reach the end: \"."}, {"title": "5.4. Ablation Study", "content": "We conduct extensive ablation studies to analyze the sensitivity of Obliviate to various hyperparameters. Our analysis focuses on Llama and Qwen models using representative samples from both synthetic and organic datasets.\nTop-k. We investigate the sensitivity of both forget ($L_{forget}$) and maintain ($L_{maintain}$) losses to the number of probability distributions considered, examining k \u2208 \\{5, 10, 20\\}. Results shown in  for synthetic data and in the appendix for organic data demonstrate remarkable stability across different k values. This consistency extends to model utility metrics, suggesting that Obliviate is robust to this hyperparameter choice at even modest coverage of the target probability distribution.\nStride. The token stride parameter determines the granularity of unmemorization by controlling the spacing between modified tokens. We examine stride values spanning multiple orders of magnitude: 1, 2, 5, 10, 20, 50.  presents the Longest Common Subsequence (LCS) results for synthetic data, with corresponding organic data results shown in of the appendix.\nOur results reveal the expected inverse relationship between stride length and unmemorization effectiveness, where decreasing stride length generally increases unmemorization. This is expected due to more token modifications that disrupt verbatim generation. However, it is important to note that the unmemorization remains significant even for larger strides. For example, a stride of 50-modifying only \u2248 2% of tokens-Obliviate reduces the LCS from over 380 tokens to 68 tokens. From these results, we observe that the unmemorization performance saturates for strides up to 5, where we see nearly identical unmemorization performance. Therefore, we use a stride of 5 in our experiments.\nThe impact on model utility, illustrated in  for Llama on synthetic data (with complete results in ), demonstrates significant robustness of Obliviate with a negligible effect on model utility. Contrary to initial intuition, even an aggressive stride setting of 1 (modifying every other token) maintains model utility with minimal degradation. A closer examination reveals that this behavior is explained by the probability distribution of each token remaining similar with respect to the top 9 probabilities during unmemorization (after removing the target token). We hypothesize that maintaining this distribution over the top 9 probabilities is what helps the model preserve its utility. To further validate this hypothesis, we tested unmemorizing with a stride of 5 without applying the maintain loss $L_{maintain}$. As anticipated, the model diverged and experienced a significant drop in utility.\nFinally, to evaluate the extreme scenario of modifying every token, we evaluated a stride of 0, effectively unmemorizing all tokens. In this case, model utility declined more significantly, with a 7% drop in the TruthfulQA benchmark performance and a tendency for the model to repeat words during manual evaluation.\nCandidate Token Selection Strategy. Our final ablation examines the token candidate selection strategy for the forget loss ($L_{forget}$). We compare two approaches: (1) direct selection from top-k probabilities, and (2) selection ensuring matching capitalization and leading space patterns for the k alternate tokens. Experimental results show negligible performance differences between these strategies in both memorization reduction and utility preservation.\nThese ablation studies collectively demonstrate that Obliviate maintains robust performance across a wide range of"}, {"title": "6. Discussion and Limitations", "content": "Our work introduces Obliviate as a targeted solution for preventing verbatim reproduction in LLMs, addressing a specific but crucial aspect of the broader AI copyright landscape. While our results demonstrate significant success in preventing exact sequence reproduction, it is important to clarify both the strengths and limitations of our approach within the larger scope of AI intellectual property protection."}, {"title": "6.1. Limitations", "content": "Despite Obliviate's effectiveness in preventing verbatim generation, it is important to acknowledge a key limitation: while Obliviate successfully prevents exact sequence reproduction, it does not address the broader challenge of information extraction from language models. A determined adversary could potentially extract information about the unmemorized content through careful prompt engineering, such as using a series of yes/no questions or constructing prompts that elicit semantic information without requiring exact reproduction. This limitation is inherent to our focused approach on verbatim generation and reflects the fundamental trade-off between maintaining model utility and completely removing information. Our method's effectiveness relies on the assumption that normal usage patterns primarily involve direct generation rather than adversarial probing. While this assumption aligns with typical use cases and current copyright frameworks that focus on expression rather than ideas, it may not satisfy more stringent information security requirements."}, {"title": "6.2. Broader Implications", "content": "The success of Obliviate in selectively mitigating verbatim reproduction while preserving semantic understanding further explore the area of knowledge representation in large language models. Our results indicate that surface-level text patterns can be modified effectively without significantly impacting semantic understanding, reinforcing the distinction between memorized sequences and learned concepts.\nFurthermore, Obliviate offers an efficient solution for model providers who need to address copyright concerns without resorting to complete retraining. By significantly reducing exact sequence matches while maintaining performance, Obliviate aligns with emerging standards and ongoing discussions about copyright content protection."}, {"title": "6.3. Challenges in Proving Training Data Usage", "content": "Beyond copyright considerations, our findings also have direct implications for research on determining whether a model has been trained on specific data. Traditional membership inference attacks alone are insufficient for proving such claims, as recent work suggests that true verification requires data reconstruction, often using special canary data (Zhang et al., 2024). However, these proofs typically depend on verbatim reproduction\u2014something that Obliviate effectively prevents.\nAs a result, once a model has been unmemorized using techniques like Obliviate, existing approaches to training data verification may become less effective. To validate this, we focus on a recent approach(Schwarzschild et al., 2024) that tests data memorization using Adversarial Compression. This method optimizes an adversarial prefix to prompt the model into outputting the target memorized text, such as copyrighted material. The effectiveness of this approach is measured by the length of the prefix needed to generate the target text: the shorter the prefix, the more memorized the data is, indicating a higher compression rate.\nWe evaluate the Adversarial Compression Ratio (ACR) both with and without Obliviate. Using the first five sentences of Harry Potter as our test case, we compare the ACR between the original model and versions processed with Obliviate using strides of 5 and 2.  presents results for the LLaMA-Instruct model, with additional results for LLAMA and Qwen models provided in Appendix (). As hypothesized, applying Obliviate significantly reduces the ACR below the acceptance threshold (1), indicating successful unmemorization of the samples.\nTo further validate our findings, we conducted additional experiments using synthetic modified/rephrased versions of the original Harry Potter sentences that the models had"}, {"title": "7. Conclusion", "content": "We introduced Obliviate, a novel post-training approach to prevent verbatim text reproduction in large language models by modifying token distributions. Our results across multiple architectures show that precise control over memorized content can be achieved without compromising model utility. The success of token-level interventions in preventing exact sequence generation while preserving semantic understanding suggests potential for new techniques in targeted knowledge modification in neural networks, especially for selective removal of specific patterns while maintaining underlying learned representations."}, {"title": "A. Appendix", "content": ""}]}