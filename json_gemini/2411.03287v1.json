{"title": "The Future of Intelligent Healthcare: A Systematic Analysis and Discussion on the Integration and Impact of Robots Using Large Language Models for Healthcare", "authors": ["Souren Pashangpour", "Goldie Nejat"], "abstract": "The potential use of large language models (LLMs) in healthcare robotics can help address the significant demand put on healthcare systems around the world with respect to an aging demographic and a shortage of healthcare professionals. Even though LLMs have already been integrated into medicine to assist both clinicians and patients, the integration of LLMs within healthcare robots has not yet been explored for clinical settings. In this perspective paper, we investigate the groundbreaking developments in robotics and LLMs to uniquely identify the needed system requirements for designing health-specific LLM-based robots in terms of multi-modal communication through human-robot interactions (HRIs), semantic reasoning, and task planning. Furthermore, we discuss the ethical issues, open challenges, and potential future research directions for this emerging innovative field.", "sections": [{"title": "1. Introduction", "content": "In healthcare, the need for new technology to maintain the quality and efficiency of care is paramount. This demand has been amplified by an increase in the overall older population of the world. Namely, by 2050, 22% of the global population is expected to be over 65 years old [1]. This demographic shift leads to a rising prevalence of chronic diseases such as dementia, diabetes, and heart disease, which require continuous monitoring and long-term management, further straining healthcare resources [2,3]. In 2022, Canada alone had 143,695 job vacancies for healthcare professionals [4]. In the U.S., it is predicted that by 2026 there will be a shortage of up to 3.2 million healthcare workers [5], highlighting the staggering workforce shortage [5]. Furthermore, the vast amounts of health data generated in this sector require efficient management and use to improve patient outcomes and reduce healthcare costs, a task well suited for generative AI and deep learning models [6-8]. For example, generative AI models, such as large language models (LLMs), can use the large EHR (Electronic Health Record) datasets for training to learn patterns between symptoms, diagnoses, and recommendations in order to help in healthcare with the management of data and the retrieval of information, as well as decision-making processes [9].\nThe need for new technologies in healthcare is multi-faceted, using the following: (1) classical and deep learning methods to facilitate medical imaging analysis [10], (2) deep neural networks (DNNs) to perform automated disease detection and prediction [11], and (3) LLMs for clinical decision-making and teleconsultation [12]. In particular, LLMs have already been integrated into medicine to assist with clinical note-taking by rewriting and summarizing clinicians' notes for clarity and have also been leveraged through chatbots to assist patients with reminders and general questions about medications [12]. Furthermore, LLMs have also been used to personalize treatment plans for individual patients by assisting with EHR data extraction [13]. The use of deep learning and LLMs have many benefits and advancements in the delivery of healthcare.\nIn addition to the aforementioned software technologies, innovations in robotics have allowed their integration into healthcare, including the following: (1) surgical robots being used to perform various minimally invasive surgical procedures for brain, spine, and neck surgeries, where accuracy and reliability are paramount [14,15], (2) rehabilitation robots in the form of robotic assistive wheelchairs [16], prostheses [17], and robotic arms and exoskeletons for lower and upper limb rehab [18,19], (3) mobile medication delivery robots utilized as automated medication carts [20], and (4) humanoid robots monitoring patient vital signs [21]. The need for robots to assist in healthcare was especially evident during the COVID-19 pandemic, where the healthcare sector turned to robotics in response to public health emergencies for various tasks to minimize person-to-person contact and the spread of the virus [22]. This included autonomous disinfecting robots using UV-C irradiation for surface decontamination in hospitals [23] and service robots used to facilitate social distancing measures in hospitals and long-term care homes by conducting initial screening of COVID-19 symptoms and detecting face masks [23-26]. Social robots have also been used to provide companionship to older adults in healthcare environments (i.e., long-term homes) by engaging in conversations, such as sharing stories or telling jokes, thus creating a more personal and comforting presence by interacting with users [27,28]. In general, the integration of robots into healthcare aims to enhance patient experiences and outcomes, support skill augmentation, and improve the overall quality of care while reducing the workload of care providers [29].\nDespite the use of both healthcare robots and LLMs in healthcare, the integration and deployment of these two technologies still remains unexplored. To the authors' knowledge, there have only been three instances where robots and LLMs have been applied directly for healthcare applications: two instances involving social robots [30,31] and one instance involving a surgical robot [32]. However, this potential marriage of robotics and LLMs for healthcare presents an untapped opportunity, as the combination of a robot's physical capabilities with the understanding and generative abilities of LLMs has the potential to provide person-centered care, as well as streamline operational workflows (e.g., logistical tasks), and reduce the workload of healthcare professionals. This integration will result in the utilization of vast healthcare data to further refine diagnostic, therapeutic, and predictive healthcare services. The potential of healthcare robots using LLMs to provide such services underlines the urgent need for research and development in this emerging area.\nIn this paper, we present the first investigation into the emerging field of healthcare robots using LLMs. Namely, we explore the innovative developments which aim to address the challenge of enhancing the quality and efficiency of patient care during a time where the demographic is shifting towards an older population and there is a shortage of healthcare professionals. Our objective is to identify the needed system requirements for multi-modal communication through human-robot interactions (HRIs), systematic reasoning, and task planning for designing health-specific LLM-based robotic solutions. We will also discuss the ethical issues associated with the potential development and utilization of healthcare robots leveraging LLMs and the open challenges and potential future research directions for this field."}, {"title": "2. Large Language Models (LLMs) for Healthcare", "content": "The versatility of generative AI is apparent in its ability to be trained on an array of data types from textual (i.e., EHRs) [33] and visual content (i.e., medical imaging data) [34] to genetic sequences [29] in order to learn and capture the underlying patterns and distributions from such data. This makes it especially valuable for healthcare tasks that require adaptability and continuous learning [30]. LLMs represent a significant advance-ment in generative AI. They primarily utilize the transformer architecture [35] and contain key features including multi-head attention for parallel processing, positional encodings for sequence awareness, layer normalization, and feedforward networks for data refine-ment [36]. The encoder-decoder structure in many LLMs facilitates complex tasks such as language translation and content generation [37]. The transformer architecture provides LLMs with the potential to be instrumental in healthcare robotic development due to the proficiency in generating human-like text [38], understanding of the lexical semantics of the physical world [36,39], and making decisions regarding appropriate robot behaviors to implement [40].\nThere are several common prompting techniques that are essential for the interaction between robotic systems and LLMs which can also be extended to healthcare settings. The prompts serve as the conduit through which queries or tasks are communicated to LLMs through agents (people or robots), with the model generating responses based on the given prompt structure. Outputs from LLMs are probabilistic, leading to 'prompt engineering', where different prompt structures are used to bias the output of the LLM towards a de-sired outcome [64]. Moreover, the context window of an LLM refers to the contiguous sequence of tokens considered by the LLM in a forward pass (i.e., generation time) [65]. This window, typically limited by memory constraints, determines the amount of preceding and succeeding text data the model utilizes to maintain coherence and accuracy in tasks such as word prediction [66]. This is evident in few-shot prompting, where input-output pairs provided to the model fill its context window with consistent structures, reducing variations in the input text [67,68]. This uniformity in the context window influences the attention mechanism to focus more narrowly, enhancing the relevance of certain parts of the input to the generated output [69,70]. Therefore, the effectiveness of a prompt and, consequently, the utility of the model's response are highly contingent upon the prompt's design. A prompting method is critical for achieving specific objectives.\nIt is worth noting that prompts can be multi-modal, including text, images, or audio. In order for LLMs to respond to a variety of informational inputs [71], the non-textual compo-nents are converted into textual descriptions through preprocessing models such as CLIP which aligns images and text in a joint embedding space [72] or SCANREF which aligns point clouds and sentences in a joint embedding space [73]. This multi-modal integration broadens the scope of LLM applications, enhancing their adaptability and effectiveness in complex healthcare environments where diverse data types are prevalent, such as medical imaging data, recorded conversations between patients and healthcare professionals, and dedicatory images such as maps of healthcare environments or medication labels. The prompting techniques presented integrate LLMs into robotics."}, {"title": "3. Human-Robot Interaction (HRI) and Communication", "content": "The field of HRI for healthcare is focused on the development of appropriate design and implementation strategies for robots to assist with different tasks for a wide range of users from healthcare professionals [23] to patients [89]. Therefore, HRI approaches require robotic systems to understand and adapt to the needs of users. The types of HRIs used in healthcare applications include the following: (1) teleoperation, e.g., through graphical user interfaces for socially assistive robots to provide therapy and cognitive interactions [90,91]; (2) social interactions using natural communication modes, for example, for companionship [92]; and (3) the use of mechanical interfaces used, for example, for surgical robots [93]. Surgical robots require mechanical interfaces (joysticks, switches, etc.) which can result in high cognitive workloads for surgeons [94]. On the other hand, social HRI utilizes natural language processing (NLP) to recognize user verbal requests and commands in order to provide reminders and to engage in conversations [95]. These requests and commands are dependent specifically on pronunciation and word choices, which can potentially result in incorrect command calls being detected [96].\nIn general, it is important that interactive healthcare robots have intelligent commu-nication abilities using multiple modalities such as spoken natural language, gaze, facial expressions, and illustrative gestures in order to be able to recognize the intent of a user and effectively convey their own intent using these modes. Current healthcare robots are capable of both single- and multi-modal interactions; however, they still have not yet been widely adopted in hospitals, clinics, and/or long-term care homes due mainly to their linguistic limitations, which can lead to critical procedural mistakes (misinterpretation of user requests) and/or frustration and lack of trust in the technology [97].\nLLMs can address the aforementioned challenges in order to improve user experiences and the intent to use the technology [98]. The ability of LLMs to generate dynamic multi-modal communication and detect and utilize emotional cues through the use of emotional prompting techniques such as EmotionPrompt [99] choreographed by an LLM will result in natural HRIs that will be similar to human-human interactions [100]. The introduction of LLMs in healthcare robots will aim to improve HRI via the cohesion between multiple com-munication modes; however, to date, the use of LLMs in HRI has mainly only considered a single mode [101]."}, {"title": "3.1. Single-Modal Communication", "content": "Single-modal robot communication in healthcare applications has mainly consisted of either textual or verbal information exchange [102,103]. A primary challenge for such single-mode communication is ensuring that a robot does not become monotonous or repetitive, as this can reduce user engagement and trust and also negatively impact the adoption of healthcare robots [104].\nIn [76], GPT-3 was integrated into the 'Mini' small character-like social robot to provide companionship to older adults with mild cognitive impairment. The robot engaged in cognitive stimulation games and general conversations with the older adults. The Babbage and Davinci versions of GPT-3 were used to generate user-adapted semantic descriptions and to paraphrase prewritten texts in Spanish. This integration was tailored to achieve a single consistent mode of communication, enhancing the robot's capabilities in natural and adaptive dialogues. The research highlights the potential of streamlined adaptable interactions in social robotics made possible by the capabilities of GPT-3 to create tailored responses. The use of GPT-3, despite requiring a translation step from English, was justified by its high performance and adaptability to the specific requirements of the application compared to other models including T5 multilingual [105], PEGASUS [106], and BERT2BERT [107].\nIn [32], a natural language interface using GPT-3.5 was implemented into a daVinci surgical robot [108] to provide a user-friendly interface for surgeons. The aim was to minimize the cognitive load of surgeons and improve efficiency. A surgeon inputs a verbal command using a microphone, which is then preprocessed using an off-the-shelf text-to-speech model and prompted to GPT-3.5. This prompt also includes a dictionary of possible actions that the daVinci robot can perform. GPT-3.5 is asked to match the natural language input to a robot action through generating an output based on the contents of its context window. The specific actions for the robot to execute include camera position settings, video and picture recording, and finding and tracking surgical tools based on the surgeon's command. A Robot Operating System (ROS) [109] node structure was used to directly provide execution commands to the daVinci robot. The system usability was tested in a laboratory where 275 natural language commands were given to the system. It was able to correctly identify and execute the intended robot action with a success rate of 94.2%. A time delay existed between the command request and the execution of the robotic action, which was attributed to computation time to respond of the GPT-3.5 model."}, {"title": "3.2. Multi-Modal Communication", "content": "In HRI, multi-modal communication is far more engaging and effective in building trust [113] when compared to single-modal communication, providing a more interactive and nuanced patient experience through the use of gestures (e.g., animated speech) and body poses and varying speech intonations [114,115]. Multi-modal HRI emulates human interaction patterns closely [116], and LLMs can generate emblems (non-verbal gestures or body language that have specific meanings) for a robot to display in a zero-shot approach. LLMs are also adaptable to variations in user interactions due to word-level annotation accuracy and their attention head mechanisms which dynamically increase the importance of relevant parts of the input text. These mechanisms allow a contextual understanding of user interactions which is required while generating an appropriate speech response and emblem. The ability of LLMs to generate contextually accurate responses and emblems in HRI without the need for training every possible interaction makes LLMs suitable for facilitating HRI in healthcare settings, especially considering the significant variance in demographics and the diverse nature of interactions encountered in such settings.\nLLM frameworks applied in multi-modal HRI have the potential to make HRI in healthcare settings more engaging. For example, by aligning non-linguistic commands to natural language and generating dynamic responses to user queries, they can potentially provide more natural communication during robotic-guided therapy sessions, breaking down language barriers and providing emotional support. For example, in [82], GPT-3.5 was used to align non-linguistic communication cues with the natural language responses of any robot capable of multi-modal communication. The Empathetic Social Robot Design Framework utilized consisted of the following modules: Speech, Action, Facial Expression, and Emotion (SAFE), alongside the user request. For Speech, GPT-3.5 considered seven types of speech, from 'high and fast speech' to 'slow speech in neutral tones'. Action encompassed seven gestures, such as turning the head towards the speaker, nodding, shaking the head, interlocking hands on the table, and eye contact. Facial Expressions included ten options including frown, light smile, pout, no expression, bright smile, raised eyebrows, grin, lowered eyebrows, jaw drop, and widened eyes. For Emotion, the framework provided GPT-3.5 with ten emotional states for the robot to display: joy, liveliness, sadness, surprise, anger, worry, calmness, indifference, absence of emotion, and disgust. In a user study analogous to the Turing test, GPT-3.5 was prompted with a specific problem according to the SAFE prompt structuring, such as \u2018I am too nervous for the upcoming internship interview'. It was also provided with an example of how it should respond. The response generated by the GPT-3.5 was then compared to a response of a human presented with the same problem. The average alignment score for speech, action, facial expression, and emotion was 26%, 10%, 31%, 32%, and 25%, respectively. Such a robot system can be potentially useful for Reminiscence/Rehabilitation Interactive Therapy and Activities [117] for those living with dementia. A social robot can engage older adults in reminiscence activities such as music, TV shows, and movies. It can interpret users' non-verbal responses and adapt its interactions to suit their emotional states, providing cognitive engagement and fostering emotional well-being.\nIn [83], the text-davinci-003 model of GPT-3.5 was used to generate the dynamic responses of the Furhat robot to visitor questions in regards to news and research being conducted at the National Robotarium in the UK. The integration of GPT-3.5 into Furhat was to enable human-like speech and contextually accurate gestures while creating consistency between these modes of communication. The Furhat SDK provided the following: (1) an automatic speech recognition (ASR) module to transcribe speech to text, (2) a natural language understanding (NLU) module to identify user intent, (3) a dialogue manager (DiaL) to maintain conversational flow, and (4) natural language generation (NLG) using GPT-3.5. More specifically, the NLG module generated responses based on engineered prompts containing the user request from the NLU module, Furhat's personality, and past dialogue histories from the DiaL module. The responses generated by the NLG module had an associated emblem which was parsed by the Furhat SDK and presented by the robot using both verbal and non-verbal communication."}, {"title": "3.3. Summary and Outlook", "content": "The integration of LLM frameworks for HRI into healthcare robots can potentially improve the cohesiveness and engagement of interactions between healthcare robots and patients, visitors, and stakeholders. LLMs have been embedded into social robots to improve HRI by (1) providing non-repetitive single-mode (verbal) [76] and multi-modal communication, where the latter consists of embedding non-verbal communication into the verbal responses of healthcare robots (i.e., gestures, eye contact, and facial expressions) to users [82,83], and (2) identifying the linguistic patterns of the user [31,81]. Moreover, LLMs have the potential to provide a more efficient interaction interface through the use of natural language to control surgical robots such as the daVinci robot [32].\nThe use of LLMs for multi-modal communication can augment robots in order to extend their use in healthcare environments by accommodating non-verbal communication with individuals living with cognitive impairments, autism spectrum disorder, and/or learning disabilities, where verbal communication alone is not always feasible. Bidirectional multi-modal communication can be used when a healthcare robot needs to effectively convey and recognize various non-verbal cues such as gestures/body language, facial expressions, and vocal intonations. For example, a healthcare robot should recognize when a patient is stressed or upset using these non-verbal cues and respond with an appropriate emotional behavior (i.e., concerned) rather than being cheerful or happy, thereby aligning its responses with the patient's emotional states."}, {"title": "4. Semantic Reasoning", "content": "Significant amounts of information must be processed in healthcare, including images, data, and text, in order to minimize errors and improve the efficiency of personalized care techniques [118]. The semantic reasoning of healthcare information requires experts to identify relationships between different factors such as genetic predispositions, lifestyle choices, environmental exposures, and/or social determinants of health which may all influence the health of an individual [119,120]. Therefore, semantic reasoning encompasses the understanding of meanings, concepts, and relationships between data and medical knowledge. In particular, ontologies and knowledge graphs created from patient EHRs are currently used to interpolate how various symptoms, diseases, and treatments are interrelated and influence one another in order to make predictions for clinical decision-making and patient care [121].\nCOVID-19 increased the demand for telehealth by 367% in adults aged 55\u201365 and by 406% for adults aged 65 years and older [122]. However, frameworks such as the eCoach personalized referral program to help people stay active and achieve physical activity goals [123] and the Babylon Chatbot which provides healthcare consolations through a mobile app [124] both require complex ontologies and access to large amounts of contextual information to generate personalized recommendations. Therefore, it is not only a matter of creating healthcare-focused datasets and ontologies to train deep learning and NLP models to be able to provide predictions and inference; there needs to also be an effective approach to creating such inferences in order to (1) facilitate the seamless exchange of semantic reasoning frameworks among healthcare institutions [125] and (2) simplify the creation of frameworks that can effectively capture and convey the complex semantics of medical datasets, terminologies, and environments (hospitals, clinics, etc.) [126].\nTraditional robotic semantic reasoning frameworks consist of three core compo-nents [127]: (1) knowledge resources (raw data), (2) computational frameworks (math-ematical models), and (3) world representations (scene/environment representations). Knowledge resources include the data from which semantic knowledge is extracted (i.e., EHRs, clinical trial results). These data are used to train the LLMs. Computational frameworks consist of models such as transformers (LLMs) [128], probabilistic models (Bayesian networks) [129], or deep learning models (long short-term memory (LSTM) networks) used to capture temporal dependencies in data [130]. These computational frameworks are the models that encode the relationships between concepts [131]. They then use the encoded knowledge to perform inference [127]. World representations are used by robots to model their surrounding environments and their own behaviors [132]. In the case of LLMs, the world representation provides an LLM with a scene description of a robot's working environment in the context window and thereby 'grounds the LLM' to its current environment [20]. The aforementioned core components allow robots to perceive, understand, and generalize semantic knowledge in order to improve performance in real-world tasks. The incorporation of LLMs for semantic reasoning in healthcare robots offers multiple advantages. Namely, healthcare robots need semantic reasoning to identify relationships between a task, an environment, and a user command. Semantic reasoning can be used (1) when generating object manipulation plans for surgical tools, equipment, instrumentation, and medical supplies, (2) to navigate to specific areas and regions in the environment such as the OR and/or patient rooms, (3) to interact with people (to provide assistive HRI) and other robots, and (4) to complete task management functionalities or specific tasks and services (patient education, guidance, informatics).\nTo date, only a handful of robots have been incorporated with LLMs for the purpose of semantic reasoning. For example, in [77], the semi-humanoid robot NICOL (Neuro-Inspired COLlaborator) used GPT-3 to reason about multi-modal inputs (sound, haptics, visuals, weight, texture) in order to improve robotic perception in object manipulation to help differentiate between visually similar objects. Namely, GPT-3 was used to perform interactive multi-modal perception and robot behavior explanations. The MATCHA (multi-modal environmental chatting) prompting technique provided specific action prompts to GPT-3 which included stored information about the robot's environment. Namely, these action prompts included descriptions of the actions, as well as an example of an expected response. GPT-3 provided instructions to the robot to determine the target block by knocking on and weighing the blocks one by one. The robot then reported the findings (stored information) of each step back to GPT-3. This process was repeated until GPT-3 was able to determine which was the correct block with high accuracy (>90%). The system has been tested only in simulation by prompting NICOL to pick up a specific block made of metal out of three blocks, where the characteristics of the blocks were provided rather than perceived by the robot. Even though this task has not been implemented directly in a healthcare setting, the potential of MATCHA can be explored particularly for improving operational efficiency and decision-making for patient care in terms of multi-modal information gathering and reasoning for robot manipulation tasks such as retrieving and handing over (1) medical supplies on shelves in a supply room or (2) surgical tools on a table in the OR.\nIn [87], the 'SayCan' method integrated the PaLM 540 B parameter model, PALM-SayCan, into a mobile manipulator [133] for semantic reasoning in the context of human-centered environments (i.e., a kitchen in an office). The manipulator provided the perception and manipulation capabilities, while PaLM provided high-level semantic knowledge about tasks to promote successful task completion. The system was trained on a mock kitchen and tested in an office kitchen environment. SayCan is not only prompt-based but also uses a temporal difference reinforcement learning (RL) approach. This approach learns the rewards of each action (completing the objective vs. course of action) and therefore prioritizes the executable actions based on the robot's current environment. PaLM 540B is provided with prompts that include the robot's capabilities and their descriptions and the actions the robot can take expressed as a function (e.g., \u2018goto()'). PaLM 540 B is given a task, such as find object X, and asked to generate multiple possible actions paired with the likelihood of each prediction. The action probabilities are then multiplied by the probability of success (acquired through the temporal difference method in RL). The action function with the highest probability which is the output of the LLM is then executed by the robot's low-level control system. This process is repeated for all subsequent steps in the generated plan. The approach was benchmarked by obtaining the plan success rate and the execution success rate in completing a task such as bringing a bag of rice chips from a drawer. The SayCan development improved task execution in human-centered environments by leveraging semantic reasoning. The potential healthcare applications of 'SayCan' can be extended to mobile social robots directing patients/visitors to specific rooms or departments in a hospital and/or mobile manipulators fetching or localizing essential medical supplies.\nIn [85], the LLM-Grounder, an open-vocabulary zero-shot LLM-based 3D visual grounding pipeline, was introduced. LLM-Grounder integrated GPT-4 and a robotic simulated agent to reason about the semantic relationships between high-level commands given by a user (i.e., find the grey monitor on top of the smaller curved desk) and the work-ing environment (simulated office) for object localization tasks. GPT-4 was used to break down natural language commands into their semantic constituents, the target and a land-mark. The ability of GPT-4 to (1) identify the landmark and the target and (2) differentiate between the two provides an efficient method to generate from high-level user commands to robot actions to perform in order to complete a goal. Namely, OpenScene [134], which is a 3D visual grounding method that uses the transformer architecture to generate 3D scene layouts based on textual descriptions, was used. GPT-4 provided OpenScene with (1) the target name and its attribute (monitor, light grey) and (2) the landmark name and relation (curved desk, small). OpenScene returned to GPT-4 the respective bounding box volumes and distances. GPT-4 was then used to reason about the size of objects and their relative location to the landmark, deciding on which found target and landmark pair(s) has the highest likelihood to be correct. Therefore, GPT-4 is used to provide an efficient method for robots to understand the semantics of high-level user commands and for them to act on these commands. LLM-Grounder was evaluated on the ScanRefer [73] benchmark, which is a standard dataset for assessing 3D vision\u2013language grounding capabilities. The ScanRefer dataset consists of scenes ranging from wildlife to home environments, where each point cloud and image has a text description [73]. LLM-Grounder demonstrated state-of-the-art performance when used for zero-shot open-vocabulary grounding, excelling in complex language (increased # of nouns in command) query understanding over approaches that do not use LLMs and rely solely on CLIP. The ability of a robot to correctly understand user commands in the context of locating objects in human-centered environments is crucial for the successful implementation of healthcare robots. A robot deployed in a healthcare environment must be able to identify the correct target (out of many) in cases where items are not easily distinguishable, such as identifying a specific medication on a medication cart where labels are not visible. This can only be achieved if the healthcare robot is able to understand the pill bottle description and semantic relationship to the landmark specified by the user in natural language.\nIn [74], the 'Lang2LTL' method integrated the SPOT quad-pedal robot [135] with GPT-4 and the T5 base to provide SPOT with the semantic reasoning capabilities required to understand and act on user speech commands in the context of navigating environments ranging from offices to city streets. Lang2LTL used these LLMs to break down navigational commands (i.e., \u2018Go to the store on Main Street but only after visiting the bank') to Linear Temporal Logic (LTL) in the form of sequential objectives, such as (1) go to the bank and (2) go to the store on Main Street after. User commands in the form of natural language were given to GPT-4 to identify the referring expressions (RFs) such as the store on Main Street or the bank. After identifying the RFs, the RFs were grounded to known physical locations (retrieved from a database) by being compared to known proposition embeddings which are location description/coordinate pairings using cosine similarity. The grounded RFs were used as an input into the fine-tuned T5 base model to generate LTL formulas. The fine-tuned T5 model was then used to (1) generalize the initial input command (go to 'a' but only after visiting 'b') and (2) generate the LTL formula required by the planner to facilitate navigation via the robot's low-level controller. Lang2LTL was tested on SPOT in an indoor environment consisting of bookshelves, desks, couches, elevators, tables, etc. SPOT successfully grounded 52 commands, including 40 satisfiable (executable) and 12 unsatisfiable ones (unsafe to execute). The potential applications of 'Lang2LTL' in healthcare are robot delivery tasks within clinical settings. Namely, the added ability to identify unsafe robotic action executions can aid a robot planner. This can be achieved by identifying the semantic importance of user commands and the order of operations, such as retrieve a walking aid for a patient, then visit the patient's room to provide them with the aid.\nIn general, the integration of LLM frameworks for semantic reasoning into healthcare robotics has the potential to improve robot autonomy in complex and dynamic healthcare environments. Currently, LLM-based semantic reasoning has increased robot autonomy, as the transformer architecture of LLMs semantically reasons about the entire input at once, facilitating faster and more accurate decision-making in comparison to traditional sequential processing models such as Recurrent Neural Networks (RNNs) [39]. This approach allows robots to understand complex relationships among object characteristics, robotic action outcomes, the spatial importance of target objects versus landmarks, and user-imposed constraints. Healthcare robots can use frameworks such as MATCHA [77] for autonomous capabilities such as the detection and manipulation of medical supplies based on characteristics such as weight and texture. Additional applications include robots augmented with SayCan [87], LLM-Grounder [85], or Lang2LTL [74] frameworks for (1) navigating efficiently within healthcare facilities by understanding spatial layouts and identifying potential unsafe surfaces (e.g., wet floors), (2) executing patient-specific tasks by interpreting natural language commands given by care providers such as retrieving a vital signs monitor cart before visiting a patient in their room, or (3) increasing the surgical team's situational awareness by alerting them to equipment needs and preemptively managing robotic tool positions, thus ensuring sustained operational focus and efficiency."}, {"title": "5. Planning", "content": "An existing challenge for healthcare robotics is autonomously planning safe and effective behaviors in real time for healthcare tasks. These tasks can include (1) navigating through complex hospital environments to find healthcare professionals and patients and to escort visitors, (2) managing and delivering medications and medical supplies, (3) assisting in surgery by tool handovers and supporting precise tool movements, and (4) facilitating both physical and cognitive rehabilitation with different user groups. Each of these tasks requires a robot to perceive and interpret its surroundings and make real-time decisions that ensure safety and task effectiveness. For example, surgical robots operating within soft tissues and navigating curved paths face a particularly uncertain and dynamic environment due to the complex and variable nature of human anatomy. Soft tissues can shift or deform during procedures, altering expected pathways and requiring real-time adjustments in the robot's movements. Additionally, the inherent variability in patient anatomy means that pre-planned paths may not always apply precisely, necessitating continuous sensory feedback and adaptive control strategies to accurately guide surgical tools without causing unintended damage. This environment demands high levels of precision and adaptability from surgical robots to ensure safety and effectiveness in their operations.\nIn general, healthcare robotic task plans need to be adaptable to different situations and people. Currently, the majority of surgical robots are teleoperated by a surgeon [136,137", "138": ".", "139": "."}, {"139": ".", "140": "attempt to enhance the surgical field of view without making additional incisions, they do rely on complex intra-camera tracking to maintain an expanded view. Namely, this system determines the correspondence between different cameras at the initialization stage and updates the expanded view frequently when there is enough overlap between views. However, this can still lead to inaccurate mosaicking results due to error accumulation over time [141", "142": 2, "143": "and (3) Simulated Annealing [144", "145": 2, "146": "and (3) LSTM networks [147", "148": ".", "149,150": "."}]}