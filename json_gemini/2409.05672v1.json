{"title": "ZERO-SHOT OUTLIER DETECTION VIA PRIOR-DATA FITTED NETWORKS: MODEL SELECTION BYGONE!", "authors": ["Yuchen Shen", "Haomin Wen", "Leman Akoglu"], "abstract": "Outlier detection (OD) has a vast literature as it finds numerous applications in environmental monitoring, cybersecurity, finance, and medicine to name a few. Being an inherently unsupervised task, model selection is a key bottleneck for OD (both algorithm and hyperparameter selection) without label supervision. There is a long list of techniques to choose from \u2013 both classical algorithms and deep neural architectures - and while several studies report their hyperparameter sensitivity, the literature is quite slim on unsupervised model selection\u2014limiting the effective use of OD in practice. In this paper we present FoMo-0D, for zero/0-shot OD exploring a transformative new direction that bypasses the hurdle of model selection altogether (!), thus breaking new ground. The fundamental idea behind FoMo-0D is the Prior-data Fitted Networks, recently introduced by M\u00fcller et al. (2022), which trains a Transformer model on a large body of synthetically generated data from a prior data distribution. In essence, FoMo-0D is a pretrained Foundation Model for zero/0-shot OD on tabular data, which can directly predict the (outlier/inlier) label of any test data at inference time, by merely a single forward pass-making obsolete the need for choosing an algorithm/architecture, tuning its associated hyperparameters, and even training any model parameters when given a new OD dataset. Extensive experiments on 57 public benchmark datasets against 26 baseline methods show that FoMo-0D performs statistically no different from the top 2nd baseline, while significantly outperforming the majority of the baselines, with an average inference time of 7.7 ms per test sample.", "sections": [{"title": "1 INTRODUCTION", "content": "Outlier detection (OD) finds many practical applications in different domains such as security, environmental monitoring, manufacturing, finance, and so on. This popularity brings about a large literature that offers a plethora of detection techniques to choose from given a new OD task. Further, these techniques exhibit several hyperparameters (HPs) that need careful tuning to which they are often quite sensitive Ma et al. (2023). What makes it notoriously difficult for achieving effective OD performance in practical applications is model selection (both algorithm and HPs) in the absence of any labels, provided most tasks are unsupervised.\nIn fact, while deep learning and modern neural architectures have revolutionized many areas of machine learning (ML), it has not quite been the case for OD. That is mainly because deep OD models Pang et al. (2021) exhibit many more HPs (including those for architecture, regularization, and optimization), as compared to their classical/shallow counterparts that have only a few HPs, to which detection performance remains sensitive Ding et al. (2022).\nMost recent advances in ML have been through large foundation models, which are (pre-)trained on massive amounts of data. The most notable progress has been in natural language and image domains, thanks to the admirable quantity and quality of public text and image datasets. On the other hand, public (benchmark) datasets for OD is minuscule in comparison Han et al. (2022); Zhao et al. (2021); Steinbuss and B\u00f6hm (2021). Another obstacle for foundation models for point-cloud OD has been"}, {"title": "2 PROBLEM AND PRELIMINARIES", "content": ""}, {"title": "2.1 SEMI-SUPERVISED OUTLIER DETECTION", "content": "Outlier detection (OD) methods can be categorized based on the availability of labeled data. In supervised OD, the task is similar to binary classification with imbalanced classes (as outliers typically make up only a small portion of the overall data). The more difficult unsupervised setting assumes the \"contaminated\" training data contains both inliers and outliers, but without any labels. A semi-supervised or one-class classification approach lies between these two extremes, where only inlier data is available for training, but unknown outliers may appear during inference. Semi-supervised OD is used in practice where it is easy to gather inlier data, but learning from known, labeled outliers is undesirable because outliers are hard to collect and/or new, unknown outlier types are likely to arise in future test data that renders learning only from the known outliers suboptimal/risky.\nNote that semi-supervised OD may be a misnomer from the supervised ML perspective, where semi-supervised classification assumes the presence of some labeled instances from all classes in the training data. As such, model selection continues to be as difficult for semi-supervised OD as unsupervised OD, where no labeled outliers exist in the input/training data in both settings.\nIn this paper, we focus on semi-supervised OD. Formally, let $D_{in} = \\{(x_1, y_1) . . ., (x_n, y_n)\\}$ denote the input data containing only inlier points $x_i \\in R^d$, where $y_i = 0 \\forall i \\in [n]$, and $D_{test}$ depicts the test dataset comprising both inliers and outliers. The task is to assign labels to $x_i \\in D_{test}$ given the inlier-only input data $D_{in}$."}, {"title": "2.2 BACKGROUND ON PRIOR-DATA FITTED NETWORKS", "content": ""}, {"title": "Posterior Predictive Distribution (PPD)", "content": "In the Bayesian framework for supervised learning, the prior defines a hypotheses space $\\Phi$ which expresses our beliefs about the data distribution before seeing any data. Each hypothesis $\\phi \\in \\Phi$ describes a mechanism by which the data is generated. The posterior predictive distribution $p(y_{test}|x_{test}, D_{train})$ provides a framework for making prediction on new, unseen test data $x_{test}$, conditioned on observed training data $D_{train} = \\{(x_1, y_1), ..., (x_n, y_n)\\}$. Based on Bayes' Theorem, the PPD can be derived by the integration over the space of hypotheses $\\Phi$:\n$p(y_{test} | x_{test}, D_{train}) = \\int_{\\Phi} p(y_{test} | x_{test}, \\phi) p(D_{train} | \\phi) p(\\phi) d\\phi,                                                        \\qquad(1)$\nwhere $p(\\phi)$ denotes the prior probability and $p(D|\\phi)$ is the likelihood of the data $D$ given $\\phi$."}, {"title": "PFNs and PPD Approximation", "content": "As obtaining the above PPD is generally intractable, Prior-data Fitted Networks (PFNs) are proposed to approximate the PPD M\u00fcller et al. (2022). Unlike traditional machine learning models that are trained directly on observed datasets, PFNs are pre-trained offline on simulated datasets that are generated according to a prior distribution. Specifically, it contains the pre-training and inference stages described as the following.\nPre-training on synthetic data. At the beginning of the pre-training stage, massive synthetic training datasets are generated, by first sampling a hypothesis (i.e., the generating mechanism) $\\phi \\sim p(\\phi)$, and then sampling a dataset $D \\sim p(D|\\phi)$. For training purposes, each dataset $D$ can be split as $D_{test} \\subset D$ and $D_{train} = D \\setminus D_{test}$. Thus the PFN with parameters $\\theta$ can be optimized by making predictions on data points in $D_{test}$. For a test point $(x_{test}, y_{test}) \\in D_{test}$, the training loss is formulated as\n$\\mathcal{L} = \\mathbb{E}_{(\\{x_{test}, y_{test}\\}, D_{train})\\sim p(D)}[ -\\log q_{\\theta}(y_{test}|x_{test}, D_{train}) ].                                                                \\qquad(2)$\nThe above loss can also be interpreted as minimizing the expected KL divergence between $p(\\cdot | x, D)$ and $q_{\\theta}(x, D)$ M\u00fcller et al. (2022). In practice, a PFN model $q_{\\theta}$ is typically implemented by a Transformer-based architecture Vaswani et al. (2017), which takes $(x_{test}, D_{train})$ as input, where $x_{test} \\in D_{test}$ and $D_{train}$ contains an arbitrary number of instances. The output is the conditional class probabilities for $x_{test}$. As the whole training set $D_{train}$ is passed as input/context to the Transformer, it learns to predict class labels through sample-to-sample attention.\nInference on real-world data. In the inference stage, a fresh real-word dataset $D_{train}$ and some test instance $x_{test}$ are fed into the (frozen) pre-trained model, which computes the PPD $q_{\\theta}(\\cdot | x_{test}, D_{train})$ in a single forward process. Importantly, PFNs do not require gradient-based parameter tuning on data observed at inference time, where the training and prediction are delivered through a one-step forward process in less than a second Hollmann et al. (2023).\nIn summary, PFNs are trained once offline, and can be used many times for zero-shot inference when new datasets with different characteristics are input. The main benefit is that no training or tuning is required at the inference stage. This type of learning ability is also termed as in-context learning (ICL) Xie et al. (2021), which was shown to be an effective paradigm for various tasks in NLP with the stream of large language models Brown et al. (2020). In fact, ICL with PFNs is recently shown to be a promising paradigm for supervised classification on tabular datasets Hollmann et al. (2023)."}, {"title": "3 FoMo-0D: A NEW PFN FOR 0-SHOT OD \u2013 MODEL SELECTION BYGONE!", "content": "Inspired by the recent PFNs M\u00fcller et al. (2022) and their successful applications in supervised classification Hollmann et al. (2023) and time series forecasting Dooley et al. (2023), we propose FoMo-0D, a prior-data fitted Foundation Model for 0-shot Outlier Detection. FoMo-0D is (pre)trained on a large body of synthetically generated OD datasets toward zero-shot inference on a new dataset. Most notable of our zero-shot FoMo-0D is its elimination of the need not only for model training on a new dataset, but especially also for model selection (both algorithm and HPs), which is notoriously-hard without any labeled data. By breaking such new ground, and its unreasonable effectiveness on many benchmark datasets compared to classical and modern baselines even at its first-attempt stage, we expect FoMo-0D will become a milestone in future research and practice of OD. The new FoMo-OD paradigm (right) versus the typical OD setting (left) is illustrated in Figure 1.\nIn this section, we present details on our data prior for OD, training of FoMo-0D on prior-simulated datasets and inference on new datasets, and our specific model architecture and improvements for scalable training."}, {"title": "3.1 DESIGNING A DATA PRIOR FOR OUTLIER DETECTION", "content": "Arguably, what has triggered the recent breakthroughs in NLP and CV is the massive amounts of datasets available for (pre)training, along with high-capacity model architectures. In comparison to the natural language and image domains, the quantity (and quality) of publicly available tabular OD datasets is minuscule. Even in the presence of large quantities of data, in training their Chronos foundation models for time series forecasting, Ansari et al. (2024) show that using synthetic data in combination with real-world data improves the overall zero-shot performance. For these reasons, we design a new data prior from which we simulate numerous OD datasets for pretraining FoMo-0D.\nIdeally the data prior should reflect distributions as general and diverse as seen in real-world datasets, however, \"finding a prior supporting a large enough subset of possible [data generating] functions isn't trivial\" Nagler (2023). Surprisingly, in contrast, our first (and final) attempt has been sufficient to achieve astonishing performance even with a relatively straightforward and simple-to-implement data prior, which we describe next. (Informally: we didn't even try hard!)\nInlier synthesis: We designate the data prior for inliers to simply be a Gaussian Mixture Model (GMM) with m-clusters in d-dimensions, with centers $\u03bc_{jk} \u2208 [\u22125, 5]$, $j \u2208 [m]$, $k \u2208 [d]$ and diagonal $\\Sigma_j$ with entries also \u2208 [-5,5]. For each step of the every epoch of pretraining FoMo-0D, we create batch size B different GMMs with varying $m \u2264 M$ and $d \u2264 D$ chosen uniformly at random from [M] and [D], respectively. From each GMM, we draw a set of S inlier points, defined as instances within the 90th percentile of the GMM.\nOutlier synthesis: Following the previous literature on outlier synthesis Han et al. (2022), we generate subspace outliers by first drawing a subset of dimensions K at random, where $|K| \u2264 d$, and then generate S points from the corresponding \u201cinflated\u201d GMMs, which share the same centers \u03bc's with the original GMM but with the inflated (diagonal) covariances $5 \u00d7 \\Sigma_{j,kk}$'s for $k \u2208 K$. Outliers are defined as points outside the 90th percentile of the original GMM. We decide whether a drawn sample is within/outside the specified percentile based on its Mahalanobis distance computed analytically (see Property A.2 in the Appendix).\nSpecifically, we simulate datasets containing 2S = 10,000 samples (half inlier, half outlier) from the two corresponding GMMs (original and inflated) with up to M = 5 clusters and up to D = 100 dimensions. Example 2D synthetic datasets are illustrated in Appendix B."}, {"title": "3.2 (PRE)TRAINING AND INFERENCE", "content": "Model (Pre)Training (Once, Offline): FoMo-0D is a Prior-data Fitted Network (PFN, see Section 2.2) based on the Transformer architecture. In the synthetic prior-data fitting phase, it is trained on datasets drawn from our new OD prior for tabular data that we introduced in Section 3.1. Each dataset is simulated from a different GMM configuration based on randomly drawn parameters, and consists of varying number of training samples and dimensions to capture the diversity in real-world tabular datasets. Detailed steps are outlined in Algo. 1 in Appendix C.2, and described as follows.\nEach time, we first draw a hypothesis (i.e. GMM configuration) uniformly at random, that is $\\phi = \\{d \\in [D], m \\in [M], \\{\\mu_j\\}_{j=1}^m \\in [-5, 5]^d, \\{\\Sigma_j\\}_{j=1}^m ; diag(\\Sigma_j) \\in [-5, 5]^d\\}$, and then generate a dataset $D = \\{D_{in}, D_{out}\\}$ containing synthetic inlier and outlier samples from the drawn hypothesis and its variance-inflated variant, respectively.\nWe optimize FoMo-0D's parameters \u03b8 to make predictions on $D_{test} = \\{D_{test}^{in}, D_{test}^{out}\\}$, conditioned on the inlier-only training data $D_{train} \\subset D_{in}$ based on the cross-entropy loss (see Eq. (2)). During training, $D_{test}$ contains a balanced number of inlier and outlier samples, where $D_{test}^{in} = D_{in} \\setminus D_{train}$, and $D_{test}^{out} \\subset D_{out}$ contains an equal number of samples as $D_{test}^{in}$. To vary the training data input size, we subsample $D_{train}$ of randomly drawn size $n \u2208 [N_L, N_U ]$, where $n_L$ and $n_U$ denote the lower and upper bounds. In our current implementation, we set $n_L = 500$, and $n_U = 5,000$.\nFoMo-0D is trained on 200,000 batches (200 epochs \u00d7 1,000 steps/epoch) of B = 8 generated datasets in each batch. While this pretraining phase can be expensive, it is done only once, offline. Moreover, we introduce several scalability improvements to speed up pretraining, as discussed later in Section 3.3. Full details on the training and implementation of FoMo-0D are given in Appendix C.\nZero-shot Inference (on Unseen Dataset): During the inference phase, our pretrained-in-advance FoMo-0D can be employed on any unseen real-world dataset. In fact, we apply the same single pretrained network on all benchmark datasets in our experiments in this paper.\nSpecifically, for a new semi-supervised OD task with inlier-only training data $D_{train}$ and mixed test data $D_{test}$, feeding $(D_{train}, x_{test})$ as input to FoMo-0D (for each $x_{test} \u2208 D_{test}$ separately) yields the PPD $q_\u03b8(y | x_{test}, D_{train})$ in a single forward pass. As such, FoMo-0D performs model \u201ctraining\u201d and prediction simultaneously at test time. In fact, as the entire training data is passed as context, FoMo-0D leverages in-context learning (ICL) Xie et al. (2021); Garg et al. (2022) for inference. The key contribution of FoMo-0D goes beyond eliminating gradient-based model training for a new dataset: because no model training is required, one thus neither needs to choose any specific OD model to train, nor grapple with tuning any hyperparameters of the said model-rendering model selection an obsolete concern for the future of OD! Additionally, the speedy, easily parallelizable inference (for less-than-a-second per test sample) is then the \u201cicing on the cake\".\nFor a visual summary, Figure 1 (right), illustrates (top) pretrain & (bottom) test phases of FoMo-0D."}, {"title": "3.3 ARCHITECTURE AND SCALABILITY", "content": "Architecture and sample-to-sample attention: Like existing PFNs in the literature, FoMo-0D is based on the Transformer architecture Vaswani et al. (2017), encoding each sample's feature vector as a token, and allowing token representations to attend to each other, hence enabling sample-to-sample attention. We also adopt the three adaptations of TabPFN Hollmann et al. (2023), which (1) computes self-attention among all the training samples but only cross-attention from test samples to the training samples, (2) enables variable feature dimensionality by zero-padding, and (3) randomly rotates input samples while omitting positional encodings to achieve model invariance to sample permutations in the dataset. We defer the architecture details to the original papers.\nGiven $D_{train} = \\{x_1, ..., x_n\\}$, each self-attention layer outputs n embeddings $\\{z_i\\}_{i=1}^n$; where the i-th token is mapped via linear transformations to a key $k_i$, query $q_i$ and value $v_i$ based on which the i-th output is computed by weighing all $v_j$'s by the normalized dot product between $q_i$ and all the $k_j$'s (i.e. sample-to-sample dot product similarity) as\n$z_i = \\sum_{j=1}^n softmax(\\frac{\\{q_i, k_j\\}\\}_{j=1}^n).v_j,                                                                            \\qquad(3)$\nAs such, the sample-to-sample attention presents an intriguing intuition from the perspective of OD: Many classical shallow algorithms for OD Aggarwal (2013) are based on nonparametrics; in particular, they make use of the distances to the k nearest neighbors (kNNs) of a point to compute its outlierness (where k is a critical hyperparameter (HP)). One can think of FoMo-0D as mimicking non-parametric models but by using parametric attention mechanisms. Interestingly, PFNs are much more robust and flexible than kNN based OD approaches, for (1) sample-to-sample relations are not pre-specified but rather learned through attention weights, and thus (2) they are not limited to just the nearest neighbors but rather can learn which training points are worth attending to, and last but not least (3) as attention is dataset-wide across all points, there is not need for specifying a cut-off HP like k, to which most previous OD techniques are extremely sensitive to Aggarwal and Sathe (2015); Campos et al. (2016); Goldstein and Uchida (2016); Ding et al. (2022)\u2014to reiterate, algorithm & HP selection is bygone with FoMo-0D.\nWhile intuitively beneficial for OD, \u201cvanilla\" attention among the training samples incurs quadratic complexity. To be able to seize the benefits with scale, we incorporate a scalable architecture to our design, as we describe next. The scale up also unlocks a larger context (i.e. dataset) size for FoMo-0D, enabling its pretraining on larger datasets for potentially better generalization.\nScaling up attention with \u201crouters\u201d: The O(n\u00b2) quadratic sample complexity at pretraining presents an obstacle for achieving high performance at inference. From dataset size perspective, it limits pretraining to relatively small training datasets. From context size perspective, it limits in-context learning that typically benefits from longer context lengths Xie et al. (2021).\nToward a high-performance pretrained model, therefore, we scale up FoMo-0D's attention via the \u201crouter mechanism\u201d recently developed by Zhang and Yan (2023). As shown in Figure 2, the main idea is to learn a small fixed number ($R \u226a n$) of \u201crouters\u201d or representatives, which gather information from all n samples and then distribute the gathered information back to the n output embeddings, creating what-looks-like a \u201cbottleneck\u201d attention mechanism-reducing complexity from O(n\u00b2) to O(2Rn) = O(n). This design allows FoMo-0D training to scale linearly with respect to both dataset dimensionality d and also dataset size n.\nConcretely, the representatives first aggregate information from all samples by serving as query in multi-head self-attention (MSA) and the embedding array of all samples becomes both key and value:\n$M = MSA1(R, Z, Z),                                                                                                                                                     \\qquad(4)$\nwhere $R \u2208 R^{R\u00d7d}$ depicts the learnable vector array of representatives and M denotes the aggregated messages. Then, the routers distribute the received information among samples by using the sample embeddings as query and the aggregated messages as both key and value:\n$Z = MSA2(Z, M, M).                                                                                                                                                       \\qquad(5)$\nFinally, we obtain $Z = LayerNorm(\\hat{Z} + Z)$ after layer normalization. Note that the test samples only attend to the training samples' embeddings, computed in the described manner across layers, which finally feed into the prediction head for estimating each test sample's PPD at the output layer."}, {"title": "Scaling up (pre)training data synthesis with linear transforms", "content": "Besides the scalability challenge associated with architecture/attention, another computational challenge in pretraining FoMo-0D arises from drawing samples from the data prior. That is, generating samples from a pre-specified data distribution requires considerable time, especially in high dimensions, provided the large number of datasets we sample (concretely, a batch size of 8 datasets over 1,000 steps each for 200 epochs).\nTo give an idea, sampling a dataset with n = 10,000 points in d = 100 dimensions using 10 CPUs in parallel takes \u22480.4 seconds (see Appendix Figure 12). Across 200 training epochs with 1,000 steps each, it adds up to more than 177 hours just to generate 1,6 million datasets on-the-fly. Of course, one can trade storage with compute-time by generating all these datasets apriori via massive parallelism. Nevertheless, synthetic data generation demands considerable time (and/or storage).\nTo scale up data synthesis, FoMo-0D employs two distinct strategies. First, we propose reuse at epoch level: that is, one can reuse the same 8K unique datasets at every epoch, or in general, the same 8K\u00d7P datasets periodically at every P epochs. A larger P would lead to more diversity in terms of the overall pretraining data used, albeit a longer pretraining time.\nSecond, and more innovatively, we propose reuse at dataset level via transformation: that is, having generated one unique dataset $X \u2208 R^{n\u00d7d}$ from a GMM, we propose a linear transform T(x) of the form Wx + b for randomly drawn parameters $W \u2208 R^{d\u00d7d}$ and $b \u2208 R^d$ (see Appendix A.1). This simple yet efficient transformation creates a new dataset, akin to one being drawn from another GMM with centers $T(\u03bc_j) = W\u00b5_j + b$ and covariance $T(\u03a3_j) = W\u03a3_jW^T , \u2200j \u2208 [m]$. Note that we do not actually materialize these parameters but only transform the dataset. As we show in the following, such transformations preserve the Mahalanobis distances as well as the percentile thresholds for labeling points as inlier/outlier. All details and proofs are given in Appendix A.\nLemma 1 Linear transform T with invertible W on $G_\\theta^m$ preserves Mahalanobis distances.\nLemma 2 Linear transform T with invertible W on $G_\\theta^m$ preserves the percentiles of the GMM.\nThe implication of these lemmas is that a linear transformation of a dataset from a GMM retains the identity of the inliers and outliers, i.e. no relabeling is required. Moreover, notice that as a byproduct we obtain a transformed dataset as though drawn from a GMM with a non-diagonal covariance matrix which, besides the time savings, offers a slightly more complex data prior."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 SETUP", "content": "Pre-training Dataset Synthesis: During pretraining, we generate unique GMM datasets by first drawing a configuration, which includes feature dimensionality d \u2208 [D], number of com- ponents/clusters m \u2208 [M], cluster centers {$\u03bc_j$}$_{j=1}$ (each $\u03bc_j \u2208 [\u22125, 5]^d$) and covariances {$\\Sigma_j$}$_{j=1}$ (diag($\\Sigma_j$) \u2208 [\u22125,5]^d). We set M = 5 and vary D \u2208 {20, 100} to study pretraining with relatively small and high dimensional datasets, respectively. We then sample S = 5,000 points that are within the 90th percentile of the GMM. To synthesize outliers, we \u201cinflate\u201d a subset of dimensions by randomly choosing $|K| \u2208 [D]$ dimensions and multiplying the corresponding variances by \u00d75 (following Han et al. (2022)), i.e. 5 \u00d7 $\u03a3_{j,kk}$\u2019s for k \u2208 K, and then draw S = 5,000 samples from the inflated GMM that are outside the 90th percentile of the original GMM.\nTo speed up data synthesis via linear transformations, we first draw 500 unique datasets using m \u2208 [5] and $d\u2208 \\{1, 2, ..., 100\\}$ (i.e. 5\u00d7100) and transform each one 15\u00d7 using varying parameters (W, b)"}, {"title": "3.2 (PRE)TRAINING AND INFERENCE", "content": "Model (Pre)Training (Once, Offline): FoMo-0D is a Prior-data Fitted Network (PFN, see Section 2.2) based on the Transformer architecture. In the synthetic prior-data fitting phase, it is trained on datasets drawn from our new OD prior for tabular data that we introduced in Section 3.1. Each dataset is simulated from a different GMM configuration based on randomly drawn parameters, and consists of varying number of training samples and dimensions to capture the diversity in real-world tabular datasets. Detailed steps are outlined in Algo. 1 in Appendix C.2, and described as follows.\nEach time, we first draw a hypothesis (i.e. GMM configuration) uniformly at random, that is $\\phi = \\{d \\in [D], m \\in [M], \\{\\mu_j\\}_{j=1}^m \\in [-5, 5]^d, \\{\\Sigma_j\\}_{j=1}^m ; diag(\\Sigma_j) \\in [-5, 5]^d\\}$, and then generate a dataset $D = \\{D_{in}, D_{out}\\}$ containing synthetic inlier and outlier samples from the drawn hypothesis and its variance-inflated variant, respectively.\nWe optimize FoMo-0D's parameters \u03b8 to make predictions on $D_{test} = \\{D_{test}^{in}, D_{test}^{out}\\}$, conditioned on the inlier-only training data $D_{train} \\subset D_{in}$ based on the cross-entropy loss (see Eq. (2)). During training, $D_{test}$ contains a balanced number of inlier and outlier samples, where $D_{test}^{in} = D_{in} \\setminus D_{train}$, and $D_{test}^{out} \\subset D_{out}$ contains an equal number of samples as $D_{test}^{in}$. To vary the training data input size, we subsample $D_{train}$ of randomly drawn size $n \u2208 [N_L, N_U ]$, where $n_L$ and $n_U$ denote the lower and upper bounds. In our current implementation, we set $n_L = 500$, and $n_U = 5,000$.\nFoMo-0D is trained on 200,000 batches (200 epochs \u00d7 1,000 steps/epoch) of B = 8 generated datasets in each batch. While this pretraining phase can be expensive, it is done only once, offline. Moreover, we introduce several scalability improvements to speed up pretraining, as discussed later in Section 3.3. Full details on the training and implementation of FoMo-0D are given in Appendix C.\nZero-shot Inference (on Unseen Dataset): During the inference phase, our pretrained-in-advance FoMo-0D can be employed on any unseen real-world dataset. In fact, we apply the same single pretrained network on all benchmark datasets in our experiments in this paper.\nSpecifically, for a new semi-supervised OD task with inlier-only training data $D_{train}$ and mixed test data $D_{test}$, feeding $(D_{train}, x_{test})$ as input to FoMo-0D (for each $x_{test} \u2208 D_{test}$ separately) yields the PPD $q_\u03b8(y | x_{test}, D_{train})$ in a single forward pass. As such, FoMo-0D performs model \u201ctraining\u201d and prediction simultaneously at test time. In fact, as the entire training data is passed as context, FoMo-0D leverages in-context learning (ICL) Xie et al. (2021); Garg et al. (2022) for inference. The key contribution of FoMo-0D goes beyond eliminating gradient-based model training for a new dataset: because no model training is required, one thus neither needs to choose any specific OD model to train, nor grapple with tuning any hyperparameters of the said model-rendering model selection an obsolete concern for the future of OD! Additionally, the speedy, easily parallelizable inference (for less-than-a-second per test sample) is then the \u201cicing on the cake\".\nFor a visual summary, Figure 1 (right), illustrates (top) pretrain & (bottom) test phases of FoMo-0D."}, {"title": "4.3 ABLATION ANALYSES", "content": "In this section, we perform various ablations to study the effect of different design choices in FoMo- 0D; namely, A1. maximum pretraining data dimensionality D, A2. the number of routers R on a. cost and b. performance, A3. context size (both for training and inference), A4. number of unique datasets used for pretraining (i.e., reuse periodicity P), A5. data transformation T during synthesis on a. performance and b. speed up, A6. data diversity and prolonged training, and finally, A7. quantile transforming the benchmark datasets preceding inference.\nUnless stated otherwise, most ablation results are performed using FoMo-0D with $D = 20$, as it is faster to pretrain under these many varying settings."}, {"title": "A1. Effect of D:", "content": "How does FoMo-0D's generalization performance change by increasing dimensionality of the pretraining data?\nWe start by comparing FoMo-0D pretrained on datasets with up to D = 20 versus D = 100 dimensions. Note that learning on higher dimensional datasets is harder, as evident from the relatively larger pretraining loss as shown in Appendix Figure 10. While the statement is accurate in general, it is also partly because subspace outliers \"hide\" better in higher dimensions.\nComparing Table 1 (D = 100) with Table 2 (D = 20) w.r.t. p-values over All datasets, we find that FoMo-0D at larger scale does better, where all p-values are larger for D = 100 than D = 20. We find that FoMo-0D with D = 20 performs well on datasets with d \u2264 20 (i.e., \u201con its own game\u201d), however beyond its pretraining setting, e.g. on datasets with d \u2264 50, D = 100 is superior to D = 20 as shown in Appendix Table 8."}, {"title": "A2.a. Effect of routers on cost:", "content": "What is the running time and memory cost of FoMo-0D with & w/out router-based attention?\nFigure 4(left) shows the average inference time per test sample, comparing FoMo-0D using a router- based attention mechanism with R = 500 routers (in green) versus FoMo-0D using typical attention without any routers (in blue). As inference context size increases, running time for traditional attention grows quadratically while router mechanism scales linearly."}, {"title": "A2.b. Effect of routers on performance:", "content": "What is the impact of the number R of routers (or representatives) on performance?\nRouter-based mechanism allows to trade-off running time with expressiveness of the attention and hence performance. Figure 4(right) shows the p-values of the Wilcoxon signed rank test as the number of routers R is increased from 100 to 200 and 500, comparing FoMo-0D to each of the top-6 baselines. We notice that FoMo-0D performance tends to increase monotonically with more routers."}, {"title": "A3. Effect of context size:", "content": "What is the impact of context size, both during model pretraining as well as during inference?\nTo study how performance changes by context size, we train FoMo-0D with varying context size in {1K,2K,5K} and employ each pretrained model for inference with varying context size in {1K,2K,5K, 10K}. Table 4 shows the results, where performance is depicted by the average rank of FoMo-OD (the lower, the better).\nWe find that training with a larger context improves performance at any inference context"}]}