{"title": "Synthetic Data Generation for Residential Load Patterns via Recurrent GAN and Ensemble Method", "authors": ["Xinyu Liang", "Ziheng Wang", "Hao Wang"], "abstract": "Generating synthetic residential load data that can accurately represent actual electricity consumption patterns is crucial for effective power system planning and operation. The necessity for synthetic data is underscored by the inherent challenges associated with using real-world load data, such as privacy considerations and logistical complexities in large-scale data collection. In this work, we tackle the above-mentioned challenges by developing the Ensemble Recurrent Generative Adversarial Network (ERGAN) framework to generate high-fidelity synthetic residential load data. ERGAN leverages an ensemble of recurrent Generative Adversarial Networks, augmented by a loss function that concurrently takes into account adversarial loss and differences between statistical properties. Our developed ERGAN can capture diverse load patterns across various households, thereby enhancing the realism and diversity of the synthetic data generated. Comprehensive evaluations demonstrate that our method consistently outperforms established benchmarks in the synthetic generation of residential load data across various performance metrics including diversity, similarity, and statistical measures. The findings confirm the potential of ERGAN as an effective tool for energy applications requiring synthetic yet realistic load data. We also make the generated synthetic residential load patterns publicly available\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "THE crucial role of individual user load data in power system operation, such as distribution system operation and home energy management, underscores the necessity of having access to such data for effective decision-making. Specifically, load data, which characterizes users' electricity demand, can be utilized for various studies, including residential load forecasting [1], load disaggregation [2]\u2013[4], electricity theft and anomaly detection [5], [6], and electric vehicle and changing detection [7]\u2013[9]. These studies are crucial for understanding energy consumption behaviors and facilitating subsequent applications such as renewable energy integration, home energy management, demand response, transactive energy, distribution grid planning, and voltage control [10]. The operational effectiveness of these applications relies largely on accessing and utilizing such load data [11], [12].\nNevertheless, obtaining accurate and granular load data of residential users presents unique challenges. Unlike the industrial and commercial sectors with their relatively predictable load profiles, residential load is influenced by a myriad of factors - weather conditions, geographical location, household size, and diverse appliance usage patterns [10], [13]. With the increasing adoption of residential renewable energy systems and smart home technologies, these patterns have become even more complex, underscoring the need for high-quality residential load data. However, accessing and collecting real-world residential load data are often hindered by stringent privacy concerns and formidable logistical complexities [14]. As a result, the development of methods for generating synthetic residential load data, which accurately mimics real-world patterns without infringing upon privacy norms, is rapidly gaining attention.\nIn response to the growing need for residential load data, numerous studies have sought to develop load data generation methods. A prominent approach within this body of research is employs physical modeling methods [15]\u2013[20]. These models typically account for various physical factors that influence residential energy consumption, such as the type and number of electric appliances, the insulation characteristics of the housing, and the behavior patterns of the residents. More specifically, these models often simulate the operation of each individual appliance within a household based on user be- haviors, weather conditions, appliance use, and other housing characteristics. The resulting consumption patterns of each individual component are then aggregated to form the overall load profile of the household. Although this method can yield highly accurate and detailed synthetic load data, it is inher- ently complex in collecting information concerning driving factors and requires substantial computational resources. For example, collecting detailed appliance and user behavior data can be challenging and often infringe on privacy, presenting a significant limitation to this approach.\nTo tackle these limitations, traditional statistical or prob- abilistic methods have emerged as a more feasible alterna- tive to physical modeling, particularly for directly simulating aggregate-level load data [21]\u2013[25]. By leveraging the statis- tical characteristics of historical load data, this approach gen- erates synthetic profiles that are representative of residential electricity consumption. In essence, these methods transcribe aggregated load data into synthetic equivalents, bypassing the needs for detailed appliance, housing, or occupant specific information. This approach not only preserves privacy but also simplifies the data collection process. Various techniques, such as Markov chain models, Copulas models, and Gaussian mix- ture models, fall into this category and have exhibited satisfac- tory performance in extracting key characteristics and patterns of load data. However, traditional statistical or probabilistic methods have their limitations. Owing to the multifaceted fac- tors influencing electricity consumption, a key shortcoming of this approach lies in its inability to accurately capture complex and non-linear relationships inherent in residential load data. The oversimplification caused by these methods often leads to synthetic data, which is statistically similar but lacks fidelity to real-world patterns. Therefore, while these traditional methods address some of the issues encountered in physical modeling, they fall short of comprehensively capturing the complexities of residential load profiles.\nRecognizing the limitations of traditional statistical and probabilistic methods in directly modeling residential-level load, there has been a shift towards deep learning based techniques in recent years. Such techniques have demonstrated successful applications in a broad spectrum of energy data related topics, such as renewable energy scenario generation [26], renewable power forecasting [27], battery lifecycle fore- casting [28], wind farm turbine fault detection [29], residential load forecasting [30], and system level load data generation [31], [32]. The distinguishing feature of these methodologies is their innate ability to learn and model complex data patterns. However, the direct application of these methods for resi- dential load data generation presents unique challenges. The distinct granularity, dynamics, and heterogeneity are inherent in residential load profiles necessitate tailored solutions that can effectively capture this intricacy and faithfully generate synthetic data without compromising on diversity or realism.\nA recent study has successfully employed the Auxiliary Clas- sifier Generative Adversarial Network (ACGAN) to generate residential load data [33]. Despite the relative advancement offered by ACGAN, it has still been identified to potentially compromise the diversity in the generated data. This limitation primarily stems from the dual functionality of the discrimina- tor within the ACGAN model which is tasked with classifying real or synthetic data while simultaneously determining the data class. This conditional Generative Adversarial Network (GAN) based method can inadvertently result in limiting the generator's ability to learn from diverse data patterns that belong to the same class, thus affecting its diversity.\nIn response to the research gap identified above, we de- velop the Ensemble Recurrent Generative Adversarial Network (ERGAN) model. ERGAN effectively integrates ensemble learning and recurrent GAN architectures, aimed at effectively capturing the complexity of diverse household load patterns. In addition, ERGAN incorporates statistical properties in the loss function, alongside adversarial losses. This dual focus ensures a closer resemblance of the generated load patterns to the original distribution. This innovation enhances the realism and diversity of synthetic load data, thereby presenting a promising advancement for power system operation research relying on load data. The contributions of this paper are summarized as follows.\n\u2022 This work presents an effective framework, named ER- GAN, to generate synthetic residential load patterns. It effectively encapsulates the complexity and diversity of residential load patterns, ensuring that the synthetic data maintains high fidelity with real-world scenarios.\n\u2022 By leveraging the strengths of an ensemble of recurrent GANS, ERGAN diversifies and elevates the quality of the generated synthetic data. This distinctive architecture sets the ERGAN framework apart from existing studies, especially in terms of data diversity.\n\u2022 The ERGAN framework introduces a unique loss func- tion implementation that integrates statistical property differences along with the adversarial loss. This further ensures the generated data's alignment with the original distribution. This approach contributes significantly to the model's superior performance.\n\u2022 We evaluate the ERGAN framework against several state- of-the-art benchmarks in residential load pattern data generation across different performance metrics, such as diversity, similarity, and statistical measures, ensuring a comprehensive assessment."}, {"title": "II. METHODOLOGY", "content": "The methodology of this study is established on the syner- gistic integration of K-means clustering and GANs to generate synthetic residential load patterns, as shown in Figure 1 and Algorithm 1, which we will explain in detail later. In the first phase, the data is divided into K discrete clusters utilizing K-means clustering. For each of these K clusters, a separate GAN model is trained independently to learn the data distribution specific to that cluster. This design allows the ERGAN framework to capture the unique characteristics and variabilities of different clusters more effectively than training a single GAN model on the entire dataset. To do so, this partitioned data serves as the input for subsequent GAN mod- els, which encompass a generator and a discriminator. Both these components employ Bi-Directional Long Short-Term Memory (Bi-LSTM) [34] networks for data generation. The integrated loss function in our models combines adversarial losses with statistical property differences, thereby maintaining alignment with the original data distribution. Upon completion of the GAN models' training, the generated outputs from all models are consolidated to yield the final synthetic residential load data. The following subsections provide a more detailed exposition of each component of the ERGAN framework."}, {"title": "A. Problem Formulation", "content": "The residential load pattern generation can be formulated as a time-series generation problem. Let us consider a dataset Dof residential load patterns, each denoted as a time-series sequence x = (xt)T t=1, where T represents the time duration and xt represents power consumption in t-th time slot.\nOur goal is to create a generative framework Gen that can generate synthetic residential load patterns \\(\\hat{x} = (\\hat{x}_t)_{t=1}^T\\) to construct synthetic dataset \\(\\hat{D}\\). More specifically, at the dataset level, the objective is to minimize the divergence between the distribution of the real load patterns \\(P_D(x)\\) and the distribution of the generated load patterns \\(p_{\\hat{G}}(\\hat{x})\\), formulated as:\n\nmin_{Gen} Div (PD(x)||p\u011c(x)),\n\n(1)\nwhere Div is a divergence measure.\nSince the load patterns exhibit temporal dependencies, the generative framework should capture these dependencies to generate realistic patterns. Thus, the generative model should minimize the divergence between a synthetic power consump- tion pattern \\(\\hat{x}_m\\) and original power consumption pattern \\(x_m\\) at any time slot \\(m \\in \\{2, ...T \u2013 1\\}\\) conditioned on both the previous and future time slots:\n\nmin_{Gen} Div(PD (xm|(xt)_{t=1}^{m-1}, (xt)_{t=m+1}^T ) ||\np_{G} (\\hat{x}_t |(\\hat{x}_t)_{t=1}^{m-1}, (\\hat{x}_t)_{t=m+1}^T )).\n\n(2)\nThe generative framework thus can generate synthetic load patterns that not only follow the historical consumption pattern up to the current time slot but also anticipate the future consumption pattern."}, {"title": "B. K-means Clustering and Davies-Bouldin Score for Optimal Cluster Selection", "content": "As shown in Figure 1, the construction of our ERGAN framework begins with the application of K-means clustering to the residential load pattern dataset \\(D = \\{x_i\\}_{i=1}^N\\), where N represents the total number of load patterns and \\(x_i\\) represents the i-th load pattern in the dataset. Our method creates an initial grouping of the load patterns into K distinct clusters. By dividing the load patterns in this manner, we aim to capture the inherent structures and variances within the dataset, laying the foundation for the production of synthetic load patterns that are both diverse and representative.\nFor each cluster denoted as Ck, where k = 1,..., K, we denote the centroid as ck. The objective of the K-means clustering algorithm is to minimize the within-cluster sum of squares (WCSS), mathematically defined as:\n\nmin \\sum_{k=1}^K \\sum_{x_i \\in C_k} ||x_i - c_k||^2.\n\n(3)\nThe clustering algorithm iteratively executes two operations: cluster assignment and centroid updating. In the cluster assign- ment step, each load pattern profile x\u2081 is assigned to the cluster with the closest centroid cj. Mathematically, this assignment is expressed as:\n\nCluster assignment: \\(x_i \\in C_k\\), \\(k = \\arg \\min_j ||x_i - c_j||^2\\),\n\n(4)\nwhere j \u2208 {1...K} is an index used to iterate over all clusters, and cj is the centroid of cluster j.\nIn the centroid updating step, the position of each centroid ck is recalculated based on the current members of its cluster. This step is defined as:\n\nCentroid update: \\(c_k := \\frac{1}{|C_k|} \\sum_{i \\in C_k} x_i\\),\n\n(5)\nwhere |Ck| symbolizes the number of load patterns associated with the cluster Ck. These two stages are iterated until a steady state is reached (the cluster assignments no longer fluctuate), or until a pre-determined maximum number of iterations has been achieved.\nThe selection of the number of clusters K is a critical aspect of the algorithm's execution. An appropriate K cap- tures the granularity of the load patterns, therefore, to guide this selection, we employ the Davies-Bouldin (DB) index-a measurement that quantifies the average similarity between each pair of clusters [35]. The DB index is formally defined as:\n\nDB = \\frac{1}{K} \\sum_{k=1}^K \\max_{k' \\neq k} (R_{kk'}),\n\n(6)\nwhere Rkk' is a measure of the similarity between two clusters Ck and Ck', given by the formula:\n\nR_{kk'} = \\frac{s_k + s_{k'}}{d_{kk'}},\n\n(7)\nin which sk denotes the average distance of all patterns in cluster Ck to its centroid ck, defined as:\n\ns_k = \\frac{1}{|C_k|} \\sum_{x_i \\in C_k} ||x_i - c_k||,\n\n(8)\nand dkk' is the Euclidean distance between centroids ck and ck' of clusters Ck and Ck', calculated as:\n\nd_{kk'} = ||c_k - c_{k'}||_2.\n\n(9)"}, {"title": "C. Generative Model Design and Construction", "content": "After identifying the clusters and their corresponding cen- troids, for each cluster Ck, we construct a recurrent GAN model, which consists of a generator and a discriminator.\nResidential load profiles exhibit strong temporal correlations influenced by factors such as household activities, weather conditions, and energy usage habits. This results in power consumption that often depends on past consumption and po- tential future consumption. Bi-LSTM networks are particularly suitable for this context as they can process sequences in both forward and backward directions, enabling them to capture complex temporal dependencies present in residential load patterns. Specifically, Bi-LSTM offers a superior capacity to model dependencies between load consumption both back- ground and forward across various time slots, making them highly proficient in generating residential load profiles. Thus, both the generator and the discriminator are implemented using Bi-LSTM networks.\nFor the GAN model of the k-th cluster, we denote the gener- ator as Gk and the discriminator as Dk. For each GAN model, the generator Gk takes a random noise vector z sampled from a predefined noise distribution pz(z) and produces synthetic load patterns \\(\\hat{x}^k = (\\hat{x}^k_1, \\hat{x}^k_2,..., \\hat{x}^k_T)\\).\nThe following Equations (10) to (13) detail the step-by-step process within Gk to generate synthetic load patterns \\(\\hat{x}^k\\). At each time step t, the generator produces a forward hidden state \\(\\overrightarrow{h_t}^{G_k}\\) and a backward hidden state \\(\\overleftarrow{h_t}^{G_k}\\) using the Bi-LSTM cells:\n\n\\overrightarrow{h_t}^{G_k} = LSTM(z_t, \\overrightarrow{h_{t-1}}^{G_k};\\theta_G^k),\n\n(10)\n\n\\overleftarrow{h_t}^{G_k} = LSTM(z_t, \\overleftarrow{h_{t+1}}^{G_k};\\theta_G^k).\n\n(11)\nThe forward hidden state \\(\\overrightarrow{h_t}^{G_k}\\) captures the dependencies on the previous time steps, while the backward hidden state \\(\\overleftarrow{h_t}^{G_k}\\) captures the dependencies on the future time steps. These forward and backward hidden states are then concatenated to form the combined hidden state \\(h_t^{G_k}\\) at time step t:\n\nh_t^{G_k} = [\\overrightarrow{h_t}^{G_k}, \\overleftarrow{h_t}^{G_k}].\n\n(12)\nThe combined hidden state \\(h_t^{G_k}\\) encapsulates both past and future dependencies, enabling the generator to generate realistic load patterns that follow the temporal dependencies present in the original data. Finally, the load value at time step t is generated by passing the combined hidden state \\(h_t^{G_k}\\) through a non-linear function f, which is a fully connected layer:\n\n\\hat{x}^k_t = f(h_t^{G_k};\\theta_G^k),\n\n(13)\nwhere \\(\\theta_G^k\\) represents the trainable parameters of the generator network. The generator Gk trained on data from cluster Ck, utilizes the above process to generate synthetic load patterns that are inherently associated with the corresponding cluster. The collection of synthetic load patterns is used to form the synthetic clustered dataset denoted as \\(\\hat{C}_k\\), which will be formally defined in Equation (23)."}, {"title": "D. Generative Model Training", "content": "The training process for each GAN model involves a two- player min-max game between Gk and Dk, with the value function being:\n\nmin_{G_k} \\max_{D_k} V (D_k, G_k) =\\mathbb{E}_{x^k\\sim p_{C_k}(x^k)}[log D_k(x^k)]+\n\\mathbb{E}_{z \\sim p_z(z)} [log(1\u2013 D_k(G_k(z)))],\n\n(18)\nwhere \\(p_{C_k}(x^k)\\) is the distribution of real load patterns in the k-th cluster. In the minimax game, both the generator and discriminator are optimized alternately, where Gk is trained to generate synthetic load patterns that Dk cannot distinguish from the real ones, while Dk is simultaneously trained to improve its ability to distinguish real patterns from generated ones. Formally, we define the loss function for Gk and Dk as:\n\nL_{G_k} =\\mathbb{E}_{z\\sim p(z)} [log(1 \u2013 D_k(G_k(z)))]+\n\\lambda(||\\mathbb{E}_{z\\sim p(z)} [\\mu_{G_k}(z)] - \\mathbb{E}_{x^k\\sim p_{C_k}(x^k)} [\\mu_{x^k}]||+\n||\\mathbb{E}_{z\\sim p(z)} [\\sigma_{G_k}(z)] - \\mathbb{E}_{x^k\\sim p_{C_k}(x^k)} [\\sigma_{x^k}]||),\n\n(19)\n\nL_{D_k} = - \\mathbb{E}_{x^k \\sim p_{C_k}(x^k)} [log D_k(x^k)]-\n\\mathbb{E}_{z\\sim p(z)} [log(1-D_k(G_k(z)))],\n\n(20)\nwhere \\(\\mu_{G_k}(z)\\) and \\(\\sigma_{G_k}(z)\\) represent the mean and variance of the synthetic load pattern generated by the generator G; \\(\\mu_{x^k}\\) and \\(\\sigma_{x^k}\\) represent the mean and variance of the original load pattern x; \\(\\lambda\\) is a factor controlling the importance of the statistical match.\nIn our work, we set \\(\\lambda\\) to a large value (i.e., 100) to heavily emphasize the importance of the statistical match in the generator's optimization process. This choice is due to the application of K-means clustering process, where the residential load patterns within one cluster should have sta- ble statistical properties while still demonstrating temporal variability. Thus, by setting a large value for \\(\\lambda\\), we heavily emphasize the generator's ability to match these statistical properties of the real load patterns. This mechanism effectively pushes the generator to not only capture the temporal dynamics but also to replicate the overall statistical characteristics of the real data. In effect, the large \\(\\lambda\\) value acts as strong guidance for the generator, ensuring that the synthetic load patterns generated are representative and realistic at both the micro (time-dependent fluctuations) and macro (overall statistical properties) levels. The detailed training hyperparameters are provided in Table II in Section IV."}, {"title": "E. Generation of Synthetic Dataset via Ensemble Method", "content": "After training the GANs for each distinct cluster Ck, we have effectively obtained a set of generators denoted as Gk. Each Gk is proficient in generating synthetic load patterns that effectively capture the distinct characteristics associated with their respective residential load clusters. The final step in this framework involves creating a comprehensive synthetic dataset that encapsulates the wide-ranging diversity present in the original dataset. To achieve this, we employ an ensemble approach for combining the synthetic data generated from each individual recurrent GAN.\nGiven a desired volume M for the synthetic dataset, we commence by identifying the proportion \\(\\alpha_k\\) of the original dataset D that each cluster Ck constitutes. This can be math- ematically expressed as:\n\n\\alpha_k = \\frac{|C_k|}{N}\n\n(21)\nSubsequently, we calculate the number of synthetic load patterns Mk that ought to be generated from each GAN Gk. This is achieved by multiplying the desired synthetic dataset volume M by the respective cluster's proportion \\(\\alpha_k\\):\n\nM_k = [M \\cdot \\alpha_k].\n\n(22)\nFollowing this, we generate Mk synthetic load patterns from each GAN corresponding to each cluster Ck, resulting in a synthetic clustered dataset \\(\\hat{C}_k\\). This strategic approach ensures that the synthetic dataset \\(\\hat{D}\\) resonates with the original dataset D in terms of exhibiting a diverse spectrum of load characteristics. Subsequently, we merge the synthetic load pat- terns generated from all GANs to assemble the final synthetic dataset \\(\\hat{D}\\):\n\n\\hat{C}_k = G_k(Z_k),\n\n(23)\n\n\\hat{D} = \\bigcup_{k=1}^K \\hat{C}_k,\n\n(24)\nwhere Zk symbolizes the input noise vector for the k-th GAN, with each Zk being a matrix with the size of \\(M_k \\times T\\)."}, {"title": "III. BENCHMARK MODELS AND EVALUATION METHODS", "content": "In this section, we present our selection of benchmark mod- els and the corresponding evaluation methodologies employed to assess the performance of synthetic residential load pattern generation. The benchmark models considered in this study include the ERGAN-baseline, Auxiliary Classifier Generative Adversarial Network (ACGAN), Wasserstein Generative Ad- versarial Network (WGAN), and Continuous RNN-GAN (C- RNN-GAN), each providing unique attributes and benefits rel- evant to our problem setting. We provide the rationale behind these selections and highlight their differences compared to ERGAN. Subsequently, we present the evaluation methods deployed to assess the quality and diversity of the synthetic load patterns, facilitating a comprehensive and interpretable comparison between ERGAN relative and above-mentioned benchmark models."}, {"title": "A. Benchmark Models", "content": "In this study, we perform a comparative evaluation of our proposed ERGAN framework alongside state-of-the-art mod- els, providing a robust assessment of ERGAN's performance in generating realistic synthetic residential load patterns. The selected benchmark models for comparison covers a range of techniques that have demonstrated considerable promise in the task of generative modeling, particularly time-series data generation. Each of these models brings unique capabilities, providing a robust basis for comparison with our proposed ERGAN framework. Details are discussed as follows.\n\u2022 ERGAN-Baseline serves as a benchmark model in our study, representing the ERGAN framework without the K-means clustering and ensemble methods. By com- paring its performance against the complete ERGAN framework and other benchmark models, we can evaluate the added value and effectiveness of techniques developed in this paper, such as clustering and ensemble methods, in generating diverse and realistic synthetic residential load patterns.\n\u2022 Auxiliary Classifier GAN (ACGAN) [36] extends the traditional GAN structure by conditioning both the gen- erator and the discriminator on class labels, thereby enabling the generation of specific classes of data. It has been previously utilized in a similar context for residential load generation [33], showing its suitability as a benchmark for our study. Their conditioning of ACGAN is similar to our use of cluster information in generating load patterns, although the methods differ substantially. ACGAN is a form of conditional GAN where the same model is used for all categories, con- ditioned on the category label. In contrast, our ERGAN framework employs an ensemble of GANs, each tailored to a specific cluster, and thus being capable of capturing distinct characteristics across clusters. However, despite its merits, ACGAN uses convolutional neural network (CNN) which does not incorporate the advantages of Bi- LSTM in handling time-series data and does not focus on the preservation of statistical properties, as our framework does.\n\u2022 Wasserstein GAN (WGAN) [37] provides an alterna- tive approach to the traditional GAN loss function to address the issue of training instability, which is known as a common challenge in training GAN models. Its notable characteristic of improved training stability and convergence offers a valuable benchmark for comparison with our ERGAN model, which also emphasizes stable and efficient training. Nevertheless, WGAN also uses CNN which might not inherently accommodate the global temporal dependencies present in our load pattern data as well as our approach does.\n\u2022 C-RNN-GAN [38] is a robust model for time-series generation, harnessing the power of recurrent neural networks (RNN) and adversarial training from GANs. It uses LSTM units, ideal for learning and remembering long-term dependencies in the data, which are vital in the context of residential load patterns. Despite these strengths, the C-RNN-GAN does not explicitly cater to the diversity observed in residential load patterns across different households, a feature our ensemble of Bi-LSTM GANs addresses. Furthermore, C-RNN-GAN, although using Bi-LSTM units in the discriminator, does not incorporate a bidirectional structure in its generator which lacks consideration of the time-series data forward.\nBy comparing our ERGAN framework with these diverse benchmark models, we aim to thoroughly evaluate and val- idate the performance and effectiveness of our approach in generating synthetic residential load patterns."}, {"title": "B. Evaluation Methods", "content": "The quality of the synthetic residential load patterns gener- ated by our ERGAN framework and the selected benchmark models are evaluated via three distinct but complementary methods, with details described as follows.\n\u2022 Visual Examination of Real and Synthetic Load Patterns and Their Autocorrelation: We randomly select multiple real load pattern samples and compare them to the samples generated by different models. Each generated sample is selected to match the real samples based on the minimum Euclidean distance. Additionally, we em- ploy auto-correlation techniques to examine correlations between observations at different points in the time series, enriching our analysis of similarities and differences.\n\u2022 Comparative Histograms of Original and Synthetic Residential Load Patterns: This metric employed histogram-based comparison to visualize the distribu- tional attributes of original and synthetic data. In this approach, all load patterns produced by a generative model are aggregated together, and its histogram is plot- ted alongside that of the original data. By investigating to what extent the histogram of generated synthetic load patterns aligns with that of the original data, we can assess the model's ability to accurately replicate the data's global statistical properties.\n\u2022 Hourly Comparative Boxplots for Original and Syn- thetic Load Patterns: Our evaluation methodology fur- ther incorporates a time-centric examination of the gen- erated load patterns. Owing to the data's hourly nature, we create box plots for each time slot, showcasing the distribution of generated and original data per time step. This approach can provide insights into the model's capacity to mimic the temporal fluctuations of residential load patterns.\n\u2022 Comparative Visualization of T-SNE Dimension-Reduced Load Patterns Another component of our evaluation method is the use of t-SNE visualizations. This high-dimensional data visualization technique enables us to compare the manifold structures of original and syn- thetic load patterns. The closeness of synthetic data points to those of the original data in this reduced-dimension space indicates the model's proficiency in preserving the manifold structure of high-dimensional load patterns.\nQuantitative Evaluation using Statistical Distances: The final component of our evaluation method is to assess the statistical similarity between the original and synthetic datasets is by comparing key statistical prop- erties: the mean, variance, 25th percentile (Q1), and 75th percentile (Q3) profiles. The L1 distance (sum of absolute differences) is calculated for each property's profile between the original and synthetic datasets. The model demonstrating the lowest L1 distances across these properties is deemed to have generated synthetic data that most closely aligns with the statistical characteristics of the real data."}, {"title": "IV. RESULT AND ANALYSIS", "content": "This section firstly provides an in-depth description of the residential load pattern data used in this paper. Following this, we present a comprehensive performance evaluation using the metrics described in Section III, including the real and synthetic sample sets visualized in pattern and autocorrelation depicted in Figure 3, comparative histograms depicted in Figure 4, hourly comparative boxplots depicted in Figure 5, and comparative visualization of t-SNE dimension-reduced load patterns depicted in Figure 6. Through these robust assessments, we aim to demonstrate the effectiveness of our ERGAN framework in generating synthetic residential load patterns that accurately capture the statistical, temporal, and structural properties of real-world data."}, {"title": "A. Data Description and Model Setup", "content": "In this study, we use the Pecan Street dataset [39], which provides hourly residential energy consumption data from 417 households for the entire year from January 1, 2017 to December 31, 2017. This dataset contains hourly smart meter readings, and we specifically use total household energy consumption from all electrical sources within each household.\nWhile our model does not explicitly separate the data by day types or seasons, the inclusion of data spanning all days of the week and all seasons ensures that our synthetic load patterns reflect the inherent variability associated with different times of the year. To ensure the integrity of our analysis, each 24-hour load profile is segmented from the smart meter data, and all sequences containing missing values are prudently removed. Subsequently, to concentrate on load patterns rather than the magnitude of electricity consumption, each sequence is normalized using linear scaling. This normalization process ensures that each sequence falls within the range [0,1], al- lowing for consistent analysis and comparison across various households. The well-curated dataset is then partitioned into training, validation, and testing subsets, with 70% of the data dedicated to training, and the remaining 30% for validation. For our proposed ERGAN model, we present its model architecture in Table I and training hyperparameter setting in Table II, respectively."}, {"title": "B. Number of Cluster Selection and its Impact", "content": "In the ERGAN framework, the choice of the number of clusters K is a key factor that influences both the quality of synthetic data and the computational efficiency of the model. The Davies-Bouldin (DB) index is employed to assess clustering quality by evaluating the separation and cohesion of clusters, where a lower DB index generally suggests a better configuration for capturing residential load patterns. However, it is important to note that there is no definitively optimal K; the choice of K is inherently dependent on the method used and the specific characteristics of the dataset. While the DB index provides useful guidance, the primary goal is to generate synthetic data that reflects the diversity and complexity of real-world residential loads while managing computational overhead.\nChoosing K involves balancing the benefits of a more detailed data representation against the challenges of increased computational demands. For example, a larger K can offer a more detailed representation of the data distribution, po- tentially enhancing the diversity and realism of the synthetic data. However, such a larger K also requires training more generative models, leading to increased computational over- head and a greater risk of overfitting, where models may capture noises rather than meaningful patterns. Additionally, too many clusters can fragment coherent data structures, impacting the interpretability and quality of the generated data. Conversely, selecting a smaller K may reduce computational costs but risks underfitting, where the model oversimplifies the data and fails to capture its variability. This can result in synthetic data that lacks the necessary diversity to accurately represent residential energy consumption patterns. To balance these trade-offs, we calculated the DB index for cluster sizes from 2 to 12 as shown in Figure 2. The results suggest that K = 10 provides a practical balance, effectively capturing diverse load patterns without excessive computational costs or sacrificing data quality. This choice aligns with our study's objectives and offers a reasonable balance between complexity, efficiency, and quality. The effectiveness of this clustering strategy is evident in the ERGAN model's performance, which shows better results than benchmark models in capturing the statistical and temporal characteristics of residential load data. The detailed performance comparisons and results of ERGAN with K = 10 are discussed in the subsequent sections."}, {"title": "C. Results Analysis and Insights", "content": "1) Outstanding Performance of ERGAN over Benchmark Models in Ensuring Diversity and Similarity in Synthetic Load Patterns: The effectiveness of ERGAN in creating synthetic residential load patterns is affirmed through an comprehensive comparison with the benchmark models. Figure 3 reveals that all models", "Models": "The evaluation of generative models re- veals the superiority of RNN models, specifically the ERGAN, ERGAN-baseline, and C-RNN-GAN models, over the CNN based models, such as WGAN and ACGAN. This finding is supported by the observed characteristics in the evaluation plots. Specifically, the comparative histograms shown in Fig- ure 4 demonstrate that the synthetic load patterns generated by the recurrent GAN models in ERGAN closely align with the distributional attributes of the original data, indicating their proficiency in replicating the global statistical proper- ties. The quantitative results, presented in Table III, further solidify this observation. The RNN-based models, including ERGAN, ERGAN-baseline, and C-RNN-GAN, exhibit consid- erably lower L1 distances compared to WGAN and ACGAN, indicating their superior ability to replicate the statistical characteristics of the real data. Moreover, the hourly boxplots shown in Figure 5 showcase the ability of the recurrent models to capture the temporal fluctuations present"}]}