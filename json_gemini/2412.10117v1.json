{"title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models", "authors": ["Zhihao Du", "Yuxuan Wang", "Qian Chen", "Xian Shi", "Xiang Lv", "Tianyu Zhao", "Zhifu Gao", "Yexin Yang", "Changfeng Gao", "Hui Wang", "Fan Yu", "Huadai Liu", "Zhengyan Sheng", "Yue Gu", "Chong Deng", "Wen Wang", "Shiliang Zhang", "Zhijie Yan", "Jingren Zhou"], "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.", "sections": [{"title": "1 Introduction", "content": "In recent years, neural text-to-speech (TTS) synthesis models have garnered significant attention for surpassing traditional concatenative and statistical parametric methods [1-7]. These models have achieved high fidelity and naturalness on pre-defined specific speakers. Recent studies show that zero-shot TTS models are able to synthesize speech for any speaker by imitating the timbre, prosody and style of a reference speech [8]. Beyond their in-context learning (ICL) capability, zero-shot TTS models benefit from large-scale training data, achieving synthesis quality and naturalness nearly indistinguishable from human speech.\nRecent zero-shot TTS models can be broadly divided into three categories: codec language models, feature diffusion models and their hybrid systems. Codec language models utilize a speech codec model to extract discrete speech representation [9-11] and employ an autoregressive [8, 12\u201317] or masked [18] language model to predict the speech tokens, which are then synthesized to waveforms via codec vocoders [19,20]. Continuous speech representations are also explored in [21]. Language model-based TTS can generate varied and prosody-consistent speech via autoregressive sampling."}, {"title": null, "content": "Inspired by advances in image generation, denoising diffusion [22, 23] and flow matching models [24] have been introduced into non-autoregressive (NAR) speech synthesis. Early diffusion-based TTS models required duration prediction for each text (phone) to address the length disparity between text and speech features [25-28]. However, this rigid alignment can affect naturalness, resulting in flat prosody. To mitigate this issue, cross-attention and Diffusion Transformers (DiT) have been introduced into NAR TTS models [29,30]. Recent research indicates simpler approaches for text-speech alignment in NAR TTS models, such as E2 TTS [31], F5-TTS [32] and Seed-TTS [33]. In these models, input text is padded with special tokens to match the total speech length which is either automatically predicted by the utterance duration prediction module or specified by the user in advance. Since NAR TTS models are not constrained by codec vocoders, they can achieve superior speech quality.\nHybrid systems combine the text-to-codec language model and codec-to-feature diffusion model [33-35]. The language model addresses the alignment between text and speech as well as the utterance duration prediction, while the codec-to-feature diffusion model synthesizes speech features (Mel spectrum) based on the generated codec and other conditions. By leveraging the strengths of both generative models, hybrid systems achieve high diversity, prosody consistency and speech quality.\nDespite the success of recent zero-shot TTS models, they generally operate in non-streaming (offline) mode, which involves complete input text and requires synthesizing the entire utterance before returning the waveform. This results in high latency, negatively impacting user experience in applications like voice chat [36,37]. To address this issue, streaming synthesis has been explored for language model-based zero-shot TTS models [38-41], but diffusion-based TTS models and hybrid systems lack well-established streaming solutions.\nBuilding on the success of CosyVoice [34], we introduce CosyVoice 2, a streaming zero-shot TTS model with improved prosody naturalness, content consistency, and speaker similarity. Our contributions include:\n\u2022 Unifying streaming and non-streaming synthesis in a single framework and proposing the unified text-speech language model and chunk-aware causal flow matching model, leading to lossless streaming synthesis compared to offline mode.\n\u2022 Simplifying the LM architecture by removing the text encoder and speaker embedding, allowing pre-trained textual large language models (LLMs) to serve as the backbone, enhancing context understanding.\n\u2022 Replacing vector quantization (VQ) in the speech tokenizer with finite scalar quantization (FSQ), improving codebook utilization and capturing more speech information.\n\u2022 Upgrading the instructed TTS capacity to support more instructions, including emotion, accent, role style, and fine-grained control. In CosyVoice 2, the instruction and zero-shot capacity are integrated into a single model, enabling more versatile and vivid synthesis.\nThrough the above systemic modification and optimization, CosyVoice 2 achieves human-parity synthesis quality and is nearly lossless in streaming mode. The unified framework loosens deployment requirements, enabling a single model to support both streaming and non-streaming synthesis. The upgraded instructed TTS capacity provides a more powerful and easier approach for users to generate various speeches. In addition, the chunk-aware flow matching design can also be applied to NAR TTS models, which suggests the potential for streaming NAR models."}, {"title": "2 Cosy Voice 2", "content": "CosyVoice 2 builds on the similar design philosophy of its predecessor [34] by separating the semantic and acoustic information of speech signals and modeling them independently. The speech generation process is redefined as a gradual semantic decoding procedure, where conditional information is progressively incorporated. Specifically, the text-speech language model (LM) focuses solely on semantic information, decoding high-level text tokens into supervised semantic speech tokens. In the Flow Matching model, acoustic details, such as timbre, are introduced through speaker embeddings and reference speech, converting speech tokens into the Mel spectrum for a given speaker. Finally, a pre-trained vocoder model reinstates the phases, transforming the Mel spectrum back into"}, {"title": "2.1 Text Tokenizer", "content": "Cosy Voice 2 uses the raw text as input directly, which is tokenized using a BPE-based text tokenizer. This eliminates the need for a frontend model that obtains phonemes via the grapheme-to-phoneme (g2p) transformation. This approach not only simplifies the data preprocessing workflow but also enables the model to learn the pronunciations of words within various contexts in an end-to-end manner. Unlike the tokenizers commonly used in textual LLMs, Cosy Voice 2 masks out the one-to-many tokens. This prevents the pronunciation of a token from becoming excessively long and reduces corner cases caused by data sparsity. Specifically, if a BPE token encodes more than one Chinese character, it will be masked out, and each character will be encoded separately during the tokenization process. Other languages, such as English, Japanese, and Korean, are not subject to special handling."}, {"title": "2.2 Supervised Semantic Speech Tokenizer", "content": "As shown in Figure 1 (a), we insert the finite scalar quantization (FSQ) module [42] into the encoder of SenseVoice-Large ASR model [43]. At the training stage, the input speech X goes through the Encoder1 to obtain the intermediate representations, where Encoder1 consists of six Transformer blocks with the rotary positional embedding [44]. Then, the intermediate representations are fed into the FSQ module for quantization, and the quantized representations are passed through the rest of SenseVoice-Large modules, including Encoder2 and ASR Decoder, to predict the posterior probabilities of corresponding text tokens.\nIn the FSQ module, the intermediate representations H are firstly projected into a D-dimensional low-rank space, and the values of each dimension are quantized into [\u2212K, K] with the bounded round operation ROUND. Then, the quantized low-rank representations H are projected into the original dimension H for the following modules:\n$\\begin{aligned}\nH &= \\text{ROUND}(\\text{Projdown}(H)) \\\\\nH &= \\text{Projup}(H)\n\\end{aligned}$\nAt the training stage, the straight-through estimation is used to approximate the gradients of FSQ module and Encoder1. The speech token \u00b5i can be obtained by calculating the index of quantized"}, {"title": null, "content": "low-rank representation hi,j in the (2K + 1)-ary system:\n$\\mu_i = \\sum_{j=0}^{D-1} h_{i,j} (2K + 1)^j$\nThe Encoder1, low-rank projector of FSQ module, bounded round operation and index calculation form the speech tokenizer for CosyVoice 2. Our speech tokenizer works at a token rate of 25 Hz, i.e., 25 speech tokens per second."}, {"title": "2.3 Unified Text-Speech Language Model", "content": "In CosyVoice 2, the pre-trained textual LLM, Qwen2.5-0.5B [45], is used as the text-speech language model to generate the speech tokens autoregressively with the input text as a prompt. Similar to other LMs, the text-speech LM is also trained in a next-token-prediction scheme as shown in Figure 1 (b). Different from the previous CosyVoice, we remove the speaker embedding to avoid information leaking. More importantly, we find that such utterance-level vector contains not only speaker identify but also language and paralanguage information, which harms the prosody naturalness and cross-lingual capability of the text-speech LM. Besides, we also abandon the text encoder of the previous CosyVoice, since we find that the Qwen2.5-0.5B model is powerful enough to align the text and speech tokens, and the text encoder is no longer needed.\nBenefiting from the simplicity of text-speech LM, we can build a unified model for both streaming and non-streaming synthesis. Here, \u201cstreaming mode\u201d means the input text is received in a continuous flow rather than being known as a complete sentence in advance. In CosyVoice 2, the difference between streaming and non-streaming modes is only the way of sequence construction for LM:\n\u2022 For the Non-Streaming mode, the \u201cstart of sequence\" S, all text tokens, \u201cturn of speech\" token T, all speech tokens and the \u201cend of sequence\u201d E are concatenated sequentially as shown in the bottom of Figure 2. Ignore token means that their losses are ignored while minimizing the cross-entropy objective function.\n\u2022 For the Streaming mode, we mix up the text and speech tokens in a pre-defined ratio of N:M, i.e. every N text tokens are followed by M speech tokens seen in the top of Figure 2. If the next token is a text token, the model is expected to predict a filling token (rather than the text token), which indicates that the next N text tokens should be concatenated at the inference stage. Once the text tokens are ran out of, the \u201cturn of speech\u201d token T and the remaining speech tokens are concatenated sequentially, forming the hybrid text-speech token sequence in the streaming mode.\nBy training the text-speech LM on the above two sequences simultaneously, we can perform streaming and non-streaming speech generation within a single unified model. In real-life scenarios, such as speaker fine-tuning (SFT) and in-context learning (ICL), the inference sequence differs as follows:\n\u2022 ICL, Non-Streaming: In ICL, the LM requires prompt text and speech tokens from the ref-erence audio to imitate the accent, prosody, emotion and style. In the non-streaming mode,"}, {"title": null, "content": "the prompt and to-synthesize text tokens are concatenated as the whole entity, and the prompt speech tokens are treated as the pre-generated results and are fixed: \u201cS, prompt_text, text, T, prompt_speech\". The autoregressive generation of LM is started from such sequence until the \"End of sequence\" token E is detected.\n\u2022 ICL, Streaming: In this scenario, we assume the to-generate text is already known and the speech tokens should be generated in a streaming manner. Similarly, we treat the prompt and to-generate text as a whole entity. Then, we mix it up with the prompt speech tokens on the ratio of N:M: \"S, mixed_text_speech, T, remaining_speech\". If the length of text is greater than that of prompt speech tokens, the LM will generate \"filling token\". In this situation, we manually pad N text tokens. If the text tokens run out of, the \u201cTurn of speech\" token T will be added. In the streaming mode, we return generation results every M tokens until the E is detected.\n\u2022 SFT, Non-Streaming: In the SFT scenario, the LM is fine-tuned on a specific speaker, and the prompt text and speech are no longer needed. Thus, the initial sequence is very simple: \"S, text, T\". Starting from this, the text-speech LM can generate speech tokens autoregressively until T.\n\u2022 SFT, Streaming: In the streaming mode of SFT, we start the speech generation from the following sequence: \"S, first_N_text\". Then, the LM will generate M speech tokens, and we manually pad the next N text tokens. We repeat the above process until all text tokens run out of, and then Tis added. Note that this mode can also be adopted by the speech-to-speech multi-modal large language models to obtain an extremely low latency."}, {"title": "2.4 Chunk-aware Flow Matching", "content": "In Cosy Voice 2, we employ the Mel spectrogram as the acoustic feature with the frame rate of 50 Hz and the sampling rate of 24000. Due the frame-rate mismatch between speech tokens and Mel features, we up-sample the speech tokens with the ratio of two to match the frame rate of Mel spectrogram. Before the up-sampling operation, we add an additional look-ahead convolution layer to provide the future information for the following causal modules. The look-ahead layer is implemented by a right-padded 1-D convolution with the pad size of P and the kernel size of P +1. After these, several chunk-aware causal Transformer blocks are followed to align the representation space of speech tokens to match acoustic features.\nSubsequently, our goal is to further decode the speech tokens into the Mel spectrogram specified by the speaker embedding and reference speech. To achieve this, we employ a conditional flow matching (CFM) model to sample the Mel spectrogram, given speech tokens, reference speech and speaker embedding as conditions. In the CFM model, the distribution of target Mel spectrogram is described by a probability density path from a prior distribution po(X) and the data distribution q(X). The probability density path can be defined by a time-dependent vector field. For sampling efficiency, we employ the optimal-transport (OT) flow to match the vector field wt, which is given"}, {"title": null, "content": "by an ordinary differential equation (ODE):\n$\\begin{aligned}\n\\omega_t(\\Phi_t^{OT}(X_0, X_1)|X_1) &= X_1 - X_0 \\\\\n\\Phi_t^{OT}(X_0, X_1) &= (1 - t)X_0 + tX_1 \\\\\nX_0 &\\sim p_0(X) = \\mathcal{N}(0, I) \\\\\nX_1 &\\sim q(X)\n\\end{aligned}$\nA causal convolutional Transformer UNet is employed to learn the above ODE with the up-sampled token \u00b5, masked Mel spectrogram X1, speaker embedding\u00b2 v and timestep t as the conditions:\n$\\nu_t(\\Phi_t^{OT}(X_0, X_1)|\\theta) = UNet_{\\theta} (\\Phi_t^{OT}(X_0, X_1), t; v, {\\mu}_{1:L}, X_1)$\nAt the training stage, the masked Mel spectrogram is obtained by randomly masking out 70% to 100% of the final frames in X1. As for the inference, it is provided by the Mel spectrogram extracted from the reference speech. By minimizing the L1 loss between the predicted and ground-truth ODE, we can optimize the UNet parameters \u03b8 as follows:\n$\\theta = \\arg\\min_{\\theta} \\mathbb{E}_{p_0(X), q(X), t} |w_t(\\Phi_t^{OT}(X_0, X_1)) - \\nu_t(\\Phi_t^{OT}(X_0, X_1)|\\theta; \\mu, X_1, v)|_1$\nAt the training stage, the timestep follows a uniform distribution U[0,1]. However, during the inference, we employ the cosine scheduler to offer more steps for the initial generation stage:\n$t:= 1-\\cos^2(\\frac{\\pi r}{2})$\nBesides, we also train the model on both conditional and non-conditional situations to enable the classifier-free guidance (CFG) [46\u201348] at the inference stage:\n$\\tilde{\\nu}_t(\\Phi_t^{OT}(X_0, X_1)|\\theta; \\Psi) = (1 + \\beta) \\cdot \\nu_t(\\phi_t^{OT}(X_0, X_1)|\\theta; \\Psi) - \\beta \\cdot \\nu_t(\\Phi_t^{OT}(X_0, X_1)|\\theta)$\nwhere \u03a8 denotes the conditions {v, \u00b5, X\u2081}. The CFG strength \u03b2 and the number of flow estimation (NFE) are set to 0.7 and 10, respectively, according to the experimental results.\nThe current flow matching models always work on a offline mode, i.e., only all the speech tokens are generated, the Mel spectragram can be sampled, which is not friendly for the streaming synthesis. To overcome this issue, we treat the multi-step flow estimation as a stacked deeper neural network, which repeats the UNet ten times. Thus, by making the unfolded neural network causal, we can apply it on the streaming synthesis. We construct four masks to satisfy different application situations:\n\u2022 Non-causal Mask is used for offline mode, which can achieve the best performance by attending all frames of conditions. Non-causal mask is suitable for the latency-insensitive situations.\n\u2022 Full-causal Mask is designed for scenarios required extremely low latency, in which only the past frames can be attended.\n\u2022 Chunk-M Mask is a trade off between latency and performance, which can leverage the information of the past and M future frames. This mask is more suitable for the first chunk of generation with low latency.\n\u2022 Chunk-2M Mask can achieve a approximate performance of offline mode by sacrificing more latency, which can be used for the cascade generation chunk for better performance.\nFor each training case in a mini-batch, we randomly sample a mask from the above four masks under the uniform distribution. In this manner, one flow matching model can be compatible to different scenarios, lowering the deployment complexity. Another advantage of this chunk-aware training is that the masks with more context sever as a teacher for the ones with less context, benefiting from the implicit self-distillation scheme."}, {"title": "2.5 Latency Analysis for Streaming Mode", "content": "The first-package latency is an important metric for streaming synthesis models, which significantly affects the user experience especially in LLM-based voice chat applications, such as GPT-40 [36]. In the context of TTS, the to-synthesize text is known in advance, and the latency comes from the aspects of speech token generation, Mel spectrogram reconstruction and waveform synthesis. Thus, the first-package latency LTTS of Cosy Voice 2 can be obtained as follows:\n$L_{TTS} = M\\cdot d_{lm} + M.d_{fm} + M.d_{voc}$\nwhere d\u0131m denotes the computation time of LM to generate one speech token, dfm represents the computation time of Flow Matching model to generate the frames of Mel spectrogram for one speech token and duoc stands for the computation time of vocoder to synthesize waveforms corresponding to one speech token. In the context of LLM-based voice chat, the length of first-package-required text should also be considered, and the first-package latency Lchat becomes as follows:\n$L_{chat} \\leq N d_{lm} + L_{TTS}$\nwhere dum represents the computation time of a LLM to generate one text token. Note that, since the multi-character tokens are masked out in CosyVoice 2's text tokenizer, the text tokens used by text LLMs always encode longer raw text than those of CosyVoice 2. Thus, the the first-package latency Lchat must be lower than the summation of N dum and LTTS."}, {"title": "2.6 Instructed Generation", "content": "To enhance the controllability of Cosy Voice 2, we integrated the instructed dataset into the base training set. We have collected 1500 hours of instructed training data, which includes both natural language instructions and fine-grained instructions, as outlined in Table 1. For natural language in-structions, we prepend a natural language description and a special end token, \u201c<|endofprompt|>\u201d before the to-synthesize input text. These descriptions cover aspects such as emotion, speaking rate, role-playing, and dialects. For fine-grained instructions, we insert vocal bursts between text tokens, using markers like \u201c[laughter]\u201d and \u201c[breath]\u201d. Additionally, we apply vocal feature tags to phrases; for instance, \u201c<strong>XXX</strong>\u201d indicates emphasis on certain words, while \u201c<laughter>XXX</laughter>\u201d signifies speaking with laughter."}, {"title": "2.7 Multi-Speaker Fine-tuning", "content": "Fine-tuning the pre-trained model on specific speakers (SFT) can further improve the generation quality and speaker similarity. In this report, we introduce the multi-speaker fine-tuning (mSFT), in"}, {"title": null, "content": "which the pretrained model is fine-tuned on multiply speakers simultaneously rather than a single speaker. This approach ensures comprehensive prosody and pronunciation coverage across multiple speakers and mitigates potential catastrophic forgetting from the pretrained models. To avoid timbre confusion between various speakers, we prepend speaker-prompt tags, \u201cSpeaker A<|endofprompt|>\u201d to the input text for a specific speaker. If a training sample is not labeled to a speaker, a special tag, \"unknown<|endofprompt|>\", is utlized. The learning rate is set to le-5 during the whole multi-speaker fine-tuning process."}, {"title": "2.8 Reinforcement Learning for SFT", "content": "Reinforcement learning is a commonly used method in the training of large language models, which can make the LM output align with human preference. In CosyVoice 2, we employ speaker similarity (SS) and recognition word error rate (WER) from the ASR system as the reward function to improve speaker similarity and pronunciation accuracy in the fine-tuning stage. We use WER and SS to distinguish preferred sample xw and rejected samples x\u00b9 and optimize the TTS system with direct preference optimization (DPO) [49] as follow:\n$\\begin{aligned}\nL_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = &-\\log (\\sigma(\\beta \\log \\frac{\\pi_{\\theta}(\\mu^w|y)}{\\pi_{ref}(\\mu^w|y)} - \\beta \\log \\frac{\\pi_{\\theta}(\\mu^l|y)}{\\pi_{ref}(\\mu^l|y)}))\n\\end{aligned}$\nwhere \u03bc\u2122 and \u03bc' are the speech token extracted from the preferred and rejected samples xw and xl.\nHowever, this method is time-consuming and computation-consuming as it should synthesis the audios through the TTS system repeatedly to obtain distinguishable preference and rejected samples. During training, four forward operations are needed for one training step. To simplify the process, we recover the LM predicted token \u03bci \u2208 {0,1, . . ., (2K + 1)D \u2212 1} into quantized low-rank rep-resentations H, and directly use the ASR backend of the speech tokenizer to re-predict the input text. Then the predicted log posterior can be regarded as the ASR reward function to optimize the text-speech language model. During training, the ASR backend parameters are frozen.\n$\\begin{aligned}\nh_{i,j} &= \\frac{\\mu_i}{(2K + 1)^j} \\mod (2K + 1) \\\\\n\\hat{H} &= Proj_{up}(H) \\\\\nL_{ASR} &= - \\log P(Y|\\hat{H};\\theta_{ASR})\n\\end{aligned}$\nwhere Y is the input text, and H are the recovered speech low-rank representations. As the sample operation of the ui ~ P(\u03bc\u03b9|\u03bc1:i\u22121, Y; 0LM) still prevent us to optimize the model directly, we use the gumbel softmax sampling to make it differentiated and then optimize the OLM by the LASR."}, {"title": "3 Experimental Settings", "content": "3.1 Training Data for Speech Tokenizer\nA 200,000-hour dataset is used to train the speech tokenizer with normalized transcriptions as labels. Detailed data information is listed in Table 2. The training data comes from three different resources: open source ASR datasets, internal industrial datasets and TTS generation datasets. Although we only used Chinese and English data when training the speech tokenizer, as shown in Table 2, sub-sequent experiments revealed that the speech tokenizer had zero-shot capability for other languages. It can be also used for speech synthesis in languages such as Japanese and Korean."}, {"title": "3.2 Training Data for Cosy Voice 2", "content": "Cosy Voice 2 shares the same training data as its previous version [34]. We first collect the speech-only data with internal speech processing tools, including speech detection, signal-to-noise ratio (SNR) estimation, speaker diarization, and separation. Subsequently, the Paraformer [50] and Sen-seVoice [43] are employed to generate pseudo text labels for Chinese and other languages, respec-tively. We also employ an internal force-alignment model to filter out low-quality data and enhances the accuracy of punctuation. Data details are provided in Table 3."}, {"title": "3.3 Evaluation Settings", "content": "We evaluate our CosyVoice 2 on two test sets. The first one is constructed from the test-clean set of Librispeech corpus [51], denoting as test-clean. This test set is used to evaluate Cosy Voice 2 on a limited English domain. The Whisper-large V3 is used as the ASR model to evaluate the content consistency. As for the speaker similarity (SS), we employ the ERes2Net model [52] to extract speaker embeddings of prompt and generated utterances, and their raw cosine similarity is treated as the speaker similarity. NMOS score [53] is used to evaluate the objective quality.\nThe second evaluation is conducted under the SEED settings [33], which is widely used to evaluate recent TTS models, covering various text domains and reference speeches. In this evaluation, about 2,000 Chinese and 1,000 English samples are selected from CommonVoice datasets, denoting as test-zh and test-en, respectively. In addition, about 400 hard test cases are also included to evaluate the robustness of TTS models on text repetition, tongue twister and other challenging synthesis cases, denoting as test-hard in this report. The Paraformer is employed to recognize the synthesis results of test-zh and test-hard, while the Whisper-large V3 is adopted for test-en to evaluate the content consistency."}, {"title": "3.4 Benchmark for Japanese and Korean", "content": "We prepare two test sets, denoted as test-ja and test-ko, for the evaluation on Japanese and Korean speech synthesis. The test-ja consists 1,000 samples extracted from the CommonVoice dataset, which are used to measure the model's performance on various metrics, such as WER, SS, MOS. Specifically, we randomly shuffle and pair the entire CommonVoice JA-test set as reference utterance and target utterance spoken. Considering the wide range of utterances' text lengths of JA-test set, we randomly selected 1,000 pairs of reference-target utterances from the length range from 8 to 32 characters as our final test set. For the test-ko, we selected 1,000 speech samples with a WER of less than 5% and no deletion or insertion errors, utilizing the Whisper-Large V3 [54] as the ASR model. These samples were used as reference utterances for the Korean speech synthesis. For the input text, we randomly selected 1,000 text samples from the remaining data. We have released the lists of prompt speeches, prompt transcriptions and input text from these two test sets are released to facilitate result reproduction. By providing this open-source data, we aim to establish a benchmark for evaluating Japanese and Korean TTS models. The Whisper-large V3 is used as the ASR model for Japanese and Korean evaluations."}, {"title": "4 Experimental Results", "content": "4.1 Evaluations on Speech Tokenizer\nAn ideal speech tokenizer is supposed to effectively utilizes the codebook, preserves information at a high fidelity, and demonstrates speaker independence. In this part we evaluate our supervised"}, {"title": "4.2 Comparison Results with Baselines", "content": "We first evaluated our Cosy Voice 2 models on a limited English text domain and compared it with several open-source models, such as ChatTTS [56], GPT-SOVITs [57], OpenVoice [58], ParlerTTS [59], EmotiVoice [60], and its predecessor CosyVoice [34]. The objective results are presented in Table 5, including content consistency (WER), speech quality (NMOS) and speaker similarity (SS). From the table, we can see that CosyVoice 2 achieves state-of-the-art performance on the Librispeech test-clean set, surpassing all baseline models acros all evaluation metrics. Notably, CosyVoice 2 even demonstrates higher content consistency, speech quality, and speaker similarity than human utterances, indicating its human-parity synthesis quality."}, {"title": "4.3 Modular Ablation Study", "content": "We conducted a modular ablation study on the text-speech language model to assess the impacts of our modifications, including LLM initialization, removing speaker embedding, and utilizing FSQ. Table 7 illustrates the step-by-step development of CosyVoice 2 from its predecessor. By replacing the randomly initialized language model with a pretrained LLM, we achieved relative improvements in content consistency of 18.46% and 15.40% on the test-zh and test-hard sets, respectively. Next, we removed the speaker embedding from the text-to-speech language model, which helps prevent information leakage and disturbances in in-context learning. This change resulted in a significant reduction in content errors while maintaining speaker similarity, indicating that content information is primarily modeled by the LM, and speaker information is mainly recovered by the flow matching model. Finally, by replacing VQ with FSQ, we achieved the CosyVoice 2 model, noting much higher content consistency and unchanged speaker similarity. By fully utilizing the codebook, FSQ captures more content information and context variation, leading to better alignment between text and speech tokens. Furthermore, we conducted a comparative experiment by incorporating pitch loss as a constraint during the training of the FSQ-based speech tokenizer. We found that this approach led to improved performance in downstream TTS tasks, as indicated in the last row of Table 7. In future versions of Cosy Voice, we plan to carry out more detailed experiments and analyses."}, {"title": null, "content": "We also conducted another modular analysis to evaluate the impact of streaming modules on the synthesis performance. Table 8 shows the results for content consistency and speaker similarity. We fount that the streaming LM has a minimal impact on typical cases from the test-zh and test-en sets, indicating the effectiveness of our unified training framework. The primary impact of the streaming LM is observed in challenging cases from the test-hard set, likely due to the loss of contextual information in streaming mode. Interestingly, the streaming flow matching model results in slightly higher speaker similarity compared to the offline mode. This may be due to the higher prompt-to-generation ratio of initial chunks in streaming mode, whereas the prompt-to-generation ratio in offline mode can be very low, with many padding tokens. The negative effect of the streaming flow matching model on content consistency is much less pronounced compared to streaming LMs, thanks to the semantic-acoustic decoupled modeling in Cosy Voice 2."}, {"title": "4.4 Results on Japanese and Korean Benchmarks", "content": "In addition to Chinese and English, CosyVoice 2 also supports Japanese and Korean. We evalu-ated the content consistency, speaker similarity and speech quality on our constructed Japanese and Korean test sets. As shown in Table 9, CosyVoice 2 performs significantly better on Korean than on Japanese across all evaluation metrics. This discrepancy is primarily due to the overlap in the character set between Japanese and Chinese, which leads to Chinese pronunciations in Japanese contexts. In the future work, we plan to explore ways to enhance linguistic context for multilingual"}, {"title": "4.5 Results on Instructed Generation", "content": "To evaluate the performance of instructed generation, we have created a Chinese test set comprising 290 samples. This set includes 29 types of instructions, shown in Table 1, each with 10 differ-ent input texts. We utilize five audio prompts and speaker embeddings from five speakers (three female and two male) as conditions for the flow matching model. Our testing is conducted in of-fline mode. We objectively evaluate content consistency (CER), speaker similarity (SS), and speech quality (NMOS). Subjectively, we assess the accuracy and naturalness of instruction using the Mean Opinion Score for Instruction (MOS-I), which ranges from 1 to 5. Each sample is assessed by 10 native Chinese speakers, with scores assigned in increments of 0.5. The evaluation criteria focus on whether the speech adheres to all specified instructions, such as emotional expression, speech rate adjustment, dialect usage, and role-playing. Fine-grained controls, including the insertion of laugh-ter, speaking with laughter, breath control, and emphasis, are evaluated for naturalness and accuracy. As illustrated in Table 10, Cosy Voice 2 exhibits superior content consistency (CER), speaker similar-ity (SS), and accuracy and naturalness in instruction control (MOS-I), while maintaining comparable speech quality to Cosy Voice-Instruct. When input instructions are removed from Cosy Voice 2, there is a notable decline in MOS-I; however, improvements are observed in content consistency (CER), speaker similarity (SS), and speech quality (NMOS). This indicates that instruction controllability is difficult to implicitly emerge from content text."}, {"title": "4.6 Results on Speaker Fine-tuned Models", "content": "During the fine-tuning phase, we employ unsupervised clustering on the speaker embeddings of the same speaker to ensure the stability of the speaker's timbre. We have demonstrated that a target speaker with as few as 4"}]}