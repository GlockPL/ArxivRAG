{"title": "Enhancing Data-Limited Graph Neural Networks by Actively Distilling Knowledge from Large Language Models", "authors": ["Quan Li", "Tianxiang Zhao", "Lingwei Chen", "Junjie Xu", "Suhang Wang"], "abstract": "Graphs have emerged as critical data structures for content analysis in various domains, such as social network analysis, bioinformatics, and recommendation systems. Node classification, a fundamental task in this context, is typically tackled using graph neural networks (GNNs). Unfortunately, conventional GNNs still face challenges in scenarios with few labeled nodes, despite the prevalence of few-shot node classification tasks in real-world applications. To address this challenge, various approaches have been proposed, including graph meta-learning, transfer learning, and methods based on Large Language Models (LLMs). However, traditional meta-learning and transfer learning methods often require prior knowledge from base classes or fail to exploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based methods may overlook the zero-shot capabilities of LLMs and rely heavily on the quality of generated contexts. In this paper, we propose a novel approach that integrates LLMs and GNNs, leveraging the zero-shot inference and reasoning capabilities of LLMs and employing a Graph-LLM-based active learning paradigm to enhance GNNs' performance. Extensive experiments demonstrate the effectiveness of our model in improving node classification accuracy with considerably limited labeled data, surpassing state-of-the-art baselines by significant margins.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs have become increasingly recognized as one of the powerful data structures to perform real-world content analysis [6, 34, 35, 45]. They are adept at representing complex relationships and uncovering hidden information between objects across various domains. Among various tasks on graphs, node classification stands out as a classic task with broad applications, such as sentiment analysis [30] and user attribute inference [29]. Recently, graph neural networks [1, 14, 25], inspired by the idea of convolutional neural networks (CNNs) [32], have shown great power in node classification. Generally, GNNs aggregate neighborhood information similar to CNNs, but they exploit the graph's connectivity structure, facilitating the implicit propagation of information from labeled nodes to unlabeled ones. This strategy has substantially enhanced performance across various benchmark datasets [47].\nDespite the great success of GNNs in node presentation learning and node classification, they often struggle to generalize effectively when labeled data is scarce. However, in many real-world applications, due to various issues such as labeling cost and privacy issues, one often needs to train a GNN classifier with sparse labels, which is known as few-shot node classification. For example, labeling a large number of web documents can be both costly and time-consuming [15, 44]; similarly, in social networks, privacy concerns limit access to personal information, leading to a scarcity of attribute labels [29]. Consequently, when confronted with such datasets, GNNs may exhibit poor generalization to unlabeled nodes. To tackle the few-shot learning problem, various methods have been proposed, such as meta-learning [8, 9, 11], transfer learning [42, 52], and adversarial reprogramming [5]. However, they still require a substantial amount of labeled nodes in each class to achieve satisfactory results [49] or require auxiliary labeled data to provide supervision.\nRecently, Large Language Models (LLMs) have demonstrated their outstanding generalizability in zero-shot learning and reasoning [7, 26]. Several efforts have been taken in introducing LLMs to graph learning, such as pre-processing of textual node attributes or taking textual descriptions of rationales as inputs [18, 33], leveraging LLMs to construct graph structure [13, 23], and generating new nodes [50]. For example, Chen et al. [7] first leveraged LLMs as annotators to provide more supervision for graph learning. Yu et al. [50] leveraged the generative capacity of LLMs to address the few-shot node classification problem. These works demonstrate that LLMs can enhance GNNs from different perspectives. However, they typically treat LLMS merely as annotators or generators for node classification tasks, overlooking their untapped potentials, such as the capacity to uncover hidden insights within the results and their zero-shot reasoning ability, which could significantly enhance GNNs' performance for few-shot learning tasks.\nIn this paper, we introduce a novel few-shot node classification model that enhances GNNs' capabilities by actively distilling knowledge from LLMs. Unlike previous approaches, our model uses LLMs as \"teachers\", capitalizing on their zero-shot inference and reasoning capabilities to bolster the performance of GNNs in few-shot learning scenarios. However, there are two primary challenges: (i) LLMs cannot consistently deliver accurate predictions for all nodes. How to select nodes that LLMs can provide high-quality labels that can benefit GNN most; and (ii) How to effectively distill the knowledge from LLMs to GNNs. To address these challenges, we propose an active learning-based knowledge distillation strategy that selects valuable nodes for LLMs and bridges the gap between LLMs and GNNs. This approach significantly enhances the efficacy of GNNs when labeled data is scarce. We first explore the metrics that affect the correctness of LLMs' predictions. Then, We employ LLMs as a teacher model and leverage them to perform on the limited training data, generating soft labels for training nodes along with logits and rationales. These outputs are used to supervise GNNs in learning from two perspectives: probability distribution and feature enhancement at the embedding level. In this way, GNNs can learn the hidden information from unlabeled nodes and the detailed explanation provided by LLMs. Furthermore, we introduce a novel Graph-LLM-based active learning approach to establish a connection between LLMs and GNNs, which effectively select nodes for which GNNs fail to provide accurate pseudo-labels but LLMs can offer reliable pseudo-labels, thereby enabling GNNs to leverage the zero-shot capabilities of LLMs and enhance their performance with limited data. Afterward, the selected pseudo-labels are merged with the true labels to train the final few-shot node classification model. In summary, the major contributions of our paper are:\n\u2022 We analyze the correctness of LLMs' predictions and conduct preliminary experiments to explore metrics that affect the correctness of LLMs' predictions.\n\u2022 We innovate a semi-supervised learning model by distilling knowledge from Large Language Models and leveraging the enhanced rationales provided by Large Language Models to help GNNs improve their performance.\n\u2022 We design and implement a Graph-LLM-based active learning paradigm to enhance the performance of GNNs. This is achieved by identifying nodes for which GNNs struggle to generate reliable pseudo labels, yet LLMs can provide dependable predictions, which leverage the zero-shot learning and reasoning ability of LLMs to enhance the performance of GNNs.\n\u2022 Extensive experiments on various benchmark datasets demonstrate the effectiveness of the proposed framework for node classification tasks even with limited labeled data."}, {"title": "2 RELATED WORK", "content": "In this section, we introduce related works, including graph neural networks, few-shot node classification, LLMs for text-attributed graphs, and active learning."}, {"title": "2.1 Graph Neural Networks", "content": "Graph Neural Networks (GNNs) have garnered widespread attention for their effective exploitation of graph structure information. There are two primary types of GNNs: spectral-based and spatial-based. Kipf and Welling [25] followed the idea of CNNs and proposed the Graph Convolutional Network (GCN) to aggregate information within the spectral domain using graph convolution. Different from GCN, Graph Attention Network (GAT) [41] and GraphSAGE [14] emerged as spatial-based approaches. GAT applies the attention mechanism to learn the importance of the neighbors when aggregating information. GraphSAGE randomly samples the number of neighbors of a node and aggregates information from these local neighborhoods. Despite their extensive application across various domains, GNNs often face challenges due to limited labeled data. Existing convolutional filters or aggregation mechanisms struggle to effectively propagate labels throughout the entire graph when only few labeled data points are available [28]."}, {"title": "2.2 Few-shot Node Classification", "content": "In real-world graph learning tasks, obtaining high-quality labeled samples can be particularly challenging due to various factors such as the high cost involved in annotation processes or the limited access to node information. Thus, researchers proposed different methods to improve the performance of GNNs with only few labeled data. Most recent advancements in few-shot node classification (FSNC) models have mainly developed from two approaches: metric-learning and meta-learning. Metric-learning models aim to learn a task-invariant metric applicable across all tasks to facilitate FSNC [16, 24]. Prototypical network [38] and relation network [40] are two classic examples, where the former uses the mean vector of the support set as a prototype and calculates the distance metrics to classify query instances, and the latter trains a neural network to learn the distance metric between the query and support set. Meta-learning models use task distributions to conduct meta-training, learning shared initialization parameters that are then adapted to new tasks during meta-testing [9, 22, 51]. These approaches have demonstrated effectiveness compared to metric-learning, which often struggles due to task divergence issues. However, meta-learning requires significant amounts of data for meta-training, sourced from the same domain as meta-testing, thereby severely limiting its practical applicability. Different from metric-learning and meta-learning models, in this paper, we propose to distill knowledge from LLMs to GNNs, leverage the LLMs' zero-shot ability and reasoning ability to improve GNNs for few-shot node classification."}, {"title": "2.3 LLMs for Text-Attributed Graphs", "content": "Recently, Large language models (LLMs) have garnered widespread attention and experienced rapid development, emerging as a hot topic in the artificial intelligence area. By training on extensive datasets, these models have learned to understand and generate natural language, demonstrating remarkable capabilities across a variety of tasks including text generation [27], question answering [31], translation [48], as well as more complex reasoning and creative tasks. Researchers are integrating Large Language Models (LLMs) into diverse fields such as computer vision, time series analysis, and Graph Neural Networks (GNNs), expanding their applicability beyond natural language processing.\nWithin the graph domain, LLMs show their generalizability in dealing with Text-Attributed Graphs (TAGs). Chen et al. [7] demonstrated the power of LLMs' zero-shot ability on node classification tasks. Moreover, LLMs also demonstrate their power in providing rationales to enhance node features [18] and construct edges in graphs [39]. Liu et al. [33] further proposed OFA to encode all graph data into text and leverage LLMs to make predictions on different tasks. Despite their remarkable proficiency in understanding text, LLMs still face limitations when it comes to processing graph-structured data. Therefore, leveraging LLMs' zero-shot ability and integrating them with GNNs has emerged as the latest state-of-the-art approach in text-attributed graph learning [7]."}, {"title": "2.4 Active Learning", "content": "Active learning (AL) [2-4, 12, 37, 46] is a widely adopted approach across various domains for addressing the issue of label sparsity. The core concept involves selecting the most informative instances from the pool of unlabeled data. Recently, many works [4, 12, 20] integrate GNNs with AL to improve the representative power of graph embeddings. However, how to leverage AL to build connections between LLMs and GNNs and improve the performance of GNNs has emerged as a problem. Chen et al. [7] first leverage active learning to select nodes that are close to the cluster center under the label-free setting and use LLM as an annotator to create labels for these nodes. However, their approach simply leverages LLMs to annotate nodes and ignores the benefits of unlabeled nodes and the zero-shot reasoning ability of LLMs. And, under the few-shot setting, GNNs themselves can provide relatively high-quality pseudo-labels for those nodes close to the cluster center, which waste resources if we use LLMs to generate pseudo-labels for those nodes. Moreover, prior research primarily concentrated on selecting data with the highest confidence score during the AL process. In our work, instead of focusing on nodes where GNNs have high confidence, we prioritize nodes where GNNs struggle to provide pseudo-labels with high confidence scores but LLMs can provide reliable predictions. This approach is motivated by our integration of LLMs as teacher models to enhance the performance of GNNs by leveraging LLMs' zero-shot pseudo-labeling and reasoning ability. Through active learning, we integrate LLMs into GNNs, enabling LLMs to instruct GNNs with data that GNNs find challenging to label confidently."}, {"title": "3 PRELIMINARIES", "content": "In this section, we conduct preliminary experiments to reveal the metrics that can affect LLMs to generate high-quality pseudo labels and formulate the problem."}, {"title": "3.1 Notations", "content": "We use G = (V, E) to denote a graph, where V = {v_1, v_2, ..., v_N} is a set of N nodes and E is a set of edges. We use A to denote the adjacency matrix, where A_ij = 1 means nodes v_i and v_j are connected; otherwise A_ij = 0. The text-attributed graph can be defined as G_T = (V, A, X), where X = {X_1,X_2,...,X_N} denotes the set of raw texts and can be encoded as text embeddings X = {x_1, x_2, ..., x_n}. In semi-supervised learning, The node set V can be divided into two different sets: (1) the labeled node set V_l and (2) the unlabeled node set V_u. Moreover, we use V_s to denote the labeled node set including both original labeled data and the data selected through active learning, and use y to represent the label set, where y = {y_1, y_2,\u2026, y_N}."}, {"title": "3.2 Understanding LLM's Capability", "content": "As the sparse label challenges GNN, in this paper, we aim to imbue GNNs with the zero-shot learning prowess of LLMs, thereby elevating their performance in scenarios with limited labeled data. However, LLMs might be good at classifying certain nodes while performing poorly on other nodes. Thus, it is important to identify nodes that LLMs can provide superior pseudo-labels with rationales, whereas GNNs cannot, which can better enhance GNNs' performance. Hence, we first conduct preliminary experiments to understand the key metrics that are pivotal for LLMs in generating reliable pseudo-labels.\nLLMs may benefit from various metrics to perform node classification well. Particularly, in graph G, certain metrics exert a more pronounced influence on the correctness of LLM predictions on nodes, which include: 1) node features, such as the title and abstract in the citation network; 2) degree of a node, and 3) the homophily ratio. As experimentally demonstrated in [18, 21], LLMs can provide better classification results with nodes that contain richer information. Therefore, we decided to leverage LLMs as a teacher model to provide enhanced rationales to teach GNNs from the feature perspective at the embedding level. However, when using active learning to identify valuable nodes, we primarily rely on the original node features. Both degrees and the homophily ratio are important for a node. The former indicates how many nodes will be affected by a node, while the latter suggests that the node tends to connect with others having similar features. Therefore, it's crucial to examine how the degree and the homophily ratio affect the performance of LLMs' predictions.\nWe conduct preliminary experiments to understand how these factors influence the classification performance of LLMs. Specifically, we use the following equation to compute the homophily ratio: HR = \\frac{\\text{# of neighbors have same label}}{\\text{total # of neighbors}}\nWe divide degree and homophily into 3 categories: highest, middle, and lowest, and select 200 nodes for each category. Specifically, We sort the nodes based on the degrees and the homophily ratio in descending order, evenly selecting 200 nodes from the head, tail, and middle of the node list for the highest, lowest, and middle categories, respectively. The GPT-3.5-turbo is used for testing. We provide the raw text X_i and the potential classes to the LLMs, asking them to assign a label from the given class to X_i. Then, we compare the results from LLM and the ground truth labels for evaluation."}, {"title": "3.3 Problem Statement", "content": "As LLMs could not give reliable knowledge for all the nodes, in this paper, we study a novel problem of how to effectively leverage LLMs to enhance the performance of few-shot node classification over graphs. Given a text-attributed graph G_T = (V, A, X) with a very limited labeled node set V_l (i.e. |V_l| < |V_u|) and their label set Y, a budget size B (note that the budget size B is the number of nodes per class), and a large language model LLM, we aim to train a GNN model that can have better performance with only few available labeled nodes by querying LLM within the budget size B. The details will be introduced in Section 4."}, {"title": "4 PROPOSED MODEL", "content": "Though GNNs have shown great power in node classification, the vanilla GNNs suffer from low generalizability with few labeled data for training [10]. Thus, in order to enhance the generalizability of GNNs, we propose a framework that integrates GNNs with LLMs and employs a novel Graph-LLM-based active learning strategy to actively distill knowledge from LLMs. Our proposed model uses GNN as the backbone model and takes advantage of LLMs' zero-shot pseudo-labeling and reasoning capabilities, especially for nodes that are difficult for GNN to give accurate predictions. In these instances, LLMs can offer reliable pseudo-labels and provide enhanced rationales, thereby improving the few-shot learning capability of GNNs from distinct perspectives.\nAn illustration of the framework is shown in Figure 2. Specifically, LLM serves as a teacher model, instructing the student model (GNNs) from two distinct perspectives: (1) it imparts the \"correct\" answers to the student model along with the probability distribution for all potential categories, drawing upon its vast knowledge, which will teach GNNs with the output logits; and (2) it explains the rationale behind its decision-making process, providing insights into why certain decisions are made, which will serve as the feature teacher to teach GNNs at embedding level. Then the knowledge obtained from LLMs will be distilled to GNNs, and GNNs propagate label information to all unlabeled nodes. We leverage Graph-LLM-based active learning to identify nodes that GNNs struggle to generate reliable pseudo labels but LLMs can provide reliable predictions. These selected nodes are then added to the train set with pseudo labels, and fed to LLMs for logits and rationales, which can further enhance the capability of GNNs under the guidance of LLMs. Finally, we train the ultimate student model, enhancing its generalizability with limited available data. Next, we introduce each component in detail."}, {"title": "4.1 Base GNN Classifier", "content": "As GNNs have shown great power in semi-supervised node classification, we adopt Graph Neural Networks (GNNs) as the backbone models, which can be used to capture the structure information between entities and naturally propagate the information to all unlabeled nodes efficiently. We first use SBERT [36] to encode raw texts X to text embeddings X. Then, we use GNNs to perform on the given graph and these embeddings. Specifically, the GNN takes the graph G_T as input and learns the node representation as\nH = \\text{GNN}(A, X)\nwhere Hf is the node representation matrix from the last layer of GNN. The final prediction results can be computed as:\nZ = \\text{softmax}(H_f)\nwhere Z is the probabilities for all nodes in the graph. The loss function for training the GNN will be introduced in 4.4."}, {"title": "4.2 Obtaining Knowledge from LLM", "content": "Despite GNNs showing success in dealing with graph data, the generalizability of GNNs with few available data is still limited. To tackle this challenge, we introduce LLMs as teacher models, leveraging their zero-shot ability [26] to instruct GNNs in classification tasks and provide insights into the reasoning behind these decisions. In this way, GNNs can learn hidden label distribution information and enhanced feature information from LLMs, which empower the capabilities of GNNs with scarce labeled data\nTo effectively distill knowledge from LLMs to GNN, we consider two types of knowledge: (1) soft labels and logits; and (2) rationales behind LLMs' decision-making process. Soft labels and logits reveal hidden distribution information for unlabeled data, while rationales contribute richer node information. This combination allows GNNS to benefit from the unlabeled data and get enhanced node features. We prompt the prediction and reasoning in a two-step manner: first, we input the raw texts into the LLMs to generate the soft labels and logits with the probability distribution. We then let LLMs explain the reason why they make these decisions. Examples of prompts are shown in Table 1. Due to the possibility of LLMs producing outputs that deviate from the desired format when prompts contain numerous tasks, thereby increasing uncertainty in the parsing process, we propose to utilize two separate prompts to obtain logits and rationales. Next, we give the details."}, {"title": "4.2.1 Soft Labels and Logits Generation", "content": "For a node v_i, our proposed model first feeds raw text X_i into LLMs to generate soft labels \\hat{y}_i for X_i and the logits l_i for all possible categories. An example of the prompt for soft labels and logit generation is shown in the first row of Table 1. We leverage the zero-shot ability of LLMs to generate relevant reliable soft labels and logits so that GNNs can leverage the hidden information of unlabeled data with knowledge distillation. This can be formally written as\ny_i, l_i = \\text{LLM}(X_i; \\text{prompt})"}, {"title": "4.2.2 Rationales for Feature Enhancement", "content": "Traditional knowledge distillation methods primarily utilize the soft labels and logits from the teacher model. Nonetheless, incorporating the rationales behind text decisions can significantly enhance the learning capabilities of GNNs [18]. In this context, GNNs are able to learn more informative features from the LLM at the embedding level. Consequently, we introduce LLMs as a feature teacher, guiding GNNs to assimilate more informative features in their decision-making process. Different from previous works that concatenate the enhanced embeddings and the node embeddings or simply replace the node representations directly, we will use a loss function to minimize the difference between them, which will help GNNs learn the enhanced representation while retaining the original node representations. The loss function will be detailed in Section 4.3\nFor a node v_i, LLMs will output the classification result for X_i with a detailed explanation of the decision-making process. The enhanced explanation R_i can be represented as follows:\nR_i = \\text{LLM}(X_i; \\text{prompt})\nAn example of the prompt for rationales is shown in the second row of Table 1. Since the rationales we get from LLMs are all textual explanations, we further need to transform them into the embedding level to teach GNNs the more informative features. We use a pre-trained language model such as Sentence BERT (SBERT) [36] to get the embeddings for R_i, which can be represented as follows:\n\\tilde{r}_i = \\text{LM}(R_i)\nwhere \\tilde{r}_i means the embedding of i-th rationale.\nHowever, since the dimensionality of \\tilde{r}_i may differ from the dimension of the final layer of GNNs, alignment between these representations is necessary. While min/max pooling can effectively reduce dimensionality for alignment purposes, it tends to lose information during the pooling process. To retain the enriched information from these rationales, we train a Multi-Layer Perceptron (MLP) using text embeddings X1 of the limited labeled node set Vl and their corresponding ground truth labels Yl, applying the cross-entropy loss function. This MLP is tasked with aligning the representations between the rationales y and the outputs Hf of the final layer of GNNs, ensuring that valuable information is retained throughout the alignment process. The final representation for i-th rationale is generated as follows:\nr_i = \\text{MLP}(\\tilde{r}_i)\nwhere r is the embedding that has the same dimension as the final layer's outputs Hf in GNNs."}, {"title": "4.3 Distilling Knowledge to GNN", "content": "With the knowledge from LLM represented as r_i and l_i, we use knowledge distillation [19] to distill this knowledge into GNNs. Through this process, GNNs can tap into the hidden information behind unlabeled nodes by using output logits to enhance their performance. Moreover, they can achieve improved node representations by incorporating the rationales generated from LLMs to further enrich the depth and quality of the information being processed. Specifically, LLMs serve as a pre-trained teacher model to teach the student model (GNNs) from two distinct perspectives: 1). soft labels and the probability distribution (logits) and 2). the rationales at the embedding level."}, {"title": "4.3.1 Loss for Knowledge Distillation", "content": "Let V_s be the set of nodes including the original training data and the data selected through active learning (to be introduced in Section 4.5). Following [29], for each v_i \\in V_s, we first convert the logits l_i from LLM as:\np(y_i = j | \\text{LLM}) = \\frac{\\exp(l_{ij}/t)}{\\sum_{c=1}^{C} \\exp(l_{ic}/t)}\nwhere C is the number of classes, t is the knowledge distillation (KD) temperature to control how much of the teacher's knowledge is distilled to student model and l_{ij} is the j-th elements of l_i. Then, the student can learn the distilled knowledge from the teacher by optimizing the following loss function:\nL_T = - \\frac{1}{|V_s|} \\sum_{v_i \\in V_s} \\sum_{j=1}^{C} p(y_i = j | \\text{LLM}) \\log Z_{ij}\nwhere Z_ij is the probability that v_i belongs to class j by GNN. This enhances the model's capacity to get insights from unlabeled data and augment its overall learning capabilities."}, {"title": "4.3.2 Loss for Feature Alignment", "content": "We also introduce rationales to augment the node representation from a feature perspective at the embedding level. With the embeddings r we get from 4.2, Mean Square Error (MSE) is used to calculate the loss between the rationales and node embeddings Hf at the final layer of GNNs for all the nodes in the current training set Vs as:\nL_F = \\frac{1}{|V_s|} \\sum_{i=1}^{|V_s|} ||H_i - r_i||_2^2\nBy employing this approach, the GNNs learn informative rationales from LLMs, enhancing their learning capabilities from a feature perspective at the embedding level."}, {"title": "4.4 Objective Function of Proposed Framework", "content": "The student model itself computes training loss between predictions and ground truth (hard attribute label), which is defined as:\nL_s = \\frac{1}{C} \\sum_{V_i \\in V_s} \\sum_{c=1}^{C} I(Y_i = c) \\log Z_{ic}\nwhere I is the indicator function which outputs 1 if y_i = c otherwise 0. V_s is the set of labeled or pseudo-labeled nodes, and yi is the label or pseudo-label of v_i.\nWith the knowledge distillation from LLM, the final loss function of our proposed model can be formalized as follows:\n\\underset{\\text{GNN}}{\\text{min}} L = (1 - \\alpha - \\beta)L_s + \\alpha L_T + \\beta L_F\nwhere \\alpha and \\beta are both balance parameters that are set up to adjust the relative weight of knowledge distillation loss and feature embedding loss, respectively."}, {"title": "4.5 Graph-LLM-based Active learning", "content": "To further improve GNNs' few-shot learning ability, we introduce a novel Graph-LLM-based active learning strategy to select valuable nodes for LLMs and add them to the training set iteratively. We seek to select B nodes where GNNs exhibit low confidence in classification results, yet LLMs can offer high-quality pseudo-labels based on their inherent knowledge. Through iterative selection, we progressively enhance the GNNs' capabilities.\nAs indicated by the preliminary experiment results presented in Section 3, LLMs demonstrate the ability to generate high-quality pseudo-labels for nodes with higher homophily ratios and more degrees. Thus, we define an evaluation metric that combines the confidence score of GNN's prediction, homophily ratio, and degrees to evaluate if the node in the unlabeled node set Vu is valuable for our proposed model. The evaluation metric is defined as follows:\nSGL_i = RS(p_i) + RS(HR_i) + RS(D_i)\nwhere SGL_i means the evaluation score for i-th node, p_i (i.e. p_i = \\text{max}(Z_i)), HR_i, and D_i denote the final output confidence score, the homophily ratio, and degree for i-th node, respectively. The RS represents a ranking function used to calculate scores for each evaluation metric. Specifically, we arrange the nodes in ascending order according to the evaluation metric results, excluding pi, and assign scores ranging from 0 to 1 with a step of 1/|V_u|. Note that RS assigns scores to the pi in descending order, prioritizing nodes for which GNNs cannot generate reliable pseudo-labels.\nConsidering the fact that some nodes can better contribute to label propagation and model improvement in the graph, we would like to add a metric to evaluate the importance of the node and to facilitate selecting the most valuable pseudo-labels. Here, we utilize neighborhood entropy reduction to assess the importance of a given node v_i [43]. Specifically, for each node v_i in the candidate set Vc, we compute the entropy reduction in the neighbors' softmax vectors by removing v_i from the node set Vn, which contains the v_i and its neighbors. The basic intuition is that a node is more informative when it can greatly change uncertainty (entropy) within its neighborhood. In other words, the more changes in entropy, the more important a node is. Then we rank and assign scores to these nodes based on the change of entropy. The score of entropy change is defined as follows:\nSE_i = RS(h(\\hat{y}_{V_n - v_i}) - h(\\hat{y}_{V_n}))\nwhere SE_i is the score of entropy change for v_i, h(\\cdot) denotes the entropy function, and \\hat{y} represents the pseudo-labels from GNNs, which is computed based on nodes' logits vectors and the activation function. Thus, the final evaluation metric for v_i is:\nS_i = SGL_i + SE_i\nIn each stage, we select subsets of valuable nodes with high S_i, each consisting of b nodes per class. We query LLM to obtain the pseudo-label, logits, and rationals. We then add these nodes to the label set and retrain our model using Eq. 11. We continue this process until the total number of nodes meets the budget size B times the number of classes C. Here, B is a relatively small budget size, achieving a balance between the cost of querying LLMs and the resultant performance improvement. Finally, the selected nodes with pseudo-labels are used to train the final GNN model. This approach makes GNNs benefit from the various abilities of LLMs, enhancing their performance with scarce labeled data."}, {"title": "5 EXPERIMENTAL RESULTS AND ANALYSIS", "content": "In this section, we present the evaluation results of our proposed few-shot node classification model on several benchmark datasets. We aim to address the following research questions:\n\u2022 RQ1: How does our proposed model perform compared with the state-of-the-art baselines under consistent settings?\n\u2022 RQ2: How do different hyper-parameters impact the performance of our model?\n\u2022 RQ3: How do different components in our proposed model contribute to the performance?"}, {"title": "5.1 Experimental Setup", "content": "5.1.1 Datasets. We evaluate our proposed model using three public citation datasets: Cora", "25": ".", "2": "and use the average accuracy as our final results. For our Graph-LLM-based active learning strategies", "models": "graph convolutional network (GCN) [25", "14": "and Graph Attention Network (GAT) [41", "model": "Meta-PN [8", "42": 1}, {"model": "MVGRL [17", "50": ".", "GCN": "The vanilla GCN conducts convolution operations on graph-structured data", "GAT": "GAT incorporates an attention mechanism into GNN for feature aggregation", "GraphSAGE": "GraphSAGE samples neighbors and employs mean aggregation to learn node embeddings", "MVGRL": "MVGRL is a benchmark in GNN self-supervised learning by using data augmentation to create diverse views for contrastive learning", "CGPN": "CGPN introduces the concept of poison learning and utilizes contrastive learning to propagate limited labels across the entire graph efficiently.\n\u2022 Meta-PN: Meta-PN uses meta-learning and employs a bi-level optimization method to generate high-quality pseudo-labels.\n\u2022 LLM-based model: This LLM-based model leverages LLMs to generate the pseudo nodes for each class, uses LM to encode these nodes, and uses an MLP to build edges.\nNote that the LLM-based model does not provide the original code; therefore, we independently developed the model based on the description provided"}]}