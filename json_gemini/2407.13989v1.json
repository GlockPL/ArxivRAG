{"title": "Enhancing Data-Limited Graph Neural Networks by Actively Distilling Knowledge from Large Language Models", "authors": ["Quan Li", "Tianxiang Zhao", "Lingwei Chen", "Junjie Xu", "Suhang Wang"], "abstract": "Graphs have emerged as critical data structures for content analysis in various domains, such as social network analysis, bioinformatics, and recommendation systems. Node classification, a fundamental task in this context, is typically tackled using graph neural networks (GNNs). Unfortunately, conventional GNNs still face challenges in scenarios with few labeled nodes, despite the prevalence of few-shot node classification tasks in real-world applications. To address this challenge, various approaches have been proposed, including graph meta-learning, transfer learning, and methods based on Large Language Models (LLMs). However, traditional meta-learning and transfer learning methods often require prior knowledge from base classes or fail to exploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based methods may overlook the zero-shot capabilities of LLMs and rely heavily on the quality of generated contexts. In this paper, we propose a novel approach that integrates LLMs and GNNs, leveraging the zero-shot inference and reasoning capabilities of LLMs and employing a Graph-LLM-based active learning paradigm to enhance GNNs' performance. Extensive experiments demonstrate the effectiveness of our model in improving node classification accuracy with considerably limited labeled data, surpassing state-of-the-art baselines by significant margins.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs have become increasingly recognized as one of the powerful data structures to perform real-world content analysis [6, 34, 35, 45]. They are adept at representing complex relationships and uncovering hidden information between objects across various domains.\nAmong various tasks on graphs, node classification stands out as a classic task with broad applications, such as sentiment analysis [30] and user attribute inference [29]. Recently, graph neural networks [1, 14, 25], inspired by the idea of convolutional neural networks (CNNs) [32], have shown great power in node classification. Generally, GNNs aggregate neighborhood information similar to CNNs, but they exploit the graph's connectivity structure, facilitating the implicit propagation of information from labeled nodes to unlabeled ones. This strategy has substantially enhanced performance across various benchmark datasets [47].\nDespite the great success of GNNs in node presentation learning and node classification, they often struggle to generalize effectively when labeled data is scarce. However, in many real-world applications, due to various issues such as labeling cost and privacy issues, one often needs to train a GNN classifier with sparse labels, which is known as few-shot node classification. For example, labeling a large number of web documents can be both costly and time-consuming [15, 44]; similarly, in social networks, privacy concerns limit access to personal information, leading to a scarcity of attribute labels [29]. Consequently, when confronted with such datasets, GNNs may exhibit poor generalization to unlabeled nodes. To tackle the few-shot learning problem, various methods have been proposed, such as meta-learning [8, 9, 11], transfer learning [42, 52], and adversarial reprogramming [5]. However, they still require a substantial amount of labeled nodes in each class to achieve satisfactory results [49] or require auxiliary labeled data to provide supervision.\nRecently, Large Language Models (LLMs) have demonstrated their outstanding generalizability in zero-shot learning and reasoning [7, 26]. Several efforts have been taken in introducing LLMs to graph learning, such as pre-processing of textual node attributes or taking textual descriptions of rationales as inputs [18, 33], leveraging LLMs to construct graph structure [13, 23], and generating new nodes [50]. For example, Chen et al. [7] first leveraged LLMs as annotators to provide more supervision for graph learning. Yu et al. [50] leveraged the generative capacity of LLMs to address the few-shot node classification problem. These works demonstrate that LLMs can enhance GNNs from different perspectives. However, they typically treat LLMS merely as annotators or generators for node classification tasks, overlooking their untapped potentials, such as the capacity to uncover hidden insights within the results and their zero-shot reasoning ability, which could significantly enhance GNNs' performance for few-shot learning tasks.\nIn this paper, we introduce a novel few-shot node classification model that enhances GNNs' capabilities by actively distilling knowledge from LLMs. Unlike previous approaches, our model uses LLMs as \"teachers\", capitalizing on their zero-shot inference and reasoning capabilities to bolster the performance of GNNs in few-shot learning scenarios. However, there are two primary challenges: (i) LLMs cannot consistently deliver accurate predictions for all nodes. How to select nodes that LLMs can provide high-quality labels that can benefit GNN most; and (ii) How to effectively distill the knowledge from LLMs to GNNs. To address these challenges, we propose an active learning-based knowledge distillation strategy that selects valuable nodes for LLMs and bridges the gap between LLMs and GNNs. This approach significantly enhances the efficacy of GNNs when labeled data is scarce. We first explore the metrics that affect the correctness of LLMs' predictions. Then, We employ LLMs as a teacher model and leverage them to perform on the limited training data, generating soft labels for training nodes along with logits and rationales. These outputs are used to supervise GNNs in learning from two perspectives: probability distribution and feature enhancement at the embedding level. In this way, GNNs can learn the hidden information from unlabeled nodes and the detailed explanation provided by LLMs. Furthermore, we introduce a novel Graph-LLM-based active learning approach to establish a connection between LLMs and GNNs, which effectively select nodes for which GNNs fail to provide accurate pseudo-labels but LLMs can offer reliable pseudo-labels, thereby enabling GNNs to leverage the zero-shot capabilities of LLMs and enhance their performance with limited data. Afterward, the selected pseudo-labels are merged with the true labels to train the final few-shot node classification model. In summary, the major contributions of our paper are:\n\u2022 We analyze the correctness of LLMs' predictions and conduct preliminary experiments to explore metrics that affect the correctness of LLMs' predictions.\n\u2022 We innovate a semi-supervised learning model by distilling knowledge from Large Language Models and leveraging the enhanced rationales provided by Large Language Models to help GNNs improve their performance.\n\u2022 We design and implement a Graph-LLM-based active learning paradigm to enhance the performance of GNNs. This is achieved by identifying nodes for which GNNs struggle to generate reliable pseudo labels, yet LLMs can provide dependable predictions, which leverage the zero-shot learning and reasoning ability of LLMs to enhance the performance of GNNs.\n\u2022 Extensive experiments on various benchmark datasets demonstrate the effectiveness of the proposed framework for node classification tasks even with limited labeled data."}, {"title": "2 RELATED WORK", "content": "In this section, we introduce related works, including graph neural networks, few-shot node classification, LLMs for text-attributed graphs, and active learning."}, {"title": "2.1 Graph Neural Networks", "content": "Graph Neural Networks (GNNs) have garnered widespread attention for their effective exploitation of graph structure information. There are two primary types of GNNs: spectral-based and spatial-based. Kipf and Welling [25] followed the idea of CNNs and proposed the Graph Convolutional Network (GCN) to aggregate information within the spectral domain using graph convolution. Different from GCN, Graph Attention Network (GAT) [41] and GraphSAGE [14] emerged as spatial-based approaches. GAT applies the attention mechanism to learn the importance of the neighbors when aggregating information. GraphSAGE randomly samples the number of neighbors of a node and aggregates information from these local neighborhoods. Despite their extensive application across various domains, GNNs often face challenges due to limited labeled data. Existing convolutional filters or aggregation mechanisms struggle to effectively propagate labels throughout the entire graph when only few labeled data points are available [28]."}, {"title": "2.2 Few-shot Node Classification", "content": "In real-world graph learning tasks, obtaining high-quality labeled samples can be particularly challenging due to various factors such as the high cost involved in annotation processes or the limited access to node information. Thus, researchers proposed different methods to improve the performance of GNNs with only few labeled data. Most recent advancements in few-shot node classification (FSNC) models have mainly developed from two approaches: metric-learning and meta-learning. Metric-learning models aim to learn a task-invariant metric applicable across all tasks to facilitate FSNC [16, 24]. Prototypical network [38] and relation network [40] are two classic examples, where the former uses the mean vector of the support set as a prototype and calculates the distance metrics to classify query instances, and the latter trains a neural network to learn the distance metric between the query and support set. Meta-learning models use task distributions to conduct meta-training, learning shared initialization parameters that are then adapted to new tasks during meta-testing [9, 22, 51]. These approaches have demonstrated effectiveness compared to metric-learning, which often struggles due to task divergence issues. However, meta-learning requires significant amounts of data for meta-training, sourced from the same domain as meta-testing, thereby severely limiting its practical applicability. Different from metric-learning and meta-learning models, in this paper, we propose to distill knowledge from LLMs to GNNs, leverage the LLMs' zero-shot ability and reasoning ability to improve GNNs for few-shot node classification."}, {"title": "2.3 LLMs for Text-Attributed Graphs", "content": "Recently, Large language models (LLMs) have garnered widespread attention and experienced rapid development, emerging as a hot topic in the artificial intelligence area. By training on extensive datasets, these models have learned to understand and generate natural language, demonstrating remarkable capabilities across a variety of tasks including text generation [27], question answering [31], translation [48], as well as more complex reasoning and creative tasks. Researchers are integrating Large Language Models (LLMs) into diverse fields such as computer vision, time series analysis, and Graph Neural Networks (GNNs), expanding their applicability beyond natural language processing.\nWithin the graph domain, LLMs show their generalizability in dealing with Text-Attributed Graphs (TAGs). Chen et al. [7] demonstrated the power of LLMs' zero-shot ability on node classification tasks. Moreover, LLMs also demonstrate their power in providing rationales to enhance node features [18] and construct edges in graphs [39]. Liu et al. [33] further proposed OFA to encode all graph data into text and leverage LLMs to make predictions on different tasks. Despite their remarkable proficiency in understanding text, LLMs still face limitations when it comes to processing graph-structured data. Therefore, leveraging LLMs' zero-shot ability and integrating them with GNNs has emerged as the latest state-of-the-art approach in text-attributed graph learning [7]."}, {"title": "2.4 Active Learning", "content": "Active learning (AL) [2\u20134, 12, 37, 46] is a widely adopted approach across various domains for addressing the issue of label sparsity. The core concept involves selecting the most informative instances from the pool of unlabeled data. Recently, many works [4, 12, 20] integrate GNNs with AL to improve the representative power of graph embeddings. However, how to leverage AL to build connections between LLMs and GNNs and improve the performance of GNNs has emerged as a problem. Chen et al. [7] first leverage active learning to select nodes that are close to the cluster center under the label-free setting and use LLM as an annotator to create labels for these nodes. However, their approach simply leverages LLMs to annotate nodes and ignores the benefits of unlabeled nodes and the zero-shot reasoning ability of LLMs. And, under the few-shot setting, GNNs themselves can provide relatively high-quality pseudo-labels for those nodes close to the cluster center, which waste resources if we use LLMs to generate pseudo-labels for those nodes. Moreover, prior research primarily concentrated on selecting data with the highest confidence score during the AL process. In our work, instead of focusing on nodes where GNNs have high confidence, we prioritize nodes where GNNs struggle to provide pseudo-labels with high confidence scores but LLMs can provide reliable predictions. This approach is motivated by our integration of LLMs as teacher models to enhance the performance of GNNs by leveraging LLMs' zero-shot pseudo-labeling and reasoning ability. Through active learning, we integrate LLMs into GNNs, enabling LLMs to instruct GNNs with data that GNNs find challenging to label confidently."}, {"title": "3 PRELIMINARIES", "content": "In this section, we conduct preliminary experiments to reveal the metrics that can affect LLMs to generate high-quality pseudo labels and formulate the problem."}, {"title": "3.1 Notations", "content": "We use G = (V, E) to denote a graph, where V = {v_1, v_2, ..., v_N} is a set of N nodes and & is a set of edges. We use A to denote the adjacency matrix, where A_ij = 1 means nodes v_i and v_j are connected; otherwise A_ij = 0. The text-attributed graph can be defined as G_T = (V, A, X), where X = {X_1, X_2, ..., X_N} denotes the set of raw texts and can be encoded as text embeddings X = {x_1, x_2, ..., x_n}. In semi-supervised learning, The node set V can be divided into two different sets: (1) the labeled node set V_l and (2) the unlabeled node set V_u. Moreover, we use V_s to denote the labeled node set including both original labeled data and the data selected through active learning, and use y to represent the label set, where y = {Y_1, Y_2,\u2026, Y_N}."}, {"title": "3.2 Understanding LLM's Capability", "content": "As the sparse label challenges GNN, in this paper, we aim to imbue GNNs with the zero-shot learning prowess of LLMs, thereby elevating their performance in scenarios with limited labeled data. However, LLMs might be good at classifying certain nodes while performing poorly on other nodes. Thus, it is important to identify nodes that LLMs can provide superior pseudo-labels with rationales, whereas GNNs cannot, which can better enhance GNNs' performance. Hence, we first conduct preliminary experiments to understand the key metrics that are pivotal for LLMs in generating reliable pseudo-labels.\nLLMs may benefit from various metrics to perform node classification well. Particularly, in graph G, certain metrics exert a more pronounced influence on the correctness of LLM predictions on nodes, which include: 1) node features, such as the title and abstract in the citation network; 2) degree of a node, and 3) the homophily ratio. As experimentally demonstrated in [18, 21], LLMs can provide better classification results with nodes that contain richer information. Therefore, we decided to leverage LLMs as a teacher model to provide enhanced rationales to teach GNNs from the feature perspective at the embedding level. However, when using active learning to identify valuable nodes, we primarily rely on the original node features. Both degrees and the homophily ratio are important for a node. The former indicates how many nodes will be affected by a node, while the latter suggests that the node tends to connect with others having similar features. Therefore, it's crucial to examine how the degree and the homophily ratio affect the performance of LLMs' predictions.\nWe conduct preliminary experiments to understand how these factors influence the classification performance of LLMs. Specifically, we use the following equation to compute the homophily ratio:  HR = # of neighbors have same label / total # of neighbors\nWe divide degree and homophily into 3 categories: highest, middle, and lowest, and select 200 nodes for each category. Specifically, We sort the nodes based on the degrees and the homophily ratio in descending order, evenly selecting 200 nodes from the head, tail, and middle of the node list for the highest, lowest, and middle categories, respectively. The GPT-3.5-turbo is used for testing. We provide the raw text X_i and the potential classes to the LLMs, asking them to assign a label from the given class to X_i. Then, we compare the results from LLM and the ground truth labels for evaluation."}, {"title": "3.3 Problem Statement", "content": "As LLMs could not give reliable knowledge for all the nodes, in this paper, we study a novel problem of how to effectively leverage LLMs to enhance the performance of few-shot node classification over graphs. Given a text-attributed graph G_T = (V, A, X) with a very limited labeled node set V_l (i.e. |V_1| < |V_u|) and their label set Y, a budget size B (note that the budget size B is the number of nodes per class), and a large language model LLM, we aim to train a GNN model that can have better performance with only few available labeled nodes by querying LLM within the budget size B. The details will be introduced in Section 4."}, {"title": "4 PROPOSED MODEL", "content": "Though GNNs have shown great power in node classification, the vanilla GNNs suffer from low generalizability with few labeled data for training [10]. Thus, in order to enhance the generalizability of GNNs, we propose a framework that integrates GNNs with LLMs and employs a novel Graph-LLM-based active learning strategy to actively distill knowledge from LLMs. Our proposed model uses GNN as the backbone model and takes advantage of LLMs' zero-shot pseudo-labeling and reasoning capabilities, especially for nodes that are difficult for GNN to give accurate predictions. In these instances, LLMs can offer reliable pseudo-labels and provide enhanced rationales, thereby improving the few-shot learning capability of GNNs from distinct perspectives."}, {"title": "4.1 Base GNN Classifier", "content": "As GNNs have shown great power in semi-supervised node classification, we adopt Graph Neural Networks (GNNs) as the backbone models, which can be used to capture the structure information between entities and naturally propagate the information to all unlabeled nodes efficiently. We first use SBERT [36] to encode raw texts X to text embeddings X. Then, we use GNNs to perform on the given graph and these embeddings. Specifically, the GNN takes the graph G_T as input and learns the node representation as\nH = GNN(A, X) (1)\nwhere H_f is the node representation matrix from the last layer of GNN. The final prediction results can be computed as:\nZ = softmax(H_f) (2)\nwhere Z is the probabilities for all nodes in the graph. The loss function for training the GNN will be introduced in 4.4."}, {"title": "4.2 Obtaining Knowledge from LLM", "content": "Despite GNNs showing success in dealing with graph data, the generalizability of GNNs with few available data is still limited. To tackle this challenge, we introduce LLMs as teacher models, leveraging their zero-shot ability [26] to instruct GNNs in classification tasks and provide insights into the reasoning behind these decisions. In this way, GNNs can learn hidden label distribution information and enhanced feature information from LLMs, which empower the capabilities of GNNs with scarce labeled data\nTo effectively distill knowledge from LLMs to GNN, we consider two types of knowledge: (1) soft labels and logits; and (2) rationales behind LLMs' decision-making process. Soft labels and logits reveal hidden distribution information for unlabeled data, while rationales contribute richer node information. This combination allows GNNS to benefit from the unlabeled data and get enhanced node features. We prompt the prediction and reasoning in a two-step manner: first, we input the raw texts into the LLMs to generate the soft labels and logits with the probability distribution. We then let LLMs explain the reason why they make these decisions. Examples of prompts are shown in Table 1. Due to the possibility of LLMs producing outputs that deviate from the desired format when prompts contain numerous tasks, thereby increasing uncertainty in the parsing process, we propose to utilize two separate prompts to obtain logits and rationales. Next, we give the details."}, {"title": "4.2.1 Soft Labels and Logits Generation", "content": "For a node v_i, our proposed model first feeds raw text X_i into LLMs to generate soft labels \\hat{y}_i for X_i and the logits l_i for all possible categories. An example of the prompt for soft labels and logit generation is shown in the first row of Table 1. We leverage the zero-shot ability of LLMs to generate relevant reliable soft labels and logits so that GNNs can leverage the hidden information of unlabeled data with knowledge distillation. This can be formally written as\nY_i, l_i = LLM(X_i; prompt) (3)"}, {"title": "4.2.2 Rationales for Feature Enhancement", "content": "Traditional knowledge distillation methods primarily utilize the soft labels and logits from the teacher model. Nonetheless, incorporating the rationales behind text decisions can significantly enhance the learning capabilities of GNNs [18]. In this context, GNNs are able to learn more informative features from the LLM at the embedding level. Consequently, we introduce LLMs as a feature teacher, guiding GNNs to assimilate more informative features in their decision-making process. Different from previous works that concatenate the enhanced embeddings and the node embeddings or simply replace the node representations directly, we will use a loss function to minimize the difference between them, which will help GNNs learn the enhanced representation while retaining the original node representations. The loss function will be detailed in Section 4.3\nFor a node v_i, LLMs will output the classification result for X_i with a detailed explanation of the decision-making process. The enhanced explanation R_i can be represented as follows:\nR_i = LLM(X_i; prompt) (4)\nAn example of the prompt for rationales is shown in the second row of Table 1. Since the rationales we get from LLMs are all textual explanations, we further need to transform them into the embedding level to teach GNNs the more informative features. We use a pre-trained language model such as Sentence BERT (SBERT) [36] to get the embeddings for R_i, which can be represented as follows:\n\\tilde{r}_i = LM(R_i) (5)\nwhere \\tilde{r}_i means the embedding of i-th rationale.\nHowever, since the dimensionality of \\tilde{r}_i may differ from the dimension of the final layer of GNNs, alignment between these representations is necessary. While min/max pooling can effectively reduce dimensionality for alignment purposes, it tends to lose information during the pooling process. To retain the enriched information from these rationales, we train a Multi-Layer Perceptron (MLP) using text embeddings X_l of the limited labeled node set V_l and their corresponding ground truth labels Y_l, applying the cross-entropy loss function. This MLP is tasked with aligning the representations between the rationales \\tilde{r} and the outputs H_f of the final layer of GNNs, ensuring that valuable information is retained throughout the alignment process. The final representation for i-th rationale is generated as follows:\nr_i = MLP(\\tilde{r}_i) (6)\nwhere r is the embedding that has the same dimension as the final layer's outputs H_f in GNNs."}, {"title": "4.3 Distilling Knowledge to GNN", "content": "With the knowledge from LLM represented as r_i and l_i, we use knowledge distillation [19] to distill this knowledge into GNNs. Through this process, GNNs can tap into the hidden information behind unlabeled nodes by using output logits to enhance their performance. Moreover, they can achieve improved node representations by incorporating the rationales generated from LLMs to further enrich the depth and quality of the information being processed. Specifically, LLMs serve as a pre-trained teacher model to teach the student model (GNNs) from two distinct perspectives: 1). soft labels and the probability distribution (logits) and 2). the rationales at the embedding level."}, {"title": "4.3.1 Loss for Knowledge Distillation", "content": "Let V_s be the set of nodes including the original training data and the data selected through active learning (to be introduced in Section 4.5). Following [29], for each v_i \u2208 V_s, we first convert the logits l_i from LLM as:\np(y_i = j|LLM) = exp (l_{ij}/\u03c4) / \u2211_{c=1}^{C}exp (l_{ic}/\u03c4) (7)\nwhere C is the number of classes, \u03c4 is the knowledge distillation (KD) temperature to control how much of the teacher's knowledge is distilled to student model and l_{ij} is the j-th elements of l_i. Then, the student can learn the distilled knowledge from the teacher by optimizing the following loss function:\nL_T = - 1/|V_s| \u2211_{v_i \u2208 V_s} \u2211_{j=1}^{C} p(y_i = j|LLM) log Z_{ij} (8)\nwhere Z_{ij} is the probability that v_i belongs to class j by GNN. This enhances the model's capacity to get insights from unlabeled data and augment its overall learning capabilities."}, {"title": "4.3.2 Loss for Feature Alignment", "content": "We also introduce rationales to augment the node representation from a feature perspective at the embedding level. With the embeddings r we get from 4.2, Mean Square Error (MSE) is used to calculate the loss between the rationales and node embeddings H_f at the final layer of GNNs for all the nodes in the current training set V_s as:\nL_F = 1/|V_s| \u2211_{i=1} ||H_f - r||_2^2 (9)\nBy employing this approach, the GNNs learn informative rationales from LLMs, enhancing their learning capabilities from a feature perspective at the embedding level."}, {"title": "4.4 Objective Function of Proposed Framework", "content": "The student model itself computes training loss between predictions and ground truth (hard attribute label), which is defined as:\nL_s = 1/|V_s| \u2211_{V_i \u2208 V_s} \u2211_{c=1}^{C} I(Y_i = c) log Z_{ic} (10)\nwhere I is the indicator function which outputs 1 if y_i = c otherwise 0. V_s is the set of labeled or pseudo-labeled nodes, and y_i is the label or pseudo-label of v_i.\nWith the knowledge distillation from LLM, the final loss function of our proposed model can be formalized as follows:\nmin_{GNN} L = (1 - \u03b1 - \u03b2)L_s + \u03b1L_T + \u03b2L_F (11)\nwhere \u03b1 and \u03b2 are both balance parameters that are set up to adjust the relative weight of knowledge distillation loss and feature embedding loss, respectively."}, {"title": "4.5 Graph-LLM-based Active learning", "content": "To further improve GNNs' few-shot learning ability, we introduce a novel Graph-LLM-based active learning strategy to select valuable nodes for LLMs and add them to the training set iteratively. We seek to select B nodes where GNNs exhibit low confidence in classification results, yet LLMs can offer high-quality pseudo-labels based on their inherent knowledge. Through iterative selection, we progressively enhance the GNNs' capabilities.\nAs indicated by the preliminary experiment results presented in Section 3, LLMs demonstrate the ability to generate high-quality pseudo-labels for nodes with higher homophily ratios and more degrees. Thus, we define an evaluation metric that combines the confidence score of GNN's prediction, homophily ratio, and degrees to evaluate if the node in the unlabeled node set V_u is valuable for our proposed model. The evaluation metric is defined as follows:\nS_{GL_i} = RS(p_i) + RS(HR_i) + RS(D_i) (12)\nwhere S_{GL_i} means the evaluation score for i-th node, p_i (i.e. p_i = max(Z_i)), HR_i, and D_i denote the final output confidence score, the homophily ratio, and degree for i-th node, respectively. The RS represents a ranking function used to calculate scores for each evaluation metric. Specifically, we arrange the nodes in ascending order according to the evaluation metric results, excluding p_i, and assign scores ranging from 0 to 1 with a step of 1/|V_u|. Note that RS assigns scores to the p_i in descending order, prioritizing nodes for which GNNs cannot generate reliable pseudo-labels.\nConsidering the fact that some nodes can better contribute to label propagation and model improvement in the graph, we would like to add a metric to evaluate the importance of the node and to facilitate selecting the most valuable pseudo-labels. Here, we utilize neighborhood entropy reduction to assess the importance of a given node v_i [43]. Specifically, for each node v_i in the candidate set V_c, we compute the entropy reduction in the neighbors' softmax vectors by removing v_i from the node set V_n, which contains the v_i and its neighbors. The basic intuition is that a node is more informative when it can greatly change uncertainty (entropy) within its neighborhood. In other words, the more changes in entropy, the more important a node is. Then we rank and assign scores to these nodes based on the change of entropy. The score of entropy change is defined as follows:\nSE_i = RS (h(\\hat{y}_{V_n-v_i}) \u2013 h(\\hat{y}_{V_n})) (13)\nwhere SE_i is the score of entropy change for v_i, h(\u00b7) denotes the entropy function, and \\hat{y} represents the pseudo-labels from GNNs, which is computed based on nodes' logits vectors and the activation function. Thus, the final evaluation metric for v_i is:\nS_i = S_{GL_i} + SE_i (14)\nIn each stage, we select subsets of valuable nodes with high S_i, each consisting of b nodes per class. We query LLM to obtain the pseudo-label, logits, and rationals. We then add these nodes to the label set and retrain our model using Eq. 11. We continue this process until the total number of nodes meets the budget size B times the number of classes C. Here, B is a relatively small budget size, achieving a balance between the cost of querying LLMs and the resultant performance improvement. Finally, the selected nodes with pseudo-labels are used to train the final GNN model. This approach makes GNNs benefit from the various abilities of LLMs, enhancing their performance with scarce labeled data."}, {"title": "5 EXPERIMENTAL RESULTS AND ANALYSIS", "content": "In this section, we present the evaluation results of our proposed few-shot node classification model on several benchmark datasets. We aim to address the following research questions:\n\u2022 RQ1: How does our proposed model perform compared with the state-of-the-art baselines under consistent settings?\n\u2022 RQ2: How do different hyper-parameters impact the performance of our model?\n\u2022 RQ3: How do different components in our proposed model contribute to the performance?"}, {"title": "5.1 Experimental Setup", "content": "5.1.1 Datasets. We evaluate our proposed model using three public citation datasets: Cora, Citeseer, and PubMed [25]. These datasets are among the most commonly utilized citation network datasets for evaluating GNN models in node classification tasks. In these datasets, nodes represent publication documents with topics serving as labels, edges depict citation links between publications, and node features are derived from the title and abstract of publications. The dataset statistics are shown in Table 2.\n5.1.2 Implementation. We randomly select n-shot (i.e. n \u00d7 C) samples as training data. Then, from the remaining data, we randomly choose 200 instances as test data and 200 as validation data, respectively. It is important to note that since we randomly select n-shot nodes as training nodes for the few-shot setting, the choice of seeds will influence the quality of the initial nodes, thereby impacting the classification performance. Hence, we conduct experiments with different seeds [0, 1, 2] and use the average accuracy as our final results. For our Graph-LLM-based active learning strategies, we set the budget size B = 3, indicating the selection of 3 samples per class in total during the graph active learning process. The balanced parameters are configured as \u03b1 = 0.3 and \u03b2 = 0.1, and the KD temperature \u03c4 = 3 is used for distilling knowledge from LLMs to GNNs. Additionally, we assess the impacts of different training sizes N, the sample size per class B for active learning, the balance parameter \u03b1 and \u03b2, and the KD temperature \u03c4 in Section 5.3.\n5.1.3 Baselines. We use 7 state-of-the-art GNN models as baselines, including 3 backbone GNN models: graph convolutional network (GCN) [25], Graph-SAGE [14], and Graph Attention Network (GAT) [41]; 2 GNN based few-shot learning model: Meta-PN [8] and CGPN [42]; 1 graph self-supervised model: MVGRL [17]; and 1 LLM-based few-shot learning method [50].\n\u2022 GCN: The vanilla GCN conducts convolution operations on graph-structured data, which aggregates information from the neighboring nodes to iteratively update node representations.\n\u2022 GAT: GAT incorporates an attention mechanism into GNN for feature aggregation, which allows GATs to focus on more important neighbors and get better node representations.\n\u2022 GraphSAGE: GraphSAGE samples neighbors and employs mean aggregation to learn node embeddings, efficiently capturing the graph's structural information.\n\u2022 MVGRL: MVGRL is a benchmark in GNN self-supervised learning by using data augmentation to create diverse views for contrastive learning, employing graph diffusion, and subgraph sampling to enhance its performance.\n\u2022 CGPN: CGPN introduces the concept of poison learning and utilizes contrastive learning to propagate limited labels across the entire graph efficiently.\n\u2022 Meta-PN: Meta-PN uses meta-learning and employs a bi-level optimization method to generate high-quality pseudo-labels.\n\u2022 LLM-based model: This LLM-based model leverages LLMs to generate the pseudo nodes for each class, uses LM to encode these nodes, and uses an MLP to build edges.\nNote that the LLM-based model does not provide the original code; therefore, we independently developed the model based on the description provided in the paper. All experimental results are obtained by us under consistent settings, including the initial training nodes and initial embeddings."}, {"title": "5.2 Comparison with Baselines", "content": "We conduct comprehensive experiments to evaluate the performance of our proposed model compared with 7 state-of-the-art baseline models under consistent settings and aim to answer RQ1. Specifically, we compare our model with seven state-of-the-art models using shots n = [1, 3, 5, 7]. Additionally, we assess our model with different backbone models, including GCN, GAT, and Graph-SAGE. The experiment results that are compared with baselines can be found in Table 3, Table 4, and Table 5. From the experimental results, we can observe that the LLM-based few-shot model either matches or surpasses both traditional GNN models and meta-learning-based models. Obviously, our proposed model with GCN as the backbone GNN model outperforms all state-of-the-art baselines by a large margin in different shots. For example, with the 3-shot setting, the improvement margin of accuracy is (4, 3```json\n1)% for Cora, (2, 23)% for Citeseer, and (2, 24)% for PubMed. These observations highlight that our proposed model can achieve state-of-the-art performance with fewer labeled nodes, rendering it a promising approach for few-shot node classification tasks.\nFrom the experimental results presented in these tables, we can also observe that the backbone model influences the performance of our proposed model. Our model tends to achieve better performance when using a backbone model that already demonstrates superior performance. However, when comparing our proposed model with these backbone models, our proposed model exhibits a significant improvement in performance. This underscores the effectiveness of our approach in enhancing the learning capability of GNNs through the integration of LLMs as teacher models."}, {"title": "5.3 Evaluation of Hyper-parameters", "content": "We evaluate the impact of different hyper-parameters to answer RQ2. We evaluate our proposed model with different hyper-parameter settings: training size N\u2208 {C \u00d7 1, C \u00d7 3, C \u00d7 5, C \u00d7 7}; the budget size B\u2208 [1, 2, 3, 4, 5, 7, 10, 15, 20, 25] for graph-LLM based active learning with N = C \u00d7 3; the balance parameters \u03b1 \u2208 (0, 0.5] with \u03b2 = 0.1 and \u03b2\u2208 [0.01, 0.03, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5] with \u03b1 = 0.3, respectively; and the KD temperature \u03c4\u2208 [1, 5]. The evaluation results for training size are shown in Tables 3, 4, and 5. The hyper-parameter evaluations are illustrated in Figure 3.\n\u2022 From Table 3, Table 4, and Table 5, we can easily observe that when we increase the training size N, the performance in terms of classification accuracy consistently improves. Especially when N is increased from 1 to 3, the performance significantly improves.\n\u2022 In Figure 3(a), we observe that as the value of \u03b1 increases, the performance initially improves, and then reaches a peak around \u03b1 = 0.3. However, beyond this optimal point, the performance experiences a sharp decline with further increases in \u03b1.\n\u2022 Figure 3(b) illustrates that the performance increases when \u03b2\u2208 [0.01, 0, 1] and reaches the highest performance at \u03b2 = 0.1. Then the performance keeps decreasing when we enlarge \u03b2, where the performance drops slightly when \u03b2 is less than 0.3, and it significantly drops after 0.3. The observed trend is quite understandable: as we increase the value of the balance parameters, the proportion of loss based on the ground truth gradually decreases. When the proportion falls below a certain threshold, the loss is primarily driven by the teacher loss and feature embedding loss. However, the quality of these pseudo-labels and feature embeddings generated from LLMs cannot be guaranteed.\n\u2022 Figure Figure 3(c) shows the results of the evaluation on KD temperature \u03c4. It indicates that as we increase \u03c4, the performance initially experiences a significant improvement, stabilizes at a high level for \u03c4\u2208 [3, 5], and then drastically drops when \u03c4 changes from 5 to 9. It's not difficult to understand this trend: when \u03c4 is relatively small, the soft label probabilities distilled from the teacher model are informative and assist in optimizing the student model. However, when \u03c4 becomes large, the distilled knowledge becomes ambiguous, potentially leading to a smoothing effect on the student model's inference ability.\n\u2022 For the impact of budget size B, as shown in Figure 3(d). As B increases from 1 to 7, we observe a consistent improvement in performance. However, beyond this range, the performance remains stable or even drops, indicating an upper limit to the benefits gained from enlarging B. At this point, any further increase in the budget size results in higher costs, but only minimal gains in performance. This suggests that while increasing the budget can enhance performance up to a certain point, we need to make a trade-off between the performance and the cost."}, {"title": "5.4 Ablation Study", "content": "In this section, we design the ablation study to further investigate how different components contribute to the performance of our model and answer RQ3. Our model proceeds with LLMs and Graph-LLM-based active learning. For the LLMs, we further have two distinct perspectives: 1) soft labels and logits and 2) enhanced rationales. Thus, we investigate three components in our model design: soft labels and logits, enhanced rationales, and Graph-LLM-based active learning. (1) Soft labels and logits refer to the soft labels and logits that get from LLMs, which are used for knowledge distillation; (2) rationales refer to getting the enhanced explanation from LLMs, which will provide insights from the feature perspective at the embedding level. (3) Graph-LLM-based active learning refers to selecting valuable nodes for the model. The hyper-parameter is set to N = k \u00d7 3, \u03b1 = 0.3, \u03b2 = 0.1, \u03c4 = 3, B = 3, and backbone model is GCN. Note that when we solely apply active learning to the model, we select nodes with high confidence scores and prioritize the most important nodes. However, when we integrate the LLM into the model, we employ our Graph-LLM-based active learning strategy.\nAs illustrated in Table 6, all these components contribute to the performance of our model. Among these components, the enhanced rationales have a relatively small impact on the performance. When we add soft-labels and active learning independently, the performance improves by a considerable margin. When we add these two components together, the performance has a significant improvement. The experimental results also showcase that LLMs can effectively enhance the performance of GNNs. Whether we incorporate the logits and enhanced rationales independently or combine them, there is a significant performance improvement. Furthermore, when all these components are integrated, our model achieves state-of-the-art performance.\nFor the rationale alignments, we further evaluate the two different alignment strategies: 1) max pooling and 2) our MLP-based alignment approach. For active learning, we evaluate different selection strategies: 1) randomly select nodes with pseudo-labels 2). select all valuable nodes at once with Graph-LLM-based AL; 2) select nodes in an iteration method with our Graph-LLM-based AL, which means we will select b nodes per class until the total number of selected nodes reach B \u00d7 C. As shown in Table 7, compared with the MLP-based alignment strategy, the improvement by using max pooling is lower, which is reasonable because max pooling will lose some information during the pooling process. For the active learning strategy, the performance of the model with our Graph-LLM-based AL is better than selecting nodes randomly. Moreover, despite selecting all valuable nodes at once has shown significant performance improvement, the performance reaches a new high when we apply the iteration selection strategy. This enhancement is attributed to the iterative active learning process, where GNNs benefit from the LLM's zero-shot inference and reasoning ability to refine their predictions iteratively."}, {"title": "6 CONCLUSION", "content": "In this paper, we extend the task of node classification to a more challenging and realistic case where only few labeled data are available. To tackle this challenge, we propose a novel few-shot node classification model that leverages the zero-shot and reasoning ability of Large Language Models. We treat LLMs as a teacher to teach GNNs from two different perspectives including the logits from the distribution side and enhanced rationales from the feature side. Moreover, we proposed a Graph-LLM-based active learning method to further improve the generalizability of GNNs with few available data by actively selecting and distilling knowledge from LLMs. To assess the effectiveness of our model, extensive experiments have been conducted on three citation networks. The evaluation results demonstrate that our model achieves state-of-the-art performance and LLMs can effectively provide insights to GNNs from different perspectives, reaffirming its effectiveness in node classification, its superiority over baseline methods, and its practical significance in addressing the challenges of few-shot node classification."}]}