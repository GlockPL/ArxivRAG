{"title": "ANALOGXPERT: AUTOMATING ANALOG TOPOLOGY\nSYNTHESIS BY INCORPORATING CIRCUIT DESIGN EX-\nPERTISE INTO LARGE LANGUAGE MODELS", "authors": ["Haoyi Zhang", "Yibo Lin", "Runsheng Wang", "Shizhao Sun", "Jiang Bian"], "abstract": "Analog circuits are crucial in modern electronic systems, and automating their de-\nsign has attracted significant research interest. One of major challenges is topol-\nogy synthesis, which determines circuit components and their connections. Recent\nstudies explore large language models (LLM) for topology synthesis. However,\nthe scenarios addressed by these studies do not align well with practical applica-\ntions. Specifically, existing work uses vague design requirements as input and out-\nputs an ideal model, but detailed structural requirements and device-level models\nare more practical. Moreover, current approaches either formulate topology syn-\nthesis as graph generation or Python code generation, whereas practical topology\ndesign is a complex process that demands extensive design knowledge. In this\nwork, we propose AnalogXpert, a LLM-based agent aiming at solving practical\ntopology synthesis problem by incorporating circuit design expertise into LLMs.\nFirst, we represent analog topology as SPICE code and introduce a subcircuit li-\nbrary to reduce the design space, in the same manner as experienced designers.\nSecond, we decompose the problem into two sub-task (i.e., block selection and\nblock connection) through the use of CoT and in-context learning techniques, to\nmimic the practical design process. Third, we introduce a proofreading strategy\nthat allows LLMs to incrementally correct the errors in the initial design, akin to\nhuman designers who iteratively check and adjust the initial topology design to\nensure accuracy. Finally, we construct a high-quality benchmark containing both\nreal data (30) and synthetic data (2k). AnalogXpert achieves 40% and 23% suc-\ncess rates on the synthetic dataset and real dataset respectively, which is markedly\nbetter than those of GPT-40 (3% on both the synthetic dataset and the real dataset).", "sections": [{"title": "1 INTRODUCTION", "content": "Analog circuits form the backbone of many modern electronic systems, playing a crucial role in\nprocessing continuous signals to achieve a variety of tasks in the devices that permeate our everyday\nlives. The significance of analog circuits lies in their ability to excel in situations where information\nmust be captured and processed directly from natural sources, such as sound Zhu & Feng (2024),\nlight Muramatsu et al. (2003), temperature Wang et al. (2013), or pressure Wang et al. (2017).\nAnalog circuit design usually can be divided into three stages: (1) Topology synthesis. Zhao &\nZhang (b) Topology synthesis determines the whole analog circuit topology. This stage will select\nthe basic devices (e.g. transistors, capacitors, resistors) and give the connection relationship among\nthese basic devices. (2) Circuit Sizing. Xiaohan Gao (2024); Wang et al. (2020); Chen et al. (2022);\nCao et al. (2022) The selected basic devices need to be applied with the appropriate parameters in\norder to maximize the circuit performance for a given topology. (3) Layout synthesis. Chen et al.;\nZhang et al. (2024; 2023); Dhar et al. This stage performs the placement and routing for an analog\ncircuit to generate the final layout.\nAnalog topology synthesis is the foundation of the entire circuit design and determines the perfor-\nmance of the circuit. Given the importance of analog topology synthesis, some research has emerged"}, {"title": "2 RELATED WORK", "content": "2.1 ANALOG TOPOLOGY SYNTHESIS AUTOMATION\nAnalog topology synthesis is the most challenging step in the analog design flow and thus has\nattracted extensive research interest. Before the introduction of LLMs, some research has al-\nready attempted to automate analog topology synthesis with versatile AI methods. For example,\nRLATS Zhao & Zhang (b;c) leverages reinforcement learning(RL) to build up the analog circuit\nstep by step. RLATS establishes a subcircuit library to simplify the topology synthesis problem so\nthat the RL agent is able to handle it. However, the design diversity of analog circuits makes it\ndifficult to transfer RL agents between different circuit types. CKTGNN Dong et al. builds up a 10k\ndataset to train a graph neural network which achieves an impressive performance. The drawback\nis that CKTGNN leverages the ideal model to reduce the design space. The ideal model can not\nrepresent the entire legalized design space and can not be directly converted to the final topology.\nWith the introduction of LLM, more research works on the automation of topology synthesis have\nemerged. LADAC Liu et al. (a) and Artisan Zihao Chen (2024) leverage prompting and supervised\nfinetuning(SFT) to enhance the analog design ability of LLM, respectively. However, they only\nvalidate the proposed framework on a few (<5) real analog cases. This is not enough to demonstrate\nthat LLM has adequate analog circuit design capabilities. LaMAGIC Chang et al. build up a 120k\nsynthetic dataset to support the SFT of LLM, but its design tasks are simpler than the actual analog\ndesign tasks. LaMAGIC actually focuses on a kind of radio frequency circuit with limited basic\ndevices (\u2264 6) including a capacitor(2 terminals), inductor(2 terminals), and switch(2 terminals)."}, {"title": "2.2 LARGE LANGUAGE MODELS", "content": "Large language models with pre-trained parameters such as GPT, LLaMa Dubey et al. (2024); Tou-\nvron et al. (2023), Claude, have demonstrated terrific ability in versatile tasks. Recent research on\nprompting has further enhanced the LLM ability. For example, Chain-of-Thought(CoT) Wei et al.\n(2023) is a technique that encourages language models to generate intermediate reasoning steps in\na step-by-step manner, rather than directly providing the final answer. This method improves the\nmodel's ability to solve complex tasks by making its reasoning process more transparent and struc-\ntured. Tree-of-Thought(ToT) Yao et al. (2023) is a famous ToT evolution of CoT, which allows\nmultiple possible solutions or reasoning paths to be explored simultaneously. This approach helps\nimprove decision-making in complex tasks by evaluating different branches of thought before con-\nverging on a final answer. Some other researches focus on In-context learning Dong et al. (2024)\nwhich involves giving a language model examples of tasks or instructions within the same prompt\nto improve the final performance. In addition to pre-generation promptings, post-generation feed-\nback is equally important. Some feedback techniques such as self-reflection Renze & Guven (2024);\nMadaan et al. (2023), can also improve the LLM performance. These techniques provide the foun-\ndation for constructing agents with practical functions, such as gene-editing Huang et al., math Ahn\net al. (2024), and chip designs Liu et al. (b); Blocklove et al.; Lai et al. (b); He et al.. Based on these\ntechniques, we introduce the circuit design expertise, which allows LLM to really deal with analog\ntopology synthesis problem."}, {"title": "3 METHOD", "content": "Method Overview. In this section, we elaborate on the detailed methods of AnalogXpert (as Fig-\nure 2 shows), an efficient training-free analog design agent incorporating human design expertise.\nAnalogXpert introduces a brand new task formulation of conditional analog topology synthesis\nwhich can further demonstrate the design ability of LLM agents (Section 3.1). Leveraging a well-\ndesigned subcircuit library, AnalogXpert constructs a novel and effective analog circuit represen-\ntation which greatly helps LLM agents design analog circuits concisely (Section 3.2). A domain-\nspecific prompting design flow based on CoT is proposed and further enhanced by the in-context\nlearning (Section 3.3). Although the methods mentioned above can improve the design capability of\nLLM agents, the single-round generation mode still struggles to handle complex tasks. Therefore,\nAnalogXpert further introduces human experience-based proofreading to help LLM agents gradu-\nally correct the mistakes and finally generate the correct topology in several rounds (Section 3.4).\nWith the cooperation of these techniques, AnalogXpert has a decent performance in the conditional\nalog topology synthesis tasks."}, {"title": "3.1 PROBLEM FORMULATION", "content": "We propose a brand new formulation of analog topology synthesis as Equation 1 shows. We for-\nmulates the topology synthesis as a real-world SPICE code generation problem. The devices {D;} \nand there connection relationships {$\\sum_{n=1}^{N} N_j(D_i, T_k)n$} are represented directly in the real world\nSPICE code. Our formulation is very different from ideal model Dong et al.; Chang et al.; Zi-\nhao Chen (2024) and Python code-based formulation Lai et al. (a). For ideal model formulations,\nthey have different types of basic devices {D\u00bf} and smaller N indicates less connection relation-\nships. For Python code-based formulation, the {D}and{$ \\sum_{n=1}^{N} N_j(D_i, T_k)n$} are represented in"}, {"title": "3.2 ANALOG CIRCUIT REPRESENTATION", "content": "The conventional SPICE code is built up from devices, such an approach leads to a flexible but com-\nplex generation task which is hard for LLM agents to deal with. In the real analog design process,\nthe analog designers often use the subcircuits instead of the devices. Inspired by this, we propose\na novel subciruit-level SPICE code representation that is built up from subcircuits. For example (as\nFigure 3 shows), devices01-03 belongs to the same subcircuit01 and thus can be simplified to one"}, {"title": "3.3 DESIGN TASK DECOMPOSITION", "content": "AnalogXpert mainly leverages CoT and in-context learning. The CoT has two major steps: (1)\nBlock Selection. Based on the previous subcircuit library, the analog topology synthesis task can be\nperformed like the human design process. In this step, LLM agents select the appropriate subcircuits\nfrom the library according to the design requirements. (2) Block Connection. With the selected\nsubcircuits, LLM agents then determine the connection relationships and generate the final analog\ntopology. For in-context learning, AnalogXpert selects some design task examples as prompts. The\nprinciple of selection is the similarity of design requirements. For a given design requirement,\nAnalogXpert will give the most similar design task as an example. With the CoT and in-context\nlearning, the basic generation functions can be achieved."}, {"title": "3.4 HUMAN EXPERIENCE-BASED PROOFREADING", "content": "During the generation of analog circuit topology, we actually expect the LLM agent to follow some\ndesign rules. A straightforward idea is that we summarize these rules as text and then use them as\nprompts. However, when dealing with a series of rules, the prompt gets longer and longer, and it\nbcomes difficult for an LLM agent to fully understand these rules and follow them strictly. There-\nfore, we propose human experience-based proofreading to get LLM agents out of this dilemma. The\nbasic idea of proofreading is depicted in Figure 4. In practice, the initial netlist does not provide\nenough information for the rule-based checker. We annotate these netlists with the terminal types\nincluding current output (I/O), current input (I/I), and voltage (V). In this way, the previous sub-\ncircuit library can be summarized into 10 types, making the error-checking process simpler (shown\nas the right-top part of Figure 4). After the annotation, a proofreading checker will detect the cor-\nresponding errors, which are mainly categorized into block selection errors and block connection\nerrors. For the accuracy of checking, the proofreading checker is implemented with deterministic\nprograms rather than LLM agents. The detected errors as well as the violated rules make up the"}, {"title": "4 EXPERIMENTS", "content": "Baseline. We compare AnalogXpert with pure GPT-3.5 and GPT-40, which are advanced models\nwith strong code generation ability and rich cross-disciplinary knowledge. For other work related to\ntopology synthesis, as they handle different problems with us, the comparison with them is infeasible\nand thus is excluded. Specifically, AnalogXpert tackles concrete structure design requirement while\nmost related work explores ambiguous design specifications (Dong et al.; Zhao & Zhang, b; Liu\net al., a; Chang et al.; Zihao Chen, 2024; Lai et al., a). Besides, AnalogXpert directly works on\nreal-world device-level model and SPICE code while the related work focuses on ideal model (Dong\net al.; Chang et al.; Zihao Chen, 2024) and Python code-based format (Lai et al., a). If these methods\nare used for our tasks, the results will be close to pure LLMs (e.g. GPT-40, GPT-3.5) performance\n(as listed in Table 2) due to the different task formulation.\nBenchmark. We construct two benchmarks for the final evaluation, including a real data benchmark\nand a synthetic data benchmark. The real data benchmark is collected from a commercial tool named\nAnalog DesignToolbox Anonymous. There are approximately 60 different analog design topologies,\nand we select the most representative 30 analog topologies as the real data benchmark. The synthetic\ndata benchmark is built by a random generation Python code leveraging the subcircuit library. Each\nsynthetic data consists of four parts, the stage number, the input blocks, other given blocks, and\nthe maximum number of blocks. The generation task on synthetic data is selecting some subcircuit\nblocks based on the input blocks and other given blocks to form the final circuit design. The total\nnumber of used blocks should be less than the maximum number of blocks. The stage is set from\none to three, the input blocks and other blocks are randomly selected from the proposed subcircuit\nlibrar, and the maximum number of blocks is randomly selected from a range related to the stage\nnumber(e.g. for one stage the range is 2-5). Finally, we generate 2k synthetic data with totally\ndifferent structure design requirements.\nMetrics. The metric of AnalogXpert is whether the LLM agent can generate the correct analog\ntopology for the given design requirements in one trial. For real data, the correctness can only\nbe determined if all blocks and connections exactly match the design requirements. Such strict\ncorrectness requires humans to check the analog topology results directly. For synthetic data, the"}, {"title": "Main Results", "content": "In the main experiment, each method conducts seven different tasks including six\ntasks on synthetic data and one task on the real data. The design requirements of Task 1-3 have\ndifferent input stage numbers (1-3) without any given blocks. The design requirements of Task 4-6\nhave different input stage numbers (1-3) with an extra given block. The pure GPT-3.5 Turbo and\nGPT-40 are only tested on one hundred synthetic data because the automatic check program can\nnot be performed without the subcircuit library-based representation. Thus, the results are checked\nby humans. On synthetic data Pure GPT-40 and pure GPT-3.5 Turbo can only achieve a correct\nratio of 3% and 1%, respectively. Such experimental results demonstrate the importance of the pro-\nposed subcircuit library-based representation method. We can also observe that AnalogXpert(40%)\noutperforms GPT-3.5Turbo+Ours(28%) on synthetic data. This result indicates that the proposed\nframework needs models with sufficient comprehension to generate the topology more accurately.\nOn real data, only AnalogXpert achieves a 23% correct ratio, other methods have near-zero correct\nratios. The failure of GPT-3.5Turbo+Ours on real data is due to poor model comprehension and a\nsignificant increase in task difficulty. The experimental results on both benchmarks demonstrate the\neffectiveness of the proposed AnalogXpert in dealing with complex connection relationships and\nreal-world topology synthesis problems."}, {"title": "Ablation Study", "content": "We perform the ablation study on each component of AnalogXpert except for the\nsubcircuit library-based representation. Subcircuit library-based representation is the foundation of\nCoT and proofreading and the framework will become pure GPT-40 without it. The experimental\nresults indicate that CoT & In-context learning has less impact on performance compared to the\nproofreading strategy. We also conduct the experiments with different proofreading rounds. The ex-\nperimental results are also consistent with the intuition that the correct ratio increases as the number\nof proofreading rounds increases, proving the effectiveness of this strategy."}, {"title": "Visulization", "content": "Three visualized results of the real data are shown in Figure 5. Each case shows the\nfailed circuit design in the generation process and is then corrected by AnalogXpert itself during the\nproofreading step. The failure reason in case 1 is the connection error due to the floating current\ninput and output terminal which is not allowed. In real case 2, the AnalogXpert makes a mistake\nin the selection of subcircuits. the cascode stage should be N-type but AnalogXpert first selects the\nP-type. Real case3 is a two-stage amplifier, the AnalogXpert connects the input of the second stage\nto the input of the first stage which destroys the structure of the two stages."}, {"title": "5 CONCLUSION", "content": "In this work, we propose AnalogXpert, a powerful analog topology synthesis tool based on training-\nfree LLMs. AnalogXpert leverages the subcircuit-based circuit representation, CoT&in-context\nlearning, and human experience-based proofreading to imitate the human design process and im-\nprove the design accuracy. Both real data and synthetic data benchmarks are constructed on the\nstructural requirements tasks. The experimental results (40% and 23% correct ratio in real and\nsynthetic data) demonstrate the effectiveness of the AnalogXpert."}, {"title": "Limitation and Future Directions", "content": "Although this work has constructed both a real dataset and a\nsynthetic dataset, the size of the real dataset is still small. A larger real dataset may lead to a brand\nnew prompting method in the analog topology synthesis problem which can achieve a better correct\nratio. In the future, AnalogXpert can be used to generate sufficient synthetic data of the analog\ncircuit topologies in the SPICE code format. With high-quality synthetic data, some small models\ncan be fine-tuned to achieve a competitive performance. Having fine-tuned mini-models that can\nbe run locally ensures the security of design data, which is important for commercial circuit design\ncompanies."}, {"title": "A APPENDIX", "content": "A.1 PROMPTS OF THE ANALOGXPERT\nThe prompt of real tasks are provided as follows."}, {"title": "User (Design prompt for real tasks)", "content": "You are a professional analog designer, and now you need to design the\nrequired analog circuits with the given design libaray of some analog basic\ncomponents. Here is the libaray details, including Cell NAME, PinINFO\nand detail Description:\n[Subcirtuit Library Prompt]\n1. Cell Name: CascodeStageN\nPININFO: DRAIN(I/I) SOURCE(I/O) VBIAS(B) GND(P)\nDescription: Single NMOS Cascode\n2. Cell Name: CascodeStageNPair\nPININFO: DRAIN1(I/I) SOURCE1(I/O) DRAIN2(I/I) SOURCE2(I/O) VBIAS(B)\nGND(P)\nDescription: A Pair of NMOS Cascode\n3. Cell Name: CascodeStageP\nPININFO: DRAIN(I/O) SOURCE(I/I) VBIAS(B) VDD(P)\nDescription: Single PMOS Cascode\n4. Cell Name: CascodeStagePPair\nPININFO: DRAIN1(I/O) SOURCE1(I/I) DRAIN2(I/O) SOURCE2(I/I) VBIAS(B)\nVDD(P)\nDescription: A Pair of PMOS Cascode\n5. Cell Name: DiodeConnectedN\nPININFO: DRAIN(I/I) GND(P)\nDescription: DiodeConnected Signle NMOS\n6. Cell Name: DiodeConnectedP\nPININFO: DRAIN(I/O) VDD(P)\nDescription: DiodeConnected Signle PMOS\n7. Cell Name: CommonSourceN\nPININFO: DRAIN(I/I) VIN(V) GND(P)\nDescription: common source single NMOS amplifier\n8. Cell Name: CommonSourceP\nPININFO: DRAIN(I/O) VIN(V) VDD(P)\nDescription: common source single PMOS amplifier\n9. Cell Name: CurrentMirrorCSN\nPININFO: 01(I/I) O2(I/I) VBIAS(B) GND(P)"}, {"title": "User (Design prompt for synthetic tasks)", "content": "You are a professional analog designer, and now you need to design the\nrequired analog circuits with the given design libaray of some analog basic\ncomponents. Here is the libaray details, including Cell NAME, PinINFO\nand detail Description:\n[Subcirtuit Library Prompt]\n*** Inputblocks Start ***\n1. PinInfo: T1(I/I) T2(V)\nCellName: CommonSourceN\n2. PinInfo: T1(I/O) T2(V)\nCellName: CommonSourceP\n3. PinInfo: T1(I/O) T2(I/O) T3(V) T4(V)\nCellName: DifferentialPairP DifferentialPairPBS"}, {"title": "[In-context Learning Prompt]", "content": "Here is an example:\nUserquerry:\nInput Stage: 1\nInput blocks: DifferentialPairP\nOther blocks:\nMax blocks number: 2\nResponse:\n***Netlist Start***\nDifferentialPairP / net01(I\u2014O) net02(I\u20140)\nCurrentMirrorCSN / net01(I\u2014I) net02(II)\n***Netlist End***"}, {"title": "A.2 THE SPICE CODE OF THE SUBCIRCUIT LIBRARY", "content": "The detailed SPICE code of the proposed subcircuit library is shown as follows."}, {"title": "[Task1 Example]", "content": "User Query:\nInput Stage: 1\nInput blocks: DifferentialPairPBS\nOther blocks:\nMax blocks number: 4"}, {"title": "[Task2 Example]", "content": "User Query:\nInput Stage: 2\nInput blocks: CommonSourceP DifferentialPairN\nOther blocks:\nMax blocks number: 4"}, {"title": "[Task3 Example]", "content": "User Query:\nInput Stage: 3\nInput blocks: CommonSourceP DifferentialPairP CommonSourceP"}, {"title": "Real Dataset Example", "content": "Eaxmple1:\n[User Query: ]\nStage Numbers: 1\nCompensation: None\nFeadBack: Type: None, FB Network: None\nInputSignall: Differential\nOutputSignal1: Single-Ended\nInput Type1: NMOS\nTopology1: Common Source\nLoad1: Simple Mirror\nTailBias1: Ground\n[Golden Answer: ]\n.SUBCKT S1 Vbiasn Vin Vip Vout VDD GND\nXI0 net01 Vout VDD / CurrentMirrorP\nXI1 net01 Vout Vbiasn Vin Vip GND / DifferentialPairN"}, {"title": "Eaxmple2:", "content": "[User Query: ]\nStage Numbers: 1\nCompensation: None\nFeadBack: Type: None, FB Network: None\nInputSignal1: Differential\nOutputSignal1: Differential\nInput Type1: NMOS\nTopology 1: Telescopic\nLoad1: Wide-Swing Mirror\nTailBias1: Simple Mirror\n[Golden Answer: ]\n.SUBCKT S2 Vcascp Vcasen Vin Vip Voutn Voutp Vbiasp VDD GND\nXI0 Voutn Voutp VDD Vcascp Vbiasp VDD / CurrentMirrorCSPB\nXI1 Voutn net01 Voutp net02 Vcascn GND / CascodeStageNPair\nXI2 net01 net02 Vbiasn Vin Vip GND / DifferentialPairN"}, {"title": "Eaxmple3:", "content": "[User Query:]\nStage Numbers: 2\nCompensation: Ahuja\nFeedBack: Type: None, FB Network: None\nInputSignal1: Differential\nOutputSignal1: Single-Ended\nInput Type1: Pmos(B2S)\nTopology 1: Folded\nLoad1: Wide-Swing Morror\nTailBias1: Simple Mirror\nInputSignal2: Single-Ended\nOutputSignal2: Single-Ended\nInput Type2: Pmos\nTopology2: CS\nLoad2: Simple Mirror\nTailBias2: Ground\n[Golden Answer: ]\n.SUBCKT S3 VDD GND Vin Vip Vout Vbiasn Vb1 Vbiasp Vcasen\nXI0 net1 net2 VDD / CurrentMirrorP\nXI1 net1 net3 net2 net4 Vb1 GND / CascodeStageNPair"}]}