{"title": "IV-Mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis", "authors": ["Shitong Shao", "Zikai Zhou", "Lichen Bai", "Haoyi Xiong", "Zeke Xie"], "abstract": "The multi-step sampling mechanism, a key feature of visual diffusion models, has significant potential to replicate the success of OpenAI's Strawberry in enhancing performance by increasing the inference computational cost. Sufficient prior studies have demonstrated that correctly scaling up computation in the sampling process can successfully lead to improved generation quality, enhanced image editing, and compositional generalization. While there have been rapid advancements in developing inference-heavy algorithms for improved image generation, relatively little work has explored inference scaling laws in video diffusion models (VDMs). Furthermore, existing research shows only minimal performance gains that are perceptible to the naked eye. To address this, we design a novel training-free algorithm IV-Mixed Sampler that leverages the strengths of image diffusion models (IDMs) to assist VDMs surpass their current capabilities. The core of IV-Mixed Sampler is to use IDMs to significantly enhance the quality of each video frame and VDMs ensure the temporal coherence of the video during the sampling process. Our experiments have demonstrated that IV-Mixed Sampler achieves state-of-the-art performance on 4 benchmarks including UCF-101-FVD, MSR-VTT-FVD, Chronomagic-Bench-150, and Chronomagic-Bench-1649. For example, the open-source Animatediff with IV-Mixed Sampler reduces the UMT-FVD score from 275.2 to 228.6, closing to 223.1 from the closed-source Pika-2.0.", "sections": [{"title": "1. Introduction", "content": "In the large foundation models era, maximizing the inference potential of foundation models [4, 37] that require high pre-training costs has become a research staple for academics [40, 41]. Efficient plug-and-play algorithms [25, 45] can significantly drive large-scale models to reach their full potential and outperform the original counterparts due to low trial-and-error costs. In contrast to popular inference-heavy algorithms (e.g., chain-of-thought (CoT) [40] and OpenAI's Strawberry [26]), which consistently emerge in the large language model (LLM) field, text-to-video (T2V) synthesis still faces the prevalent challenge of low-quality synthesized videos that lack semantic faithfulness [12, 39]. This limitation severely hampers the deployment and application of video diffusion models (VDMs). Motivated by the success of inference scaling laws in LLMs, we inevitably wonder whether a Markovian chain exists within VDMs similar to that of the autoregressive models [9, 46], allowing us to utilize its properties to enhance VDMs' inference performance? The answer is obvious because the diffusion model is established on differential equations that incorporate both a forward and a reverse process [35], allowing it to be fully considered a Markov chain during inference.\nMotivation. VDM-based training-free algorithms typically face a significant challenge: their performance ceiling is constrained by the VDM itself. To be specific, the videos synthesized from most open-source VDMs [12, 39] exhibit several inherent problems, such as weak semantic consistency between the videos and the prompts, as well as low quality. A widely accepted perspective [41, 44] on this phenomenon is that underperforming VDMs cannot overcome their intrinsic limitations because they are trained on low-quality, small-scale T2V datasets. In contrast, image diffusion models (IDMs) can now reliably serve commercial application scenarios [3], thanks to the well-established dataset ecosystem created by the AIGC community. This discrepancy naturally prompts us to consider whether we can view IDMs as tools to enhance VDMs and how to make IDMs effective assistants for VDMs.\nSince diffusion models employ a multi-step sampling mechanism and the quality of synthesized samples from IDMs is significantly higher than that from VDMs, it is reasonable to use IDMs and VDMs alternately for score function estimation throughout the reverse sampling process. However, this straightforward paradigm leads to a loss of temporal coherence in the synthesized videos during our initial empirical explorations. Inspired by SDEdit [23], we enhance the quality of each frame at every denoising step by performing the following additional operation: 1) first adding Gaussian noise and 2) then denoising using IDMs. Unfortunately, as illustrated in Fig. 2, the approach \u201cR-[.]\u201d, which use Gaussian noise to perform the forward diffusion process, result in significantly lower quality of the synthesized video compared to the standard DDIM process (i.e., Origin in Fig. 2). This phenomenon arises because \u201cR-[.]\u201d over-introduces invalid information (i.e., Gaussian noise) into the synthesized video during denoising. Given this, we consider the more robust deterministic sampling method to"}, {"title": "2. Preliminary", "content": "We review VDMs, SDEdit, DDIM & DDIM-Inversion in this section and further bootstrap how past work has designed VDM-based plug-and-play algorithms in Appendix A.3.\nDiffusion Models. Diffusion models [15, 33, 35] including IDMs and VDMs consist of a forward process and a reverse process. Given $x_0$ represent a D-dimensional random variable sampled from the real data distribution $q_0(x_0)$. The forward process injects Guassian noise $\\epsilon_t \\sim N(0, I)$ to the clean data as follows:\n$x_t = a_t x_0 + \\sigma_t \\epsilon_t,$\nwhere $t \\sim U[\\eta, 1]$ ($\\eta$ is a very small quantity defaulting to 1e-5) and $a_t$ and $\\sigma_t$ are components of a predefined noise schedule. $\\sigma_t$ is monotonically increasing from 0 to 1 in all diffusion models, while $a_t$ can either remain constant (e.g., EDM [16]) or decrease monotonically (e.g., VP-SDE [35] and Rectified Flow [19]) from 0 to 1. The forward process in Eq. 1 can be rewritten as the following stochastic differential equation (SDE):\n$dx_t = f(t)x_t dt + g(t)dw_t,$\nwhere $f(t)$ and $g(t)$ denote the drift coefficient of $x_t$ and the diffusion coefficient of $x_t$, respectively. $w_t$ refers to a standard Wiener process. Eq. 2 has a corresponding ODE-based reverse process defined as:\n$dx_t = [f(t)x_t - \\frac{1}{2}g^2(t)\\nabla_{x_t} log q_t (x_t)]dt,$\nwhere $\\nabla_{x_t} log q_t (x_t)$ denotes the score function $\\nabla_{x_t} log p_t (x_t)$. Since $\\nabla_x log q_t(x)$ cannot be accessed during the reverse process, it must be replaced by a linear transformation $\\epsilon_\\theta(\\cdot, \\cdot)$ using the noise estimation model $\\epsilon_\\theta(\\cdot, \\cdot)$ or the score function estimation model $s_\\theta(\\cdot, \\cdot)$.\nVideo Diffusion Model vs. Image Diffusion Model. The most critical differences between IDM and VDM lie in their training sets and model architectures. Regarding training sets, IDM typically utilizes large-scale, high-quality datasets, enabled by the increasing availability of both real and synthetic image datasets [31]. Conversely, the performance of VDMs is significantly constrained by the limited number of publicly available video datasets, such as Webvid-10M [2] and Pandas-70M [7], which contain low-quality real data. In terms of model architectures, several VDMs differ from their IDM counterparts because video data $x_0 \\in R^{b\\times c\\times t \\times h \\times w}$ includes an additional time dimension t, unlike image data $x_0 \\in R^{b\\times c \\times h \\times w}$, where b, c, h, and w refer to batch size, number of channels, height, and width, respectively. Therefore, most VDMs incorporate both spatial and temporal blocks, where the spatial block aligns with the module used in the mainstream IDMs, while the temporal block employs 3D convolutions [12] or tokens in the time dimension [21] to capture temporal information.\nSDEdit. SDEdit [23] is an image editing method that produces a latent variable capable of reconstructing the input"}, {"title": "3. Approach", "content": "Observing that IDMs produce high-quality samples without ensuring temporal coherence, while VDMs ensure temporal continuity but generate low-quality video, we propose IV-mixed Sampler to combine the strengths of both IDMs and VDMs (see Fig. 3). In this section, we first describe how to sample from $x_t$ to $x_{t+\\triangle t}$ and from $x_{t+\\triangle t}$ to $x_t$ using IDM and VDM. We then introduce IV-mixed Sampler and outline its design space of hyperparameters, followed by a theoretical analysis. Finally, we discuss the effect of sampling in the latent space on IV-mixed Sampler.\n3.1. Forward (Go!!) and Reverse (Back!!)\nA crucial step in overcoming the bottleneck of VDM $\\epsilon_\\theta(\\cdot, \\cdot)$ with IDM $\\epsilon_\\theta(\\cdot, \\cdot)$ is to address the domain gap between IDM and VDM caused by differences in training data. Therefore, a specialized rescheduling paradigm is required to achieve the mapping $x_t \\rightarrow \\pounds^{-1}(x_t) \\rightarrow \\pounds(\\pounds^{-1}(x_t)) = x_t$, where $\\pounds(\\cdot)$ is a deterministic function with an inverse $\\pounds^{-1}(\\cdot)$. Given this, we can modify $x_{t+\\triangle t} \\approx \\pounds^{-1}(x_t)$ without disrupting the standard sampling process. We utilize DDIM-Inversion and DDIM to implement $\\pounds^{-1}(\\cdot, \\cdot)$ and $\\pounds(\\cdot, \\cdot)$, respectively. By introducing CFG, we can define a new paradigm to implement IDMs' information injection via the operator G, i.e. semantic information injection:\n$x_1 = I(x_t, w_{go}, w_{back}, G, \\epsilon, \\epsilon_{back}) = \\pounds(u + G(u), w_{back}),$\nwhere $u = \\pounds^{-1}(x_t, w_{go})$.\nIn Eq. 6, $w_{go}$ and $w_{back}$ stand for the CFG scales for the DDIM-Inversion and DDIM, respectively. $\\epsilon^\\theta(\\cdot, \\cdot)$ and $\\epsilon^{back}(\\cdot, \\cdot)$ are the noise estimation models used to perform $\\pounds^{-1}(\\cdot, \\cdot)$ and $\\pounds(\\cdot, \\cdot)$, respectively. G is any function whose mission is to make modifications to u. It is worth noting that for IDMs, we first reshape $x_t$ from the shape $b\\times c \\times t \\times h \\times w$ to $(b\\times t) \\times c \\times h \\times w$, and then pass it through the noise estimation model to perform the operation I in the practical implementation. If $w_{go} = w_{back}, \\epsilon^\\theta(\\cdot, \\cdot) = \\epsilon^{back}(\\cdot, \\cdot)$ and G(.) is an identity operator, then $x_1 = x_t$. To understand how I injects semantic information, we rewrite Eq. 7 using a first-order Taylor expansion as\n$I(x_t, w_{go}, w_{back}, G, \\epsilon, \\epsilon_{back}) = \\pounds(\\pounds^{-1} (x_t, w_{go}) + G(\\pounds^{-1} (x_t, w_{go})), w_{back})\\approx \\pounds(\\pounds^{-1} (x_t, w_{go}), w_{go}) + (w_{back} - w_{go})\\frac{\\partial \\pounds(\\pounds^{-1} (x_t, w_{go}), w_{go})}{\\partial \\pounds^{-1} (x_t, w_{go})}\\frac{\\partial \\pounds^{-1}(x_t, w_{go})}{\\partial w_{go}} + G(\\pounds^{-1} (x_t, w_{go})) \\frac{\\partial \\pounds(\\pounds^{-1} (x_t, w_{go}), w_{go})}{\\partial \\pounds^{-1} (x_t, w_{go})} + O((w_{back} - w_{go})^2) + O(G(\\pounds^{-1} (x_t, w_{go}))^2) = x_t # define J = a_{t+\\triangle t} + \\sigma_{t+\\triangle t} - a_t\\sigma_{t+\\triangle t} + (w_{back} - w_{go}) [\\frac{\\partial(\\pounds^{-1} + [J[(\\omega_{back} + 1)\\epsilon_{\\theta}(x_{t+\\triangle t}, t + \\triangle t, c) - \\omega_{back}\\epsilon_{\\theta}(x_{t+\\triangle t}, t + \\triangle t, \\varnothing)]/a_{t+\\triangle t}]))}{\\partial w_{go}}] - w_{back} + (w_{back} - w_{go}) [\\frac{(\\sigma_{t+\\triangle t} - a_{t+\\triangle t}\\sigma_t)[(\\omega_{go} + 1)\\epsilon^\\theta (x_t, t, c) - \\omega_{go}\\epsilon^\\theta (x_t, t, \\varnothing)]/a_{t+\\triangle t}]}{\\partial w_{go}}] + G(\\pounds^{-1} (x_t, w_{go}))\\frac{\\partial \\pounds(\\pounds^{-1}(x_t, w_{go}), w_{go})}{\\partial \\pounds^{-1} (x_t, w_{go})} # Ignore second-order and higher terms.\\approx x_t + (w_{back} - w_{go})\\frac{\\partial \\pounds(\\pounds^{-1}(x_t, w_{go}), w_{go})}{\\partial \\pounds^{-1} (x_t, w_{go})}\\frac{\\partial \\pounds^{-1}(x_t, w_{go})}{\\partial w_{go}} + G(\\pounds^{-1} (x_t, w_{go})) \\frac{(\\sigma_{t+\\triangle t} - a_{t+\\triangle t}\\sigma_t)[(\\epsilon^\\theta (x_t, t, c) - \\epsilon^\\theta (x_t, t, \\varnothing))]}{a_{t+\\triangle t}}$\nObserving the pink term in Eq. 7, we can inject semantic information from both $\\epsilon^\\theta(\\cdot, \\cdot)$ and $\\epsilon^{back}(\\cdot, \\cdot)$ into $x_t$ by setting $w_{back} \\approx w_{go} > 0$. In particular, we set $w_{back} \\approx w_{go}$ and $w_{back} > 0$ by default. Given this, we only need to replace $\\epsilon_{\\theta}(\\cdot, \\cdot)$ with $\\epsilon^\\theta(\\cdot, \\cdot)$ or $\\epsilon^\\Psi(\\cdot, \\cdot)$ to inject specific semantic information, thereby enhancing the visual quality or temporal coherence of the synthesized video.\n3.2. IV-mixed Sampler\nThe paradigm defined in Eq. 6 enables us to easily describe all the algorithms shown in Fig. 2 that do not use Gaussian noise for score function estimation. For instance, \u201cI-I\u201d and \u201cV-V\u201d can represent $I(x_t, -h, h, G, \\epsilon_b, \\epsilon_b)$ and $I(x_t, -h, h, G, \\epsilon_v, \\epsilon_v)$, respectively, under the condition h > 0 and G is an identity operator. Clearly, this represents only a single-step injection of semantic information. We can construct IV-mixed Sampler to allow multi-step injections of semantic information using G and the recursive definition:\n$x_1 = G^1(x_t) = I(x_t, h, h, G^2, \\epsilon_{1,go}, \\epsilon_{1,back}),$\n$G^2(y) = I(y, -h, h, G^3, \\epsilon_{2,go}, \\epsilon_{2,back}),\n$\\vdots$\n$G^N(y) = I(y, -h, h, G^{N+1}, \\epsilon_{N,go}, \\epsilon_{N,back}),$\ns.t. N$\\geq$1,\nwhere N and $G^{N+1}$ refer to the number of semantic information injection and the identity operator, respectively. Through Eq. 8, we can easily represent IV-mixed Sampler with different N. For instance, following the definition in Fig. 2, \u201cIV-VI\u201d can be described as $x_4 = G^1(x_t) =$"}, {"title": "3.3. Discussion", "content": "Hyperparameter Design Space. In this paper, we elucidate three design choices: the $C_2^2$ combinations mentioned in Sec. 3.2, the intervals at which IV-mixed Sampler is performed during standard DDIM sampling, and the dynamic CFG scale. All three forms are expected to find the empirically optimal solution, with the first two being natural explorations and the last addressing the domain gap between IDMs and VDMs, which is expected to be mitigated by adjusting the CFG scale. For the last one, to be specific, we re-express -h and h in Eq. 8 as -$h g^{go}(t)$ and $h^{back}(t)$. Inspired by the Karras's noise schedule [16], we define $h^{go}(t)$ and $h^{back}(t)$ as\n$h^{go}(t) = (\\gamma_{begin}^{go} + t(\\gamma_{end}^{go} - \\gamma_{begin}^{go})),\\rho}),$\n$h^{back}(t) = (\\gamma_{begin}^{back} + t(\\gamma_{end}^{back} - \\gamma_{begin}^{back})),\\rho}),$\nwhere $\\gamma_{end}^{go}$ and $\\gamma_{end}^{back}$ represent the CFG scales when t = 1, while $\\gamma_{begin}^{go}$ and $\\gamma_{begin}^{back}$ denote the CFG scales when t = 0. The parameter $\\rho$ controls the concave and convex properties of the CFG scale curve with respect to t. The experiments in Sec. 4 demonstrate that the dynamic CFG scale can be adjusted to achieve performance improvements for specific aspects (e.g., semantic faithfulness).\nTheoretical Analysis. As shown in Theorem 3.1, IV-mixed Sampler can be elegantly transformed into an ODE, taking the same form as Eq. 3. Consequently, IV-mixed Sampler preserves the standard sampling process (e.g., DDIM or Euler-Maruyama), enabling a trade-off between temporal coherence and visual quality by adjusting the parameters $w_{IDM}^{go\\_back}w_{VDM}^{go\\_back}$, and $\\omega$.\nTheorem 3.1. (the proof in Appendix C) IV-mixed Sampler can be transferred to an ODE. For example, the ODE corresponding to \u201cIV-IV\u201d is\n$dx_t = f(t)x_t - \\frac{1}{2}g^2(t)[(1 + \\frac{w\\_go\\_back}{w}) \\nabla_x log q_{DM}(c|x) + (\\frac{w\\_back}{w})\\nabla_x log q_{VDM}(c|x)]$,\nHere, $\\omega$ refers to the vanilla CFG scale, while both $\\frac{w\\_go\\_back}{w}$ and $\\frac{w\\_back}{w}$ are CFG scales that are greater than 0. Let $\\nabla_x log q_{DM}(c|x)$ and $\\nabla_x log q_{VDM}(c|x)$ represent the score function estimated for pt(xt) using IDM and VDM under classifier-free guidance."}, {"title": "4. Experiment", "content": "In this section, we present experiments to demonstrate the effectiveness of IV-mixed Sampler. Specifically, we perform qualitative and quantitative comparisons across various benchmarks on multiple T2V diffusion models, including ModelScope-T2V [39], Animatediff [12], and VideoCrafterV2 [5]. Additionally, we compare IV-mixed Sampler with three heavy-inference algorithms, FreeInit [41], Unictrl [8] and I4VGen [11], on VDM. Note that I4VGen is also an algorithm designed to enhance VDM performance using IDM. However, its IDM is configured to be consistent with VDM, achieving image synthesis by reshaping from bxcxtxhxw to (bxt)\u00d7c\u00d71\u00d7hxw before passing it through VDM. For further details, please refer to Appendix A.3. Finally, we conduct extensive ablation studies and present visualization to validate the optimal solution of various design choices. More implementation details can be found in Appendix A.\n4.1. Main Results\nChronomagic-Bench 150 & 1649. We evaluate the effectiveness of IV-mixed Sampler on three different VDMs: VideoCrafterV2, ModelScope-T2V, and Animatediff (SD V1.5, Motion Adapter V3). The comparison results on Chronomagic-Bench-150 (w.r.t, 150 prompts) are presented in Table 1. We employ three metrics in this benchmark: UMT-FVD (for visual quality), UMTScore (for semantic faithfulness), and GPT40-MTScore (for temporal coherence and metamorphic amplitude) to assess our proposed method. The experimental results in Table 1 show that IV-mixed Sampler significantly outperforms both the standard"}, {"title": "4.2. Ablation Studies", "content": "As described in Sec. 3.3, we elucidate three design choices, namely the sampling interval of IV-mixed Sampler, the $C_2^2$ species combinations, and the dynamic CFG scale. For 1) the sampling interval of IV-mixed Sampler, the results in Fig. 4 clearly illustrate that performing IV-mixed Sampler across all sampling steps is optimal. Another evident conclusion is that the closer the sampling interval of IV-mixed Sampler is to t = 1, the more significant the performance gain. This suggests that to save computational overhead, IV-mixed Sampler can be applied within the 0%-50% sampling interval or even restricted to the 0%-25% interval. For 2) the $C_2^2$ species combinations, we present its ablation results in Fig. 5. It is evident that \u201cIV-IV", "IV-IV": "or all comparison experiments. For 3) the dynamic CFG scale, the conclusions are not as intuitive as the first two design choices. Specifically, we considered a total of 15 combinations of dynamic CFG scales, where the CFG scale varies from $\\gamma_{end}^{go}$ to $\\gamma_{begin}^{go}$ across five convex, five straight, and five concave functions. In Fig. 6, we set $\\rho$ to 7, 1, and 1/7 to model changes in the CFG scale as convex, straight, and concave functions, respectively. For each specified $\\rho$, we consider five combinations of the IDM's CFG scale $w_{IDM}^{go\\_back}$ and VDM's CFG scale $w_{go\\_back}$: (1) both remaining constant, (2) $w_{IDM}^{go\\_back}$ increasing and $w_{go\\_back}$ increasing, (3) $w_{IDM}^{go\\_back}$ decreasing and $w_{go\\_back}$ decreasing, (4) $w_{IDM}^{go\\_back}$ increasing and $w_{VDM}^{go\\_back}$ decreasing, and (5) $w_{IDM}^{go\\_back}$ decreasing and $w_{VDM}^{go\\_back}$ increasing. For the \u201cconstant\u201d case we make $\\gamma_{end}^{go} = \\gamma_{begin}^{go} = 4$, for the", "decreasing": "ase we make $\\gamma_{end}^{go} = 6, \\gamma_{begin}^{go} = 2$, and for the", "increasing": "e make"}, {"title": "5. Limitation", "content": "Althrough IV-mixed Sampler significantly improves the performance of VDM, it introduces additional computational costs. For \u201cIV-IV\u201d on Animatediff, it increases the number of function evaluation (NFE) from 50 to 250. In the practical implementation, the computational overhead went up from 21s to 92s at a single RTX 4090 GPU. This problem could potentially be addressed in the future by distillation algorithms similar to accelerated sampling [30, 32, 34]. This exploration of INFERENCE SCALING LAWS first, and then distilling the performance gains it achieves back to the foundation model may be a viable path for the future."}, {"title": "6. Conclusion", "content": "In this paper, we propose IV-mixed Sampler to enhance the visual quality of synthesized videos by leveraging an IDM while ensuring temporal coherence through a VDM. The algorithm utilizes DDIM and DDIM-Inversion to correct latent representations $x_t$ at any time point t, enabling seamless integration into any VDM and sampling interval. IV-mixed Sampler can be formulated as an ODE, achieving a trade-off between visual quality and temporal coherence by adjusting the CFG scales of both the IDM and VDM. In the future, we plan to fine-tune several stronger IDMs, such as FLUX, to better adapt the latent space of target VDMs, thereby further enhancing the performance of VDMs. We anticipate IV-mixed Sampler will be widely applicable in vision generation tasks."}, {"title": "Ethics Statement", "content": "We present IV-mixed Sampler, a method designed to enhance the semantic accuracy and visual quality of video produced by Video Diffusion Models. Although our approach does not directly engage with real-world datasets, we are dedicated to ensuring the ethical use of prompts, while respecting user autonomy and striving for positive outcomes. Acknowledging the commercial potential of IV-mixed Sampler, we emphasize a responsible and ethical deployment of the technology, aiming to maximize societal benefits while carefully mitigating any potential risks."}, {"title": "A. Additional Implementation Details", "content": "We present the relevant metrics and benchmarks used for comparison in the main paper..\nUCF-101-related FVD. The UCF-101 dataset is an action recognition dataset comprising 101 categories, with all videos sourced from Youtube. Each video has a fixed frame rate of 25 frames per second (FPS) and a resolution of 320x240. Several previous works [4, 8, 41] have validated the generation performance of VDMs on the UCF-101 dataset using Fr\u00e9chet Video Distance (FVD) [38]. However, a comprehensive evaluation benchmark for UCF-101 is still lacking. To address this, we follow the methodology of FreeInit, utilizing the prompts listed in [10] to synthesize videos and assess inference performance with FVD. Specifically, we synthesize 5 videos for each of the 101 prompts provided by [10], resulting in a total of 505 synthesized videos. We then compute the FVD between these 505 synthesized videos and 505 randomly sampled videos from the UCF-101 dataset (5 per class), using the built-in FVD evaluation code from Open-Sora-Plan.\nMSR-VTT-related FVD. The MSR-VTT dataset [42] is a large-scale dataset for open-domain video captioning, featuring 10,000 video clips categorized into 20 classes. The standard split of the MSR-VTT dataset includes 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing. For our evaluation, we utilize all 497 validation videos. To ensure evaluation stability, we synthesize a total of 1,491 videos based on prompts from these validation videos, with each prompt producing 3 different videos. We assess the results using the built-in FVD evaluation code from Open-Sora-Plan.\nChronomagic-Bench-150. Chronomagic-Bench-150, introduced in [43] and recently accepted by NeurIPS 2024's dataset and benchmark track, serves as a comprehensive benchmark for metamorphic evaluation of timelapse T2V synthesis. This benchmark includes 4 main categories of time-lapse videos: biological, human-created, meteorological, and physical, further divided into 75 subcategories. Each subcategory contains two challenging prompts, leading to in a total of 150 prompts. We consider three distinct metrics in Chronomagic-Bench-150: UMT-FVD (\u2193), UMTScore (\u2191), and GPT40-MTScore (\u2191), each addressing different evaluation aspects. Specifically, UMT-FVD (\u2193) [20] leverages the UMT [18] feature space to compute FVD, assessing the visual quality of the synthesized video. UMTScore (\u2191) utilizes the UMT [18] feature space to compute CLIPScore [13], evaluating the text relevance of the synthesized video. Lastly, GPT40-MTScore (\u2191) is a fine-grained metric that employs GPT-40 [1] as an evaluator, aligning with human perception to accurately reflect the metamorphic amplitude and temporal coherence of T2V models.\nChronomagic-Bench-1649. Chronomagic-Bench-1649, introduced in [43] and recently accepted by NeurIPS 2024's dataset and benchmark track, is a comprehensive benchmark designed for the metamorphic evaluation of timelapse T2V synthesis. While it shares 75 subcategories with Chronomagic-Bench-150, it offers a more extensive evaluation framework with 1649 prompts, making it significantly more comprehensive than its lightweight counterpart Chronomagic-Bench-150. Chronomagic-Bench-1649 includes 4 key metrics: UMT-FVD (\u2193), MTScore (\u2191), UMTScore (\u2191), and GPT40-MTScore (\u2191), each serving to evaluate different aspects of video synthesis. Specifically, UMT-FVD (\u2193) [20] utilizes the UMT [18] feature space to compute FVD, assessing the visual quality of the synthesized videos. MTScore (\u2191) measures metamorphic amplitude, indicating the degree of change between frames. UMTScore (\u2191) leverages the UMT [18] feature space to compute CLIPScore [13], evaluating the text relevance of the synthesized videos. Finally, GPT40-MTScore (\u2191) is a fine-grained metric that employs GPT-40 [1] as an evaluator, aligning with human perception to accurately reflect the metamorphic amplitude and temporal coherence of T2V models. As with to Chronomagic-Bench-150, we choose to ignore the MTScore (\u2191) metric in our experiments due to its limitations."}, {"title": "A.2. Video Diffusion Models", "content": "We describe the VDMs utilized in this work. Specifically, we employ 3 VDMs with distinct architectures: ModelScope-T2V [39], Animatediff [12], VideoCrafterV2 [6].\nModelScope-T2V. ModelScope-T2V incorporates spatio-temporal blocks to ensure consistent frame generation and smooth motion transitions. Its key features include the utilization of 3D convolution andtraining from scratch. The input"}, {"title": "A.3. Heavy-inference Algorithm on VDM", "content": "Here we discuss two popular VDM-based heavy-inference algorithms FreeInit [41] and I4VGen [11].\nI4VGen. I4VGen [11] is a training-free and plug-and-play video diffusion inference framework that enhances text-to-video synthesis by leveraging robust image techniques. To be specific, I4VGen decomposes the process into two stages: anchor image synthesis and anchor image-guided video synthesis. A well-designed generation-selection pipeline is used to create visually realistic and semantically faithful anchor images, while score distillation sampling (SDS) [27] is employed to animate the images into dynamic videos, followed by a video regeneration process to refine the output. In its official implementation, both phases are realized by VDM, where the anchor image synthesis is performed by merging the time dimension into the batch size dimension through VDM. In essence, I4VGen does not introduce true IDMs to improve the quality of synthesized video obtained from VDMs.\nFreeInit. FreeInit [41] is a novel inference-time strategy designed to enhance temporal consistency in video generation using diffusion models. This approach addresses a key issue: the difference in the spatial-temporal frequency distribution of noise between training and inference, which leads to poor video quality. FreeInit iteratively refines the low-frequency components of the initial noise during inference, bridging this gap without requiring additional training."}, {"title": "A.4. Hyperparameter Settings", "content": "For all comparison experiments, we used the form \"IV-IV\u201d and perform IV-mixed Sampler at all time steps of the standard DDIM sampling. In addition, $\\gamma_{end}^{go}, \\gamma_{end}^{VDM}, \\gamma_{begin}^{Ygo}, and \\gamma_{begin}^{begin}$ all are set as 4. For both Animatediff and ModelScope-T2V, we use"}, {"title": "D. Additional Ablation Study", "content": "We present Fig. 7 here as a supplement to Fig 4 (w.r.t., the sampling interval of IV-mixed Sampler) in the main paper. As illuatrated in Fig. 4 and Fig. 7, it can be noticed that \u201cIV-IV\u201d performs significantly better than \u201cVI-IV\u201d under almost all settings. Furthermore, we visualize Karras's noise schedule of our proposed dynamic CFG scale in Fig. 8 for clear understanding."}, {"title": "E. Visualization", "content": "In order to avoid the size of the paper being too large for the reader, we downsample the video frames and present them here. We present the synthesized video visualization of Animatediff (SD V1.5, Motion Adapter V3) in Fig. 9-10, the synthesized video visualization of ModelScope-T2V in Fig. 11 and the synthesized video visualization of VideoCrafterV2 in Fig. 12."}]}