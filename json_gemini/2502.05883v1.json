{"title": "NeuralPrefix: A Zero-shot Sensory Data Imputation Plugin", "authors": ["Abdelwahed Khamis", "Sara Khalifa"], "abstract": "Real-world sensing challenges such as sensor failures, communication issues, and power constraints lead to data intermittency. An issue that is known to undermine the traditional classification task that assumes a continuous data stream. Previous works addressed this issue by designing bespoke solutions (i.e. task-specific and/or modality-specific imputation). These approaches, while effective for their intended purposes, had limitations in their applicability across different tasks and sensor modalities. This raises an important question: Can we build a task-agnostic imputation pipeline that is transferable to new sensors without requiring additional training? In this work, we formalise the concept of zero-shot imputation and propose a novel approach that enables the adaptation of pre-trained models to handle data intermittency. This framework, named NeuralPrefix, is a generative neural component that precedes a task model during inference, filling in gaps caused by data intermittency. NeuralPrefix is built as a continuous dynamical system, where its internal state can be estimated at any point in time by solving an Ordinary Differential Equation (ODE). This approach allows for a more versatile and adaptable imputation method, overcoming the limitations of task-specific and modality-specific solutions. We conduct a comprehensive evaluation of NeuralPrefix on multiple sensory datasets, demonstrating its effectiveness across various domains. When tested on intermittent data with a high 50% missing data rate, NeuralPreifx accurately recovers all the missing samples, achieving SSIM score between 0.93-0.96. Zero-shot evaluations show that NeuralPrefix generalises well to unseen datasets, even when the measurements come from a different modality.", "sections": [{"title": "I. INTRODUCTION", "content": "Data intermittency is one of the most important problems encountered in the sensing and pervasive computing domains. Unavoidable issues such as transient failures, communication issues, and power constraints lead to intermittent observations that are known to negatively impact the data analysis pipelines and machine learning models acting on the observations. Such a critical problem has been addressed by many research works [1]. Most notable among the proposed techniques are the learned generative models [13] that made a remarkable progress in spatio-temporal data imputation. These models [47] are trained to fill the missing data points by conditioning on observed values.\nA common assumption in learned data imputation is that the test domain is homogeneous to the training domain. Thus,\nZero-shot imputation (ZSI) (Figure. 1). Inspired by the concept of zero-shot learning (ZSL) [32], this paper introduces zero-shot sensory data imputation as a key enabler for han-dling missing data across diverse sensors without requiring re-training. ZSL is a paradigm where a model can make predictions on categories or tasks it has never seen during training by leveraging prior knowledge that relates to the unseen data. Given this, a question that emerges is Can we develop a learned data imputer that generalises to unseen sensory modalities without re-training? This paper answers this question affirmatively.\nTo achieve this, we can build on the observation that intermediate missed frames can be thought of a product of the interaction between appearance (content) and dynamics (mo-tion) features. While appearance features vary across sensor modalities, underlying dynamics (human motion) tend to be similar across dataset and can therefore be leveraged as the zero-shot prior. To illustrate, consider the example sensory data in Figure. 3 from RF and pressure mat sensors: the left RF frames show a hand gesture, while the right frames depict walking on a pressure mat. Despite the disparity in actions and modalities, common dynamics can be observed in the data space. Specifically, the brightest blob in each frame follows a semi-linear trajectory (motion) while being contin-ually deformed (content) over time. This example reflects a broader trend in real datasets, where sensors capture natural phenomena that tend to change smoothly over space and time, following certain continuous physical dynamics. By learning these dynamics, we can impute the missing signal at any time step. Therefore, we propose leveraging these smooth and predictable changes to design an effective generative model that weighs more on the dynamics side (rather than content). This approach enables sensory data imputation pipeline that can be transferred to novel sensory modalities in a zero-shot manner (without the need for re-training).\nTo build on this, we introduce a novel independent com-"}, {"title": "This work makes the following contributions:", "content": "Formalising the concept of zero-shot sensory data impu-tation as a key enabler for handling missing data across diverse sensors without requiring re-training.\nIntroducing NeuralPrefix; a novel and simple architec-tural paradigm (prefix model) that decouples the data intermittency handling mechanism from the target model. Thus, upgrading traditional models with intermittency robustness without re-training or fine-tuning. To the best of the authors' knowledge, this is the first task/sensor-agnostic data intermittency handling solution.\nComprehensive experimental demonstration of Neural-Prefix by evaluating it using large-scale spatio-temporal human action datasets, which include two RF datasets and one tactile carpet dataset, with extensive compar-isons against several traditional and recent imputation approaches."}, {"title": "II. RELATED WORK", "content": "Data Imputation. The field of data imputation has seen a broad range of techniques, from traditional statistical methods to more advanced approaches. Early methods like K-Nearest Neighbors (KNN) [21] and probabilistic approaches such as Expectation-Maximisation (EM) [48] are simple to implement but often serve as baselines due to their inability to capture spatial and temporal dependencies, resulting in limited re-covery quality for practical use. Spatio-temporal multi-view learning [45] improves data imputation by considering both spatial and temporal dependencies, approximating missing values using low-rank matrix completion [14], [6] and tensor decomposition [10], [3]. However, these methods often fail to capture the deeper, inherent dependencies in spatio-temporal sensor data.\nDeep Generative Models. Recently, deep learning ap-proaches have emerged for data imputation [40], [39], in-cluding the use of generative models [1], [37], offering more advanced solutions to address these limitations. This work falls in the category of generative data imputation by building on the same principles. To the best of the author's knowledge, this is the first work to employ generative modelling for zero-shot sensory data imputation. Beyond this main distinction, the framework possesses a number of technical qualities that col-lectively enable a broader range of applications. Specifically, the capability to perform interpolation and extrapolation using an independent parameter-efficient design, where the Neural ODE parameters and memory budget are fixed regardless of the sequence sizes. These qualities collectively enable a broader range of applications.\nNeural ODE works. Our architecture is inspired by the Neural ODE-based video generation models, including MODE-GAN [22] VidODE [31], and [22], with a key distinction in its focus on out-of-domain generalisation; an aspect not explored in these studies. Additionally, NeuralPrefix is designed for sparse (lower-dimensional) sensory data rather than vision data. For this, our approach is purposefully simpler (e.g. no GAN as followed in [31], [22]), and the design choices tailored for the considered data type, such as the shrinkage loss (Sec. III-B).\nTime Series Foundational Models. Concurrent with our efforts, recent advances in foundational models show the potential of task-agnostic data imputation (among other tasks). Inspired by Large Language Models (LLMs), these works [2] posit that a time series model trained on a large collection of multi-domain datasets can generalise to unseen domains without re-training (i.e. zero-shot). While being a promising direction, these models are extremely large, making them much more costly to train and less practical for sensory appli-cations. For example, Amazon's Chronos [2] leverages the T5 Transformer (710M parameters) for time series, requiring 504 hours of training on A100 GPU, whereas NeuralPrefix takes a mere 6 hours of training on RTX4090."}, {"title": "III. METHOD", "content": "Brief Notation. Lowercase bold symbols (v) denote vectors and uppercase bold symbols (V) for tensors. The symbol {i} denotes the sequence [1,\u2026\u2026, N].\nProblem. Our goal is developing the prefix G that takes an intermittent input sequence X whose data samples are ob-served at points {ti} and recovers the data points impacted by intermittency at the temporal coordinates {m}. Specifically, the prefix model is trained on the intermittence-free dataset similar to that used for training the target model. This simple design, given that the prefix model is detached from the host model and target task, is very flexible. Thus, one can use the same prefix for multiple sensors (task models) or even multiple modalities (as we will see in Sec. IV-F)\nFormally, the problem is framed as sequence to sequence regression. Specifically, we can build a generative model to perform the mapping G: {x(t\u1d62)}\u1d62=\u2081\u1d3a \u2192 {x'(m\u1d62)}\u1d62=\u2081\u1d37. The input data frames x(t\u1d62) \u2208 \u211d\u1d42\u02e3\u1d34\u02e3\u1d9c, where W, H and C denote width, height and number of channels; respectively, aren't uniformly spaced in time (i.e. (t\u1d62 - t\u1d62\u208b\u2081) \u2260 (t\u1d62\u208b\u2081 - t\u1d62\u208b\u2082)). Formulated this way, this mapping is interpolation if we set m\u2081 > t\u2081 and m\u03ba <t\u1d67 and extrapolation if m\u043a > m\u2081 > tN.\nAt a high level, we build G as Encoder-Decoder archi-tecture (Sec. III-B) with a learning objective of minimising the data reconstruction error. During training, we simulate data intermittency by masking some samples from the input sequences and tasking the model with reconstructing them. Yet, modelling the sporadic data is challenging, and the key idea is to treat the latent state as a continuous trajectory. The next section introduces this idea.\nA. Continuous Latent Dynamics Primer\nA typical data-driven approach for modelling sequence-to-sequence prediction is recurrent modelling (e.g., RNN). The input data is processed sequentially with the help of internal state h(t\u1d62) corresponding to the temporal coordinate t\u1d62 as follows:\nh(t\u1d62) = RNN(h'(t\u1d62), x(t\u1d62)), h(t\u1d62\u208b\u2081) \u2190 T\u1d3f\u1d3a\u1d3a, h'(t\u1d62) (1)"}, {"title": null, "content": "where TRNN manages the state transition across temporal steps in a uniform manner regardless of the temporal gap between incoming samples. Since Equation (1) assumes uniformly spaced data samples, its internal state is kept unchanged until the next data point arrives as shown in Fig. 4 (top right). Consider the long temporal gap between the square and triangle in the figure. The internal state (and thus the output) won't evolve in that stretch due to the discrete nature of TRNN. A possible solution is to gradually deviate from the last state as time progresses using approaches such as temporal decay [34]. A better alternative is reflecting the continuous nature of the incoming signal into the model's latent space by requiring the internal state transitions to follow a continuous trajectory (identified by an ODE). In effect, combining the data with an ODE state transition prior TODE. Thus, at any temporal point, even when the observation is missing, the model will estimate the internal state trajectory by relying on the last known state and the expected state dynamics (i.e. the rate of change) as depicted in Fig. 4 (bottom left). In other words, we frame the problem as state estimation in a continuous dynamical system.\nSpecifically, and given X, we seek to compute the state h(t) \u2208 \u211d\u1d48 at arbitrary times t\u1d62 (equivalently at intermittent points m\u1d62). We compensate for missing data by relying on a component g(h(t), t) = \\frac{dh(t)}{dt} that captures the evolution of model's internal state. Initially, assume that g(h(t), t) is given (we relax this later). Given the above, h(t) can be determined from the last known state at tk, where tk <ti, as follows:\nh(t\u1d62) = h(t\u2096) + \u222bt\u2096\u1d57\u1da6 g(h(t), t)dt (2)"}, {"title": null, "content": "Intuitively, Equation (2) is an ODE initial value problem (IVP) that continually integrates the rate of change (of the system's state) to find the next state. One step of this process (in a discrete-time step) is depicted in Fig. 4 (top). This is fundamental in solving problems involving dynamic systems.\nBefore continuing, we note two things. First, the dynamics of the hidden state g(h(t),t) aren't known in practice. We follow [5] and parametrise that component as a neural network g@ (h(t), t), where @ denotes the network's weights, to learn the dynamics in end-to-end manner. Second, there is no closed form solution for the integral in Equation (2). However, off-the-shelf ODE solvers, such as Euler and Runge-Kutta, can be leveraged. Consequently and considering the changes, we can re-write Equation (2) as :\nh(t\u1d62\u208a\u2081) = ODESolve (g@, h(t\u1d62), (t\u1d62, t\u1d62\u208a\u2081)) (3)"}, {"title": null, "content": "where ODESolve abstracts the integral computation within a black-box ODE solver, which offers interesting benefits. The solver can be upgraded/adapted during the inference time without requiring re-training. It allows for different trade-offs between computational efficiency and accuracy at various stages of the model's deployment. Note that the end-to-end network training can be done efficiently in the presence of a black-box solver using the Adjoint Sensitivity Method. Details were omitted due to space limitations. The interested reader can consult [5].\nA unique property of continuous modelling is the flexibility that allows catering for various intermittency modes and appli-cations, as shown in Figure 5. In addition to interpolation, the prefix can be configured on extrapolation. Thus, upgrading the task model with early action prediction [42], [23] capabilities. Even more interesting, as ODE can be solved in the backward direction, retrospective discovery can be done. Thus, one can estimate the dynamics preceding the first incoming frame.\nSo far, we demonstrated the mechanism by which we can enable continuous state transition in a neural network. Next, we explain how this mechanism is used in NeuralPrefix encoder-decoder architecture.\nBefore presenting NeuralPrefix architecture, we note a key distinction between the dynamics in NeuralODE (g@) and those in traditional (neural network-free) differential equations (g). Such distinction makes each suitable for a different scenario depending on the application's requirements.\nInterpretability and Data-Efficiency. Indeed, standard differential equations can be used for imputation applications. For example, in a wearable sensing context, one might model human arm movements as a damped harmonic motion. Thus, the dynamics g are explicitly captured (not learned) using the differential equation \\ddot{x} + 2\u03b6\u03c9\u2080\\dot{x} + \u03c9\u2080\u00b2x = 0 where x is the displacement from the equilibrium (rest position), wo is the natural frequency of the system and \u03b6 is the damping ratio. After calibration (i.e. estimating the parameters wo and \u03b6), one can leverage the analytical formulation for imputation directly. A notable advantage here is data efficiency, as the parameters can be estimated using much fewer samples. However, such simplified modelling can fail to capture complexities such as individual variability, fatigue, and the impact of muscle dynamics. Standard differential equations are more suitable in low-data regimes and when the dynamics are well understood.\nNon-rigid Dynamics. Neural differential equations, on the other hand, allows for more flexibility as it learns the dynamics function (g@) in a data-driven way while incorporating neural networks for advanced feature extraction. This approach is especially useful when the true dynamics are unknown or too complex to describe analytically, enabling its application to a wider range of sensory problems. Additionally, it is more powerful when the data is noisy [16], [15]. However, the effectiveness of data-driven methods depends on the quantity and diversity of the data. With the growing availability of large sensory datasets, and given that our approach leverages only unlabeled data, we see significant potential for its application.\nB. NeuralPrefix Archiecture\nOur architecture is a continuous-time encoder-decoder pipeline similar to [36] trained to reconstruct the masked data frames. During training, frames of the complete samples are randomly masked to imitate intermittency. The model is then trained to recover the masked samples by conditioning on the observed ones.\nWe follow the standard blueprint of encoder-decoder ap-proaches. We design continuous-time enconder and decoder. In the encoding stage, we perform forward consolidation in which all the available frames are encoded into one (con-densed) latent state vector. Similar to recurrent approaches, we process the frames sequentially. Unlike them, we base the latent state transition on NeuralODE. For generation, we apply backward unrolling; an autoregressive process that continually generates all the missing frames (in reverse order) from the latent state vector. Directly predicting the target"}, {"title": null, "content": "frame in \"one go\" is typically employed when operating in the in-domain (i.e., training and test samples are expected to be homogeneous). However, this doesn't generalise to out-of-domain settings. To resolve this, we integrate a modular frame generation component. Roughly speaking, we think of the generated frame as mostly a perturbation of the last observed frame. Thus, the decoder is tasked with estimating the perturbations as dictated by the cross-frame dynamics rather than the appearance. Fig. 6 provides an overview of the NeuralPrefix architecture, with details of its components outlined below.\nEncoder (forward consolidation). Convolutional gated re-current unit ConvGRU is used in the encoder GE to capture the spatio-temporal patterns of the sensory data. ConvGRU is an extension of the standard Gated Recurrent Unit (GRU) designed specifically to process spatio-temporal data, such as video frames or sequences of images. It combines the recurrent nature of GRUs, which handle temporal dependencies, with convolutional operations, which handle spatial patterns. Check [4] for more details. At each step, the input is processed using convolutional embedding prior to applying ConvGRU. The latent state is continually estimated at all times as an ODE solution using Equation (3) where a fully connected network MLP approximates the dynamics component ge. As shown in the unrolled view in Figure 6, which is passed to the decoder to be used for frames imputation.\nDecoder (backward unrolling). The decoder GD generates the missing samples at the specified time steps. First, using ODESolve, we estimate the latent codes {h(mi)} (green circles in Fig. 6) at the target temporal coordinates {mi}. Then, the decoder iteratively maps the latent codes into the target frame x' (mi).\nModular Frame Generation. An important question that we address in NeuralPrefix is how to generate frames for unseen modalities? While it is possible to train the decoder to generate the whole frame directly, this approach creates issues in out-of-domain settings. Essentially, it will cause the appearance features to be \"baked\" into the generative model weights. Thus, the output will resemble the training data even when operating on a new modality. To address the issue, we predict the frame differences components rather than the actual frames. Specifically, we adopt a modular generation in which we generate a set of elementary motion components that when combined with the last observed/generated frame recovers the new frame. We predict the motion flow information that represents cross-frames motion of pixels. Then, we compose the actual frame guided by the predicted motion flow and the last observed (i.e. seen or generated) frame. To understand this, one can think of the generated frame as a weighted sum of a transformed (wrapped) version of the last frame and the new contents (appearance residuals) that will appear only in the new frame. Formally [38], [46], the generated frame is :\nx'(m\u1d62) = \u0393m\u1d62Cm\u1d62 + (1 \u2212 \u0393m\u1d62) R'm\u1d62 (4)\nCm\u1d62 = W(Fm\u1d62, x'(m\u1d62\u208b\u2081)) (5)"}, {"title": null, "content": "where \u0393, W and F denote a binary composition mask, the image warping operator and the forward motion flow; respectively. We task the decoder to learn the components of the equation above. The mask is ensured to be bounded in the range [0,1] by applying the sigmoid function. Notably, the motion flow is readily interpretable (akin to optical flow in the vision domain) and can offer insights into the system behaviour under various settings (Sec. IV-F). We can see that the content term is created as a warping of the frame generated by the model at the last timestep. during training, we use a residual loss Lresidual = R'm\u1d62 - Rm\u1d62 where Rm; is calculated from the ground truth as the pixel-wise difference between consecutive frames. Additionally, we employ a content loss Lcontent as the MSE between all the frames generated at mi and the corresponding ground truth.\nShrinkage Loss. The sparsity of sensory (e.g. RF frames) results in an imbalance between the foreground (e.g. RF reflections from hand and body) and background pixels (e.g. pixels with low energy). Making the reconstruction tricky compared to natural images. For example, the Mean Squared Error (MSE) penalises the reconstruction error for the whole image pixels in a uniform way. While this is desirable for natural images with rich content and texture, it can be sub-optimal for data where zero pixels and low energy noise dominate. Without proper weight, the loss unrealistically small even for poor predictions. To mitigate the issue, we suggest attaching more importance to the few, but hard to reconstruct, informative pixels. Inspired by a similar issue in object de-tection [25] and object tracking [26], we adopt the shrinkage loss Lshrinkage (Equation (6) in [26]) that down weights the easy pixels. Figure 7 contrasts it to the MSE. Putting it all together, the model is trained in end-to-end manner with the following loss:\nL(G) = \u03bb\u2081Lshrinkage + \u03bb\u2082Lresidual + \u03bb\u2083Lcontent (6)"}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "where \u03bb\u2081, \u03bb\u2082 and \u03bb\u2083 are hyperparameters controlling the contribution of each loss term.\nA. Datasets\nWe test NeuralPrefix on three publicly available sensory datasets. The first two datasets contain radar sensor measure-ments of human gestures, while the third is a pressure mat dataset capturing daily human activities such as locomotion, exercises, and resting. For all datasets, NeuralPrefix treats the samples as 4D heatmaps with time, spatial (width & height), and channel dimensions. Our motivation for considering the radar datasets stems from their prominence in the RF domain [43] and the significant impact of deployment considerations on the measurements [43]. Even simple changes in either the receivers' placements, the user location, the user's orientation or the furniture in the environment will alter the received signal considerably. While this holds for other sensing modalities such as IMUs for human activity recognition, the domain shift is often more challenging in RF [30]. This presents a nice playground for testing zero-shot imputation performance under deployment domain shift (domain-OOD). Specifically, by training the model in one configuration (user orientation and device placement) and testing it on the rest, we can assess the system's performance in unseen domains. Additionally, the radar and the pressure mat datasets enable us to evaluate the system's performance on unseen modalities (modality-OOD) by training on one dataset and testing on the others. Details of the datasets are provided below.\nGoogle's Soli Dataset [41]. This dataset contains 11 ges-tures from 10 subjects collected over multiple sessions. Each frame of the dataset is represented as a Range Doppler Image (RDI). In an RDI, one axis denotes the radial distance (or range) between the hand and the radar, while the other axis represents velocity. The pixel intensity corresponds to the energy reflected from objects (e.g. hands). The Soli sensor was employed in various applications, including gesture interaction with wearables [41], and objects [9], as well as material sensing [8].\nMCD dataset [24]. This dataset contains RF measure-ments (using TI AWR1843 mmWave radar sensor) from 750 domains (6 environments \u00d7 25 subjects \u00d7 5 lo-cations). The frames are Dynamic Range Angle Image (DRAI) measurements of subjects performing hand ges-tures. In the RF datasets, we use half of the data for training and the other half for testing.\nIntelligent Carpet dataset[27]. This dataset is used as an out-of-domain modality for the zero-shot experiment in Sec. IV-F. A random subject data (24_10_TZ) was used for testing."}, {"title": "B. Metrics", "content": "To report the performance, standard metrics used in spatio-temporal data imputation are employed. Namely, the Mean Squared Error (MSE), the Structural Similairy (SSIM) [44], Peak Signal-to-Noise Ratio (PSNR), and Learned Perceptual Image Patch Similarity (LPIPS) [49]. These metrics capture the distance/similarity between the model's prediction and the ground truth. SSIM is bounded in the range [-1,1]. Additionally, we report the performance on a downstream task of hand tracking after applying the imputation.\nC. Baselines and Training\nIn the evaluation, we compare against several traditional approaches commonly used as efficient baselines in time series imputation works [12], [28]. Additionally, we consider a theoretically-principled training-free approach based on op-timisation and a spatio-temporal interpolation deep model. Specifically, we consider the following traditional approaches:\nMean. Missing values are imputed as the mean of the observed values.\nLast Observation Carried Forward (LOCF). Missing values are replaced with the last observed value for that variable.\nExpectation Maximisation (EM) [11]. We adopt the expectation-maximisation algorithm to impute the missing frames in \u03a7. We consider imputing the temporal trajectory of each pixel Xh,w,c = {xh,w,c(01),\u2026\u2026,Xh,w,c(0q)} where h, w denotes the spatial coordinates, c is the channel coordinate and 01,\u2026\u2026\u2026, Oq is the set observed temporal coordinates. We assume Gaussian Mixture Model (GMM) as the data distribution. The algorithm alternates between probabilities computation (E Step) and parameters update (M Step). After convergence, imputation is done by sampling from the fitted GMM ~ \u03a3\u03ba=1 \u03c0\u03baN(x | \u03bc\u03ba, \u03c3\u00b2\u03ba) where K = 3 is the number of components. \u03c0\u03ba, \u03bc\u03ba, and \u03c3\u00b2\u03ba are the weight, mean, and variance of the k-th Gaussian component; respectively.\nOptical Flow (OF) [17]. OF leverages the vector field describing the apparent motion of each pixel between two adjacent frames for imputation.\nAdditionally, we consider the training-free baseline; Opti-mal Transport (OT) [20]. OT is a mathematical framework that enables us to compute the most efficient way to \"move\" one set of data points (such as probability distributions, images, or signals) to another while minimising some notion of cost. In the context of data interpolation, OT can be used to create smooth transitions between two frames by finding an optimal transportation plan between them. Specifically, we treat two frames as probability distributions P and Q. Then, we compute the optimal transportation plan using Equation 2 in [20]. The imputed data is computed using the transportation plan as a series of intermediate distributions that gradually transition from P to Q. For the training-based baseline, we consider a generative model. For fair evaluation, we compare against a Neural ODE -based architecture [18].\nTraining and Implementation Details. In all experiments, the model is trained for 500 epochs on a Nvidia RTXA6000 GPU. The training takes about 6 hours. Adam optimiser is used, and the learning rate is set as 10\u207b\u00b3, then exponentially decayed at a rate of 0.99 per epoch. We consider a batch size of 64. The parameters \u03bb\u2081, \u03bb\u2082 and \u03bb\u2083 are set to 0.05, 0.5 and 1; respectively."}, {"title": "D. Zero-shot Imputation Performance (domain-OOD)", "content": "We first evaluate the model's imputation performance in two modes; interpolation and extrapolation. In each mode, we drop 50% of the data samples at the points mi. In interpolation mode, the samples are taken at random position mi between t\u2081 and tN. In extrapolation mode, the model observes the first half of the window and then reconstructs the remaining half. We set the window size to 10.\nTable I shows the imputation performance. Quantitatively, NeuralPrefix outperforms all the baselines on both Soli and MCD datasets for most metrics. The performance gap is bigger in the MCD dataset. One can notice that the traditional approaches Mean, LOCF and OF are slightly worse than the learned approaches on the Soli dataset. However, the performance gap is much bigger in the MCD case. This is because MCD dynamics are more complex as the data contains reflections from multiple objects (subject's arm, subject's body and background reflections). On the other hand, Soli data is dominated by hand reflections with minimal background interference. Also, MCD has a much lower sampling rate than Soli (20 FPS in MCD vs 40 FPS in Soli). Thus, cross-frame transitions are much less smooth in MCD. The results reveal that Expectation Maximisation is the least performing on the MCD dataset despite being much more computationally demanding than simpler approaches such as Mean and LOCF. EM has several limitations, including data modelling assump-tions (e.g., assuming Gaussian Mixture Models). Recent works [29], [35] address this by combining EM with deep models. Optimal Transport (OT) also underperforms NeuralPrefix as it relies only on the physical principles (i.e., the most efficient transportation of pixels between frames) without leveraging the knowledge in the training dataset. We exclude traditional approaches from further evaluations.\nQualitatively, it can be seen from Figure 8 that the learned baseline output is smeared out in regions with high energy. NeuralPrefix, on the other hand, preserves the sharp details as its frame composition (Equation (4) and Equation(5)) explicitly learns the motion flow and the difference (residual) across frames."}, {"title": "E. Impact on Downstream Task", "content": "While the results in Sec. IV-D quantify the visual resem-blance between the model's predictions and the ground truth, it is also informative to quantify the impact of this on a downstream task. We perform hand tracking on the MCD frames generated by NeuralPrefix. Note that DRAI sequences encode the location and motion information of the hand and the body. We first locate the hand in each frame as the centre (r, \u03b8) of the blob with the highest magnitude (following Sec. 3.2.7 in [24]), where r and \u03b8 denote the range and angle polar coordinates; respectively. We assume that the location in the ground truth frames is perfect (i.e. error = 0cm) and calculate the deviation with respect to it. The error is reported as euclidean distance d = \u221ar\u1d62\u00b2 + r\u2089\u00b2 - 2r\u1d62r\u2089 cos(\u03b8\u1d62 \u2013 \u03b8\u2089), where the subscripts i and g denote the coordinates in the imputed and ground truth frames; respectively. The results are shown in Figure 9. NeuralPrefix median error is 14.85cm compared to 56.85cm by the baseline. This is a 3.8x improvement in the tracking accuracy. This is a direct consequence of the improved visual quality of NeuralPrefix's output. As the details of frames are sharper in NeuralPrefix case, the hand's blob can be located accurately."}, {"title": "F. Zero-shot Imputation Performance (modality-OOD)", "content": "This section evaluates the zero-shot performance of Neural-Prefix. The model is trained on one dataset (seen) and tested on another (unseen) dataset from a different sensory domain. This is done without re-training or adaptation. Thus, testing the model's zero-shot generalisation capacity. The results in Table. II are promising, demonstrating good performance on the unseen dataset with only minor degradation compared to the in-domain reference (where the target and source datasets are the same). Note that although MCD and Soli are both RF datasets, they were collected using different sensors with different configurations, capturing semantically different measurements. The findings become even more interesting in the second part of Table. II, where we test on the Intelligent Carpet dataset. In this case, Soli is picked as the seen dataset, as it is visually more similar to the carpet dataset than the MCD.\nUpon further analysis, we observe the model can still predict the dynamics on the unseen dataset correctly. In Figure 10 (zoomed-in part), the ground truth reveals the subject motion transition from two feet on the mat to just one. Visually, NeuralPrefix wasn't able to recover the frames exactly. Yet, the forward flow predicted by the model (Fm; in Eq.(4)) caused the active blob to gradually shrink closing the gap between the two shapes.\nRecall from Sec. III-B that the motion flow information (Fm; in Equation. 4) is one of the outputs of the modular frame generation component. Since the motion flow can be visualised as vector flow, we can understand NeuralPrefix's behaviour in unseen modalities by inspecting them. Figure 11 shows the motion flow in seen and unseen modalities. In this case, NeuralPrefix was trained on MCD dataset and tested on Soli then on Carpet without fine-tuning or re-training. First, we note that the motion flow correctly captures the pixels motion across frames in MCD dataset. Interestingly, we noticed that occasional inaccuracies in motion vectors positions can hap-pen without impacting the final frame quality! This happens because the final frame is a composition of the motion flow Fm, and residual Rm\u2081. We notice that when the network fails to exactly recover the motion flow, the estimated residual Rmi compensates for it.\nMoving to the unseen modalities, the dynamics produced for the unseen modalities (Carpet and Soli) are significantly different from the reference modality (MCD) as shown in Figure 11. For example, on Carpet, the vectors magnitudes are much smaller than that of either Soli or MCD. Signifying the true fact that most pixels in the frames belong to the background. This suggests that NeuralPrefix learned transfer-able dynamics from MCD and didn't memorise to the seen dynamics. The dashed box superimposed on the motion flow frames indicates the position of the active object (human foot in Carpet, reflection from hand in Soli) in the original frames. Ideally, these regions should contain the highest magnitude of motion. While it is observable in Soli, it is less observable in Carpet due to the high noise in the background."}, {"title": "G. Computationally Adaptive NeuralPrefix", "content": "One interesting aspect of our architecture is its computa-tional efficiency and elasticity. During the inference stage, NeuralPrefix takes an average of 124 milliseconds for pro-cessing a single sequence X when the window size t is set to 10. For a window size of 20, it takes an average of 196 milliseconds. Additionally, NeuralPrefix offers a very natural way to trade off the accuracy and computational load without changing the current architecture. Thus, having the potential to adapt to the computational requirements of various ubiquitous devices. Specifically, the ODESolve in NeuralPrefix can be expedited by limiting the number of iterations. This can be done very simply by changing the tolerance parameter which controls the accuracy of the ODE solution. It sets the acceptable error threshold that the numerical solver uses when approximating the solution at each time step.\nWe experimented with the tolerance of the ODESolve in the Encoder and Decoder components. The default tolerance is set to 10\u207b\u2075. On the MCD dataset, when increasing it to 10\u207b\u00b3, we gain a processing speedup by 20% without impacting the quality of the frames (SSIM:0.9122 compared to 0.9399 in the default case). A major speedup of 50% can be gained by setting the tolerance to 0.5. However, it comes at the cost of degraded quality of output (SSIM: 0.8134). Note that, this approach is more flexible than weight pruning or quantisation [7], since the tolerance can be adjusted even after the model has been deployed."}, {"title": "V. DISCUSSION AND LIMITATIONS", "content": "In our effort to demonstrate the feasibility of zero-shot imputation through the development of NeuralPrefix, we learned a number of lessons. We highlight both limitations and opportunities that can inspire future research in this field below.\nPerformance on Synthetic Measurements. In our ini-tial investigation, we considered the Widar 3."}]}