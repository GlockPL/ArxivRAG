{"title": "One-cycle Structured Pruning with Stability Driven Structure Search", "authors": ["Deepak Ghimire", "Dayoung Kil", "Seonghwan Jeong", "Jaesik Park", "Seong-heum Kim"], "abstract": "Existing structured pruning typically involves multi-stage training procedures that often demand heavy computation. Pruning at initialization, which aims to address this limitation, reduces training costs but struggles with performance. To address these challenges, we propose an efficient framework for one-cycle structured pruning without compromising model performance. In this approach, we integrate pre-training, pruning, and fine-tuning into a single training cycle, referred to as the 'one cycle approach'. The core idea is to search for the optimal sub-network during the early stages of network training, guided by norm-based group saliency criteria and structured sparsity regularization. We introduce a novel pruning indicator that determines the stable pruning epoch by assessing the similarity between evolving pruning sub-networks across consecutive training epochs. Also, group sparsity regularization helps to accelerate the pruning process and results in speeding up the entire process. Extensive experiments on datasets, including CIFAR-10/100, and ImageNet, using VGGNet, ResNet, MobileNet, and ViT architectures, demonstrate that our method achieves state-of-the-art accuracy while being one of the most efficient pruning frameworks in terms of training time. The source code will be made publicly available.", "sections": [{"title": "1. Introduction", "content": "Over the past decade, deep neural networks (DNNs) have demonstrated superior performance at the expense of substantial memory usage, computing power, and energy consumption [8, 17, 21, 63]. For compressing and deploying these complex models on low-capability devices, pruning has emerged as a particularly promising approach for many embedded and general-purpose computing platforms. Structured pruning [5, 6, 13, 32, 52, 56] and unstructured pruning [11, 14, 19, 51] represent the two predominant methods in pruning. Also, there has been increasing interest in middle-level pruning granularity [38, 47, 65], which provides the capability to achieve fine-grained structured sparsity combining the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity.\nWith the emergence of large-scale DNN models [3, 46, 63], there is a growing need to reassess the conventional pruning approach of pre-training, pruning, and fine-tuning, which demands excessive resources. Conventional pruning often requires multi-stage pipelines with repeated fine-tuning procedures or offline re-training. Without efficient learning mechanisms, it is not only a waste of computational power and time but also less applicable to real-time scenarios. One-cycle training and pruning, also known as one-cycle pruning, presents itself as a viable alternative solution to this problem. Obviously, it is important to explore whether a one-cycle pruning framework can rapidly learn new tasks or adapt to new environments in various real-world applications, such as autonomous vehicles, drones, robots, and so on [5, 15].\nTo address the limitations of conventional pruning, we introduce One-Cycle Structured Pruner (OCSPruner), which eliminates the requirement for pre-trained base network for pruning. Considering scenarios where networks need to learn from scratch within limited resource budgets, the efficiency gained from avoiding pre-training the base model becomes more compelling. Hence, we focus on efficient and stable structured pruning, ensuring model performance in a more sustainable way. Basically, structured pruning offers the advantage of creating slimmer yet structured networks that do not rely on specific hardware or software; however, it can occasionally lead to performance drops, particularly when dealing with high pruning ratios. Considering the insights from the Lottery Ticket Hypothesis [14], which suggests that sparse, trainable subnetworks can be identified within larger networks, the timing and method of pruning become critical. Our focus on one-cycle structured pruning involves pruning the network at initialization [7, 14, 30]. However, this type of method may suffer from potential sub-optimal network structures that result in performance degradation. Thus, rather than pruning at initialization, our proposed strategy involves early-phase pruning during training while still maintaining a one-cycle approach. Despite related methods like pruning-aware training (PaT) [52] and loss-aware automatic selection of pruning criteria (LAASP) [18], they overlook structural parameter grouping during saliency estimation, leading to reduced performance [13]. Specifically, PaT [52] focuses solely on total channel numbers, not considering individual channel identities during stable sub-network selection, while LAASP [18] lacks stability estimation criteria and involves complex iterative pruning. This paper addresses these limitations and introduces additional algorithmic enhancements for early-phase structured pruning during training from scratch, resulting in state-of-the-art performance."}, {"title": "3. Methods", "content": null}, {"title": "3.1. Problem Formulation", "content": "Let us assume a convolutional neural network (CNN) with L layers, each layer parameterized by $W\\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K \\times K}$, K being the kernel size and $C_{out}$ and $C_{in}$ being number of output and input channels, respectively. Given a dataset D consisting of N input-output pairs $\\{(x_i, y_i)\\}_{i=1}^N$, learning the network parameters with a given pruning ratio \u03b1 can be formulated as the following optimization problem:\n$\\arg \\min_{M} \\mathcal{L}(M, D)$, s.t. $\\frac{\\Psi(f(M, x_i))}{\\Psi(f(W, x_i))} > \\alpha$,\n(1)\nwhere $M \\subset W$ are the parameters after pruning, $\\mathcal{L}$ is the network loss, f() encodes the network function, and $\\Psi ()$ maps the network parameters to the pruning constraints, i.e., in our case, float point operations (FLOPs) of the network. The method can easily be scaled to other constraints, such as latency or memory."}, {"title": "3.2. Group Saliency Criteria", "content": "Conventional saliency estimation for pruning convolutional filters considers only the weight values within this filter to measure its importance. However, in this paper, we also take into account its structurally associated coupled parameters for estimating the saliency, which we call group saliency. The DepGraph [13] algorithm is used to analyze parameter dependencies within W, and entire trainable parameters are partitioned into several groups $\\mathcal{G} = {g}$. Simple norm-based criteria [32] is utilized for group saliency estimation. Each group $g = \\{w_1, w_2, ..., w_{|g|}\\}$ has an arbitrary number of sets of parameters with different dimensionality. Given our adoption of global pruning, we propose to incorporate local normalization into the calculation of group importance, thus facilitating the global ranking of filters for the pruning process. The overall saliency score of a group, as determined by the l2-norm used in this study, is defined as:\n$S(g) = \\frac{\\sum_{w \\in g} \\Vert w \\Vert_2/|w|}{\\vert g \\vert}$,\n(2)\nwhere $|.|$ denotes set cardinality.\nIn Eq. (2), we calculate the normalized l2-norm [1] of the set of elements within a group. The resulting local norms are summed up and again normalized with the cardinality of the group. Such normalization is induced because the group"}, {"title": "3.3. Pruning Stability", "content": "During the pruning sub-network search process, at the end of each training epoch, we temporarily prune the network for a given pruning constraint and ratio using group saliency criteria defined in Eq. (2). Suppose $M_{t-i}$ and $M_t$ denotes the pruned sub-network structures at training epoch t \u2013 i and t, respectively. Those sub-networks are used to check for pruning stability defined by their similarity. As soon as the sub-network stabilizes in subsequent training epochs, the pruning is made permanent, and the resulting pruned sub-network is further trained for convergence.\nPaT [52] uses a similar approach, but they check for pruning stability calculated using only the number of channels in each layer. The limitation of their approach is that the two sub-networks could be identical in terms of the number of channels in each layer. However, the identity of those channels derived from the original network could still differ, leading to unstable pruning.\nTo solve this issue, in this paper, we propose utilizing the total number of retained filters and the identity of derived filters from the original network to check for the pruning stability.\nSuppose $F_i$ and $F_t$ be the set of filters in temporarily pruned sub-networks at training epoch t \u2013 i and t, respectively. The similarity between these two sub-networks based on the cardinality of the filter set, as well as their corresponding identity, in each layer, is defined using Jaccard Similarity J:\n$J(M_{t-i}, M_t) = \\frac{1}{L} \\sum_{l=1}^L \\frac{|F_{t-i}^l \\cap F_t^l|}{|F_{t-i}^l \\cup F_t^l|}$.\n(3)\nThe similarity value J ranges from 0 to 1, where 1 means two sub-networks are the same and vice-versa. In practice, we use the average of past r similarities:\n$J_{avg} = \\frac{1}{r}\\sum_{k=0}^{1-r} J(M_{t-k-i}, M_{t-k})$.\n(4)\nThe stability score determines when sparsity learning begins ($t_{sl-start}$) and is also used to identify the stable pruning epoch ($t^*$). $t_{sl-start}$ is found when consecutive epochs show minimal change in average similarity ($J_{avg}$) within a window (r), using a threshold value (\u03c4). Therefore, the $t_{sl-start}$ is estimated using the following expression:\n$t_{sl-start} = \\min \\{t : (J_{avg}^t - J_{avg}^{t-i}) \\leq \\tau \\}$.\n(5)\nAdditionally, the stable pruning epoch ($t^*$) is estimated using the following expression:\n$t^* = \\underset{t}{argmax} (J_{avg} \\geq 1-\\epsilon)$.\n(6)\nHere, $t^*$ is determined as the epoch where the average similarity reaches a value greater than or equal to $(1 - \\epsilon)$, with $\\epsilon$ representing the threshold value."}, {"title": "3.4. Structured Sparsity Regularization", "content": "The structured sparsity regularization scheme in our approach is motivated by pruning methods DepGraph [13] and GReg [56]. DepGraph [13] pruning scheme gradually sparsifies the structurally coupled parameters at the group level based on the normalized group importance scores. During the training process, the regularization is applied to all the parameter groups while driving the groups with low importance scores toward zero for pruning. On the other hand, GReg [56] imposes a penalty term on those filters selected for pruning during the training process while gradually increasing the regularization penalty. However, the GReg [56] technique ignores the structurally coupled parameters while imposing growing regularization for sparsity learning.\nIn this study, we also employ growing regularization to enhance sparsity learning, addressing structurally linked parameters at the group level. Algorithm 1 outlines the step-by-step procedure of the proposed pruning algorithm. In each training epoch, given group saliency scores and pruning ratio, global pruning is used to temporarily partition the structurally grouped parameters into pruning and non-pruning groups (Line 3 ~ 5). The binary search algorithm is applied to quickly partition the groups for a given pruning ratio. In every training epoch, if a group's saliency score, which was previously partitioned as a pruning group, increases, it may move to the non-pruning group, and vice versa. Then, our algorithm applies regularization to the newly partitioned pruning groups, leaving the non-pruning groups unaffected (Line 18).\nIn early network training, weights are volatile, thus, large regularization is not advisable. Therefore, initially, a small penalty is used for sparsity learning. As training advances the penalty is also increased to speed up regularization. The growing penalty factor used in our approach for structured sparsity regularization is defined as:\n$\\lambda_t = \\lambda_{t-1} + \\delta \\times [\\frac{\\lfloor(t - t_{sl-start})}{\\Delta t}\\rfloor]$.\n(7)\nIn Eq. (7), t represents the training epoch, and \u03b4 denotes the growing factor. The term $t_{sl-start}$ corresponds to the starting epoch for structured sparsity regularization. From $t_{sl-start}$ onward, the penalty factor increases by \u03b4 in every \u0394t epoch interval, as determined by the floor function $\\lfloor . \\rfloor$ (Line 17).\nSuppose, at training epoch t, $\\mathcal{G}_{prune}$ and $\\mathcal{G}_{non-prune}$ denotes the partitioning of structural groups into pruning and non-pruning groups. First, similar to GReg [56], we impose a growing l2-norm penalty estimated using a pruning group of parameters to the network's original loss function defined as:\n$\\mathcal{L}_{total} = \\mathcal{L}_{orig} + \\lambda_t \\sum_{g \\in \\mathcal{G}_{prune}} \\sum_{w \\in g} ||w||_2^2$,\n(8)\nwhere $\\mathcal{L}_{orig}$ is the network original loss before imposing penalty term.\nThe structured sparsity regularization imposed by Eq. (8) gradually drives the pruning group parameters to zero. But, as our algorithm intends to finalize the pruning process as early as possible, we define an additional sparsity learning scheme to further drive the pruning group of parameters to zero. To achieve this, we propose to directly multiply pruning group parameters with a multiplication factor estimated using the network's current learning rate and growing penalty factor as follows:\n$\\mathcal{G}_{prune} = \\{g(1 - \\lambda_t \\times learning\\_rate_t) \\vert g \\in \\mathcal{G}_{prune} \\}$.\n(9)\nIn the early stage of sparsity learning, Eq. (9) slightly decreases the weight parameters, and this process becomes more aggressive as training progresses. This two-step structured sparsity learning ensures both smooth and fast regularization so that pruning becomes stable in the early stage of network training leaving sufficient time for fine-tuning the pruned model."}, {"title": "4. Results and Discussion", "content": null}, {"title": "4.1. Experimental Setup", "content": "To evaluate our pruning algorithm, we conducted experiments on the CIFAR-10/100 [27] and ImageNet [49] datasets. Initially, we pruned the relatively simple single-branch VGGNet [53] on the CIFAR-10 and CIFAR-100 datasets. Next, we evaluate the pruning of more complex multi-branch ResNet [21] models, on both CIFAR-10 and ImageNet datasets. Finally, we evaluated the performance of pruning a compact network, MobileNetV2 [50], on the ImageNet dataset.\nBoth CIFAR and ImageNet datasets are trained with a Stochastic Gradient Descend (SGD) optimizer with a momentum of 0.9 using data argumentation strategies adapted from official PyTorch [48] examples. For training on CIFAR-10/100 datasets, the models are trained for 300 epochs with batch size 128, in which the learning rate follows the MultiStepLR scheduler. Conversely, for training on the ImageNet dataset, the learning rate follows a cosine function-shaped decay strategy, gradually approaching zero. The training settings for MobileNetV2 on the ImageNet dataset are adapted from HBONet [31]. Please refer to the supplement for more comprehensive information on the training and pruning parameters employed in our experiments."}, {"title": "4.2. VGG16/19 on CIFAR-10/100", "content": "Tab. 1 compares the pruning of VGG16/19 on CIFAR-10/100 datasets using our method against several state-of-the-art approaches. The VGG16 pruning on CIFAR-10 is compared with RCP [34], LAASP [18], PGMFP [4], CPGCN [26], OTO [5, 6], DLRFC [24], and DCFF [39]. Among these pruning methods, like our method, OTO and DCFF also pruned the network from scratch. While OTO, CPGCN, and DLRFC achieve higher parameter reduction rates, their top-1 accuracy is lower than ours. Notably, OTO uses a specially designed training optimizer, whereas our method works with the standard SGD optimizer. With only 26.01% and 21.22% of the original network FLOPs remaining, we achieved top-1 accuracy of 93.88% and 93.76%, respectively\u2014the highest reported for similar pruning rates. Notably, while methods like DLRFC and DCFF show improvements in pruned model performance, this is largely due to their comparison against relatively lower baseline accuracy.\nTab. 1 also shows our method's VGG19 pruning performance in comparison with EigenDamage [54], GReg [56], and DepGraph [13] on the CIFAR-100 dataset. We achieved the highest top-1 accuracy of 70.47% while reducing the network parameters around 95% and FLOPs around 89%. Although GReg [56] also utilizes the concept of growing regularization and uses a fully trained network for pruning, its top-1 accuracy is nearly 3% lower than ours."}, {"title": "4.3. ResNet on CIFAR-10", "content": "The pruning of ResNet on CIFAR-10 using our method consistently outperforms state-of-the-art methods across depths 56 and 110 (Tab. 2). While our proposed algorithm automatically learns slimmer sub-networks from scratch, we not only compare its performance with other automatic methods like FPGM [23], and LAASP [18] but also assess its effectiveness against various other state-of-the-art techniques such as DLRFC [24], HRank [36], ResRep [12], DCFF [39], DepGraph [13], GReg [56], C-SGD [10], and Rethink [43]. Similar to ours, LAASP [18] employs a partially trained network for pruning. However, it ignores structurally coupled parameters while estimating filter saliency scores and adapts manually defined stable pruning epochs. In contrast to our technique, which gradually moves pruning parameters towards zero, FPGM [23] directly sets filters with low saliency scores to zero, while still allowing updates in the next training epoch. We prune ResNet56 and ResNet110 models for two different FLOPs reduction rates. Notably, with 38.88% reduced FLOPs, ResNet56 achieves 93.65% top-1 accuracy. Again, ResNet110 achieves a remarkable 94.13% top-1 accuracy with only 33.10% FLOPs."}, {"title": "4.4. ResNet50/MobileNetV2 on ImageNet", "content": "We evaluate the performance of ResNet50 and MobileNetV2 on the ImageNet dataset across different pruning ratios, comparing our results with those of various state-of-the-art methods from the literature (Tab. 3). Our method consistently surpasses the performance of several state-of-the-art approaches. Notably, similar to our approach, FPGM [23], PaT [52], NuSPM [29], and OTOv1 [5] also achieve a fully trained pruned network using one-cycle training from scratch. Remarkably, with a mere 43% of the original FLOPs remaining in the network, our algorithm achieves the highest accuracy (75.49% top-1 and 92.63% top-5) among state-of-the-art methods in the literature for similar pruning ratios.\nFig. 1 graphically illustrates the relationship between network FLOPs and top-1 accuracy, providing an approximation of the training speed-up associated with each pruning method in reference to the baseline network training. This clearly shows that our proposed technique consistently achieves higher top-1 accuracy compared to several other state-of-the-art methods while also being the most efficient pruning framework among them.\nTab. 3 also presents the pruning outcomes for MobileNetV2 on ImageNet and compares them with several state-of-the-art methods, including NetAda [59], CC [33], AMC [22], DCFF [39], LAASP [18], and DepGraph [13]. To ensure a fair comparison, we exclusively consider results for the 1.0 scale version of MobileNetV2. Experiments are conducted at 30% and 55% pruning ratios to align with commonly used benchmarks. In both instances, our proposed method attains superior top-1 accuracy compared to the other methods.\nThe last column in Tab. 3 presents the comparison results for training speed enhancement (higher the better) relative to the baseline ResNet50 model training cost. As shown, our proposed technique achieved up to a 1.41\u00d7 increase in training speed while pruning 67% of ResNet50 FLOPs. We approximated training speedup for state-of-the-art methods based on factors such as the total number of training rounds and whether pruning was done iteratively or in one shot. This comparative speedup analysis demonstrates that our method is the most efficient among the existing techniques."}, {"title": "4.5. Ablation Studies", "content": "Correlation between Sub-network Stability Score and final Accuracy. Fig. 4 illustrates the correlation between the stability score and the final accuracy of models pruned at various epochs until the training process reaches a stable pruning epoch. We measured the final accuracy of each temporarily pruned model obtained during the optimal sub-network search process guided by the pruning stability indicator. The Fig. 4 clearly shows that as the stability score approaches its highest value, the best-performing pruned sub-network is also selected. The Fig. 4 also shows that if we further delay the pruning process even after the stability score has peaked, the final accuracy of the pruned network begins to decline. This occurs because, once the pruning has stabilized, the structure of the pruned sub-network hardly changes, and there will be fewer and fewer training epochs left for fine-tuning the pruned network.\nInfluence of Varying Sparsity Learning Start Epoch ($t_{sl-start}$). The start epoch for sparsity learning significantly influences the proposed method's overall training cost and performance. The algorithm behavior concerning training speed and performance for different values of $t_{sl-start}$ is illustrated in Tab. 4. After a few initial training epochs, superior results were achieved by applying sparsity learning. Once the proposed stability indicator shows minimal variation across consecutive epochs, it indicates an optimal time to commence sparsity learning, as indicated by Eq. (5). The Tab. 4 shows that delaying this process only leads to performance degradation along with increased training time.\nFor additional results and ablation studies, we direct the readers to the supplementary material of this paper."}, {"title": "5. Conclusion", "content": "In this work, we introduce an efficient and stable pruning framework named OCSPruner. Unlike traditional methods that rely on fully trained networks for pruning, our approach integrates pruning early in the training process, achieving competitive performance within a single training cycle. OCSPruner automatically identifies stable pruning architecture in the early training stage guided by the pruning stability indicator. An integral aspect of any pruning procedure lies in determining the optimal sub-network based on parameter saliency criteria. Consequently, our future research will focus on exploring better saliency criteria suitable for our pruning framework, surpassing the current reliance on simple norm-based criteria. Furthermore, we aim to evaluate and adapt our pruning methodology for diverse tasks such as object detection, segmentation, diffusion models, and large language models.\nLimitation. While our method effectively employs regularization to encourage the reduction of unimportant network structures, additional attention is required for fine-tuning hyper-parameters tailored to each model architecture and dataset. Moreover, the proposed algorithm is particularly well-suited for learning schedules characterized by gradual variations, such as cosine or linear schedules, enabling a smoother detection of pruning stability by avoiding abrupt changes in learning rates."}]}