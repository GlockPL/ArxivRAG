{"title": "An LLM-based Agent for Reliable Docker Environment Configuration", "authors": ["Ruida Hu", "Xinchen Wang", "Chao Peng", "Cuiyun Gao"], "abstract": "Environment configuration is a critical yet time-consuming step in software development, especially when dealing with unfamiliar code repositories. While Large Language Models (LLMs) demonstrate the potential to accomplish software engineering tasks, existing methods for environment configuration often rely on manual efforts or fragile scripts, leading to inefficiencies and unreliable outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully automate environment configuration and generate executable Dockerfiles for arbitrary Python repositories. We address two major challenges: (1) enabling the LLM agent to configure environments within isolated Docker containers, and (2) ensuring the successful configuration process is recorded and accurately transferred to a Dockerfile without error. To achieve this, we propose atomic configuration synthesis, featuring a dual-environment architecture (internal and external environment) with a rollback mechanism to prevent environment \"pollution\" from failed commands, guaranteeing atomic execution (execute fully or not at all) and a Dockerfile generator to transfer successful configuration steps into runnable Dockerfiles. We evaluate Repo2Run on our proposed benchmark of 420 recent Python repositories with unit tests, where it achieves an 86.0% success rate, outperforming the best baseline by 63.9%.", "sections": [{"title": "1 Introduction", "content": "Configuring the environment is typically the first step for developers to run and test the code. Meanwhile, Large Language Models (LLMs) are increasingly being integrated into tools such as chatbots and coding assistants, showcasing their potential to accomplish various software engineering tasks [32, 12, 4]. As a result, the research community explores how LLMs can be leveraged to assist with more complex real-world tasks in software development. Furthermore, LLM-based agents [13, 5, 1] are also increasingly being utilized in software engineering. With the advancement of LLM-based agents and their applications in automated programming, many techniques now require running tests in the runtime environment to acquire more comprehensive information. However, previous approaches often rely on manually configuring environments [14], and writing individual configuration scripts for each task, which is both time-consuming and labor-intensive."}, {"title": "2 Environment configuration", "content": "In this part, we formulate the task of the environment configuration. It involves identifying an appropriate base image (B) and a configuration process (\u0393). The objective is to ensure that the final system state (Sf) successfully runs all the unit tests in the repository."}, {"title": "2.1 State Transition", "content": "Environment State (S): The environmental state (S) represents the current state of the computer system, which encompasses all variables, files, cache, etc. Any state S \u2208 S, where (S) denotes the set of all possible environment states."}, {"title": "Command (C)", "content": "The command C represents an individual instruction or action that can be executed in the environment through interfaces such as bash, thus directly changing the system state."}, {"title": "State Transition Function (\u03b4)", "content": "The state transition function (\u03b4) defines the process through which the system transitions from one state to another state upon execution of a given command, i.e.,\n\u03b4:S\u00d7C\u2192S,\n8(S, C) = S'\nHere, S' is the new state after execution of command C."}, {"title": "2.2 Base image", "content": "Empty initial state (Sp): The empty initial state (S) represents a completely unconfigured, bare operating system or a purely hypothetical state without applications and configurations. This state is used for theoretical purposes to define the starting point of a system's configuration. S\u00f8 \u2208 S.\nBase image (B): From the empty initial state (Sp), we can construct the base image B by applying a series of actions. The base image B is essentially the final state achieved by applying a sequence of instructions starting from Sp:\nB = 8m (S\u00f8, CB,CB,...,CR)\nHere, CB, CB,..., Ch are the individual commands that constitute the base image."}, {"title": "2.3 Configuration process and target state", "content": "Configuration Process (\u0393): The configuration process, denoted as \u0393, involves transforming the environment starting from the base image B by applying a series of actions. The result of this process is a state Sf, which we refer to as the resultant state. Formally, the configuration process can be represented as:\nSf = \u03b4\u03b7 (B, C1, C2,..., Cn)\nFollowing this transformation, it is crucial to verify whether Sf meets all the required conditions to be considered the target state. This verification is performed using the state verification function.\nState verification (\u20ac): The state verification function (\u20ac) determines whether the resultant state (Sf) successfully runs all tests within the repository after executing the configuration process (\u0413). It maps a given state (S) to a binary value indicating whether all tests could be run in the resultant configuration: \u20ac(S) returns 0 if all tests are successfully run, and 1 otherwise. If \u20ac(Sf) = 0, it indicates that the configuration process has finished and the resultant state Sf is considered the target state. Otherwise, it means that Sf does not meet the necessary conditions."}, {"title": "3 Repo2Run", "content": "In this part, we introduce the design of Repo2Run and atomic configuration synthesis. As shown in Repo2Run Workflow in Figure 3, the dual-environment architecture consists of an external environment and an internal environment. The internal environment is a Docker container where the actual environment configuration is executed, while the external environment assists the internal environment in setting up the configuration. After successfully configuring the environment, the Dockerfile generator will create the Dockerfile."}, {"title": "3.1 External environment", "content": "\u2022 Action-observation interaction: In each turn of the environment-agent interaction, the event history Et = (a1, 01), (a2, 02), ..., (at\u22121, Ot\u22121) records the historical actions and observations up to time step t. Additionally, the sequence of successfully executed commands C1, C2, ..., Cn helps remind the LLM agent of the current system state.\nAt time step t, given the event history Et and the sequence of successfully executed commands C1, C2,..., Cn, the LLM agent generates a response including an action at:\nat = \u03c0(\u0395t, C1, C2,..., Cn)\nwhere is the function of the LLM agent's decision. The environment state then transitions based on the action at:\nSt+1 = d(St, at)"}, {"title": "\u2022 Rollback (p)", "content": "As illustrated in Figure 1, commands Cfail may lead to an uncertain state (Su) if not successfully executed. When a command executes failed (i.e., the command returns a non-zero return code), the system transitions into an uncertain state as defined by:\nSu = (Sk-1, Cfail)\nHere, Cfail is the failed executed command. If a command fails, the system will transition to the uncertain state Su. To avoid such uncertainty, we introduce rollback (p). In the event of a failure at any step, the rollback mechanism restores the environment to the last certain state:\np:S\u00d7C\u2192S, p(Su, C1, C2, ..., Ck) = Sk\u22121\nSpecifically, each time an instruction (C) is executed, we utilize the \"docker commit\" command to take a snapshot of the current state. Once the instruction is completed, we check its return code. If the return code is not 0, it indicates a failure in executing the instruction, and we replace the image with the most recently committed one. Besides, some instructions, such as \"cat\", generally do not"}, {"title": "3.2 Internal environment", "content": "As shown in the Repo2Run workflow of Figure 3, the internal environment consists of a Docker container with a base image B and the code repository to be configured. According to the latest data\u00b3 from 2025, we select Python 3.10 as our default Docker base image due to its increasing usage and wide application among all Python 3 versions. Additionally, we categorize the executable actions within the internal environment into five types: environment monitoring, dependency installation, bash commands, code editing, and test running.\n\u2022 Environment monitoring: It serves as the eyes of the LLM agent in the internal environment, allowing it to observe the current state of the environment. These commands usually do not change the state of the environment. For instance, commands like \u201cls\" and \"cat\" are employed to inspect directories and files. Additionally, commands like \u201cpip list\u201d and \u201cpipdeptree\"4 are utilized to get the installed version of third-party libraries in the current environment. For example, in the fourth part of Event Stream in Figure 3, the LLM agent calls the \"find\" command to search for requirements.txt within the environment to determine which dependencies need to be installed.\n\u2022 Dependency installation: It is primarily used to install the third-party libraries necessary for running tests, including libraries managed by both \"pip\" and \u201capt-get\u201d. To avoid dependency conflicts that may arise when downloading multiple packages, we implement a dependency management system in the external environment. Initially, the third-party libraries to be downloaded are added to a waiting list. If there are different constraints for the same library (e.g., \u201cA>=1.0\u201d and \u201cA<1.0\u201d), they will be moved to a conflict list, and the LLM agent will determine the appropriate version to download.\nWe design a set of tools for the LLM agent to manage the installations. For instance, as shown in Figure 3, the action \u201cwaitinglist addfile\u201d adds all elements from requirements.txt to the waiting list, while executing the \"download\" command, third-party libraries are taken one by one from the waiting list and installed in the internal environment. The installation commands (e.g., \"pip install\" and \"apt-get install\") are executed. If the installation succeeds, the state of the environment is updated; if it fails, a rollback is performed. We show all our designed tools in Appendix C.\n\u2022 Test running: It detects whether the current environment is successfully configured by running \"pytest\" unit tests. If the state (Sf) of the current environment passes the tests, it indicates that the LLM agent has successfully configured the environment within the Docker container, and the internal environment's process concludes. Conversely, if some tests fail, the error information is communicated back to the LLM agent, allowing it to perform further configurations and adjustments based on the errors.\n\u2022 Code editing: It allows the LLM agent to modify the code within the internal environment, including codes inside and outside the repository. Besides, to prevent the LLM agent from spuriously running the tests by directly modifying or deleting the test files, we do not permit the LLM agent to modify or delete the original test files within the repository. For instance, Figure 3 demonstrates code editing to correct a syntax error resulting from the improper use of double quotes in a Python f-string.\n\u2022 Bash commands: Aside from the commands above, we also enable the LLM agent to directly invoke bash commands to interact with the internal environment. As shown in Figure 3, the LLM agent uses the command \u201cexport PYTHONPATH=/repo/src\"."}, {"title": "4 Experiment", "content": "We evaluate the effectiveness of Repo2Run on 420 Python code repositories. As the popular option, we select gpt-4o-2024-05-13 for subsequent experiments, with the temperature uniformly set to 0.2."}, {"title": "4.1 Benchmark", "content": "To the best of our knowledge, there is no prior work similar to Repo2Run that constructs an environment for arbitrary Python repositories. To validate the capability of Repo2Run in environment configuration, we create our new benchmark consisting of selected Python repositories from GitHub based on the following criteria:\n\u2022 Creation date: To minimize the impact of data contamination, we carefully selected repositories created in 2024, ensuring they are not part of the training data for mainstream large language models.\n\u2022 Star count: To ensure the quality of the repositories, we only select those with a star count greater than 100.\n\u2022 Test directory: Pytest is a leading Python testing framework that is compatible with many other Python testing tools, such as unittest. It identifies and runs test files and test functions that are named with a \u201ctest\u201d prefix or a \u201ctest\u201d suffix. Python repositories usually place their test files in the \u201ctest\u201d or \u201ctests\u201d directory. To effectively filter repositories with unit tests, we keep repositories with the \u201ctest\u201d or \u201ctests\u201d directory in their root directory to filter the repositories that most probably have unit tests.\nThrough the selection process, we crawl all 449 repositories that meet all the above requirements5. Then, we filter 420 repositories containing at least one unit test. These repositories constitute our benchmark for subsequent experiments. The details of the benchmark are presented in Appendix D."}, {"title": "4.2 Evaluation metrics", "content": "\u2022 Dockerfile Generation Success Rate (DGSR): It indicates the percentage of attempts where the method successfully generates a runnable Dockerfile. To be considered successful, the generated Dockerfile must be able to build without errors. If the Dockerfile for a code repository successfully builds, it is regarded as a successful generation. Generating runnable Dockerfile is fundamental for successfully configuring the environment.\n\u2022 Environment Configuration Success Rate (ECSR): It represents the percentage of attempts where the method successfully configures environments. For a successful configuration, the generated Dockerfile must not only build successfully but also allow tests to run by \u201cpytest\u201d in the Docker container. We are only concerned with whether tests can be executed, regardless of whether they pass or fail, as outcomes of tests may inherently vary within the repository."}, {"title": "4.3 Baselines", "content": "\u2022 pipreqs: It is an automated tool that generates a \u201crequirements.txt\u201d file by analyzing the import statements in the Python scripts and identifying the necessary dependencies without LLM. Using the requirements.txt file generated by pipreqs, we create a Dockerfile. The detail is provided in Appendix E.\n\u2022 LLM generator: The \u201cREADME\" file in a code repository usually contains environment configuration instructions. Therefore, we directly drive the LLM to read the \u201cREADME\" file and generate an executable Dockerfile accordingly.\n\u2022 SWE-agent [26]: SWE-agent establishes a custom agent-computer interface (ACI) that facilitates the LLM agent's interaction with the repository environment by allowing actions such as reading"}, {"title": "4.4 Experimental Results", "content": "The results of different baselines are presented in Table 1. We observe that Repo2Run consistently outperforms other baselines on both DGSR and ECSR. Repo2Run ultimately completed environment configuration for 361 code repositories, achieving an ECSR of 86.0%. It is 63.9% higher than the highest rate achieved by other methods, demonstrating great advantages. Due to the design of atomic configuration synthesis, Repo2Run successfully generates Dockerfiles that can be built successfully for all 420 code repositories, which other tools cannot guarantee.\nFor pipreqs, the main failures come from two reasons. First, generating the requirements.txt fails when there are issues within the repository, such as encoding errors or syntax errors in the files. This happens in 30 repositories (7.1%). Second, even when requirements.txt is generated, it might not download properly due to package version conflicts. This occurs in 265 repositories (63.1%). Besides, both the LLM generator and SWE-agent fail to ensure that the generated Dockerfile can be successfully built due to the lack of an ensuring mechanism. Surprisingly, the ability of SWE-agent, a general agent framework, to generate Dockerfiles is even weaker than simply letting the LLM read the \"README\" file. This indicates that a general agent framework cannot guarantee the generation of runnable Dockerfiles. Ensuring mechanisms like atomic configuration synthesis are necessary to effectively use the interactive information from the agent to generate runnable Dockerfiles."}, {"title": "4.5 Ablabtion of Repo2Run", "content": "To investigate the impacts of the dual-environment architecture and Dockerfile generator separately, as two parts of atomic configuration synthesis, we separately remove each component of them. For the experiment without the dual-environment architecture, we retain only the internal environment's bash commands as the most basic interface and remove all other tools. For the experiment without the Dockerfile generator, we directly instruct the LLM to generate a runnable Dockerfile.\nExperimental result of the ablation study is shown in Table 2. We observe that removing the dual-environment architecture and retaining only bash commands results in a 7.6% decrease in DGSR. The main reason for this drop is the removal of rollback and other designs, making the system more prone to entering uncertain states and subsequently failing to reproduce. In addition, ECSR shows a 44.3% decrease, primarily because the simplification of design makes it more difficult for the LLM agent to complete configuration in the internal environment. Besides, removing the Dockerfile generator directly leads to an 80.5% drop in DGSR. This indicates that having the LLM directly generate Dockerfiles is unlikely to fully follow the event history, resulting in Dockerfiles that fail to build successfully. This also directly causes a sharp decline in ECSR.\nIt is also observed that Repo2Run without the Dockerfile generator is outperformed by the LLM generator. This is because the LLM generator leverages the \u201cREADME\u201d file, which provides a clear, simple and high-level overview of the environment configuration, allowing for more accurate Docker-file generation. In contrast, the event history-based approach lacks this context, making it harder for the LLM to fully understand the configuration goals. However, the Dockerfile generator effectively utilizes the detailed event history, highlighting the complementary roles of both components of Repo2Run in generating a reliable Dockerfile."}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Tool usage frequency", "content": "As shown in Figure 5, we analyze the invocation times of various action types in 361 successfully configured projects, including the five actions within the internal environment and the action of changing the base image. Bash commands are the most frequently used action, as they encompass the majority of instructions. Additionally, we observe that the LLM agent tends to call dependency installation quite frequently, averaging about 18 times per configuration, which means roughly 18 dependencies are installed per configuration on average. Moreover, the LLM agent calls test running approximately 3.5 times per configuration on average, which typically helps the agent better identify issues. Among successful configurations, we see 48 instances of changing the base image, accounting for 13.3% of success cases, indicating that the initial selection of the base image is often incorrect and requires subsequent adjustments."}, {"title": "5.2 Time consumption", "content": "Figure 6 shows the distribution of time spent successfully configuring 361 code repositories. The average time for successful configuration using Repo2Run is 29.03 minutes. 111 (30.7%) of the repositories are successfully configured in less than 10 minutes. Additionally, our empirical study through sampling indicates an average manual configuration time of 21.33 minutes (Appendix A). Considering network differences and randomness, Repo2Run achieves a time consumption comparable to manual configuration. Additionally, for complex issues, Repo2Run shows greater advantages over manual configuration. Repo2Run successfully configures all the cases that were manually successful in our empirical study. Moreover, the cases where Repo2Run fails are also not successfully configured manually."}, {"title": "5.3 Case study", "content": "For the code repositories that fail to configure, we manually inspect the reasons for failures and find that most are due to issues within the repositories themselves. Figure 7 illustrates a \u201cmodule not found\" error when configuring the repository \u201cjialuechen/deepfolio\u201d. In this case, the issue arises from the absence of updating the unit tests. The test file \u201ctest_stats.py\" attempts to import modules from \u201cdeepfolio.stats\", but \u201cstats\u201d does not exist in \u201cdeepfolio\u201d. Consequently, no matter how the LLM agent operates, it cannot directly run this test. This highlights the importance for developers to continuously update existing unit tests as the code repository evolves.\""}, {"title": "6 Related Work", "content": ""}, {"title": "6.1 Environment configuration and Dockerfile generation", "content": "Oss-Fuzz-Gen [17] relies on predefined build instructions (such as \"./bootstrap.sh\", \"./configure\u201d, \u201cmake\u201d) to build projects for fuzzing but lacks of flexibility for diverse projects when specified files are absent.\nExisting solutions that help developers write Dockerfiles broadly fall into three categories: (1) Template-based generators that create Dockerfiles based on project context [2, 18], (2) Task-specific tools such as DockerizeMe which supports environment dependency inference for Python projects [11] and DockerGen [27] for dependency recommendations based on knowledge graphs built from existing Dockerfiles, and (3) Code completion tools including GitHub Copilot [7] and HumpBack [8] that generate suggestions for developers while writing Dockerfiles. The use of deep learning models to generate Dockerfiles based on natural language specifications of software requirements is also investigated [21]. While these approaches provide valuable assistance, they either require significant manual input from developers or are limited to specific use cases."}, {"title": "6.2 LLM-based agent", "content": "LLM-based agents typically consist of four key components: planning, memory, perception, and action [25]. Planning is crucial for agent systems, as it schedules agents to ensure a smooth process. LLM-based agents employ various planning strategies, including single [3] or multiple planners [15], single [29] or multi-turn planning [33], and single [9] or multi-path planning [31]. The memory component in LLM-based agents stores historical data to support coherent reasoning and complex tasks. Implementations vary in terms of memory duration (short-term [6] or long-term memory [28]), memory ownership (specific [20] or shared memory [10]). For perception, LLM-based agents primarily utilize textual input [19, 28] (natural language and programming language) and visual input [24, 22] (images and diagrams) to perceive and process information. To extend capabilities beyond interactive dialogue, the action component employs various external tools [30, 16], such as searching tools, file operations, and GUI operations. Nowadays, LLM-based agents have demonstrated superior performance compared to standalone LLMs in various software engineering tasks [13, 5, 1]. However, no LLM-based agents are specifically designed for environment configuration currently. To fill this gap, this paper employs a novel approach for automated coding environment configuration and Dockerfile generation."}, {"title": "7 Conclusion", "content": "In this paper, we propose Repo2Run, the first LLM-based agent for automated coding environment configuration and Dockerfile generation for Python repositories based on atomic configuration synthesis. With a dual-environment architecture and a Dockerfile generator, Repo2Run is able to select and change the base image, manage and install dependencies based on action observation and rollback mechanism, and utilize bash commands and existing test suites. Our evaluation of 420 popular Python repositories hosted on GitHub demonstrates the effectiveness of Repo2Run with an 86.0% success rate."}, {"title": "Impact Statement", "content": "Repo2Run demonstrates the potential and impact that LLM-based agents can have towards automating coding environment configuration and docker file generation. We are hopeful that Repo2Run may inspire more works that advance broader aspects of automated software engineering. There might be more potential societal consequences of our work, none of which which we feel must be specifically highlighted here."}, {"title": "A Manual experiment", "content": ""}, {"title": "A.1 Settings", "content": "To ensure a fair comparison among participants, we conducted training and demonstrated examples before the experiment to ensure everyone understood the procedure. Additionally, to minimize discrepancies in time consumption due to network factors, all participants conducted the experiment in the same network environment."}, {"title": "A.2 Survey", "content": "We selected eight technical staff from internet companies to participate in the experiment and conducted a survey regarding their backgrounds prior to the experiment. Their development experience ranges from 4 to 11 years, with an average of 7 years in software development and 3.8 years in Python development. Seven participants have experience in complex development projects, while one has experience in multiple small-scale projects.\nRegarding environment configuration, three participants indicated that they spend a significant amount of time configuring the environment when faced with an unfamiliar code repository; another three stated that, although they spend a long time, it is generally manageable; two participants reported spending minimal time.\nIn terms of successful environment configuration in their regular work, three participants mentioned they only fail with extremely complex environments, while five indicated they can build environments for most medium-scale repositories. As for their confidence in successfully configuring unfamiliar environments, six participants expressed that they are usually successful, and two said they are sometimes successful.\nWhen it comes to the amount of time they are willing to wait to configure an unfamiliar repository, four participants are willing to wait for over 90 minutes, two are willing to wait 40-60 minutes, one is willing to wait 20-40 minutes, and one is only willing to wait 10-20 minutes.\nParticipants' overall evaluation of configuring code running environments is diverse: one finds it very troublesome with many issues, four find it somewhat troublesome, two indicate moderate difficulty, and one finds it relatively simple.\nAdditionally, all participants expressed a high or very high willingness to use a tool that could automatically configure the environment for an unfamiliar repository."}, {"title": "A.3 Experiment guideline", "content": "We request all participants to conduct the experiment following the guidelines below:\n1. Environment Setup\nConfigure the Docker environment. Verify the installation is successful: If installed correctly, you should be able to use the following command (python:3.10 is just an example; select the base image according to your requirements):\ndocker run -it python:3.10 bash\n2. Overall Procedure\nOur objective is to install the given package in a Docker container, configure its environment, and be able to run its internal tests. During the process, record the time developers spend configuring the environment and eventually save the logs using docker logs.\n2.1 Review the Repository to be Configured (Optional)\nReview the GitHub repository that needs to be configured.\n2.2 Determine the Docker Base Image (Generally start with python:3.10)"}, {"title": "2.3 Create and Enter the Container", "content": "Using the determined base image name (e.g., python:3.10), enter the container with the following command:\ndocker run -it --name mytest python:3.10 bash\nHere, mytest is the container name, recorded for log export later. It can be freely named, just keep track of it to avoid losing it later. Note: If any issues arise here, check if Docker is correctly installed and if the image name is valid, and troubleshoot accordingly."}, {"title": "2.4 Install Relevant Tools", "content": "APT tool downloads:\napt-get update && apt-get install -y curl\nDownload pytest:\npip install pytest"}, {"title": "2.5 Download the Repository", "content": "Select the repository to be configured and download its GitHub repository to a location (generally directly in the root directory):\ngit clone https://github.com/{full_name}.git"}, {"title": "2.6 Configure the Environment", "content": "Now, use your skills to configure: First, enter the downloaded file directory, for example:\ncd wddbfs\nSwitch to the specified branch SHA (refer to the corresponding SHA of the repository), for example:\ngit checkout 5c68aa\nOur goal is to successfully run pytest (not necessarily to pass all tests, just to run them). A simple criterion is to successfully run:\nppytest --collect-only -q\nAt this point, you can use your experience and information from the repository documentation and debugging error messages to configure. However, there are a few restrictions:\nDo not directly edit the test files! (Files starting with test or ending with_test.py). Do not directly delete test files! Editing the original repository files is not recommended.\nDuring this process, you may perform various operations, including but not limited to pip, apt-get, and other tool downloads, as well as searching online or using GPT for debugging help.\nAdditionally, if there are long download times requiring waiting, you may decide according to your situation whether to leave this running and do other things (just don't forget about this task)."}, {"title": "2.7 Completion and Logging", "content": "A task can conclude in two scenarios:\nScenario One: If \"pytest --collect-only -q\" runs without issues, you can then execute pytest. If pytest completes successfully, the task is done. Scenario Two: If you feel the package is extremely"}, {"title": "2.8 Fill Out the Form and Record Information", "content": "You need to fill out the form according to your feelings."}, {"title": "A.4 Repository assignment", "content": "We randomly assigned each participant four unique code repositories that were successfully configured by Repo2Run. Additionally, each participant was assigned two code repositories that Repo2Run failed to configure. To avoid chance occurrences, each failed repository was assigned to two different participants. Below is the list of selections:\nSuccessfully configured:\n[alexwlchan/safari-webarchiver, ManiMozaffar/aioclock, mixedbread-ai/batched, mobiusml/gemlite, circlemind-ai/fast-graphrag, knowsuchagency/promptic, mbodiai/embodied-agents, mod-"}, {"title": "Epipreqs baseline settings", "content": "Figure 8 shows the template of a Dockerfile generated using \"requirements_pipreqs.txt\" created by pipreqs."}, {"title": "B Safe commands", "content": "If Repo2Run executes the following commands without using \u201c>\u201d or \u201c>>\" for output redirection, they are regarded to be safe commands that typically do not affect the system. Therefore, rollback is not necessary, and they are not added to the generated Dockerfile.\n[\"cd\", \"ls\", \"cat\", \"echo\", \"pwd\", \"whoami\", \"who\", \u201cdate\u201d, \u201ccal\u201d, \u201cdf\u201d, \u201cdu\u201d, \u201cfree\u201d, \u201cuname\u201d, \u201cuptime\u201d, \u201cw\u201d, \u201cps\u201d, \u201cpgrep\u201d, \u201ctop\u201d, \u201cdmesg\u201d, \u201ctail\u201d, \u201chead\u201d, \u201cgrep\u201d, \u201cfind\u201d, \u201clocate\u201d, \u201cwhich\u201d, \u201cfile\u201d, \u201cstat\u201d, \u201ccmp\u201d, \u201cdiff\u201d, \u201cxz\u201d, \u201cunxz\u201d, \u201csort\u201d, \u201cwc\u201d, \u201ctr\u201d, \u201ccut\u201d, \u201cpaste\u201d, \"tee\", \"awk\", \"env\", \"printenv\u201d, \u201chostname\u201d, \u201cping\u201d, \u201ctraceroute\u201d, \u201cssh\"]"}, {"title": "C Repo2Run tools", "content": "Showing in Table 6, we design the following actions for Repo2Run to facilitate its invocation."}]}