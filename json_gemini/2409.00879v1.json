{"title": "Beyond Parameter Count: Implicit Bias in Soft Mixture of Experts", "authors": ["Youngseog Chung*", "Dhruv Malik*", "Jeff Schneider", "Yuanzhi Li", "Aarti Singh"], "abstract": "The traditional viewpoint on Sparse Mixture of Experts (MoE) models is that instead of training a single large expert, which is computationally expensive, we can train many small experts. The hope is that if the total parameter count of the small experts equals that of the singular large expert, then we retain the representation power of the large expert while gaining computational tractability and promoting expert specialization. The recently introduced Soft MoE replaces the Sparse MoE's discrete routing mechanism with a differentiable gating function that smoothly mixes tokens. While this smooth gating function successfully mitigates the various training instabilities associated with Sparse MoE, it is unclear whether it induces implicit biases that affect Soft MoE's representation power or potential for expert specialization. We prove that Soft MoE with a single arbitrarily powerful expert cannot represent simple convex functions. This justifies that Soft MoE's success cannot be explained by the traditional viewpoint of many small experts collectively mimicking the representation power of a single large expert, and that multiple experts are actually necessary to achieve good representation power (even for a fixed total parameter count). Continuing along this line of investigation, we introduce a notion of expert specialization for Soft MoE, and while varying the number of experts yet fixing the total parameter count, we consider the following (computationally intractable) task. Given any input, how can we discover the expert subset that is specialized to predict this input's label? We empirically show that when there are many small experts, the architecture is implicitly biased in a fashion that allows us to efficiently approximate the specialized expert subset. Our method can be easily implemented to potentially reduce computation during inference.", "sections": [{"title": "Introduction", "content": "It has been well established that scaling the size (i.e., parameter count) of models is necessary for state of the art prediction power [1, 2], but naively scaling the model size is infeasible due to hardware constraints and computational costs. Mixture of Experts (MoE) layers in a model allow one to achieve this goal, while mitigating the increased computational costs for training and inference that accompany a naively larger model. These have been successfully deployed in practice, in a variety of contexts such as language [3] and vision [4], and MoE layers are considered critical to achieve state of the art performance in today's models [5].\nThe archetypical MoE layer is the Sparse MoE [6]. Here, each token is only given to a subset of experts, and a router is trained to discretely match a token to its expert subset. Since a single (large) expert can represent complex functions, the traditional viewpoint is that one should partition the large expert into multiple (small) experts, so that the total parameter count of all experts is unchanged. The hope is that the model's representation power is similar since the total parameter count is the same and since experts can specialize to the tokens they see (rather than all tokens) [7]. Yet, training and inference are faster, because any single token activates only a subset of experts rather than all of them.\nWhile the Sparse MoE allows scaling of model size, its discrete routing causes optimization issues and load balancing difficulties during training. To tackle these issues, many variants of Sparse MoE have been introduced, such as routing a token to only a single expert [3], incorporating linear programs to ensure load balancing [8] or having the experts select tokens (instead of tokens selecting experts) [9]. However, all these approaches remain discrete in nature, and thus suffer from at least some degree of training instability.\nTo alleviate these issues, the recently introduced Soft MoE [10] eschews discrete matching in favor of a smoother approach. It computes for each expert a convex combination of the input tokens, and the expert only sees this convex combination. The final output of the model is then a convex combination of each expert's output. This approach is fully differentiable, and hence is more stable than the Sparse MoE. This novel Soft MoE architecture has been shown to outperform all other baselines on challenging large scale vision tasks, and can scale to thousands of experts [10]. Moreover, recent results show that the Soft MoE is a promising avenue towards providing empirical scaling laws for deep reinforcement learning [11].\nThus, while the Sparse MoE constructs a discrete mapping between tokens and experts, the Soft MoE computes convex combinations of tokens that are fed to experts, and then computes convex combinations of the expert outputs, which together promote stabler and faster training.\nThe majority of prior work on MoE focuses on computational issues, such as efficient and stable training. In our paper, we adopt an orthogonal perspective. In particular, it remains unclear whether Soft MoE's specific manner of combining tokens and experts creates any unexpected implicit architectural biases. Indeed, it is not even clear that its soft gating mechanism preserves the traditional MoE dogma that many small experts have similar representation power to a single large expert with the same total parameter count. It is also unclear whether combining tokens and experts completely destroys the possibility (or discoverability) of expert specialization, which is what one traditionally desires from an MoE (especially in the regime of many experts) [12, 13]. Thus, we investigate for the existence of such biases, through the lens of varying the number of experts. In this paper, we make progress along this line of investigation by making the following contributions:\n\u2022 We prove that the Soft MoE with a single neural network expert, even with arbitrarily many parameters, cannot represent simple convex functions (while empirically we show that multiple experts can). Thus, in contrast to the traditional viewpoint, having multiple experts is actually necessary to have non-trivial representation power in Soft MoE.\n\u2022 We introduce a notion of specialization for Soft MoE. While discovering specialized experts generally seems intractable, we empirically demonstrate that as we increase the number of experts, even while fixing the total parameter count, the architecture is implicitly biased in a manner that allows us to efficiently approximate the specialized expert set (see Figure 1).\n\u2022 Our method for discovering specialized experts can be easily implemented for reducing computation at inference.\nThese contributions thus show there are benefits to using a large number of small experts relative to a small number of large experts, and notably, these benefits are often non-computational."}, {"title": "Problem Formulation", "content": "We begin by briefly discussing the Soft MoE architecture [10]. Throughout our paper, we assume there is a single slot per expert, since this is the most performant setting in practice. Let $X \\in \\mathbb{R}^{m\\times d}$ denote the tokenized input, so that there are m tokens each in Rd. The MoE layer is equipped with n experts $\\{f_j : \\mathbb{R}^d \\to \\mathbb{R}^d\\}_{j=1}^n$, each of which is typically implemented as a feedforward network. The router is parameterized by $\\Phi \\in \\mathbb{R}^{d\\times n}$. Given an input X, the parameters \u03a6 are used to compute matrices D(X), C(X) \u2208 Rm\u00d7n which are defined elementwise as\n$D(X)_{ij} = \\frac{\\exp ((X \\Phi)_{ij})}{\\sum_{i'=1}^m \\exp ((X \\Phi)_{i'j})}$ and $C(X)_{ij} = \\frac{\\exp ((X \\Phi)_{ij})}{\\sum_{j'=1}^n \\exp ((X \\Phi)_{ij'})}$ (1)\nNote that each column of D(X) and each row of C'(X) sums to one. With this notation in hand, we formally define the Soft MoE layer below.\nDefinition 1 The Soft MoE is a function sMoE{f; }=1 : Rmxd \u2192 Rmxd defined as\n{fi}=1\nsMoE{f;}(X) = C(X)\u1ef8(X) where \u1ef8(X) =\n{fi}=1\n$\n\n\nf_1 ((D(X)^T X)_1)\nf_n ((D(X)^T X)_n)\n:\n$\n\nThe Soft MoE thus computes n different convex combinations of the tokens in X, where the weights of the jth convex combination are given by the jth column of D(X). It then applies expert fj to the jth convex combination, for each j = 1,2...n. Finally, it computes m different convex combinations of these expert outputs, where the weights of the ith convex combination are given by the ith row of C'(X). Note that each expert processes a single vector in Rd, and that sMoE is differentiable whenever the experts are. This results in more stable training relative to Sparse MoE, where each expert is given a subset of the m tokens via a discrete matching algorithm. The Soft MoE has shown significant empirical success in vision [10] and reinforcement learning [11]."}, {"title": "Our Investigation", "content": "At a high level, the Sparse MoE is designed with the following principle. Say we desire a model with b total parameters, because we believe that b allows for sufficiently large representation power. Instead of using a single large network (n = 1) with b parameters, we use n > 1 smaller experts each with b/n parameters. The hope is that we have similar representation power to the n = 1 case since the total parameter count is the same, but we have faster computation because each token only activates a small subset of experts [6, 3]. Moreover, one also hopes that each expert can specialize to the specific type of tokens it sees [7, 12, 13].\nThe Soft MoE is motivated in a similar fashion. It also splits a single large model with b parameters into n \u2265 1 smaller experts each with b/n parameters, hoping that representation power is unchanged. However, Soft MoE differs significantly from Sparse MoE in how it uses these experts. While Sparse MoE discretely assigns tokens to experts, Soft MoE computes convex combinations of tokens and expert outputs. Due to this significant difference, it is unclear how the original motivations for Sparse MoE apply to Soft MoE.\nAs one example of how Soft MoE might deviate from the original motivations for Sparse MoE, consider the extreme case when n = 1. Here, Sparse MoE's router trivially routes all tokens to the expert, and so classical results show that Sparse MoE can represent arbitrary continuous functions [14, 15]. But in Soft MoE, the gating function is non-trivial even in n = 1, and so it is possible that Soft MoE has poor representation power even when the expert is very powerful. If this were true, then it would challenge the conventional wisdom that Soft MoE's empirical success with n > 1 smaller experts (each with b/n parameters) is simply because they mimic (albeit computationally efficiently) the representation power of a single large expert (with b parameters). Instead, it would suggest that Soft MoE has some implicit bias that enables its practical success. To this end, we ask the following question."}, {"title": "Representation Failure of a Single Expert", "content": "In this section, we answer Q1 from Section 2.2. We first recall the definition of Lipschitz functions.\nDefinition 2 A function h : Rk1 \u2192 Rk2 is L-Lipschitz if $||h(x) \u2013 h(y)||_2 \u2264 L||x \u2212 y||_2$ for all x, y \u2208 Rk1.\nRecall that any neural network is Lipschitz [16]. Our main result shows that Soft MoE with a single Lipschitz expert f is incapable of representing simple target functions. This result holds even when this expert f is arbitrarily powerful (and possibly non-parametric). It also holds when the output of the Soft MoE layer is passed to an arbitrarily powerful Lipschitz function g (as would be the case in a practical implementation, since the MoE would be a layer that prepends a powerful neural network function, rather than a standalone layer). Below we formally state our result.\nTheorem 1 Fix any m \u2265 2, d > 1 and n = 1. Define the target function t : Rmxd \u2192 Ras t(X) = $||X||_2$. Assume the existence of \u03a6 \u2208 Rd\u00d71, f : Rd \u2192 Rd and g : Rm\u00d7d \u2192 R such that\n$|t(X) - g (sMoE(X))| \u2264 max \\{1,t(X)/20\\}$ for all X \u2208 []\u00aem\u00d7d.\nThen there are no Lf, Lg \u2265 0 such that f is Lf-Lipschitz and g is Lg-Lipschitz.\nThe proof is deferred to Appendix A. Let us discuss this result.\nNotion of Approximation. The theorem says that we cannot approximate t over X \u2208 Rmxd, up to an error that scales as max{1,t(X)/20}. While the domain is unbounded, which is slightly non-standard, our approximation error is also unbounded since we allow it to scale with t(X) (unlike traditional results which require approximation to within a fixed constant tolerance). Indeed, it is trivial that the constant function zero can approximate t(X) over all of Rd up to an error of t(X). Our notion of error is only slightly smaller than this.\nResidual Connections. In a practical implementation, one would use residual connections so that the function g would typically receive both X and sMoE(X) as input instead of just sMoE(X). In such a setting, our result would of course not apply, since one could use g alone to approximate t(X), while completely ignoring sMoE(X). Nevertheless, we believe it is worth studying the setting without residual connections, because if g did not leverage sMoE(X), then there would be no point of using a Soft MoE layer at all.\nBenign Target. The target function t is extremely benign. Indeed, it is convex and 1-Lipschitz. So it a priori seems intuitive to try and approximate it with a Lipschitz expert f and subsequent Lipschitz architecture g. Yet, we cannot approximate t even when f, g have arbitrarily large Lipschitz constants. This is in stark contrast to the classical neural network approximation literature, where relatively simple feedforward networks can represent arbitrary continuous functions [14, 15].\nPermutation Invariant Target. Leto : Rm\u00d7d \u2192 Rm\u00d7d be any function that permutes the rows of its input. It is easily shown that o commutes with SMOEf; } This implies that one cannot"}, {"title": "Specialization of Experts", "content": "In this section, we tackle Q2 from Section 2.2.\nWhat Does Specialization Mean?\nWe begin by noting that since the Soft MoE combines tokens before feeding them to experts, it seems a priori unlikely that experts specialize, as acknowledged in the seminal work [10]. Indeed, it is unclear what it even means for an expert to specialize; for instance it seems difficult to consider specialization of an expert to a subset of tokens.\nInstead, our notion of specialization is whether there exists, for any input X \u2208 Rm\u00d7d, an X-dependent (relatively small) subset of experts that are sufficient to accurately predict the label for that input X. Recall from Definition 1 that the output of the Soft MoE layer is C'(X)\u1ef8(X), where \u1ef8(X) \u2208 Rn\u00d7d stores the output of expert i in row i. Hence, zeroing out a row i of Y (X) means that expert i does not contribute anything to the final prediction. If we can zero out many rows of \u1ef8 (X) without affecting the final prediction, then the remaining experts can be understood to have specialized to the input X, since this means that only these remaining experts were actually required to make an accurate prediction, and these experts did not require contributions from the other zeroed out experts. By contrast, if zeroing out rows of Y(X) changes the prediction, then this indicates a lack of expert specialization, since the knowledge required to predict correctly on X was non-trivially spread out across each of the n experts.\nDoes Specialization Exist?\nTo test whether specialization occurs, we consider the following small experiment. For n \u2208 {4, 8, 16} we train a simple neural network that comprises of a Soft MoE layer with n experts, followed by a linear prediction head, on the MNIST dataset [18]. The experts in the MoE are each (identical architecture) MLP with one hidden layer and ReLU activation, and we denote the number of parameters in a single expert to be de,n. As we increase n, we decrease the width of each expert (i.e. the number of units in the hidden layer), so that the total (i.e., summed over all experts) number of parameters nde,n is constant for all values of n. By holding the total number of expert parameters constant, we keep the total expressivity constant, and thus ensure that we do not bias the results for larger numbers of experts. See Appendix D.1 for further details of this experimental setup."}, {"title": "An Algorithm For Best Expert Selection", "content": "The results of the experiment in Section 4.2 demonstrate the existence of expert specialization, albeit in a small experiment. However, the same results show that identifying the best subset of experts cannot be done as simply as via random selection. Since identifying the true best subset of k experts ostensibly has \u03a9 ($(\\binom{n}{k})$) computational complexity, it is of interest to discover an efficient algorithm which can rapidly approximate the best subset, especially before moving on to larger experiments. Beyond our current motivations of checking for the existence of expert specialization, such an algorithm could also be useful at inference time to reduce computational expense without loss in accuracy (see Section 4.5).\nTo this end, we recall from Definition 1 that given an input X, the final output of the Soft MoE is C(X)\u1ef8 (X), i.e. row i of the final output is a convex combination of the rows of Y(X) where the combination weights are given by row i of C'(X). So a natural attempt to identify the most important experts are those given the most weight by C'(X). Algorithm 1 formalizes this intuition.\nAlgorithm 1 Best Expert Subset Selection\nRequire: number of experts k to use for prediction, Soft MoE SMOE{f; }_\u2081, input X \u2208 Rm\u00d7d\nsMoE{f; }_\u2081,\n1: Compute C(X) \u2208 Rm\u00d7n as in Equation 1.\n2: Define Csum \u2208 Rn entrywise as $C_{sum,j} = \\sum_{i=1}^m C(X)_{ij}$ for j = 1,2... n.\n3: Define Sk \u2282 {1,2... n} to be the indices corresponding to the k largest entries of Csum.\n4: Define \u0176(X) \u2208 Rn\u00d7d entrywise as\n\u0176(X)ij =\n$\n\n\n (f_i ((D(X)^T X)_\u2081)); if i \u2208 S_k\n0 if i \u2209 S_k\n\nfor each i = 1,2 . . . n and j = 1, 2 . . . d.\n5: return Y(X).\nNote that Algorithm 1 is computationally efficient, requires no separate training, and can be implemented in just a few lines of code. We also note that it can be easily adapted to handle a batched input in a vectorized fashion (see Appendix C). Its output \u0176(X) \u2208 Rn\u00d7d equals \u1ef8(X) on k rows,"}, {"title": "Empirical Performance of Algorithm 1", "content": "In line with the experimental setup assumed by the seminal work [10], we demonstrate the efficacy of Algorithm 1 on a suite of image classification tasks. We provide results across a wide range of model scales and architectures, including architectures beyond the ViT model class that was used to introduce Soft MoE.\nExperimental Setup\nWe experiment on 4 datasets: MNIST, CIFAR10, CIFAR100 [19] and ImageNet-1k [20]. For MNIST, we use the same experimental setup as in Section 4.2. Recall that the network used is very simple, and consists entirely of a Soft MoE layer and a prediction head. This small network has merely 318K total parameters, of which 307K are expert parameters. As a more practical setting, we use the Astroformer-1 architecture [21] for CIFAR10 and CIFAR100. This hybrid transformer-convolutional architecture achieves excellent performance on these datasets without leveraging extra training data. We modify Astroformer-1 by replacing the MLPs in the transformer blocks with Soft MoE layers, analogous to the standard practice that is followed in ViTs [10]. This model is larger, and has 180M total parameters, of which 150M are expert parameters. Finally, we adopt the same Soft MoE variant of the ViT architecture [22] used by the seminal work [10] on the ImageNet-1k dataset. Specifically, it replaces the MLPs in the latter half of the encoder blocks of the ViT Base model with Soft MoE layers. This is the largest architecture we consider, and it has 513M total parameters, of which 454M are expert parameters. Due to compute constraints, scaling up our experimental protocol further (either with external pretraining data or larger model sizes) is infeasible. Nevertheless, our setup spans a range of model architecture types and scales, as well as several canonical datasets. We defer further details of the various architectures and their modifications to Appendix D.2.\nFor each dataset and associated network architecture, we do the following. We train a suite of models where each model has a different n (total number of experts) in each Soft MoE layer. Each model is trained from scratch without utilizing any external data. As discussed in Section 4.2, when we increase n we correspondingly decrease the number of parameters in each expert, to ensure that the total expert expressivity is (roughly) constant. After training each network, we select for each n a model that has the same test accuracy (as discussed in Section 4.2, this is important because we are investigating the impact of varying n on Algorithm 1's test performance, and so we must ensure the original networks have similar test performance). For each of these trained networks and various values of k, we then evaluate Algorithm 1's performance on the test set. Concretely, we compute the test accuracy of using the k experts found by Algorithm 1 for each datapoint in the test set, and compare this to the test accuracy of randomly selecting k experts for each datapoint in the test set."}, {"title": "Experimental Results", "content": "In Figures 2 and 3 we depict, for each dataset and various choices of k, the performance of Algorithm 1 relative to the random selection scheme. We make two main observations.\nThe first observation is that Algorithm 1's accuracy may deteriorate (relative to using all experts) when k < n and n is small. But if n is big, then Algorithm 1's accuracy is often very close to that of using all the experts, even when k < n. For instance, on MNIST, using the k = n/2 experts selected by Algorithm 1 gives nearly the same performance as using all experts when n \u2265 64, but has poor performance when n \u2264 8. This shows that as n increases, Algorithm 1 is better poised to discover specialized experts, even though the number of expert parameters is invariant to n. While we do not have a formal explanation for why this phenomenon occurs, we believe it is an instance of the implicit bias that exists in Soft MoE. Concretely, the experts found by Algorithm 1 for input X are those which are assigned the highest weight by C'(X). Our result thus shows that as n increases, the Soft MoE trains in a manner such that the C(X) matrix is more informative for which experts are useful in correctly predicting the label of X, even though such a property is not explicitly enforced during training! Since the role of C'(X) is ultimately just to adaptively combine expert outputs, this implicit bias is very benign. A different view of the same results in Figure 2a is provided in Figure 1.\nThe second observation is that Algorithm 1's test performance dominates that of random selection. And often, Algorithm 1 is far better than random selection, as is particularly evident in CIFAR10 and ImageNet-1k, where random selection is very poor (see Figure 2). There are instances where random selection has decent accuracy, such as in CIFAR100 (see Figure 3). So to further validate our result, we use the following statistic to measure how much better Algorithm 1 is relative to random selection. We first compute the standard deviation of the random selection test accuracies (which were obtained by averaging over 10 random seeds). Our statistic is then the number of standard deviations by which Algorithm 1 exceeds the mean random selection accuracy. When this statistic is large, it means that Algorithm 1 found an expert subset whose performance is statistically significantly larger than that of the typical random expert subset. The results are shown in Table 2 where we observe very large values for this statistic. This shows that our Algorithm 1 significantly outperforms random selection for CIFAR100, even though random selection has decent performance on this dataset. Analogous tables for the other datasets are provided in Appendix D.3."}, {"title": "Faster Inference via Algorithm 1", "content": "In practice, one typically trains a very large model (with many Soft MoE layers, each with many experts) a single time, then the model is deployed and repeatedly utilized for inference. It is thus of great interest to speed up computation through Soft MoE modules, since this leads to faster inference. Algorithm 1 is well suited for this purpose. The preceding sections show that when the number of experts n is large, then Algorithm 1 can rapidly and cheaply find a small subset of experts that can predict any datapoint with minimal expected accuracy loss. So it can potentially be used for faster inference through the Soft MoE.\nA full examination of the usefulness of Algorithm 1 for faster inference is beyond the scope of our paper. A proper analysis would require massive industry-scale models, and would also be very application specific because it would depend on the type and number of devices used for inference, as well as the extent of distributed computing and per-device parallelization. We lack the compute capacity to do such a thorough study. Nevertheless, it is immediate that in the regime of many"}, {"title": "Related Work", "content": "Mixture of Experts. Our work falls squarely in the literature on MoEs, which originated several decades ago [23, 24] and has been revived as the Sparse MoE [6]. Nevertheless, there is a significant difference between our investigation and the majority of past work. The majority of past work in MoE focuses primarily on computational considerations, such as (in the context of Sparse MoE) routing a token to only a single expert for efficiency [3], and ensuring load balancing by incorporating linear programs [8] or having the experts select tokens [9] or re-routing dropped tokens [25]. Indeed, the Soft MoE [10] was recently introduced to address the various training instabilities suffered by Sparse MoE, and performs very well in applications like vision [10], RL [11] and audio processing [26]. By contrast, our investigation begins with a more fundamental question \u2013 what are the implicit biases that occur as a result of Soft MoE's particular manner of combining tokens and expert outputs? While analogous fundamental investigations of the gating function do exist in the context of Sparse MoEs [7, 27] and some of its variants [28, 29], to the best of our knowledge this line of investigation is novel in Soft MoE.\nExpert Specialization. The use of MoE layers in a network is often motivated by the desire for expert specialization, since this implies a more efficient use of the network parameters. To our knowledge, all prior work on expert specialization has been conducted in the context of Sparse MoE. While some studies demonstrate that expert specialization may occur for Sparse MoE [7, 27], others show that specialization is often limited and requires architectural innovations [12, 13, 30]. These innovations are different than Soft MoE, where it is not even a priori clear what expert specialization means. Indeed, the seminal work acknowledges this limitation [10]. In our paper, we offer a notion of expert specialization in Soft MoE, and show not only that it occurs but can be efficiently detected.\nSparse Activation & Pruning. Our work is also related to the literature on sparsely activating or pruning a network, since our Algorithm 1 can be viewed as a form of pruning at inference. For instance, there is work on using RL to conditionally activate only certain units in a generic network for faster training and inference [31], pruning CNN kernels for efficient inference [32], and developing adaptive computation modules for transformers [33]. While a complete survey of this vast area is beyond the current scope [34], we emphasize that our form of pruning is entirely specific to Soft MoE, since it relies on the C(X) matrix. It can be used with or without many other types of pruning."}, {"title": "Discussion", "content": "Limitations. Our work has a number of limitations. While our Theorem 1 is a negative result for a single expert's representation power, we are unable to prove a corresponding positive result for multiple experts (although in Appendix B we provide empirical evidence for increased representation power as the number of experts increases). A different limitation is that we are unable to provide a thorough analysis of the extent to which Algorithm 1 can reduce computation at inference (due to a lack of compute capacity). We believe both limitations provide exciting directions for future work."}, {"title": "Proof of Theorem 1", "content": "Here", "f": "Rd \u2192 Rd and g : Rm\u00d7d \u2192 R such that\n$|t(X) - g (sMoE(X))| \u2264 max \\{1", "n$\\begin{bmatrix}\n1/m\\\n1/m\\": "n1/m\\\n\\end{bmatrix}$\nLet A1, B1 denote the first column of A, B. The above equality implies that\n$||D(B)^TB \u2013 D(A)^TA||_2 = |D(B)^TB_1 - D(A)^TA_1| = |m \\frac{a}{m} - ( \\frac{a}{2m} + \\frac{a}{2m} ) |$\n$\\qquad$ = 0.\nNote via definition of t that t(B) \u2013 t(A) = (1 \u2013 1/$\u221a{2}$)a. Now recalling Eq. (2) and Eq. (3), we know for large a > 0 that\n($\u221a{2}$ \u2013 1)a = t(B) \u2013 t(A)\n$\\leq |g (sMoE (B"}]}