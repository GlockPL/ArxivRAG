{"title": "SoK: A Systems Perspective on Compound AI Threats and Countermeasures", "authors": ["Sarbartha Banerjee", "Prateek Sahu", "Mulong Luo", "Anjo Vahldiek-Oberwagner", "Neeraja J. Yadwadkar", "Mohit Tiwari"], "abstract": "Large language models (LLMs) used across enterprises often use proprietary models and operate on sensitive inputs and data. The wide range of attack vectors identified in prior research\u2014targeting various software and hardware components used in training and inference\u2014makes it extremely challenging to enforce confidentiality and integrity policies. As we advance towards constructing compound AI inference pipelines that integrate multiple large language models (LLMs), the attack surfaces expand significantly. Attackers now focus on the AI algorithms as well as the software and hardware components associated with these systems. While current research often examines these elements in isolation, we find that combining cross-layer attack observations can enable powerful end-to-end attacks with minimal assumptions about the threat model. Given, the sheer number of existing attacks at each layer, we need a holistic and systemized understanding of different attack vectors at each layer. This SoK discusses different software and hardware attacks applicable to compound AI systems and demonstrates how combining multiple attack mechanisms can reduce the threat model assumptions required for an isolated attack. Next, we systematize the ML attacks in lines with the Mitre Att&ck framework to better position each attack based on the threat model. Finally, we outline the existing countermeasures for both software and hardware layers and discuss the necessity of a comprehensive defense strategy to enable the secure and high-performance deployment of compound AI systems.", "sections": [{"title": "1. Introduction", "content": "AI-powered applications like chatbots [1]\u2013[3], and ML tools like autonomous driving [4] and OCR [5], have become widespread due to advances in neural networks and transformers. Recent developments in large language models (LLMs) with billions of parameters support tasks like program and image generation. These production-ready models train on massive data, often containing sensitive information, and utilize proprietary architectures running on expensive compute resources, making them targets for adversaries. Literature in ML security and privacy have explored several attacks [6]\u2013[10] and developed efficient defenses [11]\u2013[15], but many focus individually on algorithms or platforms, ignoring the complex interactions in modern AI systems. A recent study [16] also highlighted that privacy-preserving models can leak data when deployed alongside other software components, challenging existing security guarantees. Emerging compound AI systems [17] are created by integrating multiple AI models with numerous software components, all deployed across distributed hardware. Figure 1-left provides a cross-stack view of a compound AI system: The application layer comprises of multiple AI models, vectorized databases for knowledge storage, and associated tools used by AI agent models. The software layer includes frameworks like Langchain [18] to design AI pipelines, packages like Pytorch [19] to design models, and libraries like cuDNN [20] to interface with devices. Finally, the hardware layer comprises of compute units like CPUs, GPUs, and TPUs along with memory and network components. While literature have proposed attacks across different AI/ML models and systems, vulnerabilities and attack surfaces across different components in these layers can be composed to create new attack vectors or relax the threat model assumptions for existing attacks. The complexity of layered software, distrustful entities, and diverse hardware creates a broad attack surface, highlighting the need for a systematization of existing literature. The volume of proposed attacks and defenses across different stack layers makes it challenging to grasp threat models and associated defenses. Heterogeneous hardware \u2013 CPUs, GPUs, ASICS, FPGAs \u2013 introduces further risks, including digital and physical side-channel attacks, that could expose sensitive data or model parameters. Device ownership between cloud and edge deployments, adds complexity to the trust landscape, expanding the overall attack surface. Figure 1-right enumerates an example of how complex cross-layer observations can lead to an end-to-end attack. An attacker performs a prompt injection attack that can exploit a software library bug, eventually triggering repeated memory access to adjacent memory rows containing model parameters. Repeated access can result in Rowhammer [21], that changes the weights tensors due to bit-flips. These tampered weights can change the model prediction and misclassify objects which eventually generates incorrect response. However, no prior work has systematized the impact of vulnerabilities in system software (e.g., frameworks, packages, libraries) or hardware (e.g., side-channels, fault injection) in a heterogeneous deployment platforms. We categorize these as system attacks and focus on organizing them with a view to the complex threat models emerging from compound Al systems. Algorithmic attacks [6], [7], [9], [22], that target ML algorithms and training data in the application layers, are well-studied by prior SoKs [23]\u2013[25]. Our work also explores attacks at the software and the hardware layers that attackers can use for composing cross-layer widgets to perform an end-to-end attack. Understanding the threat model assumptions and attacker capabilities can inform system designers of cross-stack attack composition as well as define defense mechanisms at the software and hardware layer. This also informs algorithm designers of system and hardware impacts, and help build robust models. In this paper, we focus on different types of system attacks and defenses in a compound AI system. Specifically, we make the following contributions 1) We explore a range of software, and hardware vulnerabilities, as well as the defenses impacting AI models and study how they affect compound AI pipelines. To the best of our knowledge, this is the first effort to methodically categorize system attacks and defenses for Al systems. 2) We focus on the cross-stack vulnerabilities introduced by the emergence of compound AI pipelines in this domain and systematize existing attacks and vulnerabilities in a well-known cyber-security framework \u2013 Mitre Att&ck [26]. This can be used a foundation for threat model developments and methodologies for individual defenses. 3) We explore case-studies which utilize cross-stack vulnerabilities to mount an end-to-end attack. We discuss the learning and gaps of current security practices and bring out open research questions that will drive us to build better and safer AI applications. The rest of the paper is organized as follows. Section 2 introduces emerging compound AI pipelines and related AI/ML security literature. Section 3 discusses the assets, trust entities and threat models under study. Section 4 and Section 5 explores various software and hardware attacks and defenses of existing AI/ML systems. Section 6 works on systematizing the attacks in a well formed framework and explores new attack paths that are possible within the paradigm of modern compound AI pipelines. Finally, Section 7 addresses key learning and gaps of current security practices before concluding in section 8."}, {"title": "2. Background", "content": "Language models are popular for next token prediction, given an input prompt. With the prolific use of accelerated hardware (GPUs, TPUs [27] etc.), such models have grown to billions of parameters with the capability to understand complex structures and provide high accuracy responses \u2013 making large language models (LLMs). Compound AI: The growing demands of LLMs and their widespread use bring new system and design challenges. Modern AI inference pipelines often involve multiple ML and LLM models working together, from prompt engineering to token generation and AI-safety checks. The complexity increases with the use of multi-modal inputs (e.g., Gemini [2]) and fine-tuned mixtures of experts (e.g., GitHub Copilot [28], Amazon Q Developer [29], MistralAI [30]), resulting in Compound AI systems [17]. These pipelines integrate diverse software and ML/LLM components to optimize resource usage and improve accuracy. In a typical workflow shown in fig. 2, a user query, potentially containing text, images, and videos, is processed to generate embedded context [31]. Optimizations like few-shot training or chain-of-thought reasoning are applied, and facts may be retrieved using a Retrieval Augmented Generator (RAG) [3]. The query is then sent to an LLM for token generation, often using a mixture-of-experts approach. Once generated, responses undergo grounding and safety checks before being presented back to the user. This complexity requires sophisticated software orchestration to manage model batching, data transfer, and resource allocation across heterogeneous platforms, going beyond traditional GPU or accelerators to meet time-sensitive user needs. Algorithmic Attacks and Defenses: Existing ML attacks primarily target vulnerabilities in the algorithm itself, regardless of the deployment environment. Such attacks include membership inference, model extraction, and data poisoning, among others and are classified as algorithmic attacks in our document. Membership inference attacks [6] aim to determine if specific data were part of a model's training set, risking exposure of sensitive private information to the public. Techniques like machine unlearning [14] can defend against these attacks. Model extraction attacks [7], [8] seek to replicate a model's architecture and parameters, which are valuable intellectual properties requiring significant computational resources. Thus, it is important to prevent model extraction [13]. Data poisoning attacks [9] involve injecting corrupted data into the training set, either reducing overall model accuracy or targeting specific predictions. Data augmentation [15] is a defense strategy against such attacks. These attacks rely solely on model inputs and outputs during training or inference, without considering side-channel data tied to the deployment environment. They are distinct from system attacks we categorize in this work."}, {"title": "3. Threat Model Categorization", "content": "The diversity of machine learning deployments demands a broad and adaptable threat model. Software and hardware defenses often depend on different threat models, complicating the evaluation of an attack's relevance in a given context. Moreover, studies vary in their focus on secret assets and their assumptions about trusted entities. To address this, we first align the AI/ML attack landscape with the Mitre Att&ck framework [32], providing a structured approach to understand threat models. We then introduce various assets within a compound AI system, and identify the trusted entities in these deployments."}, {"title": "3.1. MITRE ATT&CK Framework", "content": "MITRE ATT&CK [32] is a knowledge base that models cyber adversary behavior using various real-world observations. This framework reflects the various phases of an adversary's attack lifecycle and the platforms they are known to target. We align the system and hardware attacks on AI/ML systems to the MITRE framework to enable threat model development and explore defense methodologies in AI applications. Figure 3 translates various attacks seen across literature and Common Vulnerability and Exposure(CVE) databases as as techniques across different categories from the framework: 1) Reconnaissance and Initial Access lists the techniques used by an attacker to identify and locate a victim model or target device to mount an attack. For example, poisoning of training data or exploiting co-location patterns [33] on cloud allows attackers to target specific applications or models. 2) Resource Development lists down methodologies an attacker utilizes to prepare for an exploit. These might include employing shared memory regions with victim [34] and exploiting access control violations. 3) Privilege Escalation and Execution enumerates methods for adversaries to realize an attack. These methods can be software or hardware contention [35], [36], and passing illegal prompts that lead to system errors or confused deputy [37] issues. 4) Collection and Exfiltration identifies the types of metrics or data that an attack might expose, revealing sensitive information (e.g., performance counters, power traces, execution time) or causing service-level disruptions (e.g., misclassification, Denial of Service). 5) Impact lists down the confidentiality, integrity and availability aspects of an attack result as seen in the \"Collection and Exfiltration\" techniques."}, {"title": "3.2. Threat Models", "content": "We divide the attacker's scope into categories that we define here as four threat models. In fig. 3, we enumerate techniques in increasing order of threat model severity. Darker colored rows indicate a higher privilege adversary, with attacks having a higher severity impact. Remote software access describes a threat model where an adversary only has remote access to a compound AI system. This is typical for cloud-hosted applications providing API access to AI applications. Here, the attacker is least-privileged since she can only influence execution indirectly (e.g. resource contention) or leak data via prompt injection. Privileged software access involves an adversary with elevated software control, such as root access to an OS, hypervisors, or firmware. This includes untrusted system administrators in a public cloud environment and users exploiting privilege escalation vulnerabilities in AI pipelines. An adversary can monitor execution via system traces, device drivers, or disrupt operations by manipulating schedulers. Digital hardware access includes adversaries that can issue system commands for observing hardware metrics (e.g., performance counters) or launch covert-channel attacks to leak information like model weights and layer architecture. This threat model is common in public cloud environments where attackers co-locate malicious workloads with AI pipelines. Physical hardware access represents the most privileged adversary, with full physical access to hardware running AI inference. This is common in edge ML deployments where devices running sensitive models are controlled by untrusted owners, allowing potent attacks like cold-boot, memory dumps, or access to debug ports."}, {"title": "3.3. Secret Assets", "content": "Secret assets refer to confidential information that attackers aim to exploit. Each component of a compound AI system possesses distinct secret assets, as outlined below: Model training data A compound AI system is composed of multiple large language models (LLMs). These models are trained on extensive datasets, some of which include private or confidential information. Furthermore, fine-tuning these models is often carried out using proprietary data. Prior work [16] leaked confidential information including user ssh key, user name, credit card info, address etc. from training data. Membership inference attacks determine whether specific information was included in the training data. Lastly, certain attacks analyze the distribution of the training data and use it to compromise the inference process. Data poisoning attacks can either skew a model's decisions or diminish its overall effectiveness. Such attacks are more challenging to be detected in multi-modal training due to adversarial noise highlighting the importance of integrity in training data. AI model IPs LLMs contain billions of parameters and takes significant training resources, making them lucrative attack targets. Attackers have tried to violate confidentiality of ML models by extracting the weights, architecture and hyperparameters. The model architecture can be private for proprietary foundation models and confidential adapters for fine-tuned models. Similarly, model hyperparameters including learning rate, and others are lucrative for attackers. Additionally, the semantic relationships between tokens in an embedding model are confidential, as attackers can exploit this information to carry out membership inference attacks. The integrity of the model data is equally important. Altering model hyperparameters can degrade model accuracy, while tampering with model weights may lead to misclassifications or the spread of misinformation. Knowledge database The LLM models leverage recent knowledge from vector databases, which are constantly being populated with new information. The integrity of the vector database is critical for generating relevant and correct information. Similarly, the vector DBs should not be susceptible to availability attacks like denial-of-service, which can generate stale response. Inference data User queries to a compound AI system can contain sensitive information like medical records, personally identifiable information(PIIs) or financial documents. For such private data a user would like to ensure confidentiality from the software, model owners and other tenants of the compound AI application."}, {"title": "3.4. Trust Entities", "content": "The relationship between different trust entities plays key role in defining a threat model. The entities include: Hardware manufacturer has access to the hardware design and the manufacturing supply chain. Hardware Trojans introduce backdoors that can be used to exploit CIA guarantees for all assets. Since, a lot of ML systems are designed on FPGAs, the role of hardware manufacturer includes the FPGA manufacturer and the hardware design bitstream owner. Platform owner refers to a cloud-service provider or an admin of the hosted hardware. A platform owner multiplexes different model run on the same hardware to improve the resource utilization of the cluster. A platform owner or a hypervisor admin can have access to privileged software like devise drivers or have physical access to hardware to mount various snooping/physical attacks. Training data owner refers to owner of private data that is used in training an ML model. Although modern LLMs are typically trained on a large corpus of public data, many expert models are then fine-tuned by training on private and sensitive data (e.g. medical images, financial documents). Algorithmic attacks [6] have been shown to divulge training data using membership inference attacks. Model owner refers to an owner of an ML model where the model weights, layer architecture and the hyperparameters could be private and sensitive information. Model owners are susceptible to information leakage via algorithmic attacks [7], [8] and from distrusted co-located tenants via side-channels [38], [39]. Inference data owner is a client side user who provides data for inference to the AI/ML application. Similar to training data, these inputs can be sensitive and contain PIIS which an user would want to keep confidential from other users and tenants on the platform. Software developers refer to owners of third-party services and framework components that are associated with an end-to-end ML/AI application. Bugs and vulnerabilities can enable attackers to gain unrestricted access to the runtime service or the ability to mount man-in-the-middle attacks to leak sensitive information."}, {"title": "4. Software vulnerabilities and defenses", "content": "Design of compound AI systems involve a development stage followed by a deployment stage that involve a wide variety of software frameworks, packages and libraries. The development stage focuses on model training, knowledge integration and designing of LLM pipelines that are built using frameworks like Langchain [18] and HuggingFace [40] and model compilation packages(Tensorflow [41], Pytorch [19]). Vulnerabilities like trojans, misconfigured access control of databases, and buggy libraries and drivers can lead to privilege escalation and initial access into systems that are leveraged to mount sophisticated attacks. Verification of large codebases in compound Al systems are unfeasible because of the computational difficulty involved in with extensive repositories and models. Additionally the challenge of accurately modeling the interactions between libraries and continuous evolution of such deployments make it impractical to rely only on software verifications. The deployment stage relies on various data storage (Apache Spark [42], Hadoop [43] and Snowflake [44]) and model orchestration (BentoML [45], Kubernetes [46]) to deploy AI pipelines across heterogeneous platforms that utilize device libraries and packages (Pyyaml [47], CuDNN [20]) to provide optimized executions. Vulnerabilities and lack of safeguards across the software stack and device drivers can leak to black-box attacks like prompt injection and membership inference, or model tampering and model fingerprinting attacks that violate the confidentiality, integrity and availability of compound Al systems."}, {"title": "4.1. Attacks", "content": "We searched the CVE database to report several vulnerabilities across different components (Frameworks, Packages and Libraries) of the compound AI stack in table 1. The columns categorize the attacks based on the attackers' motive, which includes violating data confidentiality, integrity, and availability (C,I,A) of the system. The last two columns enable the attacker to either run arbitrary code in the victim machine, or performing privilege escalation. These attacks enable the attacker to collect victim information including access to system logs, scheduling processes, linking malicious libraries, or introduce ransomware. These capabilities often lead to model hijack and tampering attacks. We describe each of the attack types in detail. Data Confidentiality An attacker can leak asset classes including model weights and hyperparameters (layer architecture, learning coefficient, temperature etc.), input query, training data and documents in the vector database. The common weaknesses leading to data confidentiality violation include (1) Buffer out-of-bound reads (OOB Read); (2) Use-after-free memory errors; (3) Backdoor introduced by malicious packages; and (4) System bugs leading to insufficient access control. The first column in table 1 lists several CVEs leading to data confidentiality violation, with the green boxes needing privileged system access. The main causes of information leakage are insufficient access control leading to active or stale data leakage from registers and memory. Software bugs or malicious packages trigger memory safety errors and The majority of compound AI software is written in Python and C++ which are not type safe languages, and hence lacking compiler protections. Data Integrity Data integrity is essential to prevent misclassification in edge deployments and the spread of misinformation in complex RAG systems. The common weaknesses include (1) Buffer out-of-bound writes (OOB Writes) by an attacker into victim memory. (2) Access control violations enable an unprivileged attacker to gain access to victim data while cleartext writes in databases enable admin to change document contents. (3) Directory traversal enables attackers to gain write access to model files while memory corruption on compression libraries (Python zlib) either tampering or deleting sensitive data. Crash/Denial-of-Service System or platform availability can be restricted with system crashes or denial-of-service (DoS) attacks as shown in the third column of table 1. The system crashes are triggered with unexpected behavior including: (1) Heap overflow attacks leading to kernel panic; or (2) System exceptions specifically in floating point units(FPE) or integer overflows leading to system crash. The DoS attacks include (1) An attacker taking over the entire system or model resource preventing other tenants from accessing the model; (2) An attacker can also target the Kubernetes orchestration to prevent other tenants from accessing a compute node. (3) Creating DoS by engaging the system in a complex regular expression (ReDoS) is shown in NLTK framework. System availability is important in mission critical applications (autonomous vehicles, industry quality-check installations) or real-time usecases (Chatbots). Code execution Several compound AI systems are deployed in a large cloud infrastructure. A code execution vulnerability can be exploited to reveal system configuration or identity. Moreover, arbitrary code can interfere with system robustness or trigger inaccessible tools inside the AI pipeline. (1) Arbitrary code execution (ACE) enables attackers to convert non-executable memory regions into malicious scripts. (2) Remote code execution (RCE) attacks perform the same on a remote system and is even more catastrophic. (3) Command injection attacks are able to query secret datatypes by inserting a command. For instance, victim database entries are deleted by inserting delete() command in a Snowflake database. (4) The OOB write vulnerability in the vGPU driver [48] flips device configuration enabling an attacker to perform unauthorized code execution, leading to device hijack. Privilege escalation The green boxes in table 1 requires attackers to have a privileges system access. However, a non-privileged attacker can mount privilege escalation attack to become the root user of the system. The privilege escalation vulnerabilities include: (1) Access control (ACL) violations in software frameworks and packages pose significant risks. Many of these frameworks, packages, or libraries include kernel modules, allowing attackers to exploit ACL violations or API bugs to gain privileged access. (2) Directory traversal or the lack of input validation is exploited by attackers with blackbox access to access privileged data structures. (3) Server side request forgery (SSRF) is used by a remote attacker to gain unauthorized system access and we found one such CVE in Langchain [49]."}, {"title": "4.2. Defenses", "content": "The complex software stack in a compound Al system is riddled with vulnerabilities at all levels as discussed in section 4.1. While many of the frameworks, libraries and packages might have individual safety nets, it is clearly insufficient to protect against different vulnerability classes. Moreover, the active development and the sheer size of the codebase makes it infeasible for formal verification techniques [50] or even high-coverage fuzzing methods [51]. A key observation though is that, some vulnerabilities like buffer overflow, use-after-free, and memory errors etc. can be thwarted with existing defenses. However, certain other attacks like victim co-location, input validation or installation of malicious package catalyze certain algorithmic or hardware attacks. In this section, we propose defenses against direct software vulnerabilities as well as mandate certain software practices to prevent leveraging system software in performing algorithmic and hardware attacks. Software supply-chain defenses Protecting the software supply-chain is a first-line defense against the listed vulnerabilities. The Compound AI software stack incorporates numerous dependencies sourced from a wide range of vendors. Prior work [52] proposes user credential and role validation to prevent package tampering. The second challenge is the authentication of package registry. The attacker registers several malicious packages with similar names to trick the victim. Prior work [53] employs administrative safeguards as well as remove unused dependencies to protect against these attacks. Amalfi [54] uses ML classifiers to test package reproducibility from source code to automatically detect malicious packages. Access control policies Declaration of variables with an appropriate access modifier such as private, public and protected as provided by languages like Python, C++ and Java is necessary to avoid secret data leakage and tampering due to improper access control policies. Secret variables should always be declared as private to prevent access from different class methods. Code review and information flow tracking compiler passes [55]\u2013[57] can help prevent secret data leakage from training datasets, model parameters and knowledge vector database. Fine-grained identity and access management policies (IAM) should also be validated [58] for datasets, model and other resources. Input queries and retrieved context should undergo input validation to prevent privilege escalation attacks. Input validation should verify token lengths, detect malicious regular expressions, and sanitize retrieval commands. Exhaustive testing of the trained model eliminates unknown behavior when presented with crafted inputs. Safeguards should be placed at different compound AI components both on the client and server side to prevent access control attacks."}, {"title": "4.3. Key Takeaways", "content": "Memory safety We discovered several CVEs for different types of buffer overflow attacks, dangling pointers and use-after-free errors. Illegal memory accesses lead to confidentiality, integrity and availability concerns. While defenses are proposed for these well-known problems, the deployment scale and the active development leads to these errors. There is an urgent need to shift the software backend from C,C++ to memory safe languages like Rust. The software community is developing critical packages [59] and linux OS [60] in Rust language to ensure memory safety. Running compound AI systems in a sandbox environment [61]\u2013[64] reduces the risk of privilege escalation and DoS attacks, which are particularly catastrophic in cloud deployments. Existing memory safety protections including address-space-linear randomization (ASLR), stack canaries and use-after-free protections should be enabled for all production systems. Memory tagging techniques like CC, Cheri [65] and Morpheus [66] isolate data from different security domains and are implemented in latest Intel and arm processors to deter memory safety errors. Node resiliency and check-pointing Training of foundational models take place in vast distributed systems with hundreds of GPUs and terabytes of storage. Orchestration frameworks are used for compute scheduling and efficient data movement. This frameworks should be fault tolerant and be able to deal with node crashes. For instance, orchestration frameworks can migrate computations away from specific nodes [67] based on system's health to prevent large scale infrastructure failures. The ML model should be check-pointed at regular and frequent intervals to ensure minimal loss during a large scale failure. Moreover, efficient log collection and forensic analysis is required for root causing the failure. Omnilog [68] collects Linux audit logs efficiently and ensure log integrity after system compromise. Recent works have employed machine learning techniques to find malicious attack signatures for faster attack detection. Takeaway 1: Safeguarding the software supply chain: The compound AI software stack contains a large number of software packages from multiple vendors. This creates a distributed attack surface enabling an attacker to inject a malicious package or software backdoors to extract secret data. Software supply chain defenses should validate the developer, leverage packages from well-known sources to ensure secret data confidentiality and integrity protection. Takeaway 2: Design with type-safe languages: A majority of data leakage and privilege escalation attacks are a result of improper access control policies and memory safety errors. Memory- and type-safe languages like rust should be used to prevent these attacks. Takeaway 3: Software defenses for heterogeneous platforms: Many software defenses (e.g. ASLR, stack canary) found on CPU environments are crucial on platforms like GPUs, ML accelerators [27] and even DPUs [69] since rise in Al deployments have exposed these platforms to threat models similar to CPU systems. Takeaway 4: System wide software health monitors Small errors in multiple software components can cause a catastrophic system failure. Hence, system-wide software monitors should be deployed in the orchestration layer to monitor the health of each deployed node. Resource hogging or access control violations should be detected proactively to minimize system impact."}, {"title": "5. Hardware vulnerabilities and defenses", "content": "Beyond the algorithmic and software attacks, an attacker can mount direct, side-channel and bit-flip attacks in hardware. These attacks can be performed both during training and query inference in a compound AI system. We define direct attacks as those in which the attacker gains unauthorized access to read or manipulate confidential data directly from storage, memory, interconnect or on-device buffers. A timing attack is a side-channel attack where the attacker extracts victim secret data from execution timing variation. An attacker can use hardware performance monitors, or high-resolutions timers to extract fine-grained victim execution timing. A power attack extracts power signatures to infer victim execution. Bitflip and injection attack tampers secret data. A bitflip attack is non-invasive, while an injection attack requires hardware physical access. In this section, we will first enlist different attacks performed on ML systems to extract different assets including training data privacy, model parameters and hyperparameters, and user inference input. Next, we will discuss hardware defense mechanisms deployed to protect confidentiality and integrity of ML execution, with a final key takeaway section summarizing the main design trends and their mapping to the attack vectors."}, {"title": "5.1. Attacks", "content": "We categorize existing hardware attacks into different attack categories. It includes attacks on heterogeneous hardware including CPUs, GPUs and ML accelerators. We categorize the hardware components into memory, interconnect and compute components as shown in table 2. Memory attacks encapsulate data leakage and tampering in main memory. Interconnect attacks cover leakage from memory bus and I/O interconnect. Compute attacks encompass micro-architectural attack on on-chip memory, buffers and processing units. The attacker can either perform hardware attacks remotely (digital attacks) or require physical system access (physical attacks). Digital attacks include timing side-channels and resource utilization attacks while physical attacks include power and fault injection attacks that require device physical access to snoop signals or tamper with execution. Memory and storage attacks Memories are used to store proprietary ML models and secret user data. CPUs, GPUs and accelerators are typically connected to DDR rams, SD rams or an HBM storage. Attackers target data stored in these memory components in order to compromise its confidentiality and integrity. Direct attacks dump secret data stored as plaintext from memory. Boot attacks exploit the cell retention of volatile memories across reboots to leak secret data, violating confidentiality of model parameters (M), secret user inputs (I) and training data (T). Timing attacks retrieve model architecture (Ma) by recording the timing difference between row buffer hits and misses. NVLeak [74] attack leaks secret vector database knowledge stored in persistent storage. Bitflip attacks like Rowhammer [21] and Rowpress [117] can flip model parameters to reduce model performance. Many large models are resilient to bitflip attacks, especially for early layer bitflips. However, these attacks can be used to tamper user input queries leading to either mis-classification or model hallucination. Rambleed attacks use rowhammer to leak secret data from adjacent rows. This can also be used to infer user queries, as well as targeted training data and model parameters. Despite rowhammer mitigations [118] in latest generations of memory, recent work [117] demonstrated rowhammer in HBM memories widely used in latest GPUs. Power attacks are demonstrated on processing-in-memory (PiM) and processing-near-memory (PnM) devices, which are gaining popularity due to data transfer minimization. Specifically RRAM PiMs emit power signatures based on layer architecture. PnM access pattern is also used to infer confidential DNA sequence input. Finally, laser is used inject faults in memory, which can poison training data samples or tamper model parameters during inference. Interconnect attacks The next attack surface is on the memory bus and I/O interconnect. This is widely used to data transfer between the compute units like CPUs, GPUs and accelerators and memory and storage blocks like DRAM and nvme drives. Direct attacks include bus hijack by either connecting a malicious device over the PCIe or using a bus monitor. Bus hijack attacks can be used to either read secret data during transfer or perform a man-in-the-middle attack to tamper input queries or ML models. The interconnect is widely used across heterogeneous compute units in a compound AI system, making it lucrative for attackers. Hijacking the interconnect enables the attacker to control the decisions taken by the LLM agent and emit malicious response. Demand access pattern extracted from memory bus is also used to infer model layer architecture. Finally, several performance counters reveal read/write data volumes to fingerprint service usage or layer connections in a ML model. While, performance counters are mostly disabled in production systems, CSPs can use them for analytics, leading to application and service fingerprinting attacks. Timing attacks include leakage from data-driven optimizations like model sparsity. Prior work [97] uses AXI bus monitors to reveal sparsity ratio from data volume. Bus contention [90] is used to snoop network traffic, revealing model architecture. Finally, bus utilization is used to fingerprint model layer dimensions. Hardware Trojan tampers model parameters to misclassify ML inference. Physical side-channels include ring oscillators to snoop power consumption, which is used as a proxy to infer model dimensions. These attacks reveal model architecture, which is then used to perform membership inference or model extraction attacks with white-box assumption. Voltage viruses [104] can be used to tamper with model inference in multi-tenant FPGA deployments. Compute attacks On-chip microarchitectural sharing of multiple tenants expose several attack vectors. Sesame [11] observed several attacks for resource sharing in multi-tenant accelerators. These include sharing of compute blocks and use-after-free attacks in a spatially shared scratchpad. Timing attacks include CPU and GPU caches. Cachetelepathy [39] leveraged prime+probe and flush+reload attacks to leak ML model architecture. Similar attacks were demonstrated in GPU caches as well. Moreover, compute execution time is inferred from EM emission [109] and from floating-point unit [112] and used to fingerprint different layer types and dimensions. These works also use novel algorithms to efficiently reverse engineer the model architecture."}, {"title": "5.2. Defenses", "content": "Prior works has proposed several defense mechanisms to protect secret data from the attacks discussed in section 5.1. Some of the defenses target specific micro-architectural components like caches, scratchpad and compute units, while others strive to design isolated execution systems. Memory and storage defenses Several prior defenses considered offchip memory and storage outside of their trust boundary and proposed confidentiality and integrity protection. Direct attacks are thwarted by encrypting data in memory [119", "120": "and storage [121", "122": [123], "124": "that optimize the memory traffic by storing the message authentication code (MAC) in the ECC storage, while MGX [125", "118": [126], "128": "propose Rowhammer and other bitflip mitigation in memory. Timing channels are prevented by restructuring the data structures to prevent bank conflicts or defenses in memory controller [129", "132": ".", "133": "while injection attacks can be prevented with use of data integrity blocks.It is important to monitor health of SSDs since these are used as storage buckets for training data and knowledge databases. RL-Watchdog [134", "135": "computes checksum to prevent data poisoning attacks during ML training. Interconnect defenses IO information (including address, data, and timing) can leak information [89", "136": "is a general way of protecting address and data from leaking information by obfuscating the address and the data content. However, its high performance overhead prevents practical adoption. Practical secure accelerators perform encrypted data transfer to prevent direct attacks [125", "137": ".", "138": "and RDMA protocol with SRDMA [139", "119": "Nvidia CC [140", "141": "ensure that performance counters are disabled during trusted execution. The timing channel with IO [99", "142": "can reduce the data leakage granularity. The contention and fingerprinting attacks can be mitigated by memory traffic shaping [12", "143": [144], "145": ".", "146": "."}]}