{"title": "FLEKE: Federated Locate-then-Edit Knowledge Editing", "authors": ["Zongkai Zhao", "Guozeng Xu", "Xiuhua Li", "Kaiwen Wei", "Jiang Zhong"], "abstract": "Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating large language models (LLMs) without full retraining. However, existing methods assume a single-user setting and become inefficient in real-world multi-client scenarios, where decentralized organizations (e.g., hospitals, financial institutions) independently update overlapping knowledge, leading to redundant mediator knowledge vector (MKV) computations and privacy concerns. To address these challenges, we introduce Federated Locate-then-Edit Knowledge Editing (FLEKE), a novel task that enables multiple clients to collaboratively perform LEKE while preserving privacy and reducing computational overhead. To achieve this, we propose FedEdit, a two-stage framework that optimizes MKV selection and reuse. In the first stage, clients locally apply LEKE and upload the computed MKVs. In the second stage, rather than relying solely on server-based MKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine similarity, enabling knowledge re-edit and minimizing redundant computations. Experimental results on two benchmark datasets demonstrate that FedEdit retains over 96% of the performance of non-federated LEKE while significantly outperforming a FedAvg-based baseline by approximately twofold. Besides, we find that MEMIT performs more consistently than PMET in the FLEKE task with our FedEdit framework.", "sections": [{"title": "1 Introduction", "content": "Locate-then-Edit Knowledge Editing (LEKE) has emerged as a key paradigm for updating large language models (LLMs) by directly identifying and modifying model parameters associated with newly acquired knowledge, eliminating the need for costly full-model retraining (Meng et al., 2022a,c; Gupta et al., 2024). It has proven effective in mitigating hallucinations (Huang et al., 2024), detoxifying outputs (Wang et al., 2024), and improving factual recall (Zhang et al., 2024).\nHowever, existing methods are all conducted in single-client scenarios. Considering real-life applications, as shown in Fig. 1(a), traditional LEKE methods suffer from redundant gradient descent computations of mediator knowledge vectors (MKVs) (Meng et al., 2022a,c; Li et al., 2024a), leading to inefficiencies in knowledge updates, especially for organizations within the same domain (e.g., different hospitals) that often process overlapping information. This not only exacerbates these inefficiencies but also raises privacy concerns due to data sharing (El Ouadrhiri and Abdelhadi, 2022; Yazdinejad et al., 2024). To mitigate these issues, federated learning (FL) (Kone\u010dn\u1ef3, 2016; McMahan et al., 2017a; Yang et al., 2018) enables collaborative model training while preserving data privacy, making it particularly suitable for such sensitive domains like healthcare and finance.\nTo extend the LEKE task to federated settings, we propose a new task: Federated Locate-then-Edit Knowledge Editing (FLEKE), enabling multiple clients to collaboratively edit knowledge while reducing computational costs and preserving privacy. As shwon in Fig. 1(b), In FLEKE, each client runs the LEKE algorithm locally to generate MKVs representing knowledge updates. These MKVs are uploaded to a central server, where they are stored and shared, preventing redundant computations. When a predefined time slot arrives, clients retrieve relevant MKVs from the server to refine their knowledge.\nTo accomplish FLEKE, several critical challenges need to be addressed: (1) How to define MKVs for client update: Unlike traditional LEKE in federated settings, which requires frequent recalculation of MKVs for the same knowledge, FLEKE computes them only once and shares them across clients via a central server, so selecting appropriate MKVs for upload is crucial. They must effectively encode essential knowledge while remaining computationally efficient. (2) How to retrieve relevant MKVs for client download: Efficient retrieval is crucial to minimize computational and storage costs while ensuring clients access only the most relevant MKVs. A key issue is dynamically selecting MKVs that best match each client's needs, balancing retrieval efficiency and knowledge quality.\nTo address the first challenge, we explored various representations for mediator knowledge vectors (MKVs) and found \\( z_i \\) vectors introduced in Meng et al. (2022c) to be suitable. As shown in Fig. 2, our analysis of the zsRE dataset (Levy et al., 2017) identified 2,000 edit pairs where the cosine similarity between the core text and corresponding \\( z_i \\) vectors exceeded 0.65, indicating shared information. Statistical validation confirmed a strong positive correlation, with a Pearson coefficient of 0.74 (Cohen et al., 2009). These findings show that \\( z_i \\) vectors effectively encode original knowledge while improving computational efficiency, making them well-suited as MKVs.\nTo address the second challenge, we propose FedEdit, it operates in two stages: first, at predefined intervals, clients apply existing LEKE algorithms to update multiple layers of their models, uploading the computed MKVs to the server. Then, in the re-editing stage, clients periodically evaluate the similarity between their local data and the vectors stored on the server. And a re-editing condition is established, if the similarity meets a predefined threshold, the server's vectors can be reused for further editing, allowing clients to refine their models without redundant computations.\nWe reorganize two large-scale counterfactual datasets zsRE and COUNTERFACT (Meng et al., 2022a) to simulate the FLEKE task,. Extensive experiments on GPT-J (6B) (Wang and Komatsuzaki, 2021) and GPT-NeoX (20B) (Black et al., 2022) show that even in the FLEKE setting, the proposed FedEdit method retains at least 96% of the performance of state-of-the-art methods in non-federated environments. The key contributions of this work are summarized as follows:\n1) We introduce FLEKE, a task enabling multi-client collaborative knowledge editing in dynamic scenarios. To the best of our knowledge, this is the first work to apply LEKE in the federated setting.\n2) We introduce FedEdit, a two-stage editing framework designed to improve multi-client editing efficiency for related knowledge, where a re-editing condition is established to efficiently select mediator knowledge vectors from the server.\n3) We reorganize the zsRE and COUNTERFACT datasets to simulate FLEKE. Experimental results show that, under FLEKE conditions, FedEdit achieves performance at or above 96% of that of state-of-the-art methods in non-federated settings."}, {"title": "2 Related Work", "content": "Locate-then-Edit Knowledge Editing. The locate-then-edit approach in knowledge editing identifies and modifies specific weights in pre-trained models to achieve desired outputs (Mitchell et al., 2022; Yao et al., 2023). Various methods have been proposed within this framework. ROME (Meng et al., 2022b) updates the feedforward network to encode new knowledge, while MEMIT (Meng et al., 2022d) extends this for large-scale editing. PMET (Li et al., 2024b) enhances MEMIT's performance with a residual attribution strategy. Additionally, ROME (Meng et al., 2022b) and MEMIT (Meng et al., 2022d) use input prompts to locate and edit knowledge neurons. However, existing works do not address multi-client scenarios and multi-editing tasks. In this paper, we propose a federated locate-then-edit knowledge editing framework to improve editing efficiency in such settings.\nFederated Learning in LLMs. Research on combining large language models (LLMs) and federated learning (FL) primarily focuses on pre-training and prompt engineering (Chen et al., 2024). Pre-trained models, trained on large datasets, serve as a foundation for FL, significantly reducing training time (Tan et al., 2022; Liu et al., 2024) and helping address data and system heterogeneity (Nguyen et al., 2022). Some studies incorporate pre-trained models into FL frameworks for various tasks (Agarwal et al., 2023; Zhang et al., 2023). Prompt-based techniques have shown strong performance in LLMs (Guo et al., 2023). The pFedPT framework personalizes models efficiently using personalized prompts (Li et al., 2023), while DiPrompT (Bai et al., 2024) applies adaptive prompts to tackle domain generalization challenges in FL. To the best of our knowledge, this is the first work to apply FL for optimizing LEKE in LLMs."}, {"title": "3 Method", "content": "In this section, we provide a detailed introduction to the FLEKE task and the FedEdit framework. First, we discuss the relationship among the hidden states of each Transformer layer in the LLM and the relationship between the hidden states and the input in section 3.1, which is essential for calculating the MKVs. Next, we introduce the FLEKE task and explain its connection to the LEKE task in section 3.2, and we also analyze how to optimize and solve it. Then, we focus on solving the LEKE task and extracting the relevant knowledge vector in section 3.3. Finally, we propose the FedEdit framework to address the FLEKE task in section 3.4."}, {"title": "3.1 Preliminaries", "content": "This section introduces the foundational concepts of autoregressive and decoder-only LLM models, focusing on the relationship between the hidden states of each Transformer layer and the input. These foundations are essential for calculating the MKVs.\nAutoregressive and decoder-only LLMs denoted as \\( F_{\\theta} \\) encode input sequences \\( x \\) into \\( z \\) token sequences \\( x_1, ..., x_z \\), which are processed through \\( L \\) Transformer decoder layers. The probability of the next token \\( x_{z+1} \\) is computed as:\n\\[F_{\\theta}(x_1, ..., x_z) = softmax \\left( \\psi \\left( W_E \\gamma \\left( h_L + a_L + m_L \\right) \\right) \\right) \\\\\n= P(x_{z+1} | x_1, ..., x_z), \tlabel{eq:1}\\]\nwhere \\( W_E \\) and \\( \\gamma \\) are the embedding matrix and layer norm, respectively, and \\( a_l \\), \\( m_l \\) are the hidden states of the MHSA and FFN at the \\( l \\)-th layer. \\( a_l^j \\), \\( m_l^j \\) for the \\( j \\)-th token at layer \\( l \\) are:\n\\[a_l^j = W_{\\theta}^{MHSA} \\mathcal{M}HSA\\left( \\gamma \\left( h_{l-1}^j, \\left\\{ h_{l-1}^{j'} \\right\\}_{j'=1}^{j-1} \\right) \\right), \tlabel{eq:2}\\]\n\\[m_l^j = W_{\\theta}^{FFN} \\gamma \\left( W_I \\gamma \\left( h_{l-1}^j \\right) \\right),\\]\nwhere \\( W_{\\theta}^{MHSA} \\) and \\( W_{\\theta}^{FFN} \\) are weights for MHSA and FFN, and \\( \\gamma \\) is the activation function."}, {"title": "3.2 FLEKE Task Formulation", "content": "In this section, we present the FLEKE task and explain its connection to the traditional LEKE task. The FLEKE refers to the collaborative execution of the LEKE task by multiple clients in a federated scenario. Assuming that each client \\( c \\) has a fact data set \\( E_c \\) to be edited in time slot \\( t \\), the goal of FLEKE is to insert the fact data \\( E_c \\) of all clients by editing the internal parameters of LLM. Overall, for each client \\( c \\) between predefined time slots, FLEKE optimizes an objective function to obtain target weights (Meng et al., 2022c):\n\\[W_{t, k_l}^c = argmin_{W_{t, k_l}^c} \\sum_{i=1}^{n} ||W_{t, k_l}^c - v_{i}||_2^2 + \\lambda \\sum_{i=n+1}^{n+u} ||W_{t, k_l}^c - v_{i}||_2^2\\]\nHere, \\( k_{i,l}^c \\) and \\( v_{i} \\) represent the sets of keys and values, respectively, encoding the subject-related knowledge in the \\( l \\)-th layer at time \\( t \\) on client \\( c \\). The term \\( \\sum_{i=1}^{n} ||W_{t, k_l}^c - v_{i}||_2^2 \\) indicates that we aim to retain \\( n \\) pieces of knowledge, while \\( \\lambda \\sum_{i=n+1}^{n+u} ||W_{t, k_l}^c - v_{i}||_2^2 \\) suggests that we intend to modify a much larger number of knowledge pieces, denoted as \\( u \\gg 1 \\). Here, the keys and values are represented as matrices stacked horizontally: \\( K = [k_{1,l}^c ... k_{n,l}^c] \\) and \\( V = [v_{1,l}^c ... v_{n,l}^c]^\\mathsf{T} \\). The target weight \\( W_l \\) is the sum of the original weight \\( W_l \\) and the incremental weight \\( \\Delta_l \\), i.e., \\( W_l' = W_l + \\Delta_l \\). Based on the derivation from MEMIT (Meng et al., 2022c), the formal expression for the incremental weight is given as:\n\\[\\Delta_l^c = R_l \\left( C_0 + \\lambda K_l K_l^\\mathsf{T} \\right)^{-1},\tlabel{eq:3}\\]\nwhere \\( R_l = V_l' - W_l K_l \\) represents the residual between the values \\( V_l' \\) (namely the target knowledge representations) corresponding to the keys \\( K_l \\) of the target knowledge and the client \\( c \\) model's original knowledge \\( W_l K_l \\). \\( C_0 = \\mathbb{E}_{K_l} [k k^\\mathsf{T}] \\) is an estimate of the set of previously memorized keys obtained through sampling, and \\( \\lambda \\) is a hyperparameter that balances the degree of model modification and preservation."}, {"title": "3.3 LEKE", "content": "This section delves into the LEKE method, emphasizing how knowledge updates are performed across multiple layers of the Transformer. For instance, as shown in Fig. 3, MEMIT (Meng et al., 2022c) employs optimized transformer layer hidden states to perform subtle updates on the FFN weights. In contrast, PMET (Li et al., 2024a) simultaneously optimizes the transformer component hidden states of both MHSA and FFN, but only applies the optimized TC hidden states to the FFN. In this paper, we take MEMIT as an example of a LEKE method and further elaborate on its approach to updating multiple layers in the FLEKE task. Specifically, we calculate the target knowledge set of the first and last critical layer \\( L_0 = \\min(R) \\), \\( L_l = \\max(R) \\). For each edit \\( (S_{ci}, r_{ci}, O_{ci}) \\in E_c \\) (sucbject \\( s \\), relation \\( r \\), object \\( o \\)) on client \\( c \\), we (i) compute \\( z_{ci} \\) to replace \\( h_l \\) such that adding \\( \\delta_{ci} \\equiv z_{ci} - h_l \\) to the hidden state at layer \\( L \\). Then, for each layer, we (ii) modify the MLP at layer \\( l \\) by spreading \\( \\Delta_{tl} \\) over layer \\( l \\).\n(i) Computing \\( z_{ci} \\). For the \\( i \\)-th edit on client \\( c \\), \\( z_{ci} \\) is derived by optimizing the residual vector \\( \\delta_{ci} \\) via gradient descent:\n\\[z_{ci} = h_l + \\underset{\\delta_{ci}}{\\operatorname{argmin}} - \\frac{1}{M} \\sum_{j=1}^M log P_{F_c} \\left( h_i + \\epsilon \\delta_{ci} \\right) [O_{ci} | x_{cj} \\oplus \\rho(S_{ci}, r_{ci})].\tlabel{eq:4}\\]\nIn words, we optimize \\( \\delta_{ci} \\) to maximize the client \\( c \\) model's prediction accuracy for the desired object \\( o_{ci} \\), given a set of factual prompts \\( \\{ x_{cj} \\oplus \\rho(S_{ci}, r_{ci}) \\} \\) that concatenate random prefixes \\( x_{cj} \\) to a templated prompt to aid generalization across contexts. \\( F_c(h_i + \\epsilon = \\delta_{ci}) \\) indicates that we modify the transformer execution by substituting the modified hidden state \\( z_{ci} \\) for \\( h_i \\).\n(ii) Spreading \\( \\Delta_{tl} \\) over layer \\( l \\). We follow the same algorithm steps as MEMIT that are presented in Algorithm 2 in Appendix C. Next, we'll mainly describe how to implement our FedEdit framework with the update step."}, {"title": "3.4 FedEdit Framework", "content": "In this section, we propose the FedEdit framework to address the FLEKE task in a federated setting. The framework is designed to adapt LEKE tasks to a federated scenario, where each client interacts with the server and collaboratively edits the knowledge. As shown in Fig. 4, the workflow of the FedEdit framework is as the following steps:\nStep (1): Starting at \\( t = 0 \\), each client runs the Edit algorithm locally, which can be any LEKE method. In this paper, we select MEMIT (Meng et al., 2022c) and PMET (Li et al., 2024a). This process generates the MKVs.\nStep (2): The MKVs are then uploaded to the server.\nStep (3): At a predetermined time slot, each client selects some MKVs from the server according to the re-editing conditions defined later.\nStep (4): If at least one vector is chosen by the client, it continues editing on the model.\nOn the timeline, steps (1) and (2) occur within the intervals between given time slots, while steps (3) and (4) are executed when the predetermined time slot is reached.\nFurthermore, to define the MKVs and the re-editing conditions, we summarize our framework FedEdit in Algorithm 1, which consists of two main steps:\n(i) Editing. Between the time slots in T, each client executes the Edit algorithm parallelly and independently (Step (1)). Here we still take MEMIT as an example i.e., Algorithm 2 in Appendix C. In this algorithm process, \\( z_{ci}, k_l \\) are related to the data records \\( E_c \\), and we define the mediator knowledge vectors (MKVs) of client \\( c \\) at time \\( t \\) as \\( Z_t \\):\n\\[Z_t = \\{ (z_{ci}, k_l) \\},\tlabel{eq:5}\\]\nwhere \\( (z_{ci}, k_l) \\) are all generated by client c during the time interval from \\( t - 1 \\) to \\( t \\), \\( (S_{ci}, r_{ci}, O_{ci}) \\in E_c \\), and the keys \\( k_l \\) at the \\( l \\)-th layer are defined as follows (Meng et al., 2022c):\n\\[k_{l_o}^{ci} = \\frac{1}{M} \\sum_{j=1}^{M} k(x_{cj} + S_{ci}),\tlabel{eq:6}\\]\nwhere \\( k(x) = \\sigma(W_l(h_{l-1}(x))) \\). Once a client has finished editing, it uploads the obtained \\( Z_t \\) to the server (Step (2)).\n(ii) Re-editing. Once the time reaches any time \\( t_i \\in T(i = 1, ..., m, m \\) is the total number of time slots), where \\( m \\) is the server \\( s \\) distributes the previously stored \\( Z_t \\) between \\( t_{i-1} \\) to \\( t_i \\) to each client. Each client selects \\( Z_{t_i} \\) from the \\( Z_t \\) that are beneficial to it, i.e., positively correlated with its own data \\( E_c \\) indirectly, through the \u201cre-edit\u201d condition:\n\\[\\sum(\\text{similarities} > \\alpha) \\ge \\frac{\\text{len(similarities)}}{2}\tlabel{eq:7}\\]\nwhere \\( similarities \\) is the cosine similarity between the \\( q \\)-th traversed \\( Z_t \\) in the server and \\( Z_{t_i} \\), i.e., line 10 of Algorithm 1. \\( \\alpha \\) means similarity threshold, which is a hyperparameter. \\( \\sum(\\text{similarities} > \\alpha) \\) is the number of \\( Z_t \\) in \\( Z_t \\) that satisfy the similarity threshold requirement. \\( \\text{len(similarities)} \\) is the number of all \\( (z_{ci}, h_i, k_l) \\) in the client \\( c \\) as of the current time slot \\( t \\). In summary, iterates through each \\( Z_t \\) in the server, calculates the cosine similarity between the \\( Z_t \\) and the \\( Z_t \\) of client \\( c \\), and if more than half of the \\( (z_{ci}, h_i, k_l)'s \\) in client \\( c \\) are greater than the similarity threshold \\( \\alpha \\), then the \\( Z_t \\) is said to satisfy the current client's \u201cre-edit\u201d condition. Then the \\( Z_t \\) will be selected by client \\( c \\) (Step (3)).\nWhen the screening process is finished, each client performs Algorithm 2 again on the basis of the model edited_model that has been edited earlier (Step (4)). The process is repeated until \\( t_i = t_m \\)."}, {"title": "4 Experiments", "content": "Experimental Setup\nDatasets. We conducted counterfactual update experiments on two datasets: Zero-Shot Relation Extraction (zsRE) (Levy et al., 2017) and COUNTERFACT (Meng et al., 2022a). The zsRE dataset contains 10,000 real-world facts (Meng et al., 2022c), while COUNTERFACT includes 21,919 factual statements (Meng et al., 2022a). To simulate FLEKE, we reorganized the datasets using different clustering methods. For zsRE, we clustered data based on the \"src\" value, which represents the subject (e.g., \"What university did Watts Humphrey attend?\" with the subject \"Watts Humphrey\"). We applied spectral clustering after transforming the text into word vectors to assign data to different clients. For COUNTERFACT, we grouped data with the same \"relation id\" into one client, and randomly assigned about 1/10 of the data from other clients to each client.\nBaselines. We select six knowledge editing methods as baselines: (1) FT-W is a simple fine-tuning approach that applies weight decay to prevent forgetfulness. (2) MEND (Mitchell et al.) transforms the fine-tuning gradient of an updated fact by decomposing the weight matrix into rank-1 form using a pre-trained hyper-network. (3) ROME (Meng et al., 2022a) locates factual retrievals within a specific set of MLP modules and updates knowledge by directly writing new key-value pairs into the MLP module. (4) MEMIT (Meng et al., 2022c) extends ROME to insert multiple memories by modifying the MLP weights of several critical layers. (5) PMET (Li et al., 2024a) uupdates FFN weights by optimizing the hidden states of both MHSA and FFN, using only the FFN hidden states for weight updates. (6) EditAvg is a variant of FedAvg for solving the FLEKE task, where any LEKE method can replace \"Edit.\" Please refer to Appendix B for the detail settings.\nMetrics. Following Meng et al. (2022a), we use GPT-J (6B) (Wang and Komatsuzaki, 2021) and GPT-NeoX (20B) (Black et al., 2022) as the backbone for FLEKE. Following prior work (Meng et al., 2022c), we evaluate models using the following metrics: (1) Efficacy, measuring editing success; (2) Paraphrase, assessing success on rephrasings of the original statement; (3) Specificity, ensuring unrelated facts remain unchanged; and (4) Score, the harmonic mean of these three metrics, balancing reliability (efficacy and paraphrase) and specificity. Additionally, in COUNTERFACT experiments, we include (5) Fluency, evaluating degradation due to repetition, and (6) Consistency, measuring semantic coherence in generated text. All results are weighted averages across clients.\nHyper-parameters. We set the number of clients to 8, with a total of approximately 10,000 edits, and define T to consist of 10 time slots. Covariance statistics are collected on GPT-J using 100,000 samples from Wikitext, and on GPT-NeoX using 50,000 samples from Wikitext. Please refer to Appendix D for more details."}, {"title": "4.2 Results of COUNTERFACT", "content": "Table 1 presents the results of all methods on 10K counterfactual edits. FedMEMIT and FedPMET achieve 99.2% and 97% of the performance of centralized methods, respectively. In contrast, applying the FedAvg algorithm to MEMIT and PMET results in only 73.3% and 41.6%, respectively. This demonstrates that our method performs well in FLEKE. It also highlights that simply combining federated learning algorithms like the classical FedAvg with knowledge editing methods does not yield effective results. In the trade-off between editing reliability and specificity, FedMEMIT and FedPMET, like MEMIT and PMET, prioritize reliability. On the other hand, MEND, MEMITAvg, and PMETAvg focus more on specificity. Moreover, FedMEMIT and FedPMET outperform non-federated methods in terms of specificity and generalization, respectively. However, in terms of specificity, they fall behind the meta-learning-based method MEND.\nNext, we applied the FedEdit framework to perform 10K edits on GPTNeoX (20B) using the COUNTERFACT dataset. The results are shown in the lower part of Table 1. We find: FedMEMIT and FedPMET significantly outperform MEMITAvg and PMETAvg, consistently favoring reliability and consistency. Additionally, both FedMEMIT and FedPMET surpass their respective non-federated methods in generalization. This may be due to our proposed \"re-edit\" condition, which selects data with similar types for re-editing, thereby enhancing reliability. We further explore this in the following ablation experiments."}, {"title": "4.3 Results of ZSRE", "content": "The zsRE dataset tests the ability to add correct information. The results of editing 10K knowledge on the zsRE dataset are shown in Table 2. These results demonstrate that our method performs very close to the original method in the federated scenario, both in efficacy and generalization metrics, and even slightly outperforms it in terms of specificity. Specificity refers to the model's argmax accuracy on a randomly sampled, unrelated fact that should not have changed (Meng et al., 2022c). EditAvg (MEMITAvg and PMETAvg), averages the \\( \\Delta t \\) of each client before inserting it into the server's model, making it naturally stable in the case of random sampling. Additionally, compared to the original method, our approach includes an extra re-editing step. This step allows for editing additional vectors that are more suitable for the current client, improving performance."}, {"title": "4.4 Ablation Study", "content": "The ablation study in Table 3 examines the impact of removing the re-editing condition control from the FedMEMIT and FedPMET methods on their performance across different editing scales (1K, 5K, and 10K edits). The results show that: (1) The re-editing condition is crucial for the FLEKE task. Removing it causes a decline in the score for all FedEdit-related experiments, indicating that the model updates facts unrelated to its own data. This negatively impacts the model's ability to edit its own knowledge accurately. (2) The more similar the knowledge edited on a single client, the better the model's reliability. A clear trend emerges: reliability (efficacy and paraphrasing) decreases more, while specificity decreases less. This suggests that as the number of client edits increases, the re-editing condition improves reliability."}, {"title": "4.5 Robustness Study", "content": "We conducted a robustness study on the proposed framework using the zsRE dataset. Specifically, we assigned 10,000 facts to 2, 3, 4, 5, 6, 7, 8 clients for editing. As shown in Fig. 5, the experimental charts show that as the number of clients increases, FedMEMIT consistently performs well and remains stable across all metrics. In contrast, while FedPMET's performance improves initially, it declines as the number of clients grows, likely due to the effects of multiple edits. Notably, the Specificity indicator fluctuates with the number of clients, which may be influenced by the number of single edits."}, {"title": "4.6 Impact of the Number of Time Slots", "content": "Fig. 6 illustrates the superior performance of FedMEMIT compared to FedPMET and the ablation variants, FedMEMIT-w/o and FedPMET-w/o. While both FedMEMIT and FedPMET show a decline in performance as the number of time slots increases, FedMEMIT consistently outperforms FedPMET across all metrics. FedMEMIT achieves higher scores, maintains better efficiency, and demonstrates more stable paraphrasing quality and specificity, especially in long-term editing tasks. The gradual decline in FedMEMIT indicates its better ability to preserve edit quality over time, compared to the more significant performance drops seen in FedPMET. This highlights FedMEMIT's robustness, showing advantages in efficiency, editing reliability, and specificity preservation. Although FedMEMIT-w/o shows improvements in stability, FedMEMIT remains the most effective for achieving high-quality, sustainable editing performance in federated scenarios."}, {"title": "4.7 Case Study", "content": "Table 4 demonstrates that FedMEMIT correctly generates text in both cases, in contrast to MEMITAvg. This highlights the limitations of selecting \\( \\Delta t \\) as the mediator vector and validates the appropriateness of choosing \\( z_c \\) as the mediator vector. Moreover, the table illustrates a scenario where two highly similar cases are edited on two different clients. Specifically, case 15874 on client 1 is first edited, and the resulting \\( z_c \\) vector is uploaded to the server. Client 2 then retrieves this vector from the server and edits it. As a result, when a similar case (case id: 15911) is edited, the text is generated correctly. However, if the vector has not been edited, client 2 generates incorrect text. This further demonstrates the effectiveness of FedEdit."}, {"title": "5 Conclusion", "content": "We introduce FLEKE, a novel task that enables collaborative knowledge editing across multiple clients while ensuring privacy and reducing computational costs. To achieve this, we propose FedEdit, a two-stage framework comprising editing and re-editing. In the editing stage, clients locally perform knowledge editing and upload MKVs to a central server. In the re-editing stage, clients retrieve relevant MKVs via cosine similarity for further refinement. Experimental results demonstrate that FedEdit outperforms strong baselines in FLEKE, paving the way for more effective knowledge editing in federated settings and inspiring future research in this direction."}, {"title": "Limitation", "content": "We acknowledge the following limitations in our work: (1) The FLEKE task may face challenges due to non-IID data across clients. The heterogeneous data distributions can cause instability in the model, particularly when personalization is required for different tasks. While we have addressed this issue through the FedEdit framework, which uses clustering for selecting MKVs and re-editing conditions to improve the knowledge editing process, it remains a challenge in environments with diverse data. (2) Our work focuses on a simulated federated learning scenario, and thus does not account for certain external factors, such as environmental changes or system anomalies, that may impact the performance of the deployment in real-world settings. We plan to conduct additional experiments to further explore these challenges."}, {"title": "Ethics Consideration", "content": "In the development and application of federated learning systems, we prioritize ethical sourcing and privacy protection. Our proposed FLEKE task ensures that the research complies with data privacy regulations, and all datasets used in this study (zsRE and COUNTERFACT) are open-source and publicly available. These datasets do not contain any personally identifiable information or sensitive data. To mitigate privacy risks, our proposed FedEdit framework ensures that only mediator knowledge vectors (MKVs) are uploaded to the server, rather than raw data. This design ensures that sensitive data is never directly shared, and knowledge editing is performed in a manner that prevents leakage of private information.\nAdditionally, while federated learning frameworks enable collaboration among different organizations, we acknowledge the importance of safeguarding intellectual property and ensuring fairness in model training. Our work is designed to facilitate efficient knowledge editing while preventing misuse or unintended consequences. As such, we have implemented careful oversight measures to ensure that the server-based aggregation of MKVs does not inadvertently expose confidential information. Furthermore, all experiments were conducted with transparency and respect for the principles of fairness and data protection. We do not authorize the use of the datasets for any commercial purposes, and our results are strictly intended for academic and research purposes. Our study demonstrates the potential of federated learning to enhance the efficiency and privacy of knowledge editing tasks, while adhering to ethical standards of data use and model deployment."}, {"title": "A Federated Learning", "content": "Training Objective. Federated learning aims to optimize the following objective function:\n\\[min_w F(w) = \\sum_{i=1}^N p_iL_i(w)\\", "a)": "."}]}