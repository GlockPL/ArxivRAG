{"title": "Learning to Synthesize Graphics Programs for Geometric Artworks", "authors": ["Qi Bing", "Chaoyi Zhang", "Weidong Cai"], "abstract": "Creating and understanding art has long been a hallmark of human ability. When presented with finished digital artwork, professional graphic artists can intuitively deconstruct and replicate it using various drawing tools, such as the line tool, paint bucket, and layer features, including opacity and blending modes. While most recent research in this field has focused on art generation, proposing a range of methods, these often rely on the concept of artwork being represented as a final image. To bridge the gap between pixel-level results and the actual drawing process, we present an approach that treats a set of drawing tools as executable programs. This method predicts a sequence of steps to achieve the final image, allowing for understandable and resolution-independent reproductions under the usage of a set of drawing commands. Our experiments demonstrate that our program synthesizer, Art2Prog, can comprehensively understand complex input images and reproduce them using high-quality executable programs. The experimental results evidence the potential of machines to grasp higher-level information from images and generate compact program-level descriptions.", "sections": [{"title": "1 Introduction", "content": "Humans can easily understand the procedure that generates an image, no matter the drawing or characters, which necessitates understanding its underlying structure. However, inferring the drawing process from only the final image presents significant challenges. These challenges stem primarily from the occlusion of shapes and inherent ambiguity, as multiple interpretations can often be equally valid. Recent research has proposed various definitions for the process leading to the final image, including sketch colorization [23,13,28,26], color segmentation [1] and time-lapse video generation [29]. Though these methods accomplished their tasks, their results still suffer from low resolution, distortion and noise to different degrees. There are also similar methods that aim to bridge the gap between images and other forms of description, such as vector graphics, by utilizing different types of parametric primitives, such as closed paths [18,16] or strokes [14,30]. Though these works produce promising vector-based results, they do not target to reason their generations. Instead, they approach the process more akin to vector-level segmentation. To address these issues, we design a graphics program that can comprehensively represent the drawing process, mirroring the methods used by artists with digital drawing tools (e.g., straight or curved lines, paint buckets and layer blendings). By introducing an executable graphics program, images can be represented as structured drawing commands, enabling their reconstruction at any resolution (see Fig. 1 as an example). Beyond its reconstruction capabilities, our proposed graphics program offers a representation of graphics that is not only readable and editable but also semantically meaningful. This makes it an ideal candidate for further applications, including drawing instruction.\nOur work builds upon recent developments in graphics program synthesis [5,19,8], which have demonstrated the potential of program synthesis in decomposing complex shapes into a series of commands. However, the recurrent inference of the drawing process for colored images remains largely unexplored, and is the main focus of this paper. Unlike most vectorization-based methods [16,12,14,30,18], our approach does not rely on a differentiable rasterizer for path optimization or supervision in the pixel space. Instead, we train a program synthesizer to generate codes directly from a single image input. Our experimental results indicate that Art2Prog outperforms state-of-the-art optimization-based vectorization approaches in reconstruction accuracy while executing the inferred programs. Additionally, our method is capable of representing more complex graphics compared to existing graphics program synthesizers.\nIn summary, the contributions of this paper are threefold:"}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Graphics program synthesis", "content": "The task of learning to synthesize 2D graphics programs is not novel, with numerous recent papers being focused on reproducing 2D binary shapes. [5,19,8] primarily focused on reproducing CSG-based shapes, which are binary representations of solid shapes formed by applying boolean operations to simple shapes like circles and rectangles on the canvas. These methods successfully reconstructed solid shapes comprising up to 20 objects. Similarly, [4] constructed complex shapes by stacking a pre-defined binary shape (bricks of different lengths) built on the idea of Lego bricks. Written in a subset of LATEX, [6] defined programs as line shapes (e.g., circle, rectangle and straight line) rendered on an empty canvas. Building on this concept, [7] conceptualized the program as controlling a 'pen' that draws binary lines on an empty canvas, and [27] followed a similar approach for reconstructing CAD sketches by sequentially drawing lines. Additionally, [9] introduced parameterized brushstrokes as programs and generated blurry paintings from photos.\nTo the best of our knowledge, our proposed Art2Prog is the first work that explicitly targets the inference of complex 2D graphics programs that include lines, colored surfaces, and overlapping layers with different blending modes."}, {"title": "2.2 Image vectorization", "content": "Different from image rasterization, vectorization is inherently more complex due to the potential non-uniqueness of its results. Traditional methods [3,11,21,22,25,10] normally build specific algorithm-based methods that rely on image segmentation to conduct vectorization. To address this issue, recent research has tried to leverage the power of learning-based approaches. Studies [16,12,14,30] approached vectorization by optimizing a fixed number of parametric strokes relying on differentiable rasterizers. However, these methods needed to fully account"}, {"title": "3 Graphics Programs", "content": "This section defines the domain-specific language (DSL) used for our 2D graphics program. As depicted in Table 1, our graphics program is structured hierarchically. A final image comprises several overlapping layers, each utilizing one of two distinct color blending modes. To construct each layer, multiple drawing commands must be executed sequentially:\n1. A Create (x, y) command that initiates a new layer on the current canvas and sets the starting position at coordinates (x, y).\n2. Multiple Straight (x, y, O) commands that draw continuous straight lines from the last position to a relative position (x, y). These commands also help define enclosed shapes that can be filled with color.\n3. Multiple Circle (x, y, r, dir, O) commands that draw circular arcs. These arcs extend from the current position to a specified relative position (x, y), defined by a radius r and a direction dir, which can be either clockwise or counterclockwise.\n4. A Fillcolor (C, O) command that applies the color C to an enclosed shape. This enclosed shape is determined by the line path resulting from an earlier operation O, which is defined by the arrangement of lines in the current layer.\nNormal (O, O) or Multiply (O, O) employs two distinct layer blending modes to connect a pair of object layers, denoted as O. All of the variables N, R, D and C in Table 1 are defined as tokens in this paper. We incorporate two types of layer blending modes 'normal' and 'multiply' to enhance visual variety in the output (controlled by corresponding tokens). To further constrain the search space, we divide the positions on a canvas into an 8\u00d78 grid. Additionally, instead of using separate tokens in color channels (RGBA), we build a color list for C that contains 54 different colors to choose from. This allows a single token to correspond to a wide range of colors, akin to a palette commonly used in modern drawing software and traditional paintings. In summary, we define our goal of 2D graphics program inference as follows: reconstructing an input image (at a resolution of 64 \u00d7 64) by executing an inferred graphics program."}, {"title": "4 Methodology", "content": "In this section, we begin with a detailed description of the model architecture in Section 4.1 and introduce the tokenization strategy in Section 4.2 that enables our language model to interpret graphics programs as a sequence of tokens. Subsequently, we will discuss the training process in Section 4.3, followed by an explanation of the inference pipeline in Section 4.4."}, {"title": "4.1 Model architecture", "content": "We developed a deep architecture capable of efficiently inferring an executable graphics program from a randomly drawn image under our defined DSL. The comprehensive neural design of our network is illustrated in Fig. 2(a), including two trainable modules: an image encoder (ResNet) to encode the executed image results and a program decoder (GPT-2) to learn the probability distribution of the tokenized program sequence. To bridge the gap between program syntax and semantics effectively, our approach integrates information from both image and program spaces. Inspired by REPL [5], the graphics program is executed sequentially, line-by-line, to generate intermediate images through a non-differentiable, off-the-shelf rasterizer. These images are then concatenated with the target image, creating an 8-channel input image to be fed into an image encoder. The encoder follows the architecture of ResNet without pretraining on other datasets.\nTo process the graphics program, we flatten it into a sequence of tokens, appending a special token '<NEXTLine>' at the end of each line to signify its termination. This tokenized program sequence is then embedded and concatenated with the image embeddings on a token-wise basis, as illustrated in Fig. 2(b). Given that each line of code can only be executed following the prediction of a '<NEXTLine>' token, each token within the same line of code is associated with the same executed image. Therefore, to enable the concatenation of images and programs in a practical manner, we duplicate the images to match the token length for each executed line of code. Only when a '<NEXTLine>' token is predicted is the execution result of the current program updated. We build our program decoder on the basis of an existing language model (GPT-2 [17]) to decode the concatenated embeddings of the current state into the program sequence for the next state. Additionally, we train our decoder from scratch without pretraining on other datasets. Consequently, we simplify the program inference problem by predicting the next token based on the current program sequence. The prediction distribution for a graphics program can thus be factorized as follows:\n$p(S|I_T) = \\prod_{k=1}^{K} p(t_k|g_\\theta(f_\\phi(I_T, I_j), t_j))_{j=1}^{k-1})$,\nwhere $t_1...t_k$ are the tokens in the target flattened program sequence S, K is the length of program sequence that varies across different programs, $g_\\theta$ is the sequence decoder (GPT-2), and $f_\\phi$ is the image feature extractor (we use ResNet-18 in this paper). $I_T$ and $I_j$ are the target image and canvas rendering at token $t_j$ respectively."}, {"title": "4.2 Tokenization", "content": "Art2Prog employs a unique tokenization strategy to convert a graphics program into a sequence of tokens, facilitating feature concatenation and sequence decoding. The graphics program P can be represented as P = ($O_1, ..., O_N$), where $O_i$ indicates the $i$th line of code in the program. As mentioned in Table 1, we first quantize arguments $X_i \\in$ {N, R, D, C} into distinct intervals as tokens, and similarly, assign tokens to command classes $C_i$ (such as create, sline, circle, color, normal, and mul). Therefore, a single line of code $O_i$ may contain 2 types of tokens: $O_i$ = ($C_i$, v, ..., v). Here, we do not tokenize the pointer to the former line $O_{i-1}$ because we execute our code line-by-line by default so that it can be simplified. For the layer combination commands normal and mul, our detokenizer refers to $O_{i-1}$ and the second last color command, which should indicate the end of a layer. Additionally, we introduce 3 special tokens {'STARTING', 'ENDING', '<NEXTLine>'} that indicate the start of a sequence, the end of a sequence, and the end of a line respectively."}, {"title": "4.3 Training", "content": "The primary training objective of our model is to minimize the cross-entropy loss for the predicted tokens at each position in the program sequence. Given the target program sequence S and a corresponding target image $I_T$, we train our model to minimize:\nl(\u015c, S) = CE(\u015c, S|IT; \u04e8),\nwhere CE() refers to the cross-entropy function, and \u015c is the output program sequence of our model.\nInspired by recent transformer-based language models, we shift the input sequence s to the right by one position, as shown in Fig. 2(a). Thus, the input sequence starts with a special token 'STARTING' and the output sequence ends with a special token 'ENDING'. For each line of code $O_n$, we assume that the preceding lines ($O_1,..., O_{n-1}$) have been correctly successfully inferred; thus, we execute and render these partial lines to produce n - 1 intermediate images ($I_1, ..., I_{n-1}$). As illustrated in Fig. 2(b), these intermediate images, concatenated with the target image, are fed to ResNet to generate n - 1 image embeddings. Since the lines are flattened to sequence before being fed into the GPT-2, the number of image embeddings should match the token length. Thus, we repeat each image embedding I to the length $L_0$ + 1, where $L_0$ is the token length of the code to which these embeddings pertain. The addition of 1 accounts for the additional special token '<NEXTLine>'. We then concatenate the extended image embeddings and program embedding in a token-wise manner prior to being fed into GPT-2."}, {"title": "4.4 Inference", "content": "As shown in Fig. 2(b), our model infers a single token at a time, beginning with the initial input sequence ['STARTING'] and terminating with the prediction of 'ENDING'. This design allows our model to infer graphics programs of varying lengths based solely on a target image as input. Our approach does not rely on specific search algorithms, such as beam search or Sequential Monte Carlo (SMC), which can significantly slow down the inference process. Instead, we employ a simple greedy search strategy while supporting early stopping if the program has already been correctly inferred (indicated by MSE = 0).\nThe implementation details of our graphics program inference scheme are shown in Algorithm 1. For each inference, we repeatedly conduct inference from empty until timeout. After that, we compare all of the generated programs by their $IoU_{rgba}$ distance from the target raster image to find the best match. The metric $IoU_{rgba}$ is defined as a modified version from IoU, which aims to measure the similarity of RGBA graphics with objects of different sizes."}, {"title": "5 Experiments and Results", "content": null}, {"title": "5.1 Data preparation and experiment settings", "content": "We collect data for training by randomly generating graphics programs with up to 10 layers with reference to the defined DSL as described in Section 3. During training, our model was exposed to approximately 6 million examples. We utilize the Adam optimizer with a learning rate of 1 \u00d7 10-3, using a batch size of 32 across one RTX 3090 GPU for all settings in Table 2. For evaluation, we built an eval set with 1000 generated data up to 13 layers in each program. Also, inspired by the design of IoU, we define a modified version to fairly compare the similarity among RGBA images with objects of different sizes:\n$IoU_{rgba}(\\hat{I}, I) = \\frac{\\sum_{p=1}^{P}(\\hat{I}_p = I_p)}{\\sum_{p=1}^{P}(\\hat{I}_p(A) > 0 and I_p(A) > 0)},$\nwhere \u00ce and I denote the predicted image and the target image, respectively. \u00cep and Ip indicate the pixel value of \u00ce and I at position p. \u00cep(A) and Ip(A) are the pixel values at the alpha channel of images \u00ce and I, which indicate the transparency in the RGBA color space."}, {"title": "5.2 Ablations", "content": "In order to ablate our architecture, we assessed the impacts of two key components in Art2Prog: the image encoder and the program generator. Additionally, we evaluated the influence of the training set on model performance. Our models are tested on an evaluation set comprising up to 13 layers. Thus, we conducted training on two different sets, one with layers up to 5 and another up to 10, to determine whether training with longer programs enhances the performance of the synthesizer.\nWe first compared the evaluation scores of different image encoders, including sequentially stacked Conv2d blocks with ReLU activation (CNNs) and ResNet-18. Notably, we introduced a minor modification to the original ResNet structure by removing Batch Normalization. This alteration led to a substantial improvement in performance. As indicated in the $A_{L=5}$ and $B_{L=5}$ of Table 2, compared with CNNs, ResNet demonstrates higher reconstruction accuracy ($IoU_{rgba}$, IoU and MSE) and the ability to precisely infer longer programs (with the highest rate of successfully inferred programs and maximum program length). When trained with longer programs (up to 10 layers), ResNet demonstrates a slight improvement in performance. Conversely, CNNs encounter difficulties in training under these conditions. By comparing the qualitative results in Fig. 4, it is evident that utilizing ResNet facilitates the construction of complex programs in the majority of scenarios. However, in specific instances, such as those illustrated in the third column of this figure, CNNs demonstrate superior performance in accurately reproducing a given image.\nSubsequently, we demonstrated the necessity of using GPT-2 as the program generator, as opposed to Pointer Network (PtrNet) [24]. When utilizing PtrNet as the generator, the program is not flattened but treated as separate lines of code as in [5]. We observed that GPT-2 consistently demonstrates superior accuracy,"}, {"title": "5.3 Comparison with the state-of-the-art methods", "content": "We compared our proposed method against two other state-of-the-art (SOTA) image vectorization methods: LIVE [16] and REPL [5]. We conducted this evaluation using a consistent dataset comprising 1000 data points, each generated under our DSL and containing up to 13 object layers per program. The results of this comparison are presented in Table 3 and Fig. 5."}, {"title": "6 Discussion", "content": "In this section, we examine the instances where our method failed to produce accurate predictions, as illustrated in Fig. 6. These examples demonstrate failures in replicating the input raster image through generated graphic programs, particularly highlighting our system's tendency to neglect minor details, as evident in the first column of examples in Fig. 6. To improve accuracy, enhancing the image encoding component to more effectively capture detailed image features may be beneficial.\nFailures in accurately determining the appropriate color blending mode or the exact color in scenarios where one layer completely encompasses another are depicted in the second column. These challenges stem from the ambiguity in distinguishing between blended colors and the dominant color of the upper layer. Incorporating a dedicated, trainable module specifically designed for layer combination might mitigate this issue.\nMoreover, in complex multi-layered images, our synthesis algorithm often misses segments, indicating difficulty in generating longer programs. This limitation points to potential advancements in program generation capabilities, possibly by exploring innovative architectural solutions or integrating more sophisticated large language models for future enhancement."}, {"title": "7 Conclusion", "content": "This paper presents a learning-based program synthesizer that aims to write a graphics program to represent the input image comprehensively. We propose a novel graphics program definition by separating the drawing steps towards a target RGBA image into several steps: (1) creating a new layer on the canvas,"}]}