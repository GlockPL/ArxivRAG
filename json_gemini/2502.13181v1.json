{"title": "RingFormer: Rethinking Recurrent Transformer with Adaptive Level Signals", "authors": ["Jaemu Heo", "Eldor Fozilov", "Hyunmin Song", "Taehwan Kim"], "abstract": "Transformers have achieved great success in effectively processing sequential data such as text. Their architecture consisting of several attention and feedforward blocks can model re- lations between elements of a sequence in par- allel manner, which makes them very efficient to train and effective in sequence modeling. Even though they have shown strong perfor- mance in processing sequential data, the size of their parameters is considerably larger when compared to other architectures such as RNN and CNN based models. Therefore, several approaches have explored parameter sharing and recurrence in Transformer models to ad- dress their computational demands. However, such methods struggle to maintain high perfor- mance compared to the original Transformer model. To address this challenge, we propose our novel approach, RingFormer, which em- ploys one Transformer layer that processes in- put repeatedly in a circular, ring-like manner, while utilizing low-rank matrices to generate input-dependent level signals. This allows us to reduce the model parameters substantially while maintaining high performance in a va- riety of tasks such as translation and image classification, as validated in the experiments.", "sections": [{"title": "Introduction", "content": "Transformer models, since their introduction (Vaswani et al., 2017), have dramatically trans- formed the landscape of deep learning, particu- larly excelling in tasks involving sequential data such as natural language processing (Brown et al., 2020; Radford et al., 2019) and machine translation (Ott et al., 2018). Not long after their inception, they have also shown strong performance in vari- ous other domains such as reinforcement learning (Chen et al., 2021), image classification (Dehghani et al., 2023; Dosovitskiy et al., 2020; Liu et al., 2021), object detection (Carion et al., 2020) and image generation (Jiang et al., 2021; Peebles and Xie, 2022; Zhang et al., 2022). Their core architec- ture, characterized by self-attention mechanisms and feedforward neural networks, enables effective handling of long-range dependencies and parallel processing of input sequences. The ability of this architecture to model intricate relationships within data has led to significant breakthroughs, making it a foundation model across many modern large- scale AI systems (Anthropic, 2023; Google, 2024; OpenAI et al., 2024; Touvron et al., 2023).\nHowever, the impressive capabilities of trans- former models come with substantial computa- tional and memory costs (Brown et al., 2020; Doso- vitskiy et al., 2020). The standard Transformer ar- chitecture consists of multiple layers, each contain- ing millions of parameters that need to be trained and stored. This results in high memory usage and significant computational demands, often requiring specialized hardware. Moreover, deploying these models in resource-constrained environments, such as mobile devices or edge computing scenarios, be- comes challenging due to their size and complexity. These limitations have spurred a growing interest in developing more parameter-efficient Transformer architectures (Dehghani et al., 2019; Pires et al., 2023) that can retain their powerful performance while being more accessible and less resource in- tensive.\nIn this paper, we introduce a Transformer archi- tecture that recurrently leverages a single shared Transformer block in a novel way by integrating input-dependent level signals at each block itera- tion, which are shown to be crucial for adapting the shared block to different stages of the model. The level signals are generated by depth-specific low-rank transformations applied to the input in the attention and feedforward layers within the Trans- former block. Our RingFormer model can also be viewed as stacking Transformer layers whose pa- rameters combine (1) a set of global parameters"}, {"title": "2 Background", "content": "2.1 Transformer Architecture\nThe Transformer architecture (Vaswani et al., 2017) comprises multiple layers of the same structure stacked together, with each layer consisting of two main modules: Attention and Feedforward Network described in Equations (1) and (2), respectively. Each of these modules is accompanied by residual connections and layer normalization. In addition, to provide information about the position of to- kens in the sequence, the Transformer model adds static sinusoidal or learnable positional encodings to the input embeddings. These encodings allow the model to capture the order within a sequence. The following equations describe the mechanism of two main modules:"}, {"title": null, "content": "Attention(Q, K, V) = softmax(\\frac{QKT}{\\sqrt{d}})V (1)\nFFN(x) = \\sigma(xWup + bup)Wdown + bdown (2)\nHere, Q, K, and V are the results of projecting the input vectors through their respective matrices. At- tention module can be classified into self-attention (when the Q, K, and V input vectors are the same) and cross-attention (when the Q input vector is different from the K and V input vectors), while the feedforward block consists of up-projection and down-projection transformations with non-linearity function \u03c3 between them.\nIt is well known that Transformer architecture follows a scaling law for both vision tasks and NLP tasks (Dehghani et al., 2023; Hoffmann et al., 2022). This scaling law demonstrates that the per- formance of Transformer models improves pre- dictably as the model size and computational re- sources increase. Due to the steep slope of the scal- ing law, the parameter sizes of Transformer models have continued to grow, leading to significant ad- vancements in their capabilities. However, this growth has also made training and using such mas- sive models increasingly infeasible without sub- stantial GPU resources."}, {"title": "2.2 Related Work", "content": "To address the challenge of requiring extensive hardware resources for large Transformer models, researchers have explored various methods to en- hance efficiency.\nOne approach is related to pruning of Trans- former model layers, which involves removing less important layers or weights to streamline the model. It was found that many deep layers in large lan- guage models are redundant (Gromov et al., 2024), and by pruning up to half of these layers, it was possible to significantly reduce the model size with minimal accuracy degradation.\nAnother strategy is sharing parameters across different layers or components in Transformers, re- ducing the model's complexity and memory usage. The Universal Transformers (Dehghani et al., 2019) introduces a model where parameters are shared across layers using a recurrent mechanism with layer-dependent positional encoding, which main- tains good performance in various NLP tasks while reducing the number of parameters. People have"}, {"title": "3 Method", "content": "3.1 Overview\nIn this section, we provide a detailed explanation of our proposed work, covering the specific details about the structure of our model.\nThe encoder or decoder Transformer-based mod- els consist of several layers with the same structure, where each layer is a combination of sub-layers such as attention and feedforward layers. Those models can be formulated in the following way:"}, {"title": null, "content": "F(x) = fn(fn\u22121(...f2(f1(x))))\n= f(f(...f(f(x, P1), P2)), PN\u22121),PN) (3)\nwhere N, F, f, x and pi denote the number of layers, entire encoder (or decoder), each encoder (or decoder) block, input and parameters of each ith layer, respectively. The general formulation of the recurrent Transformer model with level transition functions can be written as below:"}, {"title": null, "content": "F(x) = fn(fn\u22121(...f1(x)))\nfi(x) = fr(x, gi(x)) (4)\nwhere fr denotes the recurrent Transformer block and gi(x) represents a generic level transition func- tion specific for each level. In Universal Trans- formers (Dehghani et al., 2019), it was shown that using static spatio-temporal positional embed- dings can serve as level transition functions for the recurrent Transformer layer and have good model performance. Specifically, in that work, level transition function gi(x) can be represented as gi(x) = x+l(i, xp), where l is a function that re- turns a positional embedding vector based on level depth i and the position xp of the vector x, while the ith Transformer block function fi(x) can be represented as fi(x) = fr(gi(x)).\nBelow, we describe our way of constructing and integrating level transition function gi(x) to gener- ate adaptive level signals."}, {"title": "3.2 Adaptive Level Signals", "content": "To have effective transition between the levels when using recurrent Transformer block, we make gi(x) directly dependent on the input in the fol- lowing way: gi(x) = Mi \u00b7 x, where M is a learn- able transformation matrix. Since the main role of level signals is to nudge the input vectors in the right direction, which is an easier task compared to the main input transformation done by the re- current layer, we hypothesize that making the M matrix low-rank while keeping the recurrent layers at full-rank will let us have parameter-efficiency and high performance at the same time. We draw inspiration for such a low-rank matrix construc- tion and its weight initialization from the parame- ter efficient fine-tuning (PEFT) technique, LORA (Hu et al., 2021), and decompose Mi into two low- dimensional matrices, A\u017c and B\u2081 described in Equa- tion 5."}, {"title": null, "content": "Mi = Ai B, Ai, Bi \u2208 Rd\u00d7r and r\u226ad (5)\nSince a Transformer layer consists of an atten- tion block and a feedforward block, we generate two distinct signals gai(x) and gFi(x): one for the attention block and the other for the feedforward block, respectively. Additionally, since the Trans- former block also has layer normalization applied"}, {"title": "3.2.1 Attention Block", "content": "For the attention mechanism, which calculates rele- vance between elements of a sequence using three projection matrices (query, key, and value), we gen- erate level signals for each of those projections using separate low-rank matrices. We integrate signals after the projection of the input vector x by WQ, WK and Wy matrices (shared across the levels) in the following way:"}, {"title": null, "content": "Qi = WQ \u2022 x + 9AQi(x),\nK\u2081 = WK.x + 9\u0410\u043a\u0456(\u0445),\nVi = Wv. x + 9Avi(x), (6)\nwhere_gAQi(x) = MQi\u00b7x, 9Aki(x) = MKi.x,\n9Avi(x) = Mvi.x. By incorporating the level functions separately for Q, K, and V, we enable fine-grained control over depth-dependent modifi- cations to each component of the attention mecha- nism. Also, adding the level signals in this manner avoids direct input changes to the main recurrent projections, which was found to be beneficial in our experiments. This can be because such a direct"}, {"title": "3.2.2 Feedforward Block", "content": "For feedforward block, the projection of input to intermediate vector of this module requires rela- tively large number of parameters. Furthermore, there have been various explorations regarding the role of feedforward network in Transformers. One such study (Geva et al., 2021) argues that the feed- forward network can be interpreted as a key-value memory pair, where the matrix of the first linear layer is involved in the coefficients of input factors, and the matrix of the second linear layer relates to information about the training corpus. Considering parameter-efficiency and the previous finding, in our approach, for the feedforward network, we add signals before projecting the input using the up- projection layer to guide the coefficient formation of the input in the following way:"}, {"title": null, "content": "FFN(x) = \\sigma((x+gFi(x))Wup)Wdown (7)\nwhere gFi(x) = MFi \u00b7x, the function o is a non- linear function such as GELU (Hendrycks and Gim- pel, 2023), and the bias terms were omitted for brevity.\nIn encoder-decoder models, we iteratively reuse a single Transformer block consisting of attention"}, {"title": null, "content": "4 Experiments"}, {"content": "We evaluate the performance of our RingFormer model and baseline models across two tasks: ma- chine translation and image classification.\nFor the baseline models, we choose the vanilla Transformer model (Dosovitskiy et al., 2020; Vaswani et al., 2017), one recurrent transformer model, Universal Transformer (Dehghani et al., 2019), and one partially recurrent model, One Wide Feed Forward model (Pires et al., 2023), with spe- cific adaptations to the corresponding tasks de- scribed below."}, {"title": "4.1 Experimental Details", "content": "In this section, we provide detailed description of each downstream task to facilitate the reproduction of our experimental results. For all of our models, the rank of the decomposed matrices for the level signals is fixed at the input hidden dimension di- vided by 16. We perform ablations for the different ranks and show the results in Table 4.\nTranslation Transformer model is firstly pro- posed in translation task (Vaswani et al., 2017). Thus, we also test our model on the translation task, with two model sizes shown in Table 1. We train all models on WMT-14 (Bojar et al., 2014) German-English dataset which consists of 4.5M pairs of sentences. For evaluation, we calculate BLEU score (Papineni et al., 2002) for the WMT- 14 German-English test set and we employ BiBERT vocabulary with bi-lingual tokenizer with vocab size equal to 52K (Xu et al., 2021). We set the number of layer (iteration), batch size and entire training step as 6, 512 and 830K for base and large setting on two A100 80GB GPUs, respectively. In the training session, we used Adam (Kingma and Ba, 2017) optimizer with a cosine learning rate scheduler having 40K of warm-up steps. Also, we"}, {"title": null, "content": "Image Classification As the ViT (Dosovitskiy et al., 2020) model became very prevalent in the vi- sion domain, especially in image classification, we decided to test our model and other baseline mod- els on this task. The models are adjusted to have only encoder layers, which take image patches with a class token attached as an input, and perform the prediction using the hidden state of the class token from the last layer. For the Vision Transformer (ViT) model (Dosovitskiy et al., 2020), we stick to the original architecture, while for the Universal Transformer (Dehghani et al., 2019), static sinu- soidal spatio-temporal positional embeddings are used as level transition function between the levels in the encoder. For the One Wide Feed Forward model (Pires et al., 2023), the feedforward layer is shared across the levels, while the attention layer parameters are distinct for each level.\nWe first train smaller models on a subset of the original ImageNet-1K dataset (Deng et al., 2009) for 100 epochs. We randomly chose 100 classes with the total number of 100K training samples (1K per each class) from the original training set, and 5K testing samples (50 per each class) from the original validation set. For easy referencing, we call that subset ImageNet-small. As the size of the dataset is relatively small, we decided to train mod- els having only 6 layers / iterations (in the case of recurrent models, we say iterations or levels instead of layers). For bigger size models with 12 layers / iterations, we trained on the whole ImageNet-1K for 50 epochs due to limited resources.\nThe additional training and ImageNet-small dataset details are given in Appendix A.1 and A.3. The model hyperparameters, parameter size and ex- periment results on ImageNet-small and ImageNet- IK are given in Table 2 and 3."}, {"title": "4.2 Experimental Results", "content": "our design choice for level-signals is more effective than adding input-independent sinusoidal vectors."}, {"title": null, "content": "Translation The details of experimental results on translation are presented in Table 1. Our Ring- Former model achieves competitive performance with Vanilla Transformer model (Vaswani et al., 2017) and One Wide FFN model (Pires et al., 2023) with less number of parameters for base and large size models. RingFormer outperforms Universal model (Dehghani et al., 2019), while having sim- ilar parameter size. These results also imply that"}, {"title": null, "content": "Image Classification The experimental results for image classification are shown in Table 2 and 3. Using ImageNet-small, we conducted experi- ments on the ViT (Dosovitskiy et al., 2020) model, downscaled One Wide FFN (OWFd) (Pires et al., 2023), UiT (Dehghani et al., 2019) and our Ring- Former model. The results, presented in the up- per half of Table 2, indicate that the ViT model"}, {"title": null, "content": "achieves the highest accuracy, which is expected as it has more than four times the number of pa- rameters compared to the other models. However, our RingFormer model has the second best per- formance, outperforming the other models of the same size. In the below half of Table 2, where we scale all the models to the size of One Wide FFN model, our model shows the best performance, which shows the effectiveness of our approach.\nWe observed similar tendency when we trained bigger size models on the ImageNet-1K dataset, for which the results are shown in Table 3. When comparing the models with the same input hidden dimension and feedforward block dimension, ViT model showed the best result, but when we up- scaled our RingFormer model (RingFormers) to match the size of OWF model, it outperformed the two baseline models (OWF and UiT\u00b3), and also showed slightly higher performance compared to the ViT model.\nAdditionally, we calculated the forward GFLOPS for models with the same H/FF shown in Table 3, namely ViT, OWF, UiT and RingFormer. For an RGB input image of size 224x224 (applied with 16x16 patch size), ViT, UiT and OWF models ex- hibit similar computational costs of around 17.636 GFLOPS, while RingFormer requires slightly more computations at 19.03 GFLOPs. This increase is attributed to the additional depth-specific and input- dependent level signals used in RingFormer to im- prove performance while maintaining a lower pa- rameter count compared to standard Transformers."}, {"title": "Representation Similarity Analysis", "content": "To analyze representations across layers / iterations between the original Transformer model and other models,"}, {"title": null, "content": "Mean Attention Distance Analysis To study the qualities of attention heads in the vision models, we perform MAD analysis, which is conducted in in Table 3). We computed mean attention distances of 500 images ran- domly taken from the ImageNet-small validation set and took their average. The MAD analysis plots for each model above are shown in Figure 3.\nWe observe that, in the ViT model, different attention heads yield different attention distances suggesting they use both local and global informa- tion from an image. But as we go deeper in the Transformer blocks, the heads tend to focus more on global aggregate information. The same type of phenomenon occurs in the case of One Wide FFN model, which is expected as its attention layers are not recurrent. In the case of our RingFormer model, the properties of its attention heads are also very similar to those of the ViT model. It can be seen as a validation of our hypothesis that the level signals could successfully steer the behavior of a recurrent Transformer model as it goes through a series of iterations. When it comes to the Universal Trans- former model, it is found that the types of signals that exist in that model could not sufficiently help it simulate its attention module behavior as it is considerably different compared to that of the ViT model."}, {"title": "4.3 Ablation Study", "content": "In this section, we conduct an ablation study with various experiments on the translation task to vali- date the effectiveness of our proposed method. The"}, {"title": "5 Conclusion", "content": "In this paper, we introduce RingFormer, a parameter-efficient recurrent Transformer architec- ture that employs a single Transformer layer recur- rently while integrating input-dependent signal vec- tors created using low-rank matrices for each level. This approach significantly reduces the number of parameters while maintaining high performance in tasks such as machine translation and image classi-"}, {"title": null, "content": "fication. We hope that our research on enhancing recurrent Transformer with adaptive level signals can enable smaller organizations and research insti- tutions to train powerful models without the need for extensive computational resources, thus democ- ratizing access to advanced AI capabilities."}, {"title": "6 Limitations", "content": "Our approach introduces additional computations compared to the original Transformer due to the integration of depth-specific and input-dependent signals. However, this trade-off is necessary to maintain the performance of standard Transformers while significantly reducing parameter count com- pared to other recurrent Transformer models. Due to computational constraints, we were not able to conduct experiments on large-scale language mod- eling tasks, which require significantly more data and training resources, and our experiments were limited to relatively smaller scale models. While our design choices suggest that RingFormer should retain its advantages at larger scales, future work can focus on further validating its performance on billion-parameter models and explore its effective- ness in domains such as language modeling."}, {"title": "A Appendix", "content": "A.1 Implementation Details\nTranslation Models are trained based on the two size variations, base size and large size. The base size models are trained based on the following model configuration settings: 6 Transformer layers, 8 attention heads, 512 hidden dimension size, 2048 feedforward dimension with maximum sequence length 50. For training, their maximum learning rate is 7e-4 with 17K step cosine warm-up sched- uler and total 210K training steps on two A100 80GB GPUs. The large size models are trained based on the following model configuration set- tings: 6 Transformer layers, 16 attention heads, 1024 hidden dimension size, 4096 feedforward di- mension with maximum sequence length 50. For training, their maximum learning rate is 2e-4 with 17K step cosine warm-up scheduler and total 210K training steps on two A100 80GB GPUs."}, {"title": null, "content": "Image Classification For the models trained on ImageNet-small dataset, we used 224x224 image resolution, 16x16 patch size, 6 Transformer layers, 8 attention heads, learning rate of 1e-3, cosine learning rate scheduler with 2K warm-up steps, batch size of 1024, and training for 9775 steps (100 epochs) with one RTX 3090 GPU. For the models trained on ImageNet-1K dataset, we used the same image resolution and patch size as mentioned above, 12 Transformer layers, 12 attention heads, learning rate of 5e-4, cosine learning rate scheduler with 3128 warm-up steps (5 epochs), batch size of 4096, 16 gradient accumulation steps, and training for around 15650 steps (50 epochs) on two RTX 3090 GPUs.\nFor all models, we used dropout rate of 0.1, gra- dient clipping of 1.0 during training and GELU"}, {"title": "A.2 Additional Analysis", "content": "In Figure 4, we share the representation similar- ity analysis for big size models in the Translation task. This analysis also has been conducted under the same conditions as in the base size case. Sim- ilar with the results in Figure 2, One Wide FFN"}, {"title": "A.3 ImageNet-small Dataset", "content": "We sampled a subset of ImageNet-1K (Deng et al., 2009) that contains randomly selected 100 classes, with 100,000 images for training and 5000 images for testing, in order to perform experiments on smaller size models. In the project Github repository, we will share the names of all the sampled images for training and testing as a json file."}]}