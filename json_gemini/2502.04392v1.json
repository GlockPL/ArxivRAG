{"title": "Division-of-Thoughts: Harnessing Hybrid Language Model Synergy for Efficient On-Device Agents", "authors": ["Chenyang Shao", "Xinyuan Hu", "Yutang Lin", "Fengli Xu"], "abstract": "The rapid expansion of web content has made on-device AI assistants indispensable for helping users manage the increasing complexity of online tasks. The emergent reasoning ability in large language models offer a promising path for next-generation on-device Al agents. However, deploying full-scale Large Language Models (LLMs) on resource-limited local devices is challenging. In this paper, we propose Division-of-Thoughts (DoT), a collaborative reasoning framework leveraging the synergy between locally deployed Smaller-scale Language Models (SLMs) and cloud-based LLMs. DoT leverages a Task Decomposer to elicit the inherent planning abilities in language models to decompose user queries into smaller sub-tasks, which allows hybrid language models to fully exploit their respective strengths. Besides, DoT employs a Task Scheduler to analyze the pair-wise dependency of sub-tasks and create a dependency graph, facilitating parallel reasoning of sub-tasks and the identification of key steps. To allocate the appropriate model based on the difficulty of sub-tasks, DoT leverages a Plug-and-Play Adapter, which is an additional task head attached to the SLM that does not alter the SLM's parameters. To boost adapter's task allocation capability, we propose a self-reinforced training method that relies solely on task execution feedback. Extensive experiments on various benchmarks demonstrate that our DoT significantly reduces LLM costs while maintaining competitive reasoning accuracy. Specifically, DoT reduces the average reasoning time and API costs by 66.12% and 83.57%, while achieving comparable reasoning accuracy with the best baseline methods.", "sections": [{"title": "1 Introduction", "content": "As web content continues to grow exponentially, on-device AI assistants have become essential tools for helping users navigate the increasingly complex online landscape. This trend has led to the widespread adoption of personal assistants such as Google Assistant, Apple Siri, Amazon Alexa, Alibaba Tmall Genie, and Xiaomi Xiao AI [16, 26], which have demonstrated their effectiveness in helping users digest enormous web content for tasks like web browsing [4, 19, 45], content searches [34], online shopping [28], and travel planning [7, 33]. These AI-powered agents enable web applications to harness the rapid advancements in AI technology, delivering a more personalized and convenient user experience. Amid recent AI breakthroughs in Large Language Models (LLMs), the emergent capabilities of commonsense reasoning [38] and in-context learning [8] are widely regarded as a key component for the next generation of on-device agents [12]. Therefore, revolutionizing AI personal assistants with LLM agents has become an important research problem and a critical focus for applications [21, 22].\nHowever, deploying LLM agents on local devices presents significant challenges, as it is impractical to run LLMs with trillions of parameters on resource-constrained devices such as smartphones and personal computers [41]. Conversely, relying solely on cloud-based commercial LLMs raises concerns over privacy risks, unreliable connections, and high monetary costs [22]. Recent research has focused on training smaller-scale language models and developing model compression techniques [23, 24, 40], with the goal of creating sub-10B parameter models that can be practically deployed on local devices, such as Llama 3 series [10] and Phi 3 series [1]. However, this approach introduces additional computational costs for training or compressing these models and inevitably results in"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Reasoning with LLMs", "content": "Recent years have seen remarkable advancements in LLMs, with increasing model scale leading to powerful emergent reasoning capabilities. [37, 39]. Moreover, various prompt engineering techniques have been proposed to further extend LLMs' reasoning capability. Chain-of-Thought (CoT) [38] improves reasoning performance by incorporating manually crafted decomposition steps into the prompt, allowing the LLM to follow the step-by-step resolution process. Building on this, Zero-shot CoT [18] achieves similar effects by simply adding the phrase \"Let's think step by step,\" enabling LLMs to automatically decompose and execute tasks. From the linear structure of CoT, more complex frameworks have been introduced, such as Tree-of-Thought [43] and Graph-of-Thought [2], which push the boundaries of reasoning through branching reasoning paths. However, these methods face increased resource demands and time complexity, and they heavily rely on manually predefined steps tailored to specific tasks, which significantly limits their generalizability. Additionally, there are some approaches, differing from the idea of predefined reasoning structures, that focus on how to decompose a complex problem into simpler ones for more effective resolution [17, 31, 32, 44].\nUnderlying these methods is the intuition of task decomposition and extensive empirical evidence has demonstrated its effectiveness. This inspires us to explore the potential of task decomposition in edge-cloud collaboration, allowing part of the task to be addressed by cloud-based models while the rest is handled by on-device models, thereby achieving a fine-grained collaborative framework."}, {"title": "2.2 On-device LLM Agent", "content": "The rise of LLMs has revolutionized AI applications, sparking interest in personal AI assistants and mobile automation tools. As on-device intelligent agents gain popularity, users expect seamless, real-time AI support on their smartphones [11]. However, the limited computational and storage capabilities of edge devices pose challenges in deploying powerful models for these agents. Under these constraints, edge-cloud collaboration is a practical solution. Apple's latest research [12] exemplifies this synergy by combining an efficient on-device model, AFM-on-device, with a powerful cloud-based model, AFM-server. This approach balances device limitations with the high-performance needs of Apple's AI features. Similarly, [6] addresses edge-device limitations by splitting task planning and execution between two models-Octo-planner and"}, {"title": "3 Preliminaries", "content": "Problem Definition Denote the local deployed SLM as MD, and the cloud-based LLM as MC. The user's original query is restricted to the edge model for task decomposition and allocation, while the resulting sub-tasks can be resolved by either MD or MC. The entire set of reasoning tasks is represented as T = {T1, T2, . . ., T\u03b7}.\nLet the reasoning accuracy over the entire task set be denoted as Acc, with the API cost represented by CApi, and the completion time denoted as CTime. For each task T, denote the decomposition process as:\nT\u2192 {t\u00b9, t\u00b2,...tk} \nBased on the decomposed subtasks t\u00b9, the model allocation scheme can be denoted as:\nM: t\u00b2\u2192 {MD, Mc}\nwhich prioritizes assigning simple subtasks to on-device SLM, while invoking the cloud-based LLM for handling complex subtasks.\nThe goal of our optimization is to minimize the discrepancy between the model's allocation scheme M and the optimal scheme M*:\nmin |M - M*|\nThe optimal scheme M* is derived through a search strategy that maximizes SLM usage while maintaining accuracy. During the optimization process, as the allocation scheme gradually approaches the optimal solution, both time cost CTime and API cost CApi decrease, while Acc remains well-maintained."}, {"title": "4 Methodology", "content": null}, {"title": "4.1 Division-of-Thoughts Framework", "content": "The division-of-labour is a prominent concept in economics, famously introduced by Adam Smith. It refers to the separation and allocation of tasks within any economic system or organization, enabling participants to specialize in specific areas based on their unique capabilities. This specialization allows individuals, organizations, and nations to optimize productivity by leveraging specialized skills, equipment, and resources. This concept inspires us to consider a similar approach in the context of edge-cloud collaboration, where user queries can be intricately divided: simpler sub-tasks can be assigned to SLMs while more complex ones are allocated to LLMs. This sub-task-level division of labor is expected to reduce reasoning costs while maintaining reasoning performance, enabling more efficient collaboration.\nBuilding on this intuition, we develop the DoT edge-cloud collaboration framework. As illustrated in Figure 1, our framework is divided into three components. The first component is the Task Decomposer, which breaks down the user's query into several simpler and independent sub-tasks. The second component is the Task Scheduler, which is leveraged to determine the pairwise dependencies between sub-tasks and constructing a task dependency graph based on these dependencies. The third component is the plug-and-play LLM adapter, responsible for assigning each sub-task to the appropriate models. The adapter extracts sentence embeddings from the SLM and maps them to difficulty coefficients, which serve as the basis for the model allocation for sub-tasks. Importantly, the training of the adapter does not require modifying the LLM's parameters, ensuring that the LLM's question-answering remains unaffected. Once the appropriate models have been assigned to each sub-task, reasoning proceeds along the order defined by the constructed dependency graph, leading to the final results. Figure 2"}, {"title": "4.2 Decomposing User Query", "content": "As emphasized in the division-of-labour, an effective division method serves as the foundation for collaborative work. The granularity and accuracy of division directly influence the quality and efficiency of the collaboration. Prior research on model collaboration has primarily focused on query-level granularity and relied heavily on manual task decomposition strategies. A typical example is the ToT approach in Game-of-24, which employs a fixed two-step process of proposal generation and evaluation. This rigid, manually-designed workflow lacks generalizability across diverse tasks, underscoring the necessity for automated, flexible, and fine-grained task decomposition methods.\nTo address these challenges, we develop a fine-grained task decomposition method based on the powerful In-Context Learning (ICL) capabilities of LLMs. We leverage a meta-prompt that incorporates \"chain-of-thought\"-like prompting with hand-crafted task decomposing examples, to elicit the inherent planning abilities in SLMs. For each benchmark, We randomly selected 8 samples, manually performed step-by-step task decomposition, and incorporated these steps into the prompts. To enable LLMs to independently solve each sub-task and effectively use the answers from preceding tasks as references for subsequent reasoning, we place great emphasis on the independence and clarity of the decomposed sub-tasks and have incorporated targeted cues in the prompts design."}, {"title": "4.3 Task Scheduling via Dependency Graph", "content": "We employ a three-step approach to schedule all the sub-tasks effectively: Dependency Judgement. Prompting LLM agent to assess pairwise dependencies between sub-tasks. The prompts are as follows: \"Please list the dependencies in the format 'Subproblem A [xxx] -> Subproblem B [xxx]' indicating that Subproblem A must be completed before Subproblem B can start.\" Graph Construction. Converting dependencies into a dependency graph. Based on the clear directional dependencies, constructing the graph is straightforward. However, we further trace back from the final sub-task node to calculate the depth of each sub-task node in the graph (analogous to the height in a tree structure). Sub-tasks at the same depth are independent of each other and can be inferred in parallel. Depth serves as the inference batch, with batches processed sequentially while tasks within a batch are reasoned in parallel. As shown in Figure 3, compared to sequentially reasoning through"}, {"title": "4.4 Task Allocation with Reinforced SLM", "content": "For the decomposed sub-tasks, we aim to assess their difficulty based on the task descriptions and allocate either cloud-based or edge-side models for execution. Using LLMs to evaluate difficulty introduces additional inference costs and often fails to accurately assess the sub-task's complexity, which motivates us to train a model specifically designed for task allocation with sentence embeddings. Sentence embeddings, which map a sentence to a fixed-size vector capturing its semantic meaning and context, have seen extensive application in natural language processing for their lightweight accessibility and the strong ability to capture sentence semantics. We can obtain a sentence embedding for each sub-task's prompt and then map the embedding to the corresponding difficulty. Given that LLMs are already deployed on the edge side and have been shown to serve as effective sentence embedders, we plan to leverage the local deployed SLM to produce sentence embeddings.\nHowever, autoregressive LLMs lack specialized tokens such as BERT's [MASK] or [CLS], which are typically used in transformer-based models for embedding tasks. The exploration of Ting Jiang et al. [15] helps us overcome this limitation. Prompt-based method is refined specifically for autoregressive LLMs by instructing the model to generate the next word that captures the semantic meaning of the input sentence. Specifically, a simple but effective template can be constructed for each piece of text: This sentence: \"[text]\" means in one word: \", where [text] represents the input sentence, and the LLM is prompted to generate the next token that encapsulates the core meaning of the sentence in a single word. The last generated token plays a critical role, as we extract its hidden state from the model and use it as the sentence embedding.\nBoosting SLM with Plug-and-Play Adapter. Despite the varying lengths of sub-task descriptions, the sentence embeddings maintain a consistent dimensionality. For efficiency, we append a multilayer perceptron (MLP) to the transformer module of the LLM to map the embeddings to a difficulty score. Given that both the edge"}, {"title": "In our implementation", "content": "we employ the SLM to answer all decomposed sub-questions, recording the sampling probability corresponding to each token. We then apply a consistent alpha value to extract the \u03b1-quantile from each probability sequence and ranked the \u03b1-quantile values for all sub-task responses. Since higher task difficulty leads to greater uncertainty in the model's answers, resulting in lower token sampling probabilities, we reverse the \u03b1-quantile ranking to obtain the difficulty order of the sub-tasks."}, {"title": "Creating Training Data with \u03b1-Tree Algorithm", "content": "How cam we construct a dataset to train our adapter? We develop an efficient allocation optimization strategy, \u03b1-Tree, which consists of two key components: the sub-task difficulty ranking based on the \u03b1-quantile token probability, and a tree-based search strategy guided by the ranking, allowing us to generate a large-scale optimal allocation dataset with low cost and high speed. \u03b1-quantile refers to calculating a specific quantile from the token probabilities generated by LLM during inference. Unlike traditional methods that aggregate probabilities through summation or averaging, \u03b1-quantile focuses on a specific portion of the uncertainty distribution, such as the minimum (\u03b1 = 0) or a higher percentile (e.g., \u03b1 = 0.8), which provides a fine-grained measure of uncertainty. Denote the input context as x, and the output tokens as \u0177. The \u03b1-quantile value is denoted as:\nSquant(x, \u03b1) = quantilea (p(1) (\u0177\u2081 | x), p(\u01772 | x, y,\u2081), ..., pl\u0177n | x, y, \u0177n-1))\nIn our implementation, we employ the SLM to answer all decomposed sub-questions, recording the sampling probability corresponding to each token. We then apply a consistent alpha value to extract the \u03b1-quantile from each probability sequence and ranked the \u03b1-quantile values for all sub-task responses. Since higher task difficulty leads to greater uncertainty in the model's answers, resulting in lower token sampling probabilities, we reverse the \u03b1-quantile ranking to obtain the difficulty order of the sub-tasks."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Experimental Setup", "content": "Benchmarks. We validate the effectiveness of our framework across seven open-source benchmarks. For each benchmark, we randomly select 8 instances for manual annotation to serve as in-context learning examples of task decomposition. Additionally, we"}, {"title": "Selection and Deployment of LLMs", "content": "We used GPT-40 [29] as cloud-based LLM and Llama 3-8B [27] as on-device LLM as its parameter scale is theoretically deployable on edge devices. The Llama 3-8B model is deployed on a local A100 GPU with a context length limit of 8192. The parameter count of our adapter is only 13,109,249, approximately 1/800th of the parameters in local Llama."}, {"title": "Evaluation", "content": "For evaluation, we establish three metrics: Acc, Ctime, CAPI. Six of the benchmarks, excluding WebShop, have deterministic results. For instance, the results for CSQA are single-letter options, allowing direct determination of correctness. Similarly, for P3, the reasoning output can be passed into a problem function, and if the function returns True, the reasoning is deemed correct. For WebShop, we use the evaluation environment provided by the original benchmark to average the reward of all purchased items on the level of satisfaction according to the constraints."}, {"title": "5.2 Main Results", "content": "Comparison between our method and baselines is shown in Table 1, we have highlighted in bold the highest accuracy results among the five baseline experiments on each benchmark, while the associated costs are underlined. We compute the relative improvement of our results compared to the baseline with the highest accuracy.\nThe experimental results demonstrate that our approach significantly reduces costs while maintaining accuracy within an acceptable range of decline. Across seven benchmarks, the relative changes in accuracy compared to the best baseline results are: -2.38%, -7.35%, -13.89%, -6.35%, 1.75%, 5.59%, and 0%. Notably, positive improvements are observed in some cases, suggesting that our framework has the potential to enhance LLMs' reasoning capability. This enhancement is further discussed in ablation study 5.4. At the same time, our approach achieves a substantial reduction in cost compared to the baseline with the highest accuracy, with an average time reduction of 66.12% and an average API cost reduction of 83.57% which far exceeded the decline in accuracy.\nMoreover, on P3 and SCAN benchmarks, CoT achieves the best performance, indicating that a sequential linear reasoning structure is better suited for these task types. In contrast, on more complex mathematical reasoning benchmarks like MATH, CHAMP, and WebShop, ToT achieves the highest reasoning accuracy, demonstrating the effectiveness of the multiple-proposal strategy. However,"}, {"title": "5.3 Efficiency & Quality of Dataset Construction", "content": "To validate the effectiveness of our method for difficulty assessment and optimal allocation search in dataset construction, we compared our \u03b1-tree with other baseline search methods. We establish two baseline methods: Zero-shot LLM: This approach utilizes a LLM to assess the difficulty of sub-tasks. The evaluation model employs GPT-40, utilizing a question-and-answer format that incorporates the task content, all sub-tasks, and the current sub-task into the"}, {"title": "5.4 Ablation Study", "content": "In this ablation study, we demonstrate the impact of the graph structure and task allocation mechanisms on the efficiency and effectiveness of our DoT model. Upon removing the graph structure, a sequential reasoning framework is used instead. In the absence of task allocation, a single language model or simple referral strategy is adopted. As shown in Table 3, the full DoT model, which includes both components, achieves high accuracy across various tasks while maintaining low time and API costs. When the graph component is removed, both accuracy and efficiency are reduced, indicating the impact of graph structure on enhancing reasoning performance and the effectiveness of graph-based parallel execution.\nAdditionally, removing the task allocation mechanism leads to higher API costs and increased time without substantial accuracy gains. For example, in the P3 task, removing allocation and solely using GPT-40 incurs a cost of 4.80\u00a2compared to DoT's 1.58\u00a2, despite only a modest increase in accuracy (43% vs. 41%). This demonstrates that the allocation mechanism helps balance accuracy with efficiency."}, {"title": "5.5 Trade-off Between Accuracy and Cost", "content": "The ablation study demonstrates that our on-graph reasoning method significantly enhances the reasoning capability of LLMs, though at a higher computational cost. In this section, we delve into the detailed relationship between cost and reasoning accuracy to identify the achievable performance upper bound under different budget constraints. Based on our original task allocation strategy, we generate diverse model allocation schemes by proportionally assigning more sub-tasks to either the cloud-based LLM or the local deployed SLM according to the difficulty level of sub-tasks, resulting in the various data points shown in the figure. We also introduce DataShunt as a baseline, where the proportion of tasks handled by the LLMs is adjusted by tuning the LLM evaluation thresholds. We test our method on four benchmarks, plotting the trade-off curves for accuracy versus API cost and accuracy versus time cost. As illustrated in Figure 6, our curve consistently remains above the baseline curve. Under the same accuracy, both the token cost and time expenditure of DoT are significantly lower than those of DataShunt, demonstrating that the model allocation method of DoT is quite efficient. Furthermore, as the budget increases, our DoT approach achieves a higher performance upper bound."}, {"title": "6 Conclusion", "content": "In conclusion, our proposed Division-of-Thoughts collaborative reasoning framework effectively enhances the reasoning performance of LLMs on edge devices by intelligently distributing tasks between local and cloud resources. DoT not only breaks down complex tasks into manageable subtasks but also leverages a directedacyclic graph to optimize the reasoning path, facilitating parallel edge-cloud processing. Furthermore, the plug-and-play adapter accurately assesses the difficulty of sub-tasks for allocation, ensuring high reasoning performance while significantly reducing costs of money and time. Looking ahead, we aim to apply our reasoning framework across a broader and more diverse range of scenarios, striving to contribute to the advancement of edge AI."}, {"title": "Acknowledgments", "content": "This work is supported in part by the National Natural Science Foundation of China under 23IAA02114 and 62472241, in part by the joint project of Infinigence AI & Tsinghua University."}, {"title": "A Supplementary Experiments", "content": null}, {"title": "A.1 Generalizability of Adapter", "content": "We conduct two supplementary experiments to evaluate the adapter's generalization performance across different tasks. Intra-category generalization: Within three mathematical benchmarks, train on two and test on the remaining one. Cross-category generalization: Train on six benchmarks and test on the remaining one. We also evaluate the performance of the adapter in the absence of any training. Results are shown in Table 4, demonstrating that intra-category generalization within math-related benchmarks is almost as effective as task-specific training. And although there is a slight performance decrease in cross-category application, it still outperforms no training."}, {"title": "A.2 Evaluation of Task Decomposition Quality", "content": "For the quality of the task decomposition, we compare our method with a vanilla prompting strategy, focusing on evaluating the independence of subtasks and the impact of the decomposition strategy on both the final reasoning accuracy and the reasoning cost. For independence, we aim to determine whether the generated subtasks are independent (i.e., without overlapping mathematical computation) and calculate the independence rate for performance ealuation. We first manually annotate the independence of the decomposition results for 50 tasks in the MATH benchmark. Our method achieves 91.3% in independence rate, significantly outperforming the vanilla prompting strategy, which only reaches 74.2%. This demonstrates the high quality of our task decomposition approach.\nSubsequently, we employ GPT-40 to evaluate the independence of subtasks generated by both methods and compared its results to manual annotations. The GPT-40 assessments achieve a 93% consistency with human annotations, indicating that GPT-40 can provide annotation quality comparable to that of humans and can be effectively used for large-scale evaluations. Therefore, we utilize GPT-40 to evaluate the independence of task decomposition across the entire test set, with the results presented in Table 5. The results indicate that our task decomposition method achieves an average improvement of 15.7% in subtask independence compared to the baseline, demonstrating a clear advantage. Moreover, this improvement contributes to enhanced reasoning accuracy and reduced reasoning costs."}, {"title": "A.3 Impact of the Number of Sub-tasks on Reasoning Efficiency", "content": "As problems become more complex and the number of decomposed sub-tasks increases, whether our framework can achieve a significant improvement in reasoning efficiency while maintaining accuracy remains a valuable research question. Therefore, we conduct supplementary experiment, where we compare our method with baseline that sequentially reason all sub-tasks on SLM and perform statistical analyses based on varying numbers of sub-tasks. The results in Table 6 demonstrate that our model consistently maintains high reasoning efficiency across tasks with different numbers of sub-task nodes.\nFurthermore, as the number of sub-tasks increases, the enhancement in efficiency becomes increasingly pronounced. When the"}, {"title": "In our implementation, we employ the SLM to answer all decomposed sub-questions", "content": null}, {"title": "B Benchmarks and Implementation Details", "content": null}, {"title": "B.1 Mathematics", "content": "MATH [14]. Mathematics Aptitude Test of Heuristics (MATH), comprises 12,500 problems from prestigious U.S. mathematics competitions like AMC and AIME. These problems, collected from platforms such as AoPS, test advanced problem-solving skills beyond standard K-12 math. Each problem"}, {"title": "B.2 Logic", "content": "P3 [30]. Python Programming Puzzles (P3), introduces a new type of programming challenge for evaluating program synthesis. P3 contains 397 Python-based puzzles, where the goal is to find an input that makes a given function return True. The puzzles span various difficulty levels, from simple string manipulations to complex algorithmic problems.\nScan [20]. This benchmark is used to evaluate the sequence-to-sequence learning ability to translate simplified natural language commands into action sequences. SCAN contains 20,910 commands generated by a phrase-structure grammar, which describe basic actions such as \"jump\" or \"walk\" and their combinatorial variations (e.g., \"jump around left\"). The logical structure and constraints involved in the translation process make SCAN ideal for assessing LLM reasoning capabilities."}, {"title": "B.3 Commonsense", "content": "COMMONSENSEQA [36]. This benchmark is a challenging dataset designed to test commonsense question answering. It consists of 12,247 multiple-choice questions created using concepts from CONCEPTNET. Each question is authored by crowd-workers to differentiate between multiple target concepts that share a semantic relation with a source concept, encouraging the use of prior knowledge and complex reasoning."}, {"title": "B.4 On-device AI Assistant Application", "content": "Webshop [42]. WebShop serves as a simulated e-commerce environment featuring 1.18 million real-world products and 12,087 crowd-sourced text instructions. Designed to evaluate language agents, it challenges them to navigate diverse web pages and perform actions based on natural language product specifications. Agents encounter obstacles such as interpreting compositional instructions, reformulating queries, and understanding noisy text on webpages, while strategically exploring to fulfill product requirements. The modular design of WebShop separates website navigation from task-specific elements, allowing for easy adaptation to new tasks and domains. This dataset provides a robust platform for assessing the capabilities of language agents in an interactive, real-world-inspired setting, emphasizing their ability to comprehend and act on complex instructions."}, {"title": "B.5 Implementation Details across Benchmarks", "content": "MATH. For MATH tasks, we employ a structured workflow comprising four key stages: Task Decomposition, Model Allocation, Dependency Graph Construction, and Step-by-Step Reasoning Based on the Graph. In the task decomposition phase, we prompt the LLM with exemplars of manually decomposed complex problems. The LLM is then instructed to generate manageable subtasks that collectively solve the primary challenge. Subsequently, we task the LLM with establishing subtask dependencies, where a relationship Stepi \u2192 Stepj indicates that Stepi must precede Stepj. Using the derived dependencies, we construct a reasoning graph via Breadth-First Search (BFS). Subtasks at the same depth are processed in parallel. Upon completion of all subtasks, we conduct a final query"}, {"title": "C Prompts", "content": null}, {"title": "C.1 Task Decomposition", "content": "I will now give you a [Based on the type of problem]. The type of problem is type. Please break this problem down into several easy-to-solve steps.\n1 examples are as follows: [Manual Written Examples]\nNow the command is question, please decompose it into easyto-solve steps like the examples. Answer Format: (Please write each broken-down question step on a separate line, starting with a number.)\nTo solve the question \"xxx\", we need to know: \"1. question step1\", \"2. question step2\", \"3. question step3\"."}, {"title": "C.2 Dependency Construction", "content": "System Prompt:\nNow we have a problem, which we have broken down into many sub-problems. I want you to understand the connection between these sub-problems\nUser Prompt:\nThe init problem is question. And the sub-problems are steps. Please provide your understanding of the relationships between these sub-problems. Your response must be concise.\nNow we need to create standardized connections for the relationships between these sub-problems. Now Given the following subtasks for question: [question], determine the dependencies between them:\n[List of Steps]\nPlease list the dependencies in the format 'Subproblem A [xxx] -> Subproblem B [xxx]' indicating that Sub-problem A must be completed before Sub-problem B can start. Please identify any potential conditional dependencies from a logical perspective.\nAnswer format: (Please strictly follow the format. Each dependency should be separated by a new line. No explanation is required.)\nStep ID; [ sub-problem i ] -> Step ID; [ sub-problem j ]\nStep IDj [sub-problem m ] -> Step IDn [sub-problem n] ..."}, {"title": "C.3 Subtask Reasoning", "content": "System Prompt:\nHere is a math word problem. I will first provide a passage of the problem to set the context. Then, I will ask a specific question that requires you to use the information from the problem description, along with calculation and reasoning, to solve it.\nPassage: [passage] Question: [question]\nI have broken this math question down into several smaller questions. I will assign you sub-questions one by one, and provide the results of previous sub-questions as a reference for your reasoning. Please solve the question according to mathematical logic.\nFor each steps So far, the answers to the resolved sub-questions are as follows: The format is Sub-question-Id: xxx; Sub-question: xxx; Answer: xxx. Sub-question-Id: [Corresponding ID]; Sub-question: [Corresponding Step]; Answer: [Corresponding Solution for the step]\nAmong them, sub-questions predecessors are directly related to this sub-question, so please pay special attention to them. The subquestion to solve now is xxx: {subtask} Based on the information above, please provide a concise and clear answer"}]}