{"title": "Citations and Trust in LLM Generated Answers", "authors": ["Yifan Ding", "Matthew Facciani", "Amrit Poudel", "Ellen Joyce", "Salvador Aguinaga", "Balaji Veeramani", "Sanmitra Bhattacharya", "Tim Weninger"], "abstract": "Question answering systems are rapidly advancing, but their opaque nature may impact user trust. We explored trust through an anti-monitoring framework, where trust is pre- dicted to be correlated with presence of citations and in- versely related to checking citations. We tested this hypothe- sis with a live question-answering experiment that presented text responses generated using a commercial Chatbot along with varying citations (zero, one, or five), both relevant and random, and recorded if participants checked the citations and their self-reported trust in the generated responses. We found a significant increase in trust when citations were present, a result that held true even when the citations were random; we also found a significant decrease in trust when participants checked the citations. These results highlight the importance of citations in enhancing trust in AI-generated content.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) (Achiam et al. 2023; Tou- vron et al. 2023) stand at the forefront of contemporary artificial intelligence (AI), wielding immense potential to reshape the way humans interact with technology and in- formation. However, a critical question persists: Are these LLMs and their by-products trusted by users? The answer holds profound implications for the future of AI in society. Trust (Baier 1986), a cornerstone of human relationships and societal functioning, plays an indispensable role in the shar- ing and acceptance of information.\nHuman trust dynamics are complex, often deeply rooted in social norms and interpersonal relationships. Humans navigate complex social structures by building trust through shared experiences, reputations, and accountability mecha- nisms (Muir 1987). Leading social theories, like the Princi- ple of Social Proof, suggest that social conventions might predispose individuals to favor human sources over algo- rithmic sources (Cialdini 2009). This predicts that responses from an Al system might be more trusted when a hu- man source corroborates its response. Conversely, responses from an Al system might be more trusted when the human touch is absent because AI systems lack anthropomorphic traits and social nuances (Miller 2019). In other words, AI systems, which are largely devoid of social motives, may in- crease trust among users who seek impartial and objective information (Sambrook 2012; Rosen 1999).\nHuman trust in AI is a complicated subject that depends on several factors, with accuracy being the most important contributor (Lucassen and Schraagen 2011) and explain- ability (Rawal et al. 2021) following not far behind. Ac- curacy refers to the AI's ability to provide correct infor- mation consistently. Explainability entails the articulation of the model's decision-making pathways, rendering them transparent and comprehensible to users. Explainability is crucial for building a trust-based relationship between hu- mans and AI (Stephanidis et al. 2019), significantly influ- encing users' willingness to engage with AI systems (Hoff and Bashir 2015).\nExisting research on explainability primarily focuses on enabling Al engineers to understand model behav- ior. However, empirical investigations of explainability's role in user trust are limited, and the evidence is mixed regarding whether explainability indeed increases user trust (Scharowski et al. 2023; Nothdurft, Hein- roth, and Minker 2013). Some studies report positive ef- fects (Ehsan et al. 2019), while others find no significant impact (Poursabzi-Sangdeh et al. 2021; Zhang, Liao, and Bellamy 2020; Cheng et al. 2019). These mixed results may stem from differences in how user trust is defined and measured: some studies rely on simple questionnaires, like"}, {"title": "Trust and Large Language Models", "content": "Trust in AI is a flourishing research area with diverse per- spectives, including Trust as Anti-Monitoring and Social Proof Theory integrated into RAG systems and Chatbots. Scholars across these domains have investigated the dynam- ics of user trust with AI, offering insights into the mecha- nisms shaping human-machine interactions.\nThe first step when investigating users' trust in LLMs is to define trust. Trust is context-dependent, for example, a person may trust a mechanic to repair their car but not to prepare their taxes. Additionally, trust is dynamic and is built over a series of human interactions or events; each time the mechanic successfully repairs the car, the individual's trust increases, whereas a failure to fix it properly would decrease their trust.\nIn scenarios such as judicial decisions and wikis, citations are crucial in building trust, as they add credibility and trans- parency to content, regardless of the category of content. This raises an intriguing question: do individuals inherently trust LLM-generated responses more when they are accom- panied by citations?"}, {"title": "Trust as Anti-Monitoring", "content": "How best to measure trust is a complicated and hotly debated topic (Baier 1986; Ferrario and Loi 2022). Trust, as Annette Baier's work suggests, can be construed as \u201canti-monitoring,\" where an indication of trusting an entity is a reduction in monitoring their behav- ior (Baier 1986; Archard et al. 2013). Monitoring, in the ex- ample above, refers to the intuition that if a customer trusts a mechanic, the customer is willing to allow the mechanic to repair the car without supervision. This implies that the level of monitoring someone performs is inversely related to the level of trust they have in the person or thing be- ing monitored. The concept of trust as anti-monitoring of- fers a measurable framework for understanding trust. Cita- tions serve as a monitoring mechanism, allowing people to check the LLM's response and determine if it aligns with the user's expectations and reasoning. This ties into the social- psychological Principle of Social Proof, where individuals look to external cues and validations to form trust.\""}, {"title": "Principle of Social Proof", "content": "The Principle of Social Proof is particularly useful for understanding interactions with Chat- bots. This framework suggests that people are more likely to adopt a behavior if they see the social proof of others doing the same (Cialdini 2009). Social proof can, therefore, act as a proxy for trustworthiness, as individuals are more likely to use, and in turn trust, a product when they observe others us- ing it (Lins and Sunyaev 2023; Kim, Choi, and Fotso 2024; Venkatesh and Davis 2000).\nIn Chatbot interactions, citations within the outputs serve as strong indicators of social proof because they signal to users that the output is endorsed by some source. Con- sequently, the presence of citations theoretically enhances trust, because users perceive the content as more credible and reliable.\nThe Principle of Social Proof aligns with the anti- monitoring framework of trust, where having the ability to monitor or check a response positively contributes to the trustworthiness of a response, regardless of whether or not the user chooses to check the source. However, it remains uncertain whether high-quality citations significantly impact trust, or if any citation, regardless of quality, is sufficient."}, {"title": "RAG and Chatbots", "content": "RAG is an Al framework that incor- porates information retrieved from external sources into the generation process (Asai et al. 2023, 2024). This approach tends to reduce inaccurate responses(Lewis et al. 2020) and includes citations within its generated responses, providing users with a clear trail of the sources that support the pre- sented information.\nUser trust dynamics in LLMs is beginning to receive some much-needed attention and research has uncovered factors that influence users' perceptions and behaviors (Sun et al. 2022). For example, users are more likely to engage with chatbots they perceive as trustworthy (Choudhury and Shamszare 2023). However, concerns about government use of chatbots can lead to distrust (Aoki 2020). Despite occa- sional inaccuracy and unreliability in current chatbot ver- sions, users often express intentions to continue using them, indicating a resilient trust in these systems (Amaro et al. 2023). Research also suggests that users tend to trust chat- bots with more human-like characteristics (Kaplan et al. 2023), highlighting the interplay between trust, utility, per- ceived reliability and accuracy, and the humanization of AI in shaping user engagement."}, {"title": "Citations and Trust Experiment", "content": "We developed a bespoke Web site for data collection. On this Web site, users were introduced to the task with an an- imation that showed the example question: \"How far is it from the earth to the moon?\". If the participants agreed to participate, they were provided a simple query box, stylized to look like a standard input Web form. On this form users were prompted by instruction-text to: \u201cAsk any question\".\nThe participant's responses were stored on a Web server owned and managed by research team. Each question was then fed directly to ChatGPT4 and the responses were col- lected. Responses were truncated if they were longer than three sentences.\nThe experiment was a Randomized Controlled Trial (RCT) with a between-subjects 3 by 2 factorial design (see Fig. 2). The first factor corresponded to the number of cita- tions: zero, one, or five; the second factor corresponded to the nature of the citations: valid or random.\nFor the first factor: in the no citation condition, the re-"}, {"title": "Participants", "content": "We used Prolific\u00b2 to recruit participants for this study (Palan and Schitter 2018). Data collection occurred on March 13, 2024. Participants were paid two US dollars and took a me- dian of 17 minutes to complete. The study had 303 total participants who were randomly assigned to the experimen- tal groups (i.e., between-subjects design). Participants either saw zero (N=108), one (N=96), or five (N=101) citations. Of the two groups who saw citations (N=197), a random split of the participants received a random citation (N=87) or valid citation (N=110).\nParticipants were asked to enter ten questions and rate each response; finally an exit interview was conducted with a battery of demographic questions (see Supplement A and Table S1 for demographic battery and response codes)."}, {"title": "Ethical Statement", "content": "This study received approval from the redacted Institutional Review Board (protocol no. redacted). Participants were fully briefed on the study's purpose, ensuring informed con- sent and voluntary participation. Aside from broad demo- graphic questions, personal identifiable information was not collected.\nOur study aims to understand and measure trust and in- form best practices in the incorporation of citations into AI systems. However, potential risks include misinterpretation of results leading to over-reliance on citations and privacy concerns related to participant data. We are committed to addressing these issues by adhering to ethical standards, en- suring transparency, and carefully evaluating the broader im- pact of our work."}, {"title": "Data, Materials, and Software Availability", "content": "All participant questions, their responses and, their ratings are available in an Excel file. This file is publicly available online at redacted and in the Supplement. We used Stata Software for data and statistical analysis. The Stata codes used for all analysis in the main manuscript is included in the Supplementary Material."}, {"title": "Results", "content": "We expect that the presence of citations in an AI chatbot's output should enhance response transparency and should improve perceived trustworthiness. The Principle of Social Proof suggests that observing evidence of others endorsing a behavior increases the likelihood of adopting that behav- ior ourselves. We also predict that the quality of the cita- tions matters. Citations that accurately support the chatbot's answer will be evaluated as more trustworthy than random citations.\nIn out initial analysis, the Dependent Variables (DVs) were citation and no citation coded 1 or 0 respectively. Ran- dom and Accurate citation were coded 1 or 0 respectively. Controlling for various demographic factors, results of lin- ear regression analyses indicate a statistically significant in- crease in perceived trustworthiness for AI chatbot responses with citations compared to those without (See Fig. 3 and Supplement B Table S2)."}, {"title": "Does the Quality of Citation Matter?", "content": "Additional analysis examined the influence of random ver- sus valid citations on trustworthiness. Again, controlling for demographic factors, results revealed that answers contain- ing random citations were significantly less trustworthy (See Fig. 3 and Supplement B Table S2)."}, {"title": "Do the Number of Citations Matter?", "content": "Using an analysis of variance (ANOVA), we examined whether perceived trustworthiness varied among the zero citation, one citation, and five citation conditions. The analysis revealed significant differences between groups (F(2, 3037) = 10.23, p < .001). Post-hoc Bonferroni (Bon- ferroni 1936) tests indicated that both the one citation and five citation conditions were rated significantly higher on trustworthiness compared to the zero citation condition (see Supplement B Table S3).\nNotably, there was no significant difference between the one citation and five citation conditions (p >.05). In other words, five citations in an answer are not perceived as more trustworthy than an answer with one citation. This negative result is contrary to our initial hypothesis. One plausible reason for this result might be due to the principle of di- minishing returns, wherein participants may perceive that a single, well-chosen citation is sufficient to confirm the AI's response."}, {"title": "Social Demographics and Trust", "content": "In addition to the directional hypotheses, we also explored how various social demographics predict the perceived trust-worthiness of AI chatbot answers. While there is limited re- search specifically on how different groups respond to AI chatbots, some studies investigate how different demograph- ics react to new technologies such as AI (Tyson and Kikuchi 2023; Rainie et al. 2022). For instance, individuals with more conservative values tend to be more skeptical toward AI and new technologies (Castelo and Ward 2021). How- ever, both individuals with liberal and conservative views are more receptive to technology when it is framed in a way that aligns with their political values (Claudy, Parkinson, and Aquino 2024). Furthermore, higher levels of education and familiarity with chatbots may increase perceived trustwor- thiness; on the other hand, greater awareness of AI limita- tions, which often accompany higher education and famil- iarity, could decrease trustworthiness.\nWe did not find significant differences in trustworthiness ratings among most demographic categories, the slight in-clination of nonwhite participants to trust the answers more suggests avenues for exploratory research (SI Appendix B Table S2). For instance, future studies could look further into the underlying factors driving this trend and explore po-tential cultural or societal influences on trust perceptions in AI-generated content."}, {"title": "Does Checking Citations Indicate a Reduction in User Trust?", "content": "The theory of trust as anti-monitoring predicts that individ- uals who are skeptical of an answer will be more likely to check the source of the citation. This predicts that check- ing a citation indicates reduced trust, as users are no longer relinquishing control and trusting the other party to be accu- rate. We tracked the frequency of participants that manually checked citations with their mouse while reviewing the AI chatbot's answers. There were 1,976 answers in our dataset that had at least one reference. Of these, only 193 (9.77%)"}, {"title": "Illustrating Question Semantics", "content": "The data we collected includes hundreds of interesting and unique real-world, human-generated questions, along with trust ratings for their answers.\nThis data permitted an exploration of the types of ques- tions asked and if they affected trustworthiness. Political information is particularly susceptible to bias (Ditto et al. 2019; Poudel and Weninger 2024), and we believe that ques- tions of a political nature may vary in their perceived trust-worthiness. Individuals might be more likely to ask ques- tions that confirm their pre-existing political biases, which could increase the perceived trustworthiness of the answer if it aligns with their views. Conversely, if the chatbot's answer contradicts their beliefs, the perceived trustworthiness of the response may decrease.\nWe analyzed the semantics of the questions asked by par- ticipants using a comparative approach by juxtaposing them with questions from established question-answering datasets such as AskReddit and Quora. Using sentence transformers, we embedded each question into a high-dimensional vec- tor space, capturing their semantic representations (Reimers and Gurevych 2019). Subsequently, we used UMAP, a di- mensionality reduction technique, to project these question embeddings onto a 2-dimensional plot (McInnes, Healy, and Melville 2018), as depicted in Fig. 5. In this plot, partici- pant's questions are depicted in black. An interactive plot can be found in the Supplement."}, {"title": "Does the Type of Question Impact User Trust?", "content": "Using linear regression, and accounting for demographics, citations, and random citations, we examined the impact of question type on user trust ratings. We then used ChatGPT4 to label each question as being Political, Factual, and Com- plex questions as 0 or 1; these labels need not be disjoint (i.e., a question can be both factual and political).\nOur results indicate that political questions received sig- nificantly higher self-reported user trust ratings compared to non-political questions, even after adjusting for demo-graphic factors. Manual analysis of the questions and an-swers suggests that this effect is not due to the chatbot's awareness of the participant's political stance, but rather because political questions are often framed in a way that aligns with users' preconceived notions, thereby eliciting more favorable responses. These findings support the theory of social proof, where the chatbot's responses act as a form of social endorsement, enhancing perceived trustworthiness.\nWe found that fact-based questions were significantly more trusted than non-fact-based questions. This aligns with the previous result that political questions (which may also be factual) are also trusted, but it highlights different aspects of trustworthiness in AI chatbot responses. As previously discussed, political questions tend to be framed in a way that aligns with the participant's pre-existing beliefs and in-group biases, leading to higher trust ratings. Furthermore, fact-based questions are typically grounded in objective, ver-ifiable information. Participants can cross-check these facts against known data or their existing knowledge, leading to higher trust ratings. These findings suggest that trust in AI chatbot responses can be driven by both the objective accu-racy of the information (fact-based questions) and the social alignment with the participant's beliefs (political questions). While fact-based questions benefit from their verifiability, political questions benefit from framing and ingroup valida-tion."}, {"title": "Complexity and Trust", "content": "Previous studies suggest that LLMs provide more accurate responses when prompted with more familiar language (Go- nen et al. 2022). For example, asking \u201cWho was the first president of the United States?\u201d is clear and straightforward, leading to an accurate response. In contrast, a more complex question like \"How have the economic policies of U.S. pres-idents influenced income inequality in the United States?\" is less direct, causing the AI to infer more, which may reduce accuracy. This suggests that simpler, familiar prompts yield more accurate and trusted responses from AI systems.\nLanguage perplexity was used as a proxy for the model's familiarity with the question, where perplexity was mea- sured as $PPL(x) = exp(-\\frac{1}{N} \\Sigma_{i=1}^{N} log_2 P(w_i))$ where N is the total number of words in the response, wi is the ith word in the response, and P(wi) is the is the probability of the ith word given in ChatGPT4. In this context, lower per- plexity indicates a more familiar prompt, leading to more accurate (perhaps more trusted) results. Guided by these prior findings, we compared user-reported trust as a function of the perplexity of the prompt. We found that higher per- plexity is slightly (negatively) correlated with trust (Pearson R=-0.06, p=0.002), i.e., answers to complex questions are (slightly) less trusted.\nIn a similar exploratory analysis, we also found that the length of a prompt (number of tokens) is (slightly) nega- tively correlated with trust (Pearson R=-0.04, p=0.041). In other words, although we do not find statistical differences in trust between simple and complex questions, we did find that responses to simpler questions (in terms of perplexity and length) were more trusted. This suggests that users may have a higher level of trust in chatbots when the prompts are simpler, potentially indicating a preference for straight-forward and concise queries that yield more understandable answers."}, {"title": "Discussion", "content": "The present study investigated how variations in citations in- fluenced the perceived trustworthiness of answers provided by an AI chatbot. Drawing upon the anti-monitoring frame- work, we conceptualized trust and extended this framework to incorporate the Principle of Social Proof. We hypothe- sized that participants would trust AI chatbot responses with citations more than those without citations, as citations pro- vided the opportunity for verification (or monitoring) of the output. Moreover, these citations, linked to supporting orga- nizations, served as a form of social proof, enhancing trust-worthiness.\nOur findings supported this hypothesis, revealing that AI chatbot answers with citations were perceived as more trust-worthy compared to those without citations. Furthermore, we investigated the significance of the number of citations, finding no significant difference in perceived trustworthiness between responses with one or five citations.\nBeyond the number of citations, we also investigated those participants who manually inspected the citations. We found that responses containing random citations were rated lower in trustworthiness compared to those with accurate ci-tations.\nFurthermore, we explored whether a higher frequency of citation-checks, indicated by mouse hovers, correlated with lower perceived trustworthiness. Consistently, participants who checked citations tended to rate the answers as less trustworthy. This finding aligns with the theory of trust as anti-monitoring, as skeptical participants sought to verify (i.e., monitor) the source of the information provided by the chatbot. Next, we investigated whether the type of question asked was associated with the perceived trustworthiness of the answers. We categorized questions into three groups: po- litical, factual, and complex.\nOur analysis revealed that political questions were rated significantly more trustworthy than non-political questions. This trend may be attributed to participants posing politi- cal questions that already aligned with their political biases, leading them to be more inclined to trust the answers. We also found that fact-based questions were significantly more trusted than others. This difference could be due to the con-crete nature of factual questions, instilling confidence in par-ticipants that they already know the correct answer, whereas non-factual questions were more subjective.\nRegarding complex questions, we hypothesized that they might elicit greater trust as they could potentially demon-strate the chatbot's capability to handle challenging in-quiries. However, we did not observe a significant difference in trust ratings between complex problem-solving ques-tions compared to those categorized as more straightfor-ward. Given the limited number of questions coded as com-plex, we cannot assert the absence of an effect with confi-dence.\nAlong the way, we evaluated if demographic variables predicted trustworthiness. We only found that nonwhite par-ticipants were slightly more likely to trust the answers. We also found that males, individuals with liberal views, and people in urban areas were significantly more likely to check the citations (i.e., mouse-hover over the references) given in the chatbot answer. Given our small sample size, we are hes-itant to read too much into these results, but it may be an area for future research."}, {"title": "Limitations", "content": "Our study is not without several important limitations. First, our sample was comprised of participants entirely from the online data collection platform Prolific. These participants may be more technologically savvy than the typical individual who does not sign up for online re-search surveys. Additionally, since our sample was 65% white, we did not have sufficient statistical power to eval-uate different racial groups and combined them into a sim-ple nonwhite category. While this nonwhite category trusted their chatbot answers more than the white category, future research will have to investigate what could have caused this difference or if it was an artifact. Our study also did not con-trol for what questions were asked so it could be that people of different demographic groups may ask the chatbot dif-ferent questions, which could alter their trustworthiness. Fi-nally, our measure of trustworthiness was a simple, one-item variable. It's possible that different forms of trustworthiness may yield interesting results. Future research can incorpo-rate more robust measures of trustworthiness to assess how different questions and outputs influence different elements of trust."}]}