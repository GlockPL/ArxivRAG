{"title": "A Benchmark for Long-Form Medical Question Answering", "authors": ["Pedram Hosseini", "Jessica M. Sin", "Bing Ren", "Bryceton G. Thomas", "Elnaz Nouri", "Ali Farahanchi", "Saeed Hassanpour"], "abstract": "There is a lack of benchmarks for evaluating large language models (LLMs) in long-form medical question answering (QA). Most existing medical QA evaluation benchmarks focus on automatic metrics and multiple-choice questions. While valuable, these benchmarks fail to fully capture or assess the complexities of real-world clinical applications where LLMs are being deployed. Furthermore, existing studies on evaluating long-form answer generation in medical QA are primarily closed-source, lacking access to human medical expert annotations, which makes it difficult to reproduce results and enhance existing baselines. In this work, we introduce a new publicly available benchmark featuring real-world consumer medical questions with long-form answer evaluations annotated by medical doctors. We performed pairwise comparisons of responses from various open and closed-source medical and general-purpose LLMs based on criteria such as correctness, helpfulness, harmfulness, and bias. Additionally, we performed a comprehensive LLM-as-a-judge analysis to study the alignment between human judgments and LLMs. Our preliminary results highlight the strong potential of open LLMs in medical QA compared to leading closed models.", "sections": [{"title": "1 Introduction", "content": "The majority of existing LLM evaluation benchmarks in medical question answering (QA) have focused on automatic metrics and multiple-choice questions [14, 9, 19, 22]. Although valuable, such metrics and question formats fall short of reflecting the realistic settings of real-world clinical scenarios [24, 19] and do not fully assess or capture the nuances and factual accuracy of LLMs in the medical domain [9, 23, 25]. Additionally, there are concerns about the potential leakage of well-known benchmarks into the training data of LLMs that are subsequently evaluated on those same benchmarks [3]. Furthermore, benchmarks that have not leaked may contain label errors or be outdated [18], leading to flawed and unrealistic evaluations. Moreover, the limited research that has explored human evaluation of long-form medical QA has not made the associated labels publicly"}, {"title": "2 Data Preparation", "content": "To create a dataset of real-world consumer medical questions, we collected queries by users on our platform, Lavita Medical AI Assist.2 In this phase, we collected all queries from 2023-10-31 (the date we launched our platform) to 2024-02-12 containing 4,271 inputs in 1,693 conversations. Conversations could be single-turn (1,011) or multi-turn (682). However, for now, we ignored the conversation types and processed queries independently, without considering whether they were part of the same conversation. The distribution of queries in conversations is shown in Figure 2. After collecting the queries, we deduplicated them, removed those from our sample question pool, and filtered non-English entries using Lingua.4 This step resulted in 2,698 queries."}, {"title": "2.1 Finding Medical Questions", "content": "User queries could be quite noisy and not all of them ask a clear and direct medical or health-related question. Additionally, some queries, even though medical, contain grammatical and/or spelling errors. Since manually filtering and correcting these cases is a time-consuming process, we prompted GPT-45 to 1) tell if a user query contains a direct medical or health-related question, and 2) correct"}, {"title": "2.1.1 Human Validation", "content": "Before relying on GPT-4's annotations for medical question detection, we created a random sample of queries and had two human annotators label the same questions we asked GPT-4. We created a representative sample size of 337 based on Cochran's formula [2]. There is a 94% agreement among the two human annotators on whether a query contains a direct medical or health-related question. And, in those cases the two human annotators agreed, there's also 91% agreement with GPT-4's predictions. One common pattern we observed in disagreements between humans and GPT-4 was in cases in which it is hard to find a clear, direct, and independent question. For example, What dose of spermidine did they use? Even though this question is health-related and medical, it is not clear who they is referring to. These cases happen especially when a question is part of a multi-turn conversation which is not the focus of our study here. Considering the high agreement between human annotators and GPT-4, we separated all queries labeled by GPT-4 as a medical question."}, {"title": "2.1.2 Quality Check", "content": "To ensure the grammatically corrected version of queries by the model are not significantly different than the original ones, we computed the similarity between the original and corrected queries with an 85% threshold after manually inspecting some samples to determine a reasonable threshold. This process resulted in 1,446 queries identified as medical questions. It is worth noting that using GPT-4's annotation, we were able to automatically cut 46% of queries that are non-medical. And, our prompt and pipeline could also be used in the future to filter out non-medical questions at a higher scale. Even though this is not the focus of our study, this experiment also shows the potential of using highly capable LLMs for medical question identification or classification.\nTo further check the quality of our questions, as the final step, one human annotator went through the 1,446 medical questions and removed any question that might have been wrongfully annotated by GPT-4 as a direct medical question. This process resulted in 1,298 remaining higher-quality medical questions."}, {"title": "2.2 Semantic Deduplication", "content": "In order to minimize similarity among the questions in our benchmark and to perform semantic deduplication, we conducted an intra-dataset similarity analysis to assess the degree of semantic similarity between the questions. For this analysis, we first computed the embeddings of all questions using OpenAI's text-embedding-3-large model with a dimension size of 1024. We then constructed a distance matrix of the embeddings using cosine distance, defined as 1.0 minus the cosine similarity"}, {"title": "2.3 Inter-Dataset Similarity Analysis", "content": "We also compared our dataset with three other well-known consumer health question datasets to measure the novelty of questions in our benchmark. These datasets include MedRedQA [15] with 51k pairs of consumer questions and their corresponding expert answers from Reddit (r/AskDocs/), HealthSearchQA [20] that includes 3,375 commonly searched consumer medical questions and introduced as a benchmark in the Med-PaLM project [20], and MASH-QA [28] with more than 34k question answer pairs sourced from WebMD. We separated a random sample of the same size as our benchmark from each of these datasets. To determine the similarity score between every two datasets, we used OpenAI's text-embedding-3-large model to create 1024-dimensional embeddings for the questions. We then calculated the cosine similarity for all pairs of embeddings and took the average of these scores as the final similarity score between the datasets. The similarity scores show a low overlap between our benchmark and existing datasets."}, {"title": "2.4 Difficulty Level Annotation", "content": "Before creating our annotation data batches, to ensure that each batch is diverse and contains questions with various levels of sophistication, we annotated the difficulty level of questions by GPT-4. \u03a4\u03bf define a scheme for difficulty levels of medical questions, we refined the medical tasks difficulty level scoring system introduced in Zhang et al. [26] and made it fit the medical question answering task. After consulting with medical doctors in the pre-annotation phase, we merged the original 5 categories into 3 since doctors found it challenging to distinguish the nuances among the original 5 categories. Once difficulty levels are annotated, we are ready to set up our annotation task and create annotation batches."}, {"title": "2.5 Annotation Batches", "content": "We create annotation batches each with 100 questions. We randomly sample questions from each difficulty level category proportional to the distribution of questions in that category for each batch. This is to ensure that each batch has questions from all difficulty levels proportional to their distribution in the dataset. Next, using the selected models for evaluation (Section 3.1) for each batch, we generate answers for each question in each batch and randomize the order of the answers to prevent any association between the answer order and a specific model. The prompt for answer generation is"}, {"title": "3 Annotation and Evaluation", "content": "In building our benchmark, we aim to gain insights into the following research questions for long-form medical QA:\n\u2022 RQ1) How do smaller-scale open medical LLMs, trained on different vanilla models, perform compared to one another?\n\u2022 RQ2) What is the performance gap between open medical/non-medical models and closed state-of-the-art (SOTA) models?\n\u2022 RQ3) What is the effect of additional pretraining using medical data on relatively strong open vanilla models?\nThese insights could help us better understand the capabilities of open models and, ultimately, build improved open medical models that are preferred over commercial models due to the latter's lack of transparency, privacy concerns, and unclear path toward HIPAA-compliant deployment."}, {"title": "3.1 Models", "content": "We selected the following set of open and closed medical and general-purpose LLMs for our evaluation: AlpaCare [26], a series of LLMs instruction-tuned for medical tasks. Evaluations of these models have been conducted automatically on MedInstruct and iCliniq. The instruction tuning"}, {"title": "3.2 Evaluation Criteria", "content": "There have been few studies that define a fine-grained annotation scheme for individual and pairwise evaluation of long-form answers in the medical QA domain. Perhaps the most comprehensive study is the series of works conducted in the development of the Med-PaLM models [20, 21].\nOur initial draft of the annotation scheme was inspired by Med-PaLM's approach. We merged some overlapping criteria in this scheme-for example, combining the \"extent of possible harm\" and \"likelihood of possible harm\" into a single criterion called harmfulness. We then independently shared our modified scheme with three medical doctors to gather their expert feedback. Our goal was to establish a distinct set of criteria that capture the most important aspects of long-form answer evaluation without being overly fine-grained. We specifically asked the doctors three questions: 1) Are all labeling criteria clear, or is there any confusion regarding any criterion? 2) Are there any additional overlapping criteria that could be merged? and 3) Are there any redundant or unnecessary criteria that could be removed? The doctors suggested further refinements, such as combining the axes of Unnecessary additional content and Missing important content into a single criterion called efficiency, which reflects how well an answer provides accurate medical knowledge without omitting relevant facts or including extraneous information. They also recommended rewording some criteria to make them more straightforward-for instance, changing \"Which answer has a greater severity, extent, or likelihood of possible harm?\" to \"Which answer poses a higher risk of causing harm?\". Our final annotation scheme is shown in Table 2."}, {"title": "3.3 Human Evaluation", "content": "We conducted our human evaluations with a group of three medical doctors, with two doctors assigned per batch, specializing in radiology and pathology. Before starting the main round of annotations, we shared our annotation scheme with the doctors and conducted a trial round with each on a small sample of questions.11 We then gathered the doctors' feedback to ensure that all annotation criteria were clear and that there was no ambiguity regarding the instructions. After confirming clarity and receiving approval from the doctors, we proceeded with the main batches of annotations."}, {"title": "3.4 LLM-as-a-Judge", "content": "To design our LLM-as-a-judge prompt template we merged the pair-v2 system prompt in Zheng et al. [27], and the pairwise evaluation template in WildBench [12]. Our full prompt is shown in Figure 12. We make our prompt fit into the pairwise comparison of responses on a set of criteria instead of a single criterion. We ran two LLMs as our judges including gpt-40-2024-05-13 and claude-3-5-sonnet-20240620."}, {"title": "3.4.1 Testing Robustness", "content": "Before using LLM judgments and comparing them with human evaluations, we ran an analysis to see how robust and consistent LLMs are in their judgment. We ran each LLM six times on each batch -three times with the default order of responses, Response A and B (we refer to the three votes from these three times as an ab run), and another three times with a reversed order of responses (votes referred to as a ba run) to ensure the LLM judgments were not affected by positional biases of responses. Out of all 4,800 runs12 given by LLMs, there were 174 cases (%4) among ab runs and 199 cases (%4) in ba runs where the LLM was not consistent in its judgment. And these inconsistencies are all from the gpt-40 model. Before moving to the next step, we resolved the inconsistencies by finding the majority vote for each run. There were only 3 cases in total where there was not a majority vote. In those cases, since tie was among the votes, we chose that as the majority.\nAfter finding the majority votes for each run, we compared the ab and ba run votes for each model individually. We found that in 806 cases (%17 of 4,800 total runs, 397 for gpt-40 and 409 for claude) there's a disagreement in model judgments between ab and ba runs. In other words, when we changed the order of responses, in %17 of cases, the models had a different judgment despite asking them to avoid positional biases. The highest disagreements were related to the efficiency, correctness, and reasoning criteria, and the least disagreement was for the bias criterion with only one case and only for the claude model (details can be found in Appendix A.2.) Following Zheng et al. [27], we took a conservative approach and set the vote as tie and neither (for harmfulness and bias criteria) to resolve the disagreements. Then we compared gpt-40 and claude judgments and measured the correlation among their votes. There's %96 percentage and %94 Cohen's kappa agreement between the two models with chance agreement being %33. In the end, we resolve the disagreement between the two models based on the conservative approach described earlier and consider one final vote as our LLM-as-a-judge vote."}, {"title": "4 Results and Analysis", "content": "The distributions of the cumulative number of votes by medical doctors, as well as LLM-as-a-judge votes for the four batches, are shown in Figure 4. Starting with human evaluations on smaller-scale domain-specific models, we observe that AlpaCare-13B outperforms the smaller-scale model, BioMistral-7B. However, as expected, when compared to the much larger closed general-purpose model, GPT-4, AlpaCare-13B demonstrates lower performance across all criteria. The results become more interesting when comparing one of the flagship closed models, GPT-40, with the state-of-the-art open model at the time, Llama-3.1 405B-Instruct, where Llama-3.1 outperforms GPT-4o across all aspects. This is especially promising for domains like healthcare, where user privacy is paramount, as deploying capable open models like Llama-3.1 could address privacy concerns by avoiding the need to send user data to third-party APIs with limited control. Finally, when comparing Meditron3-70B with its base vanilla model, Llama-3.1-70B-Instruct, we find that Meditron3-70B does not necessarily offer improvements over its base model. While we acknowledge that more rigorous and comprehensive evaluations are needed to generalize these conclusions, these results suggest that previous assumptions about the superiority of domain-specific medical or clinical models over general models or in-context learning approaches [11] may need to be revisited.\nTurning to LLM-as-a-judge results and their comparison with human evaluations, we find general agreement across all batches and criteria. However, there remains a noticeable gap in the alignment between LLM votes and human labels."}, {"title": "4.1 Annotator Agreement", "content": "The annotator agreements for all batches across evaluation criteria are shown in Table 3. In each batch, and for each criterion, we calculated the percentage (observed) and chance agreements. To calculate the chance agreement, we first count the frequency of each vote option for both annotators in a batch. Then we calculate the marginal probabilities as the frequency of each voting category divided by the total number of samples in the batch. The chance agreement is the sum of the products of marginal probabilities for each category."}, {"title": "5 Conclusion", "content": "In this work, we introduced a new publicly available benchmark with human expert annotations for long-form consumer medical question answering. Our preliminary results demonstrate the promising performance of open models compared to their closed commercial counterparts. Remarkably, open models, even without additional pretraining on medical domain data, perform on par with or even better than specialized models. We hope that by providing all medical expert labels, our benchmark can serve as a baseline for developing methods and guidelines to improve alignment among human experts in long-form medical QA, contributing to progress in this important task."}]}