{"title": "Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation", "authors": ["Yuhui Zhang", "Yuchang Su", "Yiming Liu", "Xiaohan Wang", "James Burgess", "Elaine Sui", "Chenyu Wang", "Josiah Aklilu", "Alejandro Lozano", "Anjiang Wei", "Ludwig Schmidt", "Serena Yeung-Levy"], "abstract": "The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.", "sections": [{"title": "1. Introduction", "content": "With the rapid advancements in vision language models (VLMs) [2, 27, 40, 51], rigorous and reliable evaluation methods are critical to gauge model performance and guide further research. Visual question answering (VQA) has emerged as a standard evaluation protocol, typically structured with open-ended or multiple-choice questions. Open-ended questions [13, 35, 58] allow models to generate free-form, natural language responses, while multiple-choice questions [31, 47, 59] ask models to choose the answer from predefined options. Most existing VQA benchmarks are in open-ended format [10, 23] because of the complexity and extensive resources required to design high-quality multiple-choice questions.\nWe revisit current evaluation strategies for open-ended questions, focusing on the challenge of measuring semantic similarity between model-generated and ground-truth answers (\u00a73). Two main evaluation methods exist: rule-based, which evaluates word or phrase level overlap, and model-based, which uses large language models (e.g., GPT-\n40) for semantic matching. While rule-based evaluation is computationally efficient, our experiments on the VQAv2 dataset [13] demonstrate that it fails to capture semantic nuances and formatting differences, yielding a poor correlation (0.09) with true VLM performance. In contrast, model-based metrics, though more semantically accurate, are costly and unstable updates in model versions (e.g., GPT-40 from 0513 to 0806) constantly increase scores by 6% on the MMVet dataset [58], making results incomparable and raising future reproducibility issues.\nTo mitigate these issues, we propose an alternative for VLM evaluation by converting open-ended VQA questions into multiple-choice format. This shift allows for more objective and reproducible assessment by providing defined options and simplifying answer validation. Nonetheless, creating multiple-choice questions is inherently complex, particularly in generating distractor options that are plausible yet challenging given the correct answer a process traditionally requiring extensive human expertise and effort to simulate different errors students could make.\nWe introduce AutoConverter (\u00a74), a novel multi-agent system powered by GPT-40, designed to automatically convert open-ended questions into challenging yet correct multiple-choice questions. To enhance difficulty, specialized distractor proposer agents collaborate with a reviewer agent in an iterative manner to generate a large pool of challenging distractors focused on common error types, including conceptual misunderstandings, visual misinterpretations, and reasoning errors. A selector agent then chooses"}, {"title": "2. Related Works", "content": "Vision language model evaluation. With the rapid development of vision language models (VLMs) [2, 27, 40, 51], numerous benchmarks have emerged to assess various capabilities of VLMs, including general understanding [6, 13,\n14, 24, 35, 47, 58], reasoning [17, 31, 32, 53, 56, 59],\nOCR [39, 49], and document and chart comprehension [19,\n20, 36\u201338]. These datasets typically use either open-ended or multiple-choice formats. Open-ended questions are easier to create but challenging to evaluate accurately, while multiple-choice questions simplify evaluation but demand greater effort in their creation. Previous studies have found limitations of open-ended evaluation for VLMs [5, 21, 34], often focusing on case studies and comparisons with human performance. Their proposed solutions are model-based evaluation methods [11, 64]. In this work, we systematically analyze various VLMs on two widely-used open-ended VQA datasets in zero-shot evaluation settings, quantitatively revealing significant challenges in VLM evaluation for open-ended questions. We demonstrate how rule-based evaluation poorly correlates with actual model rankings in zero-shot scenarios. Additionally, we highlight the substantial limitations of model-based evaluation: model-based evaluations are inconsistent across versions, which leads to serious reproducibility issues. These issues seem"}, {"title": "3. Open-Ended Question Evaluation Challenge", "content": "In this section, we discuss the challenges of evaluating vision language models (VLMs) using open-ended questions. The primary issue lies in accurately and robustly measuring the semantic similarity between model-generated answers and ground-truth answers, a long-standing challenge in natural language processing [12, 62, 64].", "subsections": [{"title": "3.1. Rule-based Evaluation", "content": "Background. When ground-truth answers are short, rule-based evaluation metrics such as exact match or quasi-exact match (e.g., BLEU [42], ROUGE [25]) are typically used to measure word-level overlap between the ground truth and model predictions. However, these methods are limited in accounting for semantic variations, such as synonyms, paraphrasing, or formatting differences. This limitation is even more pronounced when evaluating current VLMs in zero-shot settings, where models are instruction-tuned to generate slightly more detailed and lengthier responses for clarity.\nExperiment. To examine the limitations of these evaluation metrics, we tested 12 state-of-the-art VLMs on the widely-used VQAv2 dataset [13]. In a zero-shot setting, we prompted the models with, \"Please try to answer the question with short words or phrases if possible.\" and used VQAv2's official rule-based evaluation code. We then conducted a detailed analysis of the cases where GPT-40 did not receive full marks, using human judgment to correct scores where the rule-based evaluation failed. Based on these human annotations, we developed a model-based evaluation method using GPT-40 with a well-crafted prompt to assess the semantic match between model outputs and ground-truth answers. Our model-based metric achieved a 0.95 agreement with human evaluations on VQAv2, making it a reliable proxy for human judgment.\nFindings. Analysis of GPT-40's errors revealed that approximately 66% of the misclassifications were due to evaluation failures rather than the model's inability to answer the questions. With human corrections, GPT-40 achieved an accuracy of 88% on VQAv2, significantly higher than the 32% accuracy reported by rule-based metrics. A comparison between rule-based and model-based evaluations (i.e., human-proxy evaluation) showed a Spearman correlation of only 0.09, as seen in Figure 2 (left), demonstrating that rule-based metrics provide nearly random and unreliable scores, failing to distinguish between state-of-the-art VLMs."}, {"title": "3.2. Model-based Evaluation", "content": "Background. When ground-truth answers are longer, reflecting more realistic use cases for VLMs, rule-based metrics become even less effective at capturing semantic equivalence between predictions and answers. Consequently, the community increasingly relies on model-based evaluation [11, 64], which leverages advances in language models like GPT-40 with carefully designed prompts to generate similarity scores between predictions and answers. Although these similarity scores correlate well with human evaluations, changes in model versions can lead to significant shifts in evaluation results.\nExperiment. To evaluate the stability and robustness of model-based metrics, we tested 12 state-of-the-art VLMs on the MMVet dataset [58], where answer length is 63 words on average. Using two versions of GPT-40 (GPT-40-0806 and GPT-40-0513) as evaluators, we kept all other variables constant, such as model responses and evaluation prompts.\nFindings. While model-based evaluation is reliable in ranking models comparatively, it is costly and introduces notable shifts in absolute scores. As shown in Figure 2 (right), we observed a perfect 1.0 correlation in VLM rankings between the two GPT-40 versions, underscoring the model-based evaluation's strong alignment with human judgment. However, the absolute scores from GPT-40-0806 were consistently 6% higher than those from GPT-40-0513. Further analysis indicated that GPT-40-0806 tended to assign a score of 1.0 instead of 0.9 for nearly-correct responses, while GPT-40-0513 showed the opposite tendency. These differences in absolute values hinder the comparability of results across different model versions, significantly impacting the reproducibility of research findings, espe-"}]}, {"title": "4. AutoConverter: An Agentic Pipeline Generating Challenging Multi-Choice Questions", "content": "Given the challenge of evaluating open-ended questions for vision language models (VLMs) detailed in \u00a73, how can we mitigate these issues? We propose to convert open-ended questions into a multiple-choice format, capitalizing on the simplicity and objectivity of evaluating multiple-choice questions. However, traditionally, creating multiple-choice questions, especially reasonable yet challenging distractor options, requires substantial human expertise and effort. In this section, we introduce AutoConverter, an agentic pipeline that automatically generates high-quality multiple-choice questions from open-ended ones.", "subsections": [{"title": "4.1. Problem Formulation and Key Desiderata", "content": "We define the multiple-choice question conversion process as follows: given an image, a question, and the correct answer $(v, q, \\alpha) \\in X \\times Y \\times Y$, where $X$ and $Y$ represent sets of images and texts respectively, we aim to generate a set of distractor choices $C_a = \\{c_i \\in Y | i \\in [N]\\}$, where $N$ is the number of distractors. We define the candidate choice set as $C = C_a \\cup \\{a\\}$, forming the final multiple-choice question as $(v, q, C)$. We choose $N = 3$ in our work because 4-choice is the most common configuration for multiple-choice questions and minimizes the risk of option selection bias for language models [63].\nTwo key desiderata are crucial for the multiple-choice conversion process. The first is correctness, ensuring that the multiple-choice question is valid with only one correct answer. The second is difficulty, ensuring that the ques-"}, {"title": "4.2. Ensuring Correctness", "content": "Correctness is essential when converting open-ended questions to a multiple-choice format, defined as having exactly one correct answer. Since our conversion process retains the original open-ended question stem and its answer, we only need to ensure that the other distractors are incorrect given the question and its correct answer. To achieve this, we employ a multi-agent framework to rigorously evaluate distractor options, further enhancing the correctness of the generated multiple-choice questions through iterative refinement.\nEvaluating correctness. We use GPT-40 as a correctness checker, which evaluates each distractor's similarity to the correct answer and assigns a 5-point Likert score for the overall correctness of the question. A score of 5 indicates strong confidence in having a single correct answer, while a score of 1 suggests ambiguity or the presence of multiple correct answers. The evaluator was developed based on a small-scale study of correct and incorrect questions. To validate the effectiveness of this evaluator, we conducted large-scale human annotations on 2,400 questions to assess their correctness (details in \u00a75). Our results indicate that 51%, 51%, 63%, 84%, and 95% of questions are deemed correct when assigned correctness scores of 1, 2, 3, 4, and 5, respectively, by our evaluator. Thus, we can use this correctness evaluator as a judge to assess question correctness and refine questions with low correctness scores.\nRefining questions to enhance correctness. Since the correctness evaluator can accurately flag problematic ques-"}, {"title": "4.3. Increase Difficulty", "content": "Difficulty is another critical factor in designing effective multiple-choice questions. Our goal is to prevent questions from being trivially answered due to obviously incorrect options or shortcuts, ensuring that each question retains the discriminative power needed to rigorously test VLM capabilities. Inspired by human strategies for creating challenging distractors, we introduce a multi-agent framework powered by GPT-40. This framework iteratively generates and refines a large set of difficult distractors from multiple perspectives and ultimately selects those that pose the highest level of difficulty.\nGenerating a diverse set of distractors. We first create a large pool of candidate distractors based on common error types to capture a range of potential challenges. We categorize these error types as follows: concept misunderstanding, visual misinterpretation, reasoning error, data processing error, and question bias. Detailed definitions for each error type are provided in the Appendix. For each error type, GPT-40 proposes a set of distractors based on the image, question, and correct answer, along with accompanying rationales. This pool of candidates forms a comprehensive"}, {"title": "4.4. AutoConverter Results", "content": "To evaluate the effectiveness of AutoConverter in generating high-quality multiple-choice questions, we leverage three existing multiple-choice VQA datasets: MMMU [59], MathVista [32], and AI2D [19]. We retain all questions with four choices and regenerate the distractors based on the image, question, and correct answer. We then compare AutoConverter-generated distractors with the original human-crafted ones by evaluating various VLMs on both the original and converted datasets.\nAutoConverter generates correct multiple-choice questions. Using a correctness evaluator and refiner pipeline, we ensure the accuracy of the generated questions. Our analysis shows that the generated questions maintain a correctness level comparable to that of the original human-crafted questions. After filtering to keep only questions with a correctness score of 5, large-scale human annotation indicates that only 3% of questions are marked as incorrect for MMMU, MathVista, and AI2D. Among these, 52% of errors are due to incorrect original answers, while only 48% are introduced by the AutoConverter process. These results demonstrate that most AutoConverter-generated questions are accurate and suitable for reliable VLM evaluation.\nAutoConverter generates challenging multiple-choice questions. As shown in Figure 4, AutoConverter produces highly challenging questions, with a broad range of VLMs achieving similar or even lower accuracy on the generated distractors compared to the original human-crafted ones across MMMU, MathVista, and AI2D. Notably, MMMU is regarded as one of the most challenging VQA datasets, as it is sourced from exams and textbooks. These results highlight AutoConverter's capability not only in converting open-ended questions to multiple-choice but also in refining existing multiple-choice questions to enhance their difficulty. Furthermore, AutoConverter could have applications beyond VLM evaluation, such as generating challenging questions for educational purposes.\nEach agent in AutoConverter contributes to jointly improving correctness and difficulty. To analyze each agent's role in AutoConverter, we conduct an ablation study by removing each component, as shown in Table 3b. First, removing specialized error proposers results in a decrease in difficulty, with an average increase of 1.6% in relative accuracy. Next, omitting the reviewer for iterative distractor refinement leads to a 4.9% relative increase in average VLM performance, indicating lower difficulty. Finally, removing the question evaluator and refiner, which serve as correctness safeguards, causes an 8.7% relative decrease in correctness score. We also compare AutoConverter with a naive baseline that uses the prompt \"Please generate 3 distractors given the question, answer, and image.\" As shown"}]}, {"title": "5. VMCBench: A Unified Multiple-Choice Visual Question Answering Benchmark", "content": "Using our AutoConverter method, detailed in \u00a74, we introduce VMCBench a benchmark that unifies 20 existing visual question answering (VQA) datasets into a consistent multiple-choice format. VMCBench spans a diverse array of visual and linguistic contexts, rigorously testing various model capabilities. By transforming open-ended questions into multiple-choice format, VMCBench enhances vision language model evaluation by mitigating ambiguities while preserving the complexity of the tasks. This benchmark provides a reliable and reusable resource for the evaluation of future vision language models (VLMs).", "subsections": [{"title": "5.1. Benchmark Overview", "content": "VMCBench transforms 20 widely-used VQA datasets into a unified multiple-choice benchmark. These datasets can be broadly categorized to assess general capabilities of VLMs (VQAv2 [13], OKVQA [35], MMVet [58], VizWiz [14], A-OKVQA [47], MMStar [6], SEEDBench [24]), reasoning capabilities (MathVision [53], GQA [17], MMMU [59], RealWorldQA [56], MathVista [32], ScienceQA [31]), OCR tasks (OCRVQA [39], TextVQA [49]), and document and chart understanding (DocVQA [37], InfoVQA [38], ChartQA [36], TableVQABench [20], AI2D [19]).\nAmong these, 12 datasets consist of open-ended questions, which we convert into the multiple-choice format to facilitate evaluation. The remaining 8 datasets are already in multiple-choice format; for these, we apply AutoConverter to refine distractors, increasing their difficulty. These refined datasets can further test model robustness and help identify dataset contamination [15, 41, 46, 61].\nFor each dataset, we randomly sampled up to 500 questions, as recent studies suggest this sample size may suffice for evaluating model performance [44, 45], resulting in a total of 9,450 questions across the 20 datasets. We apply AutoConverter to these 9,450 questions.\nTo ensure the correctness of the converted questions, AutoConverter includes an internal correctness evaluator that assigns a correctness score to each converted question, ensuring only one correct answer exists. Out of the 9,450 converted questions, 103, 162, 399, 635, and 8,151 questions received correctness scores of 1, 2, 3, 4, and 5, respectively. Six PhD student annotators reviewed all questions with correctness scores below 5 and randomly sampled 1,101 questions with a score of 5, resulting in a total of 2,400 annotated questions. The correctness rates for questions with scores of 1, 2, 3, 4, and 5 were 51%, 51%, 63%, 84%, and 95%, respectively. This high accuracy of the correctness evaluation supports the reliability of questions with a confidence score of 5 for future dataset construction. Additionally, we annotated the source of errors, distinguishing between errors from the ground-truth answers (original dataset errors) and those introduced by AutoConverter. Notably, 59% of errors were due to issues in the ground-truth answers, indi-"}]}, {"title": "5.2. Evaluation Results", "content": "We evaluated 33 state-of-the-art vision language models (VLMs) on the VMCBench test set to assess their performance, including GPT-4 [40], Gemini-1.5 [51], Claude-3.5 [2], Qwen2-VL [54], Molmo [8], Cambrian [52], VILA [26], CogVLM [16], Phi [1], Idefics2 [22], DeepSeek-VL [29], PaliGemma [3], LLaVA1.5 [27], Chameleon [50], and InstructBLIP [7]. The evaluation is conducted in the zero-shot setting with prompt \u201cQuestion: {question} Options: A. {A} B. {B} C. {C} D. {D} Answer with the option's letter from the given choices directly.\" The results are summarized in Table 1. Detailed results are in Appendix.\nOur findings reveal several key insights:\nPrivate models remain the top performers, but the gap is narrowing. As shown in Table 1, the best-performing model on VMCBench is GPT-40, achieving 80.6% overall accuracy. However, the gap between GPT-40 and the second-best model, Qwen2-VL-7B, is only 2.0%. This open-source model demonstrates that the performance difference between private and public models is decreasing. Rapid advances in VLM development. Another notable trend is the significant improvement from Instruct-BLIP to Qwen2-VL-7B, nearly doubling in performance (40.9% vs. 78.6%). These models were released in 2023 and 2024, respectively, illustrating the rapid pace of progress in VLMs.\nScaling up models generally improves performance. As expected, scaling up model sizes typically leads to performance gains. For instance, across different model families (such as VILA, Cambrian, Chameleon, DeepSeek, and LLaVA), we observe a clear improvement with larger models, similar to scaling trends in language models [4, 18].\nEvaluating across diverse datasets is essential to uncovering model limitations. A comparison between Phi-3 and Phi-3.5 reveals that while Phi-3.5 achieves better performance on some reasoning datasets such as MMMU and MathVista, it performs significantly worse on OCR and document/chart understanding tasks, which reduces its overall performance. This underscores the importance of a holistic evaluation of models across a broad range of datasets. Previously, this was challenging due to variations in dataset formats and evaluation protocols. VMCBench addresses this by unifying the task into a multiple-choice format, thus reducing evaluation costs and standardizing comparisons.\nContinuous feature inputs outperform discrete tokens. Chameleon is the only model in our experiments that utilizes VQGAN discrete tokens, yet it performs significantly worse than smaller models trained with continuous features using less computational resources. This observation raises an intriguing question: why might VQGAN tokens be less effective for image understanding tasks? We believe that VMCBench is a valuable benchmark that"}, {"title": "6. Conclusion", "content": "We address limitations in open-ended visual question answering benchmarks for evaluating vision language models (VLMs) by introducing AutoConverter, a multi-agent system that transforms questions into multiple-choice format with challenging distractors. AutoConverter effectively creates questions that match or exceed the difficulty of human-written distractors, with VLMs often achieving similar or lower accuracy. Building on this, VMCBench provides a benchmark of 9,018 multiple-choice questions, setting a new standard for consistent and rigorous VLM evaluation."}, {"title": "Broader Impacts and Ethics Statement", "content": "AutoConverter and VMCBench simplify and standardize vision language model (VLM) evaluation, providing a valuable tool for advancing VLM development through scalable, consistent, and reproducible evaluation. Beyond VLM evaluation, AutoConverter can be applied to other domains, such as education, to generate challenging and high-quality questions. However, human verification is essential to ensure the generated questions align with their intended purpose, maintain proper value, and achieve the appropriate level of difficulty without introducing biases."}, {"title": "Reproducibility Statement", "content": "We provide an open-source implementation of our work at https://github.com/yuhui-zh15/\nAutoConverter. This will enable researchers to reproduce all the experiments in paper and conduct their own analyses."}, {"title": "Limitations", "content": "AutoConverter achieves high question correctness, but 5% of the questions with the highest correctness scores still contain errors, although half of which are due to flaws in the original datasets. Moreover, while VMCBench includes a diverse range of models and datasets, its coverage remains incomplete. Moving forward, we aim to leverage AutoConverter to expand dataset and model coverage, addressing evolving needs in VLM evaluation."}, {"title": "Summary of Appendix", "content": "\u2022 In \u00a7A, we present further analysis of the evaluation failures for open-ended questions discussed in \u00a73.\n\u2022 In \u00a7B, we provide supplementary details on the AutoConverter framework described in \u00a74.\n\u2022 In \u00a7C, we elaborate more details of VMCBench as outlined in \u00a75."}, {"title": "A. Supplementary Section 3", "subsections": [{"title": "A.1. Examples of Rule-based Evaluation Failures", "content": "In the main paper, we demonstrated that rule-based evaluation of open-ended questions leads to poor evaluation outcomes. Table 2 provides six examples of rule-based evaluation failures, clearly illustrating that these methods fail to account for semantic similarity and penalize formatting errors, resulting in highly inaccurate evaluation results."}, {"title": "A.2. Examples of Model-based Evaluation Failures", "content": "In the main paper, we demonstrated that model-based evaluation is sensitive to model versions. To investigate this further, Table 3 presents six examples of inconsistencies caused by different model-based evaluations. We observe that GPT-40-0806 often assigns a perfect score of 1.0 for similar predictions and answers, whereas GPT-40-0513 tends to assign a score of 0.9. This behavior variation introduces significant differences in evaluation results and raises concerns about reproducibility in future research."}]}, {"title": "B. Supplementary Section 4", "subsections": [{"title": "B.1. Prompts for AutoConverter", "content": "In the main paper, we introduced the agentic system of AutoConverter, comprising the proposer, reviewer, selector, evaluator, and refiner agents. This system creates a large pool of high-quality distractor options to increase question difficulty while ensuring correctness in the converted multiple-choice questions.\nDetailed prompts for the proposers, designed to create distractors addressing vision, reasoning, data, concept, and bias errors, are shown in Figures 11, 12, 13, 14, and 15, respectively. These prompts are carefully crafted to align with common types of human errors, as defined in the corresponding sections.\nFigure 16 presents prompts for the reviewer, whose feedback iteratively refines the distractors to improve their quality. The selector prompts in Figure 17 guide the selection of the three most challenging distractors to enhance question difficulty.\nFigures 18 and 19 show prompts for the evaluator and refiner, respectively. This iterative process ensures the correctness of the generated questions, guaranteeing that there is only one correct answer."}, {"title": "B.2. AutoConverter Results for Additional Datasets", "content": "In the main paper, we demonstrated AutoConverter's ability to generate challenging multiple-choice questions for three datasets: MMMU, MathVista, and AI2D. Figure 7 extends this analysis to five additional datasets with human-created distractors: A-OKVQA, RealWorldQA, ScienceQA, SEEDBench, and MMStar. Across these datasets, VLMs consistently achieved similar or lower accuracy on AutoConverter-generated questions compared to the original ones, demonstrating the system's capability to produce highly challenging multiple-choice questions."}, {"title": "B.3. AutoConverter Results for Different Models", "content": "In the main paper, we constructed AutoConverter using GPT-40. A potential concern is whether this choice introduces bias into the generated questions.\nTo examine this, we used three state-of-the-art proprietary VLMs-GPT-40, Claude-3.5-Sonnet, and Gemini-1.5-Pro-to generate questions. We evaluated various VLMs on these questions and computed the correlations of their performance rankings. If no bias exists, we expect"}, {"title": "B.4. Correlation between Open-ended and Multiple-choice Questions", "content": "In the main paper, we highlighted the challenges of accurately evaluating open-ended questions. Therefore, we propose an alternative solution to convert open-ended questions into a multiple-choice format. A key question here is whether converting open-ended questions into multiple-choice format preserves their discriminative power.\nTo address this, we treat model-based evaluation of open-ended questions as a proxy for ground-truth evaluation. Specifically, if a model-based evaluator determines that model A outperforms model B, we consider this ranking correct. This assumption is valid because model-based evaluations have a high correlation with human judgments, albeit with instability across versions. By using the same GPT version as the evaluator, we eliminate such instability.\nWe compare the correlation between model-based evaluation of open-ended questions and rule-based evaluation of multiple-choice questions against the correlation between model-based and rule-based evaluation of open-ended questions. Our findings, shown in Figure 9, reveal that the correlation for multiple-choice questions is significantly higher-0.85, 0.71, and 0.97 for VQAv2, OKVQA, and VizWiz, respectively-compared to rule-based open-ended evaluations, which achieve correlations of 0.09, 0.19, and 0.00. This demonstrates that converting to multiple-choice questions improves evaluation accuracy and retains discriminative power."}]}, {"title": "C. Supplementary Section 5", "subsections": [{"title": "C.1. Human Evaluation Results on VMCBench", "content": "VMCBench provides a scalable, consistent, and reproducible benchmark for evaluating and advancing VLMs. The current best-performing model, GPT-40, achieves an accuracy of 80.6%.\nTo assess the remaining room for improvement, we conducted a human evaluation of VMCBench. Human experts achieved an accuracy of 91.7%, highlighting significant opportunities for further model improvement. Among the 8.3% human errors, approximately three-quarters of the questions require extensive knowledge, while a quarter are ambiguous and unanswerable."}, {"title": "C.2. Full Evaluation Results on VMCBench", "content": "In the main paper, we reported model performances grouped by benchmarked capabilities. Table 4 presents the full evaluation results for 33 VLMs across 20 individual benchmarks, further validating our conclusions."}, {"title": "C.3. Scaling Trends on VMCBench", "content": "Scaling laws are a cornerstone of model development, demonstrating that larger models typically achieve better performance. To explore this, we plotted the scaling trends of VLM families in log scale based on known model sizes, as shown in Figure 10.\nSurprisingly, we observe a clear log-linear scaling trend across most VLM families, indicating that VMCBench offers a smooth evaluation gradient for varying capabilities. Certain model families outperform others, leaving further exploration of these scaling trends to future work."}, {"title": "C.4. Additional Examples from VMCBench", "content": "To provide a deeper understanding of VMCBench, we include 60 examples (three from each of the 20 sources) in Tables 5 through 11. These examples demonstrate the high quality of our dataset and offer valuable insights for readers."}]}]}