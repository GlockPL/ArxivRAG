{"title": "Simplifying Formal Proof-Generating Models\nwith ChatGPT and Basic Searching Techniques", "authors": ["Sangjun Han", "Taeil Hur", "Youngmi Hur", "Kathy Sangkyung Lee", "Myungyoon Lee", "Hyojae Lim"], "abstract": "The challenge of formal proof generation has a rich history,\nbut with modern techniques, we may finally be at the stage of making\nactual progress in real-life mathematical problems. This paper explores\nthe integration of ChatGPT and basic searching techniques to simplify\ngenerating formal proofs, with a particular focus on the miniF2F dataset.\nWe demonstrate how combining a large language model like ChatGPT\nwith a formal language such as Lean, which has the added advantage\nof being verifiable, enhances the efficiency and accessibility of formal\nproof generation. Despite its simplicity, our best-performing Lean-based\nmodel surpasses all known benchmarks with a 31.15% pass rate. We ex-\ntend our experiments to include other datasets and employ alternative\nlanguage models, showcasing our models' comparable performance in di-\nverse settings and allowing for a more nuanced analysis of our results.\nOur findings offer insights into AI-assisted formal proof generation, sug-\ngesting a promising direction for future research in formal mathematical\nproof.", "sections": [{"title": "Introduction", "content": "Writing mathematical proofs in formal language has been a research topic for\nmany years. Notably, we recall McCarthy's symbolic computation on machines\n[18], Hoare's axiomatic approach to computer programming [13], and DeMillo,\nLipton, and Perlis's skepticism towards formal proofs of theorems [8]. Due to the\nsurprising performance of AI algorithms witnessed over the last decade, such as\ntransformers in natural language processing, we believe it is an opportune time\nto revisit the formal proof challenge with the modern techniques available. As\nevidenced by our literature review below, we are not alone in this pursuit."}, {"title": "Related Work", "content": "Mathematical theorem proving in machine learning has garnered significant at-\ntention, particularly for developing artificial intelligence capable of logical rea-\nsoning. As a result, various studies have been conducted, with those most closely\nrelated to our work outlined below.\nFormal and informal languages Languages used for the integration of ma-\nchine learning with mathematical reasoning can be broadly categorized as either\nformal or informal. Informal language has a format that is familiar to the human\neye and is thus easily understood and used. The authors in [7] propose a machine\nlearning-based framework and leverage informal language to offer mathematical\ninsight, and the models in [10, 28] are fine-tuned to prove mathematical theorems\nin informal language. However, informal language proofs have the disadvantage\nof being difficult to verify. To address this issue, many studies have utilized ver-\nifiable programming languages, spanning across formal languages such as Lean\n[2, 11, 20, 32], Metamath [21], HOL4 [29], Isabelle [14, 15, 27], Coq [31], Python\n[5], or a mix of these environments [16, 23]. These formal languages can represent\nmathematical ideas precisely, making them easier to utilize in a computing sys-\ntem. However, formal languages differ vastly from the mathematical arguments\nwritten by humans, making it difficult to interpret without ample understanding"}, {"title": "Background", "content": "Lean is an interactive theorem prover based on dependent type theory, and allows\nus to use mathematical objects such as definitions, theorems, and lemmas [19].\nLean utilizes tactic expressions, as do some other formal languages, which can\nbe helpful in writing proofs. For example, have is a tactic that can create a\nnew subgoal to act as a stepping stone in a theorem's proof, enabling users\nto construct a forward proof. Lean is popular in the mathematical community,\nserving as a base for active projects such as the Aesop and the Liquid Tensor\nexperiments [17, 22]. Furthermore, there is a massive library called mathlib in\nLean, which covers various mathematical topics. Considering the above aspects,\nwe selected Lean as a proof assistant for our experiments. Our main experiment\nutilizes the GPT-4 Turbo model of ChatGPT. This model generates proofs using\nLean 3 syntax by default, and considering its pre-training period, it is unfamiliar\nwith Lean 4. Therefore, we conducted all our experiments using Lean 3, with the\nexception of our further analysis involving Llemma, for which we used Lean 4,\nfollowing the methodology outlined in the existing Llemma paper."}, {"title": "MiniF2F", "content": "The dataset miniF2F [33] is a cross-system benchmark for formal mathemat-\nical proof, formalized in the formal languages Lean, Metamath, and Isabelle.\nMiniF2F consists of problems selected from the MATH dataset [12]; Olympiad-\nlevel problems, chosen from past AIME, AMC, and IMO questions; and CUS-\nTOM problems, derived from topics in algebra, number theory and induction.\nThe MATH dataset is divided into 5 levels of difficulty: problems in levels 1 to 3\nrequire only simple calculations and straightforward approaches, while questions\nin levels 4 to 5 have more complex solutions. The Olympiad and CUSTOM prob-\nlems in miniF2F range from high school-level to undergraduate-level mathemat-\nics. In general, solving the problems in these two categories necessitates more\nlogical reasoning or stepwise proofs, sometimes by introducing premises to aid\nthe proof."}, {"title": "Proof Search Algorithm", "content": "In [11, 23, 32], the general proof search process of the proof-generating model\nvia Lean is as follows:\n(1) The model takes a tactic state which includes the hypotheses and goals in\nthe current stage.\n(2) The model generates n many tactics to advance to the next goal or to gen-\nerate new hypotheses.\n(3) Each generated tactic is sent to Lean to obtain new goals or hypotheses,\nupdating the tactic state. (We henceforth refer to this step as interaction\nwith Lean.)"}, {"title": "Characteristics of Our Model", "content": "Often, a pre-trained model is fine-tuned on newly constructed data to create a\nmodel tailored for a specific task. However, the process of constructing a proper\ndataset itself is demanding. Even if the data is well-structured, fine-tuned mod-\nels frequently exhibit suboptimal performance on out-of-distribution data (e.g.,\n[11, 32]). Thus, we choose to employ ChatGPT without any additional fine-\ntuning, which relieves the burden of further training. Additionally, our exper-\niments primarily utilize ChatGPT with prompt messages containing just two\nsimple guidelines, while COPRA [23] generates the desired output through rel-\natively complex prompt messages.\nThe development of the existing models can be broadly divided into three\napproaches: (i) creating new training data or improving existing data [11, 26, 32],\n(ii) conducting additional training after fine-tuning [16, 20, 29], or (iii) introduc-\ning appropriate proof search algorithms [9, 14, 15, 23]. We choose to focus on the\nlast method to improve performance, and as such, modify the existing algorithms\nto suit our purposes. We design two types of proof search algorithms following a\nsimilar process as in Section 2.3 - one with the properties of a breadth-first search,\nwhich we call the b-search, and the other with the properties of a depth-first\nsearch, which we call the d-search. While the b-search requires more requests to\nChatGPT and more memory, it generally achieves greater performance. In con-\ntrast, the d-search usually has lower performance but requires fewer resources.\nWe anticipate that the combination of these two types of searches with Chat-\nGPT will lead to the exploration of a wider range of tactics. The details of our\nmodified proof search algorithms are explained below.\nThe b-search follows closely with the search process outlined in Section 2.3,\nwith the number of generated tactics set to 64 through the ChatGPT hyperpa-\nrameter \"n\" in API reference5. This method resembles the beam search-based\nproof search in ReProver [32] and Llemma [3]. However, instead of looking at\nn many tactics through beam search as ReProver and Llemma do, the b-search\ngenerates tactics that have a very high probability of correctly continuing the\nproof but are likely to be redundant. This slows down the model, so we per-\nform an additional step after step (2) in Section 2.3 to eliminate the doubles\nby deduplicating the generated tactics for each tactic state. We also regulate"}, {"title": "Experiments", "content": "In this section, we present the various experiments we conducted by employing\nChatGPT, which is easily accessible to the general public, to achieve performance\ncomparable to current models. By applying our search algorithms, we attain\nslightly higher performance than that of the COPRA models. Additionally, we\ninclude the results of the ReProver and COPRA models for a comprehensive\ncomparison with our output."}, {"title": "Experimental Setup", "content": "We classify our models based on the methods used in their proof search processes,\nas follows:\n1. bChatLean, based on b-search, and\n2. dChatLean, based on d-search.\nWe incorporate a feedback algorithm using Bad(O), as discussed in Section 3,\ninto the dChat Lean model to get a new model, which we refer to as dChatLean+."}, {"title": "Main Result", "content": "Our models are significantly influenced by the diversity of output from Chat-\nGPT, which is affected by the temperature setting. In our experiments, we ex-\nplore two temperature values: 0.7 and 1.4. As for other hyperparameters, in\nbChat Lean, we fix the number of tactic samplings n in a single step to 64, and\nin dChatLean, we investigate the effect of varying the number of attempts k to\nsolve one theorem, comparing results at values of 1, 10, and 50.\nTo interact with Lean, we utilize LeanDojo [32], which allows parallel im-\nplementation, easily checks time limits, and is well-organized. The time limit\nto prove one theorem is set to 600 seconds, consistent with other papers. All\nexperiments in this section are conducted in parallel on the CPU. The source\ncode for our algorithms and experiments discussed in this paper is available on\nGitHub."}, {"title": "Ablation Studies", "content": "On the number of attempts in dChat Lean Generating proofs by using\ndChat Lean is significantly more challenging than with other models that utilize\nbreadth-first search or depth-first search algorithms. One of the reasons for this\nis that dChatLean restarts the proof generation from the initial state if it fails.\nAdditionally, any minor typos in dChat Lean's tactic generation can critically\nimpact the model's pass rate. We observe that, as the number of attempts k\nmade on a problem increases, dChat Lean's proof generation stabilizes, directly\nenhancing the overall performance of the model. In GPT-4 Turbo\nwith temperature T = 0.7, an increase of k from 10 to 50 yields a performance\nincrease of 6.97%. The same trend holds for GPT-4 Turbo under T = 1.4, as\nthe addition of attempts from 10 to 50 shows a performance rise of 8.61%.\nThis notable increase can be attributed to the model attempting many proof\ndirections by generating more diverse tactics multiple times, confirming that\nincreasing the number of attempts on a proof produces a higher pass rate.\nOn tactic diversity Since dChatLean only produces one tactic at a time, it\nis important to explore multiple possibilities by generating a variety of tactics\nin subsequent attempts. Additionally, since bChatLean uses ChatGPT's n hy-\nperparameter, it is necessary for the model to produce more diverse outputs\nto avoid repeated attempts. In this context, temperature is a crucial parameter\nthat influences the model's pass rate."}, {"title": "Evaluation Across Datasets and LLMS", "content": "We evaluate our models on the ProofNet dataset and the 2023 AMC 12 prob-\nlems to verify our models' abilities to generate proofs for problems outside of the\nscope covered by miniF2F. We also assess our algorithms using another language\nmodel, Llemma, in place of ChatGPT. Despite the simplicity of our models, the\nresults demonstrate performance comparable to established benchmarks, effec-\ntively addressing mathematical problems across diverse contexts and settings.\nOn ProofNet We used our bChat Lean and dChatLean+ coupled model on\nProofNet, a benchmark for undergraduate-level mathematics, covering Anal-\nysis, Abstract Algebra, Linear Algebra, and Topology, as well as a topic la-\nbeled \"Examinations\" that consists of some Putnam Competition problems. The\nProofNet set is composed of 350 problems drawn from mathematics textbooks\nand formalized in Lean 3. When running our model on this benchmark, we\nachieve an overall performance of 13.14%, as seen in Table 5, and generate 35\nnew Lean proofs. The performance in Table 5 outlines the pass rate across differ-\nent mathematical topics. In particular, our model performs best with Analysis\nproblems, with a pass rate of 15.91%, and conversely, finds Linear Algebra and\nTopology problems more challenging, with pass rates of 10.00% and 10.71%,\nrespectively. This domain-based comparison informs us of the specific strengths\nand weaknesses within our models, providing direction for future enhancements.\nOn AMC 12 To conduct our evaluation, we selected problems from the 2023\nAMC 12A9 and 12B10 exams, both of which make up the high school mathemat-\nics problem-solving competition that acts as a first step towards Olympiad-level\ntests. These problems were published after the training for the ChatGPT version\nthat we use for this paper, and thus were outside of the possible scope of prob-\nlems that could have slipped into the training set for the model. We formalized"}, {"title": "Using Llemma", "content": "Though our models are based on search algorithms that specif-\nically cater to ChatGPT and its properties, we verify our proof searching al-\ngorithms' capabilities with the mathematics-specialized large language model\nLlemma [3]. In parallel with the ChatGPT-based models, we call the Llemma-\nbased models bLlemLean, dLlemLean, and dLlemLean+. These models are based\non the b-search, d-search, and the d-search with Bad(O), respectively. They con-\ntinue to use Lean as their formal language, but since the established performance\nof Llemma is dependent on Lean 4, we adopt Lean 4 in this experiment rather\nthan Lean 3. The miniF2F pass rates for these models and their combinations\nare outlined in Table 6.\nUnlike with our ChatGPT models, our dLlemLean+ model performs worse\nthan our dLlemLean model, indicating that the d-search method suffers with the\nintroduction of Bad(O). We can attribute this to the fact that Bad(O) generally\nmakes longer prompts, which Llemma struggles with due to the far fewer pa-\nrameters used in comparison to ChatGPT. However, Bad(O) has its advantages\nwhen d-search is used together with b-search, as the bLlemLean and dLlemLean\ncombined model performs a little worse than the bLlemLean and dLlemLean+\ncombined model does. This suggests that Bad(O) still aids d-search in finding\nproofs that the b-search fails to find alone.\nWe achieved the highest performance of 28.28% using the combination of\nbLlemLean and dLlemLean+, surpassing the benchmark of 26.23% previously\nreported for Llemma. Although our proof search algorithms were specially cus-\ntomized to enhance the ChatGPT output for our purposes, that the Llemma-"}, {"title": "Additional Analysis with Examples", "content": "In this section, we analyze our results based on specific criteria with examples."}, {"title": "Considering the number of attempts in dChatLean+", "content": "In our experiments with dChatLean+, where k was set to be strictly greater\nthan 1, we found that for the problems solved in only one attempt, the proofs\ninvolved either performing a simple calculation or simply solving a system of\nequations, as illustrated in the left example of Figure 1. In contrast, problems\nsolved with more than one attempt included proofs demonstrating that given\nproperties hold, as shown in the right example of Figure 1. These findings suggest\nthat increasing the number of attempts in dChatLean+ may enable the model to\nidentify approaches that require additional premises beyond basic calculations.\n\nMoreover, we observe that, for the problems with successful proofs, the av-\nerage number of attempts taken with dChatLean is 10.3, while just 9.3 with"}, {"title": "Considering the feedback algorithm", "content": "The proof generation process using the Bad(O)-based feedback algorithm in\ndChatLean+ can be seen in Figure 2, where the model completed the proof\nfor the miniF2F problem \u201cmathd_algebra_171\" in 21 attempts. The completed\nproof, 21 try, begins with rwho, and one can note many previous failed attempts\nthat also started with rw ho. These attempts were stored in Bad(O) to avoid\nrepeating the same failure when faced with the same tactic state. We depict a\nfew selected trials in Figure 3 and walk through the proof search process below.\n\u2460 On the sixth trial, the model reached rw ho and simp [ho]. To continue\nthe proof, it then generated the tactic rw [\\leftarrow ho 1], but resulted in failure."}, {"title": "Considering challenging problems", "content": "Our bChat Lean model solves a problem named \"induction_1pxpownlt1pnx\" from\nthe miniF2F dataset with the proof presented in Figure 4. This problem cor-\nresponds to Bernoulli's inequality, which we believe is intended to be proved"}, {"title": "Conclusion", "content": "In this paper, we proposed two simple proof search methods: the b-search and\nthe d-search. These gave rise to our bChat Lean and dChat Lean models, which\nemploy a pre-trained large language model in ChatGPT and achieve comparable\nperformance to that of state-of-the-art models even in the absence of fine-tuning,\nas demonstrated on the miniF2F dataset. In addition, we improved the perfor-\nmance of these two models by coupling bChatLean with dChatLean+, which\nintroduces a feedback algorithm into dChat Lean. To test the capabilities of our\napproach, we applied our models and proof search algorithms on other datasets,\nnamely ProofNet and the 2023 AMC 12, and to the other large language model,\nLlemma. We analyzed our models through ablation studies on factors such as\nthe number of attempts, the temperature, and our feedback algorithm. More-\nover, we selected examples to illustrate and scrutinize the models' results and\npresented further consideration for future research in the field of formal mathe-\nmatical proof."}]}