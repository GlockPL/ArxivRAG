{"title": "FireCast Net: Earth-as-a-Graph for Seasonal Fire Prediction", "authors": ["Dimitrios Michail", "Ioannis Prapas", "Charalampos Davalas", "Spyros Kondylatos", "Ioannis Papoutsis", "Lefki-Ioanna Panagiotou", "Nikolaos Ioannis Bountos"], "abstract": "With climate change expected to exacerbate fire weather conditions, the accurate and timely anticipation of wildfires becomes increasingly crucial for disaster mitigation. In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning. For the predictive analysis, we present FireCastNet, a novel architecture which combines a 3D convolutional encoder with GraphCast, originally developed for global short-term weather forecasting using graph neural networks. FireCastNet is trained to capture the context leading to wildfires, at different spatial and temporal scales. Our investigation focuses on assessing the effectiveness of our model in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance. Our findings demonstrate the potential of deep learning models in seasonal fire forecasting; longer input time-series leads to more robust predictions, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance. Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered.", "sections": [{"title": "1 Introduction", "content": "Fire plays a pivotal role in the Earth system, significantly influencing ecosystems worldwide. While traditionally viewed as a carbon-neutral process over the long term, climate change disrupts this balance through the intensification of fire weather conditions, leading to a rise in global fire activity [10]. A feedback loop is created when fires encroach into evergreen forest regions, posing a threat to their role as carbon sinks. This situation triggers the release of stored carbon into the atmosphere, further exacerbating climate change [4]. Moreover, wildfires represent a critical natural hazard with far-reaching consequences for societies worldwide, resulting in loss of life, property, infrastructure, and ecosystem services [22]. Hence, it is imperative to improve our understanding and forecasting capabilities of wildfire phenomena within the Earth system. By doing so, we can formulate effective strategies to mitigate the adverse impacts of wildfires and climate change. These strategies may encompass enhanced forestry management, strengthened infrastructure resilience, disaster preparedness, and the implementation of more accurate warning systems.\nProducing reliable long-range wildfire forecasts, spanning up to six months, requires advanced techniques that account for the Earth as one interconnected system. This system is shaped by continuous interactions across large spatio-temporal scales. These interactions manifest in two key ways: (a) memory effects, where past conditions like fuel accumulation and drought influence future fire activity, and (b) teleconnections, where distant climate events and weather patterns"}, {"title": "2 Related Work", "content": "Wildfires are notoriously hard to model due to the non-linear interactions between the different Earth system processes that affect them [7]. Weather, vegetation and humans interact in evolving ways that contribute in the expansion or suppression of wildfires. Reichstein et al. [27] propose Deep Learning as a method to learn in a data-driven way these complex spatio-temporal interactions that influence wildfires. Several recent studies have used deep learning for wildfire-related use cases [9]. For short-term daily predictions the temporal context is mostly enough, with spatio-temporal models offering little to no advantage [25, 14]. For longer-term predictions, i.e. on subseasonal to seasonal scales, very few works have studied the effect of spatial and temporal context [11, 23, 24]. Joshi et al. [11] use monthly aggregated input to predict burned area using multi-layer neural networks. Li et al. [17] includes the temporal aspect of the input using a temporal attention network on time-series"}, {"title": "3 Seasonal Fire Forecasting", "content": ""}, {"title": "3.1 Problem formulation", "content": "We view the problem as binary classification at a particular location of the cube and a particular timestamp. Thus, given a triplet ($c, Ac, tc) of latitude, longitude and 8-day period we predict whether a fire will occur or not at that particular location in time and space. As input we use a timeseries for each variable of length ts \u2208 {6,12,24} timesteps, where each timestep corresponds to a single 8-day period. As target variable we predict the presence of burned areas at a future timestep t + h with different values of h \u2208 {1, 2, 4, 8, 16, 24}."}, {"title": "3.2 Dataset", "content": "The SeasFire Datacube [12] is an analysis-ready, open-access datacube fit for wildfire forecasting at different spatio-temporal scales that serves not only for seasonal fire forecasting worldwide but also for forecasting emissions from wildfires and predicting the evolution of wildfire regimes. It spans over 21 years (2001-2021) with an 8-day temporal and a 0.25\u00b0 spatial resolution. The dataset provides a comprehensive coverage of atmospheric, climatological, vegetation and socioeconomic factors influencing wildfires and contains 58 variables in total, along with target variables, such as burned areas, fire radiative power, and carbon emissions from wildfires. 11 different variables were utilized (see Table 1) in order to predict the existence of fire in different time horizons. In addition and when needed, we also used simple positional encodings by augmenting the feature vector with the actual cube coordinates, i.e. sin/cos of latitude and longitude. All variables, except the cube coordinates, were standardized before use."}, {"title": "3.3 Naive Forecasting", "content": "When dealing with seasonal time series, the naive seasonal forecasting method uses the observed values in the same period of the previous seasons [8]. For example in order to predict for a particular week of August in a particular year, one could use the observed value in the same week of August one year before.\nOur dataset contains multiple years, allowing for different baseline methods based on the utilization of past data. We utilize the following two:\n\u2022 To forecast the occurrence of fire at a particular location in a particular 8-day period of the year, we examine whether there were fires at that location in the specific 8-day period in any of the previous years. If there were, the model predicts a fire; otherwise, it predicts no fire.\n\u2022 To forecast the occurrence of fire at a particular location in a particular 8-day period of the year we use a majority rule. Thus, we predict fire if the number of previous years with fire in that particular 8-day period is larger than the number of previous years without fire at that period.\nThe majority baseline approach aligns effectively with the binary classification task."}, {"title": "3.4 Baseline Models", "content": ""}, {"title": "3.4.1 Gated Recurrent Units", "content": "Recurrent Neural Networks [20] are a natural choice when trying to capture temporal dependencies. We utilize a very simple architecture comprising of Gated Recurrent Units. The neural network is comprised of two GRU layers with 64 hidden channels in each layer. A dropout layer with a probability of 0.1 exists between the two GRU layers. As the task is binary classification, a final linear layer reduces the representation into a single prediction and a sigmoid function outputs the final prediction."}, {"title": "3.4.2 Convolutional GRU", "content": "To capture both temporal and spatial dependencies at the same time, we utilize a convolutional GRU network [1]. In such a network all inputs, cell outputs, hidden states and gates are 3D tensors where the last two dimensions are spatial dimensions (rows and columns). This architecture is particularly effective for spatio-temporal tasks, as it incorporates convolution operations in the update and reset gates to model spatial relationships.\nThe ConvGRU model is defined using the following set of equations, where * denotes the convolution operator, \u2299 is element-wise multiplication, and ois the sigmoid activation function:\n$z_t = \\sigma(W_{xz} * X_t + W_{hz} * h_{t-1} + b_z)$,\n$r_t = \\sigma(W_{xr} * X_t + W_{hr} * h_{t-1} + b_r)$,\n$\\tilde{h}_t = \\tanh(W_{xh} * X_t + r_t \\odot (W_{hh} * h_{t-1})+b_h)$,\n$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$.\nHere, $X_t \\in \\mathbb{R}^{F\\times H\\times W}$ represents the input tensor at time step t, where F is the number of features, and H and W are the spatial dimensions. $h_{t-1}$ is the hidden state from the previous time step, while $z_t$ and $r_t$ are the update and reset gates, respectively. The hidden state $h_t$ is updated by controlling how much of the past hidden state is carried forward (via $z_t$) and how much of the new information (via $\\tilde{h}_t$) is incorporated into the current state.\nGiven ($\u03c6_c$, $\u03bb_c$) for a particular location of interest and a particular time t we generate a 3D tensor $X_t$ whose last two dimensions are the number of rows and columns of a (2r + 1) \u00d7 (2r + 1) spatial grid. The first dimension is the number of features. The grid is centered at our location of interest while the remaining locations represent($\u03c6_c\u00b1i \u00b7 1\u00b0, \u03bb_c \u00b1 i \u00b7 1\u00b0) for i \u2208 {1, ...,r \u2013 1}.\nWe feed $X_t$ to the model for each timestep t in order to compute the hidden representation $h_t$. The final result is obtained by utilizing MultiLayer Perceptron (MLP) layers in order to gradually aggregate the information into a single prediction."}, {"title": "3.4.3 Convolutional LSTM", "content": "We also utilize a convolutional LSTM network [29]. Again all inputs, cell outputs, hidden states and gates are 3D tensors where the last two dimensions are spatial dimensions (rows and columns). The future state of a certain cell is computed from the input and the past states of its local neighbors. This is achieved by using convolution operators in different transitions such as the state-to-state and/or input-to-state.\nWe utilize a slightly different network from the one in [29]. which is implemented using the following equations:\n$i_t = \\sigma(W_{xi} * X_t + W_{hi} * h_{t-1} + b_i)$\n$f_t = \\sigma(W_{xf} * X_t + W_{hf} * h_{t-1}+b_f)$\n$o_t = \\sigma(W_{xo} * X_t + W_{ho} * h_{t-1}+b_o)$\n$g_t = \\tanh(W_{xg} * X_t + W_{hg} * h_{t-1} + b_g)$\n$C_t = f_t \\odot C_{t-1} + i_t \\odot g_t$\n$h_t = o_t \\odot \\tanh(c_t)$\nHere * denotes the convolution operator and $\\odot$ point-wise multiplication. We integrate the ConvLSTM into the classification/regression pipeline the same way as the ConvGRU architecture."}, {"title": "3.4.4 U-Net with Temporal Attention Encoder", "content": "The U-TAE [6] (U-Net with Temporal Attention Encoder) architecture is designed for spatio-temporal feature extraction and segmentation of satellite image time series. It extends the classic U-Net by incorporating a temporal self-attention mechanism to process temporal sequences of"}, {"title": "4 FireCast Net", "content": "The FireCastNet architecture is shown in Figure 1. It consists of a cube embedding block, a GraphCast block and if needed a sub-pixel convolution for upsampling. This architecture is designed to perform segmentation or regression tasks on a timeseries of multi-channel images. The input data is represented as a tensor of dimensions $X \\in \\mathbb{R}^{T\\times C\\times H\\times W}$, where T is the number of temporal steps, C is the number of input channels, and H and W are the height and width of each image, respectively."}, {"title": "4.1 Cube embedding", "content": "The first stage of FireCastNet is a 3D convolutional block that performs spatio-temporal feature extraction. The main purpose of the space-time cube-embedding is to reduce the spatial and temporal dimensions of the input and accelerate the training process [30]. The cube embedding applies a 3-dimensional (3D) convolution layer, with a kernel and stride of $T\\times \\frac{H}{H'}\\times \\frac{W}{W'}$. It also increases the number of channels to C' > C. This block can be expressed as:\n$X' = Conv3D(X; F)$.\nThe output tensor encodes the information in dimensions $X' \\in \\mathbb{R}^{C'\\times H'\\times W'}$. To keep the spatial resolution to 0.25\u00b0 one has to use $\\frac{H}{H'} = 1$ and $\\frac{W}{W'} = 1$. In scenarios where computation resources are limited one could use $\\frac{H}{H'} = 4$ and $\\frac{W}{W'} = 4$ to reduce the spatial resolution to 1\u00b0.\nWe utilize 11 variables from the datacube as depicted in Table 1. Thus C = 14. Additionally we keep the spatial resolution to 1\u00ba and choose C' = 64. The output of the cube embedding has dimensions $C' \\times H \\times W' = 64 \\times 720 \\times 1440$. Note that ERA5 is 721 x 1440 while the SeasFire datacube has 720 \u00d7 1440."}, {"title": "4.2 GraphCast", "content": "The core of the FireCastNet architecture is the GraphCast module, which uses a Graph Neural Network (GNN) in an \"encode-process-decode\" configuration. This module is designed to model long-range spatial interactions over a multi-mesh structure, which is more efficient and accurate than traditional grids.\n\u2022 Multi-mesh The multi-mesh, G = (V, E), is a graph where V denotes the set of nodes and & denotes the edges. The multi-mesh is constructed by refining a regular icosahedron over six iterations, resulting in 40,962 nodes and 81,920 faces. The graph's nodes are evenly distributed across the globe, overcoming the uneven spatial distribution of a standard latitude-longitude grid.\n\u2022 Encoder The output X' is first mapped from the regular latitude-longitude grid to the multi-mesh representation. This is done by encoding the grid-based features into node"}, {"title": "4.3 Upscaling", "content": "The final stage of FireCastNet is a sub-pixel convolution [28] block, which upscales the decoded output $X_{decoded} \\in \\mathbb{R}^{C''\\times H'\\times W''}$ to match the spatial dimensions of the original input. It rearranges the feature map into higher-resolution output, producing the final segmentation map of size H \u00d7 W. The upscaling operation is defined as:\n$X_{output} = Sub-Pixel-Conv(X_{decoded})$,\nyielding the segmentation output $X_{output} \\in \\mathbb{R}^{Cout\\times H\\times W}$, where Cout is the number of output channels for the segmentation task."}, {"title": "5 Experiments", "content": "Our methodology can be directly applied to any region of the world, producing different models per region. For our forecasting task, our split is time-based, using years 2002-2017, 2018 and 2019 for train, validation, and test, respectively. The SeasFire dataset is utilized in all the experiments (see Section 3.2). Figure 2 shows the ground truth and model predictions for September 30, 2019, trained to forecast one 8-day period ahead.\nDue to our highly imbalanced dataset, the performance of the model is evaluated using the Area Under the Precision-Recall Curve (AUPRC) [3]. We calculate the metric using the average precision score, which summarizes a precision-recall curve as a weighted mean of precisions at each threshold, with the difference in recall from the previous threshold as weight\n$AP = \\sum_t(R_t - R_{t-1})\\cdot P_t$,\nwhere $P_t, R_t$ is the respective precision and recall at threshold index t.\nModels are trained for 50 epochs. We use binary cross-entropy loss and the AdamW [19] optimizer. The initial learning rate is set to 0.001 and adjusted using SGDR [18]. The schedule contains two cycles with 10 and 40 epochs, respectively. A weight decay factor of $10^{-7}$ is used when training all models."}, {"title": "5.1 Global Model", "content": "In this section, we train and compare FireCastNet at a global scale. The naive prediction baseline models discussed in Section 3.3 exhibit 0.35 and 0.39 AUPRC. The first baseline checks whether any previous year contains some fire in the same period, while the second checks whether a majority of the previous years contains fires.\nAll models take as input the 11 inputs described in Table 1. The GRU model is comprised of a single hidden layer with 64 hidden units. The Conv-GRU and Conv-LSTM models use the same parameters as the GRU with the only difference of an additional setting of the 5 \u00d7 5 kernel. The U-TAE model is configured with 16 attention heads, and a layer size of 256. Each attention head has a key dimension of 4.\nFinally, FireCastNet processes data with spatial resolution 0.25\u00b0 and geographic bounds from -89.875\u00b0 to 89.875\u00b0 latitude and -179.875\u00b0 to 179.875\u00b0 longitude. The Conv3d block increases spatial resolution to 1\u00ba, and adjusts the hidden dimensionality to 128, with layer normalization. Additionally mesh input includes 3 dimensions for nodes and 4 for edges. The model's processor features 8 layers, with 1 hidden layer and a hidden layer dimension of 64.\nFigures 3 and 4 presents the best model results when predicting at a global scale when using timeseries 12 and 24 8-day periods respectively. A more detailed comparison between models with respect to the timeseries length can be seen at Figure 5. Different GRU, ConvGRU, ConvLSTM, UTAE and FireCastNet models were trained for each prediction time horizon and timeseries length. Regardless of the range of forecasts, the FireCastNet is clearly more potent in per-pixel classification, Especially when using a longer timeseries input. Additionally, FireCastNet is highly consistent in providing burnt area predictions whilst other methods show a decline proportional to the increase in the forecasting horizon.\nAdditionally, Table 2 provides results between all baselines and the FireCastNet model for timeseries length 24 for different time horizons. The TeleVit [24] model is also included in the comparison. TeleViT is a teleconnection-driven vision transformer for seasonal wildfire forecasting, trained also on the SeasFire dataset. It should be noted that it does not work directly with timeseries data. Instead, it extracts temporal context from Oceanic Indices (OCIs)."}, {"title": "5.2 Timeseries length", "content": "In this section, we investigate the impact of training global models using input time series of varying lengths, both with and without spatially accumulated information. This analysis aims to explore how the temporal context provided to the models affects their predictive performance across different forecasting horizons. Specifically, we evaluate models trained with three distinct time-series lengths, ts \u2208 {6,12,24}, and analyze their performance in terms of the Area Under the Precision-Recall Curve (AUPRC). For each combination of time-series length and forecasting horizon h\u2208 {1, 2, 4, 8, 16, 24}, we train a separate model to assess the influence of these parameters independently.\nFigure 5 summarizes the performance of the models under these configurations, offering insights into how time-series length affects prediction accuracy. Among the various models studied, the"}, {"title": "5.3 Ablation different time overlap", "content": "In this section, we conduct an ablation study to investigate the effect of varying sample overlaps between consecutive time series on model accuracy, as measured by the Area Under the Precision-Recall Curve (AUPRC). This study aims to understand the role of overlapping temporal context in improving model performance and its implications for training efficiency and predictive accuracy.\nIn the original setup, for a time series of length t, each sample overlaps with t - 1 identical points from the subsequent sample. This ensures that consecutive input sequences share nearly the entire temporal context, differing by only a single time step. To systematically evaluate the effect of overlap, we consider scenarios where the overlap is reduced. Specifically, for time series with t = 24, we analyze configurations where consecutive samples share 3, 6, 12, 18, and including the original full-overlap configuration of t - 1 = 23 samples"}, {"title": "5.4 GFED Regions", "content": "In this section, we present localized predictions derived using regional masks from the Global"}, {"title": "6 Conclusion", "content": "The findings of this study underscore the potential of machine learning models in advancing seasonal wildfire forecasting. Our architecture, FireCastNet, integrates 3D convolutional encoding with GraphCast, in order to capture the spatio-temporal context of wildfires, enabling predictive analysis across various forecasting horizons. Our architecture is clearly more beneficial than both baseline and contemporary methods for spatio-temporal predictions and burned area prediction via semantic segmentation."}]}