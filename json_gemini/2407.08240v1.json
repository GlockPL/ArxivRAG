{"title": "Leveraging LLMs to Predict Affective States via Smartphone Sensor Features", "authors": ["Tianyi Zhang", "Songyan Teng", "Hong Jia", "Simon D\u2019Alfonso"], "abstract": "As mental health issues for young adults present a pressing public health concern, daily digital mood monitoring for early detection has become an important prospect. An active research area, digital phenotyping, involves collecting and analysing data from personal digital devices such as smartphones (usage and sensors) and wearables to infer behaviours and mental health. Whilst this data is standardly analysed using statistical and machine learning approaches, the emergence of large language models (LLMs) offers a new approach to make sense of smartphone sensing data. Despite their effectiveness across various domains, LLMs remain relatively unexplored in digital mental health, particularly in integrating mobile sensor data. Our study aims to bridge this gap by employing LLMs to predict affect outcomes based on smartphone sensing data from university students. We demonstrate the efficacy of zero-shot and few-shot embedding LLMs in inferring general wellbeing. Our findings reveal that LLMs can make promising predictions of affect measures using solely smartphone sensing data. This research sheds light on the potential of LLMs for affective state prediction, emphasizing the intricate link between smartphone behavioral patterns and affective states. To our knowledge, this is the first work to leverage LLMs for affective state prediction and digital phenotyping tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "With the increasing prevalence of mental health issues among young adults, particularly within the student population, there is significant rationale for researching and developing systems to monitor mental health, including affective states. Daily mood monitoring may aid early detection of potential mental health issues in timely intervention and support [1]. Digital phenotyping, also called personal sensing, may serve as one of the potential solutions. Utilizing digital devices to collect behavioral traces seamlessly to analyse and predict mental health status, digital phenotyping is a potentially effective tool to understand the relationship between human activities and mental health or wellbeing.\nSmartphones are a premium device for digital phenotyping due to their ubiquity and diverse data collection capacities, including collection of multi-sensor data (e.g. GPS and accelerator) and also captures of virtual social behaviors, software usage, and screen interactions, thereby capturing the intricate relationship between humans and technology. To deliver emotional wellbeing predictions, powerful classic machine learning models such as deep neural networks [5], gradient boosting [2], and transformers [12] often face overfitting issues and are not suitable for small datasets typically found in mental healthcare. Therefore, innovative and effective new approaches are required to assist and improve digital phenotyping techniques and methods.\nRecently, large language models (LLMs) have emerged as powerful tools for natural language generation, natural language understanding, and contextual comprehension. Their ability to leverage pre-trained knowledge makes them applicable to various research problems across different domains, demonstrating superior performance in tasks such as sentiment analysis [12], activity recognition [6], and information retrieval [14]. The inherent nature of foundational LLMs, trained on vast online corpora that document human behavior and psychological knowledge, underscores their potential to reveal more nuanced healthcare information compared to traditional machine learning models in personal sensing, as exemplified by recent studies [4]. While these initial studies provide a direction for utilizing LLMs in addressing general healthcare issues, the application of LLMs to digital phenotyping for mental health (DPMH) and emotional wellbeing has been scarcely studied, particularly regarding the integration of smartphone sensing data.\nIn contrast to broader data-driven tasks, the intricacies of DPMH and emotional wellbeing involve smaller-scale personal and sensitive data and subjective measures. The nature of this sensitivity and subjectivity renders the DPMH tasks more challenging because of contextual dependence and difficulty in identifying generalized patterns. Additionally, in the nascent field of DPMH, the exploration of multimodal or multisensor activities in daily behavioral data collected via smartphones remains limited, despite its potential to indicate individuals' psychological fluctuations. Given that LLMs are well-known for capturing contextual information, they are a natural choice for analyzing digital traces inferred from smartphones. Compared with traditional ML models focused on analysing numeric features produced from individual sensors, LLMs can analyse generated lifecycle descriptions from data collected across multiple smartphone sensors at a higher level of abstraction. Then, powered by prior knowledge, LLMs are capable of unveiling the latent associations between behavioral patterns and individuals' mental health or affective status. As such, our research aims to explore whether LLMs can effectively predict affective states with minimal amounts of data, utilizing human behaviors and interactions with smartphones as input data.\nIn this study, we investigate the relationship between behavioral features collected from smartphone sensors and the affects of university students. We also demonstrate the capability of zero-shot and few-shot embedding LLMs to infer affective states based on smartphone-captured human activities. Our results suggest a discernible connection between smartphone-sensed activities of university students and their affective states, which LLMs interpret through their chains of thought. To the best of our knowledge, this is the first paper to delve into grounding LLMs with mobile sensing features for affect prediction tasks."}, {"title": "2 METHOD AND EXPERIMENTS", "content": "In this section, we describe the study design (\u00a7 2.1), data collection (\u00a7 2.1), data engineering (\u00a7 2.2) and tasks delivered to LLMs (\u00a7 2.3). Specifically, we detail in \u00a72.3 the prompts for LLMs on multiple emotional state prediction tasks, including zero-shot prompting, few-shot prompting and chain-of-thought reasoning."}, {"title": "2.1 Participants and Data Collection", "content": "Study Design. To explore the capability of LLMs in predicting individuals' general feelings based on smartphone-collected passive data, we investigated a subset of data obtained from a digital phenotyping study of Australian university students conducted in 2023. In this study [3], approximately 150 university students were observed over a full semester (17 weeks). Data was gathered from multiple smartphone sensors and a variety of psychometric assessments were conducted. Of the approximately 150 participants in this study, we chose to analyse the data of 10 students for this investigation. The study utilised the AWARE-Light smartphone sensing app [11] to collect passive sensing data. Weekly self-reported assessments were collected using an emailed link to a Qualtrics questionnaire.\nOur study continuously collected sensor data for 17 weeks from the primary smartphones university students used. Informed by our own previous work as well as existing literature, for example [7], the following sensors were chosen: battery levels, screen unlocks, location information, application usage, keyboard usage, and communication traces (calls and messages). Data collection was in accordance with ethics approval from the University of Melbourne.\nSelf-reported Measures. At the end of each week, participants were prompted to complete the International version of the Positive and Negative Affect Schedule (I-PANAS-SF) questionnaire [10]. This questionnaire comprises 10 items rated on a Likert scale ranging from 1 (indicating \"Never\") to 5 (indicating \"Always\"). Five of the items indicate positive affects (active, determined, attentive, inspired, alert) and the other five indicate negative affects (upset, hostile, ashamed, nervous, afraid)."}, {"title": "2.2 Data processing", "content": "We first cleaned the raw data from the self-reported questionnaire and passively collected smartphone data to remove duplicates and corrupted entries before carrying out data engineering. We utilized the RAPIDS tool [13] to generate and select 77 behavioral features at the daily level of granularity. These features include metrics such as time spent at home, the number of received messages, and duration of screen unlocks per day. Missing passive data was treated as such when constructing prompts, without any imputation.\nPrevious studies focusing on machine learning models for predicting individuals' mental health scales have typically targeted the total score of a questionnaire, such as the PHQ-9 for depressions or GAD-7 for anxiety. In terms of I-PANAS-SF, a total score is not feasible, as the questionnaire is not designed to provide an overall score. Whilst it is possible to sum up the five positive items to get a total positive score, and likewise for negative items, our approach was to construct a prediction for each individual scale item. By doing so, we acknowledge that each item represents a distinct affect, making our predictions more comprehensive. For instance, predicting solely the total score of I-PANAS-SF negative affect provides little insight into the extent to which a person felt upset or nervous. Therefore, we constructed the task in a manner where LLMs are prompted to predict scores for each single item of the I-PANAS-SF separately within one inference.\nGiven the nature of LLMs as language models, and their known limitations in handling numerical raw data [9], we opted to reference participants' daily features with concise English descriptions. Each feature description is presented sequentially, preceded by the corresponding date and time range in the format YYYY-MM-DD HH:MM:SS to YYYY-MM-DD HH:MM:SS (e.g., 2023-08-02 00:00:00 to 2023-08-02 23:59:59). We intentionally kept the time series information for training purposes to understand if the LLM has the capacity to complete the tasks rather than build a robust and usable predictive model."}, {"title": "2.3 Task description", "content": "To investigate whether LLMs can grasp the connection between smartphone-derived behaviours/contexts and affective states, we conducted zero-shot and few-shot tasks on I-PANAS-SF. Each week's description of daily activities consists of approximately 5,000 tokens, and we restricted our experimentation to Gemini 1.5 Pro [8] due to resource limitations in other LLMs. The LLM was tasked with providing Likert scores rather than textual descriptions. To ensure deterministic responses from the LLMs, we adjusted the temperature to zero. Although Gemini still yields slightly different answers each time, the predictions remain relatively consistent across low (i.e., 1 or 2) and high (i.e., 4 or 5) scores.\nIn this exploratory study, we assigned individual tasks to 10 subjects and randomly divided the 17-week data into a 10-week training dataset and a 7-week test dataset. We performed a repeated random train-test split on the 10-participant dataset three times. In the few-shot prompting method, we incrementally added one more example from the training dataset for each shot, starting with one shot and continuing until we reached ten shots, to test the individual instances in the test dataset."}, {"title": "3 RESULTS & DISCUSSION", "content": "In this section, we first introduce the metrics used to evaluate the LLM's predictions. Next, we discuss the zero-shot and few-shot performance of these models. Finally, we present a chain-of-thought analysis of the LLM with a zero-shot case study.\nMetrics. To assess the accuracy of our predictions, we employed a macro calculation approach for both the Mean Absolute Error (MAE) and the relative error (\u20ac). This approach involved two main steps: first calculating the error metrics for each participant individually, and then averaging these metrics across all participants. The overall Mean Absolute Error (MAE) is calculated as follows:\n$MAE_{overall} = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{n_i} \\sum_{j=1}^{n_i} |P_{ij} - T_{ij}|$\nwhere $P_{ij}$ represents the predicted value, $T_{ij}$ represents the true value for the j-th observation of the i-th participant, $n_i$ is the number of observations for participant i, and N is the total number of participants.\nThe mean of the true values for each participant is calculated as:\n$\\overline{T_i} = \\frac{1}{n_i} \\sum_{j=1}^{n_i} T_{ij}$\nFinally, the overall relative error is obtained by averaging the relative errors of all participants:\n$\\epsilon_{overall} = \\frac{1}{N} \\sum_{i=1}^N (\\frac{MAE_i}{\\overline{T_i}} \\times 100\\%)$"}, {"title": "4 FUTURE WORK AND CONCLUSIONS", "content": "In this study, we demonstrated the potential of LLMs in harnessing smartphone-collected behavioral data to predict affective states of university students by leveraging both zero-shot and few-shot learning approaches. Our results indicate that the LLM's ability to perform zero-shot tasks for mental wellbeing based on weekly summarized behaviors suffers from a relatively high error rate. However, when provided with labeled information, the LLMs achieve significantly improved predictions. This positions LLMs as an invaluable resource for future research and practical applications in wellbeing and other fields, deepening our understanding of individuals' mental health, emotions, and affective states.\nFor future work, fine-tuning tasks could be conducted to develop a model driven by daily activities, thereby incorporating more data and possibly enabling a comparison between individual and general predictive models. Given the subjective nature of self-reported datasets, models may exhibit biases stemming from imbalanced class distributions, with some classes being significantly underrepresented compared to others. Hence, increasing the dataset size or employing resampling techniques becomes imperative to construct a more robust model overall. Furthermore, leveraging LLMs to predict individual items of additional psychometric measures for prediction tasks would be advisable, facilitating the construction of more nuanced insights into people's affective states at a wellbeing level."}]}