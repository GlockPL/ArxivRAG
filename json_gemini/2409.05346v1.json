{"title": "GDFlow: Anomaly Detection with NCDE-based Normalizing Flow for Advanced Driver Assistance System", "authors": ["Kangjun Lee", "Youngho Jun", "Minha Kim", "Simon S.Woo"], "abstract": "For electric vehicles, the Adaptive Cruise Control (ACC) in Advanced Driver Assistance Systems (ADAS) is designed to assist braking based on driving conditions, road inclines, predefined deceleration strengths, and user braking patterns. However, the driving data collected during the development of ADAS are generally limited and lack diversity. This deficiency leads to late or aggressive braking for different users. Crucially, it is necessary to effectively identify anomalies, such as unexpected or inconsistent braking patterns in ADAS, especially given the challenge of working with unlabelled, limited, and noisy datasets from real-world electric vehicles. In order to tackle the aforementioned challenges in ADAS, we propose Graph Neural Controlled Differential Equation Normalizing Flow (GDFlow), a model that leverages Normalizing Flow (NF) with Neural Controlled Differential Equations (NCDE) to learn the distribution of normal driving patterns continuously. Compared to the traditional clustering or anomaly detection algorithms, our approach effectively captures the spatio-temporal information from different sensor data and more accurately models continuous changes in driving patterns. Additionally, we introduce a quantile-based maximum likelihood objective to improve the likelihood estimate of the normal data near the boundary of the distribution, enhancing the model's ability to distinguish between normal and anomalous patterns. We validate GDFlow using real-world electric vehicle driving data that we collected from Hyundai IONIQ5 and GV80EV, achieving state-of-the-art (SOTA) performance compared to six baselines across four dataset configurations of different vehicle types and drivers. Furthermore, our model outperforms the latest anomaly detection methods across four time series benchmark datasets. Our approach demonstrates superior efficiency in inference time compared to existing methods.", "sections": [{"title": "1 Introduction", "content": "The Adaptive Cruise Control (ACC) in the Advanced Driver Assistance System (ADAS) for electric vehicles assists in automatic braking based on driving conditions, road inclines, and the driver's deceleration patterns without having them to manually press the pedal [5, 9, 32]. However, the driving data collected during the vehicle development process is quite limited and lacks the diversity of various driving patterns. Due to this limitation, the ACC often relies on predefined deceleration strengths, which are generally set to three levels: strong, normal, and smooth deceleration. However, such a high level discrete categorization can lead to potential discomfort for drivers, as it may not accurately reflect individual driving preference and behavior [1, 27]. As the intensity and sensitivity of automatic braking in ADAS is determined based on the limited data collected during development, the system may operate too late or too aggressively for different users [29], which can be problematic.\nIn the existing ACC [42], DBSCAN was used to detect anomalies in the collected driving data to enhance driving stability by identifying consistent deceleration patterns. However, classical clustering algorithms such as DBSCAN are highly influenced by variations in data density, making it subject to high sensitivity w.r.t different hyperparameters. Furthermore, DBSCAN struggles under different traffic conditions, being vulnerable to high sensitivity under hyperparameters [2, 6, 27]. Moreover, the real-world ADAS in electric vehicles must handle complex and noisy multivariate time series (MTS) sensor data [15]. While several clustering methods were introduced to detect, they face challenges due to high sensitivity to hyperparameters. Also, deep learning-based methods often use supervised approaches [7, 13, 21] that require annotation, which is labor-intensive [34] and susceptible to label noise.\nIn order to tackle the aforementioned challenges, we adopt unsupervised one-class classification (OCC) approaches to effectively perform anomaly detection by learning only normal data patterns. In particular, we explore a Normalizing Flow (NF)-based model,"}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Clustering Algorithm for Electric Vehicles", "content": "In the domain of electric vehicles, clustering algorithms have been used across various applications. Xie et al. [39] discuss a methodology for clustering driving data using DBSCAN to separate high density regions from noise, which is useful for identifying meaningful patterns in braking data. This approach is advantageous compared to traditional methods such as K-means since DBSCAN does not require predefining the number of clusters and can handle arbitrary shapes and noise effectively. Moreover, Straka et al. [35] explore multiple clustering algorithms, including K-means, DBSCAN, and agglomerative hierarchical clustering, to analyze usage-related segments of electric vehicles. Their approach aims to identify patterns in charging behavior and other usage metrics of charging stations, providing insights into optimizing charging infrastructure and vehicle utilization. However, classical clustering algorithms such as DBSCAN are inherently sensitive to hyperparameters such as radius value and the minimal number of points [3]. To address this issue, STRP-DBSCAN [3] incorporates temporal attributes and uses parallel processing and an autotuning mechanism for DBSCAN parameters using deep reinforcement learning, enhancing clustering accuracy. In our case, traditional DBSCAN-based clustering algorithms also face significant challenges due to hyperparameter sensitivity, making it difficult to be realizable in real-world vehicles. Therefore, we focus on tackling this problem as an unsupervised anomaly detection from clustering algorithms."}, {"title": "2.2 Deep Clustering Network", "content": "Many studies enhance clustering algorithms using deep learning models, specifically autoencoders, to generate better representations of clustering algorithms. Lu et al. [28] employ an autoencoder to extract latent features, assigning pseudo-labels through initial clustering. Reliable samples are selected for further training in a CNN, filtering out unreliable samples and improving clustering accuracy. Similarly, Li et al. [26] use an autoencoder-enhanced clustering method for anomaly detection in image data, iterating between hypothesizing normal candidate subsets and representation learning. The reconstruction error from the autoencoder serves as a scoring function to assess normality. For time series data, DTCR [30] integrates a Seq2Seq model with a K-means clustering objective and a temporal reconstruction loss to generate cluster-specific representations. Further, Aytekin et al. [4] enhance clustering by adding an 12 normalization constraint on autoencoder representations, making feature vectors more separable and compact. Hence, deep clustering networks have been used to cluster data in ACC. However, we take a completely different and new approach using an unsupervised anomaly detection method to address the problem more effectively. We demonstrate the superiority of our model by comparing its performance with conventional deep clustering networks."}, {"title": "2.3 Multivariate Time Series Anomaly Detection", "content": "Generally, anomaly detection algorithms can be categorized into one of the following categories: reconstruction, association, and forecasting-based approaches. The reconstruction-based approach, THOC [33], uses a dilated RNN with skip connections to capture temporal dynamics across multiple scales. Moreover, M2N2 [24] incorporates a test-time adaptation strategy to adjust model parameters during inference, handling distribution shifts between training and test data. The association-based approach, established by the Anomaly Transformer (AT) [41], uses an Anomaly-Attention mechanism to capture both prior-association and series-association discrepancies, allowing the model to distinguish normal and anomalous points. Recently, NF-based models have been used for anomaly"}, {"title": "3 Background on ADAS and Dataset", "content": null}, {"title": "3.1 Advanced Driver Assistance System (ADAS)", "content": "The ADAS is a driver convenience feature that enables vehicle operation without the need to press the accelerator or brake pedals. It adjusts to driving conditions, road gradients, and the driver's driving style. The fundamental control of ADAS involves calculating the target acceleration based on the distance and relative speed of the vehicle ahead. This is achieved by combining the Constant Time Gap Policy (CTG) with the uniform acceleration motion equation. The equations for the CTG and the uniform acceleration motion are presented as follows:\n$ACTG = \\frac{1}{h}(v_{front} - v_{ego}) + \\frac{\\lambda}{\\lambda h}(e + \\lambda \\delta)$ (1)\n$a_{QAM} = \\frac{v_{front}^2 - v_{ego}^2}{2e}$ (2)\nwhere e represents the distance to the vehicle ahead, $\\lambda$ is the control parameter, h is the target time gap, and $\\delta$ is the distance error. Moreover, ufront and vego refer to the speed of the vehicle ahead and the ego vehicle, respectively. The CTG offers the advantage of preventing traffic congestion for following vehicles. Meanwhile, the uniform acceleration motion equation adjusts the distance between vehicles to calculate the target acceleration.\nHowever, if a vehicle equipped with ADAS executes automatic control with delayed braking or excessive deceleration, the driver may experience discomfort. To address these issues, various studies are being conducted to learn from the driver's deceleration data. The goal is to control the vehicle in a manner that closely resembles the driver's usual deceleration habits. Since driver braking data encompasses various deceleration scenarios, an algorithm for extracting stable deceleration data is required. This study proposes a method to extract data that represents the driver's decelerating habits through deep learning methods."}, {"title": "3.2 Deceleration Dataset Description", "content": "The real-world deceleration data can more effectively and accurately measure the model's performance. Hence, we collected"}, {"title": "4 Preliminaries", "content": null}, {"title": "4.1 Neural Controlled Differential Equations and Adaptive Graph Generation", "content": "NCDE have been shown to outperform RNN-based models and Neural Ordinary Differential Equations (NODE) on benchmark datasets over the various MTS domains [19, 23]. In particular, NCDE extends NODE [8] by modeling continuous time series data over all time intervals using differential equations combined with neural networks. To account for temporal dependencies, NCDE creates a continuous path X(t) through continuous path interpolation and considers both current and past observations to generate future observations of the time series data. Furthermore, previous research [10] combined NCDE with graph operations to process both temporal and spatial information simultaneously. Inspired by this idea,"}, {"title": "4.2 Normalizing Flow", "content": "NF is a set of reversible transformations ge that convert the original distribution px (x) of the data to a target distribution pz (z), where Z follows a standard normal distribution N(0, 1). NF gradually transforms the complex distribution of the original data into a simpler distribution of the target distribution, and the density estimation px (x) for this transformation is given by the following equation:\n$p_x(x) = p_z (g_{\\theta}(x)) |det \\frac{\\partial g_{\\theta}(x)}{\\partial x}|$ (6)\nwhere ge (x) is the data transformed by the NF ge. The probability density of the original data is calculated through the absolute value of the Jacobian determinant $det (\\frac{\\partial g_{\\theta}(x)}{\\partial x})$, which signifies density estimation in NF. This process effectively models the complex distribution of data, and in our study, we model the distribution of normal data to identify data that deviates from the normal distribution, performing anomaly detection. Additionally, the log-density can be expressed as:\n$log p_x (x) = log p_z (g_{\\theta}(x)) + log det \\frac{\\partial g_{\\theta}(x)}{\\partial x}|$ (7)\nAlthough NF is reversible, our study focuses solely on using forward flow to detect anomalies in driving data by utilizing log-likelihood (LL) estimation without using the inverse."}, {"title": "5 Our Approach", "content": null}, {"title": "5.1 Problem Definition", "content": "In this section, we define the problem of learning normal driving behavior and identifying anomalies in the deceleration data. The input deceleration data is defined as X0:T = {X0, X1, ..., XT}, where T represents the time interval of the time series. Since the deceleration data is multivariate with n sensors, the observation xt at each time step is a n-dimensional vector defined as Xt = [xx (1), x(2),...,x(n)]. Additionally, this dataset consists of multiple profiles p, where each profile p is defined as xp (t) = [xp1) (t), x2) (t),...,xon) (t)]. However, since each p consists of MTS with varying lengths, we apply a sliding window, resulting in Wi) = [xp(i), xp (i+1), \u2026\u2026\u2026, xp (i + w \u2212 1)]. Here, w is the window size, and s is the stride. These segments are batched for training, resulting in data with the shape Wi W(i) \u2208 Rbxnxw, where b is the batch size."}, {"title": "5.2 Overall Architecture", "content": "Figure 2 presents the overall architecture of our GDFlow. As shown, the initial input data is transformed into a continuous path X(t) through cubic spline interpolation. This transformation enables the data to be integrated into the model, continually depending on the observed data rather than changing hidden states. Our model consists of NCDE and AGG modules to capture spatio-temporal"}, {"title": "5.3 Spatio-Temporal Information Encoding", "content": "We use two CDE functions, fi and f2, to simultaneously encode temporal and spatial information. The process for encoding temporal information, H(T), is the same as in Eq. (3), while the process for encoding spatial information, Y(T), is defined as follows:\n$Y(T) = Y(T_0) + \\int_{T_0}^{T} f_2(x(t); \\theta_{f_2}) \\frac{d(x(t))}{dt} dt,$ (8)\nwhere the CDE function f consists of fully connected layers, and each row of the encoded temporal information H(t) is taken as an individual input. The CDE function f\u2081 encodes temporal information, and the CDE function f2 encodes spatial information. The encoded H(T) and Y(T) from the two CDE functions are then multiplied together to produce S(T). This allows the parameters of the NCDE to be updated using S(t + At) as shown in the following equations:\nH(t + \u2206t) = H(t) + f\u2081(H(t); 0f) \u00b7 \u2206X(t), (9)\nY(t + \u2206t) = Y(t) + f2(Y(t); 0f\u2082) \u00b7 \u2206H(t), (10)\nS(t + t) = Y(t + \u2206t) \u00b7 H(t + At). (11)\nBy encoding spatio-temporal information simultaneously and passing it to the NF, we can model the continuous changes in deceleration data more naturally. Additionally, we structure the n sensors as a graph for learning."}, {"title": "5.4 Density Estimation", "content": "We use the hidden vectors S(t) from NCDE as input to the NF to learn the distributions of MTS containing spatio-temporal information to approximate these distributions closer to the target. To achieve this, we define the target distribution as Pz (2) = N(2|\u03bc, \u03a3), where N is a multivariate normal distribution for MTS, with \u00b5 as the mean vector and \u2211 as the covariance matrix. Thus, the LL of this multivariate normal distribution can be defined as log Pz (2) = log N (2\u03bc, \u03a3). \u03a4o transform the original distribution px (x) to the target distribution, the density transformation process using the hidden vectors S(t) as input is as follows:\n$log Px (S) = log Pz (g_{\\theta} (S)) + log det \\frac{\\partial g_{\\theta} (S)}{\\partial S}|$ (12)\nwhere log Pz (ge (S)) is the LL of the transformed latent variable 2. We calculate these LL for each sensor in the multivariate deceleration data."}, {"title": "5.5 Quantile-based Maximum Likelihood", "content": "Since some normal data close to the boundary of the anomalous pattern distribution may have low LL, we use a quantile function to maximize the relatively low LL of normal data to better separate"}, {"title": "6 Experiments", "content": null}, {"title": "6.1 Datasets and Baselines", "content": "Datasets. We utilized four deceleration datasets collected from two vehicle types (IONIQ5 and GV80EV) and two different drivers (D1 and D2) as introduced in Section 3.2: IONIQ5-D1, IONIQ5-D2, GV80EV-D1, and GV80EV-D2. We designed two experimental configurations to evaluate data cleaning performance and generalization ability. First, in Section 6.3.1, we conducted four separate experiments for each deceleration dataset to assess GDFlow's data cleaning performance and hyperparameter sensitivity. Following unsupervised anomaly detection conventions, training data consisted solely of normal data. Second, in Section 6.3.2, we used a hold-out approach to evaluate generalization. One deceleration dataset was held out as test data while the others were used for training, assessing the model's performance on unseen vehicle-driver combinations.\nAdditionally, we quantitatively compared GDFlow's anomaly detection efficacy against standard anomaly detection benchmark datasets. For this comparison, we used the SMD [36], MSL [17], and SMAP [17] datasets. These datasets exhibit significant distribution shifts similar to real-world data. Details of the benchmark datasets are described in Appendix B.\nBaselines. We compared a range of selected SOTA anomaly detection algorithms against our proposed GDFlow. For comparing anomaly detection performance, we used five baselines: THOC [33], AT [41], M2N2 [24], GANF [11], and MTGFLOW [43], respectively. Note that for a comprehensive comparison, we included traditional anomaly detection approaches such as reconstruction and association-based methods, as well as newly proposed NF-based approaches. From these, the two top-performing anomaly detector baselines on the anomaly detection benchmarks were then utilized for performance comparison on our deceleration datasets.\nTo demonstrate the superiority of our anomaly detection performance over other methods, we also employed DBSCAN-DTW, SAE-DBSCAN, GAE-DBSCAN, and DCEC [4] for comparison. DBSCAN-DTW uses DTW instead of Euclidean distance for better time series modeling. SAE-DBSCAN and GAE-DBSCAN are autoencoder-enhanced DBSCAN algorithms using sparse autoencoder and GRU-autoencoders, respectively."}, {"title": "6.2 Evaluation Metrics", "content": "To evaluate the effectiveness of GDFlow against competing models, we utilize the F1-score, Area Under the Receiver Operating Characteristics curve (AUROC), and Area Under the Precision-Recall Curve (AUPRC) metrics. After analyzing the test data statistics, the best threshold is determined for the F1-score to comprehensively evaluate precision and recall through their harmonic mean. Since real-world anomalies typically span multiple timestamps, we apply a point-adjusted assessment protocol, considering the identification of any part of an anomaly segment as a correct detection [36, 40]. Additionally, we report AUROC and AUPRC over test data anomaly scores, providing an overall summary of the anomaly detector's performance across all possible thresholds. AUROC evaluates performance across a range of decision thresholds, reducing sensitivity to a specific threshold choice. AUPRC is particularly suited for imbalanced classification scenarios, offering insights into the model's precision and recall balance. Given that GDFlow is an NF-based anomaly detector, we selected GANF and MTGFLOW as our primary competing baselines. In experiments on benchmark datasets, we used only AUROC for comparison, following previous research works [11, 43]."}, {"title": "6.3 Evaluation with Deceleration Datasets", "content": "All experiments are conducted with a fixed seed, and the hyperparameters and implementation details for each model are presented in Appendix A.1, A.2, and A.3."}, {"title": "6.3.1 Data Cleaning Performance and Hyperparameter Sensitivity.", "content": "In this section, we present the evaluation results in Table 2, showing the performance gain of the data cleaning process and hyperparameter sensitivity. We measure the mean and standard deviation under the various hyperparameter configurations to optimize the experiments.\nAcross the four deceleration datasets, the M2N2 model achieved the best performance in terms of F1-PA; however, its mean AUROC performance across the datasets did not exceed 0.5. In contrast, our proposed model achieved the second-best performance on IONIQ5-D1 and the highest AUROC performance on the other three datasets. Regarding AURPC, DBSCAN-DTW, and GAE-DBSCAN achieved the best performance on IONIQ5-D1 and GV80EV-D2, respectively. However, the difference from our model was only about 0.01. Our model achieved the highest performance on IONIQ5-D2 and GV80EV-D1, particularly excelling on GV80EV-D1 with an AUPRC of 0.9995.\nThe standard deviations (values following \u00b1) were generally higher for clustering models. Specifically, GAE-DBSCAN showed a high standard deviation of 0.4342 for F1-PA on the IONIQ5-D2 dataset, indicating significant hyperparameter sensitivity even with the best threshold for F1-score in each experiment. Additionally, DBSCAN-DTW, used in existing ACC, also exhibited generally high standard deviations. Moreover, for the IONIQ5-D2 dataset, the mean AUROC was 0.5 with a standard deviation of 0, indicating the worst-case scenario where DBSCAN-DTW consistently assigned all data as normal. Nonetheless, this not only implies high hyperparameter sensitivity for DBSCAN-DTW, but also suggests significant variability in hyperparameter ranges even within datasets of the same domain."}, {"title": "6.3.2 Generalization Ability.", "content": "Table 3 shows the F1-PA, AUROC, and AURPC performance of six baselines and GDFlow on four different dataset configurations. Our model achieves the highest AUROC on three datasets and shows only a marginal difference (0.0219) with upper bound on the fourth dataset. In the third dataset, our model outperforms all others in every aspect, particularly in AUROC, where the performance gap between our model (0.9550) and the second-best model (0.5) is notably significant. Despite its use in the existing ACC, DBSCAN-DTW shows the worst AUROC results of 0.5 in the second and third datasets, indicating its inability to generalize across different vehicle-driver combinations. This limitation arises from its requirement for meticulous hyperparameter tuning in each new scenario, rendering it impractical for ADAS. In terms of F1-PA and AUPRC, our model consistently demonstrates strong performance across most datasets, showcasing stable and generalized performance even on unseen datasets."}, {"title": "6.4 Evaluation with Benchmark Datasets", "content": "To demonstrate that our model's anomaly detection capabilities are effective not only for deceleration datasets but also in other real-world scenarios, we conducted experiments on four MTS benchmark datasets. Table 4 reports the AUROC performance of our model and five SOTA anomaly detection baselines on these four benchmark datasets. GANF and MTGFLOW, which are also NF-based models, show high performance, particularly MTGFLOW on SMD (Machine-1-4) and GANF on SMAP, indicating that NF-based models are highly effective for MTS anomaly detection in real-world scenarios.\nOn the other hand, our model outperforms all baselines across the four benchmark datasets, demonstrating its superiority over existing NF models and highlighting its applicability and reliability in real-world scenarios."}, {"title": "6.5 Ablation Study", "content": "To verify the performance improvement resulting from the incorporation of NCDE and the quantile function, we conducted an ablation study with three scenarios: GDFlow without NCDE (w/o NCDE), GDFlow without the Quantile function (w/o Q), and GDFlow without both (w/o NCDE & Q). Without NCDE, a vanilla RNN was used to handle the temporal information modeling. Table 4 presents the ablation study results on benchmark datasets, highlighting the impact of NCDE and the quantile function. In our experiments, the SMD dataset showed the worst AUROC of 0.5 when the quantile function was not used, detecting every profile as normal. Performance degradation was also seen in the MSL and SMAP datasets, especially when neither NCDE nor the quantile function was used. These results indicate that the quantile function is crucial for training stability. A detailed analysis of NCDE is further presented in Appendix C."}, {"title": "7 Conclusion", "content": "In this work, we propose GDFlow to effectively detect the anomaly patterns in the vehicle braking system as a part of Adaptive Cruise Control (ACC) in Advanced Driver Assistance Systems (ADAS). We aim to tackle the inherent challenges of previous clustering algorithms, which struggle with sensitivity to hyperparameters and difficulty in handling multivariate deceleration profiles. Our approach accurately learns the distribution of normal driving patterns, and simultaneously captures spatio-temporal information from sensor data, allowing for more precise modeling of continuous changes in driving behavior. Furthermore, we enhanced the anomaly detection capabilities of our model by incorporating a novel quantile-based maximum log-likelihood objective, which enables better identification of normal driving data. Our extensive experimental results with the dataset collected from real-world operating vehicles, as"}, {"title": "A Experimental Details for Deceleration Datasets", "content": "The experiments were conducted on a system with the following specifications: Python 3.10.13, NumPy 1.26.2, SciPy 1.11.4, Matplotlib 3.8.2, PyTorch 1.12.1, CUDA 11.3, torchdiffeq 0.2.4, Ubuntu 20.04.5 LTS, AMD EPYC 7352 24-core processor, NVIDIA driver 470.141.03, and NVIDIA RTX A5000 24GB.\nWe use the AdamW optimizer with a weight decay of 5e-4. The batch size is set to 256, and the window size is set as the shortest profile length in the deceleration data with stride 1. We use 80% of the entire data as a train split and train the model for 10 epochs. During training, we save the model at each epoch if it achieves the best F1-score, and the saved best model is used for testing."}, {"title": "A.1 Experimental Setup for Data Cleaning Performance", "content": "We explored hyperparameter sensitivity using the following combinations of hyperparameters for each model:\n\u2022 DBSCAN-DTW (using sklearn): We vary the values of the hyperparameters min samples \u2208 {2, 3, 4} and \u0454 \u2208 {0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 3.0}, respectively.\n\u2022 Sparse Autoencoder-DBSCAN (SAE-DBSCAN): We vary the values of the hyperparameters min samples \u2208 {2, 3, 4} and \u20ac \u2208 {0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 3.0}, respectively.\n\u2022 GRU-Autoencoder-DBSCAN (GAE-DBSCAN): We vary the values of the hyperparameters min samples \u2208 {2, 3, 4} and \u20ac \u2208 {0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 3.0}, respectively.\n\u2022 Deep Convolutional Embedded Clustering (DCEC): We vary the values of the hyperparameters k \u2208 {2,3,4}, kernel sizes \u2208 {(1, 1, 1), (3, 3, 1), (5, 5, 3), (7, 7, 5)}, stride sizes \u2208 {(1, 1, 1), (2, 2, 2)}, and padding sizes \u2208 {(0, 0, 0), (1, 1, 1), (2, 2, 2), (0, 1, 2)}, respectively.\n\u2022 M2N2: We vary the values of the hyperparameters test-time learning rates \u2208 {1e-01, 1e-02, 1e-03, 1e-04}, latent dims \u2208 {8, 16, 32, 64}, and gammas \u2208 {0.1, 0.4, 0.7, 0.9}, respectively..\n\u2022 MTGFLOW: We vary the values of the hyperparameters learning rates \u2208 {1e-1, 3e-2, 3e-3, 1e-4}, flow blocks \u2208 {1, 2}, and hidden sizes \u2208 {8, 32, 64}, respectively..\n\u2022 GDFlow (Ours): We vary the values of the hyperparameters learning rates \u2208 {1e-1, 3e-2, 3e-3, 1e-4}, q-quantiles \u2208 {0, 0.01, 0.03, 0.05, 0.07, 0.1}, flow blocks \u2208 {1, 2}, and hidden sizes \u2208 {8, 32, 64}, respectively."}, {"title": "A.2 Experimental Setup for Generalization Ability", "content": "We summarize the hyperparameters used in our experiments to evaluate the generalization ability as follows:\n\u2022 DBSCAN-DTW: We set min samples to 3 and eps to 40. Only the test set is used since no training is needed.\n\u2022 SAE-DBSCAN: We set the learning rate to 3e-3, min samples to 2, eps to 1.0, and sparsity to 0.7. The model consists of an encoder and a decoder, each with a single layer. The embedding size is the same as the window size.\n\u2022 GAE-DBSCAN: We set the learning rate to 3e-3, min samples to 2, and eps to 0.8. The model consists of an encoder"}, {"title": "A.3 Experimental Setup for Benchmark Datasets", "content": "The performance of THOC, AT, and M2N2 follows the experimental results from [24], and thus the experimental settings are assumed to be the same as those in the referenced paper. The hyperparameter settings for other NF-based models and GDFlow, covering the four datasets SMD (Machine-1-4 and Machine-2-1), MSL (P-15), and SMAP (T-3), are as follows:\n\u2022 GANF and MTGFLOW: The model consists of 1 flow block with a batch size of 512. We set the window size to 60, the stride to 10, and the train split to 0.8. The learning rate is 2e-3, and the number of epochs is 40. The hidden size is set to 32. These settings are used for experiments on the four benchmark datasets.\n\u2022 GDFlow (Ours): The model consists of 1 flow block with a batch size of 256. We set the window size to \u2208 {60, 20, 10, 60}, the stride to \u2208 {10, 1, 10, 10}, and the train split to 1.0, meaning no validation set was used. The learning rate is \u2208 {3e-2, 3e-3, 1e-1, 1e-4}, and the number of epochs is 20. The hidden size is \u2208 {8, 64, 32, 8}. The q-quantile values are \u2208 {0.05, 0.01, 0.07, 0.1}. These settings are used for experiments on the four benchmark datasets."}, {"title": "B Benchmark datasets", "content": "The SMD dataset includes five weeks of data from 28 distinct server machines with 38-dimensional sensor inputs. For our experiments, we selected two specific server machines (Machine-1-4 and Machine-2-1) due to their distribution shift problems. The SMAP and MSL datasets, derived from spacecraft monitoring systems, also played a key role in our evaluation. The SMAP dataset comprises monitoring data from 28 unique machines with 55 telemetry channels, while the MSL dataset includes data from 19 unique machines with 27 telemetry channels. We selected data from two specific"}, {"title": "C Comparison of Memory Efficiency and Computational Cost between NCDE and RNN", "content": "For RNN models, the required memory is O(T \u00d7 h), where T is the sequence length, and h is the dimension of the hidden state. In contrast, NCDE requires only O(L + h) memory, where L is the integral time space T\u2081 To, and h is the size of the vector field. Since we use two NCDEs, our model requires O(2L + hf1 + hf2) memory. Unlike the memory requirements of RNNs, NCDE requires significantly less memory, making it not only performance-efficient but also cost-efficient.\nAdditionally, the ODE solvers used in NCDE address the slow convergence of traditional SDE solvers, and unlike RNNs that converge monotonically, they model dynamics over [To, T\u2081], enabling faster convergence and fewer discretizations of the solver [12, 18, 31]. Thus, GDFlow, as an NCDE-based model, can perform matrix-vector multiplication without large computational costs [20, 23]. This efficiency helps mitigate the increasing computational load as driving data accumulates in electric vehicles, making it suitable for real-world industrial deployment."}]}