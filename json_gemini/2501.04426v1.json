{"title": "Dual-Force: Enhanced Offline Diversity Maximization under Imitation Constraints", "authors": ["Pavel Kolev", "Marin Vlastelica", "Georg Martius"], "abstract": "While many algorithms for diversity maximization under imitation constraints are online in nature, many applications require offline algorithms without environment interactions. Tackling this problem in the offline setting, however, presents significant challenges that require non-trivial, multi-stage optimization processes with non-stationary rewards. In this work, we present a novel offline algorithm that enhances diversity using an objective based on Van der Waals (VdW) force and successor features, and eliminates the need to learn a previously used skill discriminator. Moreover, by conditioning the value function and policy on a pre-trained Functional Reward Encoding (FRE), our method allows for better handling of non-stationary rewards and provides zero-shot recall of all skills encountered during training, significantly expanding the set of skills learned in prior work. Consequently, our algorithm benefits from receiving a consistently strong diversity signal (VdW), and enjoys more stable and efficient training. We demonstrate the effectiveness of our method in generating diverse skills for two robotic tasks in simulation: locomotion of a quadruped and local navigation with obstacle traversal.", "sections": [{"title": "Introduction", "content": "Leveraging demonstration data has established itself as one of the main directions for large-scale learning systems. This is due to the abundance and ubiquity of demonstration data from various sources, such as videos, robots, and more. There are several arguments as to why we should not stop at naive learning from demonstrations. First, they are often not ego-centric and come externally to the agent, meaning that the state space of the demonstration needs to be matched to the agent. Second, the agent may not be able to fully replicate the demonstration due to limited capabilities (Li et al., 2023). This suggests that an agent must necessarily adapt the demonstration to its capabilities, which is achieved by extracting diverse behaviors that are close to the demonstration (Vlastelica et al., 2024). Moreover, another important aspect is robustness to distribution shifts. Tasks may be solved in various ways, some are more robust than others. Extracting diverse policies enables us to choose the more robust alternatives (Vlastelica et al., 2024). Alternatively, if we can quantify the risk of acting with a particular policy, we can encourage risk-averse behavior, which is an orthogonal approach (Vlastelica et al., 2022).\nPrevious work on maximizing diversity under various constraints has been formalized in the Constrained Markov Decision Process formulation (Zahavy et al., 2023; Vlastelica et al., 2024; Cheng et al., 2024). Solving the underlying constraint optimization problem involves a Lagrangian relaxation (a two-phase alternating scheme) in which the constraints are lifted to the (reward) objective and scaled by Lagrangian multipliers that adaptively reduce constraint violations (Zahavy et al., 2023; Cheng et al., 2024). In contrast to the online setting (Zahavy et al., 2023; Cheng et al., 2024), we focus here on the offline setting without environment interaction and relax constraints that enforce near-optimal returns by considering imitation constraints, as in Vlastelica et al. (2024). Our approach to offline learning from demonstrations crucially relies on the Fenchel"}, {"title": "Preliminaries", "content": "We utilize the framework of Markov decision processes (MDPs) (Puterman, 2014), where an MDP is defined by the tuple (S, A, R, P, \\rho_0, \\gamma) denoting the state space, action space, reward mapping R : S \\times A \\rightarrow R, stochastic transition kernel P(s'|s, a), initial state distribution \\rho_0(s) and discount factor \\gamma. A policy \\pi : S \\rightarrow \\Delta(A)"}, {"title": "Constrained Markov Decision Process (CMDP)", "content": "Zahavy et al. (2023) studied a CMDP formulation that seeks to compute a set of policies \\Pi^n = {\\pi_i}_{i=1}^n that satisfy\n$\\max_{\\Pi^n} Diversity(\\Pi^n) subject to \\langle d_{\\pi}, r_e \\rangle \\geq \\alpha v, \\forall \\pi \\in \\Pi^n,$\nwhere $r_e$ is an extrinsic reward and $v_a$ an optimal extrinsic value. Intuitively, eq. (1) computes a set of diverse policies while maintaining a certain level of extrinsic optimality specified by a parameter $\\alpha \\in (0,1]$. They designed a heuristic for optimizing convex diversity objectives by solving a sequence of standard RL problems, each with an intrinsic reward equal to the gradient of the objective evaluated at the previous step (say kth) time-averaged state-action occupancies {d1k,...,dnk}, namely\n$r_i^{k+1} = \\nabla_{d_i} Diversity(d_1^k,...,d_n^k), \\forall i \\in {1, ..., n}.$\nThe Lagrange relaxation of the CMDP in eq. (1) becomes an RL problem with a reward function that depends on Lagrange multiplier $\\lambda > 0$ that balances the extrinsic and intrinsic reward (Borkar, 2005)\n$r_i^{k+1} = r_e + \\lambda_i r_i^{k+1}, \\forall i.$\nwhere the Lagrange multipliers are minimizing the following loss\n$L_\\lambda = \\sum_{i=1}^n \\lambda_i (\\langle d_i, r_e \\rangle - \\alpha v).$\nIntuitively, a Lagrange multiplier increases when the associated constraint is violated and decreases otherwise. The practical implementation considers an extrinsic and intrinsic advantage coupled with bounded Lagrange multipliers (Stooke et al., 2020; Cheng et al., 2024), i.e., applying Sigmoid activation $\\sigma(\\mu_i)$ to an unbounded variable $\\mu_i \\in \\mathbb{R}$, for all i \u2208 {1, ..., n}."}, {"title": "Functional Reward Encoding (FRE)", "content": "Recently, Frans et al. (2024) proposed an information bottleneck method (Tishby et al., 2000; Alemi et al., 2017) for encoding state-reward samples into a latent representation using a transformer-based variational auto-encoder. Specifically, for a reward function r sampled from a prior probability distribution, the latent representation z, encoded from an arbitrary subset of state-reward samples Lenc = {(s,r(s)) : s\u2208 Denc} should be as compressive as possible, while being maximally predictive for decoding the rewards of other arbitrary subset of state-reward samples Ldec. The empirical evaluation demonstrated, in standard D4RL offline environments (Fu et al., 2020), that optimizing a latent-conditioned policy \\pi(\\cdot|\\cdot, z) with fixed Functional Reward Encoding (FRE) pretrained on diverse class of reward functions (random linear, MLP, and goal-reaching), enables zero-shot solving of downstream tasks. Namely, given arbitrary state-reward samples Lenc, the latent-conditioned policy \\pi(\\cdot|\\cdot, z_r) with latent representation $z_r = FRE(L_{enc})$ optimizes the reward r."}, {"title": "Problem Formulation", "content": "We aim to solve the following optimization problem,\n$\\max_{d_1,...,d_n} Diversity (d_1,..., d_n)$\nsubject to $DKL (d_i(S)||d_e(S)) \\leq \\epsilon \\forall i \\in {1, ..., n},$\nwhere de(S) is a state-only expert occupancy. This puts us in a similar setting as Vlastelica et al. (2024), with two key differences: (i) we shall introduce a more stable diversity objective; and (ii) we shall relax the state occupancy constraints while preserving their state-only occupancy nature, allowing for more freedom in diversity maximization."}, {"title": "Diversity Measures", "content": "Vlastelica et al. (2024) used a variational lower bound on a mutual information I (S; Z) between states and latent skills, resulting in the following diversity objective\n$I (S; Z) \\geq E_{p(z),d_z(s)} [log q(z|s)] + H (p(z)) = \\sum_{z \\in Z} E_{d_z(s)} [log (\\frac{q_{z|s}}{|Z|})],$\nwhere p(z) is a categorical distribution over a discrete set Z of |Z| many distinct indicator vectors in R|Z| and dz(s) := d\\pi_z(s) is a state occupancy induced by a skill-conditioned policy \\pi_z. While they showed that this objective can be estimated off-policy using a DICE importance sampling approach, this comes at the cost of learning a skill-discriminator and makes the training unstable.\nZahavy et al. (2023) modeled a diversity objective, based on a distance measure in (Abbeel and Ng, 2004), as a maximization of a minimum squared l2 distance between successor features of different skills, namely\n$\\max_{d_1,...,d_n} \\sum_{i=1}^n 0.5 min_{j \\neq i} || \\psi^i \u2013 \\psi^j ||_2^2.$\nMore specifically, given a feature mapping \\phi : S \u2192 Rn, the successor features (Dayan, 1993; Barreto et al., 2016) are defined by \\psi_i = Edi(s)[\\phi(s)]. An important property of this convex objective is that its gradient is given in closed form, derived for completeness in Thm. C.1, eliminating the need to learn a skill discriminator.\nFurthermore, Zahavy et al. (2023) introduced a physically inspired objective based on Van der Waals (VdW) force, and considered the following optimization objective\n$\\max_{d_1,...,d_n} \\sum_{i=1}^n 0.5 [l_i - \\frac{l_0}{l_i^2}],$\nwhere $l_i := || \\psi_i - \\psi_j ||_2$ and j := arg $\\min_{j\\neq i} ||\\psi_i \u2013 \\psi_j||_2$. In this work, we use this formulation as it allows the level of diversity to be controlled by a parameter $l_0$. When the successor features are in close proximity $l_i<l_0$, the repulsive force dominates, whereas when $l_i > l_0$ the attractive force prevails. In the setting when $l_0\\rightarrow\\infty$, the formulation is eq. (8) is recovered."}, {"title": "Method", "content": ""}, {"title": "Van der Waals Force", "content": "Our key technical insight is that in the context of KL-divergence imitation constraints, see Sec. 3, all relevant quantities in the approach of (Zahavy et al., 2023), including dual-conjugate variables and successor features, can be estimated off-policy using a DICE importance sampling procedure."}, {"title": "Offline Estimators using Fenchel Duality", "content": "The DICE framework solves offline the KL-regularized RL problem in eq. (12) by considering its dual formulation, which reads\n$V^* = \\underset{V(s)}{argmin}(1 \u2212 \\gamma)E_{s\\sim\\rho_0} [V(s)] + log E_{d_0(s,a)} exp {\\mathcal{R}(s, a) + \\gammaTV(s,a) \u2212 V(s)},$\nwhere we denote by TV(s, a) := Ep(s'|s,a)V(s'). The temporal difference (TD) error is given by\n$\\delta_t(s, a) = \\mathcal{R}(s, a) + \\gammaTV^*(s, a) \u2212 V^*(s).$\nThen, the primal solution of Problem 11 is given by\n$\\pi_i(s, a) := \\frac{d_i(s, a)}{d_0(s, a)} = softmax_{d_0(s, a)} (\\delta_i(s, a)) = \\frac{exp{\\delta_i(s, a)}}{E_{d_0(s',a')} exp{\\delta_i (s', a')}}.$"}, {"title": "Handling Non-Stationary Rewards", "content": "To optimize Problem 5 offline, we extend the heuristic by Zahavy et al. (2023) and propose an alternating optimization scheme whose pseudocode is presented in Algorithm 1. While on fixed reward input, the DICE framework computes offline an optimal-dual valued function, in our setting the reward is changing in every iteration. This non-stationarity of reward presents a practical challenge in training the value function and policy. As noted by Vlastelica et al. (2024), the naive approach of training the value function (neural network) to match a moving target tends to be unstable, due to the non-stationary rewards and is further exacerbated by the fact that in each iteration only a single gradient update is made for this reward.\nIn this work, we address the preceding challenge by conditioning the value function (and policy) on a latent representation of a Functional Reward Encoding (FRE) (Frans et al., 2024), which is pre-trained on random linear functions, random two-layer neural networks with different hidden units, and simple human-engineered rewards. Further details on pre-training are given in Supp. D. In each iteration, given a fixed reward r we compute its encoded FRE latent representation $z_r (S)$ over a subset of state-reward samples L(r, S) := {(s,r(s)) : s \u2208 S}, where S is subset of states sampled uniformly at random from States [Do]. Further, to reduce the variance, we sample uniformly at random several state subsets {S1,..., Sm} and take the mean \\bar{z_r} over their FRE latent representations \\bar{z_r} (Si)."}, {"title": "Pseudocode of Dual-Force", "content": "In line with standard deep learning practices, the value function and policy are parameterized with neural networks and consequently updated with a single gradient step over batches sampled uniformly at random. Given a batch B, the policy loss becomes \\mathcal{L}^\\pi_{i_k+1} \\approx \\sum_{(s,a)\\in B} w^{k+1}_i(s, a) log \\pi_i(a|s, z)."}, {"title": "Experiments", "content": ""}, {"title": "Locomotion Task", "content": "Data Collection. The expert dataset is collected from a uni-modal expert trained to walk straight with constant linear velocity and middle base-height. The offline dataset contains non-expert behaviors achieving constant linear and angular velocity, as well as walking movements with different base-heights (low, middle, orange).\nResults [Uni-Modal]. Figure 2 demonstrates that our method finds diverse skills that achieve constant linear and angular velocity and, more importantly, recovers all base-height movements in the offline dataset, including the expert behavior. Figure 3 (a,b) shows that the successor features of the learned skills are clustered into three groups (according to the base-height). While the l2 pairwise distance between the successor features within a cluster is small, the distance between clusters is large. Here we use the UMAP (McInnes et al., 2018) algorithm to project the successor features into 2D space."}, {"title": "Obstacle Navigation Task", "content": "Data Collection. The expert dataset is collected from a multi-modal expert that initialized in front of a box is trained to reach a target position behind the box by either going over the box or surrounding it from"}, {"title": "Related Work", "content": "Skill discovery. In the unconstrained and online setting, various approaches of unsupervised skill discovery algorithms have been proposed (Eysenbach et al., 2019; Campos et al., 2020; Achiam et al., 2018; Strouse et al., 2022). These methods struggle to learn large numbers of skills (Campos et al., 2020; Achiam et al., 2018). Sharma et al. (2020) make use of skill predictability as a proxy for guiding skill discovery. Strouse et al. (2022) introduce an ensemble-based information gain formulation. Kim et al. (2021) are able to learn disentangled and interpretable skill representations. All of these methods are online methods that require extensive environment interactions.\nUnsupervised RL. The goal of unsupervised reinforcement learning is to extract diverse policies that are optimal for a particular family of reward functions. To this end, successor features have been utilized (Dayan, 1993; Barreto et al., 2016). These methods have also been coupled with intrinsic motivation to enhance diversity (Gregor et al., 2017; Barreto et al., 2016; Hansen et al., 2020).\nConstrained skill discovery. We are not the first to consider a constrained diversity maximization approach. Zahavy et al. (2023) proposed an online skill discovery method that handles value constraints. Cheng et al. (2024) extended their approach to the setting of multiple constraints, while still remaining in an online setting. The diversity objectives in (Zahavy et al., 2023; Cheng et al., 2024) both use the VdW force. Vlastelica et al. (2024) proposed an offline algorithm for maximizing a mutual information objective subject to imitation constraints.\nOff-policy estimation. Our work builds upon the \"DIstribution Correction Estimation (DICE)\" framework and provides a robust importance sampling technique for off-policy learning (Nachum and Dai, 2020) which finds applications in computing policy gradients from off-policy data (Nachum et al., 2019), offline imitation learning with imperfect demonstrations (Kim et al., 2022; Ma et al., 2022a), and off-policy evaluation (Dai et al., 2020). Our off-policy approach is also similar to (Lee et al., 2021; 2022; Vlastelica et al., 2024)."}, {"title": "Conclusion", "content": "In this work, we introduce Dual-Force, a novel offline algorithm designed to maximize diversity under imitation constraints based on expert state demonstrations. Our main contribution is to propose off-policy estimator of the Van der Waals force, eliminating the need for a skill discriminator, thus enhancing training stability and efficiency. Furthermore, by conditioning the value function and policy on a pre-trained Functional Reward Encoding, our method handles non-stationary rewards better and provides zero-shot recall of all skills encountered during training. Experimental results demonstrate the effectiveness of the proposed method in generating diverse skills for robotic tasks in simulation. Notably, the learned skill set includes various modalities derived from both expert and offline datasets, highlighting the method's robust capabilities for versatile skill discovery."}]}