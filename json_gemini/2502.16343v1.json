{"title": "Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading Agents", "authors": ["David Byrd"], "abstract": "Companies across all economic sectors continue to deploy large language models at a rapid pace. Reinforcement learning is experiencing a resurgence of interest due to its association with the fine-tuning of language models from human feedback. Tool-chain language models control task-specific agents; if the converse has not already appeared, it soon will. In this paper, we present what we believe is the first investigation of an intelligent trading agent based on continuous deep reinforcement learning that also controls a large language model with which it can post to a social media feed observed by other traders. We empirically investigate the performance and impact of such an agent in a simulated financial market, finding that it learns to optimize its total reward, and thereby augment its profit, by manipulating the sentiment of the posts it produces. The paper concludes with discussion, limitations, and suggestions for future work.", "sections": [{"title": "Introduction", "content": "Social media pervades every area of the human experience. Autonomous reinforcement learning (RL) based agents drive on our roads and trade in our financial markets. Large language models (LLMs) are filling the internet with \"AI slop\". This article ties together all of these areas with an eye towards the social good, presenting the first integrated effort to evaluate market manipulation through the generation of natural language social media posts by an LLM-enabled RL trading agent in a realistic market simulation. This section provides background on the several areas which must be combined to approach this problem."}, {"title": "Market Manipulation", "content": "Market manipulation refers to a set of practices intended to produce temporary distortions in the price of a marketable security independent of its actual current supply, demand, or fundamental value. Usually these practices are deployed to enter or exit a position at a more advantageous price than currently offered in the market. From an AI perspective, two of the most relevant types of manipulation are:\n\u2022 Pump and dump occurs when one makes false or exaggerated claims related to an owned security with the intent to pump up its price before dumping one's shares into the market.\n\u2022 Spoofing occurs when one places bids or offers with the intent to cancel before execution, usually to narrow the spread and give a false appearance of supply or demand at certain price levels, which other participants may legitimately join.\nIn the United States, the pump and dump scheme has been considered securities fraud since at least the Securities Exchange Act of 1934. Spoofing, a more modern and technical form of manipulation, was one of the \u201cdisruptive trading practices\" banned by the Dodd-Frank Act of 2010 in the wake of the 2008 financial crisis.\nThe adoption of spoofing, either intentionally or inadvertently, by intelligent trading agents has recently become a topic of research interest. (Wang and Wellman 2017) introduced what may be the first agent-based spoofing model and analyzed its effect on other traders. (Wang and Wellman 2020) explored spoofing again in the adversarial context of detection and evasion. (Byrd 2022) demonstrated that a sufficiently capable RL-based agent, in the presence of order book aware traders, can inadvertently discover spoofing as the optimal policy for profit maximization.\nThis work breaks ground in the study of the other listed form of manipulation: the pump and dump scheme. As language models are agglomerated to computational systems of all kinds, it has now become possible for an intelligent agent to learn to manipulate market prices through the generation and transmission of natural language. That is, we will soon become aware of either the accidental or deliberate adoption of social media \"pumping\" by an autonomous trading agent seeking enhanced returns."}, {"title": "Financial Market Simulation", "content": "Computational simulation of complex financial markets is an established but unperfected field of study with advantages and drawbacks compared to other empirical approaches: natural experiments may not occur in a timely manner, field studies can be very expensive, and the undergraduate participants in most laboratory studies do not necessarily behave rationally. Simulation addresses all of these problems, but introduces its own: it can be difficult to rigorously demonstrate that a simulated market is a sufficient substitute for the real thing."}, {"title": "", "content": "A common criticism of the computational simulation of electronic markets is that participant strategies must be ex-ogenously specified rather endogenously chosen. This issue has been successfully addressed by recent advances in market simulation. introduced a computer market simulation, MarketSim, in which agents could change their strategic parameters between trials, reacting to their prior success or failure. presented an open source simulation called ABIDES which focused on experimental RL-based agents that continuously alter their behavior in response to market conditions. released PyMarketSim, a modernized version of the earlier simulation which added a Deep RL agent as one of its key features.\nThis work utilizes a minified version of the open source ABIDES simulation, streamlined to be smaller, faster, and simpler than the original, with a greater focus on multi-agent interactive backtesting using high frequency historical data. The historical data used is the complete Nasdaq order flow at all price levels and nanosecond time resolution, under academic license from LOBSTER."}, {"title": "Large Language Models", "content": "The field of language modeling has seen exponential capabilities growth the last few years, which can be traced most directly to the transformer and dot-product attention mechanism introduced by (Vaswani 2017). This mechanism has been combined with bidirectional encoding, pre-trained foundation models, retrieval augmented generation, reinforcement learning with human feedback, and other advances to produce the current generation of leading LLMs including ChatGPT, Claude, Gemini, and Llama. In this work, we use the 1B and 3B parameter Llama 3.2 models from Meta for all text generation.\nOur approach separately employs a language model for sentiment analysis. (Devlin et al. 2019) introduced BERT, or Bidirectional Encoder Representations from Transformers, based on the earlier use of bidirectional RNN language encoding by (Bahdanau 2014). BERT accepts encoded natural language of up to 256 tokens at a time, and returns a positive, neutral, or negative sentiment classification as well as a confidence score. In this work, we employ the latest version of ROBERTa, an enhanced version of BERT by (Liu et al. 2019). For both text generation and sentiment analysis, all model inference is performed locally."}, {"title": "Reinforcement Learning", "content": "Reinforcement learning is an experimental approach to the optimization of an action-selection problem of unknown parametrization. (Sutton and Barto 2018) An autonomous agent interacts with its environment, attempting actions initially randomly, and learns a policy mapping states to optimal actions through a recurrent process of trial and error. The underlying problem is codified as a Markov Decision Process (MDP) with unknown parameters: S, a set of states; A, a set of actions; So, an initial state; SF, a set of final or terminal states; T or Pr(s'|s, a), a transition probability matrix, and R(s, a, s'), a function producing the rewards the agent seeks to maximize in the long run."}, {"title": "", "content": "One of the most common problem formulations for RL is Q-learning, an off-policy, greedy, model-free approach in which the agent maintains an estimate of the Q-function, or the expected sum of all immediate and discounted future rewards to be received by taking a particular action from some state, and then continuing to follow the policy implied by the current Q-function. Q-learning iteratively optimizes a version of the Bellman equation:\n$Q^{n}(s, a) = R_{s}(a) + \\gamma \\sum_{s' \\in S} Pr(s' | s)V(s')$  (1)\nwhere $\u03c0$ is the current policy mapping states to actions, V is a value or utility function, and other variables retain the semantics of the underlying MDP.\nThe RL models in the current work are instances of Deep RL, an advance that saw the replacement of Q-tables, which force quantization to a discrete state and action space and worsen the \"curse of dimensionality\", with Q-networks for continuous neural inference. We focus on Deep Deterministic Policy Gradient (DDPG), an approach which permits arbitrarily continuous states and actions. In DDPG, an actor function \u03bc(s|\u03b8\u03bc) represents the current policy, deterministically mapping any continuous input state to a continuous output action. A separate critic function Q(s, a) estimates the Q-function for an input continuous state-action pair. Both functions are approximated by deep neural networks. We programmed a strategic market agent using a refinement of this technique called Twin-Delayed Deep Deterministic Policy Gradient (TD3). In this variant, one actor network is supported by two critic networks to address the overestimation of Q values during training in a manner similar to the earlier Double Q-learning in a tabular setting. The actor network selects continuous action output values according to:\n$a = \\pi_{\\varphi}(s) + \\epsilon$ (2)\nwhere \u03c6 is the actor parametrization and \u03b5 is normally-distributed exploration noise, and updates from mini-batches on:\n$y = r + \\gamma min_{i=1,2} Q_{\\theta_{i}}(s', \\bar{a})$  (3)\nwhere $Q_{\\theta}$ are the parametrized critic networks and a is the successor action implied by the current actor policy. Critic networks receive a state-action pair as input, estimate its value, and are sampled and trained similarly to basic Q-learning. Actor and critics each have a target network, which slowly adjusts toward the current parametrization of the paired network according to an interpolation rate \u03c4:\n$\\theta' = \\tau\\theta_{i} + (1 - \\tau)\\theta$ (4)\n$\\varphi' = \\tau\\varphi_{i} + (1 - \\tau)\\varphi$ (5)\nThese changes are suggestive of the \"twin delayed\" name, and improve stability and convergence of the learning agent."}, {"title": "Experimental Approach", "content": "The experimental reinforcement learning agent in this investigation generates potentially misleading social media"}, {"title": "", "content": "posts. To avoid inadvertently committing securities fraud, we therefore simulate an isolated, offline social media feed. The feed is generated by replaying every visible order for selected stocks for each included date. Orders are selected uniformly randomly at a rate of one hundred per minute. For each selected order, the preceding sequence of ten orders is also retrieved. These order sequences are given to a language model to produce two kinds of simulated social media posts. Analyst posts are generated with the following prompt:\nYou are an opinionated stock analyst who likes to post on social media. You have recently observed the following stock orders: [orders]. Write a social media post analyzing the stock. Pay attention to the orders when forming your opinion. Do not mention a sentiment score. Do not write any text outside of the post itself.\nand trader posts are generated with:\nYou are an opinionated day trader who likes to post on social media. You have recently observed the following stock orders placed by other traders: [orders]. Write a [sentiment] social media post giving your informed opinion based on the observed orders. Do not mention a sentiment score. Do not write any text outside of the post itself.\nwhere the requested sentiment depends on the direction of the following order, assumed to be placed by this trader in response to the prior orders.\nFollowing this method, we generate simulated social media posts from analysts and traders for every required stock and date, which are indexed with a \"time of posting\" corresponding to the last order in the sequence. Here is an example synthetic analyst post:\nAPPLE INVESTORS REJOICE!\nJust analyzed a flurry of AAPL trades and I've got my finger on the pulse! There's a clear buying frenzy around $223.50-$223.52, with multiple orders coming in at these levels. The volume is also increasing, indicating confidence among investors.\nHowever, notice what's happening on the other side there are SELL orders at $224.75 and $223.66, indicating some sellers trying to capitalize on the short-term rally. BUT, these orders seem to be getting snubbed, with no significant fills.\nThe recent buy orders at $223.48-$223.49 suggest a potential support level is being tested. Will buyers continue to push AAPL up or will sellers take control? Stay tuned for further updates! #AAPL #StockMarket #Investing\nThese posts are sampled by the sentiment agent in order to make its trading decisions as described below."}, {"title": "Sentiment Agent", "content": "To evaluate the potential for a learning agent to manipulate others through language generation, there must be an agent capable of being manipulated. For this evaluation, we construct a sentiment-based trading agent to fill the role."}, {"title": "", "content": "The sentiment agent acts approximately once per minute. At each market arrival, it receives a random sample of the pre-generated social media posts for the correct date, time, and stock, that it has \u201cseen\u201d since its previous action. It separately performs sentiment analysis on each post in the feed using the RoBERTa model described in the Introduction.\nEach post receives a sentiment label of \u201cnegative\u201d, \u201cneutral\", or \"positive\u201d, as well as a real-valued confidence score in the range [0, 1]. The agent sentiment heuristic for each label is a confidence-weighted sum:\n$v_{l \\in L,p \\in P} : h(l) = \\sum_{p}^{p^{l=l}}$  (6)\nwhere L and P are sets of labels and posts, and pl and pe are the sentiment label and confidence score for post p. The label with the highest score is selected as the current sentiment. The agent places trades on the assumption that recent sentiment will correspond with short term momentum. This assumption may or may not not be valid, but is nevertheless common among retail traders who frequent Reddit's r/wallstreetbets and similar forums."}, {"title": "RL-based Trading Agent", "content": "The primary experimental trading agent uses the TD3 algorithm, as explained in the Introduction, to learn a policy mapping continuous states to continuous actions. The agent receives two state components: internal and environmental. The internal state contains the agent's holdings and open orders. The environmental state consists of a sequence of limit order book snapshots taken at five second intervals. The length and depth of this component are hyperparameters, and all observations are normalized. While it is not part of the environmental observation, the recent raw order stream of the simulated market is also captured and given to the RL agent. This information is used by the RL agent to produce social media posts in some experiments.\nThe action space for the agent consists of two outputs. The first is in the range a\u00ba \u2208 [-2, 2] and is interpreted as a request to sell (negative) or buy (positive) a number of shares corresponding to 1000a\u00ba. To prevent excessive leverage, a holdings limit of 1000 shares is enforced in either direction. The second action is in the range a\u00b9 \u2208 [-1,1] and is interpreted as the agent's current sentiment towards the stock, where -1.0 is extremely negative and 1.0 is extremely positive. Note that the agent independently controls this sentiment and learns it as part of the actor policy. The RL agent reward is the percent change in its total portfolio value since its last action. To avoid arbitrary effects based on starting cash and value-at-risk, all agents start with zero dollars (and may spend negative) and a fixed offset is added to all portfolio value calculations to keep the percent changes on a similar scale.\nWhen the RL agent is configured to make social posts after each action selection, the following prompt is used:\nYou are an opinionated stock analyst who likes to post on social media. You have recently observed the following stock orders: [orders]. You feel [sentiment]"}, {"title": "", "content": "about the stock. Write a social media post analyzing the stock. Pay attention to the orders when forming your opinion. Also include any relevant knowledge you have about the company's products, management, or competition. Do not mention a sentiment score. Do not mention individual trade quantities. No bullet lists. Do not write any text outside of the post itself.\nThe prompt includes the most recent orders that have passed through the simulated exchange and a sentiment based on a\u00b9. Early experiments attempted to give the raw action values to the language model, with an explanation of the sentiment scale used, but the results were inconsistent. In the final experiments, the selected sentiment actions were quantized to appropriate English text, for example -1.0 as \u201cextremely negative\" and 0.0 as \u201cneutral\u201d. Here is an example post generated by the RL trading agent after it selected a very high sentiment output:\nBREAKING: Apple (AAPL) is on the RISE!\nI'm analyzing the latest orders and I'm blown away by the bullish momentum:\nMultiple SELLs at $229.xxx, indicating investors are confident in AAPL's value and ready to lock in profits.\nBut WAIT, there's more! The sudden surge of BUYS at $227.xxx & $229.xxx tells me these savvy investors see a buying opportunity that will soon propel AAPL to NEW HEIGHTS!\nDid you know Apple is on track to release the most anticipated iPhone yet? With its A16 Bionic chip, improved cameras, and sleek design - this new iPhone is going to DOMINATE the market!\nAnd let's not forget about AAPL's loyal customer base, which has remained STABLE despite global economic uncertainty. This dedicated fanbase ensures a steady stream of sales for Apple.\nBut what really gets me excited is the buying pressure building up in the stock price. The recent SELLS have cleared out the market, paving the way for the incoming BUY orders to send AAPL SOARING!\nStay tuned, folks! I'm calling it now - Apple (AAPL) is about to experience a MAJOR MOONSHOT! Get ready to ride this wave of success all the way to the top! #AAPL #AppleStock #InnovationNation\nThe RL-based agent can be configured not to generate social media posts. When it does generate them, they are added to the social media feed observed by the sentiment agent."}, {"title": "Experiments and Results", "content": "This section describes a sequence of empirical studies that investigate the efficacy of our RL agent and sentiment agent against historical data flows routed through a simulated exchange, first separately and then interactively together in various configurations."}, {"title": "Reinforcement Learning Backtest", "content": "To ensure the RL agent is correctly implemented, we first conduct experiments to evaluate its ability to learn patterns in historical data and demonstrate some positive predictive ability. For these experiments, the simulation is configured in its backtest mode: all orders including pre-market are replayed through the simulated exchange, which maintains an accurate limit order book. The RL agent trades through the"}, {"title": "", "content": "simulated exchange, thus impacting the LOB, with realistic communication and computation latency delays that create slippage as expected. No additional fixed transaction costs are applied.\nNine actively-traded symbols are tested including one ETF. Five different days of full order flow data are used, for a total of 45 stock-days. Each training run consists of five passes through the training period of a single day, followed by an in-sample evaluation on the same period, and a single out-of-sample evaluation on a period later in the same day. A simple hyperparameter grid search was performed on in-sample data to select reasonable values without excessive tuning. For all experiments, the TD3 agent uses similar actor and critic networks with layer normalization between three fully-connected layers. The actor network additionally learns an embedding of the environmental observation sequence using an LSTM layer, which is then concatenated with the internal state.\nDetails of the hyperparameter search are omitted for brevity. The selected hyperparameters were: order book depth 10 with sequence length 20, sequence embedding size 6, network size 32, learning rate 0.01, exploration noise 0.2, policy noise 0.4, batch size 32, policy frequency 2, tau 0.02, and gamma 0.99. The RL agent trades approximately every 30 seconds, or twice as often as the sentiment agent.\nThe entire experiment for each stock-day was repeated 16 times, resulting in 3600 total training periods across models, and 720 each of in-sample and out-of-sample evaluations. The in-sample period was one hour; the out-of-sample period was thirty minutes. The in-sample results are presented in Table 1 and out-of-sample in Table 2. As the mean and median of all symbols are positive in-sample, with an average performance across symbols of $532.87, we conclude that the agent is locating actionable information in the state space. The agent's out-of-sample performance is mixed but reasonable, with an average across symbols of $20.29. Our aim is to examine the impact of this trader on a sentiment agent, rather than to beat the market, so this is an acceptable result."}, {"title": "Sentiment Backtest", "content": "The sentiment agent is also backtested separately to validate that there is useful information in the simulated social media posts it receives. As explained in the Social Media Feed and Sentiment Agent sections above, this agent receives a ran-"}, {"title": "", "content": "dom sample of recent posts appropriate to the current simulated time, date, and stock, which were generated previously from the real historical order flow. In the current approach, this is not a learning agent, so results are reported only on the in-sample period in Table 3. The strategy was evaluated for 16 trials on each stock-day, or 80 total trials per symbol. The agent performs well in the median case. Negative outlier days cause the mean performance to dip negative for certain stocks, but the average performance over all symbols is still positive at $10.91."}, {"title": "Indirect Interaction Backtest", "content": "An indirect interaction backtest is one in which two agents affect one another only through their order activity at the simulated exchange, at which historical order flow data is also being replayed. This experiment serves as the baseline case for the RL and sentiment agent being active in the same market, with the RL agent not configured to impact the social media feed. A total of 180 trials were conducted. Agent interaction, rather than learning generalization, is the focus of this experiment, so in-sample outcomes only are presented in Table 4. The addition of the sentiment agent to its environment increases the profit value of the TD3 agent's converged policy by approximately $14.00 (2.5%) aggregated across all symbols and days. The sentiment agent's performance is reduced by approximately $28.00 (250%, albeit of a small value). We take this as further confirmation that the TD3 agent is exploiting any available information in the interactive limit order book feed. The sentiment agent is likely unable to anticipate trades placed by the learning agent, because they are not reflected in its social media feed."}, {"title": "Direct Interaction Backtest", "content": "In the direct interaction backtest, the agents participate together in an interactive backtest as described in the previous section. However, the TD3 agent additionally uses its sentiment output to produce social media posts which are injected into the sentiment agent's feed. Thus, the TD3 agent can influence the sentiment agent's perceived state, if it is able to learn to do so. Again, a total of 180 trials were conducted. As with the previous experiment, the focus is on the agent interaction, so only in-sample results are presented in Table 5.\nThe TD3 agent successfully learns to control the sentiment of its posts to influence the sentiment agent to make trades in a direction beneficial to the TD3 agent. Compared with the"}, {"title": "Discussion", "content": "In this paper, we presented results in support of a new research question: can an LLM-enabled Deep RL agent improve its returns by learning to influence the social media feed observed by a sentiment agent? Importantly, no direction was ever given to the RL agent other than \u201cmaximize rewards\" and the agent did not know what its actions meant.\nWe first backtested the TD3-based reinforcement learning agent on five days of full-depth order flow. Its in-sample mean and median daily return were positive for all nine tested symbols, indicating learning. Out-of-sample results were mixed but positive in the mean across symbols. We thus claim the TD3 agent is able to find actionable information in the order flow.\nThen we backtested the sentiment-based agent, which receives realistic samples of social media posts generated by Llama 3.2. The samples are based on the same historical order flow used for simulation, and delivered to the sentiment agent at the appropriate time. The sentiment agent evaluated the posts using the RoBERTa sentiment classifier and computed a weighted sentiment score, which served as its trading indicator. The simple sentiment approach employed showed mixed performance, with some symbols' social feeds performing better than others. The overall strategy was slightly profitable in the mean. We thus claim the sentiment agent approach is sensible enough for further study.\nTwo experiments were conducted with both agents present in the market simultaneously. In the indirect interaction test, each agent trades on the same exchange, but there is no additional inter-agent influence. In the direct interaction test, the TD3 agent produces and controls social media posts"}, {"title": "", "content": "which are placed into the sentiment agent's feed. In the indirect interaction test, we found that the TD3 agent improved its performance by about 2.5% over the solo backtest. In the direct interaction test, it improved its performance by an additional 50% over the indirect interaction test. We thus claim that the TD3 agent is learning to use its social media action to influence market prices in a manner that increases its total rewards. The distribution of returns across both experiments is visualized in Figure 1 on the References page.\nThis project required pre-generation of over 500,000 social media posts from historical data, plus another 150,000 generated by the TD3 agent during interactive simulation. We note from casual inspection of these posts that while the current state of the art local models perform the requested task quite well, they show the expected lack of understanding of the underlying phenomena. Two illustrative examples are included:\nALERT! What's going on with all these AAPL SELL ORDERS? It looks like everyone is trying to ride the wave down, but I'm calling foul. Too many consecutive orders at nearly identical prices (223.58-64) - someone's trying to pump and dump!\nMeanwhile, these new orders are just asking for a bounce: 10,000 shares being unloaded in a matter of minutes? That's not a sell strategy, that's a desperation move.\nWith some irony, we observe that the learning agent which potentially executes pump-and-dump manipulation accuses other market actors of the same. It also hallucinates an important detail: ten thousand shares were not, in fact, sold near the time of this post. The second example:\nALERT! Just watched the order book for AAPL and I'm calling BS. Who else is seeing this? Multiple consecutive SELLs at $223.65, followed by huge chunks of selling at $223.67 & $223.72... This is a PER-"}, {"title": "", "content": "FECTLY formed BULL TRAP. These orders are being placed by traders who think they're covering up a price drop that's NOT happening. The order book is a joke. I'm shorting big time and waiting for the inevitable price bounce #AAPL #ShortSelling #Bull-Trap\nIt might be reasonable to suggest that concentrated selling activity is a bull trap, and that going short itself would be consistent with the agent's stated belief. However, to suggest that the agent is \"shorting big time\" while waiting for a price bounce is illogical."}, {"title": "Conclusion and Future Work", "content": "As part of a foray into a new area of study, these initial results suggest a wide range of potentially productive future work. Given sufficient computational resources, the simple sentiment agent could be replaced with another RL-based agent that learns when and how to best use social media sentiment data to make trades. This would have an equilibration benefit and strengthen the results, as the sentiment agent would then react to manipulation of its social feed.\nWith such an intelligent sentiment agent, successor studies could identify a threshold level of manipulation at which the social feed begins to be ignored. Since the manipulation of the social media feed, while undirected and inadvertent, is clearly non-normative and could run afoul of law or regulation, the application of normative reinforcement learning techniques to control the timing and content of output social posts could be investigated.\nOverall, our results point to a potential emerging problem at the intersection of finance, social media, and AI for good. Namely, that the careless addition of language models to other kinds of autonomous agents may have unforeseen and non-normative consequences that could cause inadvertent harm, and that further work is needed to determine whether and how autonomous agents can be safely augmented with control over a large language model."}]}