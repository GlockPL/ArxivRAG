{"title": "Unsupervised Anomaly Detection for Tabular Data Using Noise Evaluation", "authors": ["Wei Dai", "Kai Hwang", "Jicong Fan"], "abstract": "Unsupervised anomaly detection (UAD) plays an important role in modern data analytics and it is crucial to provide simple yet effective and guaranteed UAD algorithms for real applications. In this paper, we present a novel UAD method for tabular data by evaluating how much noise is in the data. Specifically, we propose to learn a deep neural network from the clean (normal) training dataset and a noisy dataset, where the latter is generated by adding highly diverse noises to the clean data. The neural network can learn a reliable decision boundary between normal data and anomalous data when the diversity of the generated noisy data is sufficiently high so that the hard abnormal samples lie in the noisy region. Importantly, we provide theoretical guarantees, proving that the proposed method can detect anomalous data successfully, although the method does not utilize any real anomalous data in the training stage. Extensive experiments through more than 60 benchmark datasets demonstrate the effectiveness of the proposed method in comparison to 12 baselines of UAD. Our method obtains a 92.27% AUC score and a 1.68 ranking score on average. Moreover, compared to the state-of-the-art UAD methods, our method is easier to implement.", "sections": [{"title": "Introduction", "content": "In the realm of data analysis, anomaly detection (AD) stands as a pivotal challenge with far-reaching implications across various domains, including cybersecurity (Siddiqui et al. 2019; Saeed et al. 2023), healthcare (Yang, Qi, and Zhou 2023; Abououf et al. 2023), finance (Hilal, Gadsden, and Yawney 2022), and industrial processes (Fan, Chow, and Qin 2022; Roth et al. 2022). Existing deep learning-based unsupervised AD methods often rely on an auxiliary learning objective such as auto-encoder, generative model, and contrastive learning. These methods indirectly detect anomalous data using other metrics such as reconstruction error, which lack generalizability and reliability guarantees (Hussain et al. 2023). Explicitly learning a one-class decision boundary may resolve this issue. Many well-known unsupervised AD methods assume the normal training data has a special structure in their data space or embedding space (Sch\u00f6lkopf et al. 2001; Tax and Duin 2004; Ruff et al. 2018; Goyal et al. 2020; Zhang et al. 2024; Xiao et al. 2025). Such assumptions may not hold or be guaranteed in practice and sometimes place a burden on the model training (Cai and Fan 2022; Fu, Zhang, and Fan 2024). For instance, in deep SVDD (Ruff et al. 2018), the optimal decision boundary in the embedding space may be very different from the learned hypersphere, leading to unsatisfactory detection performance (Zhang et al. 2024). Given that tabular data is probably the most common data type and other types of data such as images can be converted to tabular data using feature encoders or pretrained models, in this work, we focus on the tabular data only. We propose a novel unsupervised AD method for tabular data without making any assumption about the distribution of normal data. Since hard anomalies are often close to normal data, it is reasonable to hypothesize that hard anomalies are special cases of perturbed samples of normal data. Therefore, if the diversity of the perturbations or added noises on normal data is sufficiently high, we can obtain hard anomalies. Consequently, if a model can recognize highly diverse perturbations or noises, it can detect hard anomalies as well as easy anomalies successfully. By directly learning from the diverse noise patterns and the clean data patterns, we can learn an effective decision boundary around the normal data, generalizing well to unseen data.\nOur contributions are highlighted as follows.\n\u2022 We propose a novel AD method for tabular data using noise evaluation. Our scheme generates highly diverse noise-augmented instances for the normal samples. By evaluating the noise magnitude, our method can accurately identify anomalies.\n\u2022 The proposed method provides a simple yet effective scheme that does not make assumptions about the normal training data. In addition, the noise generation is straightforward without any extra training. Compared with (Wang et al. 2021; Goyal et al. 2020; Yan et al. 2021; Cai and Fan 2022), our method is more lightweight for training (requires less module for training.)\n\u2022 We theoretically prove the generalizability and reliability of the proposed method.\n\u2022 We conduct extensive empirical experiments on 47 real datasets in an unsupervised anomaly detection setting and 25 real-world tabular datasets in a one-class classification setting to demonstrate the performance of the proposed schemes. The results show that our method achieved superior performance compared with 12 baseline methods including the state-of-the-art."}, {"title": "Related Work", "content": "Unsupervised anomaly detection (UAD) is also known as the one-class classification (OCC) problem, in which all or most training samples are assumed to be normal. The learning objective is to learn a decision boundary that distinguishes whether a sample belongs to the same distribution of the normal training data or not. There is another similar problem setting known as outlier detection on contaminated dataset (Huang et al. 2024). The goal is to detect noised samples or outliers within the training data (Ding, Zhao, and Akoglu 2022). This line of work is orthogonal to our UAD problem. In the past decades, many UAD methods have been studied (Liu, Ting, and Zhou 2008; Chang et al. 2023). Traditional methods like proximity-based (Breunig et al. 2000; Angiulli and Pizzuti 2002; Papadimitriou et al. 2003; He, Xu, and Deng 2003), probabilty-based (Yang, Latecki, and Pokrajac 2009; Zong et al. 2018; Li et al. 2020), and one-class support vector machine (Sch\u00f6lkopf et al. 2001; Tax and Duin 2004) approaches struggle with high dimensionality and complex data structures. Deep neural network-based methods have been proposed to address these issues. For instance, auto-encoder methods identify outliers by detecting high reconstruction errors, as outlier samples do not conform to historical data patterns (Aggarwal 2016; Chen et al. 2017; Wang et al. 2021). Generative model methods compare latent features or generated samples to spot anomalies (Schlegl et al. 2017; Liu et al. 2019; Zhang et al. 2023; Tur et al. 2023; Xiao et al. 2025). For example, (Xiao et al. 2025) proposed an inverse generative adversarial network that converts the data distribution to a compact Gaussian distribution, based on which the density of test data can be calculated for anomaly detection. Contrastive learning (Sohn et al. 2020; Jin et al. 2021; Shenkar and Wolf 2022) leverages feature representation differences to detect anomalies. Unlike autoencoder-based methods that focus on reducing reconstruction error and reducing dimensionality to remove noise, our method aims to evaluate noise level, which is similar to the denoising diffusion model (Ho, Jain, and Abbeel 2020). Some works explicitly build an anomaly detection or OCC objective (Ruff et al. 2018; Goyal et al. 2020; Yan et al. 2021; Chen et al. 2022; Cai and Fan 2022). For instance, Deep SVDD (Ruff et al. 2018) trains a neural network to construct a hypersphere in the output space to enclose the normal training data. DROCC (Goyal et al. 2020) assumes normal samples lie on low-dimensional manifolds and treats identifying the ideal hypersphere as an adversarial optimization problem. PLAD (Cai and Fan 2022) outputs an anomaly score by learning a small perturbation of normal data as the negative sample with a classifier. Unlike PLAD, which uses extra additive and multiplicative perturbations requiring a perturbator, our method generates negative samples without extra parameters, making the training efficient.\nIt is worth mentioning that there are vision UAD methods utilizing synthetic anomalous data. However, the normality and abnormality can be visualized directly and there naturally exists prior knowledge about anomalies (e.g., visible spots or blurs) in visual data. Regarding tabular data, we do not have such prior knowledge. Vision AD methods require prior pre-trained models or external reference datasets to obtain the negative samples, such as DREAM (Zavrtanik, Kristan, and Sko\u010daj 2021) with the Describable Textures Dataset or Anomaly Diffusion (Hu et al. 2024) with a stable diffusion model. Such resources are costly and violate the principle of unsupervised learning. Tabular data, however, spans diverse domains (e.g., medical, industrial), making external datasets and pretrained model infeasible."}, {"title": "Proposed Method", "content": "Problem Formulation and Notations\nGiven a data set $X = \\{X_1,X_2, ..., X_N \\}$, where the element $x_i$ are drawn from an unknown distribution $D$ in $\\mathbb{R}^d$ (deemed as a normal data distribution). In the context of UAD, the primary objective is to develop a function $f : \\mathbb{R}^d \\rightarrow \\{0,1\\}$, which effectively discriminates between in-distribution (normal) and out-of-distribution (anomalous) instances. This discriminative function is formulated to assign a binary label, where $f(x) = 1$ indicates $x$ does not belong to $D$ and $f(x) = 0$ corresponds to $x$ coming from $D$. The main notations used in this paper are shown in Table 1."}, {"title": "Anomalous Data Decomposition", "content": "Let $X$ be the set consisting of all anomalous data drawn from some unknown distribution $D$ deemed as an anomalous distribution. For any $x \\in X$, we decompose it as\n$x = x + \\epsilon$,\nwhere $x \\in D$ is a normal counterpart of $x$ and $\\epsilon$ denotes the derivation of $x$ from $x$. The magnitude of $\\epsilon$, denoted as $|\\epsilon|_1$, measures how anomalous $x$ is. Note that this decomposition is not unique and hence one may seek the one with the smallest $|\\epsilon|_1$. If we can learn a model $h$ to predict $\\epsilon$ for $x$, i.e.,\n$\\epsilon = h(x)$,\nwe will be able to determine whether $x$ is normal or not according to $|\\epsilon|_1$. The challenge is that there is no available information about $X$ in the training stage and we can only utilize $X$. Although $X$ is unknown, we further theoretically partition $X$ into two subsets without overlap, i.e.,\n$X \\triangleq X_E \\cup X_H, X_E \\cap X_H = \\emptyset, X_E \\sim D_E, X_H \\sim \\tilde{D}_H$.\n$X_E$ denotes an easy set, drawn from the easy part $D_E$ of $D$, in which $|\\epsilon|_1$ for each sample is sufficiently large, while $X_H$ denotes a hard set, drawn from the hard part $D_H$ of $\\tilde{D}$, in which $|\\epsilon|_1$ for each sample is small. After the partition, we can assert that $X_H$ is closer to the normal data. Consequently, it is easier for a model to recognize samples in $X_E$ than those in $X_H$, as the $|\\epsilon|_1$ values of samples in $D_E$ are significantly larger than those in $D_H$. \nHere, we focus on how to detect the samples in $X_H$ or drawn from $D_H$. Since $X_H$ is very close to the normal data, it is reasonable to hypothesize that hard anomalies are special cases of perturbed samples of normal data. We propose to generate a noisy dataset $\\tilde{X} \\subset \\mathbb{R}^d$ from $X$ by adding various noise to $X$, and assume $\\tilde{X}$ is drawn from certain perturbed distribution $\\tilde{D}$, i.e.,\n$\\tilde{X} \\leftarrow Gen(X) \\sim \\tilde{D}$,\nwhere $Gen$ denotes the noisy data generator and $|\\tilde{X}| \\gg N$. Let the diversity of added noise is sufficiently large, such that\n$X_H \\subset \\tilde{X}$,\ni.e., anomaly patterns of the hard set $X_H$ are included in $\\tilde{X}$. Even if (5) does not hold, as shown by Theorem 1, it is still possible to obtain correct detection, provided that $\\tilde{D}$ is not too far from $D_H$, i.e.,\n$dist(\\tilde{D}, D_H) \\leq \\gamma$,\nwhere $dist(\\cdot,\\cdot)$ is some distance or divergence measure between two distributions and $\\gamma$ is not too large. Therefore, a model $h$ learned from $\\tilde{X}$ is able to generalize to $X_H$ and then detect anomaly."}, {"title": "Noise Evaluation Model", "content": "To generate $\\tilde{X}$, we add random noise to the elements of each sample $x \\in X$. This operation will reduce the quality of the data, that is, the quality of $\\tilde{X}$ is lower than that of $X$, supported by\nProposition 1. Adding random noises independently to the entries of $X$ makes the data more disordered (higher entropy).\nWe would like to learn a deep neural network $h(\\cdot) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ parameterized by $\\theta$ to quantify the quality of the input data. Its output is a vector with the same size as the input while each entry tells whether the input feature is noised or not. We generate the noised dataset by\n$\\tilde{X} = X_1 \\cup X \\cup \\hat{X}_K$,\nwhere each $\\hat{X}_k$ is composed of the samples generated by\n$\\hat{x} = x + \\epsilon, \\epsilon \\sim Noise_k(0, \\sigma_k), x \\in X$.\nIn (8), $Noise_k(0, \\sigma_k)$ (to be detailed in the next section) is a multivariate noise distribution with 0 mean value and $\\sigma_k$ standard deviation in $\\mathbb{R}^d$, abbreviated as $Noise(\\sigma_k)$. For instance, $Noise_k$ can be a Gaussian distribution. We also denote the standard deviation of the noise as noise level 1. According to (7) and (8), we see that $\\tilde{X}$ can contain different types of noise distributions with different standard deviations, and the diversity is controlled by $\\Sigma_k^1 |\\sigma_k|$. Refer to Table 3 for the choices of the noise generator. Then, we write the learning objective as\n$\\min_{\\theta} \\sum_{x_i \\in X} ||h_{\\theta}(x_i) - 0||_2 + \\sum_{ \\tilde{X}} ||h_{\\theta}(\\tilde{x}_i) - |\\epsilon_i|||_2$,\nwhere 0 is a d-dimension vector with all zero values, $\\epsilon$ is drawn from some $Noise(0, \\sigma)$, and $\\tilde{x} = x + \\epsilon$ for some\nWe use noise level and standard deviation of noise interchangeably in this paper."}, {"title": "Algorithm 1: Noise Generation", "content": "Input: maximum noise level $\\sigma_{max}$, number of noise distributions m, batch size b, the dimensionality of data d.\nmake m intervals\nInitialize $\\mathcal{E}$ with empty\nfor $j \\in \\{1, ..., b\\}$ do\nfor $i \\in \\{1, ..., m\\}$ do\n//random noise level\n$\\$\\sigma \\leftarrow Uniform(\\Delta[i], \\Delta[i + 1])$\n[d:d]$\\$\\leftarrow Noise(0,\\sigma)$\\//generate noise from m noise levels for m parts\nend for\n//shuffle position of noise elements\n$\\mathcal{E} \\leftarrow Shuffle(e)$\n$\\$\\mathcal{E}[j] \\leftarrow e$\nend for\nOutput: Generated noise $\\mathcal{E}$ for a batch of input data\nlearnable parameter is required. A comparative study on time cost is shown in Appendix I."}, {"title": "Theoretical Analysis", "content": "In the proposed method, we learn a one-class classification decision boundary closely around the normal samples. In Figure 1, we divide the anomaly region into two non-overlap distributions, easy $D_E$, and hard $D_H$, respectively. In this section, we theoretically show the anomaly detection ability of the model meeting easy anomaly samples $X_E$ from $D_E$ and hard anomaly samples $X_H$ from $D_H$ in Theorem 1 and Theorem 2, respectively. Without loss of generality, we let $h_\\theta \\in \\mathcal{H}$, where $\\mathcal{H}$ is the hypothesis space of ReLU-activated neural network with $L$ layers and the number of neurons at each layer is in the order of $p$. We give the following definition. Definition 1. For the noise evaluation hypothesis $h_\\theta$, the risk of the hypothesis on a distribution $D$ is defined by\nDefinition 2. (Based on Kifer et al., 2004 (Kifer, Ben-David, and Gehrke 2004)) Given a domain X with D and D' probability distributions over X, let H be a hypothesis class on X and denote by I(h) the set for which h \u2208 H is the characteristic function; that is, x \u2208 I(h) \u2194 h(x) = 1. The H-divergence between D and D' is\nProof of Theorem 1 \nBefore proving the theorem, we provide the following necessary lemma according to (Ben-David et al. 2010; Bartlett et al. 2019). \nLemma 1. Let H be a hypothesis space on $R^d$ with VC dimension dvc. h \u2208 H is a ReLu-activated deep neural network with L layers, and the number of neurons at each layer is in the order of p. If X and X' are samples of size N from two distributions D and D' respectively and $d_\\mathcal{H}(X, X')$ is the empirical $\\mathcal{H}$-divergence between samples, then for any \u03b4\u2208 (0, 1), with probability at least 1 \u2013 \u03b4,\nFollowing (Ben-David et al. 2010), we have an additional lemma.\nLemma 2. For any hypotheses h, h' \u2208 H,\nProof. By the definition of \u0394H-distance, \nThen, we have the proof. \nProof. Define"}, {"title": "Experimental Settings", "content": "Datasets We evaluate our method in two common settings: unsupervised anomaly detection and one-class classification. In the anomaly detection setting, where anomalous samples are few, we use 47 real-world tabular datasets from (Han et al. 2022), covering domains like healthcare, image processing, and finance. For one-class classification setting, we collected 25 benchmark tabular datasets used in previous works (Pang et al. 2021; Shenkar and Wolf 2022). The raw data was sourced from the UCI Machine Learning Repository (Kelly, Longjohn, and Nottingham) and their official websites. For categoric value, we use a one-hot encoding. We test on all classes in multi-class datasets, reporting the average performance score per class, which is similar to one-class classification on image dataset (Cai and Fan 2022). For datasets with validation/testing sets, we train on all normal samples. If only a training set is available, we randomly split 50% of normal samples for training and use the rest with anomalous data for testing. Data is standardized using the training set's mean and standard deviation. Baseline Methods We select 12 baseline methods for comparative analysis, including probabilistic-based, proximity-based, deep neural network-based, ensemble-based methods, and recent UAD methods that can be applied to tabular data. Implementation All experiments are implemented by Py-torch (Paszke et al. 2017) on NVIDIA Tesla V100 and Intel Xeon Gold 6200 platform. We utilize two network architectures for the evaluation, VanillaMLP (MLP) and ResMLP."}, {"title": "Unsupervised Anomaly Detection Results", "content": "The average result under the unsupervised anomaly detection setting of ten runs is reported in Figure 4. We conducted a comparative analysis of 11 tabular UAD methods across 47 datasets, evaluating average AUC, average F1, average ranking in AUC, and average ranking in F1. It is seen that our methods not only significantly outperform the AE methods, but also reach the highest ranking. The detailed results on each dataset and p-value from paired t-test are reported in Appendix F (Tables 6 and 7), which emphasizes the statistical significance of the improvements achieved by our methods."}, {"title": "One-Class Classification Results", "content": "We compared our noise evaluation method with 12 baseline methods on the OCC dataset setting. The results in Table 2 show that our anomaly detection techniques, Ours-ResMLP and Ours-MLP, consistently outperform baselines across various tabular datasets, with mean AUC values of 92.68\u00b111.10 and 92.27 \u00b1 11.1, indicating greater effectiveness and lower variability. While traditional methods like IForest and KNN perform well, our methods excel, especially on complex datasets like musk and optdigits. Though our methods may not always lead in AUC, the difference is minimal, around 1%. For faster inference, the MLP model is recommended. Compared with many popular UAD methods, our noise evaluation method has the following advantages:\n\u2022 Noise evaluation shows superior performance in the extensive empirical experiments, indicating our method can accurately identify anomalies in tabular data.\n\u2022 The generalization and anomaly detection ability of noise evaluation can be theoretically guaranteed, whereas many deep learning based methods lack such guarantees (Hussain et al. 2023).\n\u2022 The implementation of our method is easier. The perturbed sample generation can be pre-generated without extra training. In addition, no hyper-parameter is tuned in the learning objective."}, {"title": "Ablation Study", "content": "As discussed in the Proposed Method Section, we have several alternatives. In this section, we study how different noise levels, noise ratios, and noise types affect the performance of our method. We utilize the OCC setting to perform the ablation study. Detailed results are shown in Appendix H.\nSensitivity of Different Noise Levels To explore how different noise levels affect performance, we use Gaussian noise and generate 3 noised instances per training batch, consistent with previous settings. We test noise levels in [0.1, 0.2, 0.5, 0.8, 1.0, 2.0, 3.0, 5.0]. The results are reported in Figure 5. Results show that too small noise levels confuse the model due to the minimal distance between normal samples and anomalies, while too high noise levels expand the output value range and sampling space, reducing effectiveness as theorem 1 suggested. Hence, the optimal noise level is around 1.0.\nSensitivity of Different Noise Types We explore different noise types with a mean of 0 and a standard deviation (noise level) of \u03c3. We use Salt&Pepper noise, Gaussian, Laplace, Uniform, Rayleigh, Gamma, Poisson, and Bernoulli distributions. For Salt&Pepper and Bernoulli noise, a probability vector from a uniform distribution generates a binary vector, dictating feature value alterations by changing values or reversing signs. Other distributions are adjusted to have zero mean and o standard deviation. Qualitative Visualization We utilize t-Distributed Stochastic Neighbor Embedding (t-SNE) visualization (Van der"}, {"title": "Conclusion", "content": "In conclusion, we presented a novel noise evaluation-based method for unsupervised anomaly detection in tabular data. We assume the model can learn the anomalous pattern from the noised normal data. Predicting the magnitude of the noise shows inspiration on how much and where the abnormality is. We theoretically proved the generalizability and reliability of our method. Extensive experiments demonstrated that our approach outperforms other anomaly detection methods through 47 real tabular datasets in the UAD setting and 25 real tabular datasets in the OCC setting. An ablation study suggests that using Gaussian, Rayleigh, and Uniform noise has a stable performance. One potential limitation of this work is that we focused on tabular data only. Nevertheless, it is possible to extend our method to image data via the following two steps: 1) extract features of input images using a pretrained encoder; 2) apply our method to the extracted image features. This could be an interesting future work."}, {"title": "Appendix E: Neural Network Architecture", "content": "VanillaMLP\nThe VanillaMLP is a plain Relu-activated feed-forward neural network with 4 fully connected layers. ResMLP\nThe ResMLP is a Relu-activated feed-forward neural network with 5 residual blocks."}, {"title": "Appendix F: Detailed Anomaly Detection Results", "content": "The detailed results in terms of AUC score and F1 score under the UAD setting are reported here in Table 6 and 7. We do not include the results for AnoGAN baseline because it is not efficient to run."}, {"title": "Appendix G: F1 Score Results on OCC setting", "content": "The performance in terms of F1 score under the OCC setting is presented in Table 8. Our approach also obtains the highest mean F1 score and mean rank out of 12 baselines."}, {"title": "Appendix H: Ablation Experiment Results", "content": "Sensitivity of Different Noise Levels\nTo explore how different noise levels contribute to the performance, we keep the noise type as Gaussian noise. Sensitivity of Different Noise Ratios\nThe noise ratio means the percentage of noise appearing the feature in a sample. Sensitivity of Different Noise Type\nWe study utilizing different noise types to generate the noise. For a fair comparison, we sample all noise with 0 mean and o standard deviation (noise level)."}, {"title": "Appendix I: Time Cost Analysis", "content": "Time Cost for Noise Generation\nSuppose the batch size is b and the data dimensionality is d, the noise generation time complexity is O(bd) according to Algorithm 1.Time Cost for Training and Inference"}, {"title": "Appendix J: Number of Hyper-parameters", "content": "We compare the number of hyper-parameters with other recent deep learning based UAD methods. The comparison is listed in Table 11. Note that all hyper-parameters of our method are solely related to noise generation, and no hyper-parameter is involved in the learning objective except for the network training design (which exists in all deep learning based methods)."}, {"title": "Discussions", "content": "Interpretability For those deep learning-based methods that only output an anomaly score, it is hard to explain the forwarding non-linear process of the neural network black box. Extension to other data types Here, we provide insights for adapting noise evaluation to other data types like sequential, textual, and graph data. Adapting our approach to images, time series, text, and graphs is non-trivial but realizable, as each data type has distinct characteristics.\n\u2022 Image data Images exhibit local smoothness or piecewise linear patterns across pixel values, so directly adding random noise (e.g., Gaussian) to pixel values is suboptimal.\n\u2022 Time series Time series data, such as voice recordings, exhibit natural dependencies between adjacent samples, and anomalies may represent meaningful sequences.\n\u2022 Textual data Defining \"noise\" in textual data is non-trivial. A practical approach leverages recent large language models to extract latent features from text tokens.\n\u2022 Graph data Arbitrarily adding noise to the adjacency matrix is not feasible. \nFor all data types, instead of noise on raw data, we can apply diverse noise to the latent features extracted by some (pre-trained) feature extractors and then predict the noise amplitude. Effectiveness on Dataset with Multiple Dense Regions"}]}