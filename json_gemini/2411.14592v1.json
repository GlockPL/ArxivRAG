{"title": "G-RAG: Knowledge Expansion in Material Science", "authors": ["Radeen Mostafa", "Mirza Nihal Baig", "Mashaekh Tausif Ehsan", "Jakir Hasan"], "abstract": "In the field of Material Science, effective information retrieval systems are essential for facilitating research. Traditional Retrieval-Augmented Generation (RAG) approaches in Large Language Models (LLMs) often encounter challenges such as outdated information, hallucinations, limited interpretability due to context constraints, and inaccurate retrieval. To address these issues, Graph RAG integrates graph databases to enhance the retrieval process. Our proposed method processes Material Science documents by extracting key entities (referred to as MatIDs) from sentences, which are then utilized to query external Wikipedia knowledge bases (KBs) for additional relevant information. We implement an agent-based parsing technique to achieve a more detailed representation of the documents. Our improved version of Graph RAG called G-RAG further leverages a graph database to capture relationships between these entities, improving both retrieval accuracy and contextual understanding. This enhanced approach demonstrates significant improvements in performance for domains that require precise information retrieval, such as Material Science.", "sections": [{"title": "1 Introduction", "content": "LLMs exhibit impressive capabilities but encounter challenges such as hallucinations, outdated information, and untraceable, opaque reasoning. The RAG approach addresses these issues by combining the strengths of LLMs with the vast, continuously updated resources of external databases [1]. Graph-enhanced RAG methods build on this by leveraging rich semantic interconnections and relational data, enabling more precise entity linking, enhanced semantic context, and improved knowledge extraction [2, 3]. Additionally, researchers have introduced innovative graph-based context adaptation techniques that refine word embeddings to better capture semantic relationships, consistently outperforming traditional methods in various Natural Language Processing (NLP) tasks [4, 5]. Graph-based RAG provides a more nuanced and accurate representation of complex domains, enabling LLMs to generate responses with enhanced factual precision and contextual relevance [6]. This capability is especially valuable for domain-specific applications in fields such as material science and biomedicine, where accurate and detailed information is crucial [7, ?, 9]. Serving as a domain-specific knowledge server, the Semantic Context Enhancer extracts and delivers detailed descriptions of relevant concepts and entities, including their interrelationships, thereby equipping the LLM with a deeper semantic understanding [9]. Additionally, leveraging graph structures to improve knowledge retrieval and response generation, as exemplified by methods like AriGraph, has shown significant enhancements in decision-making and planning capabilities [10]. This study explores the improvement of information retrieval and knowledge generation in complex, specialized domains through the integration of the G-RAG pipeline, addressing limitations of existing approaches and advancing performance in targeted fields."}, {"title": "2 Methodology", "content": "The retrieval process of Naive RAG includes a diverse range of MatIDs, which ensures variety but can also introduce less relevant information. This issue can be mitigated through prompt engineering in the RAG configuration, allowing the LLM to continue generating accurate responses [11]. However, there are two main limitations to this approach. First, LLMs have a fixed context window, which restricts the number of tokens they can process simultaneously. This limitation hinders the model's ability to manage large volumes of retrieved data effectively [12], especially when the dataset is extensive and varied. Despite advancements like Google's Gemini, which uses a caching system to handle extended contexts, the fixed context window of LLMs remains a significant constraint [13, 14]. Although providing the model with more relevant information might seem beneficial, increasing the context length does not necessarily improve the accuracy of information retrieval or response generation [15]. This problem becomes even more pronounced when the retrieved context includes a mix of diverse but only marginally relevant data, potentially diluting the focus on the critical entities or concepts needed for an accurate response\u00b2. This is where Graph RAG proves to be valuable, as it enhances the retrieval process by focusing on the most relevant information."}, {"title": "2.1 Graph RAG vs G-RAG", "content": "Graph RAG effectively merges the strengths of retrieval-based and generative methods to enhance LLMs' capability to generate accurate, relevant, and contextually enriched responses [16]. While supplying an LLM with text chunks from extensive documents may result in issues with context, factual precision, and language coherence, Graph RAG addresses these limitations by utilizing a knowledge graph as a source of structured, factual information [5]. The knowledge graph provides detailed entity information, including attributes and relationships, allowing the LLM to gain a deeper understanding and produce more informed, precise responses. In our G-RAG system, entity linking is a fundamental component, enabling the extraction of specific entities (key terms or concepts) from the text using an entity extractor like a Span Parser. These identified entities are then used to query an external retriever, which fetches relevant MatIDs and their corresponding information from a Wikipedia knowledge base\u00b3. This targeted retrieval process ensures that the selected MatIDs are highly relevant and accurate, thereby preserving the integrity of the constructed knowledge graph\u2074. Following this, an LLM formulates a query that is sent to the graph database. The graph database retrieves relevant information, which is processed by the LLM to generate a final, comprehensive response. The complete architecture of our G-RAG system is illustrated in Figure 1."}, {"title": "2.2 PDF Parsing", "content": "We parse PDFs by categorizing their content into text, figures, and tables. For figure extraction, we employ the Phi-3.5 Vision Instruct model, specifically tailored to identify material science-related images using a vision agent system. We utilize Microsoft's Table Transformer in the tabular data extraction process. Furthermore, we apply a smart chunking technique to enhance the precision of data segmentation. Accurate parsing is essential for subsequent tasks such as Entity Linking, Relation Extraction, and Graph Retrieval Augmented Generation, as it ensures the accuracy and relevance of the answers retrieved from the database. Appendix A.1 provides a detailed overview of our document parsing process."}, {"title": "2.3 Entity Linking and Relation Extraction", "content": "Entity Linking (EL) refers to the process of mapping ambiguous mentions in a text to specific, identifiable named entities within a knowledge base [17]. It involves recognizing all potential entities mentioned in the given input and accurately associating them with corresponding entries in a reference knowledge base, such as Wikipedia. Relation Extraction (RE) refers to the process of identifying and classifying semantic relationships between entities mentioned within a given text. This task involves mapping the detected entities to specific relation categories defined in a reference knowledge base, such as Wikipedia. The entity linking and relation extraction process is depicted in Appendix A.2."}, {"title": "2.4 Span Parser", "content": "The Span Parser module functions as our G-RAG system's initial information retrieval component, employing an approach inspired by the Retrieval Process [18]. This module operates on the principle of semantic similarity between the current knowledge base (KB) and a comprehensive collection of textual passages (Wikipedia Database) representing entities and relations. At its core, the Span Parsing module utilizes an encoder to generate dense vector representations of both the knowledge base (KB) q and each passage p in the additional knowledge base collection. These representations, denoted as $E(q)$ and $E(p)$ respectively, are high-dimensional embedding that capture the semantic content of the text. The module computes a similarity score between the current Knowledge Base and additional Knowledge Base (Wikipedia data) using a dot product operation, yielding the most relevant relations with respect to the extended knowledge base q:\n$sim(q,p) = E(q). E(p)$\nThis score quantifies the relevance of each passage of the additional knowledge base to the given cur- rent KB passage's sentence, enabling the module to rank and retrieve the most pertinent information."}, {"title": "2.5 Passage Processor", "content": "The Passage Processor (PP) component in our G-RAG system employs a unified approach to process the existing knowledge base and retrieved passages. Given a current Knowledge Base (KB) Q and a set of N retrieved passages {$P_1,..., P_n$}, the Passage Processor constructs chunks of current KB. In each chunk, we utilize each input sequence $S = [Q; T_0; P_1; T_1; . . . ; P_n; T_n]$, where $t_i$ are delimiter tokens. This sequence is encoded using a Transformer model T, producing contextual embedding $E = T(S)$. The Passage Processor subsequently identifies relevant spans within Q through a two-stage process [18]. Initially, it computes start probabilities $P_s (q_i)$ for each token $q_i$ in Q using a learned function $f^{\\theta} (E)$. Subsequently, for each potential start position s, it calculates end probabilities $P_e(q_j | s)$ for tokens $q_j$ (where $j > s$) using another learned function $f^{\\epsilon}(E, s)$. This formulation enables the prediction of overlapping spans, enhancing the model's capability to handle complex queries. During the process, spans (s, e) are predicted if $P_s(q_s) > \\theta_s$ and $P_e(q_e | s) > \\theta_e$, where $\\theta_s$ and $\\theta_e$ are predefined thresholds. This design enables the Passage Processor to process the entire knowledge base chunk by chunk efficiently, identifying relevant text spans for downstream tasks such as entity linking and relation extraction."}, {"title": "3 Experimental Settings", "content": "Our dataset consists of ten carefully designed handwritten queries, aimed at evaluating and differen- tiating the capabilities of various RAG systems. Sample queries from this dataset are presented in Appendix A.3. To evaluate the performance of RAG systems, we employ various metrics, including correctness, faithfulness, context, and answer relevancy scores. Correctness assesses the accuracy of the generated response, while faithfulness evaluates the factual accuracy based on the retrieved documents. Finally, the context and answer relevancy score measures how well the response aligns with the given query. A detailed description of these evaluation metrics is provided in Appendix A.4. For entity linking and relation extraction, we use the relik-entity-linking-large model, while the jina-embeddings-v2-base-en model, with a sequence length of 8192, is employed for embeddings. Additionally, we utilize LLama 3.1 8B and LLama 3.1 70B as large language models, both of which produce comparable results."}, {"title": "4 Results and Discussion", "content": "This section presents all of our experimental results. We conducted the computational tasks using the NVIDIA Tesla A100 Ampere 40 GB GPU. The performance of the Naive RAG, Graph RAG, and the G-RAG system was evaluated using our dataset. Appendix A.5 provides example queries and the corresponding responses from the RAG systems, evaluated across different metrics. The experimental results are summarized in Table 1.\nThe comparative analysis of three RAG pipelines - Vector/Naive RAG, G-RAG, and Graph RAG - showed interesting patterns in their performance across three critical dimensions. A one-way Analysis of Variance (ANOVA) as described in Appendix A.4.5 was performed, examining correctness F(2,24) = 2.39, p = 0.113, faithfulness F(2,27) = 1.04, p = 0.368, and context and answer relevancy F(2, 27) = 1.04, p = 0.368. While no statistically significant differences were found at the standard significance level (a = 0.05), the descriptive statistics highlighted meaningful variations in performance. Specifically, Vector/Naive RAG outperformed the others in terms of context relevancy, with a mean score of 0.3875. This was followed by G-RAG (mean score of 0.3375), while Graph RAG exhibited the lowest mean score of 0.1750. The substantial standard deviations observed across all pipelines, ranging from 0.2630 to 0.3162, suggest notable performance variability depending on the query. This variability highlights the challenge of consistency in RAG systems. The superior performance of G-RAG over the basic Graph RAG can be attributed to the inclusion of a material science knowledge base, emphasizing the critical role of domain-specific knowledge in enhancing model accuracy. The superior context relevancy performance of the traditional Vector/Naive RAG challenges the assumption that graph-based approaches inherently provide better retrieval capabilities. G-RAG has proven to be a well-rounded solution, effectively balancing the metrics of correctness, relevancy, and faithfulness. The significant drop in relevancy scores for Graph RAG highlights the critical role of entity linking in G-RAG's design. This suggests that the effectiveness of knowledge integration mechanisms, including entity linking, plays a substantial role in improving retrieval performance. These findings indicate that while graph-based approaches show promise, their success heavily depends on the quality of knowledge integration and the sophistication of the entity-linking."}, {"title": "5 Conclusion and Future Work", "content": "Our findings indicate that integrating graph-based techniques and ensuring robust entity linking with external databases can significantly enhance the performance of the Graph RAG pipeline, particularly in terms of response relevance and accuracy. This approach also mitigates the challenge of maintaining relevance observed in standard Graph RAG implementations. Future work could include developing a larger knowledge base tailored to material science as an extended information source, as well as creating a material science-specific entity linking model. Additionally, establishing a comprehensive evaluation metric for Graph RAG would provide deeper insights into the process and its effectiveness. The code will be available at https://github.com/RadeenXALNW/G-RAG_1.0."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Documents Parsing Method", "content": "This section illustrates our document parsing pipeline, as shown in Figures 2 and 3. Efficient document parsing is crucial for enabling RAG systems to generate responses with high factual accuracy and precision."}, {"title": "A.2 Entity Linking and Relation Extraction", "content": "In this section, we provide a visual representation of the entity linking and relation extraction process, as depicted in Figures 4, 5, 6, and 7. These processes are essential components of our G-RAG system.\nCoreference Resolution: Coreference resolution, mentioned in Figure 4 involves identifying different expressions in a text that refer to the same entity. This process is crucial for understanding the relationships between various mentions of an entity within a given context."}, {"title": "A.4 LLM RAG Evaluation Metrics", "content": "This section provides detailed descriptions of the various evaluation metrics used for RAG systems."}, {"title": "A.4.1 Correctness", "content": "Given a query q, a generated answer g, and an optional reference answer r, the CorrectnessEvaluator computes a score s using an LLM. This score is then compared against a threshold T to determine whether the generated answer is correct or passing.\nPrompt Constructed from q, g, r\n$E(g,q,r)$ LLM Response to Prompt\n$(s, reasoning)$ parser_function($E(g,q,r)$)\npassing $s\\geq T$\nEvaluationResult {q, g, passing, s, reasoning}"}, {"title": "A.4.2 Faithfulness Evaluation", "content": "Given a query q, a response r, and a set of context documents C, the FaithfulnessEvaluator performs the following steps:\nTransform C into Document objects\nCreate SummaryIndex from Document objects\nCreate query engine using LLM, eval_template, and refine_template\nPerform a query on the response using the query engine\nObtain raw_response_txt from the query engine\n$\\{\n        \\begin{array}{ll}\n            True & \\text{if yes is found in raw_response_txt} \\\\\n            False & \\text{otherwise}\n        \\end{array}\n\\$\n$\\{\n        \\begin{array}{ll}\n            1.0 & \\text{if passing is True} \\\\\n            0.0 & \\text{otherwise}\n        \\end{array}\n$\nraw_response_txt\nEvaluationResult\n{q, r, C, passing, score, feedback}"}, {"title": "A.4.3 Answer Relevancy", "content": "Let q be the query, r the response, and {$C_1, C_2,..., C_n$} the contexts. Define the following:\n{$d_i | d_i = Document(text = c_i)$}\nSummaryIndex(Documents)\nQuestion: q Response: r\nEvaluate the query-response pair with:\nresponse_obj = QueryEngine(Index).aquery(query_response)\nraw_response_txt = str(response_obj)\n$\\{\n        \\begin{array}{ll}\n            True & \\text{if \"yes\" is in raw_response_txt.lower()} \\\\\n            False & \\text{otherwise}\n        \\end{array}\n\\$"}, {"title": "A.4.4 Context Relevancy", "content": "Let q be the query, {$C_1, C_2, ..., C_n$ } be the contexts. Define:\n{di | di = Document(text = ci)}\nSummaryIndex(Documents)\nquery_engine = Index.as_query_engine(llm, eval_template, refine_template)\nraw_response_txt = str(response_obj)\nscore, reasoning = parser_function(raw_response_txt)\n4.0\nscore = score / score_threshold\nEvaluationResult = {q, {$C_1,..., C_n$ }, score, feedback = raw_response_txt, invalid_result, invalid_reason}"}, {"title": "A.4.5 Analysis of Variance (ANOVA)", "content": "ANOVA is a fundamental statistical method used to compare means across multiple groups to determine if there are statistically significant differences between them. This study utilizes a one- way ANOVA, which examines the effect of a single independent variable - in this case, the type of RAG pipeline - on a dependent variable (performance metrics). The mean score reflects the average performance of each method across all 10 queries, offering an overall assessment of its effectiveness for the given metrics. A mean score closer to the highest possible value suggests that the method consistently delivers superior results, indicating strong performance across various queries. Conversely, a lower mean score points to weaker overall performance, highlighting areas where the method may be less effective. Essentially, the mean score serves as a summary indicator of each method's typical efficacy, providing a clear comparison of their relative strengths in achieving the desired outcomes.\nThe F-statistic in ANOVA quantifies the ratio of variance between groups to variance within groups, with larger F-values indicating greater differences among the groups. The degrees of freedom (df) are denoted as F(2, 24) for correctness and F(2, 27) for faithfulness and relevancy, indicating the number of independent values that can vary in the calculation. Here, the first value (2) represents the degrees of freedom for the groups (number of groups minus 1), and the second value (24/27) represents the degrees of freedom for the error term (total observations minus the number of groups). The p-value assesses the probability of observing such differences between groups. Typically, a p-value below the significance level (a = 0.05) suggests statistically significant differences between the groups."}, {"title": "A.5 Examples of Different Metrics", "content": "This section presents three example queries along with their corresponding responses from the RAG systems. Providing optimal responses to these queries requires effective information retrieval from text, figures, and tables. Additionally, we illustrate the application of various RAG evaluation metrics through these examples. Table 3 compares the performance using high-entropy alloy documents, demonstrating that G-RAG delivers contextually more relevant responses. The query in Table 4 necessitates accurate information extraction from Figure 8. The responses show that G-RAG with the parsing method outperforms other RAG systems, providing results that are closely aligned with the ground truth values. Finally, the query in Table 5 requires accurate value extraction from the table presented in Figure 9. In this case, G-RAG with the parsing method also outperforms other RAG systems. These results demonstrate that the integration of an efficient parsing method significantly enhances the performance of the G-RAG system."}]}