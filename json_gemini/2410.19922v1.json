{"title": "Disentangling Genotype and Environment Specific Latent Features for Improved Trait Prediction using a Compositional Autoencoder", "authors": ["Anirudha Powadi", "Talukder Zaki Jubery", "Michael C. Tross", "James C. Schnable", "Baskar Ganapathysubramanian"], "abstract": "This study introduces a compositional autoencoder (CAE) framework designed to disentangle the complex interplay between genotypic and environmental factors in high-dimensional phenotype data to improve trait prediction in plant breeding and genetics programs. Traditional predictive methods, which use compact representations of high-dimensional data through handcrafted features or latent features like PCA or more recently autoencoders, do not separate genotype-specific and environment-specific factors. We hypothesize that disentangling these features into genotype-specific and environment-specific components can enhance predictive models. To test this, we developed a compositional autoencoder (CAE) that decomposes high-dimensional data into distinct genotype-specific and environment-specific latent features.\nOur CAE framework employs a hierarchical architecture within an autoencoder to effectively separate these entangled latent features. Applied to a maize diversity panel dataset, the CAE demonstrates superior modeling of environmental influences and 5-10 times improved predictive performance for key traits like Days to Pollen and Yield, compared to the traditional methods, including standard autoencoders, PCA with regression, and Partial Least Squares Regression (PLSR). By disentangling latent features, the CAE provides powerful tool for precision breeding and genetic research. This work significantly enhances trait prediction models, advancing agricultural and biological sciences.", "sections": [{"title": "1 INTRODUCTION", "content": null}, {"title": "1.1 Background and Overview", "content": "Advances in imaging and robotic technologies are making both high-resolution images and sensor data increasingly accessible to plant biologists and breeders as tools to capture measurements of plant traits. These data types can be used to measure or predict traits which are labor intensive or costly to measure directly including variation in plant architectural and biochemical traits as well as resistance or susceptibility to specific biotic stresses. A growing body of evidence suggests high dimensional trait datasets can also be useful to predict crop productivity (e.g. grain yield) (Adak et al., 2023; Jin et al., 2024). However, like the plant traits plant biologists and breeders seek to predict, sensor data and the high dimensional traits extracted from that data reflect the impact of both genetic and environmental factors. Traditionally, the data are used in raw form or through handcrafted and engineered features without explicitly decomposing the genotype (G) and environment (E) factors. The very high dimensionality of sensor data often makes hand-crafting features a very non-trivial and challenging task. Under this 'curse of dimensionality', hand-crafted and engineered features can fail to capture the full variability within the dataset, potentially limiting accuracy and/or impairing the ability of trained models to translate to new environments or new sets of plant genotypes. Latent features derived from these data are better than hand-crafted or engineered features because they can capture the underlying patterns and variability in the dataset without being biased by human assumptions or limited by predefined feature sets (Feldmann et al., 2021; Aguate et al., 2017). This can lead to more accurate and generalizable models that can better predict complex traits and their interactions.\nAlthough sensors and images capture a great deal of information about a target plant or target plants, not all of that information is likely to be useful. Latent phenotyping is an emerging approach to plant phenotyping that seeks to minimize human bias in defining differences between plants by reducing the dimensionality of the data via unsupervised or self-supervised approaches (Gage et al., 2019; Ubbens et al., 2020; Feldmann et al., 2021; Tross et al., 2023). Traditionally, machine learning methods like PCA (Principal component analysis), Linear Discriminant Analysis (LDA), T-distributed Stochastic Neighbor Embedding (t-SNE), and autoencoders have been used to extract the \u2018latent representation' from high- dimensional data (Alexander et al., 2022; Zhong et al., 2016; Kopf and Claassen, 2021; Song et al., 2023; Gomari et al., 2022; Iwasaki et al., 2023). A specific advantage of autoencoders, relative to many of the other methods employed to extract latent representations from sensor or image data is their ability to capture non-linear relationships within data as well as being extremely configurable. An autoencoder has two parts. The first part works to produce a compressed but highly informative latent space, while the second part works to rebuild the original input from this latent space. This process enables the autoencoder to effectively learn a compact, yet robust, representation of the input data, capturing essential features and patterns in a lower-dimensional latent space, which is crucial for accurate data reconstruction.\nDimensionality reduction techniques endeavor to produce a compressed but highly informative \u2018latent representation' from the input data. This latent representation is often used to predict or classify some quantity of interest related to the input data. This representation although very informative, is not categorized. This can be explained by a simple example of generating a latent space of an image of a corn plant. Let us say that our goal is for the latent space to effectively capture and quantify various external attributes such as \u2018number of leaves', 'plant height', \u2018chlorophyll concentration (greenness)', and 'position of the corn cob'. However, a typical challenge arises in this process: the elements within the latent space tend to be intermixed or \u2018entangled.' This means that instead of having distinct areas in the latent"}, {"title": "1.2 Related Work and Contributions", "content": "There are several ways to achieve disentanglement, but disentanglement often comes at the cost of reconstruction accuracy. One approach is via regularization techniques, where additional terms are added to the loss function (of, say, vanilla and variational autoencoders, VAE) (Kingma and Welling, 2019) to encourage independence among latent variables. Examples include \u03b2-VAE (Higgins et al., 2017), which balances reconstruction fidelity and disentanglement. FactorVAEs (Kim and Mnih, 2019) impose a total correlation penalty on the latent variables to promote their independence and disentanglement. Mutual information-based approaches, like InfoGAN and StyleGAN, maximize the mutual information between latent variables and generated outputs to ensure distinct factors of variation are captured. Additionally, supervised or semi-supervised approaches can be employed, leveraging labeled data to guide the learning of disentangled representations (Kulkarni et al., 2015; Kingma et al., 2014; Kingma and Welling, 2022). Latent feature disentangling has been successfully applied to a diverse array of applications including music (Banar et al., 2023), text understanding (Wang et al., 2022), facial image generation (Karras et al., 2019), protein structure variations (Tatro et al., 2021).\nDisentanglement can be broadly classified into two types: hierarchical disentanglement and latent space disentanglement. Hierarchical disentanglement organizes the latent space into multiple levels, where higher levels capture abstract and global features, and lower levels capture specific and local details. On the other hand, latent space disentanglement ensures that each dimension in the latent space corresponds to a distinct and independent factor of variation, promoting independence among latent variables. Both approaches aim to create more interpretable and modular representations of data, each with a different focus on the structure and independence of the latent features. Approaches like \u03b2-Variational autoencoder (Burgess et al., 2018; Higgins et al., 2017) and its variants (Guerrero-L\u00f3pez et al., 2022; Zheng and Sun, 2019; Watters et al., 2019; Cha and Thiyagalingam, 2023) perform latent space disentanglement \u2013 disentangling features that control size, position and orientation of an input image. Similarly, StyleGAN and its variations (Liu et al., 2022; Niu et al., 2023; Wei et al., 2023) start directly with a latent vector picked from a Gaussian distribution, and associate specific features of the image with individual components of the latent vector. Hierarchical disentanglement has been richly explored on applications involving speech (Sun et al., 2020), video sequences (Comas et al., 2021), multi-modal temporal data (Chen and Zhang, 2023). These approaches use concepts of attention (Cui et al., 2024), context addition (Li et al., 2021), graph convolution (Bai et al., 2022), or contrastive learning (Xie et al., 2023). Although these papers have shown exciting results, a limitation is the inability to simultaneously utilize multiple sources of data to perform hierarchical disentanglement. This issue is circumvented in approaches like Orthogonal denoising autoencoder (Ye et al., 2016) and Factorized latent space paper (Jia et al., 2010) that disentangle while learning the features presented by side views of an object. Another approach (Sun et al., 2019) works on disentanglement via enforcing a correlation loss that penalizes correlation between identity and facial expression representation. These two concepts were leveraged in our work along with the concept of latent disentanglement."}, {"title": null, "content": "High-dimensional sensor data have been recently used for plant phenotyping due to their capability to quantify plant features in an unbiased fashion while scaling to larger experiments. Autoencoders have been used to extract a concise representation from this high-dimensional data while also filtering out noise (Gage et al., 2019; Ubbens et al., 2020; Tross et al., 2023). This extracted representation comes with certain drawbacks. First, it is not interpretable, i.e, we do not know what information is exactly embedded into it. Second, it does not have any structure or hierarchy to it. Finally, it does not encode different factors of variation in the data separately. To tackle these problems, we employ a combination of hierarchical and latent disentanglement strategies."}, {"title": "2 MATERIALS AND METHODS", "content": null}, {"title": "2.1 Equipment and Dataset", "content": "Hyperspectral data is being increasingly adopted by plant scientists as a method to measure or predict plant traits in field and greenhouse settings (Kaleita et al., 2006; Zhang et al., 2023; Yendrek et al., 2016; Tross et al., 2023). For the purposes of this study, we employed data from 578 inbreds, which represent a subset of the Wisconsin Diversity panel (Mazaheri et al., 2019), grown and phenotyped in 2020 and 2021 at the Havelock Farm research facility at the University of Nebraska-Lincoln. In each year, measurements were collected on two replicated plots of each inbred grown in different parts of the field, for a total 2 \u00d7 2 \u00d7 578 = 2312 observed plots. Each plot consisted of two rows of genetically identical plants with approximately 20 plants per row, as previously described in Mural et al. (2022). Hyperspectral data was collected using FieldSpec4 spectroradiometers (Malvern Panalytical Ltd., Formerly Analytical Spectral Devices) with a contact probe. This equipment captures 2151 wavelengths of electromagnetic radiation ranging from 350 nm to 2500 nm. Hyperspectral data was collected from a single fully expanded leaf per plot, selected from a representative plant, avoiding edge plants whenever possible. Three spectral measurements were taken at each of the three points located at the tip, middle, and base of the adaxial side of each leaf. Values were averaged across the nine wavelength scans to generate a final composite spectrum for each plot sampled (Tross et al., 2023). Figure 4 illustrates the distribution and variability of mean reflectance among the genotypes across two years, which in this paper are referred to as two different environments. We divide the environment into field-level (or macro-environment) and plot-level (or micro-environment) Guil et al. (2009). For the latent features extraction, the data was then normalized using min-max normalization. This normalization is given as:\n$X_{normalized} = \\frac{x - min_{dataset}}{max_{dataset} - min_{dataset}}$\nFrom the equation 1, 'mindataset' and 'maxdataset' are the minimum and maximum values in the entire dataset respectively."}, {"title": "2.2 Vanilla Autoencoder", "content": "We implemented a standard autoencoder (see Figure 5) as a baseline for comparison which we refer to below as the 'vanilla autoencoder' (AE). Both the encoder and decoder portions of our vanilla autoencoder implementation are made up of multiple fully connected layers stacked together with the non-linear activation function \u2018SeLu.\u201d The encoder encodes the input data (2151 wavelengths) into smaller dimensions (latent space) and decoder works to reconstruct back the original input from this latent space. The tables 1 and 2 show the details of each of the layers that constitute the encoder and decoder. For training the vanilla autoencoder, data from each plot in each year is considered as one sample, resulting in a total of 2312 input samples."}, {"title": "2.3 Compositional Autoencoder", "content": null}, {"title": "2.3.1 Architecture", "content": "The compositional autoencoder extends the vanilla autoencoder architecture in a way that aims to disentangle the latent space, partitioning the impact of different factors that influence the data into different variables. It consists of an encoder, decoder, and a fusion block. The network operates as follows:\n1. Encode Individual Plant Data: The encoder processes data from four plants of the same genotype, compressing it into latent features.\n2. Fuse Encoded Data: These encoded representations from all the plants are then fused into a single latent feature.\n3. Disentangle Latent Factors: This fused latent feature is then partitioned into three distinct parts: genotype-specific features (common across all plants), macro-environment-specific features (shared by plants from the same environment), and micro-environment-specific features (unique to each plant).\n4. Reconstruct Individual Plants: Finally, for each plant, the genotype, macro-environment, and micro- environment features are assembled. This assembled disentangled representation is then decoded to reconstruct the original plant data."}, {"title": "2.3.2 Loss Function", "content": "We trained the CAE network using a two-part loss function consisting of a reconstruction loss and a correlation loss.\nReconstruction Loss: The mean squared error (MSE), was used as the reconstruction loss for the compositional autoencoder. This loss function encourages the network to learn a meaningful disentangled latent space that can be accurately decoded back to the original hyperspectral data.\nCorrelation Loss: A correlation loss was employed to ensure that all parts in the disentangled latent space remain uncorrelated throughout the training process. This loss is defined in Equation (2).\n$Correlation\\ Loss = \\sum_{i=1}^{N} \\sum_{j=i}^{N} |CorrMat_{ij} - I_{ij}|$\nwhere:\n\u2022 CorrMatij represents the correlation coefficient between dimensions i and j in the latent space.\n\u2022 N is the dimension of the square correlation matrix, which corresponds to the number of dimensions in the latent space.\n\u2022 Iij is the identity matrix, ensuring that the diagonal elements (where i = j) contribute zero to the loss.\nThe correlation coefficient used here is the Pearson correlation coefficient (r), a measure of the linear correlation between two variables. It is calculated using Equation (3).\n$r= \\frac{\\sum_{i=1}^{n}(p_{i} - \\bar{p}) (k_{i} - \\bar{k})}{\\sqrt{\\sum_{i=1}^{n}(p_{i}-\\bar{p})^{2} \\sum_{i=1}^{n}(k_{i}-\\bar{k})^{2}}}$\nwhere:\n\u2022 n is the number of data points.\n\u2022 pi and ki are the elements of the latent space.\n\u2022 p and kare the means of the pth dimension and kth dimension, respectively."}, {"title": "2.3.3 Training Parameters", "content": "The data was divided into training and validation with a 85%-15% split. Furthermore, we trained these networks with SGD, Adam, and LBFGS optimizers and found that LBFGS gave us faster convergence (10x). Therefore, all the experiments were carried out using the LBFGS optimizer. The training setup included early stopping criteria, which monitored validation loss and stopped training after it observed no improvements in the metric for 15 epochs."}, {"title": "2.3.4 Parameter Tuning for Downstream Tasks", "content": "To improve the performance of latent representations for downstream tasks, we investigated several tuning techniques for both the network and its inputs.\n\u2022 a) We explored masking a portion of the input data. This technique encourages the model to focus on reconstructing the missing parts, potentially leading to increased robustness and reduced overfitting (Bachmann et al., 2022). We performed a search for the optimal masking percentage.\n\u2022 b) Considering our dataset size, we conducted a basic architecture search to strike a balance between model complexity and data availability. This helps to mitigate overfitting and improve generalization. We evaluated different network architectures with varying numbers of layers and dimensions in the encoder and decoder.\n\u2022 c) To ensure the latent representations captured the necessary data complexity, we experimented with different latent space dimensions and their composition of genotype, field-level, and plant-level environmental features."}, {"title": "2.4 Downstream Tasks Performance Metrics", "content": "To confirm our hypothesis that the disentangled latent representations enhance the latent feature's ability to predict useful traits, we generated disentangled latent features (disentangled encoded output from the encoder) for all 2312 data points. We then used these features to train models to predict two traits, namely, 'Days to Pollen' and 'Yield (grams)'. We trained several regression models Random Forests, XGBoost, Ridge Regressions, and PLSR (Partial-Least Square Regression) to identify a high performing model. We compare the performance of the models trained on the disentangled latent representations from the CAE against the performance of models trained on the latent representations from a vanilla autoencder. The resulting prediction performance was evaluated using an R2 metric representing the coefficient of determination.The coefficient of determination, R2, is defined as:\n$R^2 = 1-\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$\nwhere:\n\u2022 yi is the observed value,\n\u2022 \u0177 is the predicted value, and\n\u2022 y is the mean of the observed data."}, {"title": "3 RESULTS AND DISCUSSION", "content": null}, {"title": "3.1 Disentangled Representation from CAE", "content": "The compositional autoencoder (CAE) successfully disentangled the latent space into genotype, macro- and micro- environmental effects. The figure 7 shows a comparison of the original reflectance versus factor- specific (genotype and environments) reflectance. Here, factor-specific reflectance is obtained by modifying the latent space to only keep the effects of either the genotype, or the environments; and subsequently reconstructing the reflectance from them. Therefore, genotype-specific is obtained by replacing the environment components in the latent space with an average of all the environments, and similarly, genotype components are replaced by their average to reconstruct the environment-specific reflectance. Figure 7b shows genotype-specific reflectance. As we are focusing on just 1 genotype in this figure, all the replicates will have the same latent space and therefore, the same reflectance. Figure 7c shows macro environment- specific reflectance. The distinction between the two macro-environments is visualized by calculating the difference between macro-environment-specific reflectance and genotype-specific reflectance for the two macro-environments. Similarly, figure 7d shows micro-environment-specific reflectance. The visualization shows the difference between genotype-specific reflectance, macro-environment-specific reflectance, and micro-environment-specific reflectance.\nTo further verify the degree of environment disentanglement, we calculated the distribution of the two macro environments for the original reflectance (Figure 8a) and disentangled environments' reflectance (Figure 8b). A successful disentanglement should yield completely separated distributions. We use KL- divergence to measure the difference between the distributions. We can clearly see that KL-divergence of distributions representing two environments generated from the sensor data is quite low (0.62) while the same for the disentangled reflectance is quite large (2.79). This strongly indicates that the latent representation is, in fact, able to represent the two environments distinctly."}, {"title": "3.2 Performance of Latent Representations on Downstream tasks", "content": "We first report on the performance of our baseline model the vanilla autoencoder. The latent representation from the vanilla AE was used to train a multiple machine learning models to predict the two traits. We present the Ridge regression model performance here as it yielded the best results among all the models (Random Forests, PLSR, and XgBoost). Figure 9 shows this performance. We see that the performance for both the traits in question is quite low (r2 = 0.01).\nNext, we compare this against the performance of the CAE based disentangled representation (similarly trained with multiple machine learning models out of which XgBoost yielded the best results and its performance is reported here). Figure 10 shows the performance of the structured latent representation generated by the CAE. The Compositional Autoencoder (CAE) performs exceptionally well for the 'Days to Pollen' trait, achieving an r\u00b2 value of 0.74. While its performance in predicting 'Yield' is lower, with an r\u00b2 value of 0.34, this is unsurprising given the complexity of the genetic architecture governing yield. Accurate prediction of yield is inherently challenging due to its intricate genetic influences. Previous studies with these genotypes (Jin et al., 2024) involved costly and labor-intensive genotyping and manual trait measurements. These methods require significant time and effort. Considering these factors, achieving such performance using leaf hyperspectral reflectance collected only at a single time point is significant."}, {"title": "3.3 Consistency of Latent Representations", "content": "We evaluate the consistency of the disentangled latent representations by training the model with multiple initial conditions and evaluating its performance across different regression models. This enhances confidence in the reliability and generalizability of the learned latent representations.\nThe initialization of model parameters can impact the training process and the final performance of the model. Different initializations can lead to the model getting to different local minima, resulting in variable performance. To check the consistency of the performance, we trained both the networks (CAE and vanilla AE) using 4 different initial conditions. By training the model with multiple initial conditions, we can evaluate its robustness and consistency in learning informative latent representations. The tables 7 (Days to Pollen) and 8 (Yield) show a comparison of performance between a vanilla auto-encoder and compositional autoencoder for the traits of 'Days to Pollen' and 'Yield' after performing a 5-fold cross-validation. We clearly see the consistency of prediction accuracy across different model initializations.\nWe finally report on varying various hyperparameters of the CAE, and their sensitivity to the downstream performance:\n\u2022 Masking: We evaluated the effect of input masking. Input masking improves the robustness and generalization of autoencoders by forcing them to reconstruct missing or corrupted data, which helps the model learn more significant features and patterns. This technique also acts as a regularization method, preventing overfitting and enhancing performance in various downstream tasks. Table 9 shows the reconstruction accuracy as a function of masking fraction and suggests that 20% masking is a good choice. We also observed that performance on the downstream task also improved by using masking while training. Table 10 shows R\u00b2 observed for different masking percentages.\n\u2022 Network depth: Network depth is an important hyperparameter to explore because it directly influences the model's capacity to learn complex patterns and hierarchical representations within the data. Deeper networks can capture more intricate features and dependencies, potentially leading to improved performance on complex tasks, but they also require careful tuning to avoid issues such as vanishing gradients and overfitting."}, {"title": "4 CONCLUSION", "content": "This study introduced a novel compositional autoencoder (CAE) framework designed to disentangle genotype-specific and environment-specific features from high-dimensional data, thereby enhancing trait prediction in plant breeding and genetics programs. The CAE effectively separates these intertwined factors by leveraging a hierarchical disentanglement of latent spaces, leading to superior predictive performance for key agricultural traits such as \"Days to Pollen\" and \"Yield.\u201d Our results demonstrate that the CAE outperforms traditional methods, including Principal Component Analysis (PCA) and vanilla autoencoders, in capturing relevant information for trait prediction. The evaluation of various network architectures, latent space dimensions, and hyperparameter tuning further validated the robustness and generalizability of the CAE model. Specifically, the CAE showed consistent performance improvements across different initialization conditions and regression models, underscoring its reliability in practical applications.\nBy effectively disentangling genotype and environment-specific features, the CAE offers a powerful tool for improving the accuracy and reliability of predictive models in agriculture, ultimately contributing to more informed decision-making in breeding programs and agricultural management. There are several avenues for future work. First, it will be interesting to explore the viability of compositional autoencoders for making trait predictions using the disentangled GXE features using other sensing modalities Shrestha et al. (2024) like (a) UAV-based hyperspectral imagery and (b) satellite-based multispectral imagery. Second, applying CAE to time-series high-dimensional data collected on diversity panels can produce disentangled low-dimensional time trajectories that could provide biological insight. Finally, integrating these disentangled latent representations with other data (crop models, physiological measurements) may be a promising approach for creating accurate end-of-season trait prediction models using mid-season data.\nWe conclude by identifying the following limitations of our work: (a) We evaluated the performance of the CAE on two specific traits that were phenotyped in the field experiments. Our future work will focus on evaluating the CAE on a broader range of traits; (b) Our study is based on hyperspectral reflectance data from a specific maize diversity panel. Our future work is focused on extending this to other datasets and environments; (c) While we demonstrate the technical advantages of disentanglement, it is not immediately clear how to connect these disentangled features to biological insights."}, {"title": "CONFLICT OF INTEREST STATEMENT", "content": "The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "T.Z.J and B.G conceived the approach. M.C.T and J.C.S led field experiments and data collection. A.P and T.Z.J developed the machine learning pipeline. A.P and M.C.T performed computational experiments and analysis. All authors participated in the project implementation and completion. All authors contributed to the final manuscript production."}, {"title": "1 SUPPLEMENTARY MATERIALS", "content": null}, {"title": "1.1 Data Normalization", "content": "We use Min-Max normalization to normalize the data. Figure S1 is the data visualization after normalization."}, {"title": "1.2 Neural Network Training", "content": "A neural network is a mathematical model inspired by the structure and function of biological neural networks. It consists of interconnected units or nodes called neurons, organized into layers. The most basic type of neural network is the feedforward neural network, where information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any), and to the output nodes.\nMathematical Representation:\n1. Input Layer: The input layer consists of input neurons that receive the input features (denoted as X1,X2,...,xn)."}, {"title": "Powadi et al.", "content": "2. Hidden Layers: Each neuron in the hidden layers applies a weighted sum to its inputs and then passes it through an activation function. The output of the j-th neuron in the k-th layer can be expressed as:\n$a^{(k)}_{j} = \\sigma (\\sum_{i=1}^{n} W_{ij}^{(k)} x_{i} + b_{j}^{(k)})$\nwhere $W_{ij}^{(k)}$ is the weight associated with the connection between the i-th neuron in the (k \u2013 1)-th layer and the j-th neuron in the k-th layer, $b_{j}^{(k)}$ is the bias term for the j-th neuron in the k-th layer, and $\\sigma$ is the activation function (e.g., sigmoid, ReLU).\n3. Output Layer: The output layer produces the final output of the network. The process is similar to that of the hidden layers, but the output might pass through a different activation function suitable for the specific task (e.g., softmax for classification).\nTraining the Neural Network:\nThe training of a neural network involves adjusting the weights and biases to minimize the difference between the predicted output and the actual output. This process is typically done using the backpropagation algorithm and an optimization technique like gradient descent.\n1. Loss Function: A loss function (L) measures the difference between the predicted output (y) and the actual output (y). Common loss functions include mean squared error for regression tasks and cross-entropy loss for classification tasks.\n2. Backpropagation: This algorithm computes the gradient of the loss function with respect to each weight and bias in the network by applying the chain rule of calculus. It starts from the output layer and propagates the error backward through the network. The gradient of the loss with respect to the weights in layer k is given by:\n$\\frac{\\partial L}{\\partial W_{ij}^{(k)}} = \\frac{\\partial L}{\\partial a^{(k)}_{j}} \\frac{\\partial a^{(k)}_{j}}{\\partial W_{ij}^{(k)}}$\nwhere $\\frac{\\partial L}{\\partial a^{(k)}_{j}}$ is the error propagated from the next layer, and $\\frac{\\partial a^{(k)}_{j}}{\\partial W_{ij}^{(k)}}$ is the derivative of the activation function with respect to the weights.\n3. Optimizer: An optimizer uses the gradients calculated by backpropagation to update the weights and biases. The L-BFGS optimizer is a quasi-Newton method that approximates the Hessian matrix to guide the search for the minimum. The weight update equation for L-BFGS can be expressed as:\n$w^{(k+1)} = w^{(k)} \u2013 \\alpha H_{k} \\nabla L(w^{(k)})$\nwhere w(k) are the weights at iteration k, $\\alpha$ is the step size, Hk is the approximate inverse Hessian matrix, and $\\nabla L(w^{(k)})$ is the gradient of the loss function at iteration k.\nThis iterative process of forward pass, computing loss, backward pass, and updating weights is continued until the model is sufficiently trained. After training, the neural network can be used for predictions or further analysis."}, {"title": "1.3 Optimizer", "content": null}, {"title": "1.3.1 LBFGS", "content": "To train this neural network, we used the LBFGS optimizer. In the case of a neural network, the optimizer plays a very important role. It drives the weights of the network to a point in the N-dimensional space (N is the number of parameters of the neural network) for which, the loss function for this network is at its minimum. LBFGS is a quasi-newton method of optimization that approximates the hessian instead of calculating it at every iteration to reduce the complexity and time. Newton's method is used to find the minima of a non-quadratic function. It typically starts with a random point and approximates a quadratic around that point and finds the minima for that quadratic and repeats these steps until a minima for the non-quadratic is found. Quadratic is approximated using the \u2018Taylor series' expansion.\n$g(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!} (x\u2212a)^2 + \\frac{f'''(a)}{3!} (x-a)^3 +...$\nwhere:\n\u2022 $f^{(n)}(a)$ denotes the n-th derivative of f evaluated at the point a,\n\u2022 n! is the factorial of n.\nHessian Approximation in L-BFGS:\nL-BFGS maintains a history of the last m updates of the position vectors (sk) and the gradient vectors (yk), where k indexes the iteration. The position and gradient vectors are defined as:\n$s_{k} = w^{(k+1)} - w^{(k)}, y_{k} = \\nabla L(w^{(k+1)}) \u2013 \\nabla L(w^{(k)})$\nThe approximate inverse Hessian matrix (Hk) is updated at each iteration using this history. The update formula is derived from the BFGS update formula but modified to use limited memory. The approximation starts with an initial estimate $H_{0}^{k}$, which is often chosen as a scaled identity matrix:\n$H_{0}^{k} = \\gamma I$\nwhere \u03b3k is a scaling factor that can be computed using different strategies, one common choice being:\n$\\gamma_{k} = \\frac{s_{k-1}^{T} y_{k-1}}{y_{k-1}^{T} y_{k-1}}$\nThen, the approximate inverse Hessian Hk is updated using the formula:\n$H_{k} = (V_{k}^{T} H_{k}^{k}V_{k} + \\rho_{k} s_{k} s_{k}^{T})$\nwhere $V_{k} = I - \\rho_{k} y_{k} s_{k}^{T}$ and $\\rho_{k} = \\frac{1}{y_{k}^{T} s_{k}}$"}, {"title": "Powadi et al.", "content": "Step Size Determination in L-BFGS:\nThe step size (\u03b1k) in L-BFGS is typically determined using a line search method that satisfies the Wolfe conditions. The line search aims to find a step size that ensures a sufficient decrease in the loss function and a sufficient slope of the gradient. The Wolfe conditions are:\n1. Sufficient Decrease Condition (Armijo Condition):\n$L(w^{(k)} + \\alpha_{k} p_{k}) \\leq L(w^{(k)}) + c_{1} \\alpha_{k} \\nabla L(w^{(k)})^{T} p_{k}$\n2. Curvature Condition:\n$\\nabla L(w^{(k)} + -\\alpha_{k} p_{k}) \\alpha_{k} p_{k} \\geq c_{2} \\nabla L(w^{(k)})^{T} p_{k}$\nwhere 0 < c1 < c2 < 1 are constants, pk = -HkVL(w(k)) is the search direction, and ar is the step size.\nThe line search algorithm iteratively adjusts \u03b1k until the Wolfe conditions are satisfied, ensuring that the step size leads to a sufficient decrease in the loss function while maintaining the curvature condition."}, {"title": "1.4 Parameter Exploration", "content": "Table S1. Table shows the performance observed for different masking percentages. Val. Loss = Coereff Loss + L2 Loss.\nAutoencoders are often trained with many different pre-text tasks. A very prevalent method is masking. During the training, the input is partially masked (with zeros) and fed into the Autoencoder, and the reconstruction loss is calculated with respect to the original input data. This forces the network to learn better. The table S1 shows the results with different masking percentages."}, {"title": "1.5 Downstream Models", "content": null}, {"title": "1.5.1 Random Forests", "content": "Random Forest is an ensemble method composed of numerous decision trees. For regression tasks, it averages the outputs of these trees to obtain the final output, while for classification tasks, it typically uses majority voting. Decision trees recursively split the data into subsets based on attribute tests that maximize information gain. Information gained in each split can be calculated using either entropy or the Gini index. The formula for entropy is $Entropy ="}]}