{"title": "LOGICGAME: Benchmarking Rule-Based Reasoning Abilities of Large Language Models", "authors": ["Jiayi Gui", "Yiming Liu", "Jiale Cheng", "Xiaotao Gu", "Xiao Liu", "Hongning Wang", "Yuxiao Dong", "Jie Tang", "Minlie Huang"], "abstract": "Large Language Models (LLMs) have demonstrated notable capabilities across various tasks, showcasing complex problem-solving abilities. Understanding and executing complex rules, along with multi-step planning, are fundamental to logical reasoning and critical for practical LLM agents and decision-making systems. However, evaluating LLMs as effective rule-based executors and planners remains underexplored. In this paper, we introduce LOGICGAME, a novel benchmark designed to evaluate the comprehensive rule understanding, execution, and planning capabilities of LLMs. Unlike traditional benchmarks, LOGICGAME provides diverse games that contain a series of rules with an initial state, requiring models to comprehend and apply predefined regulations to solve problems. We create simulated scenarios in which models execute or plan operations to achieve specific outcomes. These game scenarios are specifically designed to distinguish logical reasoning from mere knowledge by relying exclusively on predefined rules. This separation allows for a pure assessment or rule-based reasoning capabilities. The evaluation considers not only final outcomes but also intermediate steps, providing a comprehensive assessment of model performance. Moreover, these intermediate steps are deterministic and can be automatically verified. LOGICGAME defines game scenarios with varying difficulty levels, from simple rule applications to complex reasoning chains, in order to offer a precise evaluation of model performance on rule understanding and multi-step execution. Utilizing LOGICGAME, we test various LLMs and identify notable shortcomings in their rule-based logical reasoning abilities.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have shown notable abilities in a wide range of tasks and even complex problem-solving [4; 37; 7; 12]. Their ability to reason, encompassing the understanding of intricate scenarios, strategic planning, and multi-step execution, is crucial for developing advanced AI agents and decision-making systems [20; 30; 6]. These capabilities allow LLMs to understand complex user instructions, make logical decisions, and execute tasks accurately.\nAs alignment becomes integral to the application of LLMs [25; 1; 27; 5], the primary goal is to align with human intentions and accurately execute their instructions. Simultaneously, these models must possess strong reasoning abilities to handle complicated scenarios. However, evaluating LLMs as effective rule-based executors and planners remains underexplored. Traditional benchmarks usually"}, {"title": "Related Work", "content": "The capability to reason has long been a crucial aspect of language models. Research [34] has demonstrated that as the size of models increases, their ability to reason emerges, making it a fundamental attribute of LLMs. To elicit this reasoning ability, techniques like chain-of-thought prompting [35] and specialized training [23] have become widely adopted. Multi-step reasoning, in particular, is essential for complex decision-making and planning tasks, such as those undertaken by LLM agents [20].\nNumerous benchmarks have been established over time to rigorously evaluate the reasoning capabili-ties of neural network models. Early research has concentrated on logical reasoning [3; 8; 36]. These studies cover various forms of logic, including inductive, deductive, and abductive reasoning, and aim to assess whether models can infer answers based on given conditions. Mathematical reasoning"}, {"title": "LOGICGAME", "content": "This section outlines our systematic approach to dataset construction, which comprises four key phases:\n1. Design rule-based problems inspired by real-world scenarios (\u00a73.1.1).\n2. Develop output constraints to standardize evaluation formats (\u00a73.1.2).\n3. Implement a spectrum of difficulty levels and incorporating exemplars (\u00a73.1.3).\n4. Create bilingual versions through meticulous translation of problems and instructions (\u00a73.1.4)."}, {"title": "Problem Collection and Design", "content": "Collection and extraction of real-world scenarios. The integration of rule-following and reasoning is a critical aspect of many real-world tasks, yet existing benchmarks often fail to adequately capture this well. To address this gap, we developed a novel problem set through extensive research and crowdsourcing. We found that these tasks are similar to some game mechanics, since real-world tasks often share features with games, such as having specific rules to follow, requiring decision-making. This insight led us to adopt a gamification approach, allowing for a nuanced evaluation of models' rule-following reasoning capabilities.\nThe dataset is structured into two primary domains: Execution and Planning."}, {"title": "Output Constraint Design", "content": "To facilitate precise evaluation and streamline the matching process, we mandated a structured JSON output format for model responses. Our evaluation criteria are tailored to the complexity of each problem. For single-step problems categorized as Level 0, models are only required to output the final answer, and evaluation is based solely on the correctness of this answer. However, for problems involving multiple steps or more complex reasoning, which include Levels 1, 2, 3, and certain Level 0 problems, we evaluate both the answer and the process.\nIn both cases, the output JSON structure includes 'answer', which is a list of strings representing the final solution(s), and for second cases the output also includes 'process', a list of strings detailing each step of the problem-solving process."}, {"title": "Difficulty Levels and Exemplars", "content": "To comprehensively assess models' reasoning capabilities, we have structured our benchmark with four distinct difficulty levels (0, 1, 2, and 3) for each task. The difficulty gradient is determined by two key factors: the complexity of the rules involved and the number of reasoning steps required to arrive at the solution. Each successive level systematically introduces additional rules and reasoning steps. In general, our problems are difficult for models, and some are also challenging for humans.\nFurthermore, to evaluate models' capacity for rule acquisition and application, we have developed two distinct exemplars for each question. These exemplars consist of a given question, the correct answer, a step-by-step solution process, and detailed explanations. By providing these examples, we aim to test not only the models' baseline performance but also their ability to learn from demonstrations and apply newly acquired rules to similar problems."}, {"title": "Building Bilingual Benchmark", "content": "Initially, our questions were designed in Chinese. However, we recognized that this could potentially bias the benchmark against LLMs primarily trained on English data. To ensure fairness and broader applicability of our benchmark, we developed a comprehensive bilingual benchmark containing both zh(Chinese) and en(English) version."}, {"title": "Evaluation Protocol", "content": "Each model is prompted with a set of rules specific to the given problem, along with a corresponding question and a JSON format constraint for the output, encompassing both the answer and the process, as illustrated in Figure 2. For few-shot trials, example(s) are inserted between the rules and the question to assess the model's in-context learning capabilities. The model responses are then collected and subjected to automated evaluation. As previously mentioned, the evaluation protocol is designed to assess not only the correctness of the answer but also the correctness of the process that led to the answer. Scoring for each problem's answer is determined by comparing the model's response to the reference answer. Similarly, scoring for each problem's process, as defined by the JSON format constraint, is achieved by assessing the degree of alignment between the model's process and the reference process. Specifically, the LOGICGAME defines three metrics related to each problem for scoring:\n1. Answer Accuracy(A-Acc): This metric evaluates the correctness of the answers for all given questions, providing a binary assessment (0/1) for each answer to indicate whether it is correct or not.\n2. Process Accuracy(P-Acc): This metric assesses the correctness of the process, measuring the percentage match based on character-level similarity between the provided process and the expected process. In rare cases where no process is provided in level 0 questions as single-step reasoning, process accuracy are considered equally with answer accuracy for scoring.\n3. Answer Process Accuracy(AP-Acc): This composite metric evaluates the overall accuracy of both the answer and the process. Its calculation involves an aggregate score derived by combining answer accuracy and process accuracy using a logical AND operation."}, {"title": "Experiments", "content": "We evaluate 11 popular LLMs. Closed-source models included versions from Claude [1] and GPT-4 [26] series. Open-source models encompassed LLaMA 3 [33], Qwen [2], GLM [12], Mistral [17], and InternLM [32] variants."}, {"title": "Main Results and Analysis", "content": "The average performance of 11 LLMs on LOGICGAME across our Chinese and English bilingual dataset, measured by AP-Acc, is presented in Table 2. The results indicate that reasoning remains a significant challenge for LLMs. Even the strongest model can't reach an overall score of 20%. As the difficulty level increases, the performance of LLMs drops significantly, with many models dropping to 0% score at Level 3, especially in Planning tasks. This suggests that even advanced LLMs struggle with complex reasoning tasks.\nThe performance drop from Level 0 to Level 3 is not uniform across models. Some models (e.g. claude-3.5-sonnet) show a more gradual decline, while others drop off sharply after Level 1. This may suggest that some models have better consistency across task complexities.\nInterestingly, the performance of models in Execution and Planning tasks varies. Some top-performing models, such as claude-3.5-sonnet, demonstrates superior performance in Execution compared to Planning (22.51% vs. 15.80%). Conversely, llama-3-70b-chat excels in Planning over Execution (11.00% vs. 6.53%). Even within model families, the relative performance differences are evident. In the GPT family, gpt-4-turbo-0409 outperforms gpt-4o in Planning tasks (15.32% vs. 13.21%), while gpt-4o shows better performance in Execution tasks (19.73% vs. 15.89%).\nDuring the evaluation, it was observed that some models occasionally failed to adhere to the require-ment of producing JSON format output. Detailed results and analysis are provided in Appendix B. Despite this, the overall error rates are low across most models, resulting in minimal differences in rankings among those with similar performance."}, {"title": "Few-shot Results", "content": "We conducted experiments to analyze the change of model's performance on 0-shot, 1-shot and 2-shot settings. And models of gpt-4o, qwen2-72b-instruct and qwen2-7b-instruct, llama-3-70b-chat and llama-3-8b-chat, glm-4-9b, mistral-7b-instruct and internlm-2.5-7b-chat are chosen for this trial. The analysis demonstrated in Figure 3 and Figure 4 reveals mixed results of LOGICGAME's zh version in the \"Planning\" and \"Execution\" categories, difficulty levels across the various shot settings. Appendix D shows the results of LOGICGAME's en version.\nIn the \"Execution\" category, models demonstrate notable improvements in accuracy with increased shot contexts demonstrated in Figure 3. Specifically, stronger models (as indicated in Table 2) like gpt-4o, qwen2-72b-instruct shows a greater increase in the AP-Acc score when transitioning from 0-shot to 1-shot and 2-shot settings than weaker ones, indicating enhanced execution accuracy with more contextual information. However, the effects of 1-shot and 2-shot settings vary across\nmodels Performance variations by difficulty levels, as shown in Figure 4, indicate that models benefit most from 1-shot and 2-shot contexts at Level 0. And in general, the influence of shot contexts diminishes as the difficulty level increases. This consistency suggests that simpler tasks (Level 0) allow models to leverage additional context more effectively, enhancing their execution capabilities across the board.\nConversely, the \"Planning\" category presents more heterogeneous results. Models often show declines in performance when moving from 0-shot to 1-shot or 2-shot settings demonstrated in Figure 3. These results suggest that while additional context can enhance performance for some models, it can introduce noise for others, potentially obfuscating key elements necessary for planning tasks. Overall, these observations highlight that the added context's efficacy is highly contingent on the task and model characteristics. Furthermore, Figure 4 illustrates that the negative impact of 1-shot contexts is most pronounced at Level 0. As the difficulty level increases, the influence of 1-shot contexts diminishes, leading to smaller performance fluctuations, while 2-shot contexts are more unstable with no pattern found."}, {"title": "Discussion", "content": "Case study on Revesi game. From Appendix E, it is evident that all models perform poorly in the Reversi game. Consequently, we conducted a case study on this particular game scenario. The responses from various models tasked with determining the outcome of a Reversi game are analyzed as shown in Figure 5. Despite all models except llama-3-8b-chat adhering to the instruction format and correctly interpreting the initial setup, all models failed to provide the correct answer, demonstrating various types of inaccuracies. The key reasons for failure include:\n1. Mismanagement of Details: For instance, claude-3.5-sonnet misplaced markers or incorrectly transformed pieces, showing that while the general rules were understood, the model failed to apply specific game rules correctly.\n2. Inadequate Execution/Planning Understanding: Models like qwen2-72b-instruct produced incorrect board states following what should have been straightforward captures, revealing a fundamental misunderstanding of the game's piece-flipping mechanisms as well as the initial conditions outlined in the problem.\n3. Excessive Alterations: The llama-3-8b-chat model drastically altered the board state in an unrealistic manner, adding rows and altering more positions than the rules allow, suggesting a misinterpretation of the core principles of the game, particularly with regards to matrix operations and the understanding and execution of piece-flipping mechanisms."}, {"title": "Conclusion", "content": "In this paper, we introduce LOGICGAME, a novel benchmark designed to evaluate the rule-based reasoning capabilities of LLMs. LOGICGAME encompasses multiple difficulty levels, focusing on assessing models' understanding of rules, execution based on these rules, and planning abilities. Moreover, we have developed methods to evaluate both outcomes and reasoning processes, ensuring that models follow the given rules faithfully rather than merely guessing answers. Extensive experiments indicate that current large models still exhibit significant deficiencies in rule-based reasoning tasks. More effort needs to be devoted to further enhancing models' abilities to handle complex reasoning scenarios."}, {"title": "JSON prompt", "content": "The evaluation JSON constrain prompt template we used to evaluate the performance of different models is shown in Figure 6."}, {"title": "JSON Prompt Template", "content": "Please generate a JSON object that follows standard JSON formatting and indentation, containing a field named 'answer'. The 'answer' field should be a list of strings, where each string represents The 'process' field should be a list of strings, where each string\nExample: String Synthesis\nInput:\nNow there are four different types of blocks: [A], [B], [C], and A, which satisfy the following rules:\n1. One [A], one [B], and one [C] can be synthesized into one {A}\n2. One [A] and one [B] can be synthesized into one [C]\nQuestion: If we currently have four [A], four [B], and three [C], what will be the result after synthesis?\nJson Constraints:\nPlease generate a JSON object that follows standard JSON formatting and indentation, containing a field named 'answer'. The 'answer' field should be a list of strings, where each element represents the number of different types of blocks, in the order of [A], [B], [C], A. The 'process' field should be a list of strings, where each string records the instructions for each step from the initial state to the final state. First output the blocks that need to be synthesized, followed by the \"->\" symbol, then output the synthesized block, without adding any extra explanations. For example: [\"[A] [B] [C] -> {A}\", \"[A] [B] -> [C]\"].\nOutput:\n{\n\"answer\": [\"0\", \"3\", \"6\",\"1\"],\n\"process\": [\n\"[A] [B] [C] -> {A}\",\n\"[A] [B] -> [C]\",\n\"[A] [B] [C] -> {A}\",\n\"[A] [B] -> [C]\"\n]\n}"}, {"title": "Data overview of different categories", "content": "Table 3 provides a detailed classification of categories along with the corresponding sample sizes as shown below."}, {"title": "Additional evaluation metrics and error analysis", "content": "During the evaluation process, it is observed that some models occasionally fail to adhere to instructions regarding the constraint of JSON format output. In this context, we have defined two error metrics and one correctness metric for thorough analysis:\n1. JSON Error (JSError): This metric tracks instances where there is an error in parsing the JSON format, usually due to incomplete or improperly formatted JSON outputs."}, {"title": "Instruction Following Error (IFError)", "content": "This metric measures the number of instances in which a JSON format could not be successfully extracted."}, {"title": "Answer Process Accuracy based on non-IFError and non-JSError (NIJ-Acc)", "content": "This is the correctness metric, which evaluates the Answer Process Accuracy (AP-Acc) concerning non-IFError and non-JSError. The formula is provided in Equation 1.\n$$S_{NIJ-Acc} = \\frac{S_{AP-Acc}}{1 - S_{IFError} - S_{JSError}}$$"}, {"title": "Few shot results of en version", "content": "Figure 7 and Figure 8 shows few-shot results of LOGICGAME's en version. For 'Execution' category, the observed conclusion is aligned with zh version. Shot contexts increases P-Acc score in most cases except glm-4-9b and llama-3-8b-chat. When difficulty levels are increased, the positive effect of shot contexts declines. For 'Planning' category, most models decrease or has no change when extra shot contexts are added except for llama-3-70b-chat and mistral-7b-instruct. No notable effect on shot contexts on different difficulty levels except for level 1."}, {"title": "Problems all models fail", "content": "Figure 9 categorizes the five areas where all models exhibit the poorest performance, presenting the average AP-ACC scores for each category in a heat map. The horizontal axis corresponds to the model capabilities as outlined in Table 2. This figure highlights that models generally struggle most in two sub-categories within the execution scenario, particularly with 'Reversi', where many models score close to zero. Conversely, in planning scenarios such as 'Constrained Linear Arrangement', there is a slight variation in performance across different models."}]}