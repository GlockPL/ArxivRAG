{"title": "Multi-Tiered Self-Contrastive Learning for Medical Microwave Radiometry (MWR) Breast Cancer Detection", "authors": ["Christoforos Galazis", "Huiyi Wu", "Igor Goryanin"], "abstract": "The pursuit of enhanced breast cancer detection and monitoring techniques is a paramount healthcare objective, driving the need for innovative imaging technologies and diagnostic approaches. This study introduces a novel multi-tiered self-contrastive model tailored for the application of microwave radiometry (MWR) breast cancer detection. Our approach encompasses three distinct models: Local-MWR (L-MWR), Regional-MWR (R-MWR), and Global-MWR (G-MWR), each engineered to analyze varying sub-regional comparisons within the breasts. These models are cohesively integrated through the Joint-MWR (J-MWR) network, which leverages the self-contrastive data generated at each analytical level to enhance detection capabilities. Employing a dataset comprising 4,932 cases of female patients, our research showcases the effectiveness of our proposed models. Notably, the J-MWR model distinguishes itself by achieving a Matthews correlation coefficient of 0.74 \u00b1 0.018, surpassing existing MWR neural networks and contrastive methods. These results highlight the significant potential of self-contrastive learning techniques in improving both the diagnostic accuracy and generalizability of MWR-based breast cancer detection processes. Such advancements hold considerable promise for further investigative and clinical endeavors. The source code is available at: https://github.com/cgalaz01/self_contrastive_mwr", "sections": [{"title": "1 Introduction", "content": "Breast cancer, marked by the uncontrolled and rapid growth of cells due to genetic mutations, significantly impacts global health, as it records one of the highest incidence rates of cancer. In 2020 alone, it was estimated to account for 2.3 million new cases, becoming the primary cause of death among women with nearly 700,000 deaths [1]. Disturbingly, future forecasts suggest a continued rise in both the occurrence and death rates associated with breast cancer [2].\nThe pivotal role of early detection in reducing mortality rates and reducing the healthcare load cannot be overstated. In this context, Microwave Radiometry (MWR) emerges as a promising imaging modality that passively captures the natural microwave emissions of human tissues [3]. Its utility spans a broad spectrum of clinical areas, including but not limited to, the breasts [3, 4, 5, 6], brain [7, 8], lungs [9], veins [10], and musculoskeletal structures [11]. Within the domain of breast cancer screening, MWR leverages the fact that cancerous tissues, due to their increased metabolic rate, emit more heat than normal tissue [4]. Its advantages and manifold, offering non-invasive, safe, mobile, and economical options for diagnosis. However, the relatively novel integration of MWR into breast cancer diagnostics introduces challenges, particularly in data interpretation and the integration of this technology into existing medical workflows. Overcoming these hurdles necessitates the deployment of artificial intelligence (AI) models to refine and streamline the application of MWR in a clinical setting."}, {"title": "2 Materials and Methods", "content": "We standardized the model settings utilized across all presented models. Initialization of model weights employed Glorot uniform initialization [27], with initial biases set to 0. Weight optimization utilized the Adam optimizer [28] with an initial learning rate of 0.0001, and the remaining parameters, \u03b21, and \u03b22 set to 0.9 and 0.999, respectively. The learning rate was reduced by a factor of 0.1 if the validation loss did not decrease after 5 epochs. A batch size of 4 was determined to be suitable for all evaluated models. The weights were updated based on the class-balanced binary cross-entropy loss."}, {"title": "2.3 Base Model", "content": "To streamline our approach, we introduce a fundamental building block called the MWR-Block, which is a residual fully connected (FC) component outlined in Figure 3A. Each MWR-Block consists of an FC layer, layer normalization, a Rectified Linear Unit (ReLU) activation function [29], another FC layer, layer normalization, ReLU, and addition with the block's input.\nOur baseline model, hereafter defined as the \"base model\", is the neural network proposed in [12]. However, to accommodate the larger dataset under evaluation, we made adjustments to the base model's parameters. Specifically, it now comprises 4 MWR-Blocks, each FC layer containing 256 units. The output FC layer of unit size 1 uses a sigmoid activation function."}, {"title": "2.4 Self-Contrastive MWR Neural Networks", "content": "Inward learning, not outward wandering Our proposed models utilize self-contrastive learning to optimize the embedding space for distinguishing between healthy and cancerous samples. The embedding space of healthy and cancerous cases is pushed to distinct clusters. Unlike traditional contrastive learning methods, our approach focuses on features within individual cases rather than across samples."}, {"title": "2.4.1 L-MWR Neural Network", "content": "The Local-MWR (L-MWR) network processes each temperature point individually, excluding reference measurements, resulting in 18 inputs, each consisting of skin and internal values (see Figure 3B). The network is structured into feature extraction and feature comparison. Feature extraction involves 4 MWR-Blocks, followed by a single unit FC layer with ReLU activation, in which the weights are shared. Small activations below a learnable threshold are filtered to 0. Feature comparison computes the mean absolute feature differences between pairwise inputs. The prediction is obtained through a single unit FC layer with the tanh activation function, as the activations are already bound below to 0."}, {"title": "2.4.2 R-MWR Neural Network", "content": "The Regional-MWR (R-MWR) network compares left and right breasts as self-contrastive regions (see Figure 3C), with 2 inputs of vector size 24 each, including respective breast and reference points. Shared feature extraction includes 4 MWR-Blocks with 256 units, followed by an FC layer with ReLU activation, and 12 normalization. Element-wise absolute differences are computed, filtered, and summed for comparison. Prediction is generated via a single unit FC layer with tanh activation."}, {"title": "2.4.3 G-MWR Neural Network", "content": "The Global-MWR (G-MWR) network utilizes features from both breasts (see Figure 3D). However, to perform self-contrastive learning, we use as the second input the inverse positions, in which the values of the left breast are used as the right and vice versa. Thus, G-MWR takes two pairs of input, each of size 44, and uses the same architecture as R-MWR, described in section 2.4.2."}, {"title": "2.4.4 J-MWR Neural Network", "content": "The Joint-MWR (J-MWR) network combines the L-MWR (section 2.4.1), R-MWR (section 2.4.2), and G-MWR (section 2.4.3) pre-trained models to leverage their complementary features (see Figure 3E). Weighted outputs from each sub-network, using individual FC layers, are concatenated and passed through a final single unit FC layer with tanh activation. As we are only fine-tuning the weights of the sub-networks, the learning rate was reduced to le-7."}, {"title": "2.5 Experiments", "content": "In our experiments, we conducted three model executions with different initialization seeds, and the reported results represent the average across these three seeds on the test set. The model evaluation is based on the Matthews correlation coefficient (MCC) to assess the performance of imbalanced data when we consider both positive and negative cases equally important [30]. We also assessed the accuracy and receiver operating characteristic (ROC) area under the curve (AUC) for the main results as a reference. Our analysis involved comparing our proposed self-contrastive models with commonly used batch-wise contrastive losses, both individually and in combination.\nSpecifically, we utilized contrastive loss [23], triplet hard loss [24], triplet semi-hard loss [25], and N-pairs loss [26], where N is the number of negatives in the batch. The weight assigned to the batch-wise contrastive losses was experimentally determined to be 0.1, while the weight for cross-entropy classification remained at 1.0."}, {"title": "3 Results", "content": "Our proposed J-MWR model has the highest predictive capabilities in correctly identifying breast cancer from MWR data. It obtains an MCC score of 0.74 \u00b1 0.018, a 0.08 margin from the second-best performing model, R-MWR. This translates to an accuracy of 0.95 \u00b1 0.003 and an ROC AUC of 0.96 \u00b1 0.001. In comparison, the base model obtains an MCC score of 0.58 \u00b1 0.004, an accuracy of 0.88 \u00b1 0.003, and an ROC AUC of 0.93 \u00b1 0.006. The results for all models can be seen in Table 1.\nWe can consider J-MWR as a meta-classifier of the sub-networks. In this case, when we compare individual models, we can observe that both R-MWR and G-MWR outperform the base model with an MCC of 0.66 \u00b1 0.012 and 0.61 \u00b1 0.045, respectively. L-MWR is the only model that performs substantially worse than the base model, with an MCC score of 0.43 \u00b1 0.002. This is expected as the model only learns features from a single point."}, {"title": "3.2 Batch-wise Contrastive Loss Evaluation", "content": "When employing batch-wise contrastive loss, J-MWR with triplet hard loss achieves the highest performance, as illustrated in Table 2, with an MCC score of 0.74\u00b10.03. Overall, J-MWR consistently achieves the highest MCC score compared to other models, regardless of the batch-wise contrastive loss used. However, none of the configurations surpass J-MWR without batch-wise contrastive learning.\nInterestingly, both R-MWR and J-MWR experience a reduction in MCC score when utilizing batch-wise contrastive learning. On the other hand, L-MWR shows a slight improvement of 0.01, while G-MWR demonstrates an improvement ranging from 0.03 to 0.06. The base model also benefits from contrastive and N-pairs losses, with an MCC score increase of 0.02 for both configurations."}, {"title": "3.3 Embedding Space", "content": "In this analysis, we explore the properties and characteristics of the embedding space generated by the L-MWR, R-MWR, G-MWR, and base contrastive models. The 2D projections of the embedding spaces can be observed in Figure 4. Our findings reveal that the base contrastive model, in comparison to our proposed models, excels at delineating a clearer boundary between the healthy and cancerous groups, with a mean between-class distance of 6.57 \u00b13.71. This result is expected, given its explicit training for this task. However, notable disparities within the groups emerge, as reflected by a mean within-class distance of 4.53 \u00b1 2.85, contributing to its relatively lower performance.\nOur proposed models, despite being trained on comparing features within themselves, we observe that cancerous cases tend to have similar embedding properties across cases. However, these cases are more intertwined with the healthy cases, making it challenging to discern a clear boundary between them. For instance, R-MWR demonstrates a mean within-class distance of 2.93 \u00b1 1.46 and a mean between-class distance of 3.94 \u00b1 1.17. This close proximity of each group sufficiently mitigates the lack of a clear boundary, thereby improving performance.\nThese findings suggest the potential for the two methods, self-contrastive and batch-wise con-trastive, to complement each other and further enhance performance."}, {"title": "3.4 Data Constraint Training", "content": "We retrained the models using randomly selected subsets of the training set at 75%, 50%, and 25%. This approach allows us to gauge how the models perform when trained with limited data, offering insights into their scalability as more data becomes available. The performance of the models is summarized in Figure 5a. J-MWR consistently demonstrates the highest MCC across all subsets,"}, {"title": "3.5 Batch Size Dependency", "content": "In our investigation into the impact of batch size on performance, we analyzed values ranging from 1 to 128. J-MWR consistently outperforms other models, achieving its peak MCC at a batch size of 4 with a value of 0.74\u00b10.018, followed by a gradual decline, as shown in Figure 5b. Notably, while most models experienced a decline in performance with increasing batch size, the base contrastive model maintained near-consistent performance levels, a trend we anticipate extending to larger batch sizes.\nThis suggests that, for self-contrastive models, smaller batch sizes are preferable, although this comes with a trade-off between training speed and accuracy. Smaller batches may yield higher accuracy but result in longer training times. In contrast, batch-wise contrastive losses for MWR are less influenced by batch size."}, {"title": "3.6 Generalizability", "content": "To assess the generalizability of our trained models, we subjected them to an augmented test set comprising various out-of-distribution transformations. These transformations included adding Gaussian noise (Figure 6a), applying dropout to the temperature points and setting their value to the mean of the remaining points (Figure 6b), adjusting the temperature of all values (Figure 6c), and rotating the breast points around the nipple (Figure 6d).\nAs depicted in Figure 6, J-MWR consistently demonstrated better generalization performance, evidenced by its higher MCC score compared to other models, both in the presence of data corruptions (Figures 6a and 6b) and data drifts (Figures 6c and 6d). However, when subjected to large Gaussian noise (\u03c3 > 0.25), R-MWR showed greater resilience. This decline in J-MWR's performance can be attributed to the poorer performance of G-MWR under noisy conditions. We note that while L-MWR shows the smallest changes across the different transformations its performance"}, {"title": "3.7 Ensemble Methods", "content": "In the J-MWR network, each sub-network contributes almost equally to the final prediction. Specifically, the L-MWR, with a weight of 0.998, holds comparable significance to both the R-MWR and G-MWR networks, each possessing weights close to 1.0. Moreover, their biases are all approximately 0. From Figure 7, it is apparent that J-MWR surpasses other ensemble techniques, including averaging and majority voting, as well as meta-classifiers such as logistic regression, SVM, and decision tree. While J-MWR bears a resemblance to averaging voting, the fine-tuning step applied to the sub-networks enables substantial enhancement in MCC performance.\nWhen averaging the predictions of L-MWR, R-MWR, and G-MWR, the second-best MCC value of 0.66 \u00b1 0.02 is achieved, which is comparable to that obtained from R-MWR alone. Following closely is the majority voting approach. Notably, despite best efforts, the remaining meta-classifiers exhibited overfitting to the training data, leading to a large decline in MCC performance on the test set."}, {"title": "4 Discussion", "content": "In this study, we have demonstrated the successful adaptation of self-contrastive learning to tackle MWR breast cancer detection. Our proposed architectures, namely L-MWR, R-MWR, and G-MWR, offer a novel approach to integrating hand-engineered features capturing thermal asymmetries alongside data-driven methods. Our combined model, J-MWR, achieves an MCC score of 0.74\u00b10.018 surpassing all existing models, showcasing its efficacy in detecting breast cancer with high accuracy and generalizability. This underscores the potential of self-contrastive learning in enhancing the performance of breast cancer detection systems.\nThe final layer of J-MWR acts as an ensemble averaging due to the weights of each sub-network being near 1. However, J-MWR gains an improvement of 0.08 MCC compared to the traditional ensemble averaging. This improvement is attributed to the fact that J-MWR serves as a fine-tuning meta-classifier. Specifically, it enables the indirect sharing of information between each self-contrastive tier, thereby enhancing performance. However, it's important to note that this benefit comes at the cost of increased model complexity.\nFurthermore, batch-wise contrastive learning, while it only contributed to minor improvements, we see potential in its use. It presents a minimally disruptive enhancement to existing MWR models. Our future efforts will focus on refining positive and negative sampling strategies tailored to physiological characteristics, thus advancing beyond mere classification considerations. Its current subpar performance can be attributed to the inherent variability in breast temperature readings, influenced by factors such as age and menstrual cycle phase [3]."}, {"title": "5 Conclusion", "content": "Our research presents promising advancements in MWR breast cancer detection, offering potential clinical benefits and avenues for future exploration. Moving forward, further refinement through NAS and tailored sampling strategies holds promise for further enhancing diagnostic accuracy. Additionally, evaluating our proposed model across various anatomical locations and under different physiological conditions will be crucial for expanding its applicability and effectiveness in diverse clinical settings. To further improve breast cancer predictions, we aim to adapt our proposed self-contrastive learning approach by including mammogram, gene expression, miRNA and other multi-omics data."}]}