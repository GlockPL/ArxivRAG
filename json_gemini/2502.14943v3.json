{"title": "GenAI vs. Human Fact-Checkers: Accurate Ratings, Flawed Rationales", "authors": ["Yuehong Cassandra Tai", "Khushi Navin Patni", "Nicholas Daniel Hemauer", "Bruce Desmarais", "Yu-Ru Lin"], "abstract": "Despite recent advances in understanding the capabilities and limits of generative artificial intelligence (GenAI) models, we are just beginning to understand their capacity to assess and reason about the veracity of content. We evaluate multiple GenAI models across tasks that involve the rating of, and reasoning about, the credibility of information. The information in our experiments comes from content that subnational U.S. politicians post to Facebook. We find that GPT-40, one of the most used Al models in consumer applications, outperforms other models, but all models exhibit only moderate agreement with human coders. Importantly, even when GenAI models accurately identify low-credibility content, their reasoning relies heavily on linguistic features and \"hard\" criteria, such as the level of detail, source reliability and language formality, rather than an understanding of veracity. We also assess the effectiveness of summarized versus full content inputs, finding that summarized content holds promise for improving efficiency without sacrificing accuracy. While GenAI has the potential to support human fact-checkers in scaling misinformation detection, our results caution against relying solely on these models.", "sections": [{"title": "1 Introduction", "content": "Generative Artificial Intelligence (GenAI) has transformed workflows across industries and academia since its surge in popularity in 2022 [1]. In scientific research, its adoption has accelerated at an unprecedented pace [25], demonstrating transformative potential in diverse applications. Researchers have successfully employed GenAI models to impersonate survey respondents [4], generate counterfactual images for experimental research [8], annotate text with human-comparable accuracy[12], identify conspiracy theories [9], and preemptively debunk election misinformation [19]. Among these diverse applications of GenAI, a fundamental question remains about its ability to assess content credibility.\nWe assess the capacity of GenAI models to rate the veracity of content. The online proliferation of unreliable and misleading information, including misinformation, poses threats to democracies and societies, spurring political violence, undermining trust in democratic institutions globally, and endangering public health [10, 17]. Combating misinformation requires reliable detection tools; however, the complexity of fact-checking remains largely beyond the capabilities of even state-of-the-art AI systems [20]. Given these limitations, one well-established alternative is to assess content reliability by using the credibility of source domains as a proxy [14, 16]. This domain-based approach raises concerns about false positives, as not all content from unreliable domains is misinformation.\nScholars have explored the potential of large language models (LLMs) as fact-checkers, building on foundational work showing that language models can verify claims without external knowledge bases [18]. For example, using datasets like news headlines labeled for credibility [11], researchers found that GenAI models without fine-tuning achieve moderate performance in detecting political misinformation [26]. This suggests that if GenAI can reliably assess credibility through zero-shot prompting, it could alleviate human resource demands in large-scale fact-checking. To advance this line of inquiry, we explore the perceived reasoning patterns that emerge during content credibility assessment in zero-shot settings, offering insights into the internal processes of GenAI. Specifically, we address the following research questions:\nRQ1 (Viability): Can GenAI models match or exceed the reliability of human coders?\nRQ2 (Efficiency): Can summaries generated by zero-shot GenAI models provide efficient credibility ratings?\nRQ3 (Functionality): How do GenAI models reason about information credibility?"}, {"title": "2 Method", "content": "Despite the ongoing debate surrounding the definition of misinformation, we adopt a broad conceptualization, encompassing both fake content and misleading framing of factual content. This approach aligns with scholarly consensus that both forms constitute misinformation [15]. For example, a video titled \"Fauci Admits On Camera That He Lied About Masks\" with the blurb \"Wow, this is getting crazy\" was labeled as low-credibility because it misrepresented Dr. Fauci's statements on mask use\u00b9.\nData: We constructed our dataset based on original data collected and presented by Biswas et al. [5]. They collected over 493k posts shared on Facebook by 5,147 state legislators between 2020 and 2021. To focus on the most misleading content, we refined the \"unreliable\" category [22], originally from Media Bias/Fact Check's (MBFC) ratings, by selecting URL domains coded as low or very low factual to create a low-factual reference. Using this reference, we identified low-factual posts. The low-factual post dataset is particularly well-suited for our study, as the shared posts by these politicians are political texts, making them highly relevant to misinformation-related social harms.\nWe sampled 500 posts from this dataset. The URLs in the sampled posts linked to text and/or video content. We scraped titles, blurbs, and content in the shared URLs. For videos, we converted them to audio files, and employed Google's audio-to-text API to generate transcripts. Due to some links being inaccessible or consisting solely of event announcements, we retained 415 available pieces of content. Before hand-coding the content, two human raters independently coded a random set of 30 posts according to our definition of misinformation. The coding achieved a high level of intercoder reliability (Cohen's Kappa = 0.8). To construct the dataset for rating, we included both link titles and their associated content. When the original content was inaccessible, we combined titles and blurbs. This decision aligns with our definition of misinformation, as well"}, {"title": "3 Results", "content": "Overall, most models followed the majority of prompts to rate content credibility and provide reasoning for their ratings. Except"}, {"title": "4 Discussion", "content": "Using a dataset of 500 samples manually coded with high intercoder reliability, we conducted experiments across five GenAI models. These models were evaluated on their ability to rate credibility using both summarized content and combined original content with titles. GenAI models exhibit moderate capacity in rating content credibility, but fall short of matching human accuracy. Among the models tested, GPT-40 demonstrated superior performance when using original content, while Gemma2-9b performed better with summarized content. GenAI models have potential to assist human fact-checkers by scaling the identification of potential misinformation, they cannot be relied upon as standalone tools."}]}