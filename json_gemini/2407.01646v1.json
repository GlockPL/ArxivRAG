[{"title": "Training of Code Summarization Model", "authors": ["Chunrong Fang", "Weisong Sun*", "Yuchen Chen", "Xiao Chen", "Zhao Wei", "Quanjun Zhang", "Yudu You", "Bin Luo", "Yang Liu", "Zhenyu Chen"], "abstract": "-(Source) code summarization aims to automatically generate succinct natural language summaries for given code snippets. Such summaries play a significant role in promoting developers to understand and maintain code. Inspired by neural machine translation, deep learning-based code summarization techniques widely adopt an encoder-decoder framework, where the encoder transforms given code snippets into context vectors, and the decoder decodes context vectors into summaries. Recently, large-scale pre-trained models for source code (e.g., CodeBERT and UniXcoder) are equipped with encoders capable of producing general context vectors and have achieved substantial improvements on the code summarization task. However, although they are usually trained mainly on code-focused tasks and can capture general code features, they still fall short in capturing specific features that need to be summarized. In a nutshell, they fail to learn the alignment between code snippets and summaries (code-summary alignment for short). In this paper, we propose a novel approach to improve code summarization based on summary-focused tasks. Specifically, we exploit a multi-task learning paradigm to train the encoder on three summary-focused tasks to enhance its ability to learn code-summary alignment, including unidirectional language modeling (ULM), masked language modeling (MLM), and action word prediction (AWP). Unlike pre-trained models that mainly predict masked tokens in code snippets, we design ULM and MLM to predict masked words in summaries. Intuitively, predicting words based on given code snippets would help learn the code-summary alignment. In addition, existing work shows that AWP affects the prediction of the entire summary. Therefore, we further introduce the domain-specific task AWP to enhance the ability of the encoder to learn the alignment between action words and code snippets. We evaluate the effectiveness of our approach, called ESALE, by conducting extensive experiments on four datasets, including two widely used datasets JCSD and PCSD, a cross-project Java dataset CPJD, and a multilingual language dataset CodeSearchNet. Experimental results show that ESALE significantly outperforms state-of-the-art baselines in all three widely used metrics, including BLEU, METEOR, and ROUGE-L. Moreover, the human evaluation proves that the summaries generated by ESALE are more informative and closer to the ground-truth summaries.", "sections": [{"title": "INTRODUCTION", "content": "ODE comments play a key role in facilitating code comprehension [1], [2], [3], [4] and software mainte- nance [5], [6], [7], [8], [9]. For example, prior works [1], [2], [10] show that code comments can help improve code readability. Commenting code has been recognized as a good programming practice [6], [8]. However, writing high-quality code comments is a labor-intensive and time- consuming task [6], [11]. As a result, good comments are often absent, unmatched, and outdated during the software evolution [12]. (Source) code summarization is an active research field [4], [9], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], which aims at designing advanced techniques to support automatic generation of code comments (also called summaries). Given a code snippet (a method or func- tion) by the developer, code summarization can generate summaries describing the functionality of the code snippet. Existing code summarization techniques can mainly be categorized into keywords-based methods, retrieval-based meth- ods, and deep learning-based methods. Keywords-based methods extract critical terms from code snippets to constitute their summaries [13], [24]. Such methods may fail to generate accurate summaries if the source code contains poorly named identifiers or method names [25], [26]. Retrieval- based methods first leverage code clone detection techniques to retrieve similar code snippets and then use their cor- responding comments to summarize other code snippets. Similar code snippets can be retrieved from existing open- source platforms (e.g., GitHub [25]) or software Q&A sites (e.g., Stack Overflow) [27]. Such methods rely on whether similar code snippets can be retrieved [28] and how similar the code snippets are [26]. In addition, code snippets may contain some information inconsistent with the content in comments of their similar code snippets [29], making retrieval-based methods ineffective in many cases. Deep learning-based methods leverage powerful generative models trained on a large-scale code-comment corpus to translate code snippets in programming languages into summaries in natural language [26], [30]. Such methods can model the semantic mapping relations between code snippets and summaries and can generate high-quality summaries [30]. Recently, with the success of the pre-training and fine-"}, {"title": "", "content": "tuning paradigm in the field of natural language processing (NLP) (e.g., BERT [31] and T5 [32]), many works in software engineering (SE) have introduced this paradigm to boost further code-related tasks, including code summarization (e.g., CodeBERT [33], CodeT5 [34], and UniXcoder [35]). In practice, these works typically pre-train a model with general language modeling tasks, such as masked lan- guage modeling (MLM) [31] and unidirectional language modeling (ULM) [36], followed by fine-tuning on code summarization tasks. In deep learning-based methods, code summarization is widely considered as the neural machine translation (NMT) task where the source text is the code snippet in programming language and the target text is the summary in natural language [4], [37], [38], [39]. Inspired by NMT, code summarization models widely adopt the encoder-decoder framework. The encoder is responsible for transforming the code snippet into a context vector. The decoder is responsible for decoding the context vector into a natural language summary. Intuitively, context vectors tell the decoder what content needs to be translated, which indicates that the encoder plays a significant role in a code summarization model. Therefore, to achieve high-quality code summarization, a good encoder should be able to produce context vectors that capture the code features that need to be translated by the decoder. However, although the advanced pre-trained encoders have achieved signifi- cant progress in producing general vector representations (i.e., context vectors) for given code snippets, they are still insufficient in capturing specific code features that need to be translated. These pre-trained encoders are primarily trained with code-focused tasks that teach them to learn the relationship among the tokens of code snippets rather than the relationship between code snippets and summaries. In other words, these pre-trained encoders are still insufficient in capturing the alignment between code snippets and sum- maries (i.e., code-summary alignment), detailed in Section 2. In this paper, we propose a novel approach to improve code summarization. Our approach is built upon large-scale pre-trained code encoders that have been shown to be su- perior in capturing and representing general code features. To improve the ability of our encoder to learn the code- summary alignment, we exploit the multi-task learning (MTL) paradigm to train it. In the MTL paradigm, multiple tasks are simultaneously learned by a shared model [40], [41], [42]. Such a paradigm can improve data efficiency, reduce overfitting through shared representations, and ac- celerate learning speed by leveraging auxiliary information. SE researchers have also introduced MTL to address SE tasks. For example, Aung et al. [43] find that both two important tasks, i.e., developer recommendation and issue type classifying involved in the bug triage process, rely on historical bug descriptions and code snippet information. Therefore, they train a multi-triage model to resolve both tasks simultaneously via MTL, and demonstrate this model can reduce task repetition and leverage the correlating information between tasks. MTL has also demonstrated promise in training a pre-trained code representation model (e.g., MulCode [44] and UniXcoder) and achieved promising efficacy on various downstream SE tasks. In our setting, we perform MTL on three summary-focused pre-training tasks, including ULM, MLM, and action word prediction"}, {"title": "", "content": "(AWP) [20]. The three tasks are simultaneously learned by a shared encoder. ULM and MLM are two general tasks borrowed from the field of NLP. ULM and MLM are im- portant because they can facilitate the language model to capture the relationships of words in the text [31], [36]. Unlike pre-trained models (e.g., CodeBERT and UniXcoder) that predict masked code tokens based on unmasked parts of code snippets, we design ULM and MLM to predict masked words in summaries based on code snippets. In- tuitively, predicting masked words in summaries based on code snippets would help the encoder learn the alignment between masked words and code snippets. In addition, existing work [20] shows that AWP affects the prediction of subsequent words and thus the prediction of the entire sum- mary. Therefore, we further introduce the domain-specific task AWP to enhance the ability of the encoder to learn the alignment between action words and code snippets. In summary, all three summary-focused tasks can enhance the encoder to learn the code-summary alignment such that it can capture the specific code features that need to be summarized. In practice, to reduce the training cost, instead of training the shared encoder from scratch, we initialize the shared encoder with an existing pre-trained encoder, e.g., UniXcoder's encoder. After obtaining the pre-trained shared encoder, we further train a code summarization model capable of generating a succinct natural language summary for a given code snippet. Specifically, we fine-tune the pre-trained shared encoder on the code summarization task and simultaneously train a decoder. In summary, we make the following contributions. \u2022 We propose a novel approach named ESALE to improve code summarization. ESALE devises three summary-focused pre-training tasks (two general tasks ULM and MLM, and one domain-specific task AWP) to enhance the encoder to learn the code- summary alignment. \u2022 We introduce a domain-specific task (i.e., AWP) as one of the important pre-training tasks. We perform an in-depth analysis of the effect of the AWP task, and statistical results show that this task can sig- nificantly improve code summarization performance (detailed in Section 4.2.2). \u2022 We conduct extensive quantitative experiments on four datasets, including two widely used Java and Python datasets (called JCSD and PCSD), a cross- projected Java dataset (called CPJD), and a multi- lingual dataset (called CSN), to evaluate ESALE. Ex- perimental results show that ESALE significantly out- performs baselines in terms of all three widely used automatic metrics BLEU, METEOR, and ROUGE-L (detailed in Section 4.2.1). The source code of ESALE and all the data used in this paper are released and can be downloaded from the website [45]. \u2022 We conduct a qualitative human evaluation to evalu- ate the summaries generated by ESALE and baselines in terms of four aspects: similarity, naturalness, infor- mativeness, and relevance. And statistical results of human scores show that the summaries generated by ESALE are more informative and closer to the ground-truth summaries (detailed in Section 4.2.4)."}, {"title": "MOTIVATING EXAMPLE", "content": "This section takes a real-world code snippet and cor- responding summaries generated by different techniques as examples to illustrate our motivation. The code snippet C1 in Fig. 1(a) is from the test set of the JCSD dataset (numbered 8,335) [12] (see details in Section 4.1.1). The first line of Fig. 1(b) shows the comment written by the developer for c1. We consider the comment as a reference summary. According to the grammar rules in natural language, we can simply divide the reference summary into three parts: \"get\" (Blue font), \"backup partitions\" (Red font), and \"for speci- fied node id\" (Orange font). Lines 2-5 show the summaries generated by baselines SiT [46], CodeBERT, UniXcoder, and our ESALE, respectively. SiT is one of the advanced code summarization techniques. CodeBERT is a representative pre-trained model for source code. UniXcoder is the state-of- the-art pre-trained model for source code. Both CodeBERT and UniXcoder can also be used for code summarization tasks by fine-tuning. More details on the three baselines are introduced in Section 4.2.1. From Fig. 1, it can be observed that compared with the reference summary, 1) SiT and CodeBERT can cover some words in the second and third parts (i.e., \"partitions\" and \"node id\"); 2) UniXcoder is better than SiT and CodeBERT, and can cover the word \"partitions\" in the second part and all words in the last part (i.e., \"for specified node id\"); 3) ESALE performs the best, successfully covering all three parts. Compared to UniX- coder, our ESALE can correctly generate the word \"backup\". To intuitively understand how ESALE can perform bet- ter in code summarization, we visualize the cross atten- tion between encoder and decoder (also called encoder- decoder attention [47]) using the attention visualization tool"}, {"title": "METHODOLOGY", "content": "Our approach produces a code summarization model via two sequential training phases: (a) training a shared en- coder, followed by (b) training a code summarization model based on the encoder generated in phase (a). In phase (a), ESALE decomposes the training procedure of the shared encoder into two steps: preprocessing and shared encoder training. In the first step, ESALE takes in pairs of code snippets and comments in the training data and produces two sequences, i.e., input sequences and masked input sequences, detailed in Section 3.2.1. In the second step, ESALE utilizes the two sequences to train a shared encoder. In this paper, we aim to enhance the encoder"}, {"title": "", "content": "to learn the code-summary alignment, thereby improving code summarization performance. Therefore, we exploit the MTL paradigm to train a shared encoder on three summary-focused tasks. Specifically, for input sequences, ESALE utilizes a shared encoder to transform them into em- bedding representations, which will be used in the AWP and ULM tasks. For masked input sequences, ESALE utilizes the shared encoder to transform them into embedding repre- sentations, which will be used in the MLM task. The models for the three tasks are jointly trained and determine the parameters of the shared encoder, detailed in Section 3.2.2. In phase (b), after getting the pre-trained shared encoder, ESALE fine-tunes it and trains a decoder simultaneously on the downstream code summarization task. The fine- tuned shared encoder and fine-tuned decoder compose a well-trained code summarization model, detailed in Sec- tion 3.3. When the well-trained code summarization model is deployed online, it can take in a code snippet given by the developer and generate a natural language summary, detailed in Section 3.4."}, {"title": "Training of Shared Encoder", "content": "ESALE takes in raw training data where each sample consists of a code snippet and its corresponding comment. ESALE follows common practices [33], [35], [51] and uses the tok- enizer provided by Roberta [52] to tokenize code snippets and comments and produce token sequences and word sequences, respectively. We also use Byte Pair Encoding (BPE) within Roberta to split tokens into subtokens. As [9], we call the basic unit of preprocessed source code a token and the basic unit of summary a word. ESALE further masks parts of words in word sequences to produce masked word sequences. Specifically, we follow existing works [33], [35] and randomly choose 15% of the words in a word sequence, and change them into a special token . Next, two special tokens  and  are added at the beginning and the end of the token sequences, respectively. The special token  is appended as a suffix for word sequences and masked word sequences. Then we concatenate pairs of token sequences and word sequences to produce input sequences. We concatenate pairs of token sequences and masked word sequences to produce masked input sequences. Unlike pre-trained models (e.g., CodeBERT and UniXcoder) that first concatenate pairs of token se- quences and word sequences and then mask parts of the input sequences, we only mask parts of word sequences. Both input sequences and masked input sequences will be used to train the shared encoder in the second step."}, {"title": "Shared Encoder Training", "content": "Shared Encoder. The shared encoder is a deep neural net- work or pre-trained model responsible for transforming the input sequences into vector representations (i.e., embed- dings). In practice, we build our shared encoder upon the existing pre-trained encoder. There are two benefits to doing this: 1) compared to training the encoder from scratch, the scheme based on the pre-trained encoders can significantly reduce the training cost; 2) existing pre-trained encoders"}, {"title": "", "content": "have achieved almost optimal performance on the code summarization task, providing a high starting point. Specifically, we tried to build our shared encoder upon two pre-trained encoders provided by CodeBERT and UniX- coder. We first initialize the shared encoder with the pa- rameters of their pre-trained encoders. Then, we fine-tune the shared encoder with three summary-focused tasks, i.e., AWP, ULM, and MLM. The experimental results show that the shared encoder built upon UniXcoder's encoder is better than that built on CodeBERT's encoder on the downstream code summarization task, detailed in Section 4.2.1. Next, we introduce the design of the three summary- focused tasks. (i) AWP. An \u201caction word\u201d in a summary is typically a verb that broadly classifies what the code does [20], such as \"get\", \"add\", and \"remove\". Programmers tend to write summaries containing only one action word, typically positioned at the beginning of the summary sentence (i.e., the first word). AWP is a classification task, where the input of the model is a code snippet, and the output is the predicted label with respect to the action word [20]. In this paper, we use this task to train a model capable of predicting the action words of summaries based on given code snippets. Formally, let c = {t1, t2,..., tm} denote the token sequence of the code snippet, where m is the length of the token sequence, and y = {Y1, Y2,..., tc } denote the set of possible classes, where C is the number of classes of action words. The summary- focused AWP can be defined as follows: Definition 1 (Summary-focused AWP). A summary-focused AWP is a multi-classification task denoted as \u0177 = arg maxyey P(y|c), where: \u2022 P(y c) represents the probability of the class y given the code snippet c. \u2022 arg max denotes the operation that selects the class label with the highest probability. The model we train is composed of the shared encoder and a classification layer. The classification layer is a fully connected network of size N * C, where N is the output size of the shared encoder. Given an input sequence x, we first utilize the shared encoder to transform x into the embedding ex. Then, a classification layer is used to classify ex into predicted action word classes. Given the embedding ex, we obtain the logits by \u0177\u2081 = Wex + b, where W is the weight matrix and b is the bias term. We optimize the model by minimizing the categorical cross-entropy loss: $L_{AWP}(\\Theta) = - \\sum_{i=1}^{C} y_i \\log \\frac{exp(\\hat{y_i})}{\\sum_{j=1}^{C} exp(\\hat{y_j})}$ (1) where denotes trainable parameters of the model (i.e., W and b); \u0177i and yi are the predicted score and target score for each class i \u2208 C. In practice, we follow [20] and use the top-40 setting; that is, the model attempts to predict the forty most-common action words, or \u201cother\u201d if predicted to be a less-common action word. The top 40 action words are selected based on their frequency in the comments of all samples in each dataset. Here, we give a brief explanation of why we consider AWP as one of the pre-training tasks. First, the production"}, {"title": "", "content": "of good summaries relies on the production of the action words in those summaries. Code summarization models are widely built on the encoder-decoder framework, where the decoder predicts words one by one according to previous words and the context produced by the encoder. So, if the first word is wrong, it is difficult for the decoder to predict the entire summary correctly. This situation can be exaggerated by the aggressive use of attention mechanisms, which can attend previous words in the predicted summary to parts of the code snippet [20]. Therefore, it is crucial for code summarization models to predict accurate action words. Second, our experiments found that ESALE equipped with AWP performs better than without. In practice, before deciding to add AWP as one of the pre-training tasks, we also followed [20] and conducted experiments on the perfor- mance of encoders of different seq2seq models on the AWP task with 41 classes. We compared five techniques, includ- ing AttendGRU, CodeBERT, UniXcoder, ESALE w/o AWP, and ESALE. AttendGRU is representative of seq2seq-like approaches as proposed by Iyer et al. [30]. In the paper [20], AttendGRU performs the best, so we also consider it as a baseline. For AttendGRU, we build a classification model by appending a fully connected network to its encoder, and train the model from scratch. For CodeBERT, UniXcoder, ESALE w/o AWP, and ESALE, we build classification models by appending a fully connected network to their encoders as classification layers and train the models by fine-tuning."}, {"title": "", "content": "(ii) ULM. The ULM task is defined as the problem of left-to-right language modeling [36]", "follows": "Definition 2 (Summary-focused ULM). A summary-focused ULM is a probabilistic model denoted as P(W1", "where": "P(Wi Wi-1", "function": "L_{ULM"}, "Theta) = \\sum_{i=0}^{n-1}logp(w_i|e_{\n    },\n    {\n      \"title\":", ",\n      \"content\": ", "The model we train is composed of a shared encoder and a fully connected network. The design of the fully connected network is the same as in the summary- focused ULM task. Given a masked input sequence x, we first use the shared encoder to transform x into embedding ex. Then, the fully connected network is used to predict the likelihood score of each word in the vocabulary being the masked word. We optimize the model by minimizing the following objective function: $L_{MLM}(\\Theta) = \\sum_{w_i \\in S_m}logp(w_i|e_{mask})$ (3) where e mask is the embedding of x; Sm is the set of masked words that need to be predicted. We use Fig. 4 to visually illustrate the differences in the masked proportion and position between the baselines (i.e., CodeBERT and UniXcoder) and our ESALE. Fig. 4(a) shows an example of an original input sequence consisting of a code token sequence and a comment word sequence. Fig. 4(b)-(d) show the masked input sequences used in CodeBERT, UniXcoder, and ESALE, respectively. In Fig. 4(b), we follow CodeBERT and randomly replace 15% of the tokens in the input sequence with [MASK] tokens (the blue blocks labeled [M] in the figure). In Fig. 4(c), we follow UniXcoder and first sample 15% of the tokens from the input sequence, and then randomly replace 80% (i.e., about 10% of the input sequence) of them with a [MASK] token and leave another 10% of them unchanged. In Fig. 4(d),"], "content": "After obtaining the pre-trained shared encoder, we further train a code summarization model capable of generating a succinct natural language summary for a given code snippet. Specifically, we fine-tune the pre-trained shared encoder on the code summarization task and simultane- ously train a decoder. Given training data consisting of pairs of code snippets and comments, ESALE first leverages the pre-trained shared encoder to transform code snippets into context vectors eCode. Then, ESALE leverages a decoder to generate predicted summaries. The decoder takes in eCode and predicts words one by one, detailed in Section 3.3.1). Fi- nally, ESALE computes the loss (Lcs (\u04e8)) based on predicted summaries and ground-truth summaries (i.e., comments) and iteratively updates the model parameters, detailed in Section 3.3.2)."}, {"title": "Decoder", "content": "In this step, we utilize the decoder to generate natural language summaries. The decoder takes in the context vec- tors eCode and predicts words one by one. Specifically, the decoder based on a neural network (e.g., LSTM [56] and Transformer [47]) is to unfold the context vectors eCode into the target sequence (i.e., the word sequence of the summary) through the following dynamic model, $h_t = f(y_{t-1}, h_{t-1}, eCode)$ $p(y_t|Y_{<t}, X) = g(y_{t-1}, h_t, eCode)$ (5) where f(.) and g(\u00b7) are activation functions, ht is the hidden state of the neural network at time t, yt is the predicted target word at t (through g(\u00b7) with Y<t denoting the his- tory {Y1, Y2,\u2026, Yt\u22121}. The prediction process is typically"}, {"title": "", "content": "a classifier over the vocabulary. It can be seen from Equa- tion (5) that the probability of generating the target word is related to the current hidden state, the history of the target sequence, and the context vectors eCode. The essence of the decoder is to classify the vocabularies by optimizing the loss function in order to generate the vector representing the feature of the target word yt. After the vector passes through a softmax function, the word corresponding to the highest probability is the result to be output. In practice, we design our decoders with different schemes suggested by CodeBERT and UniXcoder, respec- tively. CodeBERT only provides a pre-trained encoder, while UniXcoder provides a pre-trained encoder and a pre-trained decoder. Therefore, when the shared encoder is built upon the pre-trained encoder provided by CodeBERT, we build our decoder upon Transformer [47]. When the shared en- coder is built upon the pre-trained encoder provided by UniXcoder, we build our decoder upon the pre-trained decoder provided by UniXcoder."}, {"title": "Model Training", "content": "During the training of the code summarization model, the two components (pre-trained shared encoder and decoder) are jointly trained to minimize the following objective func- tion: $L_{cs}(\\Theta) = - \\frac{1}{N} \\sum_{n=1}^{N} logp(Y_n|x_n)$ (6) where is the model parameters of the code summarization model, and each (xn, Yn) is a (code snippet, comment) pair from the training set."}, {"title": "Deployment of Code Summarization Model", "content": "After the model is trained, we can deploy it online for code summarization service. For a code snippet e given by the developer, ESALE first uses the fine-tuned encoder to trans- form cinto a context vector, which will be fed to the fine- tuned decoder to generate a summary in natural language. In practice, we can consider the well-trained ESALE as a black-box tool that takes in a code snippet and generates a succinct natural language summary."}, {"title": "EVALUATION", "content": "To evaluate our approach, in this section, we aim to answer the following four research questions: RQ1: How does ESALE perform compared to the state- of-the-art baselines? RQ2: How do the three pre-trained tasks (i.e., AWP, ULM, and MLM) affect the performance of ESALE (ablation study)? RQ3: How does the robustness of ESALE perform when varying the code length and comment length? RQ4: How does ESALE perform in human evaluation?"}, {"title": "Experimental Setup", "content": "We conduct experiments on four datasets. JCSD provided by Hu et al. [12] is a Java dataset. PCSD provided by"}, {"title": "RQ1: ESALE vs. Baselines", "content": "Re\u00b2Com [29] adopts an LSTM-based encoder-decoder architecture with an attention mechanism. It first uses an information retrieval technique to retrieve a similar code snippet and treat its comment as an exemplar. Then, it uses an LSTM-based seq2seq neural network that takes the given code, its AST, its similar code, and its exemplar as input, and leverages the information from the exemplar to generate summaries. SiT [46] adopts a Transformer-based encoder-decoder architecture. It proposes a structure-induced transformer to capture long-range dependencies and more global informa- tion in AST sequences of code snippets. SCRIPT [61] adopts a Transformer-based encoder- decoder architecture. It proposes two types of Transformer encoders to capture the structural relative positions between tokens for better learning code semantics. In addition to these non-pre-trained techniques above, since our method is based on the pre-training and fine- tuning paradigm, we also compare two techniques follow- ing such a paradigm. CodeBERT [33] is a representative pre-trained model of code. It is trained with the MLM and Replaced Token Detection (RTD) tasks. The authors of CodeBERT fine-tune and test it on the code summarization task (also called the code documentation generation task in their paper). UniXcoder [35] is the state-of-the-art pre-trained model of code. It is trained with four tasks: MLM, ULM, Denoising"}, {"title": "", "content": "Objective DeNoiSing (DNS), and Code Fragment Represen- tation Learning. Unlike CodeBERT which only pre-trains the encoder, UniXcoder pre-trains both the encoder and the decoder. The authors of UniXcoder also fine-tune and test it on the code summarization task. For non-pre-training-based models (i.e., Re\u00b2Com, SiT, and SCRIPT) and pre-training-based models (i.e., Code- BERT and UniXcoder), we train and fine-tune them sep- arately on the training set of each code summarization dataset, and evaluate them on the corresponding test set. 2) Results: TABLE 4 shows the performances of our ESALE and baselines in terms of the three evaluation metrics, i.e., BLEU, METEOR, and ROUGE-L. In TABLE 4, \"ESALE + CodeBERT\" refers to that we build the shared encoder based on the pre-trained encoder provided by CodeBERT. Analogously, in \u201cESALE + UniXcoder\u201d, the shared encoder and decoder are built upon the pre-trained encoder and decoder provided by UniXcoder. In practice, we initialize our encoder and decoder with the model parameters of the CodeBERT and UniXcoder. From TABLE 4, it can be observed that, in all non- pre-training baselines, SCRIPT and SiT perform the best on JCSD and PCSD datasets in terms of all three met- rics, respectively. However, SCRIPT requires complex pre- processing for code snippets and does not release pre- processing code implementation. Thus, we re-run SCRIPT on their preprocessed datasets. Although the preprocessed datasets are derived from JCSD and PCSD datasets, they have different training and test sets. Therefore, we mainly compare our ESALE to SiT in subsequent sections. ESALE built on CodeBERT or UniXcoder is more powerful than SiT and achieves more impressive performance. On the JCSD dataset, compared with SiT, ESALE built on UniX- coder improves by 6.83% in BLEU, 13.62% in METEOR, and 6.39% in ROUGE-L. On the PCSD dataset, ESALE built on UniXcoder also clearly outperforms SiT, improving by 7.73% in BLEU, 12.27% in METEOR, and 6.23% in ROUGE- L. Because ESALE built upon UniXcoder performs the best, unless explicitly stated, ESALE appearing alone refers to \u201cESALE + UniXcoder\" by default. In addition, it can be observed that, our method consis- tently improves the performance of the original pre-trained models, i.e., CodeBERT and UniXcoder on both datasets in general. It should be noted that the values in TABLE 4 are the average scores of all test samples. For a more compre- hensive comparison, we further compare the distribution of the scores of CodeBERT, UniXcoder and ESALE on all test samples,"}, {"title": "RQ2: Effect of Each Pre-train Tasks (Ablation Study)", "content": "We use three tasks (AWP, MLM, and ULM) to enhance the ability of our model to learn code-summary alignment. We conduct ablation studies to reveal the influence of each task on the performance of ESALE. The study results are shown in TABLE 10, in which \u201cESALE w/o AWP\u201d, \u201cESALE w/o MLM\", and \"ESALE w/o ULM\" mean that we train ESALE without the AWP, MLM, and ULM tasks, respectively. It is observed that the performance of ESALE degrades when any of the three tasks are ignored. Therefore, it can be concluded that all three tasks play an important role in improving the code summarization performance of ESALE.\""}, {"title": "", "content": "In addition, from TABLE 10, it can be observed that the AWP task has the most significant effect on the performance of ESALE. We further delve into the contribution of the AWP task to ESALE, which is a task especially designed for the code summarization [20]. InTABLE 11, the second column shows the total number of samples in the JCSD and PCSD test set, and the \u201cNum.\u201d and \u201cPro.\u201d columns show the number and proportion of samples whose action words are included in the top-40 common list, respectively. From this table, it can be observed that the top-40 setting only covers about 61% of samples in test sets. In other words, many action words (in the remaining 39% samples) are still not included. We further compute the AWP accuracy of ESALE, and the results are shown in the \u201cAWP Acc.\u201d column of TABLE 11. It can be observed that the average AWP accuracy of ESALE is 64.89%. We also check whether the performance of ESALE can be improved when the action words are correctly predicted. TABLE 12 shows the results of ESALE and ESALE w/o AWP on the samples whose action words are included in the top 40 common list and predicted correctly. In TABLE 12, \u201cESALE w/o AWP\u201d means that we train ESALE without the AWP task; \u201c# Improved\u201d denotes the number of the samples for which ESALE can generate higher quality summaries (i.e., larger BLEU-4, METEOR, and ROUGE-L) when correctly predicting their action words. From TABLE 12, it can be observed that ESALE can generate higher quality summaries for 914 Java and 2,283 Python code snippets on average when predict- ing their action words correctly. For each metric, we also perform the paired Wilcoxon-Mann-Whitney signed-rank test on all scores got by ESALE w/o AWP and ESALE at a significance level of 5%. The test results are presented in the \"p-value\" columns of TABLE 12. We can intuitively observe that in all three metrics, ESALE outperforms ESALE w/o AWP on both the JCSD and PCSD datasets, which means the AWP plays a significant role in facilitating ESALE to generate high-quality summaries, as claimed by [20]."}, {"title": "RQ3: Robustness of ESALE", "content": "To analyze the robustness of ESALE, we study two pa- rameters (i.e., code length and comment length) that may influence the embedding representations of code snippets and comments. Fig. 7 shows the length distributions of code snippets and comments on the test sets of the JCSD and PCSD datasets. For a code snippet, its length refers to the lines of the code snippet. For a comment, its length refers to the number of words in the comment. From Fig. 7 (a) and (c), it can be observed that most code snippets are between 20 and 40 lines. From Fig. 7 (b) and (d), it is noticed that almost all comments are less than 20 in length. This also confirms the challenge of capturing the correlation between the long code snippet and its short comment (summary). Fig. 8 shows the performance of two best baselines (i.e., CodeBERT and UniXcoder) and ESALE based on the BLEU metric with varying parameters. In this figure, the version of"}, {"title": "Human Evaluation", "content": "Many works [21], [22], [28], [29], [30], [39], [58], [80] find that it is not enough to use only automatic evaluation be- cause the automatic metrics (BLEU, METEOR, and ROUGE- L) mainly calculate the textual similarity rather than the semantic similarity between the generated summaries and the reference summaries. Hence, we conduct a human eval- uation by following the previous works [29], [39], [46], [58], [80] to evaluate the summaries generated by SiT, UniXcoder, and our ESALE. The selection of SiT and UniXcoder as com- parison models stems from their status as advanced code summarization techniques representing distinct paradigms: SiT is a non-pre-training-based technique, while UniXcoder is a pre-training-based technique. Specifically, we invite ten volunteers with more than three years of programming experience and excellent English skills to carry out the eval- uation. Each volunteer scores the generated summaries from 0 to 4 (the higher, the better) from four aspects: similarity (similarity of the generated summaries and the reference summaries), naturalness (grammaticality and fluency), in- formativeness (the amount of content carried over from the input code snippets to the generated summaries, ignoring fluency) and relevance (the degree to which the generated summaries are relevant with the input code snippets). We randomly select 100 code snippets, including 50 from the JCSD dataset and 50 from the PCSD dataset, the corre- sponding summaries generated by SiT, UniXcoder, and our ESALE, and the reference summaries (i.e., ground-truth), re- spectively. We divide the 100 samples into two groups, and each of them includes 50 samples, of which 25 belong to the JCSD dataset and 25 belong to the PCSD dataset. We place all samples to be evaluated in text files. Each participant is provided with two files containing 25 samples randomly selected from the test set of the JCSD and PCSD datasets,"}, {"title": "CASE STUDY", "content": "In this section, we provide case studies to understand the generated summaries of ESALE compared to SiT, CodeBERT, and UniXcoder. Fig. 10 shows two real-world examples c4 and c5 from the test sets of the JCSD and PCSD datasets, respectively. Fig. 10(b) shows the reference summary of c4 (line 1) and summaries generated by SiT, CodeBERT, UniXcoder, and our ESALE for c4 (lines 2-5). According to the grammar rules in natural language, we can simply divide the reference summary into three parts: \u201cexpand\u201d (Blue font), \u201call paths\u201d (Red), and \u201cin the tree\u201d (Orange font). It can be observed, compared with the reference summary, 1) both SiT and CodeBERT only correctly cover the first part (i.e., \u201cexpand\u201d); 2) UniXcoder can cover the last two parts (i.e., \"all paths\" and \"in the tree\"); 3) our ESALE can successfully cover all three parts. Similarly, for the python code snippet c5 in Fig. 10(c), our ESALE can successfully cover all three parts of the reference summary, as shown in Fig. 10(d), while SiT only covers the first part (i.e., \u201csearch for\u201d) and CodeBERT and UniXcoder can cover the first two parts (i.e., \"search for\" and \"artists\"). In summary, based on the above two examples and observations, it can be concluded that our ESALE outperforms SiT, CodeBERT, and UniXcoder in learn- ing the mapping between code snippets and summaries, and has more a powerful code summarization performance."}, {"title": "THREATS TO VALIDITY", "content": "There are three main threats to validity. First, we cannot guarantee that the scores of human evaluation are fair. To mitigate this threat, we evaluate every generated summary by five evaluators and use the average score of the five evaluators as the final result. In addition, the standard deviations of all techniques are small (less than 0.7), indicating that scores by humans are about the same degree of concentration (detailed in Section 4.2.4). Second, in neural network model design, there are many orthogonal aspects such as different token embeddings, whether to use beam search or teacher forcing. When show- ing the generality of ESALE, we have done the experiments in a controlled way. A future work is to do all experiments in a more controlled way, and the performance of ESALE could rise further when combined with all other orthogonal techniques. Third, we use datasets across six programming lan- guages to validate the effectiveness of ESALE we proposed in this paper. Although ESALE only takes token sequences of code snippets as input and does not require other complex code features (e.g., AST and CFG), we do not know whether ESALE is equally applicable to other programming language summarization tasks. Therefore, it is necessary to conduct experiments on more programming language datasets (e.g., C/C++) to verify the reliability of ESALE."}, {"title": "RELATED WORK", "content": "Code summaries can help developers quickly understand the functionalities of the program. More and more re- searchers are exploring code summarization techniques, which aim to automatically generate code summaries. Nowadays, NMT-based models have been widely used to generate summaries for code snippets with encoder- decoder neural networks [4], [26], [30], [37], [39], [46], [61], [73], [82], [83], [84], [85], [86]. For example, Iyer et al. [30] are the first to apply deep learning to automatic code summarization. They adopt LSTM networks [56] with attention to leverage the code vectors and generate nat- ural language sentences that describe C# code snippets and SQL queries. Hu et al. [12] use one additional en- coder to encode API sequences and improve the summary generation by learning the API knowledge. Subsequently, various additional information is incorporated to further improve DL-based code summarization performance, such as abstract syntax trees (ASTs) [37], [38], [46], [61], [69], [73], [83], value flows [87], data flow graph [88], code prop- erty graphs [89], similar code snippets [28], [29], important code statements [86], [90], file context [91], project-specific knowledge [92], etc. In addition, with the success of the pre-training and fine-tuning paradigm in the field of NLP (e.g., BERT [31] and T5 [32]), many works have introduced this paradigm to further boost neural code summarization, such as CodeBERT [33], CodeT5 [34], and UniXcoder [35]. These works first pre-train a model with general language modeling tasks, such as MLM and ULM. Then, they fine- tune the pre-trained models on code summarization [33],"}, {"title": "CONCLUSION", "content": "In this paper, we propose an approach for code summa- rization, namely ESALE, which improves the code summa- rization performance by enhancing the encoder to code- summary alignment. ESALE is first trained using a multi- task learning paradigm with three summary-focused tasks, and then fine-tuned on the code summarization task. We conduct quantitative comprehensive experiments and qual- itative human evaluations to verify the effectiveness of ESALE. And all results show that our ESALE is significantly better than state-of-the-art baselines."}]