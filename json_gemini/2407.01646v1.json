{"title": "ESALE: Enhancing Code-Summary Alignment Learning for Source Code Summarization", "authors": ["Chunrong Fang", "Weisong Sun*", "Yuchen Chen", "Xiao Chen", "Zhao Wei", "Quanjun Zhang", "Yudu You", "Bin Luo", "Yang Liu", "Zhenyu Chen"], "abstract": "(Source) code summarization aims to automatically generate succinct natural language summaries for given code snippets. Such summaries play a significant role in promoting developers to understand and maintain code. Inspired by neural machine translation, deep learning-based code summarization techniques widely adopt an encoder-decoder framework, where the encoder transforms given code snippets into context vectors, and the decoder decodes context vectors into summaries. Recently, large-scale pre-trained models for source code (e.g., CodeBERT and UniXcoder) are equipped with encoders capable of producing general context vectors and have achieved substantial improvements on the code summarization task. However, although they are usually trained mainly on code-focused tasks and can capture general code features, they still fall short in capturing specific features that need to be summarized. In a nutshell, they fail to learn the alignment between code snippets and summaries (code-summary alignment for short). In this paper, we propose a novel approach to improve code summarization based on summary-focused tasks. Specifically, we exploit a multi-task learning paradigm to train the encoder on three summary-focused tasks to enhance its ability to learn code-summary alignment, including unidirectional language modeling (ULM), masked language modeling (MLM), and action word prediction (AWP). Unlike pre-trained models that mainly predict masked tokens in code snippets, we design ULM and MLM to predict masked words in summaries. Intuitively, predicting words based on given code snippets would help learn the code-summary alignment. In addition, existing work shows that AWP affects the prediction of the entire summary. Therefore, we further introduce the domain-specific task AWP to enhance the ability of the encoder to learn the alignment between action words and code snippets. We evaluate the effectiveness of our approach, called ESALE, by conducting extensive experiments on four datasets, including two widely used datasets JCSD and PCSD, a cross-project Java dataset CPJD, and a multilingual language dataset CodeSearchNet. Experimental results show that ESALE significantly outperforms state-of-the-art baselines in all three widely used metrics, including BLEU, METEOR, and ROUGE-L. Moreover, the human evaluation proves that the summaries generated by ESALE are more informative and closer to the ground-truth summaries.", "sections": [{"title": "INTRODUCTION", "content": "CODE comments play a key role in facilitating code comprehension [1], [2], [3], [4] and software mainte-nance [5], [6], [7], [8], [9]. For example, prior works [1], [2], [10] show that code comments can help improve code readability. Commenting code has been recognized as a good programming practice [6], [8]. However, writing high-quality code comments is a labor-intensive and time-consuming task [6], [11]. As a result, good comments are often absent, unmatched, and outdated during the software evolution [12]. (Source) code summarization is an active research field [4], [9], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], which aims at designing advanced techniques to support automatic generation of code comments (also called summaries). Given a code snippet (a method or func-tion) by the developer, code summarization can generate summaries describing the functionality of the code snippet. Existing code summarization techniques can mainly be categorized into keywords-based methods, retrieval-based meth-ods, and deep learning-based methods. Keywords-based methods extract critical terms from code snippets to constitute their summaries [13], [24]. Such methods may fail to generate accurate summaries if the source code contains poorly named identifiers or method names [25], [26]. Retrieval-based methods first leverage code clone detection techniques to retrieve similar code snippets and then use their cor-responding comments to summarize other code snippets. Similar code snippets can be retrieved from existing open-source platforms (e.g., GitHub [25]) or software Q&A sites (e.g., Stack Overflow) [27]. Such methods rely on whether similar code snippets can be retrieved [28] and how similar the code snippets are [26]. In addition, code snippets may contain some information inconsistent with the content in comments of their similar code snippets [29], making retrieval-based methods ineffective in many cases. Deep learning-based methods leverage powerful generative models trained on a large-scale code-comment corpus to translate code snippets in programming languages into summaries in natural language [26], [30]. Such methods can model the semantic mapping relations between code snippets and summaries and can generate high-quality summaries [30]. Recently, with the success of the pre-training and fine-"}, {"title": "INTRODUCTION", "content": "tuning paradigm in the field of natural language processing (NLP) (e.g., BERT [31] and T5 [32]), many works in software engineering (SE) have introduced this paradigm to boost further code-related tasks, including code summarization (e.g., CodeBERT [33], CodeT5 [34], and UniXcoder [35]). In practice, these works typically pre-train a model with general language modeling tasks, such as masked lan-guage modeling (MLM) [31] and unidirectional language modeling (ULM) [36], followed by fine-tuning on code summarization tasks. In deep learning-based methods, code summarization is widely considered as the neural machine translation (NMT) task where the source text is the code snippet in programming language and the target text is the summary in natural language [4], [37], [38], [39]. Inspired by NMT, code summarization models widely adopt the encoder-decoder framework. The encoder is responsible for transforming the code snippet into a context vector. The decoder is responsible for decoding the context vector into a natural language summary. Intuitively, context vectors tell the decoder what content needs to be translated, which indicates that the encoder plays a significant role in a code summarization model. Therefore, to achieve high-quality code summarization, a good encoder should be able to produce context vectors that capture the code features that need to be translated by the decoder. However, although the advanced pre-trained encoders have achieved signifi-cant progress in producing general vector representations (i.e., context vectors) for given code snippets, they are still insufficient in capturing specific code features that need to be translated. These pre-trained encoders are primarily trained with code-focused tasks that teach them to learn the relationship among the tokens of code snippets rather than the relationship between code snippets and summaries. In other words, these pre-trained encoders are still insufficient in capturing the alignment between code snippets and sum-maries (i.e., code-summary alignment), detailed in Section 2. In this paper, we propose a novel approach to improve code summarization. Our approach is built upon large-scale pre-trained code encoders that have been shown to be su-perior in capturing and representing general code features. To improve the ability of our encoder to learn the code-summary alignment, we exploit the multi-task learning (MTL) paradigm to train it. In the MTL paradigm, multiple tasks are simultaneously learned by a shared model [40], [41], [42]. Such a paradigm can improve data efficiency, reduce overfitting through shared representations, and ac-celerate learning speed by leveraging auxiliary information. SE researchers have also introduced MTL to address SE tasks. For example, Aung et al. [43] find that both two important tasks, i.e., developer recommendation and issue type classifying involved in the bug triage process, rely on historical bug descriptions and code snippet information. Therefore, they train a multi-triage model to resolve both tasks simultaneously via MTL, and demonstrate this model can reduce task repetition and leverage the correlating information between tasks. MTL has also demonstrated promise in training a pre-trained code representation model (e.g., MulCode [44] and UniXcoder) and achieved promising efficacy on various downstream SE tasks. In our setting, we perform MTL on three summary-focused pre-training tasks, including ULM, MLM, and action word prediction"}, {"title": "INTRODUCTION", "content": "(AWP) [20]. The three tasks are simultaneously learned by a shared encoder. ULM and MLM are two general tasks borrowed from the field of NLP. ULM and MLM are im-portant because they can facilitate the language model to capture the relationships of words in the text [31], [36]. Unlike pre-trained models (e.g., CodeBERT and UniXcoder) that predict masked code tokens based on unmasked parts of code snippets, we design ULM and MLM to predict masked words in summaries based on code snippets. In-tuitively, predicting masked words in summaries based on code snippets would help the encoder learn the alignment between masked words and code snippets. In addition, existing work [20] shows that AWP affects the prediction of subsequent words and thus the prediction of the entire sum-mary. Therefore, we further introduce the domain-specific task AWP to enhance the ability of the encoder to learn the alignment between action words and code snippets. In summary, all three summary-focused tasks can enhance the encoder to learn the code-summary alignment such that it can capture the specific code features that need to be summarized. In practice, to reduce the training cost, instead of training the shared encoder from scratch, we initialize the shared encoder with an existing pre-trained encoder, e.g., UniXcoder's encoder. After obtaining the pre-trained shared encoder, we further train a code summarization model capable of generating a succinct natural language summary for a given code snippet. Specifically, we fine-tune the pre-trained shared encoder on the code summarization task and simultaneously train a decoder. In summary, we make the following contributions.\nWe propose a novel approach named ESALE to improve code summarization. ESALE devises three summary-focused pre-training tasks (two general tasks ULM and MLM, and one domain-specific task AWP) to enhance the encoder to learn the code-summary alignment.\nWe introduce a domain-specific task (i.e., AWP) as one of the important pre-training tasks. We perform an in-depth analysis of the effect of the AWP task, and statistical results show that this task can sig-nificantly improve code summarization performance (detailed in Section 4.2.2).\nWe conduct extensive quantitative experiments on four datasets, including two widely used Java and Python datasets (called JCSD and PCSD), a cross-projected Java dataset (called CPJD), and a multi-lingual dataset (called CSN), to evaluate ESALE. Ex-perimental results show that ESALE significantly out-performs baselines in terms of all three widely used automatic metrics BLEU, METEOR, and ROUGE-L (detailed in Section 4.2.1). The source code of ESALE and all the data used in this paper are released and can be downloaded from the website [45].\nWe conduct a qualitative human evaluation to evalu-ate the summaries generated by ESALE and baselines in terms of four aspects: similarity, naturalness, infor-mativeness, and relevance. And statistical results of human scores show that the summaries generated by ESALE are more informative and closer to the ground-truth summaries (detailed in Section 4.2.4)."}, {"title": "MOTIVATING EXAMPLE", "content": "This section takes a real-world code snippet and cor-responding summaries generated by different techniques as examples to illustrate our motivation. The code snippet C1 in Fig. 1(a) is from the test set of the JCSD dataset (numbered 8,335) [12] (see details in Section 4.1.1). The first line of Fig. 1(b) shows the comment written by the developer for c1. We consider the comment as a reference summary. According to the grammar rules in natural language, we can simply divide the reference summary into three parts: \"get\" (Blue font), \"backup partitions\" (Red font), and \"for speci-fied node id\" (Orange font). Lines 2-5 show the summaries generated by baselines SiT [46], CodeBERT, UniXcoder, and our ESALE, respectively. SiT is one of the advanced code summarization techniques. CodeBERT is a representative pre-trained model for source code. UniXcoder is the state-of-the-art pre-trained model for source code. Both CodeBERT and UniXcoder can also be used for code summarization tasks by fine-tuning. More details on the three baselines are introduced in Section 4.2.1. From Fig. 1, it can be observed that compared with the reference summary, 1) SiT and CodeBERT can cover some words in the second and third parts (i.e., \"partitions\" and \"node id\"); 2) UniXcoder is better than SiT and CodeBERT, and can cover the word \"partitions\" in the second part and all words in the last part (i.e., \"for specified node id\"); 3) ESALE performs the best, successfully covering all three parts. Compared to UniX-coder, our ESALE can correctly generate the word \"backup\". To intuitively understand how ESALE can perform bet-ter in code summarization, we visualize the cross atten-tion between encoder and decoder (also called encoder-decoder attention [47]) using the attention visualization tool"}, {"title": "METHODOLOGY", "content": "Our approach produces a code summarization model via two sequential training phases: (a) training a shared en-coder, followed by (b) training a code summarization model based on the encoder generated in phase (a). In phase (a), ESALE decomposes the training procedure of the shared encoder into two steps: preprocessing and shared encoder training. In the first step, ESALE takes in pairs of code snippets and comments in the training data and produces two sequences, i.e., input sequences and masked input sequences, detailed in Section 3.2.1. In the second step, ESALE utilizes the two sequences to train a shared encoder. In this paper, we aim to enhance the encoder"}, {"title": "Training of Shared Encoder", "content": "ESALE takes in raw training data where each sample consists of a code snippet and its corresponding comment. ESALE follows common practices [33], [35], [51] and uses the tok-enizer provided by Roberta [52] to tokenize code snippets and comments and produce token sequences and word sequences, respectively. We also use Byte Pair Encoding (BPE) within Roberta to split tokens into subtokens. As [9], we call the basic unit of preprocessed source code a token and the basic unit of summary a word. ESALE further masks parts of words in word sequences to produce masked word sequences. Specifically, we follow existing works [33], [35] and randomly choose 15% of the words in a word sequence, and change them into a special token <MASK>. Next, two special tokens <SOS> and <EOS> are added at the beginning and the end of the token sequences, respectively. The special token <EOS> is appended as a suffix for word sequences and masked word sequences. Then we concatenate pairs of token sequences and word sequences to produce input sequences. We concatenate pairs of token sequences and masked word sequences to produce masked input sequences. Unlike pre-trained models (e.g., CodeBERT and UniXcoder) that first concatenate pairs of token se-quences and word sequences and then mask parts of the input sequences, we only mask parts of word sequences. Both input sequences and masked input sequences will be used to train the shared encoder in the second step."}, {"title": "Shared Encoder Training", "content": "Shared Encoder. The shared encoder is a deep neural net-work or pre-trained model responsible for transforming the input sequences into vector representations (i.e., embed-dings). In practice, we build our shared encoder upon the existing pre-trained encoder. There are two benefits to doing this: 1) compared to training the encoder from scratch, the scheme based on the pre-trained encoders can significantly reduce the training cost; 2) existing pre-trained encoders"}, {"title": "Training of Shared Encoder", "content": "have achieved almost optimal performance on the code summarization task, providing a high starting point. Specifically, we tried to build our shared encoder upon two pre-trained encoders provided by CodeBERT and UniX-coder. We first initialize the shared encoder with the pa-rameters of their pre-trained encoders. Then, we fine-tune the shared encoder with three summary-focused tasks, i.e., AWP, ULM, and MLM. The experimental results show that the shared encoder built upon UniXcoder's encoder is better than that built on CodeBERT's encoder on the downstream code summarization task, detailed in Section 4.2.1. Next, we introduce the design of the three summary-focused tasks. (i) AWP. An \u201caction word\u201d in a summary is typically a verb that broadly classifies what the code does [20], such as \u201cget\u201d, \u201cadd\u201d, and \u201cremove\u201d. Programmers tend to write summaries containing only one action word, typically positioned at the beginning of the summary sentence (i.e., the first word). AWP is a classification task, where the input of the model is a code snippet, and the output is the predicted label with respect to the action word [20]. In this paper, we use this task to train a model capable of predicting the action words of summaries based on given code snippets. Formally, let c = {t1, t2,..., tm} denote the token sequence of the code snippet, where m is the length of the token sequence, and y = {Y1, Y2,..., tc } denote the set of possible classes, where C is the number of classes of action words. The summary-focused AWP can be defined as follows:\nDefinition 1 (Summary-focused AWP). A summary-focused AWP is a multi-classification task denoted as \u0177 = arg maxyey P(y|c), where: P(y c) represents the probability of the class y given the code snippet c.\narg max denotes the operation that selects the class label with the highest probability.\nThe model we train is composed of the shared encoder and a classification layer. The classification layer is a fully connected network of size N * C, where N is the output size of the shared encoder. Given an input sequence x, we first utilize the shared encoder to transform x into the embedding ex. Then, a classification layer is used to classify ex into predicted action word classes. Given the embedding ex, we obtain the logits by \u0177\u2081 = Wex + b, where W is the weight matrix and b is the bias term. We optimize the model by minimizing the categorical cross-entropy loss:"}, {"title": "Training of Shared Encoder", "content": "$\\\\LAWP(\\Theta) = \\\\sum\\limits_{i=1}^{C} -y_i \\\\log \\\\frac{\\\\exp(y_i)}{\\\\sum\\\\limits_{j=1}^{C} \\\\exp(y_j)}\\\\\\(1)\\\\\\$where denotes trainable parameters of the model (i.e., W and b); \u0177i and yi are the predicted score and target score for each class i \u2208 C. In practice, we follow [20] and use the top-40 setting; that is, the model attempts to predict the forty most-common action words, or \u201cother\" if predicted to be a less-common action word. The top 40 action words are selected based on their frequency in the comments of all samples in each dataset. Here, we give a brief explanation of why we consider AWP as one of the pre-training tasks. First, the production\""}, {"title": "Training of Shared Encoder", "content": "of good summaries relies on the production of the action words in those summaries. Code summarization models are widely built on the encoder-decoder framework, where the decoder predicts words one by one according to previous words and the context produced by the encoder. So, if the first word is wrong, it is difficult for the decoder to predict the entire summary correctly. This situation can be exaggerated by the aggressive use of attention mechanisms, which can attend previous words in the predicted summary to parts of the code snippet [20]. Therefore, it is crucial for code summarization models to predict accurate action words. Second, our experiments found that ESALE equipped with AWP performs better than without. In practice, before deciding to add AWP as one of the pre-training tasks, we also followed [20] and conducted experiments on the perfor-mance of encoders of different seq2seq models on the AWP task with 41 classes. We compared five techniques, includ-ing AttendGRU, CodeBERT, UniXcoder, ESALE w/o AWP, and ESALE. AttendGRU is representative of seq2seq-like approaches as proposed by Iyer et al. [30]. In the paper [20], AttendGRU performs the best, so we also consider it as a baseline. For AttendGRU, we build a classification model by appending a fully connected network to its encoder, and train the model from scratch. For CodeBERT, UniXcoder, ESALE w/o AWP, and ESALE, we build classification models by appending a fully connected network to their encoders as classification layers and train the models by fine-tuning. TABLE 2 shows the experimental results where columns 2-4 report the weighted average precision, recall, and f-measure computed by the classification_report function pro-vided by scikit-learn 2. The experimental dataset consists of pairs of code snippets and action words extracted from the JCSD dataset. From the table, it is observed that, overall, 1) compared with the encoder from AttendGRU and trained from scratch, the pre-trained encoders from CodeBERT, UniXcoder, ESALE w/o AWP perform better in terms of f-measure; 2) compared with ESALE w/o AWP, ESALE's en-coder treating AWP as one of the pre-training tasks further improves the AWP performance. More details of comparing ESALE w/o AWP and ESALE are described in Section 4.2.2. In summary, equipping ESALE with AWP aims to en-hance the shared encoder to learn the code pattern that is the key feature to predict the action word. In this way, the code summarization model based on the shared encoder can better generate the action word of the summary."}, {"title": "Training of Shared Encoder", "content": "(ii) ULM. The ULM task is defined as the problem of left-to-right language modeling [36], which only allows words to attend the previous words and itself to predict the next word [35], [53]. Unlike existing works on predicting the next code token [35], [53], we use this task to train a model capable of predicting the next summary word one by one conditioned on the code token sequence and unmasked/preceding parts of the summary sequence. It can be done using a ULM mask matrix for the attention mask. We refer to such ULM as summary-focused ULM. Formally, w = {W1,W2,..., wn} denote the word sequence of the summary, where n is the length of the word sequence. The summary-focused ULM can be defined as follows:\nDefinition 2 (Summary-focused ULM). A summary-focused ULM is a probabilistic model denoted as P(W1, W2, ..., Wn|c) = \u03a0=1 P(Wi Wi\u22121, Wi\u22122, . . ., W1, C), where:\nP(Wi Wi-1, Wi-2, ..., W1, c) represents the probabil-ity of the word w\u2081 given the preceding summary words Wi-1, Wi\u22122, ..., W\u2081 and the code snippet c.\nI denotes the product of probabilities over the entire word sequence of the summary.\nThe model we train also includes the shared encoder followed by a fully connected network. The size of the fully connected network is N * |V|, where N is the output size of the shared encoder and |V| is the vocabulary size. Given an input sequence x, we first utilize the shared encoder to produce its embedding ex. Then, the fully connected network is used to predict the likelihood score of each word in the vocabulary being the next word. The model is optimized by minimizing the objective function:"}, {"title": "Training of Shared Encoder", "content": "$\\$L_{ULM}(\\Theta) = \\\\sum\\limits_{i=0}^{n-1}\\\\log p(w_i|e_{<i}).\\\\\\$(2)\\\\\\$where ei represents the embedding of the word sequence appearing on the left of the word wi. self-attention masks are used to control the behavior of the model, i.e., preventing from attending. UniXcoder di-rectly exploits the general ULM in NLP [54], which uses a triangular matrix for attention mask, predicting the next token in the entire input sequence. Different UniXcoder, ESALE introduces a summary-focused ULM, which is used to train ESALE to predict the next summary word only in the summary word sequence by attending the entire code token sequence and the left summary words. (iii) MLM. The MLM task is defined as the problem of predicting the original tokens of masked tokens based on their bidirectional contextual tokens [31]. Unlike ULM, which can only be trained unidirectionally, bidirectional conditioning in MLM allows each word to indirectly see itself, simplifying the prediction of the target word in a multi-layered context. Therefore, in this paper, this task is designed to train a model capable of predicting masked to-kens based on all tokens in the code snippet and unmasked words in the summary. Similarly, we refer to such MLM as summary-focused MLM. Formally, the summary-focused MLM can be defined as follows:\nDefinition 3 (Summary-focused MLM). A summary-focused MLM is a probabilistic model denoted as P(W1, W2, ..., Wn|C) = \u03a0=1 P(WiW1,..., Wi\u22121, Wi+1,..., Wn, c), where:\nP(WiW1,..., Wi\u22121, Wi+1, . . ., Wn, c) represents the probability of the masked word wi given the un-masked summary words w\u2081,..., Wi\u22121, Wi+1, ..., Wn and the code snippet c.\nI denotes the product of probabilities over the entire word sequence of the summary.\nIn this task, the model we train is composed of a shared encoder and a fully connected network. The design of the fully connected network is the same as in the summary-focused ULM task. Given a masked input sequence x, we first use the shared encoder to transform x into embedding ex. Then, the fully connected network is used to predict the likelihood score of each word in the vocabulary being the masked word. We optimize the model by minimizing the following objective function:"}, {"title": "Training of Shared Encoder", "content": "$\\\\L_{MLM}(\\Theta) = \\\\sum\\limits_{W_i \\\\in S_m}\\\\log p(w_i|e_{mask}).\\\\\\$(3)\\\\\\$where e mask is the embedding of x; Sm is the set of masked words that need to be predicted. We use Fig. 4 to visually illustrate the differences in the masked proportion and position between the baselines (i.e., CodeBERT and UniXcoder) and our ESALE. Fig. 4(a) shows an example of an original input sequence consisting of a code token sequence and a comment word sequence. Fig. 4(b)-(d) show the masked input sequences used in CodeBERT, UniXcoder, and ESALE, respectively. In Fig. 4(b), we follow CodeBERT and randomly replace 15% of the tokens in the input sequence with [MASK] tokens (the blue blocks labeled [M] in the figure). In Fig. 4(c), we follow UniXcoder and first sample 15% of the tokens from the input sequence, and then randomly replace 80% (i.e., about 10% of the input sequence) of them with a [MASK] token and leave another 10% of them unchanged. In Fig. 4(d),"}, {"title": "Training of Shared Encoder", "content": "our ESALE randomly replaces 15% of the tokens in the comment word sequence with [MASK] tokens. From the figure, it is observed that ESALE is summary-focused and significantly different from CodeBERT and UniXcoder in the masked proportion and position. Fig. 3(c), (d), and (e) also visually present the self-attention masks used by CodeBERT, UniXcoder, and ESALE, respectively.\nModel Training. The training procedure of the above task models follows the existing MTL paradigm [53]. In MTL, models are trained with data from multiple related tasks simultaneously while using a shared representation to learn the common features between all these tasks, and what is learned for one task can help other tasks be learned bet-ter [55]. The shared representation increases data efficiency and can potentially yield a faster learning speed for related or downstream tasks, helping to alleviate the well-known weaknesses of deep learning: large-scale data requirements and computational demand [40]. In this paper, we exploit the MTL paradigm to train a shared encoder with three summary-focused tasks, i.e., AWP, ULM, and MLM. The weight parameters of the shared encoder are learned to minimize the sum of the cross-entropy losses of the three pre-training tasks, and are shared among all three tasks. The final loss function is computed as:\n$\\$\\\\min\\limits_{\\Theta} L_{AWP}(\\Theta) + L_{ULM}(\\Theta) + L_{MLM}(\\Theta)\\\\\\(4)\\\\\\$Intuitively, during pre-training, the AWP model predicts the label corresponding to action words based on the in-put sequence. Simultaneously, the ULM model predicts the next token based on the left tokens of the input sequence. Meanwhile, the MLM model predicts the original tokens of masked tokens of the input sequence. The AWP model, ULM model, and MLM model share an encoder (i.e., the encoder of ESALE). After obtaining a pre-trained encoder (referred to as the pre-trained shared encoder in this paper), we further use pairs of code snippets and summaries to fine-"}, {"title": "Training of Code Summarization Model", "content": "tune it along with the decoder. The fine-tuning process is elaborated upon in the subsequent section. After obtaining the pre-trained shared encoder, we further train a code summarization model capable of generating a succinct natural language summary for a given code snippet. Specifically, we fine-tune the pre-trained shared encoder on the code summarization task and simultane-ously train a decoder. Given training data consisting of pairs of code snippets and comments, ESALE first leverages the pre-trained shared encoder to transform code snippets into context vectors eCode. Then, ESALE leverages a decoder to generate predicted summaries. The decoder takes in eCode and predicts words one by one, detailed in Section 3.3.1). Fi-nally, ESALE computes the loss (Lcs (\u04e8)) based on predicted summaries and ground-truth summaries (i.e., comments) and iteratively updates the model parameters, detailed in Section 3.3.2)."}, {"title": "Decoder", "content": "In this step, we utilize the decoder to generate natural language summaries. The decoder takes in the context vec-tors eCode and predicts words one by one. Specifically, the decoder based on a neural network (e.g., LSTM [56] and Transformer [47]) is to unfold the context vectors eCode into the target sequence (i.e., the word sequence of the summary) through the following dynamic model,\n$\\$h_t = f(y_{t-1}, h_{t-1}, e_{Code})\\\\ p(y_t|Y_{<t}, X) = g(y_{t-1}, h_t, e_{Code})\\\\\\$(5)\\\\\\$where f(.) and g(\u00b7) are activation functions, ht is the hidden state of the neural network at time t, yt is the predicted target word at t (through g(\u00b7) with Y<<t denoting the his-tory {Y1, Y2,\u2026, Yt\u22121}. The prediction process is typically"}, {"title": "Deployment of Code Summarization Model", "content": "After the model is trained, we can deploy it online for code summarization service. For a code snippet e given by the developer, ESALE first uses the fine-tuned encoder to trans-form cinto a context vector, which will be fed to the fine-tuned decoder to generate a summary in natural language. In practice, we can consider the well-trained ESALE as a black-box tool that takes in a code snippet and generates a succinct natural language summary."}, {"title": "EVALUATION", "content": "To evaluate our approach, in this section, we aim to answer the following four research questions:\nRQ1: How does ESALE perform compared to the state-of-the-art baselines?\nRQ2: How do the three pre-trained tasks (i.e., AWP, ULM, and MLM) affect the performance of ESALE (ablation study)?\nRQ3: How does the robustness of ESALE perform when varying the code length and comment length?\nRQ4: How does ESALE perform in human evaluation?"}, {"title": "Experimental Setup", "content": "We conduct experiments on four datasets. JCSD provided by Hu et al. [12] is a Java dataset. PCSD provided by"}, {"title": "Experimental Results", "content": "Re\u00b2Com [29] adopts an LSTM-based encoder-decoder architecture with an attention mechanism. It first uses an information retrieval technique to retrieve a similar code snippet and treat its comment as an exemplar. Then, it uses an LSTM-based seq2seq neural network that takes the given code, its AST, its similar code, and its exemplar as input, and leverages the information from the exemplar to generate summaries.\nSiT [46] adopts a Transformer-based encoder-decoder architecture. It proposes a structure-induced transformer to capture long-range dependencies and more global informa-tion in AST sequences of code snippets.\nSCRIPT [61] adopts a Transformer-based encoder-decoder architecture. It proposes two types of Transformer encoders to capture the structural relative positions between tokens for better learning code semantics. In addition to these non-pre-trained techniques above, since our method is based on the pre-training and fine-tuning paradigm, we also compare two techniques follow-ing such a paradigm.\nCodeBERT [33] is a representative pre-trained model of code. It is trained with the MLM and Replaced Token Detection (RTD) tasks. The authors of CodeBERT fine-tune and test it on the code summarization task (also called the code documentation generation task in their paper).\nUniXcoder [35] is the state-of-the-art pre-trained model of code. It is trained with four tasks: MLM, ULM, Denoising"}, {"title": "Experimental Results", "content": "Objective DeNoiSing (DNS), and Code Fragment Represen-tation Learning. Unlike CodeBERT which only pre-trains the encoder, UniXcoder pre-trains both the encoder and the decoder. The authors of UniXcoder also fine-tune and test it on the code summarization task. For non-pre-training-based models (i.e., Re\u00b2Com, SiT, and SCRIPT) and pre-training-based models (i.e., Code-BERT and UniXcoder), we train and fine-tune them sep-arately on the training set of each code summarization dataset, and evaluate them on the corresponding test set. 2) Results: TABLE 4 shows the performances of our ESALE and baselines in terms of the three evaluation metrics, i.e., BLEU, METEOR, and ROUGE-L. In TABLE 4, \"ESALE + CodeBERT\" refers to that we build the shared encoder based on the pre-trained encoder provided by CodeBERT. Analogously, in \u201cESALE + UniXcoder\u201d, the shared encoder and decoder are built upon the pre-trained encoder and decoder provided by UniXcoder. In practice, we initialize our encoder and decoder with the model parameters of the CodeBERT and UniXcoder."}, {"title": "Effect of Each Pre-train Tasks (Abl"}]}