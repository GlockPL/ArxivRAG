{"title": "SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task Learning with Deep Representation Surgery", "authors": ["Enneng Yang", "Li Shen", "Zhenyi Wang", "Guibing Guo", "Xingwei Wang", "Xiaochun Cao", "Jie Zhang", "Dacheng Tao"], "abstract": "Model merging-based multitask learning (MTL) offers a promising approach for performing MTL by merging multiple expert models without requiring access to raw training data. However, in this paper, we examine the merged model's representation distribution and uncover a critical issue of \"representation bias\". This bias arises from a significant distribution gap between the representations of the merged and expert models, leading to the suboptimal performance of the merged MTL model. To address this challenge, we first propose a representation surgery solution called Surgery. Surgery is a lightweight, task-specific module that aligns the final layer representations of the merged model with those of the expert models, effectively alleviating bias and improving the merged model's performance. Despite these improvements, a performance gap remains compared to the traditional MTL method. Further analysis reveals that representation bias phenomena exist at each layer of the merged model, and aligning representations only in the last layer is insufficient for fully reducing systemic bias because biases introduced at each layer can accumulate and interact in complex ways. To tackle this, we then propose a more comprehensive solution, deep representation surgery (also called SurgeryV2), which mitigates representation bias across all layers, and thus bridges the performance gap between model merging-based MTL and traditional MTL. Finally, we design an unsupervised optimization objective to optimize both the Surgery and SurgeryV2 modules. Our experimental results show that incorporating these modules into state-of-the-art (SOTA) model merging schemes leads to significant performance gains. Notably, our SurgeryV2 scheme reaches almost the same level as individual expert models or the traditional MTL model. The code is available at https://github.com/EnnengYang/SurgeryV2.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-task learning (MTL) leverages a single model to predict multiple related tasks, thereby reducing parameter costs and facilitating cross-task knowledge transfer [2, 3, 4, 5]. MTL has been extensively applied across various domains, including computer vision [6, 7, 8, 9, 10, 11], natural language processing [12, 13], recommendation systems [14, 15, 16] and speech recognition [17, 18]. However, the traditional MTL paradigm requires pre-collection of training data for all tasks and then collaboratively training an MTL model, which may lead to privacy leakage of valuable data and additional data management costs, particularly when dealing with large datasets or numerous tasks.\nRecently, a novel approach known as \u201cmodel merging\" or \"model fusion\" [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33] has emerged within the machine learning com-munity, demonstrating promise in addressing these challenges. The goal of model merging is to directly merge multiple expert models that have been trained individually on various tasks, to perform MTL without accessing their raw training data. In other words, model merging relies solely on the trained model parameters for each task, eliminating the need for centralized management of MTL training data and joint training. This innovative approach significantly broadens the potential appli-cations of MTL. Despite these advancements (\u00a7II), a notable performance gap persists between the most advanced model merging-based MTL methods [34, 35, 24, 21, 20, 21, 26] and traditional MTL or individual expert models. This gap motivates our investigation into the underlying issues of model merging and develops solutions to close this gap.\nIn this paper, we revisit several representative model merg-ing schemes [34, 36, 21, 26] from the perspective of the representation distribution, identifying a common challenge: \"representation bias\" (\u00a7IV-A). Representation bias is defined as the gap between the representations extracted by the merged model and those extracted by the individual models. Recall that the primary goal of model merging is to create a unified model that retains the capabilities of all individual expert mod-els. Therefore, to quantify the alignment between the merged model's representations and those of the expert models, we introduce a novel metric and conduct extensive evaluations across eight tasks (e.g., SUN397 [37], Cars [38], etc.), three architectures (i.e., ViT-B/32, ViT-B/16, and ViT-L/14 [39]), and four representative model merging methods (i.e., Weight Averaging [34], Task Arithmetic [36], Ties-Merging [21], AdaMerging [26]). By examining the relationship between the representation bias of the merged model and its final MTL performance, we empirically demonstrate that representation bias is a critical factor limiting the performance of the merged model. Consequently, reducing representation bias can achieve better model merging performance.\nTo solve the \"representation bias\u201d problem in model merg-ing, we first propose a novel representation surgery solution (\u00a7IV-B), termed Surgery, which aligns the representation of the merged model at the last layer with the representation of the individual expert model at the last layer. Since raw training data are not available, we design an unsupervised surrogate objective to update parameters in the Surgery module. Notably, this approach achieves a new SOTA performance among existing model merging methods. However, there is still a relatively obvious representation bias after using the repre-sentation surgery scheme, and the MTL performance of the merged model is still far behind the traditional MTL method. This result prompted us to conduct a deeper analysis of the representation surgery method to further reduce representation bias and narrow the performance gap.\nWe carefully analyze the correlation between the capacity of the Surgery module and the degree of representation bias (\u00a7IV-C), finding that simply increasing the capacity can effectively reduce the bias and improve merging performance. However, beyond a certain threshold, further increasing ca-pacity no longer yields benefits. Additionally, our layer-by-layer analysis reveals that representation bias exists at each layer of the merged model, with the bias magnitude increasing monotonically with the depth of the layer in the merged model. In summary, representation bias exists at every layer of the merged model, and addressing only the last layer\u2014even with a substantial capacity\u2014fails to fully resolve systemic representation bias. This is because the biases introduced at each layer can accumulate and interact in complex ways. To overcome this limitation, we then propose a deep represen-tation surgery solution (\u00a7IV-D), abbreviated as SurgeryV2, which effectively alleviates layer-wise representation bias and bridges the performance gap between model merging based MTL and traditional MTL. Specifically, our SurgeryV2 aligns the representation of the merged model with that of the individual models at each layer, thereby effectively mitigating MTL performance degradation.\nWe conduct numerous experiments to demonstrate the SOTA performance of our Surgery and SurgeryV2 schemes (\u00a7V). Our comprehensive evaluation covered eight vision datasets, five text datasets, four representative model merging methods, and four model architectures within the computer vision (CV) and natural language processing (NLP) domains. Our observations include the following: (i) when Surgery and SurgeryV2 schemes are applied to existing model merging methods, the merged model achieves signif-icantly better MTL performance. In particular, as shown in Fig. 1, our SurgeryV2 model merging scheme achieves com-parable or even slightly better performance than the traditional MTL method, which is noteworthy. (ii) By monitoring the per-formance changes during the optimization process, we find that with the same capacity, SurgeryV2 can achieve significantly higher accuracy than Surgery with fewer iterations. (iii) Our proposed SurgeryV2 is effective even in wild data settings, significantly broadening its range of application scenarios."}, {"title": "II. RELATED WORKS", "content": "In this section, we briefly introduce the related work, with a more comprehensive discussion provided in Appendix VII.\nRecent studies have attempted to achieve MTL using model merging techniques [32, 22, 40, 41, 33]. However, the per-formance of simple parameter averaging [34, 42] degrades significantly as the number of tasks increases, resulting in a substantial gap between its performance and that of traditional MTL model. Many recent works have made various attempts to fill this gap [36, 21, 29, 24, 43, 25, 23, 44, 26, 33]. The first type of method explores how to better weigh and combine multiple models [19, 45, 40]. For example, Fisher-Merging [19] performs weighted merging utilizing the im-portance of each parameter through the Fisher information matrix [46]. RegMean [24] reweights and linearly combines rows in weight matrices based on statistics from training data. AdaMerging [26] leverages unlabeled test data to automati-cally learn a set of task-level or layer-level model merging coefficients. The second type of method explores how to merge models in sparse subspaces to reduce interference [47, 48, 30, 49, 50, 31, 51]. For example, Ties-Merging [21] removes the smaller magnitude parameters and eliminates the issue of parameter sign conflicts during model merging. DARE [29] removes a large number of useless neuron updates and then scales the neurons for merging. Concrete [52] finds a shared subspace between multiple tasks for model merging. The third type of method dynamically merges multiple expert modules during inference [53, 54, 55, 56, 57]. For example, WEMOE [53] dynamically merges linear layers by routing, and static merges nonlinear layers. It should be noted that the dynamic merging method requires additional maintenance of more model parameters than the first two categories of methods, and it also reduces the inference efficiency.\nWhile existing methods predominantly concentrate on merg-ing in weight space, they often neglect a crucial concern stemming from weight merging-the representation bias. A substantial disparity emerges in the representation space between the merged model and individually trained expert models. In contrast, our method addresses this gap, aiming to minimize the representation discrepancy. Moreover, our approach operates in the representation space, offering a com-plementary and orthogonal perspective to traditional weight-space merging methods. Consequently, our method can be seamlessly integrated with them."}, {"title": "III. PRELIMINARY", "content": "In this section, we first define the notation within the context of model merging (\u00a7III-A), and then introduce some representative model merging solutions (\u00a7III-B).\nA. Notations and Problem Setting\nNotations: Assume there are T models (\u0398(1),..., \u0398(T)) of the same architecture that needs to be merged, and they are fine-tuned into a pretrained model \u0398(0) on tasks 1-T respectively. The parameters (or weights) of the t-th model \u0398(t) are represented as \u0398(t) = {{\u03b8(t)l }Ll=1, where l denotes the l-th layer 1 of the model, and L is the total number of layers. In addition, the test set of the t-th task is defined as D(t) = {(x(t)i , y(t)i )} |D(t)t i=1, where x(t)i represents the i-th input sample and y(t)i represents the corresponding label, and Dt is the number of samples. Without loss of generality, the forward propagation of a deep neural network model can be represented as a directed acyclic computation graph, where the output of each layer serves as the input for the subsequent layer. Based on these notations, we give definitions of representations and model merging in Definition III.1 and Definition III.2, respectively.\nDefinition III.1 (Representation). Given a model \u0398(t), we define the output (also referred to as representation) of sample x(t)i at the l-th layer as z(t)l,i = \u03a6(t)l (x(t)i ) \u2208 Rdl , and dl represents the output dimension of the l-th layer. Similarly, the representation of all samples for task t at the l-th layer is denoted by Z(t)l = {z(t)l,i }(t)=(t)D i=1\nDefinition III.2 (Model Merging). Given multiple expert models fine-tuned on different tasks, the goal of model merg-ing is to merge the parameters {\u0398(1), . . ., \u0398(T)}, and finally obtain the new parameters \u0398(mtl) = merge(\u0398(1), ..., \u0398(T)), and \u0398(mtl) can successfully perform tasks 1 \u2013 T. In other words, the objective of model merging is for the MTL model \u0398(mtl) to achieve the lowest loss on test sets of all tasks, which is defined as follows:\narg min \u0398(mtl) 1 TTD \u2211t=1 i=1 L(t)(\u03a6\u0398(mtl) (x(t)i ), y(t)i ), (1)\nwhere \u0398(mtl) = merge (\u0398(1),..., \u0398(T)),\nwhere L(t)(\u00b7) is the loss function for the t-th task, such as cross-entropy loss or mean squared error.\nTaking two tasks as an example, as illustrated in Fig. 8, model merging hopes to combine the expert models trained individually on task A and task B (i.e., Fig. 8(a)) to obtain a new model (i.e., Fig. 8(b)) capable of performing both tasks A and B. Mainstream model merging research focuses on designing more advanced merge(\u00b7) strategies to alleviate task conflict and task interference [35, 36, 24, 21, 29, 26, 48].\nB. Representative Model Merging Methods\nIn this section, based on the popularity and performance of model merging methods, we select four representative methods to introduce: Weighted Averaging [34], Task Arithmetic [36], Ties-Merging [21], and AdaMerging [26].\nWhen merging models, one of the simplest solutions is Weighted Averaging, defined as \u0398(mtl) = 1 T T \u2211t=1 \u0398(t). However, it often results in suboptimal performance due to potential task conflicts. As mentioned in \u00a7II, a lot of advanced work has been devoted to developing better merge operations (i.e., the merge(\u00b7) function in Eq. 1) to reduce the perfor-mance degradation of the merged model \u0398(mtl) compared"}, {"title": "IV. METHOD", "content": "This section first re-examines these representative merging schemes from the perspective of representation distribution and establishes that they suffer from \"representation bias\" (\u00a7IV-A). Then, we propose a representation surgery (i.e., Surgery) method to mitigate representation bias (\u00a7IV-B). Despite the new SOTA performance achieved by Surgery, it still has gaps with traditional MTL or expert models. Next, we conduct an in-depth analysis of the Surgery scheme and reveal the phenomenon of layer-wise representation bias (\u00a7IV-C). Finally, we propose a deep representation surgery scheme (i.e., SurgeryV2) scheme to perform representation alignment at each layer of the merged model (\u00a7IV-D).\nA. Revisiting Representation Bias\nThis section analyzes representative merging schemes men-tioned in \u00a7III-B from the perspective of the representation distribution of the merged model and expert models and identifies a key issue that limits the MTL performance of the merged model, namely, \u201crepresentation bias\". We give a clear definition of representation bias in Definition IV.1.\nDefinition IV.1 (Representation Bias). Given merged model \u03a6\u0398(mtl) and individual expert models {\u03a6\u0398(1), . . ., \u03a6\u0398(T) }, the representation bias b(t)L w.r.t. task t is defined as the discrepancy between the representations Z(t)L,ind and Z(t)L,mtl extracted from the final (i.e., L-th) layer of the individual expert model \u03a6\u0398(t) and the merged model \u03a6\u0398(mtl):\nb(t)L = 1 Dt dL \u2211 Di=1 (Z(t)L,mtl ,i , Z(t)L,ind ,i ), (2)\nwhere (\u00b7) represents any distance-measuring function, such as L1 or L2 loss, and dL is the dimension of the representation at the final layer (e.g., ViT-B/32 and ViT-B/16 are 512, and ViT-L/14 is 768).\nWithout loss of generality, we perform analysis on the fol-lowing representative model merging methods, tasks/datasets, and architectures: (i) Methods: We analyze the four model merging methods mentioned in \u00a7III-B. (ii) Tasks: We follow Task Arithmetic [36] and use the following eight datasets as eight tasks for model merging: SUN397, Cars, RESISC45, Eu-roSAT, SVHN, GTSRB, MNIST, DTD. We provide a detailed dataset description in Appendix VIII. (iii) Architectures: We merge eight expert models into one model and experiment with three ViT architectures [39] with different parameter scales: ViT-B/32, ViT-B/16, and ViT-L/14.\nTo reveal the performance gap between the merged model \u03a6(mtl)L and the individual expert models {\u03a6(1)L , . . ., \u03a6(T)L }, we use the t-SNE [58] tool to visualize the representations Z(t)L,ind and Z(t)L,mtl of the expert and merged models, as well as directly measure the representation bias in Definition IV.1. We find that \"representation bias\" exists across tasks, across merging methods, and across architectures:"}, {"title": "B. Surgery: Representation Surgery", "content": "In this section, to alleviate the representation bias, a novel representation surgery scheme is proposed, which involves aligning Z(t)L,ind and Z(t)L, mtl for task t using a representation surgery module \u03a9w(t) (\u00b7) as Fig. 8(c). The formal objective function is defined as:\nmin {W(1),...,W(T)} T 1 \u2211t=1 Dt (Z(t)L,mtl , Z(t)L,ind ), (3)\nwhere Z(t)L,mtl = Z(t)L,mtl \u2212 \u03a9w(t) (Z(t)L,mtl ),\nwhere (\u00b7) is an arbitrary distance function, \u03a9w(t) (\u00b7) is an optimizable task-private lightweight module, which can be an arbitrary implementation (such as multiple fully connected layers, etc.). Without loss of generality, in this paper, we implement it as an Adapter-like structure [59], that is,\n\u03a9w(t) (Z(t)L,mtl ) = W (t)Up ReLU(W (t)Down (Z(t)L,mtl )T ), (4)\nwhere W (t)Up \u2208 RdL\u00d7r, W (t)Down \u2208 Rr\u00d7dL represent two learnable matrices, i.e., W (t) = {W (t)Up , W (t)Down }, ReLU(\u00b7) is a nonlinear activation function, dL is the dimension of representation as mentioned in Definition III.1, and r is a hyperparameter, also called rank, which we set to 16 by default.\nIn particular, our Surgery scheme does not rely on any labeled training data. Instead, inspired by test-time adaptation [60, 61], it leverages the unlabeled test data {Dt(X, Y )}T t=1 and individual models {\u03a6\u0398t}T t=1 as a self-supervised signal to optimize the parameters of the Surgery module, denoted as {W (1), W (t), . . ., W (T)}.\nDiscussion. The representation surgery proposed in this paper complements existing model merging schemes. We next discuss why the proposed Surgery scheme is effective. As mentioned in Eq. 4, the goal of the representation surgery is to reduce the discrepancy in the representation distributions between the merged and individual models. Comparing the discrepancy in representation distributions in Fig. 2 (w/o Surgery) and Fig. 5 (w/ Surgery), we can observe that the distributions of the merged and individual models in Fig. 5 are closer (i.e., higher overlap). In addition, we quantify the \"representation bias\" (in Definition IV.1) between the two distributions. As illustrated in Fig. 4, the proposed repre-sentation surgery (red column) significantly reduces the L1 distance for the merged model compared to the model merged without representation surgery (blue column). For instance, on the EuroSAT dataset, Task Arithmetic-based model merging decreases the representation bias from 0.30 to 0.13 following representation surgery, representing a 56% relative reduction. These findings indicate that the proposed Surgery effectively mitigates the problem of representation bias."}, {"title": "C. Revisiting Representation Surgery", "content": "The representation surgery scheme mentioned in \u00a7III-B has indeed achieved significant performance improvements com-pared to existing model merging approaches. However, there is still a certain gap between it and the individual model or the traditional MTL model, irrespective of whether we consider representation distribution visualization or performance com-parison. For example, in Fig. 5, the red and blue distributions exhibit imperfect overlap. Furthermore, as depicted in the left subfigure of Fig. 6, the optimal performance of Task Arithmetic with Surgery is 85.8, while the individual model is 90.5 and the traditional MTL model is 88.9. This encourages us to intensify our efforts towards narrowing these gaps. In this section, by revisiting the Representation Surgery scheme (in \u00a7IV-B), we discerned two key observations.\nObservation 1: Increasing the surgery module\u2019s capacity can enhance performance and reduce representation bias, but beyond a certain threshold, additional capacity has a negligible effect. The Surgery module \u03a9w(t) (\u00b7) in Eq. 3 is a learnable module (e.g., a multi-layer perceptron or an Adapter-like module [59]), prompting a natural conjecture that expanding its capacity could ameliorate the representation bias and enhance performance. As depicted in Fig. 6, when we in-crease the capacity from 1\u00d7 to 20\u00d7, the average performance of the merged model increases from 80.9 to 85.8 (left subplot). Concomitantly, the representation bias at the last layer also decreased from 0.13 to 0.11 (right subplot). However, further increments in capacity, such as by a factor of 100\u00d7, do not yield substantial improvements in performance or bias. This suggests a ceiling (significantly lower than traditional MTL) on the efficacy of performing representation surgery at the final layer in alleviating representation bias.\nObservation 2: The \u201crepresentation bias\u201d problem exists at every layer of the merged model and increases monotonically with the layer\u2019s depth. Given that the forward propagation pro-cess of a deep neural network model follows a directed acyclic computation graph, the vanilla Surgery in \u00a7IV-B measures the representation bias and performs the representation surgery at the last layer. However, we argue that representation bias exists at every layer and interacts in complex ways. This leads to systemic bias in the final output that cannot be completely corrected by adjusting only the last layer. Specifically, as shown in Fig. 7 3, by extending Definition IV.1, we quantify the representation bias {b(t)1 ,..., b(t)L } of layer {1,...,L}"}, {"title": "D. SurgeryV2: Deep Representation Surgery", "content": "In this section, to more effectively address the issue of \"representation bias\" in the merged model, we introduce a novel deep representation surgery scheme, referred to as SurgeryV2 (as shown in Fig. 8(d)).\nUnlike vanilla Surgery in \u00a7IV-B, which accumulates the representation biases from all layers to the last layer using a \"lazy alignment\" strategy, our SurgeryV2 scheme employs an \"immediate alignment\" strategy to perform representation surgery at each layer. Specifically, we first add a lightweight surgery module W(t)l to each layer l w.r.t task t of the merged model \u03a6\u0398(mtl). Next, we align the representation Z(t)l,mtl of the merged model \u03a6\u0398(mtl) at layer l \u2208 {1,...,L} with the representation Z(t)l,ind of the individual model \u03a6\u0398(t) at the corresponding layer l \u2208 {1, ..., L}. Finally, the overall optimization goal of our SurgeryV2 method is defined as:\nmin {W(t)l } TL t=1l=1 1 |Dt| (Z(t)l,mtl , Z(t)l,ind ), (5)\nwhere Z(t)l,mtl = Z(t)l,mtl \u2212 \u03a9\u2032(t) l (Z(t)l,mtl ),\nwhere T and L denote the number of tasks and layers, respec-tively. (.) denotes an arbitrary distance function, which could be L1 loss, mean squared error (MSE), or negative cosine similarity. Detailed comparisons of these different distance functions are provided in Tab. IX of Appendix IX-D. In addition, \u03a9\u2032(t)l is a learnable lightweight module. Without loss of generality, we adopt an adapter-like [59] structure as described in \u00a7IV-B, which is,\n\u03a9\u2032(t)l (Z(t)l, mtl ) = W (t)l,Up ReLU(W (t)l,Down Z(t)l,mtl ), (6)\nwhere W (t)l = {W (t)l,Up , W (t)l,Down }, and W (t)l,Up \u2208 Rdl\u00d7r and W (t)l,Down \u2208 Rr\u00d7dl are two learnable weights within the SurgeryV2 module \u03a9(t)l , and r denotes a hyperparameter, which we set to 16 by default.\nHowever, in the context of model merging, the labeled orig-inal training data is unavailable. To optimize {W(t)l }Ll=1,t=T in SurgeryV2, drawing inspiration from test-time adapta-tion [60, 62], we leverage unlabeled test data. Furthermore, we also verified that the proposed SurgeryV2 can still work with wild data, although with a slight reduction in performance. Specifically, (i) In the case of SurgeryV2 with Unlabeled Test-time Data, we set D in Eq. 5 to Dt as detailed in \u00a7III-A. (ii) In the case of SurgeryV2 with the"}, {"title": "V. EXPERIMENT", "content": "This section presents the main results. Due to page limita-tions, additional experimental results and analysis are placed in the Appendix IX-A and Appendix IX-D, respectively.\nA. Experimental Setup\nThe datasets, architectures, and baselines used in this paper are listed as follows. Further details can be found in Ap-pendix VIII-B.\nDatasets: Our experiments cover eight vision datasets and five text datasets. Following the setup of Task Arithmetic [36] and AdaMerging [26], we treat the following eight datasets as eight tasks to perform model merging in the CV domain: SUN397 [37], Cars [38], RESISC45 [64], EuroSAT [65], SVHN [66], GTSRB [67], MNIST [68], DTD [69]. In addi-tion, we treat the following five datasets from [70, 71] as five tasks to perform model merging in the NLP domain: AGNews (news classification), Yelp (sentiment analysis), Amazon (sen-timent analysis), DBPedia (Wikipedia article classification) and Yahoo (questions and answers categorization).\nArchitectures: For vision data, we use three architectures: ViT-B/32, ViT-B/16, and ViT-L/14 from CLIP [72]. For text data, we use the commonly used BERT [73] model.\nBaselines: We compared three non-model merging methods: Pretrained, Individual, and Traditional MTL models; seven popular model merging methods: Weight Averaging, Fisher Merging [19], RegMean [24], Task Arithmetic [36], Ties-Merging [21], AdaMerging [26], Concrete AdaMerging [52].\nB. Performance Comparison\nComputer Vision Domain. As shown in Tabs. I and II, we merged the models of ViT-B/32 and ViT-L/14 architectures well-trained on eight tasks, respectively. The observations are as follows: (i) The Individual model performs the best but requires separate model parameters for each task. Traditional MTL collaboratively trains a model on the original training data, and the performance is second-best. Pretrained model performs worst due to the lack of task-specific knowledge. (ii) Advanced model merging methods, such as Fisher Merging, RegMean, Task Arithmetic, Ties-Merging, AdaMerging and Concrete AdaMerging, outperform simple weight averaging. However, due to representation bias, a significant performance gap remains between these methods and the Traditional MTL paradigm. (iii) When our Surgery scheme is used in the ex-isting model merging scheme, the performance of the merged model is significantly improved. However, since the represen-tations are aligned only in the last layer, this scheme still cannot reach the level of Traditional MTL most of the time. (iv) Our SurgeryV2 scheme enables the merged model\u2019s performance to reach the Traditional MTL model\u2019s level. For example, when AdaMerging is combined with SurgeryV2, the merged model (89.3 in ViT-B/32) even slightly exceeds the traditional MTL (88.9 in ViT-B/32).\nNatural Language Processing Domain. We merged BERT models trained separately on five distinct text datasets. As shown in Tab. III, there is a clear performance gap between model merging based MTL (Weight Averaging and Task Arithmetic) and Traditional MTL. After performing our vanilla Surgery, the performance of Weight Averaging and Task Arithmetic improved from 55.8 and 64.3 to 74.8 and 74.6, respectively, yet it remains below that of Traditional MTL at 75.0. However, after performing our SurgeryV2, the performance of the merged model can be further improved to 75.8 and 76.2, which is very close to the performance of the individual model (i.e., 76.2). This demonstrates the effectiveness of our approach in the NLP domain.\nC. Experimental Analysis\nSurgery and SurgeryV2 with the Wild Data. In cases where unlabeled test data is insufficient for optimizing surgery modules (Eq. 3 in \u00a7IV-B and Eq. 5 in \u00a7IV-D), we investigated the possibility of utilizing public/auxiliary data as an alterna-tive. For example, in the upper section of Tab. IV, we merge the models trained on the four datasets of AG, Yelp, Amazon, and Yahoo, and use Wikipedia data for surgery. SurgeryV2 still achieves the same result as the Traditional MTL, which is 69.3, while the original Surgery only has 66.9. As shown in the lower section of Tab. IV, it is similar to using AG News as wild data to merge the corresponding models of Yelp, Amazon, Yahoo, and DBPedia. These results indicate that even with wild data, SurgeryV2 also demonstrates the wide applicability and potential to calibrate the merged model without requiring unlabeled test data.\nPerformance w.r.t. Iteration. As shown in Fig. 11, we show the performance changes during iterations of our Surgery and our SurgeryV2. For a fair comparison, the two maintain consistent representation capacity. We can observe that under the four model merging methods (Weight Averaging, Task Arithmetic, Ties-Merging, and AdaMerging), using our SurgeryV2 achieves higher final performance than the vanilla Surgery. Even the performance of SurgeryV2 in the 200-th iteration exceeds the performance of Surgery in 1000 iterations. This means our SurgeryV2 is more efficient."}, {"title": "VI. CONCLUSION AND FUTURE WORKS", "content": "In this paper, we first demonstrate that state-of-the-art model merging methods are affected by representation bias. To address this issue, we propose a representation surgery approach (i.e., Surgery) that aligns the merged model\u2019s representations with those of the independent expert model at the final layer, thereby mitigating the bias. However, the initial Surgery scheme still leaves a performance gap. Further analysis reveals that representation bias occurs across all layers of the merged model. To effectively alleviate this, we introduce a deep representation surgery scheme, termed SurgeryV2, which significantly reduces the representation bias. Experiments across CV and NLP tasks, merging methods, and architectures show that our solution matches traditional MTL performance. There are several directions for future research in our work: (i) developing a representation surgery strategy that eliminates the need for training; (ii) investigating model merging with different initializations or architectures; and (iii) extending the proposed representation surgery to a broader range of model merging scenarios."}, {"title": "VII. RELATED WORKS", "content": "In this section", "direc-tions": "model merging for multi-task learning in \u00a7VII-A and traditional multi-task learning in \u00a7VII-B.\nA. Model Merging for Multi-Task Learning\nRecent studies in the machine learning community have at-tempted to achieve MTL using model merging techniques [32", "33": ".", "42": "degrades significantly as the number of tasks increases", "stages": "before and during merging.\n(i) The main concern before merging is how to provide more favorable preconditions for model merging, such as lin-earization or orthogonalization. Specifically, some works [20, 74", "75": "of the pre-trained model and demonstrate that this helps disentangle the weight space from the input space, leading to better model merging. Similarly, Linearization-LoRA [28", "76": "in Tangent space. In addition, Task Arithmetic [36", "77": "GitReBasin [78", "79": "rearrange the neurons in each layer to facilitate weight interpolation.\n(ii) The main focus during merging is how to mitigate interference and conflicts between models [36, 43, 21, 29, 24, 25, 23, 44, 26, 33", "40": ".", "19": "performs weighted merging utilizing the importance of each parameter through the Fisher information matrix [46", "24": "reweights and linearly combines rows in weight matrices based on statistics from training data. AdaMerging [26", "51": ".", "21": "removes the smaller magnitude parameters and eliminates the issue of parameter sign conflicts during model merging. DARE [29", "52": "finds a shared subspace between multiple tasks for model merging. PCB-Merging [48", "57": ".", "53": "dynamically merges linear layers by routing, and static merges nonlinear layers. It should be noted that the dynamic merging method requires additional mainte-nance of more model parameters than the first two categories of methods, and it also reduces the inference efficiency.\nWhile existing methods predominantly concentrate on merg-ing in weight space, they often neglect a crucial concern stemming from weight merging-the representation bias. A substantial disparity emerges in the representation space between the merged model and individually trained expert models. In"}]}