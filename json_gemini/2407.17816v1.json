{"title": "NC2D: Novel Class Discovery for Node Classification", "authors": ["Yue Hou", "Xueyuan Chen", "He Zhu", "Ruomei Liu", "Bowen Shi", "Jiaheng Liu", "Junran Wu", "Ke Xu"], "abstract": "Novel Class Discovery (NCD) involves identifying new categories within unlabeled data by utilizing knowledge acquired from previously established categories. However, existing NCD methods often struggle to maintain a balance between the performance of old and new categories. Discovering unlabeled new categories in a class-incremental way is more practical but also more challenging, as it is frequently hindered by either catastrophic forgetting of old categories or an inability to learn new ones. Furthermore, the implementation of NCD on continuously scalable graph-structured data remains an under-explored area. In response to these challenges, we introduce for the first time a more practical NCD scenario for node classification (i.e., NC-NCD), and propose a novel self-training framework with prototype replay and distillation called SWORD, adopted to our NC-NCD setting. Our approach enables the model to cluster unlabeled new category nodes after learning labeled nodes while preserving performance on old categories without reliance on old category nodes. SWORD achieves this by employing a self-training strategy to learn new categories and preventing the forgetting of old categories through the joint use of feature prototypes and knowledge distillation. Extensive experiments on four common benchmarks demonstrate the superiority of SWORD over other state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph data is commonly used to reveal interactions between various entities, such as academic graphs, social networks, recommendation systems, etc [47\u201349, 65, 66]. In the last few years, node classification has received considerable attention [34, 43, 45, 50, 56] with the rise of significant advances in graph neural networks (GNNs) [22]. Most of the existing works on node classification primarily focus on a single task, where the model is tasked to classify unlabeled nodes into fixed classes [20, 22, 32, 68]. In practice, however, many graph data grow continuously, generating new classes as new nodes and edges emerge, for example, in a citation network, the appearance of new papers and their related citations, or even new interdisciplinary; the addition of new users leads to the emergence of new social groups. The categories of nodes are gradually expanding, usually accompanied by few labels due to their new emergence or lack of exploration. This requires new category discovery on graphs using GNNs, which currently has limited research on tasks regarding node classification.\nThe task of automatically discovering new classes in an unsupervised manner while utilizing previously acquired knowledge is called novel class discovery (NCD) [10, 15, 16, 62, 63]. NCD has recently received widespread attention for its ability to effectively learn new classes without relying on large amounts of labeled data.\nMost of the currently proposed NCD solutions employ stage-wise [16, 18, 19] or joint [10, 15, 62] learning schemes for labeled and unlabeled data. It has been demonstrated that NCD achieves superior learning performance when clustering unlabeled data and jointly training with labeled data [15, 62]. However, joint training relies on the availability of labeled data, which presents two limitations: (1) labeled data may become unavailable after the pre-training phase due to practical issues such as storage constraints or privacy protection; (2) retraining labeled data from old classes can make the entire learning process more cumbersome and expensive. Therefore, a more practical stage-wise NCD setting is needed, where labeled data is discarded and only pre-trained models can be migrated for learning new classes. Although such a training scheme appears more practical, it gradually leads to the elimination of all previously acquired information about base (or old) classes. This phenomenon is known as catastrophic forgetting in neural networks [8], where the model\u2019s performance on previous tasks experiences a significant decline after being trained on a new task.\nIn light of the above analysis, we propose a novel class discovery task for node classification on graphs called NC-NCD. The key insight of the NC-NCD setting is to learn a model with a stage-wise scheme that effectively clusters unlabeled nodes from new classes by leveraging knowledge and potential commonalities from labeled nodes belonging to old classes. Given the inherent limitations of existing NCD settings [27], we argue that the ideal NCD approach is supposed to focus on strictly learning new classes using only unlabeled nodes while simultaneously maintaining performance on the base classes. Moreover, most of the existing NCD methods mentioned above, whether trained in a stage-wise or joint manner, prioritize learning the current task without considering the global context of all classes. We find these approaches impractical in real-world scenarios, as models adapted to new classes become unusable for base classes and retraining is infeasible. Inspired by class-IL [40], NC-NCD evaluates the performance on nodes from all classes in the inference phase, thereby overcoming the reliance on the task-id of both old and new classes and truly achieving task-agnostic NCD.\nNotice that the NC-NCD setting is essentially different from incremental learning and NCD, and Figure 1 illustrates the distinctions among these settings. The goal of NC-NCD is to continue training the model using unlabeled nodes of new categories after learning knowledge from old categories without requiring old category data, which enables the model to classify all categories. The key distinction between our NC-NCD and the NCD setting lies in the fact that NC-NCD additionally demands the model to maintain performance on old categories, unlike NCD, which focuses solely on the model\u2019s performance on new categories. Novel Class Discovery without Forgetting (NCDwF) [21] is closely related to NCD but it relaxes certain key assumptions. Notably, existing NCD methods assume access to both labeled and unlabeled data during training, a condition often unrealistic in real-world scenarios. Both NCDwF and NC-NCD emphasize the non-concurrent utilization of labeled and unlabeled data during training. However, in contrast to NC-NCD, NCDwF fails to train a classifier capable of predicting across the entire label space. Consequently, the distinction between new and old categories in NCDwF during evaluation still necessitates the utilization of task-ids. Incremental learning can be categorized into task increment (task-IL) and class increment learning (class-IL), aiming to prevent catastrophic forgetting during the learning of a series of categories that follow a different setting. Specifically, as depicted in Table 1, after learning old category nodes, NC-NCD trains on unlabeled nodes of new categories, while task-IL and class-IL require training on both labeled old category and new category nodes. NCD only utilizes new category nodes for training and testing. During the evaluation, task-IL needs to specify the task-id in a task-specific manner, allowing the model to test old or new category nodes with the knowledge of whether the current categories are old or new. In contrast, our NC-NCD distinguishes in a task-agnostic way between all categories seen during evaluation. Additionally, NC-NCD liberates itself from the constraint of maintaining a consistent number of categories learned in each phase. Clearly, our NC-NCD setting is more aligned with real-world application scenarios.\nIn this work, we propose a novel Self-training With prototype Replay and Distillation framework named SWORD, to implement the NC-NCD task effectively. Specifically, an encoder for extracting node features is first pre-trained on the labeled old category nodes and used directly in the NCD-training phase. To facilitate the learning of unlabeled new categories, we set up a classifier specific to the new classes, optimized with robust rank statistics. Besides, since the NC-NCD setting is task-agnostic, we maintain a joint classifier that is trained with pseudo-labels generated by a task-specific classifier, to classify all the classes encountered throughout the learning process. Inspired by the effectiveness of replay-based incremental learning [3, 5, 35], we replay the prototype of base class nodes after clustering to prevent catastrophic forgetting of base classes. In addition to features encoded from the prototype, feature-level knowledge distillation is employed to uphold the effectiveness of the model\u2019s feature extraction. The main contributions are as follows:\n\u2022 Contrary to existing research in incremental learning and NCD, we introduce a more practical NCD setting for node classification, named NC-NCD.\n\u2022 We propose a novel framework, SWORD, adapted to our NC-NCD setting to address the challenges in the current NCD scenarios formulated with node classification tasks.\n\u2022 Our SWORD trains a task-agnostic joint classifier with a self-training strategy in the NCD-training phase and employs prototype replay and knowledge distillation to prevent forgetting.\n\u2022 Extensive experiments are conducted with multi-backbones to confirm the effectiveness of SWORD by comparing with the state-of-the-art (SOTA) methods on various benchmarks regarding node classification."}, {"title": "2 RELATED WORK", "content": "As information grows, the task of discovering new categories from data and making accurate classification predictions has become a popular research direction [15, 16, 27, 38, 62, 67]. Here, we devote our attention to NCD for node classification, the most relevant topic of this work."}, {"title": "2.1 Graph Representation Learning", "content": "Graph representation learning aims to encode both the feature information from nodes and the topological structure of the incoming graph. Inspired by word2vec [31], several methods, such as DeepWalk [33] and node2vec [13], generate sequences of nodes by random walks, and apply skip-gram approach to map nodes within the same context to similar vector representations. Although these methods show success in various tasks like node classification and link prediction, they overemphasize proximity information at the expense of structural information and fail to incorporate node attributes.\nRecently, GNNs have received a great deal of attention, such as graph convolutional networks (GCNs) [22], GraphSAGE [14], graph attention networks (GATs) [41] and their extensions [2, 6, 11, 25, 52, 60, 61]. GNNs take in high-dimensional node features and map them to low-dimensional vector embeddings that are used for different downstream tasks. Based on methods of aggregation, GNNs are mainly classified into spectral and spatial embedding. The spectral approaches [9, 22] learn graph representations by the Fourier transform of graph data, while spatial approaches perform simple spatial aggregation of features from neighbors [14, 41]. However, most of these works are designed for a single learning task and are not directly applicable to incremental learning problems, thus encountering catastrophic forgetting problems when learning a series of new tasks."}, {"title": "2.2 Novel Class Discovery", "content": "NCD task is to migrate knowledge learned from labeled datasets to unlabelled ones to discover new classes, assuming that the classes in labeled and unlabelled sets are disjoint. The proposed NCD methods can be broadly divided into two categories. The first category of approach utilizes a joint training scheme, assuming the availability of both labeled and unlabeled data [44, 55, 57]. The other category employs a stage-wise isolated training scheme, where data from old and new classes cannot be used simultaneously. In this context, the labeled data of the base classes are used only during the supervised pre-training phase and not thereafter. Instead, an unsupervised clustering loss is employed to fine-tune unlabelled data during the phase of discovering unknown new classes [19, 54, 62].\nAccording to [27], NCD methods that rely on joint training always outperform methods with isolated training. However, it is not always practical to have access to labeled data after the pre-training phase due to concerns about data privacy or storage limitations. Therefore, training on the old and new classes jointly may not be a viable option in many cases. Moreover, earlier approaches based on isolated training schemes fail to adequately address the issue of catastrophic forgetting, which can result in models losing their ability to classify base classes. To tackle this problem, a novel task called Novel Class Discovery without Forgetting (NCDwF) is proposed in [21], which presents a more challenging domain derived from NCD. ResTune [27] and FROST [38] are the first isolated training methods to address NCDwF. ResTune incorporates the use of the Hungarian Assignment [24] that concatenates classifiers specific to the old and new classes. However, despite these efforts, ResTune still incorrectly evaluates performance due to confusion between the old and new classes.\nIn this work, since the implementation of NCD tasks on the graph is almost non-existent, we migrate NCD to the node classification task with GNN and propose a more reasonable NC-NCD setting. Specifically, the assumption of NC-NCD differs from that of the current NCD, primarily manifested in the inference phase of performance evaluation for classifications, employing a task-agnostic joint classifier."}, {"title": "2.3 Incremental Learning", "content": "In the past, incremental learning (i.e., continual learning or lifelong learning) has been explored in several areas such as computer vision [17, 46], and reinforcement learning [37]. Essentially, it aims to address the problem of catastrophic forgetting while being flexible enough to continue learning on new tasks.\nIncremental learning can be divided into task-incremental (task-IL) and class-incremental (class-IL) settings, based on whether task indicators are provided to the model during testing:\n\u2022 task-IL: the model needs to reason about the specified task using task indicators, and the model only needs to distinguish between classes within the current task without considering classes from pre-existing tasks.\n\u2022 class-IL: no task indicators are provided, and the model must differentiate between all classes in both current and pre-existing tasks.\nWhile conventional incremental learning methods are abundant, there have been limited efforts to apply incremental learning techniques to graphs, i.e., graph incremental learning (a.k.a. continual graph learning) [1, 4, 7, 23, 42, 53]. Existing approaches on graph incremental learning can be categorized into three types, i.e., memory replay methods (e.g., Experience Replay Graph Neural Network (ER-GNN) [64]) which stores representative nodes which are re-played when learning new tasks; parameter isolation methods (e.g., Hierarchical Prototype Networks (HPNs) [59]) which adaptively select different parameter combinations for different tasks; and regularization based methods (e.g., Topology-aware Weight Preserving (TWP) [26]) which preserves crucial parameters and topologies via regularization.\nFor graph incremental learning tasks regarding node classification, such as the common application scenarios of multi-class classification like citation networks or social networks, it is evident that the class-IL setting is more practical and challenging. However, existing graph incremental learning methods mentioned earlier are limited to a specific set of tasks with labeled data, which restricts its practicality and does not align with our setting.\nIn this paper, our proposed SWORD is the first attempt in the NC-NCD setting on graphs, to effectively mitigate the forgetting of old categories while learning new categories when labeled data are no longer available."}, {"title": "3 METHODOLOGY", "content": "In this section, SWORD are elaborated below. First, several preliminary definitions and notations are given."}, {"title": "3.1 Novel Class Discovery for Node Classification", "content": "3.1.1 Problem Definition and Notation. Formally, a graph G = {V, 8}, where V is the set of N nodes and & is the edge set, can be alternatively represented by G = (A,X). Let A \u2208 \\mathbb{R}^{N\u00d7N} be the adjacency matrix and X \u2208 \\mathbb{R}^{N\u00d7d_o} denotes the node attribute matrix, where \\(d_o\\) denotes the dimension of node attributes. To solve the NC-NCD problem, we split the dataset G into two folds with disjoint categories, G\\(^l\\) and G\\(^u\\) to represent data from the base and new classes respectively. G\\(^l\\) is randomly split into G\\(_{\\text{train}}\\), G\\(_{\\text{val}}\\), and G\\(_{\\text{test}}\\) for pre-training phase. G\\(^u\\) also has three splits of G\\(_{\\text{train}}\\), G\\(_{\\text{val}}\\), G\\(_{\\text{test}}\\). To simplify the writing, we uniformly use G\\(^l\\), G\\(^u\\), G to denote data from base, new and all classes.\nFor supervised task T\\(^l\\), given labeled G\\(^l\\) = {(A, x, y)} with N\\(^l\\) node samples, where x \u2208 X\\(^l\\) represents the feature of the current node and y \u2208 Y is -dimensional one-hot labels. Once the task T\\(^l\\) is completed, G\\(^l\\) is discarded and we get n\u201c instances from the"}, {"title": "3.1.2 Evaluation Protocol", "content": "Previous NCD methods [10, 62] trained task-specific classification heads for the base and new classes respectively. To overcome the limitation of NCD being restricted to specified tasks, ResTune [27] introduced the use of Hungarian Assignment [24] to concatenate these two classification heads to achieve an evaluation of all classes during inference. However, the Hungarian Assignment which is a direct maximum match of the outputs from the two heads, lacks the ability to distinguish between old and new classes when they become completely confused.\nIn the present work, our NC-NCD setting uses a more reasonable evaluation protocol. Specifically, after learning the labeled nodes in base classes, since the unlabeled data in the NCD stage is used for isolated training, we employ a classification head for new classes to assign labels specifically and use pseudo-labels to train a single joint classifier that can classify both the base and new classes. When evaluating the model, the trained task-agnostic joint classifier is used directly without specifying the task-id, and the classification results are inferred on all seen class data, compared with the ground-truth labels of the samples. It is clear that our evaluation protocol is more reasonable, such as being able to discern when a new category is classified as one of the old categories is an error, which is a desirable behavior."}, {"title": "3.1.3 Overall Framework", "content": "Within NC-NCD setting, the function f consists of two major components: a GNN encoder g(\u00b7) to learn node representations and a linear classification head h() to output logits. Our proposed SWORD runs in two phases, a pre-training phase on the supervised task T\\(^l\\) and an NCD-training phase on the unsupervised task T\\(^u\\).\nPre-training. In this stage, we utilize the labeled nodes from G\\(^l\\) to learn the mapping function f\\(^l\\) = h\\(^l\\)\u25e6g in a supervised manner, which identifies samples belonging to the category C\\(^l\\). The feature extractor g and the classifier h\\(^l\\) are parameterized by \\(\\theta_g\\) and \\(\\theta_{h^l}\\), respectively. The latent features of the old category nodes are defined as z\\(^l\\) = g(G\\(^l\\)). We aim to learn these two parameters {\\(\\theta_g\\), \\(\\theta_{h^l}\\)} of model f\\(^l\\) by using supervised cross-entropy loss:\n\\(L_{ce} = - \\frac{1}{||C_l||} \\sum_{k=1}^{||c||} y_k log(\\sigma_k(h^l(z^l)))\\)        (1)\nwhere the softmax function \\(\\sigma_k = \\frac{exp(l_k)}{\\Sigma_j exp(l_j)}\\) represents the likelihood corresponding to the kth output from the model and \\(c\\) is the number of classes in the task T\\(^l\\). In addition, before the next stage, we will compute the feature prototype \\(\\mu_c\\) and variance \\(\\nu_c^2\\) with feature z\\(^l\\) for each class c in G\\(^l\\).\nNCD-training. In the second stage, we learn new categories on the unlabeled data G\\(^u\\) by reusing the parameters from the pre-trained model f\\(^l\\). In the NC-NCD setting, the model f should have a joint classifier to accommodate C = C\\(^l\\) +C\\(^u\\) categories. Therefore, we directly extend the classifier h\\(^l\\) into h and provide a specific classifier h\\(^u\\) separately for the new category\u2019s classification on the T\\(^u\\) task. These two classifiers are parameterized by {\\(\\theta_{h^u}\\), \\(\\theta_{h^a}\\)}. To be specific, we train f\\(^u\\) = h\\(^u\\)\u25e6g, where G\\(^u\\) \u2192 Y\\(^u\\), through the SWORD framework using a clustering objective that provides supervision on previously learned information using robust rank statistics and prevents forgetting of old categories through prototype replay and distillation. Function f = h\\(^a\\)\u25e6g, where G \u2192 Y\\(^l\\)\u222aY\\(^u\\), is used during inference. More details are given in the next section."}, {"title": "3.2 Self-training with Prototype Replay and Distillation", "content": "We aim to learn a model that has a better capability of clustering unlabeled nodes after pre-training with labeled data while preserving the performance on previously seen categories without visiting nor storing previously labeled nodes. Our SWORD framework will achieve the above task requirements through self-training, prototype replay, and distillation, which are explained in detail below."}, {"title": "3.2.1 Pairwise Similarity and Pseudo Label", "content": "In the NCD-training phase, an essential step in learning new classes is to train f\\(^u\\) = h\\(^u\\)\u25e6g. On the one hand, we update the weights of GNN encoder g to maintain the feature extraction capability on new category nodes. On the other hand, we train the task-specific classifier h\\(^u\\), which is initialized and lacks the ability to discriminate the features on unlabeled nodes. We adopt the ideology from the NCD method AutoNovel [15] to infer pairwise similarity between features of a pair of unlabeled nodes and employ it in a weakly supervised form during NCD-training steps. Specifically, given a pair of node-featured pairs (x\\(^i\\), x\\(^j\\)) taken from the unlabeled G\\(^u\\), we use GNN encoder g to compute z\\(^i\\) = g(G) and z\\(^j\\) = g(G), respectively, to obtain the pair of node representations (z\\(^i\\),z\\(^j\\)) on the graph. Then we multiply the node representation pair between the two samples with dot product and normalize it using a logistic function \\(\\sigma(\\cdot)\\). That is, we calculate the pairwise similarity using the following:\n\\(S_{ij} = \\sigma(h^u(z^i) \\odot h^u(z^j)).\\)       (2)\nSince the class of nodes in this stage is unknown, robust rank statistics are utilized to compare the node representation pairs. We consider that if the top-k dimensions of node representation pair (z\\(^i\\), z\\(^j\\)) are the same, the two nodes corresponding to index i and j are considered to belong to the same class. In this way, another pairwise similarity of the two sample nodes is generated, referred to as a pairwise pseudo label to weakly supervise the training of classification head h\\(^u\\) for new classes. The pairwise pseudo labels are formulated as:\n\\(\\hat{y}_{ij} = \\mathbb{1} \\{topk(z^i) = topk(z^j)\\}\\),        (3)\nwhere topk: z\\(^u\\) \u2192 S \\{(1, . . ., k)\\} \u2282 P \\{(1, . . ., ||z\\(^u\\)||\\}\\}. In particular, ranking the values by size in node representation vectors, if z\\(^i\\) and z\\(^j\\) are identical in the top-k most activated dimensions, then pairwise pseudo label \\(\\hat{y}_{ij}\\) = 1 for nodes i and j, and 0 otherwise.\nBased on the pairwise pseudo label \\(\\hat{y}_{ij}\\) and pairwise similarity S\\(_{ij}\\), the pairwise labeling objective with parameters {\\(\\theta_g\\), \\(\\theta_{h^u\\)}\\} is trained with binary cross-entropy loss as:\n\\(L_{Pseudo} = - \\frac{1}{N_u^2} \\sum (\\hat{y}_{ij} log(S_{ij}) + (1 - \\hat{y}_{ij}) log(1 - S_{ij})).\\)   (4)\nThis equation trains a task-specific classifier for the learning of new category nodes, which allows the model to cluster unlabeled nodes on task T\\(^u\\) for the next NCD-training steps."}, {"title": "3.2.2 Self-training with Joint Head", "content": "To accommodate the NC-NCD setting, the inference step of our training framework should ideally not depend on task-id. We thus train the joint classifier h\\(^a\\) by self-training with the help of pseudo labels computed from f\\(^u\\) = h\\(^u\\)\u25e6g. In summary, given the goal of the learning model f, we utilize the task-specific classification head h\\(^u\\) to assign the pseudo label \\(\\hat{y}^u\\) to the unlabeled nodes and use this pseudo label to supervise the training of the joint classification head h\\(^a\\), where the pseudo label \\(\\hat{y}^u\\) of unlabeled nodes can be computed as:\n\\(\\hat{y} = ||C_l|| + \\underset{k \\in ||C_u||} {argmax} \\, h_k^u(z^u).\\)           (5)\nThe self-training loss is described as:\n\\(L_{Self} = - \\frac{1}{||C_u||} \\sum_{c=1}^{||C_u||} log \\sigma_k(h^a (z^u)).\\)                (6)"}, {"title": "3.2.3 Encoder Output Perturbation", "content": "Since the pairwise pseudo labels obtained in Eq.(3) can be noisy, it will lead to poor supervised training of h\\(^u\\). Then it will affect the pseudo labels in Eq.(5), which adversely affects the training of the joint classifier h\\(^a\\). Therefore, to minimize the impact of the noise cascade propagation from h\\(^u\\), inspired by stochastic data augmentation, instead of changing the structural information of the graph itself, we add perturbations to encoder output features on node representation level [51] to perturb classifier h\\(^u\\). Specifically, we directly add the perturbation of random Gaussian noise to the node representation z\\(^u\\) obtained with GNN encoder, described mathematically as,\n\\(z_k^u = z_k^u + \\eta \\cdot \\Delta z_k;  \\,  \\Delta z_k \\sim \\mathcal{N}(0, \\sigma^2),\\)                   (7)\nwhere \\(\\eta\\) is the coefficient that scales the magnitude of the perturbation, and \\(\\Delta z_k\\) is the perturbation term which samples from Gaussian distribution with zero mean and variance \\(\\sigma^2\\). The h\\(^u\\) is reused for representation with perturbation z\\(^{u'}\\) and original z\\(^u\\), with a mean-squared error loss to further optimize the parameters {\\(\\theta_g\\), \\(\\theta_{h^u\\)}\\):\n\\(L_{Perturb} = \\frac{1}{||C_u||} \\sum_{k=1}^{||cu||} \\sum [\\sigma_k(h^a (z^u)) - \\sigma_k(h^a (z^{u'}))]^2.\\)      (8)"}, {"title": "3.2.4 Prototype Replay and Distillation", "content": "Although the self-training described above helps the model f = h\\(^a\\) \u25e6 g discover new categories, at the same time it loses the ability to predict old categories on task T\\(^u\\). So, we propose feature-level prototype replay and distillation. In the NC-NCD setting, with labeled data G\\(^l\\) on task T\\(^u\\) that has been discarded, we use the node representation z\\(^l\\) at the end of supervised training for task T\\(^l\\) to compute and record the prototype \\(\\mu_c\\) and the variance \\(\\nu_c^2\\) in advance for each base class as:\n\\(\\mu_c = \\frac{1}{N} \\sum_{i=1}^{N_c} g(G^l), \\,   \\nu_c^2 = \\frac{1}{N} \\sum_{i=1}^{N_c} [g(G^l) - \\mu_c]^2,\\)          (9)\nwhere the number of samples belonging to class c in labeled data G\\(^l\\) is N\\(_c\\).\nIn the task T\\(^u\\) of learning new nodes, we specifically replay these prototype features from the class-specific Gaussian distribution \\(\\mathcal{N} (\\mu_c, \\nu_c^2)\\), in order to update the parameter \\(\\theta_{h^a}\\) and maintain the performance on base classes. The prototype replay loss is given as:\n\\(L_{Replay} = - \\frac{1}{||C_l||} \\sum_{c=1}^{||C||} log \\sigma_k(h^a (z_c^v)),\\)    (10)\nwhere \\((z_c^v) \\sim \\mathcal{N} (\\mu_c, \\nu_c^2)\\).\nIn addition, the parameters of the feature extractor g are optimized and updated during the self-training step, and the encoder output perturbation only updates the parameters \\(\\theta_g\\) based on new category nodes, which can cause the prototype of the base class to become obsolete. To prevent the encoder from losing its effectiveness for prototype feature extraction, we add an additional regularization to g by feature distillation, given as follows:\n\\(L_{Distill} = \\frac{1}{N} \\sum ||g^\\prime (G) - g(G)||^2,\\)                    (11)\nwhere g\\(^\\prime\\) is the feature extractor from the previous task and is kept frozen with parameters \\(\\theta_g^\\prime\\). Our objective is to encourage the alignment of the feature extractors, denoted as g and g\\(^\\prime\\), with the distribution of new category data, minimizing their differences. This alignment facilitates the knowledge distillation from the old model to the new model."}, {"title": "3.3 Overall Training", "content": "As aforementioned, the above five loss functions can be mainly divided into two types, one is the novel classes self-training loss to ensure the discovery and clustering of unlabeled nodes in the NCD-training stage, and the other is the base classes replaying loss to prevent the forgetting of the old class nodes in the previous task loss. Novel classes self-training loss can be written as:\n\\(L_{Novel} = L_{Pseudo} + \\beta_1 L_{Self} + \\beta_2 L_{Perturb},\\) (12)\nwhere \\(\\beta_1\\) and \\(\\beta_2\\) are ramp-up functions to ensure stability in learning. Base classes replaying loss can be formulated as:\n\\(L_{Base} = L_{Replay} + \\omega_{fd} L_{Distill},\\)              (13)\nwhere \\(\\omega_{fd}\\) is a coefficient set to 10 by default from FROST [38]. In addition, the parameter \\(\\lambda\\) is introduced to provide the model with the ability to better balance the performance of base and new classes. The overall loss is defined as the combination of the novel classes self-training loss and the base classes replaying loss:\n\\(L = L_{Novel} + \\lambda L_{Base}\\)                    (14)\nThe overall process of SWORD is shown in Algorithm 1 in Appendix A.1."}, {"title": "4 EXPERIMENT", "content": "4.1 Experimental Setups\nDataset Settings. We evaluate the proposed SWORD on four representative datasets: Cora [29], Citeseer [12], Pubmed [39] and Wiki-CS [30], whose details are provided in Appendix A.2. We also provide statistics and splitting for the datasets in Table 6.\nBaselines. We choose representative GNNs as backbones including GCN [22], GAT [41] and GraphSAGE [14]. Additionally, we compare our SWORD with several SOTA baselines, including 4 traditional NCD methods (AutoNovel [15], ResTune [27], NCL [62] and DTC [16]), and 4 graph incremental learning methods in both task-IL and class-IL settings (GEM [28], ER-GNN [64], TWP [26] and CPCA [36]). Table 2 provides a further summary and comparison of these baseline methods with our SWORD. A more detailed analysis of these baseline methods is described in Appendix A.3. We also provide implementation details and parameter settings in Appendix A.4."}, {"title": "4.2 Comparison with SOTA Methods", "content": "Under our practical setting for node classification (i.e., NC-NCD), we compare SWORD with the NCD baseline methods and record the performance on the old, new and all categories after unified classification using the joint classifier ha on all unlabeled samples, as shown in Table 3. The best results are highlighted in bold, while the runner-ups are underlined. From the comprehensive views, we make the following observations and analyses."}, {"title": "4.2.1 Analysis of Performance Comparison", "content": "First, our SWORD stands out among the baseline methods as it effectively balances the classification of old and new categories. After learning old category nodes on task T\\(^l\\) and learning new category nodes on task T\\(^u\\), SWORD successfully distinguishes between old and new categories using the joint classifier and performs better on the classification of all category nodes.\nIn contrast, most of the NCD baseline methods struggle to differentiate new nodes using task-agnostic joint classifiers, prioritizing the maintenance of performance on old classes. Although these NCD methods fail to classify new category nodes in the NC-NCD setting, they still achieve better overall performance by relying on the classification of old categories. We acknowledge that the performance on all categories is largely influenced by the classification accuracy on the old categories. Further analysis with confusion matrices of this phenomenon will be conducted in Appendix A.5.2. Regarding the aforementioned graph incremental learning methods, they require labeled nodes for both new and old categories during phase 2 when learning new categories. Therefore, \u0421\u0420\u0421\u0410 in the class-IL setting fails to differentiate new categories in the absence of new category labels. In the context of task-IL methods, task-ids need to be specified for evaluation under the assumption of known category membership (whether a category belongs to the new or old set). By leveraging the powerful node representation learning capabilities of GNNs, these methods excel in the classification of new category nodes. However, in the NC-NCD setting where labeled data for old categories is absent, they struggle to differentiate among old categories, resulting in weak performance across all categories. Due to the loss of old category node classification capability and the inability to balance the classification performance between new and old categories, the aforementioned task-IL methods prove inadequate for NC-NCD tasks.\nIn addition, we utilize average accuracy (AA) and average forgetting (AF) [58] to assess the average performance in learning new classes and the resistance to forgetting in Appendix A.5.1."}, {"title": "4.2.2 Performance on Multiple Backbones", "content": "We implement SWORD with multiple backbones and evaluate the node classification accuracy for both old and new categories. Our results show that SWORD consistently outperforms other SOTA methods in achieving a balanced performance on old and new categories. Specifically"}, {"title": "NC2D: Novel Class Discovery for Node Classification", "authors": ["Yue Hou", "Xueyuan Chen", "He Zhu", "Ruomei Liu", "Bowen Shi", "Jiaheng Liu", "Junran Wu", "Ke Xu"], "abstract": "Novel Class Discovery (NCD) involves identifying new categories within unlabeled data by utilizing knowledge acquired from previously established categories. However, existing NCD methods often struggle to maintain a balance between the performance of old and new categories. Discovering unlabeled new categories in a class-incremental way is more practical but also more challenging, as it is frequently hindered by either catastrophic forgetting of old categories or an inability to learn new ones. Furthermore, the implementation of NCD on continuously scalable graph-structured data remains an under-explored area. In response to these challenges, we introduce for the first time a more practical NCD scenario for node classification (i.e., NC-NCD), and propose a novel self-training framework with prototype replay and distillation called SWORD, adopted to our NC-NCD setting. Our approach enables the model to cluster unlabeled new category nodes after learning labeled nodes while preserving performance on old categories without reliance on old category nodes. SWORD achieves this by employing a self-training strategy to learn new categories and preventing the forgetting of old categories through the joint use of feature prototypes and knowledge distillation. Extensive experiments on four common benchmarks demonstrate the superiority of SWORD over other state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph data is commonly used to reveal interactions between various entities, such as academic graphs, social networks, recommendation systems, etc [47\u201349, 65, 66]. In the last few years, node classification has received considerable attention [34, 43, 45, 50, 56] with the rise of significant advances in graph neural networks (GNNs) [22]. Most of the existing works on node classification primarily focus on a single task, where the model is tasked to classify unlabeled nodes into fixed classes [20, 22, 32, 68]. In practice, however, many graph data grow continuously, generating new classes as new nodes and edges emerge, for example, in a citation network, the appearance of new papers and their related citations, or even new interdisciplinary; the addition of new users leads to the emergence of new social groups. The categories of nodes are gradually expanding, usually accompanied by few labels due to their new emergence or lack of exploration. This requires new category discovery on graphs using GNNs, which currently has limited research on tasks regarding node classification.\nThe task of automatically discovering new classes in an unsupervised manner while utilizing previously acquired knowledge is called novel class discovery (NCD) [10, 15, 16, 62, 63]. NCD has recently received widespread attention for its ability to effectively learn new classes without relying on large amounts of labeled data.\nMost of the currently proposed NCD solutions employ stage-wise [16, 18, 19] or joint [10, 15, 62] learning schemes for labeled and unlabeled data. It has been demonstrated that NCD achieves superior learning performance when clustering unlabeled data and jointly training with labeled data [15, 62]. However, joint training relies on the availability of labeled data, which presents two limitations: (1) labeled data may become unavailable after the pre-training phase due to practical issues such as storage constraints or privacy protection; (2) retraining labeled data from old classes can make the entire learning process more cumbersome and expensive. Therefore, a more practical stage-wise NCD setting is needed, where labeled data is discarded and only pre-trained models can be migrated for learning new classes. Although such a training scheme appears more practical, it gradually leads to the elimination of all previously acquired information about base (or old) classes. This phenomenon is known as catastrophic forgetting in neural networks [8], where the model\u2019s performance on previous tasks experiences a significant decline after being trained on a new task.\nIn light of the above analysis, we propose a novel class discovery task for node classification on graphs called NC-NCD. The key insight of the NC-NCD setting is to learn a model with a stage-wise scheme that effectively clusters unlabeled nodes from new classes by leveraging knowledge and potential commonalities from labeled nodes belonging to old classes. Given the inherent limitations of existing NCD settings [27], we argue that the ideal NCD approach is supposed to focus on strictly learning new classes using only unlabeled nodes while simultaneously maintaining performance on the base classes. Moreover, most of the existing NCD methods mentioned above, whether trained in a stage-wise or joint manner, prioritize learning the current task without considering the global context of all classes. We find these approaches impractical in real-world scenarios, as models adapted to new classes become unusable for base classes and retraining is infeasible. Inspired by class-IL [40], NC-NCD evaluates the performance on nodes from all classes in the inference phase, thereby overcoming the reliance on the task-id of both old and new classes and truly achieving task-agnostic NCD.\nNotice that the NC-NCD setting is essentially different from incremental learning and NCD, and Figure 1 illustrates the distinctions among these settings. The goal of NC-NCD is to continue training the model using unlabeled nodes of new categories after learning knowledge from old categories without requiring old category data, which enables the model to classify all categories. The key distinction between our NC-NCD and the NCD setting lies in the fact that NC-NCD additionally demands the model to maintain performance on old categories, unlike NCD, which focuses solely on the model\u2019s performance on new categories. Novel Class Discovery without Forgetting (NCDwF) [21] is closely related to NCD but it relaxes certain key assumptions. Notably, existing NCD methods assume access to both labeled and unlabeled data during training, a condition often unrealistic in real-world scenarios. Both NCDwF and NC-NCD emphasize the non-concurrent utilization of labeled and unlabeled data during training. However, in contrast to NC-NCD, NCDwF fails to train a classifier capable of predicting across the entire label space. Consequently, the distinction between new and old categories in NCDwF during evaluation still necessitates the utilization of task-ids. Incremental learning can be categorized into task increment (task-IL) and class increment learning (class-IL), aiming to prevent catastrophic forgetting during the learning of a series of categories that follow a different setting. Specifically, as depicted in Table 1, after learning old category nodes, NC-NCD trains on unlabeled nodes of new categories, while task-IL and class-IL require training on both labeled old category and new category nodes. NCD only utilizes new category nodes for training and testing. During the evaluation, task-IL needs to specify the task-id in a task-specific manner, allowing the model to test old or new category nodes with the knowledge of whether the current categories are old or new. In contrast, our NC-NCD distinguishes in a task-agnostic way between all categories seen during evaluation. Additionally, NC-NCD liberates itself from the constraint of maintaining a consistent number of categories learned in each phase. Clearly, our NC-NCD setting is more aligned with real-world application scenarios.\nIn this work, we propose a novel Self-training With prototype Replay and Distillation framework named SWORD, to implement the NC-NCD task effectively. Specifically, an encoder for extracting node features is first pre-trained on the labeled old category nodes and used directly in the NCD-training phase. To facilitate the learning of unlabeled new categories, we set up a classifier specific to the new classes, optimized with robust rank statistics. Besides, since the NC-NCD setting is task-agnostic, we maintain a joint classifier that is trained with pseudo-labels generated by a task-specific classifier, to classify all the classes encountered throughout the learning process. Inspired by the effectiveness of replay-based incremental learning [3, 5, 35], we replay the prototype of base class nodes after clustering to prevent catastrophic forgetting of base classes. In addition to features encoded from the prototype, feature-level knowledge distillation is employed to uphold the effectiveness of the model\u2019s feature extraction. The main contributions are as follows:\n\u2022 Contrary to existing research in incremental learning and NCD, we introduce a more practical NCD setting for node classification, named NC-NCD.\n\u2022 We propose a novel framework, SWORD, adapted to our NC-NCD setting to address the challenges in the current NCD scenarios formulated with node classification tasks.\n\u2022 Our SWORD trains a task-agnostic joint classifier with a self-training strategy in the NCD-training phase and employs prototype replay and knowledge distillation to prevent forgetting.\n\u2022 Extensive experiments are conducted with multi-backbones to confirm the effectiveness of SWORD by comparing with the state-of-the-art (SOTA) methods on various benchmarks regarding node classification."}, {"title": "2 RELATED WORK", "content": "As information grows, the task of discovering new categories from data and making accurate classification predictions has become a popular research direction [15, 16, 27, 38, 62, 67]. Here, we devote our attention to NCD for node classification, the most relevant topic of this work."}, {"title": "2.1 Graph Representation Learning", "content": "Graph representation learning aims to encode both the feature information from nodes and the topological structure of the incoming graph. Inspired by word2vec [31], several methods, such as DeepWalk [33] and node2vec [13], generate sequences of nodes by random walks, and apply skip-gram approach to map nodes within the same context to similar vector representations. Although these methods show success in various tasks like node classification and link prediction, they overemphasize proximity information at the expense of structural information and fail to incorporate node attributes.\nRecently, GNNs have received a great deal of attention, such as graph convolutional networks (GCNs) [22], GraphSAGE [14], graph attention networks (GATs) [41] and their extensions [2, 6, 11, 25, 52, 60, 61]. GNNs take in high-dimensional node features and map them to low-dimensional vector embeddings that are used for different downstream tasks. Based on methods of aggregation, GNNs are mainly classified into spectral and spatial embedding. The spectral approaches [9, 22] learn graph representations by the Fourier transform of graph data, while spatial approaches perform simple spatial aggregation of features from neighbors [14, 41]. However, most of these works are designed for a single learning task and are not directly applicable to incremental learning problems, thus encountering catastrophic forgetting problems when learning a series of new tasks."}, {"title": "2.2 Novel Class Discovery", "content": "NCD task is to migrate knowledge learned from labeled datasets to unlabelled ones to discover new classes, assuming that the classes in labeled and unlabelled sets are disjoint. The proposed NCD methods can be broadly divided into two categories. The first category of approach utilizes a joint training scheme, assuming the availability of both labeled and unlabeled data [44, 55, 57]. The other category employs a stage-wise isolated training scheme, where data from old and new classes cannot be used simultaneously. In this context, the labeled data of the base classes are used only during the supervised pre-training phase and not thereafter. Instead, an unsupervised clustering loss is employed to fine-tune unlabelled data during the phase of discovering unknown new classes [19, 54, 62].\nAccording to [27], NCD methods that rely on joint training always outperform methods with isolated training. However, it is not always practical to have access to labeled data after the pre-training phase due to concerns about data privacy or storage limitations. Therefore, training on the old and new classes jointly may not be a viable option in many cases. Moreover, earlier approaches based on isolated training schemes fail to adequately address the issue of catastrophic forgetting, which can result in models losing their ability to classify base classes. To tackle this problem, a novel task called Novel Class Discovery without Forgetting (NCDwF) is proposed in [21], which presents a more challenging domain derived from NCD. ResTune [27] and FROST [38] are the first isolated training methods to address NCDwF. ResTune incorporates the use of the Hungarian Assignment [24] that concatenates classifiers specific to the old and new classes. However, despite these efforts, ResTune still incorrectly evaluates performance due to confusion between the old and new classes.\nIn this work, since the implementation of NCD tasks on the graph is almost non-existent, we migrate NCD to the node classification task with GNN and propose a more reasonable NC-NCD setting. Specifically, the assumption of NC-NCD differs from that of the current NCD, primarily manifested in the inference phase of performance evaluation for classifications, employing a task-agnostic joint classifier."}, {"title": "2.3 Incremental Learning", "content": "In the past, incremental learning (i.e., continual learning or lifelong learning) has been explored in several areas such as computer vision [17, 46], and reinforcement learning [37]. Essentially, it aims to address the problem of catastrophic forgetting while being flexible enough to continue learning on new tasks.\nIncremental learning can be divided into task-incremental (task-IL) and class-incremental (class-IL) settings, based on whether task indicators are provided to the model during testing:\n\u2022 task-IL: the model needs to reason about the specified task using task indicators, and the model only needs to distinguish between classes within the current task without considering classes from pre-existing tasks.\n\u2022 class-IL: no task indicators are provided, and the model must differentiate between all classes in both current and pre-existing tasks.\nWhile conventional incremental learning methods are abundant, there have been limited efforts to apply incremental learning techniques to graphs, i.e., graph incremental learning (a.k.a. continual graph learning) [1, 4, 7, 23, 42, 53]. Existing approaches on graph incremental learning can be categorized into three types, i.e., memory replay methods (e.g., Experience Replay Graph Neural Network (ER-GNN) [64]) which stores representative nodes which are re-played when learning new tasks; parameter isolation methods (e.g., Hierarchical Prototype Networks (HPNs) [59]) which adaptively select different parameter combinations for different tasks; and regularization based methods (e.g., Topology-aware Weight Preserving (TWP) [26]) which preserves crucial parameters and topologies via regularization.\nFor graph incremental learning tasks regarding node classification, such as the common application scenarios of multi-class classification like citation networks or social networks, it is evident that the class-IL setting is more practical and challenging. However, existing graph incremental learning methods mentioned earlier are limited to a specific set of tasks with labeled data, which restricts its practicality and does not align with our setting.\nIn this paper, our proposed SWORD is the first attempt in the NC-NCD setting on graphs, to effectively mitigate the forgetting of old categories while learning new categories when labeled data are no longer available."}, {"title": "3 METHODOLOGY", "content": "In this section, SWORD are elaborated below. First, several preliminary definitions and notations are given."}, {"title": "3.1 Novel Class Discovery for Node Classification", "content": "3.1.1 Problem Definition and Notation. Formally, a graph G = {V, 8}, where V is the set of N nodes and & is the edge set, can be alternatively represented by G = (A,X). Let A \u2208 \\(\\mathbb{R}^{N\u00d7N}\\) be the adjacency matrix and X \u2208 \\(\\mathbb{R}^{N\u00d7d_o}\\) denotes the node attribute matrix, where \\((d_o\\)\\) denotes the dimension of node attributes. To solve the NC-NCD problem, we split the dataset G into two folds with disjoint categories, G\\(^l\\) and G\\(^u\\) to represent data from the base and new classes respectively. G\\(^l\\) is randomly split into G\\((_{\\text{train}}\\), G\\((_{\\text{val}}\\), and G\\((_{\\text{test}}\\) for pre-training phase. G\\(^u\\) also has three splits of G\\((_{\\text{train}}\\), G\\((_{\\text{val}}\\), G\\((_{\\text{test}}\\). To simplify the writing, we uniformly use G\\(^l\\), G\\(^u\\), G to denote data from base, new and all classes.\nFor supervised task T\\(^l\\), given labeled G\\(^l\\) = {(A, x, y)} with N\\(^l\\) node samples, where x \u2208 X\\(^l\\) represents the feature of the current node and y \u2208 Y is -dimensional one-hot labels. Once the task T\\(^l\\) is completed, G\\(^l\\) is discarded and we get n\u201c instances from the"}, {"title": "3.1.2 Evaluation Protocol", "content": "Previous NCD methods [10, 62] trained task-specific classification heads for the base and new classes respectively. To overcome the limitation of NCD being restricted to specified tasks, ResTune [27] introduced the use of Hungarian Assignment [24] to concatenate these two classification heads to achieve an evaluation of all classes during inference. However, the Hungarian Assignment which is a direct maximum match of the outputs from the two heads, lacks the ability to distinguish between old and new classes when they become completely confused.\nIn the present work, our NC-NCD setting uses a more reasonable evaluation protocol. Specifically, after learning the labeled nodes in base classes, since the unlabeled data in the NCD stage is used for isolated training, we employ a classification head for new classes to assign labels specifically and use pseudo-labels to train a single joint classifier that can classify both the base and new classes. When evaluating the model, the trained task-agnostic joint classifier is used directly without specifying the task-id, and the classification results are inferred on all seen class data, compared with the ground-truth labels of the samples. It is clear that our evaluation protocol is more reasonable, such as being able to discern when a new category is classified as one of the old categories is an error, which is a desirable behavior."}, {"title": "3.1.3 Overall Framework", "content": "Within NC-NCD setting, the function f consists of two major components: a GNN encoder g(\u00b7) to learn node representations and a linear classification head h() to output logits. Our proposed SWORD runs in two phases, a pre-training phase on the supervised task T\\(^l\\) and an NCD-training phase on the unsupervised task T\\(^u\\).\nPre-training. In this stage, we utilize the labeled nodes from G\\(^l\\) to learn the mapping function f\\(^l\\) = h\\(^l\\)\u25e6g in a supervised manner, which identifies samples belonging to the category C\\(^l\\). The feature extractor g and the classifier h\\(^l\\) are parameterized by \\(\\theta_g\\) and \\(\\theta_{h^l}\\), respectively. The latent features of the old category nodes are defined as z\\(^l\\) = g(G\\(^l\\)). We aim to learn these two parameters {\\(\\theta_g\\), \\(\\theta_{h^l}\\)} of model f\\(^l\\) by using supervised cross-entropy loss:\n\\(L_{ce} = - \\frac{1}{||C_l||} \\sum_{k=1}^{||c||} y_k log(\\sigma_k(h^l(z^l)))\\)        (1)\nwhere the softmax function \\(\\sigma_k = \\frac{exp(l_k)}{\\Sigma_j exp(l_j)}\\) represents the likelihood corresponding to the kth output from the model and \\(c\\) is the number of classes in the task T\\(^l\\). In addition, before the next stage, we will compute the feature prototype \\(\\mu_c\\) and variance \\(\\nu_c^2\\) with feature z\\(^l\\) for each class c in G\\(^l\\).\nNCD-training. In the second stage, we learn new categories on the unlabeled data G\\(^u\\) by reusing the parameters from the pre-trained model f\\(^l\\). In the NC-NCD setting, the model f should have a joint classifier to accommodate C = C\\(^l\\) +C\\(^u\\) categories. Therefore, we directly extend the classifier h\\(^l\\) into h and provide a specific classifier h\\(^u\\) separately for the new category\u2019s classification on the T\\(^u\\) task. These two classifiers are parameterized by {\\(\\theta_{h^u}\\), \\(\\theta_{h^a}\\)}. To be specific, we train f\\(^u\\) = h\\(^u\\)\u25e6g, where G\\(^u\\) \u2192 Y\\(^u\\), through the SWORD framework using a clustering objective that provides supervision on previously learned information using robust rank statistics and prevents forgetting of old categories through prototype replay and distillation. Function f = h\\(^a\\)\u25e6g, where G \u2192 Y\\(^l\\)\u222aY\\(^u\\), is used during inference. More details are given in the next section."}, {"title": "3.2 Self-training with Prototype Replay and Distillation", "content": "We aim to learn a model that has a better capability of clustering unlabeled nodes after pre-training with labeled data while preserving the performance on previously seen categories without visiting nor storing previously labeled nodes. Our SWORD framework will achieve the above task requirements through self-training, prototype replay, and distillation, which are explained in detail below."}, {"title": "3.2.1 Pairwise Similarity and Pseudo Label", "content": "In the NCD-training phase, an essential step in learning new classes is to train f\\(^u\\) = h\\(^u\\)\u25e6g. On the one hand, we update the weights of GNN encoder g to maintain the feature extraction capability on new category nodes. On the other hand, we train the task-specific classifier h\\(^u\\), which is initialized and lacks the ability to discriminate the features on unlabeled nodes. We adopt the ideology from the NCD method AutoNovel [15] to infer pairwise similarity between features of a pair of unlabeled nodes and employ it in a weakly supervised form during NCD-training steps. Specifically, given a pair of node-featured pairs (x\\(^i\\), x\\(^j\\)) taken from the unlabeled G\\(^u\\), we use GNN encoder g to compute z\\(^i\\) = g(G) and z\\(^j\\) = g(G), respectively, to obtain the pair of node representations (z\\(^i\\),z\\(^j\\)) on the graph. Then we multiply the node representation pair between the two samples with dot product and normalize it using a logistic function \\(\\sigma(\\cdot)\\). That is, we calculate the pairwise similarity using the following:\n\\(S_{ij} = \\sigma(h^u(z^i) \\odot h^u(z^j)).\\)       (2)\nSince the class of nodes in this stage is unknown, robust rank statistics are utilized to compare the node representation pairs. We consider that if the top-k dimensions of node representation pair (z\\(^i\\), z\\(^j\\)) are the same, the two nodes corresponding to index i and j are considered to belong to the same class. In this way, another pairwise similarity of the two sample nodes is generated, referred to as a pairwise pseudo label to weakly supervise the training of classification head h\\(^u\\) for new classes. The pairwise pseudo labels are formulated as:\n\\(\\hat{y}_{ij} = \\mathbb{1} \\{topk(z^i) = topk(z^j)\\}\\),        (3)\nwhere topk: z\\(^u\\) \u2192 S \\{(1, . . ., k)\\} \u2282 P \\{(1, . . ., ||z\\(^u\\)||\\}\\}. In particular, ranking the values by size in node representation vectors, if z\\(^i\\) and z\\(^j\\) are identical in the top-k most activated dimensions, then pairwise pseudo label \\(\\hat{y}_{ij}\\) = 1 for nodes i and j, and 0 otherwise.\nBased on the pairwise pseudo label \\(\\hat{y}_{ij}\\) and pairwise similarity S\\(_{ij}\\), the pairwise labeling objective with parameters {\\(\\theta_g\\), \\(\\theta_{h^u\\)}\\} is trained with binary cross-entropy loss as:\n\\(L_{Pseudo} = - \\frac{1}{N_u^2} \\sum (\\hat{y}_{ij} log(S_{ij}) + (1 - \\hat{y}_{ij}) log(1 - S_{ij})).\\)   (4)\nThis equation trains a task-specific classifier for the learning of new category nodes, which allows the model to cluster unlabeled nodes on task T\\(^u\\) for the next NCD-training steps."}, {"title": "3.2.2 Self-training with Joint Head", "content": "To accommodate the NC-NCD setting, the inference step of our training framework should ideally not depend on task-id. We thus train the joint classifier h\\(^a\\) by self-training with the help of pseudo labels computed from f\\(^u\\) = h\\(^u\\)\u25e6g. In summary, given the goal of the learning model f, we utilize the task-specific classification head h\\(^u\\) to assign the pseudo label \\(\\hat{y}^u\\) to the unlabeled nodes and use this pseudo label to supervise the training of the joint classification head h\\(^a\\), where the pseudo label \\(\\hat{y}^u\\) of unlabeled nodes can be computed as:\n\\(\\hat{y} = ||C_l|| + \\underset{k \\in ||C_u||} {argmax} \\, h_k^u(z^u).\\)           (5)\nThe self-training loss is described as:\n\\(L_{Self} = - \\frac{1}{||C_u||} \\sum_{c=1}^{||C_u||} log \\sigma_k(h^a (z^u)).\\)                (6)"}, {"title": "3.2.3 Encoder Output Perturbation", "content": "Since the pairwise pseudo labels obtained in Eq.(3) can be noisy, it will lead to poor supervised training of h\\(^u\\). Then it will affect the pseudo labels in Eq.(5), which adversely affects the training of the joint classifier h\\(^a\\). Therefore, to minimize the impact of the noise cascade propagation from h\\(^u\\), inspired by stochastic data augmentation, instead of changing the structural information of the graph itself, we add perturbations to encoder output features on node representation level [51] to perturb classifier h\\(^u\\). Specifically, we directly add the perturbation of random Gaussian noise to the node representation z\\(^u\\) obtained with GNN encoder, described mathematically as,\n\\(z_k^u = z_k^u + \\eta \\cdot \\Delta z_k;  \\,  \\Delta z_k \\sim \\mathcal{N}(0, \\sigma^2),\\)                   (7)\nwhere \\(\\eta\\) is the coefficient that scales the magnitude of the perturbation, and \\(\\Delta z_k\\) is the perturbation term which samples from Gaussian distribution with zero mean and variance \\(\\sigma^2\\). The h\\(^u\\) is reused for representation with perturbation z\\(^{u'}\\) and original z\\(^u\\), with a mean-squared error loss to further optimize the parameters {\\(\\theta_g\\), \\(\\theta_{h^u\\)}\\):\n\\(L_{Perturb} = \\frac{1}{||C_u||} \\sum_{k=1}^{||cu||} \\sum [\\sigma_k(h^a (z^u)) - \\sigma_k(h^a (z^{u'}))]^2.\\)      (8)"}, {"title": "3.2.4 Prototype Replay and Distillation", "content": "Although the self-training described above helps the model f = h\\(^a\\) \u25e6 g discover new categories, at the same time it loses the ability to predict old categories on task T\\(^u\\). So, we propose feature-level prototype replay and distillation. In the NC-NCD setting, with labeled data G\\(^l\\) on task T\\(^u\\) that has been discarded, we use the node representation z\\(^l\\) at the end of supervised training for task T\\(^l\\) to compute and record the prototype \\(\\mu_c\\) and the variance \\(\\nu_c^2\\) in advance for each base class as:\n\\(\\mu_c = \\frac{1}{N} \\sum_{i=1}^{N_c} g(G^l), \\,   \\nu_c^2 = \\frac{1}{N} \\sum_{i=1}^{N_c} [g(G^l) - \\mu_c]^2,\\)          (9)\nwhere the number of samples belonging to class c in labeled data G\\(^l\\) is N\\(_c\\).\nIn the task T\\(^u\\) of learning new nodes, we specifically replay these prototype features from the class-specific Gaussian distribution \\(\\mathcal{N} (\\mu_c, \\nu_c^2)\\), in order to update the parameter \\(\\theta_{h^a}\\) and maintain the performance on base classes. The prototype replay loss is given as:\n\\(L_{Replay} = - \\frac{1}{||C_l||} \\sum_{c=1}^{||C||} log \\sigma_k(h^a (z_c^v)),\\)    (10)\nwhere \\((z_c^v) \\sim \\mathcal{N} (\\mu_c, \\nu_c^2)\\).\nIn addition, the parameters of the feature extractor g are optimized and updated during the self-training step, and the encoder output perturbation only updates the parameters \\(\\theta_g\\) based on new category nodes, which can cause the prototype of the base class to become obsolete. To prevent the encoder from losing its effectiveness for prototype feature extraction, we add an additional regularization to g by feature distillation, given as follows:\n\\(L_{Distill} = \\frac{1}{N} \\sum ||g^\\prime (G) - g(G)||^2,\\)                    (11)\nwhere g\\(^\\prime\\) is the feature extractor from the previous task and is kept frozen with parameters \\(\\theta_g^\\prime\\). Our objective is to encourage the alignment of the feature extractors, denoted as g and g\\(^\\prime\\), with the distribution of new category data, minimizing their differences. This alignment facilitates the knowledge distillation from the old model to the new model."}, {"title": "3.3 Overall Training", "content": "As aforementioned, the above five loss functions can be mainly divided into two types, one is the novel classes self-training loss to ensure the discovery and clustering of unlabeled nodes in the NCD-training stage, and the other is the base classes replaying loss to prevent the forgetting of the old class nodes in the previous task loss. Novel classes self-training loss can be written as:\n\\(L_{Novel} = L_{Pseudo} + \\beta_1 L_{Self} + \\beta_2 L_{Perturb},\\) (12)\nwhere \\(\\beta_1\\) and \\(\\beta_2\\) are ramp-up functions to ensure stability in learning. Base classes replaying loss can be formulated as:\n\\(L_{Base} = L_{Replay} + \\omega_{fd} L_{Distill},\\)              (13)\nwhere \\(\\omega_{fd}\\) is a coefficient set to 10 by default from FROST [38]. In addition, the parameter \\(\\lambda\\) is introduced to provide the model with the ability to better balance the performance of base and new classes. The overall loss is defined as the combination of the novel classes self-training loss and the base classes replaying loss:\n\\(L = L_{Novel} + \\lambda L_{Base}\\)                    (14)\nThe overall process of SWORD is shown in Algorithm 1 in Appendix A.1."}, {"title": "4 EXPERIMENT", "content": "4.1 Experimental Setups\nDataset Settings. We evaluate the proposed SWORD on four representative datasets: Cora [29], Citeseer [12], Pubmed [39] and Wiki-CS [30], whose details are provided in Appendix A.2. We also provide statistics and splitting for the datasets in Table 6.\nBaselines. We choose representative GNNs as backbones including GCN [22], GAT [41] and GraphSAGE [14]. Additionally, we compare our SWORD with several SOTA baselines, including 4 traditional NCD methods (AutoNovel [15], ResTune [27], NCL [62] and DTC [16]), and 4 graph incremental learning methods in both task-IL and class-IL settings (GEM [28], ER-GNN [64], TWP [26] and CPCA [36]). Table 2 provides a further summary and comparison of these baseline methods with our SWORD. A more detailed analysis of these baseline methods is described in Appendix A.3. We also provide implementation details and parameter settings in Appendix A.4."}, {"title": "4.2 Comparison with SOTA Methods", "content": "Under our practical setting for node classification (i.e., NC-NCD), we compare SWORD with the NCD baseline methods and record the performance on the old, new and all categories after unified classification using the joint classifier ha on all unlabeled samples, as shown in Table 3. The best results are highlighted in bold, while the runner-ups are underlined. From the comprehensive views, we make the following observations and analyses."}, {"title": "4.2.1 Analysis of Performance Comparison", "content": "First, our SWORD stands out among the baseline methods as it effectively balances the classification of old and new categories. After learning old category nodes on task T\\(^l\\) and learning new category nodes on task T\\(^u\\), SWORD successfully distinguishes between old and new categories using the joint classifier and performs better on the classification of all category nodes.\nIn contrast, most of the NCD baseline methods struggle to differentiate new nodes using task-agnostic joint classifiers, prioritizing the maintenance of performance on old classes. Although these NCD methods fail to classify new category nodes in the NC-NCD setting, they still achieve better overall performance by relying on the classification of old categories. We acknowledge that the performance on all categories is largely influenced by the classification accuracy on the old categories. Further analysis with confusion matrices of this phenomenon will be conducted in Appendix A.5.2. Regarding the aforementioned graph incremental learning methods, they require labeled nodes for both new and old categories during phase 2 when learning new categories. Therefore, \u0421\u0420\u0421\u0410 in the class-IL setting fails to differentiate new categories in the absence of new category labels. In the context of task-IL methods, task-ids need to be specified for evaluation under the assumption of known category membership (whether a category belongs to the new or old set). By leveraging the powerful node representation learning capabilities of GNNs, these methods excel in the classification of new category nodes. However, in the NC-NCD setting where labeled data for old categories is absent, they struggle to differentiate among old categories, resulting in weak performance across all categories. Due to the loss of old category node classification capability and the inability to balance the classification performance between new and old categories, the aforementioned task-IL methods prove inadequate for NC-NCD tasks.\nIn addition, we utilize average accuracy (AA) and average forgetting (AF) [58] to assess the average performance in learning new classes and the resistance to forgetting in Appendix A.5.1."}, {"title": "4.2.2 Performance on Multiple Backbones", "content": "We implement SWORD with multiple backbones and evaluate the node classification accuracy for both old and new categories. Our results show that SWORD consistently outperforms other SOTA methods in achieving a balanced performance on old and new categories. Specifically, GraphSAGE"}]}]}