{"title": "ExBody2: Advanced Expressive Humanoid Whole-Body Control", "authors": ["Mazeyu Ji", "Xuanbin Peng", "Fangchen Liu", "Jialong Li", "Ge Yang", "Xuxin Cheng", "Xiaolong Wang"], "abstract": "This paper enables real-world humanoid robots to maintain stability while performing expressive motions like humans do. We propose Advanced Expressive Whole-Body Control (ExBody2), a generalized whole-body tracking framework that can take any reference motion inputs and control the humanoid to mimic the motion. The model is trained in simulation with Reinforcement Learning and then transferred to the real world. It decouples keypoint tracking with velocity control, and effectively leverages a privileged teacher policy to distill precise mimic skills into the target student policy, which enables high-fidelity replication of dynamic movements such as running, crouching, dancing, and other challenging motions. We present a comprehensive qualitative and quantitative analysis of crucial design factors in the paper. We conduct our experiments on two humanoid platforms and demonstrate the superiority of our approach against state-of-the-arts, providing practical guidelines to pursue the extreme of whole-body control for humanoid robots.", "sections": [{"title": "1. Introduction", "content": "Humanoid robots, with their similar morphology to humans, possess the potential to perform various tasks and motions that humans can accomplish in daily life. However, due to the high-dimensional state space and the complexity of control, it remains challenging to develop emergent human-like behaviors, limiting their real-world applications. With the growing availability of large-scale human motion datasets [4, 45], a practical approach to address this challenge is learning to replicate versatile movements by tracking and mimicking human motions [8, 20, 23, 24]. However, when considering the hardware, humanoid robots and humans are still quite different, which hinders the robot's ability to replicate human motion completely. This poses a compelling research question: how can we pursue the expressive, human-like capabilities of humanoid robots while maintaining their stability and robustness, given their physical limitations?\nIn this paper, we introduce Advanced Expressive Whole-Body Control (ExBody2), an effective framework that maximizes the expressiveness of feasible whole-body motions on humanoid robots. The framework falls into a Sim2Real pipeline, where the policy will take a reference kinematic motion as input, and outputs the action that controls a real humanoid to conduct the motion in the real world. We train a single policy that generalizes across different input motions. We identify four technical designs to achieve this:\n(i) Constructing feasible and diverse training datasets. Human motion datasets like AMASS [45] often contain complex movements beyond a robot's physical capabilities, making tracking overly challenging and reducing performance. Some works address this by refining datasets. ExBody[8], for instance, filters out infeasible motions using language labels, though ambiguous descriptions (e.g., \"dance\") can still include unsuitable movements. Other approaches, such as H2O[24] and OmniH2O [24], employ the SMPL model to simulate virtual humanoids and filter out complex motions. However, SMPL avatars can perform extreme actions that real robots cannot, creating a gap between simulation and real-world feasibility that can still impact training effectiveness. We systematically analyze dataset difficulty and diversity, focusing on upper-body movement diversity for stability and tracking accuracy, and lower-body feasibility for robust training. Experiments demonstrate that diversity and feasibility in datasets are crucial for expressive and stable behaviors in both training and testing phases.\n(ii) Two-stage training. ExBody2 adopts a two-stage teacher-student training framework as illustrated in Figure 2. We first train a teacher policy using a standard RL algorithm, PPO [60] with privileged information, which includes real root velocity, accurate positions for each body link, and other physical properties (e.g. frictions) in simulation. This results in an effective policy to accurately mimic versatile human motion. We then learn a deployable student policy by performing DAgger[59]-style distillation on the teacher policy. For the student policy, the privileged information is replaced by a series of historical observations.\n(iii) Local keypoint tracking strategy. Previous whole-body tracking approaches, such as H2O [24] and OmniH2O [23], rely on global tracking of keypoint positions. This approach often leads to tracking failures in immediate next steps when robots struggle to align with current global keypoints, limiting their applications to highly stationary scenarios. In contrast, ExBody2 converts keypoints in the local frame, and decouples keypoint tracking from velocity control. To further improve the robustness of tracking, we periodically reset the global tracking keypoints to the local frame. This approach ensures sustained and robust whole-body tracking.\n(iv) Long-range motion with CVAE. With enhanced whole-body tracking capabilities, our robot can now perform long-duration motion imitation. However, the motions in existing datasets are relatively short, presenting a challenge when aiming to enable continuous motion execution without manual resets. To overcome this, we train a Conditional Variational Autoencoder (CVAE) [64, 79] model to synthesize future motion. As shown in Figure 2, the CVAE takes on past motion information and generates future motion sequences, allowing the robot to execute complex and expressive movements seamlessly during deployment.\nIn the experiments, we evaluate four baseline methods on two robot platforms (Unitree G1 and H1) to assess the effects of different tracking methods, motion control strategies, and training techniques. Our approach demonstrates superior performance across full-body tracking accuracy, upper and lower body tracking, and velocity tracking, particularly excelling in velocity tracking and overall precision. Ablation studies further highlight that a curated dataset with moderate diversity and feasibility achieves optimal tracking performance, avoiding the drawbacks of overly simplistic or noisy datasets. By incorporating a teacher-student structure and an optimal history length, our approach achieves stable and efficient tracking in dynamic environments."}, {"title": "2. Related Work", "content": "Humanoid Whole-Body Control Whole-body control for humanoid robots remains a challenging problem due to the system's high non-linearity. Traditional approaches predominantly rely on dynamics modeling and control [10\u201313, 26, 28, 33, 35, 49\u201351, 58, 73, 76]. Recent advances in reinforcement learning and sim-to-real transfer have demonstrated promising results in enabling complex whole-body skills for both quadrupedal and humanoid robots [1, 6, 7, 14, 16\u201319, 21, 29\u201332, 37, 39\u201342, 47, 48, 52, 56, 57, 61\u201363, 66, 72, 74, 75, 80]. Notably, works such as [8, 20, 24] have focused on humanoid whole-body control for expressive motions, with applications in manipulation and imitation learning. However, these approaches still exhibit limitations in expressiveness and maneuverability, highlighting the untapped potential of humanoid robots.\nRobot Motion Imitation Robot motion imitation can be categorized into two primary areas: manipulation and expressiveness. For manipulation tasks, robots-often wheeled or tabletop-prioritize precise control over balancing and ground contact, making humanoid morphology unnecessary. Such robots typically utilize data from teleoperation [2, 3, 79] or human demonstrations [5, 34, 65, 70]. In contrast, expressive motion imitation focuses on learning life-like behaviors from human or animal motion capture data. This task is more challenging due to the need for fine-grained control of contacts and balance. While reinforcement learning has enabled physics-based character motion imitation in simulation [22, 43, 44, 54, 55, 67, 68, 71, 77], transferring such methods to real robots remains a significant challenge [8, 15, 16, 20, 23, 25, 53].\nHuman Motion Data Human motion capture datasets [4, 36, 45] offer a rich source of reference motions for training low-level policies. Additionally, recent advances in generative modeling enable the creation of multi-modal motion data using diffusion models conditioned on text inputs [69, 78] and motion variational autoencoders (VAEs) [43]. Our work leverages these kinematically feasible motion datasets to train physically plausible controllers, while the acquisition of motion data remains outside our scope."}, {"title": "3. ExBody2: Learning Expressive Humanoid Whole-Body Control", "content": "We propose Advanced Expressive Whole-Body Control (ExBody2), a simple and effective sim-to-real framework for expressive and robust whole-body control. As illustrated in Figure 2, ExBody2 consists of four main components: dataset curation, policy learning, motion synthesis, and real-world deployment. In this section, we will introduce these components of ExBody2 in detail."}, {"title": "3.1. Motion Dataset Curation", "content": "In the curation of our motion datasets, significant emphasis is placed on the detailed analysis and selection of actions, particularly distinguishing between the capabilities required for upper versus lower body movements. This strategic approach aims to optimize the diversity of movements within feasible limits for robotic implementations.\nUpper Body Actions Our dataset includes a wide array of upper body movements. This variety is crucial to challenge and enhance the robot's adaptability and readiness for real-world applications, where flexibility in response to unexpected scenarios is vital.\nLower Body Actions The selection of lower body movements is approached with greater conservatism due to the mechanical limitations and stability requirements of robots. Basic locomotive actions such as walking and subtle stance adjustments are included. However, highly dynamic movements such as jumping or complex rotational movements are carefully assessed. Training on such infeasible actions would introduce noise into the learning process, thus reducing its effectiveness.\nBalancing Act and Dataset Integrity The process of curating the datasets involves a delicate balance between challenging the robotic capabilities and maintaining actions within a feasible spectrum. Overly simplistic tasks could limit the training policy's ability to generalize to new situations, while overly complex tasks might exceed the robot's operational capabilities, leading to ineffective learning outcomes. Part of our dataset preparation, therefore, includes the exclusion or modification of entries that featured complex lower body movements beyond the robot's capabilities. This step is crucial to ensure the datasets are rich in actionable content without overwhelming the robot's learning algorithms with impractical tasks.\nThrough meticulous planning, we curate distinct datasets that are optimally balanced for robotic training. Extensive experimental validation will demonstrate the superiority of our dataset selection approach in Section 4."}, {"title": "3.2. Policy Learning", "content": "ExBody2 aims at tracking a target motion more expressively in the whole body. To this end, ExBody2 adopts an effective two-stage teacher-student training procedure as in [37, 38]. Specifically, the oracle teacher policy is first trained with an off-the-shelf reinforcement learning (RL) algorithm, PPO [60], with privileged information that can be obtained only in simulators. For the second stage, we replace the privileged information with observations which are aligned with the real world, and distill the teacher policy to a deployable student policy. We train our policies using IsaacGym [46] with efficient parallel simulation."}, {"title": "3.2.1. Teacher Policy Training", "content": "We can formulate the humanoid motion control problem as a Markov Decision Process (MDP). The state space \\(S\\) contains privileged observation \\(X\\), proprioceptive states \\(O\\) and motion tracking target \\(G\\). A policy takes \\({p_t, O_t, g_t}\\) as input, and outputs action \\(a_t\\), as illustrated in Figure 2, the teacher policy. The predicted action \\(a_t \\in \\mathbb{R}^{23}\\) is the target joint positions of joint proportional derivative (PD) controllers. We use off-the-shelf PPO [60] algorithm to maximize expectation of the accumulated future rewards \\(\\mathbb{E}[\\sum_{t=0}^{T}R(S_t, a_t)]\\), which encourages tracking the demonstrations with robust behavior. The predicted \\(a_t \\in \\mathbb{R}^{23}\\), which is the target position of joint proportional derivative (PD) controllers.\nPrivileged Information The privileged information \\(p_t\\) contains some ground-truth states of the humanoid robot and the environment, which can only be observed in simulators. It contains the ground-truth root velocity, real body links' positions, and physical properties (e.g. friction coefficients, motor strength). Privileged information can significantly improve the sample efficiency of RL algorithms, which is often leveraged to obtain a high-performing teacher policy.\nMotion Tracking Target Similar to Exbody [9], ExBody2 learns a policy that can be controlled by the joystick commands (e.g. the linear velocity and body pose) when accurately tracking a whole-body motion. The motion tracking target consists of two components, which are (1) the desired joints and 3D keypoints in both the upper and lower body and (2) target root velocity and root pose."}, {"title": "3.2.3. Decomposed Tracking Strategy", "content": "Motion tracking comprises two objectives: tracking DoF (joint) positions and keypoint (body keypoint) positions. Keypoint tracking usually plays a crucial role in tracking motions, as joint DoF errors can propagate to the whole body, while keypoint tracking is directly applied to the body. Existing work like H2O, OmniH2O [23, 24] learns to follow the trajectory of global keypoints. However, this global tracking strategy usually results in suboptimal or failed tracking behavior, as global keypoints may drift through time, resulting in cumulative errors that eventually hinder learning. To address this, we map global keypoints to the robot's current coordinate frame, and instead utilize velocity-based global tracking. The coordination of velocity and motion allows tracking completion with maximal expressiveness, even if slight positional deviations arise. Moreover, to further enhance the robot's capabilities in following challenging keypoint motions, we allow a small global drift of keypoints during training stage and periodically correct them to the robot's current coordinate frame. During deployment, we strictly employ local keypoint tracking with velocity-decomposed control."}, {"title": "3.3. Continual Motion Synthesis", "content": "The existing motions are usually short in length, which limits humanoid robots from performing interesting and extended behaviors continuously. To overcome this, we train a Conditional Variational Autoencoder (CVAE) [64, 79] to synthesize future movements that can be directly employed by the whole-body tracking policy, allowing the robot to execute complex and expressive movements seamlessly during deployment.\nAs shown in Figure 2 (c), the CVAE takes on past M-step motions \\(m_{t-M:t}\\), and synthesizes future H-step motions \\(m_{t+1:t+H+1}\\) autoregressively. \\(m_t\\) includes the current DoF positions, root pose, velocity, and angular velocity. Specifically, we utilize a transformer-based CVAE architecture, similar to the architecture proposed in [79]. We first separately tokenize DoF positions and root information through two separate MLPs, and get tokenized motion information. We compute the posterior \\(q\\phi(M_{t-M:t}, M_{t+1:t+H+1})\\) through a transformer encoder network, using the features from the [CLS] token. For notational simplicity and consistency with standard CVAE, we denote the condition \\(c = m_{t-M:t}\\) to be the history context, and \\(x = M_{t+1:t+H+1}\\) to be the prediction target. During training time, the latent variable \\(z\\) is sampled from \\(\\mathcal{N}(\\mu(c, x), \\sigma(c, x)^2)\\). A transformer decoder will take \\(z\\) with \\(c = m_{t-M:t}\\), and positional embeddings to predict H-step future motions . The CVAE loss consists of the reconstruction loss \\(l_{recon} = ||x - \\hat{x}||^2\\) and the KL divergence loss:\n\\[\\begin{aligned}\nl_{KL} = \\frac{1}{2}\\sum_{i=1}^{d} (\\sigma_i^2(\\hat{x}, c) + \\mu_i^2(\\hat{x}, c) - 1 - log(\\sigma_i^2(\\hat{x}, c)))\n\\end{aligned}\\]\nwhere d is the dimensionality of the latent space. To improve the smoothness of the predicted motion, we also apply a smoothness loss:\n\\[\\begin{aligned}\nl_{smooth} = ||m_{t+1} - m_{t}||^2 + \\sum_{i=t+1}^{t+H} ||m_{i+1} - m_{i}||^2\n\\end{aligned}\\]\nThe total loss for CVAE training is: \\(l_{motion} = l_{recon} + \\alpha l_{KL} + \\beta l_{smooth}\\) where we set \\(\\alpha = \\beta = 0.5\\) in our experiments. During inference, \\(z\\) is set to the mean of the prior (i.e. zero). We adopt a similar temporal ensemble strategy as in Zhao et al. [79] to get predicted future motions. Further details regarding the encoder and decoder architecture of the CVAE are in the supplementary materials."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Experimental Setup", "content": "We conduct our experiments in IsaacGym [46] simulator across two robot platforms (Unitree G1 and H1)."}, {"title": "4.2. Main Results", "content": "Table 2 and Figure 3 present the results of different algorithms on two robot platforms, Unitree G1 and H1, where ExBody2 outperforms all other methods in all the tracking metrics. Our approach maintains consistent lower tracking error across the entire time frame with minimal fluctuations, indicating a stable and precise tracking performance. To be more precise, we evaluate the following metrics:\n1. Upper Body Tracking Accuracy: Our algorithm performs slightly lower than Exbody in upper body tracking accuracy. However, Exbody sacrifices lower body tracking to maintain balance, whereas our algorithm can perform well with balanced tracking accuracy for both upper body and lower body.\n2. Lower Body Tracking Accuracy: Our algorithm demonstrates higher accuracy in lower body tracking compared to other algorithmes for both G1 and H1.\n3. Velocity Tracking Accuracy: Our algorithm also excels in velocity tracking accuracy, outperforming other methods. This improvement is attributed to the teacher-student training paradigm, where the distillation from privileged information to historical observation results in a student policy with better velocity-tracking.\nOverall, our algorithm achieves significant improvements in full-body tracking accuracy for both upper and lower body, and velocity tracking accuracy compared to the baseline algorithms, demonstrating stable and effective tracking performance in dynamic environments."}, {"title": "4.3. Ablation on Datasets", "content": "We manually design four datasets of varying sizes, where the largest being the complete CMU AMASS dataset, containing in total 1,919 motions. The remaining datasets, with sizes 50, 250, and 500, are subsets of the CMU dataset, each constructed with different levels of action diversity:\n\u2022 50-action dataset (D50): This subset includes only fundamental static actions, such as standing and walking. Both upper and lower limb actions lack diversity, primarily covering basic and easily feasible motions.\n\u2022 250-action dataset (D250): In addition to basic actions, this subset introduces more diverse upper limb actions and a variety of lower limb actions, such as running and simple dance movements. These actions are feasible for robots, albeit moderately challenging.\n\u2022 500-action dataset (D500): This subset includes increasingly complex lower limb actions, such as single-leg jumps and intricate dance moves, rendering the dataset somewhat noisier with less feasible motions.\n\u2022 Full CMU dataset (DCMU): The complete CMU dataset comprises 1,919 actions, including extreme movements like push-ups, rolling on the ground, and somersaults. This dataset is unfiltered and introduces significant noise.\nWe train ExBody2 on the above four datasets, and conduct three sets of evaluations on the G1 robot:\n1. Evaluation on D50: Policies trained on D50 achieve the highest tracking accuracy for in-distribution actions, as reflected in metrics across all categories. This suggests that additional data does not necessarily benefit in-distribution tasks.\n2. Evaluation on DCMU: Policies trained on D250 achieve the best performance on DCMU, surpassing those trained on both D500 and the full DCMU dataset. Unexpectedly, the D250-trained policy generalizes better than those trained on D500 and DCMU.\n3. Evaluation on DACCAD: This experiment further emphasizes the importance of clean datasets. Policy trained on D250 outperforms the others, achieving the best tracking accuracy.\nIn conclusion, the D250 dataset yields the best performance, providing a balance between diversity and feasibility. It includes a rich variety of upper limb actions and moderately challenging lower limb actions that remain feasible for robots. This dataset avoids the drawbacks of overly simplistic datasets with insufficient diversity and noisy datasets that degrade tracking performance."}, {"title": "4.4. Ablation on Policy Training", "content": "Finally, we ablate the design choices in ExBody2, including the effectiveness of the teacher-student setup, the optimal history length to train the policy, and the mechanism of using delayed keypoints reset.\nAs shown in Table 4 (c), the tracking accuracy drops significantly without teacher-student training. When no extra history is used, the policy struggles to learn effectively. Among the nonzero history lengths, most policies perform similarly while the history length of 25 yields the best results. Lastly, we evaluate the impact of adding small drifting by using a delayed global keypoint reset strategy. We allow the keypoints to move in the global frame for short periods before resetting to the robot's local frame. This design notably improves velocity tracking, enabling the robot to use absolute position adjustments to compensate for discrepancies when tracking velocities."}, {"title": "5. Conclusion", "content": "In this work, we develop ExBody2, a novel algorithm that achieves stable and expressive whole-body motion for humanoid robots by utilizing a curated dataset and a decoupled whole-body tracking framework. Our approach separates key body tracking from velocity control, enabling precise and stable motion replication. While we have identified the characteristics of datasets best suited for humanoid robot training, an automated method to select high-quality datasets remains an open challenge. Addressing this limitation will be a focus of our future work."}, {"title": "A. Environments", "content": "Our real robot employs a Unitree G1 platform, with an onboard Jetson Orin NX acting as the primary computing and communication device. The control policy receives motion-tracking target information as input, computes the desired joint positions for each motor, and sends commands to the robot's low-level interface. The policy's inference frequency is set at 50 Hz. The commands are sent with a delay kept between 18 and 30 milliseconds. The low-level interface operates at a frequency of 500 Hz, ensuring smooth real-time control. The communication between the control policy and the low-level interface is realized through LCM (Lightweight Communications and Marshalling)[27]."}, {"title": "A.2. Simulation Tasks", "content": "In this section, we provide detailed state space information for the policy training.\nRobot Proprioceptive States The robot proprioceptive states for the teacher and the student policy can be found in Table 6. Note that the student policy is trained on longer history length compared to the teacher, as it cannot observe privileged information but have to learn from a longer sequence of past observations.\nPrivileged Information The teacher policy leverages privileged information to obtain accurate motion-tracking performance.\nTracking Target Information Both the teacher policy and student policy also take the motion tracking goal as part of their observations, which consists of the keypoint positions, DoF (joint) positions, as well as root movement information.\nAction Space The action is the target position of joint proportional derivative (PD) controllers, which is 23 dimensions for Unitree G1 and 21 dimensions for Unitree H1. Note that we are using an upgraded version of the Unitree H1. The older H1 has 19 DOF, while the new H1 has 21 DOF."}, {"title": "B. Model and Training Details", "content": null}, {"title": "B.1. Baseline Implementation", "content": "Exbody[9]: The implementation of Exbody on the Unitree H1 robot is fully consistent with the original Exbody design, tracking only the upper-body keypoints and joint positions.\nExbody: Exbody is the full-body version of Exbody.\nOmniH2O* [23]: The main difference between OmniH2O* and our method lies in the training phase.Apart from this, we ensured that the observation space and reward design were consistent with the original OmniH2O implementation."}, {"title": "B.2. CVAE Architecture", "content": "Our CVAE model contains a transformer-based encoder and decoder. The encoder leverages bidirectional attention to extract the latent posterior variable z based on past M-step motions mt-M:t and future H-step motions Mt+1:t+H+1,\nDuring training, the transformer encoder fenc takes mt-M:t+H+1 and a classification token [CLS] and gets the output sequence. We adopt the corresponding output from the [CLS] token to obtain a posterior Gaussian distribution.\nThe CVAE is trained on the AMASS CMU motion capture dataset, conditioned on the past 50 frames of motions to predict the future 15 motions.\nDuring deployment, the humanoid robot takes the predicted motion from the CVAE and tracks it seamlessly in the real world."}, {"title": "B.3. Policy Training Hyper-parameters", "content": "ExBody2 adopts a teacher-student training framework.\nThe student policy is trained with Dagger [59]without privileged information, but using longer history."}, {"title": "C. More results of ExBody2", "content": "Figure 4 highlights representative correspondences during specific motion tracking processes, showcasing the alignment between the original human SMPL model, simulation results, and real-world tracking outputs, which demonstrate high consistency."}]}