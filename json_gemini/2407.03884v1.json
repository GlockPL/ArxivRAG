{"title": "Planning with Large Language Models for Conversational Agents", "authors": ["Zhigen Li", "Jianxiang Peng", "Yanmeng Wang", "Tianhao Shen", "Minghui Zhang", "Linxi Su", "Shang Wu", "Yihang Wu", "Yuqian Wang", "Ye Wang", "Wei Hu", "Jianfeng Li", "Shaojun Wang", "Jing Xiao", "Deyi Xiong"], "abstract": "Controllability and proactivity are crucial properties of autonomous conversational agents (CAs). Controllability requires the CAs to follow the standard operating procedures (SOPs), such as verifying identity before activating credit cards. Proactivity requires the CAs to guide the conversation towards the goal during user uncooperation, such as persuasive dialogue. Existing research cannot be unified with controllability, proactivity, and low manual annotation. To bridge this gap, we propose a new framework for planning-based conversational agents (PCA) powered by large language models (LLMs), which only requires humans to define tasks and goals for the LLMs. Before conversation, LLM plans the core and necessary SOP for dialogue offline. During the conversation, LLM plans the best action path online referring to the SOP, and generates responses to achieve process controllability. Subsequently, we propose a semi-automatic dialogue data creation framework and curate a high-quality dialogue dataset (PCA-D). Meanwhile, we develop multiple variants and evaluation metrics for PCA, e.g., planning with Monte Carlo Tree Search (PCA-M), which searches for the optimal dialogue action while satisfying SOP constraints and achieving the proactive of the dialogue. Experiment results show that LLMs finetuned on PCA-D can significantly improve the performance and generalize to unseen domains. PCA-M outperforms other CoT and ToT baselines in terms of conversation controllability, proactivity, task success rate, and overall logical coherence, and is applicable in industry dialogue scenarios. The dataset and codes are available at XXXX.", "sections": [{"title": "1 Introduction", "content": "Conversational agents (CAs) or dialogue systems are designed to offer social support or functional services to human users via natural language interactions. Existing mainstream approaches can be categorized into conversational question answering (CQA) (Anantha et al., 2021; Singhal et al., 2023; Zhuang et al., 2024), open-domain dialogue (ODD) (Li et al., 2017; Roller et al., 2021; Ouyang et al., 2022), task-oriented dialogue (TOD) (Budzianowski et al., 2018; Sun et al., 2022; Quan et al., 2020; Moradshahi et al., 2023) and conversational recommender systems (CRS) (Wu et al., 2019; Zhang et al., 2021; Wang et al., 2023a). The goal of CQA and ODD is to passively respond to users to provide knowledgeable or engaging conversations, thus lacking proactive. CRS sets goals by specifying a topic, such as \u201cmovie recommendation, 'King of Comedy',\" and autonomously plans the best path of dialog actions to smoothly guide the conversation towards recommending \u201cKing of Comedy\". However, in complex tasks, dialogue actions frequently adhere to strict sequential constraints, as commonly found in the widely accepted standardized operating procedures (SOPs) established by humans across various domains (Akyar, 2012; Zhou et al., 2023). As shown in Figure 1, the prerequisite for persuading credit card activation is that the user must be the cardholder. CRS lacks planning globally controllable path of dialog actions. TOD aims to provide functional services according to the service process, and their controllability depends on manually drawing dialogue trees or predefined intent, slot, and action training data. It can proactively guide users to provide slot information, but cannot handle situations where users uncooperate, such as persuasive dialogue or negotiation dialogue (Wang et al., 2019; Chawla et al., 2021). We posit that dialogue's proactivity, controllability, and reduced reliance on manual intervention are crucial to building autonomous business CAs. Recently, prompting large language models (LLMs) has become the predominant approach to zero-shot language tasks, and its applications in dialogue have received much attention. However, integrating LLMs into conversational agents while addressing the limitations of the aforementioned dialogue systems remains a significant challenge.\nTo address this challenge, we propose a novel dialogue framework, namely planning-based conversational agents (PCA) as shown in Figure 1. In contrast to prior research, our approach only requires humans to provide task definitions and goals for the LLMs to plan the core and necessary SOP offline before the conversation. During the conversation, LLM refers to the SOP to plan the best action path online and generate responses to achieve process controllability. For actions not in SOP, such as answering questions and persuasion, LLM can plan autonomously to enable proactive dialogue. To further this research, we must address several challenges: (1) How to construct a conversation dataset that meets both initiative and controllability? (2) What are the PCA implementation schemes? (3) How to accurately evaluate the effects of different schemes?\nFor the first issue, the most straightforward approach would be to manually edit flowcharts and dialogues, which are immensely costly. Recently, advanced LLMs, e.g., ChatGPT, can outperform crowd workers on many annotation and evaluation tasks (e.g. (Gilardi et al., 2023; Wang et al., 2023b; Pan et al., 2023)). Therefore, as shown in Figure 2, we design a four-step LLM role-playing system to assist manual correction in creating a dialogue dataset, which is more cost-effective and quickly scalable.\nFor the last two challenges, we first implemented the chain of thought (CoT) and supervised fine-tuning (SFT) for SOP prediction in offline planning. In online planning, we first attempt to encode SOP directly into the prompt of LLMs, using in-context learning (ICL) to enable them to understand and follow these constraints. Additionally, inspired by the use of the Monte Carlo Tree Search (MCTS) algorithm in the chess game such as AlphaGo which effectively harnesses human prior rules to control and enhance game AI (Silver et al., 2017), we draw an analogy between dialogue task SOPs and game rules. MCTS is a recent and strikingly successful example of a decision-time planning approach (Sutton and Barto, 2018). By utilizing these SOPs' rules within MCTS's expansion and simulation phases, we can better search for the optimal dialogue action that satisfies SOP. Finally, We evaluated the overall/controllable/proactive action accuracy in single-round dialogue and integrated dialogue dimensions. Additionally, manual evaluation was conducted to address the limitations of automatic evaluation regarding dialogue diversity.\nOverall, we propose conversational agents based on LLMs with proactive and controllability and lower reliance on manual intervention. To the best of our knowledge, this problem has not been fully studied in prior work. Therefore, we advance the research on this issue from three aspects: task definition, data creation, and modeling. Our contributions are summarized as follows:\n\u2022 We propose and define the proactivity and controllable conversational agents with LLMs, which are promising yet unexplored.\n\u2022 We propose a semi-automatic dialogue dataset curation framework and develop a high-quality Chinese dialogue dataset (PCA-D). It not only provides insights into this task but also facilitates more research on it.\n\u2022 We develop multiple variants and evaluation metrics based on ICL, SFT, CoT, and MCTS modifications, which demonstrate superior performance in implementing offline planning of SOP and online planning of dialogue prediction."}, {"title": "2 Related Work", "content": "Dialogue Planning. In the process of dialogue planning, previous work focuses on subgoal generation (Zhang et al., 2021), the next round of dialogue transition strategy (Tang et al., 2019), hierarchical strategy (Kishinami et al., 2022). Recent work further involves planning dialogue paths based on basic knowledge, a goal-oriented dialogue planning framework to plan a series of dialogue actions, and guiding the system to proactively transition between different dialogue stages (Wang et al., 2022a). Existing research employs greedy, single-turn prediction strategies and overlooks the interdependencies of global policies in dialogue processes, resulting in global dialogue uncontrollable. We propose a SOP-based planning approach to address this issue.\nPlanning and Reasoning of LLMs. LLMs show prowess in planning and reasoning. Examples include Chain-of-Thought (Wei et al., 2022), its variants (Kojima et al., 2022), Self-Consistency (Wang et al., 2022b), and Least-to-most prompting (Zhou et al., 2022). self-assessment (Paul et al., 2023; Welleck et al., 2022; Shinn et al., 2023). Recent research uses more complex reasoning processes, offering new avenues for improving and optimizing the reasoning process. Yao et al. (2023) apply heuristic-based searches, such as depth-first and breadth-first search, to discover optimized reasoning pathways. Zhu et al. (2022); Hao et al. (2023) introduce MCTS to reasoning steps for complex math or logical reasoning. We focus on applying MCTS to implement controllable dialogue systems, with encoding SOPs' constraints into the MCTS's expansion and simulation steps."}, {"title": "3 Task Definition", "content": "Given a dialogue corpus {(Ti, Ci, Di)}i=1,2,...,N, where N is the number of dialogues. In the i-th dialogue, Ti (Pi, Ai, Si) represents the task definition, which encompasses task and user profile Pi defining the task goal, relevant knowledge, and user personal profile; Agent actions Ai contains nodes from SOP and proactive actions outside of SOP; User states Si reflecting the user's state in completing the task. Ci {dir(np, mq)}np,mq\u2208Ai\u222aSi represents SOP, is a directed graph with agent actions and user states as nodes, and dir \u2208 {\u2212, \u2192, \u2190, \u2194 } representing direction between node n and m. The field descriptions and examples for task definitions can be found in A.1. Di {(u, s, a, r)}t=1,2,...,T represents a dialogue containing T turns of interaction between agent and user. Each dialogue turn t comprises user utterance ut and corresponding agent response rt. Additionally, we annotate the updated user state st resulting from the user utterance (e.g., the user is the cardholder) and the corresponding agent action at for response to this status (e.g., introduce activation activity).\nThe PCA comprises two tasks.\nTask 1 is SOP Prediction, defined as\ndir = \\underset{dir\\in\\{-\\,,\\rightarrow,\\leftarrow,\\leftrightarrow\\}}{argmax} \\  p(dir|T_i, n_p, n_q) \\tag{1}\nwhich means that given ith dialogue's task definition Ti, the model needs to predict the constraint relationship Ci between agent actions Ai and user states Si by classifying the direction of any two nodes into four categories.\nTask 2 is Dialogue Generation, defined as\ns_i^{t-1} = arg\\underset{s_i^{t-1}\\in S_i}{max} p(s_i^{t-1}|T_i, H^{t-1})\\tag{2}\na_i^t = arg\\underset{a\\in A_i}{max} p(a_i^t|T_i, H^{t-1}, C_i, s_i^{t-1}) \\tag{3}\nr_i^t = [\\hat{w}_1,\\hat{w}_2...,\\hat{w}_n]\\hat{w}_n\\in W\\tag{4}\n\\hat{w}_n =p(w_n|T_i, C_i, H^{t-1}, a_i^t, w < n) \\tag{5}\ngiving the i-th dialogue's task definition T_i, constraint relationship C_i, previous (t \u2013 1) turns' history H^{t-1} = D_i^{t=1,...,t-1}, and vocabulary W, the model first predicts the (t \u2212 1)-th turn's user state s_i^{t-1} and then selects the next turn's agent action a_i^t from A_i that not only helps to achieve the goal but also follows the task constraint Ci, and generates the corresponding response r_i^t."}, {"title": "4 Dataset Construction", "content": "4.1 Dataset Curation Framework\nRecent studies (Wang et al., 2023a; Sandler et al., 2024) have demonstrated that the deployment of LLMs in role-playing scenarios is effective for the development of high-quality dialogue datasets. To efficiently obtain high-quality data at a low cost, we devise a method where LLMs are utilized to simulate various agents' roles in the creation of dialogue data, with human intervention for subsequent revisions. Figure 2 illustrates the whole framework, which involves a four-step process: defining the task, planning the task SOP, creating dialogue scenarios, and generating the actual dialogues.\nIn the first step, we design 53 unique tasks within 45 diverse domains, an example shown in Appendix A.2. In step 2 (i.e. Task 1), we implement prompting techniques that enable the LLMs to assume a planner to plan the SOP by the defined task. In Step 3, we first sample a dialogue path from the manually corrected SOP, which consists of a sequence of agent actions and user states, and ensure that the subsequent dialogue process satisfies the SOP process, thus ensuring controllability. However, this sequence lacks proactive actions typical in natural dialogues, such as answering questions and persuasion. We specify the LLM as a screenwriter, instructing it to insert additional agent actions and user states based on a given dialogue path, to create personalized and comprehensive dialogue scenarios. In the last step, We assigned the LLM as a scriptwriter to write dialogues for each agent action and user state in the sequence of dialogue paths, to create a complete conversation. In this step, we use a simulator to generate user profile for the task definition, ensuring that each conversation involves independent and non-repetitive users. This enhances the diversity and authenticity of the conversations. All prompts are detailed in Appendix A.2.\n4.2 Data Quality Control\nTo ensure the quality and consistency of the PCA-D, we recruite 7 outstanding annotators. Before annotation, we developed a training manual\u00b9, organize annotation training, and conduct pre-annotations. During the annotation process, we sample the data in batches at a rate of 20% and assign it for cross-annotation by three annotators. Any instances with an inter-annotator agreement rate below 95% are removed.\n4.3 Data Statistics\nTable 1 presents a comparison between PCA-D and pertinent datasets in the field. To our knowledge, PCA-D is the inaugural dataset dedicated to controllable and proactive dialogues. Despite the larger volume of dialogues in previous proactive dialogue datasets, they are limited to the singular task of topic recommendation. In contrast, our dataset spans 45 domains and encompasses 53 distinct professional tasks, thereby offering greater versatility. In addition, previous models heavily rely on training data, and the high cost of data annotation hinders further research and application promotion. The advent of LLMs has greatly reduced reliance on training data (Kojima et al.,"}, {"title": "5 Our Approach", "content": "To implement the PCA system, we need to complete two tasks: 1. Design and implement a comprehensive framework for end-to-end multi-turn dialogue. 2. Implement the two algorithm tasks as described in Section 3: SOP prediction and dialogue generation.\n5.1 PCA Framework\nPCA framework, depicted in Figure 3, comprises a CA as controller and five scheduled components. Before the dialogue, CA reads user-configured task information and passes SOP vertexes to the offline planner to obtain an adjacency list of the SOP graph. During the dialogue, CA first invokes the working memory module to assemble the prompts required for the dialogue. Subsequently, CA inputs the prompt into the online planning module to obtain the dialogue response. The LLMs used by each module are uniformly managed and accessed in the LLMs module. Next, we describe the implementation algorithms for the offline and online planning modules.\n5.2 SOP Prediction\nWe establish three effective baselines: adjacency list (AL), translation CoT (TCoT), and SFT. For AL, we directly prompt the LLMs to output an adjacency list in JSON format. For TCoT, we prompt the LLMs first to describe each vertex and its children vertexes in natural language along with the reasons, and then prompt the LLMs to translate this description into an adjacency list in JSON format. Finally, we implemented full-parameter SFT on different sizes of Llama and Qwen by the task of generating one vertex's adjacency vertexes every time. Prompts' detail is shown in Appendix A.2.\n5.3 Dialogue Prediction\nCoT and ToT (Wei et al., 2022; Yao et al., 2024) significantly improve LLM's complex reasoning ability. Therefore, we use them as our baseline. In CoT, we prompt LLM to infer user state, agent actions, and response step by step. In ToT, these steps are split into 3 times calling LLM, with each step corresponding to a layer of tree nodes. We obtain multiple nodes through multiple sampling, and finally, prompt LLM vote to choose a decision path to obtain agent action and response. To add SOP, we calculate the edit distance (ED) between the generated dialogue path and each subpath of the SOP. The smallest ED subpath's children were added to the prompt to guide the agent in adhering to the SOP. Hyperparameters and prompts are in Appendix A.3.\nPCA-M is an extension of the MCTS with SOP for dialogue path online planning, designed to optimally leverage SOP's constraint during the expansion and simulation phases of future dialogues. MCTS strategically explores the space of dialogue trees and strikes a proper balance between exploration and exploitation to find high-reward dialogue traces efficiently while satisfying constraints.\n5.4 MCTS planner with SOP\nSpecifically, as shown in Figure 3, PCA-M constructs a dialogue tree, where each node represents a turn of dialogue state (i.e., working memory) generated by the agent choosing an action to interact with the user, and each edge represents an agent action and the transition from the current state to the next state after applying that action. Given a dialogue state s0, PCA-M searches for the next best action by iteratively performing node selection, node expansion, dialogue simulation, and back-propagation to update the tree statistics. After n iterations, PCA-M predicts the next best action for s0. This process continues until the specified computational budget is reached (e.g., number of iterations), at which point the resultant trajectory can be extracted from the tree. Prompts and algorithms about PCA-M are shown in Appendix A.3 and 1. Below we describe each stage of the algorithm.\nSelection Starting from the root node (i.e., the initial state s0), at each level of the tree, we select a child node as the next node. This stage ends when a leaf node of the current tree is reached. PCA-M applies the Upper Confidence bounds applied to Trees (UCT) (Kocsis and Szepesv\u00e1ri, 2006) to select each child node to balance exploration (less visited nodes) and exploitation (high-value nodes):\na^* = arg \\underset{\\alpha\\in A(s)}{max} Q(s, a) + w \\sqrt{\\frac{ln N(s)}{N(c(s, a))}} \\tag{6}\nwhere N(s) is the number of times node s has been visited in previous iterations, and c(s, a) is the child node of applying a in state s. The less a child node has been visited (i.e., the more uncertain the child node is), the higher the second term. The state-action value function Q(s, a) estimates the expected future reward of taking action a in the state s:\nQ(s, a) = 0.5 \u00d7 (LLM(s, a) + 0.5 \u00d7 state(s, a)) \\tag{7}\n0  <  LLM(s, a)  < = 1\\tag{8}\nstate(s, a) = \\begin{cases} 0.3 & \\text{if s is a task terminal state} \\\\ 0.7 & \\text{if s is a task success state} \\\\ 0 & \\text{otherwise} \\end{cases} \\tag{9}\nwhere LLM(s, a) function represents the logical rationality of the current action, as assessed by the LLMs. It is computed as the mean of several binary (0 or 1) evaluations derived from prompt-based sampling by the LLMs. The state(s, a) function assigns discrete values to measure task completion: 0.3 for the defined termination state, and 0.7 for the success state.\nExpansion When the selected leaf node is not a terminal one, we employ the LLM as a dialogue action prior, using prompt to sample d possible dialogue actions for expansion. In addition, PCA-M first searches for a local subgraph in the SOP constraint graph based on the dialogue path, and adds the next two levels of child nodes pointed to by the current state node for expansion. This not only utilizes the constraint relations but also retains the autonomy of the LLM. When the selected leaf node is already a terminal node (either a dialogue end node or the maximum search depth has been reached), we will skip the expansion phase and proceed to back-propagation.\nSimulation To estimate the reward generated by future dialogue, this phase involves simulating, also known as roll-out, future dialogue for each expanded state node. To improve efficiency, we reduce the randomness of the simulation. Specifically, we follow a process similar to the expansion phase mentioned above, that is, we only simulate downward for candidate dialogue policies that are sampled from LLM and guided by the SOP.\nBackpropagation Once we reach the terminal state in the aforementioned stage, we use the UCT formula to retroactively update the UCT values along the entire path. When the predetermined total number of MCTS iterations is reached, we terminate the algorithm. Finally, in the constructed tree, we selected the current node's child with the highest UCT value to guide the next turn of conversation."}, {"title": "6 Experiments", "content": "6.1 Data Split\nThe dataset split is shown in Table 3. To assess the generalization ability of SFT on unseen tasks, we split the data from the task dimension, not the dialogue to ensure no task overlap among the three sets. Due to the limited number of samples for task 1, we employed 5-fold cross-validation in the experiments.\n6.2 Automatic Evaluations\nTask 1 Metrics We evaluated the predicted SOP from two perspectives: the SOP graph structure and the usability for dialogues. We used the graph edit distance (GED) function from Python library networkx to calculate the number of operations as well as the ratio (GEDR) required to manually edit the predicted SOP into the ground-truth SOP. For the dialogue dimension, we evaluated the precision, recall, and F1 score of the dialogue paths between predict and ground-truth SOP. The dialogue path is all traversable paths from the starting vertex to the end vertex, and the loops in the path are only iterated once.\nTask 2 Metrics We calculated the accuracy of single-turn dialogue actions (Acc T), SOP actions (Acc C), and proactive actions (Acc P), to measure the controllability and proactive effects of the dialogue. The accuracy of the entire dialogue (acc D) is computed as the number of correct dialogues in each turn divided by the total. We employed BLEU to evaluate the relevance between the generated responses and ground truth.\nTask 1 Results Table 4 reports the results for the prediction of SOPs. We can find that with the powerful reasoning capabilities of GPT-40, the availability of guided dialogue has reached 71.85, and the required manual correction proportion is only 5.4%. Secondly, our proposed TCoT method can effectively improve poorer models' performance by 10 to 20 points. Thirdly, SFT based on PCA-D significantly enhances the performance on unseen tasks, comparable to the strongest GPT-40. This suggests that our dataset effectively enhances the ability of LLM to plan SOP. The rapid development of LLM will enable the direct use of the planned SOP in conversations without human intervention.\nTask 2 Results were shown in Table 5, we can observe that adding SOP guidance can effectively improve the controllability and success rate of dialogue for any model and basic method. In addition, the SFT results show a marked performance improvement, approaching that of the commercial GPT-40, which indicates the effectiveness of PCA-D in enhancing the model's conversational abilities. Thirdly, the tree search algorithm based on MCTS is more effective than ToT, and can further improve performance. Especially in open-source models, it can exceed the CoT baseline of GPT-40 without SFT.\n6.3 Human Evaluations\nMetrics Considering that is difficult to unify a task's SOPs formulated by different users and is also language diversity in response, we manually"}, {"title": "6.4 Case Study", "content": "Figure 4 shows the dialogue action selected and the responses generated by different models when given the same context. As shown in the first case, before checking in, it is necessary to verify identity. After SOP was applied, both CoT and MCTS chose an action that complied with the SOP, making the dialogue more accurate and controllable. However, when the best action was not in SOP (e.g., try to persuade), as shown in the right case, CoT+SOP did not choose to continue proactively persuading. In contrast, MCTS+SOP, through simulating and searching deeper dialogue paths, selected a more goal-favorable persuasion attempt action. This also reflects the planning nature of the MCTS algorithm."}, {"title": "7 Conclusion", "content": "In this work, we have explored a new CAs framework and created a high-quality, controllable dialogue dataset. In addition, we proposed an unsupervised algorithm that encodes SOP constraints into the expansion and simulation steps of Monte Carlo Tree Search. Benefiting from our approach developed based on the advanced large language models' in-context learning, it holds promise to become the next generation of enterprise-level dialogue systems with low cost and high controllability."}, {"title": "Limitations", "content": "Hallucinations Our approach is based on the context learning of LLMs, such as ChatGPT and GPT-4. As LLMs may produce outputs containing hallucinations (Bang et al., 2023), our system might provide information beyond the task definition. We intend to enhance the veracity of responses through post-processing steps, such as training a dedicated safety model and incorporating checks and revisions into the post-processing phase.\nRuntime One significant limitation of PCA-M is the runtime. The more exhaustive the tree search is (e.g. increasing n or k), the more likely the algorithm is to find the optimal dialogue policy. However, this comes at the cost of longer simulation times, which may impact the overall user experience. We believe that parallelizing the tree search or reusing portions of the simulated subtrees could help to speed up the runtime. We anticipate that with the advancement of LLMs research, the speed of inference will continue to improve."}, {"title": "Ethics Statement", "content": "Given the independent behavior of agents in goal-oriented dialogue, it's imperative to scrutinize ethical implications. Our approach does not force the agent to achieve a specified goal, nor does it force the user to accept the agent's request. Instead, our work highlights the criticality of directing agents to adhere to human-defined limitations. While our measures are potent, we advocate for the stringent regulation of goal signals, particularly when implementing goal-oriented dialogue systems in specialized fields. Currently, the targeting process must uphold factual accuracy, respect user privacy norms, and comply with societal laws."}, {"title": "A Appendix", "content": "A.1 Task Definition and Example\nA task definition for dialogue should include the definitions of fields user_profile, conversation_profile, agent_action, user_state and sop. The definitions of these fields are as follows:\n\u2022 user_profile: The information about user business and personal information held by agents generally comes from the company's user management system. This information is used for identity verification or providing personalized services in conversations.\n\u2022 conversation_profile: Task information for providing business content, accomplishment goals, and relevant background knowledge to Agents.\n\u2022 agent_action: Summary of key actions to be carried out during the process of agent dialogue, intended to guide and constrain the content of the agent's dialogue in accordance with business regulations. Typically corresponds to the node names in the SOP or the strategy names of proactive dialogue.\n\u2022 user_state: The status of the task summary from the user's final response combined with the preceding dialogue, serves as a prompt for the agent to select the optimal next action. This typically corresponds to the user node in the SOP or the proactive dialogue state of the user.\n\u2022 sop: Standardized operating procedures established by business experts to standardize the business processing process. SOP usually includes key nodes and sequential dependencies between nodes.\nAn example of SOP definition is shown in 5. An example of task definition is shown in 1.\nA.2 Dataset Curation Details\nThe prompt for step 2 (i.e. Task 1) is shown in Table 7. The prompt for step 3 is shown in Table 8. The prompt for step 4 is shown in Table 9. The prompt for TCoT is shown in Table 16 and 17. Picture 5 is an example of a SOP definition.\nA.3 Experiment Details\nAll open-source models' experiments were completed on 4 Nvidia A800 GPUs. For SFT, we fine-tuned all parameters useing 5 epochs, 50 warm steps, and le-6 learning rate. In the experiment of task 1, the temperature was uniformly set to 0.1 and the top-p was set to 0.1. For task 2, the temperature was uniformly set to 1 and the top-p was set to 0.95. The prompt for CoT and CoT+SOP is shown in Table 14 and 15.\nIn MCTS, the number of generated actions d is set to 3, depth limit L is set to 8, the number of roll-outs N is set to 3, and exploration weight w is set to 1. The prompt for PCA-M sample action is shown in Table 10. The prompt for PCA-M generates response is shown in Table 11. The prompt for the PCA-M reward function is shown in Table 12. The prompt for PCA-M predicts the user state is shown in Table 13.\nTo maintain a search scale similar to MCTS, in ToT, the number of generated actions d is set to 3, depth limit L is set to 8. The prompt for ToT sample action is shown in Table 10. The prompt for ToT generates response is shown in Table 11. The prompt for ToT predicts the user state is shown in Table 13."}]}