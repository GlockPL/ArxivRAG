{"title": "An Overlooked Role of Context-Sensitive Dendrites", "authors": ["Mohsin Raza", "Ahsan Adeel"], "abstract": "To date, most dendritic studies have predominantly focused on the apical zone of pyramidal two-point neurons (TPNs) receiving only feedback (FB) connections from higher perceptual layers and using them for learning. Recent cellular neurophysiology and computational neuroscience studies suggests that the apical input (context), coming from feedback and lateral connections, is multifaceted and far more diverse, with greater implications for ongoing learning and processing in the brain than previously realized. In addition to the FB, the apical tuft receives signals from neighboring cells of the same network as proximal (P) context, other parts of the brain as distal (D) context, and overall coherent information across the network as universal (U) context. The integrated context (C) amplifies and suppresses the transmission of coherent and conflicting feedforward (FF) signals, respectively. Specifically, we show that complex context-sensitive (CS)-TPNs flexibly integrate C moment-by-moment with the FF somatic current at the soma such that the somatic current is amplified when both feedforward (FF) and C are coherent; otherwise, it is attenuated. This generates the event only when the FF and C currents are coherent, which is then translated into a singlet or a burst based on the FB information. Spiking simulation results show that this flexible integration of somatic and contextual currents enables the propagation of more coherent signals (bursts), making learning faster with fewer neurons. Similar behavior is observed when this functioning is used in conventional artificial networks, where orders of magnitude fewer neurons are required to process vast amounts of heterogeneous real-world audio-visual (AV) data trained using backpropagation (BP). The computational findings presented here demonstrate the universality of CS-TPNs, suggesting a dendritic narrative that was previously overlooked.", "sections": [{"title": "I. INTRODUCTION", "content": "Going beyond long-standing integrate-and-fire pyramidal Point Neurons (PNs) [1]-upon which current deep learning is based [2] recent breakthroughs in cellular neurophysiology [3] have revealed that pyramidal neurons in the mammalian neocortex possess two major points of integration: apical and basal, termed TPNs. [4]. To date, most studies, including the latest burst-dependent synaptic plasticity (BDSP) [5] and single-phase deep learning in cortico-cortical networks [6] have predominantly used FB information at the apical zone of TPNs for learning (aka. credit assignment) [7]\u2013[12]. However, the apical input, coming from the feedback and lateral connections, is far more diverse with far greater implications for ongoing learning and processing in the brain [13]-[15].\nThe apical zone receives input from diverse cortical and subcortical sources as a context that selectively amplifies and suppresses the transmission of coherent and conflicting FF signals, respectively received at the basal zone [13], [14], [16]. To uncover the true computational potential of TPNs, it is critical to emphasize the importance of understanding and defining the roles of different kinds of contexts arriving at the apical tuft [13]. Hence, dissection of contextual field (CF) into sub-CFs is imperative to better understand the amplification and suppression of relevant and irrelevant signals, respectively [14]. Specifically: (i) what kinds of information arrive at the apical tuft? (ii) how are they formed? (iii) how do they influence the cell's response to the FF signals? [14]\nThe Conscious Multisensory Integration (CMI) theory [13]\u2013[15] suggested that the apical tuft of the TPN receives modulatory sensory signals coming from: the neighbouring cells of the same network e.g., audio as P, other parts of the brain, in principle from anywhere in space-time e.g., visuals as D, and the background information/overall coherent information across the multisensory (MS) network as U. These contextual signals play a decisive role in precisely selecting whether to amplify/suppress the transmission of relevant/irrelevant FF signals, without changing the content e.g., which information is worth paying more attention to? This, as opposed to, unconditional excitatory and inhibitory activity, is called conditional amplification and suppression [13].\nThis view of context-sensitivity is also called cooperative context-sensitive computing [4], [14], [15] whose processing and learning capabilities are shown to be well-matched to the capabilities of the mammalian neocortex [10], [17]. In this approach, CS-TPNs receive P, D, and U fields to conditionally segregate relevant and irrelevant FF signals or transmit their FF message only when it is coherently related to the overall activity of the network. Individual neurons extract synergistic FF components as U by first conditionally segregating the coherent and incoherent multisensory information streams and then recombining only the coherent multistream. The U is broadcasted to other brain areas which are received by other neurons along with the current local context (P and D) [14], [18], [19]. These complex CS-TPNs when used in artificial neural network approaches enabled faster learning (beating Transformer-the backbone of ChatGPT) [20] and consumed significantly fewer resources compared to convolutional neural nets (CNNs) [14], [15] in some experimental settings.\nBuilding on these recent findings, the goal of this work is to find the biologically plausible validation for why deep neural network (DNN) approaches with CS-TPNs processing and standard backpropagation-based learning [14], [15], [20], manage to enable faster learning and consume fewer resources in some experimental settings compared to PNs-based DNNs. The main contributions of this paper are as follows:\ni. We integrate the features of CMI-inspired CS TPNs [13]\u2013"}, {"title": "II. CMI-INSPIRED COOPERATIVE CS TPNS + BDSP", "content": "[15] into a spiking TPNs-inspired local BDSP rule [5], demonstrating accelerated 'local' and 'online' learning compared to BDSP approach alone. This validates the efficient and effective information processing capabilities of CS-TPNs, paving the way toward 'local', 'on-the-fly', 'online' training and processing on neuromorphic chips.\nii. Addressed the limitations raised in the BDSP rule [5] by incorporating a thalamic circuitry proposed in [13], [14], [21], [22] into our spiking model. The thalamic circuitry (termed as U) stores coherent information across the sensory hierarchy and extracts synergistic information. The thalamic inputs are projected to the apical dendrites of the CS-TPNs. We show one of the ways how U (as a Gate) can mediate the signalling between apical and basal inputs, depending on their strength, thereby selectively strengthening either short-term facilitation (STF) or short-term depression (STD). We show that the incorporation of this phenomenon, when integrated into a hierarchical circuit, helps the network learn faster.\niii. We show that the spiking neural network composed of CS-TPNs significantly reduces the required number of events, including both singlets and bursts, for the task at hand compared to simple TPNs.\niv. We scaled up the two-layer CS TPNs-driven CNN [14], [15] to a 50-layer deep CNN for AV speech processing, solely to demonstrate the scalability and information processing efficiency of CS-TPNs in larger networks. In some cases, it even surpasses the generalization capabilities of PNs-inspired CNNs. This shows the universality of our context-sensitive TPNs-inspired processing regardless of the learning mechanism.\nIn the standard BDSP [5], the simple TPNs (Figs la and 1b) integrate the FF information at the soma to create an event (spike or burst) disregarding the contextual input. Once the event is generated, the feedback information (context) at the apical dendrites decides whether this already generated event should be a singlet (incoherent information) or a burst (coherent information). Although BDSP uses the apical zone to solve the 'online' credit assignment problem, the processing is still driven by PNs. In short, the learning is inspired by TPNs, but the processing is not [14], [15], [22].\nIn contrast, our approach shows that complex CS-TPNs (Figs 1c and 1d) at the soma flexibly integrate moment-by-moment rich contextual current (including FB, proximal, distal, and thalamus (universal)) with the FF somatic current such that the somatic current is amplified when both FF and context are coherent; otherwise, it is attenuated. This generates the event only when the somatic and apical currents are coherent, which is then translated into a singlet or a burst based on the FB information. Simulation results show that this flexible integration of somatic and contextual currents requires a reduced number of overall events (both singlets and bursts) in the system to generate external stimuli (Fig 1d). Raster plots shown in the later sections show that CS-TPNs burst far more (transmitting coherent information) than singlets (transmitting incoherent information), hence they learn faster.\nThe local weights are updated based on local burst firing information. The bursting (y) is controlled by a novel asynchronous modulatory function (MOD) [14], [15] that ensures neurons burst only when R is coherent with C, leading to the suppression of conflicting information (singlets), and simultaneous faster processing and learning [14], [15]. The updated BDSP learning rule is detailed in Table 1 and compared with standard BDSP and backpropagation (BP). Adapted from [5], somatic membrane potential dynamics and the apical dendrite dynamics of CS-TPNs are represented by the following simplified differential equations (see Eqs: 1 - 14):\n$V_s = \\frac{1}{\\tau_s} (- (V_s - E_L) + \\frac{1}{C_s} Mod(I_s, I_c) - w_s)$ (1)\n$\\dot{w_s} = \\frac{1}{\\tau_{ws}} (-w_s) + bS(t)$ (2)\nThe MOD function is capturing the complex interactions between somatic and dendritic currents moment-by-moment at the soma, enabling intrinsic adaptation mechanism, allowing nuanced control over the neuron's firing rate and pattern. $I_c$, including $I_P$, $I_D$, and $I_u$, is solely integrated at the apical site using the TPN dynamics defined in [5]. The network evolves on the same time scale, as coincidental detection is key in two-point neuron operations [3], [24]. Since $I_u$ is the most reliable contextual field, constituting the coherent/synergistic signals across the MS network and acquired by first conditionally segregating the coherent and incoherent MS information streams and then recombining only the coherent multistream, it has been awarded the maximum weight in the integration function. To make it equivalent to G (coupling gate between apical and basal sites) (Aru et al., TICS, 2020), its influence is embedded within the MOD function at the soma, such that terms with $I_u$ dominate the overall coupling between $I_s$ and $I_s$. The third term tends to zero in the absence of $I_u$. The offset value of 2 is empirically calculated. Nevertheless, there could certainly be a better way to model the whole dynamics.\nWhere $V_s$ represents the membrane potential of the somatic compartment (soma), $\\tau_s = 16ms$ is the membrane time constant, $C=370pF$ is the membrane capacitance, $E_L=-70mV$ is leak reversal potential, and $w_s$ is an adaptation variable. The total current applied to the soma is represented by $Mod(I_s, I_c, I_u)$ that is the basal current $I_s$ modulated with the contextual current $I_c$ and the synergistic components $I_u$. The basal current here is the integrated synaptic inputs (i.e., receptive excitatory and inhibitory inputs) along with basal noise. The contextual current is the accumulated effect of proximal signal, distal signal, and feedback error with dendritic noise. Whereas, the synergistic components $I_u$ is the extracted synergistic FF components. The adaptation variable $w_s$ is defined by the (2) where $S(t)$ is the spike train of the neurons, and $b = 200$ is the strength of spike-triggered adaptation. A spike occurs, every time $V_s$ crosses a dynamic"}, {"title": "III. RESULTS", "content": "threshold (i.e., -50mV). Subsequently, the threshold increases by 2mV immediately following a spike and returns to -50mV with a time constant of 27ms. After a spike occurs, the $V_s$ is reset to a resting voltage $V_r = -70mV$. These values of the parameters are adopted from the [5].\nThe CS-TPNs model receives the contextual signals (i.e., FB (E), P, D, and U) at the apical dendritic site and integrates them individually. Their respective membrane potentials, termed $V_e$, $V_p$, $V_d$, and $V_u$ are defined by the following simplified differential equations adopted from [5]:\n$\\dot{V_e} = \\frac{1}{\\tau_e} (- (V_e - E_e) + \\frac{1}{C_e} I_e - w_e)$ (3)\n$\\dot{w_e} = \\frac{1}{\\tau_{we}} (-w_e + \\frac{1}{\\tau_{we}} -a w_e (V_e - E_L)$ (4)\n$\\dot{V_p} = \\frac{1}{\\tau_p} (- (V_p - E_p) + \\frac{1}{C_p} I_p - w_p)$ (5)\n$\\dot{w_p} = \\frac{1}{\\tau_{wp}} (-w_p + \\frac{1}{\\tau_{wp}} -a w_p (V_p - E_L)$ (6)\n$\\dot{V_d} = \\frac{1}{\\tau_d} (- (V_d - E_d) + \\frac{1}{C_d} I_d - w_d)$ (7)\n$\\dot{w_d} = \\frac{1}{\\tau_{wd}} (-w_d + a w_d (V_d - E_L)$ (8)\n$I_c = (f(V_p) + f(V_e) - f(V_d))$ (9)\n$V_u = \\dot{V_u} = \\frac{1}{\\tau_u} (- (V_u - E_u) + \\frac{1}{C_u} I_u - w_u)$ (10)\n$\\dot{w_u} = \\frac{1}{\\tau_{wu}} (-w_u + \\frac{1}{\\tau_{wu}} -a w_u (V_u - E_L)$ (11)\n$I_u = g f(V_d)$ (12)\nIn the aforementioned equations, $\\tau_d$, $\\tau_g$, $\\tau_c$ are membrane time constants (7ms each). $C_d$, $C_g$, and $C_e$ are membrane capacitance (170pF each). Similarly, the $\\tau_{wd}$, $\\tau_{wg}$, and $\\tau_{we}$ are adaptation time constants (30ms each). $\\alpha_w = 13nS$ is sub-threshold adaptation. g = 1,300pA in the eq(9) is the dendrosomatic coupling. The simulation of the proposed model uses the same settings as [5] including but not limited to dendrite-targeting inhibition, perisomatic inhibitions, noise, and synapses. The top-down effect of the apical compartment is $I_c$, represented by eq(9) and $I_u$ in eq(12) is the gating current referring to synergistic components.\nContrary to the multiplexing approach [25] used in BDSP [5], the contextual modulatory function $Mod(I_s, I_c)$ detailed in [13], [19] is scaled as follows to suit the spiking neural net (SNN) in the CS-TPNs circuit.\n$Mod(I_s, I_c) = I_s + I_c(0.1 + |I_s|)$ (13)\nThe scaling is necessary to align with the working range values and the scaling is calculated empirically. This however, keeps the essence of the contextual field (C) overruling the typical dominance of the receptive field (R), and therefore discourages and encourages amplification of neural activity when C is weak and strong, respectively [13], [19]. As discussed earlier, to incorporate the synergistic components U as the Gate, the modulatory function is further modified (see Eq 14).\n$Mod(I_s, I_c, I_u) = I_s + I_c(0.1 + |I_s|) + I_cI_u(2 + |I_s|)$ (14)\nIn the Eq 14, the gating current is $I_u$. In this modified modulatory function, the third term incorporates both 'U' and the integrated context \u2018C' with higher weight than the second term. In the presence of U, the third term dominates the overall results, whereas the third term tends to zero in the absence of U. The offset value of 2 is empirically calculated.\nTo simulate the spiking CS-TPNs trained using BDSP, we adopt a 3-layer network structure presented in [5] with similar settings for exclusive or (XOR) tasks (Fig. 2a). The proposed network consists of two neural pools (termed ensemble) at the input, two ensembles in the hidden layer and one ensemble at the output. Each ensemble has a fixed population of interconnected CS-TPNs. To perform an XOR function, the network must respond with a high output if only one input is active, and a low output if neither or both input pools are active.\nThe XOR is a classical non-linear example to demonstrate the capability of a network. The network is initialized such that the output ensemble treats any input combination as roughly equivalent.\nFor analysis, the network performance and neural activity of CS-TPNs is compared to a similar network but with context-insensitive TPNs. The cost function for the network is binary cross entropy. The population size in each ensemble for BDSP only and for CS-TPNs+BDSP network is 175 neurons. The total population of these two networks is equal (i.e., 875 neurons). The CS-TPNs+BDSP network with U (CSM-TPNs+BDSP) has an additional ensemble representing U with a population of 50 PNs. To retain the network size, the input, hidden and output ensemble population size is reduced to 150 neurons each. Thus the total population size of the CSM-TPNS+BDSP network is 800. Fig. 2b shows the convergence of the cost function over 250 epochs for each network. The cost functions are compared to demonstrate the learning speed and convergence of BDSP alone (blue), CS-TPNs+BDSP (orange), and CSM-TPNs+BDSP (green). It can be seen that BDSP with CS TPNs learns faster than the BDSP, whereas, the BDSP with CSM-TPNs learns even quicker. The network output is shown in Fig. 2c,d,e. See the region where the mean of the function at (0, 1) and (1,1) inputs is distant from the threshold in CS-TPNs+BDSP and CSM-TPNs+BDSP (Fig. 2d, e) compared to BDSP alone (Fig. 2c). Fig. 3a shows the raster plots of standard BDSP and CS-TPNs+BDSP. The blue dots represent the event rate and the red dots represent the bursts. Higher bursts are associated with the high output whereas, the lesser events and bursts represent low output. It can be seen that the network with CS-TPNs distinguishes between the high and the low outputs more clearly than BDSP alone CS-TPNs. Specifically, it is to be observed that CS-TPNs tend to remain largely silent when information is less relevant (close to zero) but become active (bursting) when information is relevant (close to one). It also implies that CS-TPNs burst more when apical and basal, both the inputs are stronger, thus amplifying and suppressing the transmission of coherent and conflicting information, respectively. This is because the neighbouring proximal TPNs and distal TPNs in the same and other ensembles influence the perception of each TPN in each ensemble. They are sharing their perceptions with each other with the goal to minimise the conflicting information and maximise the coherent information, achieving harmony across the network. Additionally, it is notable that context-insensitive TPNs in BDSP burst more frequently than CS-TPNs, implying that neurons are not sharing information and burst even when the received information is not very relevant to the task at hand. Fig. 3b shows the development of the membrane potential of a single randomly selected neuron. During a given time frame, the context-insensitive TPNs in BDSP fire three times more than CS-TPNs. The CS-TPNs fire only when the proximal TPNs and the distal TPNs emphasize the need to fire, thus quickly distinguishing between the irrelevant and"}, {"title": "B. Deep AV speech processing with 50-layered CNN composed of CS-TPNs", "content": "relevant events for the task at hand. It can be convincingly concluded that CS-TPNs are more robust and efficient, capable of learning with a smaller population of neurons compared to the context-insensitive TPNs in BDSP.\nThe Fig la shows the results reproduced from the BDSP which demonstrates that the BDSP with short-term plasticity supports multiplexing of FF and FB signals. The graphs also reflect the signal-processing behaviour of the neuronal population. Fig. la shows a conventional TPNs circuit and Fig. 1b is a circuit consisting of CS-TPNs. The circuit architecture remains similar in both cases, having two populations of TPNs (Pop 1 and Pop 2) and two populations of PNs (disk and square). The PNs population provides inhibition. The external input to pop 1 is somatic current ($I_s$) and that to pop 2 is dendritic current ($I_d$). The output of Pop 1 is somatic input for Pop 2 whereas, the Pop 2 output is fed to the apical at Pop 1 as feedback. For simplicity, in the CS-TPNs network (Fig. 1b), the D and U are set to zero and only P is connected. The top two graphs in Fig. 1a, b reflect the activity in Pop 2, i.e., the burst probability of the Pop 2 neurons against the applied current $I_d$ and event rate against the FF input from Pop 1, respectively. Similarly, the two bottom graphs in Figs la and 1b represent the activity at Pop 1. These graphs show the burst probability at pop 1 due to the feedback from pop 2 and the event rate against the applied current $I_s$. The modulatory function of CS-TPNS results in dynamic and variable neural activity with an increase in burst probability and event rate compared to conventional TPNs. This suggests a higher responsive system influenced by contextual inputs while retaining the multiplexing behaviour of BDSP.\n1) AV Dataset for Speech Enhancement [19]: The AV ChiME3 dataset was created by blending the clean Grid videos [26] with ChiME3 background noises [27] (such as cafe, street junction, public transport (BUS), and pedestrian areas) across SNR levels from 6 to 6dB, using a 3dB increment. The preprocessing steps include adding prior visual frames to the data. To capture temporal dynamics, the system utilizes six preceding frames of both audio and visual data, enhancing the correlation between visual and auditory features. The Grid corpus includes recordings from 34 speakers, with each speaker delivering 1000 sentences. From this group, a subset of 4 speakers was chosen (comprising two white females and two white males), each contributing 900 command sentences. This selection aims to maintain speaker diversity. Additional information is detailed in [14], [28]. For training and evaluation, the data is divided into a 75%-25% split, with one sample reserved from both the training and testing portions as a representative proxy.\n2) Network Architecture:: The two-layered CNN composed of CS-TPNs [14] is scaled to a 50-layered CNN. Each TPN in each convolutional layer used the novel MOD block. The Mod block utilizes a structure and a modulatory transfer function as described in [14], [19], adjusting its scale based on the input values range. The multi-modal AV network architecture integrates two parallel streams for audio and visual modalities. The audio modality processes the magnitude of the noisy speech's Short-Time Fourier Transform (STFT), while the visual modality processes a series of images through 2D convolution layers to align with the audio stream's dimension. These deep models are tasked with reconstructing a clean STFT of the audio signal from both noisy audio and visual"}, {"title": "IV. CONCLUSION", "content": "activity, perceptual evaluation of speech quality (PESQ), and short-time objective intelligibility (STOI). It can be seen (see Fig. 4) that the CS-TPNs inspired CNN generalises better than baseline with up to 330x fewer neurons for all SNRs. Same proportion could be observed in required number of MACS (Multiply-Accumulate Operations) and FLOPs (Floating Point Operations).\nThe reliance on PNs and BP continues. Despite their remarkable performance improvements in a range of real-world applications, including large language models (LLMs), current AI technology based on them is rapidly becoming economically, technically, and environmentally unsustainable. The limitations of current AI systems therefore persist: issues with scalability, slow and unstable training, processing of irrelevant information, and catastrophic forgetting-all requiring a massive number of electronic components, which leads to trade-offs between cost, speed, power, size, accuracy, generalization, and inter-chip congestion. CMI-inspired CS-TPNS driven deep networks proposed here have shown the capability of processing large amounts of heterogeneous real-world AV data using a significantly smaller number of neurons compared to standard PNs-inspired deep nets, with better generalization in some cases. The demonstration of biologically plausible CS-TPNs simulation with BDSP shows that this efficient information processing approach is universal regardless of the learning algorithm. Future work includes further scaling up the proposed artificial and spiking TPNs inspired neural nets for a range of different real-world problems."}]}