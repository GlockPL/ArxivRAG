{"title": "Towards a Deeper Understanding of Transformer for Residential Non-intrusive Load Monitoring", "authors": ["Minhajur Rahman", "Yasir Arafat"], "abstract": "Transformer models have demonstrated impressive performance in Non-Intrusive Load Monitoring (NILM) applications in recent years. Despite their success, existing studies have not thoroughly examined the impact of various hyper-parameters on model performance, which is crucial for advancing high-performing transformer models. In this work, a comprehensive series of experiments have been conducted to analyze the influence of these hyper-parameters in the context of residential NILM. This study delves into the effects of the number of hidden dimensions in the attention layer, the number of attention layers, the number of attention heads, and the dropout ratio on transformer performance. Furthermore, the role of the masking ratio has explored in BERT-style transformer training, providing a detailed investigation into its impact on NILM tasks. Based on these experiments, the optimal hyper-parameters have been selected and used them to train a transformer model, which surpasses the performance of existing models. The experimental findings offer valuable insights and guidelines for optimizing transformer architectures, aiming to enhance their effectiveness and efficiency in NILM applications. It is expected that this work will serve as a foundation for future research and development of more robust and capable transformer models for NILM.", "sections": [{"title": "I. INTRODUCTION", "content": "The efficient use of energy has always been a critical challenge for humanity [1]. Due to the rise in energy demand and the depletion of fossil fuel reserves, there is an ongoing global trend to adopt sustainable Renewable Energy Systems (RES). Smart grids equipped with Demand-Side Management (DSM) have the capability to modify and optimize the power consumption patterns of end users over time through skims like dynamic power pricing [2]. With advanced RES systems, the focus shifts to smart Residential Energy Management Systems (REMS). REMS are considering data-driven approaches for modelling energy consumption behaviour because consumption patterns vary across borders. After the massive roll-out of Smart energy Meters (SMs) and Advanced Metering Infrastructures (AMIs), the NILM technology became a promising solution for modelling user energy consumption behaviour which offers 5% to 12% energy-saving [3].\nNILM, also known as energy disaggregation, is a single-channel blind source separation problem where the aggregate level electric load is broken down into appliance-level loads in a fully automated and non-intrusive manner. This process can be viewed as the decomposition of the aggregate power signal of a household into its additive sub-components, i.e., the power signals of each domestic appliance. NILM operates at the main power entering point of the house, i.e., SMs, and communicates with the in-home display (IHD) monitor. NILM works on the basis that each appliance has an unique power drawing signature. If we can this learn uniqueness of appliance signals, NILM algorithm can access individual appliances' power usages from the total power usages of the house in a non-invasive manner. Here \"non-intrusive\" refers to the fact that the whole process of electric power usage monitoring and disaggregating individual's power consumption is non-invasive and happens without any disruption of the activity or operation of the users. Figure 1 gives an overview of NILM in a residential setup.\nNILM is a challenging task to implement in real-time. The difficulty arises from the unknown power consumption patterns of appliances. The presence of multiple appliances with similar power consumption patterns and additional noise signals further complicates the task. However, machine learning, particularly deep learning approaches, has made significant advancements in addressing the challenges of NILM. Below we provide a discussion of these approaches."}, {"title": "A. Related works", "content": "Based on our survey, existing approaches can be divided into two mainstream approaches based on their internal mechanisms and functional algorithms, namely, classical machine learning and deep learning approaches.\nClassical machine learning approaches: These approaches assume that power signals can be broken down into a series of power features, with each feature corresponding to a different appliance. By extracting these features, the power consumption of individual appliances can be identified from the aggregated signal. Machine learning algorithms, such as support vector machine (SVM) [4], genetic algorithm [5], and sparse coding [6], have been used to decompose the aggregated features into the power features of each appliance. Mostly these approaches are based on event detection of the appliances. However, there are several drawbacks to these approaches. Firstly, a pre-processing step is required to extract features and tag data, which requires specialist knowledge and may introduce errors. Secondly, these approaches do not scale well to new houses or appliances, as they assume the aggregated power data is the sum of pre-defined features from training houses. In practice, the number and types of appliances in testing houses may be different.\nDeep learning approach: Deep learning has emerged from solving mainstream computer vision problems and influenced other data-driven research fields. Kelly and Knottenbelt are one of the first to propose a deep learning based NILM framework [7] that outperformed classical machine learning methods. The advantage of deep learning architectures is that they can directly process the raw power signal without the need for manual feature extraction or data tagging. They also accommodate noise signals and unknown appliances without assuming that the aggregated power signal fully represents the sum of the exact individual appliance power signal. Zhang et al. [8] enhanced their CNN model's capacity which surpassed the previous methods. However, CNN models effectively capture the local features within power consumption sequences but they may not adequately capture global features and dependency correlations between different positions in the sequence.\nAttention mechanisms [9], originally formulated in the field of natural language processing, happen to show promising results with NILM problems, especially with transformers. Yue's BERT4NILM [10] incorporates a bidirectional transformer architecture. Followed by their work methods such as, Sykiotis's ELECTRIcity-NILM [11], Wang's Midformer [12], Yue's ELTransformer [13], Kamyshev's COLD [14], Zhou's TTRNet [15] employed transformer in NILM. Transformer's capability to capture long-range temporal dependencies made them highly applicable for time-series power consumption data."}, {"title": "B. Our contribution", "content": "While BERT4NILM pioneered the use of transformers for NILM, it provides a limited understanding of the transformer with respect to its various hyper-parameters such as the number of layers and attention heads. There are also subsequent works that improved the BERT4NILM with various training mechanisms but their provided understanding of transformer with respect to NILM is also limited. Motivated by this gap and to foster future research in NILM with transformer, in this work, a comprehensive understanding of transformer architecture for residential NILM has been provided. Specifically, comprehensive experiments on various hyper-parameters of transformer architecture such as the number of attention heads, hidden dimensions and layers have been provided. Such experiments reveal the optimal transformer model, which is crucial for obtaining better performance. Based on the survey, this work is the first to analyze the performance of transformer with such large-scale experiments. In addition, this work provides comprehensive experiments on the BERT training strategy with the masking mechanism [10] which has not been provided by the existing works. It is hope that this analysis gives researchers further understanding of transformers that are non-existent in the existing works and motivates new transformer architectures. Note that these findings can also be useful for non-residential cases."}, {"title": "II. PRELIMINARIES", "content": "Transformer [16] is a deep learning architecture based on multi-head self-attention (MHSA) shown to the better performing than the CNN for a wide variety of tasks. An attention mechanism [17] is a mapping that takes a query, along with key-value pairs from the input sequence to produce an output representation. These input sequences including the queries, keys, values and output sequences, are all represented as matrices. The output is generated by calculating the weighted sum of the input values, with the weights assigned to each value calculated by evaluating the correlation between the query and the respective keys."}, {"title": "A. Single-head self-attention mechanism", "content": "The single-head self-attention (scaled dot-product attention) can be formulated with Q (Query), K (Key) and V (Value) matrices, obtained by linear transformation of the input matrix. The input queries Q, keys K are of dimension $d_k$ and values V of dimension $d_v$. Q and K are first multiplied and divided by $\\sqrt{d_k}$, which is then processed by a softmax operation to construct soft attention before being multiplied with V and returns a weighted value matrix. Finally, V is multiplied with the correlation matrix of Q and K to obtain the self-attention output. This process can be formulated with the following equation.\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$ (1)\nWhere the softmax function converts a matrix of set of real numbers into a probability distribution. For applying softmax function, suppose our input $X \\in \\mathbb{R}^{d_k \\times d_k}$ denotes the matrix $\\frac{QK^T}{\\sqrt{d_k}}$, where $X = [X_1, X_2, X_3, ..., X_{d_k}]$."}, {"title": "III. PROBLEM FORMULATION", "content": "In this section, residential NILM which is the focus of this work has been discussed. A home consists of many appliances, which can be grouped into four categories based on their operational states. Category I: simple on/off appliances like lights and microwaves. Category II: appliances with fixed operating states and repeatable patterns, like fridges and dishwashers. Category III: appliances with variable states, like dimmers and power drills. Category IV: constant-power appliances which operate, like smoke and fire detectors. Category I and Category II appliances are the most common and consume the major portion of electricity in the home, These two appliance categories are the main focus in the existing work, e.g., [10], [11], [18]. The formulation of NILM problem has been given as follows.\nLet N be the number appliances in residential home and i be the index referring to the $i^{th}$ appliance (i = 1, 2, . . ., N) [7]. The aggregated power consumption $P_{agg}$ at a time stamp t has been given which is sum of power consumption of individual appliances N (denoted by $P_i$), where i = (1, 2, ..., N). Thus, NILM is defined as follows:\n$P_{agg}(t) = \\sum_{i=1}^{N} P_i + E_{noise}(t)$ (4)\nWhere $E_{noise}$ is some additive noise from random appliances. In a real-time NILM framework, only $P_{agg}(t)$ is given. The task is to calculate the estimation values $P_i(t)$ of the actual values of the appliance signal $P_i$ from the aggregate signal values $P_{agg}(t)$."}, {"title": "IV. METHODOLOGY", "content": "In this section, a generalized NILM system including data acquisition, model testing, and testing steps, suitable for residential use cases has been presented. The proposed transformer model for NILM has been discussed in detail."}, {"title": "A. System Architecture", "content": "Data acquisition: In a practical real-world NILM system, a large pool of power consumption data is collected from subscriber's appliances and whole home power consumption. In practice, smart meter datasets have been used for building NILM systems, e.g., REDD. These datasets provides both aggregate level consumption and individual appliance level consumption data. In this work, the REDD dataset [19] has been used which consists of 6 house power consumption data. A particular appliance has been selected for training the model and the NILM system operates as shown in 2.\nModel training and testing: If F(\\theta) is the deep learning model and $\\theta$ being its corresponding parameters, the goal for model training is to make predicted appliance power sequence data ($F(\\theta, X), Y^2$) and real appliance power data as close as possible by changing $\\theta$ iteratively to minimize the loss function as follows:\n$\\theta = arg min C (F(\\theta, X), Y^2)$ (5)\nOne model F(\\theta) is a transformer model (shown shortly) and trained for each appliance. The structure of all models are same, the difference comes from the model parameters $\\theta$ (discussed shortly). The input is training data, and the model make predictions and iterates till the optimal model is selected.\nThe input data is aggregated channel power consumption data from the training set, e.g., mains2, mains3, mains4, mains5, mains6, and the output is the predicted appliance power consumption data for selected appliance, e.g., fridge2, fridge3, fridge4, fridge5, fridge6), in the corresponding period of time. The obtained optimal model F(\\theta*) is tested"}, {"title": "B. Proposed transformer architecture", "content": "This section describes the transformer model discussed above. Inspired by the BERT4NILM [10] which pre-trains a transformer with a BERT-like mechanism for energy dis-aggregation, we also employ a similar-looking architecture and training mechanism to study the transformers for NILM. The proposed architecture is depicted in Figure 3. Following the pre-processing step of the REDD dataset (the same pre-processing strategy used in [10] has been followed), input signals are passed through an embedding block to produce a set of embedding vectors. The embedding block is comprised of a convolution and pooling layer. The convolution layer $conv(.)$ produces a set of feature maps (which can be regarded as embedding vectors) and the pooling layer $pool(.)$ reduces their dimension for computational efficiency. Formally, the convolution layer computes feature maps from the input signal X' as follows.\n$Y^2 = conv (X', W^2), W \\in \\mathbb{R}^d$ (6)\nwhere W is learnable weight and Y is the feature map. Note that there can be an additional learnable bias parameter in the $conv(.)$. Pooling reduces the size of Y as follows.\n$Z^2 = pool (Y^2, a)$ (7)\nPositional information is found to be effective in the literature for transformers to obtain improved performance. A learnable positional vector e is produced and concatenate it with the pooled embedding vector Z. Formally,\n$Z'i = Zi + e$ (8)\nwhere Z' is the final embedding vector and + is position-wise concatenation. Given the Z' transformer, blocks compute self-attention operation based on the mechanism described in the preliminaries section. The Q, K, V are created from Z' as follows. Q, K,V = Z', Z', Z'. The output of one transformer block becomes the input of the other transformer block.\nGiven the output of the transformer block F, the reconstruction blocks perform the reconstruction of the original signal. It has two steps, namely, deconvolution and linear projection. In the deconvolution step, we perform a deconvolution operation to recover the size of the feature maps before the pooling operation. The linear projection operation reduces the feature maps to the original size. Formally, the deconvolution operation performs the following.\n$M^2 = tanh(deconv(F, W^2)), W \\in \\mathbb{R}^d$ and $M \\in \\mathbb{R}^{n \\times d}$ (9)\nwhere tanh is an activation function such as ReLU. The linear projection operation performs the following operation.\n$Q = linear(M, W^2),W \\in \\mathbb{R}^d$ and $Q \\in \\mathbb{R}^d$ (10)\nFor training stability, layer normalization and dropout operations has been performed during training. Various combinations of transformer blocks has been used which will be discussed in the following section.\nLoss function: The below loss function is used for training.\n$L(x, s) = \\frac{1}{T}\\sum_{i=1}^{T} (x_i - \\hat{x_i})^2 + \\frac{1}{T}\\sum_{i=1}^{T} log(1 + exp(-\\hat{s_i}s_i)) + \\lambda_{DKL} D_{KL} (softmax(\\frac{x}{\\tau})\\softmax(\\frac{\\hat{x}}{\\tau})) + \\lambda (1-\\frac{1}{T}\\sum_{i=0}^{T} |\\hat{x_i} - x_i|)$ (11)\nwhere $x, \\hat{x} \\in [0,1]$ represent the ground truth and prediction of output sequence divided by maximum power limit. $\\hat{s}, s\\in \\{-1,1\\}$ are the appliance state label and prediction. T stands for total time steps that is sequence length and O refers to the set of time steps when either the status label is on or the prediction is incorrect. In this equation, we also introduce hyperparameters $\\tau$ and $\\lambda$ for tuning sequence softmax temperature and reduction of absolute error."}, {"title": "V. EXPERIMENTAL SETTINGS AND RESULTS", "content": "Experiments with the REDD dataset has been conducted following the data pre-preprocessing, training and testing protocols used in [10]. There are six house data in the REDD dataset from which house 1 data for testing and the remaining house data for training has been used. Pytorch has been used to develop the transformer and conduct the experiments. The models are trained from 100 epochs with AdamW optimiser using a P100 GPU on a cloud HPC. The obtained results has been compared with GRU+, LSTM+ and CNN [10].\nExperiment with the following hyper-parameters of transformers, namely, hidden dimension of attention layers, number of transformer layers L, number of attention heads and dropout ratio in attention layers has been done. For a comprehensive understanding, experiment with a large variety of these hyper-parameters have been conducted. Table I provides the list of hyper-parameters and their different values. Since the combinations of these hyper-parameters are large in number, only experiment with one hyper-parameter at a time has been conducted while keeping the other hyper-parameters fixed. The optimal hyper-parameter setting proposed in [10], i.e., hidden dimension = 256, number of layer/attention heads = 2 and dropout ratio = 0.1 has been used for fixing the hyper-parameters that are not being investigated. In addition, experiment with the masking ratio of BERT training has been conducted. In [10], masking ratio of 0.25 is used."}, {"title": "B. Analysis of results", "content": "1) Evaluation of various number of layers: The number of layers plays a vital role in defining the capacity of the models, i.e., the number of learnable parameters. As shown in Table I, higher capacity models yield better MAE and MRE metrics, however, at the expense of learnable parameters. We noticed that two layers are sufficient for obtaining good results across all four metrics. The higher capacity models usually require large samples to learn effective features which are non-existent in REDD dataset.\n2) Evaluation of various numbers of attention heads: Attention heads are also related to the model capacity. Based on the Table I results, 4, 32 and 128 heads produce better results, however, they contribute to higher computational cost. In comparison, two heads produce comparable results at a lower cost, therefore, we suggest using only two heads.\n3) Evaluation of size of hidden dimension: Hidden dimension size is crucial for defining model capacity to capture complex patterns and relationships in data. Higher hidden dimension size contributes to larger learnable parameters and higher computation costs. Based on the Table I results, 16 appears to be an optimal hidden dimension size overall while higher sizes sometimes bring extra performance for some appliances.\n4) Evaluation of dropout ratio and masking ratio: Dropout plays the role of regularisation to prevent issues such as overfitting. Based on the Table I results, we think that a dropout ratio of 0.5 appears to be best among other ratios. Masking ratio selection for BERT training is also essential for good performance. Based on the obtained results, 0.3 appears to be the most effective masking ratio."}, {"title": "VI. OPTIMAL MODEL SELECTION", "content": "Based on the analysis above, the transformer model has been selected with the optimal hyper-parameter. Table I (last row) shows the results. It is clear that the optimal transformer has surpassed the original BERT4NILM architecture across various metrics on all four appliances. This finding justifies that selecting optimal hyper-parameters is crucial for obtaining good performance with transformer architecture. As shown in Fig. 4, our optimal transformer trains more efficiently than other ones with higher capacity."}, {"title": "VII. CONCLUSION AND FUTURE WORKS", "content": "In this paper, a comprehensive analysis of transformer model have been performed. Based on the findings, it is clear that the transformer is robust to a large set of model hyper-parameters. Based on this observation, a compact transformer model has been proposed that uses 150x less number of parameters than the BERT4NILM transformer. The proposed compact transformer can achieve good performance on four appliances of REDD datasets. In the future, experiments can be conducted with larger datasets. Moreover qualitative analysis of results can be conducted."}]}