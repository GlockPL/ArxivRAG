{"title": "HOW TO SELECT SLICES FOR ANNOTATION TO TRAIN BEST-PERFORMING DEEP LEARNING SEGMENTATION MODELS FOR CROSS-SECTIONAL MEDICAL IMAGES?", "authors": ["Yixin Zhang", "Kevin Kramer", "Maciej A. Mazurowski"], "abstract": "Automated segmentation of medical images highly depends on the availability of accurate manual image annotations. Such annotations are very time-consuming and costly to generate, and often require specialized expertise, particularly for cross-sectional images which contain many slices for each patient. It is crucial to ensure the best use of annotation resources. In this paper, we systematically answer the question of how to select slices of cross-sectional medical images in order to maximize performance of the resulting deep learning segmentation models. We conducted experiments on 4 medical imaging segmentation tasks with varying annotation budgets, numbers of annotated cases, numbers of annotated slices per volume, slice selection techniques, and mask interpolations. We found that: 1) It is almost always preferable to annotate fewer slices per volume and more volumes given an annotation budget. 2) Selecting slices for annotation by unsupervised active learning (UAL) is not superior to selecting slices randomly or at fixed intervals, provided that each volume is allocated the same number of annotated slices. 3) Interpolating masks between annotated slices rarely enhances model performance, with exceptions of some specific configuration for 3D models.", "sections": [{"title": "1 Introduction", "content": "Convolutional Neural Networks (CNNs) and Visual Transformers (ViTs) are leading architectures in deep learning for image processing due to their high performance. However, their training requires large, annotated datasets, which are frequently unavailable in medical imaging. Although fine-tuning models that have been pre-trained on extensive natural image datasets (such as COCO, ImageNet, or ADE20K) appears promising, Alzubadi et al. demonstrated that these models might only capture limited features from CT/MRI images due to differences in feature spaces between natural and medical images. [1] As a result, researchers often need to create new annotations for their specific medical imaging tasks."}, {"title": "2 Methods", "content": "Our research aims to offer guidance on how to annotate 3D volumes effectively when faced with annotation budget constraints. We address this issue through three primary dimensions."}, {"title": "2.1 The three primary dimensions of interest", "content": "Our research aims to offer guidance on how to annotate 3D volumes effectively when faced with annotation budget constraints. We address this issue through three primary dimensions."}, {"title": "2.1.1 Fewer Annotated Slices Per Volume for More Volumes or More Annotated Slices Per Volume for Less Volumes?", "content": "We explored whether it is more advantageous to annotate a few volumes with more annotated slices per volume or to spread annotations across a larger number of volumes with fewer annotated slices per volume. To simplify the description, we define the parameters involved in this experiment as annotation density (p) and volume count (s), respectively. Annotation density (p) represents the percentage of annotated slices per volume. It takes values from the set {5%, 10%, 20%, 40%, 80%}. Volume count (s) represents the fraction of volumes with annotations from all volumes reserved in the original training set. It takes value from the set {1/8,1/4,1/2,1/1}. For a dataset configuration with (p=10%, s=0.25), one-fourth of all available training volumes are selected for this dataset, and 10% slices per selected volume are annotated. Multiplying annotation density and volume count will then give the total annotation budget as the fraction of the budget for all volumes and all slices.\nFor all segmentation tasks, we created the sparsely annotated datasets by iterating through all combinations of parameter pairs (p,s). We trained models on these datasets three times for each unique set of hyper-parameters, and then compared their average performance (IoU). In this experiment, for each chosen budget level, we report the average performance across the different (p, s) combinations using the three slice sampling methods."}, {"title": "2.1.2 Impact of Mask Interpolation (M.I.)", "content": "We also examined whether interpolating masks between annotated slices before training improves model performance in both 2D and 3D segmentation settings. We first verified that our implementation of M.I. achieved comparable performance to Sli2Vol [3] and SSA [4] on organ-like objects. Following the procedure outlined in Algorithm 1, we generated pseudo-masks for the unannotated slices in a sparsely annotated dataset."}, {"title": "Algorithm 1 Slices with Human Annotations", "content": "1: pos_queue \u2190 {human annotated masks}\n2: while pos_queue is not empty do\n3:  maski \u2190 pos_queue.dequeue()\n4:  maski\u22121,maski+1 \u2190 mask_prop(slicesi, maski)\n5:  if slicei-1 is new and slice\u017c\u22121 contains object then\n6:   pos_queue.addback(maski\u22121)\n7:  end if\n8:  if slicei+1 is new and slicei+1 contains object then\n9:   pos_queue.addback(maski+1)\n10:  end if\n11: end while\n12: Assign 0 to the rest of the unannotated slices.\nWith the setup using all volumes (s=1), we vary the annotation density p to evaluate whether, and under what conditions, models trained with M.I. yield better performance compared to those that ignore the gradients of unannotated slices or voxels. We report on the quality of the interpolated mask based on its IoU with the corresponding human annotation, and compare model performance on the same sparsely annotated volumes, both with and without the M.I. technique."}, {"title": "2.1.3 Impact of Slice Sampling Methods within Volumes", "content": "Assuming a fixed configuration of annotation density (p), volume count (s), we examined different ways of selecting slices for annotation impact model performance. The methods of selection considered are:\n1. Selection at fixed interval (i.e., annotating every n-th slice)\n2. Selection at uniform random (i.e., randomly picking slices with uniform distribution)\n3. Selection by UAL (i.e., using a \"smart\" way to select samples)\nFor each (p, s) configuration, we train three models on datasets with annotated slices selected by the aforementioned sampling methods. We measure each model's performance relative to the triplet's average and display the results in a bar plot. If a slice selection method is superior, its bar should be noticeably higher than the others for the same dataset."}, {"title": "2.2 Datasets", "content": "Our experiments involved three datasets across four tasks. Table 1 summarizes the dataset specifications after preprocessing, ensuring all annotated volumes have uniform dimensions and consistent voxel spacing."}, {"title": "2.3 Model Training", "content": "The model architecture and hyper-parameters paired with each dataset are listed in Table 2. We selected various variants of representative segmentation models commonly used in the computer vision community and paired them with each dataset based on its level of difficulty. In other words, more challenging datasets were matched with models having larger parameter sizes and greater prediction capabilities. This approach allows us to cover a wide range of training settings and datasets without having to repeat experiments across all possible (dataset-model) combinations. The hyper-parameters for model training were tuned using the original densely annotated dataset to ensure that model performance falls within the operational range reported in existing literature for similar tasks. In our 2D segmentation experiments, we ensure that samples in each batch are drawn uniformly at random from the annotated slices across all datasets to maintain consistency. For 3D segmentation, we implement different sampling strategies based on the dataset. In LiTS17, we sampled patch centers with a 50% probability for liver, 30% for tumor, and 20% for background or unannotated voxels to focus on the areas of interest. For the DBC and FGT datasets, patch centers are sampled randomly without specific probabilistic constraints. In the ATLAS dataset, we sample patch centers with a 30% probability for background, 55% for lesions, and 15% for unannotated voxels to ensure a diverse representation of the different regions. These sampling strategies are designed to optimize the training process by effectively representing the various anatomical features and ensuring balanced learning. The results of these experiments will be detailed in the following section."}, {"title": "3 Results", "content": null}, {"title": "3.1 Fewer Annotated Slices Per Volume for More Volumes or Converse?", "content": null}, {"title": "3.1.1 2D Segmentation Setting", "content": "Fig. 1 presents the performance of models trained on sparsely annotated datasets relative to those trained on densely annotated counterparts, averaged across all datasets for different(p,s) configurations. This figure highlights an important observation: under 2D segmentation setting, regardless of the total annotation budget, it is consistently more advantageous, in terms of model performance, to annotate more volumes with fewer slices per volume, provided that the total number of annotated slices remains constant. This finding also holds across all levels of annotation density. In fact, the impact of the density of the annotated volume on performance was generally quite low. This means that annotations of many slices could be skipped with very low loss in performance (until the density is less than 0.2). The driving factor for model performance was clearly the number of volumes which contained annotations.\nFor LiTS17 (Liver Tumor Segmentation, Fig. 2a, 2e), higher annotation density (p) consistently resulted in lower performance when compared to increasing volume counts, regardless of the budget. Doubling p did not improve performance as effectively as doubling the volume count (s). The same performance trend observed in LiTS17-tumor was seen in both tasks of the DBC-FGT datasets (Fig. 2b, 2c, 2f, 2g). For these tasks, improving performance was more effectively achieved by increasing the volume count rather than the annotation density.\nIn the ATLAS dataset (Brain MRI, Fig. 2d, 2h), we also observed a trend not captured in the previous few datasets at four budget levels (1181, 2362, 4725, and 37800 slices): some lower volume count configurations achieved comparable or even superior performance. There are two cases for such exception, one occurs when both the total budget and annotation density are small (i.e., budget < 4725 slices, p <0.1). The other occurs when both the total budget and annotation density are large (i.e., budget > 37800 slices, p \u22650.4).\nBased on these results, if a dataset is prepared for training 2D models, spreading annotation budgets across more volumes at diverse locations is always more cost-effective. A small annotation density (e.g., 5% ~ 20% slices labeled) is often sufficient for training models with similar performance to the counterparts trained with the full dataset."}, {"title": "3.1.2 3D Segmentation Setting", "content": "n the 3D segmentation setting, most patterns observed in the 2D models also apply, but with some notable differences. As shown in Fig. 1b, we observed that excessively low annotation density can hinder model performance in certain conditions.\nFor LiTS17-tumor and DBC-FGT tasks in 3D, the trends mirror those in 2D (Fig. 3a, 3b, 3c, 3e, 3f, 3g): low annotation density does not significantly affect performance due to clear object-background contrast. However, for the ATLAS dataset, the behavior was slightly different. At low annotation density p = 0.05, the model failed to learn useful features and showed no prediction capability, thus omitted in our report. Instead, increasing annotation density in the range of 0.1 \u2264 p < 0.4 led to noticeable performance improvements, as shown in Fig.3d, 3h."}, {"title": "3.2 Should Mask Interpolation Be Used?", "content": "We examined whether imputing labels for unannotated voxels before training provides an advantage over training directly with sparse annotations. Table 4 presents the IoUs between interpolated dense annotations and their human-annotated counterparts across different annotation densities.\nIn 2D segmentation, mask interpolation had minimal impact on model performance across all datasets. In 3D segmentation, the effects varied: for DBC breast segmentation, the impact was minimal, while LiTS17 tumor segmentation often experienced a performance drop, particularly at low annotation densities (p = 0.05). Significant performance drops were noted with FGT (fibro-glandular tissue segmentation). This may be due to loss of tissue details in interpolated masks, such as gaps and spaces in tree-like structures, yet the reason for the lack of performance drop in 2D cases remains unclear. Conversely, ATLAS showed improved performance at low annotation densities (p \u22640.2), indicating that mask interpolation can be beneficial for certain types of objects."}, {"title": "3.3 Is the way slices are selected for annotation important?", "content": "In the previous section, we compared how the trade-off between annotation density and volume count affects model performance. Now, we examine the impacts of the different slice selection policies.\nFig. 5 shows that the performance differences between the three slice selection policies are minor. Neither method consistently outperforms the others in 2D or 3D settings. In 3D models, while overall trends are similar to those in 2D, there is greater variation in how different selection policies affect performance."}, {"title": "4 Discussion", "content": "In our 2D segmentation experiments, we found it advantageous to distribute the annotation budget across a greater number of volumes with fewer annotated slices per volume. This approach helps mitigate redundancy in adjacent slices, as focusing annotation on fewer volumes results in many highly similar slices. Annotating a broader range of volumes introduces the model to a wider variety of variations between volumes, which data augmentation may not replicate. In 3D deep learning methods, the principle that \"sparser annotations are better\" generally holds true as well. We notice an exception to this rule for the ATLAS dataset when a very low number of slices per volume is annotated. When annotation density falls below 20%, prioritizing a higher volume count over annotation density does not result in improved model performance. Our preliminary data on the topic points to the likely impact of the network architecture (the generally observed trend appears to be maintained when AttentionUNet-3D [23] is used in place of UNETR) and a potential impact of an interaction between this dataset and the architecture. More experiments are needed to further elucidate this exception.\nRegarding slice selection policies, we did not find any one method to be significantly superior to the others. All three tested methods produced comparable results. Although previous literature suggests that slice selection by UAL-based algorithms can outperform that at fixed interval or random, this advantage was not consistently observed across all datasets in our study. This discrepancy might be because UAL tends to pick visually distinct slices, which may not always be the most relevant for our segmentation tasks. Additionally, we applied UAL-based selection within each volume to keep the experiment uniform and reduce computational load, rather than across the entire dataset. This method could have limited UAL's effectiveness compared to dataset-level selection, where the latter might better identify and exclude less informative volumes.\nOur findings recommend that annotation budgets be distributed across as many volumes as possible for both 2D and 3D segmentation tasks. For 3D cases, it is also important to maintain a minimum number of annotated slices per volume to ensure effective model training and convergence. Since all examined slice selection methods yielded comparable results, slice selection at fixed interval is proved sufficient for simplicity. In our study, we did not include biased selection method such as those including only slices from the beginning of the volume, as we focused on those that would distribute the annotated slices more evenly across the volume. Future studies may explore the impact of violating this assumption, such as if slices were exclusively taken from a single lesion rather than multiple lesions within a volume.\nOur study assumed each slice consumes consistent time and resources to annotate. In practice, slices containing target objects are more time-consuming to annotate as compared to the empty counterparts. Also, annotating slices across multiple volumes can be more challenging than annotating the same number of slices within a single volume due to the additional effort required for frequent switching between images and re-recognizing the context in the images. Future study may explore a more advanced modeling of the resources and time consumed under these settings."}]}