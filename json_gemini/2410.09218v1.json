{"title": "Continual Learning with Neuromorphic Computing: Theories, Methods, and Applications", "authors": ["Mishal Fatima Minhas", "Rachmad Vidya Wicaksana Putra", "Falah Awwad", "Osman Hasan", "Muhammad Shafique"], "abstract": "To adapt to real-world dynamics, intelligent systems need to assimilate new knowledge without catastrophic forgetting, where learning new tasks leads to a degradation in performance on old tasks. To address this, continual learning concept is proposed for enabling autonomous systems to acquire new knowledge and dynamically adapt to changing environments. Specifically, energy-efficient continual learning is needed to ensure the functionality of autonomous systems under tight compute and memory resource budgets (i.e., so-called autonomous embedded systems). Neuromorphic computing, with brain-inspired Spiking Neural Networks (SNNs), offers inherent advantages for enabling low-power/energy continual learning in autonomous embedded systems. In this paper, we comprehensively discuss the foundations and methods for enabling continual learning in neural networks, then analyze the state-of-the-art works considering SNNs. Afterward, comparative analyses of existing methods are conducted while considering crucial design factors, such as network complexity, memory, latency, and power/energy efficiency. We also explore the practical applications that can benefit from SNN-based continual learning and open challenges in real-world scenarios. In this manner, our survey provides valuable insights into the recent advancements of SNN-based continual learning for real-world application use-cases.", "sections": [{"title": "1 Introduction", "content": "Humans continuously learn from their interactions with the environment throughout their lifetime [1], exhibiting remarkable cognitive flexibility of brains to enable simultaneous learning and retention of multiple skills. For instance, a person fluent in English can also master Spanish and French, each with unique grammatical structures and vocabulary, without losing proficiency in their native language. This ability is rooted in the brains' delicate balance between plasticity and stability [2, 3]. Plasticity allows for the acquisition of new skills, while stability ensures that existing knowledge is preserved. Naturally, we expect Artificial Intelligence (AI) systems [4-6], wielding Neural Network (NN) algorithms [7], to develop a similar learning capability to effectively operate and adapt in real-world scenarios. While state-of-the-art AI systems excel in single task-based static environments, such as image recognition [8], face identification [9], and speech recognition [10], they often struggle in dynamic environments where data or tasks may change in structure, distribution, or characteristics over time. For instance, when a new task is encountered after a resource-intensive training, NNs are often retrained from scratch despite the high computational costs [11]. Therefore, robustness to multiple tasks and sequential experiences, remains a significant research challenge for AI systems. When faced with incremental learning of different tasks, most NNs underperform due to suffering from rapid performance degradation, a phenomenon known as Catastrophic Forgetting (CF) or interference [12-14].\nIn recent years, Continual Learning (CL) [15-17] emerges as a conceptual solution for addressing CF in AI systems. CL aims to balance the system's ability to learn new tasks without forgetting the previous knowledge, known as the stability-plasticity dilemma [18, 19] (Fig. 1). It also seeks to achieve generalizability across tasks. In addition to avoiding CF, the primary objectives of CL also include ensuring scalability of NNs, minimizing reliance on old data, realizing controlled forgetting, and enabling rapid adaptation and recovery, which will be further discussed in Section 2.2.\nNumerous CL methods have been proposed to address CF in the conventional Artificial/Deep Neural Network (ANN/DNN) domain, which showed notable performance improvements [21-27]. However, most methods are resource-intensive, demanding additional memory and computational power. Therefore, DNN-based CL methods often fail to account for storage usage during incremental training, necessitating a significant addition of memory for parameters [28]. These conditions are not suitable for autonomous embedded systems, that use portable-battery and need quick adaptation to new data within limited resources. However, designing resource- and energy-efficient systems with CL capabilities (i.e., CL systems) is still a major challenge.\nRecently, the brain-inspired Neuromorphic Computing (NC) paradigm [29-33] has emerged as a promising field for enabling efficient and low-power information"}, {"title": "1.2 Our Novel Contributions", "content": "Our survey addresses the key questions in developing CL with neuromorphic computing (SNNs), as the following.\n\u2022 What are the desiderata, evaluation scenarios, and performance metrics for CL?\n\u2022 What are the existing CL methods in the ML field to overcome CF, and how can they be systematically categorized?\n\u2022 What are the limitations of the existing CL methods?\n\u2022 Why are the energy-efficient CL algorithms crucial for resource-constrained autonomous embedded systems?\n\u2022 What techniques do the state-of-the-art propose to enable energy-efficient CL with SNNs (i.e., NCL)?\n\u2022 What are the real-world application use-cases that benefit from NCL?\n\u2022 What are the open challenges to be considered for enabling energy-efficient NCL?\nBy addressing the above key questions, our paper makes the following major novel contributions:\n1. We introduce the fundamentals of CL problems by explaining its basic formulation, desiderata, and possible learning scenarios. Furthermore, we highlight the OCL paradigm and its importance for practical applications.\n2. We discuss the existing methods for enabling CL, which are pre-dominantly from DNN domain. Afterward, we provide different possible CL settings, and provides metrics for evaluating CL.\n3. We discuss the background of NC (SNNs), then provide an up-to-date comprehensive review of state-of-the-art works for NCL.\n4. We survey a range of real-world applications that benefit from the energy-efficient NCL. Toward the end, we discuss the open research challenges to provide a perspective on future research directions."}, {"title": "1.3 Paper Structure", "content": "The rest of this survey paper is organized as shown in Fig. 3. Section 2 delves into the CL fundamentals. Section 3 provides an overview of SNNs and the state-of-the-art NCL methods. In Section 4, a summary of real-world application use-cases of NCL is presented. Then, Section 5 discusses open research challenges for meeting the CL desiderata with SNNs. Finally, Section 6 presents the conclusion."}, {"title": "2 Continual Learning", "content": "In recent years, DNNs have revolutionized the field of AI [51, 55, 56], due to their effective training technique (i.e., gradient descent-based backpropagation), while assuming the training data are independently and identically distributed (i.i.d) [57, 58]. However, a critical question arises: how can we integrate new data into existing models? One approach is to retrain with new data, while using current parameters as the initial state [58]. However, successive training tends to cause NNs to suffer from CF [59, 60]. Although retraining from scratch effectively tackles CF issues, but it is inefficient and almost impossible in some cases, such as real-time learning scenarios where the model must update and adapt to continuous data streams instantly with limited resources (e.g., developmental learning for autonomous agents [61]).\nCL (also known as incremental learning or lifelong learning) addresses CF by accumulating knowledge from a continuous data stream throughout their lifetime [1]. As stability-plasticity dilemma requires trade-off between preserving past knowledge (stability) and adapting to new experiences (plasticity) [62], researchers have developed several CL methods to provide the same capability [63-65]. While many CL methods focused on the offline learning with supervised task-based incremental learning, which operates under the assumption of i.i.d. data and constant task identification, these assumptions often diverge from real-world scenarios. In practical applications, input data streams are typically not i.i.d. and task identity may not be available. It highlights the complexity of CL problems, showing the importance of understanding the CL foundations. Therefore, in this section, we delve into the basic formulation and desiderata of CL, and explore various learning scenarios, methods, settings, and evaluation metrics.\n\n2.1 Basic Formulation\nA CL algorithm must adapt to new tasks without full access to previous training data while maintaining performance on respective test sets. Formally, defined by parameters $ \\theta = \\bigcup_{t=1}^{T} \\theta^{(t)} $, where $ \\theta^{(t)} = {e^{(t)}, \\psi} $, $e^{(t)}$ is the task-specific parameters and $ \\psi $ is the task-sharing parameters for a task t. A batch of training samples for a task t is denoted as $ D_{t,b} ={X_{t,b}, Y_{t,b}} $ where $X_{t, }$ represents input data, $Y_{t,b}$ denotes data labels, and $t\\in T = {1,\\dots,T}$ signifies the task identity, with $b \\in B_t$ as the batch index (T and $B_t$ indicate their respective spaces). Task t is defined by its training samples $D_t$, with distribution $D_t := p(X_t, Y_t)$, $D_t$ encompasses the entire training set, omitting the"}, {"title": "2.2 Desiderata of CL", "content": "In this section, we explain the desired characteristics (desiderata) of a CL algorithm, and show an overview of key requirements to achieve CL capability using a robotic arm as an example in Fig. 4.\n\n2.2.1 Scalability\nA CL algorithm should effectively train on a large or potentially unlimited number of tasks without expanding NNs excessively. The increasing of NN capacity and computational costs have to be minimal or sub-linear, thereby ensuring the scalability of a CL algorithm [53]. To achieve high scalability, the major challenges include scalability of network model size [16, 66] and scalability of regularization terms (e.g., weight regularization [67]).\n\n2.2.2 No/Minimal Usage of Old Data\nMinimizing or eliminating the reliance on data from previous tasks is crucial for a CL algorithm, because of storage constraints and privacy concerns. Recent researches have made progress to achieve this goal [68-73]. For instance, previous studies utilized mutual information maximization and compressed gradients to minimize the reliance on old data [68-70], and employed representative subsets to preserve knowledge [71].\n\n2.2.3 Task Agnostic (TA)\nA CL algorithm should function independently from pre-defined task boundaries during the training and inference phases, known as Task Agnostic (TA). For instance, a robot in an ever-changing environment should adapt to new tasks, such as picking up different objects or navigating new terrains without explicit information of task identity. It has to learn from environment and experiences, and dynamically adjust to new tasks. State-of-the-art attempted to achieve TA using methods studied in [74-81].\n\n2.2.4 Positive Forward Transfer (FWT)\nA CL algorithm should leverage previous knowledge to enhance performance on new tasks, known as Positive Forward Transfer (FWT). Formally, $FWT_k$ evaluates the average influence of all old tasks on the current k-th task, and can be stated as:\n$ FWT_k = \\frac{1}{k-1} \\sum_{j=2}^{k} (\\tilde{a}_{j,j} - a_j) $"}, {"title": "2.2.5 Positive Backward Transfer (BWT)", "content": "A CL algorithm should transfer knowledge from later tasks to past tasks for improving performance on the past tasks, known as Positive Backward Transfer (BWT). Here, zero backward transfer suggests zero forgetting [53]. Formally, $BWT_k$ evaluates the average influence of learning the k-th task on all old tasks, and can be stated as:\n$ BWT_k = \\frac{1}{k-1} \\sum_{j=1}^{k-1} (a_{k,j} - a_{j,j}) $"}, {"title": "2.2.6 Controlled Forgetting", "content": "A CL algorithm should forget old yet insignificant knowledge to make room for learning new information, known as controlled forgetting. Although humans do not experience sudden memory loss, a gradual decrease in memory (forgetting) over time is natural, hence controlled forgetting is beneficial for managing memory retention [92]. Related methods include [93-95] for DNNs and [41, 42, 96, 97] for SNNs."}, {"title": "2.2.7 Fast Adaptation and Recovery", "content": "A CL algorithm should quickly learn new tasks while minimizing the loss of previous knowledge (fast adaptation), and regain previous performance after encountering new tasks that cause degradation in learned knowledge (recovery). Related methods include regularization [67] and gradient-based meta-learning or fast optimization [98]."}, {"title": "2.3 Scenarios in CL", "content": "CL scenarios can be categorized using a taxonomy presented in Fig. 5, while their formal comparison is provided in Table 2. The initially-identified scenarios are Task-Incremental Learning (Task-IL), Domain-Incremental Learning (Domain-IL), and Class-Incremental Learning (Class-IL). While these scenarios cover some desiderata of CL, they fail to meet the requirement of TA [53]; see Fig. 6. They assume that tasks have clear and well-defined boundaries during training. Therefore, recent studies proposed expanding the CL scenarios by introducing Discrete TA (DTA) [74], Continuous TA (CTA) [74], and Open-World TA (OWTA) [53].\n\n2.3.1 Task-Incremental Learning (Task-IL)\nIn Task-IL, a model sequentially learns to solve a number of distinct tasks [99-101]. Each task has disjoint output spaces {Vt-1 \u2260 Vt} and Task IDs are known in training"}, {"title": "2.3.2 Domain-Incremental Learning (Domain-IL)", "content": "In Domain-IL, a model learns to solve the same problem in different tasks [99-101]. Tasks have the same data label space {Yt-1 = Yt} but different input distributions p(Xt-1) \u2260 p(Xt). The model does not require Task IDs in the inference phase, as each task has the same possible outputs (e.g., same classes in each task). The use of a single-headed model (i.e., same output head for every task) ensures that the output space remains consistent. An example of Task-IL is incrementally learning to recognize objects midst varying lighting conditions (e.g., indoor and outdoor) [102]."}, {"title": "2.3.3 Class-Incremental Learning (Class-IL)", "content": "In Class-IL, a model must distinguish the global labels (classes) [99\u2013101]. Here, Task IDs are only provided in the training phase, as they will be inferred in the inference phase. Due to the multi-class property, p(t-1) \u2260 p(Vt) is a natural consequence, thereby posing more challenges as compared to Task-IL and Domain-IL."}, {"title": "2.3.4 Discrete Task Agnostic (DTA)", "content": "In DTA, a model learns from a sequence of distinct tasks, but without the need for inferring the Task ID during training and inference phases [53, 74]. The input distributions differ between tasks, making p(Xt\u22121) \u2260 p(Xt) true. Meanwhile, the target label across tasks have the same distributions, and the output space is the same across tasks, making p(Vt-1) \u2260 p(Vt) and {Vt-1 \u2260 Vt} false."}, {"title": "2.3.5 Open-World Task Agnostic (OWTA)", "content": "In OWTA, a model learns from a sequence of distinct tasks, but needs to infer the Task ID during training and inference phases [53, 74]. It requires the CL algorithm to handle an open set of tasks and classes, potentially facing unknown categories that are not encountered during initial training. This makes the OWTA as one of the most challenging scenarios."}, {"title": "2.3.6 Continuous Task Agnostic (CTA)", "content": "In CTA, data stream is represented as a continuous function over time without explicit task boundaries [53, 74], which makes CTA as the most challenging scenario. Here, a CL algorithm must learn from evolving distribution without any task-specific guidance, as neither task boundaries are clear nor Task ID is known during the training phase [53], making p(Xt\u22121) \u2260 p(Xt), p(Yt\u22121) \u2260 p(Vt) and {Yt-1 \u2260 Yt} false."}, {"title": "2.4 Online Continual Learning (OCL)", "content": "In OCL, a model learns from a continuous data stream, so that each input sample during inference is used as training data for updating the models' knowledge [46, 103, 104]. As a new sample arrives, it is immediately used for inference, and for updating the model on-the-fly. This capability is highly desired for systems that face highly dynamic environments (e.g., autonomous mobile agents), where new data is encountered in real-time and immediate updates to the model are required. Its challenges include integrating new knowledge and retaining previous information, while considering limited memory and power/energy budgets. In this survey, we emphasize that OCL is the expected capability for autonomous embedded systems."}, {"title": "2.5 CL Methods", "content": "In this section, we discuss existing methods for addressing CF in various CL scenarios. These methods are categorized into five approaches based on representation, regularization, rehearsal/replay, optimization, and architecture [16], as follows.\n\n2.5.1 Representation-based Approach\nThis approach is characterized by exploiting the strengths of data representations [16]; see Fig. 7. The related methods are as follows.\nA. Self-Supervised Learning (SSL): It makes the model learn to generate useful data representations without relying on explicit labels, by creating supervisory signals from the input data to uncover the underlying structure of the data [105, 106]. Related studies are discussed in [107-115].\nB. Pre-training for Downstream CL: It is the initial phase of model training on a large diverse dataset to learn general-purpose representations, which can be fine-tuned for specific downstream tasks, hence having strong knowledge transfer [83, 116-118]. To maintain generalizability for future tasks, several strategies have been developed, such as Pre-trained Representations [88, 119\u2013122], Task-adaptive Prompts [16, 123-128], Saving Prototypes and Enhancing Classifiers [129-131], and Optimizing an Updatable Backbone [83, 132\u2013135].\nC. Continual Pre-Training (CPT): It includes techniques studied in [136-142].\n\n2.5.2 Regularization-based Approach\nThis approach is characterized by adding explicit regularization terms to balance the old and new tasks [16], which usually requires storing a frozen copy of the old model for reference; see Fig. 8. The related methods are as follows.\nA. Weight Regularization: It includes techniques such as Elastic Weight Consolidation (EWC) [67], Synaptic Intelligenc (SI) [63], Memory Aware Synapses (MAS) [143], Riemannian Walk (RWalk) [144]. Meanwhile, other techniques are categorized into Expansion-Renormalization [87, 95, 145, 146], Quadratic Penalty Refinement [147-149], and Online Variational Inference [93, 150\u2013155].\nB. Function Regularization: It regulates the variations in models' function $f_\\theta$ over time, and applies constraints directly to the function. A generalized form of function regularization can be expressed as:\n$\\underset{\\Theta}{\\text{arg min}} \\{\\frac{1}{N_t} \\sum_{i=1}^{N_t} \\mathcal{L}_t(f_\\theta(x_i), y_i) + \\lambda R_f(f_\\theta, D_{1:t-1})\\}$"}, {"title": "2.6 CL Settings", "content": "2.6.1 Supervised Learning\nIt is an ML setting that trains models with labeled datasets. Therefore, in the Supervised CL setting, the goal is to train a model on a sequential data stream. Its challenge is to efficiently perform the training, since the supervised-based learning typically requires huge memory and energy requirements. Moreover, obtaining a large labeled dataset can be time consuming and expensive.\n\n2.6.2 Unsupervised Learning\nIt is an ML setting that trains models with unlabeled data (e.g., identifying patterns, structures, or relationships) [261]. Therefore, in the Unsupervised CL setting, the goal is to develop a model that can discover similarity in samples that arrive sequentially [261]. Its challenge is to extract meaningful features without explicit labels. This setting is particularly useful for tasks where data labeling is impractical or expensive.\n\n2.6.3 Reinforcement Learning (RL)\nIt is an ML setting that trains models to make sequential decisions based on the feedback (i.e., rewards or penalties) [262, 263]. Through trials and errors, the model learns to associate actions with outcomes, aiming to discover the optimal strategy or policy to achieve its objectives over time. Its challenge is to efficiently perform the training, as RL typically requires high memory and energy requirements. Moreover, the nature of trials and errors can also be time-consuming and expensive."}, {"title": "2.7 Evaluation Metrics", "content": "To evaluate the performance of an efficient CL system, metrics are essential. To assess the overall performance of a CL algorithm, average accuracy (\u0410\u0410) [86, 264] and average incremental accuracy (AIA) [157, 178] are often used. Model size is used to measure the memory size for storing the model parameters [265], while sample storage size (SSS) is used to measure the storage size for storing data samples. To evaluate the computational resources required for the training and inference phases, metrics such as the number of floating-point operations-per-second (FLOPS) and time complexity are often used. Meanwhile, power/energy efficiency measures the efficiency gains of a CL system during its operational life-time."}, {"title": "3 Neuromorphic Computing", "content": "3.1 Overview of SNNS\nSpiking Neural Networks (SNNs) mimic the brains' functionality through the utilization of spikes for transmitting information [37, 266, 267]. Therefore, SNNs encode information into discrete spike trains. Popular encoding techniques include rate coding [268] (e.g., spike count, spike density, population activity [269]), and temporal coding (e.g., burst [270], time-to-first spike (TTFS) [271], phase [272], and rank-order [273]). Each spiking neuron processes the input spikes, and its behavior (i.e., neuronal dynamics) depends on the neuron model, such as Hodgkin-Huxley (HH) [274], Leaky Integrate and Fire (LIF) [275], Izhikevich Model [276], and Adaptive Exponential Integrate-and-Fire (AdExIF) [277]. Neuron model selection typically considers the expected neuronal dynamics and computational complexity [278, 279]. The output spikes are generated only when neurons' membrane potential reaches the threshold, and transmitted through synapses, enabling ultra-low power/energy consumption [52, 280-283].\nSNN training can be performed using bio-plausible or analytical learning rules. The bio-plausible ones are typically characterized by local learning mechanism, like Hebbian [284], Spike-Timing-Dependent Plasticity (STDP) [285], Spike-Driven Synaptic Plasticity (SDSP) [286, 287], and Reward-Modulated STDP (R-STDP) [288, 289]. Meanwhile, the analytical ones encompass DNN-to-SNN conversion [290], Surrogate Gradient Learning [291] (e.g., Backpropagation Through Time (BPTT) [291-293], Spatio-Temporal Backpropagation (STBP) [294], and Deep Continuous Local Learning (DECOLLE) [295]), Spike-Triggered Local Representation Alignment (ST-LRA) [296], and Bayesian Learning [297].\nSNN processing demands a suitable computing hardware to maximize its potentials in accuracy, latency, and power/energy efficiency. Conventional von-Neumann architecture-based hardware platforms (e.g., CPU and GPUs) have been widely used to perform Python-based SNN processing using generic arithmetic units [298-301], thereby leading to sub-optimal efficiency gains. To address this, CMOS-based neuromorphic hardware accelerators facilitate efficient spike transmission and computation [302], allowing their implementation in the Field-Programmable Gate Array (FPGA) or Application-Specific Integrated Circuit (ASIC). Popular accelerators include Neurogrid [303], ROLLS [304], TrueNorth [305], and Loihi [306]. Beyond CMOS-based technologies, the processing-in-memory (PIM) or compute-in-memory (CIM) paradigm has been explored to further reduce latency and energy in data transfer between memory and compute units [307, 308] by leveraging non-volatile memory (NVM) technologies, such as Resistive Random Access Memory (RRAM), Magnetic RAM (MRAM), and Phase Change Memory (PCM) [309]."}, {"title": "3.2 Neuromorphic Continual Learning (NCL)", "content": "NCL is an advancement of bio-plausible SNNs for facilitating continual adaptation in dynamic environments. Here, we discuss the state-of-the-art NCL methods, including their key ideas and limitations, whose details are summarized in Table 4.\n\n3.2.1 Enhancements on Unsupervised STDP Learning\nWhile conventional STDP enables efficient online unsupervised learning, employing STDP alone may still suffer from CF [41, 42]. To address these challenges, proposed techniques have been proposed in the literature, as discussed below.\n\u2022 Weight Decay: It prevents overfitting in NNs by penalizing large weight values by adding a regularization term, which encourages the model to keep the weights small. For instance, Adaptive Synaptic Plasticity (ASP) [97], SpikeDyn [41] and lpSpikeCon [42] leak the weights to gradually remove old and insignificant information and ensure the weights do not grow too large.\n\u2022 Adaptive Threshold Potential: It refers to the dynamic adjustment of the neurons' threshold based on its spiking activity. It allows neurons to adapt their sensitivity to incoming spikes, making the network more flexible/robust in processing information. A neuron typically fires when its membrane potential exceeds a pre-defined threshold. However, in an adaptive threshold model (as employed by SpikeDyn [41] and lpSpikeCon [42]), the threshold changes based on the neurons' recent activity, thus helping the neuron provide the following features.\n1. Regulating the firing rates to prevent excessive firing and maintain a stable firing rate over time to avoid overactive neurons (homeostasis) [266, 268].\n2. Leveraging the temporal information to improve the neurons' capability to respond to varying input patterns.\n3. Improving the signal-to-noise ratio (SNR), by separating the significant input patterns from the noise.\n\u2022 Adaptive Learning Rate: It dynamically adjusts the learning rate in training to improve convergence. The learning rate controls how much the models' weights are updated. ASP [97], SpikeDyn [41] and lpSpikeCon [42] employ adaptive learning rates to determine the potentiation and depression factors in the STDP-based learning based on the spiking activities. By prioritizing the adjustment of highly active synapses, it enhances learning efficiency and reduces unnecessary energy expenditure on less active connections. This selective learning process mimics biological synaptic behavior.\nThese techniques are often employed together in NCL methods as discussed below.\nAdaptive Synaptic Plasticity (ASP) [97]: It integrates weight decay with the STDP-based weight updates to balance forgetting and learning, while leveraging time-dependent learning rate. ASP uses a two-phase weight update process (i.e., recovery and decay), allowing weight updates based on spiking activities and gradually leak toward a baseline value; see Fig. 13. Despite its strengths, ASP requires larger quantities of input samples from earlier distributions than later ones, hence requiring the knowledge of task changes [96], making it unsuitable for OCL scenario."}, {"title": "3.2.2 Predictive Coding", "content": "This method is used by the Spiking Neural Coding Network (SpNCN) [296] to predict incoming data and then correct the prediction based on the actual inputs [310, 311]; see Fig. 17. This iterative process of \u201cguess-and-check\u201d allows the network to adjust its weights continually and learn from data streams without repeated exposure to same data. Key ideas of SpNCN are as follows.\n\u2022 Prediction of neuron activity and error correction using local synaptic updates.\n\u2022 Weight adaptation using a coordinated ST-LRA, which adjusts weights based on the mismatch between predictions and actual activity. It is also combined with STDP as regularizer.\n\u2022 A memory module and task/context-modulated lateral inhibition to enhance memory retention across tasks."}, {"title": "3.2.3 Active Dendrites", "content": "Studies in [312] proposed an SNN model leveraging active dendrites [313] to facilitate task-specific sub-network formation (Fig. 18). The spike time tj of a neuron is modulated by a function of the selected dendritic segment ujn for the current task. The dendritic activation function f(u) modulates the spike time dynamically, allowing the model to adapt its behavior based on the task context. It also leverages TTFS encoding to introduce a gating mechanism, that enables selection of sub-networks for various tasks. However, this method requires labeled data or supervisory signals, hence making it challenging for deployments under the OCL scenario."}, {"title": "3.2.4 Bayesian Continual Learning", "content": "This method represents weights with parameters that quantify the epistemic uncertainty based on prior knowledge and observed data, and employs Bayesian methods for handling uncertainty over time by determining which knowledge to retain and which to forget [297] (Fig. 19). For real-valued synapses, it uses a Gaussian variational distribution to adjust the values; while for binary synapses, it uses a Bernoulli variational distribution with Gumbel-softmax [314]. Although, this method provides better-calibrated decisions and better detection compared to conventional frequentist approaches [315], the uncertainty quantification incurs high computational complexity, thereby making it challenging for deployments in OCL scenario."}, {"title": "3.2.5 Architecture-based Approach", "content": "Dynamic Structure Development of Spiking Neural Networks (DSD-SNN) enhances the SNN structure by growing new neurons for new tasks and pruning redundant neurons [316] (Fig. 20). It employs a deep SNN architecture comprising of multiple convolutional and fully-connected (Fc) layers, which is equipped with random growth, adaptive pruning, and freezing of neuron mechanisms. This study advanced multi-task learning while enhancing memory capacity and efficiency. However, the development of dynamic structure adds complexity to SNN implementation, which is challenging for OCL scenario.\nSelf-Organized Regulation SNN (SOR-SNN) employs a pathway search module to adaptively activate task-specific sparse neural pathways based on fundamental weights Wt [317] (Fig. 21). Each synapse has two states (i.e., active and inactive), and is determined by learnable synaptic selection parameters. The model decides whether to activate or inhibit each weight by comparing the learnable parameter As with the threshold As, where activation is preferred if As > As. This study showed that SOR-SNN enables adaptive reorganization, BWT, and self-repair capabilities. Despite its benefits, this method requires complex dynamic connectivity capabilities, hence making it challenging for deployments under OCL scenario."}, {"title": "3.2.6 Rehearsal/Replay-based Approach", "content": "A memory replay approach using Experience Replay (ER) method has been developed, where a convolutional SNN is trained to learn in Class-IL and TA scenarios [318]. For resource-constrained devices, the memory-efficient Latent Replay (LR)-based method has been developed, which stores compressed data representations [292]. The Latent Replay training involves a pre-training phase, where SNN is divided into two parts i.e., frozen and learning layers (Fig. 22). The network stores latent replays for memory, and only trains the learning layers on new data. Here, the main challenges are related to the efficient store-load mechanisms for replay data under OCL scenario."}, {"title": "3.2.7 Regularization-based Approach", "content": "This approach employs regularization terms to balance the old and new tasks, such as Noise Regularization [320], Freezing Large Weights, and Stochastic Langevin Dynamics [289]. For instance, Langevin Dynamics exploit the fact that each weight w\u2081 can vary without impacting the accuracy in the domain Di if constrained to a specific space when learning new tasks; see Fig. 23. This study highlighted that Langevin dynamics can exhibit very similar performance as the replay-based methods while being less memory-intensive. However, its compute requirements are relatively high, thus requiring further studies for OCL scenario."}, {"title": "3.2.8 Hebbian Learning", "content": "Hebbian Learning-based Orthogonal Projection (HLOP) method leverages lateral connections and Hebbian learning to achieve CL [319]. It employs orthogonal gradient projection to modify pre-synaptic activity traces (Fig. 24) to ensure that weight updates for new tasks do not interfere with the weights associated with the old tasks. Hebbian learning is useful to extract the principal subspace of neural activities, and updates synaptic weights based on the correlation between pre- and post-synaptic neuron activities, while the activity traces are updated using lateral signals."}, {"title": "3.3 Efficiency Enhancement Methods for NCL", "content": "To reduce the memory footprint and energy consumption of SNNs with CL capabilities (NCL)", "by": "n\u2022 Leveraging sparse neurons and sparse synapses to reduce the number of neuron operations and connections/weights, respectively.\n\u2022 Utilizing temporal encoding uses timing of spikes to encode information in a way that can reduce computational complexity compared to rate encoding.\n\u2022 Employing simple neuron models like LIF instead of complex models to reduce the computational complexity required per neuron.\nSpikeDyn [41", "97": "which consisted of input, excitatory, and inhibitory layers (Fig. 25), thereby eliminating the operations in the inhibitory layer.\n\n3.3.2 Weight Quantization\nThis method enhances SNNs efficiency by representing network weights with lower precision instead of using full-precision floating-point numbers [281-283, 321-324", "325": ".", "42": "reduced SNN weights using uniform quantization"}]}