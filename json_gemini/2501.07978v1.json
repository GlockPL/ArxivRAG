{"title": "Facial Dynamics in Video: Instruction Tuning for Improved Facial Expression\nPerception and Contextual Awareness", "authors": ["Jiaxing Zhao", "Boyuan Sun", "Xiang Chen", "Xihan Wei"], "abstract": "Facial expression captioning has found widespread ap-\nplication across various domains. Recently, the emergence\nof video Multimodal Large Language Models (MLLMs) has\nshown promise in general video understanding tasks. How-\never, describing facial expressions within videos poses two\nmajor challenges for these models: (1) the lack of adequate\ndatasets and benchmarks, and (2) the limited visual token\ncapacity of video MLLMs. To address these issues, this\npaper introduces a new instruction-following dataset tai-\nlored for dynamic facial expression caption. The dataset\ncomprises 5,033 high-quality video clips annotated man-\nually, containing over 700,000 tokens. Its purpose is to\nimprove the capability of video MLLMs to discern subtle\nfacial nuances. Furthermore, we propose FaceTrack-MM,\nwhich leverages a limited number of tokens to encode the\nmain character's face. This model demonstrates superior\nperformance in tracking faces and focusing on the facial\nexpressions of the main characters, even in intricate multi-\nperson scenarios. Additionally, we introduce a novel evalu-\nation metric combining event extraction, relation classifica-\ntion, and the longest common subsequence (LCS) algorithm\nto assess the content consistency and temporal sequence\nconsistency of generated text. Moreover, we present FEC-\nBench, a benchmark designed to assess the performance of\nexisting video MLLMs in this specific task. All data and\nsource code will be made publicly available.", "sections": [{"title": "1. Introduction", "content": "Facial expressions play a crucial role in daily communica-\ntion and are a vital component of human interaction. Their\nsignificance has led to growing interest from researchers\nin fields such as human-computer interaction (HCI) [74],\ndriving assistance [68], and mental health [55]. As re-"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Video Caption Dataset", "content": "Existing video captioning datasets can be categorized\ninto three groups based on video duration. Short video\ndatasets [6, 65, 77, 80] typically contain video clips ranging\nfrom 5 to 30 seconds in length. Longer video datasets [21,\n28, 54, 96] include videos that vary in length from 1 to 5\nminutes. Very long video datasets [17, 22, 95] consist of\nvideos that can last for hours. However, these datasets often\nemphasize overall content and action recognition in videos.\nSpecifically, such descriptions typically include specific ac-\ntions being performed (for example, \u201cA person is drinking\ncoffee.\"), the presence of certain objects in the scene (for ex-\nample, \"There is a book on the table.\"), or the development\nof a sequence of events over time (for example, \"The video\nbegins with a man watching TV, then he stands up, walks to\na table, and engages in conversation with a woman\"). De-\nspite the growing importance of facial expressions in down-\nstream tasks such as generative models [13, 38] and digital\nhuman representations [62, 73], there is currently a lack of\ndatasets that specifically focus on detailed descriptions of\nhuman facial expressions. Existing datasets often empha-\nsize overall content and action recognition in videos, rather\nthan the details of facial expressions. To address this gap,\nwe introduce a dynamic facial expression caption dataset\ncontaining 5,033 high-quality video clips, annotated man-\nually with over 50,000 facial expression words and over\n700,000 tokens. We have built a comprehensive benchmark\nbased on this dataset, laying the foundation for further ex-\nploration in this direction."}, {"title": "2.2. Multimodal Large Language Models", "content": "Existing large models can be categorized into two primary\ntypes: proprietary large models and open large models. Pro-\nprietary large models are typically developed by private or-\nganizations or companies, with restricted access and usage\nrights, often protected by intellectual property laws. In con-\ntrast, open large models are designed to be accessible to the"}, {"title": "3. Dataset", "content": "In this paper, we construct a Dynamic Facial Expres-\nsion Caption dataset, FDA, to bridge the gap between the\nbroader video understanding research community, which\nhas traditionally focused on narrative progression and main\ncontent, and the downstream applications that greatly ben-\nefit from detailed captions of facial changes in videos. In\nthis section, we first introduce the sources of the data in our"}, {"title": "3.1. Data Sources", "content": "Our data sources are primarily divided into two parts. The\nfirst part is extracted from existing datasets, specifically\nfrom video datasets related to emotions, to ensure a rich\nvariety of facial expressions. We refer to this as the exist-\ning dataset. The second part consists of self-collected data,\nwhich we obtained through web scraping. We refer to this\nas the collected dataset.\nSpecially, to ensure diversity in the dataset, we sam-\npled 500 videos each from existing datasets such as\nMAFW [46], DFEW [24], AFEW [27], MER [19],\nCAER [29], FERV39K [78], and RAVDESS [48]. Fur-\nthermore, during the dataset sampling process, we perform\nuniform sampling based on emotion categories to maintain\nan approximately equal distribution of videos across differ-\nent emotions. Additionally, we collected 500 videos from\nPexels Videos using keywords such as \u201cman\u201d, \u201cgirl\u201d, \u201cfam-\nily\", and \"woman\u201d, and 500 videos from Cesituwang. Each\nvideo is no longer than 20 seconds. We processed these\nsampled videos by removing the header and footer infor-\nmation and any irrelevant content. Furthermore, we ran-\ndomly extracted video clips from a large number of famous\nmovies. For each clip, we performed face tracking to ensure\nthat at least one face was present and that the face region\noccupied more than 5 percent of the frame. We discarded\nany clips that did not meet these criteria. Ultimately, we\nextracted 1,000 clips that satisfied the requirements. Sub-\nsequently, we conducted a manual screening process to re-\nmove videos that contained too many people or static scenes\nwith no movement. We present the statistical data on the\nsources in Figure 2a, including the proportions of data ex-\ntracted from each existing dataset and the proportions from\neach self-collected dataset."}, {"title": "3.2. Annotation Process", "content": "Our annotation process is primarily divided into two parts:\nthe first part is the generation of preliminary annotations,\nand the second part is manual correction. We utilized care-"}, {"title": "3.3. Dataset Attributes", "content": "As mentioned earlier, our data can be categorized into two\ntypes based on the source: existing data and self-collected\ndata. Here, we analyze the properties of these two types of\ndata. Comparative analyses with other datasets are provided\nin the supplementary materials. In Figure 2b, we present the\nstatistics of the number of annotated tokens in the dataset.\nIt can be observed that the average number of tokens in the\nexisting data is 134.7, while the average number of tokens\nin the self-collected data is 125.3.\nIn Figure 2c, we present the statistics of the average du-\nration of videos from the two sources. The average duration\nof videos from the existing data is 3.13 seconds, while the\naverage duration of videos from the self-collected data is\n4.22 seconds. This video length is efficient for capturing\nsubtle facial changes, reducing the computational burden of\ndata processing, facilitating annotation and model training,\nand making it suitable for real-time applications and emo-\ntion recognition.\nWe used ChatGPT [58] to extract facial action-related\ndescriptors from the annotations, such as \"tilted head\","}, {"title": "3.4. Temporal Event Matching", "content": "Traditional evaluation metrics based on n-grams, such as\nCIDEr [72], struggle when assessing long video descrip-\ntions. This is because there are numerous ways to con-\nvey the same meaning in extended text, and many of these\nequivalent descriptions may have minimal n-gram overlap\nwith the reference text. Moreover, manually assigning qual-\nity scores to descriptions is both labor-intensive and subjec-\ntive. To address these issues, Maaz et al. [53] propose using\nChatGPT [58] to rate descriptions on a scale from 1 to 5.\nHowever, the specific meanings of these ratings are ambigu-\nous, and the ratings themselves have not been standardized.\nRecently, Tarsier [75] introduced an evaluation method\ncalled AutoDQ, which extracts text descriptions into a set of\nevents and then calculates precision and recall by evaluating\nthe relationships between the generated text and the refer-\nence event set, and vice versa, ultimately deriving the F-\nmeasure. While AutoDQ makes significant progress in cap-\nturing the semantic consistency and completeness of events,\nit does not account for the order of events. In many appli-\ncations, especially those involving temporal and causal re-\nlationships, the order of events is crucial. Incorrect event\nordering can lead to logically incoherent text, affecting the\noverall quality. To overcome these limitations, we propose\na new evaluation metric named Temporal Event Matching\n(TEM) that combines event extraction, relation classifica-\ntion, and the longest common subsequence (LCS) algorithm\nto assess the semantic consistency and event ordering of\ngenerated text. Our method calculates the LCS score, nor-\nmalizes it to a range of 0 to 1, and then averages it with the\nF-measure [75] of generated events and reference events to\nprovide a comprehensive evaluation result. By integrating\nthese components, TEM ensures that the generated text not\nonly captures the correct events and their relationships but\nalso maintains the correct order and coherence, providing a\nmore holistic assessment standard for long video captions.\nAs shown in Algorithm 1, our evaluation metric calcula-\ntion is structured into five key steps. In the first step, draw-\ning on the methodology of AutoDQ, we utilize the Chat-\nGPT model to extract a set of events, denoted as EG from\nthe generated text and ER from the reference text, respec-\ntively. This initial extraction process aims to identify and\nisolate the critical events within both texts, setting the stage\nfor subsequent comparison and assessment.\nIn the second step, we leverage ChatGPT once more to\nevaluate the relationship between each event in Eg and each\nevent in ER. These relationships are categorized into three\nprimary types: Same Meaning, Opposite Meaning, and No\nRelation. By classifying these relationships, we can achieve\na more precise understanding of the semantic alignment be-"}, {"title": "Algorithm 1 Temporal Event Matching", "content": "Require: Generated Text G, Reference Text R\nEnsure: Evaluation Score S\n1: Event Extraction: Extract events from G and R to get\nEG and ER\n2: Relation Classification: Classify relations in EG and\nER\n3: function LCS-SCORE(EG, ER)\n4: Initialization: Initialize m \u2190 |EG|, n \u2190 |ER|\n5: Array Creation: Create a 2D array L of size (m +\n1) \u00d7 (n + 1)\n6: for i 0 to m do\n7: Column Initialization: L[i][0] \u2190 0\n8: end for\n9: for j 0 to n do\n10: Row Initialization: L[0][j] \u2190 0\n11: end for\n12: for i 1 to m do\n13: for j 1 to n do\n14: if EG[i - 1] == ER[j - 1] then\n15: Match Found: L[i][j] \u2190 L[i - 1][j\n- 1] + 1\n16: else\n17: No Match: L[i][j] \u2190 max(L[i -\n1][j], L[i][j - 1])\n18: end if\n19: end for\n20: end for\n21: Return Normalized LCS Score: return $\\frac{L[m][n]}{m}$\n22: end function\n23: Calculate LCS Score: lcs \u2190 LCS-SCORE(EG, ER)\n24: F-Measure Calculation: Calculate precision, recall,\nand F-measure using EG and ER\n25: Comprehensive Evaluation Score: S \u2190 $\\frac{lcs+F-measure}{2}$\n26: Return Final Score: return S\ntween the generated and reference texts."}, {"title": "4. Method", "content": "In this section, we first introduce the motivation behind\nour work. We then describe the architecture of our model\nand finally detail the training process and implementation\nspecifics."}, {"title": "4.1. Motivation", "content": "Since existing video large language models primarily focus\non understanding the main content and narrative progres-\nsion of videos, their ability to encode detailed video infor-\nmation is limited. Directly fine-tuning these models is in-\nsufficient for adequately modeling facial regions in videos.\nIn [36], Li et al. designed a facial prior expert to provide\nfacial feature encoding for large language models. Inspired\nby this work, we propose a method that accurately models\nfacial regions in videos, resulting in precise facial descrip-\ntion results."}, {"title": "4.2. Architecture", "content": ""}, {"title": "4.2.1 Overall", "content": "As shown in Figure 1, our model primarily consists of a pre-\ntrained visual encoder, a visual projector, a dynamic face\nextraction module, an auxiliary facial visual encoder, and\na large language decoder. Specifically, during the training\nprocess, for the input video, we first use a pre-trained im-\nage encoder to extract features from the selected frames.\nThen, we use the STC [11] module to map these features\nto the text domain. Additionally, for the input video, we\nuse the dynamic face extraction module to extract facial\ninformation of the main characters. These facial features\nare subsequently processed by an auxiliary facial visual en-\ncoder to extract visual features. These visual features are\nalso mapped to the text domain. Finally, the visual features,\nprior features, and text features are combined and fed into\nthe large-scale language model for text generation.\nThe pre-trained visual encoder, visual projector, and\nlarge-scale language model all follow the design of Vide-\noLLaMA2 [11]. The visual encoder uses CLIP-ViT-Large\n[64], the visual projector is the STC module, and the large-\nscale language model employs Mistral-7B-Instruct [23].\nWe use the FaceXFormer [56] architecture, designed for\nface analysis, as the auxiliary facial visual encoder."}, {"title": "4.2.2 Dynamic Video Face Tracking Module", "content": "Compared to directly using a face feature extractor to ex-\ntract features from each frame, which may result in multiple\nfaces being detected per frame and faces between frames\nnot matching one-to-one, our dynamic video face track-\ning module incorporates Face Detection and Multi-Object\nTracking techniques. This ensures that the facial features of\nthe main characters are consistently and accurately tracked\nacross the video. The module can be broken down into the\nfollowing steps:\nVideo Frame Downsampling and Face Feature Ex-\ntraction. Downsample the video to a uniform frame rate\nof 16 fps. This step helps reduce computational load\nwhile retaining sufficient spatiotemporal information. Sub-\nsequently, perform face keypoint detection DaMOFD [47]\nand face feature extraction TransFace [12] on each down-\nsampled video frame.\nMulti-Object Tracking. We applied the StrongSORT\n[15] multi-object tracking algorithm to extract target trajec-\ntories. This algorithm provides robust trajectory informa-\ntion, including the position, size, and motion path of each\ntarget\nFace Trajectory Feature Extraction. For each gener-\nated trajectory, we first calculate the total area occupied by\nthe trajectory in the video. The total area is related to the\ntemporal proportion of the face trajectory in the video and\nthe spatial proportion in each frame. We then compare the\ndistance matrix of the face feature vectors within the tra-\njectory to obtain the average cosine similarity within the\ntrajectory.\nMain Trajectory Selection. For each trajectory, we use\nthe total area and the average cosine similarity as its fea-\ntures and perform K-means clustering with 2 cluster centers,\nrepresenting the main character cluster and the background\ncharacter cluster, respectively.\nThrough this process, we can obtain one or multiple fa-\ncial trajectories. To ensure that the extracted video frames\ncontain the faces of the main characters while maintaining\nas even a distribution as possible, during the frame extrac-\ntion process, if a frame does not contain the face of a main\ncharacter, we replace it with the nearest frame that does con-\ntain the face of a main character."}, {"title": "4.3. Instruction Tuning", "content": "Since our data is entirely manually annotated, it is of high\nquality but relatively limited in quantity. We fine-tuned the\nmodel using LORA [20] on the VideoLLaMA2-7B [11] base\nmodel. We generated 100 similar instructions using Chat-\nGPT and manually selected 20 of them, some of which are\nshown in the supplementary material. For each annotated\nvideo, we randomly selected one of the 20 instructions to\nform the instruction data."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Implementation Details", "content": "Our code is based on VideoLLaMA2 [11]. The LoRA [20]\nparameters we used are set as follows: lora_r is set to 64,\nlora_alpha is set to 128. The batch size is set to 32, and"}, {"title": "5.2. FEC-Bench", "content": "We randomly selected 1,000 samples from our constructed\ndataset to form the test set. In addition to the metrics we\nproposed, we employed a variety of evaluation metrics,\nincluding four ChatGPT-related metrics [53], the n-gram\nbased metric CIDEr [72], and AutoDQ [75], to comprehen-\nsively assess the performance of a large number of video\nMLLMs, including proprietary models. Due to the lack of\ninstruction tuning specifically on our dataset, the generated\ntext does not always match the format of the reference text.\nThis discrepancy leads to biased results when using Chat-\nGPT for evaluation. Therefore, before evaluating the gen-\nerated text using ChatGPT, we first use ChatGPT to convert\nthe generated text into a similar format as the reference text\nwithout altering its content. The specific prompts used and\nthe effects before and after conversion are detailed in the\nsupplementary materials. This extensive benchmarking not\nonly provides a robust comparison framework but also aids\nin identifying the strengths and weaknesses of each model."}, {"title": "5.3. Ablation Study", "content": "In Table 2, we present the methods and their effects by grad-\nually increasing the utilization of facial information. Our\nbaseline is VideoLLaMA2. First, we performed instruction\ntuning directly on our training set using VideoLLaMA2, ob-\nserving a significant improvement with around a 7 percent\nincrease in two event-based evaluation metrics. Next, we\nused RetinaFace to extract and concatenate facial regions,\nrandomly selecting 2 faces if more were detected per frame.\nThese concatenated images were fed into a CLIP model\nfor feature extraction, but this approach showed almost no\nimprovement. Then, we replaced RetinaFace with our dy-\nnamic face tracking module, which significantly improved\nall metrics. Finally, we used a specialized encoder to extract\nfacial features, further enhancing the results."}, {"title": "5.4. Qualitative Analysis", "content": "Here, we present the visualization results of some open-\nsource community models and our proposed model. As\nshown in Fig. 4, it is evident that due to data and model\nlimitations, existing models lack the capability to provide\ndetailed, accurate, and hallucination-free descriptions of fa-\ncial expressions in videos. Specifically, Qwen2VL [76] not\nonly misjudges emotions but also exhibits certain hallucina-\ntions, while LLaVA-OneVision [31] can only provide sim-\nple descriptions and overlooks changes in expressions. In\ncontrast, our method can accurately describe the facial ex-\npression states and changes of the main characters in the\nvideo. It identifies the dynamics of emotions and facial ex-\npressions, showcasing its proficiency in retrieving and in-"}, {"title": "6. Conclusion", "content": "In this paper, we tackle the challenges of dynamic facial\nexpression captioning using video Multimodal Large Lan-\nguage Models (MLLMs). We introduce a new instruction-following dataset containing 5,033 high-quality video clips\nwith over 700,000 tokens, designed to enhance MLLMs'\nability to capture subtle facial differences. We also pro-\npose FaceTrack-MM, a model that efficiently tracks and fo-\ncuses on the main character's facial expressions in complex\nscenes using a limited number of tokens. Additionally, we\ndevelop a novel evaluation metric that combines event ex-\ntraction, relation classification, and the Longest Common\nSubsequence (LCS) algorithm to assess the quality of gen-\nerated captions. Finally, we present FEC-Bench, a bench-\nmark for evaluating MLLMs performance in facial expres-\nsion captioning task. Our results demonstrate significant\nimprovements in capturing and describing dynamic facial\nexpressions, contributing to the advancement of this field\nand providing valuable resources for future research."}]}