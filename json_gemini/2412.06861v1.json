{"title": "Mining Limited Data Sufficiently: A BERT-inspired Approach for CSI Time Series Application in Wireless Communication and Sensing", "authors": ["Zijian Zhao", "Fanyi Meng", "Hang Li", "Xiaoyang Li", "Guangxu Zhu"], "abstract": "Channel State Information (CSI) is the cornerstone in both wireless communication and sensing systems. In wireless communication systems, CSI provides essential insights into channel conditions, enabling system optimizations like channel compensation and dynamic resource allocation. However, the high computational complexity of CSI estimation algorithms necessitates the development of fast deep learning methods for CSI prediction. In wireless sensing systems, CSI can be leveraged to infer environmental changes, facilitating various functions, including gesture recognition and people identification. Deep learning methods have demonstrated significant advantages over model-based approaches in these fine-grained CSI classification tasks, particularly when classes vary across different scenarios. However, a major challenge in training deep learning networks for wireless systems is the limited availability of data, further complicated by the diverse formats of many public datasets, which hinder integration. Additionally, collecting CSI data can be resource-intensive, requiring considerable time and manpower. To address these challenges, we propose CSI-BERT2 for CSI prediction and classification tasks, effectively utilizing limited data through a pre-training and fine-tuning approach. Building on CSI-BERT1, we enhance the model architecture by introducing an Adaptive Re-Weighting Layer (ARL) and a Multi-Layer Perceptron (MLP) to better capture sub-carrier and timestamp information, effectively addressing the permutation-invariance problem. Furthermore, we propose a Mask Prediction Model (MPM) fine-tuning method to improve the model's adaptability for CSI prediction tasks. Experimental results demonstrate that CSI-BERT2 achieves state-of-the-art performance across all tasks with relatively fast computation speeds. To facilitate future research, we will make our code and dataset publicly available upon publication.", "sections": [{"title": "I. INTRODUCTION", "content": "Channel State Information (CSI) plays a critical role in both wireless sensing and wireless communication systems by capturing the propagation characteristics of the wireless channel. This information provides valuable insights into the radio environment, including the channel conditions, signal quality, and the motion and location of objects. In wireless communication systems, CSI is essential for optimizing various processes such as channel compensation, adaptive modulation and coding, user selection and scheduling. By effectively utilizing CSI, these systems can achieve better performance, increased spectrum efficiency, and more reliable communication, ultimately enhancing the overall user experience [1]\u2013[3]. In wireless sensing applications, CSI can enhance the ability to detect and track objects, leading to improved environmental awareness. With the advantage of privacy protection, low cost, and penetration ability, wireless sensing has been widely used in many areas like fall detection [4], localization [5], and people identification [6]. Despite the importance of CSI in wireless systems, it is challenging to acquire, process, and utilize CSI data. For example, many deep learning methods in wireless systems require substantial data for training, but the data available in wireless communication is often limited, and public datasets may have different formats, making it challenging to use them together [6]. Additionally, collecting CSI datasets on one's own can be a difficult task. A common solution for data scarcity is pre-training, which has been widely utilized in fields such as Natural Language Processing (NLP) [7], [8], Computer Vision (CV) [9], [10], and Music Information Retrieval (MIR) [11], [12]. The pre-training process typically employs a self-supervised approach, enabling models to learn the underlying structure and patterns of the data. This can enhance the model's performance on downstream tasks, as well as improve its convergence speed and generalization capacity, particularly in scenarios with limited training data [13], [14]. Pre-training not only allows models to learn and understand the data more effectively, but it also enables the use of additional unlabeled data for training. Currently, pre-training is not exclusively applied to address data scarcity. In fields such as NLP and CV, as datasets continue to grow, pre-training has emerged as a powerful strategy to bolster model generalization capacity [15], [16]. Moreover, even when the inference scenario differs from the training scenario, models can quickly adapt to new contexts through transfer learning methods [17], [18], such as fine-tuning. Even though, pre-training and fine-tuning models on different datasets is uncommon in certain specialized fields, such as MIR and signal processing, which often face challenges due to limited data. On one hand, some tasks lack additional data"}, {"title": "II. PRELIMINARY", "content": "Following [27], in Wi-Fi communication, CSI characterizes the way Wi-Fi signals propagate from the transmitter (TX) to the receiver (RX). The amplitude and phase of CSI are influenced by multipath effects, which include amplitude attenuation and phase shift. Each element of the CSI Matrix H can be given by:\n\n$H(f;t) = \\sum_{n=1}^{N} a_n(t)e^{-j2\\pi f \\tau_n(t)}$, (1)\n\nwhere N is the number of paths between the TX and the RX, $a_n(t)$ is the amplitude attenuation factor, $\\tau_n(t)$ is the propagation delay, f is the carrier frequency, and t is the timestamp.\n\nIn wireless communication, obtaining the CSI matrix is a challenging task, especially in future mmWave scenarios [28]. Considering the large number of antennas, the dimension of the CSI matrix will also be substantial, significantly increasing the complexity of the channel estimation algorithms. On the other hand, when objects in the environment are moving at high speeds, the channel coherence time will be greatly"}, {"title": "III. METHODOLOGY", "content": "The proposed CSI-BERT2 model is a BERT-based approach designed for CSI prediction, and classification, as illustrated in Fig. 1. The workflow of our model includes three phases: pre-training, fine-tuning, and inferring, as shown in Fig. 3. During pre-training, the CSI sequence is randomly destructed, and the model is trained to recover the missing components in a self-supervised manner, to make the model learn the CSI pattern and inner relationship. After that, the model can be fine-tuned for prediction tasks or classification tasks. Finally, the trained model can be used to infer for specific downstream tasks. In this section, we leverage the CSI sequence c = $[C_1, C_2,..., C_n]$ and the corresponding timestamps t = $[t_1, t_2,..., t_n]$ as input. Each ci represents a flattened vector of the CSI matrix at time ti, which contains the amplitude and phase information of subcarriers.\n\nDue to package loss, the received CSI series are always incomplete. Before inputting it to our model, we need to first identify where the package loss happens and use a placeholder [PAD] to occupy the place of lost packages. We judge whether package loss happens according to the gap between two successive timestamps. Given a sampling rate f, the ideal time gap between two successive timestamps should be $At = \\frac{1}{f}$ seconds. Assume we receive a CSI series $[C_1,C_2,...,C_m]$ with corresponding timestamps $[t_1, t_2,...,t_m]$ with the unit of seconds. Then we can identify whether there is package loss and how many packages were lost between $c_i$ and $c_{i+1}$. We calculate the lost package amount according to:\n\n$k = round(\\frac{t_{i+1} - t_i}{At})$, (2)\n\nThen we will add k [PAD] tokens between $c_i$ and $c_{i+1}$. Besides, we also need to add k corresponding timestamps between $t_i$ and $t_{i+1}$. We define the jth added timestamp between $t_i$ and $t_{i+1}$ as:\n\n$t_{i,j} = t_i + \\frac{t_{i+1}-t_i}{k} j + e_j$, (3)\n\nwhere $e_j$ is a small noise. Our method to identify package loss is reasonable because after adding the [PAD] tokens, we found the package amount is very close to the theoretical amount when there is no package loss in our dataset.\n\nThe model structure of two generations of CSI-BERT is shown in Fig. 1. We utilize BERT [20] (the encoder part of a Transformer model [38]) as the backbone, but replace the bottom embedding layer and the top heads to adapt our data format and tasks.\n\nCompared to CSI-BERT1 [24], the main change is in the embedding layer. Our model's embedding layer consists of three parts: token embedding, time embedding, and position embedding.\n\nIn the token embedding, we first use a standardization layer along the time dimension, as shown in Eq. 4:\n\n$\\mu^{(j)} = \\frac{\\sum_{i=1}^{n} c_i^{(j)}}{n}$,\n\n$\\sigma^{(j)} = \\sqrt{\\frac{\\sum_{i=1}^{n}(c_i^{(j)} - \\mu^{(j)})^2}{n}}$,\n\n$Standard(c_i^{(j)}) = \\frac{c_i^{(j)} - \\mu^{(j)}}{\\sigma^{(j)}}$, (4)\n\nwhere $c_i^{(j)}$ represents the jth dimension of $c_i$, $\\mu$ and $\\sigma$ represent the mean and standard deviation, respectively. We noticed that the distribution of CSI in each subcarrier changes significantly over time, so the standardization operation can help the model learn features better and mitigate the covariate\n\n$T = MLP(c)c$, (5)\n\nwhere c is input, T is output token, and MLP represents a Multilayer Perceptron. ARL is a mechanism similar to attention but simpler and more efficient. It first uses an MLP to generate a weight vector and then multiplies it with the original input. This process ensures that the network assigns higher significance to subcarriers with more informative content and ignores those without much variation, such as guard carriers. Thereby, it can help enhance the model's precision and effectiveness. After that, another MLP will be used in the dimension of subcarrier to extract feature further.\n\nFor the time embedding layer, which accounts for the varying time intervals between consecutive CSI samples, we adopt a similar strategy as CSI-BERT1 but with an extra MLP to further embed the timestamp, as shown in Eq. 6:\n\n$TE(t_i)^{(j)} = MLP(\\begin{cases}sin(Norm(t_i)), j=2k \\ cos(Norm(t_i)), j=2k+1\\end{cases})$, (6)\n\n$Norm(t_i) = \\frac{t_i - min(t)}{max(t) - min(t)}$,\n\nwhere j represents the $j^{th}$ dimension of TE($t_i$), k is a positive integer, L is the maximum length of CSI sequence, and d is the dimension of each CSI vector $c_i$. We first normalize the timestamp and then use the positional embedding method to encode it. However, due to the permutation-invariant nature of the Transformer architecture, it can only capture distance information but not order information. Studies [25] have also shown that Transformer-based models do not perform very well on time series data. To solve this problem, we use an MLP to capture time information further. Then, the time embedding is added to the token embedding and fused together using Layer Normalization.\n\nFinally, the position embedding layer is the same as in the traditional Transformer model, and the position embedding result is added to the fusion of token embedding and time embedding. It is used to represent the relative position, while the time embedding is used to represent the absolute position.\n\nSimilar to most BERT-based models, our CSI-BERT also has two branches for different tasks. The token-level head is used for CSI recovery and prediction tasks, while the sequence-level head is used for CSI classification tasks.\n\nFor the token-level head, a de-standardization layer is used to ensure that the output has a similar distribution as the input, as shown in Eq. 7:\n\n$De-Standard(y_i^{(j)}) = (y_i^{(j)} + \\mu^{(j)}) * \\sigma^{(j)}$, (7)\n\n$\\[MASK]^{(j)} \\sim N(\\mu^{(j)}, \\sigma^{(j)})$, (8)\n\n$min_R max_D V(D, R) = min_R max_D E_{c}[log(D(c))] + E_{c}[log(1 \u2013 D(R(c)))]$, (9)\n\n$L_1 = MSE(c, \\hat{c})$, $L_2 = MSE(\\mu, \\hat{\\mu})$,$L_3 = MSE(\\sigma, \\hat{\\sigma})$,$L_4 = CrossEntropy(D(\\hat{c}), 1)$, (10)\n\n$L_{dis} = CrossEntropy(D(\\hat{c}), 0) + CrossEntropy(D(c),1)$, (11)\n\nUnlike CSI-BERT1 that divides the CSI by a sliding window before training, we randomly select a CSI sequence from the whole dataset during training. This is an approach to mitigate the risk of over-fitting, as the fixed data division may make the model dependent on the absolute position. However, the absolute position actually only depends on the data division.\n\nEncoder-based methods like BERT [20] are often considered not suitable enough for generation or prediction tasks due to their bi-directional attention mechanisms. This is also influenced by their pre-training task, where MLM is used to train the model to recover the inner data, not the future data. However, decoder-based methods like GPT [8] and encoder-decoder based methods like BART [7] also suffer from the problem of error accumulation [25]. Since they generate data in an autoregressive manner, an error may also influence the further generation results. In contrast, encoder-based methods have the advantage that they can generate all data in one step.\n\nTo make CSI-BERT2 more suitable for the prediction task, we design a Mask Prediction Model (MPM). MPM has a similar format as MLM, but it only masks tokens at the end of the sequence instead of masking tokens in arbitrary random positions. Specifically, we randomly mask 15% to 40% of the tokens at the end of the sequence. Except for this, the other training strategy is exactly the same as the pre-training.\n\nFor the CSI classification task, we adopt the approach commonly used for pre-trained models, where we freeze the bottom layers of CSI-BERT2 and only train the top layers for specific classification tasks. Different from CSI-BERT1, in this phase, we use [MASK] tokens to replace not only all [PAD] tokens but also some other input tokens randomly, to imitate the situation of different packet loss rates. By this way, our model has better generalization capacity, not limited to the scenarios in the training set.\n\nIn the inferring phase of the prediction task, we first use [MASK] to replace all [PAD] tokens, and then we fix the last 20% of tokens in the input"}, {"title": "IV. EXPERIMENT", "content": "We illustrate our model configurations in TableI. We trained our model on an NVIDIA RTX 3090. During training, we observed that CSI-BERT occupied approximately 2500MB of GPU memory.\n\nIn our experiment, we utilize three datasets: the publicly available WiGesture dataset [24], the WiFall Dataset [26], and a novel dataset named WiCount, which is proposed along with this paper. All these datasets are collected using the ESP32-S3 device as RX and a home router as TX, and share the same data format, with a sample rate of 100Hz, 1 antenna, and 52 subcarriers. For our experiment, we divide the data into 1-second samples, resulting in each sample having a length of 100 data points. We take the first 90% of the samples as the training set and the last 10% as the testing set within each category.\n\nWe demonstrate that CSI-BERT2 effectively learns the CSI pattern, we compare the recovery performance of our method against that of CSI-BERT1 and various traditional interpolation methods. Specifically, we assess the recovery performance by randomly deleting 15% of the packets and comparing the model's recovery results with the ground truth. We employ Mean Squared Error (MSE), Symmetric Mean Absolute Percentage Error (SMAPE), and Mean Absolute Percentage Error (MAPE) to quantify the error, as represented in Eq. 12:\n\n$MSE(\\hat{c_i}, c_i) = \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{c_i}-c_i)^2$,\n\n$SMAPE(\\hat{c_i}, c_i) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{2 |c_i - \\hat{c_i}|}{(|c_i| + |\\hat{c_i}| + \\epsilon)}$,\n\n$MAPE(\\hat{c_i}, c_i) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{|c_i - \\hat{c_i}|}{(|c_i| + \\epsilon)}$, (12)\n\nwhere ci and $\\hat{c_i}$ represent ground truth and model output respectively, n represents the number of samples, $\\epsilon$ is a small number used to avoid division by zero. The metrics are calculated only on the 15% of the deleted CSI. Furthermore, to ensure fairness, we compute the error solely on the testing set, rather than on the entire dataset as done in CSI-BERT1. We also compare the time required to recover the full dataset across different models, using only the CPU for this comparison. Our results indicate that CSI-BERT2 achieves relatively low recovery error and recovery time across most datasets.\n\nFollowing the methodology of CSI-BERT1 [24], we provide two methods for CSI recovery, named \"recover\" and \"replace\":\n\n$CSI^{replace} = \\hat{c} $, $CSI^{recover} = (1 \u2013 IsPad) \u00b7 c +IsPad\u00b7\\hat{c}$, (13)\n\n$IsPad = (c == [PAD])$,\n\nwhere IsPad indicates whether the position of the original CSI c corresponds to the [PAD] token.\n\nTo illustrate the help of our pre-training mechanism for downstream tasks, we compare the model's performance with and without pre-training, as shown in Fig. 7 and Table VI. In the CSI classification task, the improvement from pre-training is modest but not particularly significant. However, in the CSI prediction task, pre-training proves to be highly beneficial, likely because the prediction task is more closely related to the mask recovery task.\n\nTo demonstrate the effectiveness of the modifications we made to BERT [20], we first compare the recovery performance of the original BERT (where we simply use a linear layer as the embedding and output layer to support the CSI format) with our modified version, as shown in Fig. 8. We observe that the amplitude spectrum of the original BERT appears smooth across all subcarriers, indicating that it tends to map all tokens within each subcarrier to similar value. Although BERT can also achieve a relatively low MSE of 5.26 in WiGesture dataset, it fails to capture any valuable information."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce CSI-BERT2, a multifunctional model designed for CSI time series, encompassing CSI prediction, and classification. Through the proposed pre-training and fine-tuning methods, CSI-BERT2 effectively extracts valuable insights from limited CSI data. We have developed a novel time embedding mechanism to enhance the suitability of Transformer-based models for time series analysis. Additionally, we employed an ARL model to improve the model's ability to capture relationships between different subcarriers. We also propose the MPM task, which fine-tunes BERT-based models specifically for prediction tasks. Furthermore, we have optimized the training process of CSI-BERT1, resulting in a significant enhancement in the model's performance.\n\nThrough extensive experimentation, we demonstrate the effectiveness of CSI-BERT2. In the CSI prediction task, it significantly outperforms traditional models while maintaining relatively fast computation speeds. In the CSI classification task, it achieves state-of-the-art performance and can effectively handle scenarios where the training and testing sets contain samples with different sampling rates.\n\nThe CSI-BERT2 model can be further applied in other CSI-related scenarios and time-series tasks. Moreover, we aim to explore how to leverage multi-source heterogeneous CSI data to train our CSI-BERT simultaneously. By doing so, we hope to replicate the generalization capabilities of Large Language Models (LLMs) and Large Vision Models (LVMs) in the context of wireless sensing."}]}