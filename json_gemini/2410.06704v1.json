{"title": "PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs", "authors": ["Krishna Kanth Nakka", "Ahmed Frikha", "Ricardo Mendes", "Xue Jiang", "Xuebing Zhou"], "abstract": "In this work, we introduce PII-Scope, a comprehensive benchmark designed to evaluate state-of-the-art methodologies for PII extraction attacks targeting LLMs across diverse threat settings. Our study provides a deeper understanding of these attacks by uncovering several hyperparameters (e.g., demonstration selection) crucial to their effectiveness. Building on this understanding, we extend our study to more realistic attack scenarios, exploring PII attacks that employ advanced adversarial strategies, including repeated and diverse querying, and leveraging iterative learning for continual PII extraction. Through extensive experimentation, our results reveal a notable underestimation of PII leakage in existing single-query attacks. In fact, we show that with sophisticated adversarial capabilities and a limited query budget, PII extraction rates can increase by up to fivefold when targeting the pretrained model. Moreover, we evaluate PII leakage on finetuned models, showing that they are more vulnerable to leakage than pretrained models. Overall, our work establishes a rigorous empirical benchmark for PII extraction attacks in realistic threat scenarios and provides a strong foundation for developing effective mitigation strategies.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated a tendency to memorize training data, which includes not only benign useful content but also sensitive Personal Identifiable Information (PII) of individuals that are part of the pretraining dataset. Notably, LLMs are typically pretrained on a vast corpus of data scraped from the internet, which can contain sensitive PII. With the recent enforcement of regulations such as the AI Act and GDPR ensuring the privacy of data subjects has become paramount.\nDue to growing privacy concerns, early research primarily focused on the memorization of general, non-sensitive suffixes, while more recent studies have specifically investigated the memorization of PII, highlighting the significant privacy risks associated with this phenomenon. However, these studies often vary in their experimental setups and assumptions regarding data access, leading to unstandardized comparisons across studies. Consequently, a clear and unified understanding of PII extraction attacks has not been established yet in the literature. Furthermore, several works have evaluated privacy leakage as part of the larger goal of assessing LLM trustworthiness including safety, harmfulness, and other hazards. These studies, however, are limited to few isolated privacy attack scenarios from Huang et al. highlighting a crucial absence of comprehensive evaluations. This situation underscores the urgent need for critical benchmarking of PII attacks to effectively assess and mitigate PII leakage."}, {"title": "Related Work", "content": "The extraction of verbatim training data, particularly long suffix tokens, has been widely studied in recent years. Many works demonstrated that LLMs can memorize training data and emit it, even with random or empty prompts. Additionally, Zhang et al.; Ozdayi et al. (2023) showed that soft prompts can effectively control this memorization phenomenon. Recent work further shows that training data can be extracted more effectively with higher query counts. However, these studies predominantly focus on general training data extraction rather than sensitive PII information.\nIn contrast, several studies have explicitly examined PII leakage from training data, analyzing both simple prompting techniques and learning-based approaches, such as soft prompts Consequently, PII leakage has become a critical component of LLM alignment evaluation, and is included in popular trustworthiness benchmarks like TrustLLM and Decoding Trust. Concurrently, LLM-PBE explores privacy risks, including membership inference attacks (MIA), system prompt leakage, and true-prefix PII attacks.\nWhile previous surveys have detailed broader privacy and security threats in LLMs, they mainly focus on general training data extraction without explicitly addressing PII extraction in depth. Our work complements these efforts by explicitly focusing on sensitive PII extraction and providing an empirical evaluation of PII attacks. Furthermore, we rigorously study the sensitivity of different hyperparameters within each attack and also evaluate PII leakage under more realistic threat settings, such as higher query budgets and novel continual attack scenarios, offering a more thorough understanding of the privacy risks faced by data subjects in the pretraining dataset."}, {"title": "PII Attacks Taxonomy", "content": "To enable a detailed analysis of PII attacks, we categorize current PII attacks in the literature based on two key dimensions: access to the model and access to the pretraining dataset. Figure 1 illustrates the categorization of threat settings and the potential PII attacks within each setting. We distinguish between black-box and white-box settings (i.e., whether the attacker has access to the target LLM's parameters) at the first level, and consider the attacker's access to the pretraining data at the second level. The latter can occur at three distinct levels: 1. access to the true prefix of the query data subject, 2. knowledge of PII pairs related to a few other data subjects included in the pretraining dataset, and 3. access to the true prefixes of a few other data subjects in the pretraining dataset different from the target data subject."}, {"title": "Experimental Setting", "content": "Following the overview of the PII attacks, we now turn our attention to the benchmark assessment set, which is crucial for evaluating these attacks.\nBenchmark Dataset. The original Enron PII leakage assessment dataset contains 3,333 non-Enron data subjects, each with a name and email pair. Upon exploring this dataset, we observed significant email-domain overlap among the data subjects. Despite the dataset comprising 3,333 data points, there were only 404 unique email domains. Figure 5 illustrates the frequency of the top-30 email domains out of 404 domains, which account for almost 45% of the data subjects. Additionally, the user-part of the email PII is often confined to a few predictable patterns, meaning that knowing the domain-part can make extracting the full email PII much easier, almost a trivial task.\nWe emphasize that this unintended overlap in email domains among data subjects can lead to potential biases in PII attack evaluations, especially when subsets of this data are used for demonstrations (e.g., ICL attack Huang et al. (2022a)) or soft-prompt tuning (e.g., SPT ). In such cases, the email domains in the evaluation set may overlap with those in the subsets, leading to data contamination. In real-world attack scenarios, the evaluated data subjects typically have unknown domains that are not part of the subset available to the attacker."}, {"title": "Sensitivity of PII Attacks", "content": "From this section, we shift our focus to the empirical evaluation of PII attacks. To critically understand the strengths and weaknesses of each attack, we first systematically investigate the robustness of each PII attack with regard to its internal hyperparameters in single-query budget, i.e., LLM is queried only once per query data subject.\nTable 3 outlines the key hyperparameters for each attack, allowing us to explore how sensitive the attacks are to these internal factors. We argue that understanding these sensitivities is crucial for both effective threat assessment and the design of potential mitigations. The following sections detail the sensitivity of each PII attack to its internal factors."}, {"title": "True-Prefix Attack", "content": "The first and strongest attack uses the true prefix $r_q$ of the query data subject $q$ to prompt the victim LLM $f$. Typically, $r_q$ is tokenized, and only the last $L$ tokens are used to prompt the victim LLM $f$. As illustrated in Figure 6, the PII extraction rate improves with the token length $L$ and reaches 21.5% accuracy with $l = 150$ tokens. This attack is considered the gold standard in PII extraction.\nWe use this attack as the upper bound for PII extraction. However, from a practical perspective, it is unrealistic to assume that the adversary has access to the exact true prefixes of query data subjects."}, {"title": "Template Attack", "content": "This attack strategy crafts manual template strings based on the query subject name $s_q$, as illustrated in Figure 3. The results of this prompting strategy are presented in Figure 7a. Notably, we observe that templates with structure D achieve a 3.92% extraction rate, outperforming other templates. The superior performance of Template D can be attributed to the frequent occurrence of similar sequences within the email conversations in the Enron email dataset .  Moreover, Template D often appears as a substring within the true prefixes of the data subjects. This similarity to the true prefixes increases the likelihood of PII extraction\u2014an observation that the PII-Compass attack leverages to launch more effective attacks."}, {"title": "ICL Attack", "content": "ICL attacks enhance template attacks by incorporating k demonstrations, which are selected from $D_{adv}$ and prepended to the query template $T_q$. Although the implementation of this attack is relatively straightforward, our analysis reveals several critical design choices that greatly influence its effectiveness.\nFor each demonstration size $k = \\{2,4,6,8, 16, 32\\}$, we perform random sampling using 21 different random seeds. For each seed, we select k PII pairs from the available pool of $M = 64$ PII pairs in $D_{adv}$, generating 21 distinct sets of demonstrations for each value of k. As shown in Figure 7b, the random seed used to select k demonstrations from the $M = 64$ subjects significantly impacts performance. Each vertical boxplot represents the distribution of extraction rates for a given k number of shots, obtained using 21 different seeds for demonstration selection.\nNotably, we observe substantial variance in extraction rates across the 21 different seeds for a fixed number of demonstrations k. This implies that not only the number of demonstrations but also the specific data subjects chosen as demonstrations play a crucial role in determining the attack's success. For instance, with template B, using just two well-chosen demonstrations can achieve a PII extraction rate of approximately 7.8%, which is comparable to the rate achieved with larger demonstration sizes, such as 32. This suggests that in ICL attacks, the quality of the selected demonstrations is more important than the quantity\u2014a finding that aligns with prior research on ICL for general tasks .\nMoreover, it is important to note that ICL attack does not scale well with a large number of demonstrations, as the increasing prompt length introduces practical limitations in terms of efficiency."}, {"title": "PII Compass Attack", "content": "In this setting, the adversary has access to the true prefixes $\\{r_j\\}_{j=1}^M$ of data subjects present in $D_{adv}$. The attacker prepends a single $r_j$ to the template prompt $T_q$, increasing the likelihood of PII extraction due to enhanced prompt grounding.\nHere, we are particularly interested in the sensitivity to the choice of $r_j$ and the number of tokens $L$ in $r_j$. To investigate this, we vary the true prefixes $r_j$ by iterating over $j = [1, 2, ..., M = 64]$ in $D_{adv}$, prepending each to $T_q$, resulting in $M = 64$ predictions for each data subject $q$.\nFigure 7c shows the extraction rates across the 64 different choices of $r_j$, further stratified by different prefix lengths $L = \\{25, 50, 100\\}$. We observe significant variance in extraction rates, with differences as large as 8% as $r_j$ varies. This suggests that extraction performance highly depends on the specific $r_j$ used. A well-chosen $r_j$ can yield extraction rates as high as 8%, while a poor choice may result in performance even lower than the baseline template attack using $T_q$ alone, as shown in Figure 7a. Each vertical boxplot in Figure 7c represents the distribution of extraction rates obtained using $M = 64$ different true-prefixes $\\{r_j\\}_{j=1}^{M=64}$ for a given prefix length.\nInterestingly, the number of tokens in the true-prefix $r_j$ has minimal impact on performance. Even with $L = 25$ tokens, sufficient contextual information exists to ground the victim LLM $f$ effectively, achieving performance similar to that of larger token lengths, such as $L = 150$."}, {"title": "Soft-Prompt Tuning Attacks", "content": "The SPT attack optimizes a set $S$ of $L$ soft embeddings using the $M = 64$ PII pairs $\\{(s_j,p_j)\\}_{j=1}^M$ from the dataset $D_{adv}$. The learned PII-evoking soft prompt embeddings are then prepended to the template prompt $T_q$."}, {"title": "Evolving Attack Capabilities", "content": "In the previous section, we studied the sensitivity of PII attacks in a single-query setting. In this section, we extend our analysis to a multi-query setting to thoroughly examine the maximum extraction rates for each PII attack and better understand their overall efficacy. Several studies on training data extraction assess memorization rates in LLMs by prompting the model multiple times. We adopt a similar experimental approach in the context of PII extraction. Moreover, in real-world scenarios, adversaries are likely to make a reasonable number of queries during their attacks, which motivates our exploration of the multi-query setting.\nTo this end, we evaluate PII extraction in two realistic scenarios with a higher query budget: 1) a static attacker, who uses repeated or diverse input prompts to query the LLM multiple times, and 2) an adaptive attacker, who iteratively leverages previously extracted PIIs to enhance subsequent extractions. We discuss these two scenarios in detail below."}, {"title": "Multi-query Attacks", "content": "In this experiment, we report the aggregated PII extraction rates, which measure the success rate of extracting PII at least once across K input queries. To explore this, we launch each PII attack with multiple queries to the LLM and analyze the resulting aggregated PII extraction rates. Specifically, we employ either diverse input prompts or use model sampling to diversify the generated outputs.\nThe key results of this study are summarized in Table 4. The first four columns outline the threat setting for each attack, and the fifth column reports the model accessibility in each threat scenario. We report the aggregated extraction rate across K queries in the last column, and the highest extraction rate achieved among these K queries in the second-to-last column.\nIn summary, our findings show that extraction rates improve by 1.3 to 5.4 times across all attack methods when multiple queries (fewer than 1000) are employed. To illustrate this, let's first consider the true-prefix attack in the first row of Table 4.\nWe observe that the true-prefix attack , combined with top-k model sampling (with k set to 40), increases the extraction rate to 39.0% after 256 queries. This evaluation is conducted across four different true-prefix context sizes $L = \\{25, 50, 100, 150\\}$, with each context size prompt queried 64"}, {"title": "Continual PII Extraction", "content": "In this section, we explore PII attacks in a novel, adaptive attack setting, inspired by the observation that few-shot examples of data subjects in the adversary set $D_{adv}$ in ICL and SPT can improve extraction rates for other data subjects in the evaluation set $D_{eval}$. We investigate a scenario where, after successfully extracting PIIs from the evaluation set, the attacker leverages these extracted PIIs in future attacks. This approach assumes the adversary can determine when a PII has been successfully extracted, which may be feasible for certain types of PIIs. For instance, an attacker could verify extraction success by sending an email or contacting the individual via a mobile number.\nAs a case study, we conduct an experiment using the SPT attack in a continual learning setting. We select SPT attacks because they rely solely on PII pairs in $D_{adv}$ and scale more efficiently than ICL attacks, which become less efficient as the number of input tokens increases with the growing number of demonstrations. In contrast, the length of the soft-prompt in SPT attacks can be kept the same, independent of the number of PII pairs in $D_{adv}$.\nThe core idea is to use the V successfully extracted PII pairs $\\{s_v,p_v\\}_{v=1}^V$ from the evaluation set $D_{eval}$, incorporate them into the adversary's knowledge set $D_{adv}$, retrain the soft-prompt embeddings $S$ on this augmented adversary dataset, and continue the SPT attack on the evaluation set. This process is repeated over 10 rounds, using 5 different prompt initializations across 4 templates.\nFigure 10 shows the PII extraction rates over the 10 rounds. We observe that the average PII extraction rates (across 5 initializations) at the end of round 1 are 3.95%, 5.79%, 6.00%, 7.25% improving to 8.27%, 9.99%, 9.99%, and 10.5% by the end of 10 rounds for the four templates, respectively. We also observe that extraction rates tend to saturate after 5 rounds. This experiment demonstrates that with adaptive attack capabilities, PII extraction rates can nearly double over successive rounds."}, {"title": "Ablation Studies", "content": "In this section, we conduct several ablation studies on different PII attack methods to gain deeper insights into the extraction process.\nSynthetic Data for PII Extraction. Advanced PII attacks such as ICL , SPT, and PII-Compass typically assume access to few-shot PII pairs $\\{(s_j,P_j)\\}_{j=1}^M$ or true prefixes $\\{r_j\\}_{j=1}^M$ of a limited number of data subjects in $D_{adv}$. In this ablation study, we relax this assumption by experimenting with synthetically generated PII pairs and prefixes. Specifically, we create synthetic datasets with varying levels of realism.\nFor example, given a real PII pair {Karen Arnold, klarnold@flash.net} in the adversary dataset $D_{adv}$ as shown in Figures 21 and 22, we generate synthetic PII pairs in two variations: 1. Altering only the name with email-domain retained (e.g., {\"Cameron Thomas\", \"cthomas@flash.net\"}, as shown in Figures 23 and 24 in the Appendix). 2. Altering both the name and the domain with synthetic ones (e.g., {\"Cameron Thomas\", \"cthomas@medresearchinst.org\"}, as shown in Figures 25 and 26 in the Appendix).\nFor synthetic prefixes in the PII-Compass attack, we use GPT-3.5 to generate email conversation sentences of 50 tokens in length between employees of an energy corporation like Enron, as illustrated in Figures 27 and 28.\nThe results of PII attacks on these synthetic data experiments are presented in Figures 11 for ICL, PII-Compass, and SPT attacks in three columns, respectively. Overall, our observations are as follows: 1. When both the name and domain are replaced with synthetic data, the extraction rates for both ICL and SPT attacks are notably lower (shown in purple bars) compared to the original performance with real PII pairs (shown in yellow bars). 2. When only the name part is anonymized, the performance of the ICL attack (shown in green bars) remains closer to the original performance with real PII pairs (shown in yellow bars). In contrast, the performance of SPT attacks in this setting shows a significant drop in performance (shown in green bars) from that with original PII pairs (shown in yellow bars) and in fact, the SPT attack, does not even surpass the performance of simple template prompting, as shown in Figure 7a. 3. With synthetic prefixes generated by GPT-3 (OpenAI, 2023), the performance (shown in purple bars) is substantially lower than the original performance with real prefixes from subjects in $D_{adv}$, as illustrated in Figure 11c. Our experiments suggests that for effective PII extraction with PII-Compass, having a prefix that closely resembles the true domain is essential."}, {"title": "Transferability of Soft-prompt embeddings", "content": "Typically, the template structure used during the training of soft-prompt embeddings and at attacking stage remains same . We modify this setting and study the transferability of soft-prompt embeddings from one template structure to another. To illustrate this with an example, during the training stage, the soft prompt embeddings are prepended to the source template structure \"A\" and trained with CE loss on the adversary dataset $D_{adv}$. However, at the inference stage, we can prepend the learned soft-prompt embeddings on other template structures.\nWe visualize the results of soft-prompt transferability in Figure 13. Notably, we observe that soft prompt embeddings trained with template structure \"D\" exhibit the best transferability when applied to other templates. For example, soft prompt embeddings trained with template D achieve extraction rates of 5.0%, 6.2%, and 6.0% when transferred to templates A, B, and C, respectively. In contrast, templates A, B, and C achieve 3.8%, 5.7%, and 6.1% when using their own template structures for soft-prompt training. Additionally, the transferability of soft prompt embeddings trained on templates A, B, and C is less effective when transferred to other templates. While this study serves as a preliminary effort in understanding soft-prompt transferability across different templates, we believe that learning highly transferable soft-prompt embeddings can be helpful for extracting PIIs in other domains within the pretraining dataset. Furthermore, more work towards prompt transferability could lead to even more powerful attacks, especially in scenarios where the adversary dataset $D_{adv}$ is limited or scarce."}, {"title": "PII Attacks on Finetuned Model", "content": "We now shift our focus from PII extraction on the pretrained model to the finetuned model. The pretrained model is trained on the vast PILE dataset , where the Enron email dataset constitutes only a small portion. However, we are also interested in studying PII extraction on a model recently finetuned on a single downstream dataset. To this end, we finetune GPTJ-6B on the email body portions of the Enron email dataset , which contains 530K data points. We use 80% of these data samples for the finetuning process for 2 epochs, reserving the rest for hyperparameter tuning. Let us now examine the key findings of PII attacks on the finetuned model in comparison to the pretrained model. We will keep the discussion brief, as a similar analysis for the pretrained model has been covered in previous sections.\nSingle-query setting. In Figure 14, we visualize the performance of PII attacks using the true-prefix and template attack , shown on the left and right, respectively. As expected, the finetuned model exhibits higher privacy risks than the pretrained"}, {"title": "Research Directions", "content": "In this section, we discuss potential research directions for further improving the efficacy of PII attacks and gaining a deeper understanding of the mechanisms behind PII leakage.\nHow to Select Demonstrations in ICL Attacks? In \u00a7 5.3, we highlighted the sensitivity of ICL attacks to the method of demonstration selection, using naive random selection as our approach. However, the literature on ICL provides substantial insights into more advanced techniques, such as input-specific adaptive demonstration selection  and the impact of demonstration order Given these complexities, we believe that ICL attacks, when further refined and tailored for PII extraction tasks, have significant potential to increase PII leakage.\nWhy do PII Attacks Succeed? Numerous studies have examined the internal workings of LLMs from a safety perspective. Few recent works have shifted the focus toward privacy concerns, identifying neurons responsible for data leakage , using activation steering techniques, or exploring unlearning processes . A key limitation of these approaches is their reliance on simple zero-shot template attacks for evaluation , raising concerns about the robustness of these interpretability-based mitigations. For example, (Patil et al., 2023) shows that LLM unlearning does not fully erase private data, which can still be retrieved by probing internal layers . Furthermore, a recent work reveals that unlearning techniques are prone to obfuscation, and a simple few-shot finetuning can restore unsafe capabilities. Therefore, a thorough analysis of privacy assessments against strong adversaries and an understanding of the underlying factors behind successful attacks is crucial.\nHow to Construct the PII Leakage Evaluation Set? A major challenge in PII assessment is the lack of comprehensive benchmark datasets. Currently, PII benchmark evaluations primarily rely on the Enron email dataset . However, LLM memorization can be influenced by factors such as data repetition and the positioning of data points during training . As a result, PII leakage may depend not only on the effectiveness of the PII attack but also on other factors present during pretraining. Therefore, developing a more principled approach to constructing a PII leakage evaluation dataset is essential for accurately assessing privacy risks."}, {"title": "Summary and Conclusion", "content": "In this work, we introduce PII-Scope, an empirical benchmark for assessing PII leakage from LLMs in different treat settings. We first evaluated the robustness of each PII attack method with respect to its internal hyperparameters. Our analysis uncovered key findings: hard-prompt attacks are highly sensitive to prompt structure and context, while soft-prompt attacks are influenced by prompt initialization and the number of training epochs. Furthermore, we demonstrated that PII attacks in a single-query setting significantly underestimate the extent of PII leakage. We show that attackers can exploit various combinations within these methods to launch multi-query attacks, and can dynamically adapt their strategies in continual settings, and achieve up to a 5.4x boost in extraction rates with modest query budgets.\nAdditionally, we compared the extraction rates of finetuned model to pretrained model, empirically demonstrating the significantly elevated privacy risks in finetuned settings. We achieved extraction rates exceeding 60% on the finetuned model with fewer than 500 queries. Overall, we hope that our work provides a fair and realistic benchmark for evaluating PII leakage, offering insights into how attackers can enhance extraction rates, and emphasizing the need for more robust defenses."}, {"title": "Limitations", "content": "Our work has several limitations. Firstly, recent LLMs such as LLama and Gemma do not disclose the sources of their pretraining datasets. Due to the lack of publicly available PII benchmark datasets that were part of these pretraining datasets, our analysis is confined to examining a single type of PII\u2014email\u2014found in the Enron email dataset . Additionally, our evaluations are limited to base LLMs and do not extend to instruction-tuned counterparts, i.e., aligned LLMs, which may exhibit different behaviors in response to PII extraction prompts. Specifically, in the aligned LLM setting, the focus shifts towards jailbreaking aligned models back to their base configurations, where the attacks discussed in this paper could be applied to extract PII from the jailbroken models. In the future, we plan to conduct an empirical evaluation of PII jailbreaking techniques on aligned LLMS."}, {"title": "Reproducibility", "content": "We are committed to the reproducibility of our experiments. To this end, we provide exhaustive details for each experiment, adhering closely to the reproducibility best practices . Implementation. We adapt the FederatedScope library  by removing federated functionalities such as broadcasting and aggregation, leveraging its robust modular implementations of dataloaders, trainers, and splitters. The experiments are conducted using the software stack: PyTorch 2.1.3, Transformers 4.39.0 , and PEFT 1.2.0 . To ensure reproducibility, all experiments are carefully seeded to maintain determinism, confirming that our results are fully reproducible. Unless otherwise stated, we use greedy decoding and generate 25 tokens from the LLM. Subsequently, we extract the email portion from the generated string using the below regex expression."}, {"title": "SPT attacks", "content": "Impact of Number of Training Epochs. In Figure 18, we present the PII extraction rates for each template across 41 different initializations\u201420 task-aware, as shown in Figure 19, and 21 random strings, as shown in Figure 20. We observe significant variance in the PII extraction rate at each epoch, indicating that determining the optimal number of epochs for each configuration and template requires careful tuning with a separate validation set."}]}