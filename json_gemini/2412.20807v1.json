{"title": "Two Heads Are Better Than One: Averaging along Fine-Tuning to Improve Targeted Transferability", "authors": ["Hui Zeng", "Sanshuai Cui", "Biwei Chen", "Anjie Peng"], "abstract": "With much longer optimization time than that of untargeted attacks notwithstanding, the transferability of targeted attacks is still far from satisfactory. Recent studies reveal that fine-tuning an existing adversarial example (AE) in feature space can efficiently boost its targeted transferability. However, existing fine-tuning schemes only utilize the endpoint and ignore the valuable information in the fine-tuning trajectory. Noting that the vanilla fine-tuning trajectory tends to oscillate around the periphery of a flat region of the loss surface, we propose averaging over the fine-tuning trajectory to pull the crafted AE towards a more centered region. We compare the proposed method with existing fine-tuning schemes by integrating them with state-of-the-art targeted attacks in various attacking scenarios. Experimental results uphold the superiority of the proposed method in boosting targeted transferability. The code is available at github.com/ zengh5/Avg_FT.", "sections": [{"title": "I. INTRODUCTION", "content": "Building an interpretable and trustworthy deep neural network (DNN) is critical in many security-sensitive applications. Adversarial example (AE) is a valuable tool for uncovering the opaqueness and vulnerability of DNNs [1, 2]. In a broad sense, AE generation optimizing pixel values against DNN models is analogous to DNN training optimizing model weights guided by (data, label) pairs. Consequently, improving AEs' transferability across various models is akin to enhancing DNNs' generalizability to unseen data. Many tricks for enhancing model generalization in machine learning have been applied to improve AE transferability. For example, the widely accepted momentum method [3] has been used to stabilize the AE optimization [4, 5], and input transformation-based attacks [5-7] can find their consistency-enforcing [8] counterparts in machine learning.\nThis paper centers on the transferability of targeted attacks, which is much less studied yet more daunting than their untargeted counterparts. Previous studies have shown that feature-space fine-tuning is an efficient and effective way of boosting targeted transferability [9-11]. However, these methods only utilize the endpoint of the fine-tuning and fail to exploit the trajectory information fully. Inspired by the fact that averaging high-quality model weights leads to better generalization [12, 13], we propose averaging AEs along the fine-tuning trajectory to generate more transferable AEs. We hypothesize and empirically validate that by temporal averaging, the crafted AE is calibrated towards a more centered region of the loss surface (Fig. 1), thus exhibiting stronger transferability. This paper substantially improves [11] in the following aspects:\n\u2022 We discover that AEs obtained by [11] typically oscillate around the periphery of a flat region of the loss surface.\n\u2022 Based on this observation, we propose a novel targeted attack, averaging along fine-tuning (AaF), where crafted AEs are located in a more central area than that in [11], thereby exhibiting stronger transferability across different DNNs.\n\u2022 Experiments demonstrate that AaF can improve state-of-the-art targeted attacks notably with negligible overheads."}, {"title": "II. THREAT MODEL AND RELATED WORK", "content": ""}, {"title": "A. Threat model", "content": "We define our threat model from three perspectives: the goal, the knowledge, and the capability of the adversary.\nAdversary's goal. This paper copes with the targeted attack, in which an adversary aims to mislead a DNN-based classifier f() to predict an AE I' as a specific label yt, i.e., f(I') = yt.\nAdversary's knowledge. We address the transfer-based scenario in which an adversary does not know victim models."}, {"title": "Adversary's capability.", "content": "Under human inspection, well- crafted AEs should be non-suspicious. Without loss of generalizability, we constrain the perturbation budget with the Lo norm in this paper. To improve the attack power, resource- intensive schemes train extra, target-specific classifiers [14] or generators [15-17]. However, the training time will be prohibitive when the number of targeted classes is enormous. Hence, we follow the conventional simple iterative attacks that require neither additional data nor model training in this study."}, {"title": "B. Transferable targeted attack", "content": "A targeted attack is strictly more challenging than an untargeted one since the latter can be regarded as targeted towards the easiest label [18]. Moreover, unique challenges prevail in targeted attacks, e.g., gradient vanishing [19, 20] and the restoring effect [19, 21], which necessitates tailored schemes to craft transferable targeted AEs.\nTo address the gradient vanishing challenge, the Po+Trip attack [19] and the Logit attack [20] replace traditional cross- entropy (CE) loss with the Poincare distance loss and Logit loss, respectively. More recently, the Margin attack [22] argues that downscaling the logits with a temperature factor or an adaptive margin can also alleviate the issue caused by gradient vanishing. To overcome the restoring effect, [19] introduces a triplet loss to push I' away from yo, and the SupHigh attack (SH) further suppresses all high-confidence labels [21]. In addition, the self-universality method (SU) introduces a feature similarity loss to encourage the adversarial perturbation to be self-universal [23]. The activation attack (AA) [24] pushes I' towards a carefully selected natural image of the target class in the feature space. The clean feature mixup method (CFM) mixes features from other images in the same batch to encourage competition during optimization [25]. Strictly speaking, AA and CFM go beyond simple gradient methods since additional images are involved in attacking.\nAnother interesting research direction is fine-tuning an existing AE in the feature space. The intermediate level attack (ILA) [9] maximizes the scalar projection of the AE on the direction f\u2097(I') \u2013 f\u2097(I), where f\u2097() is the feature presentation in the l-th layer. On the other hand, the feature-space fine-tuning method (FFT) [11] is guided by a combined aggregate gradient:\n$\\Delta_{combine} = \\Delta_{I',t} - \\beta\\Delta_{I',0}$"}, {"title": "where \\$\\Delta_{I',t}\\$ is the aggregate gradient measuring the yt-related feature importance", "content": "\\$\\Delta_{I',0}\\$ measures the yo -related feature importance. Combining them together with a predefined weight \u1e9e could encourage the features contributing to yt and suppress yo -related features simultaneously. Because targeted attacks demand significantly more iterations to converge than untargeted ones [20], fine-tuning is a promising strategy for enhancing targeted transferability in practice.\nResource-intensive attacks have reported competitive targeted transferability. In the feature distribution attack [14], a one-versus-all classifier is trained for each yt at each specific layer to predict the probability that a feature map is from yt. The transferable targeted perturbation (TTP) [15] trains class- specific generators to synthesize targeted perturbation. However, training a dedicated generator for every (source model, target class) pair is costly in practice. This limitation is partially lifted by training a conditional generator [26] to target multi-class simultaneously [16, 17]. However, the number of targeted labels that a single generator can cover is limited, and as the number of targets increases, the attack ability inevitably decreases."}, {"title": "III. PROPOSED SCHEME", "content": ""}, {"title": "A. Motivation", "content": "Prior research points out that AE generation can be regarded as a dual optimization problem of DNN training [4, 5]. Following this line, AEs' transferability corresponds to DNNs' generalizability. For example, model ensemble, analogous to data augmentation in DNN training, has been proven to effectively enhance attack transferability [27]. However, an ensemble of substitute models may not be feasible in practice. Hence the question: Can we elevate targeted transferability with a single substitute model?\nThe answer to the question above is analogous to improving DNNs' generalizability without training data augmentation. In [12, 13], the authors find that averaging high-performing snapshots along the training trajectory results in a more generalizable DNN. Similarly, during the fine-tuning process of FFT [11], many 'high-quality' snapshots of AEs show muscular attack strength against the surrogate model. Fully utilizing fine- tuning trajectory information rather than just the endpoint inspires the proposed method."}, {"title": "B. Averaging along fine-tuning", "content": "This paper follows the fine-tuning framework in [11] but attempts to explore better the knowledge contained in the fine- tuning trajectory, as displayed in Fig. 2. Similar to FFT, we start from an AE I' generated by a baseline attack (e.g., CE or Logit). The first step is calculating the feature importance to guide the subsequent fine-tuning. Here, we adopt the aggregate gradient to measure the importance of the feature. Let $\\Delta_l = \\frac{\\partial l_t(I')}{\\partial f_k(I')}$, where lt() denotes the logit output w. r. t. Yt, The aggregate gradient $\\Delta_{I',t}^k$ is calculated as:\n$\\overline{\\Delta}_{I',t}^k = G(\\Sigma \\Delta_l)$\n$\\overline{\\Delta}_{I',0}^k = G(\\Sigma \\Delta_l)$"}, {"title": "where \\$\\overline{\\Delta}\\$ is a normalization term, Mn is a random mask to neutralize model-specific information", "content": "Unlike FFT, which adopts a pixel-wise mask, in this paper, we employ a patch-wise mask as proposed in [28]. Motivated by [29] that CNN models' discriminative regions are often clustered, we apply Gaussian smoothing G() to the aggregate gradients to further highlight the class-related features. Similarly, yo-related feature importance is calculated as\n$\\Delta_{I',0}^k = G(\\Sigma \\Delta_l)$"}, {"title": "Following [11], the aggregate gradients \\$\\overline{\\Delta}_{I',t}^k\\$ and \\$\\overline{\\Delta}_{I',0}^k\\$ are combined, as in (1), to push the subsequent optimization closer to yt and away from yo", "content": "In [11], I' is optimized according to the following objective:\n$\\underset{I_{ft}}{argmax} (\\Delta_{combine}.f(I'_{ft}))$, s.t. $\\left | \\left | I'_{ft} - I \\right | \\right |_{\\infty} \\leq \\epsilon$\nIn contrast, this paper utilizes the whole trajectory of fine-tuning, as illustrated in Fig. 1, and it can be realized as:\n$I'_{aaf} = \\gamma^{Nft-1}I'_{ft,0} + \\gamma^{Nft-2}I'_{ft,1} + ... + I'_{ft,Nft-1}$"}, {"title": "where \\$I'_{ft,i}\\$ represents the i-th snapshot during fine-tuning", "content": "Nft is the number of fine-tuning iterations, and y is the decaying factor. Eq. (5) reduces to a simple average when y = 1, and it means only the endpoint is kept when y = 0, as done in [11]. Eq. (5) can be memory-friendly implemented as:\n$I'_{aaf,0} = I'_{ft,0}$,\n$I'_{aaf,i} = \\gamma I'_{aaf,i-1} + I'_{ft,i}$"}, {"title": "In the remaining, we abbreviate the proposed fine-tuning scheme (5) as Average along Fine-tuning (AaF).", "content": "Recall that the snapshots involved in (5) are expected to possess 'high-quality.' To ensure that, warming-up fine-tuning iterations are adopted before AaF. We find that just a few iterations of warming up (Nwu) are sufficient to ensure the quality of the snapshots and set Nwu = 5 in our experiments."}, {"title": "C. What is AaF doing?", "content": "AaF is based on averaging high-quality AEs along the fine- tuning trajectory. We expect the averaging to encourage the AE generation towards a more centered region than the plain fine- tuning. To verify our conjecture, we plot the logit values (w.r.t."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We compare the proposed AaF method against two fine- tuning strategies: the targeted version of ILA [9] and FFT [11]. Five representative simple iterative targeted attacks, CE [34], Logit [20], Margin [22], SH [21], and SU [23], are used to craft the baseline AEs for fine-tuning. The Po+Trip attack [19] is omitted because more recent methods claim domination over it. We also investigate how simple iterative attacks with fine-tuning can approach generative methods: TTP [15] and C-GSP [16]. All the iterative schemes start with the TMDI attack [4, 6, 7]. More experimental results are available in our supplementary material: github.com/zengh5/Avg_FT/supp.pdf."}, {"title": "A. Experimental settings", "content": "Dataset. Our experiments are conducted on the ImageNet- compatible dataset comprised of 1000 images [35]. All these images are cropped to 299 \u00d7 299 pixels before use.\nNetworks. Following [20, 22], we use four pretrained models of diverse architectures: Inc-v3, Res50, Dense121, and VGG16 as the surrogates. The AEs' transferability is evaluated on models that have not been used as surrogates. In Sec. IV. C, an adversarially trained model Res50adv serves as the surrogate. We also test four transformer-based models, Swin [36], vit_b_16 [37], pit_b_24 [38], and visformer [39], in the supplementary file.\nParameters. For all competitors, the maximum perturbation is set to e\u2264 16, and the step size is 2. Following [11], we set the baseline iteration number N=160 (N=200 when fine-tuning is disabled), the fine-tuning iterations Nfr=10, and the balance weight \u1e9e=0.2. For the fine-tuning layer k, we select Mixed_6b for Inc-v3, Conv4_3 for VGG16, and the last layer of the third block for Res50 and Dense121. The decaying factor y in (5) is experimentally set to 0.8. The ablation study on y is provided in the supplementary file. Throughout this section, the best results are in bold."}, {"title": "B. Normal surrogates", "content": "Table I reports the targeted success rates (random-target) for various (surrogate, victim) pairs. All the baseline attacks benefit from fine-tuning, and the improvement led by the proposed AaF is more significant than that of ILA and FFT. As a rule of thumb, the weaker the baseline, the more significant the improvement. Hence, the upturn is particularly salient for the CE attack. For example, when transferring from Dense121 to VGG16, the success rate of the CE+AaF attack is four times higher than that of the CE attack (50.3% vs. 11.3%). Another takeaway is that the more challenging the transfer scenario, the more significant the improvement brought out by fine-tuning. Let Logit be the baseline attack, its gain from AaF is about ten percent in the case"}, {"title": "C. Robust surrogate", "content": "Next, we craft AEs with an adversarially trained model Res50adv (the L2 budget of the AEs for training the model is 0.01), which is believed to be conducive to transferability [40]. The victim models are the same as those in the last section except for Res50. Table II presents the targeted transferability in this scenario. AEs crafted against a slightly robust model indeed show stronger transferability in most cases. Nevertheless, AaF can further boost their attack ability by a clear margin. Taking 'Res50adv\u2192Inc-v3' as an example, Logit+AaF surpasses Logit by 31% (34.2% vs 26.1%)."}, {"title": "D. Iterative vs. generative attacks", "content": "Last, we compare AaF+simple iterative attacks with two generative attacks: TTP and C-GSP. Since training dedicated generators for each target label and each source model is prohibitive for our used ImageNet-compatible dataset, we download ten author-released generators and follow the '10- Targets' setting of [15]. These generators are trained with Res50 being the discriminator. For C-GSP, we train a 10-target conditional generator with Res50 being the discriminator on the ImageNet 'train' dataset.\nWe evaluate all competitors under two perturbation budgets: \u20ac = 8 and \u20ac = 16. Table III shows that fine-tuned iterative attacks yield comparable or even better transferability (when Dense121 or VGG16 is the victim model) than generative methods. Because the generative methods heavily hinge on semantic patterns [20] (refer to the supplementary file for examples), their performance significantly deteriorates as the budget decreases. In contrast, the iterative methods are more resilient when the budget is low. Taking \u2018Res50\u2192Dense121' for example, Logit+AaF is equivalent to TTP at e = 16, but has a significant advantage at e = 8 (44.8% vs 38.6%)."}, {"title": "V. CONCLUSION", "content": "Learning from others' strengths is an ancient wisdom. In this paper, we hypothesize and empirically validate that AEs' transferability can be advanced by fully exploiting fine-tuning trajectories. Specifically, we propose a novel targeted attack called Averaging along Fine-tuning (AaF) to encourage AE generation towards flatter regions than the vanilla feature-space fine-tuning. The superiority of the proposed AaF is validated by integrating it with state-of-the-art iterative targeted attacks in various transfer scenarios. Experimental results corroborate that our AaF is superior to existing fine-tuning schemes and can boost targeted transferability universally, while its overhead compared to baseline attack is negligible."}]}