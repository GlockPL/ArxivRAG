{"title": "SkelMamba: A State Space Model for Efficient Skeleton Action Recognition of Neurological Disorders", "authors": ["Niki Martinel", "Mariano Serrao", "Christian Micheloni"], "abstract": "We introduce a novel state-space model (SSM)-based framework for skeleton-based human action recognition, with an anatomically-guided architecture that improves state-of-the-art performance in both clinical diagnostics and general action recognition tasks. Our approach decomposes skeletal motion analysis into spatial, temporal, and spatio-temporal streams, using channel partitioning to capture distinct movement characteristics efficiently. By implementing a structured, multi-directional scanning strategy within SSMs, our model captures local joint interactions and global motion patterns across multiple anatomical body parts. This anatomically-aware decomposition enhances the ability to identify subtle motion patterns critical in medical diagnosis, such as gait anomalies associated with neurological conditions. On public action recognition benchmarks, i.e., NTU RGB+D, NTU RGB+D 120, and NW-UCLA, our model outperforms current state-of-the-art methods, achieving accuracy improvements up to 3.2% with lower computational complexity than previous leading transformer-based models. We also introduce a novel medical dataset for motion-based patient neurological disorder analysis to validate our method's potential in automated disease diagnosis.", "sections": [{"title": "1. Introduction", "content": "Human action recognition is the task of classifying actions based on human movements. The problem is often tackled by leveraging the rich contextual features in RGB videos -at the cost of exposing information about people's identities. Skeleton-based action recognition has emerged as a privacy-preserving alternative for sensitive applications, from patient monitoring and physical therapy to assisted living environments.\nSkeletal 3D joint representations are compact and robust to environmental conditions (e.g., background clutter and light variations) yet their sparse nature makes this task in-herently challenging. In the medical domain, precisely capturing the dynamic spatio-temporal relationships between joints is of paramount importance for precise analysis of subtle movements indicative of various diseases. For example, analyzing a patient's gait can provide insights into neurological disorders, musculoskeletal abnormalities, and other health conditions.\nThe skeleton joints and their connections (i.e., bones) correspond to the vertices and edges of a graph structure. Our community has recently proposed skeleton action recognition methods that model the spatial and temporal dependencies among skeletal joints via Graph Convolution Networks (GCN) or transformer-like architectures. GCN-based methods introduced adaptive graph structures (e.g., [9, 48, 49]), specialized joint encodings (e.g., [4, 32]), and explored multiple modalities (e.g., [34]) for learning robust representations (e.g., [24, 32, 74]). Transformer-based methods tackle the long-range dependencies in skeletal data that GCN-based methods often struggle with. Existing methods model the skeletal spatio-temporal relationship between physically neighboring and distant joints/frames using the self-attention mechanism (e.g., [11, 75]) in one-shot settings (e.g., [77]) or for joint training across different action tasks and datasets (e.g., [14]).\nTransformer-like architectures are computationally demanding and GCN-based methods struggle to model the relation of physically distant joints \u2013captured through the direct propagation of information between physically connected joints. This motivates the introduction of our novel state-space model (SSM)-based architecture, which models all joint relationships with an efficient spatial-temporal scanning strategy \u2013designed to analyze skeletal data for patient disease recognition.\nOur novel approach introduces a structured decomposition of skeletal motion data operating across three complementary dimensions. Given an input sequence, we first partition its channel representations into specialized groups for spatial, temporal, and spatio-temporal analysis. The spatial and temporal streams capture local patterns and short-range frame-wise transitions, while the spatio-temporal stream introduces State Space Models (SSMs) for complex motion modeling.\nNeurological disorders impact distinct body parts during locomotion, resulting in disease-specific motion patterns. Within the spatio-temporal stream, we further partition the input according to anatomically meaningful body parts (e.g., legs, torso, arms) and their key interactions (e.g., arms-legs coordination) that are analyzed by separate SSMs. Each SSM features our novel four-way scanning strategy that splits each anatomical group into four channel subgroups. A specific scanning direction is applied to a specific subgroup. This enables efficient parallel processing -while reducing the computational demands of our model- and allows us to collectively analyze the motion patterns across both space and time: from temporal to spatial domain, spatial to temporal domain, and their respective inverse directions. This multi-directional scanning enables comprehensive capture of both local joint relationships and global motion patterns while being computationally efficient.\nThis new anatomically-aware architecture proves particularly effective for automated medical diagnosis, where subtle motion abnormalities often manifest through complex interactions between different body parts over time. Nevertheless, our model remains generic and demonstrates significant improvements over state-of-the-art results on existing challenging action recognition datasets, showcasing its versatility and robustness.\nOur contribution is threefold:\n\u2022 We propose a novel multi-stream architecture leveraging SSMs that effectively decomposes motion analysis into spatial, temporal, and spatio-temporal streams through channel partitioning, enabling efficient parallel processing of distinct motion characteristics.\n\u2022 We introduce an anatomically-aware partitioning scheme that guides the SSM analysis based on meaningful body parts and their interactions, capturing both local joint dynamics and complex cross-body motion patterns crucial for medical diagnosis.\n\u2022 We develop a channel-split scanning mechanism where input features are partitioned into four subgroups, each processed by direction-specific SSMs. This approach enables comprehensive multi-directional motion analysis while maintaining computational efficiency through reduced channel dimensionality.\nThrough extensive experiments on both medical diagnosis tasks (we introduce a dataset for the analysis of patient walking styles to assist in automated neurological disease diagnosis) and standard action recognition benchmarks, we demonstrate that our method achieves state-of-the-art performance while being very computationally efficient."}, {"title": "2. Related Work", "content": "Graph Convolutional Networks (GCNs) have been initially explored for skeleton-based action recognition in [65]. The seminal work introduced the Spatial-Temporal Graph Convolutional Network (ST-GCN) framework for modeling human joints with spatiotemporal graph structures. Recent architectural innovations have evolved along multiple dimensions, from joint-bone fusion networks [43] and multi-scale feature extraction [51] to adaptive graph topology learning [48]. Context-aware architectures [7, 71] and dual-stream temporal models [50] have enhanced feature extraction capabilities, while adaptive graph structures [37, 67] and efficient convolutions [8, 12] have optimized computational overhead. These developments, complemented by multi-modal integration approaches [34], have significantly advanced the field's ability to capture rich motion characteristics while maintaining robustness across varying conditions.\nGCN-based approaches rely on local graph operations and predefined adjacency matrices, limiting their ability to capture long-range dependencies and dynamic motion patterns. Our approach diverges by introducing state-space models (SSMs) for dynamic latent space partitioning, efficiently capturing local and global interactions.\nTransformers are a powerful alternative architecture for skeleton-based action recognition, primarily due to their capacity to model complex joints and temporal motion patterns through the self-attention mechanism [55]. Spatial-temporal attention frameworks [14, 44] enable joint modeling of structural and dynamic information, while global-local attention mechanisms [26] selectively focus on key motion patterns across different temporal scales. Frequency-aware architectures [61] improve data efficiency through spectral augmentation, while specialized designs [11] optimize performance for specific motion types. Multi-modal approaches [57] incorporate complementary sensor data to enhance robustness across varying conditions. Self-supervised pretraining strategies [75] leverage unlabeled data for improved representation learning, while efficient architectural designs [40, 41] maintain high accuracy with reduced computational demands.\nDespite these advances, transformer-based methods face inherent limitations due to their quadratic computational complexity in modeling pairwise attention, motivating our exploration of more efficient architectures for real-time applications.\nState-space models (SSMs) offer an efficient approach to sequence modeling with long-range dependencies. Initial work on linear state-space layers [18] laid the foundation for S4 [19], while subsequent variants [22, 23, 53] demonstrated comparable performance with simplified architectures. Mamba [17] addressed content-based reasoning limitations through input-dependent parameters [16]. For vi-"}, {"title": "3. SkelMamba", "content": "The SkelMamba architecture is illustrated in Figure 1. A skeleton sequence is denoted as $X \\in \\mathbb{R}^{T \\times V \\times C}$, where $T$ is the sequence length, $V$ is the number of joints per frame, and $C$ represents the joint coordinates.\nThree linear layers project the low-dimensional skeleton data onto a higher-dimensional embedding space. A learnable time-space token is added to the embedding, then input to $L$ Time-Space Mamba Blocks (TSMB), each containing a Part-Group Mamba Block (PGMB) modeling the skeletal-temporal relations and a feed-forward network (FFN) for feature refinement. To retain temporal dynamics information while reducing the computational complexity, after two TSMB blocks, a TDown layer consisting of a Conv1D with a stride of 2 followed by BN is applied. The features obtained after the $L$ TSMBS undergo skeletal-temporal average pooling followed by a Linear layer producing $\\hat{y} \\in \\mathbb{R}^{C}$, with $C$ denoting the number of classes."}, {"title": "3.1. Time-Space Mamba Block (TSMB)", "content": "To design our novel TSMB, we follow a similar structure to traditional transformer blocks [55]. The first part of the block models the spatial-temporal dynamics via part-grouped interactions\n$X = X + PGMB(LN(X))$ (1)"}, {"title": "with the PGMB block computing", "content": "$[X_s, X_m, X_t] = Split(Linear(X))$ (2)\n$X_s = SpatialConv(X_S)$ (3)\n$X_m = PGM(X_m)$ (4)\n$X_t = TemporalConv(X_t)$ (5)\n$X = Linear(Concat(X_s, X_m, X_t))$ (6)\nwhere $LN$ is the Layer Normalization operator, and $Split$ and $Concat$ are the channel splitting and concatenation functions. For input $X$, we split it into $X_5$, $X_m$, and $X_t$ with $C/4$, $C/2$, and $C/4$ channels, respectively.\nFollowing a similar spirit to multi-head attention mechanisms, we have $H/4$ parallel $PGM$, $SpatialConv$, and $TemporalConv$ operators, with $H$ denoting the number of heads. The novel $PGM$ layer models spatial-temporal relationships of different body parts in $X_m$. Each $SpatialConv$ is a one-layer GCN with a learnable $(H/4, V, V)$ matrix \u2013instead of a predefined adjacency matrix- that captures diverse joint spatial connectivity patterns in $X_5$. To model temporal dynamics of patterns in $X_t$, every $TemporalConv$ performs a $H/4$-grouped Conv1D filtering operation, with kernel size $k_t$.\nThe second part of the block refines the captured skeletal spatial-temporal dynamics by computing\n$X = X + Linear(GELU(Linear(LN(X))))$\nFFN\n(7)\nwhich corresponds to the output of a TSMB."}, {"title": "3.2. Part-Grouped Mamba (PGM)", "content": "This layer is designed following three key insights: (i) spatial ($SpatialConv$) and temporal ($TemporalConv$) operators model short-range temporal dynamics of the whole body; (ii) different diseases affect specific body parts which exhibit distinct yet interrelated long-range temporal dynamics. Following such intuitions, our novel PGM introduces channel-wise driven scanning and part-based decomposition strategies in State Space Models [19] to efficiently capture long-range local and global motion patterns.\nState Space Models (SSMs) are designed to map a 1D input sequence $x(t) \\in \\mathbb{R}$ to an output $y(t) \\in \\mathbb{R}$ using a hidden state $h(t) \\in \\mathbb{R}^{N}$. Formally, this mapping is governed by the following ordinary differential equations (ODEs):\n$h'(t) = Ah(t) + Bx(t)$ \n$y(t) = Ch(t)$ (8)\nwhere $A \\in \\mathbb{R}^{N \\times N}$ is the system's evolution matrix, and $B \\in \\mathbb{R}^{N \\times 1}$, $C \\in \\mathbb{R}^{1 \\times N}$ are projection matrices. Modern SSMs [19] discretize (8) using the zero-order hold (ZOH) method\n$A = exp(\\Delta A)$ \n$B = (\\Delta A)^{-1}(exp(\\Delta A) \u2013 I) \\cdot \\Delta B$ (9)\nwith timescale parameter $\\Delta$-which can be seen as the resolution of the continuous input x(t)- leading to the discrete state-space equations\n$h_t = Ah_{t-1} + Bx_t$\n$Y_t = Ch_t$ (10)\nthat can be efficiently computed by the convolution\n$K = (CB, C\\bar{A}B, \u2026\u2026\u2026, CA^{\\bar{L}-1}B)$ \n$y = x * K$ (11)\nwhere $L$ denotes the length of the input sequence x and K is the SSM convolutional kernel.\nUnlike traditional linear time-invariant SSMS, Mamba [17] introduces a Selective Scan Mechanism (S6) such that parameters B, C, and $\\bar{A}$ are directly derived from the input data, hence allowing input-dependent interactions along the sequence.\nChannel-Wise Spatio-Temporal SSM (C-2D-SSM): The Mamba architecture has been extended from 1D to 2D bidirectional modeling (e.g., [2, 35, 64]) showing promise in image-related tasks but exhibiting instability when scaled to large parameter spaces [42]. This is due to the Mamba block's extensive input and output projections, whose computational and parametric complexities scale linearly with the input channel dimensionality.\nTo effectively mitigate these issues, we leverage the insight that different channel groups in skeletal data may represent distinct yet complementary aspects of movement. As"}, {"title": "shown in Figure 2, we decompose the input X along the channel dimension to get equally sized tensors", "content": "$[X_{ts}, X_{s\\to t}, X_{t\\gets s}, X_{s\\gets t}] = Split(X)$ (12)\nthat are independently processed by direction-specific 2D-SSMs (i.e., spatial-temporal SSMs), then concatenated back to produce the final output\n$Y = Concat\\begin{pmatrix} SSM_{ts}(X_{ts}),\\\\ SSM_{s\\to t}(X_{s\\to t}),\\\\ SSM_{t\\gets s}(X_{t\\gets s}),\\\\ SSM_{s\\gets t}(X_{s\\gets t}) \\end{pmatrix} = C2DSSM(X)$ (13)\npreserving the input tensor's dimensionality. t and s denote temporal and spatial dimensions, respectively, and the arrows indicate the direction of spatial-temporal scanning.\nThis novel parallel approach captures complementary movement features across channel groups, while significantly reducing the computational complexity by operating on C/4 channel inputs. The direction-specific spatial-temporal scanning mechanism effectively captures diverse contextual information, enhancing the model's capacity to learn both local and global dependencies."}, {"title": "Part-Grouped Modeling: Joint locations change over time depending on the disease type. Different deficits are caused by the involvement of multiple systems and structures (e.g., cerebellar, pyramidal, extrapyramidal). For instance, hereditary paraplegia mostly involves lower body part joints [45], while Parkinson's and Cerbella Ataxia involve the whole body [15]. Motivated by these intuitions, we introduce both part-based and global SSMs to effectively capture fine-grained local motion details at individual body parts' level, while maintaining the global understanding of full-body motion", "content": "We decompose the body keypoints into multiple partitions corresponding to key body parts (arms, legs, torso) and their relevant combinations (arms-legs, arms-torso, torso-legs). By focusing on these specific partitions, we enable our model to capture both localized movements and inter-segment temporal dynamics crucial for disease recognition. For each partition $p \\in \\{1,\u2026\u2026, P\\}$, we apply:\n$X_p = C2DSSM_p(b_p + X[:, I_p])$ (14)\nwhere $I_p$ is the index set, $b_p \\in \\mathbb{R}^{T \\times |I_p| \\times C}$ is a learnable partition token, and $C2DSSM_p$ is our novel C-2D-SSM for partition p.\nThe learnable partition token $b_p$ enables each SSM to learn a specialized representation of the motion dynamics specific to each body part or combination, enhancing the model's ability to differentiate between fine-grained movements. By processing these partitions separately, our model can efficiently capture part-specific temporal patterns, such as the rhythmic alternation of arm and leg movements during walking, or the stabilizing role of the torso in maintaining balance."}, {"title": "To ensure that full-body motion patterns are not missed when focusing solely on individual parts, we capture holistic motion characteristics as $X_9 = C2DSSM(X)$.", "content": "Attentive SSM:The outputs from these specialized SSMs are then integrated through a learnable weighted sum:\n$X_{SSM} = \\Sigma_p \\beta_p X_p + \\beta_g X_g$\n(15)\nwhere $\\beta_p$ and $\\beta_g$ are learnable parameters. This integration strategy allows our model to dynamically adjust the importance of part-specific and global motion information based on the input data.\nTo further refine our feature representations and enhance the model's adaptability, we incorporate a channel attention mechanism after the SSM processing. We first compute\n$w = \\sigma(Linear(GELU(Linear(Pool(X_{ssm})))))$ (16)\nwhere $Pool$ is the spatial-temporal average pooling operator and $\\sigma$ is the sigmoid function. Then, we obtain the output of our novel PGM as\n$PGM(X) = (X_{SSM} + X) \\odot w$ (17)\nwhere $\\alpha$ is a learnable parameter and $\\odot$ denotes the Hadamard product. The residual connection $(+X)$ ensures efficient gradient flow during training, while the channel attention $(w)$ allows our model to adaptively recalibrate channel-wise feature responses [25], focusing on the most informative features for each input sequence."}, {"title": "4. Experiments", "content": "4.1. Datasets\nMedical Diagnosis: Our objective is to provide a method for automated diagnosis of motion-related disorders. To assess the capabilities of our method in such a context, we validate it on one newly collected dataset and a publicly available benchmark. Both datasets present a challenging scenario for automated diagnosis from skeleton action recognition, requiring models to differentiate between multiple neurological conditions and healthy controls based on subtle motion characteristics.\nNeurological Disorders (ND). We collected a new dataset focused on automated diagnosis of neurological disorders. Our dataset comprises 396 video sequences from 40 subjects across four distinct classes: primary degenerative cerebellar ataxia (11 patients, 112 sequences), hereditary spastic paraparesis (12 patients, 105 sequences), idiopathic Parkinson's disease (7 patients, 80 sequences), and healthy controls (10 subjects, 99 sequences). Data collection was"}, {"title": "4.2. Implementation Details", "content": "We trained our model on an NVIDIA L40S GPU using the PyTorch framework for 500 epochs on 128-sample batches, using the AdamW optimizer with weight decay equal to 5e-4. We adopted a linear learning rate warm-up from le-7 to le-3 over the first 25 epochs, followed by a cosine annealing scheduler. We applied clipping for gradients having l2-norm exceeding 1. The label-smoothed cross-entropy loss with $\\alpha$ = 0.1 was used for optimization. For the ND and KOA-PD-NM datasets, we used [54] to obtain the skeleton joints. For others, we used the provided keypoints."}, {"title": "4.3. State-of-the-art Comparison", "content": "We present a comprehensive performance comparison of our method against recent state-of-the-art skeleton-based action recognition methods. Following [11, 74, 75], we considered three different modalities: (i) only joints (Mj) (ii) joints and bones (Mjb); and (iii) joints and bones with motions (Mjbm). We trained a model for each modality and ensembled their outputs."}, {"title": "Medical Diagnosis: Table 1 provides a comparative evaluation of different models on three medical diagnosis datasets Our proposed model achieves the highest accuracy across all datasets and ensemble metrics. For the ND dataset, we reach 99.35% on Mj and Mjb, further improving to 99.64% on Mjbm, demonstrating robust performance in diagnosis of neurological disorders", "content": "In the KOA-PD-NM dataset, we significantly improve other methods reading a 98.62% accuracy using the Mjbm. This indicates a strong capability in classifying grouped severity levels. On the more challenging KOA-PD-NM-Severity dataset, our method better handles fine-grained severity distinctions outperforming all existing models."}, {"title": "Generic Actions: Results in Table 2 show that our noble method achieves leading performance across diverse generic action recognition benchmarks. We consistently outperformed prior methods, demonstrating the effectiveness of our modeling approach. On the NTU RGB+D X-Sub60 dataset, we recorded top scores of 91.8%, 92.8%, and 93.4% across the three considered ensemble strategies, showing the capabilities of our approach in capturing spatial-temporal dependencies. For the more complex NTU RGB+D 120 dataset, our method demonstrates excellent performance by improving over prior GCN and Transformer-based methods. Similarly, on the NW-UCLA dataset, we obtain the highest accuracy at 97.6% on the leaderboard.", "content": "Similar comments can be made for the results obtained comparing our method with state-of-the-art human interac-"}, {"title": "Complexity Analysis: Table 4 provides a comparative computational complexity analysis with existing architectures. Our approach achieves the highest accuracy, recording an average joint modality accuracy of 94.3% and 88.1% on the NTU RGB+D and NTU RGB+D 120 datasets respectively -surpassing all existing methods. While Skel-Mamba exhibits a slightly higher parameter count (6.84M) and FLOPs (9.7G) than some lighter GCN models, its inference time of 7.06 ms shows it is remarkably efficient. This trade-off positions our method as the most accurate and fastest model.", "content": "4.4. Ablation Study\nPart-Grouped Mamba Block Components: Table 5 presents an ablation study on the novel part-grouped mamba block, investigating the contributions of its spatial, temporal, and spatio-temporal components. The ablation of either"}, {"title": "Body partitioning and SSM scanning: Table 6 presents the ablation study of the Part Grouped Mamba (PGM) layer. Transitioning from 1D SSM to C-2D-SSM notably improves performance, highlighting the benefit of spatial-temporal dependency modeling. Incorporating attentive SSM enhances performance by capturing long-range dependencies, while channel attention refines feature representation, yielding additional gains. Altogether, the fully integrated PGM layer achieves the highest accuracy,", "content": "validating its design for robust part-based skeleton action recognition."}, {"title": "5. Conclusion", "content": "We presented a novel skeleton-based action recognition framework that integrates anatomically-aware State Space Models (SSMs) for fine-grained analysis of spatio-temporal motion patterns. Our approach introduces a multi-stream architecture that partitions skeletal data into spatial, temporal, and spatio-temporal streams, enabling efficient and targeted analysis of complex human motions. Through anatomically-guided body part segmentation and a multi-directional scanning strategy, our method captures both local joint dynamics and global motion interactions crucial for applications requiring precision, such as automated medical diagnosis.\nExtensive evaluations on standard benchmarks (NTU RGB+D, NTU RGB+D 120, NW-UCLA) demonstrate our model's superiority by significantly improving over existing methods. On a new medical dataset for gait analysis, our framework also exhibits strong potential for clinical diagnostics, accurately identifying subtle movement patterns indicative of neurological disorders. However, our medical dataset remains limited in size, as expanding it requires complex data-gathering processes constrained by privacy concerns and regulatory restrictions. We are actively working to increase this dataset, but this endeavor remains resource-intensive."}]}