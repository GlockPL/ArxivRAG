{"title": "FROM ENGLISH TO ASIC: HARDWARE IMPLEMENTATION WITH LARGE LANGUAGE MODEL", "authors": ["Emil Goh", "Maoyang Xiang", "I-Chyn Wey", "Tee Hui Teo"], "abstract": "In the realm of Application Specific Integrated Circuit (ASIC) engineering, the landscape has been significantly reshaped by the rapid development of Large Language Model (LLM), paralleled by an increase in the complexity of modern digital circuits. This complexity has escalated the requirements for Hardware Descriptive Language (HDL) coding, necessitating a higher degree of precision and sophistication. However, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications.\nTo address these challenges, a strategy focusing on the fine-tuning of the leading-edge nature language model and the reshuffling of the HDL code dataset has been developed. The fine-tuning aims to enhance models' proficiency in generating precise and efficient ASIC design, while the dataset reshuffling is intended to broaden the scope and improve the quality of training material. The model demonstrated significant improvements compared to the base model, with approximately 10 to 20% increase in accuracy across a wide range of temperature for the pass@1 metric. This approach is expected to facilitate a simplified and more efficient LLM-assisted framework for complex circuit design, leveraging their capabilities to meet the sophisticated demands of HDL coding and thus streamlining the ASIC development process.", "sections": [{"title": "1 Introduction", "content": "The complexity associated with Application-Specific Integrated Circuits (ASIC) has seen a significant upsurge, de-manding a more refined and sophisticated approach to Hardware Description Language (HDL). Modern processors are designed to perform highly specialized functions, and their development requires integrating billions of transistors into a single chip. For instance, the design and fabrication of a cutting-edge chip for a smartphone requires a detailed orchestration of various components such as the CPU, GPU, memory controllers, and connectivity modules (Wi-Fi, Bluetooth, cellular networks) on a microscopic scale. This scenario exemplifies the heightened complexity in ASIC design, demanding a higher degree of precision and sophistication in every aspect of the chip's development, from conceptualization through to physical implementation.\nIn parallel, an Artificial Intelligence (AI) research company called OpenAI shook the world by introducing ChatGPT in Year 2022, a powerful generative AI capable of producing natural language based on text prompts. As the technology has advanced over the past year, generative AI is now capable of generating not only natural language, but also images, sounds, and even code. Since the release of ChatGPT, competition in the field has been intense and rapidly evolving. Major companies like Google, Meta, and Amazon have invested heavily in generative Al technology, with most developing their own large language model (LLM) and integrating the capabilities into their products and services. Such advancements have not only enhanced the capabilities of natural language processing but also underscored their potential to transform a variety of sectors. This convergence of advancements presents a promising field for the application of LLM in ASIC design, offering a novel approach to tackling the intricate challenges of HDL coding.\nNevertheless, the implementation of LLM in HDL coding has confronted significant obstacles, particularly evident in their efficacy in generating precise coding outputs. Beyond the inherent challenges posed by the intricacies of circuit design and the high precision necessitated therein, generative AI models are prone to producing syntax errors within Verilog codes. Such discrepancies become increasingly pronounced in scenarios involving complex design architectures or specific, hardware-oriented syntax. While these errors might appear minor, they possess the potential to engender incorrect hardware behavior or provoke complications during the simulation and compilation phases. This issue is exacerbated by the scarcity of high-quality code datasets and exemplars, as demonstrated by the limited availability of intricate Verilog code examples. These examples are indispensable for the effective training of LLMs, enabling them to accurately comprehend and execute the nuanced requirements essential for ASIC design.\nTo tackle these issues, a two-fold approach has been proposed: fine-tuning of forefront LLMs to improve their efficacy in HDL coding, coupled with the expansion of the coding dataset to encompass a more diverse array of coding examples and complexities. This paper endeavors to explore the application of generative AI in the production of HDL, with a particular focus on Verilog. In pursuit of this objective, the Mistral 7B Large Language Model (LLM) will undergo a process of fine-tuning, aimed at augmenting its proficiency in generating Verilog code with heightened effectiveness[1].\nThe organization of this paper follows a logical progression, starting with an exploration of the current developments in LLMs in Section 2. The following section details our methodology for fine-tuning the model specific to HDL. Section 4 evaluates the effectiveness of the proposed model, which is followed by summarizing the advantages and limitations of our approach in Section 5. This structure aims to comprehensively present our innovative approach towards leveraging LLM technology for a more efficient and simplified process in ASIC design."}, {"title": "2 Literature Review", "content": "Recent advancements in the field of AI, specifically in LLMs have shown remarkable capabilities in understanding and generating human-like text. Among these, Mistral 7B, a 7-billion-parameter LLM, presents a promising avenue for application in various domains, including the generation of HDL code, which is crucial for modern chip design[1]. This section review explores the background, LLM, and the potential of fine-tuning Mistral 7B to generate Verilog code, a popular HDL used in digital circuit design."}, {"title": "2.1 Generative AI and LLMs", "content": "Generative AI encompasses a broad spectrum of AI algorithms designed for the creation of novel content through the processing of extensive training datasets. Within this domain, LLMs stand out as a pivotal subset, providing a remarkable capability for generating text that is both coherent and contextually relevant. Its transformative impact on the field was notably augmented by the introduction of the transformer architecture and the self-attention mechanism by Google in 2017[2]. The transformer, a neural network architecture that relies exclusively on self-attention, has significantly advanced language understanding capabilities. It has demonstrated superior performance over traditional recurrent and convolutional models across various translation benchmarks. Its encoder-decoder configuration architecture maps input sequences to continuous representations, which are then processed by the decoder to generate output sequences in an auto-regressive fashion. This means each subsequent element is produced based on the preceding elements, leveraging layers of self-attention and fully connected layers for efficient operation.\nThe advent of transformer architecture has catalyzed the development of Generative AI, marking a departure from the predictable outcomes of conventional deterministic AI algorithms. As a form of stochastic AI, Generative AI thrives on the diversity of training data spanning multiple domains, enabling the generation of inventive content across text, images, audio, and synthetic data realms that align with the input prompt's context.\nAn example of innovation within this space is the Mistral 7B model, developed by Mistral AI. This 7-billion-parameter, open-source language model has been designed to balance high-level performance with efficient inference, claiming to outperform larger models from notable entities like Meta in various benchmarks. Notably, it has excelled in generating functional code and solving mathematical tasks, surpassing the LLaMa 34B model. Mistral 7B leverages advanced attention mechanisms, such as grouped-query attention (GQA) and sliding window attention (SWA), to enhance inference speed and reduce memory requirements for decoding, effectively handling long sequences with lower computational demand[3, 4, 5]. The Mistral 7B model builds on the transformer architecture."}, {"title": "2.2 Fine Tuning of Large Language Model", "content": "Fine-tuning involves additional training on a pre-trained model with a smaller dataset to perform a specific task. This section highlights the different concepts and methods that are essential for the fine-tuning of LLM."}, {"title": "2.2.1 Transfer Learning", "content": "Transfer learning is the crucial technique for pre-trained models to be re-purposed on another related task with minimal additional training through the transfer of knowledge [6]. This technique enables existing trained models to be used as a foundation for another task's development.\nUsing the method of transfer learning, enormous resources that are required to train LLM could be saved. However, transfer learning only works if the initial training task is general [7]. This form of learning mechanism by improving performance through the learning of a different but related knowledge is also known as inductive transfer learning."}, {"title": "2.2.2 Low-Rank Adaptation (LoRA)", "content": "LLMs are known to contain billions of parameters and it is not efficient and effective to train LLMs from scratch during fine-tuning. Low-Rank Adaptation or LoRA [8] is a technique used to accelerate the fine-tuning of pre-trained models with minimal computational overhead and memory consumption.\nThe LoRA approach freezes the original pre-trained weights, identifies adaptation points, and introduces low-rank matrices into the different layers of the model's architecture. These matrices can later be trained with the new data while keeping the original weights frozen. The original and adapted weights are then merged to produce the fine-tuned model. The LoRA approach is the common method of fine-tuning and is used in the Parameter Efficient Fine-Tuning (PEFT) library from Hugging Face."}, {"title": "2.2.3 DeepSpeed ZeRO", "content": "DeepSpeed is a library designed to optimize training processes and facilitate the fitting of large models onto GPUs[9]. It is powered by Zero Redundancy Optimiser (ZeRO), which is an algorithm used to enhance communication between GPUs. It also addresses common challenges faced by LLM fine-tuning such as memory constraints and slow training times.\nThe work presented in this paper utilizes DeepSpeed ZeRO Stage 3 offloading capabilities [10]. This enables the training of very large models on limited GPU resources by leveraging on the GPU, CPU, and NVMe to enable scalability."}, {"title": "2.2.4 Quantization", "content": "Quantization techniques are used commonly in the training and inference of LLMs. These methods reduce memory usage and computational cost by representing parameters at lower precision. Utilizing quantized data types allows users to load larger models into limited memory space and speed up model inference. As LLMs are massive in size, half-precision floating point format (FP16) is commonly used for training and inference. It uses 16 bits instead of the usual 32 bits (FP32) for single precision.\nIn this section, various mechanisms that enable the LLMs and generative AI were introduced. On top of that, methods to optimize fine-tuning processes of LLMs were discussed which lay a solid foundation for the proposed language model for LLM."}, {"title": "3 Method", "content": "Moving on to the LLM and generative AI portion of the implementation flow, this section looks into the fine-tuning of existing LLM to generate HDL, specifically Verilog."}, {"title": "3.1 Dataset", "content": "The lack of labels in existing datasets causes less quality LLM to be produced and hinders many future developments. This includes the utilization of fine-tuning methods such as Direct Preference Optimisation (DPO). To solve this issue, OpenAI's GPT-3.5 Turbo was employed to label the unlabelled datasets found on Hugging Face's repositories.\nTo begin, a Hugging Face dataset repository was chosen and processed before deploying the GPT-3.5 Turbo API to label the data entries. In this case, a Verilog dataset, wangxinze/Verilog_data [11], was selected for processing as the dataset consists of a single module per entry but is not well-labelled.\nThe dataset, in CSV format, was first converted into a data frame using the Pandas Python data analysis library for easier manipulation. Using Python's regular expression operation, the modules within each entry are extracted such that each entry would start with 'module' and end with 'endmodule'. Duplicates and empty entries are removed to reduce biases when training LLM. Next, each module's name was extracted and placed in a separate column, corresponding to the rows, for annotation. The GPT-3.5 Turbo API was finally deployed after this step to generate a short description for each of the data entries.\nThe result of this process is a Verilog dataset with 68,122 data entries that are written by humans. This is by far the largest labeled Verilog dataset available on Hugging Face and probably online."}, {"title": "3.2 Training Process", "content": "Mistral 7B is selected for fine-tuning as it is open-source and capable of supporting code[1]. To optimize the training process, some measures and strategies were taken."}, {"title": "3.2.1 Checkpoint and Evaluation", "content": "Unfortunately, there is no good solution to evaluate code generated automatically at this point. Evaluating the generated code would involve the calling of an external simulator to run the code and check for errors. This is complicated to implement when training is going on at the same time. Hence, training checkpoints are used such that the model can be evaluated from time to time at different training steps.\nFor this work, checkpoints were saved and evaluated at steps 10,000, 20,000, 40,000, and 60,000. At these checkpoints, inferences were carried out and 10 samples were generated with a temperature of 0.2 for each problem in the VerilogEval-Machine benchmark descriptions. Verilog modules in the generated samples were roughly extracted and evaluated using the VerilogEval-Machine benchmark. Although the data were not carefully cleaned, it gives a rough gauge of the model's accuracy at the particular step and warns of an unlikely case of overfitting.\nApart from evaluations, implementing training checkpoints also prevents training progress from being lost due to runtime errors. With this measure in place, users can retrieve trained data from the last saved checkpoint and continue with the training process instead of starting over."}, {"title": "3.2.2 DeepSpeed ZeRO", "content": "Fine-tuning a large language model requires large RAM space. DeepSpeed ZeRO [9] was implemented during training to allow the checkpoint weights to be offloaded to the CPU which has more RAM space but lower latency. The offloaded weights are then moved from the CPU to the GPU, which has less RAM space but high latency, as the input reaches the different layers. This helps optimise memory utilization in the system and prevent runtime errors due to the lack of memory space to occur."}, {"title": "3.3 Inference Strategy", "content": "The fine-tuned model is later deployed on Google Colaboratory's A100 GPU again and used for inference to generate samples for evaluation. Just like training, model inference takes time and requires high VRAM. Hence, Google Colaboratory was used to gain access to NVIDIA's A100 GPU. The following are some strategies adopted to accelerate the speed of inference and optimize memory usage including caching, flash attention, and token limits."}, {"title": "3.3.1 Caching", "content": "Caching during inference enables the model to save and reuse intermediate hidden states from previous time steps. As the generation of new tokens depends on what was generated previously, caching enables the model to avoid recomputation and speeds up the inference process. By enabling cache, memory usage for inference could also be optimized. Large, duplicated intermediate states do not have to be stored again and could benefit when generating long sequences."}, {"title": "3.3.2 Flash Attention", "content": "Flash attention [12] is enabled to reduce the time taken to generate the samples. It is used to optimize transformer-based models specifically by improving the operations of the attention mechanism.\nHigh bandwidth memory (HBM) has high memory capacity, but is slow in processing. On the other hand, SRAM has a fast processing speed, but smaller memory capacity. Hence, the standard attention mechanism often uses HBM to store, read, and write keys, queries, and values, and the GPU on-chip SRAM to perform the attention mechanism. The process involves the loading of the elements from the HBM to the SRAM, and writing the results back to the HBM, and is repeated for every attention step.\nFlash attention optimizes the operation by loading the aforementioned elements only once, reducing the writing and loading process. It fuses the operations of the attention mechanism and finally writes the results back after the completion of the fused operation."}, {"title": "3.3.3 Token limits", "content": "Lastly, the token limit of the generated samples is limited to 500 tokens. From early experiments, it was found that the LLM would sometimes generate the corresponding testbench which is not needed for evaluation and could result in difficult extraction of the targeted Verilog code. By limiting the number of generated tokens, the excessive generation of unwanted tokens after the sample Verilog code is prevented. This also speeds up the generation of the samples for evaluation later."}, {"title": "3.4 Post-Processing", "content": "After the generation of samples, regular expression operations in Python are used to extract the Verilog module. The operation searches the sample for the keywords, 'module' and 'endmodule', before extracting the text including the two keywords. Next, the keywords, 'module' and ';', are searched for and removed from the extracted text. The final output will be the module's logic ending with 'endmodule'. This is the format required for VerilogEval, which is the benchmark used for evaluation.\nThis section highlights the processes and measures taken in the labeling of the dataset containing Verilog code using GPT-3.5 API, the fine-tuning of Mistral 7B to generate Verilog code, and the inference process of the model to generate samples for further evaluation."}, {"title": "4 Result", "content": "This section evaluates the fine-tuned model's ability to generate Verilog code. It begins with an overview of the VerilogEval benchmark, employed for this assessment[13]. The model's performance is scrutinized at various stages and its final output through the VerilogEval-Machine benchmark[13]. A comparison with similar models in the field provides a comprehensive analysis. Lastly, an examination of selected code samples from both the proposed and foundational models identifies errors, offering insights for future improvements."}, {"title": "4.1 VerilogEval", "content": "VerilogEval [13] is a benchmark for LLMs to evaluate the capability of Verilog code generation by a research team from NVIDIA. It contains two parts to the evaluation set - Machine and Human. Both evaluation sets contain problems from HDLBits [14] which is a website with digital circuit design exercises to learn Verilog HDL.\nAs questions on the HDLBits website are in the form of images, diagrams, and tables, they are not compatible with non-multimodal LLMs. Thus, gpt-3.5-turbo was employed by the research team to automate the generation of the problem descriptions according to each problem's answer. This forms the description-answer pairing for VerilogEval-Machine. The final output was a total of 143 problems available for evaluation.\nSimilarly, VerilogEval-Human consists of problems from HDLBits like VerilogEval-Machine. However, in this case, the researchers manually examined the problem descriptions on the website and converted them into text. VerilogEval-Human contains a total of 156 problems, 13 more problems than VerilogEval-Machine as the conversion was done manually.\nAs mentioned before, VerilogEval uses the same evaluation metric as HumanEval but with fewer generated samples per problem for evaluation. In VerilogEval, the number of generated samples for evaluation is 20, while HumanEval uses 200 generated samples for evaluation. Apart from that, the VerilogEval benchmark requires the use of iVerilog for the simulation of the generated samples."}, {"title": "4.2 Evaluation with VerilogEval-Machine", "content": "VerilogEval[13] consists of two sections: Machine and Human. For the purpose of this study, VerilogEval-Machine is used for evaluation. VerilogEval-Machine is particularly valuable in assessing a model's capability in comprehending instructions and generating Verilog code that is functional and syntactically correct. Also, the dataset used for training does not contain text or code that is produced extensively by GPT-3.5, thus minimizing the chances of having positive results due to similarities in questions and answers within VerilogEval-Machine's problem sets."}, {"title": "4.2.1 Evaluation at Checkpoints", "content": "The model is evaluated at different checkpoints to observe its performance against training loss. However, the evaluation of generated samples is just a gauge and not accurate. The evaluation results of each checkpoint step can be seen in Table 3, where the metrics pass@k is used to evaluate the model's performance, specifically in tasks such as code generation or answering questions which is introduced by HumanEval[15]. These metrics measure the model's ability to produce correct or satisfactory answers within a specified number of attempts or samples. It uses the pass@k metric to evaluate functional correctness. Equation 1 is used to represent the evaluation metric.\npass@k := \\sum_{problems} [1 - (\\binom{n}{k})^{-1}\\sum_{i=1}^{c}\\binom{n}{i}]\nWhere:\n\u2022 n: the number of samples generated per task\n\u2022 c: the number of samples that pass test, c \u2264 n\n\u2022 k: evaluation budget\nUsing the pass@k metric, a problem is marked as solved if at least one of the generated samples passes the test within the evaluation budget."}, {"title": "4.3 Case study of generated samples", "content": "Erroneous responses from LLM are expected. Through different case studies, future work and improvements can be made to the dataset, training process, or inference process of the model. Different case studies also demonstrate that the final accuracy of the different models shown in the later part of this section might be less than the models' true accuracy."}, {"title": "4.3.1 Erroneous Response by Proposed Model", "content": "Case studies of erroneous responses generated by the proposed model are highlighted in this sub-section.\nAmong the samples generated by the proposed model, a common error encountered was the omission of the 'endmodule' keyword at the end of module declarations. In the example below, the generated Verilog code for a 'NOT' gate would have been considered correct if it had included the 'endmodule' keyword at the end. Apart from that, the lack of 'endmodule' keyword results in difficulties extracting the Verilog modules for evaluation during post-processing.\nThe fine-tuned model is also susceptible to hallucination and produces nonsensical outputs in some cases. Below is an example where the comment, 'Verilog code', is repeated redundantly multiple times without generating meaningful code. Similar issues were also observed in the generated samples of the base model and will be highlighted in the later sub-section.\nAnother common issue is the model attempting to instantiate undefined modules to solve particular problems. This phenomenon leads to a syntax error when the code undergoes simulation."}, {"title": "4.3.2 Erroneous Response by Mistral 7B", "content": "Next, common erroneous responses by the base model are presented.\nOne common type of erroneous response by the base model is syntax errors. As the base model has not been exclusively trained on Verilog codes, it could confuse the syntax rules of Verilog with those of other languages. Below is an example of a generated sample with syntax error by the base model. In this example, '#' is included in every line which caused an error.\nIn some cases, despite the clear instruction to generate a Verilog module, the generation of code in other programming languages was observed. The following illustrates an instance of such case. In this example, the base model produced a function in the C language instead of Verilog."}, {"title": "5 Conclusion", "content": "This work also produced possibly the first labeled Verilog dataset with code written by humans. This dataset is also likely to be the largest labeled Verilog dataset available on Hugging Face, containing a total of 68,122 data entries. Meanwhile, this research showcased a model that is fine-tuned from Mistral 7B to generate Verilog code. Through the evaluation, the model achieved approximately 15 to 20% improvement in accuracy for pass@1 at various temperatures compared to the base model.\nResults have shown that there are still imperfections in the fine-tuned model and more can be done to improve the quality of Verilog code produced. Hallucinations still persist in the generated samples. A potential solution could be shortening the context length of the dataset. As such, the attention mechanism could better manage the relationship between the label and the different parts of the input. Shortening the context length would also lead to fewer entries in the dataset, thus allowing more training epochs to be done during fine-tuning.\nAn advantage of having a labelled training dataset is that data can be filtered more easily and fine-tuning efforts could be more targeted. In this study, the model was fine-tuned to address diverse circuit types and problem sets. By organizing and categorizing the dataset according to the circuits' functionality, the fine-tuned model could be specialized at handling specific circuit types and possibly yield more functional and valid results.\nApart from that, the proposed fine-tuned model in this work saw little to no improvements for problems that require more logical reasoning. This issue could be resolved by integrating the model with retrieval augmented generation (RAG) which is a database that LLMs could refer to for few-shot learning.\nLastly, although the fine-tuned model is published on Hugging Face, it is at the moment unable to be deployed locally and used in chat mode. Quantizing and converting the model into the GPT-Generate Unified Format (GGUF) would allow users to run the model locally and design circuits through conversation. This also enables more functionally correct and possibly complicated circuits to be generated as few-shot and chain-of-thought prompting could be utilized.\nThe proposed implementation flow in this work leverages the latest advancements in LLM and generative AI. While these technologies may already seem to be powerful, generative AI is still in its early phase of development and this is just the beginning of a new technological revolution. There are still many possibilities and much more work to be done in bridging NLP and IC design."}]}