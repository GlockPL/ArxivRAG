{"title": "Self-Supervised Anomaly Detection in the Wild: Favor\nJoint Embeddings Methods", "authors": ["Daniel Otero", "Rafael Mateus", "Randall Balestriero"], "abstract": "Accurate anomaly detection is critical in vision-based infrastructure inspection,\nwhere it helps prevent costly failures and enhances safety. Self-Supervised Learn-\ning (SSL) offers a promising approach by learning robust representations from\nunlabeled data. However, its application in anomaly detection remains underex-\nplored. This paper addresses this gap by providing a comprehensive evaluation of\nSSL methods for real-world anomaly detection, focusing on sewer infrastructure.\nUsing the Sewer-ML dataset, we evaluate lightweight models such as ViT-Tiny\nand ResNet-18 across SSL frameworks, including BYOL, Barlow Twins, SimCLR,\nDINO, and MAE, under varying class imbalance levels. Through 250 experiments,\nwe rigorously assess the performance of these SSL methods to ensure a robust\nand comprehensive evaluation. Our findings highlight the superiority of joint-\nembedding methods like SimCLR and Barlow Twins over reconstruction-based\napproaches such as MAE, which struggle to maintain performance under class\nimbalance. Furthermore, we find that the SSL model choice is more critical than\nthe backbone architecture. Additionally, we emphasize the need for better label-\nfree assessments of SSL representations, as current methods like RankMe fail to\nadequately evaluate representation quality, making cross-validation without labels\ninfeasible. Despite the remaining performance gap between SSL and supervised\nmodels, these findings highlight the potential of SSL to enhance anomaly detection,\npaving the way for further research in this underexplored area of SSL applications.", "sections": [{"title": "Introduction", "content": "Self-Supervised Learning (SSL) is a machine learning paradigm where models are trained on\nunlabeled data by creating surrogate labels through pretext tasks that exploit inherent structures or\npatterns within the data. As a result, this approach enables learning meaningful representations that\ncan be applied to various downstream tasks without the need for explicit manual labeling [4]. Because\nof this, SSL is particularly advantageous for semi-supervised anomaly detection problems where\nobtaining labeled data is costly, labor-intensive, impossible, or undesirable [1]. Despite these benefits,\nanomaly detection is frequently underrepresented in SSL research, with recent large-scale ablation\nstudies often neglecting its inclusion in benchmarking [10, 2]. In fact, common benchmarks such\nas ImageNet [7] and CIFAR [16], are object-centric and do not accurately reflect the complexity of\nreal-world environments, where images are more diverse and less structured [11].\nCloser to the anomaly detection scenario, recent works have started to stress-test SSL on more\\orealistic scenarios with uncurated data. Albeit still employing a classification task evaluation, it\nhas been shown that SSL may be sensitive to the quality of the data and in particular to balance"}, {"title": "Controlled evaluation of self-supervised anomaly detection", "content": "We will first describe our methodology in detail and then provide results and discussions at the end of\nthis section."}, {"title": "Methodology", "content": "Prior to conducting the SSL ablation study, an initial hyperparameter search was performed to optimize\nthe data augmentation pipeline for the dataset. The focus was placed on tuning image resolution and\naugmentation settings to achieve a balance between model performance and computational efficiency.\nDetailed results and configurations are provided in Appendix A.\nMethods and models. Our study conducts an ablation analysis on anomaly detection using self-\nsupervised learning methods, with a particular emphasis on their robustness to distribution imbalances.\nWe primarily focus on joint-embedding architectures-specifically Barlow Twins, SimCLR, BYOL,\nand DINO [19, 6, 12, 5]\u2014which aim to learn an embedding space by aligning representations of\ndifferent augmented views of the same input while avoiding collapse. To provide a comparative\nperspective, we also evaluate Masked Autoencoders (MAE) [14], a self-supervised approach that\nreconstructs missing parts of the input data. For our backbone architectures, we use lightweight"}, {"title": "Results", "content": "We present in Table 1 the performance of various SSL models compared to their supervised coun-\nterparts. Training was conducted on imbalanced data, while validation adhered to a balanced set,\naligning with benchmark standards. Although previous studies have noted that CNN-based architec-\ntures like ResNet-18 often outperform vision transformers (ViTs) [10], our results do not indicate a\nsignificant difference in performance between these backbones architectures."}, {"title": "Robustness of joint-embedding methods.", "content": "When examining individual SSL methods, SimCLR\nachieves the best overall results especially when paired with ResNet-18 and defect proportion is\nhigher than 5%. When working with stronger distribution imbalances, particularly with 1%, BYOL\nand Resnet-18 have the best results. This might be due to the training dynamics behind these methods,\nin situations with moderate imbalance (like 5%), the contrastive approach of SimCLR can still\nadequately separate the minority class from the majority, leveraging the discriminative power of the\ncontrastive loss. This can be counter-productive when working with extremely imbalance levels.\nBYOL avoids this issue by not requiring explicit negative sampling, allowing it to maintain more\nconsistent performance even when the class imbalance becomes more extreme. Thus, while SimCLR\nthrives with moderate imbalance, BYOL proves more resilient in handling extreme class disparities.\nAnother interesting insight is Barlow Twins' competitive performance under extreme imbalance. Its\nuse of redundancy reduction loss, which both maximizes similarity between augmented views and\ndecorrelates the learned representations, may help avoid the pitfalls of overfitting to the dominant\nclass. DINO, however, shows a unique trend, performing poorly in 1% and 2% settings with ResNet-\n18, but not with ViT-Tiny. This could be attributed to DINO's focus on global feature learning through\nknowledge distillation, which aligns better with ViT's global attention approach."}, {"title": "Failure of reconstruction-based methods.", "content": "Finally, the Masked Autoencoder (MAE) delivers the\nweakest performance across all scenarios, especially at higher imbalance levels (e.g., 45%). MAE's\nreliance on reconstructing multiple classes might introduce noise that hampers its ability to generalize\nunder severe imbalance. In highly imbalanced data, MAE could face challenges in differentiating\nbetween common and rare classes, possibly due to the reconstruction bias that favors the majority\nclass. This might also be tied to the model's neural capacity, which can struggle to produce robust\nrepresentations when not sufficiently overparameterized."}, {"title": "Conclusions and discussion", "content": "Our study indicates that self-supervised learning (SSL) is effective for anomaly detection and remains\nrobust even when facing significant distribution imbalances. We also find that the choice of backbone\narchitecture is not the most critical factor in model performance, as neither ViT-Tiny nor ResNet-18\nconsistently outperforms the other across all cases. In contrast, the selection of the SSL methodology\nsignificantly impacts performance, with substantial variations observed among different SSL model\nfamilies. Therefore, for practitioners, choosing the appropriate SSL family is more crucial than\nselecting a specific backbone architecture. Furthermore, there is a pressing need to accurately measure\nthe quality of representations produced by SSL models. Our findings indicate that the RankMe metric\nis ineffective for this purpose; it aims to assess the richness of representations but fails to correlate\nwith actual performance. As shown in Appendix D, there is no correlation between performance and\nRankMe metrics, underscoring the necessity for better methods of evaluating representations quality."}]}