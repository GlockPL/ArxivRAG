{"title": "XSPN: Characteristic Interventional Sum-Product Networks for Causal Inference in Hybrid Domains", "authors": ["Harsh Poonia", "Moritz Willig", "Zhongjie Yu", "Matej Ze\u010devi\u0107", "Kristian Kersting", "Devendra Singh Dhami"], "abstract": "Causal inference in hybrid domains, characterized\nby a mixture of discrete and continuous variables,\npresents a formidable challenge. We take a step\ntowards this direction and propose Characteristic\nInterventional Sum-Product Network (XSPN) that\nis capable of estimating interventional distributions\nin presence of random variables drawn from mixed\ndistributions. XSPN uses characteristic functions in\nthe leaves of an interventional SPN (iSPN) thereby\nproviding a unified view for discrete and continu-\nous random variables through the Fourier-Stieltjes\ntransform of the probability measures. A neural\nnetwork is used to estimate the parameters of the\nlearned iSPN using the intervened data. Our ex-\nperiments on 3 synthetic heterogeneous datasets\nsuggest that XSPN can effectively capture the inter-\nmentional distributions for both discrete and contin-\nuous variables while being expressive and causally\nadequate. We also show that XSPN generalize to\nmultiple interventions while being trained only on\nsingle intervention data.", "sections": [{"title": "1 INTRODUCTION", "content": "Most real-world data, irrespective of the underlying domain,\nconsists of variables originating from multiple distributions\nsuch as continuous, discrete and/or categorical. In the realm\nof statistical modeling, understanding and accurately char-\nacterizing such data poses formidable challenges. Mixed\ndistributions, arising from the amalgamation of distinct sub-\npopulations within a dataset, exhibit a complexity that tra-\nditional statistical methodologies often struggle to capture.\nThis can lead to machine learning models becoming either\ninapplicable or producing incorrect results during inference.\nThis applies not only to correlation-based methods but can\nalso have adverse effects on causality-based methods.\nCausal inference [Pearl, 2009, Spirtes, 2010], the study\nof cause-and-effect relationships, is a fundamental pursuit\nin statistics, yet its application to mixed distributions in-\ntroduces unique challenges. Whether the data stems from\ndiverse demographic groups, heterogeneous environments,\nor multifaceted systems, the ability to infer causal relation-\nships in presence of mixed distributions is crucial for ad-\nvancing our understanding of causal phenomena in various\nreal-world domains.\nSeveral probabilistic methods, such as hybrid Bayesian net-\nworks [Monti and Cooper, 1998, Murphy, 1998], Gaussian-\nIsing mixed model [Lauritzen et al., 1989, Cheng et al.,\n2017] and variants of Markov random fields [Fahrmeir and\nLang, 2001, Fridman, 2003], have been proposed to han-\ndle hybrid domains. A major drawback of these methods is\nthe difficulty of inference [Lerner and Parr, 2001] as it can\nquickly become intractable. This becomes a glaring issue in\nwidespread adoption of these methods for causal inference\nwhich in itself is a challenging problem [Peters et al., 2017].\nTo overcome the problem of intractable inference, a lot of"}, {"title": "2 PRELIMINARIES & RELATED WORK", "content": "Before diving into the proposed XSPN model, we present\nsome necessary background on SPNs, causal models and\ncharacteristic functions."}, {"title": "2.1 SUM-PRODUCT NETWORKS", "content": "Sum-Product Networks [Domingos and Poon, 2012] are a\nclass of deep tractable models, which belong to the family of\nprobabilistic circuits. SPNs facilitate a wide range of exact\nand efficient inference routines. In particular, marginalisa-\ntion and conditioning can be done in time which is linear\nin the size of the network [Zhao et al., 2015, Peharz et al.,\n2015]. Formally, an SPN is a rooted directed acyclic graph,\ncomprising of sum, product and leaf (or distribution) nodes\nto encode joint probability distributions p(X). Given an\nSPN $S = (G, w)$ with positive parameters $w$ and a DAG\n$G = (V, E)$, the values at sum (S) and product (P) nodes\ncan be computed by\n$S(x) = \\sum_{C\\in ch(S)} w_C C(x)$\t$P(x) = \\prod_{C\\in ch(P)} C(x)$ (1)\nwhere ch(P) are the children of P. The SPN outputs are\ncomputed at the root node, $S_R(x)$. The scope of a leaf node\nis the random variable X that it models. The scope of an\ninternal node is the union of scopes of all its children. SPNs\nsatisfy the properties of completeness and decomposability.\nAn SPN S is complete if for every sum node u in S the\nscopes of its children are all the same. An SPN S is decom-\nposable if for every product node u in S the scopes of its\nchildren are pairwise disjoint.\nGated or Conditional SPNs are deep tractable models for\nestimating multivariate conditional densities $p(Y|x)$ [Shao\net al., 2022], by conditioning the parameters of vanilla SPNs\non the input using DNNs as gate functions. They introduce\ngating nodes where the weights $g_i(X)$ are parameterized by\nthe provided evidence X to encode functional dependencies\non the input."}, {"title": "2.2 CAUSAL MODELS", "content": "Structural Causal Models provide a framework to formalize\na notion of causality via graphical models [Pearl, 2009].\nDefinition 2.1 (SCM). A structural causal model is a tu-\nple M := (V, U, F, Pu) over a set of variables X =\n{X1,..., XK} taking values in X = \u03a0k\u20ac{1...K} Xk sub-\nject to a strict partial order <x, where\n\u2022 V = {X1,...,Xn} \u2286 X, N < K is the set of en-\ndogenous variables,\n\u2022 U = X \\ V = {XN+1, ..., XK} is the set of exoge-\nnous variables,\n\u2022 F = {1, ..., fn } is the set of deterministic structural\nequations, i. e. Vi := fi(X') for Vi \u2208 V and X' C\n{X; \u2208 X|X; <x Vi},\n\u2022 Pu is the probability distribution over the exogenous\nvariables U."}, {"title": "2.3 CHARACTERISTIC FUNCTIONS", "content": "Characteristic functions (CF) provide a unified view for\ndiscrete and continuous RVs through the Fourier-Stieltjes\ntransform of their probability measures. Let X \u2208 Rd be a\nrandom vector, the CF of X for t \u2208 Rd is given as:\n$\\varphi_X(t) = \\mathbb{E} \\left[ \\exp(it^T X) \\right] = \\int_{x \\in R^d} \\exp(itx) \\mu_X(dx),$ (3)\nwhere $\u00b5_X$ is the distribution/probability measure of X. The\nfollowing properties of CFs are relevant for the remaining\ndiscussion:\n1. $\u03c6_X (0) = 1$ and $|\u03c6_X(t)| \u2264 1$\n2. any two RVs $X_1$ and $X_2$ have the same distribution iff\n$\u03c6_{X_1} = \u03c6_{X_2}$\n3. two RVs $X_1, X_2$ are independent iff $\u03c6_{X_1,X_2} (s, t) =$\n$\u03c6_{X_1} (s)\u03c6_{X_2} (t)$\nWe refer to Sasv\u00e1ri [2013] for more details of CFs.\nTheorem 2.2 (L\u00e9vy's inversion theorem [Sasv\u00e1ri, 2013]).\nLet X be a real-valued random variable, \u00b5x its probability\nmeasure, and $\u03c6_X : R \u2192 C$ its characteristic function. Then\nfor any a, b \u2208 R, a < b, we have that\n$\\lim_{T \\to \\infty} \\frac{1}{2\\pi} \\int_{-T}^{T} \\frac{\\exp(-ita) - \\exp(-itb)}{it} \\varphi_X(t) dt$\n$= \\mu_X [(a, b)] + \\frac{1}{2} (\\mu_X (a) + \\mu_X (b)),$ (4)\nand, hence, $\u03c6_X$ uniquely determines $\u00b5_X$.\nCorollary 2.3. If $\\int_{R} |\\varphi_X(t)| dt < \\infty$, then X has a contin-\nuous probability density function $f_X$ given by\n$f_X(x) = \\frac{1}{2\\pi} \\int_{R} \\exp(-itx) \\varphi_X(t) dt.$ (5)\nNote that not every probability measure admits an analytical\nsolution to Eq. 5, e.g., only special cases of a-stable distri-\nbutions have a closed-form density function [Nolan, 2013],\nand numerical integration might be needed."}, {"title": "3 XSPN", "content": "We build upon the construction of interventional sum-\nproduct networks (iSPN) by Ze\u010devi\u0107 et al. [2021]. We esti-\nmate $p (V_i | do (U_j = u_j))$ by learning a function approxi-\nmator $f (G; \u03b8)$ (e.g. neural network), which takes as input\nthe (mutilated) causal graph $G \u2208 {0,1}^{N\u00d7N}$ encoded as\nan adjacency matrix, to predict the parameters $\u03c6$ of a SPN\n$g(D; \u03c6)$ that estimates the density of the given data matrix\n${V}_1^K = D \u2208 R^{K\u00d7N}$.\nWhen the iSPN is trained end to end on the log likelihood of\nthe training data, the log densities computed at leaves mod-\neling both discrete and continuous variables are propagated\nup the network to the root of the SPN. When a common\nclass of leaves is used, say one parameterized by a normal\ndistribution, it acts as a suboptimal way to model discrete\nvariables that do not quite fit this class of normals. Even\ndifferent distributions at the leaves may not fully be able\nto capture the joint distribution of a heterogeneous group\nof variables, since a sum-product combination of different\nkinds of discrete and continuous densities is likely to re-\nsult in some variables overshadowing the others in the value\ncomputed at the root of the SPN. Moreover, we are restricted\nto using only those parametric distributions at the leaves\nthat have a closed form density function. This is true only"}, {"title": "3.1 XSPN STRUCTURE", "content": "Inspired by Yu et al. [2023], we modify our iSPN to learn\nthe characteristic function $\u03c6_X(t)$ of the joint density. To\nthis end, we make the leaves of the network learn the CF\nof a univariate distribution, to model a particular random\nvariable. We modify the calculations at the product and sum\nnodes as follows.\nProduct Nodes. Decomposability of XSPN implies that\na product node encodes the independence of its children.\nLet X and Y be two RVs. Following property (3) of CFs,\nthe CF of X, Y is given as $\u03c6_{X,Y}(t,s) = \u03c6_X(t)\u03c6_Y(s)$, if\nand only if X and Y are independent. Therefore, since the\nchildren of a product node all have different scopes, with\nt = UN\u2208ch(P) tsc(N), the characteristic function of product\nnodes is defined as:\n$\u03c6_P(t) = \\prod_{N \\in ch(P)} \\varphi_N (t_{sc(N)}),$ (6)\nwhere sc denotes the scope of a node."}, {"title": "3.2 EXPRESSIVITY", "content": "The shared parameters $\u03c6$ of the XSPN allow learning of\nthe joint distribution for any dataset D\u011c conditioned on the\nmutilated causal graph G, that contains information about\nthe interventions. Neural networks have been shown to act\nas causal sub-modules e.g. Ke et al. [2019] used a cohort of\nneural nets to represent a set of structural equations which\nin turn represent an SCM, providing grounding to the idea\nof having parameters being estimated from f.\nThe XSPN also can model any interventional distribution\n$p_{G} (V | do (U))$, permitted by an SCM through interven-\ntions to construct the mutilated causal graph G by mod-\nelling the conditional distribution $p_\u011c (V | U)$. This follows\nfrom Pearl [2009] since $p_{G} (V_i = v_i | do (U_j = u_j)) =$\n$p_{\u011c} (V_i = v_i | U_j = u_j)$. The SPN can learn the joint prob-\nability $p(X_1... X_n)$ on the D\u011c generated post-intervention\nand is thus causally adequate. The question of availability of\nexperimental data is an orthogonal one. While in many ap-"}, {"title": "3.3 LEARNING", "content": "The XSPN is learned from a set of mixed distribution hetero-\ngeneous samples generated from simulating interventions\non the underlying SCM. Instead of maximising the log-\nlikelihood, which is not guaranteed to be tractable, we aim\nto learn the CF for the distribution corresponding to a given\nintervention. We use the Empirical Characteristic Function\n(ECF) [Feuerverger and Mureika, 1977] which has been\nproven to be an unbiased and consistent estimator of the\npopulation characteristic function. Given data {x}}=1 the\nECF is given as\n$\\widehat{\\varphi}(t) = \\frac{1}{n} \\sum_{j=1}^{n} \\exp (it x_j).$ (8)\nThe overall goal of learning, as shown in Fig. 2(right) is\nto approximate, as closely as possible, the underlying char-\nacteristic function of the intervened data (which we call x\ndistribution\u00b9).\nEvaluation Metric. A measure of the closeness of two\ndistributions represented by their characteristic functions\nis the squared characteristic function distance (CFD). The\nsquared CFD between two distributions P and Q is defined\nas\n$CFD^2 (P, Q) = \\int_{R^d} |\\varphi_P(t) - \\varphi_Q(t)|^2 w(t; \\eta) dt,$ (9)\nwhere w(t; \u03b7) > 0 is a weighting function parameterized\nby \u03b7 that guarantees the integral in Eq. 9 converges. When\nw(t; n) is a probability density function, Eq. 9 can be rewrit-\nten as an expectation over t sampled from w:\n$CFD^2 (P, Q) = \\mathbb{E}_{t \\sim w(t;\\eta)} [|\\varphi_P(t) - \\varphi_Q(t) |^2].$ (10)\nSriperumbudur et al. [2010] showed that using the unique-\nness theorem of CFs, CFD\u0ed6 (P, Q) = 0 iff P = Q which\nmotivates our choice of this distance metric. We refer to\nAnsari et al. [2020] for a detailed discussion on CFD.\nOur learning objective is then to minimise the squared char-\nacteristic function distance between the characteristic func-\ntion estimated at the root of XSPN and the ECF of the\nintervened dataset:"}, {"title": "3.4 TRACTABILITY OF INFERENCE", "content": "Through their recursive nature, XSPN allow efficient com-\nputation of densities in a high dimensional setting even if\nclosed form densities don't exist. To get the joint probability\ndensity over all random variables in the SCM, we perform\ninversion of the characteristic function at the root of the\nnetwork, for which we use an extension of Corollary 2.3.\nd\nLemma 3.2 (Inversion). Let C be a xSPN modeling the\ndistribution of RVs X = {X;}=1 and employing univariate\nleaf nodes. If $\\int_{R} |\\varphi_L(t)| dt < \\infty$ for every leaf L, then X\nhas a continuous probability density function $f_X$ given by\nthe integral on the d-dimensional space Rd, i.e.,\n$f_X(x) = \\frac{1}{(2\\pi)^d} \\int_{R^d} \\exp(-itx) \\varphi_C(t) d \\lambda_d(dt),$ (12)\n$=f_C(x)$\nwhere $\u03c6_C(t)$ denotes the CF defined by the root of the XSPN\nand $\u03bb_d$ is the Lebesque measure on $(R^d, B (R^d))$.\nWe can recursively compute Eq. 12 for every node. Thus,\ninversion at every inner node reduces to inversion at its\nchildren. We can invoke Corollary 2.3 to obtain density"}, {"title": "3.5 XSPN IS A UNIVERSAL FUNCTION APPROXIMATOR", "content": "The weights of the XSPN are parameterised by gating func-\ntions and distribution parameters and this allows them to in-\nduce universal approximators. By using threshold functions,\n0+\u2161(xi \u2265 c) +0\u00af\u00b7\u00ac\u2161(xi \u2265 c), c\u2208 R, one can encode test-\ning arithmetic circuits [Choi and Darwiche, 2018], which\nare proven universal approximators. This renders XSPN to\nbe universal approximators by design. Moreover, use of\ncharacteristic functions allows the leaves of the network to\ntheoretically model all probability distributions, including\nthose that do not have a density function."}, {"title": "4 EXPERIMENTAL EVALUATION", "content": "XSPN are tailored towards handling causal graphs with hy-\nbrid data i.e. graphs consisting of random variables drawn\nfrom both discrete and continuous distributions. Our exper-\niments thus focus on capturing the interventional distribu-\ntions within such exemplary causal processes. We aim to\nanswer the following questions:\n(Q1) Can XSPN effectively estimate the joint probability of\nthe heterogeneous variables conditioned on arbitrary\ninterventions?\n(Q2) How well does XSPN capture individual interventional\ndistributions?"}, {"title": "5 CONCLUSION", "content": "We presented XSPN, the first causal models that are ca-\npable of efficiently inferring causal quantities (i.e., inter-\nmentional distributions) in presence of mixed data. XSPN\ntransforms discrete and continuous variables into a shared\nspectral domain using characteristic functions in the leaves\nof the interventional SPN. This enables XSPN to capture\nthe interventional distributions effectively. In addition, we\nshow that XSPN are able to generalize to multiple interven-\ntions while being trained only on a single intervention data\nthereby showing the generality of our proposed approach.\nAs most real-world data is mixed by nature, immediate fu-\nture work includes testing the XSPN on such real world data\nsets. Incorporating rich expert domain knowledge, along-\nside observational data, proves crucial for achieving robust\ncausal inference. Extending our method to integrate such\nexpertise becomes imperative for enhancing the accuracy\nand reliability of causal models. Also, scaling XSPN to very\nlarge data sets is essential for their adaptation to complex\nreal world scenarios."}]}