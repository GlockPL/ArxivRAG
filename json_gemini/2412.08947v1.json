{"title": "Selective Visual Prompting in Vision Mamba", "authors": ["Yifeng Yao", "Zichen Liu", "Zhenyu Cui", "Yuxin Peng", "Jiahuan Zhou"], "abstract": "Pre-trained Vision Mamba (Vim) models have demonstrated exceptional performance across various computer vision tasks in a computationally efficient manner, attributed to their unique design of selective state space models. To further extend their applicability to diverse downstream vision tasks, Vim models can be adapted using the efficient fine-tuning technique known as visual prompting. However, existing visual prompting methods are predominantly tailored for Vision Transformer (ViT)-based models that leverage global attention, neglecting the distinctive sequential token-wise compression and propagation characteristics of Vim. Specifically, existing prompt tokens prefixed to the sequence are insufficient to effectively activate the input and forget gates across the entire sequence, hindering the extraction and propagation of discriminative information. To address this limitation, we introduce a novel Selective Visual Prompting (SVP) method specifically for the efficient fine-tuning of Vim. To prevent the loss of discriminative information during state space propagation, SVP employs lightweight selective prompters for token-wise prompt generation, ensuring adaptive activation of the update and forget gates within Mamba blocks to promote discriminative information propagation. Moreover, considering that Vim propagates both shared cross-layer information and specific inner-layer information, we further refine SVP with a dual-path structure: Cross-Prompting and Inner-Prompting. Cross-Prompting utilizes shared parameters across layers, while Inner-Prompting employs distinct parameters, promoting the propagation of both shared and specific information, respectively. Extensive experimental results on various large-scale benchmarks demonstrate that our proposed SVP significantly outperforms state-of-the-art methods. Our source code is available at https://github.com/zhoujiahuan1991/AAAI2025-SVP.", "sections": [{"title": "Introduction", "content": "Vision Mamba (Vim), a groundbreaking vision backbone featuring an input-dependent modeling mechanism termed selective state space, has demonstrated superior performance compared to established Vision Transformers (ViT) while maintaining computational efficiency (Zhu et al. 2024). This advancement positions Vim as a potential next-generation foundation model architecture. However, as models scale up, directly applying pre-trained Vim models to downstream tasks through full fine-tuning leads to significant computational and storage overhead. This issue has spurred the development of Parameter-Efficient Fine-Tuning (PEFT) techniques (Fu et al. 2023), which adapt pre-trained models to downstream tasks by fine-tuning only a small subset of parameters or incorporating a few additional ones, thereby substantially reducing storage requirements. Among PEFT methods, visual prompting (Han et al. 2023) has shown promising performance by integrating a few additional learnable parameters into pre-trained models.\nTherefore, visual prompting holds substantial potential for the efficient fine-tuning of Vim, enabling high performance with minimal resource consumption. However, current visual prompting methods are predominantly designed for ViT with global attention mechanisms and fail to account for the unique sequential characteristics of Vim, which processes visual information through token-by-token compression and propagation. As illustrated in Figure 1, existing approaches employ prompt sequences prefixed to the token sequence, which inadequately adapts the model to the input distribution. This limitation poses a significant challenge, as it impedes the effective updating and propagation of discriminative sequence information when directly applied to Vim. There is a pressing need to explore visual prompting techniques specifically tailored to Vim, making it both a significant research endeavor and a practical necessity.\nTo address the unique requirements of Vim, we propose a novel Selective Visual Prompting (SVP) method designed to promote discriminative information propagation within the sequence. As depicted in Figure 1, our approach employs a lightweight prompter that dynamically produces token-level prompts based on varying inputs. These prompts are then integrated into the original image tokens, which adequately adapt the model to the input distribution. This mechanism ensures the selective activation of update and output gates, which are input-dependent parameters in Vim. Consequently, our SVP method enables the model to update and compress relevant discriminative features while propagating them through the network. Simultaneously, it identifies and discards irrelevant distracting information, preventing it from contaminating the compressed state of the sequence. This targeted approach enhances the model's ability to retain and propagate discriminative information.\nFurthermore, recognizing that Vim propagates both shared cross-layer information and specific inner-layer information, we introduce a dual-path structure in our selective prompting design, comprising Cross-Prompting and Inner-Prompting mechanisms. This design aims to optimize the propagation of both types of information within Vim. The Cross-Prompting facilitates the transfer of shared information between layers, while Inner-Prompting enhances the flow of layer-specific information within each layer. To account for the varying proportions of these two types of information at different layers, we implement an element-wise scaling factor that dynamically adjusts the emphasis between the two prompts. This dual-path structure ensures a balanced and effective leveraging of both information types, significantly enhancing the model's overall performance.\nTo sum up, the main contributions of this work are: (1) To the best of our knowledge, this is the initial exploration of visual prompting within Vim. We introduce a selective prompting approach that leverages Vim's input-dependent characteristics, adaptively activating its input and forget gates to enhance the propagation of discriminative information. (2) In our SVP, a dual-path prompting structure, termed Cross-Inner, is proposed to effectively utilize both cross-layer shared information and inner-layer specific features, ensuring comprehensive and efficient information propagation. (3) Extensive experiments on various datasets demonstrate that our SVP method significantly outperforms existing visual prompting approaches, achieving superior performance with equivalent model size and pre-training datasets."}, {"title": "Related Work", "content": "The state space model (SSM) with linear complexity presented a promising approach for modeling long-range dependencies. Moreover, the Structured State-Space Sequence model (Gu, Goel, and R\u00e9 2021) improved computational efficiency while preserving theoretical strengths through novel parameterization. Expanding on this foundation, Mamba (Gu and Dao 2023) and Mamba2 (Dao and Gu 2024) introduced a data-dependent SSM layer with hidden state expansion, forming a language model backbone. Building on its success in sequence data, Vision Mamba (Zhu et al. 2024) applied pure Mamba layers to vision tasks, utilizing bidirectional scans for comprehensive modeling. Vim's linear complexity and effective performance highlighted its suitability for a large pre-trained model. The previous paradigm for adapting pre-trained models to downstream tasks was full fine-tuning. However, as the scale of Vim models increased, this approach became inefficient, driving the development of Parameter-Efficient Fine-Tuning methods (PEFT) (Fu et al. 2023).\nPEFT aimed at reducing learnable parameters while maintaining performance, categorized into partial-based, addition-based, and prompt-based methods (Xin et al. 2024b). Partial-based methods trained only select portions of model parameters, e.g., bias terms, attention, or MLP layers (Zaken, Ravfogel, and Goldberg 2021; Kornblith, Shlens, and Le 2019; Touvron et al. 2022; Basu et al. 2024). While these approaches are straightforward and simple to implement, they often lag in performance compared to full finetuning. Addition-based methods, such as Side-Tuning (Sung, Cho, and Bansal 2022) and adapters (Chen et al. 2022; Steitz and Roth 2024; Xin et al. 2024a; Dong et al. 2024), introduce auxiliary components for task-specific learning, yet their custom nature limits generalizability across different architectures.\nVisual prompt learning techniques, which operated primarily on the input, offered better generalization and were more compatible with various models. Existing visual prompt learning methods could be categorized into two types. The first type, represented by the VPT series, appended prompt tokens to the image token sequence. E2VPT (Han et al. 2023) further refined these prompts by pruning ineffective ones, while more recent approaches like InsVP (Liu, Peng, and Zhou 2024) learned prompts more relevant to the instance. SPT (Wang et al. 2024) revisited the power of VPT and extended it by self-initializing with downstream token prototypes. However, these methods struggled to effectively capture the distribution across the entire sequence in the Vim sequence model. The second type directly overlayed frame-like prompts onto the original image, as seen in DAM-VP (Huang et al. 2023) and AutoVP (Tsao et al. 2024). These methods only applied prompts at the image level and lacked input dependency. As a result, they were not effective at activating the update and forget gates in the deeper layers of Vim. Therefore, directly applying these two types of prompt learning methods to Vim led to suboptimal performance. In contrast, our SVP method selectively activates the update gate across the entire sequence, promoting discriminative information propagation."}, {"title": "The Proposed Method", "content": "The SSM-based models, Mamba (Gu and Dao 2023), and Vision Mamba (Vim) (Zhu et al. 2024) are inspired by the continuous system, which maps a one-dimensional function or sequence $x(t) \\in \\mathbb{R} \\leftrightarrow y(t) \\in \\mathbb{R}$ through a N-dimension hidden state $h(t) \\in \\mathbb{R}^N$. The hidden state evolves over time with parameters A, B, and C, following linear ordinary differential equations:\n$h'(t) = Ah(t) + Bx(t),$\n$y(t) = Ch(t),$\nwhere $A \\in \\mathbb{R}^{N \\times N}$ is the state matrix, $B \\in \\mathbb{R}^{N \\times 1}$, and $C \\in \\mathbb{R}^{1 \\times N}$ are projection parameters.\nTo adapt SSM for deep learning, it is discretized using zero-order hold (Pechlivanidou and Karampetakis 2022). The continuous parameters A, B are transformed into their discrete counterparts $A \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times 1}$ using a timescale parameter $\\Delta \\in \\mathbb{R}$:\n$A = exp(\\Delta A),$\n$B = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta B \\approx \\Delta B.$\nThus, the discrete SSM can be written as:\n$h_i = A h_{i-1}+ B x_i,$\n$Y_i = C h_i,$\nwhere $h_{i-1}, h_i \\in \\mathbb{R}^{N \\times 1}, x_i."}, {"title": "Selective Visual Prompting", "content": "To address the challenge of efficient fine-tuning in Vim, we propose a novel approach termed Selective Visual Prompt-ing (SVP). Our method effectively adapts the model to the input distribution by selectively generating prompts at the token level. It adaptively activates the update and forget gates to promote effective information propagation. The overall structure of the proposed method is illustrated in Figure 2.\nSpecifically, the process begins with dividing the input image $x \\in \\mathbb{R}^{H \\times W \\times C}$ into equally sized patches, where (H, W) represents the size of the image \u00e6, and C is the number of channels. These patches are then embedded into d-dimensional latent space as ${x_i}_{i=1}^N, x_i \\in \\mathbb{R}^{1 \\times d}$.\nGiven Vim's 24-layer hierarchical architecture, each layer in Vim should focus not only on specific inner-layer features but also on shared features with adjacent layers. To facilitate both information propagation, we designed the lightweight Selective Prompting Module with a dual-path structure, incorporating Cross-Prompting and Inner-Prompting.\nTo capture and propagate shared information across layers to ensure feature consistency, we design a Cross-Prompting Module. In this module, we utilize a fully connected cross-prompts generator (GC) with shared parameters across layers to generate cross-prompts $p^c \\in \\mathbb{R}^{1 \\times d}$ through $x_i \\in \\mathbb{R}^{1 \\times d}$. The number of layers sharing parameters is a hyperparameter set to 6, 8, or 12. The process is represented by the following formula:\n$p^c_i = G^c(x_i).$\nThis module focuses on extracting and preserving layer-specific features to enhance the model's discriminative power. To minimize tunable parameters while maintaining performance, we design a lightweight inner-prompts generator (G\u00b9) for each layer of Vim, enabling the generation of distinct inner-prompts $p^i \\in \\mathbb{R}^{1 \\times d}$. This generator includes a linear down layer ($L_{down}$), linear up layer ($L_{up}$) and a SiLU activation (Elfwing, Uchibe, and Doya 2018). The hidden dimension, which refers to both the reduced dimension in $L_{down}$ and the dimension to be expanded in $L_{up}$, is set to 64. This process is represented by the following formula:\n$p^i_i =G^i(x_i)$\n$=SiLU(L_{up}(L_{down}(x_i))).$\nOtherwise, considering the differing importance of layer-specific features and shared information across layers, we designed two element-wise dynamic scaling factors (\u03b1, \u03b2) to balance the influence of these prompts on the input distribution.\n$\\hat{p_i} = \\alpha p^c_i \\oplus \\beta p^i_i,$\nwhere $\\alpha \\in \\mathbb{R}^{1 \\times d}$ and $\\beta \\in \\mathbb{R}^{1 \\times d}$ are learnable parameters which are initialized to zero, $\\oplus$ denotes Hadamard product, $p^c_i, p^i_i \\in \\mathbb{R}^{1 \\times d}$. As shown in Equation 7, the generated prompts are then overlaid onto the original input, effectively activating the update and forget gates in Vim to promote the propagation of shared and layer-specific information.\n$x_i' = x_i + \\hat{p_i}$\nSubsequently, all prompted image tokens ${x'_i}_{i=1}^N$ along with an additional classification token $c_1 \\in \\mathbb{R}^{1 \\times d}$ are fed into the N Mamba blocks ${B_i}_{i=1}^N$ to extract features. Similar to the design of VPT-deep, we incorporate our selective prompts at the input of each layer. The output $c_{N+1}$ from the final Mamba block is then passed through a classification head H to produce the predicted probability distribution y.\nAs mentioned above, our SVP introduces only a few additional parameters:\n$M = \\{G^c,G^i,\\alpha, \\beta\\}.$\nFollowing prior works (Jia et al. 2022; Huang et al. 2023; Wang et al. 2024), we keep the pre-trained model's encoder frozen during training, allowing only the classification head H and the newly added modules M to be trainable. The optimization objective is defined as follows:\n$arg \\min_{M,H} L_{ce} (y, y_{gt}),$\nwhere $L_{ce}$ is the cross-entropy loss, and $y_{gt}$ is the image label."}, {"title": "Discussion and Analysis", "content": "In this section, we discuss how our SVP facilitates the update and forget gates across the entire sequence, thereby promoting effective information propagation.\nAs shown in Figure 3, the Mamba architecture enhances the SSM by introducing the selective state space model. The parameters $B_i \\in \\mathbb{R}^{h \\times 1}, C_i \\in \\mathbb{R}^{1 \\times h}$, and $\\Delta_i \\in \\mathbb{R}^{1 \\times d}$ are generated from $x_i$ via functions $S_B, S_C$, and $S_{\\Delta}$, thus becoming input-dependent:\n$B_i = S_B(x_i), C_i = S_C(x_i), \\Delta_i = S_{\\Delta}(x_i).$\nMamba pracatically applies the state transition Equation 3 independently to each channel of input $x_i$, leading to the following formulations:\n$h_i = A_i h_{i-1} + B_i(\\Delta_i x_i)$\n$= exp(S_{\\Delta}(x_i)A) \\odot h_{i-1} + S_B(x_i)(S_{\\Delta}(x_i) \\odot x_i),$\nwhere $A, A_i, h_{i-1},h_i \\in \\mathbb{R}^{h \\times d}$, $\\odot$ denotes extending the first dimension of the preceding matrix, followed by a Hadamard product with the subsequent matrix.\nIn our SVP, Vim's parameters $B_i, C_i$ and $A_i$ are generated through prompted inputs $x_i'$ as $B' \\in \\mathbb{R}^{h \\times 1}, C' \\in \\mathbb{R}^{1 \\times h}$ and $\\Delta'_i \\in \\mathbb{R}^{1 \\times d}$. Then the state transition equation in Vim can be rewritten as:\n$h_i = A h_{i-1} + B(x_i')$\n$=exp(S_{\\Delta}(x_i + p_i)A) \\odot h_{i-1}$\n$+ S_B(x_i + p_i)(S_{\\Delta}(x_i + p_i) x_i)$\n$+ S_B(x_i + p_i)(S_{\\Delta}(x_i + p_i) \\odot p_i).$\nFrom Equations 11 and 12, our method directly activates the update gate $(B_i(\\Delta_i x_i))$ and forget gate $(A_i)$ in Vim, promoting the updation of discriminative information into the hidden state and its propagation across the sequence. This enhances the model's adaptability to new tasks, improving overall performance. Additionally, unlike full fine-tuning, our approach keeps the pre-trained parameters of $S_B, S_C$, and $S_{\\Delta}$ fixed. This strategy leverages pre-trained knowledge effectively and mitigates catastrophic forgetting of pre-trained knowledge in downstream tasks."}, {"title": "Experiments", "content": "Following prior works (Huang et al. 2023; Pei et al. 2024), our experiments are carried out on two image classification benchmarks HTA and VTAB.\nIt collects 19 benchmarks from Visual Task Adaptation (Zhai et al. 2019), categorized into three groups: i) Natural, ii) Specialized, and iii) Structured, each with 1000 training examples. Following (Zhai et al. 2019; Jia et al. 2022), we use an 800-200 train/val split.\nComparison Methods. We compare our SVP with other visual prompting methods including VPT (Jia et al. 2022), DAM-VP (Huang et al. 2023), AutoVP (Tsao et al. 2024) and SPT (Wang et al. 2024). We apply these methods to Vision Mamba (Zhu et al. 2024). Additionally, We compare the performance of these prompting methods using ViT-Small (Dosovitskiy et al. 2020) as the backbone, with the same model size and pre-trained dataset to Vision Mamba. We also present the visual prompting results in ViT-B with much larger parameters for reference.\nImplementation Details. Our experiments primarily involve three pre-trained vision models: ViT-Small/16 and Vim-Small, both of which are pre-trained on ImageNet-1K (Russakovsky et al. 2015), and ViT-Base/16 (Dosovitskiy et al. 2020), which is pre-trained on ImageNet-21K (Krizhevsky, Sutskever, and Hinton 2012). Following (Huang et al. 2023), all methods are trained for 100 epochs across all datasets for a fair comparison. For the compared methods, we use the optimizers specified in the original papers to achieve better performance. In our approach, we utilize the AdamW (Loshchilov and Hutter 2017) optimizer for optimization and implement cosine annealing. The number of shared layers in Cross-Prompting is set to 4, 8, or 12, depending on the dataset, and the hidden dimension of the inner-prompts generator is set to 64."}, {"title": "Comparison with State-of-the-arts", "content": "We first conduct experiments on HTA datasets using the ImageNet-1k supervised ViT-Small/16 and Vim-Small as the pre-trained models. As shown in Table 1, our SVP significantly surpasses the results of existing prompting methods that use pre-trained models with the same model size and pre-training dataset. It not only achieves SOTA performance in average accuracy but also excels across nine out of ten datasets. For example, SVP achieves a notable improvement of 5.3% over VPT when applied to Vim, while also surpassing DAM-VP by 4.1%. This is because our SVP effectively activates the update and forget gates of Vim across the whole sequence, promoting the propagation of discriminative information, which leads to enhanced performance.\nAdditionally, compared to full fine-tuning, our method outperforms in 7 out of 10 datasets and exceeds 1.3% in average. This further supports the discussion in our methodology section, demonstrating that our approach mitigates the issue of catastrophic forgetting commonly seen in full fine-tuning. Our SVP retains more pre-trained knowledge while efficiently adapting to downstream tasks.\nNotably, our method achieves performance comparable to prompting methods that use ViT-B as the backbone. ViT-B has a much larger model size of 85 Million parameters and is pre-trained on the much larger dataset ImageNet-21K (Deng et al. 2009). In contrast, our method uses a significantly smaller model and a smaller pre-training dataset, yet still delivers comparable results. Our approach outperforms methods such as DAM-VP (Huang et al. 2023) and SPT (Wang et al. 2024). This can be attributed to the dual-path Selective Prompting design in Vim, which effectively promotes the propagation of both shared inter-layer information and specific intra-layer information."}, {"title": "Ablation Study", "content": "Given the specificity of Vim's linear sequence model, where token impact varies by position, we first explore appending prompts to the image sequence and assess the effect of changing their position. As shown in Table 3, placing the prompt tokens in the middle yields a slight average improvement of 0.5% over in the pre, likely due to its closeness to the class token. However, this approach does not fully consider the sequencial token-wise compression and propagation characteristics of the Vim sequence model. This limitation makes it ineffective in learning the input distribution across the entire sequence and in activating Vim's update and forget gates. In contrast, our SVP attains an average 5.2% improvement. It is because our SVP generates token-wise prompts based on the input, more effectively learning the input distribution. This selective change activates the update and forget gates in Vim, promoting the propagation of discriminative information.\nTo evaluate the effectiveness of dual-path prompting in our proposed SVP, we conduct ablation experiments on four datasets, as shown in Table 4. When no prompts are used, SVP reduces to a frozen pre-trained Vim model with a learnable classifier. On the CUB dataset, employing only the inner-prompts p\u00b9 boosts performance by 14%. Furthermore, using both the inner-prompts p\u00b9 and cross-prompts p together yields an additional 0.7% improvement.\nThis is because our dual-path SVP method captures more discriminative and richer information compared to the single-path approach. This enhancement arises from the synergy between the shared information provided by Cross-Prompting and the layer-specific details from Inner-Prompting, enabling more precise extraction and propagation of discriminative information."}, {"title": "Conclusion", "content": "In this paper, we introduce Selective Visual Prompting (SVP), an efficient and novel visual prompting method tailored for Vim. To the best of our knowledge, this is the initial exploration of visual prompting within Vim. Unlike existing approaches, our SVP leverages a dual-path strategy to achieve superior performance by leveraging both shared and layer-specific information. We find that prompts selectively generated based on input are more effective in activating Vim's update and forget gates, promoting information propagation. Visualization results further validate our approach. We believe SVP will serve as a valuable benchmark that will drive future research in visual prompting for Vim."}]}