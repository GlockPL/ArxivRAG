{"title": "Self-Supervised Keypoint Detection with Distilled Depth Keypoint Representation", "authors": ["Aman Anand", "Elyas Rashno", "Amir Eskandari", "Farhana Zulkernine"], "abstract": "Existing unsupervised keypoint detection methods apply artificial deformations to images such as masking a significant portion of images and using reconstruction of original image as a learning objective to detect keypoints. However, this approach lacks depth information in the image and often detects keypoints on the background. To address this, we propose Distill-DKP, a novel cross-modal knowledge distillation framework that leverages depth maps and RGB images for keypoint detection in a self-supervised setting. During training, Distill-DKP extracts embedding-level knowledge from a depth-based teacher model to guide an image-based student model with inference restricted to the student. Experiments show that Distill-DKP significantly outperforms previous unsupervised methods by reducing mean $L_2$ error by 47.15% on Human3.6M, mean average error by 5.67% on Taichi, and improving keypoints accuracy by 1.3% on DeepFashion dataset. Detailed ablation studies demonstrate the sensitivity of knowledge distillation across different layers of the network. Project Page: https://23wm13.github.io/distill-dkp/", "sections": [{"title": "I. INTRODUCTION", "content": "Detecting accurate keypoints is crucial in various downstream applications of computer vision, such as human pose estimation, activity recognition, and computer graphics. This task becomes even more challenging in the absence of annotated datasets [3]. Recent advancements in self-supervised learning (SSL) [2], [5], [27] have shown great promise in learning meaningful representations from the data and yield comparable performance to supervised methods.\nTo detect keypoints in an unsupervised manner, existing models rely on either learning to predict the masked portions of an image [7] or generating an image from random noise [6] [8]. However, neither objective compels these models to understand the depth in the image, which is crucial for understanding the full topology of objects of interest. This limitation leads to the following key problems. (i) In images with structured backgrounds, keypoints often appear on background elements, due to which there is a failure to distinguish between foreground and background components in images [7]. (ii) Unsupervised methods that rely solely on 2D RGB images to detect keypoints lack the necessary depth information, leading to less accurate keypoint detection [6]\u2013[8], [15]. (iii) To mitigate the problem of complex backgrounds, existing methods often use a background mask, which isolates the object of interest from the background [6]\u2013[8]. However, these masks are not always available in real-life scenarios.\nTo address these problems, we propose Distill-DKP, which employs cross-modal knowledge distillation (KD) to enhance keypoint detection. Our method utilizes RGB images and depth maps both during training but relies only on RGB images as input during inference. Our approach begins by training an SSL framework on depth maps to leverage their superior ability to distinguish between foreground and background. This trained model serves as the teacher. The student model utilizes RGB images as input and distills the depth information from the teacher model (pre-trained on depth maps). For KD, we aim to minimize the cosine similarity loss on the embedding level to ensure that the student learns from the depth information captured by the teacher. We evaluate Distill-DKP on three benchmark datasets: TaiChi [23], DeepFashion [14], and Human3.6M [9] and achieve significant performance over previous baselines across all datasets.\nOur main contributions are as follows: (1) We introduce Distill-DKP, a novel cross-modal KD framework that leverages features of depth maps to enhance keypoint detection. (2) We demonstrate significant performance improvements over existing methods across multiple benchmark datasets. (3) Through a detailed ablation study, we demonstrate the layer-wise sensitivity of KD between depth and image modalities and contribute to the understanding of cross-modal KD."}, {"title": "II. RELATED WORKS", "content": "Cross-modal KD: Traditional KD methods focus on transferring knowledge from a large teacher model to a smaller student model within the same modality. In contrast, cross-modal KD approaches focus on transferring knowledge between different modalities in a student-teacher setup, with inference restricted to the student modality. In 2020, Wang et al. [25] developed a voice conversion method using a teacher-student framework to extract linguistic features from dysarthric speech (speech impaired by muscle weakness). In 2022, Ni et al. [17] developed a Vision-to-Sensor KD method for action recognition. In 2023 Liu et al. [13] presented a cross-modality KD approach that enables a compressed-domain-based model to learn from a raw-domain-based model in video caption generation. More recently, Sarkar et al. [20] introduced a domain alignment strategy to address audio-visual discrepancies, improving cross-modal KD for video representation learning. Shome et al. [22] proposed EmoDistill, which distills the knowledge from both prosodic and linguistic teachers for speech emotion recognition. Chen et al. [1] introduced a Cross-Modal Multi-Teacher Contrastive Distillation architecture to learn medical vision-language representations. The literature demonstrates the potential of cross-modal KD methods across domains and data modalities.\nUnsupervised Keypoint Detection: Unsupervised learning has become a crucial approach in keypoint detection, enabling models to learn without requiring large, annotated datasets [6]. The most common unsupervised technique for keypoints detection is applying artificial deformation to images [15]. In this approach, keypoints are often detected in the background [29]. Building on this, in 2021, He et al. proposed LatentKeypointGAN [6] and GanSeg [8], which uses GANs to generate images from noise with keypoints. However, this method comes with the challenges of training a GAN and limited applicability. Addressing these challenges, in 2022, He et al. presented AutoLink [7], a self-supervised method to detect keypoints by representing objects as graphs where keypoints are nodes connected by learnable edges. These edge maps are then concatenated with masked images to train the keypoint detector. The literature demonstrates that unsupervised keypoint detection methods are not able to capture depth information which is crucial in distinguishing between objects of interest and background effectively."}, {"title": "III. METHODOLOGY", "content": "Our framework aims to enhance keypoint detection by leveraging cross-modal KD between depth maps and RGB images. We utilize both modalities during training but rely only on RGB images during inference. The framework consists of two main components: a depth-based teacher and an image-based student model, both adopting AutoLink framework [7].\nA. AutoLink Overview\nAutoLink [7] is an SSL framework to detect keypoints in images by representing objects as graphs where keypoints are nodes connected by learnable edges. The framework comprises three key modules as shown in Fig. 1 :\n(a) Keypoint Detector: This module detects keypoints in the input image using a ResNet with upsampling [26]. Keypoints $k_i$ are calculated using differentiable soft-argmax function:\n$k_i = \\frac{{\\sum_p H(p) \\cdot p}}{{\\sum_p \\exp(H(p))}}$ (1)\nwhere $H \\in \\mathbb{R}^{H \\times W \\times K}$ represents K heatmaps generated by the ResNet, and p is normalized pixel coordinates.\n(b) Edge Map Generator: This module generates an edge map $S_{ij}$ to connect between pairs of keypoints $k_i$ and $k_j$, represented as a Gaussian extended along the line [16]:\n$S_{ij}(p) = \\exp \\left(-\\frac{d_{ij}^2(p)}{2 \\sigma^2}\\right)$ (2)\nwhere $d_{ij}(p)$ is $L_2$ distance from pixel p to the line segment (edge) between $k_i$, $k_j$, and $\\sigma$ controls thickness of the edge.\n(c) Decoder: First, the original image $I_i$ is divided into a uniform 16 x 16 grid, where 80% of the grid cells are removed. Next, as shown in Fig. 1, the masked image is multiplied with a learnable parameter $\\alpha$ (initialized as 1) and concatenated with the edge map S, and then fed to UNet [19] based decoder. The image reconstruction is guided by perceptual loss [11]:\n$L_p = \\frac{1}{N} \\sum_{i=1}^{N} ||\\Gamma(I_i) - \\Gamma(\\hat{I}_i)||_1$ (3)\nwhere $I_i$ and $\\hat{I}_i$ are the original and reconstructed images, $\\Gamma$ is the feature extractor, and N is the total number of images in a batch. As the reconstruction of $I_i$ relies on the structure of the missing portion, the model is compelled to learn the object's structure in a self-supervised manner.\nB. Distill-DKP\nBuilding upon the AutoLink, our Distill-DKP framework introduces a depth-based teacher model that enhances keypoint detection accuracy through cross-modal KD. The overview of our method is shown in Fig. 1, and detailed as follows."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets and Evaluation Metrics\nWe evaluate Distill-DKP on three benchmark datasets: Human3.6M (with background) [9], DeepFashion [14], and Taichi [23].We follow the same data sizes, training protocols, and evaluation methods as AutoLink [7] for all benchmark datasets. To evaluate our model on Human3.6M, we normalize the regressed mean $L_2$ error by the image size. On Deep-Fashion [14], we evaluate our model by the percentage of correct keypoint within 6 pixel in 256 \u00d7 256 resolution, where the ground truth keypoints are generated by Alphapose [4]. For Taichi [23], we use 2,673 training and 285 test videos. For evaluation, we calculate Mean Average Error (MAE) by summing the $L_2$ errors on images of 256 \u00d7 256 resolution. Due to fewer training samples in the taichi dataset compared to AutoLink [7], we reproduce the results of [7] using their official code for a fair comparison.\nImplementation details. We use a single NVIDIA A100 GPU with a batch size of 64 to train our model on all datasets. The training employs the Adam Optimizer [12] with a learning rate of 1 \u00d7 10-4. We choose edge thickness ($\\sigma$) value as 5e-5 for Human3.6M and DeepFashion, and 5e-4 for Taichi dataset following AutoLink [7]. Depth maps are extracted using the MiDaS 3.1 depth estimation model [18], with DPT-Swin-2-Large384 checkpoint. Training time on each dataset is around 3.5 hours. The perceptual loss coefficient is set to $\\lambda$ = 1 for all datasets. We choose $\\gamma$ = 0.4 for Taichi dataset and $\\gamma$ = 0.1 for Human3.6M and DeepFashion datasets based on ablation experiments (see Fig. 3.). We train Distill-DKP for 20K iterations.\nB. Results and Discussion\nWe train and evaluate Distill-DKP 10 times on each dataset and report the mean and standard deviation. As shown in Table I, our model demonstrates significant performance improvements over previous methods [6]-[8], [15]. On Human3.6M (with background), Distill-DKP achieves the mean $L_2$ error of 3.62, 47.15% lower than the previous best [7]. This is also evident in Fig. 2, where AutoLink [7] often detects keypoints on the background structures, while Distill-DKP accurately detects keypoints on the human body across different poses. On Taichi, our model achieves mean average error of 306.9, 5.67% lower than the previous best [7]. On DeepFashion, Distill-DKP achieves 67.3% accuracy, 1.3% higher than the previous best [7]. We believe that modest gains on DeepFashion and Taichi are mainly due to $f_D$ being a weak teacher on these datasets (see Table II in Ablation study) compared to Human3.6M where $f_D$ is strong. Nonetheless, as shown in Fig. 2, our model's keypoints are better aligned with the body joints, particularly at the elbows in DeepFashion, and the knees and lower back in Taichi. These results collectively demonstrate the generalizability of our model in leveraging depth information across complex backgrounds and diverse human poses (Human3.6M, Taichi), along with simpler backgrounds and varying appearances (DeepFashion).\nAblation study. We conduct detailed ablation tests to understand the sensitivity of KD in different layers of the keypoint detectors and the performance of different components of Distill-DKP. As shown in Table II, we observe a significant drop in performance in the absence of depth teacher ($f_D$). Next, we test the performance of the depth teacher model ($f_D$) only and observe that on Human3.6M (WB), while there is a slight performance drop compared to Distill-DKP, it shows significant improvement over w/o $f_D$ variant. This can be attributed to the knowledge learned from the depth map, where the structure of the human body in the foreground has more weightage than the structures in the background. However, due to comparatively larger pose, and background variation in Taichi than Human3.6M and almost no background information in the DeepFashion dataset (Fig. 2), we observe a significant performance drop compared to w/o $f_D$ variant. This suggests that $f_D$ serves as a strong teacher for Human 3.6M, while as a weak teacher on Taichi and DeepFashion dataset.\nTo further understand the impact of KD across different layers, we separately apply KD using (4) and (5) with varying loss coefficient $\\gamma$ on the embeddings of three critical layers of the keypoint detector: output, mid-level Transposed Convolution (TC), and early-stage ResNet layer. We vary $\\gamma$ from 0.1 to 1 for Human3.6M and Taichi while we choose a smaller range of 0.01 to 0.1 for the DeepFashion dataset due to the simpler background. We found that the model degenerates at a higher $\\gamma$ value on DeepFashion. As shown in Fig. 3, applying KD on the output layer consistently yields the best results on all datasets. Although, on Human3.6M, performance remains better than the previous methods across different values of $\\gamma$, we found $\\gamma$ = 0.1 showing the best performance. On DeepFashion, $\\gamma$ = 0.1 and on Taichi $\\gamma$ = 0.4 yields the best performance when KD is applied on the embeddings of the output layer. KD on the mid-level TC layer also contributes positively but with slightly less impact than the output layer. In contrast, the ResNet layer has a more limited effect on overall performance. This shows that KD in the later stages of the network is more beneficial for optimizing model performance."}, {"title": "V. CONCLUSION", "content": "We introduced Distill-DKP, a framework that leverages cross-modal KD between image and depth modality. During training, our model utilizes both depth maps and RGB images, with the depth teacher providing embedding-level guidance to image student. During inference, Distill-DKP operates solely on RGB images, reducing computational overhead while maintaining high performance. Experiments demonstrate that our method outperforms previous state-of-the-art methods. Through a detailed ablation study, we highlight the sensitivity of different components of our method. Distill-DKP is particularly effective in scenarios where background complexity is a major challenge and removing the background requires additional computation. Moreover, as Distill-DKP maintains SSL framework, it does not require labeled data to operate, ensuring its applicability in diverse, real-world settings. As future work, we plan to extend our method to 3D keypoint detection and explore its applicability in more complex and occluded backgrounds."}]}