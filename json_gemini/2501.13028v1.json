{"title": "Optimizing Return Distributions with Distributional Dynamic Programming", "authors": ["Bernardo \u00c1vila Pires", "Mark Rowland", "Diana Borsa", "Zhaohan Daniel Guo", "Khimya Khetarpal", "Andr\u00e9 Barreto", "David Abel", "R\u00e9mi Munos", "Will Dabney"], "abstract": "We introduce distributional dynamic programming (DP) methods for optimizing statistical functionals of the return distribution, with standard reinforcement learning as a special case. Previous distributional DP methods could optimize the same class of expected utilities as classic DP. To go beyond expected utilities, we combine distributional DP with stock augmentation, a technique previously introduced for classic DP in the context of risk-sensitive RL, where the MDP state is augmented with a statistic of the rewards obtained so far (since the first time step). We find that a number of recently studied problems can be formulated as stock-augmented return distribution optimization, and we show that we can use distributional DP to solve them. We analyze distributional value and policy iteration, with bounds and a study of what objectives these distributional DP methods can or cannot optimize. We describe a number of applications outlining how to use distributional DP to solve different stock-augmented return distribution optimization problems, for example maximizing conditional value-at-risk, and homeostatic regulation. To highlight the practical potential of stock-augmented return distribution optimization and distributional DP, we combine the core ideas of distributional value iteration with the deep RL agent DQN, and empirically evaluate it for solving instances of the applications discussed.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL; Sutton and Barto, 2018; Szepesv\u00e1ri, 2022) is a powerful framework for building intelligent agents, and it has been successfully applied to solve many practical problems (Mnih et al., 2015; Silver et al., 2018; Bellemare et al., 2020; Degrave et al., 2022; Fawzi et al., 2022). In the standard formulation of the RL problem, the objective is to find a policy (a decision rule for selecting actions) that maximizes the expected (discounted) return in a Markov decision process (MDP; Puterman, 2014). A similar, related problem is what we refer to as return distribution optimization, where the objective is to maximize a functional of the return distribution (Marthe et al., 2024), which may not be the expectation. For example, we could maximize an expected utility (Von Neumann and Morgenstern, 2007; B\u00e4uerle and Rieder, 2014; Marthe et al., 2024), the expectation of the return \"distorted\" by some function.\nBy varying the choice of statistical functional being optimized (be it an expected utility or more general), we can model various RL-like problems as return distribution optimization, including problems in the field of risk-sensitive RL (Chung and Sobel, 1987; Chow and Ghavamzadeh, 2014; Noorani et al., 2022), and problems related to homeostatic regulation (Keramati and Gutkin, 2011) and satisficing (Simon, 1956; Goodrich and Quigley, 2004).\nThe fact that return distribution optimization captures many problems of interest makes it appealing to develop solution methods for the general problem. At first glance, the apparent benefits of solving the general problem are offset by the fact that, for many instances, optimal stationary Markov policies do not exist (see, for example, Marthe et al., 2024). This can be problematic, because it rules out dynamic programming (DP; value iteration and policy iteration; Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 2018; Szepesv\u00e1ri, 2022) and various other RL methods that yield stationary Markov policies. Defaulting to solution methods that produce history-based policies is an alternative we would like to avoid, under the premise that learning history-based policies can be intractable (Papadimitriou and Tsitsiklis, 1987; Madani et al., 1999).\nWe show that we can reclaim optimality of stationary Markov policies for many instances of return distribution optimization by augmenting the state of the MDP with a simple statistic we call stock. Stock is a backward looking quantity related to the agent's accumulated past rewards, including an initial stock (the precise definition is given in Section 2). It was introduced by B\u00e4uerle and Ott (2011)\u00b9 for maximizing conditional value-at-risk (Rockafellar et al., 2000). The MDP state and stock together provide enough information for stationary Markov policies (with respect to the state-stock pair) to optimize various statistical functions of the distribution of returns offset by the agent's initial stock.\nIncorporating stock into return distribution optimization gives rise to the specific formulation we consider in this paper, where the environment is assumed to be an MDP with states augmented by stock, and the return is offset by an initial stock. We refer to this formulation as stock-augmented return distribution optimization.\nThe optimality guarantee for stationary stock-Markov policies in return distribution optimization suggests that we may be able to develop DP solution methods for the instances where the guarantee applies. Value/policy iteration cannot cope with return dis-\n\n1. Kreps (Example b, p. 269; 1977) outlined a similar statistic in the undiscounted setting."}, {"title": "1.1 Paper Summary and Contributions", "content": "This paper is an in-depth study of distributional dynamic programming for solving return distribution optimization with stock augmentation, and we make the following contributions:\n1. We identify conditions on the statistical functional being optimized under which distributional DP can solve stock-augmented return distribution optimization, and develop a theory of distributional DP for solving this problem, including:\n\u2022 principled distributional DP methods (distributional value/policy iteration),\n\u2022 performance bounds and asymptotic optimality guarantees (for the cases that distributional DP can solve),\n\u2022 necessary and sufficient conditions for the finite-horizon case, plus mild sufficient conditions to the infinite-horizon discounted case.\n2. We demonstrate multiple applications of distributional value/policy iteration for stock-augmented return distribution optimization, namely:\n\u2022 Optimizing expected utilities (Von Neumann and Morgenstern, 2007; B\u00e4uerle and Rieder, 2014).\n\u2022 Maximizing conditional value-at-risk, a form of risk-sensitive RL, both the risk-averse conditional value-at-risk (B\u00e4uerle and Ott, 2011), and a risk-seeking variant that we introduce.\n\u2022 Homeostatic regulation (Keramati and Gutkin, 2011), where the agent aims to maintain vector-valued returns near a target.\n\u2022 Satisfying constraints, and trading off minimizing constraint violations with maximizing expected return.\n3. We show how to reduce stock-augmented return distribution optimization to stock-augmented RL (via reward design), and that, in stock-augmented settings, classic DP cannot solve not all the return distribution optimization problems that distributional DP can.\n4. We introduce DnN, a deep RL agent that combines QR-DQN (Dabney et al., 2018) with the principles of distributional value iteration and stock augmentation to optimize expected utilities. Through experiments, we demonstrate DnN's ability to learn effectively under objectives in toy gridworld problems and the game of Pong in Atari (Bellemare et al., 2013)."}, {"title": "1.2 Paper Outline", "content": "Section 2 introduces notation and basic definitions. In Section 3.1 we formalize the problem of stock-augmented return distribution optimization, and provide some basic example instances. Section 4 introduces distributional value/policy iteration and presents our main theoretical results. In Section 5, we discuss multiple applications of our results and show concrete examples of how to different problems using stock augmentation and distributional DP (Sections 5.1 to 5.5 and 5.8).2 In Section 5 we also explore implications of our results in different contexts: Generalized policy evaluation (Barreto et al., 2020; Section 5.6); reward design and the relationship between stock-augmented RL and stock-augmented return distribution optimization (Section 5.7). In Section 6, we introduce DnN and show how distributional DP can inform the design of deep RL agents. To highlight the practical implications of our contributions, in Section 7 we present an empirical study of DnN in different gridworld instances of applications considered in Section 5. In Section 8 we complement our gridworld results with a demonstration of DyN controlling returns in a more complex setting: The Atari game of Pong, where we show that a single trained DyN agent can obtain various specific scores in a range, and where we use stock augmentation to specify the scores we want the agent to achieve. Section 9 concludes our work and presents directions for future work, notably practical questions revealed by our empirical study. We provide additional results in Appendix A. Appendix B contains the full analysis of distributional value/policy iteration, and the full analysis of the conditions for our main theorems is provided in Appendix C. Appendices D to G contain proofs for the results in Section 5. Appendix H contains implementation details for D\u03b7N and our experiments.\n\n2. Some of these problems have been previously studied, and distributional DP is a novel solution approach in some cases (see Section 5)."}, {"title": "2 Preliminaries", "content": "We write \\( \\mathbb{N} = \\{1, 2, ...\\} \\) for the natural numbers excluding zero, and \\( \\mathbb{N}_0 = \\{0,1,2,...\\} \\). For a finite \\( n \\subset \\mathbb{N}_0 \\), \\( \\Delta(n) \\) denotes the \\( |n| \\)-dimensional simplex. For \\( m \\in \\mathbb{N} \\), \\( \\Delta(\\mathbb{R}^m) \\) denotes the set of probability distribution functions of \\( \\mathbb{R}^m \\)-valued random variables.\nWe study the problems where an agent interacts with a Markov decision process (MDP; Puterman, 2014) with (possibly infinite) state space \\( \\mathcal{S} \\) and finite action space \\( \\mathcal{A} \\). Rewards can be stochastic and the discount is \\( \\gamma \\in (0,1] \\). We adopt the convention that \\( R_{t+1} \\) is the"}, {"title": "3 Stock-Augmented Return Distribution Optimization", "content": "We are concerned with building intelligent agents that can do various things. When the agent can be expressed in terms of its behavior (a policy) and the outcome of the agent acting can be modeled as the stock-augmented discounted return generated by that policy, we can frame the problem of building intelligent agents as an optimization problem. A person looking to build an intelligent agent in this framework (we will call them the designer) is thus tasked with expressing what they want of agents as an objective to be optimized\u2014where the better the agent, the higher the objective value of its policy.4\nWe propose to control the distribution of the quantity \\( c_0 + G^{\\pi}(s_0, c_0) \\), which is the return generated by from the initial augmented state \\( (s_0, c_0) \\in \\mathcal{S} \\times \\mathcal{C} \\), offset by the initial stock \\( c_0 \\). We want an objective that quantifies how preferred \\( c_0 + G^{\\pi}(s_0, c_0) \\) for each policy \\( \\pi \\), so that we can phrase the problem of finding the most preferred policy. We can accomplish this with a statistical functional \\( K : (\\mathcal{D}, w) \\rightarrow \\mathbb{R} \\) that assigns a real number to each possible distribution of \\( c_0 + G^{\\pi}(s_0, c_0) \\), to phrase the optimization problem as:\n\\[\\begin{equation}\\label{eq:problem} \\sup_{\\pi \\in \\Pi_H} K(df(c_0 + G^{\\pi}(s_0, c_0))) . \\end{equation}\\]\nAs an example, the standard RL problem can be expressed in Eq. (3) by taking K to be the expectation:\n\\[ \\sup_{\\pi \\in \\Pi_H} \\mathbb{E}(c_0 + G^{\\pi}(s_0, c_0)) = c_0 + \\sup_{\\pi \\in \\Pi_H} \\mathbb{E}(G^{\\pi}(s_0, c_0)). \\]\nThe optimization, for the moment, is over the (most general) class of history-based policies \\( \\Pi_H \\). In standard RL, this problem formulation (adopted, for example, by Altman, 1999) differs from the more frequent optimization over stationary Markov policies (adopted, for example, by Sutton and Barto, 2018; Szepesv\u00e1ri, 2022), but the two formulations are equivalent in MDPs because of the existence of optimal stationary Markov policies (Puterman, 2014). For stock-augmented return distribution optimization, we have elected to introduce the problem in terms of history-based policies, and to address the existence of optimal stationary Markov policies on the solution side of the results (in connection to DP).\nBecause the supremum in Eq. (3) is over all history-based policies, it makes sense to talk about optimizing \\( Kdf(c_0 + G^{\\pi}(s_0, c_0)) \\) simultaneously for all \\( (s_0, c_0) \\in \\mathcal{S} \\times \\mathcal{C} \\). We can state this problem concisely, using an objective functional applied to the distribution function \\( \\eta^{\\pi} \\):\n\n4. In practice, the designer is also tasked with modeling the environment as an MDP. In standard RL, this means designing the states, actions and rewards. Stock-augmented MDP additionally require designing the stock and the pseudo-rewards.\n5. In terms of a problem/solution separation, incorporating stock is part of the solution (distributional DP). However, because the scope of our work is DP, it is convenient for our presentation to incorporate stock augmentation and the offset by \\( c_0 \\) as part of the problem (return distribution optimization). The simpler formulation without stock augmentation or the offset is limiting for distributional DP: Marthe et al. (2024) studied return distribution optimization without stock augmentation in the finite-horizon undiscounted setting, and concluded that only exponential utilities could be optimized through distributional DP\u2014the same class that classic DP can optimize. On the other hand, as our analysis will show, the distributional DP with stock can optimize a broader class of objectives than without, and, surprisingly, than classic DP with stock augmentation."}, {"title": "3.1 Problem Formulation", "content": ""}, {"title": "3.2 Example: Expected Utilities", "content": "Equation (4) provides a flexible problem formulation for controlling \\( c_0 + G^{\\pi}(s_0, c_0) \\), based on a choice of \\( K : (\\mathcal{D},w) \\rightarrow \\mathbb{R} \\) provided by a designer to capture what they want an agent to achieve. We have already shown that the RL problem can be recovered by taking K to be the the expectation (\\( Kv = \\mathbb{E}G, G \\sim \\nu \\)), so what else can we do? We can obtain an interesting family of objective functionals by considering the expected value of transformations of the return specified by a function \\( f: \\mathcal{C} \\rightarrow \\mathbb{R} \\): \\( Kv = \\mathbb{E}f(G) \\) (\\( G \\sim \\nu \\)).\nThese are the expected utilities, which have been widely studied in decision-making theory (Von Neumann and Morgenstern, 2007), and also used for sequential decision-making in RL (B\u00e4uerle and Rieder, 2014; Bowling et al., 2023).\n6. Expected utilities are not restricted to the scalar case, as implied by Definition 5, since the domain of f is \\( \\mathcal{C} \\). We provide some concrete examples in Section 5 of expected utilities for the vector-valued case."}, {"title": "Definition 4 (Stock-Augmented Return Distribution Optimization)", "content": "Given\n\\( K : (\\mathcal{D}, w) \\rightarrow \\mathbb{R} \\), define the stock-augmented objective functional \\( F_K : (\\mathcal{D}^{\\mathcal{S}\\times \\mathcal{C}}, w) \\rightarrow \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{C}} \\) as\n\\[\\begin{equation}\\label{eq:objective} (F_K \\eta)(s, c) = Kdf(c+G(s,c)). \\qquad \\qquad (G(s, c) \\sim \\eta(s,c)) \\end{equation}\\]\nThe stock-augmented return distribution optimization problem is\n\\[\\begin{equation}\\label{eq:opt2} \\sup_{\\pi \\in \\Pi_H} F_K \\eta^{\\pi}. \\end{equation}\\]\nWe will often drop the subscript and refer to a stock-augmented objective as F, in which case a corresponding K is implied. We will also drop df and write \\( K(G) = Kdf(G) \\).\nTo recap on Eq. (4): The stock-augmented return distribution optimization problem consists of optimizing, over all policies \\( \\pi \\in \\Pi_H \\), a preference specified by a statistical functional \\( K : (\\mathcal{D}, w) \\rightarrow \\mathbb{R} \\), over the distribution of the policy's discounted return offset by the stock (\\( c_0 + G^{\\pi}(s_0, c_0) \\)). The optimization is considered simultaneously for all \\( (s_0, c_0) \\), as allowed by history-based policies."}, {"title": "Definition 5 A stock-augmented objective functional", "content": "stock-augmented objective functional \\( F_K \\) is an expected utility if there\nexists \\( f : \\mathcal{C} \\rightarrow \\mathbb{R} \\) such that\n\\[ Kv = \\mathbb{E}f (G). \\qquad \\qquad (G \\sim \\nu) \\]\nIn this case, we write \\( F_K = U_f \\), which can be written as\n\\[ (U_f \\eta)(s, c) = \\mathbb{E} f(c + G(s,c)). \\qquad \\qquad (G(s, c) \\sim \\eta(s,c)) \\]"}, {"title": "4 Distributional Dynamic Programming", "content": "Dynamic programming (Bertsekas and Tsitsiklis, 1996; Bertsekas, 2019) is at the heart of RL theory and many RL algorithms,7. For this reason, we have chosen to establish the basic theory of solving stock-augmented return distribution optimization by studying how we can solve these problems using DP. We refer to the solution methods we introduce as distributional dynamic programming. As in the case of distributional DP for policy evaluation (Chapter 5; Bellemare et al., 2023), return distribution functions (in (DS\u00d7C, w)) are the main object of distributional value/policy iteration, whereas, in contrast, classic DP (see, for example, Szepesv\u00e1ri, 2022), namely value iteration and policy iteration, work directly with value functions.\n7. As pointed out by Szepesv\u00e1ri (2022) many RL algorithms can be thought of dynamic of programming methods, modified to cope with scale and complexity of practical problems."}, {"title": "4.1 Distributional Value Iteration", "content": "Classic value iteration computes the iterates \\( V_1, V_2, ... \\) satisfying, for \\( n \\geq 0 \\),\n\\[\\begin{equation}\\label{eq:classic-value-iteration} V_{n+1} = \\sup_{\\pi \\in \\Pi} T_{\\pi}V_n, \\end{equation}\\]\nand the procedure enjoys the following optimality guarantees. In finite-horizon MDPs, \\( V_n \\) is optimal if n is at least the horizon of the MDP and in the discounted case (Section 2.4; Szepesv\u00e1ri, 2022):\n\\[\\begin{equation} \\Vert V^* - V_n \\Vert_{\\infty} \\leq \\gamma^n \\Vert V^* - V_0 \\Vert_{\\infty} \\end{equation}\\]\npointwise for all \\( s \\in \\mathcal{S} \\), where \\( V^* = \\sup_{\\pi \\in \\Pi_H} V^{\\pi} \\) and \\( V^{\\pi} \\) denotes the value function of a policy \\( \\pi \\).\nNote how the bounds are distinct for the finite-horizon case and the discounted case. This distinction recurs in results for both classic and distributional value/policy iteration, and it will merit further discussion in the case of distributional DP.\nIn classic value iteration, the iterates correspond to the values of the objective functional being optimized, and the iteration in Eq. (5) makes a one-step decision that maximizes that objective functional. We typically use the value iterates to obtain policies via a greedy selection, and leverage a near-optimality guarantee for these greedy policies. We say \\( \\hat{\\pi}_n \\) is greedy policy with respect to \\( V_n \\) if it satisfies the following:\n\\[ T_{\\hat{\\pi}_n} V_n = \\sup_{\\pi \\in \\Pi} T_{\\pi}V_n. \\]\nClassic value iteration results give us the following optimality guarantees for the greedy policies: In finite-horizon MDPS, \\( \\hat{\\pi}_n \\) is optimal when n is at least the horizon of the MDP, and in the discounted case (Section 2.4, Szepesv\u00e1ri, 2022; Singh and Yee, 1994):\n\\[ \\Vert V^* - V^{\\hat{\\pi}_n} \\Vert_{\\infty} \\leq \\frac{2 \\gamma^n}{1-\\gamma} \\Vert V^* - V_0 \\Vert_{\\infty}. \\]\nDistributional value iteration, while similar to value iteration, maintains distributional iterates \\( \\eta_1,\\eta_2,... \\in (\\mathcal{D}^{\\mathcal{S}\\times \\mathcal{c}},w) \\), which means the iterates no longer correspond to values of the objective functional. The distributional analogue of Equation (5) makes a one-step decision that maximizes the objective functional \\( F_K \\), and this iteration of locally optimal one-step decisions gives guarantees similar to the classic case. Theorem 6 formalizes this claim:8"}, {"title": "Theorem 6 (Distributional Value Iteration)", "content": "If \\( K : (\\mathcal{D}, w) \\rightarrow \\mathbb{R} \\) is indifferent to mixtures and indifferent to y, then for every \\( \\eta_0 \\in (\\mathcal{D}^{\\mathcal{S}\\times \\mathcal{c}},w) \\), if the iterates \\( \\eta_1,\\eta_2,... \\) satisfy (for \\( n \\geq 0 \\))\n\\[ F_K \\eta_{n+1} = \\sup_{\\pi \\in \\Pi} F_K T_{\\pi}\\eta_n, \\qquad (Distributional Value Iterates) \\]\nand the policies \\( \\pi_0,...,\\pi_n \\) satisfy (for \\( n \\geq 0 \\)),\n\\[ F_K T_{\\pi_n}\\eta_n = \\sup_{\\pi \\in \\Pi} F_K T_{\\pi}\\eta_n, \\qquad (Greedy Policies) \\]\nthen the following holds."}, {"title": "4.2 Distributional Policy Iteration", "content": "Classic policy iteration computes the iterates \\( \\pi_1, \\pi_2,... \\) satisfying, for \\( n \\geq 0 \\),\n\\[ T_{\\pi_{n+1}}V_n = \\sup_{\\pi \\in \\Pi}T_{\\pi}V_{\\pi_n}, \\]\nthat is, each iterate \\( \\pi_{n+1} \\) is greedy with respect to the value of the previous iterate \\( \\pi_n \\). In finite-horizon MDPS, \\( V_n \\) is optimal if n is at least the horizon of the MDP and in the discounted case:\n\\[ V^* - V^{\\pi_n} \\leq \\gamma^n \\Vert V^* - V^{\\pi_0} \\Vert_{\\infty}. \\]\nDistributional policy iteration is similar to its classic counterpart, the main difference being that the objective functional \\( F_K \\) determines the greedy policy selection. Distributional policy iteration enjoys a similar guarantees to its classic counterpart, as formalized by Theorem 8:"}, {"title": "Theorem 8 (Distributional Policy Iteration)", "content": "If \\( K : (\\mathcal{D}, w) \\rightarrow \\mathbb{R} \\) is indifferent to mixtures and indifferent to y, for every non-stationary policy \\( \\pi_0 \\) if the iterates \\( \\pi_1, \\pi_2,... \\) satisfy (for \\( n \\geq 0 \\))\n\\[ F_K T_{\\pi_{n+1}}\\eta_{\\pi_n} = \\sup_{\\pi \\in \\Pi} F_K T_{\\pi}\\eta_{\\pi_n} \\qquad (Distributional Policy Iterates) \\]\nthen the following holds."}, {"title": "4.3 Conditions Overview", "content": "Theorems 6 and 8 only apply to objective functionals that satisfy certain properties: Indifference to y and indifference to mixtures in the finite-horizon case, plus Lipschitz continuity in the infinite-horizon discounted case. In this section we give an overview of these conditions and test them: How restrictive are these conditions? Can they be weakened? The proofs for the results in this section can be found in Appendix C. Recall that we are abusing notation and writing \\( K(G) = Kdf(G) \\)."}, {"title": "Definition 9 (Indifference to Mixtures (of Initial Augmented States))", "content": "We say \\( K : (\\mathcal{D},w) \\rightarrow \\mathbb{R} \\) is indifferent to mixtures (of initial augmented states) if for every \\( \\eta,\\eta' \\in (\\mathcal{D}^{\\mathcal{S}\\times \\mathcal{C}},w) \\) such that\n\\[ K\\eta(s,c) > K\\eta'(s, c), \\]\nfor all \\( (s, c) \\in \\mathcal{S} \\times \\mathcal{C} \\), then for all random variables (S,C) taking values in \\( \\mathcal{S} \\times \\mathcal{C} \\) we also have\n\\[ K(G(S,C)) \\geq K(G'(S,C)). \\qquad (G(s, c) \\sim \\eta(s, c), G'(s, c) \\sim \\eta'(s, c)) \\]"}, {"title": "Definition 10 (Indifference to y)", "content": "We say \\( K : (\\mathcal{D},w) \\rightarrow \\mathbb{R} \\) is indifferent to y if, for\nevery \\( \\nu,\\nu' \\in (\\mathcal{D}, w) \\)\n\\[ K\\nu > K\\nu' \\implies K(\\gamma G) > K(\\gamma G'). \\qquad (G \\sim \\nu, G' \\sim \\nu') \\]"}, {"title": "Definition 11 (Lipschitz Continuity)", "content": "We say \\( K : (\\mathcal{D}, w) \\rightarrow \\mathbb{R} \\) is L-Lipschitz (or Lipschitz, for simplicity) if there exists \\( L \\in \\mathbb{R} \\) such that\n\\[ \\sup_{\\substack{\\nu,\\nu': \\\\ w(\\nu) < \\infty \\\\ w(\\nu') < \\infty \\\\ w(\\nu,\\nu') > 0}} \\frac{K\\nu - K\\nu'}{w(\\nu, \\nu')} \\leq L. \\]\nL is the Lipschitz constant of K.\nWe believe that in general these conditions are fairly easy to verify for difference choices of the objective functional. As an example, Lemma 12 does part of the verification for expected utilities (we say \\( f : \\mathcal{C} \\rightarrow \\mathbb{R} \\) is indifferent to y if for all \\( c,c' \\in \\mathcal{C} \\) we have \\( f(c) \\geq f(c') = f(\\gamma c) \\geq f(\\gamma c') \\))."}, {"title": "Lemma 12 (Conditions for Expected Utilities)", "content": "Let \\( U_f \\) be an expected utility, which is an objective functional \\( F_k \\) with \\( Kv = \\mathbb{E}f(G) \\) (\\( G \\sim \\nu \\)). Then the following holds:\n1. K is indifferent to mixtures.\n2. K is indifferent to y iff f is indifferent to \\( \\gamma \\).\n3. K is Lipschitz iff f is Lipschitz."}, {"title": "Proposition 13", "content": "If \\( K : (\\mathcal{D}, w) \\rightarrow \\mathbb{R} \\) is not indifferent to mixtures or not indifferent to y, then there exists an MDP, an \\( \\eta^* \\in (\\mathcal{D}^{\\mathcal{S}\\times \\mathcal{c}},w) \\) and \\( \\pi \\in \\Pi \\) such that \u2137 is greedy with respect to \\( \\eta^* \\) and\n\\[ F_K \\eta^* = \\sup_{\\pi \\in \\Pi} F_K \\eta^{\\pi}, \\]\nhowever, for some \\( (s, c) \\in \\mathcal{S} \\times \\mathcal{C} \\)\n\\[ F_K \\eta^{\\pi}(s, c) < \\sup_{\\pi \\in \\Pi} F_K \\eta^{\\pi}(s,c). \\]"}, {"title": "Lemma 14 (Monotonicity)", "content": "If \\( K : (\\mathcal{D}, w) \\rightarrow \\mathbb{R} \\) is indifferent to mixtures and indifferent to y, then, for every \\( \\pi \\in \\Pi \\), the distributional Bellman operator \\( T_{\\pi} \\) is monotone (or order-preserving) with respect to the preference induced by \\( F_k \\) on \\( (\\mathcal{D}^{\\mathcal{S}\\times \\mathcal{C}},w) \\). That is, for every\nstationary policy \\( \\pi \\in \\Pi \\) and \\( \\eta, \\eta' \\in (\\mathcal{D}^{\\mathcal{c}},w) \\), we have\n\\[ F_K \\eta > F_K \\eta' \\implies F_K T_{\\pi} \\eta > F_K T_{\\pi} \\eta'. \\]"}, {"title": "Lemma 15 (Distributional Policy Improvement)", "content": "If \\( K : (\\mathcal{D}, w) \\rightarrow \\mathbb{R} \\) is indifferent to mixtures and indifferent to y, and if the MDP is finite-horizon or \\( \\gamma < 1 \\) and K is Lipschitz, then for any non-stationary policy \\( \\eta \\) and any stationary policy \\( \\pi' \\in \\Pi \\) if\n\\[ F_K T_{\\pi'}\\eta > F_K T_{\\pi}\\eta, \\]\nthen\n\\[ F_K \\eta^{\\pi'} > F_K \\eta^{\\pi}. \\]"}, {"title": "5 Applications", "content": "In many cases, we want to instruct agents to perform tasks in highly controllable environments, but not necessarily the tasks with a \u201cdo something as much as possible\" nature that are a clear fit for RL. For example, we may want to specify the task of collecting a given number of objects in a room, or obtaining a score equal to two in the game of Pong in the Atari Benchmark (Bellemare et al., 2013). The standard RL framework can be unwieldy for this type of task, but it can be easily modeled as a stock-augmented problem.\nIf we were to solve an RL problem without stock augmentation to collect a given number of apples, we would likely have to use a non-Markov reward that tracks how many apples have been collected, and give a reward of 1 to the agent when the third apple is collected, and zero otherwise. Moreover, we would have one reward function for each number of apples to be collected, which might require training one agent per reward function (which seems wasteful).\nWith stock augmentation, on the other hand, this type of task can be tackled effectively. We can frame it as a stock-augmented return distribution optimization problem with an expected utility \\( U_f \\) and \\( f(x) = -|x| \\), where the stock is the number of apples collected so far by the agent. Moreover, we can get a single stock-augmented agent to perform various instances of the same task\u2014for example, collect one apple, or collect three apples simply by changing the agent's initial stock: Without discounting and with a reward of 1 for each apple, a stock of -3 will cause an optimal stock-augmented agent to collect 3 apples, a stock of -2 will cause the agent to collect 2 apples, and so forth."}, {"title": "5.1 Generating Desired Returns", "content": ""}, {"title": "5.2 Maximizing the Conditional Value-at-Risk of Returns", "content": "The problem of maximizing conditional value-at-risk (CVaR; Rockafellar et al., 2000), also known as average value-at-risk or expected shortfall, has received attention both in the context of risk-sensitive reinforcement learning (B\u00e4uerle and Ott, 2011; Chow and Ghavamzadeh, 2014; Chow et al., 2015; B\u00e4uerle and Glauner, 2021; Greenberg et al., 2022) and in non-sequential decision-making (Rockafellar et al., 2000).\nThe \\( \\tau \\)-CVaR of returns with distribution \\( \\nu \\in (\\Delta(\\mathbb{R}), w) \\) is defined as\n\\[ CVaR(\\nu, \\tau) = \\frac{1}{\\tau} \\int_0^{\\tau} QF_{\\nu}(t) dt. \\"}]}