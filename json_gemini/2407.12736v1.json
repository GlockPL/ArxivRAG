{"title": "CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference", "authors": ["Mohammad Erfan Sadeghi", "Suhas Somashekar", "Arash Fayyazi", "Massoud Pedram"], "abstract": "Vision Transformers (ViTs) represent a groundbreaking shift in machine learning approaches to computer vision. Unlike traditional approaches, ViTs employ the self-attention mechanism, which has been widely used in natural language processing, to analyze image patches. Despite their advantages in modeling visual tasks, deploying ViTs on hardware platforms, notably Field-Programmable Gate Arrays (FPGAs), introduces considerable challenges. These challenges stem primarily from the non-linear calculations and high computational and memory demands of ViTs. This paper introduces CHOSEN, a software-hardware co-design framework to address these challenges and offer an automated framework for ViT deployment on the FPGAs in order to maximize performance. Our framework is built upon three fundamental contributions: multi-kernel design to maximize the bandwidth, mainly targeting benefits of multi DDR memory banks, approximate non-linear functions that exhibit minimal accuracy degradation, and efficient use of available logic blocks on the FPGA, and efficient compiler to maximize the performance and memory-efficiency of the computing kernels by presenting a novel algorithm for design space exploration to find optimal hardware configuration that achieves optimal throughput and latency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a 1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.", "sections": [{"title": "1 INTRODUCTION", "content": "The landscape of computer vision has been fundamentally transformed with the advent of deep learning, among which Vision Transformers (ViTs) [1-4] have emerged as a particularly promising approach. Unlike traditional convolutional neural networks (CNNs) that rely on local receptive fields, ViTs leverage the power of self-attention mechanisms to capture global dependencies within an image, enabling a more comprehensive understanding of visual data. This capability has placed ViTs at the forefront of research, demonstrating state-of-the-art performance across a wide range of tasks in computer vision. Overall, deep learning has revolutionized various domains by providing robust algorithms capable of learning complex patterns from large datasets, thus enabling unprecedented advancements in the application of artificial intelligence [5] across numerous fields, from healthcare to entertainment to scientific research.\nViTs rely on a series of identical encoder blocks to progressively extract complex features from an image. These encoder blocks consist of two principal components: Multi-headed Attention (MHA) and Feed-Forward Network (FFN), each prefaced with a layer normalization block. Embedded within MHA and FFN are linear layers, GELU, and softmax, integrated via two residual connections that bookend the normalization stages. The output of the final encoder block goes through a classifier to obtain the class predictions.\nImplementing Vision Transformers (ViTs) and other transformer-based models on Field-Programmable Gate Arrays (FPGAs) presents unique challenges that stem from the architectures' complexity and resource demands. The primary hurdles include the high demand for memory due to the extensive number of parameters and the intensive computation required for processing the self-attention across image patches. FPGAs, although flexible and capable of parallel processing, often struggle with limited on-chip memory and bandwidth, which can bottleneck the performance of ViTs. Additionally, the static nature of FPGA architectures complicates the implementation of the dynamically varying computational patterns of ViTs.\nA wide range of methods has been explored to improve the efficiency of ViTs, including approaches like quantization [6], model pruning [7], token pruning [8], and low-rank approximation [9]. However, deploying ViTs in practical applications requires innovative approaches in hardware optimization, such as strategic memory management and multi-kernel design for increased throughput. The CHOSEN framework integrates these strategies effectively in addition to its compiler to offer a complete stack for automating the FPGA deployment of ViTs. The main contributions of this paper are summarized below.\n\u2022 We present an end-to-end framework called CHOSEN, which generates high-performance ViT accelerators suited to a target FPGA device from a high-level description of the ViT model in any machine learning framework, such as PyTorch while making effective use of the available FPGA resources and memory bandwidth. Note that CHOSEN can handle any transformer-based model, while this paper focuses on ViTs.\n\u2022 We develop optimized synthesizable C++ templates, achieving high-throughput accelerator designs on FPGAs by focusing on multi-kernel design to maximize bandwidth utilization and keep all the used resources busy. The CHOSEN ViT follows static"}, {"title": "2 RELATED WORK", "content": "This section includes prior works on ViT architectures, compiler optimizations, and approximations of non-linear functions."}, {"title": "2.1 Compiler", "content": "Previous efforts have explored software-hardware co-design frameworks for efficiently deploying ViTs. The VAQF framework [10] dynamically adjusts weights precision, activations precision, and hardware settings based on FPGA resources and target FPS. ViTCOD [11] proposes methods for pruning and reconfiguring the attention maps in ViTs to create either denser or sparser patterns, which helps in managing computational workloads more effectively. However, both VAQF [10] and ViTCoD [11] do not leverage off-chip (DDR) memory parallelism and do not introduce a set of approximations for efficiently calculating non-linear functions on FPGAs."}, {"title": "2.2 Hardware architecture", "content": "Several architectures have been proposed to accelerate Vision Transformer (ViT) and Transformer inference on FPGAs, typically employing quantization techniques that reduce the model size and computational requirements by converting weights and activations to 8-bit representations such as Auto-ViT-Acc presented in [12]. \u039c\u0395-ViT [13] is a state-of-the-art ViT accelerator that mitigates the high-bandwidth demands of Vision Transformers (ViTs) during inference by employing a single-load policy and utilizes multi-purpose buffers within a memory-efficient processing element (ME-PE) to minimize memory traffic for ViT accelerators on FPGAs. They achieve this by avoiding storing and reloading in matrix multiplications and buffering the intermediate results. However, ME-ViT does not explore multiple DDR banks to achieve higher bandwidth."}, {"title": "3 CHOSEN FRAMEWORK", "content": "CHOSEN is a compilation framework that enables the optimization and deployment of ViTs on FPGAs. It takes a high-level description of the ViT model specified in PyTorch and generates a high-performance ViT accelerator design tailored to the FPGA device. The CHOSEN compiler optimizes the inference graph associated with the ViT model, aligning it with the accelerator design. Next, it produces hardware parameters for the accelerator and a static schedule for executing its various operations. A detailed explanation of the state space exploration process used to find the optimal hardware parameters is provided in Section 5. Finally, using optimized accelerator component templates, CHOSEN compiler generates synthesizable C-level code descriptions that can be used to generate an FPGA bitstream."}, {"title": "4 ACCELERATOR DESIGN OPTIMIZATIONS", "content": "This section focuses on improving the efficiency and performance of a ViT accelerator using FPGA device-aware optimizations. Solutions include data placement optimizations to reduce the number of accesses to DRAM, the use of the on-chip memory to balance computation vs. memory bandwidth, and approximations for non-linearities functions."}, {"title": "4.1 Kernel Architecture", "content": "Our accelerator is a multi-kernel design where each kernel performs the same operation. Each kernel comprises (i) a 1D array of processing elements (PEs), which are responsible for executing the MAC operations associated with the matrix multiplications, (ii) a memory hierarchy feeding data to the said 1D array compromising of register files, the on-chip memory (Block RAMs and Ultra RAMs on FPGA devices), and external off-chip memory (DRAM), and (iii) a processing unit (PU) that performs approximated non-linearity and other required operations like the addition of skip and main paths as shown in Fig. 1\nFor our array of PEs, we adopt the architecture proposed in [14]. Their architecture is used to perform efficient multiplication of A and B matrices where $A \\in R^{n\\times k}$ and $B \\in R^{k\\times m}$. The output matrix is $C \\in R^{nxm}$. The computational resources are organized into Pn of 1D processing elements, which encapsulate a vector operation of Pm compute elements (i.e., DSPs on the FPGA). To support a hierarchical hardware design, each matrix is further decomposed into several levels of tiling. We tile matrix A and B column and row-wise with factor of Tn and Tm, respectively."}, {"title": "4.2 Non-linear approximations", "content": "Implementing the non-linear functions of the Vision Transformer (ViT) presents significant challenges due to their inherent computational complexity. To enable an efficient FPGA-based implementation of ViT, we employed a set of approximations for non-linearities as presented in PEANO-ViT [15]. Specifically, they approximated the inverse square root function in layer normalization using bit manipulation operations and pre-stored fractional powers of two. For the softmax function, they utilized a Pad\u00e9-based approximation of the exponential function, complemented by bit manipulation techniques to eliminate division operations. Additionally, the GELU activation function was replaced with a piece-wise linear approximation that closely mimics the original function. These strategic"}, {"title": "4.3 Memory Layout and Data Placement", "content": "To utilize the off-chip memory bandwidth, we group activation data (A) as well as weights (B) before sending them to the on-chip memory. For both matrices (e.g., A and B), we group $2xDW^{AXI\\_WIDTH}$ of them in the column dimension. By this packing, we perform full burst read/write of data from/to the memory banks and utilize the maximum possible burst size (512-bit width) allowable on the Advanced eXtensible Interface (AXI) bus of the target FPGA board. We use all available DDR banks to load and store the data to maximize the bandwidth. We divide the original matrix column-wise into BN, which denotes the total number of available DDR banks and is 4 in our case (see Section 6 for more details regarding the target FPGA board), smaller matrices and store each smaller matrix in one DDR bank as shown in Fig. 2a. In the provided example shown in Fig. 2, we have 4 DDR banks and 4 hardware kernels. For operations outside of the head, like key, query, value matrices, and Gelu computations, we bring the data from the same row but with different DDR banks and pass them to different hardware kernels for computation (see Fig. 2c). For the head-wise operations, including softmax, we bring the data from the same bank and the same row $\\frac{N}{B_N}$ times to finish the required computations for one row where $N_h$ represents the number of heads. For instance, in the case of ViT-B, it takes three rounds to finish a row, as shown in Fig. 2b.\nWith respect to layerNorm, where each hardware kernel requires the whole row to complete its computation, such as calculating the mean and variance, CHOSEN offers efficient rotating scheduling (see Fig. 2d). Hardware kernel #1 operates on the first part of row #1 from DDR bank #1, while hardware kernel #2 performs on the second part of row #2 from DDR bank #2. Similarly, other kernels access corresponding data. In the next round, Hardware kernel #1 operates on the second part of row #1 from DDR bank#2, while hardware kernel #2 performs on the third part of row #3 from DDR bank #3. In this approach, we maximize the utilization of potential bandwidth and keep all hardware kernels busy."}, {"title": "5 CHOSEN COMPILER", "content": "This section elaborates on our compiler framework, designed to enhance the execution efficiency of transformer models on FPGAs. The compiler adeptly utilizes the underlying FPGA architecture, focusing on optimal scheduling, precise operation mapping to hardware resources, and minimizing the data movement, thereby facilitating high-performance Vit acceleration hardware on FPGAs."}, {"title": "5.1 Compiler Design and Execution Framework", "content": "Our compiler framework starts with a high-level description of a ViT model and converts it into a hardware-accelerated implementation tailored to the target FPGA's architecture. The transformation of a ViT model into an FPGA-compatible format unfolds through several steps, as explained below.\n(1) Model Conversion: Initially, the transformer model, developed using frameworks such as PyTorch or TensorFlow is transformed into a Directed Acyclic Graph (DAG). This graph outlines the model's computational dependencies and data flow, providing a basis for further analysis and optimization.\n(2) Graph Analysis and Optimization: The compiler conducts an in-depth analysis of the DAG to classify operations and deduce essential characteristics such as operation types, data sizes, and dependency chains. This information is crucial for optimizing the"}, {"title": "5.2 Compiler Optimization for Matrix Multiplications", "content": "Optimizing matrix multiplication for hardware implementations necessitates a fine-grained analysis of computational resources and data management. This section describes our approach to determining optimal tile parameters, which is crucial for enhancing the performance of matrix multiplication computations, especially in transformer models. We present a new algorithm that balances the computational load across processing elements while minimizing memory access latencies and maximizing throughput.\nThe algorithm for computing optimal tile parameters, as presented in Algorithm 1, outlines a systematic approach to identifying the most effective tiling and parallelization strategies. The algorithm explores a design space defined by the constraints of the available hardware resources, the properties of the matrix operation, and the compiler's high-level understanding of the transformer model represented as a Directed Acyclic Graph (DAG)."}, {"title": "5.2.1 Initialization and Fixed Parameters", "content": "The parallelism factor Pm is computed as $P_m = \\frac{2xDW^{AXI\\_WIDTH}}{data width}$, establishing a fixed number of computation units based on the data and AXI width. For DDR4 memory, a minimum of 512 bits as AXI width must be transferred to make up for the I/O clock multiplier, and much longer bursts are required to saturate DDR bandwidth in practice."}, {"title": "5.2.2 Exploration of Tiling Parameters", "content": "The exploration of tile sizes Tn and Tm and the parallelism factor Pn is conducted within feasible ranges determined by the hardware specifications and the nature of the matrix operations. The nested loops in the algorithm reflect an exhaustive search within these ranges, ensuring that each combination of Tn, Tm, and Pn is evaluated for its performance:\n(1) Tiling Factor Tm: Represents the number of elements from one row of matrix B that are loaded into the compute units. Each"}, {"title": "5.2.3 Cost Function for Optimizing Multiple Matrix Multiplications", "content": "In optimizing transformer models, particularly for matrix multiplications within attention mechanisms, this function is designed to minimize latency by balancing the computational load across processing elements (PEs) and computation units within PEs. The cost function, Latency, is calculated as follows:\n$Latency = \\frac{TotalCycles}{Frequency}$\n$\\frac{T_nx T_m x kx numTilesRow x numTilesCol x kernelFactor}{P_nx P_m x Frequency}$     (1)\nWhere TotalCycles is the total number of cycles required to perform all multiplication operations. Frequency is the operating frequency of the hardware accelerator.\nThe calculation of TotalCycles includes other parameters that are outlined in the following. i) Computation of Total Operations: The total operations required for a matrix multiplication between matrices A and B are determined by the tile sizes Tn and Tm, and the dimensions of the matrices involved. We modeled this calculation as: totalOps = opsPerTile\u00d7numTilesRow\u00d7numTilesCol, where each"}, {"title": "5.3 CHOSEN Algorithm for Efficient Design Space Exploration", "content": "To address the inefficiencies of the exhaustive search, a novel heuristic-based search algorithm is proposed to reduce the number of evaluation points required to converge to an optimal solution. Algorithm 2 is specifically tailored to optimize tiling parameters for deploying Vision Transformers (ViTs) on FPGA platforms, addressing constraints such as limited memory and parallel processing. The algorithm starts with a set of randomly generated tiling parameters that fit within the FPGA's specifications. It then iteratively refines these parameters to minimize latency for ViT inference.\nThe key components of our implementation are: i) Initial Param Set Creation: Parameters are generated within feasible ranges specific to FPGA constraints, ensuring a diverse starting point."}, {"title": "6 RESULTS AND DISCUSSIONS", "content": "In this study, CHOSEN-ViT's hardware model was implemented on a Xilinx UltraScale+ VU9P board running at 200 MHz. This FPGA device includes 64 GiB DDR4 ECC-protected memory with a dedicated PCIe x16 connection. There are four DDR banks. To evaluate the performance of CHOSEN-ViT, we employed the publicly available ImageNet-1K dataset [16] and two different model architectures, namely DeiT [2] and ViT [4], across various sizes (small, base, and large). It is important to point out that our experimental setup does not require extensive retraining for the proposed approximations."}, {"title": "6.1 Comparison of CHOSEN's Algorithm to the Exhaustive Search", "content": "The proposed algorithm for finding the hardware parameters greatly improved the search performance compared to exhaustive search methods. The left graph in Figure 3 depicts a decrease in the number of evaluation cases required by the proposed method compared to the exhaustive search. This reduction is consistent across all model sizes, highlighting our proposed approach's ability to target optimal configurations more directly and with fewer computations. Similarly, the right graph in Figure 3 shows a substantial decrease in execution times for the proposed method."}, {"title": "6.2 Effectiveness of CHOSEN Compiler", "content": "In this section, we assessed the effectiveness of the CHOSEN's algorithm in finding the optimal parameters for the best performance of our inference engine. The proposed algorithm demonstrates substantial effectiveness in optimizing transformer models, particularly in its ability to identify Pareto optimal evaluation points with fewer evaluated points compared to exhaustive methods. This capability is illustrated in the plots for ViT Tiny and ViT Small models, as depicted in Figs. 4 and 5.\nPareto frontier, represented by the magenta line in the plots, connects points that offer the best trade-off between latency and parallelism. The proposed approach captures nearly all the exhaustive search's Pareto-optimal points, demonstrating its efficiency. The proposed algorithm significantly reduces the number of evaluations, focusing only on configurations potentially leading to optimal outcomes. This targeted exploration is evident in the density of points along the Pareto frontier compared to the broader scatter of the exhaustive points. By focusing the search around the most promising areas of the solution space, the CHOSEN's algorithm"}, {"title": "6.3 Hardware Cost", "content": "Table 1 details the hardware metrics achieved by implementing CHOSEN-ViT. By utilizing the rapid and hardware-compatible approximations introduced by PEANO-ViT [15], the resource usage associated with hardware-intensive and costly iterative methods for exact non-linear implementation has been greatly diminished. Furthermore, Table 1 provides the resource utilization breakdown for each module in each computing kernel of CHOSEN-ViT. Please note that the hardware metrics for DeiT-Base implementation are reported. We used eight computing kernels in parallel. In processing non-linear functions such as normalization, softmax, and GELU, we simultaneously handle 16 elements, resulting in a Level of Parallelism (LoP) of 16. This LoP can be adjusted to align with resource availability and latency objectives, making CHOSEN a versatile framework for enhancing the speed of machine learning tasks. Increasing the LoP or the number of computing kernels enhances processing speed but may lead to higher resource consumption. As can be seen, our implementation can use higher DSP numbers for the matrix-matrix multiplication while having fewer LUTs and FFs due to the proposed approximations."}, {"title": "6.4 Performance Comparison with the State-of-the-Art ViT Accelerators", "content": "We compare the performance obtained by CHOSEN on different ViT models with those of the prior work. To have meaningful and fair comparisons, we compare our results only with works that have used the same or a similar FPGA board as ours, with comparable resources. Table 2 shows the performance comparison between CHOSEN-ViT and prior work references on ViT models on the ImageNet dataset. CHOSEN-ViT outperforms the state-of-the-art"}, {"title": "7 CONCLUSION", "content": "This paper introduces CHOSEN-ViT, a compiler-to-hardware optimization stack for deploying Vision Transformers (ViTs) on FPGAs. CHOSEN framework features a multi-kernel accelerator architecture that maximizes bandwidth by leveraging multiple DDR memory banks. The CHOSEN compiler enhances computing kernel performance and memory efficiency while managing data movements statically. Compared to leading ViT accelerators, CHOSEN-VIT delivers throughput improvements of 1.5x for DeiT-S and 1.42x for DeiT-B models."}]}