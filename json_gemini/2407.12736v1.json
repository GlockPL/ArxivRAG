{"title": "CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference", "authors": ["Mohammad Erfan Sadeghi", "Suhas Somashekar", "Arash Fayyazi", "Massoud Pedram"], "abstract": "Vision Transformers (ViTs) represent a groundbreaking shift in machine learning approaches to computer vision. Unlike traditional approaches, ViTs employ the self-attention mechanism, which has been widely used in natural language processing, to analyze image patches. Despite their advantages in modeling visual tasks, deploying ViTs on hardware platforms, notably Field-Programmable Gate Arrays (FPGAs), introduces considerable challenges. These challenges stem primarily from the non-linear calculations and high computational and memory demands of ViTs. This paper introduces CHOSEN, a software-hardware co-design framework to address these challenges and offer an automated framework for ViT deployment on the FPGAs in order to maximize performance. Our framework is built upon three fundamental contributions: multi-kernel design to maximize the bandwidth, mainly targeting benefits of multi DDR memory banks, approximate non-linear functions that exhibit minimal accuracy degradation, and efficient use of available logic blocks on the FPGA, and efficient compiler to maximize the performance and memory-efficiency of the computing kernels by presenting a novel algorithm for design space exploration to find optimal hardware configuration that achieves optimal throughput and latency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a 1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.", "sections": [{"title": "1 INTRODUCTION", "content": "The landscape of computer vision has been fundamentally transformed with the advent of deep learning, among which Vision Transformers (ViTs) [1-4] have emerged as a particularly promising approach. Unlike traditional convolutional neural networks (CNNs) that rely on local receptive fields, ViTs leverage the power of self-attention mechanisms to capture global dependencies within an image, enabling a more comprehensive understanding of visual data. This capability has placed ViTs at the forefront of research, demonstrating state-of-the-art performance across a wide range of tasks in computer vision. Overall, deep learning has revolutionized various domains by providing robust algorithms capable of learning complex patterns from large datasets, thus enabling unprecedented advancements in the application of artificial intelligence [5] across numerous fields, from healthcare to entertainment to scientific research."}, {"title": "2 RELATED WORK", "content": "This section includes prior works on ViT architectures, compiler optimizations, and approximations of non-linear functions."}, {"title": "2.1 Compiler", "content": "Previous efforts have explored software-hardware co-design frameworks for efficiently deploying ViTs. The VAQF framework [10] dynamically adjusts weights precision, activations precision, and hardware settings based on FPGA resources and target FPS. ViTCOD [11] proposes methods for pruning and reconfiguring the attention maps in ViTs to create either denser or sparser patterns, which helps in managing computational workloads more effectively. However, both VAQF [10] and ViTCoD [11] do not leverage off-chip (DDR) memory parallelism and do not introduce a set of approximations for efficiently calculating non-linear functions on FPGAs."}, {"title": "2.2 Hardware architecture", "content": "Several architectures have been proposed to accelerate Vision Transformer (ViT) and Transformer inference on FPGAs, typically employing quantization techniques that reduce the model size and computational requirements by converting weights and activations to 8-bit representations such as Auto-ViT-Acc presented in [12]. \u039c\u0395-ViT [13] is a state-of-the-art ViT accelerator that mitigates the high-bandwidth demands of Vision Transformers (ViTs) during inference by employing a single-load policy and utilizes multi-purpose buffers within a memory-efficient processing element (ME-PE) to minimize memory traffic for ViT accelerators on FPGAs. They achieve this by avoiding storing and reloading in matrix multiplications and buffering the intermediate results. However, ME-ViT does not explore multiple DDR banks to achieve higher bandwidth."}, {"title": "3 CHOSEN FRAMEWORK", "content": "CHOSEN is a compilation framework that enables the optimization and deployment of ViTs on FPGAs. It takes a high-level description of the ViT model specified in PyTorch and generates a high-performance ViT accelerator design tailored to the FPGA device. The CHOSEN compiler optimizes the inference graph associated with the ViT model, aligning it with the accelerator design. Next, it produces hardware parameters for the accelerator and a static schedule for executing its various operations. A detailed explanation of the state space exploration process used to find the optimal hardware parameters is provided in Section 5. Finally, using optimized accelerator component templates, CHOSEN compiler generates synthesizable C-level code descriptions that can be used to generate an FPGA bitstream."}, {"title": "4 ACCELERATOR DESIGN OPTIMIZATIONS", "content": "This section focuses on improving the efficiency and performance of a ViT accelerator using FPGA device-aware optimizations. Solutions include data placement optimizations to reduce the number of accesses to DRAM, the use of the on-chip memory to balance computation vs. memory bandwidth, and approximations for non-linearities functions."}, {"title": "4.1 Kernel Architecture", "content": "Our accelerator is a multi-kernel design where each kernel performs the same operation. Each kernel comprises (i) a 1D array of processing elements (PEs), which are responsible for executing the MAC operations associated with the matrix multiplications, (ii) a memory hierarchy feeding data to the said 1D array compromising of register files, the on-chip memory (Block RAMs and Ultra RAMs on FPGA devices), and external off-chip memory (DRAM), and (iii) a processing unit (PU) that performs approximated non-linearity and other required operations like the addition of skip and main paths as shown in Fig. 1\nfor our array of PEs, we adopt the architecture proposed in [14]. Their architecture is used to perform efficient multiplication of A and B matrices where $A \\in \\mathbb{R}^{n\\times k}$ and $B \\in \\mathbb{R}^{k\\times m}$. The output matrix is $C \\in \\mathbb{R}^{nxm}$. The computational resources are organized into $P_n$ of 1D processing elements, which encapsulate a vector operation of $P_m$ compute elements (i.e., DSPs on the FPGA). To support a hierarchical hardware design, each matrix is further decomposed into several levels of tiling. We tile matrix A and B column and row-wise with factor of $T_n$ and $T_m$, respectively."}, {"title": "4.2 Non-linear approximations", "content": "Implementing the non-linear functions of the Vision Transformer (ViT) presents significant challenges due to their inherent computational complexity. To enable an efficient FPGA-based implementation of ViT, we employed a set of approximations for non-linearities as presented in PEANO-ViT [15]. Specifically, they approximated the inverse square root function in layer normalization using bit manipulation operations and pre-stored fractional powers of two. For the softmax function, they utilized a Pad\u00e9-based approximation of the exponential function, complemented by bit manipulation techniques to eliminate division operations. Additionally, the GELU activation function was replaced with a piece-wise linear approximation that closely mimics the original function. These strategic approximations not only facilitate the efficient implementation of ViTs on FPGAs but also exhibit negligible degradation in the model's accuracy. We used all of these approximations in the CHOSEN ViT implementation to achieve high throughput and balance usage of DSP and LUTs in our design."}, {"title": "4.3 Memory Layout and Data Placement", "content": "To utilize the off-chip memory bandwidth, we group activation data (A) as well as weights (B) before sending them to the on-chip memory. For both matrices (e.g., A and B), we group $2xDW_{AXI\\_WIDTH}$ of them in the column dimension. By this packing, we perform full burst read/write of data from/to the memory banks and utilize the maximum possible burst size (512-bit width) allowable on the Advanced eXtensible Interface (AXI) bus of the target FPGA board. We use all available DDR banks to load and store the data to maximize the bandwidth. We divide the original matrix column-wise into $B_N$, which denotes the total number of available DDR banks and is 4 in our case (see Section 6 for more details regarding the target FPGA board), smaller matrices and store each smaller matrix in one DDR bank as shown in Fig. 2a. In the provided example shown in Fig. 2, we have 4 DDR banks and 4 hardware kernels. For operations outside of the head, like key, query, value matrices, and Gelu computations, we bring the data from the same row but with different DDR banks and pass them to different hardware kernels for computation (see Fig. 2c). For the head-wise operations, including softmax, we bring the data from the same bank and the same row ${N_h}\\over{B_N}$ times to finish the required computations for one row where $N_h$ represents the number of heads. For instance, in the case of ViT-B, it takes three rounds to finish a row, as shown in Fig. 2b.\nWith respect to layerNorm, where each hardware kernel requires the whole row to complete its computation, such as calculating the mean and variance, CHOSEN offers efficient rotating scheduling (see Fig. 2d). Hardware kernel #1 operates on the first part of row #1 from DDR bank #1, while hardware kernel #2 performs on the second part of row #2 from DDR bank #2. Similarly, other kernels access corresponding data. In the next round, Hardware kernel #1 operates on the second part of row #1 from DDR bank#2, while hardware kernel #2 performs on the third part of row #3 from DDR bank #3. In this approach, we maximize the utilization of potential bandwidth and keep all hardware kernels busy."}, {"title": "5 CHOSEN COMPILER", "content": "This section elaborates on our compiler framework, designed to enhance the execution efficiency of transformer models on FPGAs. The compiler adeptly utilizes the underlying FPGA architecture, focusing on optimal scheduling, precise operation mapping to hardware resources, and minimizing the data movement, thereby facilitating high-performance Vit acceleration hardware on FPGAs."}, {"title": "5.1 Compiler Design and Execution Framework", "content": "Our compiler framework starts with a high-level description of a ViT model and converts it into a hardware-accelerated implementation tailored to the target FPGA's architecture. The transformation of a ViT model into an FPGA-compatible format unfolds through several steps, as explained below.\n(1) Model Conversion: Initially, the transformer model, developed using frameworks such as PyTorch or TensorFlow is transformed into a Directed Acyclic Graph (DAG). This graph outlines the model's computational dependencies and data flow, providing a basis for further analysis and optimization.\n(2) Graph Analysis and Optimization: The compiler conducts an in-depth analysis of the DAG to classify operations and deduce essential characteristics such as operation types, data sizes, and dependency chains. This information is crucial for optimizing the execution schedule and effectively mapping the operations onto the FPGA. It is worth mentioning that the CHOSEN Compiler can optimize its nodes by leveraging combining matrices if we have enough resources. For instance, we can calculate k matrix in one shot and then separate it column-wise for each head or calculate k head-wise and name them k_h matrices. If we have enough resources on the target FPGA, we can even concatenate the weight matrices of k, q, and v to increase the second dimension of the second matrix to achieve potentially higher $T_m$. In parallel, we can increase the batch size to achieve potentially higher $T_n$. All of these cases result in larger design space.\n(3) Design Space Exploration: The compiler assesses various hardware configurations and operational parameters using a custom heuristic-based algorithm for design space exploration. This phase ascertains the optimal tiling and parallelization strategies, ensuring they align with both the hardware's capabilities and the computational demands of the model.\n(4) Code Generation and Deployment: The final phase is the generation of synthesizable C++ templates and hardware description code, culminating in the production of the FPGA bitstream. This transformation converts the high-level software model into a practical hardware implementation."}, {"title": "5.2 Compiler Optimization for Matrix Multiplications", "content": "Optimizing matrix multiplication for hardware implementations necessitates a fine-grained analysis of computational resources and data management. This section describes our approach to determining optimal tile parameters, which is crucial for enhancing the performance of matrix multiplication computations, especially in transformer models. We present a new algorithm that balances the computational load across processing elements while minimizing memory access latencies and maximizing throughput.\nThe algorithm for computing optimal tile parameters, as presented in Algorithm 1, outlines a systematic approach to identifying the most effective tiling and parallelization strategies. The algorithm explores a design space defined by the constraints of the available hardware resources, the properties of the matrix operation, and the compiler's high-level understanding of the transformer model represented as a Directed Acyclic Graph (DAG)."}, {"title": "5.2.1 Initialization and Fixed Parameters", "content": "The parallelism factor $P_m$ is computed as $P_m = {2xDW_{AXI WIDTH}}\\over{data width}$, establishing a fixed number of computation units based on the data and AXI width. For DDR4 memory, a minimum of 512 bits as AXI width must be transferred to make up for the I/O clock multiplier, and much longer bursts are required to saturate DDR bandwidth in practice."}, {"title": "5.2.2 Exploration of Tiling Parameters", "content": "The exploration of tile sizes $T_n$ and $T_m$ and the parallelism factor $P_n$ is conducted within feasible ranges determined by the hardware specifications and the nature of the matrix operations. The nested loops in the algorithm reflect an exhaustive search within these ranges, ensuring that each combination of $T_n$, $T_m$, and $P_n$ is evaluated for its performance:\n(1) Tiling Factor $T_m$: Represents the number of elements from one row of matrix B that are loaded into the compute units. Each feasible size is evaluated to find the optimal configuration of compute and memory resources.\n(2) Parallelism Factor $P_n$: Reflects the number of processing elements and is constrained by $P_n < {T_m}\\over{P_m'}$, ensuring that data for the next round of computation is already distributed to all PEs.\n(3) Tiling Factor $T_n$: Corresponds to the number of elements from one column of matrix A loaded per cycle, which, when combined with $T_m$, should not exceed the total memory capacity S as $T_n X T_m \\leq S$."}, {"title": "5.2.3 Cost Function for Optimizing Multiple Matrix Multiplications", "content": "In optimizing transformer models, particularly for matrix multiplications within attention mechanisms, this function is designed to minimize latency by balancing the computational load across processing elements (PEs) and computation units within PEs. The cost function, Latency, is calculated as follows:\n$Latency = {TotalCycles}\\over{Frequency}$ \n$TotalCycles = {T_n x T_m x k x numTilesRow x numTilesCol x kernelFactor}\\over{P_n x P_m x Frequency}$                                                                                                                                                (1)\nWhere TotalCycles is the total number of cycles required to perform all multiplication operations. Frequency is the operating frequency of the hardware accelerator.\nThe calculation of TotalCycles includes other parameters that are outlined in the following. i) Computation of Total Operations: The total operations required for a matrix multiplication between matrices A and B are determined by the tile sizes $T_n$ and $T_m$, and the dimensions of the matrices involved. We modeled this calculation as: totalOps = opsPerTile\u00d7numTilesRow\u00d7numTilesCol, where each tile's operations, opsPerTile = $T_nXT_mxk$, where sharedDimension denoted as k is the inner dimension shared between matrices A and B. numTilesRow (numTilesCol) define how many tiles matrix A (B) are divided into along its rows (columns). This equation reflects the multiplication operations needed to compute the product of submatrices defined by the tiles.\nii) Distribution of Operations Across Hardware Resources: The division of totalOps by $P_n$ and $P_m$ reflects the distribution of operations across multiple processing elements (PEs) and computation units (CU), respectively:\n$adjusted\\_cycles = {totalOps}\\over{P_n x P_m}$ x kernelFactor\nWhere $P_n$ ($P_m$) is the number of processing elements (computation units per processing element).\nThis division is critical because it ensures that the computational load is balanced across all available hardware resources, thereby optimizing the utilization of the FPGA's resources and minimizing the computation time.\niii) Adjustment for Multi-Kernel: The kernelFactor incorporates the multi-kernel nature of designed architecture, particularly pertinent for devices like the Xilinx UltraScale+ FPGA. For transformer models, where matrix multiplication is performed within multiple attention heads, this parameter is calculated as follows:\nkernelFactor = $\\begin{cases}  {num\\_heads} \\over {num\\_kernels} & \\text{if head\\_flag is true} \\\\  {1}\\over{num\\_kernels} & \\text{otherwise} \\end{cases}$\nThis adjustment is necessary to align the computational load with the physical distribution of computational resources across different kernels in the FPGA, ensuring efficient data distribution and parallel processing.\nThe exhaustive search approach to determining optimal tile parameters, as described in the Algorithm 1, explores all potential combinations of $T_n$, $T_m$, $P_n$, and $P_m$ that satisfy hardware constraints. The number of valid configurations can be exceedingly high in practical scenarios. For instance, even for small models like ViT Tiny on Xilinx UltraScale+ VU9P FPGA, we have approximately 116,144 valid combinations of tiling parameters, and it can go up to 586,554 valid combinations for ViT Small on the same FPGA. Evaluating the cost for each of these combinations demands extensive computational resources and time, resulting in a significant overhead on the compiler."}, {"title": "5.3 CHOSEN Algorithm for Efficient Design Space Exploration", "content": "To address the inefficiencies of the exhaustive search, a novel heuristic-based search algorithm is proposed to reduce the number of evaluation points required to converge to an optimal solution. Algorithm 2 is specifically tailored to optimize tiling parameters for deploying Vision Transformers (ViTs) on FPGA platforms, addressing constraints such as limited memory and parallel processing. The algorithm starts with a set of randomly generated tiling parameters that fit within the FPGA's specifications. It then iteratively refines these parameters to minimize latency for ViT inference.\nThe key components of our implementation are: i) Initial Param Set Creation: Parameters are generated within feasible ranges specific to FPGA constraints, ensuring a diverse starting point. ii) Performance Evaluation: Each configuration's effectiveness is measured specifically by its impact on latency reduction in ViT applications. iii) Evaluation Cache: Stores latency results of previously tested configurations to avoid redundant computations, optimizing the refinement process. iv)Refinement and Preservation: Ensures configurations yielding the best latency results are carried forward. v)Adaptive Refinement Strategy: Prioritizes adjustments in parameters such as $P_n$ and $T_m$, which are empirically linked to better performance in the current application, guiding the set towards potentially optimal regions of the design space.\nOur approach significantly reduces the number of evaluations required. For instance, in the case of ViT Tiny, the proposed algorithm converged to the optimal configuration with approximately 2,000 evaluations, while for ViT Small, around 3,700 evaluations were necessary. This represents a drastic reduction compared to the potential millions of evaluations required by an exhaustive search, demonstrating our algorithm's efficiency in finding near-optimal solutions within a much smaller computational budget."}, {"title": "6 RESULTS AND DISCUSSIONS", "content": "In this study, CHOSEN-ViT's hardware model was implemented on a Xilinx UltraScale+ VU9P board running at 200 MHz. This FPGA device includes 64 GiB DDR4 ECC-protected memory with a dedicated PCIe x16 connection. There are four DDR banks. To evaluate the performance of CHOSEN-ViT, we employed the publicly available ImageNet-1K dataset [16] and two different model architectures, namely DeiT [2] and ViT [4], across various sizes (small, base, and large). It is important to point out that our experimental setup does not require extensive retraining for the proposed approximations."}, {"title": "6.1 Comparison of CHOSEN's Algorithm to the Exhaustive Search", "content": "The proposed algorithm for finding the hardware parameters greatly improved the search performance compared to exhaustive search methods. The left graph in Figure 3 depicts a decrease in the number of evaluation cases required by the proposed method compared to the exhaustive search. This reduction is consistent across all model sizes, highlighting our proposed approach's ability to target optimal configurations more directly and with fewer computations. Similarly, the right graph in Figure 3 shows a substantial decrease in execution times for the proposed method."}, {"title": "6.2 Effectiveness of CHOSEN Compiler", "content": "In this section, we assessed the effectiveness of the CHOSEN's algorithm in finding the optimal parameters for the best performance of our inference engine. The proposed algorithm demonstrates substantial effectiveness in optimizing transformer models, particularly in its ability to identify Pareto optimal evaluation points with fewer evaluated points compared to exhaustive methods. This capability is illustrated in the plots for ViT Tiny and ViT Small models, as depicted in Figs. 4 and 5.\nPareto frontier, represented by the magenta line in the plots, connects points that offer the best trade-off between latency and parallelism. The proposed approach captures nearly all the exhaustive search's Pareto-optimal points, demonstrating its efficiency. The proposed algorithm significantly reduces the number of evaluations, focusing only on configurations potentially leading to optimal outcomes. This targeted exploration is evident in the density of points along the Pareto frontier compared to the broader scatter of the exhaustive points. By focusing the search around the most promising areas of the solution space, the CHOSEN's algorithm ensures that the compiler's resources are not wasted on evaluating sub-optimal configurations."}, {"title": "6.3 Hardware Cost", "content": "Table 1 details the hardware metrics achieved by implementing CHOSEN-ViT. By utilizing the rapid and hardware-compatible approximations introduced by PEANO-ViT [15], the resource usage associated with hardware-intensive and costly iterative methods for exact non-linear implementation has been greatly diminished. Furthermore, Table 1 provides the resource utilization breakdown for each module in each computing kernel of CHOSEN-ViT. Please note that the hardware metrics for DeiT-Base implementation are reported. We used eight computing kernels in parallel. In processing non-linear functions such as normalization, softmax, and GELU, we simultaneously handle 16 elements, resulting in a Level of Parallelism (LoP) of 16. This LoP can be adjusted to align with resource availability and latency objectives, making CHOSEN a versatile framework for enhancing the speed of machine learning tasks. Increasing the LoP or the number of computing kernels enhances processing speed but may lead to higher resource consumption. As can be seen, our implementation can use higher DSP numbers for the matrix-matrix multiplication while having fewer LUTs and FFs due to the proposed approximations."}, {"title": "6.4 Performance Comparison with the State-of-the-Art ViT Accelerators", "content": "We compare the performance obtained by CHOSEN on different ViT models with those of the prior work. To have meaningful and fair comparisons, we compare our results only with works that have used the same or a similar FPGA board as ours, with comparable resources. Table 2 shows the performance comparison between CHOSEN-ViT and prior work references on ViT models on the ImageNet dataset. CHOSEN-ViT outperforms the state-of-the-art ViT accelerators by delivering 1.5x and 1.42x higher frame-per-second on DeiT-S and DeiT-B. CHOSEN-ViT executes at a frequency of 200 MHz. At the same time, ME-ViT [13] reported a frequency of 300 MHz. ME-ViT [13] delivers higher FPS in the case of DeiT-T as they can bring all the weights required for this model on-chip and they do not need to tile the memory for this model as it manually optimized. Instead, CHOSEN presented an automated framework that can be used for any transformer-based model that is not limited to ViTs."}, {"title": "7 CONCLUSION", "content": "This paper introduces CHOSEN-ViT, a compiler-to-hardware optimization stack for deploying Vision Transformers (ViTs) on FPGAs. CHOSEN framework features a multi-kernel accelerator architecture that maximizes bandwidth by leveraging multiple DDR memory banks. The CHOSEN compiler enhances computing kernel performance and memory efficiency while managing data movements statically. Compared to leading ViT accelerators, CHOSEN-VIT delivers throughput improvements of 1.5x for DeiT-S and 1.42x for DeiT-B models."}]}