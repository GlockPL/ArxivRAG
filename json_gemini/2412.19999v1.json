{"title": "Comprehensive Review of EEG-to-Output\nResearch: Decoding Neural Signals into Images,\nVideos, and Audio", "authors": ["Yashvir Sabharwal", "Balaji Rama"], "abstract": "Electroencephalography (EEG) is an invaluable tool in neu-\nroscience, offering insights into brain activity with high temporal resolu-\ntion. Recent advancements in machine learning and generative modeling\nhave catalyzed the application of EEG in reconstructing perceptual ex-\nperiences, including images, videos, and audio. This paper systematically\nreviews EEG-to-output research, focusing on state-of-the-art generative\nmethods, evaluation metrics, and data challenges. Using PRISMA guide-\nlines, we analyze 1800 studies and identify key trends, challenges, and\nopportunities in the field. The findings emphasize the potential of ad-\nvanced models such as Generative Adversarial Networks (GANs), Vari-\national Autoencoders (VAEs), and Transformers, while highlighting the\npressing need for standardized datasets and cross-subject generalization.\nA roadmap for future research is proposed that aims to improve decoding\naccuracy and broadening real-world applications.", "sections": [{"title": "1 Introduction", "content": "Electroencephalography (EEG) has long been a cornerstone in the field of neuro-\nscience, offering unparalleled temporal resolution to capture the rapid dynamics\nof neural activity. Since its introduction in the early 20th century, EEG has\nundergone substantial evolution [1], transforming from a diagnostic tool primar-\nily used for identifying neurological disorders such as epilepsy [2] to a versatile\ntechnology enabling groundbreaking applications [3]. Its non-invasive nature,\nportability, and relatively low cost have made EEG an indispensable tool in\nclinical settings, cognitive neuroscience, and beyond [4,5].\nTraditional EEG applications have largely focused on signal classification\ntasks, such as detecting motor imagery, assessing mental states, or monitoring\nsleep patterns [6,7,8]. These early approaches relied on manual feature extrac-\ntion and classical machine learning methods to analyze neural signals [9,10].\nHowever, the advent of artificial intelligence (AI), particularly deep learning\ntechniques, has catalyzed a paradigm shift in how EEG data is processed and"}, {"title": "1.1 Scope and Relevance", "content": "Electroencephalography (EEG) has evolved into one of the most accessible and\nadaptable neuroimaging technologies available today, enabling researchers and\nclinicians to study the intricate interplay between neural activity and behavior in\nreal-time [4,5]. As the demand for brain-computer interfaces (BCIs) and neural\ndecoding systems grows, the ability to reconstruct perceptual experiences such as\nimages, videos, or audio from raw EEG signals represents a transformative step\nforward. Such breakthroughs could revolutionize communication for individuals\nwith severe disabilities, enhance neurofeedback applications, and redefine our\nunderstanding of the neural mechanisms underlying perception.\nThis field's significance is amplified by its potential to bridge the gap be-\ntween neuroscience and artificial intelligence (AI). Decoding perceptual stimuli\nfrom EEG data could facilitate a deeper understanding of how the brain pro-\ncesses information, leading to applications in augmented reality, cognitive train-"}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Systematic Review Process", "content": "Conducting a robust and comprehensive review of EEG-to-output research ne-\ncessitated adherence to the PRISMA (Preferred Reporting Items for Systematic\nReviews and Meta-Analyses) framework [20]. This meticulous process ensures\nthat the review is both exhaustive and transparent, providing a reliable foun-\ndation for drawing insights into the field's evolution and future trajectory. We\nfollowed a similar methodology as Jain et al. [21].\n1. Identification: Comprehensive searches were conducted across multiple\ndatabases, including PubMed, IEEE Xplore, Scopus, JSTOR, and Google\nScholar. A well-defined Boolean query was used to capture relevant studies:\n(EEG \u2229 (image U video U audio) \u2229 generative models)\nThis query targeted studies addressing the intersection of EEG and output\ngeneration via generative modeling techniques. An additional filter, publica-\ntion year (2015-2024), was applied to refine the search results, yielding an\ninitial pool of exactly 1800 studies.\n2. Screening: The first level of screening involved a rapid review of titles and\nabstracts to eliminate clearly irrelevant papers. For instance, studies focused\nsolely on traditional classification tasks [22] or non-generative EEG applica-\ntions [23] were excluded at this stage. Duplicate entries across databases were\nidentified and removed using the reference management software Mendeley.\nThis process reduced the pool to approximately 295 papers.\n3. Eligibility: A more detailed evaluation was performed on the remaining\nstudies to ensure alignment with predefined inclusion criteria:\nThe study must explicitly employ generative models (e.g., GANS, VAES,\nor Transformers) for EEG-based tasks [24]."}, {"title": "2.2 Rationale for Refinement", "content": "The systematic process described above ensures the inclusion of a representative\nyet focused subset of studies. By employing multi-tiered screening and eligibility\ncriteria, the review balances breadth and depth, capturing the state-of-the-art\nwhile remaining manageable for thorough analysis [27,28]. This structured ap-\nproach also facilitates reproducibility, allowing future researchers to replicate or\nbuild upon the findings with confidence."}, {"title": "3 Findings", "content": ""}, {"title": "3.1 Case Study: EEG-to-Image Reconstruction Using GANS", "content": "The reconstruction of images from EEG signals has seen significant advance-\nments through the application of Generative Adversarial Networks (GANs) [29].\nA notable framework is EEG2Image, which employs a two-phase approach: EEG\nfeature extraction and image synthesis using a conditional GAN (cGAN) [24].\nFeature Extraction and Triplet Loss EEG2Image utilizes a contrastive\nlearning approach with triplet loss to extract robust features from EEG signals\n[30]. The triplet loss function aims to minimize the intra-class distance while\nmaximizing inter-class distances, ensuring that EEG signals corresponding to\nsimilar visual stimuli cluster closely in the feature space [30]. Mathematically,\nthe triplet loss can be expressed as:\nLtriplet = \u2211 [|| fo(xx) \u2013 fo(xxx)|| - ||fo(xx) \u2013 fo(xx)||2 + a],\ni=1\n+'\nwhere x, x, and represent the anchor, positive, and negative samples, re-\nspectively, and a is a margin parameter ensuring a minimum separation between\npositive and negative pairs [24].\nGenerative Modeling with Mode-Seeking Regularization For image gen-\neration, EEG2Image employs a cGAN modified with mode-seeking regularization\n(MSR) to address mode collapse and improve diversity in generated outputs [31].\nThe MSR term is defined as:\nLms = ||G(21) \u2013 G(22)||1 / ||21-22||1 ,\nwhere G(z) represents the generator's output, and 21 and 22 are distinct la-\ntent vectors. By promoting diversity in generated images, MSR enhances the\nalignment of synthetic outputs with the original visual stimuli [24].\nResults and Broader Implications EEG2Image achieves state-of-the-art\nperformance, producing high-quality 128 \u00d7 128 pixel reconstructions with su-\nperior inception scores compared to other models [24]. The ability to visualize\nneural activity has profound implications for neuroscience and assistive tech-\nnology, offering a potential communication medium for individuals unable to\nverbalize their thoughts [32]. However, challenges such as dataset scarcity and\ninter-subject variability remain significant hurdles, underscoring the need for\ncross-subject generalization techniques and larger annotated datasets [33]."}, {"title": "3.2 Case Study: EEG-to-Audio Decoding and Musicality Evaluation", "content": "EEG-based reconstruction of auditory stimuli introduces unique challenges due\nto the temporal and spectral complexities of audio signals [34]. A notable work\nproposes a framework for musicality evaluation of machine-composed music using\nEEG data [35].\nMusicality Scoring with Bilinear Models The framework employs a bi-\nlinear model to quantify musicality scores based on EEG responses to auditory\nstimuli:\nf(X) = w1XmW2 + b,\nwhere X is the EEG feature matrix for subjects and stimulus m, and w\u2081 and\nw2 are projection vectors optimized to minimize inter-subject variance while pre-\nserving the ranking of musicality scores. This formulation ensures that human-\ncomposed music (HCM) scores highest, random noise sequences (RNS) score\nlowest, and partially randomized music (PRM) occupies intermediate scores [35].\nAnalysis of EEG Frequency Bands The study identifies the Gamma band\n(> 30 Hz) as the most influential in distinguishing musicality, consistent with\nfindings that link Gamma activity to emotional and auditory processing [36,37,38,39].\nAdditionally, the inclusion of DC components significantly enhances model per-\nformance, suggesting that cortical activation patterns play a critical role in mu-\nsical perception [40,41].\nImplications for Neuroaesthetics and AI Evaluation This approach not\nonly advances EEG-based audio reconstruction but also provides a quantitative\nframework for evaluating machine creativity. By aligning computational outputs\nwith human neural responses, it bridges the gap between artificial and human\ncreativity, opening avenues for the development of more intuitive AI systems in\nmultimedia applications."}, {"title": "3.3 Case Study: EEG-to-Video Synthesis", "content": "The EEG2Video framework represents a significant step in decoding dynamic vi-\nsual perception directly from EEG signals. Unlike previous methods constrained\nby static stimuli, this model reconstructs dynamic video sequences by leveraging\nEEG's high temporal resolution. The core of the framework includes a Seq2Seq\narchitecture for temporal alignment, a semantic predictor for extracting contex-\ntual information, and a novel dynamic-aware noise-adding (DANA) mechanism\nfor video synthesis [42]."}, {"title": "Methodology: Temporal Dynamics and Latent Decoding", "content": "The Seq2Seq\narchitecture in EEG2Video captures temporal dynamics by processing high-\nresolution EEG embeddings extracted via an overlapping sliding window [42].\nThe model aligns these embeddings with corresponding latent variables (zo) of\nvideo frames, minimizing reconstruction error through mean squared error loss\n[42]:\nLSeq2Seq = ||20 - 2o||2.\nAdditionally, semantic alignment is achieved using a semantic predictor that\nmaps EEG features to text embeddings derived from the CLIP encoder [42].\nThis alignment is guided by:\nLsemantic = ||e - \u00ea||2,\nwhere e and \u00ea denote ground truth and predicted embeddings, respectively.\nDynamic-Aware Noise-Adding Process (DANA) To model the diversity\nin video dynamics, the DANA module introduces a combination of static (es)\nand diverse noise (ea) into the diffusion process [42]. The balance between these\ncomponents is governed by the decoded dynamic information [42,43,44,45,46]:\nzT = \u221a\u03b1\u03c4\u03bf + \u221a1\u2212\u03b1\u03c4 (\u03b2\u03b5\u03c2 + \u221a1-\u03b2\u03b5\u03b1),\nwhere \u1e9e is dynamically adjusted based on the video's optical flow score, enhanc-\ning temporal coherence and motion realism [42].\nEvaluation: Metrics and Results EEG2Video achieves a structural similar-\nity index (SSIM) of 0.256 and a semantic-level accuracy of 15.9% on a 40-class\nvideo reconstruction task. These metrics, comparable to or exceeding fMRI-\nbased methods, highlight EEG's potential in dynamic visual decoding [47]. An\nablation study further emphasizes the importance of the Seq2Seq and DANA\nmodules, showing significant performance drops when either component is re-\nmoved.\nImplications for Brain-Computer Interfaces EEG2Video paves the way\nfor real-time applications in brain-computer interfaces (BCIs), offering tools for\nimmersive virtual reality and assistive communication. Its ability to decode dy-\nnamic, naturalistic visual stimuli could revolutionize fields such as neuroreha-\nbilitation and cognitive neuroscience. Reconstructed examples demonstrate the\nmodel's ability to generate diverse scenes, including natural environments and\nhuman activities [48]. Successful reconstructions align well with semantic and\ndynamic attributes of the source videos."}, {"title": "4 Discussion", "content": ""}, {"title": "4.1\nStrengths and Limitations of Current Approaches", "content": "The field of EEG-to-output decoding has seen tremendous progress owing to ad-\nvances in generative modeling. However, while models such as GANS, VAEs, and\nTransformers have revolutionized the reconstruction of perceptual experiences,\nthey come with inherent strengths and limitations.\nGANs excel at generating high-fidelity outputs, making them ideal for tasks\nrequiring visually or aurally realistic reconstructions [49]. Their adversarial train-\ning framework fosters creativity by encouraging the generator to outpace a con-\ncurrently trained discriminator. However, GANs are notoriously difficult to train\ndue to issues like mode collapse, where the generator fails to capture the diversity\nof the data distribution [50]. Techniques such as Wasserstein GANs and spectral\nnormalization have mitigated some of these challenges, but training instability\nremains a significant barrier [51,52].\nOn the other hand, VAEs are highly interpretable and effective at captur-\ning the latent structure of input data [53]. This makes them particularly useful\nfor tasks that demand flexibility and robustness in generating diverse outputs.\nNonetheless, the quality of images or audio generated by standalone VAEs of-\nten lags behind GAN-based models [54]. Researchers have explored hybrid ap-\nproaches, combining VAEs and GANs, to balance the strengths of both archi-\ntectures, though these frameworks require intricate tuning and longer training\ntimes [55].\nTransformers, a relatively recent addition to this field, demonstrate ex\u0441\u0435\u0440-\ntional performance in tasks involving temporal dynamics, such as video synthesis\nand speech reconstruction [56]. Their self-attention mechanism allows them to\ncapture long-range dependencies in sequential EEG data, a critical feature for de-\ncoding complex stimuli. However, their computational overhead is a significant\ndrawback, especially when applied to high-dimensional EEG signals. Further-\nmore, transformers require large datasets for optimal performance, which can be\na limiting factor given the scarcity of publicly available annotated EEG datasets\n[57].\nDespite these advancements, a common limitation across all generative ap-\nproaches is the inherent noise and variability in EEG signals. Signal artifacts\ncaused by muscle movements, environmental interference, or electrode displace-\nment can severely degrade model performance [58]. Sophisticated preprocessing\ntechniques like independent component analysis (ICA) and artifact subspace re-\nconstruction (ASR) are often employed, but these methods are not foolproof\nand may inadvertently remove useful signal components. Future innovations in\nartifact rejection, such as deep-learning-based artifact detection systems, could\nsignificantly enhance the reliability of EEG-to-output systems."}, {"title": "4.2 Challenges with Datasets and Cross-Subject Variability", "content": "One of the most pressing challenges in the domain is the lack of standardized,\nlarge-scale EEG datasets. Existing resources like DECAF and OpenNeuro pro-"}, {"title": "4.3 Ethical and Practical Considerations", "content": "As EEG-to-output decoding technologies advance, ethical considerations become\nincreasingly salient. The ability to reconstruct private perceptual experiences\nraises profound questions about consent and privacy. For instance, decoding vi-\nsual imagery or internal speech from EEG signals could potentially infringe on an\nindividual's cognitive autonomy. Ensuring that these technologies are developed\nand deployed responsibly is paramount.\nFrom a practical standpoint, the cost and complexity of EEG data acquisition\npresent additional barriers to widespread adoption. High-density EEG systems\ncapable of capturing fine-grained neural activity are expensive and require skilled\noperators, limiting their accessibility in resource-constrained settings. The de-\nvelopment of low-cost, portable EEG systems with comparable performance is\nan active area of research, with promising innovations such as dry electrodes and\nwireless EEG systems.\nReal-time applications of EEG-to-output systems, such as assistive commu-\nnication devices, also pose unique challenges. Achieving low-latency decoding\nwhile maintaining high accuracy requires substantial computational resources,\noften necessitating the use of specialized hardware like GPUs or edge computing\ndevices. Balancing these requirements with cost-effectiveness will be crucial for\ntranslating these technologies into real-world applications."}, {"title": "4.4 Future Directions", "content": "Addressing the aforementioned challenges will require concerted efforts across\nmultiple domains. One promising avenue is the integration of multimodal data,\ncombining EEG with complementary modalities such as functional near-infrared\nspectroscopy (fNIRS) or magnetoencephalography (MEG). Multimodal approaches\ncan provide richer representations of neural activity, potentially enhancing the\naccuracy and robustness of generative models.\nAdvances in model architecture also hold significant promise. Techniques\nsuch as attention-augmented convolutional networks and graph neural networks\n(GNNs) are well-suited to capturing the spatiotemporal dynamics of EEG data\n[61]. Additionally, the use of federated learning frameworks could facilitate the\ntraining of models on distributed datasets while preserving data privacy, ad-\ndressing both ethical and practical concerns.\nStandardization efforts will be equally important. Establishing open-access\nrepositories with diverse, annotated EEG datasets and creating universally ac-\ncepted evaluation benchmarks will provide a foundation for consistent and re-\nproducible research. Collaborative initiatives between academia, industry, and\nregulatory bodies could accelerate progress in this direction.\nFinally, a greater emphasis on interpretability is needed to ensure that EEG-\nto-output systems are not only accurate but also understandable to end-users\nand stakeholders. Techniques such as saliency mapping and explainable A\u0399 (\u03a7\u0391\u0399)\ncan provide insights into how models process EEG data, fostering trust and\nfacilitating adoption in sensitive applications."}, {"title": "5 Conclusion", "content": "The field of EEG-to-output decoding is poised at the intersection of neuroscience\nand artificial intelligence, offering unprecedented insights into the human brain\nand its perceptual processes. This paper has systematically reviewed state-of-\nthe-art generative approaches, highlighting their strengths, limitations, and ap-\nplications in reconstructing images, videos, and audio from EEG signals. Despite\nsignificant advancements, challenges such as dataset scarcity, cross-subject vari-\nability, and ethical considerations continue to impede progress.\nLooking ahead, addressing these challenges will require innovative solutions,\ninterdisciplinary collaboration, and a commitment to ethical development. By\nleveraging advances in generative modeling, multimodal integration, and stan-\ndardized benchmarks, the field can move closer to realizing its potential for trans-\nformative real-world applications. The roadmap proposed in this paper provides\na foundation for these efforts, aiming to catalyze further innovation and con-\ntribute to the broader understanding of brain-computer interfaces and neural\ndecoding."}]}