{"title": "THE LABYRINTH OF LINKS: NAVIGATING THE ASSOCIATIVE MAZE OF MULTI-MODAL LLMS", "authors": ["Hong Li", "Nanxi Li", "Yuanjie Chen", "Jianbin Zhu", "Qinlu Guo", "Cewu Lu", "Yong-Lu Li"], "abstract": "Multi-modal Large Language Models (MLLMs) have exhibited impressive capa-\nbility. However, recently many deficiencies of MLLMs have been found com-\npared to human intelligence, e.g., hallucination. To drive the MLLMs study, the\ncommunity dedicated efforts to building larger benchmarks with complex tasks.\nIn this paper, we propose benchmarking an essential but usually overlooked in-\ntelligence: association, a human's basic capability to link observation and prior\npractice memory. To comprehensively investigate MLLM's performance on the\nassociation, we formulate the association task and devise a standard benchmark\nbased on adjective and verb semantic concepts. Instead of costly data annota-\ntion and curation, we propose a convenient annotation-free construction method\ntransforming the general dataset for our association tasks. Simultaneously, we de-\nvise a rigorous data refinement process to eliminate confusion in the raw dataset.\nBuilding on this database, we establish three levels of association tasks: single-\nstep, synchronous, and asynchronous associations. Moreover, we conduct a com-\nphensive investigation into the MLLMs' zero-shot association capabilities, ad-\ndressing multiple dimensions, including three distinct memory strategies, both\nopen-source and closed-source MLLMs, cutting-edge Mixture-of-Experts (MoE)\nmodels, and the involvement of human experts. Our systematic investigation\nshows that current open-source MLLMs consistently exhibit poor capability in\nour association tasks, even the currently state-of-the-art GPT-4V(vision) also\nhas a significant gap compared to humans. We believe our benchmark would\npave the way for future MLLM studies. Our data and code are available at:\nhttps://mvig-rhos.com/llm_inception.", "sections": [{"title": "INTRODUCTION", "content": "Multi-modal Large Language Models (MLLMs) have recently made significant breakthroughs in\nperceiving diverse modality input and solving a broad range of tasks Zhang et al. (2024a); Carolan\net al. (2024). As GPT-4V(ision) Achiam et al. (2023) and Gemini Team et al. (2023); Reid et al.\n(2024) address challenges that researchers have been exploring for a considerable period. Subse-\nquently, numerous researchers have developed diverse open-source MLLMs AI et al. (2024); Bai\net al. (2023b); Wang et al. (2024b); Dong et al. (2024); Liu et al. (2023a); Li et al. (2024a); Ye et al.\n(2023; 2024). These models usually use the Large Language Model (LLM) as the core component\nand expand to multi-modal with a specific module Yin et al. (2023) that transfers multi-modal tokens\ninto language tokens, achieving alignment between different modality encoders.\nMLLMs demonstrated ability in visual reasoning, which requires understanding the input query\nand then making judgments based on the visual content. Much prior work has been dedicated to\nevaluating the levels of their visual reasoning capabilities. However, to the best of our knowledge,\nhow to evaluate the association ability of MLLMs is overlooked. Association is one of the most\nfundamental capabilities of human intelligence. It provides the foundation for creative thinking and\nhelps humans to summarise scattered information into structured knowledge to enhance memory\nand understanding Mednick (1962); Ausubel (1963), perception, rule discovery, embodied AI, etc."}, {"title": "RELATED WORKS", "content": "Multi-Modal Visual Reasoning. Various works are dedicated to understanding and reasoning about\nthe semantic concept between multi-images. Several works, such as Visual Genome Krishna et al.\n(2017) and Bongard-HOI Jiang et al. (2022), target human-object interaction (HOI) tasks to investi-\ngate the visual relationship between different objects. Recent work Zhang et al. (2024b) investigates\nthe MLLM's ability in low-level perception question-answering and description tasks with paired\ninput images. Despite their success in perceiving and understanding multi-images, these methods\nare confined to single-step evaluation, lacking the investigation of multi-step association ability.\nMulti-Modal Large Language Model. There has been a surge of MLLMs in the deep learning\ncommunity, which effectively use off-the-shell LLMs to support multi-modal inputs and demon-\nstrate promise for zero-shot generalization. For instance, LLaVA Liu et al. (2023a) InstructBLIP Dai\net al. (2023), InternLM-XComposer Dong et al. (2024), Qwen-VL Bai et al. (2023a), MiniGPT-"}, {"title": "\u0391\u039d\u039d\u039f\u03a4ATION-FREE ASSOCIATION CONSTRUCTION", "content": "For a general dataset with N samples, it can be formed as {(xi, Yi) | Xi \u2208 X, Yi \u2208 {C1, ..., Cn}, i =\n1, 2, . . ., N}. Let xi = (x1,...,xn) be a sample with k modalities, more specifically, for a task\nwith individual modality, the sample is described as xi = (x1, ). Yi is the annotation for sample xi,\nwhich may existed in various granularities. These annotations typically indicate a semantic concept,\nwith object categories as nouns, actions as verbs, as well as attributes, and affordance as adjectives.\nGiven the definition, labels represent a subset of concepts present in the given sample. The core of\nhuman association, on the other hand, involves identifying the overlapping concepts between newly\nacquired observations and prior practice memory. Hence, it is intuitive to use annotations from the\nraw dataset to construct the association chain, which reflects the common concepts shared by two\nsamples, such as the presence of the \"shoot ball\" action in both samples. In the following, we first\nintroduce the method for generating association chains. We then introduce a method that deduces\nthe evidence of the association to simulate the thought behind the human, which involves predicting\nthe common concepts among the samples that constitute the association.\nGenerating Semantic Concept Association Chain. For a general dataset {(xi, Yi) | Xi \u2208 X, Yi \u2208\n{C1,..., Cn}, i = 1,2,...,N}, we randomly select two samples to form a new sample pair for\nassociation tasks. If the selected samples have identical labels or share at least one common label,\nwe assign a corresponding label of 1; otherwise, we assign a label of 0. Hence, we get the new\nassociation dataset as:\n{(Xi, Xj, Zij) | Xi, xj \u2208 X, 1 < i < j < N}, where zij =\n\\begin{cases}\n1 & \\text{if } y_i \\cap y_j \\neq \\emptyset \\\\\n0 & \\text{otherwise}\n\\end{cases}\n(1)\nIn this way, for each sample xi in the original dataset, we construct a positive association set with\nK positive samples, x+ = [p,...,p], where at least one potential association concept exists.\nAdditionally, we devise another negative association set with L samples, x = [q,..., q], in\nwhich no association chain is present.\nDeducting the Evidence of Association Chain. It is crucial to deduce the evidence in the asso-\nciation task, which simulates the human reflection process. This evidence then guides and directs\nthe decision-making in the subsequent steps. To realize this, we collected the full set of shared con-\ncepts within all possible positive set \u00e6\u2021 as \u0108 = U1 {Zij | Zij = Yi \u2229 Yj, 1 < i < j \u2264 N} . In this\nsetting, the dataset deducting the evidence of the association chain is depicted as:\n{(xi, Pi, Si), xi, p\u2208 X, p\u2208 x, s\u2081 = {s},..., s} c\u0108},\nS\n(2)\nwhere p represents the correctly predicted positive sample at the stage of chain association, and\nsi = {s},...,s} denote the R common concepts."}, {"title": "ASSOCIATION BENCHMARK", "content": "In this section, we introduce a benchmark based on the semantic concepts of adjectives and verbs,\ni.e., object attributes and affordances, and human actions. Specifically, we utilize the annotation-\nfree construction method proposed in Section 3 on the Object Concept Learning (OCL) Li et al.\n(2023) to generate an attribute and affordance association datasets, and on the Pangea Li et al.\n(2024b) to generate action association dataset 1. In the following, we comprehensively investigate\nthe association ability from the single-step association, synchronous association, and asynchronous\nassociation settings as shown in Figure 2.\n4.1 CONSTRUCTING ASSOCIATION TASK\n4.1.1 SINGLE-STEP ASSOCIATION\nThe association refers to the link between the current observation and prior practice memory (Fig-\nure 2a). A single-step association represents one phase within a broader association task, where\nmemory is indispensable in decision-making. In this case, we include the correct memory to simu-\nlate prior practice, guiding the decision-making process. In the experiment, we compute the associ-\nation and deduction success ratio as the main single-step association metric (Section 5.1.2).\n4.1.2 SYNCHRONOUS ASSOCIATION\nSynchronous association, where each step adheres to the same principle throughout the entire pro-\ncess, is a core capability of human intelligence (Figure 2b). With this ability, humans can gradually\nunveil the underlying rules of the task and reduce the likelihood of errors. Utilizing our constructed\nassociation dataset, we take different input-paired samples with the same association concepts \u0109t to\nevaluate the synchronous association. In this setting, the association dataset can be depicted as:\n{(Xi, Xj, Zij) | Xi, Xj \u2208 X, 1 \u2264 i < j < N}, where Zij =\n\\begin{cases}\n1 & \\text{if } \\hat{c}_t \\subset \\{Y_i \\cap Y_j \\} \\\\\n0 & \\text{if } y_i \\cap y_j = \\emptyset\n\\end{cases}\n(4)\nIt is worth noting that current MLLMs lack memory during the inference stage, relying solely on the\ninput samples. To address this problem, we introduce a memory base to imitate the human's memory\nin the synchronous association. Specifically, we transfer the inference process into the memory base\nafter each step. The updated memory and the input sample are then used for the next step. In the\nexperiment, we compute the Max | Mean steps metric to evaluate the model (Section 5.1.2).\n4.1.3 ASYNCHRONOUS ASSOCIATION\nWhen there are multiple principles in games, the underlying principle will gradually change as the\ngame progresses. For example, the first two steps with \"metal\" as the chain, then \u201cfurry\u201d and again"}, {"title": "DATA REFINEMENT FOR ASSOCIATION TASK", "content": "Depending on annotation-free association construction methods, we transfer the general dataset for\nthe association task. The dataset has paired input samples and is labeled with whether they have\ncommon concepts for the association step. Furthermore, paired input samples with shared concepts\nwere created for the deduction step. While these support the challenges in association tasks, there is\nstill a possibility of confusing samples. To address this, we introduce a data refinement method.\nWe implemented a three-step strategy to ensure data quality, including an Image resolution filter,\nMLLM verification, and Human expert evaluation. Specifically, the Image resolution filter screened\nout all images with less than 50,000 pixels to ensure superior visual quality. The MLLM verifica-\ntion 2 takes a question-answer strategy with OpenAI's GPT4-V Achiam et al. (2023) and Google's\nGemini-1.5-Flash Reid et al. (2024) to ensure each annotation of raw data exists in the image. Then,\nthe human expert evaluation is conducted through our custom-designed interface, enabling testers to\ncomplete the association task and eliminate low-quality samples or those with ethical concerns 3.\nOur benchmarks are implemented in adjective concepts and verb concepts, which include attribute\nand affordance in OCL Li et al. (2023) and action in Pangea Li et al. (2024b). In the OCL dataset,\nwe selected eight attributes with good perception performance, such as \u201cmetal, ripe, fresh, natural,\ncooked, painted, rusty, furry\", and eight affordances \u201csit, imprint, push, carry, cut, clean, open,\nbreak\". In addition, we selected eight actions of \u201crun, hit, drive, dress, cooking, build, shake, cut\u201d.\n4.3 BASELINE FOR ASSOCIATION\nThere are various methods to improve the concept perception. Our focus lies in exploring tuning-\nfree methods, which harness the inherent capabilities of the model. To this end, we employ popular\nprompt engineers to improve the understanding of MLLMs, including common knowledge (Com-\nKnow), one-shot, and chain-of-thought (CoT).\""}, {"title": "EXPERIMENTS", "content": "5.1 SETTINGS\n5.1.1 IMPLEMENTATION DETAILS\nWe systematically conduct experiments that include concept perception, single-step, synchronous,\nand asynchronous association. Specifically, we implement the task as a multi-choice setting in all\nexperiments, taking one query image and two candidate images with one correct as input and output\ncorrect image index. Based on this, we first devise preliminary concept perceptions that involve\npopular prompt engineering skills on open-source MLLMs 4 to investigate perception capabilities\nin attribute concept. Then, we convert to evaluate MLLM's association capabilities that make de-\ncisions based on current observation and prior practice memory, i.e., input with additional content\nof previous practice. We develop single, synchronous, and asynchronous associations according to\nfixed or dynamic memory. The single-step association means the model decides with observation\nand correct prior practice. Meanwhile, the synchronous and asynchronous association set model at\na dynamic task that iteratively arrives at a decision and then deducts the underlying evidence, which\nmeans the memory may have wrong information for the next judge.\nFor single-step, synchronous, and asynchronous association, we design three types of memory\nbases, i.e., Structure Memory (StructM), Natural Language Memory (NLM), and Chain Memory\n(ChainM). In addition, we involve the baseline of No Memory (NoM) which means determining at a\ndynamic setting without the memory base. For the detailed description of the type of memory base\nand the usage in the prompt, please refer to the section A.2 in the supplementary.\nWe utilize three open-source MLLMs in preliminary concept perception: QWen-VL Bai et al.\n(2023b), LLaVA-NeXT-7B Liu et al. (2024), and LLaVA-NeXT-13B. For formal association, we\nutilize three new-versions MLLMs that break through the MLLM's capabilities in multi-images:\nLLaVA-OneVision Li et al. (2024a), QWen2-VL Wang et al. (2024b), and mPLUG-Owl3 Ye et al.\n(2024). Besides, we evaluate the performance of Mixture-of-Experts (MoE) that combined three\nopen-source MLLMs. In experiments, open-source MLLM is run on a single NVIDIA A100 80G\nGPU. Apart from open-source MLLMs, we include the evaluation of the closed-source MLLMs of\nGPT4-V Achiam et al. (2023) and Gemini-1.5-Flash Reid et al. (2024). Simultaneously, we involve\nthe results of three human experts to demonstrate the gap between MLLM with human intelligence.\n5.1.2 \u039cETRICS FOR ASSOCIATION TASK\nMax | Mean Step. In an association task, max-step indicates the maximum number of steps in one\nround of association, i.e., the maximum length of a correctly predicted association chain. While\nmean-step refers to the average maximum association step across multi-rounds of association tests."}, {"title": "ANALYSIS OF RESULTS", "content": "Analysis of Failure Case. We now analyze the failure cases in the association process. We di-\nvided them into two types according to the stage in our benchmark, i.e., Error deduction, and Error\nAssociation. In the first type, the MLLM produces the error perception about the common concepts\nin two samples. This then causes the error in the association step (Left of Figure 5). Conversely,\nin the second type, MLLM has the correct memory and makes errors due to the limited perception\ncapability (Right of Figure 5). More interesting, these failures are consistent with human tests, we\nmay derive error information from examples and further induce incorrect judgments.\nAnalysis of Attention. We demonstrate that there is a significant gap between current open-source,\nand closed-source MLLMs and human intelligence. We speculate that the underperformance arises\nfrom two main factors: a lack of multi-image instruction tuning and limitations in contextual un-\nderstanding. The first statement is also observed from existing work Song et al. (2024); Wang et al.\n(2024c), that the current MLLM has a weak ability in multi-image understanding since the lack of\nits instruction data. In addition, recent research Wang et al. (2024a) has demonstrated MLLMs'\npoor performance in handling long contexts, i.e., memory. We also visualize the attention map of\nopen-source MLLM Qwen-VL on OCL attribute concept to support this observation, as Figure 6,\nwe can easily find that response has predominate attention at the position close to response instead\nof the part for decision-making in both StructM and NLM."}, {"title": "ABLATION STUDY", "content": "We conduct ablation studies that take different example sizes in the associations to analyze the\nselection of an initialized memory base. Table 2 summarises the results for example sizes of 1, 3,\nand 5 in the LLaVA-OneVision Li et al. (2024a) and QWen2-VL Wang et al. (2024b). Regardless of\nwhether considering the max or mean step, the influence of different sample sizes is minimal, with\nan average gap of 1.51 for the maximum step and 0.05 for the mean step. As the maximum step\nindicates peak performance in some cases, while the mean step reflects more stable performance,\nwe have chosen a sample size of 3 based on the mean step results for all our experiments."}, {"title": "LIMITATIONS AND DISCUSSION", "content": "In this paper, our investigation is limited to MLLMs' zero-shot ability in association tasks across\nadjectives and verb semantic concepts. Experiments demonstrate that although MLLMs make ad-\nvances in other scenarios, they exhibit weak ability in association. We speculate that this deficiency\nmay be due to the lack of learning of unpaired data. To the best of our knowledge, current MLLMS\nare trained on image-text pairs and interleaved image-text pair data, providing them with a powerful\nability to comprehend input information. However, the association task requires MLLM to have the\ncapability of inference on unpaired sequence data, as well as the ability to gradually uncover the un-\nderlying principles through the process of all prior decision-making. In the future, an urgent study\nis still needed to develop a paradigm that links new learning with prior learned knowledge, which\nmay enhance MLLM's association capabilities. We believe that the next stage of MLLMs should\nexpand the learning of unpaired data, which may require the creation of a new learning framework.\nThis advancement will help narrow the gap between MLLMs and human intelligence."}, {"title": "CONCLUSION", "content": "In this paper, we propose a benchmark to evaluate the association ability of MLLMs via an\nannotation-free association construction method that easily transfers general datasets for association\ntasks. Using this method, we devise a standard benchmark based on adjective and verb semantic\nconcepts as the association chain. Expanding experiments demonstrate that current open-source\nMLLMs and even GPT-4V have a significant gap compared to humans."}, {"title": "IMPLEMENTATION DETAILS", "content": "A.1 IMPLEMENTATION OF DATA REFINEMENT\nIn our experiment, we comprehensively develop three steps to consolidate the data quality in our\ntasks, including Image resolution filter, MLLM verification, and Human expert evaluation.\nImage Resolution Filter. In this work, we concentrate on the object within the image, yet the\ncurrent object detection and object concept learning datasets typically represent only partial content\nof the images, posing significant perception challenges. Hence, the Image resolution filtering step\nexcludes all images with fewer than 50, 000 pixels, ensuring sufficient visual quality.\nMLLM Verification. While an image resolution filter ensures that images contain sufficient in-\nformation for downstream tasks, there remain existing some concerns. One is the correctness of\nraw annotations, and the other is perception ability in difficult categories. We propose an MLLM\nverification step that further filters input samples with erroneous annotations or those requiring ad-\nvanced perception capabilities. This step does not introduce bias, as the filtering method relies on a\nwidely used public dataset with only insignificant shortcomings. Specifically, we employ a question-\nanswers strategy to check whether the model identifies the existence of one concept. This process\nbegins with Gemini-1.5-Flash, then GPT4-V once Gemini-1.5-Flash is unable to reach a decision.\nHuman Expert Evaluation. The first two steps ensure the correctness of our benchmark, which\nreduces the image with low quality or confusing annotation. We continue to develop an online\ntesting interface for human testers, as shown in Figure 7, which further filters the images with\npotential confusion or ethical concerns from the human perspective. We have strictly followed the\nethical review, which is described in section F of the supplementary.\nA.2 IMPLEMENTATION OF MEMORY BASE\nIn our proposed association tasks, whether involving single-step, synchronous and asynchronous\nassociations, we developed three memory strategies to emulate human intelligence: Structure Mem-\nory (StructM), Natural Language Memory (NLM), and Chain Memory (ChainM). As in Table 3, the\nprompt in the association task can be divided into three parts. The first component is the memory\ncontext, including the memory instructions and base. Next is the question content, comprising the"}, {"title": "PRELIMINARY INVESTIGATION ON CONCEPT PERCEPTION", "content": "To thoroughly access the MLLM's ability in concept perception, we systematically design multiple\ntask complexities across multiple tunning-free methods. Specifically, we involve four task complex-\nity settings across two types in all models, including individual and combination perceptions. The\nindividual perception includes Task Instruction (TaskInstr) and Meta Instruction (MetaInstr). How-\never, combination perception consists of Task Instruction with Deduction (TaskInstr w/ DedPo) and\nMeta Instruction with Deduction (MetaInstr w/ DedPo), which involves another deduction step com-\npared with individual perceptions. The distinction between TaskInstr and MetaInstr lies in whether\nthe prompt includes question instruction that describes the decision-making explanations of the task.\nIn addition, with Deduction (w/ DedPo) compared to normal, is whether we only access percep-\ntion or evaluate deduction ability. On the other hand, enhancing perception capabilities can be\napproached from several directions, including expanding foundational knowledge and incorporating\npractical examples. Hence, we include several prompt engineering skills for tunning-free meth-\nads such as common knowledge (ComKnow), one-shot, and Chain-of-Thought (CoT). ComKnow\naims to provide foundational knowledge of object-related concepts, while One-shot and CoT focus\non enhancing performance through few-shot learning. Compared to One-shot, CoT introduces an\nadditional reasoning step to emulate human-like thought processes.\nWe carry out experiments on three open-source MLLMs. QWen-VL Bai et al. (2023b) uses QWen-\n7B Bai et al. (2023a) as the initialize of LLM and OpenCLIP's ViT-bigG Cherti et al. (2023) as the\nvisual encoder. LLaVA-NeXT-7B Liu et al. (2024) is constructed on the LLM of Mistral-7B Jiang\net al. (2023) and vision encoder of CLIP's ViT-L/14 Radford et al. (2021), while LLaVA-NeXT-13B\nuses the same vision encoder but is constructed on LLM of Vicuna-13B Zheng et al. (2024)."}, {"title": "ETHIC REVIEW", "content": "As shown in Figure 7, in our human evaluation protocol, we have implemented a comprehensive\nEthic Report mechanism to proactively address and mitigate potential ethical concerns. The user\ninterface incorporates dedicated options for participants to report issues related to privacy or other\nethical considerations. This is facilitated through a structured reporting system comprising cate-\ngorical buttons and an open-ended text field for additional context. This approach enables active\nparticipant engagement in ethical oversight during the evaluation phase, fostering a collaborative\napproach to responsible AI development. We prioritize transparency by empowering participants\nto articulate concerns on potential privacy infringements, algorithmic bias, or instances where the\nsystem may induce discomfort or exhibit opaque behavior. This methodology aligns with the best\npractices delineated by Zaldivar et al. (2019) Kennedy-Mayo & Gord (2024), which emphasizes\nthe criticality of integrating transparency and user feedback mechanisms to ensure fairness and ac-\ncountability in machine learning systems.\nBesides the ethical review implemented within our human testing demo, it is crucial to emphasize\nthat our approach builds upon the previous ethical review of the original datasets. They have un-\ndergone a rigorous ethical review process, particularly concerning data sourcing, privacy considera-\ntions, and bias mitigation. This prior evaluation also sets a strong foundation for ethical safeguards."}]}