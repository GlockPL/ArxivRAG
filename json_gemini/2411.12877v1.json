{"title": "The Illusion of Empathy: How AI Chatbots Shape Conversation Perception", "authors": ["Tingting Liu", "Salvatore Giorgi", "Ankit Aich", "Allison Lahnala", "Brenda Curtis", "Lyle Ungar", "Jo\u00e3o Sedoc"], "abstract": "As AI chatbots become more human-like by incorporating empathy, understanding user-centered perceptions of chatbot empathy and its impact on conversation quality remains essential yet under-explored. This study examines how chatbot identity and perceived empathy influence users' overall conversation experience. Analyzing 155 conversations from two datasets, we found that while GPT-based chatbots were rated significantly higher in conversational quality, they were consistently perceived as less empathetic than human conversational partners. Empathy ratings from GPT-40 annotations aligned with users' ratings, reinforcing the perception of lower empathy in chatbots. In contrast, 3 out of 5 empathy models trained on human-human conversations detected no significant differences in empathy language between chatbots and humans. Our findings underscore the critical role of perceived empathy in shaping conversation quality, revealing that achieving high-quality human-AI interactions requires more than simply embedding empathetic language; it necessitates addressing the nuanced ways users interpret and experience empathy in conversations with chatbots.", "sections": [{"title": "Introduction", "content": "Empathetic communication is crucial in text-based interactions (e.g., conversations) because it enables both sides of the interaction to process, understand, and respond to each other's emotional needs (Decety and Jackson 2004), thereby enhancing likability and trust in each other (Brave, Nass, and Hutchinson 2005). There has been research investigating empathetic communication in human-human and human-bot conversations (Hosseini and Caragea 2021) as well as in the development of empathetic chat agents (Casas et al. 2021).\nHowever, it remains unclear whether, how, and to what extent perceived empathy differs between chatbots and humans, and how these differences affect the perceived quality of conversations. There is a gap in understanding how users perceived empathy changes when chatting with a chatbot versus a human. Previous studies have not adequately explored whether these chatbots attain high levels of perceived empathy from users' perspective or how these factors impact overall conversation quality.\nIn human-bot conversations, empathetic chatting can provide a more personalized and supportive experience by recognizing users' feelings and concerns and offering tailored support and resources. Therefore, many current studies have been devoted to generating more empathetic responses in language (Gao et al. 2021). The empathetic level of the language-ranging from basic acknowledgment of feelings to deep empathetic engagement\u2014can significantly influence user perceptions.\nThe current study investigates how human perceptions of conversation quality are influenced by two key factors: a) the identity of the conversation agents (humans vs. chatbots) and b) user-centered perceived empathy of the chat partner. Additionally, the study examines how these factors individually affect the empathy expressed in human responses. By analyzing these dynamics, the research aims to provide insights into how knowledge of a chatbot's identity and the empathetic quality of its communication impact users' perceptions.\nThe current study makes two significant contributions to the understanding of chatbot.\n\u2022 First, perceived empathy impacts conversation quality differently for chatbots and humans: while chatbots receive slightly higher ratings for overall conversation quality, users still perceive them as less empathetic than human counterparts.\n\u2022 Second, chatbots consistently demonstrate lower perceived empathy than humans, a finding supported by alignment across large language model annotations, off-the-shelf empathy models, and user self-reports."}, {"title": "Related Work", "content": "Linguistic research on empathy in human language use has been conducted through the lens of qualitative approaches, such as conversation analysis (CA). These qualitative approaches have investigated how empathy is expressed in conversations (Alam, Danieli, and Riccardi 2018; Per\u00e4kyl\u00e4 2012), including through affiliative responses to complaint stories (Lindstr\u00f6m and Sorjonen 2012), emotion expression (Alam, Danieli, and Riccardi 2018), reactions to each other's emotions (Herlin and Visap\u00e4\u00e4 2016), and the grammatical structures used to convey empathy (Atkinson and Heritage 1984). For example, a common progression in conversations is the use of affiliative turns (Jefferson 1984).\nThat is, in research on human-bot conversations, efforts often focus on humanizing bots in various aspects, such as appearance and language use, to enhance engagement or interaction quality. This has led to the development of emotionally aware conversational systems or social chatbots that use sentiment analysis, automatic emotion recognition, and affect prediction (Alam, Danieli, and Riccardi 2018; Raamkumar and Yang 2022). The development of empathetic chatbots typically centers on recognizing emotions within conversations and delivering empathetic responses (Casas et al. 2021; Lin et al. 2020; Wardhana, Ferdiana, and Hidayah 2021).\nTherefore, previous studies on developing empathetic conversational agents have often focused on enhancing empathy through linguistic strategies, such as empathetic language expression and response formulation (Shi et al. 2021; Zhou et al. 2021; Yaden et al. 2023; Abdul-Mageed et al. 2017). For example, Zhou et al. (2021) explored the relationship between empathy and textual stylistic properties, focusing on interdependent thinking, integrative complexity, and lexical choices. Sharma et al. (2020) also effectively modeled empathy in text-based, asynchronous, peer-to-peer support conversations using three indicators: emotional reactions, interpretations of the seeker's feelings, and explorations of implicit experiences in their posts. Recently, advancements in large language models (LLMs) offer better conversational skills and hold great potential to improve empathy in human-bot interactions in conversations (Sorin et al. 2023). For example, preliminary evidence suggested that LLMs can produce responses that are consistently rated as more empathetic than those generated by humans (Lee et al. 2024b).\nHowever, these approaches often overlook a critical component: how users themselves perceive and experience empathy during interactions with these models. Existing literature that evaluates perceived empathy in LLMs or dialogue systems frequently relies on third-party annotations of LLM responses or comparisons to human responses in similar contexts (Lee et al. 2024b; Welivita and Pu 2024a). This approach provides an objective, third-party perspective, but it may miss the nuanced, subjective experience of empathy as felt by the actual users. Our study seeks to bridge this gap by focusing on user-centered evaluations, offering insights into how empathy is perceived directly by those chatting with chatbots."}, {"title": "Evaluation of Perceived Empathy in Human-Chatbot Conversations", "content": "Traditional approaches to evaluating chatbot empathy focus largely on assessing the language within responses, often overlooking the user's actual perception of empathy from the chatbot (Gao et al. 2021; Wardhana, Ferdiana, and Hidayah 2021; Rashkin et al. 2018; Xu and Jiang 2024). Studies following the EMPATHETICDIALOGUES framework (Rashkin et al. 2018) typically rely on single-question metrics that assess emotional expression in responses, such as \"How much emotional understanding does the response show?\" (Majumder et al. 2020). While recent efforts have introduced psychological theories and categorized empathy into dimensions like \u201cseeking-empathy\u201d and \u201cproviding-empathy\" (Hosseini and Caragea 2021), these approaches are still limited. Many studies depend on third-party annotations or scoring frameworks, such as Batson's Empathic Concern-Personal Distress Scale, which quantify empathy within language but may not fully capture how users perceive the interaction themselves (Batson, Fultz, and Schoenrade 1987; Lahnala, Welch, and Flek 2022; Omitaomu et al. 2022; Shetty et al. 2024).\nA major gap in these methods is the lack of direct user feedback on empathy, particularly in contexts where the conversation partner's identity-whether human or chatbot-may significantly shape the experience (Lee et al. 2024a; Curry and Curry 2023). For example, in a study on Reddit's r/AskDocs, licensed healthcare professionals rated chatbot responses as 9.8 times more empathetic than those from verified physicians (Ayers et al. 2023). However, because these ratings were provided by third-party evaluators rather than users themselves, they may not accurately reflect how users perceive empathy during direct interactions with chatbots. This gap highlights a pressing need for user-centered approaches that account for the subjective experience of empathy in conversations, moving beyond externally assessed language metrics to capture the complexities of empathy as felt by users."}, {"title": "Human versus Chatbot Identities", "content": "Perceptions of empathy in conversational agents are shaped not only by the agents' words and actions but also by their perceived identities and characteristics. While language, appearance, and behavior can suggest an agent's identity, they do not fully represent these traits. Recent studies show that chatbot identity significantly affects user responses, as users react differently to bots compared to humans. For example, Sundar et al. (2016) found that while participants preferred websites with chatbot features, they were more likely to recommend the site and seek further information when a human agent was featured. Similarly, Go and Sundar (2019) demonstrated that chatbots given human-like identities were rated as more effective. In contexts like charity donations, Shi et al. (2020) found that identifying an agent as a chatbot reduced the likelihood of donations, with users more inclined to donate when they believed they were interacting with a human.\nA human identity cue can enhance a chatbot's social presence and perceived similarity to the user (Go and Sundar 2019). When users are aware that they are interacting with a chatbot, their expectations and judgments are often influenced by preconceived notions about bots, regardless of the agent's performance (Koh and Sundar 2010). Therefore, when assessing empathy and conversation quality, it is essential to account for the identity of the agent\u2014whether human or chatbot-as this can profoundly influence user perception and interaction outcomes."}, {"title": "Data", "content": "In this paper, we combine the following three datasets:\n\u2022 Empathic Conversations Dataset (EC; Omitaomu et al. 2022)\n\u2022 WASSA 2023 shared task Dataset (Barriere et al. 2023)\n\u2022 WASSA 2024 shared task Dataset (Giorgi et al. 2024).\nAll participants were recruited via Amazon Mechanical Turk in all datasets. The three datasets used in this study are described in detail below. The current study has been approved by the Institutional Review Board (IRB) at New York University."}, {"title": "Empathic Conversations (EC) Dataset", "content": "The EC dataset, created by Omitaomu et al. (2022), was designed to explore how perceived empathy interacts with demographic and affective factors. Participants first provided demographic information and completed surveys via Qualtrics. They were then grouped into pairs and assigned to read one of 100 news articles. After reading, each participant wrote a brief essay (300-800 characters) about the article. Using the Batson survey (Batson, Fultz, and Schoenrade 1987), participants' empathy and distress levels were assessed. Following this, each pair engaged in a text-based online conversation to discuss the article. Finally, participants rated their partner's overall empathy on a 1\u20137 scale.\nThe final EC dataset comprised 75 human crowd workers and included 500 conversations collected through the process described above. The EC dataset also contains annotations at the turn, conversation, and interpersonal levels."}, {"title": "WASSA 2023 & WASSA 2024", "content": "The WASSA 2023 (Barriere et al. 2023) and 2024 (Giorgi et al. 2024) shared tasks on empathy, emotion, and personality detection expanded the EC dataset (Omitaomu et al. 2022) by adding essay-based emotion annotations. In this current study, we introduce a new, unpublished extension of these datasets, incorporating self-reported user ratings on conversational perception\nIn addition to the human-human conversations exclusive to the EC dataset, the WASSA 2023 and 2024 datasets introduced interactions between crowd workers and chatbots. In our extended dataset, participants engaged in conversations with a chatbot after reading and writing about a news article. After the conversation, they rated the chatbot on psychological dimensions, including empathy and closeness, giving us direct insights into their subjective experience. To ensure data quality, all datasets were filtered similarly to the EC dataset (Omitaomu et al. 2022), where only \"sincere\" conversations-defined as on-topic, coherent, and free from intentionally unserious responses were retained. Approximately 18% of conversations were excluded due to irrelevant or disruptive responses. Additionally, we employed GPT-3.5-turbo to further flag and remove insincere conversations, where participants did not engage \"in good faith\" or failed to complete the survey accurately (see prompt in Supplement). Each dataset also contained attention-check questions, and data with all passed attention check questions was included in final data analysis.\nIn our final analyzed data, we obtained 155 conversations in total (Human-bot: N = 96, Human-human: N = 59).\nWe analyzed psychological ratings and language data separately. For psychological ratings, we focused on participants who rated their chat partners. In WASSA 2024, this included 77 users (49.4% female, 48.1% with a Bachelor's degree, 79.2% White, Mage = 41.2, SDage = 11.9, median income = $58,000). In WASSA 2023, 55 raters participated (38.2% female, 49.1% with a Bachelor's degree, 78.1% White, Mage = 40, SDage = 10, median income = $50,000). For language analysis, we used the rated side of the conversation. There were 48 unique users rated in WASSA 2023 and 32 in WASSA 2024."}, {"title": "Chatbot Implementation", "content": "The chatbots used for WASSA 2023 and 2024 were GPT-3.5-turbo and GPT-4-0125-preview, respectively, using the following prompt to instruct the system:\nYou should act as an empathetic person who is discussing a news article from a few years ago with a stranger on Amazon Mechanical Turk as part of a crowd sourcing experiment. YOU SHOULD NOT ACT AS AN AI LANGUAGE MODEL. Also don't say \"as a human\". Your responses should be a sentence max two. Do not be verbose. You shouldn't apologize too much. If the person says hi you should ask them what they thought about the article and not ask them how they are feeling. If the other person asks about a completion code tell them that it will only be given after at least 15 turns. NEVER GIVE A COMPLETION CODE! You are instructed to talk about the article. You know the other person has skimmed the article. You should let the other person end the conversation.\nHere's the old news article below.\n[ARTICLE]\nPlease remember to act like a highly empathetic person!\nHere we provide a brief overview of the chatbot setup process as established by WASSA 2023 (Barriere et al. 2023) and 2024 (Giorgi et al. 2024). The chatbot prompt was refined through several internal and crowd worker pilot tests to ensure it could effectively answer questions about the article without generating unnaturally long responses. Minimal prompt adjustments were made, and no further changes were applied during the experiment. This prompt approach aligns with other LLM-based methods for empathetic chatbot interactions (Qian, Zhang, and Liu 2023; Welivita and Pu 2024b). If the input to GPT-3.5-turbo exceeded the context window, a brief summary of the last user turn was used to maintain continuity. With GPT-4-0125-preview, which offers an extended context window, summarization was not required for WASSA24.\nThe conversation initiation was randomized, with either the chatbot or the crowd worker starting the exchange. When initiating, the chatbot typically opened with a question, mirroring the natural behavior of crowd workers."}, {"title": "Psychological Ratings", "content": "General Empathy In all three datasets, after each conversation, participants were asked to evaluate their conversational partner's general empathy by responding to a single question: \"On a scale from 1-7, do you think your conversational partner had genuine empathy?\"\nState Empathy In addition, we consider empathy in the context of conversations to be a state consisting of a transactional and sequential cognitive process (Nezlek et al. 2007; Shen 2010). Interaction in conversations involves understanding, recognition, perspective-taking, and connection to the other conversational partner. We added 6 questions from Shen (2010) during WASSA 2023 and WASSA 2024 data collection, to assess the perceived affective (i.e., \"they experienced the same/similar emotions as you\"), cognitive (i.e., \"they can see your point of view\"), and associative (i.e., \"they can identify with the situation described in the article\") state empathy of the conversational partner processing, on a 5-point Likert scale (0 = \"None at all\" and 4 = \"Completely\"). The overall perceived state empathy of the chat partner was calculated by averaging the responses to all six questions, whereas affective, cognitive, and associative state empathy were calculated by averaging responses to two questions each.\nCloseness We added perceived closeness to the other conversation partner using a Venn diagram, revised from the Inclusion of Other in the Self Scale (Aron, Aron, and Smollan 1992; Shafaei et al. 2020), during The WASSA 2023 and 2024 dataset collection process. In this question, participants selected from six images depicting two circles-one for the participant and one for the partner with overlap levels from 1 (least overlapped) to 6 (most overlapped) to represent their perceived closeness.\nOverall Conversation Quality WASSA 2024 has participants' ratings for overall conversation quality, assessed by a single 5-point Likert question, \"How was the conversation\" (1=\"very bad\" and 5 =\u201cvery good\")."}, {"title": "Methods", "content": "We conducted four experiments\u2014analyzing psychological ratings, using LLM annotations, developing a perceived empathy model, and evaluating pre-trained empathy models to explore the link between empathy perception, chatbot identity, and language use, and how these factors relate to conversation quality in interactions with chatbots versus humans. Details and results of these studies are in the following sections."}, {"title": "Psychological Ratings", "content": "We analyzed ratings of empathy and closeness using t-tests and mixed models to compare interactions with chatbots versus humans (WASSA 2023 and 2024), and their effects on conversation quality (WASSA 2024)."}, {"title": "Large Language Model (LLM) Annotations", "content": "Where GPT-40 is used to predict perceived empathy labels at conversation-level."}, {"title": "Perceived Empathy model", "content": "We train a model to estimate general perceived empathy from human conversations and apply this to human/chatbot conversations, comparing language-estimated empathy to conversational partner ratings."}, {"title": "Pre-trained off-the-shelf empathy models", "content": "We study empathy inferences from off-the-shelf empathy prediction models, comparing predictions on chatbot versus human language."}, {"title": "Psychological Ratings", "content": "Psychological ratings were analyzed based on users' assessments of how they perceived their chat partner's empathy, closeness, and overall conversation quality, using data from WASSA 2023 and WASSA 2024. In R, we performed t-tests to examine differences in perceptions of general empathy, overall state empathy, affective state empathy, cognitive state empathy, associative state empathy, and perceived closeness between interactions with chatbots and humans. To account for potential between-subject variance, we also replicated these t-tests with participants who interacted with both humans and chatbots (Nhuman = 26, Nchatbot =22).\nWe then conducted four mixed models in R, using the lmer() package, to assess how empathy and closeness, when interacting with chatbots versus humans, influence the overall conversation quality rating. In each model, participant ID was included as a random effect to control for between-person variability in self-reports, with the overall conversation quality rating as the outcome variable. In the first model, we examined how the general empathy of the chat partner, the type of conversation (chatbots or humans), and their interaction influenced overall conversation quality. The second model assessed the impact of overall state empathy, the type of conversation (chatbots or humans), and their interactions on conversation quality. The third model explored the effects of the type of conversation (chatbots or humans), state empathy ratings (affective, cognitive, associative), and their interactions on conversation quality. The third model investigated the impact of perceived closeness, the type of conversation (chatbots or humans), and their interaction on conversation quality."}, {"title": "GPT-40 Analysis of Perceived Empathy", "content": "In this task, we examined the similarity between human ratings and those generated by the modern LLM, GPT-40, by assessing the perceived empathy of humans and chatbots in the WASSA 2023 and WASSA 2024 datasets. We input the entire conversations between pairs of participants-either human-human or human-chatbot-into GPT-40. The model was tasked with rating the perceived empathy of both participants at a conversation level:\nUnlike the users, GPT-40 was not informed whether the participant being rated was a human or a chatbot. The language model processed the entire conversation and assigned perceived empathy scores to both participants. We conducted two main analyses with the resulting data. First, we investigated whether there was a statistically significant difference in the distributions of perceived empathy ratings between humans and chatbots. Second, we correlated these machine-generated labels with the human-rated labels from our dataset."}, {"title": "Perceived Empathy Model", "content": "Here we trained a model to predict perceived general empathy from conversation language. Using the EC dataset, we concatenated all turns from a single speaker are into a single document. We then extracted unigrams, which are encoded as the relative frequency of use across a given conversation. We then removed unigrams which we not used by at least 5% of the speakers, resulting in a feature space of 1,500 unigrams. Using 10-fold cross validation with an \\(l2\\) penalized Ridge regression (regularization term \\(\\lambda\\) chosen as 10,000 using nested cross validation), we obtained a prediction accuracy of Pearson r = 0.17. This accuracy is a similar accuracy to those found in the WASSA 2024 shared task on predicting perceived empathy (Giorgi et al. 2024). This model was then applied to conversations in the WASSA 2023 and 2024, producing estimates of perceived empathy, which are then compared to the human and chabot general empathy ratings. This entire process was done using the DLATK Python package (Schwartz et al. 2017)."}, {"title": "Off-the-Shelf Empathy Model", "content": "We leveraged several pretrained empathy prediction models for obtaining empathy estimates from text. The first was developed by Lahnala, Welch, and Flek (2022) for the WASSA 2022 shared task to predict Batson empathy scores from essays, using the EC dataset. This model used pre-trained bottleneck adapters (Pfeiffer et al. 2020) to estimate empathy for each conversation.\nWe also used Sharma et al. (2020)'s ratings (0=no empathy, 1=low, 2=high) of empathy three components in Reddit mental health conversations: emotional reactions (Emo-React), explorations (Explore), and interpretations (Interpret). We aggregated the speakers' turn-level empathy predictions to study their association with conversation-perceived empathy. Empathy estimates from each model were then compared to the general perceived empathy of both humans and chatbots."}, {"title": "Results", "content": "Results from t-tests revealed that, compared to their human counterparts, chatbots were rated significantly lower in general empathy, overall state empathy, affective state empathy, cognitive state empathy, and associative state empathy. There were no statistical differences in closeness between conversation types (chatbot vs. human). Across all four mixed models, we found that chatting with a chatbot significantly led to a higher conversation quality than chatting with a human (all \\(\\beta\\) > 1.05, p < 0.01). In addition, we found perceiving the chat partner as having high general empathy (\\(\\beta\\) = 0.37, p < 0.001), higher state empathy (\\(\\beta\\) = 0.86, p < 0.001), higher associative state empathy (\\(\\beta\\) = 1.1,p < 0.001), and higher closeness to the chat partner (\\(\\beta\\) = 0.31,p = 0.003) significantly improved the conversation quality. We found that the type of conversation (chatbot vs. human) significantly interacted with perceived general empathy (\\(\\beta\\) = -0.18), overall state empathy \\(\\beta\\) = -0.37), cognitive state empathy \\(\\beta\\) = 0.77), and associative state empathy \\(\\beta\\) = -0.73) of the partner, influencing the overall conversation quality (all p < 0.05)."}, {"title": "GPT-40 Analysis of Perceived Empathy", "content": "Our findings indicated that GPT-40 consistently perceived chatbots as less empathetic than humans in the context of the overall conversation. A t-test confirmed that this difference was statistically significant, with a p-value of 0.0005. Additionally, correlation analyses with our gold-standard human labels revealed a correlation coefficient of r = 0.20 for human ratings, r = 0.06 for chatbot ratings, and r = 0.07 for the combined dataset. Note that GPT-4 was not informed of the participants' identities and still rated humans as significantly more empathetic than chatbots. These results suggested that even without explicit identification of participants as human or chatbot, GPT-40 perceived chatbots as significantly less empathetic across entire conversations."}, {"title": "Perceived Empathy Model", "content": "Table 2 shows that the mean estimated perceived empathy for humans does not differ from that of chatbots (t=0.45, p=0.65). Thus, the empathetic language of humans and the empathetic language of bots are equivalent. Further, the correlation between human ratings of empathy and their predicted empathy correlate at r=0.17, whereas the chatbot correlation is r=0.08. Thus, estimated empathy for humans matches their rating, whereas this is less-so the case for bots."}, {"title": "Off-the-Shelf Empathy Model", "content": "Shown in Table 2, the Interpret and Emo-React model predictions were significantly different between humans and chatbots, but the Batson Empathy and Explore model predictions showed no significant differences. We observed generally higher predicted empathy levels for interpretations for humans than chatbots, while perceived empathy ratings correlated positively with interpretations for humans (r = 0.16) but negatively for chatbots (r = -0.19). Meanwhile, chatbots generally scored higher on Emo-React than humans, though the predictions correlate stronger with perceived empathy for humans (r = 0.25) than for chatbots (r = 0.19)."}, {"title": "Discussion", "content": "Our study examined user perceptions of empathy and conversational quality in AI chatbot versus human interactions. Chatbots were rated higher in conversational quality but perceived as less empathetic, a finding echoed by assessments using language-based models."}, {"title": "Lower Perceived Empathy in Chatbots vs. Humans", "content": "Despite advancements in natural language processing, we found that AI chatbots designed to convey empathy are still perceived as less empathetic than humans by both human users and language models. This suggests that while chatbots can produce coherent and contextually appropriate responses, they lack the nuanced empathy that humans convey from users' perspective (Jain, Pareek, and Carlbring 2024).\nOur study further support this by providing distinctions in how various dimensions of empathy (Westman, Shadach, and Keinan 2013)\u2014general empathy, overall state empathy, associative state empathy, cognitive state empathy, and affective state empathy are perceived in chatbot versus human by the human users. Although higher perceived empathy was generally associated with better conversational quality for both chatbots and humans, models trained on human-human interactions struggled to distinguish empathy levels between chatbots and humans. This limitation suggests a potential disconnect between the empathetic language generated by chatbots and how it is perceived by users (Urakami et al. 2019). Cognitive empathy, or understanding of context, showed a smaller gap between humans and chatbots, indicating that chatbots may be somewhat effective at demonstrating comprehension, even if they lack emotional depth.\nPrevious studies on chatbot empathy may yield mixed results, partly due to the lack of direct, user-centered comparisons between chatbot and human conversations (Lee et al. 2024b). This gap may be related to chatbots' non-human identity, which users perceive as less genuine or emotionally resonant (Shi et al. 2020). Our findings also suggest that language models, like GPT-40, can identify language generated by other LLMs, potentially reinforcing perceptions of chatbot identity (Panickssery, Bowman, and Feng 2024).\nInterestingly, while LLMs like GPT-40 were able to identify and replicate the empathy gap observed by human users, three out of five off-the-shelf nor EC-language-trained empathy models failed to distinguish empathy levels between humans and chatbots. This discrepancy may be due to the limitations of these models, which were trained on human-human conversations and focus on isolated language cues rather than full conversational context. Only one model was specifically trained to predict perceived empathy, while others used varying definitions, such as self-reports from the Batson scale. Of the two models showing differences, one rated chatbots higher (Emo-React) and the other rated humans higher (Interpret) in perceived empathy. Moreover, the inconsistencies between empathy models highlight a potential disconnect between the expression of empathetic language and its perception, suggesting that current language-based empathy models may not fully capture the nuances of humans' conversations with chatbots.\nWe chose self-reports as our primary measure of empathy because they are widely considered the psychological \"gold standard\" for capturing subjective experiences, directly reflecting users' personal perceptions (Neumann and Chan 2015). Theoretical frameworks like mind perception and the Computers as Social Actors paradigm support the idea that empathy theories developed for human interactions can be applied to human-chatbot interactions as well (Gray, Gray, and Wegner 2007; Nass, Steuer, and Tauber 1994). Grounded in these theoretical perspectives, we selected validated definitions and scales of empathy that assess perceived affective, cognitive, and associative state empathy, aligning with well-established constructs in empathy research (Preston and De Waal 2002) and tools designed for digital interactions like Perceived Empathy of Technology Scale (Schmidmaier, Rupp et al. 2024). Our approach focuses on capturing empathy as users perceive it, without enforcing strict operational definitions. Likewise, we rely on subjective evaluations of conversation quality to examine perceived differences between human and chatbot interactions (Inan Nur, Santoso, and Putra 2021). This user-centered perspective enhances our understanding of empathy and quality as users experience them, offering valuable insights for optimizing human-chatbot interactions."}, {"title": "Effect of Empathy on Conversation Quality", "content": "Our findings also reveal that higher perceived empathy is positively correlated with overall conversation quality for both humans and chatbots, though this association was stronger for human interactions. This association was stronger for humans than chatbots; low perceived empathy is associated with low overall conversation quality for humans, whereas chatbot conversations were generally rated higher in quality, even at low to moderate levels of perceived empathy. This implies that users may adjust their expectations for chatbots, rating conversation quality favorably even when perceived empathy is moderate.\nSignificant interaction patterns emerged between perceived empathy (general, overall state, associative state, and cognitive state) and conversation quality, with differences depending on whether the conversational partner was a chatbot or a human. However, affective state empathy did not follow this trend; although chatbots generally received high ratings in conversational quality, they were rated significantly lower than humans in affective empathy. This discrepancy may be due to users' implicit expectations of empathy in human interactions, which chatbots struggle to meet. The \"uncanny valley\" effect (Mori, MacDorman, and Kageki 2012) may also play a role, where users find chatbots' attempts at emotional expression somewhat unsettling or artificial, creating a gap between high conversational quality and low perceived empathy. Affective state empathy proved especially challenging for chatbots, highlighting their difficulty in conveying genuine emotional resonance, even when their responses were contextually appropriate and coherent."}, {"title": "Conclusion", "content": "Our study provides a user-centered analysis of how AI chatbots are perceived in terms of empathy and how these perceptions influence overall conversation quality. By focusing on psychological ratings directly from users and language-based evaluations, we highlight the complexities of perceived empathy in human-chatbot conversations. This user-centered perspective addresses critical gaps in our understanding of AI empathy, offering insights into how users experience and evaluate chatbot interactions, and underscoring the importance of aligning chatbot design with user expectations for more empathetic and satisfying interactions."}, {"title": "Limitations", "content": "One limitation of this study is the absence of a participant group unaware they were interacting with a chatbot. Consequently, we cannot directly assess the impact of chatbot identity awareness on user perceptions during conversations. However, this approach reflects real-world conditions, as users are typically informed when they are engaging with a chatbot. Such awareness is crucial, as it influences trust and empathy-key components of effective communication.\nAdditionally, while our participant pool is not fully representative of a global population, the use of crowdsourcing aligns with standard research practices and enables broad user insights. Finally, we intentionally avoided setting arbitrary thresholds for effect sizes, prioritizing user-centered insights over strict quantitative metrics to better capture nuanced perceptions of empathy and conversation quality."}, {"title": "Ethical Statement", "content": "Understanding how empathy is expressed and perceived in human-bot interactions raises important ethical questions. The paper's findings can inform the design and development of ethical dialogue systems, especially in terms of enhancing empathy of the system (Curry and Curry 2023). Understanding the differences in users' perception and language between human-bot and human-human interactions can enhance the naturalness and effectiveness of these systems in comprehending user input and generating appropriate, empathetic responses."}, {"title": "Supplementary Material", "content": "In this Supplementary Material, we provide the additional experiments involving third-party annotations of empathy for each conversational turn (S1), identity detection using GPT-40 (S2), prompt that we used GPT-3.5-turbo to further flag and remove insincere conversations (S3), and sample conversations from the datasets utilized in our study (S4). The code for the core data analysis featured in this paper were provided in GitHub2."}, {"title": "S1: Third-party Empathy Annotations", "content": "The WASSA 2023 and 2024 data sets contained third-party annotations of empathy for each conversational turn (Barriere et al., 2023; Giorgi et al., 2024). Ratings were collected via Amazon Mechanical Turk. Workers were asked to rate the empathy of each conversational turn (on a scale from 1-5) and were given"}]}