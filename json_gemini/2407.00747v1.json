{"title": "A Comparative Study of Quality Evaluation Methods for Text Summarization", "authors": ["Huyen Nguyen", "Haihua Chen", "Lavanya Pobbathi", "Junhua Ding"], "abstract": "Evaluating text summarization has been a challenging task in natural language processing (NLP). Automatic metrics which heavily rely on reference summaries are not suitable in many situations, while human evaluation is time-consuming and labor-intensive. To bridge this gap, this paper proposes a novel method based on large language models (LLMs) for evaluating text summarization. We also conducts a comparative study on eight automatic metrics, human evaluation, and our proposed LLM-based method. Seven different types of state-of-the-art (SOTA) summarization models were evaluated. We perform extensive experiments and analysis on datasets with patent documents. Our results show that LLMs evaluation aligns closely with human evaluation, while widely-used automatic metrics such as ROUGE-2, BERTScore, and SummaC do not and also lack consistency. Based on the empirical comparison, we propose a LLM-powered framework for automatically evaluating and improving text summarization, which is beneficial and could attract wide attention among the community.", "sections": [{"title": "Introduction", "content": "Text summarization is the process of producing a concise and coherent summary while preserving key information and meaning of the source text (Allahyari et al., 2017). This technique is widely used in various fields; for example, it is commonly used to summarize scientific, medical, and legal documents, as it enables users to quickly grasp key points of lengthy texts and efficiently access relevant information.\nThere are two major approaches to automatic text summarization: extractive and abstractive. Extractive summarization involves selecting important sentences or phrases from the original document. Extractive summarization is considered to be faster, simpler and more accurate because it retains authentic sentences of the source documents. However, it is less fluent and less coherent than abstractive summarization (El-Kassas et al., 2021). On the other hand, the abstractive summary generates the summary with sentences that are different from those in the original text while not changing the central facts and ideas.\nWith the remarkable achievements of pretrained language models (PLMs) and natural language generation, recent research has shifted gears from extractive to abstractive summarization. Nevertheless, the abstractive summarization still remains a challenging task since models suffer from hallucinations (Cao et al., 2018; Maynez et al., 2020) and the generated summaries do not align with human expectations (He et al., 2020). A preliminary study of text summarization techniques indicates that nearly 30% summaries generated by existing SOTA neural abstractive summarization are unfaithful to original documents (Cao et al., 2018).\nSummarization evaluation can be divided into reference-based and reference-free. Reference-based metrics are based on the matching between the generated summary and the reference summary, whereas the reference-free is based on the source document to evaluate the generated summary. Evaluation of summaries can be done either automatically or manually. The automatic method is fast, inexpensive, and can handle large volumes of data without human intervention. They can be categorized into three groups: text overlapping (e.g., ROUGE (Chin-Yew, 2004; Ganesan, 2018) and BLEU (Papineni et al., 2002)), vector-space distance (e.g., BERTScore (Zhang et al., 2019; Kieuvongngam et al., 2020), MoverScore (Zhao et al., 2019)), and NLP task-based to measure the consistency between the generated summary and the reference (e.g., SummaC (Laban et al., 2022), QuestEval (Scialom et al., 2021)).\nSummarization evaluation is still a challenging task as there are no ideal evaluation methods. Some studies demonstrate that automatic evaluation metrics such as BLEU, ROUGE, and BERTScore are not suitable for the automatic evaluation of summaries (Sun et al., 2022; Sulem et al., 2018; Reiter, 2018; Schluter, 2017). Most of these automatic metrics, especially text overlapping and vector-space similarity measurements, are originally reference-based; therefore, they may not be appropriate to use as reference-free due to the incompatible length and information compression.\nOn the other hand, the human evaluation is considered more reliable and trustworthy. It is still the preferred choice for evaluating summaries (Deutsch et al., 2021). The process starts by sampling a small set of about 30-100 generated summaries. The recruited evaluators are asked to score the generated summary on a Likert scale such as 1-5, or 1-7 on one or more evaluation dimensions described above. Despite the advantages of human evaluation, conducting this evaluation method is time-consuming, and labor-intensive, so it is infeasible to use in model development.\nStudies that access the evaluation methods reach inconsistent conclusions. For example, Graham (2015) claim that text-overlap metrics achieve the strongest correlation with human assessment. Meanwhile, recent studies demonstrate that metrics such as BLEU, ROUGE, and BERTScore are not suitable for the automatic evaluation of summaries (Sun et al., 2022; Sulem et al., 2018; Schluter, 2017; Reiter, 2018). Therefore, there is a need to revisit existing summarization evaluation methods.\nIn this work, we conduct a comparative study to reevaluate existing automatic metrics to evaluate abstractive summarization. Besides, we assess the ability of LLMs to perform as an evaluation agent. We further propose a framework to iteratively improve the quality of LLMs-generated summaries. Our contributions can be summarized as follows:\n\u2022 We conduct a comprehensive evaluation of the latest off-the-shelf PLMs and LLMs for the patent document summarization, using both automatic and human evaluation methods.\n\u2022 We re-evaluate existing automatic metrics that are widely-used for evaluating text summarization.\n\u2022 We propose a framework based on LLMs for automatically evaluating and improving summarization."}, {"title": "Related Work", "content": "Most current abstractive models rely on neural networks based sequence-to-sequence learning (Bahdanau et al., 2015; Vaswani et al., 2017). Seq2seq summarization can be summarized into two main types of frameworks, RNN encoder-decoder (Bahdanau et al., 2015) and Transformer encoder-decoder (Vaswani et al., 2017).\nNallapati et al. (2016) introduced one of the first RNN-based summarization models, utilizing a bidirectional RNN encoder enriched with POS tags and TFIDF feature embeddings, and a unidirectional RNN decoder with an attention mechanism. However, these models often faced issues with out-of-vocabulary (OOV) words and word repetition. To address these problems, (See et al., 2017) proposed a pointer-generator network, which combines the base seq2seq model with a pointer network that decides whether to generate a word from the vocabulary or copy it from the input sequence. Additionally, a coverage mechanism was implemented to track and prevent repetition (See et al., 2017; Nallapati et al., 2016).\nAbstractive summarization using Transformer encoder-decoder framework has rapidly advanced in recent years. Transformers with self-attention layers allow parallelization learning, solving the vanishing or explosion gradient of standard RNNs. It achieves SOTA performance in machine translation (Vaswani et al., 2017). Given this success, this approach is promising in abstractive summarization. Currently, encoder-decoder Transformer models like BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020) have achieved SOTA summarization results on short text. However, BART's and PEGARUS's maximum input length limit at 1024 tokens, making it unsuitable for summarizing long text.\nThe major limitation of transformer models is the complexity of quadratic self-attention that grows rapidly with sequence length (Zaheer et al., 2020). This has significantly impeded their effectiveness in summarizing long documents. The simplest approach is truncating the document from the head or tail to produce a short valid input. However, (Meng et al., 2021) proves that Transformers with this naive method is even worse than many unsupervised algorithms, such as TextRank, LSA, etc., for long text summarization. Models like Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al.,"}, {"title": "Text Summarization", "content": "2020) incorporate sparse attention mechanisms in the encoder to reduce the computational cost of standard self-attention operation (Vaswani et al., 2017); therefore, it can handle longer contexts. Longformer which is a Transformer model that uses the BART architecture can take up to 16k input tokens while Bird can support a sequence length of 4k tokens. The proposed attention replaces the full self-attention in standard Transformers (Vaswani et al., 2017) with the attention pattern mechanism, including windowed, dilated, and global attention. The model achieved SOTA performance on arXiv dataset, a long text summarization dataset, surpassing BARD-base, Pegasus, and BigBird (Beltagy et al., 2020). BigBird reduces quadratic complexity with a sparse attention mechanism combining random, windowed, and global attentions, allowing it to handle longer sequences efficiently without significantly increasing computational resources.\nThe model pretraining is continued from Pegasus (Zhang et al., 2020) that is specified for abstractive summarization. The model performs better than base Transformers, Pegasus, BIGBIRD-ROBERTa, etc. on three long-text summarization datasets, including BigPatent, arXiv, and Pubmed (Zaheer et al., 2020)."}, {"title": "Summarization Evaluation", "content": "Summarization systems can be evaluated with or without a reference summary. Reference-based evaluation uses reference summaries to identify what content from the input document is important and then evaluates a generated summary based on how similar it is to the reference. On the other hand, reference-free evaluation directly or indirectly defines a model to capture important information in the document and uses that to evaluate the content of the candidate summary. The recent success of LLMs has raised a lot of attention about how to evaluate the generated summaries since the reference summaries are too generic or unavailable. Therefore, in this study, we focus on assessing the reference-free evaluation methods.\nEvaluation of summaries can be done either automatically or manually. The automatic method is fast, inexpensive, and can handle large volumes of data without human intervention. However, this method may not be able to measure the exact aspects of summaries that humans are interested in evaluating such as clarity, accuracy, coverage, etc. On the other hand, manual evaluation is slower, more expensive, and infeasible to use in model development, but it is considered more reliable and trustworthy. A detailed discussion of the automatic and human evaluation method is presented in Appendix A. We also present an overview of summarization quality evaluation based on existing studies in Figure 1 in Appendix A."}, {"title": "Methodology", "content": "Summarization models studied in this research are SOTA PLMs, including the T5 family, XLNet, BART, BigBird, Pegasus, and GPT-3.5. These models have demonstrated significant potential across various applications and are categorized as follows: (1) domain-specific models (HUPD_T5_small and HUPD_T5_base), (2) general-domain models (XLNet, BART, and Pegasus), (3) models for long input sequences (LongT5 and BigBird), and (4) large language models (LLMs) like GPT-3.5. This selection ensures a comprehensive evaluation covering diverse types of SOTA summarization models.\nText-To-Text Transfer Transformer (T5 family), developed by Google, is an encoder-decoder Transformer designed for a variety of NLP tasks(Raffel et al., 2020) thanks to its capability to convert any language task into an essential text-to-text task. We implement the two off-the-shelf T5-based models: HUPD-T5 (Suzgun et al., 2022) Harvard University's Policy Department (HUPD) has tailored the T5 model for legal document summarization. We employ two versions of this model: hupd-t5-base and hudp-t5-small. The two model versions are finetuned for the abstractive summarization task on the Harvard USPTO Patent Dataset (HUPD) (Suzgun et al., 2024), a large-scale corpus of utility patent applications filed to the United States Patent and Trademark Office (USPTO) between January 2004 and December 2018. Therefore, these models are ideal for the legal document summarization task that we focus on.\nlong-t5-tglobal-base-16384 + BookSum (so-called LongT5) (Zhang et al., 2023): The model is based on T5 with an expanded context window of 16384 tokens, enabling it to comprehend and summarize long text efficiently(Peter Szemraj, 2022). The model is trained on a large corpus of book summaries, providing it with a strong capability to digest and generate summaries of long texts"}, {"title": "Summarization Models", "content": "XLNet (Yang et al., 2020): an autoregressive transformer model, has 110 million parameters and is trained on an assortment of datasets such as BooksCorpus, Wikipedia, and Giga5(Yang et al., 2020). Its distinguishing feature is the permutation-based training, allowing the model to capture bidirectional contexts and understand the intricate dependencies and relationships within the text.\nBART (Lewis et al., 2020): is an encoder-decoder transformer model with 140 million parameters. The model was pre-trained on a wide variety of data sources such as BookCorpus, Wikipedia, news articles, and stories (Lewis et al., 2020). The model is popular for text summarization and question-answering tasks.\nBigBird (Zaheer et al., 2020): is a transformer model that pioneers the use of a sparse attention mechanism with 110 million parameters. It is trained on diverse data sources such as Wikipedia, BookCorpus, and news articles. The introduction of the sparse attention mechanism enables BigBird to efficiently manage very long sequences, thus, reducing the computational complexity that is generally associated with processing long-range dependencies in text (Zaheer et al., 2020). It is suitable for processing lengthy texts due to its sparse attention mechanism. However, this design might capture less context compared to models with complete attention mechanisms, potentially affecting the overall quality of the model's outputs.\nPegasus (Zhang et al., 2020): is an encoder-decoder transformer model with the pretraining objective specifically optimized for summarization. It is trained on diverse datasets like C4, HugeNews, PubMed, and arXiv. We use pegasus-x-large-booksum-1, an off-the-shelf model with 568 million parameters for the summarization experiments.\nGPT-3.5 (Liu et al., 2023): Recent GPT models have demonstrated unprecedented capabilities in understanding context, semantics, and syntactic structures, enabling them to generate summaries that are concise, coherent, and human-like. Experiments on multiple news datasets, Pu et al. (2023) even found that humans significantly prefer summaries generated by zero-shot GPT-3.5 and GPT-4 to those written by humans or generated by small finetuned PLMs such as BART (Lewis et al., 2020), T5 (Raffel et al., 2020), etc. In this study, we use GPT-3.5-turbo-16k for summarization. It features an extended context window handling up to 16k tokens, enabling it to maintain context over longer conversations for more accurate and coherent responses."}, {"title": "Dataset", "content": "The dataset for this study consists of a corpus of 1630 patent documents collected through web scraping of Google Patents 2. Human evaluation requires a good understanding of the documents. To simplify the evaluation process, we have focused on collecting patents related to communication and streaming technologies.\nAlthough a patent document includes long description of the invention details and many flow charts, the most important content in a patent document for the summarization includes its abstract and the claims of the invention. The abstract provides an overview of the invention, while the claims detail the invention's specifics that a summarization must capture. While other datasets, such as BIGPATENT(Sharma et al., 2019), consider the abstract as an abstractive summary of the patent document, the abstract itself does not cover the scope of the claim and the novelty of the patent. To generate more useful and self-contained patent summaries, we use the abstract and the claims from each patent document as inputs for the summarization models."}, {"title": "Data Sample for Evaluation", "content": "Since human evaluation is only conducted in a small-scale, we randomly sample a subset of 30 patent documents to generate summaries for evaluation with humans. Figure 3 in Appendix C shows the text-length distribution of the entire dataset and the evaluation sample. Besides, due to the high cost of human evaluation, we only evaluate summaries from five summarization models, including HUPD_T5_base, XLNet, BART, LongT5, GPT-3.5, and Llama-3. We select representative models from each group based on their performance on traditional automatic metrics."}, {"title": "Evaluation Methods", "content": "Many metrics have been proposed for text summarization evaluation; however, not all of them are suitable for evaluating summarization in long text. We select eight widely-used automatic evaluation metrics for this study. In addition, we also conduct human evaluation and introduce a LLM-based evaluation approach."}, {"title": "Automatic Metrics", "content": "ROUGE-1: measures the unigram overlap between the candidate summary (generated summary) and the reference summary (ground truth summary). It calculates the proportion of overlapping unigrams (individual words) between the candidate and reference summaries. It considers only individual words and does not capture word order or context (Lin, 2004).\nROUGE-2: measures the bigram overlap between the candidate summary and the reference summary. It calculates the proportion of overlapping bigrams (consecutive pairs of words) between the candidate and reference summaries. It captures some level of word order and context by considering pairs of words together (Lin, 2004).\nROUGE-L: measures the longest common subsequence between the candidate summary and the reference summary. It finds the longest sequence of words that appear in the same order in both the candidate and reference summaries. ROUGE-L accounts for word order and captures the informativeness of the candidate summary by considering sequences of words rather than just individual words (Lin, 2004).\nBilingual Evaluation Understudy (BLEU) is a widely-used metric for evaluating the quality of machine-generated text, especially in machine translation. The BLEU score quantifies the similarity between the machine-generated text and one or more reference texts. It considers the matching n-grams between the generated text and the reference text. BLEU combines the scores for different n-grams (usually 1 to 4) into a single score by calculating the geometric mean of the modified precision counts (Papineni et al., 2002). The final BLEU score ranges from 0 to 1; 1 indicates a perfect match with the reference, while 0 indicates no overlap in n-grams.\nBERTScore is an evaluation metric for language generation based on pretrained BERT contextual embeddings (Devlin et al., 2018). The metric computes a similarity score for each token pair between the generated text and the reference text using contextual embeddings. BERTScore has been shown to correlate well with human judgment of text quality (Zhang et al., 2019). It has been used to evaluate a variety of text generation tasks, including machine translation, summarization, and question-answering. The scores, such as Precision, Recall, and F1, range from 0 to 1, with higher scores indicating better performance.\nSummaC measures the consistency between a summary and its source text. The score is calculated by comparing the generated summary to the source text by identifying any inconsistencies between them. SummaC effectively utilizes NLI models for inconsistency detection by segmenting documents into sentence units and aggregating scores between pairs of sentences (Laban et al., 2022). The authors introduce two versions: SummaCzs, and SummaCConv in which SummaCzs use an out-of-the-box NLI model while SummaCConv involves finetuning using a convolutional neural network. SummaCConv achieves better performance on the proposed benchmark for the summary consistency detection task than SummaCzs and other models such as QuestEval (Scialom et al., 2021), FactCC-CLS, etc. Therefore, we utilize SummaCConv to evaluate the faithfulness of summarization models.\nFlesch Reading Ease (FRE) score (Flesch, 1979) assesses the readability of an English text by examining the sentence length and word length. It is calculated as $FRE = 206.835 - (1.015 * ASL) - (84.6* ASW)$, where ASL is the average sentence length and ASW is the average number of syllables per word. The score typically ranges from 0 to 100. Higher scores indicate that the text is easier to read, while lower scores indicate that the text is more difficult to read.\nDale-Chall Readability (DCR) score is another readability metric used to assess the readability of English text. It considers a set of familiar words and examines the sentence length to estimate the text's difficulty level. The DCR score is calculated: $DCR = (0.1579*PDW*100)+(0.0496*ASL)$, where PDW is the percentage of difficult words in the text, and ASL is the average sentence length. DCR provides an estimated percentage value representing the difficulty level. Lower DCR scores indicate higher difficulty. DCR and FRE have drawbacks. They primarily considers sentence length and the presence of difficult words but does not account for factors like content, coherence, or text structure, which also influence readability."}, {"title": "Human Evaluation", "content": "Existing automatic evaluation metrics do not consistently align with human expectations, and there is no available human evaluation data for legal-text summarization. Therefore, we conducted a manual evaluation study with participants who are master's students in computer and engineering fields. Given the technical nature of the texts, participants needed a solid understanding of such documents. To ensure quality, we designed test questions mixed with other questions to filter out unreliable responses from participants who failed the tests.\nBased on our overview of existing summarization evaluation (Figure 1), we choose to evaluate the following quality dimensions: (1) Clarity: whether the summary is reader-friendly and expresses ideas clearly; (2) Accuracy: whether the summary contains the same information as the source document; (3) Coverage: how well the summary covers the important information from the source document; and (4) Overall quality: how good the summary overall at representing the source document; a good summary is a shorter piece of text that has the essence of the original tries to convey the same information as the source document. The participants are asked to rate the given summary on the above-mentioned quality dimensions on a Likert scale from 1 to 5, corresponding to Poor to Excellent. Our study uses APPEN platform 3 to design and conduct the evaluation. Figure 2 shows the interface of the data collection form with APPEN. We also release the response form of the study for the purpose of reproduction. 4"}, {"title": "LLM-Based Evaluation", "content": "LLMs have demonstrated the exceptional ability to understand and follow instructions. They potentially can serve as an evaluation agent (Wu et al., 2023; Chiang and Lee, 2023). In this study, we explore the ability of LLMs to assess the quality of model-generated summaries. To ensure a fair comparison between LLMs and humans, we use the same instructions given to humans as prompts to guide LLMs on this task. Similarly, we ask LLMs to evaluate the summary quality on the four quality dimensions mentioned above on a similar Likert scale 1-5."}, {"title": "Summarization Improvement Based on LLM's Feedback", "content": "Motivated by the way humans refine written text, self-refining (Madaan et al., 2024) and self-reflection (Shinn et al., 2024) have been proposed to enhance the initial text generated by LLMs. In the context of summarization, the initial draft iteratively refined will improve the quality. We adopt this approach for summarization improvement. Particularly, we iteratively incorporate the LLM's evaluation of the summary generated in the previous round into prompt to guide LLMs in generating a better version in the next round. We provide the prompt template that we use for this experiment in Appendix E."}, {"title": "Results and Discussion", "content": "Using existing automatic methods To evaluate the performance of summarization, we use the most widely-used automatic evaluation metrics, including BLEU, ROUGE, BERTScore, SummaC, FRE, and DCR. While BLEU, ROUGE, BERTScore, and SummaC are used to assess the content quality of generated summaries, FRE and DCR are metrics to evaluate the readability level. Table 1 shows performance of summarization models on these metrics on the entire dataset (left) and on evaluation sample (right). On the content-based evaluation metrics, the results show that XLNet, BART, and GPT-3.5 are the best models. However, their generated summaries are as not as readable as T5-generated summaries. BLEU metric yields low scores (<< 0.01) for all summarization models, indicating a need to reconsider using this metric for evaluation. XLNet performs best on ROUGE and BERTScore, followed by GPT-3.5, and then BART. BigBird performs the worst among the eight models. The results conducted on the entire dataset are consistent with the results on the evaluation sample data that we sampled for human and LLM-based evaluation.\nHuman and LLM-based evaluation The human and LLM-based evaluation is conducted regarding the four dimensions, including clarity, accuracy, coverage, and overall quality. Table 2 present results of human and GPT-4 evaluation for the five selected summarization models. Overall, GPT-3.5 produces the best-quality summaries with the highest scores on all evaluation dimensions, followed by XLNet and BART. XLNet and BART have comparable accuracy and coverage quality scores. T5-base and LongT5 are the worst regarding the clarity. We further examine the summaries producted by these low-performance models to propose improvement methods. Our analysis results are presented in"}, {"title": "Evaluation on Summarization Performance", "content": "Appendix F. Surprisingly, the GPT-4 evaluation is highly consistent with humans in assessing the performance of each model, demonstrating the potential of this automatic method to replace expensive human evaluation."}, {"title": "Meta-analysis", "content": "Meta-analysis explores the pairwise statistic correlation between evaluation methods. Automatic evaluation metrics should strongly correlate with human judgments. Since the automatic evaluation metrics we utilize have not been specifically assessed for legal document summarization. Therefore, we would like to re-evaluate these automatic evaluation metrics."}, {"title": "Automatic Metrics vs. Human Evaluation", "content": "Table 3 presents results of meta-analysis between the conventional automatic metrics and human evaluation. Results indicate that ROUGE-1 and ROUGE-L correlate with human evaluation on four dimensions (0.6-0.8 on Kendall Tau-b). However, the significance test shows that the correlation is not statistically significant. The scores between BERTscore and ROUGE-2 and human evaluation show a low level of correlation (0.2-0.4). Surprisingly, SummaC, a factual consistency measurement, displays very weak or non significant correlation with human evaluation, and many other metrics such as ROUGE-1, ROUGE-L. Both readability metrics have negative correlation with most metrics, including human evaluation, ROUGE scores, and BERTscore. This implies that texts that are easier to read tend to score lower on the content-based metrics, possibly suggesting a trade-off betweeen complexity and ease-of-reading."}, {"title": "LLMs vs. Human Evaluation", "content": "Table 4 presents the results of a meta-analysis comparing evaluations by LLMs and humans. The correlation scores for the three quality dimensions (accuracy, coverage, and overall) between humans and LLMs (GPT-4 and Llama-3-8B) show an extremely high positive correlation (0.8-0.9). Significance tests indicate that these correlations are statistically significant. For the clarity dimension, LLMs and human evaluation still exhibit a high positive correlation (0.67-0.8); however, the statistical tests reveal that the correlations are not statistically significant, suggesting that the observed relationship could be due to random variation. The correlation scores are consistent for both GPT-4 and Llama-3-8B. The results indicate (1) LLMs are capable of performing summarization evaluation as effectively as humans, and (2) open-sourced LLMs such as Llama-3-8B can produce reliable evaluations similar to GPT-4. In specific domains like legal documents, where the cost of human evaluation is prohibitively high, LLMs, including open-sourced models, could be utilized to assess summarization.\nIn a previous analysis, we only used one type of correlation coefficient for our meta-analysis (Spearman's or Kendall's Tau-b). Therefore, we further compare the correlation results between GPT-4 and human evaluation using three tests: Pearson's p, Spearman's, and Kendall's Tau-b correlation coefficients. The results indicate the consistency of these three correlation coefficients. Kendall's Tau-b produces lower scores than the two other tests. See Table 6, Appendix D for more details."}, {"title": "Summarization Quality Improvement", "content": "Table 5 presents the results of our method for improving summarization It shows the impact of LLM's verbal feedback on summarization performance improvement. It demonstrates that by integrating evaluation feedback from LLMs into the prompt, the quality of the summaries significantly improves in terms of clarity (from 4.167 to 4.5) and coverage (from 3.567 to 3.833), indicating a substantial enhancement. However, we have also noticed that this method slightly reduces accuracy. In the future, we aim to enhance this quality dimension further."}, {"title": "Conclusion and Future Work", "content": "In this work, we compare various methods for evaluating abstractive summarization of legal documents, including existing automatic metrics and human evaluation. We also explore the potential use of LLMs for evaluation purposes. We conduct different meta-analyses to compare the evaluations among these methods. Our findings reveal that widely-used automatic evaluation metrics such as ROUGE-2, BERTScore, and SummaC exhibit very weak or non-significant correlation with human evaluation. Additionally, readability metrics show a negative correlation with most other metrics, including human evaluation. In contrast, our results suggest that LLMs can effectively perform summarization evaluation. Open-sourced LLMs like Llama-3-8B demonstrates reliable evaluations similar to GPT-4. Futher, we attempt to improve the summarization quality through iterative improvement by leveraging the verbal evaluation feedback from LLMs. The results indicate that the quality of the summaries significantly improves in terms of clarity and coverage, suggesting the potential of this approach. In the future, we aim to further enhance other quality dimensions such as accuracy."}, {"title": "Limitations", "content": "In this study, our focus is solely on evaluating the summarization of legal documents due to the high cost of human evaluation. Therefore, our findings may not be applicable to other domains where the document structure and vocabulary may differ. Additionally, the size of our human evaluation sample is limited, which may not accurately reflect the overall performance or reliability of the findings. This limitation could lead to the statistical insignificance of some meta-analyses we dicussed before and potentially cannot capture all possible variation within the dataset."}, {"title": "Appendix: Existing studies on text summarization evaluation", "content": "Many studies have been done to determine which automatic metrics are sufficient for evaluation. They include ROUGE (Chin-Yew, 2004; Ganesan, 2018), BLEU (Papineni et al., 2002), BERTScore (Zhang et al., 2019; Kieuvongngam et al., 2020), MoverScore (Zhao et al., 2019), SummaC (Laban et al., 2022), QuestEval (Scialom et al., 2021), etc. They can be categorized into three groups: text overlapping (including ROUGE and BLEU), vector-space distance (BERTScore, MoverScore), and NLP task-based to measure the consistency between the generated summary and the reference (SummaC, QuestEval).\nFor the simplicity of the overlapping-based evaluation methods like ROUGE and BLEU, they are the most favorable to use. The use of text-overlapping metrics to assess the quality of summarization is still a topic of debate. Graham (2015) uses summary coverage computations and human coverage scores to assert that text overlap-based metrics are suitable for evaluation. However, some studies demonstrate that evaluation tools such as BLEU, ROUGE, and BERTScore are not suitable for the automatic evaluation of summaries (Sun et al., 2022; Sulem et al., 2018; Reiter, 2018; Schluter, 2017). Similarly, Schluter (2017) shows ROUGE's limitations, specifically, its inability to attain a perfect evaluation score. Therefore, most studies combine different metrics, such as text-overlap-based (ROUGE, BLEU), vector-space-based (BERTscore, MoverScore), and QA-based (QuestEval, SummaQA) to evaluate summarization performance.\nOn the other hand, some reference-free metrics have recently been proposed since summary references are not always available and high-quality, for example, QuestEval (Scialom et al., 2021), QAEval (Deutsch et al., 2021). Compared to reference-free methods, reference-based metrics have more advantages. Additional studies also assert that reference-based metrics such as BERTScore correlate more closely with human judgments (Zhang et al., 2019)."}, {"title": "Automatic methods", "content": "Manual methods\nHuman evaluation is commonly used because automatic metrics are imperfect, and humans can perform tasks that automated methods cannot do as reliably. The method involves asking human judges to score summaries based on the given reference (called reference-based) or directly assess the generated summary according to specific criteria (called reference-free or direct assessment).\nReference-based The manual reference-based approach is found to be simpler to conduct than the manual reference-free approach, which requires a well-designed scoring scheme and rubrics, and participants must have a good understanding of the source document.\nAs mentioned earlier, we aim to explore the reference-free human evaluation methods in this study.\nReference-free Most human evaluation methods for summarization are reference-free. Current works consider multiple dimensions to evaluate summary quality, including readability (ease of reading), fluency (grammaticality), consistency (factual support from the input document), faithfulness (completeness of information from the input document), relevance (selection of important content), and content quality (inclusion of salient information). Due to the high cost, evaluations often focus on fluency, coherence, consistency, and relevance. The process starts by sampling a small set of about 50-100 generated summaries. The recruited evaluators are asked to score the generated summary on a Likert scale such as 1-5, or 1-7 on one or more evaluation dimensions described above. The results are then compared with automatic metrics using several correlation analysis tests such as Pearson's, Spearman's, and Kendall's Rank correlation coefficient.\nManual evaluation has been an important tool for measuring the quality of generated summaries. By focusing on specific dimensions and using a carefully selected group of evaluators, manual evaluation can provide valuable insights into the strengths"}]}