{"title": "AVIARY: TRAINING LANGUAGE AGENTS ON CHALLENGING\nSCIENTIFIC TASKS", "authors": ["Siddharth Narayanan", "Manu Ponnapati", "Ori Kabeli", "Samuel G. Rodriques", "James D. Braza", "Albert Bou", "Ryan-Rhys Griffiths", "Jon Laurent", "Geemi Wellawatte", "Sam Cox", "Andrew D. White"], "abstract": "Solving complex real-world tasks requires cycles of actions and observations. This is particularly\ntrue in science, where tasks require many cycles of analysis, tool use, and experimentation. Language\nagents are promising for automating intellectual tasks in science because they can interact with tools\nvia natural language or code. Yet their flexibility creates conceptual and practical challenges for\nsoftware implementations, since agents may comprise non-standard components such as internal\nreasoning, planning, tool usage, as well as the inherent stochasticity of temperature-sampled language\nmodels. Here, we introduce Aviary, an extensible gymnasium for language agents. We formal-\nize agents as policies solving language-grounded partially observable Markov decision processes,\nwhich we term language decision processes. We then implement five environments, including three\nchallenging scientific environments: (1) manipulating DNA constructs for molecular cloning, (2)\nanswering research questions by accessing scientific literature, and (3) engineering protein stability.\nThese environments were selected for their focus on multi-step reasoning and their relevance to\ncontemporary biology research. Finally, with online training and scaling inference-time compute, we\nshow that language agents backed by open-source, non-frontier LLMs can match and exceed both\nfrontier LLM agents and human experts on multiple tasks at up to 100x lower inference cost.", "sections": [{"title": "1 Introduction", "content": "Language agents [1-4] are AI agents [5] that integrate LLMs [6-8] as core components. LLMs excel at zero-shot\ngeneralization [9, 10], providing a notable advantage over traditional AI agents, such as those based on handcrafted\nrules or reinforcement learning, which often struggle to generalize to new environments [11]. While LLMs can exhibit\nflawed reasoning and logic when used in isolation [12-14], constructing a language agent by grounding LLMs in an\nenvironment with observational feedback can mitigate these issues. Early work on language agents used LLMs to\ndirectly output actions in the external environment [15-17], while more recently, language agents have been augmented\nwith internal reasoning [18, 19] and planning [20, 21] procedures, as well as long-term memory storage [22,23].\nAn emergent research challenge is to pose a theoretical description of the learning problem solved by language\nagents [4, 24] and to develop efficient methods to optimize the components of a language agent [24-26]. Here, we\ndefine common language agent tasks as language decision processes (LDPs) and frame language agents as stochastic\ncomputation graphs [27] that may be trained to solve LDPs. We show that pre-existing agents [18, 19, 21] can be\nimplemented within our stochastic computation graph framework and introduce a simple and extensible software package\nnamed LDP that enables modular interchange of environments, agents, and optimizers, simplifying experimentation\nacross a variety of settings."}, {"title": "2 Related Work", "content": "Language Agent Formalisms Although language agents have achieved impressive empirical performance across a\nrange of applications [1, 37, 40], there is still no universally agreed upon theoretical framework for defining a language\nagent. In terms of conceptual models, the cognitive architectures for language agents (CoALA) framework [4], inspired\nby ideas from production systems and cognitive architectures, taxonomizes agents according to their information storage\n(working and long-term memories), decision-making procedures e.g. planning, and action space (divided into internal\nand external actions). Similarly, in [41], the author describes language agents as consisting of memory, planning, and\ntool usage components. Theoretically, many works represent language agents as partially observable Markov decision\nprocesses (POMDPs) [42-48] yet differ in their treatment of the action space e.g. in [43] the authors partition the action\nspace into internal and external actions in a similar fashion to CoALA where internal actions are a family of functions\nthat operate on the agent's memory and external actions elicit an interaction with the environment. By contrast, in [44]\nthe authors do not make a distinction between internal and external actions. In [49] the authors introduce a general"}, {"title": "Language Agent Optimization Frameworks", "content": "Optimization of language agents may involve the learning of prompts,\ntool usage, LLM weights, LLM inference hyperparameters such as temperature, as well as more exotic language agent\ncomponents such as edges between nodes in multiagent computation graphs. Frameworks such as LangChain [50] and\nLlamaIndex [51] support manual optimization of prompts via human editing. Optimizers such as EcoOptiGen [52]\nleverage black-box optimization schemes to learn LLM inference hyperparameters such as temperature, the maximum\nnumber of tokens, and the number of completions. Prompt optimization comprises the optimization of white-box LLMs\nand black-box LLMs (LLMs that exist behind an API and for which numerical gradients are unavailable). In white-box\nprompt optimization [53-56] numerical gradients can be taken over soft prompts [57], the embedding representation of\nthe text-based 'hard' prompt. In black-box prompt optimization a multitude of techniques have been applied which\nattempt to overcome the absence of gradients [58-79]. Tool learning [80, 81] can be attempted through in-context\ndemonstrations [82] or can seek to fine tune LLM weights on example demonstrations of appropriate tool usage [30,83]\nusing techniques such as expert iteration [28, 29]. In terms of methods that seek to optimize many components of a\nlanguage agent simultaneously, the TextGrad framework, introduced in [25] backpropagates textual feedback received\nfrom an LLM. In a similar fashion, Zhou et. al [84] also backpropagate textual feedback by creating natural language\nsimulacrums of weights, losses, and gradients. In [85] the authors use a metaprompt to encourage an LLM to perform\ndiscrete optimization over an agent architecture. The Trace framework introduced in [26] proposes the OptoPrime\noptimizer which passes code execution traces in place of gradients and uses an LLM to provide textual feedback and\nperform updates. Another popular language agent optimization framework is DSPy [86-88] which parametrizes a\ncomputational graph for language agents and focuses on automatically generating and selecting useful demonstrations\nfor in-context learning. In the multi-agent setting, GPTSwarm [24] introduces a computation graph and performs\nbinary edge-level optimization and node-level optimization over prompts. Lastly, OpenR [89] is a framework for LLM\nreinforcement learning and inference-time scaling, but is targeted at token-level optimization, not tool usage."}, {"title": "Language Agent Benchmarks", "content": "Existing language agent benchmarks feature a broad range of applications including\nmachine learning tasks [90], data science [91, 92], data analysis [93, 94], quantitative reasoning [95], and causal\nreasoning [96]. In Aviary, we place particular focus on scientific tasks. Relevant work in this area has included\nDiscoveryBench, a benchmark for data-driven hypothesis generation [97], ChemBench [98] which focuses on chemistry\ntasks, BLADE [99] which is concerned with data-driven science, SciAgent [100] a benchmark for scientific reasoning,\nDISCOVERYWORLD [101] which concentrates on cycles of scientific discovery, and ScienceWorld [102] which\nis concerned with scientific reasoning. For a review focused on scientifically-relevant agents the reader is directed\nto [103]. In Aviary, we focus on sequential decision-making tasks that necessitate multiple steps of agent-environment\ninteractions. We construct environments from the pre-existing datasets such as GSM8K [33], HOTPOTQA [34], and\nLitQA2 [37] by casting them as parametrizable tools manipulating an environment state.\nOur principal contributions are: (1) A precise definition of language decision processes (LDPs) for language-agent\ntasks and encompass many proposed agent architectures as stochastic computation graphs. (2) We introduce Aviary, a\ngym framework that emphasizes multi-step reasoning and tool usage, and provide five gym implementations (including\nthree for scientific tasks). (3) We demonstrate that non-frontier LLMs, trained online with inference time sampling,\ncan match or exceed the performance of frontier models on these tasks with a modest compute budget. (4) We release\nAviary and our LDP framework as open-source software libraries to enable broader use and experimentation."}, {"title": "3 Theory", "content": ""}, {"title": "3.1 Language Decision Processes", "content": "A language decision process (LDP) is a Partially-Observable Markov Decision Process (POMDP) [104] whose action\nand observation spaces are represented in natural language. More concretely, a LDP can be defined using the tuple\n$(\\mathcal{V}, \\mathcal{S}, \\mathcal{A}, \\mathcal{O}, \\mathcal{T}, \\mathcal{Z}, \\mathcal{R}, \\gamma)$. Here, $\\mathcal{V}$ is a non-empty alphabet\u00b2, $\\mathcal{S}$ is the state space, $\\mathcal{A} \\subseteq \\mathcal{V}^*$ is the action space\u00b3,\n$\\mathcal{T}(s'|s,a) : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{P}(\\mathcal{S})$ is the transition function, $\\mathcal{R}(s, a) : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{P}(\\mathcal{R})$ is the reward function, $\\mathcal{O} \\subset \\mathcal{V}^*$ is the\nobservation space, $\\mathcal{Z}(o|s') : \\mathcal{S} \\times \\mathcal{A} \\leftrightarrow \\mathcal{P}(\\mathcal{O})$ is the observation function, and $\\gamma \\in [0, 1]$ is the discount factor."}, {"title": "3.2 Stochastic Computation Graphs", "content": "In the general case, a language agent may include both stochastic and deterministic operations. We build on the formal-\nism of stochastic computation graphs (SCG) [27]: directed, acyclic graphs with nodes corresponding to computations\nand edges corresponding to arguments.\nA deterministic node $v$ corresponds to a function $f_v$, and the node's output $o(v)$ is defined as:\n$$o(v) = f_v(\\lbrace o(w) | w \\in parents(v) \\rbrace)$$\nSimilarly, a stochastic node $u$ is defined by a (conditional) distribution $p_u$, with output:\n$$o(u) \\sim p_u(\\cdot|\\lbrace o(w) | w \\in parents(v) \\rbrace)$$\nNote that inputs to the graph can be treated as constant (deterministic) nodes. Outputs of the graph are leaf nodes.\nA language agent's policy is simply an SCG with a string input (the observation) and a string output (the action).\nLanguage agent architectures can be easily expressed as SCGs by combining deterministic and stochastic nodes. The\nSCGs of the below common language agent architectures are visualized in Figure 2."}, {"title": "3.3 Training Methods", "content": "Below we describe commonly-used imitation learning [108-110] methods employed to improve language agent\nperformance on our environments. These training methods do not optimize the SCG graph directly, instead we optimize\nonly the language model node in the SCG."}, {"title": "4 Environments", "content": "We briefly detail the environments comprising Aviary. Further details on the environments may be found in the appendix."}, {"title": "4.1 GSM8K", "content": "The GSM8K environment is based on the GSM8K dataset introduced in [33], which consists of linguistically diverse\ngrade school math word problems designed to assess multi-step mathematical reasoning. The GSM8K dataset comprises\na training set of 7,473 questions and a test set of 1,319 questions. The environment exposes a calculator tool."}, {"title": "4.2 HOTPOTQA", "content": "The HOTPOTQA environment is based on the HOTPOTQA dataset introduced in [34], which was subsequently extended\nto a language agent environment in [18]. The HOTPOTQA dataset comprises 112,779 question-answer pairs. We run\nevals on the 7,405 eval subset of questions. In the HOTPOTQA environment, the agent is provided with a Wikipedia API\nand tasked with answering the questions. There are is no given context to the agent and the API supports access to all of\nWikipedia articles and sections."}, {"title": "4.3 PaperQA", "content": "PaperQA [36,37] is a language agent/environment pairing developed for literature research and question answering\nthat leverages reranking and contextual summarization. Specifically in [37], an untrained version 2 of PaperQA, called\nPaperQA2, attained superhuman-level precision and human-level accuracy on version 2 of a literature question and\nanswer task, called LitQA2 [35]. PaperQA2 was implemented with tools and a tool calling agent, so we refactored\nPaperQA2 to be an Aviary environment as part of the version 5 release of the paper-qa Python package. To make it\neasy for the machine learning community to use, we modified the search tool to center on local storage containing a set\nof PDF, text, and HTML files using tantivy [119]. This local search is why we call this PaperQA variant \"PaperQA2\nLocal.\" The citation traversal tool was omitted for this local setting. A complete tool was added to support agents that\nrequire at least one tool selection and allow the agent to declare if the answer addresses all parts of the question.\nLitQA2 features 248 questions, 199 of which are publicly available and the remaining 49 were held out as a test set. We\nreuse the same test set here for comparability with [37]. The remaining 199 questions were randomly 80%-20% split\nsuch that the training set is 149 questions and the evaluation set is 40 questions. The test split questions can be found in\nthe aviary-paper-data Hugging Face dataset. Note this PaperQA environment is capable of doing tasks beyond\nLitQA2. For example, it can do literature review writing and contradiction detection as reported in Skarlinski et. al [37]."}, {"title": "4.4 Molecular Cloning", "content": "Molecular cloning is a fundamental technique of manipulating DNA in biomedical research, enabling a majority of\nbasic research such gene function studies, creating transgenic models, and producing recombinant proteins [120, 121].\nThe molecular cloning process results in a DNA \u201cconstruct,\" which is a general term for DNA that encodes for\nthe desired biologic molecule or genes. Molecular cloning involves assembling DNA fragments, ligating them into\nvectors, introducing the recombinant DNA into host organisms, and screening for desired clones [120]. The steps in\nmolecular cloning are usually done with a combination of human planning, specialized software, and databases of\nknown purchasable components.\nWe have formulated this into an environment. The molecular cloning environment is composed of the main tools used\nby experts in the lab: (1) an annotation tool that can predict the function of segments of a plasmid (2) a natural language\nsearch tool that retrieves sequences given text and (3) tools required to plan the protocols. The protocol specific tools\ninclude PCR primer design, ligation, codon optimization, Gibson or Golden gate assembly, and fetching genes from\nstandard organisms. Many implementations use or were derived from the Go poly library. The annotation tools were\nbuilt using MMSeqs2 [122]. The complete list of tools is given in the supporting information."}, {"title": "4.5 Protein Stability", "content": "Engineering proteins with increased stability is an essential task in protein engineering, with wide-ranging applications\nin enzyme engineering and drug design [123]. Protein stability is a general term for a protein's ability to retain function\nunder non-native conditions, such as increased temperature, lowered pH, or aggregation-inducing solvents. Numerous\nsequence-based and structure-based approaches have been developed to enhance protein stability [124], including\ndeep learning methods such as ThermoMPNN [125]. However, protein stability is determined by complex protein\nsequence and structure properties along with biological context making it challenging to predict accurately with existing\nin-silico approaches [126]. Therefore, an approach that integrates protein structure and sequence methods, including\nphysics-based methods like Rosetta, can provide a more comprehensive understanding of biophysical determinants of\nprotein stability [127].\nThe protein stability environment is composed of tools commonly used by human experts to analyze a protein sequence\nand structure. The main tools are (1)a biochemical description tool, that describes the types of bonds between any\nresidues in the protein sequence,; 2) a sequence property description tool that describes the molecular weight, aromaticity,\ninstability index, iso-electric point, sequence charge, and average hydropathy of a protein sequence; 3) a secondary\nstructure annotation tool; and (4) a Rosetta tool to compute aggregation propensity score per residue [128]."}, {"title": "5 Results", "content": "We assess the capabilities of tool-equipped language agents to solve problems in the aforementioned environments.\nThese environments require iterative cycles of tool calls and observation. We then explore behavior cloning and expert\niteration to train agents on specific tasks in environments. Finally, we explore using inference-time compute via majority\nsampling to improve performance.\nAn overview of the models used in this work and their performance on our tasks is shown in Figure 3. This includes\nboth trained (described below) and frontier language models. They are:"}, {"title": "5.1 Behavior Cloning and Expert Iteration", "content": "Using 1dp, we train language agents in the environments described above. Since these environments are challenging,\nexpert iteration initially rejects the majority of trajectories, leading to very slow learning. We therefore begin with a\nperiod of behavior cloning, using high-quality trajectories collected by rejection-sampling from a larger LLM. Once the\nlanguage agent can solve a reasonable fraction of training problems, we switch to expert iteration. All experiments are\nconducted with Llama-3.1-8B-Instruct [138] as the base language model, using Nvidia A100 GPUs."}, {"title": "5.2 Inference Compute Scaling", "content": "We assessed majority voting on two of the environments that have multiple choice answers \u2013 SeqQA and LitQA2 \u2013 to\nsee if it improves benchmarks in the LDP setting. We evaluated on test splits that we neither trained on nor should be"}, {"title": "5.3 Inference Cost Scaling", "content": "The results of the previous sections demonstrate how the performance of different agents scales as training time and\nsampled trajectories are increased. In this section, we offer a more practical metric: inference cost. This becomes\nespecially relevant in a high-throughput setting, in which agents are tasked to solve thousands of problems in parallel.\nWe focus our comparison on the Claude 3.5 Sonnet agent versus the Llama-3.1-8B EI agent. We use the following\nrates for LLM inference at time of writing:"}, {"title": "6 Discussion", "content": "We have presented a framework of a language agent and environment interacting to solve tasks that require multiple\nsteps of reasoning and tool usage, which we call a language decision process. We implemented five environments,\nincluding three related to biology problems. These three environments contain a variety of tasks, but we focus on tasks\nwith benchmarks that are easy to evaluate, namely LitQA and SeqQA (multiple choice) as well as a task focussed on\nmodifying enzymes to improve their stability. Language agents in these environments perform significantly better than\nnon-agentic LLMs.\nWe have applied two methods to improve the performance of language agents on the tasks we consider: expert iteration\nand majority voting. Smaller models, such as Llama-3.1-8B-Instruct, perform poorly without additional training."}, {"title": "7 Conclusion", "content": "We have presented Aviary, a gymnasium for language agents. Aviary currently contains five environments, three\nof which focus on challenging scientific tasks. Language agents, implemented in these environments, exceed the\nperformance of zero-shot frontier LLMs on the SeqQA, HOTPOTQA, LitQA2, and protein stability tasks. Language\nagents also exceed human performance on SeqQA and LitQA2."}]}