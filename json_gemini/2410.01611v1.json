{"title": "DRUPI: DATASET REDUCTION USING PRIVILEGED INFORMATION", "authors": ["Shaobo Wang", "Yantai Yang", "Shuaiyu Zhang", "Chenghao Sun", "Weiya Li", "Xuming Hu", "Linfeng Zhang"], "abstract": "Dataset reduction (DR) seeks to select or distill samples from large datasets into smaller subsets while preserving performance on target tasks. Existing methods primarily focus on pruning or synthesizing data in the same format as the original dataset, typically the input data and corresponding labels. However, in DR settings, we find it is possible to synthesize more information beyond the data-label pair as an additional learning target to facilitate model training. In this paper, we introduce Dataset Reduction Using Privileged Information (DRUPI), which enriches DR by synthesizing privileged information alongside the reduced dataset. This privileged information can take the form of feature labels or attention labels, providing auxiliary supervision to improve model learning. Our findings reveal that effective feature labels must balance between being overly discriminative and excessively diverse, with a moderate level proving optimal for improving the reduced dataset's efficacy. Extensive experiments on ImageNet, CIFAR-10/100, and Tiny ImageNet demonstrate that DRUPI integrates seamlessly with existing dataset reduction methods, offering significant performance gains.", "sections": [{"title": "INTRODUCTION", "content": "Dataset Reduction (DR) has attracted considerable attention in recent years, with the primary aim of compressing large datasets into smaller subsets while maintaining comparable statistical performance. Existing methods for DR can be broadly classified into two main categories: coreset selection and dataset distillation. Coreset selection methods focus on selecting a subset of samples from the original dataset (Har-Peled & Mazumdar, 2004; Welling, 2009; Toneva et al., 2018), while dataset distillation involves synthesizing unseen samples from the dataset (Wang et al., 2018; Zhao et al., 2020; Zhao & Bilen, 2022; Cazenavette et al., 2022; Wang et al., 2024).\nIn typical real-world scenarios, training models for target tasks is generally constrained to input data (e.g., images) and their corresponding labels, as these are the most readily available elements. While existing DR methods have shown strong performance (Wang et al., 2018; Zhao et al., 2020; Zhao & Bilen, 2022; Cazenavette et al., 2022; Yin et al., 2023), they typically do so by compressing datasets in the same or similar format, such as the conventional data-label structure. Even advanced dataset distillation techniques, which re-parameterize images or labels to create alternative representations (Kim et al., 2022; Zhao et al., 2023; Deng & Russakovsky, 2022; Liu et al., 2022; Wei et al., 2024), are limited by this conventional framework. As illustrated in Figure 1(a), this reliance on fixed data-label structures restricts the capacity of such methods to incorporate richer information that could further enhance model training and improve generalization.\nIn fact, DR settings offer the potential to create more diverse compressed datasets that extend beyond simple input data xi and labels yi, incorporating richer forms of information. A notable example is the concept of privileged information, first introduced in the context of statistical learning (Vapnik & Vashist, 2009; Pechyony & Vapnik, 2010). Figure 2 provides a illustration for privileged information. Let us consider a more concrete example, where x\u2081 might represent a biopsy image, and the"}, {"title": "BACKGROUND AND RELATED WORK", "content": "Similar to prior works on dataset reduction (DR), we focus on classification tasks (Welling, 2009; Zhao et al., 2020; Shin et al., 2023). Consider a multi-classification problem. Let $X \\in \\mathbb{R}^d$ represent the input space and $Y$ denote the set of possible labels. Our dataset, $D_T = \\{(x_i, y_i)\\}_{i=1}^n \\subseteq X \\times Y$, consists of n training samples, where each $x_i \\in X$ is an input vector and $y_i \\in Y$ is its corresponding label. In typical DR settings, the goal is to obtain a smaller dataset, $D_S$ with size m, where m < n. DR methods are typically divided into two categories: coreset selection, where the reduced dataset is a subset of the original, and dataset distillation, where the reduced dataset consists of synthesized data not present in the original set but learned through optimization.\nCoreset selection techniques. Coreset selection techniques are designed to identify a representative subset $D_S$ from the complete dataset $D_T$. This process typically seeks to optimize a criterion that quantifies the informativeness of $D_S$, which matches that of the original dataset $D_T$. The informativeness can be gauged by various metrics, including gradients (Paul et al., 2021; Mirzasoleiman et al., 2020; Killamsetty et al., 2021a), loss values (Toneva et al., 2018), predictive uncertainties (Coleman et al., 2019b), proximity to decision boundaries (Ducoffe & Precioso, 2018; Margatina et al., 2021), and the sharpness of the learned model (Shin et al., 2023).\nDataset distillation methods. Dataset distillation approaches offer an alternative strategy by focusing on synthesizing a distilled dataset $D_S$ from the original dataset $D_T$, rather than directly selecting a subset. This is typically accomplished through a bi-level optimization process that aligns the performance of $D_S$ with that of $D_T$. A distance metric $D$ is employed to quantify the statistical divergence between datasets, guiding the learning of $D_S$ via gradient descent. Specifically, the distilled dataset is updated as follows: $D_S \\leftarrow D_S - \\eta \\cdot \\nabla_{D_S}D(D_S, D_T)$. The choice of distance metric $D$ is versatile and can encompass various aspects such as gradients (Zhao et al., 2020; Lee et al., 2022; Zhao & Bilen, 2021), feature representations (Zhao & Bilen, 2022; Sajedi et al., 2023), training trajectories (Cazenavette et al., 2022; Du et al., 2023; Cui et al., 2023; Guo et al., 2023), and kernel information (Nguyen et al., 2020; 2021; Zhou et al., 2022).\nIn addition to direct performance matching, certain methodologies endeavor to re-parameterize input data to enhance compression efficiency. Techniques employed in this context include exploiting data regularity, as discussed in various studies (Kim et al., 2022; Zhao et al., 2023; Son et al., 2024), factorizing images to capture intrinsic structures (Liu et al., 2022; Deng & Russakovsky, 2022), and employing sparse coding to represent data effectively (Wei et al., 2024)."}, {"title": "METHOD", "content": "Although prior research on dataset reduction has shown impressive results, it mainly focuses on generating reduced datasets in conventional data-label formats, as depicted in Figure 2(a). However, more informative data, such as privileged information, can be utilized to enhance both the utility and"}, {"title": "DETERMINING PRIVILEGED INFORMATION", "content": "representational performance of reduced datasets, as illustrated in Figure 2(b). Below, we briefly explore several forms of privileged information that can be incorporated to achieve this goal.\nSoft labels. We first claim that soft labels are a form of privileged information, as they offer richer insight into how an expert model interprets predictions, providing soft probabilities rather than a single hard label. Specifically, the non-target logits can be viewed as additional information for supervision. Several works have previously explored the effectiveness of soft labels in dataset reduction (Guo et al., 2023; Cui et al., 2023; Bohdal et al., 2020; Qin et al., 2024). However, while soft labels enhance the available information, they are limited to low-dimensional discriminative representations and fail to capture more complex, high-dimensional statistics. Moreover, they do not fundamentally alter the data-label structure of the reduced dataset representation.\nFeature labels. Beyond soft labels, we propose feature labels as a more effective form of privileged information. These labels, derived from unified intermediate representations across well-trained models, encapsulate rich, high-dimensional latent statistics. By providing additional supervision, feature labels enhance model training on datasets with privileged information. Unlike approaches that focus primarily on soft labels, assigning a unified feature label to each input enriches the supervision signal for downstream tasks, effectively addressing the limitations of prior methods.\nAttention labels. Alongside feature labels, we propose attention labels as an alternative form of privileged information that provides a more memory-efficient representation. Attention labels can be derived from either spatial or channel attention of feature labels (Woo et al., 2018). For example, given a feature label of size Ch \u00d7 H \u00d7 W, spatial attention reduces the Ch dimensions through pooling operations (e.g., average pooling, max pooling), resulting in an attention label of size 1 \u00d7 H \u00d7 W. Similarly, channel attention applies pooling along the H \u00d7 W dimension to produce a reduced representation, i.e., Ch \u00d7 1 \u00d7 1. Both feature and attention labels are valuable, with attention labels offering a more efficient representation but with a possible trade-off in the richness of information. We provide more discussion on attention labels in Appendix B.2.\nIn this work, we primarily focus on generating additional feature labels\u00b9 for the reduced dataset, as these forms of privileged information provide more complementary and useful insights for model training. However, privileged information can take various forms beyond attention and feature labels. Depending on the task and the model architecture, other types of information, such as learned embeddings, domain-specific signals, or task-related metadata, could be equally beneficial in enhancing the informativeness and performance of reduced datasets. The flexibility to incorporate different kinds of privileged information allows us to tailor the dataset to specific needs and maximize the potential of the reduced data."}, {"title": "SYNTHESIZING PRIVILEGED INFORMATION", "content": "In this section, we discuss the process of generating privileged information for a given reduced dataset $D_S = \\{(x_i, y_i)\\}_{i=1}^m$, with the goal of obtaining a more informative dataset $D = \\{(x_i, y_i, f_i)\\}_{i=1}^m$. Here, $D_S$ is the reduced dataset of a larger dataset $D_T = \\{(x_i, y_i)\\}_{i=1}^n$, where m \u00ab n. Our primary focus is on incorporating feature labels as the form of privileged information. While various methods can be employed to synthesize privileged information, they can generally be categorized into two strategies: direct assignment and learning-based methods.\nDirect Assignment of Feature Labels. A straightforward approach to obtaining feature labels is by using a pre-trained model, e.g., $g_T$, to extract intermediate features for each input data $x_i \\in D_S$. Specifically, this is formalized as $f_i = g_T(x_i)$, resulting in an extended dataset represented as $D = (x_i, y_i, f_i)$. This method is computationally efficient but relies heavily on the generalization ability of the pre-trained model $g_T$. While the feature labels, which capture the implicit biases of $g_T$, may enhance the quality of the reduced dataset, they could also introduce potential drawbacks. In fact, directly assigned feature labels are often overly discriminative, reducing diversity. However, we find that suitable feature labels should strike a balance between these two properties.\nLearning Feature Labels. A more robust approach to obtaining feature labels is through learning-based methods. Many dataset distillation techniques can be adapted for learning feature labels. For instance, we can employ the Dataset Condensation (DC) (Zhao et al., 2020) method as an illustra-"}, {"title": "METHOD", "content": "tive example to guide the process of learning synthetic feature labels. Suppose we have a learned synthetic dataset Ds. In the typical DC method, we initialize a random model g with parameters \u03b8 = \u03b80 and train it for T epochs on both DT and Ds separately. The synthetic dataset Ds is updated by matching the category gradients between the two datasets, which can be expressed as follows\u00b2:\n$D_S = arg \\min_{D_S} \\mathbb{E}_{0_0\\sim P_{\\theta}} \\sum_{t=0}^T D(\\nabla_{\\theta}L(D_S; \\theta_t), \\nabla_{\\theta}L(D_T; \\theta_t))$\nwhere $L(D_T; \\theta_t) =  \\mathbb{E}_{(x_i, Y_i) \\in D_T} l_{ce} [(Y_i, \\sigma(g(x_i; \\theta_t)))]$,\nand $L(D_S; \\theta) \\triangleq L_{cls} =  \\mathbb{E}_{(x_i,Y_i)\\in D_S} [l_{ce} (Y_i, \\sigma(g(x_i; \\theta_t)))],$                                             (1)\nwhere $l_{ce}(\u00b7, \u00b7)$ denotes the cross-entropy (CE) loss, and $\u03c3(\u00b7)$ represents the softmax function. In contrast, we aim to match the performance between DT and D, where privileged information is synthesized to capture additional informativeness. Let $l_{mse}$ represent the mean square error (MSE) loss, and let $\u03c8(\u00b7)$ denote the intermediate output of model g, i.e., g = \u03c8 \u2022 \u043a, where \u03c8(\u00b7) is the classifier component of g. Therefore, our objective becomes:\n$D = arg \\min_{D} \\mathbb{E}_{0_0\\sim P_{\\theta}} \\sum_{t=0}^T D(\\nabla_{\\theta}L_{cls}(D_5; \\theta_t), \\nabla_{\\theta}L_{cls}(D_T; \\theta_t))$\nwhere $L(D_5; \\theta_t) \\equiv L_{cls} + \\lambda_{reg} \u00b7 L_{reg}$,\n$L_{cls} =  \\mathbb{E}_{(x_i,Y_i) \\in D} [l_{ce} (y_i, \\sigma(g(x_i; \\theta_t)))],$ \n$ \\mathbb{E}_{(x_i, Y_i, f_i) \\in D} [l_{mse} (f_i, \\psi(x_i; \\theta_t))],$                                                           (2)\nand $L_{reg} = \\lambda_{task}\u00b7 L_{task} $\nwhere $\\lambda_{reg}$ is a hyper-parameter to determine the scale of using privileged information. In addition to DC, other dataset distillation methods can also be employed to synthesize feature labels ft. We provide further resuls on coreset selection methods like Herding (Welling, 2009), K-center (Har-Peled & Mazumdar, 2004), Forgetting (Toneva et al., 2018), and dataset distillation methods like DC (Zhao et al., 2020), MTT (Cazenavette et al., 2022), and DATM (Guo et al., 2023).\nIn addtional, we introduce additional supervision to enhance the discriminative power of these feature labels while preserving their diversity.\n\u2022 Task-oriented synthesization. To improve the discriminative power of the feature labels, we adopt a task-oriented approach by feeding the synthesized feature labels into the classifier of the model used to extract gradients during bi-level optimization. We achieve this by performing additional CE loss between the feature label f's prediction and the ground-truth label \u1ef9i. Specifically, we have\n$L_{task} = \\mathbb{E}_{(\\f_i) \\in D} [l_{ce} (Y_i, \\sigma(\\kappa(f_i;\\theta_t)))].$                                                      (4)\nThis allows the feature labels to contribute directly to the final prediction, ensuring they become better aligned with the task at hand. The scale of task supervision is controlled by the hyper-parameter \u03bbtask. Our observations indicate that the preferred feature labels should strike a balance between discriminability and diversity, i.e., they should neither be overly discriminative nor completely lack discriminative power. As shown in Figure 3(a)(c), increasing task tends to cluster the feature labels, reducing their diversity while increasing their discriminability. We find that the optimal feature labels are not achieved with the highest task supervision. Instead, a moderate level of task supervision, as shown in Figure 3(b), strikes the right balance between diversity and discriminability. This also suggests a possible explanation for the drawbacks of directly assigning feature labels from a well-trained model, as these labels tend to be overly discriminative.\n\u2022 Versatility synthesization. Beyond task-specific supervision, we aim to preserve the generative versatility of the feature labels, i.e., a set of feature labels f* \u2208 Fi, where Fi represents the possible feature label set for input xi. This approach involves synthesizing multiple feature labels for a single data-label pair. Appropriate versatility enhancement ensures that the synthesized feature labels remain informative across different tasks and applications, providing a richer and more comprehensive representation of the data. When using multiple feature labels, we primarily employ two"}, {"title": "SYNTHESIZING PRIVILEGED INFORMATION", "content": "strategies: randomly selecting one or using the average feature label from Fi. Further discussions demonstrating the benefits of this versatility are presented in Figure 4(a) and Appendix B.3.\nWe define the overall loss function for a model trained on the reduced dataset D as follows:\n$L(D_5; \\theta_t) = L_{cls} + \\mathbb{E}_{f_t \\in F_i} [\\lambda_{reg} \u00b7 L_{reg} + \\lambda_{task} \u00b7 L_{task}],$ (5)\nwhere Fi denotes the feature label set, containing multiple f* for a single data-label pair (\u017ei, Yi). The pseudocode for learning privileged information is provided in Appendix D."}, {"title": "LEARNING USING PRIVILEGED INFORMATION", "content": "We have previously discussed how to synthesize appropriate privileged information f for a given reduced dataset Ds and extend it into Ds. We now focus on leveraging the new reduced dataset D to enhance a model's performance on unseen test data. Following the learning using privileged information (LUPI) framework (Pechyony & Vapnik, 2010; Lopez-Paz et al., 2015), we incorporate the additional privileged information f during training to build a classifier that outperforms those trained solely on the regular reduced dataset Ds.\nGiven an arbitrary model h with parameters \u03b8\u2208 \u0398, we now formally discuss how to train h with synthesized feature labels. The same loss function $L(D)$, as shown in Eq. (5), is applied to train h, with hyper-parameters (e.g., \u03bbreg, \u03bbtask) kept consistent for fairness.\n$0^* = arg \\min_{\\theta \\in \\Theta} L(D_5; \\theta)$                                                               (6)\nBesides feature labels, we can store attention labels, which can be generated by performing average pooling on the spatial or channel dimensions of learned feature labels. Attention labels contain more condensed information, which can further reduce storage cost. During LUPI, we apply the same pooling strategy for intermediate features of the given model to calculate the MSE loss between the intermediate features and given feature labels."}, {"title": "THEORETICAL ANALYSIS", "content": "We propose a theoretical analysis to elucidate the mechanisms by which the DRUPI framework enhances the quality of reduced datasets. Let $g_T \\in G_T$ denote the oracle function for the original"}, {"title": "THEORETICAL ANALYSIS", "content": "dataset DT, and $|\u00b7|_c$ represent a function class capacity measure, i.e., a measure of model performance. Consider two models: one trained on the pure reduced dataset Ds, represented by $g_S \\in G_S$, and another trained on the reduced dataset with privileged information, D, denoted by $g_{S^*} \\in G_{S^*}$.\nWe begin by briefly reviewing the well-known VC theory (Vapnik, 1998), a fundamental analytical tool in statistical learning theory that forms the basis of our theoretical framework. The VC-dimension defines the performance of a model to limited data points. Specifically, for a model g belonging to a function class G, with a finite VC-dimension $|G|_{VC}$, the expected error R(g) with probability 1 \u2212 \u03b4 is bounded as follows:\n$R(g) \\leq R_m(g) + O(\\sqrt{\\frac{|G|_{VC} - log \\delta}{m}})^\\alpha$,               (7)\nwhere the $O(\u00b7)$ term is the estimation error, and $R_m(g)$ is the training error over m data points, and \u03b1 lies between \u00bd and 1. The parameter \u03b1 represents the difficulty of the task. For more difficult, non-separable tasks, \u03b1 \u2248 \u00bd, yielding a slower learning rate of $O(m^{-1/2})$. For easier, separable tasks, where the model makes no training errors, \u03b1 \u2248 1, yielding a faster learning rate of $O(m^{-1})$. Given a student learning from a fixed dataset of size m, a good teacher model can ease the learning process by accelerating the learning rate from $O(m^{-1/2})$ to $O(m^{-1})$.\nNext, we provide theoretical analysis for DRUPI, showing how incorporating privileged information accelerates the learning process and improves the quality of the reduced dataset. Building on the top of (Lopez-Paz et al., 2015), we extend the results in dataset reduction scenarios. First, assume that the model trained on the pure reduced dataset gs learns the true function gt at a slower rate \u03b1s:\n$R(g_S) - R(g_T) \\leq O(\\frac{|G_S|_c}{m^{\\alpha_s}}) + \\epsilon_s$,                             (8)\nwhere $\\epsilon_s$ is the approximation error of Gs with respect to gT \u2208 GT. Second, assume that the model trained on the dataset with privileged information gs* learns at a faster rate \u03b1s*:\n$R(g_{S^*}) - R(g_T) \\leq O(\\frac{|G_{S^*}|_c}{m^{\\alpha_{s^*}}}) + \\epsilon_{s^*}$,                        (9)\nwhere $\\epsilon_{s^*}$ is the approximation error of $G_{S^*}$ with respect to $g_T$. Finally, assume that the performance difference gs learns from the model with privileged information gs* is\n$R(g_S) - R(g_{S^*}) \\leq O(\\frac{|G|_c}{m^{\\alpha}}) + \\epsilon$,                                             (10)\nwhere \u03b5 is the approximation error of Gs with respect to gs*, and \u00bd \u2264 \u03b1 \u2264 1. Combining Eq. (9) and Eq. (10), the learning rate for the model without privileged information learning the oracle function gT is then given by\n$R(g_S) - R(g_T) = R(g_S) \u2013 R(g_{S^*}) + R(g_{S^*}) - R(g_T)$ $ Gsc $ <0 O $0 $ + ES* \u2264 0 Gs*|c mas \n$R(g_S) - R(g_T) = R(g_S) \u2013 R(g_{S^*}) + R(g_{S^*}) - R(g_T)$ $ Gsc mas + eo + O + $ + ES* \u2264 0 Gs*|c mas \nwhere the final inequality arises because \u03b1 \u2264 1. Therefore, for \u00bd < \u03b1 < 1 in dataset reduction settings, the inequality becomes:\n$O(\\frac{|G_S|_c}{m^{\\alpha}}) + \\epsilon + O(\\frac{|G_{S^*}|_c}{m^{\\alpha_{s^*}}}) + \\epsilon_{s^*} \\leq O(\\frac{|G_S|_c}{m^{\\alpha}}) + \\epsilon_s$.  (12)\nThis inequality highlights the advantages of models trained with privileged information: training using privileged information exhibit lower generalization and approximation errors compared to those trained without privileged information. More importantly, it emphasizes that the privileged information is most beneficial in low-data regimes, which is the typical DR scenario. These benefits align with the principles of LUPI as outlined in (Vapnik et al., 2015; Lopez-Paz et al., 2015)."}, {"title": "EXPERIMENTAL SETUP", "content": "In this section, we investigate the effectiveness of our proposed method, DRUPI, through a series of experiments on diverse datasets and tasks. We begin by evaluating the efficacy of DRUPI when applied to coreset selection and dataset distillation tasks. Specifically, we followed prior works to conduct experiments on CIFAR-10/100 (Krizhevsky et al., 2009) for coreset selection methods, where ResNet-18 (He et al., 2016) is utilized for extracting importance score. For the dataset distillation methods, we conducted experiments on CIFAR-10/100, Tiny ImageNet (Le & Yang, 2015), and subsets of ImageNet (Russakovsky et al., 2015).\nFor coreset selection, we benchmarked against several representative baselines, including Random, L-conf, Entropy, Margin (Coleman et al., 2019a), Glister (Killamsetty et al., 2021b), Graig (Mirzasoleiman et al., 2020), Herding (Welling, 2009), k-Center (Har-Peled & Mazumdar, 2004), and Forgetting (Toneva et al., 2018). More detailed settings are provided in Appendix A.2.\nFor dataset distillation, we evaluated a range of advanced methods, including KIP (Nguyen et al., 2020), DM (Zhao & Bilen, 2022), DSA (Zhao & Bilen, 2021), DCC, DSAC (Lee et al., 2022), CAFE (Wang et al., 2022), IDM (Zhao et al., 2023), DC (Zhao et al., 2020), and MTT (Cazenavette et al., 2022). In line with prior studies, we used networks with instance normalization as the default setting. Unless otherwise specified, distillation was performed with a depth-3 ConvNet for CIFAR-10/100, a depth-4 ConvNet for Tiny ImageNet, and a depth-5 ConvNet for ImageNet subsets. See Appendix A.3 for more details."}, {"title": "MAIN RESULTS", "content": "Coreset selection. In our experiments, DRUPI utilizes the reduced dataset initialized with the Herding, k-Center, and Forgetting methods to assess its performance across diverse fraction on CIFAR-10/100. As shown in Table 1, by incorporating privileged information, these methods consistently outperformed the baseline across a range of fraction settings on CIFAR-10/100. Particularly, on the CIFAR-10 (fraction = 0.4%), DRUPI achieved a performance increase of 24.4% on the Forgetting method and 24.3% on the Herding method. We find that for datasets without optimized instances (e.g., selected coresets), the performance gain is much higher than those with optimized samples.\nDataset distillation. We employed DRUPI in several classical dataset distillation methods, where privileged information is obtained with DC and MTT. Table 2 summarizes the classification performances of ConvNets trained with different distillation methods. Specifically, applying DRUPI to DC on CIFAR-100 (10 IPC) resulted in a 4% improvement. For MTT, DRUPI delivered a 2.4% gain on Tiny ImageNet (1 IPC). Additionally, we evaluated its effectiveness on ImageNet subsets, as shown in Table 3, where DRUPI applied to MTT led to a 3.4% improvement on ImageMeow with 10 IPC, demonstrating strong performance even on larger datasets. Notably, learning both feature labels with DRUPI outperforms simply extracting features alone. Further results for DATM are provided in Appendix B.1."}, {"title": "CROSS-ARCHITECTURE GENERALIZATION", "content": "Cross-architecture evaluation is a critical step toward ensuring robust generalization across previously unseen architectures. We measured the quality of reduced dataset with privileged information on both pruning and distillation settings. To address the misalignment between the shapes of the learned feature/attention labels and the intermediate features of different network architectures, we introduced an additional fully connected layer that is trained alongside the evaluation model.\nFor pruning methods, we utilized ConvNet to synthesize feature labels for selected coresets, and benchmarked their performance across 6 distinct network architectures. As illustrated in Table 4,"}, {"title": "DISCUSSION", "content": "Different ways for synthesizing privileged information. As discussed in Section 3.2, there are multiple methods for synthesizing feature labels. A straightforward approach is to assign feature labels using a pre-trained model. However, as shown in Figure 4, this can sometimes degrade dataset quality. In contrast, learning feature labels offers greater flexibility and adaptability. As demonstrated in Figure 4(a), reduced datasets with learned feature labels significantly outperform those with directly assigned features. This is because feature labels extracted from a pre-trained model often lead to overly discriminative features with low diversity. The empirical results also support the observation that overly discriminative feature labels with strong task supervision can hurt performance in Figure 3. More detailed results are provided in Table 10 in Appendix B.3.\nImpact of feature label versatility and methods for utilizing feature labels. We investigated the effect of synthesizing multiple feature labels for a single data-label pair. As shown in Figure 4(b), experiments demonstrate that increasing the number of feature labels enhances performance, likely due to the greater versatility captured by additional features. However, too many feature labels for a single input can introduce excessive diversity, leading to degraded performance. This verifies the trade-off between the diversity and discriminability of feature labels. Furthermore, averaging multiple feature labels outperforms random selection, which enables us to save only the averaged feature labels. Hence, increasing the number of feature labels does not bring more storage overhead.\nLayer choice for supervision features. We conducted an in-depth analysis to determine which ConvNet layer's features are most effective for supervision. Specifically, we compared features extracted from the first, second, and final layers of a depth-3 ConvNet. Figure 4(c) shows that deeper layers consistently yielded better performance. This is likely due to the final layer's ability to capture more complex and discriminative information, effectively representing high-level semantics. Therefore, we used the last layer's features to supervise the synthesis of feature labels by default."}, {"title": "CONCLUSION", "content": "In this paper, we introduced DRUPI, a novel framework that synthesizes privileged information for reduced datasets. To the best of our knowledge, DRUPI is the first approach to go beyond the traditional data-label paradigm by utilizing synthesized feature labels. Extensive experiments on ImageNet, CIFAR-10/100, and Tiny ImageNet validate the effectiveness of DRUPI, demonstrating significant improvements in model performance when integrated with existing reduction techniques. Additionally, we showed that achieving a balance between the discriminability and diversity of the synthesized feature labels is crucial for maximizing the quality of the reduced dataset."}, {"title": "DETAILED EXPERIMENTAL SETTINGS", "content": "The training was conducted on NVIDIA GPUs, specifically RTX 4090 and A100. All coreset selection experiments were run on A100 GPUs, while all DC-related experiments were carried out with RTX 4090. For MTT and DATM experiments on CIFAR-100 with IPC 10 and 50, as well as Tiny ImageNet, we utilized four NVIDIA A100 GPUs. Results of smaller datasets and lower IPC settings were conducted with RTX 4090."}, {"title": "CORESET SELECTION", "content": "We first employed several coreset selection methods to initialize our reduced dataset, specifically using Herding (Welling, 2009), k-Center (Har-Peled & Mazumdar, 2004), and Forgetting (Toneva et al., 2018), where each data point was selected based on scores from a pre-trained ResNet-18. Next, we synthesized feature labels for the coreset using DC Zhao et al. (2020), assigning these labels to the intermediate features of a ConvNet trained for just one epoch. We also fine-tuned the initial images using the same method as for feature label learning, although we found that simply synthesizing feature labels could already bring performance improvement for reduced datasets.\nThen we provide detailed settings for hyper-parameters. The hyper-parameters include:\n\u2022\n\u2022\n\u2022 Areg: regularization coefficient, which controls the strength of the regularization term in the loss function.\n\u2022 Atask: task supervision coefficient, determining the discriminative power of the synthesized feature labels.\n\u2022 nfeat: number of feature labels synthesized for a single data-label pair, which controls the diversity.\nUnless otherwise specified, for the CIFAR-10 dataset, we set Areg to 0.5, while for CIFAR-100, it was set to 5. Across all configurations, the task supervision coefficient Atask was set to 0.001, and we only synthesized one feature label for a single data-label pair (nfeat = 1) in the reduced dataset."}, {"title": "DATASET DISTILLATION", "content": "For dataset distillation methods, we initialized the reduced datasets (images and labels) using distilled datasets and applied the same distillation method to synthesize feature labels. Specifically, we used DC (Zhao et al., 2020), MTT (Cazenavette et al., 2022), and DATM (Guo et al., 2023) for both data-label initialization and feature label synthesis.\nWe followed the original image-label synthesis settings of these distillation methods to generate feature labels. Table 6 provide the detailed hyperparameter settings used for experiments on CIFAR-10, CIFAR-100, Tiny ImageNet. Table 7 provides the parameter settings for the MTT method on the ImageNet subsets. By default, we set nfeat to 1 and Areg to 0.01. We explored different configurations of Images Per Class (IPC), specifically IPC = {1, 10, 50} for CIFAR-10 and CIFAR-100, and IPC = {1, 10} for Tiny ImageNet.\nFor each method, the hyperparameters for feature synthesis are fine-tuned across different datasets and IPC settings to achieve optimal performance. For instance, in CIFAR-10 (1 IPC), the DC method utilizes Areg = 1.5, task = 0.1, and nfeat = 1. In contrast, for 50 IPC, the same method adjusts its hyperparameters to dreg = 0.001, task = 0.005, and nfeat = 1. Similar fine-tuning is performed for all datasets and IPC values across each method.\nFor distilled datasets with feature labels, we also tried to incorporate additional forms of privileged information, such as soft labels, to further enrich the privileged information in the synthetic dataset. Specifically, we used a pre-trained network to generate soft labels for the distilled dataset. Some methods like DATM have already learned soft labels. We only synthesized soft labels for DC and MTT based reduced datasets. We provide futher results on soft labels in Table 12."}, {"title": "FURTHER PERFORMANCE RESULTS", "content": "In Section"}]}