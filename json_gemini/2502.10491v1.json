{"title": "F-StrIPE: Fast Structure-Informed Positional Encoding for Symbolic Music Generation", "authors": ["Manvi Agarwal", "Changhong Wang", "Ga\u00ebl Richard"], "abstract": "While music remains a challenging domain for generative models like Transformers, recent progress has been made by exploiting suitable musically-informed priors. One technique to leverage information about musical structure in Transformers is inserting such knowledge into the positional encoding (PE) module. However, Transformers carry a quadratic cost in sequence length. In this paper, we propose F-StrIPE, a structure-informed PE scheme that works in linear complexity. Using existing kernel approximation techniques based on random features, we show that F-StrIPE is a generalization of Stochastic Positional Encoding (SPE). We illustrate the empirical merits of F-StrIPE using melody harmonization for symbolic music.", "sections": [{"title": "I. INTRODUCTION", "content": "Owing to their remarkable ability to produce realistic, high-quality samples, deep generative models are attracting significant interest. Two interdependent factors have been clearly established as being important for their superior performance: voluminous data and ever-increasing parameter counts [1]. Domains with abundant data - text, vision and speech have successfully exploited these factors. In comparison, the limited size of publicly-available music datasets places an implicit limit on the size of the models that can be used, making music a challenging data domain.\n\nThe introduction of Transformers and attention has accelerated the advances in deep generative models and music has been no stranger to this phenomenon. However, generated music often lacks long-term coherence and organization, which are hallmarks of real music [2]. One way of improving music generation is to embed prior knowledge about musical structure into data-driven models [3]\u2013[5], for example, through the positional encoding (PE) module of Transformers [6]\u2013[9]. This is an attractive option that provides a drop-in replacement for vanilla, structure-free PE without added complexity or training pipeline changes.\n\nDespite their successes, Transformers bear a quadratic complexity in sequence length, which restricts their use on long sequences. Kernel approximations can be used to mitigate this cost [10], [11].\n\nIn this work, we unite these two strands of research, one of which aims to improve Transformers for music generation by using informative priors, and the other that employs kernel approximations to achieve low-complexity Transformers that are able to process long sequences. In particular, we propose F-StrIPE, a fast structure-informed positional encoding method that works in linear complexity. We show that F-StrIPE is a generalization of Stochastic Positional Encoding (SPE) [12], an existing structure-free positional encoding technique, as sketched in Figure 1. We do this by using Random Fourier Features [13], thereby drawing on and providing a connection to previous work on kernel approximation for efficient attention. We empirically evaluate F-StrIPE on the symbolic music generation task of melody harmonization and show that, compared to the features used by SPE, Random Fourier Features are better-suited to structure-informed PE. We demonstrate how structure can be efficiently used in Transformers for music generation, giving us the coveted twin benefits of better performance and lower computational cost."}, {"title": "II. BACKGROUND", "content": "The Transformer [14] is a sequence-to-sequence architecture that processes all timesteps in a sequence parallely using attention. Given input sequence [x1,..., xT], each element of the output sequence is:\nym = \\frac{\\sum_{n}a_{mn}v_{n}}{\\sum_{n}a_{mn}}\\text{ with }a_{mn} = \\exp \\Big( \\frac{q_{m}^{T}k_{n}}{\\sqrt{D}}\\Big) \\\\                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (1)\n\nwhere amn = qmkn is the attention coefficient or \u201csimilarity score\" for a pair of timesteps (m, n), with m, n \u2208 {1, ..., T}. The query, key and value vectors qm, km, vm are obtained by linearly transforming input xm. Since attention executes pairwise computa-tion among all timesteps, Transformers are invariant to permutations in the temporal order of inputs. Hence, positional encoding (PE) is applied to provide the model with a sense of time. Positional information can be incorporated in two places: at the input or during attention computation. The latter is called Relative Positional Encoding (RPE) and we focus on this approach here.\n\nAttention has quadratic complexity in sequence length. To address this, a kernelized form of attention was introduced:\namn = K(qm, kn) = E[$(qm)$(kn)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (2)\n\nwhere K is a positive (semi)definite kernel and $(x) : RD \u2192 RD\u00a2 defines a randomized feature map for x [11], [15]. With multiple instantiations, $ captures, on average, the relationship between qm and kn, typified by K. Thus, coefficients amn need not be computed explicitly, which produces linear-complexity Transformers.\n\nThe efficient formulation described above is not directly applicable to RPE which, as introduced in [16], requires the explicit computation"}, {"title": "III. METHODS", "content": "As we hinted in Section II, positional encoding depends on a sequence P = [P1,P2,..., \u0440\u0442] of positional indices. In standard PE, which does not utilize structure, pi = i, which makes Pa linear grid. When structure is included in PE, pi = se(i), where se(i) gives the structural label at level l (e.g. chord) for timestep i. In this case, se(i) = se(i') is possible for i \u2260 i', making Pa non-linear grid. In fact, we can consider vectorial positional indices with multiple resolutions of structural organization. Consequently, we obtain pi = s(i), where s(i) = [s1 (i), ..., Se(i), ..., SL (i)] is a vector of L structural labels. Viewed in this way, PEs with structure can simply be used as a drop-in replacement for PEs without structure by replacing the form of pi, giving us richer positional information. This is a way to flexibly represent domain-specific prior knowledge about the underlying data domain.\n\nIn order to use rich positional information in SPE, we can augment the sinusoidal feature matrix \u03a9 from (5) to be:\n[\u03a9(P, f, 0)]ij =\\begin{cases}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      cos \\big(2\u03c0f[\u03c9, :]pi + \u03b8[\u03c9]\\big) & \\text{if }j = 2\u03c9 \\\\ sin \\big(2\u03c0f[\u03c9, :]pi + \u03b8[\u03c9]\\big) & \\text{else}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\end{cases} (6)\n\nHere, we use the vectorial formulation of structure-aware positional indices pi = s(i), unlike (5) where pi = i was a sequence of structure-free positional indices linked solely to the passage of time. Whereas in (5), f[w] was a single frequency, f[w, :] in (6) is a vector of frequencies. Therefore, each frequency f[w, l] in this vector acts on the (th structural label at timestep i. We can combine (3), (4) and (6) to obtain a fast structure-aware PE technique that uses Stochastic Fourier Features. We call this method F-StrIPE:SFF.\n\nUsing Equations (3) and (4), we can express the SFF approximation of the positional matrix Pa for arbitrary timesteps m and n as:\nPa[m,n] \u2248 [[\u03a9[m, :](ZdZ)\u03a9[ :, n]]/R                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         (7)\nwhere we use the abbreviation \u03a9 = \u03a9(PQ/K, fa, 02/K) diag (xa). We observe that ZdZ\u221a = \u0108a acts as an empirical covariance matrix for the features \u03a9Qd and \u03a9Kd. Since Za has zero mean and unit"}, {"title": "IV. EXPERIMENTS", "content": "We assess the merits of our approaches on the task of melody harmonization for symbolic music.\n\nWe use the Chinese POP909 dataset [20] and three levels of struc-tural labels with different resolutions [21]: melodic pitch (16th-note), chord (quarter-note) and phrase (measure). Each MIDI file in this dataset consists of three tracks: melody, bridge (second melody) and piano (accompaniment). We use the POP909 alignment dataset [6] to correctly match the structural labels with the input. We convert the MIDI files to binary pianorolls X \u2208 B(ntracks \u00d7128) \u00d7 time, B = {0,1}, where ntracks is the number of tracks and ntime is the number of timesteps in the pianoroll.\n\nGiven the sequence for the melody and bridge tracks [xn \u2208 IB(tracks -1) \u00d7128] as input, with n\u2208 {1, ..., Ntime e}, the model must predict all tracks [yn\u2208 B tracks \u00d7128]. We expect the model to produce the complete accompaniment track for all timesteps at once, without conditioning later predictions on earlier predictions. We use two settings: (16, 16) and (16, 64). The first number is the sequence length (in measures) for training and the second is that used for testing.\n\nWe use a 2-layer causal encoder Transformer with 4 heads and 512 model dimension. Training for 15 epochs with a batch size of 8, we use gradient clipping and curriculum learning [22]. We use two learning rate schedulers: a linear warmup and an epoch-wise decay. We do a grid-search for two hyperparameters: learning rate (choices: {1,5,10} \u00d7 0.0001) and post-processing binarization strategy [6] (choices: thresholding, thresholding with merge). While the first binarization strategy uses a fixed threshold, the second additionally fills the gap between notes if the gap is less than a minimum distance."}, {"title": "V. RESULTS AND DISCUSSION", "content": "When we compare PEs without structure (NoPE, SPE) against PEs with structure (S S-RPE, F-StrIPE), we observe that the latter perform better. This matches previous findings reported in the literature [6] which argued that using structure in PE boosts performance, particularly in underdetermined problems such as melody harmonization.\n\nF-StrIPE:SFF adds structural information to SPE and F-StrIPE improves on this by using RFF in place of SFF. F-StrIPE:SFF yields marginal improvements over the performance of SPE in the (16, 16) scenario, in particular, on CS. This lends some additional support to our previous observation that structural information used in PE is useful. However, F-StrIPE, which uses a noise-free estimate of the positional matrix Pda, gives us significant boosts over the performance of F-StrIPE:SFF and, by extension, that of SPE, on both (16, 16) and (16, 64). These improvements are particularly noticeable in CS, GS and NDD and are especially pronounced in the (16, 16) setting. This emphasizes that the correct approximation techniques can strongly enhance the effect of augmenting our models with prior knowledge.\n\nAs described in Section IV-D, we perform ablations on structural levels used during training, resulting in models that are specialized to use only melody (F-StrIPE:M), only chord (F-StrIPE:C) or only phrase (F-StrIPE:P) as structural information. When we compare these models against F-StrIPE, which uses all three structures, we see first that F-StrIPE:M and F-StrIPE:P do worse than F-StrIPE on all metrics and both task settings. In fact, their performance drops lower than even NoPE. In contrast, F-StrIPE:C brings a clear advantage over F-StrIPE, with significant improvements on all metrics and both task settings. This fits our intuition that the accompaniment in pop songs can be nicely characterized by chord progressions [26], [27]. Our results show that using only chord information is better than using all structures simultaneously. This shows that while task-specific structural information can boost performance, ill-founded and generic priors can prove counterproductive. Thus, how prior knowledge is selected and incorporated into a deep-learning model should be an important consideration while designing such systems.\n\nThese results should be viewed in the context of the complexity analysis presented in Table II. Compared to S S-RPE, SPE and F-StrIPE have linear complexity in sequence length, which makes a sizeable dent in the requirement for computational resources. Moreover, scaling up from SPE to F-StrIPE only adds a factor of \u09e7, corresponding to the number of structures we use in PE, which grows the slowest compared to all other variables. In fact, since the ablations use only a single structure at a time ( = 1), the complexity of our best-performing method, F-StrIPE:C, matches that of SPE.\nThus, on the one hand, in the worst case where multiple structures are needed, it only costs a small amount of additional computational resources. On the other hand, if we already know which structure is best-suited to our task, we can benefit from F-StrIPE and leverage prior knowledge in our model without needing any extra resources.\n\nFinally, we see that models that are trained on 16 bars of music but tested on 64 bars of music reflect the same trends as seen in the models that were tested on 16 bars of music: structural information combined with RFF provides a sizeable improvement over baselines that either do not use structure or use structure but with SFF.\n\nOn CS and GS, the PEs that use SFF (SPE, F-StrIPE:SFF and F-StrIPE:SFF:C) show large improvements on the (16, 64) setting compared to the (16, 16) setting. This does not hold true for the RFF-based PEs, where some methods show small improvements and others show small deteriorations. Nevertheless, F-StrIPE:C out-performs all other PEs in the (16, 64) situation. This suggests that an in-depth comparative investigation of the characteristics of different approximation techniques is needed. Specifically, it would be interesting to understand which approximation is suited to what learning scenario and whether we can combine the strengths of different approximations to obtain a more robust, fast, structure-informed PE. Interestingly, in two metrics - SSMD and NDD - all the methods in Table I do marginally better in the (16, 64) scenario compared to the (16, 16) one.\n\nThe transfer of performance from the (16, 16) setting to the (16, 64) setting can be partly attributed to the presence of stereotypical structure and a high degree of repetition in pop songs [28], [29]. Thus, 16 measures of music could potentially contain much of the necessary information to generate much longer sequences. It would be interesting to quantatively assess whether this hypothesis is true."}, {"title": "VI. CONCLUSION", "content": "In this paper, we demonstrated how structural information can be used in linear-complexity positional encoding, thereby retaining superior performance without sacrificing efficiency. We did this by first extending SPE to accept multi-resolution, structure-aware positional indices, obtaining F-StrIPE:SFF. Then, we showed the connection between SPE [12] and Random Fourier Features [13] and developed a novel method, called F-StrIPE, framed as a structure-aware generalization of SPE. The combination of these two inter-ventions using structure and using Random Fourier Features gave us a fast, structure-informed positional encoding method that outperformed SPE, F-StrIPE:SFF and other competitive baselines on melody harmonization for symbolic music."}]}