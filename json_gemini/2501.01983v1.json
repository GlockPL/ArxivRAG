{"title": "ECG-guided individual identification via PPG", "authors": ["Riling Wei", "Hanjie Chen", "Kelu Yao", "Chuanguang Yang", "Jun Wang", "Chao Li"], "abstract": "Photoplethsmography (PPG)-based individual identification aiming at recognizing humans via intrinsic cardiovascular activities has raised extensive attention due to its high security and resistance to mimicry. However, this kind of technology witnesses unpromising results due to the limitation of low information density. To this end, electrocardiogram (ECG) signals have been introduced as a novel modality to enhance the density of input information. Specifically, a novel cross-modal knowledge distillation framework is implemented to propagate discriminate knowledge from ECG modality to PPG modality without incurring additional computational demands at the inference phase. Furthermore, to ensure efficient knowledge propagation, Contrastive Language-Image Pre-training (CLIP)-based knowledge alignment and cross-knowledge assessment modules are proposed respectively. Comprehensive experiments are conducted and results show our framework outperforms the baseline model with the improvement of 2.8% and 3.0% in terms of overall accuracy on seen- and unseen individual recognitions.", "sections": [{"title": "I. INTRODUCTION", "content": "Physiological signal-based individual recognition has emerged and received great attention in recent years. The main idea of such technology is to identify humans via their intrinsic information extracted from heart and brain activities by analyzing PPG [1], ECG [2], Electroencephalogram (EEG) [3] and etc. As one of the common physiological signals, PPG which reflects human cardiovascular activities has been widely used in the clinic and daily health monitoring due to the convenience and easy to be collected [4]. In recent years, the application of PPG-based biometric technology has emerged [5] [6] [7]. On the one hand, with the development of biometric spoofing technology, the security of appearance-based methods, e.g., face recognition [8] tend to be damaged while PPG-based methods extract intrinsic information which makes PPG difficult to forge by imposters. On the other hand, investigating the discrimination of PPG signals provides a"}, {"title": "II. RELATED WORKS", "content": "PPG-based individual identification. The main idea is to distinguish humans in large databases, which are regarded as multi-class classification tasks. Handcrafted-based method [5] [6] is proposed which tends to be effective on small-scale databases while failing on large-scale databases due to unpromising generalization capability. To tackle this issue, a novel end-to-end deep learning-based method has been proposed in recent years and shows promising results thanks to superior generalization capability [7].\nCross-modal knowledge distillation (CMKD). Knowledge distillation (KD) [13] aims at transferring knowledge from a large model to a smaller one to optimize memory cost and inference speed and was widely used in computer vision [14] [15] [16] and cross-modal learning tasks [17]. Unlike conventional KD, which accepts the same input, in CMKD, the teacher and the student use different modalities as input. The goal of CMKD is to propagate knowledge from a discriminate modality to a weaker one. For instance, Jin et al. [18] proposed CMKD to improve the performance of speaker recognition supervised by facial representations."}, {"title": "III. METHODOLOGY", "content": "Notations. Given a teacher model T and a student model S, which take ECG signal and PPG signal as input respectively. We let \\(T_f\\) and \\(S_f\\) be the feature extractor of T and S respectively. \\(T_{cls}\\) and \\(S_{cls}\\) are the classifier of T and S. \\(F_t\\) and \\(F_s\\) are the feature embedding of \\(T_f\\) and \\(S_f\\).\n\nA. CLIP-based Knowledge Alignment (CLIP)\n\nIn CMKD studies, one of the biggest challenges is the domain gap between different modalities which might damage the efficiency of knowledge propagation. Hence, in this"}, {"title": "A. CLIP-based Knowledge Alignment (CLIP)", "content": "In CMKD studies, one of the biggest challenges is the domain gap between different modalities which might damage the efficiency of knowledge propagation. Hence, in this part, we design a plug-and-play knowledge alignment module which is used to project \\(F_t\\) and \\(F_s\\) to a common feature space before transferring knowledge. As it is only used in the training phase, there are no further computation requirements at the inference stage. Specifically, we first train an alignment model containing 2 projection layers in a CLIP manner [19]. The overall architecture consists of two encoders, dubbed \\(E_t\\) and \\(E_s\\) respectively, and two projection heads, noted as \\(P_t\\) and \\(P_s\\), illustrated in Fig. 2. As an example, ResNet34 and 1 Fully Connected layer are employed as encoders and projection heads respectively. The dimension of embeddings are projected from 512 dimensions to 256 dimensions. InfoNCE [20] is adopted to train the model which is implemented as follows:\n\n\\(L_{NCE} = \\frac{1}{2N} (\\sum_{i=1}^N -log(\\frac{e^{s_{i,i}/\\tau}}{\\sum_{j=1}^N e^{s_{i,j}/\\tau}})+\\sum_{i=1}^N -log(\\frac{e^{s_{i,i}/\\tau}}{\\sum_{j=1}^N e^{s_{j,i}/\\tau}}))\\)\n\nWhere, \\(s_{j,i}\\) is the dot operation between i th ECG embedding \\(e_i\\) and j th PPG embedding \\(f_j\\). \\(\\tau\\) is temperature. It should be noted that only \\(P_t\\) and \\(P_s\\) are utilized in the next step, which projects \\(F_t\\) and \\(F_s\\) to a common latent space respectively. The aligned embedding noted as \\(F^a_t\\) and \\(F^a_s\\). Subsequently, Maximum Mean Discrepancy (MMD) [21] and Triplet Loss [8] are employed as relation-based knowledge, noted as \\(L_{rel}\\), on seen- and unseen- individual recognition testings respectively. MMD can be summarized as:\n\n\\(L_{mmd}(P_t, P_s) = ||E_{x_t \\sim P_t} [k(\\cdot, x_t)] - E_{x_s \\sim P_s} [k(\\cdot, x_s)]||_{H_k}\\)\n\nHere, \\(P_t, P_s\\) are regarded as the feature distribution of \\(T_f\\) and \\(S_f\\). \\(x_t\\) and \\(x_s\\) are the sample of \\(P_t\\) and \\(P_s\\). \\(||\\cdot ||_{H_k}\\) is the RKHS norm. k is the Gaussian kernal with bandwidth \\(\\sigma\\) summarized as:\n\n\\(k(x_t, x_s) = exp(-\\frac{||x_t - x_s||^2}{2 \\sigma^2})\\)\n\nThe Triplet loss is summarized as:\n\n\\(L_{triplet}(P_s, P_t^+, P_t^-) = max(d(P_s, P_t^+)-d(P_s, P_t^-)+m, 0)\\)\n\nHere, \\(P_t^+\\) and \\(P_t^-\\) are the teacher embedding extracted from the same and different subjects with \\(P_s\\). \\(d(\\cdot)\\) is the distance measurement. m is a hyperparameter, which is set to 1."}, {"title": "B. Cross-Knowledge Assessment (CKA)", "content": "We first implemented conventional KD proposed by [13] as follows:\n\n\\(L_{kd} = H(P_t, q_s; \\tau)\\)\n\nWhere \\(\\tau\\) is the temperature, that is used to balance the smoothness of output. In our experiments, \\(\\tau\\) is set to 4. \\(P_t\\) and \\(q_s\\) are the output of \\(T_{cls}\\) and \\(S_{cls}\\). \\(H(\\cdot)\\) indicates Kullback Leibler (KL) divergence.\n\nIn this part, CKA is proposed to evaluate the learning and teaching outcomes by the teacher and student respectively as illustrated in Fig. 3. To achieve this goal, \\(F_s\\) are fed into \\(T_{cls}\\) and obtain \\(q_t\\). \\(T_{cls}\\) is regarded as a referee to distinguish how much knowledge has been learned by S by measuring the similarity of \\(p_t\\) and \\(q_t\\). The discrepancy of the knowledge between T and S is implemented as:\n\n\\(L_t = H(P_t, q_t; \\tau)\\)\n\n\\(L_i = H(p_s, q_s; \\tau)\\)\n\nHere, \\(p_s\\) is the output of \\(S_{cls}\\) which takes \\(F_t\\) as input. Thus, the total cross-knowledge assessment loss can be written as:\n\n\\(L_{cross-kd} = \\frac{1}{2}(L_t + L_i)\\)\n\nFinally, the overall loss function can be implemented as:\n\n\\(L_{all} = L_{task} + L_{kd} + L_{rel} + L_{cross-kd}\\)\n\nwhere \\(L_{task}\\) is the task-related loss function. Specifically, in classification task \\(L_{task}\\) is regarded as cross-entropy loss while in retrieval tasks it is known as a bank of cross-entropy loss and triplet loss."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "A. Dataset\n\nMIMIC dataset [27] is employed which contains 943 subjects and 12000 records for both ECG and PPG signals. All signals are sampled as 125Hz. We only use recordings longer than 8 minutes to evaluate our proposed framework. In our work, 341 individuals with high-quality ECG signals and PPG signals are involved."}, {"title": "B. Experiments and Evaluation Metrics", "content": "A 300-point sliding window is utilized to segment signal samples from original signals into non-overlapping samples for both ECG and PPG signals. Subsequently, data standardization is employed to remove noise. In this work, we utilize 1D ResNet34 [28], MobileNetV1 [29], and ShuffleNetV1 [30] to evaluate the generalization capability of the proposed framework. Heterogeneous model architectures might introduce lower similarity between the teacher and the student compared to homogeneous models [31]. To focus on our main goal, we just investigate homogeneous models of teacher and student.\n\nWe conduct two sets of experiments, namely:\n\n1. Sample-wise individual recognition: Aiming at recognizing humans in the seen domain. 341 subjects are involved in this testing. For the training set, we use the first 6.4 minutes recordings of each subject. For the testing set, the rest 1.6 minutes of recordings are utilized for evaluation.\n\n2. Subject-wise individual recognition: The goal of this testing is to evaluate the generalization capability of the proposed framework. Specifically, 272 subjects are involved in the training set and the other 69 subjects are selected randomly in the testing phase. In other words, the test individuals are disjoint from the training individuals.\n\nOn sample-wise testing, we utilize Overall Accuracy (OA) and F1-score(F1) for classification tasks. On subject-wise testing, OA and Equal Error Rate (EER) are employed."}, {"title": "V. RESULTS AND DISCUSSION", "content": "A. Sample-wise Testing\n\nWe first conduct experiments on sample-wise recognition, which is used to identify individuals in the seen domain. Firstly, we trained a baseline model supervised by cross-entropy loss without KD on MIMIC datasets for classification tasks as shown in Tab. I. The teacher model takes ECG as input under the supervision of only cross-entropy loss as well. From Tab. I, it can be seen that employing ECG signals can significantly improve recognition performance regardless of different architectures. For example, our framework boosts the performance of ShuffleNetV1 by 2.8% and 3.1% in terms of OA and F1.\n\nB. Subject-wise Testing\n\nExperiments on recognizing subjects in the unseen domain are conducted on ResNet34. The baseline model and teacher model are supervised by a bank of cross-entropy loss and triplet loss. We adopt a N-shot manner to test our framework. Specifically, N is set to 1, 5, and 10 respectively and the result is shown in Tab. II. From the table, it can be seen that our framework achieves superior performance with the improvement of 0.4% - 3.0% and 0.6% - 1.8% in terms of OA and EER compared to the baseline model, which indicates the promising generalization capability of the proposed method.\n\nC. Compare with SOTA methods\n\nWe conduct experiments to evaluate the performance of our framework by comparing it to state-of-the-art approaches in knowledge distillation. Tab. III illustrates that the proposed method achieves state-of-the-art performance among several mainstream KD methods, which indicates that our approach can propagate knowledge effectively between different modalities.\n\nD. Ablation studies\n\nWe conduct the following ablation testing to evaluate the effectiveness of the proposed CLIP module and CKA module with ResNet34. It can be seen clearly from Tab. IV that only utilizing a CLIP module or CKA can boost the performance compared to the baseline. Besides, applying both CLIP and CKA achieves the best performance which is 95.1% and 94.5% in terms of OA and F1.\n\nE. Visualization\n\nAs shown in Fig. 4, T-SNE [32] is employed to visualize the feature distributions of the teacher and student employing ShuffleNetV1 as the backbone. We observe that the proposed framework shows better inter-class distance compared to the baseline. Hence, identities tend to be easier to distinguish. This also indicates the effectiveness of our method."}, {"title": "VI. CONCLUSION", "content": "In this study, we introduce a cross-modal knowledge distillation framework designed to enhance the performance of PPG-based individual recognition, guided by ECG modalities. To ensure efficient knowledge propagation, a CLIP-based knowledge alignment module is proposed to project both ECG and PPG modalities into a common latent space. Subsequently, cross-knowledge assessment is conducted to evaluate the outcomes of the teaching and learning processes. Experimental results demonstrate that our approach achieves state-of-the-art performance in both sample-wise and subject-wise testing.\n\nHowever, we also identify limitations within the proposed framework. Specifically, as the number of N-shots increases from 5 to 10 in subject-wise testing, our framework exhibits performance degradation, contrary to the results observed with the teacher model. We consider these findings as directions for future research."}]}