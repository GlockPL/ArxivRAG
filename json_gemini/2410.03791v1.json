{"title": "PEOPLE ARE POORLY EQUIPPED TO DETECT\nAI-POWERED VOICE CLONES", "authors": ["Sarah Barrington", "Hany Farid"], "abstract": "As generative AI continues its ballistic trajectory, everything from text to audio,\nimage, and video generation continues to improve in mimicking human-generated\ncontent. Through a series of perceptual studies, we report on the realism of AI-\ngenerated voices in terms of identity matching and naturalness. We find human\nparticipants cannot reliably identify short recordings (less than 20 seconds) of AI-\ngenerated voices. Specifically, participants mistook the identity of an AI-voice for\nits real counterpart 80% of the time, and correctly identified a voice as AI-generated\nonly 60% of the time. In all cases, performance is independent of the demographics\nof the speaker or listener.", "sections": [{"title": "Introduction", "content": "In January 2024, in the lead up to the November US presidential election, an estimated tens of\nthousands of Democrat voters received a robocall in the voice of President Biden instructing them\nnot to vote in the upcoming New Hampshire primaries. The voice was, of course, AI-generated.\nThe perpetrators of this attempted election interference were a political consultant Steven Kramer, a\nNew Orleans-based street magician and hypnotist Paul Carpenter who was paid $150 to create the\nfake audio, and a telecommunications company. Carpenter used ElevenLabs, a platform offering\ninstant voice cloning for as little as $5 a month. Kramer was fined $6 million and subsequently\ncharged with two dozen counts including impersonating a candidate and voter suppression, while\nthe broadcaster, Lingo Telecom, received a $1 million fine for transmitting the calls.\nThe ballistic rise of generative AI has super-charged all forms of frauds and scams from election\ninterference, to disinformation campaigns [1], small-[2] and large-scale [3] fraud."}, {"title": null, "content": "There is a large literature on distinguishing between real and manipulated content [4]. These\ntechniques largely operate asynchronously and not as an audio or video call is unfolding in real\ntime. The synchronous detection of fraudulent calls poses significant challenges. Until technology\ncan monitor every landline, mobile device, and video call (raising privacy concerns), consumers are\nlargely left to their own defenses to sort out the real from the fake.\nStudies focusing on visual imagery have concluded that human participants are at chance at\ndistinguishing head shots of real and AI-generated people [5, 6], while performance is only slightly\nbetter for video of people talking [7, 8].\nOn the audio front, Mai et al. [9] report that human participants were only able to accurately\ndistinguish real from AI-generated voices with an accuracy of 70.4%. This study, however, only\nused a single English and a single Chinese speaker identity, and the spoken phrases consisted of\na single sentence ranging in length from 2 to 11 seconds (by comparison the fake Biden robocall\nwas 40 seconds in length). M\u00fcller et al. [10] report a similar accuracy of 80%. This second study\nhas the advantage that it employed multiple speaker identities (107), but the spoken phrases were\nstill relatively short at one to two sentences in length. For both of these studies, the AI-generated\nvoices were not created using state-of-the-art, commercially available techniques, and both studies\nfocused on the naturalness question (is the voice real or fake) and not on the identity question (who\nis speaking).\nWe expand on these previous studies by employing the state-of-the-art voice cloning of ElevenLabs\n(used in the Biden robocall), increasing the number of speakers to over 200, and considering how\ndifferent tasks (identity and naturalness) impact our ability to distinguish AI-powered voices. This\nmore diverse study reveals that people are poorly equipped to identify short recordings (less than\n20 seconds) of AI-generated voices, both in terms of identity matching and naturalness. We do,\nhowever, find that performance improves for longer recordings."}, {"title": "Materials and Methods", "content": null}, {"title": "Study Design", "content": "Our study consists of three parts that evaluate the naturalness and identity of AI-cloned voices. For\nthe naturalness (#1 and #2), participants listened to one voice at a time and asked to classify it as\nreal or AI-generated. For the identity (#3), participants listened to two voices back-to-back (saying\nsomething different) and were asked to specify if the voices are from the same identity.\nFor parts 1 and 3, participants were randomly assigned to one of 10 batches comprising a randomized\nset of 44 stimuli, 30 of which were scripted single-sentence responses, 10 of which were unscripted\nresponses, and 4 attention checks. There was no stimuli overlap between batches. Part 2 of the\nnaturalness task was added to disambiguate a result from part 1 in which there was a confound\nbetween scripted and unscripted responses with audio length. This part contained a new set of 25\nadditional audio clips with longer scripted responses.\nFor both naturalness parts, half of the audio clips were real and half were AI-generated. For the\nidentity part, the paired audios were either both real, one real one fake, or both fake (Table 1).\nParticipants were not told of these distributions."}, {"title": "Speaker Participants", "content": "Recordings of 220 speaking participants were collected through the Prolific research recruitment\nplatform [11]. Speakers gave their consent for the use of their voice and likeness. Speakers were\nselected from a stratified sample ensuring equal distribution of gender; all participants were native\nEnglish speakers and U.S. residents. Participants were paid $7 for their time. Participant ages\nranged from 18-75 (mean=38, sd=11.4), with 109 identifying as male, 107 female, and 4 non-binary.\nRacial identities included 158 White/Caucasian, 39 Black/African American, 26 Asian, 4 American\nIndian/Alaska Native, 2 Native Hawaiian/Other Pacific Islander, 5 other.\nEach speaker was instructed to record themselves responding to 32 prompts. The first two prompts\nwere used for voice-cloning (see below). The remaining prompts were divided into four categories:\n(1) standardized scripted responses in which each speaker read the same prompt extracted from\ntranscripts of the TIMIT dataset [12]; (2) randomized scripted responses in which each speaker read\na randomized prompt from TIMIT; (3) unscripted responses in which each speaker responded to\nfour open-ended questions, and asked for a response that was close to 30 seconds in length; and (4)\ncombined responses consisting of four open-ended unscripted questions in which each speaker read\nout loud a question and then answered the question.\nBoth audio and video were recorded using a custom-built web application. The audio/video\nrecordings were converted from their initial .webm format to .mp4 at a bitrate of 192 kbps from\nwhich the audio was extracted as a .wav file. All real and fake audio files were converted to a .mp3\nformat with a sample rate of 44kHz, with an amplitude normalized between \u20131 and 1, and with\nsilences at the start and end removed."}, {"title": "Speaker Voice Cloning and Matching", "content": "A voice clone of each of the 220 speakers was generated using the ElevenLabs' Instant Voice\nCloning API. Transcripts of speakers' responses were used to create a cloned version of each\noriginal audio clip. For scripted responses, we assumed that the speaker correctly repeated the\nprompt; for unscripted responses, OpenAI's Whisper [13] was used to transcribe the audio.\nFor the identity study, participants heard two voices either of the same or different speaker identities.\nTo make this task more challenging, for each speaker in our dataset, we determined another speaker\nwith a perceptually similar voice. This matching was performed by first extracting a 192-D TitaNet\nembedding [14] of the same scripted sample. The closest matching voice was determined by\nfinding the voice of another speaker (with replacement) with the maximal cosine similarity between\nextracted embeddings (the mean similarity was 0.6 in the range [-1, 1])."}, {"title": "Listener Participants", "content": "A total of 634 participants were recruited from the Prolific crowd-sourcing platform, split into two\ngroups of 330 and 304 for the naturalness and identity studies. Listener ages ranged from 18-77\n(mean=35, sd=11.7), with 308 male, 307 female, 12 non-binary and 7 not providing their gender.\n428 listeners identified as White/Caucasian, 140 as Black/African American, 46 as Asian, 21 as\nAmerican Indian/Alaska Native, 4 as Native Hawaiian/Other Pacific Islander, 32 as other and 6\npreferred not to share."}, {"title": "Results", "content": null}, {"title": "Identity", "content": "In the identity task, participants classified an AI-generated voice as having the same identity as a real\nvoice 79.8% of the time (Table 1). By comparison, two real voices were correctly classified as the\nsame identity 92.0% of the time. When the voices corresponded to different identities in these two\nconditions, participants were correct 89.5% and 85.7%, respectively. Chance performance is 50%.\nIn other words, AI-cloned voices are not entirely convincing, but they are close. A Kruskal-Wallis\ntest reveals a statistically significant effect of condition ($\\chi^2(5) = 54.92$, $p < 0.001$). Speaker and\nlistener demographics (age and gender) had no significant impact on accuracy.\nAlthough not central to the question of realism, the Fake, same ID condition (penultimate row\nof Table 1), compared the identity of two samples of the same AI-generated voice. Participants"}, {"title": "Naturalness", "content": "In the naturalness task, for unscripted recordings (see Materials and Methods) with a mean length of\n17.9 seconds (as compared to 15.8 seconds for the identity task), the mean accuracy on distinguishing\na real from a fake voice was 76.7% (Table 1). This mean accuracy, however, is asymmetric where\nsensitivity (correctly classifying a real voice) is 85.5%, and specificity (correctly classifying a fake\nvoice) is 66.3% (chance performance is 50%). That is, participants were unreliable at detecting\nAI-generated voices with a bias to reporting a voice is real.\nFor short scripted responses (less than 10 seconds in length), performance was also quite poor, with\na mean accuracy of 59.3% and a sensitivity and specificity of 60.6% and 57.8%. The mean/median\naccuracy, however, increases to 82.7%/86.8% for audios longer than 30 seconds (Figure 1).\nBoth duration and the real condition exhibited significant effects (mixed-effects model coefficients\n56.1 and 918.3, p-values 0.002 and 0.000, using a significance level of 0.05 and square-transform"}, {"title": "Qualitative Analysis", "content": "At the end of each study, participants were asked to share any tactics they used to differentiate\nbetween real/fake voice (330 responses) and same/different voice (304 responses). Keywords were\nextracted from their responses using a qualitative coding analysis and grouped into thematic codes.\nAs shown in Figure 2, the top three most frequent codes in the identity task were \"inflection,\"\n\"breathing,\" and \"background noise.\" For the naturalness task, these were \"inflection,\" \"acccent,\"\nand \"other.\"\nWhile some of these cues are almost certainly diagnostic, not all are. For example, \"background\nnoise\" was mentioned 79 times across both studies, yet, upon analyzing the average background\nnoise in both datasets, no significant effects on performance were found."}, {"title": "Discussion", "content": "Even in these early days of generative AI, synthesized voices have nearly passed through the\nuncanny valley in terms of naturalness and identity. Given the recent trajectory, there is good reason"}, {"title": null, "content": "to believe that AI-generated voices will soon be indistinguishable from reality. While this should be\nconsidered a triumph for those on the generative side, it raises real concerns for those of us on the\nsafety side.\nWhile modern forensic techniques [15] are better at distinguishing the real from the fake, these\ntechniques typically operate asynchronously, making it difficult to protect consumers on phone/video\ncalls. Our reporting of people's ability to detect AI-generated voices is almost certainly an upper\nbound since we drew their attention to a task that may not come naturally when they are on a call\nwith a number or person they recognize.\nOne intervention that may prove useful (albeit not perfect) in mitigating the risk of AI-powered\nscams is the insertion of difficult to remove and easy to identify, imperceptible watermarks into\nAI-generated voices. With the appropriate software at the receiver, AI-generated voices can be\neasily identified. Another intervention, for now at least, is to keep the caller talking for more than\n30 seconds at which point the chance of detecting an AI-generated voice increases."}, {"title": "Data Collection and Availability", "content": "Audio data is available at https://huggingface.co/datasets/faridlab/deepspeak_v1.\nAnonymized speaker and listener data is available at https://doi.org/10.5281/zenodo.\n13654688. This study was approved by the UC Berkeley Committee for Protection of Human\nSubjects (2023-09-16711). Participants provided informed consent prior to participation; data\ncollection was performed in accordance with relevant guidelines and regulations."}]}