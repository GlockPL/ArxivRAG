{"title": "PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models", "authors": ["Kunquan Deng", "Zeyu Huang", "Chen Li", "Chenghua Lin", "Min Gao", "Wenge Rong"], "abstract": "Large Language Models (LLMs) excel in fluency but risk producing inaccurate content, called \"hallucinations.\" This paper outlines a standardized process for categorizing fine-grained hallucination types (Mishra et al., 2024) and proposes an innovative framework-the Progressive Fine-grained Model Editor (PFME)\u2014specifically designed to detect and correct fine-grained hallucinations in LLMs. PFME consists of two collaborative modules: the Real-time Fact Retrieval Module and the Fine-grained Hallucination Detection and Editing Module. The former identifies key entities in the document and retrieves the latest factual evidence from credible sources. The latter further segments the document into sentence-level text and, based on relevant evidence and previously edited context, identifies, locates, and edits each sentence's hallucination type. Experimental results on FavaBench and FActScore demonstrate that PFME outperforms existing methods in fine-grained hallucination detection tasks. Particularly, when using the Llama3-8B-Instruct model, PFME's performance in fine-grained hallucination detection with external knowledge assistance improves by 8.7 percentage points (pp) compared to ChatGPT. In editing tasks, PFME further enhances the FActScore of FActScore-Alpaca13B and FActScore-ChatGPT datasets, increasing by 16.2pp and 4.6pp, respectively.", "sections": [{"title": "1 Introduction", "content": "LLMs have demonstrated an unprecedented level of fluency and natural dialogue capabilities. Despite the significant advancements, these models occasionally generate specious outputs, such as content that does not match the user's input(Adlakha et al., 2024), deviating from the previously generated context (Liu et al., 2022), or directly contradicting factual knowledge (Min et al., 2023).\nIn traditional hallucination detection tasks, which are often domain-specific (Devaraj et al., 2022; Pagnoni et al., 2021; Dziri et al., 2022), it is usually assumed that a reference data source exists, and any deviations from the original text will be considered hallucinations (Pagnoni et al., 2021). For example, in summarization tasks, any inconsistency between the summary and the document information, or the generation of unverifiable extra information such as phone numbers or addresses, is considered hallucinated. However, for the LLMs chatbot, the data source can be considered as all the world's knowledge (Li et al., 2021). At the same time, the tolerance of the chatbot is relatively high, and it focuses more on user engagement rather than faithfulness (Ji et al., 2023). User engagement mainly depends on factualness. When the additional information generated by the LLMs is irrelevant to the user's input but is factual, this information may still be useful. Therefore, for LLMs, the types of hallucinations that require special attention are those based on factual knowledge of the world instead of detecting faithfulness errors (Zhang et al., 2023; Huang et al., 2023).\nThe handling of hallucinations in LLMs mainly involves two tasks: detection and editing. Existing hallucination detection methods tend to make binary judgments on factual errors in sentences output by large models, simply classifying them as \"factual\" or \"non-factual\" (Min et al., 2023; Gao et al., 2023). To overcome the limitations of traditional binary classification of hallucinations, Mishra et al. (Mishra et al., 2024) proposed a fine-grained hallucination classification, but there is a lack of standardized judgment processes.\nDue to the high cost of training and fine-tuning LLMs, model editing methods have been developed to edit and correct LLM outputs through low-cost methods. Based on the position of editing, existing model editing methods could be divided into three types: (1) In-context learning (ICL) in-"}, {"title": "2 Related Work", "content": "Classification of Hallucinations Hallucination classification in LLMs lacks a unified standard, with different studies proposing varied categorizations. Zhang classify hallucinations into input, context, and fact conflict types (Zhang et al., 2023), while Huang distinguish between factual and fidelity hallucinations (Huang et al., 2023). Despite this diversity, there's a consensus that deviations from the original text meaning or objective facts constitute hallucinations. In open chatbot systems, where models rely on world knowledge (Li et al., 2021; Devaraj et al., 2022), tolerance for fidelity errors is higher, but factual errors are strongly discouraged due to their impact on user engagement and information accuracy (Pagnoni et al., 2021). Hence, LLMs must prioritize detecting factual hallucinations to maintain chatbot effectiveness.\nThus, in studying hallucinations in LLMs, the focus should primarily be on factual hallucinations. However, traditional research has mainly focused on the fidelity of generated text to the source text (Ji et al., 2023). Addressing this, Mishra (Mishra et al., 2024) proposed a classification of fine-grained hallucinations, further subdividing hallucinations into six fine-grained types. We expanded upon this fine-grained hallucination classification work and established standardized judgment criteria for computerized assessment.\nModel Editing Methods In model editing, two main strategies stand out. The first modifies model parameters directly, facing difficulties in sequential tasks (Meng et al., 2022a; Mitchell et al., 2021; Meng et al., 2022b; De Cao et al., 2021; Ha et al., 2016). The second keeps parameters intact, storing edits in memory for later retrieval (Mitchell et al., 2022; Huang et al., 2022; Wang et al., 2024; Zheng et al., 2023). Depending on the objective, these edits may involve input, structural components, or output adjustments. While existing methods often excel in specific tasks, they struggle with real-world scenarios. Therefore, (Mishra et al., 2024)'s fine-grained hallucination detection and editing task presents greater challenges. This task requires editing models to locate and identify hallucinations in the output of LLMs within complex contextual environments and to edit them effectively. Our refinement in sentence-level processing enriches contextual learning, thereby aiding in this task."}, {"title": "3 Definition and Judgment Criteria of Fine-grained Hallucinations", "content": "Mishra (Mishra et al., 2024) classifies fine-grained hallucinations into 6 types: Entity, Relation, Contradictory, Invented, Subjective, Unverifiable. This classification aids in the precise identification and correction of hallucinations in LLM outputs, improving model accuracy and text quality. However, their classification is subjective and lacks standardized criteria, especially for Invented, Subjective, and Unverifiable types, and lacks linguistic support. This section redefines judgement criteria of these hallucination types to clarify their concepts\nWe first coarsely divide the hallucinations of LLMs into two types:\n(1) Verifiable Error: Statements that are directly contradicted by factual evidence, meaning the ground truth can be determined.\n(2) Unverifiable Info: Statements that cannot be directly supported or refuted by factual evidence.\nSecondly, hallucinations can be classified based on their editability: those that can be corrected through editing tasks and those that cannot:\n(1) Modifiable Error Verifiable statements that can usually be corrected by modifying a small part of the sentence. These are typically errors at the phrase level or below.\n(2) Non-modifiable Error Statements that are either unverifiable or, if verifiable, contradict factual evidence. Such errors occur in the premise of the sentence rather than in the sentence itself. Therefore, they cannot be corrected by simply modifying a small part of the sentence and are typically considered errors at the sentence level.\nBase on these classification criteria, we redefine the 6 fine-grained hallucination types as Table 1."}, {"title": "4 Progressive Fine-grained Model Editor (PFME)", "content": "PFME framework comprises two modules: the retrieval module and the detection and editing module. For the document to be edited: D = {$s_1, s_2, ..., s_n$}, the retrieval module will retrieve text chunks EV = {$ev_1, ev_2, ..., ev_k$} as evidence, and then the detection and editing module will determine the specific hallucination type $err\\_type_i$ of each sentence $s_i$ through a decision tree.\nThe PFME's innovative approach simplifies complex multi-task problems by breaking them down into more manageable, independent sub-tasks. Each sub-task is designed to detect and correct specific types of hallucinations. This strategy reduces reliance on the model's context learning capabilities and allows for the development of specialized detection and editing techniques tailored to various hallucination types. Furthermore, the PFME framework features a modular design and an ICL strategy, enhancing its adaptability and scalability for evolving tasks."}, {"title": "4.1 Real-time Fact Retrieval module", "content": "The retrieval module of PFME consists of two phases: Recall and Ranking.\nThe Recall phase includes the following steps: (1) Use a LLM for named entity recognition (NER) task on the text scheduled for editing, identifying key entities. To minimize ambiguity, it's crucial to append a brief definition after identifying each key entity. (2) Utilize the identified key entities to retrieve relevant titles from the MediaWiki search engine, accessing the core content of relevant Wikipedia articles. Additionally, extract infobox data associated with the entities to enhance the informational breadth of the retrieved evidence text. During this phase, structured infobox data is converted into declarative statements to streamline"}, {"title": "4.2 Hallucinations Detection and Edit Module", "content": "In this module, PFME first employs InstructGPT (gpt-3.5-turbo-instruct) to break down the passage into individual sentences (including clauses), establishing a basis for detailed processing. It then proceeds with a two-stage retrieval process for each sentence, initially identifying the top k relevant evidence chunks and integrating edited context from the text repository as trustworthy factual evidence.\nFollowing this, PFME enters a decision-making phase to assess the accuracy of each sentence. If evidence suggests a discrepancy with the facts, PFME identifies the sentence as containing a verifiable error. It determines whether the entire sentence or parts of it conflict with the evidence. If the entire sentence is incorrect, it's flagged as a Contradictory statement. If only parts are wrong, the sentence is directed to the editable branch.\nWithin editable branch, PFME utilizes factual evidence to modify and rectify sentences to ensure their accuracy. It highlights the erroneous parts of the original sentence, proposes corrective content, and identifies error types. If the error stems from entity inaccuracies, an Entity tag is appended to that portion; if it arises from semantic relationship errors such as verbs, pronouns, tense, etc., a Relation tag is added to that segment. Multiple Entity and Relation tags can coexist within the same sentence.\nFor sentences lacking evidence-based support, PFME categorizes them as unverifiable. It further distinguishes Subjective statements, Invented concepts, and other Unverifiable claims. Subjective statements are identified using subjective adjectives like \"terrible\" or \"best\". Invented concepts refer to entities or ideas that don't exist or concepts contradicting common sense and knowledge without evidence. Statements aligning with established knowledge but lacking direct evidence to categorize them as Unverifiable.\nFor sentences verifiably correct, PFME categorizes them as such and returns them without edits.\nUltimately, PFME applies appropriate editing strategies to all categorized sentences, such as conducting local edits for Entity and Relation, directly removing sentences flagged as Contradictory and Invented, highlighting warnings for Subjective and Unverifiable; and storing revised sentences in the text repository for subsequent assessment."}, {"title": "5 Experiment", "content": "Models For Edit Model, we leverage two prominent edit models, namely ChatGPT (version gpt-3.5-turbo-0125) (OpenAI, 2024) and Llama3 (Meta-Llama3-8B-Instruct) (AI@Meta, 2024). ChatGPT is renowned for its conversational abilities and text generation, while Llama3 is selected for its instructive capabilities, which are beneficial for fine-grained editing tasks. For Retrieval Model, we use gte-large-en-v1.5 (Li et al., 2023) for the retrieval of evidence, leveraging its capability to process long-form text and extract relevant contextual information from vast datasets.\nBenchmark and metrics For Detection Task, we utilize a subset of 500 examples from the FAVA training dataset. The rationale behind this selection is the absence of a readily available Benchmark file from FAVA. The performance is gauged using the following metrics: (1) Individual hallucination F1-score: Assesses the model's ability to correctly identify and classify individual hallucination types. (2) Overall Accuracy (OA) F1-score: Reflects the model's overall performance across all hallucination types, considering precision and recall weighted by category proportions. (3) Binary Prediction (Bi) F1-score: Measures the model's proficiency in distinguishing between fact and hallucination.\nFor Edit Task: we use the original biography generation task proposed with FActScore (Min et al., 2023). These datasets serve to verify the efficacy of PFME in enhancing the factuality of text post-detection and editing processes. The primary metric for this phase is FActScore: A metric designed to evaluate the factual accuracy of generated text. This model-based metric prompts ChatGPT and InstructGPT (gpt-3.5-turbo-Instruct) to decompose a response into a set of atomic facts and verify factuality for each using passages from a designated Wikipedia article. Since FActScore evaluates the factuality of the text, both Unverifiable and Subjective are deemed non-factual in the FActScore assessment process. Therefore, for the FActScore task, texts labeled as Unverifiable and Subjective will be deleted along with those labeled as Contradictory and Invented.\nBaseline Given the novelty of this field and the lack of relevant baselines, we mainly selected the most pertinent research (Mishra et al., 2024) as a baseline to compare with our method. Furthermore,"}, {"title": "5.1 Main Results", "content": "Detection Task As shown in Table 2, the PFME model achieved the best overall performance metrics OA (Overall Accuracy) and Bi (Bivariate classification for factual errors) across all settings, significantly outperforming existing methods.\nComparison shown in Table 3 indicates: (1) Different Methods: PFME@Llama3 outperformed FavaP@Llama3 with OA and Bi improvements of 10.0 and 17.9 percentage points (pp), respectively, corresponding to enhancements of 47.8% and 34.6%. (2) Different Methods and Different Editing Models: PFME@Llama3 achieved OA and Bi improvements of 8.7 pp and 12.9 pp over FavaP@ChatGPT, corresponding to enhancements of 39.2% and 22.8%. (3) With/Without evidence setting: The OA and Bi scores for FavaP@ChatGPT and FavaP@Llama3 showed little difference before and after introducing evidence, with potential performance declines. However, PFME@Llama3@5 showed OA and Bi improvements of 15.3 pp and 15.6 pp over PFME@Llama3@0, corresponding to enhancements of 98.1% and 28.9%.\nFurthermore, we delve into the implications of these findings: (1) Model and method comparison: The PFME method based on Llama3 significantly outperformed FavaP@ChatGPT and FavaP@Llama3 in both OA and Bi metrics. This indicates the superior capability of the PFME method in handling evidence and error detection. (2) Impact of the amount of evidence: Without evidence, PFME's performance was subpar as it is designed specifically for scenarios with factual evidence. However, even with the introduction of a single piece of evidence, PFME's performance far exceeded that of FavaP. This demonstrates PFME's high dependency on evidence, with significant performance boosts when evidence is present. (3) Influence of the editing model: FavaP@Llama3 performed worse than FavaP@ChatGPT, possibly due to Llama3's slightly inferior performance in specific editing tasks compared to ChatGPT. However, when combined with the PFME method, Llama3 exhibited significant advantages, likely due to the PFME method's ability to better leverage Llama3's model structure and capabilities.\nEdit Task The experimental results in Table 4 show the PFME editing method achieved the highest performance scores with 5 evidence counts. Applying PFME to the original text (No Edit) increased the FActScore by 3.7 pp, marking a 4.9% improvement. PFME also showed notable enhancement in factuality over No Edit with 1 evidence count. In contrast, the baseline method did not demonstrate significant improvement across different evidence counts compared to No Edit and, in some instances, led to a decline in FActScore.\nIn addition, we have recorded the processing time of the detection task in Table 5. It is noteworthy that the quantity of evidence doesn't impact operational speed in the code implementation. The results illustrate that even though the parameter scale of PFME@Llama3 is smaller and computational power is limited, it exhibits the best inference performance and highest operational efficiency across all settings. Furthermore:\n(1) For the FavaP setting, Llama3 is slower than ChatGPT because despite Llama3 having a smaller parameter scale, it operates only on a single A100 40GB GPU, thus its computational power is limited. Therefore, under the same settings, the single A100 40GB GPU running Llama3 is expected to be slower than ChatGPT in terms of invocation speed. (2) Similarly, using Llama3 as the editing model for both FavaP and PFME, the"}, {"title": "6 Analysis", "content": "To thoroughly explore how the quantity of evidence influences PFME performance, we conduct a series of ablation experiments. These experiments are designed to assess the impact of varying amounts of external knowledge evidence on PFME's effectiveness in fine-grained hallucination classification and editing tasks."}, {"title": "6.1 Detection Task Ablation: Evidence Num", "content": "In the fine-grained hallucination detection task, ablation experiments show that incorporating external knowledge evidence positively affects PFME performance. As the number of evidence chunks increases from 1 to 5 (PFME@1 to PFME@5), overall performance metrics OA and Bi enhance, as shown in Table 9. Visual analysis in Figure 3 (the \"ret\" line) indicates that OA and Bi metrics initially rise but then decline as the evidence quantity increases from PFME@1 to PFME@10. The PFME@5 setup achieves the best F1 score.\nNotably, unverifiable types of hallucinations, such as Invented, Subjective, and Unverifiable, exhibit trends similar to overall metrics (OA and Bi). This similarity arises because improved identification of verifiable hallucinations reduces the likelihood of misclassifying them as unverifiable types, thereby indirectly enhancing the accuracy of classifying unverifiable hallucinations. In summary, the results emphasize the significance of the quantity of external knowledge evidence for enhancing PFME performance and highlight the importance of fac-"}, {"title": "6.2 Edit Task Ablation: Dataset", "content": "In the FActScore dataset, the ChatGPT dataset initially received a high score before any edits, as shown in Table 6. As a result, PFME's improvements on the ChatGPT dataset were minimal, with the highest score increasing by just 4.6 pp and factuality improving by 6.1% compared to No Edit. To evaluate PFME's reliability and generalization ability in editing tasks, we analyze the Alpaca 13B dataset from FActScore. Like the ChatGPT dataset, the Alpaca 13B dataset consists of biographies generated by the Alpaca 13B for 500 individuals.\nIn our ablation study, we test PFME@1 to PFME@10 to assess the impact of using different numbers of evidence chunks on editing the Alpaca 13B dataset. We also compare its performance to PFME's performance on the ChatGPT dataset using the same number of evidence chunks. The detailed results are presented in Table 6, showing that PFME@4 achieves the highest FActScore on the Alpaca 13B dataset, with the score increasing by 16.2 pp, and improving factuality by 32.7% from the original 49.5. Specifically, PFME performs best using 4 evidence chunks on the Alpaca 13B dataset and 7 chunks of evidence on the ChatGPT dataset. This suggests an optimal balance between evidence richness and redundancy. Since this balance varies across different datasets, future research should explore mechanisms for selecting evidence based on quality, diversity, and relevance to determine the optimal number of evidence chunks universally."}, {"title": "6.3 More Ablation Experiments", "content": "We conduct additional ablation experiments as detailed in the Appendix section.\nIn Appendix A, we perform ablation experiments to evaluate how various retrieval methods affect hallucination detection and editing. We find that ranking candidate evidence with retrieval model embedding similarity provides a more consistent"}, {"title": "7 Conclusion", "content": "We introduce the Progressive Fine-grained Model Editor (PFME), a framework designed to detect and correct fine-grained hallucinations in large language models. PFME decomposes complex tasks into manageable sub-tasks and uses specialized prompts for various hallucination types, enhancing adaptability, scalability, and readability. Our experiments show PFME outperforms existing methods in detection and editing, particularly in Overall"}, {"title": "8 Limitations", "content": "The PFME framework has yielded positive results but has also shown some limitations. Firstly, the editing module uses hard prompts, which may limit its performance improvement potential. In the future, we plan to optimize it using advanced techniques such as fine-tuning small models or P-tuning. Lastly, the current classification method is still in its early stages and requires further research. We aim to incorporate more linguistic knowledge to enhance the comprehensiveness and clarity of the classification."}, {"title": "9 Ethics Statements", "content": "Our research focuses on utilizing a fine-grained hallucination taxonomy to identify and correct hallucinations in text. However, experiments have revealed that the PFME framework may still miss or mislabel hallucinations generated by LLMs.\nWe assessed our model's detection capabilities using a ChatGPT-generated, privacy-compliant training set by Mishra (Mishra et al., 2024). For editing performance, we utilized the open-source FActScore tool (Min et al., 2023), which relies on Wikipedia-based, non-intrusive datasets.\nAI Assistant Statement: As the authors are not native English speakers, we have utilized ChatGPT to check grammar and spelling errors and to refine the original expressions, which is purely in assistance with the language of the paper."}, {"title": "A Detection Task Ablation: Similarity Ranking Method", "content": "The PFME method optimizes the editing process in evidence retrieval through two core steps. Firstly, the method utilizes a retrieval model to calculate the cosine similarity between the sentence to be edited and all evidence texts, selecting the top 10 most relevant segments as candidate evidence. Secondly, PFME further refines this selection by ranking the candidate evidence to better match the sentence to be edited.\nWe conduct ablation experiments to assess the impact of different retrieval methods on hallucination detection and editing. We compare four similarity ranking methods:\n(1) Retrieval Similarity (ret): Cosine similarity is calculated using embeddings output by the retrieval model. (2) SpaCy Similarity (nlp): Integrates entity matching, extracting entities and calculating their cosine similarity using SpaCy. (3) Fu-"}, {"title": "A.1 Evidence Quantity and Performance Balance", "content": "(1) Singular Performance: At an evidence count of 5, the ret method performs best in terms of the OA metric across all settings; however, when the evidence count increases to 6, the nlp method achieves the highest Bi metric performance across all settings. (2) Comprehensive Performance: Within the evidence count range of 2 to 5, the ret method exhibits the most excellent performance in the overall accuracy (OA) and binary prediction accuracy (Bi) composite evaluations. At evidence counts of 1 or 10, the fus method performs best in the composite evaluation of OA and Bi; otherwise, the nlp method demonstrates the best overall performance.\nIn summary, the ret ranking method can more stably achieve better performance for further sorting of candidate evidence. Additionally, there may be a point in the number of evidence where there's enough information for editing without causing redundancy or conflicts. Since this balance point differs between the two datasets, future research could analyze evidence selection mechanisms, evidence quality, diversity, and relevance to determine if there exists a universally optimal balance point for evidence quantity."}, {"title": "A.2 Necessity of Evidence Similarity Ranking Methods", "content": "(1) Impact of Low Evidence Quantity: With a low evidence count, the performance of randomly selected candidate evidence is notably lower compared to results utilizing similarity sorting methods. (2) Performance Convergence at High Evidence Quantity: With a high evidence count, the model's performance tends to converge, primarily due to the task setup: considering a total of 10 candidate evidence texts, when most of them are taken into account, the differences between different similarity calculation methods mainly manifest in the sorting of evidence texts. (3) Performance Decline Due to Excessive Evidence Quantity: Beyond 9 evidence counts, the F1-score of verifiable hallucination types sharply drops as it exceeds the context window length of Llama3 in some subtasks (such as Entity and Relation). Meanwhile, at 5 to 8 evidence counts, OA performance fluctuates with a slow decline, while Bi performance remains stable, indicating information overload diluting effective information within the context.\nIn summary, introducing excessive irrelevant evidence may lead to information overload, dilution of effective information within the context, or exceeding the context window limit, thereby affecting overall performance. Therefore, within a limited context window, effective methods for selecting relevant evidence texts are needed to select the most relevant evidence while avoiding irrelevant evidence. Effective sorting methods can significantly improve model task performance with fewer evidence quantities."}, {"title": "B Detection Task Ablation: Retrieval Level", "content": "Considering that the PFME method processes text at the sentence level granularity, we design ablation experiments to test the impact of retrieval levels based on document granularity versus sentence granularity on experimental results. The experimental setup is as follows:\n(1) Sentence-level Retrieval: We employ a re-"}]}