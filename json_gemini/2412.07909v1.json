{"title": "Explaining and Mitigating the Modality Gap in Contrastive Multimodal Learning", "authors": ["Can Yaras", "Siyi Chen", "Peng Wang", "Qing Qu"], "abstract": "Multimodal learning has recently gained significant popularity, demonstrating impressive performance across various zero-shot classification tasks and a range of perceptive and generative applications. Models such as Contrastive Language\u2013Image Pretraining (CLIP) are designed to bridge different modalities, such as images and text, by learning a shared representation space through contrastive learning. Despite their success, the working mechanisms underlying multimodal learning are not yet well understood. Notably, these models often exhibit a modality gap, where different modalities occupy distinct regions within the shared representation space. In this work, we conduct an in-depth analysis of the emergence of modality gap by characterizing the gradient flow learning dynamics. Specifically, we identify the critical roles of mismatched data pairs and a learnable temperature parameter in causing and perpetuating the modality gap during training. Furthermore, our theoretical insights are validated through experiments on practical CLIP models. These findings provide principled guidance for mitigating the modality gap, including strategies such as appropriate temperature scheduling and modality swapping. Additionally, we demonstrate that closing the modality gap leads to improved performance on tasks such as image-text retrieval.", "sections": [{"title": "Introduction", "content": "Recently, significant progress has been made in multimodal learning, particularly in connecting text and image modalities through self-supervised methods that leverage large-scale paired data. These include image-text contrastive learning (Radford et al., 2021; Saharia et al., 2022a; Nichol et al., 2021), image-text matching (Li et al., 2021; Wang et al., 2022a; Li et al., 2022), and masked modeling (Li et al., 2021; Wang et al., 2022b,c). Following pre-training, shared conceptual representations enable a variety of downstream tasks, including text-to-image generation (Ramesh et al., 2021; Saharia et al., 2022a), image captioning (Wang et al., 2022c; Li et al., 2022), and vision-based question answering (Wang et al., 2022a; Dou et al., 2022). Among these, one of the most popular multimodal models is Contrastive Language\u2013Image Pre-training (CLIP) (Radford et al., 2021), which effectively learns visual concepts from language supervision through contrastive learning. CLIP jointly trains a vision model and a language model by embedding a large corpus of image-text pairs into a shared embedding space using self-supervised learning. The training process employs"}, {"title": "Problem Setup", "content": "In this section, we introduce the basic setup of the problem. We consider a set of n paired training samples {(xi, Yi)}=1 \u2286 Rdx \u00d7 Rdy. Here, (xi, yi) denotes a pair of two data points from different modalities (such as image and text) that are considered to be related to each other, e.g., yi is the text caption of image xi. In multimodal learning, we want to align the image embedding hox = fo(xi) and the text embedding of hq,y = g(yi) through training two different deep networks fo : Rdx \u2192 Rd and g\u00f8 : Rdy \u2192 Rd for each i.\nContrastive loss for multimodal learning. Let H\u04e9,x, H\u00f8,y \u2208 Rn\u00d7d collectively denote the 12-normalized embeddings of two different modalities:\nHe,x = [norm(fo(x1))\nnorm(fo(xn))], \u0397 \u03a6\u03a5 = [norm(g(y1))\nnorm(g(yn))],\nwhere the operator norm(z) = z/||z||2 for any z \u2208 Rd. As shown in Radford et al. (2021), we can jointly learn the parameters 0, $ of networks f, g respectively via\nmin l(\u03b2(v) Hex H,Y)   (1)\n\u03b8,\u03c6,\u03bd\nwhere l : Rn\u00d7n \u2192 R is a certain contrastive loss. Here, following the convention in Radford et al. (2021), \u03b2(\u00b7) : R \u2192 R+ is the inverse temperature as a function of some learnable parameter v, i.e., we have \u03b2(v) = 1/\u03c4(\u03bd) = exp(v) for training CLIP models where t is the conventional temperature. Additionally, for ease of exposition, we drop the superscripts and just write Hx, Hy.\nThe contrastive loss l is determined solely by the pairwise (\u03b2-scaled) inner products between modalities. Its objective is to maximize the diagonal entries of \u1e9eHxHy while mini-mizing the off-diagonal elements. For instance, CLIP achieves this using a specific symmetric cross-entropy (CE) loss for l, which we also adopt in this work. To derive this loss, we first define the standard (softmax) CE loss, lCE (m, e(i)), for logits m \u2208 R\u201d for a target one-hot distribution e(i) \u2208 Rn as:\nlCE(m, e(i)) := log(\u2211nj=1 exp(mj)) - mi."}, {"title": "Explaining the Modality Gap", "content": "In this section, we provide our main theoretical results that explain how modality gap emerges and remains throughout training."}, {"title": "Learning Temperature Stabilizes Modality Gap", "content": "As shown in Figure 2, modality gap \u2206 > 2y and temperature \u0442 are highly coupled, implying that learnable temperature plays a crucial role in the rate at which modality gap closes. Based upon the problem setup in Section 2, the following lemma directly reveals this relationship.\nLemma 3.1. Let v(t) and y(t) be solutions to the gradient flow dynamics given in (4). Then we have\nR=\\frac{dy/dt}{d\u00df/dt} = \\frac{2\u03b2(\u03bd)\u03b3}{\u03b2' (\u03bd)2(1 \u2013 \u03b32)}   (5)\nfor all t \u2265 0, where \u03b2' (v) denotes the derivative of \u1e9e with respect to v. Moreover, given \u03b2(v) = exp(v), we have \u03b2' (v) = \u03b2 and the following holds:\n\u03b2/ Yo=\u03b2/ \u03b2\u03bf exp (-1/1272), and R = \u0398(1/\u03b2).   (6)\nThe proof of Lemma 3.1 is provided in Appendix B.2. The lemma relates the rates at which y and \u1e9e change, while R = \u0398(1/\u03b2) implies that the ratio R decays to zero as \u1e9e grows to infinity. This implies that an increasing \u1e9e will dominate the decrease in y, preventing modality gap A from closing (i.e., its lower bound 2y remains positive). This is made precise in the following theorem.\nTheorem 3.2. Based on the problem setup in Section 2, consider the gradient flow dynamics (4) for solving (3). Suppose Z(t) = Hx(t)Hy(t) and the initial temperature \u1e9eo \u2265 log(4(n - 1))/((1 \u2013 8)) with \u03b2(\u03bd) = exp(v), and assume that the margin satisfies a(Z(t)) \u2265 & for all t > 0 for some a > 0. Then modality gap \u2206 satisfies\n\u0394(t) \u03a3\u03a9(1/log(t)^2)   (7)\nfor all t \u2265 0.\nThe proof can be found in Appendix B.3. To elaborate, consider the simplified setting where we replace l in (3) with the scalar function l(m) = exp(-m) mimicking the exponential tail of the cross-entropy loss. The gradient flow in this case is simply given by dy/dt = \u22122\u03b2\u03b3exp(-\u03b2(1 \u2212 y2)), so via the equality (6) we have dy/dt = \u03a9 (-y1/2 exp(-cy-1/2)) = \u03a9 (-y3/2 exp(-c'y-1/2)), for some c' < c. Integrating this equation and applying the inequality \u25b3 > 2\u03b3 yields \u2206(t) \u2265 \u03a9(1/log(t)2).\nRemarks. We discuss Theorem 3.2 in the following:\n\u2022 Slow closure of modality gap. The result in (7) indicates that \u2206, with rate \u03a9(1/log(t)2), approaches zero exceedingly slowly. Consequently, this lower bound implies that closing modality gap would require an impractically long training time. As shown in Figure 4a, this can be verified in practice on CLIP models trained on the MSCOCO dataset. Specifically, we sample 2048 random pairs of data and train the model from scratch for 10000 steps and record modality gap during training. In Figure 4a, we report modality gap from the 100th step when the gap begins to decrease consistently. The modality gap \u2206(t) versus \u22121/log(t)2 exhibits a linear relationship, verifying our result and slow closure."}, {"title": "Mismatched Pairs Enlarge Modality Gap at Early Training Stages", "content": "Second, we show that modality gap can be enlarged at the early stage of training, due to the large amount of mismatched pairs caused by random initialization. This further adds to the difficulty of closing modality gap.\nTheorem 3.3. Suppose the rows of Hx (0), Hy (0) are drawn independently and uniformly from Sd-1 and let a = \u03b2o(1 \u2013 8). Then with probability 1 \u03b4, we have\ndA/dt|t=0 4\u03b2\u03bf\u03b3\u03bf -1/2+2\u03c1   (8)\nwhere e = \u221alog((4n + 1)/\u0431)/(2n) and\n\u03c1 1=(2)() (10-1(a)-), 2=(2)()Ip(a),\nwhere p = (d \u2013 2)/2, \u0393 is the gamma function, and Ip(z) is the modified Bessel function of the first kind.\nThe proof of Theorem 3.3 is given in Appendix B.4. In addition to the parallel modalities assumption, we assume that embeddings are uniformly distributed on the hypersphere. This aligns"}, {"title": "Mitigating the Modality Gap", "content": "The analysis of learning dynamics in Section 3 offers valuable insights into mitigating modality gap. It inspires us to design two types of methods for reducing modality gap: (i) Temperature Control, where we propose new temperature scheduling rules for accelerating the convergence rate of modality gap, and (ii) Modality Swapping, we proposed methods to manually break the parallel constraints of two modalities by swapping them during training. To evaluate these approaches, we train models from scratch on the MSCOCO dataset with different variants of the proposed methods and then measure modality gap and evaluate the performance on downstream tasks.In the following, we introduce the proposed methods in Section 4.1 and discuss the results and implications of reducing modality gap in Section 4.2."}, {"title": "Methods", "content": "We introduce the main idea of each method, and leave details on implementations to Appendix D."}, {"title": "Experimental Results and Implications", "content": "Experimental setup. We train CLIP models from scratch on MSCOCO using the proposed methods and the original CLIP training process as a baseline. After pretraining, we evaluate two key attributes of the shared feature space: (i) modality gap between image and text features and (ii) feature space uniformity. Additionally, we assess the models on four downstream tasks: (i) zero-shot image classification, (ii) linear-probe image classification, (iii) image-text retrieval, and (iv) vision-language question answering (MMVP-VLM Tong et al. (2024)). For each training setting, we independently train three models and report averaged results for attributes and task performance. Detailed experimental setup is provided in Appendix E.\nClosing modality gap is especially helpful for image-text retrievals. We visualize the correlation between image-text retrieval accuracies and modality gap in Figure 6a, and the correlation between image-text retrieval accuracies and uniformity in Figure 6b. As shown in the figure, though these methods are different variants of Control Temperature and Swap Modality, a smaller modality gap clearly leads to a higher retrieval accuracy. This indicates reducing modality gap improves image-text retrieval. In contrast, uniformity does not show a strong correlation with the retrieval accuracy. We include more detailed results for each method variant in Appendix F."}, {"title": "Conclusion", "content": "In this work, we investigated the modality gap in training multimodal models. By analyzing gradient flow learning dynamics, we theoretically characterized how learning temperature and mismatched pairs influence this gap. Based on our analysis, we proposed principled methods to control temperature and swap information between modalities to reduce the gap, which also improves downstream task performance\u2014particularly in retrieval tasks. Our work opens up future directions, such as extending gradient flow analysis to study the difficulty of closing the gap with varying levels of shared information between modalities by modeling data distributions. Additionally, our results can provide insights into finetuning scenarios where domain differences between pretraining and finetuning data need to be considered. Motivated by recent advancements in understanding the compressibility of gradient dynamics through low-dimensional structures in data (Yaras et al., 2023, 2024; Kwon et al., 2024), a promising future direction is to investigate the compressibility of the Riemannian dynamics trajectory in multimodal learning to enhance the efficiency and performance of multimodal models."}, {"title": "Acknowledgment", "content": "We acknowledge support from NSF CAREER CCF-2143904, NSF IIS 2312842, NSF IIS 2402950, and ONR N00014-22-1-2529. Additionally, we acknowledge fruitful discussions with Yuexiang Zhai (UC Berkeley) and Liyue Shen (UMich)."}, {"title": "Related Works", "content": "Contrastive Learning In the past, contrastive learning has been demonstrated as a powerful tool to learn reasonable representations in a self-supervised way (Chen and He, 2020; Chen et al., 2020; He et al., 2020; Caron et al., 2020). Representative contrastive methods including SimCLR (Chen et al., 2020), MOCO (He et al., 2020), and SwAV (Caron et al., 2020). Intuitively, contrastive learning aims to pair together data that contain similar information, and separate pairs that contain distinct information. Such a strategy can effectively extract key features within data that are useful for various downstream tasks. For example, by utilizing contrastive learning in images or videos, we can get image or video encoders that can perform zero-shot image and video retrieval, as well as image classification Chen et al. (2020); He et al. (2020); Caron et al. (2020), action detection, action segmantation, and other tasks (Feichtenhofer et al., 2021; Chen et al., 2024a). Moreover, contrastive learning can be applied to a wide range of data, from language to protein (Fang and Xie, 2020; Heinzinger et al., 2022). Apart from application, other works approach to analyze and understand contrastive learning from the perspective of alignment & uniformity (Wang and Isola, 2020; Fang et al., 2023), specific design choices (Xue et al., 2024; Gupta et al., 2022), and so on (Hua et al., 2021; Arora et al., 2019).\nMultimodal Learning Multimodal learning aims to learn a shared representation space for different modalities such as images and texts, where the learned representation space can express the shared information hidden in different modalities (Jia et al., 2021; Radford et al., 2021; Socher and Fei-Fei, 2010). Under this context, contrastive learning has been proved applicable and powerful to multimodal learning (Radford et al., 2021). The contrastive loss is applied between two modalities (take image and text as an example) such that positive pairs (i.e., image and text pairs that contain similar information) have similar representations but negative pairs (i.e., image and text pairs that contain distinct information) are apart from each other. The pretrained multimodal models excel in various downstream tasks such as linear probe, zero-shot classification, and retrieval (Radford et al., 2021), and are even useful for medical applications (Zhang et al., 2020) and generative models (Esser et al., 2024; Luo et al., 2023; Saharia et al., 2022b). Despite its great power, recently, several works have also identified bias and flaws in the learned multimodal models, such as uni-model bias (Bravo et al., 2023), and failure in understanding detailed information (Tong et al., 2024), which add difficulty to applications such as text-to-image editing (Zhou et al., 2023; Chen et al., 2024b). Therefore, the theoretical understanding of contrastive multimodal learning is an important problem for further solving the aforementioned problems.\nModality Gap Following prior discoveries, existing works delve deeper into the modality gap from both theoretical and empirical perspectives. For instance, Shi et al. (2023) experimentally demonstrates the influence of different initialization and temperature parameters in the modality gap with toy datasets, but stops short of providing theoretical justifications for the emergence of modality gap under various conditions. Schrodi et al. (2024) investigates the role of information imbalance between modalities, suggesting that balancing information between text and image datasets helps close the modality gap. However, their claim is based solely on experiments with toy datasets and lacks a strong theoretical foundation. Instead of focusing on the reasons behind the modality gap, other studies propose practical solutions. For example, Oh et al. (2023) and Fahim et al. (2024) introduce new loss functions for fine-tuning the CLIP model to close the gap, while Eslami and de Melo (2024) encourages different modalities to share portions of their encoders. Although effective, they remain largely experimental and fail to provide a rigorous explanation for"}, {"title": "Proofs", "content": "Preliminaries\nThe gradient of l given in (2) is given by\n\n\nDefine the function ga(Z) := \u2212(Z, \u2207l(aZ)) which can be written as\n\n\n\n\n\nwhich gives the gradient flow dynamics\n\n\nwhere \u03b2' := \u03b2'(\u03bd).\nProof of Lemma 3.1\nProof. From (11), we have\n\n\nwhich gives (5). Now let \u03b2 = exp(v) so \u03b2' = \u03b2. Then we can rewrite and integrate as"}, {"title": "Proof of Theorem 3.2", "content": "Lemma B.1. Let z \u2208 Rn and i \u2208 [n].\n(a) If zi \u2265 maxj zj, then pi(z, a) \u2265 0 for all a \u2265 0.\n(b) For any a > 0, we have\n\u03bc\u2081(z, a) \u2264 \u03bc\u2081(a(e(i) \u2013 1), a)\nfor all a \u2265 log(4n \u2013 4)/x and for all z such that zi > maxj\u2260i zj \u2265 \u03b1.\nProof. We have\n (z, e(i)) = zi > max zj = \u03a3\u03c3\u03b9(az)  zi \u2265 \u03a3\u03c3\u03b9(az)\u03c4\u03b9 = ( (\u0396, \u03c3(az))\nso \u03bc\u2081(z, a) \u2265 0, giving (a).\nTo prove (b), without loss of generality, we can take i = 1 and assume z1 = 0 and zj \u2264 \u2212\u03b1 for j \u2260 1. Define w = z2:n \u2208 Rn-1 and\n\u03c8(\u03c9, \u03b1) :=\n\u03a3j wj exp(awj) / 1 + \u03a3j exp(awj),\nso that \u03bc\u2081 (z, a) = \u2212\u03c8(w, a). The statement is then equivalent to showing \u03c8(w, a) \u2265 \u03c8(\u2212a1, a) for any w such that wj \u2264 \u2212a for all j \u2208 [n \u2013 1]. It suffices to show that for any k, we have d\u03c8/dwk \u2264 0 whenever w \u2220 -a1, i.e., \u03c8 is decreasing in any argument for the region bounded above by -a1. We compute\n\n\nwhere\n\u03b1\u03c8(\u03c9, \u03b1) =\n\u03a3j awj exp(awj)/ 1 + \u03a3j exp(awj) > \u03a3\u03b1w exp(awj) \u2265 \u2212(n - 1)ax exp(-aa)\nprovided that w \u2220 -a1, where the last inequality follows from a \u2265 1/a. Now, for any x \u2265 log(4n-4), we have that (n-1) \u2264 exp(x)(1\u22121/x)by1-1/x > 1/4. Therefore, x = aa > log(4n-4) satisfies\n(n - 1) exp(-aa) \u2264 1-1/aa \u21d2 -(n-1)aa exp(-aa) \u2265 1 \u03b1\u03b1.\nCombining the above inequalities along with 1 + awk \u2264 1 -aa yields \u03b1\u03c8(w,a) \u2265 1 + awk, and thus \u03b8\u03c8/\u03b8\u03c9\u03ba \u2264 0, completing the proof.\nProof of Theorem 3.2. First, we note by (11) that \u1e9e(t) is increasing in t and y(t) is decreasing in t due to the fact that 98(1\u2212y2)(Z) \u2265 0 via perfect mismatch and (a) in Lemma B.1. Now, we can substitute the expression for \u1e9e in (6) into the dynamics of y in (11) giving"}, {"title": "Proof of Theorem 3.3", "content": "f Theorem 3.3. Let Z = Hx(0) H (0), so by Lemma B.2 we have Zij ~ fz where fz is given by (12). We also note that any given row, column, or diagonal of Z forms a collection of independent variables (but not the entire matrix).\nLet z \u2208 [-1,1]n be a given row, column, or the diagonal of Z. From Lemma B.3 we have E[z] = 0, and let \u00a71 = E[zexp(az)] and \u00a72 = E[exp(az)]. By zi \u2208 [-1,1], zi exp(azi) \u2208 [-ea, ea], and exp(azi) \u2208 [e-a, ea] for all i \u2208 [n], applying one-sided Hoeffding inequalities gives"}, {"title": "Alternate Temperature Schemes", "content": "Consider the simplified setting l(m) = exp(-m) discussed in Section 3."}, {"title": "Methods", "content": "In this section, we introduce the proposed methods in greater details, and present representative results in Table 2 as examples. We include more ablation studies in Appendix F."}, {"title": "Temperature Control", "content": "Temperature Scheduling (TS). First, we propose to not learn the temperature as (Radford et al., 2021). Instead, we schedule the increase of temperature according to the training epochs linearly. With a larger temperature in the late stage, we can make more progress on reducing the modality gap. As shown in Table 2, linearly increasing temperature from 10\u20132 to 5 \u00b7 10\u20132 over the training process reduces the modality gap and leads to better text-image retrieval performance.\nTemperature Reparameterization (TR). Towards the same purpose, TR designs different parameterization \u03c4(\u03bd) from CLIP to slow down the decreasing of \u03c4(\u03bd) when learning v. In comparison with the original CLIP with 1/\u03c4(\u03bd) = exp(v), we propose to use 1/\u03c4(\u03bd) = exp(v/s) with s > 1 being a scalar to reduce the influence of v as it grows. Another way we propose is to design through softplus to achieve the same goal: 1/\u03c4(\u03bd) = log (1 + e\u03bd). We report results using softplus in Table 2.\nSmaller Learning Rate of Temperature (SLRT). To slow down the decrease of \u03c4(\u03bd), SLRT scales down the learning rate of v directly. In Table 2, we report the results from scaling the learning rate of v by 10\u20131. The result shows improved performance with reduced gap. It implies that the original choice of v in CLIP is far from optimal, and can be improved through a careful analysis.\nFixed on Large Temperature (FLT). FLT freezes the temperature instead of learning it. We let \u03c4(\u03bd) range from 10\u20132 to 4 \u00b7 10\u20131 to find temperatures that can shrink the modality gap. The results from Table 2 is obtained with \u03c4(\u03bd) = 4\u00b7 10\u22122."}, {"title": "Swap Modalities", "content": "Hard Swapping Between Modalities (HS). Recall Hx, Hy \u2208 Rn\u00d7d denote the image and text features. HS randomly swaps Hx[i, j] and Hy[i, j] for each (i, j) independently with probability 0.5 to obtain Hx, Hy. Then, the swapped features are input to the loss function for optimizing networks."}]}