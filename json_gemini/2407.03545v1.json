{"title": "On Evaluating Explanation Utility for Human-AI Decision Making in NLP", "authors": ["Fateme Hashemi Chaleshtori", "Atreya Ghosal", "Alexander Gill", "Purbid Bambroo", "Ana Marasovi\u0107"], "abstract": "Is explainability a false promise? This debate has emerged from the insufficient evidence that explanations aid people in situations they are introduced for. More human-centered, application-grounded evaluations of explanations are needed to settle this. Yet, with no established guidelines for such studies in NLP, researchers accustomed to standardized proxy evaluations must discover appropriate measurements, tasks, datasets, and sensible models for human-Al teams in their studies. To help with this, we first review fitting existing metrics. We then establish requirements for datasets to be suitable for application-grounded evaluations. Among over 50 datasets available for explainability research in NLP, we find that 4 meet our criteria. By finetuning Flan-T5-3B, we demonstrate the importance of reassessing the state of the art to form and study human- AI teams. Finally, we present the exemplar studies of human-AI decision-making for one of the identified suitable tasks verifying the correctness of a legal claim given a contract.", "sections": [{"title": "1 Introduction", "content": "Decision makers can make use of imperfect models if they can detect when they are correct. Explanations of individual predictions are proposed to this end as they are expected to reveal useful signals about the model's reasoning process (Jacovi et al., 2021). Before undertaking realistic evaluations involving people, NLP researchers aspired to first implement working methods. Thus, prior NLP explainability work has mostly focused on overcoming technical challenges and used proxy evaluations. Consequently, human-centered evaluations of explanations grounded in real NLP applications are scarce. There is a prevailing perspective that this now needs to change since explain- ability methods passed proof-of-concept tests (see Human-centered Evaluations of Explanations Tutorial). However, given that this is a nascent NLP research space and the notable variation among prior studies (see Table 5), choosing explanation evaluation measurements, tasks, models to explain, baseline conditions, and many other study design choices is not straightforward. This paper aims to alleviate this difficulty by providing guidelines. An existing resource for the development and evaluation of explanations in NLP already includes over 50 datasets. Can these be used for application- grounded evaluations of explanations? To answer this, in \u00a73, we establish criteria to assess each dataset's suitability for computing explanation use- fulness with right measurements overviewed in \u00a72. We discover that 17/53 datasets are apt for studying appropriate reliance and complementary human-AI team performance but only involve low risk. 4/53 additionally involve higher risk and do not have quality concerns. We recommend using these four, as high stakes need explanation aid more. We introduce the final criterion for dataset se- lection based on how the likelihood of hazards, and therefore risk, changes if the performance of a state-of-the-art model peaks or is so low the model cannot be collaborated with. We show that this criterion requires continuous assessment of model performance in a rapidly evolving field like NLP. Finally, we present explanation usefulness stud- ies for a task identified by our meta-analysis: ver- ifying a legal claim given a contract. These serve as exemplars to NLP researchers planning similar studies. We use both the common human-AI setup, where people make all final decisions with AI as- sistance, and an overlooked setup, where people decide only for those referred to them by a deferral model. We isolate the effect of explanations, use strong baseline conditions, deploy multiple cogni- tive forcing functions, integrate real-world situa- tions, and implement attention checks tailored to the application. Input highlights and influential training examples do not improve human decision making assisted with the model's predictions and"}, {"title": "2 Review of Application-Grounded Explanation Evaluation", "content": "In Table 5 (Appendix), we overview prior human- centered application-grounded evaluations of expla- nations of NLP models (Lai and Tan, 2019; Feng and Boyd-Graber, 2019; Gonz\u00e1lez et al., 2021; Bansal et al., 2021; Schemmer et al., 2023; Mozan- nar et al., 2023; Joshi et al., 2023; Si et al., 2024). There is a notable variation in the choice of mod- els explained, explanations used, evaluation mea- surements, baseline conditions, datasets, and out- comes among them. To conclusively establish, or disprove, the value of explanations for human-AI decision-making in NLP, more research is needed together with a more meticulous evaluation proto- col designed to collectively guide us towards set- tling this matter. To this end, we start with an overview of explanation usefulness measurements. Taxonomy of explanation evaluation. Doshi- Velez and Kim (2017) categorize evaluations of explanations as: (1) proxy (no humans, proxy tasks; e.g., the proportion of all features selected as important), (2) human-grounded (with humans, but simplified tasks; e.g., simulatability), or (3) application-grounded (with humans, realistic tasks; e.g., human-AI decision making). Human-AI decision-making, which is the focus of this paper, is one of the six usage contexts within explainable AI (Liao et al., 2022). Forward and counterfac- tual simulatability, that are common in NLP (Xie et al., 2022; Arora et al., 2022), are human- but not application-grounded: Bu\u00e7inca et al. (2020) show that explanations affect simulatability and human-AI decision-making differently. Reliance definitions. It is often asserted that ex- planations can deter people from rejecting correct predictions, i.e., underreliance. This expectation stems from assuming that the model is correct for the right reasons, and explanations are anticipated to unveil this. Explanations could also aid people in rejecting incorrect predictions, thereby counter- ing overreliance. This becomes possible when explanations present information that appears illog- ical, self-contradictory, or inconsistent with what the person already knows. The ultimate goal is"}, {"title": "Measuring reliance.", "content": "Researchers rarely ask peo- ple to accept/reject predictions to measure reliance, except Gonz\u00e1lez et al. (2021). Instead, people are often shown model predictions and asked to make the final decision. Overreliance is measured by how often the final decision agrees with the model's when it is wrong (Vasconcelos et al., 2023). A pos- sible confounder is that people might make the same wrong decisions as the model, not because they are blindly following it, but because they gen- uinely find the same wrong answer to be correct. Schemmer et al. (2023) thus propose that partici- pants first make a guess unassisted, then reevaluate upon viewing the model's prediction. They pro- pose reporting the fraction of times a person (1) flips their initial, wrong judgment after seeing a correct model prediction, and (2) sticks with their initial, correct judgment after seeing a wrong model prediction. Wang and Yin (2021) aim for a similar procedure but allow scrolling to the model predic- tion while making the first guess, potentially influ- encing the standalone guess. Joshi et al. (2023)'s approach is similar to (1), but a person needs to flip their initial, wrong answer to the correct one upon seeing AI's explanation but not its prediction. This approach to measuring reliance where peo- ple make all final decisions with AI assistance is also how complementary human-AI team per- formance is typically measured. Human-AI teams should surpass the accuracy of both the AI alone and the human alone (Bansal et al., 2021), and ex- planations could provide a boost. For this to even be possible, the performance of a state-of-the-art model or time-constrained people alone should not already peak. It should not be too low either, be-"}, {"title": "Deferral.", "content": "Integrating a deferral model (Dvi- jotham et al., 2023), Mp, that decides whether an instance can be correctly processed by a predic- tion model, Mp, presents an alternative to having people make all final decisions with AI in the loop. Explanation usefulness has not been studied in this human-AI team setup. Feeding explanations to MD could enhance its correctness, if explanations indeed indicate when Mp is correct or wrong, as commonly assumed. They could also assist the hu- man reviewer that gets a small fraction of instances deemed hard for Mp. Knowing that a highly ac- curate model likely made a mistake on a given example, along with its reasoning for it, can nudge the reviewer to consider why the model erred and preempt them from making the same mistake."}, {"title": "3 Analysis of Task Appropriateness", "content": "In this section, we present criteria that can be used to determine the suitability of tasks for application- grounded human evaluations of explanations (\u00a73.1) and analyze 53 existing datasets introduced for de- veloping and evaluating explanations in NLP (\u00a73.2). We refer to a task as its realization in the data."}, {"title": "3.1 Task Criteria", "content": "We determine that the following criteria must be fulfilled to ensure that evaluations are rooted in genuine human-AI interactions: C1: The task has a meaningful connection to a real-world application, involving people who seek model outputs and act on them. C2: The dataset inputs must be realistic. C3: Task instances require a notable effort from people, or people are bad at them. For example, COMMONSENSEQA (Talmor et al., 2019) has no associated application as people do not need answers to questions such as \u201cAt the end of your meal what will a waiter do? serve food, eat, set table, serve meal, or present bill\u201d. PUBHEALTH (Kotonya and Toni, 2020) has actionable outputs but lacks realistic task inputs. The task is to verify a claim based on a professional fact-checking report on the same claim that won't be available for an unverified claim post-deployment. Finally, while there might be a use for sentiment classification of laptop reviews (Pontiki et al., 2014), their brief average length of only 15 words allows people to correctly and confidently gauge sentiment without assistance. Hence, concerns about under- or over- reliance do not arise in this context because people never end up really relying on anything. These three criteria are sufficient if the sole focus is on reliance/complementary performance. How- ever, the definition of human trust in AI (Jacovi et al., 2021) implies that trust inherently involves risk, as one cannot accept vulnerability when none exists. Thus, studying human trust in AI demands an extra condition: C4: There is some undesirable event that can possibly (but not certainly) occur when collaborating with models for the task. Although risk is not pivotal to defining sound studies of reliance and human-AI teams, we urge giving precedence to tasks involving higher risk because under- and overreliance have more pro- nounced consequences for them. It is more valu- able to develop explanations that boost appropriate reliance for them, and this is how the need for ex- planations is often motivated."}, {"title": "3.2 Categorization of ExNLP Tasks", "content": "We analyze all datasets that are reported on the website that collects datasets for explainable NLP (Wiegreffe and Marasovi\u0107, 2021) according to how they satisfy the criteria in \u00a73.1. In Appendix F, we report details of our decisions for each task and provide an overview in Table 1. We use \\u2713 if a benchmark criterion is satisfied, and \\u00d7 otherwise. A suitable dataset for application-grounded evalu- ations of explanations should have an application (c1) and realistic inputs (c2) as well as either require notable effort, or be a difficult task for people (c3), and ideally more than low levels of risk (c4). We"}, {"title": "3.3 Task Checks with Model Performance", "content": "The final check for a dataset's suitability is based on a model performance: C5: The model performance should be high enough to warrant collaboration, but not so high that it can operate effectively on its own without human oversight. If a model rarely makes mistakes, the likelihood and, thus, risk of hazards are typically low. These are tasks we recommend deprioritizing. Fok and Weld (2023) argue that using the predictions is viable in this case. However, do not choose a model with room for improvement when a better-performing one is available and resource- appropriate. Finetuning large language models (LLMs) is an effective method for specializing a model to a task. We do so for each task that fulfills all the criteria in \u00a73.1 and quality checks. We use Flan-T5-3B (Wei et al., 2022) due to its size and versatility stemming from instruction finetuning with data of 1.8K tasks. Details and examples of task inputs to the model (Tables 11\u201314) are in Appendix C. Tables 2 and 7 (Appendix) stress the importance of reassessing baselines. We obtain a 24.5 point improvement in the average contradiction and en- tailment F1 scores on Contract-NLI (Table 2), and a 22.8 macro-F1 point increase on SciFact-Open (Ta- ble 7). Without reassessing performance on these datasets, we would not realize that it now makes sense to team up people with models for these tasks. Moreover, we find that the baselines for ILDC and SciFact-Open \u2014 the two datasets with the reported human performance \u2013 have not reached peak per- formance. Thus, human-AI teams might provide benefits over using AI alone. The performance of EvidenceInference-v2 with retrieval remains low, and studying human-AI teams on this task is not justified without a stronger model in the loop."}, {"title": "4 Study I: People Make All Decisions", "content": "In \u00a72, we overview two strategies for human-AI decision-making: (1) people make all final deci- sions with Al assistance, and (2) a deferral model refers only a fraction to people. We aim to pro- vide an exemplar for user study design and ini- tial insights into the usefulness of explanations in improving human-AI decision making in both of these setups for one of the tasks identified in our meta-analysis: verifying claims based on the Con- tractNLI Non-Disclosure Agreements (NDAs). In this section, we use (1), and in \u00a75, we look at (2)."}, {"title": "4.1 Study Design", "content": "We overview various design choices we consider that we recommend integrating in future studies. To isolate the effects of explanations, in the first step, a participant may reveal the prediction and make their first guess. Only then we provide the explanation and ask for their final decision. To"}, {"title": "4.2 Task Conditions", "content": "In each condition, participants are given 6 in- stances; for each, they first answer an attention- check MCQ within 3 minutes. If time runs out, they are moved to the next instance. Following the MCQ, they should evaluate a statement based on the NDA in 7 minutes (Fig. 9) and then report their self-confidence (Fig. 10). In the final two conditions, after participants make the first guess, they are shown the model explanation and asked to guess again. We test the following five conditions: \u2022 (C1; Baseline) A delayed option to reveal a model prediction and its calibrated confidence. \u2022 (C2) C1 information with the don't know option and priming with roles and records. \u2022 (C3) C2 information with providing feedback. \u2022 (C4) C2 information, then input highlights in the next step, and finally feedback. \u2022 (C5) C2 information, followed by influential train examples, and at the end, feedback. Figures 1\u20133 show the instructions. In C4, partici- pants may see the NDA with top 5%, 10%, or 20% of the important words highlighted (Fig. 12), and in C5, the top 3 most influential labeled train examples with Input Gradient highlights incorporated - participants should not have to fully review three different NDAs to quickly verify if the AI's guess is correct. Appendix D.5 provides more info. For each study, we aim for 80 examples per each of the three labels. We collect 1108 annotations across all conditions. Participant recruitment is done through Prolific. More on participants in D.6."}, {"title": "4.3 Results", "content": "We provide our findings in Table 3. Across all con- ditions, time-constrained humans (H) collaborating with the model perform at least 9.5 F1 score points worse than the model alone (M). This underscores"}, {"title": "5 Study II: People Decide Only for Deferred Instances", "content": "In this section, we use the setup for human-AI deci- sion making that defers a fraction of all instances to experts. We investigate explanation usefulness for finetuning/prompting LLMs to defer, and to human decision makers."}, {"title": "5.1 Usefulness to Deferral Models", "content": "We finetune Llama-2-13B-Chat and gpt-3.5- -turbo-1106, and prompt gpt-40. We use 1.4K"}, {"title": "5.2 Usefulness to Experts", "content": "We still aim to provide initial insights into whether explanations for likely-wrong predictions help de- cision makers, as we hypothesize in \u00a72. Without workable deferral models, we do not study this in a realistic setup with deferral mistakes; all par- ticipants get instances that the model mishandles. Given the lack of usefulness of explanations so far, we opt for a small-scale study and include a free- form question about how explanations are helpful, if at all. We use the setups in C4 (w/ highlights) and C5 (w/ highlighted influential examples) in \u00a74, but modify the instructions to warn participants that AI most likely mislabeled examples they will review (see Fig. 5). We end with 23 participants for the former and 20 for the latter, each annotating 2 in- stances. The final number of annotated instances is 73 because some annotators reach the time limit. Participants, on average, rate the impact of expla- nations on their decision making with 2.56 (slightly to moderately). For 55/73 statements participants explain whether and how explanations are help- ful. We categorize these insights in Table 18 and provide examples. While explanations of a likely- wrong model can encourage different reasoning approaches as we hypothesize, highlights are more often helpful in finding relevant information in an NDA despite wrong predictions. These results suggest that the model finds necessary information, but uses it in a way that leads it to a wrong deci- sion. Participants should be informed about this possibility in future studies. We notice a low F1 score of only 34.2% in this study (Table 20, Appendix), which is a stark con- trast to the previous small-scale study of people operating alone (\u00a74.1). Five participants state they disagree with feedback mentioning the correct label (see Appendix E). Therefore, one author of this pa- per has checked 29 examples for which participants respond about their use of highlights. The gold la- bel is changed if the author's reasoning matches the participant's reasoning, and they disagree with the gold label. This occurs in 11/29 cases, suggest- ing that examples that are challenging to the model might also have noisy labels. Table 17 shows one such sample. Using the revised gold labels, par- ticipants' F1 score increases from 34.9% to 79.4% on this reviewed subset (29 samples), and AI's F1 score from 0% to 27.3%. Future work should"}, {"title": "6 Conclusions", "content": "In this paper, we provide guidelines for specify- ing a sound experimental setup for human-centered application-grounded evaluation of explanations in NLP. The dataset selection criteria we set should be used to determine a dataset's suitability for such evaluations. Future studies should consider 4 datasets we identify by validating over 50 datasets against our criteria, or validate/create new datasets guided by them. We show that the performance of a resource-appropriate state-of-the-art model should be continuously reassessed: it should neither peak nor be low, otherwise teaming the model with peo- ple is unsound. We recommend following our user study design that isolates the effect of explanations, uses strong baseline conditions, deploys multiple cognitive forcing functions, integrates real-world situations, and implements attention checks tai- lored to the application, among other things. We show that the main obstacle to effective human- AI teaming is enabling experts to make accurate and confident decisions without needing to put in considerable effort, in cases where AI cannot. Inte- grating deferral seems to be a promising direction."}, {"title": "7 Limitations", "content": "The dataset selection criteria we set (\u00a73.1) could have been used to validate some promising datasets beyond the resource with existing explainability- research datasets we used. The application of the criteria could also be enhanced. For example, we approximate effort with length, but shorter exam- ples people are capable of solving could also re- quire effort, e.g., certain math problems. On the other hand, finding an answer quickly in a longer task instance might be possible with a keyword search. Future work should continue to improve methods for quantifying human effort and abil- ity. We subjectively determine hazard severity, and sometimes likelihood, which leaves room for dis- agreement. However, acknowledging this, we pro- vide higher risk when less confident, such that a dataset is not unfairly filtered. While newer models than Flan-T5 could have been finetuned in \u00a73.3, and consequently, not all of the 4 datasets might re- main as suitable, the main takeaway of this section remains: reassess the performance of a model you plan to team people with. Finally, our user studies face challenges all user studies do. Specifically, they would be stronger with more participants, ex- amples annotated, explanation types, and datasets evaluated, among other things, which is not possi- ble due to financial restrictions. Despite our best efforts to provide reliable outcomes, as in most studies with human subjects, we cannot guarantee there are no confounders."}]}