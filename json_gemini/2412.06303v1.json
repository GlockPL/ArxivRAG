{"title": "DSAI: Unbiased and Interpretable Latent Feature Extraction for Data-Centric AI", "authors": ["Hyowon Cho", "Soonwon Ka", "Daechul Park", "Jaewook Kang", "Minjoon Seo", "Bokyung Son"], "abstract": "Large language models (LLMs) often struggle to objectively identify latent characteristics in large datasets due to their reliance on pre-trained knowledge rather than actual data patterns. To address this data grounding issue, we propose Data Scientist AI (DSAI), a framework that enables unbiased and interpretable feature extraction through a multi-stage pipeline with quantifiable prominence metrics for evaluating extracted features. On synthetic datasets with known ground-truth features, DSAI demonstrates high recall in identifying expert-defined features while faithfully reflecting the underlying data. Applications on real-world datasets illustrate the framework's practical utility in uncovering meaningful patterns with minimal expert oversight, supporting use cases such as interpretable classification.", "sections": [{"title": "Introduction", "content": "The ability to analyze large-scale datasets is a cornerstone of deriving actionable business insights. Traditionally, this task has been managed by human data scientists, but it faces several key challenges: (1) the large volume of data makes it difficult to review all information comprehensively, (2) human analysis can often include subjective bias, and (3) collaboration with domain experts is often required, leading to high operational costs.\nLarge language models (LLMs) have emerged as powerful tools for identifying patterns within massive datasets, leveraging their ability to process and generate language in context (Touvron et al., 2023; Dubey et al., 2024; Achiam et al., 2023; OpenAI, 2024; Lam et al., 2024; Wan et al., 2024). However, their application to data analysis is limited by critical shortcomings. First, LLMs often struggle to identify latent characteristic patterns in large datasets due to inherent data grounding issues, where outputs rely on pre-trained knowledge rather than the specific nuances of the input data (Kossen et al., 2024; Kenthapadi et al., 2024; Wu et al., 2024). Second, the difficulty in verifying LLM-generated responses and the lack of quantitative evaluation methods require expert oversight, which can be prohibitively expensive at scale.\nTo address these limitations, we propose Data Scientist AI (DSAI), a framework that systematically applies LLMs to extract and refine latent features from data. Unlike direct feature extraction approaches, DSAI adopts a bottom-up approach, starting with detailed analysis of individual data points, aggregating their characteristics, and deriving actionable features. The process is guided by defined perspectives which provide LLMs with a consistent framework for interpreting data points while minimizing subjective bias.\nThe DSAI pipeline operates in five stages: #1 Perspective Generation identifies data-driven perspectives from a small subset of data. #2 Perspective-Value Matching assigns values to individual data points by evaluating them against these perspectives. #3 Clustering groups values with shared characteristics to reduce redundancy. #4 Verbalization converts extracted features into a compact criterion form. #5 Prominence-based Selection determines which features to use based on a prominence intensity metric that quantifies the discriminative power of each extracted feature. Throughout the process, LLMs remain task-agnostic to minimize bias, ensuring that their outputs are grounded in data rather than domain-specific assumptions.\nTo validate our framework, we demonstrate its effectiveness on datasets specifically curated for our experiments, which include research titles (Wang et al., 2018) and slogans (Jin et al., 2023). We then showcase DSAI's practical value using real-world datasets: news headlines with CTR (Wu et al., 2020), spam detection (Kim, 2016), and Reddit comments with community engagement metrics (Magnan, 2019). These experiments highlight DSAI's ability to provide actionable insights for industries focused on content optimization, user engagement, and moderation.\nOur main contributions are as follows: (1) Minimizing bias by ensuring LLMs identify latent characteristics within large datasets; (2) Introducing a quantitative metric for feature prominence scoring, which serves as a proxy for measuring discriminative power of each feature; (3) Improving interpretability through feature-to-source traceability. (4) Enabling thorough examination of large datasets while minimizing human labor through systematic guidance of LLMs;"}, {"title": "Related Works", "content": "Recent advancements in LLMs have demonstrated their effectiveness in extracting latent features, particularly in identifying perspectives and matching values in data (Peng et al., 2023). Studies using LLM-based clustering techniques have shown promising results in extracting high-level concepts(Lam et al., 2024; Wan et al., 2024; Viswanathan et al., 2024; Pham et al., 2024), demonstrating their utility for analyzing large datasets (Wang et al., 2023; Kwon et al., 2023). Research has also shown that decomposing complex tasks into multiple stages or aspects enhances performance (Saha et al., 2023; Liu et al., 2024). Our framework combines perspective generation, multi-stage feature construction, and clustering using LLMs to conduct comprehensive latent feature extraction."}, {"title": "Bias of LLMs", "content": "LLMs face challenges in adapting to new patterns due to pre-existing knowledge biases. (Kossen et al., 2024) show that in-context learning (ICL) struggles to overcome these biases even with explicit prompts or many-shot examples. Using external knowledge bases also has limited effectiveness in reducing hallucinations and reliance on internal biases (Kenthapadi et al., 2024; Lee et al., 2023). Advanced LLMs fail to align with provided context in about 40% of predictions when the given context conflicts with their prior knowledge (Wu et al., 2024). These findings emphasize the need for robust techniques to mitigate bias and enhance grounding, especially for applications where data-driven conclusions are crucial."}, {"title": "Challenges in LLM-Driven Data Analysis", "content": "This section demonstrates that LLMs rely more on pre-trained knowledge than data-specific features, showing a tendency to depend on pre-trained knowledge when input data is absent or labels are flipped, and generating vague features when task-specific context is removed."}, {"title": "Setting", "content": "Our experimental task aims to identify the characteristics of high-quality text as defined by expert criteria (Nair and Gibbert, 2016; Tullu, 2019; Kohli et al., 2007; Padrakali and Chitra Chellam, 2017). We use two distinct datasets: (1) commercial slogans and (2) academic research titles, representing different text genres.\nTo construct the high-quality sample set, we used LLMs to evaluate each entry in the 3,000-entry dataset according to expert criteria. Binary scores were assigned for each criterion, and final ratings were aggregated. The top 600 samples formed the analysis group (\"positive\"), and the bottom 600 formed the control group (\"negative\"). See"}, {"title": "Data Grounding Issues", "content": "We explore directly extracting latent features using LLMs with two different types of input data:\n\u2022 POSDATA: Uses only positive (top-scored) samples to identify characteristics of high-quality samples.\n\u2022 MIXEDDATA: Uses both positive (top-scored) and negative (bottom-scored) samples to capture a broader range of features.\nBoth approaches have generated features that substantially overlap with expert-defined criteria (Table 1). However, since our datasets come from popular domains, it is unclear whether the LLM-generated features are truly derived from the provided data or from the model's pre-existing domain knowledge. To test this, we pose several key questions and conduct experiments to evaluate them.\n(a) Does the LLM Adapt to Input Data? We tested whether the LLM uses input data to adapt its feature generation. To evaluate this, we flip the labels and observe if the generated features change accordingly:\n\u2022 FLIPPEDPOSDATA: Negative examples are labeled as positive.\n\u2022 FLIPPEDMIXEDDATA: Positive and negative labels are reversed.\nAs shown in Table 1, features generated from POSDATA and FLIPPEDPOSDATA exhibited minimal changes, with significant overlap also observed between MIXEDDATA and FLIPPEDMIXEDDATA. This indicates that LLMs rely more on pre-existing knowledge than on input data for feature generation.\n(b) Is Pre-existing Knowledge the Primary Source of Generated Features? To determine if features are primarily sourced from internal knowledge, we generated features without any input data:\n\u2022 NODATA: The LLM generates features solely based on its pre-trained knowledge."}, {"title": "Data Scientist AI", "content": "DSAI is a 5-stage framework for automated latent feature generation, leveraging LLM capabilities with structured processes. Below, we outline the DSAI pipeline and its five stages.\n#1 Perspective Generation LLMs can extract diverse attributes from text (Peng et al., 2023). Using a subset of positive and negative samples, this stage generates distinct attributes while minimizing reliance on domain bias by excluding task-specific context. Each perspective includes a name, evaluation criterion, analysis process, and example (Figure 3 (1)). This structure provides a consistent framework for dataset interpretation. After generating perspective candidates, duplicates are removed.\n#2 Perspective-Value Matching This stage assigns values to each data point guided by the generated perspectives on the previous stage.\n#3 Value Clustering This stage groups similar values within each perspective to create interpretable feature sets with reduced redundancy. Based on LLM clustering literature (Wang et al., 2023; Wan et al., 2024; Lam et al., 2024), the process consists of two steps: generating representative cluster labels, and assigning values to appropriate clusters (Figure 3 (3)).\n#4 Verbalization This stage transforms perspective-label pairs into verbal criteria. For each pair, we compute $P(positive|D_{p,l})$, the proportion of positive examples in dataset $D_{p,l}$ corresponding to the perspective-label pair (p, l). Using this, we calculate a directional score $2 \\times P(positive | D_{p,l}) - 1$, which determines how the pair should be verbalized. Pairs with positive directional scores (>0) are directly verbalized as features describing positive data. Pairs with negative directional scores (<0) are transformed into \"avoid\" statements, which indirectly characterize positive data by specifying features to avoid. This dual transformation approach ensures coverage of both desired and undesired traits.\n#5 Prominence-based Selection Finally, DSAI employs prominence intensity as the feature selection metric, defined as the absolute value of directional score $||2*P(positive|D_{p,l}) \u2013 1||$. This metric indicates how decisively a perspective-label pair characterizes either positive or negative samples- the closer to 1, the stronger the association. Unlike our baseline approaches (Section 3.2) that merely output a list of features without any relative importance measures, our prominence intensity metric provides a quantitative way to assess and compare the discriminative power of each feature. Users can set a prominence intensity threshold to match their specific needs, balancing between stronger predictive patterns (higher threshold) and broader coverage of the dataset's latent characteristics (lower threshold).\nTraceability and Transparency The feature generation process maintains the link of each feature to its perspective-label combination, the original data samples that contributed to its creation, and the quantitative prominence score that justified its selection. Such traceability enables users to verify the legitimacy of extracted features by examining their source data and the complete derivation process."}, {"title": "Validation of Methodology Using Expert-Driven Annotation Dataset", "content": "This section validates our methodology through experiments on various datasets, focusing on three key aspects: recall of expert-defined criteria (\u00a75.1), discriminative power of generated criteria (\u00a75.2), and reliability of pipeline modules (\u00a75.3)."}, {"title": "Recall of Expert-Defined Criteria", "content": "We evaluate DSAI's ability to reproduce expert-defined criteria by applying it to slogan and research title datasets annotated as described in Section 3.1.\nWe generated criteria through our pipeline and retained those with $D_{p,l}$ > 6 and positive prominence intensity scores. This yielded 235 criteria for slogans and 198 criteria for research titles."}, {"title": "Discriminative Power (DP)", "content": "To validate our pipeline's effectiveness, we examined whether criteria with higher prominence intensity scores indeed showed stronger discriminative power in practice. We divided DSAI-generated criteria into five buckets based on prominence intensity scores to cover the full range. From each bucket, 10 features were sampled and used to annotate datasets for feature conformity (\"conform\"/\"not-conform\") where applicable. Non-applicable cases were excluded from calculations. We define DP score as follows:\nDP Score = $\\begin{cases}\nP(positive|conform) & \\text{if directional\\_score > 0,} \\\\\nP(negative|notConform) & \\text{elif directional\\_score < 0.}\n\\end{cases}$"}, {"title": "Reliability of Pipeline Operations", "content": "Due to the inherent uncertainty in LLM outputs, we evaluated the reliability of pipeline stages that rely on LLM operations by having the LLM verify its own mappings between datapoints and features. We assessed three LLM-dependent stages by asking the LLM to confirm the reliability of its outputs. Our verification process showed high consistency rates of >98%, 94%, and 98% for stages #2, #3, and #4 respectively (details in Appendix J)."}, {"title": "Real-World Application with Quantitative Datasets", "content": "We applied DSAI to three real-world user feedback datasets critical for business insights (Luo et al., 2022): (1) MIND (Wu et al., 2020), analyzing engagement features in news headlines with high CTR as the positive group; (2) spam detection (Kim, 2016), identifying patterns in spam messages as the positive group; and (3) Reddit (Magnan, 2019), exploring interaction-promoting linguistic features in highly upvoted comments as the positive group."}, {"title": "Conclusion", "content": "In this paper, we proposed DSAI, a faithful data-driven feature extraction framework that ensures LLMs identify latent characteristics from data without relying on their domain-related biases. DSAI automates thorough examination of large datasets while minimizing human labor and enhancing interpretability through source-to-feature traceability."}, {"title": "Details of Each Inference", "content": "In Stage #1 Perspective Generation, we generated 50 perspectives at per forwarding step across three steps, iteratively concatenating each step's output with the few-shot example in the prompt for the next step. For Stage #2 Perspective-Value Matching, each forwarding step processed one sentence and three perspectives as input. Stage #3 Perspective-Oriented Value Clustering used all generated values per perspective in order to effectively cluster shared characteristics. In Stage #4 Verbalization, we transformed each perspective-label pair into a compact sentence (criterion). The prompts for all stages are in Figures 23 to 27."}, {"title": "Cost Analysis", "content": "The total cost of processing 10 perspectives and 100 sentences is $2.43746, broken down as follows:\nIn Step 1: Perspective Generation, generating 10 perspectives costs $0.0304. Each perspective costs $0.00304. With a total of 2,100 input tokens and an average of 82.90 output tokens per perspective, the cost is calculated as:\n10 \u00d7 0.00304 = 0.0304\nIn Step 2: Perspective-Value Matching, each sentence requires [10/3] = 4 inferences, as each inference processes 3 perspectives. For 100 sentences, this results in:\n100 \u00d7 4 400 inferences.\nWith a cost per inference of $0.00496, the total cost for this step is:\n400 x 0.00496 = 1.984\nOn average, each inference involves 2,206 input tokens and 440 output tokens.\nIn Step 3: Perspective Value Clustering, each perspective incurs a clustering cost of $0.0087125. This includes two components:\n\u2022 Label Generation, costing $0.00579625 for 597 input tokens and approximately 1,010 output tokens.\n\u2022 Value Matching, costing $0.00291625 for 269 input tokens and 516 output tokens.\nThe total clustering cost per perspective is:\n0.00579625 +0.00291625 = 0.0087125\nFor 10 perspectives, the total clustering cost is:\n10 \u00d7 0.0087125 = 0.087125\nIn Step 4: Verbalization, it is assumed that the 10 perspectives cluster into 5 groups each, resulting in 50 perspective-label pairs. Verbalizing each pair costs $0.0018, so the total cost for this step is:\n50 \u00d7 0.0018 = 0.09\nSumming all the costs, the total processing cost is:\n0.0304 +1.984 + 0.087125 +0.09 = 2.43746"}, {"title": "Discriminative Power of Direct LLM- and DSAI-Generated Features", "content": "The accuracy with which generated features reflect the latent characteristics of the positive dataset depends on their alignment with the input data. To evaluate this alignment, we annotated the dataset using all generated features and assessed their discriminative power (DP) using two key metrics: P(positive|Conform) and P(negative|NotConform).\nThe metric P(positive|Conform) measures the proportion of positive data among instances labeled as \"Conform\" to a feature. This value indicates how effectively a feature captures the latent characteristics of the positive dataset. Scores below 0.5 suggest that the feature aligns more closely with negative data.\nP(negative|NotConform) evaluates how successfully a feature identifies traits in the negative dataset. Scores above 0.5 indicate that the feature effectively captures the negative traits, while scores below 0.5 suggest misalignment, potentially indicating positive traits incorrectly marked as \"Not Conform.\"\nWe used LLM labeling to filter out the \"Not Applicable\" cases (e.g., \"technical terms in medical domains\" for non-medical texts) before calculating DP scores.\nThis two-metric approach enables evaluation of features' effectiveness in both capturing positive characteristics and identifying traits to avoid in negative data.\nTo simplify interpretation, in the following sections, we subtracted 0.5 from all DP scores. This adjustment highlights values below 0.5, making it easier to identify features that fail to align effectively with the intended dataset. Positive values after subtraction indicate alignment, while negative values signal misalignment."}, {"title": "Discriminative Power of LLM-Generated Features", "content": "As shown in Table 4, several features from POSDATA and MIXEDDATA lacked the discriminative power and validity required for reliable analysis. In slogans, 3 out of 18 POSDATA features had DP scores below 0.1 (with 1 negative), and 8 out of 17 MIXEDDATA features were below 0.1. Similarly, in titles, 4 out of 23 POSDATA features were under 0.1 (with 3 negative), and 2 out of 13 MIXEDDATA features were under 0.1.\nThese results highlight that directly generating features from LLMs often produces features that lack sufficiently discriminative strength and reliability for robust data analysis, suggesting the need for improved methodologies."}, {"title": "Discriminative Power of DSAI-Generated Features", "content": "To evaluate the reliability of features from DSAI, we organized them into five buckets along the prominence intensity spectrum. From each bucket, we randomly sampled 10 features to annotate the entire dataset with Conform/Not Conform labels when applicable, and marked Not-Applicable when features were inapplicable. For example, we marked \"Avoid focusing solely on financial aspects when describing the business model\" as Not-Applicable for data points that did not have any business model descriptions."}, {"title": "Prominence Intensity and Occurrence Analysis", "content": "This section explores the relationship between feature frequency, prominence, and their impact on discriminative power. By analyzing feature occurrence and prominence distributions, we provide insights to help readers determine appropriate thresholds for these factors, aiding in the selection of features that optimize performance in distinguishing positive and negative traits within datasets."}, {"title": "Influence of Feature Occurrence on Discriminative Power", "content": "We analyzed the influence of feature occurrence frequency on discriminative power by grouping features into frequency buckets (e.g., <=10, 20-100, and >100) and evaluating mean/maximum prominence metrics (Figure 5a)\nLow-Frequency Features Features with low frequencies (<= 10) demonstrated higher discriminative power, with mean prominence as high as 0.482 (some reaching even 1.0) in the slogan dataset. This suggests that low-frequency features effectively represent specific traits of positive data, though their rarity may limit generalization.\nMedium-Frequency Features Features with medium frequencies 20\u2013100) displayed a balance between specificity and generality. They demonstrated consistent performance in discriminative tasks through moderate mean prominence values.\nHigh-Frequency Features Features with high frequency (> 100) were found to be more generic, showing low mean prominence values (0.087 in the slogan dataset for frequencies > 500). Though less discriminative, they provide valuable insights into the dataset's baseline characteristics. Their high frequency makes them suitable for applications where specificity is less critical."}, {"title": "Influence of Feature Prominence on Discriminative Power", "content": "We also analyzed the distribution of features across different prominence score levels of by examining frequency accumulation within prominence buckets (Figure 5b). This analysis revealed how features were concentrated across low, moderate, and high prominence scores.\nSlogan The features were concentrated in the lower prominence range, with 61 features below 0.1 and 108 features below 0.2. This distribution suggests that while the majority of features exhibit limited discriminative power, a select subset of features with higher intensities plays a disproportionate role in capturing latent positive traits."}, {"title": "Threshold Analysis on Slogans", "content": "In this section, we track how different prominence and frequency thresholds affect the recall of expert criteria, with particular attention to which criteria types are most resistant to threshold increases. In general, we observe progressive filtering of features as thresholds increase, which aligns with our criteria importance categorization. However, we also observed some exceptions diverging from their importance categorization. These findings illustrate how DSAI can identify discrepancies between theoretical feature importance and actual implementation patterns in the data."}, {"title": "Analysis of Expert-defined Criteria", "content": "We categorize expert criteria based on their importance to slogan effectiveness, ranging from critical to supplementary features. This categorization provides a framework for analyzing which features persist across different thresholds.\nThese features are essential for effective communication of the brand's core sales proposition and unique characteristics. They are considered critical because they directly influence a slogan's ability to capture and deliver the brand's main message to consumers."}, {"title": "Prominence Intensity Threshold Analysis", "content": "Recall remains stable until reaching a relatively high threshold (0.348), indicating robust baseline coverage of our pipeline. The relationship between importance levels and threshold resilience shows an overall aligned pattern, though with some complexity: while lower-importance criteria are consistently filtered out early, criteria of moderate importance and above show varied retention patterns, with some excluded at mid-level thresholds and others persisting until the highest thresholds."}, {"title": "Frequency Threshold Analysis", "content": "Recall remains stable up to a frequency threshold of 93, indicating strong coverage of expert criteria even when considering only frequently observed"}, {"title": "Threshold Analysis on Research Titles", "content": "Similar to our analysis on slogans (Appendix H), the results suggest that DSAI can effectively capture the nuanced reality of title construction, where practical implementation patterns may differ from theoretical guidelines."}, {"title": "Analysis of Expert-defined Criteria", "content": "We begin by categorizing expert-defined criteria based on their contribution to a title's primary function: effective delivery of research content to readers.\nThese features are fundamental as they directly affect a title's ability to enable readers quickly grasp the paper's topic and contributions."}, {"title": "Prominence Intensity Threshold Analysis", "content": "The majority of features are retained even at a high threshold of 0.692, suggesting that our"}, {"title": "Direct Latent Feature Extraction using LLMS", "content": "To evaluate whether the features directly generated by large language models (LLMs) are grounded in the input data or influenced by pre-existing knowledge, we conducted a series of experiments. These experiments were designed to investigate the effects of input data, label variations, and task context on feature generation (Section 3.2).\nIn these experiments, we ensured that all available data were utilized to maximize the representativeness of the input. For instance, we used all 600 positive samples for POSDATA and the complete set of 1200 samples (both positive and negative) for MIXEDDATA. This comprehensive use of data aimed to minimize bias and ensure that the features generated by the LLM were robustly grounded in the provided input.\nTo further address potential data-grounding is-"}, {"title": "Implementation Details and Cost Analysis", "content": "In this section, we provide the prompts used in each stage of the DSAI pipeline along with corresponding outputs."}, {"title": "Dropped criterion as Prominence threshold increases", "content": "patterns. The relationship between theoretical importance and threshold resilience reveals both expected alignments and interesting disparities. Critical features persist at high thresholds, and certain lower-importance features being filtered early. Some theoretically important features drop out earlier than expected, while certain lower-importance features demonstrate surprisingly high frequency in practice."}, {"title": "DSAL-Generated Top/Bottom Prominence Features of Industry Dataset", "content": "The following analysis evaluates the prominence score distribution of criteria across three datasets: Reddit, MIND, and SPAM. These scores indicate how reliably each feature identifies the latent characteristics within the datasets."}, {"title": "General vs Specific Features", "content": "DSAI-generated features capture both general characteristics shared across the dataset and highly specific features. As shown in Table 10, some features represent broad, overarching traits that are present across a significant portion of the dataset, with high frequencies (900+ instances). These include common patterns such as \"lack of logical reasoning\" or \"absence of historical reflection,\" which are applicable across diverse contexts.\nIn contrast, more specific features capture nu-"}, {"title": "#LLM Annotation", "content": "You will be provided with company slogans, names and descriptions of each company.\nYour task is to evaluate each title based on the 'Checklist/useful tips for drafting a good slogan' given below.\nFor each evaluation criterion, provide a reason for your judgment and generate a response.\nIf the slogan meets the criterion answer should be 'yes', otherwise 'na'.\nIf you cannot evaluate the slogan with the criterion, response with 'not_applicable'"}, {"title": "Figure  Prompts for direct LLM feature generation.", "content": "You are a helpful assistant.\nYou've been given {N} examples of effective {type}. Your task is to thoroughly analyze these examples and extract as many specific criteria as possible.\nGenerate a comprehensive list of conditions that make a {type} effective.\n**Make sure to refer to the given examples.\nYou are a helpful assistant.\nYou've been given {N} examples of effective {type} and {N} examples of ineffective {type}. Your task is to carefully analyze these examples and identify as many key criteria as possible that differentiate a successful {type} from an unsuccessful one.\nGenerate a comprehensive list of conditions that make a {type} effective.\n**Make sure to refer to the given examples.\nYou are a helpful assistant.\nYou've been given {N} examples of group A and {N} examples of group B. Your task is to carefully analyze these examples and identify as many key criteria as possible that differentiate group A and B.\nGenerate a comprehensive list of conditions.\n**Make sure to refer to the given examples."}, {"title": "# Perspective Generation Prompt and Outputs", "content": "Your goal is to analyze each sentence and capture its characteristics from various perspectives.\nYou will be given a list of sentences.\nThere are two things you should do. First, generate analytical attributes for evaluating the sentences. Create {numbers} distinct attributes. Second, write a step-by-step plan with example on how to analyze sentences using each generated attribute.\n1. List each word and its description without numbering.\n2. Each plan must include examples.\n3. Answer in the example should be in short n-gram.\nGive the result in json format as follows:\n{\nattribute 1: description 1,\n}\nThe sentences you should analyze are as follows: {sentences}\nOutputs"}, {"title": "# Perspective-Value Matching Prompt and Outputs", "content": "Your task is to analyze given sentences using all provided analysis criteria. You will be given a list of criteria and matching explanation.\nThe result should be structured as follows:\n'sentence': \n'(1, analysis criterion 1)': {\n'description': \n'explanation': \n'value': \n\nFrom now on, sentences and analysis criteria with explaination will be provided. Please generate your responses step by step, following the example above. Make sure to use all the criteria.\nDo not generate anything else.\nSentence = {sentence}\nAnalysis criteria = {perspectives}\n** responses must be in valid json object\nOutputs"}, {"title": "# Clustering Prompt", "content": "Your goal is to make a list of labels for {perspective} classification. The explanation of the {perspective} is as follows: {description} With given list of values for categorizing, make New label list considering the checklist below.\nEach value MUST belong to some label.\nA value cannot belong to multiple labels.\nGroup name should be distinctive. For example, if there are names (\"subtle\", \"indirect\", \"implicit.\") or (\"medium\", \"moderate\"), as they are very similar, we should group them. Because it can be confusing for classification.\nIf the main focus is on asking about presence or absence such as 'present' or 'absent', other values should be structured accordingly. (...)\nWords that have opposite meaning must belong to different group. For example, \"absent\" and \"present\" must belong to different group. Similarly, \"clear\", \"not_clear\" must be in different group.\nWords that represent \"Intensity\" or \"Degree\" must be in different group. For example, \"Low\":[\"low\"], \"medium\": [\"moderate\", \"medium\"], \"high\":[high] is only allowed.\nUse all provided values for clustering. If the main focus is on asking about presence or absence such as 'present' or 'absent', other values should be structured under the label 'present' or 'absent'.\nGenerate output in valid json format.\nYour task is to match each value to the most appropriate group from the given group names.\nYou are given a list of group names: {groups}\nYou are also given a list of values: {values}\nFor each value, find the group name that best represents the meaning or category of the value.\nMake sure each value is assigned to exactly one group.\nSome values may be more general or specific. Use your best judgment to match the values accordingly.\nIf multiple values could fit under the same group, group them together.\nPlease ensure that the output is a well-formed dictionary. Each value must be categorized under one of the given groups."}, {"title": "# Clustering Outputs", "content": "{\n\"Descriptive\": [\n\"descriptive_noun_phrase\", \"descriptive_reference\", \"descriptive_nominal\",\n\"descriptive\", \"descriptive_listing\", \"descriptive_label\", \"descriptive_information\",\n\"descriptive_heading\", \"descriptive_title\", \"descriptive_list\", \"descriptive_fragment\",\n\"descriptive_phrase\"\n],\n\"List-Related\": [\n\"entity_list\", \"noun_list\", \"listing\", \"list\", \"list_heading\", \"list_association\",\n\"list_information\", \"non-sentence_list\", \"list-like\", \"product_listing\",\n\"list_structures\", \"declarative_list\", \"title_list\", \"headline_list\"\n],\n\"Phrase\": [\n\"informational_phrase\", \"noun_phrase_complement\", \"phrase_list\", \"informative_heading\",\n\"declarative_phrase\", \"interrogative_phrase\", \"title/phrase\", \"phrase\", \"noun_phrases\",\n\"descriptive_phrase\", \"declarative_noun_phrase\", \"noun_phrase_title\",\n\"noun_phrase_label\", \"noun_phrase\", \"incomplete_phrase\"\n],\n\"Declarative\": [\n\"imperative_declarative\", \"declarative_exclamatory\", \"declarative-heading\",\n\"declarative_title\", \"declarative_headline\", \"declarative\", \"declarative and imperative\",\n\"declarative_implied\", \"declarative_incomplete\", \"declarative_slogan\",\n\"declarative_imperative\", \"declarative_statement\", \"declarative_header\",\n\"declarative_information\", \"declarative_location\", \"headline_declarative\",\n\"declarative_listing\"\n],\n\"N/A\": [\n\"n/a\", \"incomplete\", \"incomplete_descriptive\", \"incomplete_label\", \"incomplete_list\",\n\"incomplete_structure\", \"incomplete_sentence\", \"incomplete_statistic\",\n\"incomplete_listing\", \"incomplete_phrasing\", \"incomplete_noun_phrase\",\n\"incomplete_nominal\"\n],\n\"Imperative\": [\n\"imperative\"\n],\n\"Interrogative\": [\n\"interrogative\", \"interrogative_declarative\", \"question_format\"\n],\n\"Exclamatory\": [\n\"declaration_exclamation\", \"exclamatory\"\n],\n\"Promotional\": [\n\"tagline\", \"advertisement\", \"promotional_tagline\", \"promotional\"\n]\n}"}, {"title": "# Verbalization Prompt and Outpus", "content": "You are provided with four elements: Perspective, Description, Type, and Label.\nYour task is to generate a Requirement by interpreting the combined meaning of Perspective and Label.\n- If Type is \"Positive\": this means the Perspective and Label combination represents an ideal or desired condition.\nIf Type is \"Negative\": this indicates an undesirable or avoidable condition (i.e., a condition to avoid).\nEach output should have three fields:\n1. Requirement: Summarize the key requirement derived from the combined meaning of Perspective and Label.\n2. Additional Explanation: Provide further details, explaining the approach, goals, or intended focus for the requirement.\nGiven the following input, generate a requirement using the unique meaning of Perspective and Label:\nProvided Input:\nPerspective: {perspective}\nDescription: {description}\nLabel: {label}\nType: {type}"}, {"title": "# Perpective-Value Matching Consistency Check", "content": "You will receive 'perspective', 'description', 'title'"}]}