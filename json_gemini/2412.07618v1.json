{"title": "Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced Retrieval-Augmented Generation on Knowledge Graphs", "authors": ["Xiaqiang Tang", "Jian Li", "Nan Du", "Sihong Xie"], "abstract": "Despite the superior performance of Large language models on many NLP tasks, they still face significant limitations in memorizing extensive world knowledge. Recent studies have demonstrated that leveraging the Retrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs that encapsulate extensive factual data in a structured format, robustly enhances the reasoning capabilities of LLMs. However, deploying such systems in real-world scenarios presents challenges: the continuous evolution of non-stationary environments may lead to performance degradation and user satisfaction requires a careful balance of performance and responsiveness. To address these challenges, we introduce a Multi-objective Multi-Armed Bandit enhanced RAG framework, supported by multiple retrieval methods with diverse capabilities under rich and evolving retrieval contexts in practice. Within this framework, each retrieval method is treated as a distinct \"arm\". The system utilizes real-time user feedback to adapt to dynamic environments, by selecting the appropriate retrieval method based on input queries and the historical multi-objective performance of each arm. Extensive experiments conducted on two benchmark KGQA datasets demonstrate that our method significantly outperforms baseline methods in non-stationary settings while achieving state-of-the-art performance in stationary environments.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) (Chowdhery et al. 2023; OpenAI 2023; Touvron et al. 2023) excel in natural language processing tasks (Bang et al. 2023; Brown et al. 2020) but struggle with knowledge-intensive challenges, often producing unfaithful or hallucinated information (Petroni et al. 2020; Ji et al. 2023). Retrieval-Augmented Generation (RAG) (Lewis et al. 2020) has been developed to enhance LLM reasoning, effectively reducing hallucinations and providing reliable, up-to-date information. In this approach, when presented with a user query, a retriever first extracts relevant information from a knowledge base, which is then provided to the LLM to generate the final response.Recent advancements (He et al. 2024; Luo et al. 2023c; Sun et al. 2023; Xu et al. 2024) in RAG systems have increasingly incorporated Knowledge graphs (KGs) (Baek, Aji, and Saffari 2023; Luo et al. 2023b) as the underlying knowledge base. KGs store vast amounts of factual data in a structured format, which enables more dependable and systematic reasoning by LLMs.\nUnlike unstructured text databases (e.g. Wikipedia), the organized nature of KGs provides diverse retrieval methods, with significantly different capabilities and costs. For example, dense retrieval methods (Zhang et al. 2023; Yu et al. 2022) are typically fast but offer limited reasoning capabilities. In contrast, using LLMs to generate KG query languages (e.g., SPARQL) as in ChatKBQA (Luo et al. 2023a) provides high coverage and is suitable for multi-entity retrieval. Methods like RoG (Luo et al. 2023c), where LLMs function as search agents excel in complex reasoning. However, both methods require interactions with LLMs like ChatGPT (OpenAI 2024), leading to longer execution times. However, current KG-based RAG systems often rely solely on a single retrieval method or use static neural network routers (Reis et al. 2019; lla 2024), which require complete labeled data for supervision and periodic fine-tuning. Moreover, while RAG systems are often deployed in scenarios where users can provide feedback on generated responses (Gamage et al. 2024; Alan, Ayd\u0131n, and Karaarslan 2024), current systems generally neglect this feedback. Relying on a single retrieval method, or computationally intensive ensemble all retrieval results can not ensure responses that are both timely and informative. On the other hand, static neural network routers cannot effectively leverage real-time feedback to continually adapt to changing user needs and system variability.\nTherefore, deploying RAG systems in real-world scenarios faces the following challenges as described in Fig. 1: (C1): Non-stationary environments require RAG systems to adapt to two sides continuously: on the user side, the evolving nature of queries driven by trending topics, and on the server side, the backend retrieval model upgrading. (C2): In practical applications RAG systems, such as personal home assistants and customer support chatbots, balancing multi-objective, such as efficiency, coverage, and reasoning power, is crucial to providing informative and satisfying user experiences. Failing to address the diverse demands of queries"}, {"title": "Method", "content": "The diverse capabilities of different retrieval methods necessitate a strategic model for their selection. Simply running multiple retrievers and then aggregating their results often proves sub-optimal due to two main factors: the need for timely responses and the disparate performance characteristics of various retrieval methods, as highlighted in Table 1. For example, while dense retrievers provide rapid responses, KG agent-based retrievers slow down the system due to LLM inference.\nConsequently, we developed a model that dynamically assigns queries to the most suitable retrievers. Unlike static neural network routers, which require collecting complete labeled data for supervision (involving the execution of all retrieval methods) and periodic fine-tuning, limiting their adaptability to non-stationary environments. Our approach leverages real-time user feedback as a reward signal to update the model. This adaptability is crucial in the dynamic nature of RAG applications, such as shifting user interests and backend retriever upgrades requiring continuous optimization."}, {"title": "Problem Setup", "content": "The optimization of KG-based RAG systems employing multiple retriever backends and real-time feedback is structured as follows:\n\u2022 Initially, the system receives a user input query context x.\n\u2022 The PLM model $f_{\\theta}$ processes the query and selects an action a from the action space A, which includes K potential retrieval methods, each representing an arm in a multi-armed bandit.\n\u2022 Upon selection, the system receives feedback on the performance of the chosen retrieval method a (e.g. 1 indicating a good response, 0 indicating a bad response), providing \"partial-information\u201d feedback. This limitation restricts the system's ability to assess unselected methods.\n\u2022 Utilizing this feedback, the model iteratively refines its strategy to improve base retrieval method selection for future queries."}, {"title": "Deep Multi-objective Contextual Bandits", "content": "Query Encoding Model: In order to effectively select retrieval methods, it is crucial to discern patterns within user queries and associate these with the capabilities of suitable retrieval methods. Traditional linear models in contextual bandits (Li et al. 2010; Mehrotra, Xue, and Lalmas 2020), while effective in certain scenarios, often fall short due to the complex natural language patterns present in user queries.\nTo address limitations and ensure real-time service, we utilize the lightweight Pre-trained Language Model, DistilBERT (Sanh et al. 2019). As a streamlined version of BERT, DistilBERT retains approximately 97% of BERT's language understanding capabilities and increases processing speed"}, {"title": "Arm Selection Strategy", "content": "Upon receiving an action distribution estimation $z = f_{\\theta}(x)$ from the encoding model, we employ an epsilon-greedy strategy (Langford and Zhang 2007) to balance the trade-off between exploration and exploitation (Auer 2002; Auer, Cesa-Bianchi, and Fischer 2002). This balance is crucial in ensuring that the system not only leverages the information gathered so far (exploit known retrieval methods that have proven effective) but also explores new possibilities to enhance learning (explore other retrieval methods that could potentially offer better results). Specifically:\n\u2022 With a probability of $1 - \\epsilon$, the system selects the arm with the highest predicted reward, $a = max(z)$, based on the output from the encoding model.\n\u2022 Conversely, with a probability of $\\epsilon$, the system explores by randomly selecting an arm, facilitating the discovery of potentially more effective retrieval methods.\nThis strategy enables the system to predominantly rely on the best-known actions to maximize immediate rewards while maintaining the flexibility to explore new possibilities. This approach is essential to mitigate the risk of converging to a locally optimal model due to partial information feedback, fostering the discovery of superior long-term solutions through randomized exploration."}, {"title": "Learning Algorithm", "content": "After selecting a retrieval method, our model updates based on the observation associated with the chosen method, but it does not have access to the information from methods not selected (i.e., partial information feedback). Inspired by \"offline-to-online\u201d learning(Lee et al. 2022; Guo et al. 2024), we first pre-train the model in an offline environment to learn a robust initial strategy. Subsequently, we fine-tune the model in an online setting using partial user feedback, allowing it to adapt continuously to real-world conditions.\nTraditional RAG systems often focus on optimizing model accuracy. However, real-world applications of RAG systems demand not only accuracy but also real-time responsiveness, introducing the need for multi-objective optimization. We use the Generalized Gini Index (Weymark 1981) to balance system performance with retrieval time, ensuring both accuracy and efficiency are optimized simultaneously.\nDuring training, we use detailed evaluation metrics, including informativeness measures like hit and recall, to optimize for accuracy and coverage. Retrieval latency is also used as feedback to enhance efficiency. In testing, we simulate an online environment with a hit value (0 or 1) to approximate binary user feedback. This offline-to-online learning approach ensures the model is well-prepared before deployment and can adapt effectively to dynamic user interactions."}, {"title": "Experiment", "content": "Datasets: We evaluate our systems on two KGQA datasets WebQSP (Yih et al. 2016) and Complex WebQuestions (CWQ) (Talmor and Berant 2018) which contain up to 4-hop questions. The statistics of the datasets are given in Table 4.\nBaselines: To valid the effectiveness of our MAB-enhanced KG-based RAG system under stationary environment, we compared it with state-of-the-art KG-based RAG systems, including the query language-based RAG: StructGPT (Jiang et al. 2023b), ChatKBQA(Luo et al. 2023a), LLM agent-based RAG: Think-on-Graph (Sun et al. 2023) and Reason-on-Graph (Luo et al. 2023c), and dense retrieval based RAG (Zhang et al. 2023; Yu et al. 2022).\nEvaluation Metrics: Following previous works, we evaluate the performance of our Retrieval-Augmented Generation system, all results are assessed based on the final generated response's hit rate and recall. We ran at least ten independent rounds with different seeds and reported the results as mean \u00b1 standard deviation to ensure the stability of our findings.\nImplementations: To leverage the strengths of various retrieval methods, our MAB-enhanced RAG system, along with other router-based RAG approaches, utilizes DECAF, ChatKBQA, and Reason-on-Graph as its action space. We consistently use Llama-2-7b-chat-hf (Touvron et al. 2023) as the LLM generator, applying a standard RAG prompt (LlamaIndex 2024) across all methods to ensure a fair comparison. All experiments are conducted on the Nvidia Tesla V100 graphical card with the Intel Xeon Platinum 8255C CPU."}, {"title": "Research Questions and Main Results", "content": "RQ1: Can our Multi-Armed Bandit enhanced Retrieval-Augmented Generation system effectively improve performance compared to RAG systems that rely on a single retrieval method?\nThe comparative analysis, summarized in Table 1, revealed that our MAB-enhanced RAG system demonstrated superior performance across both datasets. Notably, on the CWQ dataset, which poses more intricate multi-hop reasoning challenges, our method exceeds the next-best performance by nearly 2% in hit rate and over 2.5% in recall.\nWe present examples of our MAB-enhanced RAG systems superior case. In the first case Fig. 7 derived from the challenging CWQ dataset, the query pertains to the birthplace of the lyricist for \"Stop Standing There.\" Dense retrieval fails to relate the query to relevant information, and while the SPARQL retriever approaches a correct formulation, it ultimately generates a wrong query language. Thanks"}, {"title": "RQ2: Can the MAB enhanced RAG system adapts dynamically to the non-stationary nature of real-world environments, ensuring that they continuously meet evolving query demands and operational conditions?", "content": "To evaluate our methods under non-stationary environments, we use two non-stationary settings: (1) we employed the KG agent-based retrieval method (Sun et al. 2023) during the training phase. For online testing, we switched to (Luo et al. 2023c) a method with superior performance, to simulate the effect of upgrading backend retrievers independently to enhance system functionality. This approach tests the system's ability to adapt seamlessly to improvements in retrieval methods, reflecting real-world conditions where continuous updates are crucial for maintaining system efficacy. (2) To simulate the shift in query domains resulting from changes in trending topics, we initially train our methods using the WebQSP dataset. Subsequently, we evaluate the system's adaptability by testing it on the Complex WebQuestions dataset. This approach allows us to assess how well the system can handle transitions between different types of query complexities and content, mirroring real-world scenarios where query characteristics can vary significantly due to external influences.\nThe results, as detailed in Table 2, in the first scenario, during the retriever upgrade tests, our method demonstrated the highest Test Hit and Test Recall rates f 84.80% and 72.24% respectively, with a significantly reduced Test Retrieval Delay of 5.88 seconds per query. This improvement stems from our system's capability to leverage partial information during testing to continuously refine the model. In contrast, retrieval ensemble methods, which require running all retrieval methods, struggle with denoising information from different structures of retrieval results leading to the longest Retrieval delay. Both the offline classifier and offline"}, {"title": "RQ3: How can the Generalized Gini Index be effectively utilized to balance multiple performance metrics in RAG systems", "content": "In Table 3 we evaluate our proposed method on the WebQSP dataset, results highlight the effectiveness of the Generalized Gini Index enhanced Multi-Objective Multi-Armed Bandit (GGI-MO-MAB), achieving the highest Test Hit rate and Test Recall, while maintaining the lowest retrieval delay compared to the baselines. Non-contextual baselines like UCB (Auer 2002) and Thompson Sampling (Agrawal and Goyal 2013) approximate only a single optimal retrieval method. LinUCB (Li et al. 2010) under-performs due to its inability to handle the high-dimensional, complex natural language embeddings. Single-objective deep contextual MAB models, while improving accuracy metrics such as Hit rate, often neglect retrieval time, adversely affecting user experience. Our GGI-MO-MAB can also outperform multi-objective baseline MOU-UCB (Wanigasekara et al. 2019).\nTo underscore the efficacy of our approach, we include an ablation study comparing the GGI function to a learnable weight aggregation baseline (MO-MAB), confirming the robust performance improvement of our method."}, {"title": "RQ4: What are the effects of implementing multiple retrieval methods, such as dense retrieval and KG agent retrieval methods, on the response times and accuracy under different real-world scenarios?", "content": "The comparison of different types of retrieval methods is shown in Fig. 4, we also employed the Jaccard Similarity Coefficient to assess the Hit metric across results. Our findings reveal an average coefficient of 0.738, with the lowest observed at 0.496, indicating the distinctiveness of the results obtained by different retrieval strategies.\nMoreover, while methods such as ChatKBQA and Reason-on-graph showed strong results on WebQSP, they were less effective on the more challenging CWQ dataset, highlighting the importance of retrieval method selection based on the complexity and nature of the dataset. Our system's consistent performance across different datasets underscores its robustness and adaptability, making it particularly suitable for diverse real-world applications where query demands and operational conditions can vary significantly.\nIn terms of response time, we observed significant differences in processing time; for instance, dense-vector retrieval methods average around 1 second, whereas more complex"}, {"title": "Related Work", "content": "KG-based RAG Systems: Retrieval-Augmented Generation (RAG)(Lewis et al. 2020) mitigates the hallucination issue of LLMs by retrieving external knowledge to enhance the accuracy and reliability of generation content. Recent RAG advancements have increasingly incorporated Knowledge Graphs (KGs) (Luo et al. 2023c; Sun et al. 2023; Xu et al. 2024; He et al. 2024), which store structured factual information, enabling more systematic reasoning by LLMs (Pan et al. 2024). KGs support diverse retrieval methods, each with different capabilities and costs, as detailed in ?? .\nOur analysis of retrieval methods, discussed in ??, shows that current KG-based RAG systems(Luo et al. 2023c; Sun et al. 2023; Xu et al. 2024; He et al. 2024) predominantly rely on a single retrieval method, which often fails to meet the varied demands of real-world applications. These systems typically assume a stationary environment and remain static without subsequent fine-tuning, making them unable to adapt to potential shifts in the query domain and upgrades of the backend retriever. To address these issues, our work aims to develop an MAB-enhanced RAG system that strategically combines multiple retrievers. By leveraging real-time feedback, our system can dynamically adjust retrieval strategies to meet the evolving demands of diverse application scenarios of the RAG system effectively.\nTo our knowledge, the concurrent research by (Sawarkar, Mangal, and Solanki 2024) is one of the few studies attempting to integrate multiple retrieval methods, but it focuses on textual data sources and lacks the continuous optimization crucial for RAG systems in non-stationary environments.\nMulti-Armed Bandit Algorithms: The Multi-Armed Bandit (MAB) (Katehakis and Veinott Jr 1987) framework optimizes the balance between exploiting historical data and exploring new information. It includes two main types: context-free (Bubeck, Cesa-Bianchi et al. 2012), which operates without external information, and contextual bandits (Mahajan and Teneketzis 2008), which incorporate contextual data such as user features. Traditional contextual bandits assume a linear relationship between context and expected rewards (Slivkins 2011), but recent developments have introduced non-linear models through deep learning (Collier and Llorens 2018; Zhou, Li, and Gu 2020; Shi et al. 2023). The multi-objective contextual MAB (MOCMAB) algorithm (Tekin and Tur\u011fay 2018) maximizes rewards across multiple objectives, managing both dominant and non-dominant goals. Some approaches (Busa-Fekete et al. 2017; Mehrotra, Xue, and Lalmas 2020) use the Generalized Gini Index (GGI) to convert multi-objective challenges into single-objective optimizations, simplifying decision-making in dynamic environments.\nHowever, existing contextual bandit algorithms often assume reward is linear with respect to the context feature (Tekin and Tur\u011fay 2018; Mehrotra, Xue, and Lalmas 2020; Li et al. 2010), limiting their representational capacity to match user query patterns with retrieval strategies effectively, or they focus solely on single-objective optimization (Collier and Llorens 2018; Zhou, Li, and Gu 2020; Shi et al. 2023), which does not suffice for complex RAG systems with requirements of performance and real-time limitation. Therefore, in this work, we adopt a non-linear multi-objective contextual MAB model."}, {"title": "Conclusion", "content": "In this work, we introduced a novel KG-based RAG framework enhanced by a Multi-Armed Bandit (MAB) model. By leveraging real-time user feedback, our system dynamically adapts to shifting query demands and backend upgrades. We further incorporated the Generalized Gini Index to balance multiple objectives, ensuring that the system delivers both informative and timely responses.\nOur comprehensive evaluations on two well-established KBQA datasets, WebQuestionSP and Complex WebQuestions, demonstrate that our approach not only significantly outperforms baseline methods in non-stationary environments but also surpasses state-of-the-art KG-based RAG systems in stationary settings. These results underscore the robustness, adaptability, and practical applicability of our framework in real-world scenarios where query demands and operational conditions are constantly evolving."}, {"title": "Generalized Gini Index", "content": "To further leverage the unique strengths of each method and enhance the overall efficacy of the RAG pipeline. This necessitates the formulation of a multi-objective optimization problem.\nThe Generalized Gini Index (GGI) emerges as a crucial tool in this context, offering a sophisticated framework for equitably balancing diverse criteria in multi-objective optimization scenarios. The need for such an optimization arises from the varied and often conflicting objectives associated with different retrieval methods. For instance, while one method may excel in accuracy, another might offer benefits in terms of speed. The challenge, therefore, lies in achieving an optimal balance that maximizes the overall performance of the RAG system.\nGGI has been characterized by (Weymark 1981). It encodes both efficiency as it is monotone with Pareto dominance and fairness as it is non-increasing with Pigou-Dalton transfers(Jenkins 2017). Informally, a Pigou-Dalton transfer involves increasing a lower-valued objective while decreasing a higher-valued objective by an equivalent amount, without altering the order between the two objectives. This operation seeks to equilibrate the cost vector. Formally, the GGI adheres to the following fairness principle: for any $x_i < x_j$,\n$G_w(x + \\epsilon e_i - \\epsilon e_j) \\leq G_w(x)$ (6)\nwhere $e_i$ and $e_j$ are two vectors of the canonical basis. Consequently, among vectors with an equal sum, the optimal cost vector (with respect to GGI) is the one that features uniform values across all objectives, provided that such a distribution is feasible."}, {"title": "MAB enhanced RAG systems with LLM variants", "content": "We evaluated our MAB-enhanced RAG system using various LLM generators, including Chatglm3 (Du et al. 2021) and Mistral (Jiang et al. 2023a). The results, depicted in Fig. 5, demonstrate that our system significantly improves RAG performance compared to traditional systems that utilize only a single retriever. This enhancement underscores the robustness and adaptability of our MAB-enhanced RAG approach."}, {"title": "Case study", "content": "We further present a case study Fig. 6 to illustrate the effectiveness of our system, the query inquires about influences on Frank Lloyd Wright. While dense retrieval provides a fast response, it introduces noisy and irrelevant data. The LLM agent retriever, though accessing structured knowledge graphs, fails to deliver accurate information, focusing instead on peripheral details like professions. However, the SPARQL-retriever can give a very accurate query code for this question with clear conditions, and successfully fetch all results that support our systems to offer a nuanced and accurate response."}, {"title": "Retrieval methods on KG", "content": "Dense retrieval: Dense retrieval methods (Zhang et al. 2023; Karpukhin et al. 2020) standardize and segment diverse document formats like PDF, HTML, Word, and Markdown into plain text, which is then transformed into vector embeddings for efficient searching (Karpukhin et al. 2020). Utilizing a pre-trained language model (Devlin et al. 2018), DPR creates dense embeddings from question-passage pairs, significantly enhancing accuracy over BM25 and ORQA in open Natural Questions. Additionally, recent developments by (Zhang et al. 2023) have tailored embedding models to meet the varied retrieval demands of LLMs"}]}