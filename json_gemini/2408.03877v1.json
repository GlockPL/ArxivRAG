{"title": "Knowledge Probing for Graph Representation Learning", "authors": ["Mingyu Zhao", "Xingyu Huang", "Ziyu Lyu", "Yanlin Wang", "Lixin Cui", "Lu Bai"], "abstract": "Graph learning methods have been extensively applied in diverse application areas. However, what kind of inherent graph properties e.g. graph proximity, graph structural information has been encoded into graph representation learning for downstream tasks is still under-explored. In this paper, we propose a novel graph probing framework (GraphProbe) to investigate and interpret whether the family of graph learning methods has encoded different levels of knowledge in graph representation learning. Based on the intrinsic properties of graphs, we design three probes to systematically investigate the graph representation learning process from different perspectives, respectively the node-wise level, the path-wise level, and the structural level. We construct a thorough evaluation benchmark with nine representative graph learning methods from random walk based approaches, basic graph neural networks and self-supervised graph methods, and probe them on six benchmark datasets for node classification, link prediction and graph classification. The experimental evaluation verify that GraphProbe can estimate the capability of graph representation learning. Remaking results have been concluded: GCN and WeightedGCN methods are relatively versatile methods achieving better results with respect to different tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs are a prevalent data structure and have been broadly in multiple fields [16]. For example, social networks[13, 23], molecular graph structures, and biological protein networks are universally modeled as graphs [5]. In recent decades, a lot of graph representations learning methods have been devised, ranging from matrix factorization methods [1, 6], random-walk based algorithms [14, 24], to the popular family of graph neural networks (GNN) [10, 15, 18, 22, 33, 35, 39]. The graph representation learning methods have demonstrated different performance on the classical downstream tasks, e.g. node classification, link prediction and graph classification. And the diverse graph representation learning methods have been extensively applied in multiple application areas, e.g. social network analysis [13, 23], recommender system [17, 18, 29], and protein classification [5, 9].\nThose graph representation learning methods tend to learn a mapping which embed nodes or (sub)graphs into a low-dimensional vectors by encoding relational information and structural information, and the learned embeddings are used for further downstream tasks [27]. However, there is no study to investigate and explain what kinds of graph properties have been actually coded in the learned embedding through different graph representation learning methods. It lacks a systematical evaluation to probe whether the graph inherent properties (e.g. graph proximity, graph structural information) are encoded into the learned node and graph representations with the popular but black-box graph representation learning methods."}, {"title": "2 RELATED WORK", "content": "Recently, knowledge probes have been proposed to probe knowledge in pre-trained language models (PLMs) such as ELMO [25] and BERT [11]. Probing methods are designed to understand and interpret what knowledge have been learned in the pre-trained language models, and they probe specific knowledge including linguistic knowledge Conneau et al. [7], Hewitt and Manning [19], Hou and Sachan [20], Shi et al. [32], and factual knowledge Petroni et al. [26]. For example, Hewitt et al. [19] proposed a structural probe to evaluate whether syntax trees have been encoded in a linear transformation of a neural network's word representation space. The probing results demonstrated that the transforms exist for the two PLMS ELMo and BERT. Petroni et al. proposed a LAMA benchmark to probe factual knowledge in PLMs using promt-based retrieval. The most similar work is [2] in which a probing framework has been proposed for quantify the chemical knowledge and molecular properties in graph representations for graph based neural networks. Different from previous studies, we propose a holistic graph probing benchmark to understand and interpret whether different types of inherent graph properties have been encoded into graph representation learning, from the node-wise, path-wise and structural-wise levels. The studied graph models covering a broad range of graph learning methods, ranging from random walk based methods to graph neural networks. In addition, we benchmark our"}, {"title": "2.1 Graph Representation Learning", "content": "Graph representation Learning methods map structural graph data into low-dimensional dense vectors, by capturing the graph topology structure, node-to-node relationships and other relevant information. Early methods for learning representations for nodes on graph-structured data were mainly matrix factorization methods based on dimension reduction [1, 6], and random walk approaches based on random walk statistics (e.g. DeepWalk [24] and Node2Vec [14]). For example, DeepWalk [24] was the first to input random walk paths into a skip-gram model to learn graph node embeddings. Node2vec[14] combined both breadth-first and depth-first walks to"}, {"title": "2.2 Knowledge Probe", "content": "Recently, knowledge probes have been proposed to probe knowledge in pre-trained language models (PLMs) such as ELMO [25] and BERT [11]. Probing methods are designed to understand and interpret what knowledge have been learned in the pre-trained language models, and they probe specific knowledge including linguistic knowledge Conneau et al. [7], Hewitt and Manning [19], Hou and Sachan [20], Shi et al. [32], and factual knowledge Petroni et al. [26]. For example, Hewitt et al. [19] proposed a structural probe to evaluate whether syntax trees have been encoded in a linear transformation of a neural network's word representation space. The probing results demonstrated that the transforms exist for the two PLMS ELMo and BERT. Petroni et al. proposed a LAMA benchmark to probe factual knowledge in PLMs using promt-based retrieval. The most similar work is [2] in which a probing framework has been proposed for quantify the chemical knowledge and molecular properties in graph representations for graph based neural networks. Different from previous studies, we propose a holistic graph probing benchmark to understand and interpret whether different types of inherent graph properties have been encoded into graph representation learning, from the node-wise, path-wise and structural-wise levels. The studied graph models covering a broad range of graph learning methods, ranging from random walk based methods to graph neural networks. In addition, we benchmark our"}, {"title": "3 THE GRAPH EMBEDDING PROBES", "content": "In order to investigate whether the inherent graph properties have been encoded into graph representation learning and reveal why different graph learning methods have different performance on downstream tasks, we we propose a knowledge probing framework (GraphProbe) to probe graph representation learning, and devise three knowledge probes from different levels, respectively node-wise, path-wise, and structure-wise levels. Figure 1 shows the overall architecture of our proposed GraphProbe. From the node-wise level, we devise the node centrality probes to investigate whether the influences of node properties and the local neighbourhood information can be encoded into representation learning of nodes for downstream tasks such as node classification and link prediction. From the path-wise level, we leverage the distance metrics of paths between two nodes to explore whether the path-wise topological information can be encoded into graph representation learning. From the structure-wise level, a graph structural probe is devised to investigate whether the structural information e.g. subgraph information is encoded into graph representation learning when performing the classical graph classification task. In the following parts, we firstly have a formal problem definition for the knowledge probing on graph representation leaning and then illustrate different probes in details.\nProblem Definition. We give a formal definition for the knowledge probing problem on graph representation learning. Given the constructed graph data \\(G = {V, E}\\), V is the set of nodes in G, and E is the set of the edges. \\(h_i\\) represents the feature representation of each node \\(v_i\\), and the dimension of the feature representation is d. The node representation X can be randomly initialized or initialized with meta features. With a graph-based model \\(M_k\\), we can obtain the learned node feature representation \\(h_v = M(G,X, \\theta_k)\\). The graph probe P is a function to estimate whether the learned representations encode the specified properties I, as defined in Equation 1.\n\\(S_k = P(h_v, I,T).\\)\n in which I denotes the investigated metrics used in the devised probes, and T denotes the applied downstream task. The probe score \\(S_k\\) estimates how well the learned representations from the graph-based model \\(M_k\\) encode information for the downstream task T with respect to I. In the following sections, we will describe the different probes in details."}, {"title": "3.1 The Node Centrality Probe", "content": "From the node-wise perspective, we leverage the node centrality properties of graph data as the estimated metrics I because the node centrality reflects the influence and importance of a node and to the extent captures the neighborhood information of a node [8]. We devise a node centrality probe to investigate whether the learned representations have encoded the node centrality when performing classical downstream tasks, e.g, node classification and link prediction.\nWe devise a supervised probe to compare the node centrality of two different nodes \\(v_i\\) and \\(v_j\\). We calculate the node centrality values C(\\(v_i\\)) and C(\\(v_j\\)) based on some graph node centrality metrics (e.g. eigenvector centrality and betweeness centrality), and map the pair-wise centrality comparison into a binary value \\(l_{ij}\\) as in Equation 2. The binary value l is used as the centrality label to train the node centrality probe \\(P_c(h_{v_i}, h_{v_j}, l, T)\\).\n\\(l_{ij} =\\begin{cases}1:  C(v_i) \\geq C(v_j) \\\\0  C(v_i) < C(v_j). \\end{cases}\\)\nFollowing the probe architecture in Pimentel et al. [27] which has been designed to reduce the information loss, we adopt a simple learning network two-layer perception (MLP)\u00b9 to learn the supervised probe: \\(P_k(h_{v_i},h_{v_j}) = MLP(h_{v_i} || h_{v_j})\\), and output the probability \\(p_{ij}\\) that the previous node \\(v_i\\) has a larger centrality than node \\(v_j\\). Cross-entropy is used as the loss function to train the supervised probe. Finally, the probe scores of the node centrality probe \\(S_k\\) for model \\(M_k\\) are the evaluation measure scores to measure the prediction from the probe based on the learned representations of the model \\(M_k\\) as follows:\n\\(S_k = Eval(P_k(h_{v_i}, h_{v_j}), l_{ij}).\\)\nClassical evaluation metrics can be used for Eval(e.g. F1-score, AUC, Accuracy). We report results with Accuracy and F1-score [28] in experiment section. A higher probe score \\(S_k\\) means the graph-based model \\(M_k\\) has greater ability to encode centrality information into the node representations. We explore two node centrality metrics for C(), respectively eigenvector centrality [4] and betweeness centrality [30].\nEigenvector Centrality. Eigenvector centrality[4] measures the importance of nodes in a network by exploiting adjacency and eigenvector matrices. Eigenvector centrality is a unique measure that satisfies certain natural principles for a ranking algorithm[3]. And Wang et al. [34] show that several recommendation algorithms based on node importance have been enhanced with the introduction of eigenvector centrality. A \u2208 Rnon is the adjacency matrix such that \\(a_{ij} = 1\\) if node i is connected to node j and \\(a_{ij} = 0\\) if not. The formal definition of the eigenvalue \u03bb and the eigenvector x is Ax = \u03bbx And the principal eigenvector \\(x_p = (x_1,...,x_n)^T\\) is the eigenvector corresponding to the eigenvalue with the largest modulus. The eigenvector centrality of node i can be computed as in Equation 4:\n\\(EC(i) = x_i, EC(n_i) = \\frac{1}{N(n_i)} \\sum_{n_j \\in N(n_i)}EC(n_j), \\sum_{i=1}^n x_i = A_{jj}.\\)\nwhere EC(\\(n_i\\)), EC(\\(n_j\\)) is the amount of influence that node \\(n_i\\), \\(n_j\\) carries, N(\\(n_i\\)) is the set of direct neighbors of node \\(n_i\\), and A is a constant.\nBetweenness Centrality. In graph theory, betweenness centrality[12] is a measure of centrality in a graph. For every pair of vertices in a connected graph, there exists at least one shortest path between the vertices such that either the number of edges that the path passes through (for unweighted graphs). The betweenness centrality for each vertex is the number of these shortest paths that pass through"}, {"title": "3.2 The Distance Probe", "content": "In graph data, the distance between two nodes can be estimated by the shortest path. From the path-wise perspective, we devise the distance probe to investigate whether the node representations encode the path-level distance information of graph structure. Following Hewitt and Manning [19], we devise the distance probe \\(P_d\\) as to estimate the differences between the grounded shortest paths of two nodes and the vector distance of two nodes' representations. Firstly, we define a family of inner products \\(h^T Wh\\) parameterized by any positive semi-definite, the symmetric matrix W \u2208 Smxm. Equivalently, we can view this as a linear transformation \\(B \u2208 R^{k\u00d7m}\\), such that \\(W = B^T B\\). The inner product \\(h^T Wh\\) is then equivalent to \\((Bh)^T (Bh)\\), the norm of h once transformed by B. Every inner product corresponds to a distance metric. Therefore, the definition of the distance between two nodes' embedding \\(h_i, h_j\\) is:\n\\(d_B(h_i, h_j)^2= (B(h_i - h_j))^T(B(h_i - h_j)).\\)\nThe distance probe \\(P_d\\) is trained to recreate the graph distance of node pairs in the training graph, and optimized through gradient descent in Equation 7. The parameters of our probe are exactly the matrix B, which we train to recreate the graph distance of node pairs in the training graph. Specifically, we approximate through gradient descent:\n\\(min_B \\sum_{i, j \\in G} | d_G(n_i, n_j) - d_B(h_i, h_j)^2 |.\\)\nwhere \\(d_G(n_i, n_j)\\) denotes the distance of the shortest path of two nodes \\(v_i\\), \\(v_j\\) in \\(G^2\\).\n\\(S_d^k = \\frac{1}{\\sum_{i,j \\in G} } | d_G(n_i, n_j) - d_B(h_i, h_j)^2 |.\\)\nThe probe score \\(S_d^k\\) represents the performance of the probe to recreate the graph distance. Therefore, Bigger \\(S_d^k\\) indicates better performance."}, {"title": "3.3 The Graph Structural Probe", "content": "We propose a structural probe to estimate whether the structural information has been encoded into the embedding of the entire graph. The graph representation \\(H^k\\) is constructed by aggregating the node representations through the readout operation:\n\\(H^k = readout(G, h_{v_i} \\vert v_i \\in V).\\)\nThe readout operation can obtain graph-level representation, e.g. sum, mean and max pooling\u00b3. We report the results with sum operation.\nIn order to extract the inherent structural information of graphs, we use the Weisfeiler-Lehman(WL) isomorphism test [31] as evaluation metrics to measure the similarity between graph structures."}, {"title": "4 EXPERIMENT", "content": "In order to evaluate our probing methods on different categories, we use some representative graph learning methods for experimental evaluation and report their results in Section 5 5. In general, we select graph learning methods from 4 categories, including random walk based graph embedding methods (e.g. Node2Vec [14] and DeepWalk[24]), basic graph neural networks (e.g. GCN [22] and GAT [33]), self-supervised graph learning methods (e.g. GCL [36] and VGAE [21]) and weighted graph learning methods (WGCN[38]). In addition, we add the control task, i.e. a naive two-layer MLP method to avoid potential performance bias due to the learning performance of our probes 6.\n\u2022 DeepWalk[24]: is one of random walk based graph embedding method. It adopts the local information obtained from truncated random walks to learn the latent representation of nodes via skip-Gram with hierarchical softmax.\n\u2022 Node2Vec[14]: is also a classic graph embedding method.\n\u2022 Chebyshev [10]: generalizes convolutional neural networks (CNNs) in the context of spectral graph theory and design fast localized convolutional filters on graphs for graph learning.\n\u2022 GCN [22]: performs semi-supervised learning on graphstructured data and introduces a simple and well-behaved layer-wise propagation rule for neural network models via the localized first-order approximation of spectral graph convolutions.\n\u2022 GAT [22]: incorporates masked self-attention layers on top of GCN-style methods.\n\u2022 GraphSAGE [15]: is an inductive framework for representation learning on large graphs which leverages node feature information to efficiently generate node embeddings for unseen data.\n\u2022 VGAE[21]: is unsupervised learning framework based on the variational auto-encoder.\n\u2022 GCL[36]: is a graph contrastive learning framework for unsupervised representation learning of graph data and devises four types of graph augmentations to incorporate various priors\n\u2022 WGCN[38]: considers the directional structural information for different nodes and proposes a GCN model with weighted structural features.\n\u2022 Control method (MLP): we use a simple two layer MLP model with ReLu after the first layer as the control method.\nWe perform probing evaluation on the representative graph learning methods for three classical downstream tasks of graph learning methods. In addition to learning with random initialization, we further study graph learning with meta-features initialized, and perform thorough analysis in Section 5."}, {"title": "4.1 Representative Graph Learning Methods", "content": "In order to evaluate our probing methods on different categories, we use some representative graph learning methods for experimental evaluation and report their results in Section 5 5. In general, we select graph learning methods from 4 categories, including random walk based graph embedding methods (e.g. Node2Vec [14] and DeepWalk[24]), basic graph neural networks (e.g. GCN [22] and GAT [33]), self-supervised graph learning methods (e.g. GCL [36] and VGAE [21]) and weighted graph learning methods (WGCN[38]). In addition, we add the control task, i.e. a naive two-layer MLP method to avoid potential performance bias due to the learning performance of our probes 6.\n\u2022 DeepWalk[24]: is one of random walk based graph embedding method. It adopts the local information obtained from truncated random walks to learn the latent representation of nodes via skip-Gram with hierarchical softmax.\n\u2022 Node2Vec[14]: is also a classic graph embedding method.\n\u2022 Chebyshev [10]: generalizes convolutional neural networks (CNNs) in the context of spectral graph theory and design fast localized convolutional filters on graphs for graph learning.\n\u2022 GCN [22]: performs semi-supervised learning on graphstructured data and introduces a simple and well-behaved layer-wise propagation rule for neural network models via the localized first-order approximation of spectral graph convolutions.\n\u2022 GAT [22]: incorporates masked self-attention layers on top of GCN-style methods.\n\u2022 GraphSAGE [15]: is an inductive framework for representation learning on large graphs which leverages node feature information to efficiently generate node embeddings for unseen data.\n\u2022 VGAE[21]: is unsupervised learning framework based on the variational auto-encoder.\n\u2022 GCL[36]: is a graph contrastive learning framework for unsupervised representation learning of graph data and devises four types of graph augmentations to incorporate various priors\n\u2022 WGCN[38]: considers the directional structural information for different nodes and proposes a GCN model with weighted structural features.\n\u2022 Control method (MLP): we use a simple two layer MLP model with ReLu after the first layer as the control method.\nWe perform probing evaluation on the representative graph learning methods for three classical downstream tasks of graph learning methods. In addition to learning with random initialization, we further study graph learning with meta-features initialized, and perform thorough analysis in Section 5."}, {"title": "4.2 Downstream Tasks and Datasets", "content": "We conduct performance evaluation on the classic downstream tasks of graph learning methods, including node classification [22], link prediction [29] and graph classification [35].\n\u2022 Node classification: This task is one of the most popular and widely used applications of graph learning models. The graph learning methods learn the node representations and classify nodes into different groups.\n\u2022 Link prediction: It is to predicate whether there exist a link between two nodes. For example, the recommendation problem in recommender system scenarios can be formulated as one link prediction task and construct a user-item interaction graph to predict the probability of linking a user to a item.\n\u2022 Graph classification: Its goal is to classify a whole graph into different categories. The main applications of graph classification are protein classification and chemical compound classification.\nWe adopt some benchmark datasets from different domains including citation networks, social networks, and Bio-chemical Networks. Table 1 shows the statistics and proprietress of the used benchmarks.\n\u2022 Citation Networks: Cora [23] is a dataset containing scientific papers categorized into seven classes. It commonly be used for node classification (transductive). The number of node classes is 7, and the number of node features is 1,433.\n\u2022 Social Networks: Flickr [37] is a image hosting and video hosting platform. Flickr dataset is used for node classification (inductive). The number of node classes is 7 and the number of node features is 500. In addition, we adopt two social networks datasets Yelp [29] and MovieLens [17] for Link prediction. Yelp includes 69,716 users and items as nodes, and 1,561,406 interaction records. MovieLens dataset includes 9,742 movies as nodes in the graph. It also includes over 100,836 ratings provided by around 610 users. The number of meta-features for items is 404.\n\u2022 Bio-Chemical Networks: Two datsets are used for graph classification, respectively Enzyme [5] and Mutag [9]. Enzyme dataset contains the proteins information which contribute to catalyzing chemical reactions in the body. Mutag dataset is a widely used toxicity dataset which helps assess the potential risks of exposure to various chemicals and compounds."}, {"title": "4.3 Experimental Setting", "content": "All datasets expect Yelp dataset follow use the splitting rules as previous studies. For the Yelp dataset, the data splitting ratio for the training set, validation set and test set is set as 7:2:1.\nFor Chebyshev, we use one layer structure. For GCN, GAT, GraphSAGE and MLP, we adopt the two layer structure. The length of walk for Node2Vec is set as 20. The size of the context window for the skip-gram and the walks per node are both 10. In order to make the walk unbiased, we also set the p which controls how likely the walk is to go back to the previous node, it is set as 1.0. For WGCN method, we use the same set as [38], three layers structure, compute the neighbors number for each node and use the weighted"}, {"title": "5 EXPERIMENTAL RESULTS", "content": "for node classification, we report transductive performance from Cora and inductive performance from Flickr. We can see that the best method on Cora (transductive) is GAT, and the best method is GCL on Flick (inductive) with respect to both ACC and F1 scores. Our centrality probes (EC and BC) have consistent evaluation results with the commonly used golden metrics, namely GAT ranks first. For the worst cases, our centrality prober has the same results (DeepWalk or MLP) with ACC and F1 on Flick dataset. On Cora, we find an interesting phenomenon that the simple MLP methods is better than GCL, Node2Vec and DeepWalk with respect to Acc and F1 while our centrality probing method demonstrate the worst methods are respectively MLP and Node2Vec. It might raise the question whether the final statistical metrics like accuracy and F1 actually reflect the capability of the graph representation learning. In comparison with the centrality probes, our distance probe are not very consistent with our centrality probes and the traditional metrics although they can find the worse cases. It is because the the distance probe is a path-wise probe while the node classification might emphasize to encode more topological information into graph representation learning. The path-wise probe method e.g. the distance probe might be inappropriate for knowledge probing in the node classification task as only encoding the path-wise information in graph representation learning cannot works well, e.g. random walk based methods like DeepWalk and Node2Vec.\nWhen initializing with meta-features, we have the similar results on Flick dataset, and the best method is GCL with respect to both"}, {"title": "5.1 Performance of Graph Learning Methods on Node Classification", "content": "In order to validate the knowledge probing performance of our methods with respect to the node classification task, we compare the probing scores of the representative graph learning methods with reference to commonly used metrics in node classification including Accuracy (ACC) and F1 scores. Table 2 shows the performance comparison for node classification. In additional to the absolute performance numbers, we add the overall rank numbers among the compared methods with brackets as the relative performance. For example, the highlighted number 79.9(1) indicates GAT obtains 0.799 accuracy on Cora dataset, and rank first among all compared methods. In all tables, we use bold font to highlight the best performance and underlines to indicate the worst performance"}, {"title": "5.2 Performance of Graph Learning Methods on Link Prediction", "content": "To evaluate the knowledge probing performance of our methods with respect to the link prediction task, we compare the probing scores of the representative graph learning methods with reference to commonly used metrics in the link prediction task on Yelp and Movielens dataset, including AUC and F1 scores. We evaluate both the centrality probes and the distance probes, and the performance comparison results on Yelp and MovieLens for link prediction are reported in Table 3. On Yelp dataset, the best methods is GCN and the worst method is DeepWalk except MLP, with respect to both AUC and F1 scores. The distance probe has consistent results with"}, {"title": "5.3 Performance of Graph Learning Methods on Graph Classification", "content": "We compare the structure probing scores of the representative graph learning methods with reference to commonly used metrics in the graph classification task on MUTAG and ENZYMES dataset, including accuracy (ACC) and F1 scores. Table 3 demonstrates the performance results on graph classification. On MUTAG dataset, the"}, {"title": "5.4 Visualization Analysis", "content": "In order to further compare the overall performance of different methods for different information embedding capabilities, we compute the ranking of the 9 representative methods and the MLP baseline for each probe and use radar charts to visualize their capacities in Figure 2 (I indicates the inductive and T indicates transductive, and M the Meta) . GCN and WGCN have better performance in most probing aspects in comparison with other methods. Although having the same capacities of aggregating the neighbor information, VGAE has the sub-optimal performance. Furthermore, they all hardly rely on meta informations. GCL has better performance on Node Classification (Inductive), GAT has better performance on Node Classification(transductive). Chebyshev might be better used with meta information. Node2Vec has the worst performance on all aspects.\nWe also draw the skyline plot for finding the methods on different downstream tasks which can not be dominated by other methods in Figure 3. We can investigate the joint-abilities of graph learning methods on different downstream tasks. In Link PredictionNode Classification tasks, GAT, VGAE and GCN has better performance that can not be dominated. In Graph Classification-Node Classification, GAT and GCN performs well. GAT has better graph classification capabilities and GCN has better node classification"}, {"title": "5.5 Effects of Parameters", "content": "The only hyper parameters used in our probes is the path parameter. It controls the shortest path that our probe can detect in the graph. We calculate the Correlations with the F1 scores in different path, it shows that most of our best path are between 3 and 4. We use the max score of the path for each datasets"}, {"title": "6 CONCLUSION", "content": "In this paper, we proposed a graph probing benchmark for the representative graph learning methods. Diverse probes at three different levels (node-wise, path-wise and structure-wise) are devised to investigate and interpret weather the graph properties from different levels are encoded into representation learning of the seven representative graph neural networks based methods. We conduct system evaluation and thorough analysis to investigate what kind of information have been encoded and which methods have competitive performance with different targeted downstream tasks. The experimental evaluation validate the effectiveness of GraphProbe. Furthermore, We conclude some remarking findings: GAT is superior in the node classification; GCN and WGCN are relatively versatile methods achieving better results with respect to different tasks. The benchmark codes and resources will be public after acceptance."}]}