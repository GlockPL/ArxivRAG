{"title": "Large-scale Multi-objective Feature Selection:\nA Multi-phase Search Space Shrinking Approach", "authors": ["Azam Asilian Bidgoli", "Shahryar Rahnamayan"], "abstract": "Feature selection is a crucial step in machine learning, especially for high-\ndimensional datasets, where irrelevant and redundant features can degrade\nmodel performance and increase computational costs. This paper proposes a\nnovel large-scale multi-objective evolutionary algorithm based on the search\nspace shrinking, termed LMSSS, to tackle the challenges of feature selec-\ntion particularly as a sparse optimization problem. The method includes a\nshrinking scheme to reduce dimensionality of the search space by eliminating\nirrelevant features before the main evolutionary process. This is achieved\nthrough a ranking-based filtering method that evaluates features based on\ntheir correlation with class labels and frequency in an initial, cost-effective\nevolutionary process. Additionally, a smart crossover scheme based on voting\nbetween parent solutions is introduced, giving higher weight to the parent\nwith better classification accuracy. An intelligent mutation process is also\ndesigned to target features prematurely excluded from the population, ensur-\ning they are evaluated in combination with other features. These integrated\ntechniques allow the evolutionary process to explore the search space more\nefficiently and effectively, addressing the sparse and high-dimensional nature\nof large-scale feature selection problems. The effectiveness of the proposed\nalgorithm is demonstrated through comprehensive experiments on 15 large-\nscale datasets, showcasing its potential to identify more accurate feature\nsubsets compared to state-of-the-art large-scale feature selection algorithms.\nThese results highlight LMSSS's capability to improve model performance\nand computational efficiency, setting a new benchmark in the field.", "sections": [{"title": "1. Introduction", "content": "Feature selection is a critical task in machine learning and data analysis,\nas it involves identifying the most relevant features for building efficient and\naccurate predictive models [1]. The importance of feature selection stems\nfrom its ability to enhance model performance by removing redundant or ir-\nrelevant features, thus reducing overfitting, improving model interpretability,\nand decreasing computational cost [2, 3]. In real-world applications, datasets\noften contain a multitude of features, making it imperative to employ effective\nfeature selection techniques. Accordingly, feature selection can be defined as\na large-scale optimization problem to find a set of high-quality features for a\nspecific machine-learning task such as classification [4, 5], clustering [6], and\nsegmentation [7]. Multi-objective feature selection [8] has gained prominence\nbecause it simultaneously optimizes multiple criteria, such as maximizing\naccuracy while minimizing the number of selected features, leading to more\nbalanced and practical set of features. Evolutionary algorithms (EAs) [9, 10]\nhave emerged as a promising approach for multi-objective feature selection\ndue to their inherent capability to explore large and complex search spaces\nefficiently [11, 12]. EAs mimic the process of natural evolution, employing\nmechanisms such as selection, crossover, and mutation to evolve solutions\nover successive generations. Their population-based nature allows for the\nparallel exploration of multiple regions in the search space, increasing the\nlikelihood of discovering optimal or near-optimal solutions. Despite signifi-\ncant advancements in the application of evolutionary algorithms for feature\nselection, there remain challenges that necessitate further research and devel-\nopment. One major drawback is the large-scale search space, especially when\ndealing with high-dimensional datasets. As the number of features increases,\nthe search space grows exponentially, leading to a combinatorial explosion\nthat can overwhelm the search process [13, 14]. This vast search space makes\nit difficult to identify optimal feature subsets within a reasonable timeframe,\noften resulting in suboptimal or computationally expensive solutions [15].\nAdditionally, high-dimensional search spaces exacerbate the problem of the\ncurse of dimensionality, where the distance between data points becomes less\ninformative, making it harder for the algorithm to distinguish between less\nand more prominent feature subsets. This can be more challenging where the\nfeature selection is treated as a sparse optimization problem [16] where most\nvariables (i.e., feature statuses) are zero. In such cases, every component of\noptimization process including the initialization, generative operators, and"}, {"title": "2. Multi-objective Feature Selection", "content": "Multi-objective optimization involves optimizing two or more conflicting\nobjectives simultaneously [20]. In the context of feature selection, multi-\nobjective optimization aims to identify a subset of features that balances\nmultiple criteria, such as minimizing the number of selected features while\nmaximizing the classification performance [21]. Dominance is a key concept\nin multi-objective optimization because it helps compare various solutions\nbased on their performance across all objectives. A solution is said to dom-\ninate another if it is at least as good in all objectives and strictly better in\nat least one objective. If x = (x1,x2,...,xd) and \u0155 = (x1, x2, ..., xa) are two\nvectors in a minimization problem search space, a dominates x (x < \u0453) if\nand only if\n\\begin{equation}\nVi \\in \\{1, 2, ..., M\\}, f_i(x) \\leq f_i(\\hat{x})^\\\\\n\\exists j \\in \\{1, 2, ..., M\\} : f_j(x) < f_j(\\hat{x})\n\\end{equation}\nNon-Dominated Sorting (NDS) [20] is an algorithm used to rank solutions\ninto different levels based on dominance. Solutions that are not dominated\nby any other solutions form the first Pareto front. Solutions that are only\ndominated by those in the first front form the second Pareto front, and so\non. NDS helps in identifying and preserving a diverse set of optimal trade-\noffs between the objectives. This is crucial in multi-objective optimization\nbecause it provides decision-makers with a range of choices, each representing\na different balance of the objectives. By using NDS, we can identify the best\npossible solutions that offer a compromise between competing objectives.\nIn multi-objective feature selection, one common objective is the ratio of\nselected features, which measures the proportion of features retained from\nthe original feature set. This objective is crucial because a smaller subset of\nfeatures can lead to simpler models that are easier to interpret and faster to\nexecute. The ratio of features (f\u2081) can be defined as:\n\\begin{equation}\nf1 = \\frac{\\text{Number of selected features}}{\\text{Total number of features}}\n\\end{equation}\nMinimizing f\u2081 helps in selecting a smaller number of features, which can\nreduce the risk of overfitting and improve the generalizability of the model.\nTherefore, the goal is to minimize f\u2081 to achieve a more compact feature set."}, {"title": "3. Related Works", "content": "To address large-scale feature selection, numerous studies have been con-\nducted in recent years including several review papers [22, 23, 24, 25]. Some\nresearch has focused on sparse optimization problems and applied their meth-\nods to large-scale feature selection as a case study. One notable algorithm\nis SparseEA [26], which proposes an evolutionary algorithm for large-scale\nsparse multi-objective optimization problems. This algorithm introduces a\nnovel population initialization strategy by calculating decision variable scores\nand utilizing genetic operators, considering the sparse nature of Pareto opti-\nmal solutions to maintain solution sparsity.\nIn [27], an evolutionary pattern mining approach is proposed to identify\nthe maximum and minimum candidate sets of nonzero variables in Pareto-\noptimal solutions. This method limits the dimensions for generating offspring"}, {"title": "4. Proposed Method", "content": "In this section, the details of the proposed multi-objective framework\nto select the optimal set of features from a large-scale dataset are described.\nThe algorithm contains two phases: a shrinking process and a multi-objective\nevolutionary process. In the shrinking process, the search space for the evo-\nlutionary algorithm is reduced to alleviate the challenges of large-scale space\nexploration.\nMulti-objective feature selection is inherently an imbalanced optimization\nproblem in terms of its objectives, as increasing the accuracy of the classi-\nfication is usually more difficult than reducing the number of features. To\ntackle this challenge and eliminate irrelevant and redundant features while"}, {"title": "4.1. Detecting Highly Correlated Features", "content": "The first level of elimination filters out features with low correlation to\nthe class label. In large-scale datasets, many features can be irrelevant,\nand retaining them for the optimization process is considered a waste of\nthe optimize's resources for exploring the search space. Therefore, using a\nfilter criterion like Maximal Information Coefficient (MIC) [39] can eliminate\nthese features, preventing the evolutionary optimizer from wasting resources\non finding optimal solutions. This phase specifically focuses on the primary\nobjective of selecting top-ranked features based on their correlation with the\nclass label.\nMutual Information (MI) [40] is a common method for assessing the rele-\nvance between two variables, such as X and Y. It is defined by the following\nequation:\n\\begin{equation}\nMI(X; Y) = \\sum_{X \\in X} \\sum_{Y \\in Y}p(x, y) \\log \\frac{p(x, y)}{p(x)p(y)}\n\\end{equation}\nIn this equation, p(x), p(y), and p(x, y) represent the probability distri-\nbutions of x and y, and their joint distribution, respectively. When X and\nY are two features, MI(X; Y) quantifies the amount of information shared\nbetween them. If X is a feature and Y represents the class labels, MI(X; Y)\nindicates the degree of dependence between feature X and the class labels\nY.\nMIC is a newly developed statistical measure that uses binning to apply\nMI to continuous variables. By segmenting the variable values into different\ngrids and normalizing MI, MIC can more accurately measure the relation-\nships between variables.\nThe calculation of MIC is briefly explained as follows: The original space\nG, comprising the values of variables X and Y, is divided into multiple p-by-q\ngrids. The characteristic matrix M(G)p,q represents the highest normalized\nMI of G within the p \u00d7 q partitions, as expressed in the following equation:\n\\begin{equation}\nM(G)_{p,q} = \\frac{max(MI)}{\\log \\min\\{p, q\\}}\n\\end{equation}\nmax(MI) denotes the maximum MI of G across all p \u00d7 q partitions. MIC is\nthen defined as:\n\\begin{equation}\nMIC = \\max_{0<p>q<B(n)}\\{M(G)_{p,q}\\} \\qquad B(n) = n^{0.6},\n\\end{equation}"}, {"title": "4.2. Detecting Highly Frequent Features", "content": "The selected features from the previous phase will serve as the input to a\nlightweight evolutionary process. In fact, the frequent features in several runs\nof an evolutionary process can be considered prominent features. Therefore,\na portion of the budget for the entire feature selection process is allocated\nto several runs of an evolutionary algorithm to assess the frequency of each\nfeature in the final Pareto front. Despite the stochastic nature of evolutionary\nalgorithms, when a feature is consistently selected in different runs, it can\nindicate the importance of the corresponding feature. It is important to note\nthat this algorithm is executed for a limited number of iterations (considered\nas a lightweight evolutionary process), as this is merely a filtering phase and\nnot the final feature selection phase. The frequency will be used as a metric\nfor ranking the features, and a few iterations are sufficient to compute this\nfrequency. Moreover, in most evolutionary processes, the algorithm usually\nidentifies semi-optimal solutions within just a few iterations after starting.\nTherefore, the evolutionary algorithm described in the next subsection will\nbe applied R times in t iterations. Suppose that the size of the population\nis popsize. Hence, at the end of the process, if we gather all solutions in the\nfinal population of each run, R \u00d7 popsize feature subsets will be obtained.\nTo compute the frequency of each feature, we focus on using the frequency\nas a metric to rank the features. Since feature subsets with better classifica-\ntion performance are of interest for shrinking space, we consider the top nfs%\nof solutions from all final populations across different runs to determine the\nfrequency of each feature. For each feature, we count its frequency over all"}, {"title": "4.3. NDS-Based Feature Ranking", "content": "All the fs features from the filter selection phase can be ranked based\non two criteria from the two previous steps: MIC and frequency. The NDS\nalgorithm is applied to rank these features according to these criteria. Higher\nranks correspond to features with high values in both criteria. To select a\nset of top-ranked features for the evolutionary process, we start from the\nfirst Front to choose the desired number of features (i.e., NNDS), creating the\nshrunk space."}, {"title": "4.4. Evolutionary Process", "content": "In this section, the details of the evolutionary process applied to the\ntop-ranked features from the NDS-based ranking will be discussed. The\noutput of this phase is the final Pareto front of the proposed multi-objective\nframework. The shrunk space will facilitate a more targeted search process\nfor the optimizer. This not only makes exploration easier but also increases\nthe likelihood of identifying high-quality feature subsets for classification.\nMoreover, a smaller search space can result in smaller subsets, which further\nenhances the efficiency of the optimization process."}, {"title": "4.4.1. Initialization", "content": "Similar to every population-based evolutionary algorithm, the process be-\ngins with an initial population containing individuals represented by binary\nencoding. Each cell of the binary vectors represents the status of a feature,"}, {"title": "4.4.2. Generative Operators", "content": "The next important component of the evolutionary process is to gener-\nate a set of diverse and high-quality candidate solutions at each generation.\nOne of the challenges in large-scale binary optimization, particularly feature\nselection, is the degradation of population diversity after a few iterations.\nSome features are removed in the first iterations and never get a chance to\nre-enter the population, while others remain in the population, and regular\ncrossover methods, such as uniform crossover [41], simply exchange them be-\ntween different individuals. This is one of the reasons why redundancy in the\npopulation of the feature selection problem is relatively high. Therefore, the\neffectiveness of the generative operators can play a significant role in advanc-\ning the evolutionary process towards promising regions of the search space.\nBelow, the details of the proposed crossover and mutation in order to tackle\nthese challenges are described.\nCrossover: To generate a new offspring from two parents, we propose\na novel crossover operator based on a voting mechanism between the two\nparents. This operator works as follows: two parents, P\u2081 and P2, are ran-\ndomly selected from the population. To prioritize individuals with better\nclassification accuracy, we assume P\u2081 has better features and a lower classifi-\ncation error (if P2 is superior, we swap them). The feature selection encoding\nrepresents P\u2081 and P2 as binary vectors corresponding to the total number of\nfeatures."}, {"title": "Mutation", "content": "Although, the mutation operator used is the flip bit [42], but\nan additional mutation mechanism is considered to give a chance to features\nthat were excluded in the initial generations and never had the opportunity to"}, {"title": "4.4.3. Selection", "content": "The selection phase of the proposed framework is similar to NSGA-II.\nTherefore, the NDS (Non-dominated Sorting) and crowding distance will be\napplied to select the top-ranked candidate solutions for the next phase.\nIt is worth mentioning that the proposed evolutionary process has been\nemployed for both steps: lightweight evolutionary process for selecting the\nhighly frequent features and the final evolutionary process applied to the\ntop-ranked features for the final output."}, {"title": "5. Experiments", "content": "5.1. Benchmark Techniques\nThe proposed algorithm, LMSSS is compared with the state-of-the-art\nhigh-dimensional multi-objective feature selection algorithms. They include,\nDAEA [31], SparseEA [26], PM\u039c\u039f\u0395\u0391 [27], SLMEA [28] SMMOEA [32], and\nMSKEA [29]. Some algorithms such as SparseEA is generally designed for\nlarge-scale multi-objective optimization problems but can be employed for\nbinary optimization problems (e.g., feature selection). A brief desciption of\neach of these algorithms is given in Section III. All seven algorithms were\napplied to 15 large-scale datasets represented in Table 1. These 15 datasets"}, {"title": "5.2. Parameter Settings", "content": "For each dataset, every algorithm was executed independently 31 times,\nwith the data randomly divided into a training set (70%) and a test set\n(30%). In order to provide a fair comparison, the same training and test sets\nare considered for all algorithms. During the optimization process, k-nearest\nneighbors (k-NN) [46] with one-leave-out cross validation on the training set\nis employed to compute the classification error rate. The value of k in k-\nNN is set to five to strike a balance between accuracy and efficiency. The\ntotal number of generations for all algorithms is set to 100 times while the\npopulation size is set to 200. For LMSSS, 50% of the budget is allocated to\na lightweight evolutionary process, which is independently repeated 5 times\nwith 10% of the budget (i.e., 10 iterations) for each run. The remaining 50%\nof the budget is allocated to the main evolutionary process on the shrunk\nspace to generate the final results. The parameters NMIC and NNDS are\nset to 1000 and 200, respectively. These values were selected based on our\nexperiments, where smaller values for even the smallest datasets were too\nmuch filtering and larger values for larger datasets did not result in better\noutputs. Thus, we maintained constant values for the number of features in\nthe shrunk space. The parameter nfs is set to 50% of the total number of\nsolutions on all final soltuions from the 5 runs of the lightweight evolutionary\nprocess.\nThe parameter nm is set to 10% of the total number of iterations, and\nthe mutation rate is set to 1/D, where D is the total number of features in\na dataset. The parameter pr is set to 0.7. The crossover rate for regular\nNSGA-II is set to 0.5. For each competitor algorithm, parameter values are\ntaken from their respective reference papers."}, {"title": "5.3. Results", "content": "5.3.1. Analysis on Multi-objective Evaluation Metrics\nTable 2 displays the HV values for 7 benchmark algorithms on the test set\nof 15 large-scale datasets. In the final row of the table, you can see the overall\nrank for each algorithm. Notably, LMSSS achieves the highest rank among\nall benchmark algorithms, while MSKEA takes the last position. DAEA\nsecures the second rank, with 6 losses and 9 ties compared to LMSSS. It's\nworth noting that LMSSS consistently outperforms other algorithms, never\nlosing in comparison. In other words, its HV is either higher or equal to other\nalgorithms across all benchmarks indicating more spread and distribution"}, {"title": "5.3.2. Analysis on Feature Selection Performance", "content": "In addition to multi-objective evaluation metric, the minimum error (e.i.,\nthe first objective) for LMSSS and other large-scale feature selection algo-\nrithms on the test Pareto front has been presented in Table 4. As the final\nrow shows, LMSSS achieved the top rank against all algorithms on dataset\nbenchmarks. There is no significant loss in MCE for this algorithm. Multi-\nobjective feature selection is a non-symmetric optimization problem, where\nclassification error is usually more important and more difficult to mini-\nmize. Therefore, while the algorithm tries to decrease the number of features,\nachieving the highest classification accuracy is a serious challenge for this cat-\negory of algorithms. This challenge is especially pronounced in large-scale\nfeature selection, where finding a small set of features with high accuracy is\ncrucial. From this table, LMSSS was more successful in this regard, finding\nmore accurate feature subsets for the classification task. In some cases, such\nas the Lung and 11Tumor datasets, the difference between LMSSS and other\nalgorithms is significantly large. This indicates the capability of the algo-\nrithm to handle an increasing number of features. Among the competitors,\nDAEA is in the second position with 3 ties and 12 losses compared to LMSSS.\nAdditionally, as presented, the average standard deviation of the results from\ndifferent runs using LMSSS is significantly less than that of other methods.\nIn summery, LMSSS wins 78 and draws 12 out of the 90 comparisons for\nthe test MCE results. The obvious reason behind this is the selection of\ntop-ranked features in the first phase based on their correlation with the la-\nbel. This results in removing a large set of irrelevant features and shrinking\nthe search space for the optimizer to find a more accurate set of features in\nthe second phase. The method's ability to consistently reduce the feature\nset while maintaining or improving classification accuracy underscores its ef-\nficiency in handling high-dimensional data. This can be evident from the"}, {"title": "5.3.3. Distributions of Non-dominated Solutions", "content": "To provide an intuitive analysis, the distributions of the non-dominated\nsolutions from the run with median HV of each algorithm are illustrated\nin Fig. 4. The first and second rows of these figures display all the non-\ndominated solutions for some sample datsets on the training sets and test\nsets, respectively. In other words, the solutions shown in the second row are\nderived from those in the first row with applying the NDS algorithm on re-\nsultant points. On the 11Tumor dataset, LMSSS significantly outperformed\nother algorithms by achieving a Pareto front on the training set with a very\nsmall set of features (i.e., less than 0.005 percent of the total number of\nfeatures) and a very low classification error. Similarly, on the test set, the\nsolution with the lowest error using a feature set comprising only 0.0035 per-\ncent of the total number of features belongs to LMSSS, while other datasets\ncould not achieve this even with a higher number of features. Moreover, al-\ngorithms like SparseEA or PMMOEA reached the error of 0.2 with subsets\nsized between 0.01 to 0.015 percent of the total features, whereas LMSSS\nachieved the same error rate with only 0.001 percent of the features.\nSimilarly, on the CLL_SUB dataset, the feature set with the minimum\nerror belongs to LMSSS, using a set of only 6 features. In terms of the number\nof features, all algorithms achieved a similar range (i.e., 1 to 12 features\nout of 11,000 features). However, in terms of the accuracy of the selected\nfeatures, LMSSS surpassed all algorithms, especially on the test set, with\na significant margin. A similar pattern can be observed on the Carcinoma\ndataset, where the range of resultant feature subsets for most algorithms is\nfrom 1 to 26 features out of 9,182 total features, while LMSSS achieved the\nbest classification accuracy with 19 features. None of the other algorithms\ncould reduce the error as much. This is also evident on the test set, where\nLMSSS obtained a very small set of features in the range of 1 to 19 features\nwith a minimum error of 5%, whereas the other algorithms achieved an error\nof at least 20% with this number of features. The final illustrated Pareto\nfront is on the Lung dataset, which has a total of 12,600 features. Although\nthe distribution over the number of features is similar for all algorithms,\nLMSSS again obtained solutions with the minimum classification error using"}, {"title": "5.3.4. Running Time Analysis", "content": "Fig. 5 shows the time consumed by each algorithm on different datasets.\nAs these algorithms are designed for large-scale feature selection, they are\ntailored to solve large-scale problems, which can be very time-consuming.\nTherefore, analyzing the time required is crucial. As illustrated, in most\ncases, LMSSS consumes less time than all other algorithms. The obvious\nreason, as discussed earlier, is the two-phase scheme in which many irrelevant\nfeatures are eliminated in the first phase. The time-consuming aspect of\nthese algorithms is the fitness evaluation (i.e., classification). In LMSSS, the\nshrunk space allows for evaluating a smaller set of features, resulting in less\noverall time. It is worth noting that the first phase is less computationally\nexpensive and does not significantly affect the overall time. However, all\nphases have been considered in the time calculation. Thus, shrinking the\nsearch space in large-scale feature selection is not only efficient in finding\nmore accurate solutions but also accelerates the convergence of the algorithm.\nIn this ranking, MSKEA is in the second position, while SMMOEA is in the\nlast position, resulting in very long run times."}, {"title": "5.3.5. Analysis on Component Contribution", "content": "LMSSS consists of three main components: the initialization strategy,\nthe shrinking phase and the binary crossover and mutation operators. As-\nsessing the effectiveness of each component is crucial. To demonstrate this,\nthree variant algorithms were created to highlight the contributions of these\nthree components. Each time a new component will be added to the pre-\nvious version. With these variations, the impact of each component can be\ninvestigated by checking the improvement of each algorithm compared to the\nprevious variant. Tables 5, 6 and 7 present the values of HV, IGD, and MCE\nfor all these variants. The last rows represent the ranking across all variants.\nFor each column, the comparison markers indicate the significance of the\ncorresponding algorithm compared to the previous column, highlighting the\nefficiency of adding the corresponding component.\nNSGA-II: As the selection scheme of the proposed method is similar\nto NSGA-II, the base algorithm is the original version of NSGA-II, utilizing\nuniform crossover and bit flip mutation. This algorithm is applied to the\nentire feature set, without any shrinking strategy. Initialization follows the\nstandard uniform method, generating an equal number of Os and 1s in each\nindividual, resulting in the selection of almost 50% of the total features per\nindividual. This preserves the individual-level uniformity.\nInit-NSGA-II: The second version incorporates an initialization strat-\negy into NSGA-II to investigate the impact of the initialization component.\nFor each individual, a random integer number from 1 to the total number\nof features is selected to represent the number of chosen features. This ap-\nproach ensures uniformity at both the individual and population levels. From\nTables 5 and 6 it is evident that initialization has a significant impact on the"}, {"title": "6. Conclusions", "content": "This paper presented LMSSS, a novel multi-objective evolutionary algo-\nrithm designed to tackle the challenges associated with large-scale feature\nselection. Our approach integrates several key strategies to enhance the\nefficiency and effectiveness of the evolutionary process in high-dimensional\nsearch spaces. First, we introduced a shrinking scheme that reduces the\ndimensionality of the problem by eliminating irrelevant features before the\nmain evolutionary process. This is achieved through a ranking-based filtering\nmethod that evaluates features based on their correlation with class labels\nand their frequency in an initial, cost-effective evolutionary process. Second,\na smart crossover and mutation scheme based on voting between parent so-\nlutions was developed. The crossover mechanism makes decisions about each\nfeature based on the agreement or disagreement between the parents, with"}]}