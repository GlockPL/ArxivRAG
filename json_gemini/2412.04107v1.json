{"title": "Pre-train, Align, and Disentangle: Empowering Sequential Recommendation with Large Language Models", "authors": ["Yuhao Wang", "Junwei Pan", "Xiangyu Zhao", "Pengyue Jia", "Wanyu Wang", "Yuan Wang", "Yue Liu", "Dapeng Liu", "Jie Jiang"], "abstract": "Sequential recommendation (SR) aims to model the sequential de- pendencies in users' historical interactions to better capture their evolving interests. However, existing SR approaches primarily rely on collaborative data, which leads to limitations such as the cold- start problem and sub-optimal performance. Meanwhile, despite the success of large language models (LLMs), their application in industrial recommender systems is hindered by high inference la- tency, inability to capture all distribution statistics and catastrophic forgetting. To this end, we propose a novel Pre-train, Align, and Disentangle (PAD) paradigm to empower recommendation models with LLMs. Specifically, we first pre-train both the SR and LLM models to get collaborative and textual embeddings. Next, a char- acteristic recommendation-anchored alignment loss is proposed using multi-kernel maximum mean discrepancy with Gaussian ker- nels. Finally, a triple-experts architecture, consisting aligned and modality-specific experts with disentangled embeddings, is fine- tuned in a frequency-aware manner. Experiments conducted on three public datasets demonstrate the effectiveness of PAD, show- ing significant improvements and compatibility with various SR backbone models, especially on cold items. The implementation code and datasets will be publicly available.", "sections": [{"title": "1 INTRODUCTION", "content": "With the explosive growth of the interactions with web applications and platforms, research on sequential recommender(SR) system [19, 21, 43] has garnered increasing attention. These models aim to capture the sequential dependencies in user behavior sequences, modeling both long-term and short-term preferences. However, most existing SR methods rely exclusively on tabular data or ID-based features as inputs, which often leads to challenges such as the cold-start problem [39], resulting in suboptimal performance, particularly for less frequent users, items, and scenarios.\nTo address the limitations of conventional sequential recom- mendation (SR) models, recent efforts have drawn inspiration from the success of large language models (LLMs) in understanding se- mantics and processing natural language. Several approaches have sought to enhance SR by leveraging LLM capabilities. On the one hand, methods like TALLRec [1] and LC-Rec [54] explicitly adapt LLMs for recommendation by instruction tuning. They formulate the sequential recommendation task as text generation, i.e., predict- ing the title of the next item based on a user's historical interactions. Afterward, LLaRA [27], and COLLM [53] also leverage LLM as rec- ommender and they propose to concatenating the token embedding with collaborative embedding, which enables LLM to comprehend collaborative information.\nOn the other hand, models such as CTRL [26] and Flip [47] propose aligning LLM embeddings with collaborative embeddings using contrastive learning, employing an InfoNCE alignment loss based on non-characteristic cosine or linear kernels. Besides, Taobao [39] and AlignRec [30] propose to pre-train multi-modal represen- tations and incorporates them into recommendation. Despite these advances, three practical challenges remain to be addressed:"}, {"title": "2 PRELIMINARY", "content": "In this section, we first illustrate the problem formulation and introduce maximum mean discrepancy."}, {"title": "2.1 Problem Formulation", "content": "In our problem setting, we obtain a semantic domain $D_{text} = {({h^t_i}, x_i, y_i)}_{i=1}^{n}$ and a collaborative domain $D_{rec} = {({h^r_i}, x_i, y_i)}_{i=1}^{n}$ with n samples each. Specifically, ${h^t_i}$, ${h^r_i}$, $x_i$, $x_i$, and $y_i$ denotes behavioral item sequence in semantic embedding space, behavioral item sequence in collaborative embedding space, target item in semantic embedding space, target item in collaborative embedding space, and true label. The probability distributions characterized by these two domains are P and Q.\nGiven a user's historical interaction sequence with length I con- sisting of item ID, it is first sorted by timestamps in an ascending order and mapped into collaborative embedding ${h_i}, i = 1... l$. Then sequential recommender system (SRS) $f_\\theta$ usually takes it as input and outputs user embedding, which is multiplied by target item embedding $x$ through dot product to obtain the prediction logit. Finally, the binary cross entropy (BCE) loss is often adopted.\n$\\min L = \\frac{1}{l} \\sum_{i=1}^l BCE (f_\\theta ({h_i}, x_i), Y_i)$ (1)"}, {"title": "2.2 Maximum Mean Discrepancy", "content": "Kernel function $k( , )$ characterizes how to measure similarity be- tween samples. The idea of kernel mean or mean embedding is to represent the distribution in the reproducing kernel Hilbert space (RKHS) [33]. Specifically, considering a symmetric, positive-definite kernel k and its corresponding unique RKHS Hk, each distribution P(X) is mapped into Hk through $\\mu_p \\equiv E[k(X,\\cdot)] = E[\\varphi(X)]$ where $\\varphi$ is the feature map.\nFurthermore, the idea of Maximum Mean Discrepancy (MMD) is to represent the distance between distributions as the distance between kernel mean of features [38]. One can define the distance between probability distributions P and Q in RKHS $H_k$ as:\n$D_k(P,Q) \\triangleq ||\\mu_p \u2013 \\mu_Q||_{H_k},$ (2)\nwhere $|| ||_{H_k}$ is the norm of Hr and this metric is known as MMD [38]. As illustrated by previous works [15], the choice of the kernel will affect the power of the two-sample test, whose null hy- pothesis is P = Q. If the positive definite kernel k is characteristic, i.e., the mapping $P\\leftrightarrow \\mu_p \\in H_k$ is injective, the kernel mean is proven to preserve all information of the distribution P(X) [12]. Besides, the following multi-kernel MMD (MK-MMD) [14] is intro- duced to improve test power where k is a kernel function in the function space. Formally,\n$\\mathcal{K} = {k = \\sum_{u=1}^m \\beta_u k_u, \\sum_{u=1}^m \\beta_u = d, \\beta_u \\geq 0, \\forall u}$ (3)\nfor some d \u2265 0 where ${k_u}_{u=1}^m$ are positive definite single kernels."}, {"title": "3 METHOD", "content": "In this section, we first briefly present the three phases in Sec. 3.1, and then provide a detailed description of the align and disentangle phase in Sec. 3.2 and Sec. 3.3, respectively."}, {"title": "3.1 Overview", "content": "The overview process of our model is depicted in Fig. 1, which consists of the following three phases:\nPhase 1: LLM & Recommendation Pre-train. First, the textual embedding is generated from the textual information of items like titles and descriptions, and then it is frozen as fixed semantic knowl- edge. We adopt LLM2Vec [2] which transforms the primary large language models (LLMs) with decoder-only structure like Llama3 into powerful text encoders through fine-tuning. Next, the recom- mendation (ID) expert is pre-trained only on item ID to capture collaborative information using a SASRec model.\nPhase 2: Characteristic & Rec-Anchored Alignment. After obtaining the textual and collaborative embeddings from the pre- trained LLM and recommendation model, we will align these two embeddings with each training sample. In particular, we build an alignment expert, which takes both textual and collaborative em- bedding as inputs, and adopts MK-MMD [12] as the alignment loss with characteristic kernels, thus being able to preserve all in- formation about the distribution [13]. In addition, we introduce a Binary Cross Entropy loss regarding the recommendation label so as to present the collaborative embeddings from catastrophic forgetting [24] during alignment.\nPhase 3: Collaborative Fine-tuning. In this phase, besides the alignment expert, we introduce two modality-specific experts, i.e., one takes only the textual embedding as input, while the other takes the pre-trained collaborative embedding as input. These three experts are combined via a Mixture-of-Experts (MoE) architecture through a frequency-aware gating mechanism and then fine-tuned by the collaborative supervision signals."}, {"title": "3.2 Characteristic & Rec-Anchored Alignment", "content": "Existing methods [26] mainly adopt non-characteristic kernels and alignment loss is usually the only optimization objective. Nonethe- less, they would fail to grasp all statistics of the data distribution and suffer catastrophic forgetting which will be detailed in Sec. 4.4. Therefore, to tackle these limitations, we employ an alignment expert to align the textual embeddings towards the collaborative space. Specifically, we adopt MK-MMD [15, 38] with characteristic kernels as the alignment loss since it's able to capture all infor- mation about the distribution [12, 33]. Besides, we introduce an auxiliary recommendation Binary Cross Entropy loss to conduct Rec-Anchored alignment. It could enable the collaborative embed- dings keep as much recommendation semantics as possible after the alignment, also avoiding catastrophic forgetting. The collaborative embeddings are not frozen during the alignment stage. By contrast, the pre-trained text embeddings remain frozen while the MLPs for dimension reduction are learnable. The overall loss function is:\n$\\mathcal{L} = L_{REC} + \\gamma L_{MK-MMD}$ (4)\n$L_{MK-MMD} = D^2(\\mathcal{H}_t, \\mathcal{H}^a)$ (5)\n$L_{REC} = \\frac{1}{1-n} \\sum_{i=1} BCE (f_\\theta (\\{h_i^r\\}, \\{h_i^a\\}, x_i^r, x_i^a), y_i)$ (6)\n$\\{h_i^a\\} = f_{MLP}(SG(\\{h_i^{text}\\}) ,w)$ (7)"}, {"title": "3.3 Collaborative Fine-tuning", "content": "Existing works [8, 50] simply append the aligned embeddings to the input of recommendation model and regard it as additional features. Nevertheless, these embeddings may not be fully comprehended and exploited by the model. Therefore, in this stage, in order to fully utilize the aligned representation to enhance the performance of recommendation tasks, we propose a triple-experts architecture with four embedding tables (i.e., two for each modality). Specifically, it consists of an alignment expert, an LLM-specific expert, and a recommendation-specific expert. Besides, existing works [27, 53] neglect the impact of item frequency on the credibility of different modality information. Therefore, we propose to fuse the output of these three experts via a frequency-aware gating.\nTriple-Experts Architecture. To fully exploit different modality data and alleviate catastrophic forgetting, our model consists of an aligned expert $f_{align}$, which takes the aligned $\\{h_i^a\\}$ and $\\{h_i^a\\}$ as input, and two modality-specific experts $f_{LLM}$ and $f_{id}$, which take only the original textual embedding $\\{h_i^t\\}$ or pre-trained collabo- rative embedding $\\{h_i^r\\}$ as input. The MK-MMD alignment loss is removed in the alignment expert in this phase and $L_{REC}$ only is the loss function. Besides, the textual embeddings in both the align- ment and LLM-specific experts are frozen, while the collaborative embeddings in the alignment and recommendation-specific experts are being updated. Finally, the output of each expert is obtained:\n$O_{id} = f_{id}(\\{h_i^r\\})$ (8)\n$O_{align} = f_{align} (f_{MLP} (SG(\\{h_i^a\\}, w), \\{h_i^a\\})$ (9)\n$O_{LLM} = f_{LLM} (f_{MLP}(SG(\\{h_i^t\\},w'))$ (10)\nSuch a design shares spirits with the Multi-Embedding paradigm [16, 28, 42] in the sense that for each modality we have two embed- ding tables, one for the alignment expert and the other one for the recommendation- or LLM-specific expert.\nFrequency-aware Gating. We propose to fuse the output of the three experts based on the target item frequency in an adap- tive manner. To be specific, we first divide all items into B buckets based on their frequency. Next, given the target item i and its corre- sponding bucket ID b(i), a gating network g is learned which takes b(i) and the expert embedding as input to generate the probability for each expert. Finally, we fuse the output of each expert with a frequency-aware gating mechanism to generate the final logit:\n$\\Phi = g_{id}(b(i), \\{h_i^r\\})\\cdot O_{id}$ + $g_{align} (b(i), \\{h_i^r\\}, \\{h_i^a\\}) \\cdot O_{align}$ +$g_{LLM}(b(i), \\{h_i^t\\})\\cdot O_{LLM}$ (11)\nwhere $g_{id}$, $g_{align}$, and $g_{LLM}$ denote the probability of recommendation- specific, alignment, and LLM-specific expert, respectively."}, {"title": "4 EXPERIMENTS", "content": "We conduct extensive experiments on three public datasets and answer the following six research questions:\n\u2022 RQ1: How do PAD perform, compared with other SOTA LLM encoder methods?\n\u2022 RQ2: What is the effect of recommendation anchoring in the alignment loss?\n\u2022 RQ3: How do the characteristic alignment losses perform, com- pared with non-characteristic ones?\n\u2022 RQ4: How do the modality-specific embedding and experts contribute to the performance enhancement?\n\u2022 RQ5: What is the impact of each expert (ID, alignment, and LLM-specific expert) and the proposed frequency-aware fusion?\n\u2022 RQ6: Do large language models (LLMs) have more powerful encoding capability than pre-trained language models (PLMs)?\n\u2022 RQ7: Is PAD compatible with other sequential recommendation models as a model-agnostic paradigm?"}, {"title": "4.1 Experimental Settings", "content": "4.1.1 Datasets. Our experiments are conducted on three datasets: MIND [49], Electronics, and Prime Pantry where the last two are two categories from Amazon [34]. Their statistics are summarized in Tab. 1. The detailed data preprocess and split procedure is demon- strated in Appendix. A.\n\u2022 MIND\u00b9 is collected from the Microsoft news website. It contains abundant text information with news title, abstract, body, etc. Given the historical click events of user, the task is to predict whether this user would click the target news.\n\u2022 Amazon\u00b2 is collected from the e-commerce platform Amazon in which users rate items from 1 to 5. The sequential recommen- dation task is to predict whether a user will give a rating higher than 3 to the target item."}, {"title": "4.1.2 Evaluation Metrics", "content": "We choose the widely-used top-k Hit Ratio (HR@k) and top-k normalized Discounted Cumulative Gain (nDCG@k) with k = 10 for evaluation on the whole item set. The values reported below are averaged over all users."}, {"title": "4.1.3 Baselines", "content": "The following representative baseline methods are chosen for comparison.\n\u2022 SASRec denotes the primary SASRec model, which is only trained by pure ID of items in the collaborative space.\n\u2022 Hybrid denotes Hybrid Encoding adopted in CoLLM [53] and LLARA [27]. It directly concatenates the collaborative with text embedding and feeds into the subsequent SRS.\n\u2022 MoRec [51] simply adopts an MLP to reduce the dimension of text embedding and inputs it into SRS."}, {"title": "4.1.4 Implementation Details", "content": "Our experiments are conducted on Tesla V100 GPUs and all the results shown are averaged over 3 runs. Detailed experimental settings are provided in Appendix. A."}, {"title": "4.2 Overall Performance (RQ1)", "content": "To answer RQ1, we verify PAD's effectiveness by comparing it with various baseline methods introduced in Sec. 4.1.3. The overall performance on three public datasets is shown in Tab. 2. In sum- mary, on all three datasets, our proposed PAD achieves state-of-the-art performance, beating the best performing baseline SMEM by 1.51%, 7.60%, and 9.54% on nDCG@10 on each dataset.\nMoreover, in Fig. 2 we also provide the results on different sub- sets of items, i.e., on warm, median, and cold items. We observe that PAD surpasses SASRec on all the item subsets on each dataset, and the improvement on cold items is more significant. For example, on Electronics dataset, our method achieves 63.36%, 202.02% and 462.61% relative performance lift on nDCG@10 on the warm, me- dian and cold items. The performance lift on the cold items are 7.3 times of that on the warm items. This validates that our method can mitigate the cold-start problem with the LLM knowledge effectively.\nWe further illustrate the effectiveness of our method on the cold items with the following analysis. Denote PID and PID as the group of item pairs with the top-10% and bottom-10% dis- tance based on collaborative collaborative embeddings. We present the distribution of these item pairs under the distance distribution regarding the collaborative embeddings and textual embeddings in each model in Fig. 3. We observe that all existing methods, includ- ing SASRec, SMEM, and CTRL, can't differentiate the distribution of PID and PID well. In contrast, our method succeeds in learning a generally larger textual distance for those item pairs with top collaborative distance and learning a smaller textual distance for those with bottom collaborative distance. We conclude with the following result:\nResult 1. Our proposed PAD greatly improves the performance on the cold items with better alignment of textual embeddings towards the collaborative space, leading to SOTA performance on three datasets."}, {"title": "4.3 Discrepancy Measurement", "content": "In this section, we propose a novel metric to quantify to what extent are the aligned embedding space deviates from the origi- nal one. Specifically, rather than the deviation of the aligned and original embedding for each ID itself, we are more interested in the discrepancy of embedding distance [42]. To this end, we employ the Kendall's tau [23] between the embedding distance distribution of the original embeddings and that of the aligned embeddings.\nSpecifically, in sequential recommendation, the embedding dis- tance between each behavior and target item pair is calculated. Afterward, Kendall's tau is adopted to measure the degree of con- cordance between these two variables. For example, given the user's historical interaction sequence $\\{i_1, i_2, i_3\\}$, the SRS usually takes $\\{i_1\\}$ as the behavior item sequence to predict the target item $i_2$ and takes $\\{i_1, i_2\\}$ as the behavior item sequence to predict the target item $i_3$. We map each item is into embedding $e_i$ and calculate the dis- tance d(,) (like Euclidean distance) between each behavior and tar- get item embedding pair, which includes $\\{(e_1, e_2), (e_1, e_3), (e_2, e_3)\\}$. For another model with embeddings $e'_i$s for $i_s$, its distance vari- able is $\\{(e'_1, e'_2), (e'_1, e'_3), (e'_2, e'_3)\\}$. Any pair of samples is called con- cordant if the sort order agrees, e.g., both $(e_1, e_2) < (e_1, e_3)$ and"}, {"title": "4.4 Rec-Anchored Alignment Loss (RQ2)", "content": "We investigate the effect of recommendation anchoring in avoiding catastrophic forgetting by comparing the following three alignment losses in the align phase, namely: (1) No Alignment, i.e., SMEM, which doesn't involve any alignment loss but simply concat the textual and collaborative embeddings within the alignment expert, (2) Non-Anchored Alignment, with only the alignment loss, i.e., $D_k^2 (D_{text}, D_{rec})$, (3) Rec-Anchored Alignment, which combines the alignment loss with a BCE loss: $L_{rec} + \\gamma \\cdot D_k^2 (D_{text}, D_{rec})$ and updates both collaborative and text embedding, and (4) Rec- Anchored and Frozen Alignment, which freezes the collabo- rative embeddings upon the Rec-Anchored Alignment: $L_{rec} + \\gamma \\cdot D_k^2 (D_{text}, SG(D_{rec}))$, where SG denotes stop gradient operation. This can totally avoid catastrophic forgetting of the collaborative embeddings since they are frozen. However, this may hurt the align- ment of the textual embeddings towards the collaborative space.\nThe results are shown in Fig. 4, and we can observe that: 1) Our proposed Rec-Anchored Alignment performs the best among all losses; 2) Rec-Anchored losses, including Rec-Anchored as well as Rec-Anchored and Frozen, perform in general better than those non-anchored method, indicating that recommendation anchoring can effectively avoid catastrophic forgetting; 3) The Non-Anchored Alignment sometimes performs worse than Non-Alignment, in- dicating that simple alignment without anchoring even hurt the performance; 4) Rec-Anchored Alignment performs better than Rec-Anchored and Frozen Alignment, indicating the brute force"}, {"title": "4.5 Characteristic Alignment Loss (RQ3)", "content": "To verify the advantages of characteristic kernels over non-characteristic ones in maximum mean discrepancy, apart from Gaussian kernels, we also experiment on the following three kernels and Info_NCE loss (usually adopted by contrastive learning), in which only Lapla- cian kernels are characteristic on Rd."}, {"title": "4.6 Towards Triple-Experts Architecture (RQ4)", "content": "4.6.1 Catastrophic Forgetting of Alignment. Many existing LLM4Rec methods [26] first align the collaborative embeddings of recommendation with the LLM, and then fine-tune these em- beddings in the downstream recommendation task. However, we wonder whether there is catastrophic forgetting of the alignment on the collaborative embeddings. That is, when there is only one single recommendation embedding table, whether the alignment leads to catastrophic damage to these embeddings, which CANNOT be recovered by the supervised fine-tuning."}, {"title": "4.7 Ablation Study (RQ5)", "content": "We conduct an ablation study on the MIND dataset to investigate the effect of each expert and the frequency-aware gating. The re- sults are shown in Fig. 5(a). First, we remove each expert in the last phase and observe the performance deteriorates when we remove any of the three experts. Interestingly, removing the recommenda- tion (ID) expert leads to the largest performance drop. This indicates that the alignment loss leads to inevitable information loss of the collaborative embeddings, even with our Rec-Anchored loss. There- fore, the recommendation-specific tower is essential to retain the collaborative signals.\nSecond, we try to retain only one of these three expert and com- pare their performance. Surprisingly, the single alignment model achieves the best performance. This is possibly due to the fact that, on the one hand, it keeps the collaborative semantics through the Rec-Anchored alignment loss, and on the other hand, it boosts the performance on the cold items with the help of LLM semantics.\nFinally, the model variant removing the frequency-aware gat- ing fuses all experts with the learned weights on all items equally, leading to a decrease of 3.06% in HR@10. Therefore, from the per- spective of target item frequency, this gating acts as an bridge con- necting the collaborative, semantic, and alignment space adaptively considering their different credibility."}, {"title": "4.8 LLM2Vec v.s. BERT (RQ6)", "content": "To compare the performance of LLM and pre-train language model (PLM) like BERT, we replace the LLM2Vec module with BERT-Base as encoder to obtain the text embedding of items. It achieves the per- formance of HR@10 = 18.3989 and nDCG@10 = 9.9810 on MIND, which is inferior to LLM2Vec achieving HR@10 = 18.6703 and nDCG@10 = 10.1515. We speculate the reasons are: 1) More param- eters of Llama3-8B bring in more powerful encoding capability than BERT-Base with 110 million parameters only. 2) Compared with BERT, LLM2Vec is trained with more abundant text data on wiki corpus and adopts a different Unsupervised contrastive training (SimCSE) objective."}, {"title": "4.9 Compatibility (RQ7)", "content": "To validate the compatibility of PAD, apart from SASRec [22], we also experiment on two typical sequential recommendation models as backbones including GRU4Rec [20] and Caser [45]. As shown in Tab. 3, PAD significantly enhance the performance of the original model on all three datasets, indicating that it acts as a powerful model-agnostic enhancement paradigm."}, {"title": "5 RELATED WORK", "content": "5.1 Sequential Recommendation\nSequential recommendation (SR) focuses on modeling the sequen- tial dependency over the interaction sequence to capture user pref- erence. Inspired by the success of self-attention in natural language processing [46], SASRec [22] incorporates it into SR and afterward many works explore target-attention recommendation in a simi- lar manner [4, 10, 11, 36, 40, 55-57]. Caser [45] and GRU4Rec [20] adopts convolution and recurrent networks for sequential modeling. Nevertheless, these models mainly takes ID as the only input thus suffering from the cold-start problem. By contrast, our proposed PAD paradigm is capable of enhancing them as backbones by cap- turing complex correlation of different modality data through our designed alignment and triple-experts structure.\n5.2 Large Language Model & Multi-Modal Rec\nIn the recommendation community, some works explicitly adopt alignment between tabular and textual data which could achieve information gain. For example, CTRL [26] leverages contrastive learning, and FLIP [47] uses data masking and reconstruction pre-training with contrastive learning. Afterward, some works like DisCo [8] and DaRec [50] conduct disentanglement to capture the specific information. However, they simply separate the embed- dings into different parts which may not be fully learned by the"}, {"title": "6 CONCLUSION", "content": "In this paper, we propose a novel Pre-train, Align, and Disentan- gle (PAD) paradigm to empower sequential recommendation with Large Language Models. We propose a characteristic recommendation- anchored loss to better align textual embeddings towards the collab- orative space and avoid catastrophic forgetting. We employ a triple- experts architecture, consisting of aligned and modality-specific experts with disentangled embeddings, which is fine-tuned in a frequency-aware manner. Comprehensive experiments on three public datasets validate the effectiveness of our proposed method. Notably, the framework can be extended to multi-modal modeling."}, {"title": "A EXPERIMENTAL SETTINGS", "content": "For LLM2Vec we choose the Llama3-8B as the base model with the unsupervised-trained LoRA weights. We leverage the news title on MIND and item title on Amazon to generate text embedding. Suppose I is defined as the length of user's historical interaction sequence. For all the three datasets, we select the users with no less than 5 interactions and extract the latest 23 items interacted. The task is to predict whether the user would click the (1 \u2013 2)-th, (1-1)-th, and l-th item on training, validation, and test set, respec- tively, given the previous interacted items. Besides, AdamW [31] is adopted as optimizer. Batch size is 16, the early stop epoch is set to 10, and dropout probability is 0.1. L2 regularization is adopted with weight of 0.1."}, {"title": "B PSEUDO-CODE", "content": "The general procedure of PAD is given in Alg. 1, which consists of three phases: (1) LLM & Recommendation Pre-train (from Line 1 to 6), (2) Alignment with MK-MMD (from Line 7 to 14), and (3) Recommendation Supervised Fine-tuning (from Line 15 to 23)."}, {"title": "C ADVANCED ANALYSIS", "content": "Denote PID and PID as the group of item pairs with the top-10% and bottom-10% distance based on collaborative ID embeddings. We present the distribution of these item pairs under the distance distribution regarding the collaborative and textual embeddings learned by each model on warm items in Fig. 7.\nSpecifically, we plot the collaborative and text embedding dis- tance distribution of different methods including the original SAS- Rec, PAD, SMEM, and CTRL. The green and red parts denote the bottom and top 10% distance in the collaborative space in the left figure, and we also plot the corresponding distance of these item pairs in the semantic space in the right figure. By comparing Fig. 3 and 7, it can be observed that the item pairs with close distance become more concordant in PAD, showing that PAD bring more alignment on cold items compared with other methods."}]}