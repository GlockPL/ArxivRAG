{"title": "Enhancing Retrieval Performance: An Ensemble Approach for Hard Negative Mining", "authors": ["Hansa Meghwani"], "abstract": "Ranking consistently emerges as a primary focus in information retrieval research. Retrieval and ranking models serve as the foundation for numerous applications, including web search, open domain QA, enterprise domain QA, and text-based recommender systems. Typically, these models undergo training on triplets consisting of binary relevance assignments, comprising one positive and one negative passage. However, their utilization involves a context where a significantly more nuanced understanding of relevance is necessary, especially when re-ranking a large pool of potentially relevant passages. Although collecting positive examples through user feedback like impressions or clicks is straightforward, identifying suitable negative pairs from a vast pool of possibly millions or even billions of documents possess a greater challenge. Generating a substantial number of negative pairs is often necessary to maintain the high quality of the model. Several approaches have been suggested in literature to tackle the issue of selecting suitable negative pairs from an extensive corpus. This study focuses on explaining the crucial role of hard negatives in the training process of cross-encoder models, specifically aiming to explain the performance gains observed with hard negative sampling compared to random sampling. We have developed a robust hard negative mining technique for efficient training of cross-encoder re-rank models on an enterprise dataset which has domain specific context. We provide a novel perspective to enhance retrieval models, ultimately influencing the performance of advanced LLM systems like Retrieval-Augmented Generation (RAG) and Reasoning and Action Agents (ReAct). The proposed approach demonstrates that learning both similarity and dissimilarity simultaneously with cross-encoders improves performance of retrieval systems.", "sections": [{"title": "INTRODUCTION", "content": "Retrieval models, unlike generative models, fetch real information from sources, with search engines indicating the source of each retrieved item (Sanderson and Croft, 2012). This underscores the continued importance of information retrieval (IR), even in the presence of generative LLMs, particularly in contexts where reliability is crucial. After the launch of BERT (Devlin, Chang, Lee, Google, et al., 2019), a straightforward retrieve then re-rank strategy became popular in January 2019 as a successful means of leveraging pre-trained transformers for passage retrieval (Nogueira and Cho, 2019a). This model, known as monoBERT, marks the initial manifestation of what later evolved into cross-encoders for retrieval, a category encompassing reranking models such as MaxP (Dai and Callan,\n2019), CEDR (MacAvaney et al., 2019a), Birch (Akkalyoncu Yilmaz et al., 2019) PARADE (Li et al.,\n2020), and numerous others. A proficient ranking algorithm holds potential advantages for numerous downstream tasks within information retrieval research (Han et al., 2020). Conventional algorithms like BM25 (Robertson and Walker, 1994) heavily rely on term-matching metrics, limiting their efficacy to scenarios where queries and documents share identical terms. This inherent drawback leads to performance degradation when faced with semantic differences despite identical meanings, a phenomenon known as the vocabulary mismatch problem. To enhance the understanding of users'\nsearch intentions and retrieve pertinent items, it's anticipated that ranking algorithms would engage in semantic matching between queries and documents. Driven by advancements in deep learning, particularly techniques for capturing meaning (representation learning), researchers are increasingly using Dense Retrieval (DR) models to tackle the challenge of semantic similarity (Guu et al., 2020;\nKarpukhin et al., 2020; Luan et al., 2020). DR excels at capturing the semantic essence of queries and documents by converting them into low-dimensional embeddings. This facilitates efficient document indexing and similarity search, leading to effective online ranking. Studies have shown promising results for DR models in various information retrieval tasks. (Guu et al., 2020; Qu et al., 2020)\nWhile past research using diverse training strategies for DR models has shown encouraging results, inconsistencies and even contradictions arise when comparing their findings. For example, the superiority of training with \u201chard negatives\u201d (highly similar yet irrelevant documents) over random negatives remains an open question. Additionally, many effective training methods suffer from inefficiency, making them impractical for large-scale deployments. Despite promising results, DR faces key challenges, and we are trying to investigate one of the challenges related to hard negatives mining on a custom data which has specific industry context."}, {"title": "Problem Statement", "content": "For the integration of Generative AI, every organization will require a custom retrieval system built upon their private datasets. Retrieval systems, when not trained on enterprise domain datasets, face significant challenges in delivering accurate and relevant results within organizational contexts. One of the primary issues is a lack of familiarity with the specific terminology, jargon, and nuances prevalent in the enterprise domain.\nThe major issues while training the retrieval models -\n\u2022 Hard negatives, which are non-relevant passages that closely resemble positive examples, play a crucial role in refining the model's understanding.\n\u2022 Providing both positive (relevant) and negative (irrelevant) examples is important. Negative examples, especially hard negatives, challenge the model to distinguish between relevant and irrelevant content effectively.\n\u2022 When dealing with enterprise-specific datasets, the inclusion of hard negatives is paramount.\n\u2022 Without exposure to hard negatives, the model might struggle to differentiate between similar passages, leading to inaccurate responses and compromised decision-making processes within organizations."}, {"title": "Aim and Objectives", "content": "The principal aim of this research is to propose a hard negative mining strategy which helps enterprises to fine-tune state of the art ranking/retrieval models on their private dataset. These datasets are unique to each enterprise and may contain specialized terminologies and domain-specific jargon. So, by incorporating hard negatives in the training process, retrieval models can be honed to navigate the complexities of enterprise data, ensuring precise and contextually relevant results tailored to the specific needs of the business.\nThe research objectives are derived from the purpose of this study, which are outlined as follows:\n\u2022 To suggest a robust hard negative mining strategy for domain specific private data.\n\u2022 To utilize the hard negatives for fine-tuning of cross-encoder model on domain specific data.\n\u2022 To investigate the impact of using hard negatives in training of ranking models."}, {"title": "Significance of the Study", "content": "Large Language Models (LLMs) excel at answering questions based on the knowledge they were trained on. However, their training data typically does not include recent information and specific private information stored in platforms like a company's Confluence, Google Drive, or SharePoint. For the integration of Generative Al within enterprise settings, every organization will require a custom retrieval system built upon their private datasets. Retrieval augmented systems, when not trained on enterprise domain datasets, face significant challenges in delivering accurate and relevant results within organizational contexts. One of the primary issues is a lack of familiarity with the specific terminology, jargon, and nuances prevalent in the enterprise domain. Importance of training retrieval/\nranking models with hard negatives cannot be overstated, especially within enterprise domains. This study bridges the gap between pre-trained ranking models and domain-specific data. The study also proposes a data augmentation technique to include hard negatives for given query, document pair. This study will investigate that the triplet objective function can be utilized by researchers across various NLP applications. Additionally, it aims to introduce a modified evaluation criterion suitable for use by researchers evaluating ranking models for internal dataset."}, {"title": "Scope of the Study", "content": "The scope of the study is to develop hard negative mining strategy using ensemble of embeddings. Pre-trained embedding models used here will be state of the art models like SFR-Embedding-Mistral, Jina AI, BERT etc. And to analyse impact of fine-tuning cross encoder models with mined hard negatives on the private enterprise dataset. Given resource limitations, this study will not explore training embedding models from scratch using large-scale text corpora. We will instead leverage pre-trained embeddings and adapt the size and complexity of candidate models to fit within available resources. Training models approaching the scale of leading-edge architectures is not feasible for this study."}, {"title": "Structure of the Study", "content": "The structure of this research study is outlined as follows. Chapter 1 covers details about the research proposal like background of the study, problem statement, scope and significance of doing this research.\nChapter 2 provides an in-depth examination of the literature on hard negative mining techniques and cross-encoder training for document and passage ranking tasks. Sections 2.2 and 2.3 detail the literature on hard negatives and various negative sampling techniques. Section 2.4 discusses the literature related to retrieval and ranking models, specifically focusing on bi-encoders and cross- encoders. Section 2.5 outlines advanced techniques for generating synthetic data for model training. Section 2.6 addresses downstream tasks like document ranking and the associated open-source datasets. Finally, in section 2.7 reviews various loss functions used for these downstream tasks. This study covers intersections among hard negative mining, ensembles, and cross-encoder re-rankers, mandating a comprehensive review of the applicable literature.\nChapter 3 provides a comprehensive explanation of the methods used in this study to improve the document ranking task by augmenting existing data with hard negative mining. Section 3.2 presents the proposed flow diagram. Section 3.3 details the dataset description and preprocessing steps taken to understand the data's characteristics. Following that, Section 3.4 discusses how data augmentation is applied using an embeddings ensemble and describes the model architecture proposed for mining hard negatives and re-ranking documents. The chapter concludes in Section 3.5, where it outlines the evaluation metrics that will be used to assess the model after training.\nChapter 4 outlines the implementation of the approaches mentioned in the research methodology. It includes data exploration, analysis, and experimentation aimed at uncovering new insights and observations about the enterprise dataset. Section 4.2 provides detailed information about the implementation of embedding models, the ensemble technique used to select hard negatives, and the data preparation and hyperparameters for cross-encoder training. Section 4.3 examines the distribution"}, {"title": "LITERATURE SURVEY", "content": "This section of the literature review conducts an in-depth examination of hard negative mining techniques and cross-encoder training for document and passage ranking tasks. We will initially explore the task of hard negative mining and the ways in which it has been categorized in the literature. Subsequently, we will delve into the research concerning cross-encoder models, embedding models, and several synthetic document generation methods. This study intersects hard negative mining, ensembles, and cross-encoder re-rankers, prompting a thorough review of the related literature as well."}, {"title": "Hard Negative", "content": "Previous studies highlight the critical role of negative example selection in optimizing the training process of cross-encoder models. (Karpukhin et al., 2020) conducted a study to assess the efficacy of different negative sampling strategies in training. They compared the performance of BM25 (Robertson and Walker, 1994) negatives, random negatives, and in-batch negatives. Their findings suggest that combining BM25 and in-batch negatives leads to the most favourable outcomes in terms of training effectiveness. (Xiong et al., 2020) theoretically demonstrate that employing local negatives is suboptimal for dense retrieval learning. Subsequently, they suggest a method for generating global negatives by utilizing the existing dense retrieval model concurrently during training. This approach necessitates periodic re-indexing of the corpus and retrieval. (Qu et al., 2020) additionally suggest a method for generating hard negatives by utilizing the existing dense retrieval model, albeit after the completion of training rather than dynamically during training. However, their study reveals that employing these hard negatives alone post-training could potentially hinder the training process. They note that the effectiveness of these hard negatives is improved when filtered based on an independently trained cross-encoder model. (Zhan et al., 2021) discover that the instability induced by hard negatives can be mitigated by incorporating random negatives into the training process. Furthermore, they adopt a strategy akin to the ANCE (Xiong et al., 2020) method for periodically re-generating hard negatives, with the modification of updating only the query encoder to reduce re-indexing overhead. These findings underscore the significance of hard negatives in training and demonstrate varying levels of effectiveness across different approaches. Apart from the aforementioned research, which primarily delves into hard negative training strategies for fine-tuning DPR-like bi-encoders, other studies have made comparable observations regarding diverse methodologies that benefit bi-encoders. highlight the continued importance of hard negatives even when the model undergoes additional pretraining aimed at enhancing the representation of the [CLS] token, referred to as Condenser in their study."}, {"title": "Negative Sampling Strategies", "content": "Negative sampling is a crucial aspect of training models for various tasks, including information retrieval. It involves selecting examples that are irrelevant to the query, allowing the model to learn the distinction between positive and negative examples. Different negative sampling techniques have varying effectiveness. Random negatives might be easy to generate but may not provide the most informative contrast to positive examples. BM25 negatives, leveraging a retrieval model like BM25, could be a more focused approach but might introduce bias from the BM25 model itself."}, {"title": "BM25 Negatives", "content": "This stands for \"Best Match 25\" and is a popular retrieval model used in information retrieval systems. It ranks documents based on their relevance to a query, considering factors like term frequency (how often a term appears in a document) and document frequency (how many documents contain the term)."}, {"title": "In-batch Negatives", "content": "A batch typically contains multiple queries and their corresponding candidate answer passages. Instead of searching a larger dataset for irrelevant examples, in-batch negatives leverage the passages already present within the current batch. Passages that are unlikely to be relevant to the specific query at hand are chosen as negative examples."}, {"title": "Random Negatives", "content": "\u2022 The model randomly selects documents or passages from the entire dataset, regardless of their content or relevance to the current query.\n\u2022 Randomly selecting negative examples during training can lead to suboptimal performance. These randomly chosen documents may not provide informative gradients for model updates."}, {"title": "Comparing random negatives, BM25 negatives and in-batch negatives", "content": "(Karpukhin et al., 2020) investigates different negative sampling strategies for training dense retrieval models.\n\u2022 (Karpukhin et al., 2020) compare the effectiveness of random negatives with other methods and find that random selection alone is not ideal. (Pradeep et al., 2022) The authors propose a static hard negative mining strategy called Passage-based BM25 (Nguyen et al., n.d.). Instead of selecting negatives based on the query, PassageBM25 selects negative passages based on their similarity to the positive passage.\n\u2022 Empirical studies on a Vietnamese question-answering dataset (ZAC2022) demonstrate that PassageBM25 outperforms both vanilla BM25 and dense retrievers trained with conventional query-based BM25 in terms of top-k retrieval accuracy. (Nguyen et al., n.d.)\n\u2022 (Xiong et al., 2020) theoretically prove that local negatives (such as in-batch negatives) are suboptimal for dense retrieval learning."}, {"title": "Combining BM25 and In-Batch Negatives", "content": "\u2022 (Karpukhin et al., 2020) find that a mixture of BM25 and in-batch negatives yields optimal results. By combining these two types of negatives, the model benefits from both static and dynamic negative mining strategies.\n\u2022 Additionally, RocketQA (Qu et al., 2020) proposes an optimal pipeline for training bi-encoder models, which involves increasing batch size through in-batch and cross-batch training and denoising false negative samples.\nIn summary, the choice of negative sampling strategy significantly impacts the performance of dense retrieval models. While random negatives are suboptimal, combining BM25 and in-batch negatives provides an effective approach to enhance retrieval accuracy (Pradeep et al., 2022)"}, {"title": "Static and Dynamic Hard Negatives (STAR & ADORE)", "content": "(Zhan et al., 2021) theoretically analyse the impact of negative sampling strategies on DR model performance. They shed light on why hard negative sampling (selecting challenging negatives) outperforms random sampling. Additionally, they identify potential risks associated with static hard negative sampling.\nThe paper introduces two novel training strategies:\n\u2022 Stable Training Algorithm for dense Retrieval (STAR):\n\u2022 STAR combines two types of negatives during training random negatives alongside static hard negatives.\n\u2022 Random negatives help stabilize the training process and mitigate risks associated with static hard negatives.\n\u2022 Additionally, STAR reuses document embeddings within the same batch to enhance efficiency.\n\u2022 By introducing random negatives alongside static hard negatives, STAR aims to achieve better stability during training.\n\u2022 Query-side training Algorithm for Directly Optimizing Ranking performance (ADORE):\n\u2022 ADORE dynamically selects hard negatives during training, rather than relying on a fixed set.\n\u2022 ADORE replaces static hard negative sampling with a dynamic method that directly optimizes ranking performance.\n\u2022 By dynamically selecting negatives, ADORE aims to improve overall retrieval accuracy.\nThe proposed strategies are evaluated on two publicly available retrieval benchmark datasets. Both STAR and ADORE demonstrate significant improvements over existing competitive baselines. Combining both strategies yields the best overall performance.\nDrawbacks:\n\u2022 Trade-off Between Stability and Effectiveness: STAR balances stability (due to random negatives) with effectiveness (due to hard negatives). However, finding the right balance can be challenging. Striking the optimal trade-off remains an open question.\n\u2022 Dynamic Sampling Complexity: ADORE dynamically selects hard negatives during training, which introduces additional complexity. The process of dynamically identifying suitable negatives requires careful design and efficient implementation."}, {"title": "Retrieval and Reranking models", "content": "In this section of the research study, we will discuss about various retrieval and re-ranking models. These are mostly encoder type models. Cross encoder returns a classification score between 0 to 1 which signifies the similarity between the input documents. So, the cross-encoder is used for re-ranking the documents, passages and other texts. On the other hand, bi-encoder model can return individual embeddings of the documents or texts as an output and hence these are used to get embeddings of documents. In the section 2.4.1 we will summarize the earlier research conducted in cross-encoder models and in the section 2.4.2 we will summarize the research work done in bi-encoders models."}, {"title": "Cross encoder", "content": "The initial cross-encoder for reranking, monoBERT (Nogueira and Cho, 2019a), swiftly emerged shortly after the introduction of BERT (Devlin et al., 2019a) It adhered to the approach advocated by the BERT authors for processing (query, document) input pairs and exhibited substantial advancements in effectiveness on datasets such as TREC CAR (Craswell et al., n.d.; Dietz et al., n.d.) and MS MARCO passage ranking (Bajaj et al., 2016). While the vanilla version of monoBERT (Nogueira et al., 2019) demonstrated significant enhancements in the passage retrieval task, its design did not accommodate the processing of long input sequences necessary for document retrieval. Many subsequent BERT-based cross-encoder studies (MacAvaney et al., 2019a; Li et al., 2020) aimed to tackle this challenge by either conducting multiple inferences on various segments of the document or implementing additional architectural modifications atop BERT to enhance the processing of longer document texts."}, {"title": "Bi encoder", "content": "Biencoders encode each sentence separately and map them to a common embedding space, where the distances between them can be measured. They are fast, scalable, and efficient for inference. This makes them suitable for tasks that require efficient comparison in a vector space, such as information retrieval or semantic search. However, they are generally less accurate than cross-encoders. The breakthroughs in DPR (Karpukhin et al., 2020) and ANCE (Xiong et al., 2020) have sparked a revival of biencoders in the BERT era. The aim of a bi-encoder is to learn a transformer-based mapping from queries and documents into dense fixed-width vectors, where the inner product between the query"}, {"title": "Generation of Synthetic Document for Training Cross Encoder", "content": "(Askari et al., 2023; Agarwal et al., 2024) investigates the effectiveness of using large language models (LLMs) like ChatGPT to generate synthetic documents for training cross-encoder re-rankers. Cross- encoder re-rankers are a technique in information retrieval that improves the relevance of search results by re-ordering them based on semantic similarity to the query. The study compares the performance of re-rankers trained on synthetic data generated by ChatGPT with those trained on human-written data. The findings suggest that LLM-generated documents can be a viable alternative, particularly in domains with limited labelled data. The research also introduces a new dataset, ChatGPT-RetrievalQA, which is based on HC3 dataset (Guo et al., 2023) for evaluating the effectiveness of LLM-generated data in this context. Overall, the paper contributes to the exploration of techniques for improving information retrieval systems using synthetic data. And few other research (Agarwal and Pachauri,\n2023; Agarwal et al., 2023) explores pseudo labelling and graph data structure for data augmentation."}, {"title": "Research Question", "content": "\u2022 The study aims to compare cross-encoder re-ranking models trained on ChatGPT responses with those trained on human responses.\n\u2022 Specifically, they investigate whether LLM-generated data can augment training data, especially in domains with limited labelled data."}, {"title": "Findings", "content": "\u2022 Zero-Shot Re-Ranking: Cross-encoder re-rankers trained on ChatGPT responses are statistically significantly more effective as zero-shot re-rankers than those trained on human responses.\n\u2022 Supervised Setting: In a supervised setting, human-trained re-rankers outperform LLM-trained re-rankers.\n\u2022 Generative LLMs show promise in generating training data for neural retrieval models."}, {"title": "Drawbacks", "content": "\u2022 Control and Transparency: It can be challenging to precisely control the content and style of documents generated by large language models. This lack of control over the training data might make it difficult to optimize the re-rankers for specific tasks.\n\u2022 Factual Incorrectness: Further research is needed to explore the impact of factually incorrect information in the generated responses and to validate these findings with open-source LLMs.\n\u2022 Limited Domain Transferability: ChatGPT might be less effective in generating documents for specialized domains where its training data is limited. The paper mentions limitations in ChatGPT for cross-linguistic tasks, which could also be a disadvantage.\n\u2022 Cost and Computational Resources: Training large language models like ChatGPT requires significant computational resources. This could be a barrier for wider adoption of the proposed method. Also, for domain specific human annotated dataset, expert human annotators are required, which is very expensive and tedious task."}, {"title": "Document Ranking Datasets", "content": "In the document ranking task, the goal is to rank documents from a large corpus based on their relevance to a query.\nThere are two subtasks within this task:\n\u2022 Full ranking (retrieval): Rank documents comprehensively based on relevance.\n\u2022 Top-100 reranking: Further refine the ranking by considering the top 100 retrieved documents."}, {"title": "MS MARCO", "content": "MS MARCO was introduced with a paper released at NIPS 2016. (Bajaj et al., 2016) It encompasses various datasets, each focusing on different aspects of machine reading comprehension, question answering, and ranking tasks. The initial dataset included 100,000 real Bing questions paired with human-generated answers. Subsequent releases expanded to include more diverse tasks and larger question sets.\nThe corpus for the document ranking task contains 3.2 million documents. The training set consists of 367,013 queries with associated relevance labels for documents. Researchers use this dataset to"}, {"title": "TREC (Text REtrieval Conference) Dataset", "content": "The TREC Deep Learning (DL) Track (Dietz et al., n.d.; Craswell et al., 2021) focuses on information retrieval in a large training data regime. Specifically, it provides large-scale training data with tens of thousands or even hundreds of thousands of queries, each having at least one positive label. This mirrors real-world scenarios, including training based on click logs and labels from shallow pools. The DL Track aims to investigate which methods perform best under these conditions. Key questions include whether methods effective on small data also work on large data, how much performance improves with more training data, and how external data and weak supervision enhance results. In summary, the track offers valuable resources for advancing deep learning research in information retrieval."}, {"title": "Natural Questions (NQ)", "content": "(Kwiatkowski et al., n.d.) Contains real anonymized queries issued to Google and annotated with relevant Wikipedia passages, suitable for passage ranking. It is designed to mimic the type of questions people naturally ask search engines and the type of answers they expect in return. The public release of the dataset consists of 307,373 training examples with single annotations, 7,830 examples with 5- way annotations for development data, and a further 7,842 examples 5-way annotated sequestered as test data. The NQ dataset is used for training models to answer both short and long form questions. In the paper (Kwiatkowski et al., n.d.) the authors demonstrate a human upper bound of 87% F1 on the long answer selection task, and 76% on the short answer selection task. Each example in the original NQ format contains the rendered HTML of an entire Wikipedia page, as well as a tokenized representation of the text on the page. A simplified version of the training set is also provided. The dataset is publicly available and can be accessed from the official Google Research Datasets GitHub page. It is also available on the Hugging Face Datasets library."}, {"title": "Loss Functions", "content": "In this section of the research study, we will discuss about various loss functions studied during literature survey. We discuss all the popular loss functions used in the domain of text ranking, text classification and text understanding. Going forward we will implement one of these loss functions for our training based on our dataset and model implementation."}, {"title": "Cross Entropy Loss", "content": "Cross entropy is a commonly used loss function for classification tasks. It measures the dissimilarity between predicted probabilities and ground truth labels. Specifically, for each pair of positive and negative examples, the CE loss encourages the model to assign higher probabilities to the positive example's class label. In the context of cross-encoders, CE loss aims to minimize the discrepancy between predicted relevance scores and true relevance labels."}, {"title": "Hinge Loss", "content": "Hinge loss is commonly used in support vector machines (SVMs) and ranking tasks. It encourages a margin between positive and negative examples. For cross-encoders, hinge loss penalizes the model when the predicted relevance score for a positive example is lower than that for a negative example. The hinge loss function is convex and promotes better separation between relevant and irrelevant pairs. (MacAvaney et al., 2019a) used hinge loss, also known as max margin loss, to fine-tune cross- encoders. This hinge loss is more commonly used when training neural re-rankers before the widespread adoption of pre-trained models like BERT, as evidenced in references (Hui et al., 2017)"}, {"title": "Triplet Loss", "content": "Triplet loss is a technique used to train models to distinguish between similar and dissimilar things. It works by creating sets of three data points: an \"anchor\" point, a \"positive\" point that's like the anchor, and a \"negative\" point that's different from the anchor. The model is then trained to minimize the distance between the anchor and the positive point, while maximizing the distance between the anchor and the negative point. This approach is particularly useful for training Siamese networks (Koch et al.,\n2015) and FaceNet (Schroff et al., 2015), as well as contrastive learning techniques used in self- supervised learning."}, {"title": "Noise Contrastive Estimation (NCE) Loss", "content": "NCE loss transforms the problem of estimating a probability distribution into a binary classification task. The neural network is trained to distinguish between real data and a predefined fixed noise distribution. By doing so, NCE enhances efficiency and scalability, especially in high-dimensional and sparse data scenarios. The NCE loss introduced by (Gutmann and Hyv\u00e4rinen, 2010) computes scores for both the positive instance and multiple negative instances. It then normalizes these scores into"}, {"title": "Localized Contrastive Estimation (LCE) Loss:", "content": "The LCE loss, proposed by (Gao et al., 2021) augments the NCE loss (Gutmann and Hyv\u00e4rinen, 2010) with localized hard negative examples from the first-stage retriever. It combines the benefits of both contrastive estimation and hard negative mining. By selecting hard negatives from the retriever's output, LCE encourages the cross-encoder to distinguish between relevant and challenging-to-rank examples. (Pradeep et al., 2022) describes that LCE loss outperformed CE and hinge loss on the MS MARCO (Bajaj et al., 2016) passage ranking task, demonstrating its effectiveness for cross-encoders."}, {"title": "Summary", "content": "This chapter discussed the research conducted on various advanced encoder models and their training approaches, some of the researchers detailed the generation of synthetic training data. Major part of the research reviewed here is on negative sampling methods. The downstream task like retrieval/ranking of documents and passages was the primary focus of this literature review."}, {"title": "RESEARCH METHODOLOGY", "content": "This section provides a detailed description of the methodology used in this study of augmenting existing data with hard negative mining and enhancing document ranking task. We will start with the proposed flow diagram, then dataset description and pre-processing to understand the characteristics of the data. This will be followed by data augmentation section. We will formulate the model architecture proposed in the methodology for hard negative mining and document re-ranking task as well as the evaluation metric going to be used after to training the model."}, {"title": "Proposed Flow", "content": "The proposed research methodology includes following steps:\n\u2022 Selecting and loading required dataset.\n\u2022 Exploratory data analysis of the dataset in hand.\n\u2022 Pre-processing of dataset.\n\u2022 Creating embedding representation of all the documents in the corpus.\n\u2022 Perform clustering on top k documents related to the query.\n\u2022 Select hard negatives from clustering result.\n\u2022 Fine-tune ranking model with hard negatives.\n\u2022 Evaluate and conclude the study.\nThe experiment pipeline is described in Figure3.1."}, {"title": "Dataset description and Pre-processing", "content": "In this research, the main aim is to enhance the re-ranking model performance on domain specific enterprise dataset. The research would capture and focus on the results on the private datasets of a company. The datasets will be split into train and test sets. For this research, we'll only use the test data to evaluate our results, just like other methods that are evaluated on similar dataset."}, {"title": "Private Enterprise Dataset", "content": "Company's domain specific data is used in this research study. This data consists of URLs and full document, and it is well suited for document ranking task just like MS-MARCO (Bajaj et al., 2016) document ranking dataset. The documents are related to cloud domain and provide information about various cloud services and products. Enterprise has lots of documentation published in public or private web pages. The corpus of dataset contains 36,871 URLs and with pre-processing we must scrape the data from these URLs. The given corpus comprises documents of 30+ services/products. Figure 3.2 shows the distribution of the documents across services."}, {"title": "Data Pre-processing", "content": "In this research, we aim to use enterprise dataset using documentation URLs. We will begin with document text extraction, data transformation and all text will undergo basic normalization steps including cleaning, spell correction and tokenization. We also focus on creating triplets for training once we get pair, it is augmented with hard negatives."}, {"title": "Text extraction from URLs", "content": "The dataset consists of URLs of published web pages and text extraction is a very critical pre- processing step. We will use various web scraping tools and libraries such as Beautiful Soup, Selenium, Scrapy etc. and find out which one works best for our case.\n\u2022 Beautiful Soup: Beautiful is a very powerful Python library for web scraping, especially for parsing HTML and XML documents. One of its most notable advantages is its convenience. It is very easy to use and navigate. It empowers you to try various techniques.\n\u2022 Selenium: Selenium is a versatile web scraping library that automates web browsers, enabling interaction with dynamic web content. It supports multiple programming languages, including Python, and can handle JavaScript-heavy websites that static scrapers might miss. Selenium is often used for tasks like form submission, navigation, and data extraction from complex web pages. Its capability to mimic real user behavior makes it a powerful tool for web scraping.\n\u2022 Scrapy: Scrapy is a robust Python framework for web scraping and web crawling. It allows developers to extract data from websites and process it as needed. Scrapy's architecture is"}, {"title": "Data Transformation", "content": "In the data transformation step, we transform the html/markdown text scraped from the URLs into the plain text format. The plain text format makes much more sense to humans as well as model. We use html2text library for converting html or markdown format data to text. We apply some custom cleaning to remove web page sidebars, headers and footers."}, {"title": "Data augmentation with hard negative mining", "content": "Augmenting data with hard negative is not just a pre-processing step but also a major part of the thesis to study and research how to find hard negatives which can boost model performance and help model to understand better the intricacies of domain specific dataset. We first generate embeddings for each document from different models, get the final one with ensemble and then perform clustering method to get the hard negative document for that query. A detailed explanation is provided in the next sections on hard negative mining."}, {"title": "Methodology", "content": "In this research, we will use multiple models to enhance our data with hard negatives. Initially, we will utilize multiple bi-encoder models to generate diverse embeddings for each document. These embeddings will then be combined using an ensemble technique to create a robust representation for each document. Following the embedding generation, clustering will be performed based on these embeddings. Using a similarity metric, we will identify additional positive documents for each query, as well as particularly challenging hard negative documents. This augmented dataset, enriched with hard negatives, will subsequently be used to train cross-encoder re-ranking models."}, {"title": "Embedding models (Bi encoders)", "content": "Bi-encoder models are pivotal in natural language processing (NLP) for creating document embeddings. These models independently encode input texts (e.g., queries and documents) into dense vector representations, or embeddings, in a shared latent space. This approach facilitates efficient similarity comparisons and retrieval tasks. A biencoder typically comprises two identical encoder networks, often based on transformer architectures such as BERT (Devlin et al., 2019a), ROBERTa (Liu et al., 2019). The encoders share weights, ensuring that the embeddings reside in a compatible semantic space, which is essential for measuring the relevance between the query and document vectors.\nThe primary advantage of biencoders lies in their computational efficiency during inference. And hence, these are instrumental in scenarios requiring large-scale document retrieval, where the goal is to narrow down a vast pool of documents to a manageable subset before applying more computationally intensive models, such as cross-encoders, for fine-grained ranking. In the proposed methodology we propose to use pre trained state of the art embeddings models to create ensemble. Some of the embedding models are"}]}