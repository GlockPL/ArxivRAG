{"title": "CONTINUAL LEARNING IN MACHINE SPEECH CHAIN\nUSING GRADIENT EPISODIC MEMORY", "authors": ["Geoffrey Tyndall", "Kurniawati Azizah", "Dipta Tanaya", "Ayu Purwarianti", "Dessi Puji Lestari", "Sakriani Sakti"], "abstract": "Continual learning for automatic speech recognition (ASR)\nsystems poses a challenge, especially with the need to avoid\ncatastrophic forgetting while maintaining performance on\npreviously learned tasks. This paper introduces a novel ap-\nproach leveraging the machine speech chain framework to\nenable continual learning in ASR using gradient episodic\nmemory (GEM). By incorporating a text-to-speech (TTS)\ncomponent within the machine speech chain, we support\nthe replay mechanism essential for GEM, allowing the ASR\nmodel to learn new tasks sequentially without significant\nperformance degradation on earlier tasks. Our experiments,\nconducted on the LJ Speech dataset, demonstrate that our\nmethod outperforms traditional fine-tuning and multitask\nlearning approaches, achieving a substantial error rate reduc-\ntion while maintaining high performance across varying noise\nconditions. We showed the potential of our semi-supervised\nmachine speech chain approach for effective and efficient\ncontinual learning in speech recognition.", "sections": [{"title": "1. INTRODUCTION", "content": "The exceptional performance of deep learning architectures,\nas illustrated by the Transformer model [1], has enabled\nstate-of-the-art automatic speech recognition (ASR) systems\nto reach levels of accuracy comparable to human perfor-\nmance [2, 3, 4]. These advancements have significantly\nenhanced speech recognition capabilities. However, a critical\nchallenge persists: ASR systems should be capable of rec-\nognizing a continuous stream of tasks. Despite the existence\nof large-scale speech models [5, 6] that excel in multitask\nperformance, these models demand substantial resources in\nterms of data and computational power, and they require\nthe availability of all tasks from the beginning, i.e., offline\nlearning.\nAn alternative approach to this issue is fine-tuning, which\ntransfers knowledge from one task to another, or multitask\nlearning, where the model is trained from scratch using\nboth previous and new task data simultaneously. Unfortu-\nnately, the former approach (transfer learning) can degrade\nthe model's performance on earlier tasks due to catastrophic\nforgetting [7]. Meanwhile, the latter approach necessitates re-\ntaining old data to mix with new task data, potentially leading\nto privacy concerns.\nContinual learning is a paradigm designed to allow mod-\nels to learn new tasks sequentially without compromising\ntheir ability to perform previous tasks or violating data pri-\nvacy. Its effectiveness in sequentially handling multiple\nrecognition tasks was recently demonstrated in [8].\nUnlike existing fully supervised methods for conducting\ncontinual learning experiments on ASR, this paper proposes\na semi-supervised approach within the machine speech chain\nframework [9]. Our method integrates text-to-speech (TTS)\nto support a replay mechanism in continual learning. We\nadopt gradient episodic memory (GEM) [10] as our chosen\nimplementation for this replay-based continual learning sce-\nnario.\nWe evaluate our proposed method against other prevalent\nlearning paradigms such as fine-tuning and multitask learn-\ning. Our results indicate that continual learning within the\nmachine speech chain framework offers superior performance\ncompared to these traditional methods and serves as a viable\nalternative to fully supervised continual learning. Although\nthe upper bound fully supervised continual learning achieves"}, {"title": "2. RELATED WORK", "content": ""}, {"title": "2.1. Machine Speech Chain", "content": "Machine speech chain is an architecture that connects sequence-\nto-sequence model of automatic speech recognition (ASR)\nand text-to-speech (TTS) in a closed-loop framework. This\nintegration was proposed to be representative of human\nspeech chain mechanism [9], which is listening while speak-\ning [11]. To date, machine speech chain has been used in"}, {"title": "2.2. Gradient Episodic Memory", "content": "Gradient episodic memory (GEM) is a replay method of con-\ntinual learning paradigm [10]. GEM exploits samples from\nthe past task's data when encountering the data of a new task\nto minimize the L2 distance between the gradients of the new\ntask's data and the old ones' data, i.e.,\n$\\begin{equation}\n\\begin{aligned}\n&\\min_{\\bar{g}} \\frac{1}{2} \\|g - \\bar{g}\\|^2\n\\\\\n&s.t. (\\bar{g}, g_k) \\geq 0, \\forall k \\in \\{0, ..., i - 1\\},\n\\end{aligned}\n\\end{equation}$$\nwhere $g, g_k \\in R^{|\\theta|}$ and $|\\theta|$ is the number of model's param-\neters. ASR model that was equipped with GEM in previous\nfinding (see [8]) outperformed regularization-based methods\nsuch as synaptic intelligence [15], or knowledge distillation\n[16], in a continual learning scenario with different acous-\ntic and topic domain acted as task boundary. In this paper,\nwe introduce GEM usage in machine speech chain and we\ndemonstrate first-hand to show its potential."}, {"title": "3. MACHINE SPEECH CHAIN USING GEM", "content": "We introduce a three-stage mechanism designed to en-\nable ASR models to perform continual learning in a semi-\nsupervised manner, achieving satisfactory results with mini-\nmal forgetting. These three stages, depicted in Figure 1, build\nupon the process proposed in [9], for the first and second\nstages, with our continual learning method introduced in the\nthird stage.\n1. First stage: Supervised learning on the base task.\nHere, ASR and TTS are trained separately in a super-\nvised manner to ensure strong baseline performance\nfor the subsequent training stages.\n2. Second stage: Semi-supervised learning. At this stage,\nASR and TTS mutually enhance each other by training\non unlabeled data from the base task, using unsuper-\nvised methods to improve performance.\n3. Third stage: Continual learning. ASR engages in con-\ntinual learning for new tasks using replayed inputs from\nthe base task, synthesized by TTS.\nIn our approach, the replay process for speech recogni-\ntion leverages TTS as a synthesis model to generate pseudo-\nsamples of the base task. These pseudo-samples are stored in\nepisodic memory and used by GEM to regulate the gradients\nfor both new and previous tasks."}, {"title": "4. EXPERIMENTS", "content": ""}, {"title": "4.1. Experimental Setup", "content": "We prepared two tasks for the ASR models to recognize.\nThe first task, referred to as the base task, utilized the clean\noriginal dataset of LJ Speech [19], consisting of 24 hours of\naudio. To simulate different scenario for the subsequent task,\nwe created a noisy version of the original speech dataset.\nThis noisy dataset also comprises 24 hours of audio, but with\nadded white noise at a signal-to-noise ratio (SNR) of 0. Con-\nsequently, the base task is denoted as LJ Original, and the\nsubsequent task is denoted as LJ Noisy. Both datasets were\nsplit into train, dev, and test sets with a ratio of 94%, 3%, and\n3%, respectively.\nFor the ASR architecture, we employed the Speech-\nTransformer [20], while the TTS architecture was based on\nthe Transformer-based Tacotron 2 [21]. All of the ASR\nmodels did not involve hyperparameter tuning since they al-\nready employed almost identical hyperparameters to those\nthat had been used in [20]. The architecture of the ASR\nmodels employed 12 encoder blocks, 6 decoder blocks, 4\nattention heads, and a feed-forward hidden layer size of 2048.\nWe used 80 dimensions for the Mel-spectrogram input. We\ntrained the models using the Adam optimizer with \u03b2\u2081 = 0.9,\n\u03b22 = 0.98, \u20ac = 10-9 and employed cross-entropy loss with\nneighborhood smoothing. The episodic memory that we used\nfor continual learning had size of 100 samples per task, or in\nother word 1% of dataset size.\nFor TTS models that are needed in machine speech chain\ncondition, we configured them to be consisted of 6 encoder\nblocks for the transformer-based encoder, 6 decoder blocks\nfor the autoregressive decoder, 8 heads, and a feed-forward\nhidden layer size of 2048. These values were identical to the\nbest configuration that had been used in [21]. The TTS input\nwas the character sequence, and the output was the 80 dimen-\nsions of the Mel-spectrogram. We used the Adam optimizer\nwith the same B1, B2, \u20ac values and employed cross-entropy\nloss.\nOur experiment involved training ASR models under su-\npervised conditions: lower bound and upper bound, and our\nproposed method that involved semi-supervised condition:\nmachine speech chain. The upper and lower bound refers\nto the amount of base task data provided to the ASR model"}, {"title": "4.2. Experiment Result", "content": ""}, {"title": "4.2.1. Continual Learning Performance", "content": "The experimental results, as detailed in Table 1, demonstrate\nthe efficacy of various continual learning approaches applied\nto the ASR model in both clean (LJ Original) and noisy (LJ\nNoisy) conditions. The ASRLower results show that the GEM\napproach significantly reduces the character error rate (CER)\ncompared to fine-tuning and multitask learning. For instance,\nGEM achieved a CER of 8.5% on LJ Original and 15.8%\non LJ Noisy, outperforming the fine-tuning method which re-\nsulted in CERs of 19.0% and 31.3% respectively. Multitask\nlearning, however, showed the highest CERs of 74.8% and\n76.7%, indicating its limitation in handling noise without op-\ntimal balance of data.\nThe ASRSpeechChain model trained with GEM outper-\nformed the fine-tuning method, achieving CERs of 11.1%\nand 15.5% for LJ Original and LJ Noisy respectively. This is\na significant improvement over fine-tuning, which recorded\nCERs of 12.7% and 33.1%. Furthermore, comparing the\nGEM method across different models, ASRUpper using GEM\nachieved the lowest CERs at 5.2% and 8.4%, compared to\nfine-tuning and multitask methods. However, it is impor-\ntant to highlight that the ASRSpeechChain model, despite not\nreaching the lowest error rates, still showed substantial im-\nprovements. The ASRspeechChain model with GEM achieved"}, {"title": "4.2.2. Continual Learning Comparison", "content": "We also compared our semi-supervised method to the other\ncontinual learning methods which are carried out in a fully\nsupervised scenario. These other methods were gradient\nepisodic memory (GEM) and elastic weight consolidation\n(EWC) [22]. We can see from Figure 2 that the learn-\ning curves exhibit the superiority of GEM, as models that\nleveraged GEM as their replay process were able to prevent\ncatastrophic forgetting. Although EWC had worse forgetting\nprevention, it performed better on learning new task because\nof its fully supervised scenario.\nWe also computed the continual learning metrics, such as\naverage (AVG), backward transfer (BWT), and forward trans-\nfer (FWT) character error rate, as shown in Figure 2, which\nwere useful for comparing the three models to each other. In\nour experiment, BWT is defined as the ability of a model to\ntransfer the lowest possible error to the previous task it has\nencountered, while FWT is defined as the ability of a model\nto learn a new task with the lowest possible error compared to\nthe error rate attained by the standard fine-tune method.\nGEM, when applied in a supervised ASR system, as ex-\npected, achieved the lowest of all the metrics. EWC had\na slightly lower AVG at 12.5% than ASRSpeechChain, which\nachieved 13.3%. Our model performed well in reducing for-"}, {"title": "5. CONCLUSION", "content": "We proposed a novel method to allow automatic speech\nrecognition (ASR) model to perform continual learning in a\nsemi-supervised manner of machine speech chain. We then\ndemonstrated first-hand the implementation of such replay\nmethod with gradient episodic memory (GEM). Although\nour upper bound supervised model achieved lower CER\nthan our proposed method, the machine speech chain-based\nmethod managed to get the same 40% averaged error rate\nreduction. Furthermore, we compared both machine speech\nchain that was trained under the proposed continual learning\nscenario with the machine speech chain under the fine-tuning\nscenario. We found that our method worked and achieved\nminimal forgetting, or prevented catastrophic forgetting. This\nshowed that our novel method has potential for further appli-\ncation of speech recognition and can serve as an alternative\nto the fully supervised mechanism of continual learning. We\nbelieve this paper provides the first exploration of contin-\nual learning in machine speech chain framework and makes\na step towards realizing effective and efficient learning for\nspeech recognition."}, {"title": "6. LIMITATIONS", "content": "We acknowledge the need for further experiments to assess\nthe generalizability of our approach. While this work demon-\nstrates success on a simple task boundary of noise variation,\nfuture work will involve applying our method to a wider range\nof tasks, such as multilingual speech recognition (where the\nmodel needs to adapt to different phonetic inventories) or\ntask-agnostic continual learning (where tasks are not prede-\nfined). This will allow us to investigate the effectiveness of\nour method in handling more complex scenarios and poten-\ntially lead to a more robust continual learning for ASR in\nmachine speech chain framework."}, {"title": "7. ETHICS STATEMENT", "content": "Our study followed the scientific methodology and ethics.\nThe LJ Speech dataset that we used is a public domain dataset\nwhich is not in violation of license and data ethics. LJ Speech\ndataset is an English language speech dataset consisting of\n13,100 short audio clips of a single speaker reading passages\nfrom 7 non-fiction books. The audio part was recorded and"}]}