{"title": "Unveiling Factual Recall Behaviors of Large Language Models through\nKnowledge Neurons", "authors": ["Yifei Wang", "Yuheng Chen", "Wanting Wen", "Yu Sheng", "Linjing Li", "Daniel Dajun Zeng"], "abstract": "In this paper, we investigate whether Large\nLanguage Models (LLMs) actively recall or\nretrieve their internal repositories of factual\nknowledge when faced with reasoning tasks.\nThrough an analysis of LLMs' internal factual\nrecall at each reasoning step via Knowledge\nNeurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain\ncircumstances. Instead, they tend to opt for\nalternative, shortcut-like pathways to answer\nreasoning questions. By manually manipulat-\ning the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this\nrecall process directly improves reasoning per-\nformance whereas suppressing it leads to no-\ntable degradation. Furthermore, we assess the\neffect of Chain-of-Thought (CoT) prompting,\na powerful technique for addressing complex\nreasoning tasks. Our findings indicate that CoT\ncan intensify the recall of factual knowledge\nby encouraging LLMs to engage in orderly and\nreliable reasoning. Furthermore, we explored\nhow contextual conflicts affect the retrieval of\nfacts during the reasoning process to gain a\ncomprehensive understanding of the factual re-\ncall behaviors of LLMs. Code and data will be\navailable soon.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Mod-\nels have underscored their exceptional reason-\ning prowess with natural language understanding\nacross a broad spectrum of tasks (Chen et al.,\n2023a; Kojima et al., 2022; Brown et al., 2020;\nCreswell et al., 2023). However, amidst these\nachievements, a specific form of reasoning has\nbeen somewhat overlooked and insufficiently inves-\ntigated: reasoning tasks that entail the utilization\nof internal factual knowledge associations. For\ninstance, when presented with a 2-hop question\nsuch as \"Who is the chairperson of the manufac-\nturer of the Holden Caprice?\", LLMs must first\nidentify the manufacturer of the Holden Caprice\nas General Motors, and subsequently retrieve the\nchairperson of General Motors from their inter-\nnal knowledge or parametric knowledge (Neeman\net al., 2023; Zhong et al., 2024). Factual knowledge\nis observed to emerge in both GPT (Meng et al.,\n2022) and Bert models (Petroni et al., 2019; Jiang\net al., 2020). Unlike mathematical (Floyd, 2007)\nand logical reasoning (Pan et al., 2023), factual\nreasoning heavily relies on the factual knowledge\nencoded within LLMs, acquired through extensive\npretraining on vast corpora, rather than on user-\ninputted premises. At the same time, it differs\nfrom commonsense reasoning (Zhao et al., 2023;\nTrinh and Le, 2019), which taps into general knowl-\nedge acquired through dynamic training to foster\na holistic understanding of the world, instead of\nemphasizing specific factual information.\nIntuitively, it is reasonable to expect LLMs\nto harness their extensive internal knowledge to\ntackle reasoning tasks. Yet, an important ques-\ntion emerges: How effectively can LLMs actually\nretrieve and utilize their internal knowledge for\nreasoning purposes? Delving into this question is"}, {"title": "2 Preliminaries", "content": "crucial for a multitude of reasons. First, LLMs'\nefficient use of internal knowledge may greatly\nreduce reliance on external data sources, thereby\nlowering operational costs of data retrieval and API\nusage. Second, this dynamic capability allows the\nfactual knowledge within LLMs to flow and in-\nterconnect (Onoe et al., 2023), showcasing these\nmodels as organic entities rather than static infor-\nmation repositories (Petroni et al., 2019). From\na practical perspective, LLMs' accurate retrieval\nand application of facts lead to more reliable and\ninterpretable reasoning, enhancing their utility and\ntrustworthiness in real-world applications.\nTransformer-based language models have accu-\nmulated substantial knowledge through extensive\npretraining (Vaswani et al., 2017). A significant\nbody of recent research has focused on the factual-\nity issues of LLMs (Wang et al., 2023). One stream\nof this research has concentrated on pinpointing the\nlocations within these models' architectures where\nfactual knowledge is stored and encoded (Meng\net al., 2022; Dai et al., 2022; Wallat et al., 2020;\nGeva et al., 2022, 2021). Simultaneously, there has\nbeen a concerted effort to understand the mecha-\nnism by which this knowledge is accessed during\nthe inference phase (Geva et al., 2023; Yang et al.,\n2024). Another line of work discusses the balance\nof the retrieved knowledge and its parametric coun-\nterparts (Kwiatkowski et al., 2019; Kandpal et al.,\n2022; Yu et al., 2023). However, the majority of\nthese studies have either been confined to elemen-\ntary retrieval tasks, such as recalling a single fact\nobject o from a given triplet (s, r, o), or have not\ndelved into the intricacies of factual knowledge re-\ncall and utilization in more advanced challenges,\nparticularly within complex reasoning scenarios.\nOur work addresses these limitations by examining\nthe inner dynamics of factual recall within LLMs\nduring the two-hop factual reasoning process, pro-\nviding fresh insights into the behavior of factual\nrecall in reasoning and highlighting avenues for en-\nhancing the robustness and reliability of reasoning\nthrough more sophisticated knowledge utilization\nstrategies.\nIn this work, we investigate the harness of inter-\nnal knowledge for reasoning through the lens of\nKnowledge Neurons (KNs). We focus on the basic\nsetting of factual reasoning involving the composi-\ntion of two facts (for example, \"Who is the chair-\nperson of the manufacturer of Holden Caprice?\" in\nFigure 1). To achieve this, we carefully craft two-\nhop reasoning questions dataset that seamlessly"}, {"title": "2.1 Problem Formulation", "content": "We represent facts, such as \"(Holden Caprice, man-\nufacturer, General Motors)\", as a triplet (s, r, o),\nwhere s is the subject, r is the relation, and o\nis the object. We formulate two-hop factual rea-\nsoning questions as a composition of two linked\nfacts ((s, r1, 01), (01, r2, 02)), with a bridge entity\n01 connecting them. To query LLMs, these triplets\nmust be converted into natural language queries.\nFor a single relation r, we instruct ChatGPT (gpt-\n3.5-turbo) to generate query templates as QT,(\u00b7).\nFor instance, the single-relation triplet (Holden\nCaprice, manufacturer, General Motors) can be\nconverted as QTmanufacturer(HoldenCaprice):\n\"Which company manufactures Holden Caprice?\"."}, {"title": "2.2 Knowledge Neurons", "content": "Similarly, for a composition of two relations r1\nand r2, we prompt ChatGPT to generate a query\ntemplate as QTr\u2082(r1(\u00b7)), with r\u2081(\u00b7) denoting the\ndescription of the entity related to s via r\u2081 rela-\ntion (e.g. The manufacturer of Holden Caprice).\nWe refer to the single-hop query as QT1H and the\ntwo-hop query as QT2H.\nWe consider an autoregressive language model\nF : X \u2192 Y, which accepts an input x \u2208 X and\nproduces a prediction y \u2208 Y, continuing the input\nx. We deem that the model \"knows\" a fact (s, r, o)\nif the output F(QT\u2084(s)) matches the ground la-\nbel o and that LLMs can reason a question involv-\ning two-hop fact triplets ((s,r1, 01), (01, 02, 02))\nsuccessfully if the output F(QTr\u2082 (r1(s)) matches\nthe ground label 02. It is noteworthy that query\ntemplates, even for the same single relation, are\ngenerated with diversity by ChatGPT. This diver-\nsity discourages models from making predictions\nbased on the occurrence of specific words, ensur-\ning that they recall knowledge from within them-\nselves instead. We denote the set of two-hop factual\nquestions as \u03a9, with \u03a9\u03c4 representing the subset of\nquestions that LLMs can answer correctly and \u03a9F\ndenoting the subset of questions that LLMs cannot\nanswer correctly. For simplicity, we use ( to denote\n((s, r1, 01), (01, 02, 02)), thus we have:\n\u03a9\u03c4 = {\u03b6 | Fo(QTr\u2082 (r1(s))) = 02, \u2200\u03b6\u2208\u03a9}\n\u03a9F = {5 | Fo(QTr\u2082(r1(s))) \u2260 02, \u2200\u03b6\u2208 \u03a9}\nPretrained language models store vast amounts of\nfactual knowledge and have a strong ability to re-\ncall this factual knowledge without further training\n(Petroni et al., 2019; Jiang et al., 2020). Drawing\ninspiration from the key-value-memory nature of\nfeed-forward layers (Geva et al., 2021), Dai et al.\n(2022) proposes that factual knowledge is stored\nin specific neurons within the Feed-Forward Net-\nworks (FFNs) of the Transformer models, termed\nas knowledge neurons. They find that knowledge\nneurons are activated by knowledge-expressing\nprompts. The higher the activation of these knowl-\nedge neurons is, the more significantly their corre-\nsponding facts are expressed. Therefore, to assess\nthe recall and utilization of the fact triplet (s, r, o)\nnecessary in the reasoning process, we refer to the\nactivity of KNs as an indicator of factual recall. We\nmake the following invariant assumptions: the\nKNs responsible for the expression of particular\nrelational facts remain consistent across different"}, {"title": "3 TFRKN: Two-hop Factual Reasoning\nfor Knowledge Neurons", "content": "application contexts. A specific fact is indicated by\nthe same set of KNs under both single-hop queries\nand reasoning queries, which is a cornerstone for\nsubsequent experiments. In Appendix B, We detail\na methodology utilizing integrated gradients (Sun-\ndararajan et al., 2017) to compute the contribution\nof all neurons in the intermediate layers of FFNs to\nthe correct prediction of a multi-token ground truth,\nidentifying neurons with greater contributions as\nKNs.\nTo investigate the behavior of factual recall\nin reasoning tasks for LLMs, we have devel-\noped a dataset called TFRKN (Two-hop Factual\nReasoning for Knowledge Neurons). This dataset\nis composed of two-hop factual questions, which\nare constructed with frequently occurring entities\n(Mallen et al., 2023) in Wikidata (Vrande\u010di\u0107 and\nKr\u00f6tzsch, 2014) and manually selected relations.\nThe construction method is detailed in Appendix\nA. TFRKN dataset encompasses 4,550 distinct in-\nstances that cover 213 unique relational combina-\ntions. In alignment with the KN methodology, we\nhave reformulated each fact triplet into over five\nvaried query forms with the aim of refining true-\npositive KNs from specific-form queries. (An in-\nstance in TFRKN is shown in Table 6)."}, {"title": "4 Diagnose the Pitfalls of Factual Recall\nin Reasoning", "content": "In the realm of two-hop factual reasoning, an opti-\nmal and dependable reasoning trajectory is a multi-\nhop reasoning approach (Welbl et al., 2017; Ju\net al., 2024). This process requires identifying the\nbridge entity first and then using it to solve the sec-\nond hop question, necessitating that LLMs recall\nthe relevant fact at each hop step by step, culminat-\ning in the formulation of the correct answers. In\nthis section, we investigate whether LLMs faith-\nfully retrieve factual knowledge at each hop when\nundertaking reasoning tasks."}, {"title": "4.1 KN Scores", "content": "To quantify the capacity for internal recall of spe-\ncific facts within LLMs, we devise a novel metric,"}, {"title": "4.3 Results", "content": "termed KN Scores, as follows:\nFFN)(H)) = W SiLU (H) W)\n= SiLU (H) W) [i], \u03bd\u03c9\u03b5\u03c9\nKN Scores =\n\u03a3\u03c9, \u03bd\u03c9\u03b5\u03c9\nwhere w denotes the i-th neuron in the l-th inter-\nmediate layer of FFN and symbol w represents the\nKNs associated with a specific fact triplet, denoted\nas (s, r, o). For the first-hop fact and the second-\nhop fact, we designate their respective sets of KNs\nas w\u2081 and w2. Under the context of a single-hop\nquery, we denote KN Scores as {W|QT\u2081H}. Sim-\nilarly, within the two-hop reasoning context, KN\nScores are represented as {W|QT2H}.\nSingle-hop vs. Muti-hop Reasoning In reason-\ning scenarios, LLMs access their internal knowl-\nedge less frequently in comparison to the straight-\nforward retrieval of single-hop facts. Table 1 il-\nlustrates a notable decrease in KN Scores for all\nsingle-hop facts when addressing two-hop reason-\ning questions. This observation strongly indicates\nthat, in reasoning contexts, LLMs tend to either fail\nto recall the bridge entity or struggle to identify the\nsecond-hop relation, leading to the failure of exe-\ncuting the remaining multi-hop reasoning as antic-\nipated. Compared to directly recalling single-hop\nfacts (e.g., \"Who is the chairperson of General Mo-\ntors?\"), it is more challenging for LLMs to recall\nand organize relevant facts for reasoning. LLMs\nmay take alternative salient pathways existing in\ntheir parameters, such as shortcuts, rather than en-\ngaging in systematic, step-by-step reasoning."}, {"title": "4.2 Experiment", "content": "Setup We begin by filtering out reasoning ques-\ntions where LLMs are unable to recall all individual\nfacts, ensuring that any reasoning failures are due to\nthe models' inability to retrieve factual information\nrather than a lack of the foundational knowledge\nnecessary for performing reasoning tasks. We then\nproceed to employ Fact\u2081Query and Fact2Query (in\nTable 6) from each data point to pinpoint the posi-\ntions of KNs for each-hop fact. Then we hook\nthe values of each neuron belonging to w\u2081 and\nW2 across various query scenarios to compute KN\nScores. Using the KN Scores metric, we evaluate\nthe recall of each fact under three distinct experi-\nmental conditions: no CoT, zero-shot CoT, and\nfew-shot CoT. For each condition, we record KN\nScores for both the first-hop {W1QT2H} and the\nsecond-hop {W2|QT2H} facts within the context of\ntwo-hop reasoning questions. We select the KN\nScores {1|QT1H} and {W2|QT\u2081H} under single-\nhop queries as baselines since KNs are significantly\nactive in that straightforward context. We experi-\nment with the instructed versions of three popular\nopen-source models: LLaMA2-7B (Touvron et al.,\n2023), LLaMA3-8B, Mistral-7B (Jiang et al., 2023)\n(see Appendix D for more experimental details)."}, {"title": "5 Interventions on the Recall of Facts", "content": "FFN)(H)) = W SiLU (H) W) (3)\n= SiLU (H) W) [i], \u03bd\u03c9\u03b5\u03c9 (4)\nKN Scores =\n\u03a3\u03c9, \u03bd\u03c9\u03b5\u03c9 (5)\nwhere w denotes the i-th neuron in the l-th inter-\nmediate layer of FFN and symbol w represents the\nKNs associated with a specific fact triplet, denoted\nas (s, r, o). For the first-hop fact and the second-\nhop fact, we designate their respective sets of KNs\nas w\u2081 and w2. Under the context of a single-hop\nquery, we denote KN Scores as {W|QT\u2081H}. Sim-\nilarly, within the two-hop reasoning context, KN\nScores are represented as {W|QT2H}.\nCoT vs. No CoT CoT, whether zero-shot or few-\nshot, markedly improves factual knowledge utiliza-\ntion in LLMs over no CoT (see a case in Figure\n2), which is evidenced by a higher \u0394\u03c9\u2081 and \u0394\u03c9\ncompared with no CoT setting, as shown in Table 1.\nWe posit that this enhancement is likely driven by\nthe step-by-step thinking process, which further\nstimulates the recall of facts as multi-hop reason-\ning progresses. This hypothesis can be supported\nby comparing the zero-shot CoT and few-shot CoT\nsettings. Across three models, it is clear that zero-\nshot CoT struggles to significantly improve the\nrecall of the second-hop fact compared to the re-\ninforcement of the first-hop fact recall. However,\nconsistent improvement across both triplets can be\nobserved for few-shot settings. This observation\nstrongly suggests that the reasoning direction in\nzero-shot scenarios is unclear, which prevents mod-\nels from effectively identifying which relations of\nfacts concerning the bridge entity to retrieve. In"}, {"title": "5.1 Enhance and Suppress KNs", "content": "stark contrast, few-shot scenarios often mitigate\nthis issue. Through the acquisition of knowledge\nfrom contextual demonstrations, models are more\ninclined to determine the subsequent phase in the\nreasoning trajectory and, in turn, adeptly utilize\nthe relevant factual information via their attention\nmechanisms.\nFactual Recall vs. Reasoning Accuracy Figure\n3 illustrates a positive correlation between the re-\ncall of relevant fact triplets and reasoning accuracy.\nThis relationship is especially pronounced in the\ncase of LLaMA3-8B model under few-shot CoT,\nwhere the maximum increase in the recall of both\n\u0394\u03c9\u2081 and \u0394\u03c92 leads to the highest reasoning accu-\nracy. The eliciting effect of CoT on factual recall\nacross various LLMs is not uniform. For instance,\nzero-shot CoT mitigates the forgetting of factual in-\nformation to some extent for LLaMA2-7B, whereas\nfor LLaMA3-8B, zero-shot CoT enhances the re-\ntrieval of factual information to a level comparable\nto few-shot CoT. This adequately illustrates that\nthe efficacy of CoT is also contingent upon the in-\ntrinsic capabilities of the LLMs themselves when\nthey are of nearly the same scale.\nTo gain a deeper understanding of factual recall\nbehaviors, we intervene in the retrieval of specific\nknowledge within LLMs by manually adjusting\nthe activation levels of KNs. Specifically for each\nfactual triplet (s,r,o), we modulate the internal\nrecall by adjusting the values of the KNs associated\nwith this triplet, either amplifying or diminishing"}, {"title": "5.2 Experiment", "content": "them according to Equation 6.\n{\nEnhance: w = n \u00d7 , n > 1, \u2200w \u2208 w(s,r,o)\nSuppress: w = 0, \u03c9\u03b5\u03c9(s,r,o)\n(6)\nSetup We have meticulously designed four sets\nof controlled experiments on TFRKN to monitor\nchanges in reasoning outcomes. The experimen-\ntal paradigms are as follows: (1) Base: We allow\nLLMs to respond to two-hop questions under stan-\ndard conditions (2) Enhance: For questions an-\nswered incorrectly under Base situation, we am-\nplify the activation level of KNs and subsequently\nassess the reasoning accuracy. (3) Suppress: Con-\nversely, for two-hop questions correctly answered\nin the Base scenario, we reduce the activation of\nrelevant KNs and evaluate the reasoning accuracy\nafterward. (4) Random: To establish a baseline\nfor comparison with conditions (2) and (3), we\nrandomly select an equal number of neurons and\nenhance or suppress their activation accordingly,\nfacilitating a comparative analysis.\nMetrics We design a novel metric, termed En-\nhance Ratio (ER), which serves to quantify the\nimpact of factual retrieval failures on reasoning\noutcomes. ER is calculated by calculating the per-\ncentage of reasoning instances that are initially in-\ncorrect but are successfully resolved following the\nenhancement of KNs as Equation 7. Analogously,\nwe define another metric Suppress Ratio (SR) to\nmeasure the obstructive effect of suppressed KNs\non the reasoning process. The SR is ascertained by\nevaluating the ratio of cases where correct reason-\ning is converted to incorrect after the suppression\nof KNs, as outlined in Equation 8:\nER =\n|, \u03b6\u2208\u03a9F (7)"}, {"title": "6 Analysis of Shortcuts", "content": "LLMs to retrieve more facts sometimes, as evi-\ndenced by the high KN Scores for the first-hop\nfact of LLaMA2-7B when the knowledge distrac-\nSR =\n|{5 | Fo\u3003 (QTr\u2082 (r1(s)) \u2260 02}|,\n-, \u03b6\u2208 \u03a9\u03c4 (8)\nwhere ' denotes the parameters of the enhanced\nmodel while 0\" represents the parameters of the\nsuppressed model. QTr\u2082(r1(s)) represents the rea-\nsoning question derived from two-hop fact triplets\n((s, r1, 01), (01, r2, 02)) with the ground truth 02.\nFinding 1 In Table 2, more than one-third of\nreasoning failures are caused by issues of factual\nretrieval. The ER values show a consistent and pro-\ngressive increase as the interventions progress from\ntargeting w\u2081, to KNs associated with the second-\nhop w2, and ultimately to a combined intervention\non both, w12. This pattern indicates that many ini-\ntially incorrect answers stem from retrieval failure\nof either the first hop, the second hop, or both dur-\ning the reasoning process. Additionally, recalling\nthe second-hop facts is more challenging for LLMs,\nas shown by the higher ER after enhancing W2 com-\npared to w\u2081. Suppressing factual information sig-\nnificantly harms reasoning performance, with accu-\nracy dropping by over 77% on average when both\nfactual elements are suppressed. Therefore, the\nsuccessful retrieval of factual associations at each\nreasoning step is crucial for correct reasoning.\nFinding 2 CoT strengthens a passive internal re-\ntrieval of relevant facts, implicitly prompting the\nexpression of factual triplets. Evidence 1: In Table\n3, across the scenarios of no CoT, zero-shot CoT,\nand few-shot CoT, suppression of factual KNs re-\nsults in SRNo_cot > SRzero_shot and SRNo_cot >\nSRFew_shot, which indicates that CoT likely stimu-\nlates the hydra effect (McGrath et al., 2023), which\nimplements actively self-repairing computations to\ncompensate the suppression effects caused by low\nactivation levels of KNs. Evidence 2: Similarly,\nenhancement of factual KNs results in ERNo_cot <\nERZero_shot and ERNo_cot < ERFew_shot, which\nsuggests that CoT further stimulates the internal\nrecall process within LLMs, thus strengthening the\nenhancement effects of KNs. Therefore, CoT in-\ndeed can contribute to the recalling process."}, {"title": "6.1 Experiment", "content": "In this section, we investigate whether successful\ntwo-hop reasoning implies the successful recall of\nfactual knowledge. In other words, we examine\nwhether accurate reasoning outcomes stem from a\nthorough grounding in multi-hop knowledge rea-\nsoning or are facilitated by alternative shortcuts.\nSetup We investigate the utilization of individual\nfact triplets in correctly answered two-hop ques-\ntions by analyzing the KN Scores for each triplet.\nWe compare these scores with those observed dur-\ning single-hop queries to establish a threshold, de-\nnoted as T, which serves as a benchmark for iden-\ntifying the effective use of facts in the reasoning\nprocess. If the activation level of KNs falls signifi-\ncantly below this threshold in comparison to single-\nhop queries, this indicates an under-utilization of\nthe corresponding fact. Conversely, if it exceeds the"}, {"title": "7 Impact of Contextual Conflict", "content": "is hard for LLMs to fail\nto retrieve any factual information relevant when\npresented with the clues of overlapping entities or\nrelational vocabulary in queries. For most instances\nof shortcuts, LLMs prefer to utilize the second-hop\nfact to directly answer reasoning questions, skip-\nping the intermediate reasoning steps and relying\non the object 02 in the second-hop to cheat (a high\nratio for FT). For TF cases, there might exist the di-\nrect associations between the head entity s and the\ntail entity 02 leveraged to derive correct answers.\nThe capacity of utilizing internal factual knowledge\nis contingent not solely upon the intrinsic properties\nof LLMs, but is also significantly influenced by the\ncontext within which they operate. This section\nelucidates how the presence of knowledge conflicts\nwithin a given context can impact the mechanisms\nof the retrieval process during reasoning."}, {"title": "6.2 Results Analysis", "content": "threshold, the fact is considered adequately utilized.\nUsing this criterion, we classified the correctly an-\nswered questions into four distinct categories: (1)\nFT: Unsuccessful recall of the first-hop fact but\nsuccessful second-hop recall; (2) TF: Successful\nfirst-hop recall but unsuccessful second-hop recall;\n(3) FF: Neither fact successfully recalled and (4)\nTT: Both facts successfully recalled. Except for TT,\nthe other three situations are defined as Shortcuts.\nAccording to Table 5, under normal conditions, a\nconsiderable proportion of correctly answered ques-\ntions under no CoT setting rely on shortcuts, possi-\nbly due to word associations intrinsic to LLMs,as\nobserved by Yang et al. (2024). Notably, the\nMistral-7B model stands out for its unexpected\nreliance on shortcuts to solve over 44 percent of the\nquestions successfully. Even with large-scale mod-\nels possessing 7 billion parameters, LLMs still rely\non certain segments of the reasoning chain to arrive\nat answers. The introduction of CoT effectively\ndecreases the trend of taking shortcuts by forcing\nLLMs to recall more relevant facts and engage in\nmulti-hop reasoning. Under few-shot CoT setting,\nall LLMs solve over 90 percent of questions on\naverage through multi-hop reasoning, reducing the\nratio of shortcuts to nearly zero.\nFigure 4 provides a closer look at the shortcut\nphenomenon. The percentage of FF is significantly\nlow, illustrating that"}, {"title": "7.1 Experiment", "content": "Setup For each data point, we formulate a single-\nhop conflict fact by devising a set of potential ob-"}, {"title": "7.2 Results Analysis", "content": "The presence of knowledge conflict within the\ncontext consistently augments the faithfulness of\nLLMs in the corresponding fact. According to\nFigure 5 and Figure 6, the context of knowledge\nconflict results in the highest KN Scores of the cor-\nresponding hop fact, which indicates counterfactual\ncontext significantly improves the internal retrieval\nof that corresponding hop fact. It illustrates LLMs\nexhibit greater confidence in their encoded knowl-\nedge when confronted with knowledge conflict, a\nfinding that aligns with the studies conducted by\nZhou et al. (2023) and Li et al. (2023). When the\nknowledge presented in the context conflicts with\nthe second-hop fact, it not only reinforces the re-\ntrieval of the second-hop fact but also enhances the\nrecall of the first-hop fact. It is plausible that the\nintroduction of the subject 01 encourages LLMs to\nrecall the precise triplet (s, r1,01). However, this\neffect does not extend to the first-hop fact. The\noccurrence of knowledge distraction appears not to\ncause much obstruction to the factual recall within\nLLMs. On the contrary, it may even stimulate\nLLMs to retrieve more facts sometimes, as evi-\ndenced by the high KN Scores for the first-hop\nfact of LLaMA2-7B when the knowledge distrac-"}, {"title": "8 Related Work", "content": "jects denoted as Ocandi for its r. From this set, we\ndeliberately select an object o* \u2260 0 to introduce a\nknowledge conflict. In contradistinction, we also\nfabricate an entirely unrelated fact for each data\npoint to serve as a distractor, referred to as knowl-\nedge distraction. We then respectively append the\nknowledge conflict and knowledge distraction sen-\ntences before the two-hop question, which is input\ninto LLMs. Then we observe the values of KN\nScores for each-hop fact. The examples of knowl-\nedge conflict and distraction for the first-hop and\nthe second-hop facts are shown in Table 4.\ntor corresponding to the second-hop fact appears\nin Figure 6.\nMulti-hop Reasoning Multi-hop reasoning\nposes a significant challenge for LLMs. Several\nstudies have endeavored to address this chal-\nlenge through the development of more faithful\nreasoning techniques (Creswell and Shanahan,\n2022; Chen et al., 2023b; Creswell et al., 2023).\nOne such approach is CoT, which stimulates\nLLMs to produce deductive intermediate steps,\nfostering a step-by-step analytical process (Chu\net al., 2024). Another line of research is focused\non visualizing the implicit logical structures\nwithin LLMs from the perspective of mechanistic\ninterpretability (Yang et al., 2024). For example,\na recent study by Hou et al. (2023) recovers the\nreasoning tree from models's attention patterns\nusing MechanisticProbe.\nCoT Mechanism A large body of literature is\ndedicated to the theoretical and empirical explo-\nration of the mechanism underlying CoT (Saparov\nand He, 2023; Tan, 2023; Feng et al., 2023; Prys-\ntawski et al., 2023; Xie et al., 2024). Some research\nendeavors to delve into a reverse-engineering anal-\nsis of CoT prompting, uncovering the intricate\ninformation pathways that facilitate the generation\nof responses (Dutta et al., 2024). However, the ma-\njority of these studies concentrate on the rationales\nproduced by CoT and have largely overlooked the\nbroader implications for factual retrieval processes.\nIn our current work, we complement this aspect and\npresent compelling evidence that CoT significantly\nbolsters the internal recall of factual information."}, {"title": "9 Conclusions", "content": "This paper aims to provide a comprehensive under-\nstanding of factual recall behaviors for LLMs. We\nfind that a considerable portion of reasoning fail-\nures are due to retrieval failures. Manually enhanc-\ning the internal recall within LLMs can improve\nreasoning performance. For LLMs, they not only\nrely on multi-hop reasoning but also rely on other\ninference ways in LLMs such as shortcuts. CoT\ncan significantly stimulate LLMs to recall more\nfacts by compelling models to engage in step-by-\nstep thinking, diminishing the possibilities of tak-\ning shortcuts. The knowledge conflict existing in\ncontext could improve the confidence of parametric\nknowledge, therefore enhancing the internal recall.\nWhile our study provides novel insights into the\ninternal factual recall behaviors of LLMs during\nreasoning tasks, it is important to acknowledge\nseveral limitations.\nGeneralizability: While the current study is pri-\nmarily based on specific LLMs and the TFRKN\ndataset, future research should extend these find-\nings to verify their generalizability across various\nmodels and datasets\nTheoretical Analysis: Although empirical evi-\ndence has been provided through targeted interven-\ntions, a deeper theoretical analysis is needed to\nfully comprehend the underlying reasons for the\nobserved phenomena.\nPractical Applications: The paper discusses the-\noretical aspects and potential improvements in rea-\nsoning accuracy but does not delve into how these\nfindings can be applied in practical scenarios to\nenhance the reasoning capabilities of LLMs.\nImpact of Contextual Factors: While the paper\ntouches upon the influence of contextual conflicts\non knowledge retrieval, a more comprehensive anal-\nsis of various contextual factors and their impact\non reasoning is needed."}, {"title": "A Details of Dataset Construction", "content": "Limitations\nWhile our study provides novel insights into the\ninternal factual recall behaviors of LLMs during\nreasoning tasks, it is important to acknowledge\nseveral limitations.\nGeneralizability: While the current study is pri-\nmarily based on specific LLMs and the TFRKN\ndataset, future research should extend these find-\nings to verify their generalizability across various\nmodels and datasets\nTheoretical Analysis: Although empirical evi-\ndence has been provided through targeted interven-\ntions, a deeper theoretical analysis is needed to\nfully comprehend the underlying reasons for the\nobserved phenomena.\nPractical Applications: The paper discusses the-\noretical aspects and potential improvements in rea-\nsoning accuracy but does not delve into how these\nfindings can be applied in practical scenarios to\nenhance the reasoning capabilities of LLMs.\nImpact of Contextual Factors: While the paper\ntouches upon the influence of contextual conflicts\non knowledge retrieval, a more comprehensive anal-\nsis of various contextual factors and their impact\non reasoning is needed."}, {"title": "A.1 Sampling two-hop factual triples", "content": "Our dataset is constructed based on Wikidata (Vran-\nde\u010di\u0107 and Kr\u00f6tzsch", "relations": "n\u2022 P30, P36, P35, P1037, 1308, P164, P449, P488"}]}