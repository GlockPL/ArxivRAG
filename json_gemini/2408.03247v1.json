{"title": "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons", "authors": ["Yifei Wang", "Yuheng Chen", "Wanting Wen", "Yu Sheng", "Linjing Li", "Daniel Dajun Zeng"], "abstract": "In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks. Through an analysis of LLMs' internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances. Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions. By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation. Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs. Code and data will be available soon.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models have underscored their exceptional reasoning prowess with natural language understanding across a broad spectrum of tasks (Chen et al., 2023a; Kojima et al., 2022; Brown et al., 2020; Creswell et al., 2023). However, amidst these achievements, a specific form of reasoning has been somewhat overlooked and insufficiently investigated: reasoning tasks that entail the utilization of internal factual knowledge associations. For instance, when presented with a 2-hop question"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Problem Formulation", "content": "We represent facts, such as \"(Holden Caprice, manufacturer, General Motors)\", as a triplet (s, r, o), where s is the subject, r is the relation, and o is the object. We formulate two-hop factual reasoning questions as a composition of two linked facts ((s, r1, 01), (01, r2, 02)), with a bridge entity 01 connecting them. To query LLMs, these triplets must be converted into natural language queries. For a single relation r, we instruct ChatGPT (gpt-3.5-turbo) to generate query templates as QT,(\u00b7). For instance, the single-relation triplet (Holden Caprice, manufacturer, General Motors) can be converted as QTmanufacturer(HoldenCaprice): \"Which company manufactures Holden Caprice?\"."}, {"title": "2.2 Knowledge Neurons", "content": "Pretrained language models store vast amounts of factual knowledge and have a strong ability to recall this factual knowledge without further training (Petroni et al., 2019; Jiang et al., 2020). Drawing inspiration from the key-value-memory nature of feed-forward layers (Geva et al., 2021), Dai et al. (2022) proposes that factual knowledge is stored in specific neurons within the Feed-Forward Networks (FFNs) of the Transformer models, termed as knowledge neurons. They find that knowledge neurons are activated by knowledge-expressing prompts. The higher the activation of these knowledge neurons is, the more significantly their corresponding facts are expressed. Therefore, to assess the recall and utilization of the fact triplet (s, r, o) necessary in the reasoning process, we refer to the activity of KNs as an indicator of factual recall. We make the following invariant assumptions: the KNs responsible for the expression of particular relational facts remain consistent across different application contexts. A specific fact is indicated by the same set of KNs under both single-hop queries and reasoning queries, which is a cornerstone for subsequent experiments. In Appendix B, We detail a methodology utilizing integrated gradients (Sundararajan et al., 2017) to compute the contribution of all neurons in the intermediate layers of FFNs to the correct prediction of a multi-token ground truth, identifying neurons with greater contributions as KNs."}, {"title": "3 TFRKN: Two-hop Factual Reasoning for Knowledge Neurons", "content": "To investigate the behavior of factual recall in reasoning tasks for LLMs, we have developed a dataset called TFRKN (Two-hop Factual Reasoning for Knowledge Neurons). This dataset is composed of two-hop factual questions, which are constructed with frequently occurring entities (Mallen et al., 2023) in Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and manually selected relations. The construction method is detailed in Appendix A. TFRKN dataset encompasses 4,550 distinct instances that cover 213 unique relational combinations. In alignment with the KN methodology, we have reformulated each fact triplet into over five varied query forms with the aim of refining true-positive KNs from specific-form queries. (An instance in TFRKN is shown in Table 6)."}, {"title": "4 Diagnose the Pitfalls of Factual Recall in Reasoning", "content": "In the realm of two-hop factual reasoning, an optimal and dependable reasoning trajectory is a multi-hop reasoning approach (Welbl et al., 2017; Ju et al., 2024). This process requires identifying the bridge entity first and then using it to solve the second hop question, necessitating that LLMs recall the relevant fact at each hop step by step, culminating in the formulation of the correct answers. In this section, we investigate whether LLMs faithfully retrieve factual knowledge at each hop when undertaking reasoning tasks."}, {"title": "4.1 KN Scores", "content": "To quantify the capacity for internal recall of specific facts within LLMs, we devise a novel metric,"}, {"title": "4.3 Results", "content": "Single-hop vs. Muti-hop Reasoning In reasoning scenarios, LLMs access their internal knowledge less frequently in comparison to the straightforward retrieval of single-hop facts. Table 1 illustrates a notable decrease in KN Scores for all single-hop facts when addressing two-hop reasoning questions. This observation strongly indicates that, in reasoning contexts, LLMs tend to either fail to recall the bridge entity or struggle to identify the second-hop relation, leading to the failure of executing the remaining multi-hop reasoning as anticipated. Compared to directly recalling single-hop facts (e.g., \"Who is the chairperson of General Motors?\"), it is more challenging for LLMs to recall and organize relevant facts for reasoning. LLMs may take alternative salient pathways existing in their parameters, such as shortcuts, rather than engaging in systematic, step-by-step reasoning."}, {"title": "5 Interventions on the Recall of Facts", "content": ""}, {"title": "5.1 Enhance and Suppress KNs", "content": "To gain a deeper understanding of factual recall behaviors, we intervene in the retrieval of specific knowledge within LLMs by manually adjusting the activation levels of KNs. Specifically for each factual triplet (s,r,o), we modulate the internal recall by adjusting the values of the KNs associated with this triplet, either amplifying or diminishing them according to Equation 6."}, {"title": "5.2 Experiment", "content": "Setup We have meticulously designed four sets of controlled experiments on TFRKN to monitor changes in reasoning outcomes. The experimental paradigms are as follows: (1) Base: We allow LLMs to respond to two-hop questions under standard conditions (2) Enhance: For questions answered incorrectly under Base situation, we amplify the activation level of KNs and subsequently assess the reasoning accuracy. (3) Suppress: Conversely, for two-hop questions correctly answered in the Base scenario, we reduce the activation of relevant KNs and evaluate the reasoning accuracy afterward. (4) Random: To establish a baseline for comparison with conditions (2) and (3), we randomly select an equal number of neurons and enhance or suppress their activation accordingly, facilitating a comparative analysis.\nMetrics We design a novel metric, termed Enhance Ratio (ER), which serves to quantify the impact of factual retrieval failures on reasoning outcomes. ER is calculated by calculating the percentage of reasoning instances that are initially incorrect but are successfully resolved following the enhancement of KNs as Equation 7. Analogously, we define another metric Suppress Ratio (SR) to measure the obstructive effect of suppressed KNs on the reasoning process. The SR is ascertained by evaluating the ratio of cases where correct reasoning is converted to incorrect after the suppression of KNs, as outlined in Equation 8:"}, {"title": "5.3 Results", "content": "Finding 1 In Table 2, more than one-third of reasoning failures are caused by issues of factual retrieval. The ER values show a consistent and progressive increase as the interventions progress from targeting w\u2081, to KNs associated with the second-hop w2, and ultimately to a combined intervention on both, w12. This pattern indicates that many initially incorrect answers stem from retrieval failure of either the first hop, the second hop, or both during the reasoning process. Additionally, recalling the second-hop facts is more challenging for LLMs, as shown by the higher ER after enhancing w2 compared to w\u2081. Suppressing factual information significantly harms reasoning performance, with accuracy dropping by over 77% on average when both factual elements are suppressed. Therefore, the successful retrieval of factual associations at each reasoning step is crucial for correct reasoning.\nFinding 2 CoT strengthens a passive internal retrieval of relevant facts, implicitly prompting the expression of factual triplets. Evidence 1: In Table 3, across the scenarios of no CoT, zero-shot CoT, and few-shot CoT, suppression of factual KNs results in $SR_{No\\_cot} > SR_{zero\\_shot}$ and $SR_{No\\_cot} > SR_{Few\\_shot}$, which indicates that CoT likely stimulates the hydra effect (McGrath et al., 2023), which implements actively self-repairing computations to compensate the suppression effects caused by low activation levels of KNs. Evidence 2: Similarly, enhancement of factual KNs results in $ER_{No\\_cot} < ER_{Zero\\_shot}$ and $ER_{No\\_cot} < ER_{Few\\_shot}$, which suggests that CoT further stimulates the internal recall process within LLMs, thus strengthening the enhancement effects of KNs. Therefore, CoT indeed can contribute to the recalling process."}, {"title": "6 Analysis of Shortcuts", "content": "In this section, we investigate whether successful two-hop reasoning implies the successful recall of factual knowledge. In other words, we examine whether accurate reasoning outcomes stem from a thorough grounding in multi-hop knowledge reasoning or are facilitated by alternative shortcuts."}, {"title": "6.1 Experiment", "content": "Setup We investigate the utilization of individual fact triplets in correctly answered two-hop questions by analyzing the KN Scores for each triplet. We compare these scores with those observed during single-hop queries to establish a threshold, denoted as T, which serves as a benchmark for identifying the effective use of facts in the reasoning process. If the activation level of KNs falls significantly below this threshold in comparison to single-hop queries, this indicates an under-utilization of the corresponding fact. Conversely, if it exceeds the"}, {"title": "6.2 Results Analysis", "content": "According to Table 5, under normal conditions, a considerable proportion of correctly answered questions under no CoT setting rely on shortcuts, possibly due to word associations intrinsic to LLMs,as observed by Yang et al. (2024). Notably, the Mistral-7B model stands out for its unexpected reliance on shortcuts to solve over 44 percent of the questions successfully. Even with large-scale models possessing 7 billion parameters, LLMs still rely on certain segments of the reasoning chain to arrive at answers. The introduction of CoT effectively decreases the trend of taking shortcuts by forcing LLMs to recall more relevant facts and engage in multi-hop reasoning. Under few-shot CoT setting, all LLMs solve over 90 percent of questions on average through multi-hop reasoning, reducing the ratio of shortcuts to nearly zero."}, {"title": "7 Impact of Contextual Conflict", "content": "The capacity of utilizing internal factual knowledge is contingent not solely upon the intrinsic properties of LLMs, but is also significantly influenced by the context within which they operate. This section elucidates how the presence of knowledge conflicts within a given context can impact the mechanisms of the retrieval process during reasoning."}, {"title": "7.1 Experiment", "content": "Setup For each data point, we formulate a single-hop conflict fact by devising a set of potential ob-"}, {"title": "7.2 Results Analysis", "content": "The presence of knowledge conflict within the context consistently augments the faithfulness of LLMs in the corresponding fact. According to Figure 5 and Figure 6, the context of knowledge conflict results in the highest KN Scores of the corresponding hop fact, which indicates counterfactual context significantly improves the internal retrieval of that corresponding hop fact. It illustrates LLMs exhibit greater confidence in their encoded knowledge when confronted with knowledge conflict, a finding that aligns with the studies conducted by Zhou et al. (2023) and Li et al. (2023). When the knowledge presented in the context conflicts with the second-hop fact, it not only reinforces the retrieval of the second-hop fact but also enhances the recall of the first-hop fact. It is plausible that the introduction of the subject 01 encourages LLMs to recall the precise triplet (s, r1,01). However, this effect does not extend to the first-hop fact. The occurrence of knowledge distraction appears not to cause much obstruction to the factual recall within LLMs. On the contrary, it may even stimulate LLMs to retrieve more facts sometimes, as evidenced by the high KN Scores for the first-hop fact of LLaMA2-7B when the knowledge distrac-"}, {"title": "8 Related Work", "content": "Multi-hop Reasoning Multi-hop reasoning poses a significant challenge for LLMs. Several studies have endeavored to address this challenge through the development of more faithful reasoning techniques (Creswell and Shanahan, 2022; Chen et al., 2023b; Creswell et al., 2023). One such approach is CoT, which stimulates LLMs to produce deductive intermediate steps, fostering a step-by-step analytical process (Chu et al., 2024). Another line of research is focused on visualizing the implicit logical structures within LLMs from the perspective of mechanistic interpretability (Yang et al., 2024).\nCoT Mechanism A large body of literature is dedicated to the theoretical and empirical exploration of the mechanism underlying CoT (Saparov and He, 2023; Tan, 2023; Feng et al., 2023; Prystawski et al., 2023; Xie et al., 2024). Some research endeavors to delve into a reverse-engineering analysis of CoT prompting, uncovering the intricate information pathways that facilitate the generation of responses (Dutta et al., 2024). However, the majority of these studies concentrate on the rationales produced by CoT and have largely overlooked the broader implications for factual retrieval processes. In our current work, we complement this aspect and present compelling evidence that CoT significantly bolsters the internal recall of factual information."}, {"title": "9 Conclusions", "content": "This paper aims to provide a comprehensive understanding of factual recall behaviors for LLMs. We find that a considerable portion of reasoning failures are due to retrieval failures. Manually enhancing the internal recall within LLMs can improve reasoning performance. For LLMs, they not only rely on multi-hop reasoning but also rely on other inference ways in LLMs such as shortcuts. CoT can significantly stimulate LLMs to recall more facts by compelling models to engage in step-by-step thinking, diminishing the possibilities of taking shortcuts. The knowledge conflict existing in context could improve the confidence of parametric knowledge, therefore enhancing the internal recall."}, {"title": "Limitations", "content": "While our study provides novel insights into the internal factual recall behaviors of LLMs during reasoning tasks, it is important to acknowledge several limitations.\nGeneralizability: While the current study is primarily based on specific LLMs and the TFRKN dataset, future research should extend these findings to verify their generalizability across various models and datasets\nTheoretical Analysis: Although empirical evidence has been provided through targeted interventions, a deeper theoretical analysis is needed to fully comprehend the underlying reasons for the observed phenomena.\nPractical Applications: The paper discusses theoretical aspects and potential improvements in reasoning accuracy but does not delve into how these findings can be applied in practical scenarios to enhance the reasoning capabilities of LLMs.\nImpact of Contextual Factors: While the paper touches upon the influence of contextual conflicts on knowledge retrieval, a more comprehensive analysis of various contextual factors and their impact on reasoning is needed."}, {"title": "A Details of Dataset Construction", "content": ""}, {"title": "A.1 Sampling two-hop factual triples", "content": "Our dataset is constructed based on Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), a structurally optimized database covering nearly all domains. First we show manually selected relations that are used to construct two-hop relations:\n\u2022 P30, P36, P35, P1037, 1308, P164, P449, P488, P178, P159, P286, P413, P641, P800, P937\n\u2022 P136, P106, P495, P740, P37, P407, P170, P50,P364,P112, P108, P175, P27, P40, P69, P19"}, {"title": "A.2 Generating Queries using ChatGPT", "content": "Having acquired the triplet format of reasoning queries, our current objective is to transform these triplets into natural language expressions in queries. Moreover, for effective integration of the Knowledge Neuron technique, it is essential to rephrase individual triplets into multiple natural language expressions. As knowledge neurons demonstrate indifference towards specific knowledge representations, employing diverse question formats aids in identifying authentic knowledge neurons. Whether in the formulation of reasoning queries or the generation of individual triplet queries, we capitalize few-shot learning capabilities of ChatGPT (gpt-3.5-turbo) to autonomously generate natural language questions. Concretely, we leveraged few-shot capabilities in LLMs to generate multiple queries for individual fact (s, r, o), as well as reasoning questions from two-hop facts ((81,11,01), (01, 02, 02)). For the generation of single-fact queries, we provide relation labels and relation definitions as additional information for LLMs to generate accurate subject-relation queries (Figure 8). For the generation of reasoning questions, two-hop relation labels and explanations are also provided besides four in-context demonstrations (Figure 7).\nAn instance from TFRKN is depicted in Table 6. This approach not only surpasses the limitations imposed by manual templates but also guarantees the production of high-quality and diverse questions. Overall, the dataset comprises 4,550 instances spanning 213 unique combinations of relations."}, {"title": "B Knowledge Neurons", "content": "In this part, we detailedly illustrate the methodology of the identification of KNs using the integrated gradient method. Given a specific relational fact: (s, r, o); A set of knowledge-expressing"}, {"title": "C.1 CoT Results", "content": ""}, {"title": "C.2 CoT prompting templates", "content": "In our experimental design, we have established two distinct prompting configurations for CoT: zero-shot CoT and few-shot CoT. In the case of zero-shot CoT, we simply precede the reasoning question with the directive \"Let's think step by step\". Conversely, for few-shot CoT, we provide n-shot examples to guide the model's reasoning process as in Figure 14."}, {"title": "D Experimental Details", "content": "We present a comprehensive overview of our experimental setup. Our experiments are conducted using a refined subset of TFRKN dataset. To ensure that LLMs know each factual element required by the factual reasoning questions, we meticulously filtered out unqualified data points for each model. By taking the intersection of these filtered datasets, we culled a dataset comprising 1072 qualified data points. The process of identifying KNs for each fact triplet proves to be the most computationally intensive, with each model taking 96 GPU hours to find all KNs. In the context of the location experiment, we configured the integrated gradient steps to 20 and set the parameter of the shared percentage of coarse neurons to 0.2. The experiments were executed on a system equipped with NVIDIA A100 80GB GPUs, and further details of the software environment are available in our code repository. For knowledge conflict experiments, we construct a knowledge distraction sentence pool, randomly assigned to each reasoning question while knowledge conflict in cloze task is constructed by a set"}]}