{"title": "Enhancing Video Understanding: Deep Neural Networks for Spatiotemporal Analysis", "authors": ["AmirHosein Fadaei", "Mohammad-Reza A. Dehaqani"], "abstract": "It's no secret that video has become the primary way we share information online. That's why there's been a surge in demand for algorithms that can analyze and understand video content. It's a trend going to continue as video continues to dominate the digital landscape. These algorithms will extract and classify related features from the video and will use them to describe the events and objects in the video. Deep neural networks have displayed encouraging outcomes in the realm of feature extraction and video description. This paper will explore the spatiotemporal features found in videos and recent advancements in deep neural networks in video understanding. We will review some of the main trends in video understanding models and their structural design, the main problems, and some offered solutions in this topic. We will also review and compare significant video understanding and action recognition datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "According to Cisco predictions, in 2022, 82% of the web data consisted of videos. It will take 5 million years for someone to watch it all [26]. It is interesting to note the rise in popularity of using videos for transferring or presenting data on the web and various applications. Video is a more engaging and visually appealing way to convey information to users. The video consists of images, text, and sound and can easily show the relationships and how objects interact with each other with this multimodal structure. They are also much simpler for the user to understand.\nThe complete video provides more information than the individual frames presented on their own. With a video, you can track changes and how actions are taking place. The movements, changes in shape and color, the time and order of the different events, and all other patterns seen through time are partial examples of information only available with a video, and images can not contain them. This information proved useful and very important for classification and understanding the context at a higher cognition level and for more complex tasks. For example, it is shown with data that facial movements can help us understand emotions [68], and body posture can help us with predicting action [141,156]. As demonstrated by a previous study, deep artificial neural networks have been shown to classify sports with greater accuracy by using entire sports videos instead of just a single frame [69]. Conversely, natural vision is rooted in the continuous flow of video rather than discrete image frames. The importance of classifier models trained on videos has significant implications for cognitive neuroscience and is a crucial goal in Artificial Intelligence science [150].\nVideo understanding was initially considered an extension of the previously studied and perfected image understanding models. The first models trained by the videos performed the spatial feature extraction in different video frames [4] or used intermediary media such as Optical flow [61] to map the motion and temporal features [45,120,155]. Later models, however, will treat the video as a whole and will allow the model to extract spatiotemporal features directly from it [67,138]. Inspired by the natural vision, multi-stream networks were proposed to implement this spatiotemporal feature extraction on parallel but separate streams [43,122,143]. In these models, one stream would focus more on spatial information extraction while the other would specialize in extracting temporal information. The latest and state-of-the-art models in video understanding are transformer models [127,140] that are experts in data fusion and spatiotemporal feature extraction in their self-attended feed-forward structure.\nWhat kind of information can we take from videos, and how should we use them to result in more accurate learning and understanding of this data? How do we use videos efficiently to train our models? What challenges do we face when we deal with videos instead of images? How can we deal with these challenges without losing too much information? These questions were the main concerns of many researchers in recent years regarding video understanding, and they put comprehensive effort into analyzing and proposing different methods and tools for video classification tasks. In this survey, we will first review the basis of video understanding tasks and spatiotemporal feature detection. This paper provides a holistic overview of the video understanding models. Focusing on the spatiotemporal feature detection techniques, we will discuss a few structural designs and implementations in the video understanding models. We will also discuss the most important benchmarks for video understanding and action recognition and report the results for some of the discussed models for comparison. Finally, this survey details some challenges in spatiotemporal video understanding deep neural network models and some proposed solutions with an outlook to future works."}, {"title": "2 VIDEO UNDERSTANDING", "content": "We must detect objects and their associated actions to understand and describe the surrounding environment, which necessitates the recognition of relevant nouns and verbs, along with their interconnections, to facilitate video understanding, prediction, or narration. Comprehending videos involves categorizing and grasping the entities, events, and actions depicted within them. We can break it down into two well-established and extensively studied domains: object recognition and action recognition."}, {"title": "2.1 Object Recognition", "content": "Object recognition and classification is a well-known and well-discussed subject [96]. Our goal for this task is to identify and categorize objects in the given image or video [92]. Many algorithms and neural network architectures were suggested and used for this task. From them, we can mention algorithms that will classify objects based on their appearances (with edge and corner detection or gradient-based methods) [87,116], genetic algorithms [84,95], and algorithms that work based on feature detection [8,88,93]. Features serve as crucial data that provide us with essential insights into the most significant aspects of the acquired data.\nSpatiotemporal feature extraction and analysis draw inspiration from natural vision. Some recent tools, such as deep neural networks, can resemble human vision in the task and even outperform humans [72]. Natural vision depends on feature extraction, changing the representation in various steps, and creating a complete understanding of our surroundings and the entities around us. Artificial models can rival natural vision in terms of accuracy. However, computer models can not effectively handle image variances compared to natural vision. Variations include changes in lighting, rotation, perspective, obstacles blocking the view, and depth. Since earlier methods, computer vision strived to create change invariant models based on the environment [92]. Even with that in mind, some variations remain beyond the scope of image-based object classification methods. It's important to note that some image-based classification methods may not be able to capture changes that occur to an object over time. This failure is because they have no access to temporal data that could help them better understand how an object changes and adapts over time."}, {"title": "2.2 Action Recognition", "content": "Identifying the verbs that describe the changes or lack of changes in the scene is action recognition [115]. Action detection can extend to static images, potentially yielding improved results when motion does not significantly contribute additional information.\nIn such cases, a single frame can effectively capture the essence of the action (Figure 1)."}, {"title": "3 SPATIOTEMPORAL FEATURES", "content": "The first question centers on identifying the categories of data observable within a provided video and what specific elements should receive increased focus. Historically, vision models have predominantly aimed to extract spatial features for image classification. Traditional feature detection methods such as Canny [15], Harris [56], or other conventional algorithms designed for detecting edges, corners, and blobs are usable for spatial feature extraction. Theoretically, a significant portion of the spatial information within images resides within these identified features. Consequently, with these extracted features, we can recognize and classify them with remarkable efficiency.\nWith the emergence of novel machine learning methods, computers took on the task of feature extraction. These systems would train on data, enabling them to learn the most pertinent details about images relevant for classifying them. This approach allows the model to become specialized and pick up on nuances we might not instinctively identify as significant. The model will enhance its proficiency in extracting more relevant features [128].\nThis methodology revolutionized object detection and object tracking, which previously relied on manual feature detection. Soon after, with the creation of neural network models and larger datasets such as Image-net [29], analyzing and comparing the different methods for object detection and image classification became possible. Deep neural networks started to show their true potential in feature extraction and classification tasks [57,74].\nHowever, videos introduce an additional dimension: time. The existence of this new dimension for feature extraction compels us to adopt one of these three different methodologies for feature extraction (Figure 2):\nSpatial feature extraction: to extract the spatial features of the variant frames through time and utilize them collectively as input\nSpatial and temporal features extraction: to extract the spatial features from different frames and the temporal features (reflecting changes of the spatial features over time) separately, incorporating them into two distinct streams, and utilizing both as the input for the classifier.\nSpatiotemporal feature extraction: to extract features encompassing all dimensions from the video. These novel feature, which encapsulate information for both spatial and temporal dimensions, are referred to as spatiotemporal features."}, {"title": "3.1 Spatial Feature Extraction", "content": "In the initial approach, we identify spatial features within distinct frames and employ these spatial features from all frames as input. To achieve this, we choose a single frame as the primary input or combine all frames into a unified image to act as the input. With this kind of feature extraction, we lose the \"time sense\" of the data. As a result, much information deduced from the temporal channel will be lost [4]."}, {"title": "3.2 Spatial and Temporal Features Extraction", "content": "Creating separate intermediary media that can represent the motion in different sections of videos can assist the temporal feature extraction. Optical Flow [61] is an example of those intermediary media created and used frequently in the introduced methods for representing temporal features. Leveraging tools like Optical flow [61], Object flow, or any other intermediary media that can show the motion to process on a separate and parallel stream [45] alongside the network to extract spatial features has consistently yielded noteworthy performance enhancements across various architectures. This approach represents a straightforward yet impactful means of incorporating pertinent motion data into specialized tasks [60]. Rather than undertaking the non-learnable task of generating these representations of the media, in later models, a distinct and trainable artificial neural network stream is employed in parallel to the spatial stream, with the specific purpose of temporal feature extraction. We will review these multi-stream models in subsequent sections.\nThere is a potential that the linkage between spatial and temporal data gets disrupted when segregating the temporal and spatial features into two separate streams. For example, while we can identify objects and track their movements, the concept of motion derived from temporal and the recognition of entities through spatial features might remain detached and unrelated.\nEarlier models had trouble with the fusion of the spatial and temporal details as the data each stream is modeling is complex. Convolutional neural networks, however, show good potential in linking and fusion of separated data in multi-stream models. Deep neural models have enough parameters to model such fusions and interpret spatiotemporal details.\nIn deep neural networks, comprehending and extracting deeper insights from data involves a series of transformations towards higher cognitive levels within each layer, altering the data representation through multiple stages [83]. These higher-level representations can assist us with extracting semantics from the input multimedia and subsequently classifying them. The feature extraction primarily revolves around spatial aspects when dealing with a static image input. The neural network enhances these spatial features and generates a new representation of the data in each layer. Data representation undergoes multiple changes until the model achieves its goal. The network then generates a semantic prediction, which we strive to minimize the error during the training phase.\nWhen analyzing videos, depending on the feature extraction methods, we possess distinct spatial features from various frames alongside temporal features that capture changes across the duration of the entire video. Extracted features should also be augmented with each other on both spatial and temporal perspectives in multiple steps to create a new set of features in each step that represents a higher level of semantical information about the input. However, as the depth of our model increases, there's a risk of losing the connection between spatial and temporal information within the network's layers. This concern becomes more pronounced when these two types of information are not consistently intertwined and synchronized over time. Temporal and spatial information processed separately must combine in different parts of the architecture to build spatiotemporal understanding. In preceding models, this fusion would typically occur toward the final stages of the respective streams.\nBy using multiplication residual blocks and gating, we can constantly send feedback from one of the streams to the other and help the network to go deeper [44]. Adding small feedback channels between the multi-stream will allow them to integrate and fuse interpretations of data earlier and in earlier stages of the model."}, {"title": "3.3 Spatiotemporal Feature Extraction", "content": "The final method is extracting the spatiotemporal features directly. The spatiotemporal features refer to the information (variance) we obtain from both the location and time, which provide the most relevant data for video understanding. In these networks, we see video as raw data on three dimensions of (x, y, t). We employ 3-D convolution and other associated processes to extract features and execute classification tasks [138]. From this perspective, extracting spatiotemporal features can be likened to extracting spatial features in a higher dimension. From this standpoint, time is akin to an additional input dimension. Nonetheless, this perspective doesn't universally align with feature extraction tasks, as time significantly diverges from spatial dimensions. What we seek from the time dimension extends beyond merely capturing the presence of objects over time; it encompasses motion data and the dynamics of change between distinct frames.\nModels emphasizing spatiotemporal feature extraction demand additional time and memory resources for training and recall processes, often grappling with overparameterization, particularly when handling extended video inputs. The removal of longer-term relationships between frames can lead to the loss of critical temporal information. Segmenting videos and generating shorter snippets can significantly compromise the accuracy of this task, as illustrated in (Figure 3) [138].\nFor the extraction of spatiotemporal features, 3-D convolutional layers can be employed [67]. These 3-D convolutions are adept at detecting spatiotemporal features using a dynamic kernel that traverses the current frame and the adjacent frames, thereby shaping the input for the subsequent layer, as illustrated in (Figure 4).\nIn 3-D convolutional networks, it has been observed that segregating spatial and temporal fusion leads to a noteworthy enhancement in network accuracy. One approach to achieve this is by decomposing a 3-D spatiotemporal convolutional layer into a 2-D spatial convolution followed by a 1-D temporal convolution, as depicted in (Figure 5) [107,137]. This strategy augments the nonlinearity within the network, thanks to the additional ReLU activation function situated between the 2-D and 1-D convolutions within each block. Consequently, the network, functioning as a function approximator, gains the capacity to model more intricate scenarios [20]. Additionally, this separation simplifies the optimization of weights and fusion processes [123].\n[135] introduced C3D as a generic video descriptor founded on 3-D convolutional networks. Through empirical evidence, the authors demonstrated that employing homogeneous 3x3x3 filters consistently outperforms varying the depth of temporal filters. C3D's descriptor is derived by averaging the outputs from the initial fully connected layer within the network. Furthermore, ongoing research has contributed to the refinement of spatiotemporal convolutional layer structures, yielding significant developments such as the Channel-Separated Convolutional Network [136] and Mvfnet [153]."}, {"title": "4 UNDERLYING PRINCIPLES OF VIDEO UNDERSTANDING NETWORKS", "content": "To systematically categorize the diverse range of models developed for video understanding, it is imperative to delve into critical advancements and discussions that underpin the creation of these models."}, {"title": "4.1 Preprocessing Videos", "content": "Preprocessing plays a pivotal role in effective learning from videos, and its significance can be attributed to several key reasons:\nSelective Information Relevance: Videos often contain extensive spatiotemporal information, much of which is only partially relevant to the learning task at hand. A substantial portion of this information can be extraneous and is a distraction.\nMitigating High-Dimensionality: Video data typically comes in a high-dimensional format, which can introduce various architectural challenges in neural networks. Dealing with such high-dimensional data can make it harder for models to generalize learning effectively.\nHandling Noise and Distortion: Videos are susceptible to various sources of noise and distortion. Spatial noise may involve adding random noise to images, applying transformations, introducing obstructing objects, altering the color and shape of objects, or other environmental changes. Temporal noise can arise from edited videos, disrupting the semantic flow of frames (e.g., injecting unrelated frames). Additionally, the use of multiple cameras for recording can introduce temporal variance. These issues collectively constitute the input variance problem, and deep neural networks are inherently sensitive to such variations.\nPreprocessing steps are essential to guarantee that the data is in an appropriate form for analysis and learning. These steps are vital for avoiding the aforementioned challenges and ensuring that the neural network can effectively learn and generalize from the video data.\nThere are numerous types of preprocessing techniques designed to address these concerns and prepare video data for further analysis and modeling. Certain techniques, such as the Fovea Stream, were introduced to concentrate on the most pertinent parts of videos, which contain more relevant data for our processing tasks. The core concept is to direct attention to the inherently significant regions within frames where most of the changes occur. This concept is realized by focusing on the central portion of all frames while discarding the peripheral edges due to camera bias. Camera bias arises from the tendency of cameras to prioritize the central area when capturing video footage, as this region typically contains the most detailed information within the frame.\nIn this technique, to ensure that context information from the entire image is not lost, a lower-resolution base video also exists alongside the Fovea Stream, referred to as the Context Stream (Figure 6). This approach effectively reduces the overall input size by half by reducing the context stream's resolution to one-fourth of the original value and downsizing the frame dimensions by half in each dimension for the Fovea stream [69].\nNumerous preprocessing techniques have centered around the reduction of input data size by altering data dimensions. Variance, encompassing both spatial and temporal aspects, serves as a potent representation of data changes and reveals information that aids in distinguishing entities and actions. Consequently, variance plays a pivotal role in determining the dimensions for newly represented data.\nDimension reduction methods, such as Principal Component Analysis (PCA), have gained popularity as preprocessing steps for neural networks designed to operate with video data. In some video datasets, like Youtube-8M, videos have already undergone PCA and preprocessing [102], further emphasizing the utility of dimension reduction techniques in preparing video data for neural network-based analysis.\nAnother critical objective in preprocessing is noise removal, aimed at addressing the input variance problem. In this context, noise refers to any undesired disturbance that impacts the quality of spatiotemporal features. As neural network architectures train with a finite set of supervised data, they can become sensitive to these unwanted features, potentially diminishing the network's ability to generalize effectively.\nThese disturbances typically manifest as spatial noise and can be managed using conventional noise detection and removal methods, much like those applied to single images. However, videos introduce the additional challenge of temporal noise. Temporal"}, {"title": "4.2 Temporal Frame Fusion", "content": "In a broader context, spatiotemporal convolutional neural networks exhibit a relatively rigid temporal structure, primarily because these networks require a predefined number of frames as input. To imbue the network with temporal awareness, it becomes necessary to either fuse information from multiple video frames or extract features from different frames. The specific approach to this fusion can vary, occurring either before or after the extraction of spatiotemporal features within different models, contingent on their defined structural architecture.\nThe central question revolves around the selection of frames for fusion. In earlier proposals, the fusion process involved combining adjacent video frames to generate higher-dimensional data that encompassed both short-term temporal information and the spatial data of the video frames. However, it's important to note that the fusion of neighboring frames is not the exclusive or necessarily the optimal approach. [69] introduced and explored several primary types of frame fusion methods, as illustrated in (Figure 7).\nEarly fusion is akin to the fusion that occurs in traditional 3-D convolution-based methods [67]. In early fusion, a new representation is generated that places a stronger emphasis on preserving the spatial data without significant alteration. This approach is effective for detecting short-term temporal features but may be relatively less proficient at recognizing and utilizing long-term temporal relationships between different frames. Early fused networks tend to struggle to extract long-term features or leverage long-term temporal patterns to comprehend the content of the video. Their primary strength lies in capturing short-term dynamics and spatial information within the video frames.\nLate fusion involves the fusion of frame-wise features occurring at a later layer in the network. This approach enables the network to effectively handle long-range relationships between spatial features. However, one notable challenge with late fusion networks is that there is no guarantee that long-term relationships between frames always exist in videos, particularly in web-based videos. In online videos, it's possible to switch scenes abruptly and present unrelated and distinct video frames. Additionally, temporal shifts can occur, further complicating the establishment of semantic relationships between non-neighboring frames. As a result, late fusion networks may encounter difficulties handling such scenarios where long-term relationships between frames are not readily discernible.\nSlow fusion involves the sequential processing of several consecutive sections within videos using the same network structure. This fusion approach repeats hierarchically in a pyramid-like fashion, as depicted in (Figure 7), to develop a comprehensive understanding of the entire temporal domain. Notably, slow fusion has demonstrated superior performance compared to other fusion methods, making it an effective choice for capturing and utilizing temporal information in video data. You can find the results of this comparison conducted by [69] in Table 1."}, {"title": "4.3 Data aggregation and Pruning", "content": "In their research, [163] explored various models for data aggregation, as illustrated in (Figure 9). The fundamental concept behind these models is to transform the data structure and amalgamate information at each layer, ultimately constructing a new feature set that can be effectively utilized in the subsequent layer. This information aggregation process must persist in the later stages of the network. Pooling operations play a crucial role in regulating the creation of the feature set for the next layer, offering control over the network's hierarchical feature extraction. [117] conducted research into the application of Max pooling to generate long-range temporal aggregated data, which can significantly contribute to video understanding. In their study, they devised temporal aggregation blocks (TAB) that rely on attention filters and max pooling techniques. These TABs designed to aggregate information from the recent past and more extended periods will effectively aid in predicting future actions within the video context [117].\nBefore choosing the pooling method for each section in the model, we must determine whether we should decrease, increase, or maintain the dimensions in that layer. As our network advances, we aim to reduce the dimensionality until a straightforward relationship emerges between the final feature set and our task. In essence, our goal is to streamline the dimensions and link the raw spatiotemporal features to our ground truth labels in later layers.\nA commonly employed and well-received approach involves dimension reduction and subsequent expansion, which serves as a data encryption and decryption method. This methodology can be categorized as a pruning technique, where the neural network can remove non-essential data for the classification task. This process aids the network in reconstructing features closely tied to the main context. Autoencoder-based architectures have been widely used in video understanding and classification tasks [101], particularly in applications like anomaly detection and tracking [21,113,170], video compression [47,162], video retrieval [124], and video summarization [167]. In Compressed video action recognition (CoViAR), the model processes the video through spatial and temporal streams before applying any codecs [152].\nOne of the highly effective data aggregation methods, particularly suited for temporal features, is NetVLAD [2]. The output of the neural network in this approach consists of a set of descriptors, each of which contributes to describing the entire network to some extent. These output descriptors serve as vector inputs for VLAD [3,66]. VLAD, in turn, generates a single vector output that encapsulates the described information from the final layer, including spatiotemporal details from the fused frames. NetVLAD incorporates SoftMax and normalization layers (Figure 10) to address input variance issues. As previously discussed, input variance problems pertain to how slight changes in objects or the video can impact the outcome. NetVLAD is commonly employed in various video classification and comprehension architectures [132]. For instance, ActionVLAD was among the early architectures to utilize the VLAD aggregation method for spatial and temporal streams [46], while the WILLOW network leveraged NetVLAD for data aggregation and emerged as the winner of the inaugural Youtube-8M Large-scale video understanding competition [94]."}, {"title": "4.4 Attention and Shifting", "content": "In various layers of the network, we apply different attention filters to the input data. These attention filters have varying weights and are trainable, meaning the neural network can learn to prioritize specific parts of the previous layer's data, which enables the model to determine which aspects of the previous layer's data are more important and warrant closer attention (Figure 11) [120].\nAttention-based neural networks have found applications in a wide range of recognition and classification tasks, including sound recognition, natural language processing, and image recognition. These models have demonstrated their effectiveness in improving performance and handling various types of data.\nShifting is a technique introduced to simplify attention complexity. It works similarly to attention but focuses on streamlining the data to emphasize the main context. In this approach, attention weights are transformed into a straightforward shift operation. This means that the maximum weight in an attention filter is rounded up to one, and the rest of the attention weights are set to zero (Figure 12). This reduction in attention complexity also leads to a decrease in the total FLOPS required for the neural network. Shifting is particularly useful when applied to the temporal channel, as seen in models like the TSM model [85]. However, it can also impact spatiotemporal streams in more complex models like RubicksNet (Figure 13) [39].\nShifting the time axis can occur in either direction if we have access to future frames. However, if we only have real-time information, we can only shift temporally in one direction, which is backward in time. Spatial shifting, on the other hand, can happen in both directions. To train shift networks effectively, we need an additional loss function. This loss function is essential to penalize excessive shifts that could potentially harm the spatiotemporal integrity of the data. Similar penalties should apply to control excessive changes in other attention-based networks.\nContext gating is one of the applications of attention-based or shift-based models. It's used in later layers of the network to enable it to filter out objects or actions that are not relevant to the context (Figure 14). Context gating is valuable for refining the network's output by helping it judge the output's confidence and relevance. It's particularly helpful when combining results from multiple networks to make higher-level deductions [94]."}, {"title": "5 VIDEO UNDERSTANDING MODELS", "content": "As in other domains of computer vision, a range of methods have been devised to build models for video comprehension. The next section will review fundamental approaches, considering feature extraction and overall architectural perspectives."}, {"title": "5.1 Structural Search Models", "content": "One of the initial and rewarding approaches to video comprehension involved extending image understanding models into the temporal dimension. This approach applied structural search techniques, previously employed in image understanding, to convolutional feed-forward architectures for video comprehension. The process began with a simple model, which progressively expanded and was analyzed for performance to determine its optimal state.\nIn pursuit of improved structures for video understanding tasks, [158] searched for various network structures featuring 3D and 2D convolutional layers to strike a balance between speed and accuracy. The X3D network, employing Neural architecture search and stepwise network expansion techniques, systematically extended a basic 2D image classification network along multiple axes, including spatial, temporal, weight, and height dimensions of different blocks, which resulted in a family of efficient networks tailored for video understanding and classification [42]. Similarly, AssembleNet [112] and AssembleNet++ [111] introduced a generic structural search approach to learn connections among feature representations across input modalities, delivering outstanding performance."}, {"title": "5.2 Memory-based and Recursive Models", "content": "The results of the proposed algorithm strongly suggest that training should encompass the entire video rather than relying on short clips [163]. While models can detect many actions by analyzing a continuous sequence of frames or by extracting short-term temporal features and spatial features from related video frames, there are instances where understanding actions necessitates linking events across time. Moreover, changes in the scene can disrupt the semantic connection between nearby frames.\nThere are numerous scenarios where a video loses its short-term semantic coherence. In such cases, a frame may exhibit a weaker semantic correlation with its neighboring video frames. Consequently, observing video frames over an extended duration can offer deeper insights into specific events, rendering the details more significant (Figure 16)[151]. Given these challenges, it is advantageous to retain long-term spatiotemporal features, especially when working with longer videos (rather than short snippets), web videos, or videos depicting tasks with notable long-term dependencies that can enhance action understanding. Late fusion and memory-based models are better suited for capturing long-term spatiotemporal features in such scenarios.\nMemory-based network models, which utilize architectures like LSTM [51,59,129], GRU [24], and other networks [63], effectively leverage temporal information and serve as long-term feature repositories. These structures are highly efficient and valuable when dealing with temporal features or when patterns evolve, as seen in tasks like speech recognition [50]. (Figure 17) is a visual representation of the typical structure of memory-based and multi-layer recurrent networks employed in previous video understanding works [6,33].\nAnother approach to capturing long-term features involves creating a separate feature bank alongside the spatiotemporal network. This temporal feature bank offers a comprehensive, time-wise understanding of the entire video and can influence either the spatiotemporal stream or the classifier. While the feature bank excels at extracting long-term temporal features, it is comparatively less proficient at capturing spatial features. On the other hand, the spatiotemporal stream focuses on nearby frames with a smaller window, making it more adept at handling spatial features and short-term temporal features. (Figure 18) provides a visual representation of this concept [110,151]."}, {"title": "5.3 Multi-Stream Networks", "content": "In our brains, natural vision is processed through two distinct parallel pathways: the dorsal and ventral pathways. These pathways originate in the occipital lobe and progress sequentially toward the parietal and temporal lobes. They are specialized with distinct structures for feature detection, ultimately resulting in a comprehensive vision perception after data augmentation in association areas of the brain. The ventral pathway, which extends towards the temporal lobe, specializes in semantic analysis and object detection. Conversely, the dorsal stream is specialized in motion processing, action recognition, and understanding the spatial relationships between objects and the body and other entities (Figure 19) [48].\nThe concept of these two brain streams has inspired neural network models for video understanding, incorporating two parallel streams. In these network architectures, one stream focuses on temporal analysis and feature extraction, while the second stream is dedicated to spatial feature detection. An early example of such a network is the two-stream network [122], which featured two separate visual streams. This model effectively fused both spatial and temporal information, making it a state-of-the-art approach at the time. Drawing inspiration from traditional handcrafted methods that rely on trajectory-based feature extraction, the TDD model [143] is another two-stream network that operates with aggregating trajectory-pooled features.\nIn subsequent years, newer two-stream networks were introduced, featuring frame fusion and memory-based architectures [45,155]. The stream responsible for spatial analysis can operate on a single frame or a short sequence of frames to generate short-range spatiotemporal features. Conversely, the stream dedicated to temporal analysis can consist of a simple network with inputs such as optical flow [61] or other motion-based images. Alternatively, this stream can employ a 3D convolutional network or another fusion-based architecture designed to detect spatiotemporal features.\nTwo-stream networks initially faced challenges when dealing with longer videos. To address this limitation, [144] introduced the Temporal Segment Network, which featured a two-stream structure capable of working with long videos through segmentation. Similarly, the deep Temporal Linear Encoding (TLE) model adopted a segmentation approach, breaking the video into different segments and performing two-stream feature extraction for each segment. The results were aggregated and encoded, followed by fusion [32]. One significant issue with the original two-stream networks was their reliance on intermediary motion representations like optical flow [61], which incurred a high computational cost even after training. Subsequently, multi-stream networks were proposed that eliminated the need for these intermediary representations [131,175].\nThe SlowFast network [43], draws inspiration from the natural vision pathways, utilizing two streams to extract spatiotemporal features. One of these streams, referred to as the Fast stream, processes high frame rate input with extensive temporal coverage. It is primarily focused on motion and time-based analysis. In contrast, the other stream, known as the Slow stream, handles low frame rate input with reduced temporal dominance. The Slow stream specializes in capturing categorical semantics and places more emphasis on spatial information. Importantly, the Fast stream provides continuous feedback to the Slow stream at different layers. This feedback mechanism enables the Slow stream to leverage motion information effectively (Figure 20).\nIn the process of making the final prediction, the output results from multiple streams are aggregated before the final classification. One approach to achieve this is the Mixture of Experts classifier, which employs an attention-based method. In this approach, specialized expert classifiers are trained for each stream, and gating weights are assigned to each classifier. These weights determine the significance of the results produced by each expert classifier based on the content. Following training, the network focuses on the results from various channels to generate a more accurate output, relying on the most significant findings for categorizing the input (Figure 21) [7]. Expandable structures designed to work with a vast collection of long videos can be trained using a combination of expert classifiers. This scalability enables the aggregation of data from a wide range of sources [65]."}, {"title": "5.4 Transformer Networks", "content": "Transformer models have recently exhibited outstanding performance in numerous natural language processing tasks. Prominent examples of these models include BERT (Bidirectional Encoder Representations from Transformers) [30", "108": "RoBERTa (Robustly Optimized BERT Pre-training) [90", "109": ".", "13": "boasts an impressive 175 billion parameters, while the switch transformer model [41", "27": "indicate that multi-head self-attention blocks excel in capturing both global and local features within sequences. Furthermore, they suggest that these blocks can enhance expressiveness by adaptively learning kernel weights, akin to deformable convolutions [28"}]}