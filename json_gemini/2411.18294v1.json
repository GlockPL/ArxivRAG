{"title": "Aligning Pre-trained Models for Spoken Language Translation", "authors": ["\u0160imon Sedl\u00e1\u010dek", "Santosh Kesiraju", "Alexander Polok", "Jan \u010cernock\u00fd"], "abstract": "This paper investigates a novel approach to end-to-end speech translation (ST) based on aligning frozen pre-trained automatic speech recognition (ASR) and machine translation (MT) models via a small connector module (Q-Former, our Subsampler-Transformer Encoder). This connector bridges the gap between the speech and text modalities, transforming ASR encoder embeddings into the latent representation space of the MT encoder while being the only part of the system optimized during training. Experiments are conducted on the How2 English-Portuguese dataset as we investigate the alignment approach in a small-scale scenario focusing on ST. While keeping the size of the connector module constant and small in comparison (<5% of the size of the larger aligned models), increasing the size and capability of the foundation ASR and MT models universally improves translation results. We also find that the connectors can serve as domain adapters for the foundation MT models, significantly improving translation performance in the aligned ST setting. We conclude that this approach represents a viable and scalable approach to training end-to-end ST systems.", "sections": [{"title": "Introduction", "content": "The task of speech translation (ST) is typically addressed by using either a composite cascade ST system built from pre-trained automatic speech recognition (ASR) and machine translation (MT) models, or by training an end-to-end model, typically based on the transformer (Vaswani et al., 2017) architecture. However, both approaches have their caveats. Cascade systems can sometimes suffer from error accumulation due to potential domain and vocabulary mismatch of the foundation models, amplified by first transcribing the input utterance and only subsequently translating it. While end-to-end ST systems alleviate such issues, training them from scratch requires more ST data, often employing several pre-training stages and techniques involving transfer learning and additional model supervision, to obtain better results (Inaguma et al., 2020; Kesiraju et al., 2023).\nThis work attempts to strike a middle-ground between the two ST approaches, aiming at leveraging frozen pre-trained ASR and MT models joined by a small connector module, bridging the gap between their respective representation spaces."}, {"title": "Related works", "content": "A trend for this paradigm was set by (Li et al., 2023), where a connector network called Q-Former was used to map abstract image representations from a frozen image encoder into the word embedding space of a large language model (LLM), generating text annotations. During training, the foundation models remain frozen and only the connector is trained, decreasing the cost of the training process considerably. Other works have subsequently adopted a similar approach to address different cross-modal tasks (Chen et al., 2023a; Dai et al., 2023; Zhang et al., 2023; Lyu et al., 2023; Alayrac et al., 2022; Su et al., 2023; Chen et al., 2023b).\nThe same model alignment paradigm has also been adopted for ASR and other speech-oriented tasks. In (Yu et al., 2023), the Q-Former is used to join large ASR encoders and LLMs for the ASR task. Similarly, a linear-projection connector with either a convolutional downsampling or CTC compression frontend was used in (Hono et al., 2023) for the same purpose. In (Wang et al., 2023), a large-scale multi-task training approach was used to enhance the capabilities of an instruction-tuned LLM to understand speech inputs, utilizing a transformer connector with stochastic downsampling.\nWhile these works all follow a similar approach, training the connector network, the focus is on ASR or other general large-scale speech-language tasks."}, {"title": "Alignment architectures", "content": "We experiment with two general alignment architectures, differing in the configuration of the frozen MT model, as shown in Figure 1. For both architectures, only the encoder part of the ASR model is used. Both architectures are trained using the standard cross-entropy loss objective at the output of the MT decoder."}, {"title": "ECD", "content": "The first alignment architecture (Figure 1A) \u2013 Encoder-Connector-Decoder (ECD) \u2013 consists of a frozen ASR encoder, which extracts hidden audio representations from the source speech audio. These representations are then passed to a connector module, whose outputs are further projected to the dimension of the frozen MT decoder via a fully-connected layer \u2013 the connector is used to align the ASR encoder output embedding space with the output embedding space of the MT encoder, essentially overtaking the responsibilities of the removed MT encoder, while simultaneously adapting to the speech modality\u00b9."}, {"title": "ECED", "content": "In the second Encoder-Connector-Encoder-Decoder (ECED) setting (Figure 1B), the connector module projects the speech embeddings into the input word embedding space of the MT encoder. Here, the modality gap between the output ASR embeddings and the input MT encoder text embeddings should be narrower to the ECD case, as the connector only has to propagate the raw textual information contained in the embeddings to the MT encoder. a similar approach was explored in (Wang et al., 2023), where aligning the speech encoder with a language model from the T5 (Raffel et al., 2020) family allowed the authors to leverage the original multi-task and reasoning capabilities of the foundation multi-lingual language model, only enhanced in that the final model could also process speech inputs. This encompasses first taking the instruction prompt ('translate to Portuguese:'), embedding it with the MT encoder embedding layer, and prepending these instruction embeddings to the connector module outputs."}, {"title": "Connector modules", "content": "This section presents the two connector modules used in our alignment experiments: the Q-Former and our STE connector."}, {"title": "Q-Former", "content": "The Q-Former (Li et al., 2023) (Figure 2A) is a simple transformer model, which uses a sequence of trainable queries $Q \\in \\mathbb{R}^{d_q\\times n_q}$ as input ($d_q$ is the query dimension, $n_q$ is the number of queries used, which is a hyperparameter). These queries interact with the supplied speech embedding sequence $S \\in \\mathbb{R}^{d_s\\times n_s}$ via cross-attention, extracting relevant"}, {"title": "STE connector", "content": "Additionally, we experiment with another connector variant inspired by (Wang et al., 2023; Hono et al., 2023) (Figure 2B). This variant addresses the fixed-length mapping of the Q-Former by introducing a convolutional subsampling layer in replacement of the queries and cross-attention connection, and we, therefore, refer to it as the \u2018STE\u2018 connector (Subsampler-Transformer Encoder).\nThe subsampling layer is used to reduce the time dimension granularity of the supplied ASR embeddings to better match the granularity and information density of the token sequences processed by the MT model. In (Hono et al., 2023), CTC compression and convolutional downsampling are explored, evaluating the convolutional approach as superior. In (Wang et al., 2023), similar subsampling is done by stochastically discarding a fourth of the ASR embeddings. Our subsampling module is a 2-layer stack of 1D convolutions, reducing the length of the input speech embedding sequence $S \\in \\mathbb{R}^{d_s\\times n_s}$ by a factor of 4, and simultaneously projecting the embeddings into the hidden size $d_c$ of the connector:\n$S' \\in \\mathbb{R}^{d_c\\times n_s/4} = Conv(S)$.\nThe subsampler was inspired by the 1D conv. frontend used for ASR systems in (Wang et al., 2020). The subsampler is then followed by a stack of trans-"}, {"title": "Experimental setup", "content": "We conduct our experiments within the Hugging Face Transformers (Wolf et al., 2020) ecosystem, which allows us to easily access and reuse various pre-trained off-the-shelf ASR and MT models for our experiments. All training runs were done on a single Nvidia RTX A5000 24GB GPU, with most alignment runs lasting around 10 hours on average, depending on the sizes of the foundation models."}, {"title": "Data and evaluation", "content": "All models in this work are trained and evaluated using the How2 dataset (Sanabria et al., 2018). How2 is a multi-modal dataset of English instructional videos, containing a smaller 300-hour audio subset with crowd-sourced Portuguese translations, for which the two validation and test partitions (val, test) both total just under 4 hours of speech data. How2 is a commonly used standard benchmark dataset for MT and ST systems (Niehues et al., 2019; Inaguma et al., 2020; Vydana et al., 2021; Kesiraju et al., 2023). As the data is relatively clean, it is considered to be appropriate for studying and evaluating various approaches for ST. For our ASR models, we report normalized WER. Our translation models are evaluated using 4-gram BLEU\u00b2 (Papineni et al., 2002) and chrF2\u00b3 scores (Popovi\u0107, 2015) as implemented in the Hugging Face Evaluate v0.4.0 library."}, {"title": "Foundation and baseline models", "content": "To better ground our results, we take inspiration from ESPnet (Inaguma et al., 2020) How2 recipes\u2074 and use the best-performing transformer models as reference points when establishing our ASR, MT and ST baseline systems. We use these reference models to evaluate our alignment methods in a more restricted in-domain scenario on the How2 dataset, and subsequently also choose other, more powerful off-the-shelf pre-trained ASR and MT models to evaluate the alignment approach in a more general scenario, exploring the dimensionality and domain adaptation capabilities of the connectors."}, {"title": "ASR models", "content": "For our reference ASR system (referred to as E-Branchformer small), we train a small 12-layer E-Branchformer (Kim et al., 2022) model with a 6-layer transformer decoder for English ASR on the How2 dataset. The model uses 4 attention heads, $d_{model}$ of 256, and a lower-cased unigram (Kudo, 2018) sub-word vocabulary of size 5000 with punctuation removed. In accordance to the ESPnet reference, an additional CTC objective (Karita et al., 2019) with a weight of 0.3 is used alongside the standard cross-entropy loss. We use 80-dimensional log-mel-filterbanks as features. Speed perturbation in factors [0.9, 1.0, 1.1] as well as SpecAugment (Park et al., 2019) in the \u2018LD' strategy were applied during training. The final model achieves 12.2% WER on the test set, as shown in Table 1.\nFor the off-the-shelf models, we choose two models that have not previously seen any of the How2 data. First, we use a scaled-up version of our baseline CTC/attn. ASR system \u2013 16-layer E-Branchformer model with an 8-layer decoder and $d_{model}$ of 512. This model was trained on 6k hours of English data, described in Appendix A along with evaluation results. As shown in Table 1, the model achieves 11.7% WER on the test set without fine-tuning, but we hope to leverage its representation-building capabilities in our alignment experiments. It will be further referred to as E-Branchformer medium. Lastly, we use OpenAI Whisper-small.en (Radford et al., 2023) as our last and most powerful foundation ASR model, achieving 7.9% WER on the test set."}, {"title": "MT models", "content": "Our baseline MT system adopts the MarianMT (Junczys-Dowmunt et al., 2018) transformer implementation available in Hugging Face Transformers. Again, following the ESPnet How2 recipes, both the encoder and decoder consist of 6 layers with 4 attention heads, $d_{model}$ of 256, and intermediate layer size of 2048. The source and target word embedding layers were untied in accordance with (Inaguma et al., 2020). The system was trained in a true-cased to true-cased manner with both the source and target BPE-based (Gage, 1994) vocabularies containing 8000 word units. As shown in Table 2, the model achieves 57 BLEU on the How2 test set, matching the results of ESPnet.\nAdditionally, we select an En-Pt MT model based on the T5 (Raffel et al., 2020) architecture from (Lopes et al., 2020). The model was based on the English T5-base checkpoint, which was first pre-trained on Portuguese language (Carmo et al., 2020), and then fine-tuned for English to Portuguese translation on a 5M English-Portuguese sentence subset of ParaCrawl (Espl\u00e0 et al., 2019), as well as domain-specific data in preparation for the WMT19 and WMT20 biomedical translation tasks. The encoder and decoder consist of 12 layers with a $d_{model}$ of 768. This MT model is out-of-domain on How2, achieving 'only' 38.8 BLEU on the test set. However, we chose it in hopes of being able to leverage its Portuguese text-generation capabilities in our alignment experiments. The model is freely available on Hugging Face\u2075 and will be further referred to as simply the T5 model."}, {"title": "ST baselines", "content": "From our E-Branchformer small and MarianMT models trained on How2, we construct two baseline ST systems. First, we once again adopt the ESPnet approach on How2, where we use the ASR encoder and MT decoder of the foundation systems to initialize the encoder and decoder of the ST system. The vocabulary of the decoder is the same true-cased BPE vocabulary used by the MarianMT model. The system is then trained for a maximum of 40 epochs with a learning rate of le-3, 10000 warm-up steps, batch size of 128, and early stopping to avoid overfitting. SpecAugment is no longer used, however, we keep the speed perturbation with the same factors of [0.9, 1.0, 1.1]. The encoder is kept frozen for the first 8 epochs of training. This baseline system achieves 45.6 and 45.2 BLEU on the val and test How2 subsets, respectively, which is on par with the results of the reference ESPnet system, as shown in Table 3.\nThis E2E system represents the more robust, although more costly solution to the ST task, obtaining good performance using a small model, which employs only one decoding step and does not suffer from any domain mismatch. However, optimizing all the model parameters is necessary to get the best results, leading to bigger resource consumption if the model were to scale up."}, {"title": "Alignment architecture evaluation", "content": "In the first set of alignment experiments, we evaluate the two alignment architecture variants (ECD, ECED) in conjunction with both of the connector network types (Q-Former, STE). These experiments aim to primarily determine the viability of the aligning approach to training ST systems as a whole, juxtaposing them against the baseline methods. On top of that, we test the effects of using different numbers of layers in the connector. Unless specified otherwise, all aligned models are trained for a maximum of 70 epochs with 15000 warm-up steps, a batch size of 128, and a peak learning rate of 2e-4. We adopt early stopping to avoid overfitting and apply speed perturbation with the factors of [0.9, 1.0, 1.1]. Other than the layer count, the connector network parameters are kept constant, with a $d_{model}$ of 256, 4 attention heads, and intermediate layer size of 2048, keeping the Q-Former query count at 100. The results are shown in Table 4, Appendix B then contains the full Table 8."}, {"title": "ECD evaluation", "content": "As is shown in the first half of Table 4 the STE connector outperforms the Q-Former by a significant margin, almost matching the performance of the baseline E2E system, achieving 45.0 and 44.8 BLEU on the val and test sets, respectively. It could be argued, that the STE connector perhaps benefits too much from the parameters added by the convolutional subsampler, however, even the 4-layer STE configuration still outperforms the 6-layer Q-Former, suggesting that the problem lies probably in the way the Q-Former represents and extracts information from the input speech embeddings. This is further analyzed in Section 6.3. We also find that the STE training process is more stable and 'well-behaved' than the one of the Q-Former, with faster convergence and fewer spikes in validation metrics during training."}, {"title": "ECED evaluation", "content": "For the ECED architecture, the performance falloff is not as stark when decreasing the number of connector layers, compared to ECD. The best model once again uses the 6-layer STE connector, with results on par with the equivalent ECD configuration, achieving 44.7 and 44.8 BLEU on the val and test sets, respectively. The Q-Former maintains better results in contrast to the ECD architecture, reinforcing the hypothesis that the mapping problem for the ECED architecture is a simpler one to that of ECD.\nConcluding for both architectures, most of the aligned models outperform our cascade baseline. Though none of them surpass the E2E baseline system, comparable performance is obtained only tuning a small module with less than a quarter of the parameters of the E2E system. This indicates that there is potential in the alignment approach to ST, and we further explore what can be achieved by scaling up the foundation ASR and MT models in Section 7."}, {"title": "Evaluating the connectors on different input lengths", "content": "As the STE connector outperforms the Q-Former in previous experiments, we argue that this stems from the variable to fixed-length mapping performed by the Q-Former, directly defined by the number of queries used. Contrary to (Yu et al., 2023), we find that both choosing this number to be too low (40-60) or too high (128+) comes with performance degradation, as the Q-Former either does not have enough queries to represent the necessary information or is unable to distribute it among the queries effectively. We find that for every foundation model configuration, there is a sweet spot for the number of queries used, which has to be experimentally determined. On the contrary, the STE connector output length is always monotonously correlated to the amount of represented information, reducing the risk of propagating noise to the downstream MT model."}, {"title": "Foundation model scaling", "content": "We investigate the prospect of aligning different off-the-shelf (possibly out-of-domain) pre-trained ASR and MT models, aiming to improve ST performance by increasing the capability of the aligned models. Ideally, the alignment process would overcome any domain mismatch that would appear if the same systems were used as parts of a cascade ST system\u2077."}, {"title": "Connector modules as domain adapters", "content": "Interestingly, the BLEU scores achieved by the T5 system in the ECD aligned scenario with the Whisper model far supersede the raw MT scores on How2 from Section 5.2, bringing an improvement of over 9 BLEU points on the test set compared to the base T5 MT scenario\u2079. We argue that this is due to the connector network being able to serve as a domain adapter, reinforcing the notion that the foundation ASR and MT models might not require fine-tuning on the available ST data before alignment."}, {"title": "Low resource scenario", "content": "To better understand the ST data requirements for successful alignment, we train two ECED systems with the same 6-layer STE connector on three low-resource simulation How2 splits, as similar scenarios are not discussed in the related works.\nAs shown in Table 6, the training is reasonably successful even with the 153-hour How2 split. Results for the 17 and 51-hour splits show potential for future work, as we still observe a clear performance gain from utilizing more powerful foundation models. Namely, for the 51-hour split, the ST result of the T5 model still exceeds its performance in the base MT task on the test set."}, {"title": "Conclusions", "content": "In this paper, we explored an approach of aligning pre-trained frozen ASR and MT models for ST. Our experiments show that the alignment approach is a viable generic framework for establishing new end-to-end speech ST. We find that scaling up the foundation ASR and MT models leads to universally better ST results, while the size of the connector network can remain constant, and relatively small. Our STE connector outperforms the Q-Former due to performance degradation on longer input sequences and the difficulty of choosing an appropriate number of queries for the given task. The connectors also demonstrate good domain adaptation capabilities, improving translation performance by more than 9 BLEU points on the test How2 set in case of the out-of-domain T5 model with respect to its base MT performance on the test set."}, {"title": "Limitations", "content": "We identify some limitations of the presented alignment ST approach. First, although initial experiments (Section 8) are promising, further investigation is required to study the effectiveness of the proposed framework in low-resource scenarios. We think that reducing the amount of ST data needed for successfully performing the alignment procedure should be a primary concern for future work so that this alignment paradigm can become viable even for truly low-resource scenarios.\nSecond, perhaps unsurprisingly, the small size of the connector network ultimately starts to negatively impact ST performance if the foundation model $d_{model}$ exceeds a certain threshold. We attempt to use a 233M parameter En-Pt MT model\u00b9\u2070 from the OPUS project (Tiedemann and Thottingal, 2020; Tiedemann, 2020) in the aligned scenario. This model achieves 58.2 BLEU on the test set for the MT task without fine-tuning, suggesting good prospects for the aligned ST scenario. However, we find that using the same 6-layer STE connector with $d_{model}$ of 256, the training takes longer than for other models to converge, ultimately yielding 'only' 47.3 BLEU on the test set in conjunction with Whisper in the ECD setting. As this MT model has a $d_{model}$ of 1024, we suspect that the slightly subpar performance is likely caused by the hidden size mismatch between the connector and the MT model. More experiments need to be conducted to confirm this and we leave those for future work."}]}