{"title": "LoXR: Performance Evaluation of Locally Executing LLMs on XR Devices", "authors": ["Dawar Khan", "Xinyu Liu", "Omar Mena", "Donggan Jia", "lexandre Kouyoumdjian", "Ivan Viola"], "abstract": "The deployment of large language models (LLMs) on extended reality (XR) devices has great potential to advance the field of human-Al interaction. In case of direct, on-device model inference, selecting the appropriate model and device for specific tasks remains challenging. In this paper, we deploy 17 LLMs across four XR devices-Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision Pro-and conduct a comprehensive evaluation. We devise an experimental setup and evaluate performance on four key metrics: performance consistency, processing speed, memory usage, and battery consumption. For each of the 68 model-device pairs, we assess performance under varying string lengths, batch sizes, and thread counts, analyzing the tradeoffs for real-time XR applications. We finally propose a unified evaluation method based on the Pareto Optimality theory to select the optimal device-model pairs from the quality and speed objectives. We believe our findings offer valuable insight to guide future optimization efforts for LLM deployment on XR devices. Our evaluation method can be followed as standard groundwork for further research and development in this emerging field. All supplemental materials are available at nanovis.org/Loxr.html.", "sections": [{"title": "1 INTRODUCTION", "content": "INCE the 2022 release of OpenAI's ChatGPT interface [1], relying on the GPT-3.5 Large Language Model (LLM), we have witnessed the disruptive and transformative effects of LLMs. These models are capable of describing a wide variety of topics, respond at various levels of abstraction, and communicate effectively in multiple languages. They have proven capable of providing users with accurate and contextually appropriate responses. LLMs have quickly found applications in tasks such as spelling and grammar correction [2], generating text on specified topics [3], integration into automated chatbot services, and even generating source code from loosely defined software specifications [4]. Research on language models, and on their multimodal variants integrating language and vision or other technologies has recently experienced rapid growth. For instance, in computer vision, language models are combined with visual signals to achieve tasks such as verbal scene description and even open-world scenegraph generation [5]. These technologies enable detailed interpretation of everyday objects, inference of relationships among them, and estimates of physical properties like size, weight, distance, and speed. In user interaction and visualization research, LLMs serve as verbal interfaces to control software functionality or adjust visualization parameters [6], [7]. Through prompt engineering or fine-tuning, loosely defined text can be translated into specific commands that execute desired actions within a system, supported by language model APIs. The capabilities of language models continue to improve significantly from one version to the next. Yet, this comes at the cost of a significant growth in size of the most advanced models. As an example, models released or operated by leading companies such as OpenAI, Meta, Microsoft, and Google now reach the scale of trillions of trainable parameters [1]. Due to their size and complexity, the training and inference of these models are limited to dedicated data centers equipped with considerable computational and memory resources. Another disruptive application of LLMs involves their use in wearable technology to assist users in various environments. For instance, Meta AI is being integrated into Meta's virtual reality hardware, where images captured by the device are streamed over the network to Meta's servers for model inference, enabling image interpretation or prompt response tasks.\u00b9 The overarching vision is to develop assistive hardware that is as lightweight as sunglasses but capable of aiding users in a wide range of tasks by understanding the scene they are viewing. While such technological advances have the potential to empower users in unprecedented ways, three main drawbacks must be considered due to the heavy reliance on network connectivity and cloud-based data processing. First, users will need to maintain extremely high network connectivity to ensure that remote services can perform assistive tasks without excessive latency, which would degrade the user experience. Second, data privacy poses a significant challenge. Many environments, such as medical settings and various industries, operate under strict data privacy regulations that prohibit the transmission of sensitive information to remote servers, precluding the use of these services in such contexts. Third, the reliance on subscription models for these assistive services introduces ongoing costs for users, requiring them to periodically purchase licenses, which can accumulate into significant long-term costs. Therefore, there is dedicated research effort focused on developing autonomous AI assistive technologies that can perform inference directly on a device. To this goal, several smaller"}, {"title": "2 RELATED WORK", "content": "We review related work with respect to two major research directions. The first research direction is exploring the use of LLMs on XR devices, and the second research direction explores how to make LLMs directly deployable on low-resource hardware.\n2.1 LLM-powered XR Applications\nOwing to the powerful semantic understanding and extensive general knowledge of LLMs, numerous studies have explored their application in assisting various tasks within XR scenarios. Torre et al. [9] introduced LLMR, a framework that leverages LLMs for the real-time creation and modification of interactive mixed reality experiences, enabling tasks such as generating new content or editing existing works on VR/AR devices. Jia et al. [6] developed the VOICE framework, which employs a two-layer agent system for conversational interaction and explanation in scientific communication, with a prototype deployable on VR devices. Kurai et al. [10] proposed MagicItem, a tool that allows users with limited programming experience to define object behaviors within the metaverse platform. Zhang et al. [11] introduced OdorAgent, which combines an LLM with a text-image model to automate video-odor matching. Yin et al. [12] identified potential limitations in LLM-based automated systems and proposed the systematic framework Text2VRScene to address them. Chen et al. [13] leveraged the extensive capabilities of LLMs in context perception and text prediction to enhance text entry efficiency by reducing manual keystrokes in VR scenarios. Giunchi et al. [14] developed DreamCodeVR, a tool designed to help users, regardless of coding experience, create basic object behavior in VR environments by translating spoken language into code within an active application. Wan et al. [15] presented an LLM-based AI agent for human-agent interaction in VR, involving GPT-4 to simulate realistic NPC behavior, including context-aware responses, facial expressions, and body gestures. As they stand, these previous contributions still require the deployed devices to connect to a cloud server for LLM inference, raising concerns about user privacy, latency, costs, and internet access requirements. In our work, we evaluate the local inference performance of various LLMs and propose a prototype capable of using local LLM inference.\n2.2 On-device LLMs\nRunning LLMs on edge devices, commonly called on-device LLMs, has garnered significant research interest due to their advantages in enhancing privacy, reducing latency, and operating without the need for internet connectivity. Because of the limited memory and computing capabilities, on-device LLMs usually require resource-efficient LLM deployment [16]: a trade-off between performance and model size. Cheng et al. [17] introduced the SignRound method, which leverages signed gradient descent to optimize both rounding values and weight clipping. This approach achieves outstanding performance in 2- to 4-bit quantization while maintaining low tuning costs and eliminating additional inference overhead. Ma et al. [18] developed the LLM-Pruner method, which uses structural pruning to selectively remove non-essential coupled structures based on gradient information, effectively preserving the core functionality of the LLM. Their results demonstrate that the compressed models continue to perform well in tasks such as zero-shot classification and generation. Gu et al. [19] introduced a novel knowledge distillation method that compresses LLMs into smaller models, resulting in the student model MINILLM. MINILLM demonstrates superior performance compared to baseline models, producing"}, {"title": "3 RESEARCH METHODS AND MATERIALS", "content": "The aim of this study is to deploy LLMs on XR devices and evaluate their performance in different aspects. This section outlines our research methodology, including the key challenges, the LLM deployment approach, and the evaluation metrics and models used in this study.\nProblem Statement: In this study, we deploy 17 LLMs, denoted as ${m_1, m_2,...,m_{17}}$ (see Table 1), across four XR and mobile devices: $d_1,d_2,d_3$, and $d_4$, representing Magic Leap 2 (ML2), Meta Quest 3 (MQ3), Vivo X100 Pro (Vivo), and Apple Vision Pro (AVP), respectively. Our goal is to perform a comprehensive evaluation of these models on the four devices to determine how well they handle large-scale language processing tasks in resource-constrained XR environments. We define our evaluation using six key performance metrics: $P_0, P_1, P_2, P_3, P_4, P_5$, which represent Model Quality, Performance Consistency, Performance Speed (with respect to string length and prompt length), Parallelism (with respect to thread count and varying batch size), Memory Usage, and Battery Usage, respectively. Each LLM $m_i \\in {m_1,m_2,...,m_{17}}$ is tested on each XR device $d_j \\in {d_1,d_2,d_3,d_4}$ for evaluation across each performance metric $P_k \\in {P_0, P_1, P_2, P_3, P_4, P_5}$. The evaluation can be summarized as $X_{mdk}$, as given in Eq. (1).\n\n$X_{mdk} = {X_{ijk} | i \\in {1, ..., 17}, j\\in {1,...,4},k\\in {0,...,5}}$ (1)\n\nHere, $X_{ijk}$ represents the performance of model $m_i$ on device $d_j$ for metric $p_k$, forming the basis for cross-device and cross-model analysis. Fig. 1 presents an overview of our research pipeline, where each rectangular box represents a specific procedure, and the connected ovals indicate their respective outcomes. These are discussed in detail below.\n3.1 LLM Deployment on XR Devices\nWe aim to deploy the LLMs locally on XR devices. For this purpose, we customize the Llama.cpp library [22] and build it for the four target XR devices. The resulting application is capable of loading various appropriately sized GPT-Generated Unified"}, {"title": "3.2 Initial Investigation and Factors Identification", "content": "After deploying the LLMs, we conducted an initial investigation through anecdotal informal tests, literature reviews, and by studying LLM documentations. The aim of this preliminary investigation was to identify the key challenges and factors affecting performance-prerequisites for a fair evaluation and to define our evaluation approach (Section 3.3). We observed that XR devices experience performance degradation during prolonged tasks due to factors such as overheating, battery depletion, and background processes. Moreover, they often do not perform at a stable level. For instance, with the same model-device pair and parameters, we obtained different results across different runs of the same test. We refer to the metric used for measuring this variability as Performance Consistency. Another important"}, {"title": "3.3 Evaluation Approach", "content": "Considering the key challenges and our initial investigation (see Section 3.2), we formulate a standardized evaluation approach for our assessments. First, we designed an experimental setup to minimize potential biases and ensure the validity of the results (see Section 4). This setup establishes identical parameters across all devices and defines the test types to be executed. We developed several guidelines to ensure a comprehensive and standardized evaluation. To account for potential performance variations among LLMs, we selected 17 models from five different series and evaluated them across four devices. Additionally, we conducted Model Quality analysis to provide technical insights into the selected models. These tests are device-independent, and were conducted to understand the details and properties of each model. Next, each model-device pair was subjected to various tests (see Section 3.4). To mitigate performance fluctuations, each experiment was repeated multiple times, and the average results were computed. Finally, we unified the five evaluation metrics, i.e., $[P_1,P_5]$, using Pareto Efficiency theory to identify Pareto-optimal model-device pairs. Our evaluation framework and experimental design aim to facilitate reproducibility for other researchers. Additionally, we will provide open-source test scripts to enable replication of our study and further evaluations."}, {"title": "3.4 Models and Evaluation Metrics", "content": "We use 17 LLMs from five different series (see Table 1) in our evaluations. The models were obtained from Hugging Face's model repository, a widely used platform for accessing pre-trained LLMs [23]. For consistency in naming conventions, we assigned unique IDs (m\u2081 to m17) to these models. Based on the Llama documentation [22] and our initial investigation, this evaluation comprises multiple tests to assess various performance parameters. For testing, we use Synthetic Prompts automatically generated by llama.cpp, ensuring uniform testing conditions without reliance on an external dataset. The string length is controlled via a parameter set to 64, 128, 256, 512, and 1024 tokens. The evaluation includes the following tests.\n\u2022\nPerformance Consistency: Measures stability over time.\n\u2022\nProcessing Speed: Analyzes performance across different string and prompt lengths, further divided into:\n\u2022\nPP (Prompt Processing): Evaluates encoding efficiency in handling input prompts of varying lengths.\n\u2022\nTG (Token Generation): Evaluates the device's speed in generating output tokens of varying lengths.\nParallelization and Concurrency: This includes:\nBatch Test (BT): Evaluates the device's ability to process multiple input samples simultaneously by handling different batch sizes during token generation.\nThread Test (TT): Measures performance scalability by varying thread counts during LLM execution. Here, Batch Size refers to the number of input samples processed in parallel. While larger batch sizes improve computational efficiency, they also demand more memory. Each of these metrics requires task-specific analysis for both models and devices. Furthermore, we conducted additional evaluations for Memory Usage and Battery Consumption. Finally, we performed a Pareto Optimality analysis to collectively examine the results across different models and devices."}, {"title": "4 EXPERIMENTAL SETUP", "content": "This section describes the experimental setup and methodology for evaluating processing speed and error rates. The results are presented in Section 5. Before diving into the detailed setup, we first discuss the two evaluation metrics: processing speed and error count.\nProcessing Speed: In this study, we measure the processing speed for each model-device pair ($m_i, d_j$) by recording the time $t_{ijk}$ taken by model $m_i$ running on device $d_j$ to process a string of length $L$. The instantaneous processing speed, $S_{inst.}$, expressed in tokens per second, is calculated as follows:\n\n$S_{inst.} = \\frac{L}{t_{ij}}$ (2)\n\nHowever, instead of relying on a single run, we perform 5 runs for each model-device pair to ensure accuracy and measure stability. If the coefficient of variance (CV) across the 5 runs is less than 40%, we calculate the 5-Run-Mean Speed (referred to as consistent speed, $S_c$) as:"}, {"title": "4.1 Models Quality Analysis", "content": "Due to the storage and computational limitations of edge devices, deploying LLM locally often requires a trade-off between model size and performance. To put our performance results in perspective, we first conducted an empirical analysis of the models we have used in this study. This section offers insights into the models evaluated in this work, presenting a comparative analysis. While this section is not directly related to device performance, we believe providing detailed information about the models is essential for understanding their capabilities and limitations. We select six benchmarks to evaluate each model and calculate different metrics for each run:\n\nHellaswag [24]: commonsense reasoning tasks, focusing on selecting the most plausible ending to a given situation from multiple options.\nmmlu [25]: evaluating a model's knowledge and reasoning across 57 different subjects, ranging from elementary-level topics to advanced professional knowledge.\nARC [26]: evaluating model's ability to answer complex, grade-school science questions that require reasoning and problem-solving beyond simple fact retrieval.\nTruthful-qa [27]: evaluate how accurately AI models generate truthful answers to questions, especially in cases where common misconceptions or false information could lead to incorrect responses.\nwinogrande [28]: large-scale benchmark for commonsense reasoning, specifically designed to test a model's ability to resolve ambiguous pronouns in sentences.\nWikiText-2 [29]: containing over 100 million tokens, sourced from verified Wikipedia articles, designed to support research in language modeling and text generation tasks. For the first five benchmarks, which consist of multiple-choice questions, we use accuracy as the evaluation metric. However, for WikiText-2, the task is to predict the probability of each word in a given text, reflecting the model's natural language understanding. Therefore, we use perplexity as the evaluation metric, defined as:\n\n$Perplexity(P) = e^{-\\frac{1}{N} \\sum_{i=1}^{N}logP(w_i)}$ (4)\n\nwhere $P(w_i)$ represents the probability assigned by the model to the $i$-th word, and $N$ is the total number of words in the sequence."}, {"title": "4.2 Evaluating Performance Consistency", "content": "Some devices may experience performance degradation over time. In these experiments, we evaluate how processing speed fluctuates across multiple runs. For each model $m_i$ and device $d_j$, we perform $N$ runs and compute the mean $(\\mu_{ij})$ of the speed ($S_c$), standard deviation $\\sigma_{ij}$, and coefficient of variation ($CV_{ij}$). We also record the error count, as well as the maximum and minimum speeds observed during each experiment. In our experiments, we set $N = 20$, i.e., each model-device pair is tested 20 times to evaluate performance variance. The mean speed across N runs is calculated as:\n\n$\\mu = \\frac{\\sum_{i=1}^{N} S_c}{N}$ (5)\n\nwhere $S_c$ represents the 5-Run-Mean speed measured during each run, calculated as in Eq. (3). In other words, each $S_c$ itself is the mean of 5 runs, ensuring that the measured value is not too affected by errors or hidden factors. To let the devices cool down and conduct each run in comparable circumstances, we leave each device idle for two minutes between each run of the same model. We find this to be adequate, but just to be safer, between different models, we leave each device idle for ten minutes. We noted that the Meta Quest is not very prone to heating up. Heating was more noticeable on the Vivo X100s Pro, so it is always placed on a metal plate for heat dissipation. This is not representative of actual use, but this device is included in our evaluation as a baseline, not as an example of an actual XR headset. Magic Leap features a cooling fan, as does Vision Pro. For each test, we conducted both PP and TG evaluations with the string length set to 64, i.e., PP = 64 and TG = 64. For each model-device pair, we calculated the standard deviation, mean, and CV. The results are presented in Section 5.2."}, {"title": "4.3 Processing Speed and String Length", "content": "String length, comprising both Prompt Length and Size of the Token Set, significantly affects performance. We evaluate the performance of LLMs on XR devices with varying string lengths to assess their processing speed. For each experiment, the processing speed is calculated as in Eq. (3). We conducted experiments for five different string lengths (64, 128, 256, 512, and 1024) under the following PP and TG setups.\n\u2022\nPP: The PP values were set to 64, 128, 256, 512, and 1024, while TG was fixed at 0, with all other parameters set to their default values.\nTG: The TG values were set to 64, 128, 256, 512, and 1024, while PP was fixed at 0, with all other parameters set to their default values. The results are shown in Section 5.3."}, {"title": "4.4 Parallelization with Thread Count and Batch Size", "content": "The parameters Batch Size and Thread Count, which control parallelization, also significantly affect performance. We evaluate the performance of LLMs on XR devices with varying batch sizes and thread counts to assess their processing speed, calculated using Eq. (3). The two tests conducted are described below:\nBT: We set PP to 64, TG to 0, and varied the batch sizes between 128, 256, 512, and 1024, with all other parameters kept at their default values.\nTT: We set PP to 64 and TG to 0, while varying the thread counts between 1, 2, 4, 8, 16, and 32. All other parameters were kept at their default values. The results are shown in Section 5.4."}, {"title": "4.5 Evaluation of Memory Consumption", "content": "Memory consumption $M_{ij}$ is measured in terms of the Resident Set Size (RSS) which represents the actual physical memory usage of the relevant processes. For each model-device pair ($m_i,d_j$), we measure memory usage over three runs, and report the average. The memory usage for PP and TG parameters was evaluated using the same approach as the performance consistency assessment. In each test, a fixed prompt was provided, and the model was instructed to generate outputs. The experiments were conducted with batch sizes of 128, 256, 512, and 1024, and their average memory consumption was recorded. The results are shown in Section 5.5."}, {"title": "4.6 Battery Consumption", "content": "This section presents battery consumption rates during extended PP and TG tests. From our initial investigation, we observed that only AVP exhibited a slight impact of model size on battery consumption, while the other three devices did not show significant variations with changing model sizes. Given this observation, instead of testing all 17 models, we conducted battery tests on selected models. Specifically, we chose m\u2081 as the sole model from the Qwen Series. For the remaining series, we selected the smallest and largest models: m2 and m5 from the Vikhr-Gemma Series, m6 and m11 from the Phi-3.1 Series, m12 and m13 from the LLaMA-2 Series, and m\u20814 and m17 from the Mistral-7B Series. For each experiment, we recorded the battery level at the start and end of the experiment, allowing us to calculate battery consumption over a fixed duration of 600 seconds (10 minutes). Each experiment was repeated three times, and we reported the mean values. To ensure consistent conditions, we took a 600-second break between experiments, allowing the devices to cool down. The results are presented in Section 5.6."}, {"title": "4.7 Pareto Optimality", "content": "Although we evaluate our five performance metrics across various devices and models, the metrics remain fragmented and cannot be directly compared with other metrics. To address this, we turn to Pareto efficiency theory [30], which provides a measure of efficiency in multi-objective contexts and has been widely applied in various system design approaches [31], [32]. A choice is considered Pareto optimal if no other choice exists that can improve any of its objective criteria without deteriorating at least one other criterion. In our case, x1 is considered dominated by x2 through objects f if :\n\n$f_i(x_1) \\leq f_i(x_2) \\forall i \\in {1,...,m}$  and $\\exists j \\in {1,...,m} | f_j(x_1) < f_j(x_2)$ (6)\n\nwhere i, j represent different objective indices. A choice x* is considered Pareto optimal if no other feasible option dominates it. The set of all non-dominated designs forms the Pareto front, which represents the optimal trade-offs between all objectives. To identify the optimal choices across devices and models, we define three objectives: quality, performance, and stability. To calculate the final score for each objective, we propose Eq. (7). For a given device model pair and a specific objective, we first apply min-max normalization to each metric across all device-model pairs to eliminate the impact of different scales. Then, we compute a weighted sum of all metrics for that pair to obtain a single score:\n\n$f_o(x) = \\sum_{i=1}^{n} w_i \\frac{m_i(x) - min(m_i)}{max(m_i) - min(m_i)}$ (7)\n\nwhere o is the objective index, n is the number of metrics used in the objective category, $w_i$ defines the weight for each metric, and min and max calculate the minimum and maximum values across the entire set of results for metric $m_i$. In practice, some metrics, such as perplexity, are better when lower, which is the opposite of the direction in Eq. (6). Therefore, we take the reciprocal of each perplexity value."}, {"title": "5 PERFORMANCE EVALUATION RESULTS", "content": "This section presents the results of four types of experiments described in Section 4.\n5.1 Model Quality Analysis\nAs shown in Table 1, we select five different model architectures, and for each architecture, we choose various quantization settings. While lower-bit quantization reduces model size, speeds up inference, and lowers power consumption, it typically comes at the cost of reduced model performance. The results of the model quality are illustrated in Fig. 2. As shown in Fig. 2, a consistent trend is observed across all benchmarks: applying lower-bit quantization settings results in reduced model performance. Models with more parameters exhibit better language understanding capabilities, leading to higher performance under the same quantization settings. Additionally, different benchmarks exhibit varying levels of sensitivity to the quantization settings. For example, Hellaswag, ARC and WikiText-2 are more sensitive compared to the other three benchmarks."}, {"title": "5.2 Performance Consistency Results", "content": "Figures 3 and 4 illustrate the consistency of performance across all model-device pairs, while Table 2 provides the corresponding quantitative results. Across all 20 runs, the Apple Vision Pro demonstrates the most consistent results compared to other devices, with the corresponding speed also being the fastest. Notably, the GPU results for the Apple Vision Pro exhibit greater stability and smaller variance. Among the remaining devices, the Magic Leap 2 also shows a reasonable degree of stability; however, for the first three models, its variance is surprisingly high, reaching 27%, 32%, and 26% for m2, m3, and m3 under the PP setup, and 20%, 23%, and 27% under the TG setup, respectively. In contrast, the Meta Quest 3 and Vivo X100 Pro exhibit relatively poor performance both in terms of speed and variance. Between these two devices, the Meta Quest 3 achieves higher speeds than the Vivo X100 Pro, but there is no clear difference in variance. For some models, one device shows lower variance, while for others, the variance is higher. Furthermore, as shown in Table 3 performance varies significantly for m\u2081 over Magic Leap 2 and Meta Quest 3, but it is generally consistent across the other two devices. These findings suggest that, provided model-device pairs are carefully selected, LLM performance can be considered reliable, strengthening the case for on-device LLM use in XR environments."}, {"title": "5.3 Results of Processing Speed and String Length", "content": "Figs. 5 and 6 present the processing speed results of PP and TG with varying string lengths (prompt length and token sets). Both PP and TG were tested with values of 64, 128, 256, 512, and 1024. The processing speed varies significantly across string lengths, devices, and models. String length and PP vs TG Speed: PP speed is consistently higher than TG speed, with a speedup of approximately two to three times across the four (CPU-based) devices, while on AVP (GPU), the PP speed is significantly higher, ranging from 10 to 19 times the TG speed. Similarly, increasing the prompt size slightly reduces the prompt processing speed in PP, whereas in TG, the speed reduction is more significant with a growing token set. This suggests that TG has a higher computational cost due to its sequential execution of tokens, whereas PP is somewhat parallelized, requiring only the loading of prompts. Though there is minor variation in the 64, 128, and 256 string lengths, the last two, 512 and 1024, are consistently the slowest. When examining the variation across string lengths, MQ3 exhibits the largest variance, whereas ML2 and AVP have the smallest variance. Device-Based Analysis: If we examine the devices, the overall performance trends reveal that AVP consistently shows the highest speed in both PP and TG tests. ML2 ranks second in both PP and TG performance, while Vivo X100s Pro outperforms MQ3 in TG (particularly in longer strings 512, 1024) but ranks fourth in PP. This ranking highlights the varying computational capabilities of XR devices, with AVP (both CPU and GPU) standing out as the fasted one across all conditions. ML2 shows stable performance across all variations of string lengths, with lower CV% values, indicating that it does not vary significantly with changing string lengths. In contrast, MQ3 shows a higher CV%, indicating greater variance with increasing string length. Error Counts: Figure 7 shows the errors counts during the PP and TG tests. Errors mostly occurred in AVP (CPU) and MQ3, particularly at longer prompt lengths of 512 and 1024 tokens. There are a few reasons for this. First, longer strings process more slowly, leading to abrupt changes and occasional failures. Second, memory constraints or inefficiencies in sustained generation contribute to these errors. With AVP, the general user experience is also suboptimal, as the device must be actively mounted on the head, which can cause errors if not handled carefully. Overall, MQ3 records the highest count of errors, with frequent retries affecting its reliability, whereas ML2 remains highly stable, leading to minimal inconsistencies (zero errors). As expected, AVP (GPU) also reported zero errors. Model-Based Analysis: In the model-wise analysis, smaller models such as Qwen2-0.5B and Vikhr-Gemma-2B achieve the highest speeds across devices, particularly in PP. In contrast, larger models like the LLaMA-2-7B series and Mistral-7B series exhibit significantly lower processing speeds, with higher variability and instability, especially in TG. Model size also impacts processing speed, as smaller models are faster. Here, m\u2081 is the fastest (though omitted from later analysis), while m2, m3, and m5 are the fastest among the remaining models, whereas m14 and m16 are the slowest."}, {"title": "5.4 Results of Parallelization with BT and TT", "content": "This section presents the results of BT and TT, both of which are used to achieve concurrency and parallelization. Regardless of the parallelization method, the three models m2, m3, and m5 consistently rank among the fastest across all devices, while m14 and m16 are the slowest. Results of BT: Fig. 8 presents the results of the batch test with varying batch sizes of 128, 256, 512, and 1024. Generally, increasing the batch size leads to a decrease in performance across all four devices (except for AVP (GPU)) due to increased computational overhead. AVP (GPU), with its strong computational resources, does not show a significant performance drop with varying batch sizes, although there are some fluctuations at a batch size of 256. This indicates that GPU-based processing on AVP (GPU) is highly optimized for parallel execution and can effectively manage larger batches of input without affecting processing speed. More importantly, AVP (CPU) experiences a noticeable performance drop as the batch size increases. However, the remaining devices do not follow this trend. For instance, on MQ3, batch size 128 yields the best performance, whereas for the other batch sizes, there is no significant change. Similarly, for ML2 and Vivo, variations in batch size do not appear to have a substantial impact on performance. Results of TT: Fig. 9 presents the TT results for thread counts of 1, 2, 4, 8, 16, and 32. Across all four devices, processing speed sees the most significant increase when increasing the thread count from 1 to 4. The speed gain continues at 8 threads (or shows minor degradation), but beyond 8 threads, performance begins to decline slightly, with further degradation beyond 16 threads. Apple Vision Pro (CPU) fails at thread counts of 16 and 32, indicating its limitations in achieving this level of parallelism. AVP (GPU) follows a different trend, maintaining consistently high processing speed across all thread counts. Unlike CPU-based devices, its performance remains stable even at 32 threads, demonstrating its superior ability to handle concurrent tasks. These results highlight that while the four CPU-based devices benefit from moderate threading, AVP (GPU) is significantly more efficient at scaling concurrency without experiencing notable performance degradation. Since our study primarily focuses on CPU-based implementation, we conclude that using a moderate thread count of 4, 6, or 8 yields optimal results."}, {"title": "5.5 Memory Consumption Results", "content": "Fig. 10 presents the memory consumption results for each model-device pair. The values represent the mean memory consumption from five experiments with varying batch sizes of 128, 256, 512, and 1024. As expected, memory consumption varies across models, with most showing consistent usage across different devices. Generally, within each model series, memory consumption increases with model size. However, minor exceptions exist, such as m7 on Apple Vision Pro (GPU), which consumes more memory than other models in the Phi-3.1 series [m6 \u2013 m11]. This size-based trend does not necessarily hold across series, even for models of similar sizes. For example, m2 (1.36 GB) consumes less memory than m6 (1.32 GB) despite having a larger size, as they belong to different series. Similarly, m7 (1.94 GB) requires more memory than m5 (2.0 GB) and m15 (2.05 GB), as all three belong to different series. Another example is m12 (2.63 GB) consuming more memory than m16 (2.81 GB), despite being smaller in size. From a device-specific perspective, Apple Vision Pro demonstrates significantly lower memory consumption than the other devices, averaging 0.5782 GB on the CPU and 0.5488 GB on the GPU across all 17 models. Among the remaining devices, Meta Quest 3 consumes the least memory at 2.3520 GB, followed by Vivo X100 Pro at 2.3540 GB, while Magic Leap 2 exhibits the highest at 2.3748 GB. These findings suggest that Apple Vision Pro (both CPU and GPU) is approximately four times more memory-efficient than Magic Leap 2, Meta Quest 3, and Vivo X100 Pro. Its superior memory management and hardware optimizations make it the most efficient device in terms of memory consumption."}, {"title": "5.6 Battery Consumption Results", "content": "Figure 11 presents the battery consumption results over a 10-minute experiment. We observe that for the first two series, the Qwen Series and Vikhr-Gemma Series, battery consumption remains relatively low and with lower variation across all four devices. However, for larger models in the LLaMA-2 Series and Mistral-7B Series, battery consumption increases significantly. AVP shows a strong correlation between model size and battery consumption, both for GPU and CPU usage. In contrast, the remaining three"}]}