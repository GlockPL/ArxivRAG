{"title": "Improve Value Estimation of Q Function and Reshape Reward with Monte Carlo Tree Search", "authors": ["Jiamian Li"], "abstract": "Reinforcement learning has achieved remarkable success in perfect information games such as Go and Atari, enabling agents to compete at the highest levels against human players. However, research in reinforcement learning for imperfect information games has been relatively limited due to the more complex game structures and randomness. Traditional methods face challenges in training and improving performance in imperfect information games due to issues like inaccurate Q value estimation and reward sparsity. In this paper, we focus on Uno, an imperfect information game, and aim to address these problems by reducing Q value overestimation and reshaping reward function. We propose a novel algorithm that utilizes Monte Carlo Tree Search to improve the value estimation in Q function. Even though we choose Double Deep Q Learning as the foundational framework in this paper, our method can be generalized and used in any algorithm which needs Q value estimation, such as the Actor-Critic. Additionally, we employ Monte Carlo Tree Search to reshape the reward structure in the game environment. We compared our algorithm with several traditional methods applied to games such as Double Deep Q Learning, Deep Monte Carlo and Neural Fictitious Self Play, and the experiments demonstrate that our algorithm consistently outperforms these approaches, especially as the number of players in Uno increases, indicating a higher level of difficulty.", "sections": [{"title": "I. INTRODUCTION", "content": "In previous research, there has been considerable exploration into the application of reinforcement learning in perfect information games, such as Go [14] [11], Chess [12] [13] and Atari [8] [20]. These studies have demonstrated significant success, with reinforcement learning agents con-sistently outperforming top human players through the deep neural networks to derive optimal policies. However, research in the domain of imperfect information games has been relatively scarce, and significant breakthroughs in this area remain limited. Several factors contribute to this difference. Imperfect information games are defined by the fact that players cannot access all the information within a given state, leading to varying levels of knowledge among participants, unlike in perfect information games. For instance, in most card games, players can only view their own hands, while crucial details, such as the cards held by opponents and those remaining in the deck, remain concealed. Additionally, these games are inherently non-deterministic and often include a high degree of randomness and luck, which presents signifi-cant challenges for reinforcement learning algorithms during training, potentially leading to convergence issues or high variance in results. Reinforcement learning agents in imperfect information games also encounter the challenge of sparse rewards, where obtaining rewards can be difficult, and positive feedback may be delayed for extended periods. This increases the computational demand for sampling and renders much of the sampled data ineffective. In previous research on imperfect information games like StarCraft [22] [21], Doudizhu [27] [28] [29] and Texas Holdem [5] [1], Deep Monte Carlo(DMC) [18], Q-learning [24] [20] and Neural Fictitious Self Play(NFSP) [5] have been the dominant and commonly used methods, yielding some results; however, they suffer from the issues such as overestimation of q values [20], inefficient for incomplete episodes and large variance of expected returns(rewards) [3], and often require an extensive number of training episodes to achieve satisfactory performance. These challenges highlight the necessity for new methods to improve convergence speed, reduce training steps and increase sample efficiency.\nWe choose an imperfect information game, Uno, as the training environment for our reinforcement learning agent in this paper. Uno [19] is a popular game played worldwide, suitable for 2 to 10 players. The game consists of multiple card types in four colors: red, yellow, blue, and green. The numbered cards range from 0 to 9, and there are special cards such as Skip, Reverse, Draw 2, Wild, and Wild Draw 4. Each color contains two of each number card from 1 to 9, and one card with the number 0. Additionally, each color has two Skip, Reverse, and Draw 2 cards. There are four Wild cards and four Wild Draw 4 cards, making a total of 108 cards. The rules of Uno are as follows:\n1) Initialize: Shuffle the Uno deck, deal 7 cards to each player, and place the remaining cards in the center as the draw pile.\n2) Start: Flip over the top card of the draw pile to form the target card. Players must match either the color or number of this target card, or play a special card that fits these rules, such as Wild or Wild Draw 4.\n3) Play: If a player has a valid card, they must play it, setting it as the new target card, and the old target card is discarded. If they cannot play a valid card, they must draw a card from the draw pile. If the draw pile runs out, shuffle the discard pile to create a new draw pile.\n4) Win: The first player to get rid of all their cards wins the game. If a player is left with only one card in hand, they must call out \"Uno.\" Failure to do so results in a penalty, and the player must draw 2 additional cards.\nUno, like other imperfect information games, suffers from the issue of reward sparsity, but to a more severe degree. According to the rules, only the player who plays all their cards first can win, which leads to situations where the deck is exhausted, requiring a reshuffle. This results in more rounds in Uno compared to other games."}, {"title": "II. RELATED WORK", "content": "Any traditional reinforcement learning problems can be modelled as a mathematical framework called Markov De-cision Process (MDP) [18]. In academic formalism, the MDP can be described as a five-tuple $(S, A, T, R, \\gamma)$, where:\n$\\bullet$ S is the set of all possible states in the environment.\n$\\bullet$ A is the set of all possible actions the agent can take in the environment.\n$\\bullet$ T(s'|s, a) is the state transition probability function, which is the probability of transitioning to the next state $s'$ when agents take action $a$ in state $s$.\n$\\bullet$ R(s, a, s') is the reward function, which is the immediate reward received when transitioning from state $s$ to next state $s'$ by taking action $a$.\n$\\bullet$ $\\gamma$ is the discount factor, which determines how future rewards are valued compared to immediate rewards, typ-ically ranging from 0 to 1.\nMDP assumes that all information about a state in the environment is accessible to agents. However, imperfect in-formation games do not meet this condition, so they are modeled as a broader framework: Partially Observable Markov Decision Process (POMDP) [16]. A Partially Observable Markov Decision Process can be described as a seven-tuple $(S, A, T, R, \\Omega, O, \\gamma)$, where $(S, A, T, R, \\gamma)$ are the same as in MDP, while the additional two elements can be defined as:\n$\\bullet$ $\\Omega$ is the set of all possible observations that the agent can perceive."}, {"title": "B. Q-learning and Deep Q-network", "content": "Q-learning [18] is one of the foundational and most widely-used algorithms in reinforcement learning, playing a critical role as both a core component and an essential building block for more advanced algorithms across various domains. One of the key characteristics of Q-learning is its reliance on the Bellman optimal equation for updates. With sufficient training data, Q-learning can guarantee that the agent con-verges to the optimal policy. As a model-free, bootstrapping algorithm, Q-learning learns to estimate the optimal action to take in any given state by calculating and updating Q-values-representing the expected utility of taking a particular action from a specific state. Q-learning operates on a Q-table where each state-action pair is associated with a Q-value. The algorithm updates its Q-values through the following steps in each iteration:\n1) Select and execute an action: Select an action according to the current policy, such as the $\\epsilon$-greedy strategy, which typically selects the best current action (in most cases) but with a very small probability randomly selects another action to explore unknown state-actions.\n2) Observe the outcome and reward: After executing the action, agents observe the new state and the reward received.\n3) Update the Q-value: Update the Q-table using the following update rule:\n$Q(s, a) \\leftarrow Q(s,a)+ \\alpha(r+\\gamma max_{a'}Q(s', a') - Q(s, a))$,\nwhere s and a are the original state and action taken, $s'$ is the next state after taking the action, r is the immediate reward received, $\\alpha$ is the learning rate, and $\\gamma$ is the discount factor, which determines the decay rate of the reward.\nThis iterative process allows the agent to learn an optimal strategy over time by refining its estimate of the Q-values for each state-action pair, ultimately converging on the policy that maximizes cumulative rewards.\nQ-learning is well-suited for problems with small size of state and action spaces. However, in large-scale spaces, the traditional Q-learning method becomes impractical, as the Q-table can grow excessively large, making it inefficient to learn and store. To address this, Q-learning was replaced by Deep Q-Network(DQN) [9], which employs neural networks as Q value approximators to overcome the challenges of large state spaces. In addition, DQN introduced the technique of experience replay, where the agent's experiences are stored in memory and used for batch updates rather than updating immediately after each transition. This approach increases sample efficiency by allowing the agent to learn from past experiences multiple times, improving the use of available data. Although DQN has become a benchmark algorithm in reinforcement learning, it has some drawbacks, one of the main issues being its tendency to overestimate Q-values. To address this, Double Deep Q-Network (DDQN) [20] was introduced. DDQN tackles the overestimation by using two separate neural networks: one network (estimator network) is used to predict the current Q-values for action selection, while the other network (target network), which is a past version of the estimator network, is used to calculate the target Q-values for updating. By separating these two processes and using a target network that lags behind the estimator, DDQN reduces the bias in estimating the target values, leading to more accurate and stable learning. It tries to minimize the loss between target value and estimated Q value to train the agent:\n$L_{ddqn} = (Q_{target}(s, a; \\theta') - Q_{estimator} (s, a; \\theta))^2$ (1)\nwhere\n$Q_{target}(s, a; \\theta') =r+\\gamma Q_{target} (s', arg\\mathop{max} \\limits_{a'} Q_{estimator} (s', a'; \\theta); \\theta')$ (2)\n$Q_{target}$ and $Q_{estimator}$ are the target network and the estimator network, respectively."}, {"title": "C. Deep Monte Carlo and Monte Carlo Tree Search", "content": "Deep Monte Carlo (DMC) is similar to DQN; however, while DQN is based on bootstrapping, DMC relies on com-plete trajectories. In DQN, updates are made based on esti-mated future returns, whereas DMC calculates the true return from the entire episode, based on Monte Carlo sampling. Although the DMC method is known for its high variance [18], it can be effective in certain episodic games, such as the card game DouDiZhu [27]. DMC agents usually selects a random policy at the start and optimizes $\\tau$ through the following steps:\n1) Generate an episode using $\\tau$ and store the tuple $(s_t, a_t, R(s_t))$ for each step.\n2) Initialize the return $R(s)$ of each state s at time t to 0. The average return for each s is calculated using the formula $R(s_t) = R(s_t) + \\lambda R(s_{t+1})$, where $\\lambda$ is the reward discount factor.\n3) Minimize the mean squared error between $R(s_t)$ and $Q(s_t, a_t)$, where $Q(s_t, a_t)$ is predicted by the deep neural network. Repeat steps from 1 - 3 and finally, the optimal policy is generated by updating: For each s in the episode, $\\pi(s) \\leftarrow arg \\mathop{max} \\limits_{a} Q(s, a)$.\nMonte Carlo Tree Search (MCTS) and DMC are both based on Monte Carlo sampling, but they are fundamentally different methods. MCTS builds a search tree by iterating over sim-ulations of possible future moves and backpropagating the results, which makes it a key component of the renowned Go agent AlphaGo's [11] algorithm. The MCTS process in reinforcement learning includes four principal stages:\n1) Selection: Starting from the root node, repeatedly select child nodes based on a combination of predicted state values from the neural network and the Upper Confi-dence Bound for Trees (UCT) formula.\n2) Expansion: If the selected node represents a non-terminal state and has not been fully expanded, add one of its unvisited child nodes to the tree and use the neural network to predict the value of this new state.\n3) Simulation: From the newly expanded node, simulate the game by following a policy until a terminal state is reached, where the outcome of the game is determined.\n4) Backpropagation: Once the simulation is complete, propagate the result back through the nodes along the path to the root. Update the visit counts and the average value estimates at each node based on the outcome of the simulation."}, {"title": "D. Neural Fictitious Self Play", "content": "Many reinforcement learning algorithms on multi-players' games are based on Nash equilibrium [6], with Neural Fic-titious Self-Play (NFSP) [5] being a notable example. Nash equilibrium is a crucial concept in game theory, especially in non-cooperative games. It describes a situation where multiple participants select their optimal strategies, assuming that all other players will keep their strategies unchanged. In this equilibrium, no player can improve their payoff by unilaterally altering their strategy. In the mathematical form, a Nash equilibrium of a game with two players can be expressed as:\n$u_1(s) \\geq \\mathop{max} \\limits_{s_1 \\in S_1} u_1(s_1, s_2)$\n$u_2(s) \\geq \\mathop{max} \\limits_{s_2 \\in S_2} u_2(s_1, s_2)$,\nwhere $u_1, u_2$ are the payoffs (reward) of player 1 and player 2, and $s$ is the strategy. NFSP employs neural networks to approximate Nash equilibrium by responding to opponents' average strategies. It builds on game-theoretical principles by utilizing two parallel learning processes: one focuses on learning the average policy across all agents, while the other estimates the value of the policy, typically implemented through a Q-learning-based approach. NFSP is composed of two core networks: a value network, the Deep Q-Network (Q(s, a|$\\theta_Q$)), which estimates action values, and a policy net-work $\\Pi(s, a|\\theta_\\pi)$, responsible for mimicking the agent's past best responses. Both networks are supported by corresponding memory buffers: Memory for Reinforcement Learning(MRL) stores experiences for Q-learning, while Memory for Super-vised Learning(MSL) stores the best-response behaviors. The agent's learning process is a combination of data drawn from both networks. Experiences in MRL are used to train the value network by minimizing the mean squared error between predicted values and the stored experiences in the replay buffer. At the same time, data from MSL is used to train the policy network by minimizing the negative log-likelihood between the stored actions and those predicted by the policy network. NFSP has proven to be highly effective, particularly in complex imperfect-information games like Leduc Hold'em and Limit Hold'em. At the time of its development, it demonstrated near-superhuman performance in these environments, outperforming state-of-the-art algorithms and setting a new benchmark in imperfect information game strategies."}, {"title": "III. METHODOLOGY", "content": "We use RLCard [26], which is the Python game toolkit, as our Uno framework and reinforcement learning environment. One problem for our games is how to represent the game into a reinforcement learning environment. We have to define the elements of the MDP in the context of UNO:\n$\\bullet$ State (S): The state represents the current situation of a single player, which only includes information available for that player. In our state encoding of UNO, this is characterized by the player's hand and the target card. Different players would have different states as they have different hands.\n$\\bullet$ Actions (A): Actions are the legal moves that a player can make during their turn in a round of UNO.\n$\\bullet$ Transition Function (T): The transition function defines the probability distribution of legal moves given a player's current state, which can be predicted by neural networks, which is $\\pi(a|s, \\theta)$.\n$\\bullet$ Reward (R): The reward is what a player gains after completing a round. In the basic rule of UNO, a reward of +1 is granted exclusively to the winner who has no cards left, while all others receive -1. During the ongoing game, the reward following all actions is 0. But our algorithm will reshape the reward structure based on MCTS, allowing the agent to receive rewards for certain actions taken.\n$\\bullet$ Discount Factor ($\\lambda$): This is a hyperparameter signifying the degree to which future rewards are considered relative to immediate ones. For the purpose of our analysis, we have set this value at 0.99."}, {"title": "B. Improve Q value estimation in DDQN and reshape rewards with Monte Carlo Tree Search", "content": "Some former research [23] [4] has explored the idea of combinations of Q-learning and MCTS, but mostly in simple perfect information games and not applicable to imperfect information games. Our algorithm introduces a more complex MCTS variant with modifications such as a different expansion procedure and alternative backpropagation methods. We have also modify loss functions during agent training and developed an MCTS-based reward shaping structure.\nAfter expanding a new state, the simulation continues until the game ends in the traditional MCTS. We limit each simulation to only one state expansion since this is more com-putationally efficient. Unlike standard MCTS, a single agent typically plays through the entire game via self-play, with any new state being expanded, evaluated, and backpropagated. However, due to the varying information available to different players in Uno, this approach is not feasible. The player in the new state may be different, and it's impossible for the next player to pass information back to the previous one if the two players are not the same.\nIn our approach, if the simulation starts with player 1's turn and transitions to player 2's, we skip evaluating or backpropagating from all states of player 2. Player 2 continues with their own strategy until it's player 1's turn again. If the game ends, we backpropagate the results from the last state where it was player 1's turn along the path to the start state. Our algorithm ensures that all states along the MCTS simulation path belong to the player who starts the simulation. The action taken by the agent in expansion is the legal action with highest sum of Q-value and the Upper Confidence bounds applied to Trees (UCT):\n$\\pi(a|s) = arg\\mathop{max} \\limits_{a}(Q(s, a) + UCT(s, a))$, (3)\nwhere Q(s, a) is the Q-value of the legal action a under a state s, and UCT(s, a) can be defined as:\n$UCT(s, a) = C_{puct} \\sqrt{\\frac{N(s)}{1 + N(s,a)}}$,\nwhere N(s) is the total number of visits of state s, and N(s, a) is the total number of uses of legal action a at the state s, while $C_{puct}$ is a constant representing the exploration term. When a new state is discovered, if it is not an end state, the Q-value will be predicted for each legal action under that state through neural networks. Q-value is passed from the child to the parent during backpropagation and it is updated based on:\n$Q_{new}(s, a) = \\frac{Q_{old}(s, a) \\cdot N(s, a) + Q(s', a') + r(s')}{N(s, a) + 1}$, (4)\nwhere r(s') is the immediate reward after taking a to next state s', always 0 unless the game ends. If s' is the end state, Q(s', a') will be 0, r(s') will be 1. The whole procedure of our MCTS in Uno follows (Graphical representation shown in Figure 5):\n1) Selection: This process starts from the root (start state) to find a state that has not been expanded. Record the player ID of the root node, $ID_{root}$. If the current state is during player $ID_{root}$'s turn, we select the action with the highest (Q+UCT) based on Equation 3 for simulation. If it is another player's turn, that player will choose an action based on their own policy until they transition to a state belonging to player $ID_{root}$.\n2) Expansion: This step unfolds on the previously unex-panded player $ID_{root}$'s state, selecting the action with the highest (Q + UCT) value. We try to find a child, which is also a state of $ID_{root}$, of this unexpanded state. If a game-ending state occurs during another player's turn, we immediately proceed to step 3. If not, the new state of player $ID_{root}$ is found, and this state is initialized by Q-value prediction for every legal action, after which we move to step 3.\n3) Backpropagation: This step involves updating the Q-values based on Equation 4 for all states of player $ID_{root}$ along the path from the newest found state or end state to the root. Finally, repeat the above three steps for a new round of simulation.\nIn sampling of standard DDQN, When the agent is at current state s, actions are selected via $\\epsilon$-greedy: with probability $\\epsilon$, a random action is taken, and with 1 - $\\epsilon$, the action with the highest Q-value (predicted by neural networks) is chosen. After agent transitioning to the next state $s'$ and receiving re-ward r, the tuple (s, a, s', r) is stored for further training. The sampling process in our algorithm is divided into two parts: MCTS simulation and interaction with the real environment. Assume the current state s in the real environment, which is also the start state of MCTS, is simulated by MCTS. After the simulation, each legal action a under state s is assigned a corresponding Q-value, $Q_m(s,a)$, based on our MCTS rules. We then use an epsilon-greedy strategy to select the action based on $Q_m(s, a)$. Unlike Q-values Q(s, a) are derived from a single neural network in DDQN, our $Q_m(s, a)$ are obtained from repeated MCTS simulations, reducing overestimations of Q values and making them more accurate. During the simulation, whenever an end state is reached and a reward $r_k$ (-1 or 1) is obtained, we accumulate and average these rewards:\n$r_m = \\frac{\\sum_{k=1}^{n}r_k}{N_s}$ (5)\n,where n is the number of times the reward received at end state, $N_s$ is the number of simulation and $r_m$ is the total average reward from MCTS. After the agent chooses action a based on $Q_m(s,a)$, it transitions to the next state $s'$ and receives a reward r from real environment. We then add r and $r_m$ as the total reward $r_t$ after agent takes action a. In standard environment, the agent typically receives reward of 0 after taking an action during the ongoing game, but because of rules MCTS, our agent can receive proper rewards at certain points, preventing long periods without positive feedback. We then keep tuple ($Q_m(s,a), s, a, s', r_t$) as training data.\nWe also modified the loss function for training the agent, splitting it into two components:\n$L = L_{ddqn} + L_{mcts}$ (6)\nwhere $L_{ddqn}$ is the same as in Equation 1, except that the r in Equation 2 is replaced with the total reward $r_t$, and $L_{mcts}$ is:\n$L_{mcts} = (Q_m(s, a) - Q_{estimator} (s, a; \\theta))^2$\n$Q_m(s, a)$ is Q values from MCTS."}, {"title": "IV. EXPERIMENTS AND EVALUATIONS", "content": "We conducted experiments and trained our algorithm, DDQN with MCTS, alongside three traditional algo-rithms-DDQN, DMC, and NFSP-with the same number of training episodes in two Uno environments: a three-player game and a four-player game, configurations commonly played by human players. The performance of DDQN with MCTS was then compared to the three traditional algorithms. To ensure a fair comparison of the performance and learning capabilities of the four algorithms, during training of every reinforcement learning agent, there was only one correspond-ing RL agent in each environment, while the others were random-playing agents. The evaluation and comparison are based on two main metrics: Total Average Rewards and Win Rate. Rewards were given to the agents at the end of each game by the environment, with the agent receiving either -1 or 1. The total average reward was the sum of rewards accumulated across all games divided by number of games. The win rate was calculated as the number of games won by an agent divided by the total number of games played. Given the high randomness of the Uno game, the reward data exhibits significant variance; hence, we decided to mainly focus on the overall trends in the data. To reduce the effect of variance of data, after every 1,000 training episodes, the agent was tested by playing 1,000 games with agents playing cards randomly, and the total average rewards were recorded and plotted in the training graph.\nIn the evaluation, each algorithm relied solely on its esti-mator network (DMC, DDQN, DDQN with MCTS) or policy network (NFSP) to select actions. Although DDQN with MCTS used MCTS during the sampling process, for fair comparison, it only used the trained estimator network to select the action with the highest Q-value. The code was written in Python 3.10, and agents were trained on a single NVIDIA RTX 4080 GPU. All agents shared the same neural network architecture and hyperparameters, consisting of fully connected layers with sizes 240x64, 64x64, and 64x61. The batch size was set to 32, the learning rate to 0.00005, and the reward discount factor to 0.99. During the sampling process for DDQN with MCTS, each state was simulated 50 times using MCTS. A higher number of simulations could lead to more backpropagation updates and potentially more accurate Q-value predictions, but it would also increase the sampling time. We selected 50 simulations as it provides a balance between having rewards $r_m$ based on Equation 5 in some simulations and avoiding a substantial increase in sampling time.\nWe trained the four algorithms separately until their perfor-mance began to converge. DDQN with MCTS achieves the highest average total reward greater than -0.05 in the 3-player game and greater than -0.25 in the 4-player game, indicating a win rate close to 50% in the 3-player game against random-playing agents, and a win rate higher than 37.5% in the 4-player game. In general, DDQN with MCTS consistently outperformed the other algorithms in both environments. As the number of players increases, the training difficulty for the agent also grows. While other algo-rithms tend to perform progressively worse in environments with more players, DDQN with MCTS consistently maintains a stable rate of improvement and learning. More importantly, DDQN with MCTS improves very quickly in the early stages, whereas other algorithms only begin to improve slowly in the middle and later stages. We focus more on the overall trends in the data when comparing these algorithms.DDQN with MCTS achieves a total average reward that is 0.6 to 1.3 higher than the other algorithms in the 3-player game, meaning its win rate in tests against random-playing agents is 3% to 6.5% higher than that of the other algorithms. In the 4-player game, DDQN with MCTS achieves a total average reward that is 0.7 to 1.3 higher, corresponding to a win rate 3.5% to 6.5% higher than that of the other algorithms in tests against random-playing agents."}, {"title": "A. Test with Human Players and Knowledge learned by DDQN with MCTS", "content": "In order to provide human players with an opportunity to compete against our agents and to assess the performance of our algorithms in competition with human players, we have developed a fully functional graphical user interface. Our interface is implemented using the built-in Python module Tkinter [7]. We are also implementing online multiplayer functionality using the User Datagram Protocol (UDP) [10], allowing human players from different locations to remotely compete against our algorithms in 2-players, 3-players and 4-players environments. We observed DDQN with MCTS have surpassed human per-formance, achieving approximately a 54% win rate against average human players. In both the 3-player and 4-player games, the win rate of the agent is comparable to that of human players. When we tested the DDQN with MCTS algorithms against human players, we observed that these algorithms indeed learned some patterns of the game, which typically reflect strategies also exhibited by human players. For example, when the agents possess many cards along with a 'wild draw 4', they tend to play it immediately. This strategy increases the number of cards the next player holds, making it more challenging for them to win the round. When the agents are down to only two cards, and one of them is a 'wild' or 'wild draw 4', they will save these for the final play. This is because, regardless of the target card's color or number, the game rules allow a player with a wild card to play it directly, thus creating a guaranteed winning situation by holding onto this card until the end."}, {"title": "V. CONCLUSION", "content": "We introduced the complexity of imperfect information games and discussed their research value. We selected Uno, a imperfect information game, as the basis for our research. We represented Uno as a reinforcement learning problem and presented a novel algorithm, Double Deep Q-Learning with Monte Carlo Tree Search (DDQN with MCTS), to address challenges encountered in prior work with imperfect infor-mation games, such as reward sparsity and Q-value overesti-mation. Additionally, we developed a graphical user interface (GUI) to allow human players to compete against our agents, where DDQN with MCTS outperformed the average human player in terms of win rates on one-to-one competition. DDQN with MCTS also demonstrated superior performance com-pared to three traditional methods-Double Deep Q-Learning, Deep Monte Carlo, and Neural Fictitious Self-Play-achieving higher total average rewards with fewer training steps and higher win rates during testing.\nIn our subsequent work, we plan to experiment with deeper neural networks and more complex network architectures. Due to the long computation time of MCTS, we will also explore optimizing its efficiency [15]. Furthermore, since our DDQN with MCTS can be generalized and applied to any algorithm's value estimation function, we plan to extend this improvement to state-of-the-art Actor-Critic algorithms [17] [25]."}]}