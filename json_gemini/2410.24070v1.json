{"title": "DYNAMICAL SIMILARITY ANALYSIS UNIQUELY CAPTURES HOW COMPUTATIONS DEVELOP IN RNNS", "authors": ["Quentin Guilhot", "Micha\u0142 W\u00f3jcik", "Jascha Achterberg", "Rui Ponte Costa"], "abstract": "Methods for analyzing representations in neural systems have become a popular tool in both neuroscience and mechanistic interpretability. Having measures to compare how similar activations of neurons are across conditions, architectures, and species, gives us a scalable way of learning how information is transformed within different neural networks. In contrast to this trend, recent investigations have revealed how some metrics can respond to spurious signals and hence give misleading results. To identify the most reliable metric and understand how measures could be improved, it is going to be important to identify specific test cases which can serve as benchmarks. Here we propose that the phenomena of compositional learning in recurrent neural networks (RNNs) would allow us to build a test case for dynamical representation alignment metrics. By implementing this case, we show it allows us to test whether metrics can identify representations which gradually develop throughout learning and probe whether representations identified by metrics are relevant to the actual computations executed within the network. By building both an attractor- and RNN-based test case, we can show that the recently proposed Dynamical Similarity Analysis (DSA) is more noise robust and identifies behaviorally relevant representations significantly more reliably than prior metrics (Procrustes, CKA). We also show how such test cases can be used beyond evaluating metrics to study new architectures directly. Specifically, we tested DSA in modern (Mamba) state space models, where results suggest that, in contrast to RNNs, these models may not exhibit or require changes in their recurrent dynamics due to their expressive hidden state. Overall, we develop test cases that can demonstrate how DSA's increased ability to detect dynamical motifs gives it a superior ability to identify the ongoing computations in RNNs and elucidate how tasks are learned in networks.", "sections": [{"title": "INTRODUCTION", "content": "Both neuroscience and mechanistic interpretability want to understand how neural network solve the problems they face (He et al., 2024; Lindsay & Bau, 2023; Vilas et al., 2024). As the observed networks grow and the tasks to be understood become more complex, there is a need to have tools which allow to easily compare different architectures or parts of architectures across a wide array of task conditions and training parameters, at scale. One approach which has become popular is the use of representational alignment metrics which allow to conduct such comparative systems analyses on the activations observed across network (Sucholutsky et al., 2023) (Fig. 1). In mechanistic interpretability they can be used to compare how information is represented across layers (Raghu et al., 2021) and in neuroscience they allow to test under which conditions artificial neural networks work like the ventral stream of the brain (Kietzmann et al., 2019)."}, {"title": "2 RELATED WORK", "content": "A lot of progress has been made on using representational alignment metrics to both compare and induce similarities across models and empirical data (Sucholutsky et al., 2023; Williams et al., 2021). With representational metrics gaining popularity, researchers have identified the need to properly compare metrics to understand their shortcoming (Dujmovi\u0107 et al., 2023; Soni et al., 2024). Recent work on proposing ways to benchmark static representational alignment metrics tries to give us the ability to make an informed decision which metric is best to use (Klabunde et al., 2024; Ahlert et al., 2024). The goal is not necessarily to find \u201cthe ultimate best metric\u201d but instead to highlight in which context to rely on a specific metric or sets of metrics, similar specialized statistical tests.\nOur work specifically focuses on comparing dynamic representations (as opposed to static repre-sentations; see Fig. 1). While researchers have been interested in using representational alignment metrics for dynamic use cases, they usually relied on extending static metrics to also be applied to dynamic use cases (Lin et al., 2019; Williams et al., 2018; Kietzmann et al., 2019; Maheswaranathan et al., 2019; Cloos et al., 2024). With increasing interest in dynamic metrics, we have now also seen new approaches being developed to compare dynamical representations (Redman et al., 2024; Ostrow et al., 2024; Chen et al., 2024). Generally speaking, being specialized for dynamical repre-sentations means that metrics can actively capture the momentum of traces in instead of just their shapes. For example, DSA (Ostrow et al., 2024) compares the temporal dynamics of two systems by embedding them in a high-dimensional space and using a modified Procrustes analysis to assess the similarity of their vector fields. Another example is Diffeomorphic Vector Field Alignment (Chen et al., 2024) which evaluates the similarity by learning a nonlinear coordinate transformation that aligns vector fields, providing a measure of functional similarity through orbital equivalence. While new metrics are being developed and used (Eisen et al., 2024), the question of how to decide when to use which specific metric is still an open problem for the case of dynamic metrics.\nIt has been argued that representations observed in neural systems should link to and hence help understand the computations executed by networks (Barack & Krakauer, 2021; Baker et al., 2022). This is linked to the idea of population codes in neural system and the Hopfieldian view of neural processing (Barack & Krakauer, 2021; Averbeck et al., 2006). Understanding how exactly repre-sentations and different neural codes link to computations has been an active focus of theoretical and empirical work (Tye et al., 2024; Fusi et al., 2016; Johnston et al., 2020; Dapello et al., 2022; Huber et al., 2023; Zheng et al., 2024). Visualizing the dynamics of representation in Euclidean space served researchers as an important tool to understand how the attractor dynamics observed recurrent systems link to their computations (Dubreuil et al., 2022; Vyas et al., 2020; Mante et al., 2013; Sussillo, 2014; Langdon et al., 2023). Following from this, dynamical representational align-ment metrics ought to be a tool to identify and compare the attractor dynamics that link to the computations of recurrent systems (Baker et al., 2022).\nIn the following we address these two trends in the literature: we extend the idea of benchmarking representational alignment metrics to the dynamic case, and we do so in a way that allows us to test if the representations connect to the system's computations by testing whether gradually evolving representations link to gradually developing computational abilities."}, {"title": "3 METHODS", "content": "For the following analyses we use the three metrics Centered Kernel Alignment ('CKA', Kornblith et al. (2019)), Procrustes Transformation (\u2018Procrustes', Cloos et al. (2024)) and Dynamical Similar-ity Analysis ('DSA', Ostrow et al. (2024)). CKA and Procrustes both consider the dynamics as static traces and try to either align the two traces through optimal geometric transformation (Procrustes) or measure the similarity between the kernel matrices of the feature spaces (CKA). In contrast, DSA considers the traces as dynamic and projects them into high dimensional linear spaces where it com-pares the transition matrix of traces (Fig. 1). We use DSA with the hyperparameter number of delays = 33 and delay interval = 6. Appendix 6.1 explains how parameters were set."}, {"title": "3.2 RNN / SSM TRAINING AND ANALYSIS", "content": "For every condition we trained 72 RNNs, one for each combination of the following hyperparame-ters: architecture (Leaky RNN, Leaky GRU), activation function (relu, softplus, tanh), hidden size (128, 256), learning rate (1e-2, 1e-3), batch size (64, 128, 256). Hyperparameters were chosen based on Driscoll et al. (2024). One epoch of training data consisted of 10000 trials for each task. When a network was pretrained on multiple tasks, we trained the tasks sequentially within each epoch (e.g. for each epoch, first train on task A, then B). Networks were trained in a supervised fashion (with Adam optimizer) until they reached 99% accuracy on each task or for 50 epochs, depending on what was reached earlier. We only analyzed the networks that managed to fully learn the master task to 99% accuracy, which corresponded to 91% of all models.\nThe training of the Mamba SSMs (Gu & Dao, 2023) works in the same way as RNNs. For every condition we trained 64 models, one for each combination of the following hyperparameters: num-ber of layers (1, 2), hidden dimensions (8, 16), learning rate (1e-2, 5e-3, 1e-3, 5e-4), batch size (16, 32, 64, 128). We only analyzed the networks that managed to fully learn the master task, which corresponded to 99% of all models. We used the implementation by Torres-Leguet (2024)."}, {"title": "3.3 RNN TASKS", "content": "For the RNN-based test case, we use task implementations based on Neurogym (Molano-Mazon et al., 2022). Network inputs are the standard task inputs alongside an additional task identifier which identifies the four tasks used (A, B, C, M) through one-hot encoding. We use three subtasks. In task A and B (Pro and Anti Task) the model receives two continuous numbers as separate inputs, with time-varying Gaussian noise. It needs to decide which input is higher (Pro) or lower (Anti) stimulus. Task C (Delay Task) is the same as A but with an additional delay. These are combined to form a compound 'Master' task (M, DelayAnti Task), where networks need to determine which stimulus was lower, after a delay period. Each task consisted of a stimulus presentation period (200 time steps) and a choice period (25 time steps), and an optional delay period as described above. The duration of the delay is variable during training (25, 50 or 75 time steps) but fixed during testing (100 time steps). We only analyze the hidden states during the stimulus presentation period, keeping the first twenty principal component after centering and normalization. While networks are optimized for correct fixation during stimulus presentation and delay periods, as well as correct choices after the fixation, when we report accuracy, we report it based on the choice made by the network during the response period at the end of the trial, weighted towards the last time step."}, {"title": "4 EXPERIMENTS", "content": "Through the following experiments we want to test whether representational alignment metrics can capture the representations and computations of stateful artificial neural networks. We specifically compare the performance of CKA, Procrustes, and DSA. The first test case is focused on attractor dynamics usually observed to be underlying computations, followed by analyses directly linking identified dynamics with computations of trained RNNs. We close by applying metrics to analyze newly developed SSMs. The code used in this study is available at GitHub link for reproducibility and further exploration."}, {"title": "4.1 TESTING FOR 'RATIO-RESPONSE' OF METRICS TO NOISY COMBINED ATTRACTORS", "content": "In this first section, we want to test how well metrics capture attractor dynamics in the presence of noise and when multiple attractor dynamics are combined compositionally. For this we simulate attractor dynamics using Lorenz attractors ($$\\sigma = 10; \\beta = 2.667$$). This first set of simulations is close to what is usually examined when validating new dynamic representation measures but here we focus on the specific abilities which underlie the complex test case introduced later. This allows us to test for these abilities in a simpler setting where we can also make normative statements about how metrics need to behave to act as a ratio. A measure behaves as a ratio if it preserves order, has equal intervals between values, and has a meaningful zero point, allowing for meaningful ratios between any two measurements."}, {"title": "4.2 TESTING ORDINAL RESPONSE OF METRICS TO COMPOSITIONAL LEARNING SETUP", "content": "Metrics for describing representations are naturally most useful for cases where the amount of data to be analyzed is too large to allow for fully manual description, because of long time courses and model dimensionality. As such, we next want to move to a more complex test case that still allows us to make specific predictions about the normative relationships of representations across models. To construct this test case, we build on analyses of RNNs trained to learn compound tasks in a compositional fashion (Driscoll et al., 2024; Yang et al., 2019).\nIn their work, Driscoll et al. (2024) train RNNs to learn simple tasks inspired by classical neuro-science experiments. For example, the 'Delay task' (Fig. 3a) requires networks to observe two streams of continual inputs ('Stimulus Modality 1' and 'Stimulus Modality 2') while the fixation signal is on, and then choose the stimulus with the higher average value once the fixation value dis-appears. There is a delay between the end of the stimulus period and the end of the fixation signal,"}, {"title": "4.3 TESTING METRICS TO IDENTIFY TASK RELATED COMPUTATIONS ALONGSIDE TASK RELATED DYNAMICS", "content": "While our first analysis of the RNN-based test case presented a non-trivial challenge to metrics, it falls short of linking repre-sentations to computations which is discussed a key criteria of representa-tions (Baker et al., 2022). Representations link to computations if they relate to the full transformation between the inputs and the corresponding outputs (the choice made by the net-work). In contrast, metrics could capture dynamics which transform network inputs in a 'null-space' (Kaufman et al., 2014) that ultimately does not trans-late into the choice space. This can lead to networks with different computa-tions showing seemingly similar representations (Dujmovi\u0107 et al., 2023).\nWe now want to use our RNN test case to see whether metrics directly link to the computational process of the network. The simplest way to summarize the differential behavior of two networks is two calculate their difference in task accuracy. Networks with more similar task accuracy likely implement more similar computations than networks with less similar task accuracy, especially in simple tasks such as ours where there are not multiple different strategies which allow for a correct solution. Hence, we want to test whether higher similarity in representations of two networks is"}, {"title": "4.4 METRICS' RESPONSES TO INCREASING OVERLAP OF THE TRAINING SCHEDULE, ACROSS THE DURATION OF LEARNING", "content": "With the new RNN test case we were able to show that DSA shows expected ordinal re-sponses to training schedules and can link rep-resentational dynamics to computations. This tested expectations based on Driscoll et al. (2024). Using representational metrics car-ries the promise that one can precisely quantify the relationship between dynamics, instead of just relying on qualitative observation. In this section we want to demonstrate this by test-ing whether an increasing overlap in training schedule also causes an increasing alignment of representations. Additionally, we want to ob-serve how this alignment of develops over the duration of training.\nWe start with testing for the effect of a gradu-ally overlapping training schedule on represen-tations. To test this, we run a full set of pair-wise comparisons between networks, meaning between each network from one specific train-ing schedule is compare with networks of ev-ery other training schedule (Fig. 5a). For each network comparison, we quantify to which de-gree the two networks overlap in their training schedule. For example, a network pretrained on task A before being trained on compound task M, would share 50% with a network trained only on M and 66% with a network trained on A, C, and M. Fig. 5a highlight example com-parisons. As before, we only ever compare net-works with the same hyperparameters. For this analysis we only use networks that have com-pleted training. The results depicted in Fig. 5c show that DSA can recognize the difference be-"}, {"title": "4.5 USING DSA AND THE ESTABLISHED TEST CASE TO ANALYZE THE LEARNING PROCESS OF STATE SPACE MODELS", "content": "Above we showed how test cases based on well-understood empirical phenomena can be used to compare the ability of different metrics to extract computationally relevant representations across learning. Next, we want to show that such a test case can also be used in the reverse to study how architectural changes effect learning. For the case of stateful networks we have seen new architectures being released during the last year and so we want to test whether the newly introduced Mamba architecture (Gu & Dao, 2023) learns tasks in the same way that RNNs do.\nTo compare whether SSMs learn like RNNs, we apply the same test case we applied to RNNs in Fig. 3 to SSMs. As with RNNs, we use a wide set of network parameters (see Methods) and compare across training schedules (Fig. 3f) but within hyperparameters. We observe that Mamba learns quicker (2.7 times quicker than RNNs on the Master task) and models converge more reliably (99% convergence vs. 91% convergence on Master task). The results of the representational dissimilarity analyses are depicted in Fig. 6a, showing the same analysis as in RNNs (same plot as Fig. 3g). Mamba shows that the group that was trained on the Master task but with all weights frozen, except for the input weights, is the most different from representations observed in the Master network (p = 2.7 \u00d7 10-16; other p values in Appendix 6.6 Table 6). All other groups seem to roughly be in the same range of similarity to Master, though note \u2018Partial Pretraining' is also significantly more like the Master network then the 'Full Pretraining' is (p = 6.2 \u00d7 10-4). Regardless, \u2018Master & Frozen' clearly sticks out as the most different group. This means that all training groups produce roughly the same dynamics except for the group which is only allowed to learn the task through its input weights. This suggests that the very expressive Mamba architecture, when trained without freezing on a simple task such as our, does not learn by changing its hidden state dynamics but instead mostly learns through optimizing the transformation of the out-read layer (i.e. the last MLP layer). Given that the hidden state dynamics of most trained network look like the dynamics of the untrained network (p-values given in Appendix 6.6 Table 7) would lead to the prediction that the hidden state of Mamba during this simple task functions like the reservoir of a reservoir network (Luko\u0161evi\u010dius & Jaeger, 2009). As before, this is a pattern only identified by DSA (see Appendix 6.6 Fig. 9 for CKA and Procrustes). Additional control analyses show that the RNN pattern from Fig. 3g holds true even when splitting the results into the RNN subtypes (Fig. 6b and Fig. 6c)."}, {"title": "5 DISCUSSION", "content": "Representational alignment metrics have become popular for comparing representations in neural systems across architectures and conditions. While they can be a helpful and scalable tool, recent work also has identified potential pitfalls when they react to spurious signals (Dujmovi\u0107 et al., 2023; Soni et al., 2024). Here we introduce two test cases which can serve as benchmarks for dynamical representation alignment metrics. Using these to compare DSA, CKA, and Procrustes reveals that DSA is the only metric that can identify how representation emerge throughout learning and how these then link to developing task solving abilities. Building on the combination of our test case and DSA also allows us to form a specific hypothesis about reservoir-like learning in Mamba models confronted with simple tasks.\nWe put forward two specific test cases, where the simpler attractor-based test case is aimed at funda-mental skills needed for the more complex test case. These cases naturally do not fully validate DSA and hence we shy away from calling our work a full benchmark. Instead, we see this as a first step into the direction of identifying a set of well-established empirical finding which can then form a full benchmark for dynamical representation metrics. In this process, the researchers could carefully choose what they want (or do not want) metrics to capture. We believe this step of gradually collect-ing test cases will be necessary to generate a better understanding of what metrics can and cannot do. There likely is going to be a trade-off between the complexity of test cases and the precision of normative predictions that can be made about representations, similar to what we observe in our two cases. We hope that our example can spark a productive back and forth between finding test cases and improving measures that will ultimately increase researchers' confidence in the metrics.\nWe also show that combinations of test cases with (partially) validated metrics can start to become important tools for studying architectural changes. Using our RNN-based test case and DSA, we can quickly generate two new observations: In RNNs we seem to find that the development of representations in networks as a function of partial pretraining does not seem to be gradual but in-stead blocked into relatively discrete groups of not-pretrained, partially pretrained, and fully-trained. Additionally, through applying our case to Mamba we observe results which seem to suggest that Mamba learns the relatively simple tasks of our test case in a reservoir-like way, meaning that it does not strongly change its already very expressive hidden state dynamics throughout learning and instead seems to use the layers following the hidden state for task related learning. This is unless we restrain the models so that they can only learn with their input weights, which then has a measurable effect in terms of changes to hidden state dynamics. Due to the exploratory character of these anal-yses, our results still need to be confirmed with more detailed analysis. If confirmed with further analysis, this would gradually increase our trust in these metrics."}, {"title": "5.1 LIMITATIONS", "content": "While here we focus on DSA, Diffeomorphic vector field alignment (Chen et al., 2024) has been introduced since. We could test whether it also holds up to this test case like DSA. With regards to the RNN-based test case we use a broad stroke measure of 'accuracy' to capture the networks' behavior. While that is enough to differentiate between DSA and Procrustes / CKA, a more nuanced view on behavioral strategies might be needed to compare metrics in the future. We also do not use any empirical data in this work, even though comparison to data is a use case of these metrics within neuroscience. Lastly, our work does not specifically identify why Procrustes / CKA perform worse than DSA. The attractor-analyses show better noise robustness of DSA, which in turn might help DSA to identify the true dynamics in the RNN-based test case. While plausible, these are speculations and we do not give a detailed analysis of why DSA outperforms other metrics."}, {"title": "5.2 CONCLUSIONS", "content": "Our tests identify DSA as the most powerful dynamic representation alignment metric of the set of tested metrics, which suggests that researchers should prioritize it over CKA and Procrustes when studying dynamical representations. Constructing additional test cases for representational metrics will likely help to gradually improve our understanding of these metrics and can help us to generate new hypotheses about how different network architectures learn and work."}, {"title": "6 APPENDIX", "content": null}, {"title": "6.1 SETTING DSA HYPERPARAMETERS", "content": "Unlike Procrustes and CKA, DSA has hyperparameters. We use the computationally cheaper attrac-tor analysis to sample a wide space of parameters and decide on the best parameter set. We then use the same parameters for the analyses of RNNs and SSMs. For the attractor analysis the \"number of delays\" parameter is sampled uniformly from 1 to 100. The \"delay intervals\u201d parameter is sampled uniformly from 1 to the number of time steps (fixed to 200) divided by the selected number of de-lays. We manually conduct an iterative search to find the best parameters. For each parameter, we take 10 samples within each interval. We refine the interval for each parameter two times, taking the two best candidates for each parameter as bounds for the new interval. We then select the best parameter combination based on best dissimilarity gap and linear behavior in analysis shown in Fig. 2e (number of delays = 33, delay interval = 6). Generally, the dissimilarity gap and linear behavior were correlated, so that a parameter combination doing well one measure, also did well on the other. As a result we did not have to trade-off between these two criteria. Note that the attractor analysis we run purposefully uses the same number of time steps as the analysis window in later RNN anal-ysis. Generally speaking, we found that changing the DSA parameters does quantitatively but not qualitatively change results of analyses."}, {"title": "6.2 ATTRACTORS", "content": "In an additional analysis we tested how well different metrics could differentiate the different kind of Lorenz attractors (i.e. one stable fix point, two stable fix points, two unstable fix points). We sam-ple 9 examples of attractor dynamics each with 200 time-steps and 200 trials and pairwise compare each sampled attractor with every other attractor using all three metrics, generating 81 dissimilarity values per metric (Figure 7). We then summarize these by summing all values belonging to com-parisons within a group of attractors (i.e. comparison with \"one stable fix point\" to another \"one stable fix point\") and summing all values belonging to comparisons across groups of attractors (i.e. comparison with \"two unstable fix points\" to another \"one stable fix point\"). The results of this are depicted in Figure 7b. We see that all measures can recognize whether two attractors belong to the same group or not, but DSA seems to perform slightly better in discriminating cases (average dissimilarity gap between within and across groups of 0.26 for DSA compared to 0.11 for CKA and 0.13 for Procrustes as well as non-overlapping box plots for within and across for DSA)."}, {"title": "6.3 SIGNIFICANCE OF ORDER OF TRAINING SETUPS FOR RNNS", "content": null}, {"title": "6.4 REGRESSION TO IDENTIFY TASK RELATED COMPUTATIONS ALONGSIDE TASK RELATED DYNAMICS", "content": null}, {"title": "6.5 METRICS' RESPONSES TO INCREASING OVERLAP IN TRAINING SCHEDULE, ACROSS DURATION OF LEARNING", "content": null}, {"title": "6.6 SIGNIFICANCE OF ORDER OF TRAINING SETUPS FOR STATE SPACE MODELS", "content": null}]}