{"title": "Cerberus: Efficient Inference with Adaptive Parallel Decoding and Sequential Knowledge Enhancement", "authors": ["Yuxuan Liu", "Wenyuan Li", "Laizhong Cui", "Hailiang Yang"], "abstract": "Large language models (LLMs) often face a bottleneck in inference speed due to their reliance on auto-regressive decoding. Recently, parallel decoding has shown significant promise in enhancing inference efficiency. However, we have identified two key issues with existing parallel decoding frameworks: (1) decoding heads fail to balance prediction accuracy and the parallelism of execution, and (2) parallel decoding is not a universal solution, as it can bring unnecessary overheads at some challenging decoding steps. To address these issues, we propose Cerberus, an adaptive parallel decoding framework introduces the gating mechanism to enable the LLMs to adaptively choose appropriate decoding approaches at each decoding step, along with introducing a new paradigm of decoding heads that introduce the sequential knowledge while maintaining execution parallelism. The experiment results demonstrate that the Cerberus can achieve up to 2.12x speed up compared to auto-regressive decoding, and outperforms one of the leading parallel decoding frameworks, Medusa, with a 10% - 30% increase in acceleration and superior generation quality.", "sections": [{"title": "Introduction", "content": "Generative large language models (LLMs), such as the GPT-4 (Achiam et al., 2023), LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2023), have demonstrated remarkable performance across various downstream applications. Unfortunately, these LLMs are constrained by auto-regressive decoding (Vaswani et al., 2017), which limits them to generating one token per decoding step, making inference time-consuming.\nRecently, several approaches based on parallel decoding, a type of speculative decoding algorithm, have been proposed to address this issue (Cai et al., 2024; Fu et al., 2024; Li et al., 2024; He et al., 2024; Ankner et al., 2024; Zhang et al., 2024). The parallel decoding is a Draft-then-Verify decoding paradigm (Xia et al., 2024), it introduces several decoding heads aligned with the original model head to simultaneously generate multiple token candidates. These candidates are then verified using a tree attention-based process, referred to as tree verification (Chen et al., 2024; Miao et al., 2024), allowing the LLM to generate multiple tokens at each decoding step.\nHowever, we still observe two key challenges in existing parallel decoding frameworks. Firstly, existing paradigms of decoding heads fail to balance prediction accuracy with execution parallelism. Existing frameworks either use sequentially independent decoding heads or decoding heads connected in series. The former leads to low prediction accuracy, while the latter necessitates sequential execution of each decoding head, undermining parallelism. Secondly, parallel decoding is not a universal solution to the entire decoding process. Implementing parallel decoding introduces additional overheads. And since we observed that using parallel decoding at some challenging decoding steps can lead to low prediction accuracy, resulting in speeds comparable to auto-regressive decoding (Section 3.1), it can be wasteful to employ parallel decoding at these decoding steps.\nTo address these issues, we propose the Cerberus, an effective parallel decoding framework that incorporates a novel decoding heads paradigm called Cerberus Heads and a gating mechanism for selecting different decoding approaches. Our framework not only improves the prediction accuracy without compromising the execution parallelism but also reduces the overheads during the decoding process, achieving efficient inference.\nSpecifically, the Cerberus heads, as illustrated in 4b, introduces a sequential connection between the internal modules of each decoding head. This allows each head to learn information from previous positions, capture longer contexts, and make"}, {"title": "Related Work", "content": "Researchers have introduced various technologies to improve the efficiency of LLM inference, such as batch processing (Yu et al., 2022), group attention (Ainslie et al., 2023), operator fusion (Zhao et al., 2022), offloading (Sheng et al., 2023), distillation(Zhou et al., 2023), quantization (Xiao et al., 2023), and sparsification (Liu et al., 2023, Song et al., 2023).\nSimilar to our approach are frameworks based on speculative decoding (Xia et al., 2023; Chen et al., 2023; Leviathan et al., 2023; Miao et al., 2024). Speculative decoding utilizes the draft-then-verify mechanism to empower LLMs to generate multiple tokens in a single forward propagation process. Mainstream speculative decoding frameworks can be categorized into the following two types (Xia et al., 2024)."}, {"title": "Independent Drafting", "content": "Independent drafting requires two models, a target model which is the original LLM, and a draft model which is a small model with a similar structure to the target model. During inference, the draft model generates tokens over several future time steps based on the input sequence through auto-regressive decoding, and the target model determines the ultimate output through parallel verification. The SpecDec (Xia et al., 2023) introduces"}, {"title": "Self-Drafting", "content": "Self-drafting only requires fully leveraging the power of the original target model, instead of obtaining a suitable draft model for the LLM.\nThe parallel decoding framework accelerates decoding by introducing several decoding heads to the target model. Blockwise (Stern et al., 2018) and Medusa (Cai et al., 2024) introduce independent FFN heads (illustrated in Figure 4a), each accepting the hidden states from the last transformer layer of the LLM as input, predicting the top-k (Fan et al., 2018) token candidates of the backbone model in the future independently. The Hydra (Ankner et al., 2024) utilizes a serial connection between the entire decoding heads to propose a paradigm of sequentially dependent decoding heads, hence increasing the prediction accuracy of the decoding heads. The Recurrent-Drafter (Zhang et al., 2024) is similar to Hydra, but it uses a paradigm of recurrent neural network (RNN) (Mikolov and Zweig, 2012) to introduce sequential knowledge between decoding heads instead of using a simple serial connection. Moreover, to improve the training method of the multi-token prediction paradigm, researchers (Gloeckle et al., 2024) recently introduced a new framework to achieve a better and faster model. Unlike our framework which achieves parallel decoding by fine-tuning existing models, this paradigm integrates multiple decoding heads as part of the LLM and trains from scratch, demonstrating great performance on many downstream tasks.\nBesides frameworks that introduce additional decoding heads, there are still many researchers who attempt to achieve speculative decoding by only utilizing a single target model. The EAGLE (Li et al., 2024) attempts to achieve auto-regression at the feature level by training an auto-regression head within the LLM. The REST (He et al., 2024) generates draft tokens by retrieving the existing databases, instead of utilizing the draft model. The Lookahead decoding (Fu et al., 2024), Ouroboros (Zhao et al., 2024) and CLLMS (Kou et al., 2024) improve the Jacobi iteration method to accelerate decoding."}, {"title": "Parallel Decoding is Not a Universal Solution", "content": "In this section, we present two key observations that inspired the design of Cerberus based on in-depth analysis of parallel decoding. We first demonstrate that parallel decoding is unsuitable for some decoding steps in Section 3.1. Then, we illustrate that we can decide whether to implement parallel decoding by assessing the entropy of hidden states at each decoding step in Section 3.2."}, {"title": "Unnecessary Overheads During Parallel Decoding", "content": "In this section, we present our first key observation by conducting several experiments: parallel decoding is not universally efficient for the entire decoding process. We find that Parallel decoding can bring unnecessary overheads at some challenging decoding steps.\nIn parallel decoding, several decoding heads are added on top of the original LLM, and multiple token candidates are generated in parallel at each decoding step. All these token candidates are then verified by tree verification to determine the final tokens to be accepted."}, {"title": "Entropy of Last Hidden States Can Reflect the Prediction Accuracy", "content": "In the previous section, we demonstrated that parallel decoding is efficient only at decoding steps with high prediction accuracy, otherwise it will bring unnecessary overheads. In this section, we figure out how we determine whether to implement parallel decoding during each decoding step.\nIn Figure 3, we measure the relationship between the entropy of the last hidden states and the number of accepted tokens per decoding step using three different tree settings. The result shows there is a negative correlation between the entropy of the last hidden states and the number of accepted tokens. And since more accepted tokens are always accompanied by higher prediction accuracy and a higher model confidence level, we can obtain the model's confidence level by simply assessing the entropy of the last hidden states. Therefore, this entropy can be a criterion to determine whether the current decoding step is suitable for parallel decoding.\nBased on this analysis, we propose our second key observation: the entropy of the last hidden states can reflect the prediction accuracy and help assess the model's confidence level."}, {"title": "Cerberus", "content": "In this section, We will elaborate on Cerberus and how it effectively addresses the issues in existing parallel decoding frameworks. Cerberus introduces two following modifications. (a) Cerberus heads,"}, {"title": "Cerberus Heads", "content": "The existing paradigm of decoding heads can't balance prediction accuracy and parallelism of execution. In Medusa (Cai et al., 2024), the decoding heads are referred to as Medusa heads. As illustrated in Figure 4a, each Medusa head consists of a fully connected (FC) layer and multiple residual blocks (Resblocks). And each Resblock specifically consists of an FC layer and a residual connection (He et al., 2016). Given the hidden states $h_i$ for the i-th head, the computation flow of the Resblock can be defined as follows:\n$Output = \\sigma_{SiLU}((W \\cdot h_i + b) + h_i)$\nDuring inference, each Medusa head independently predicts tokens at different positions while using the same state information from the last transformer layers of LLM. Although this independence enables the heads to generate tokens in parallel, it reduces the prediction accuracy.\nTo improve the prediction accuracy, other researchers (Ankner et al., 2024; Zhang et al., 2024) introduce a serial connection between the entire decoding heads. Although this approach can help the decoding heads capture more contextual information and improve the prediction accuracy, it will lose the parallelism of the decoding heads in this case, since each head needs to wait for the previous one to complete generation.\nConsidering all these issues, we introduce the Cerberus Heads (Figure 4b). To maintain the simplicity of the architecture, we employed the same construction as Medusa heads, where each head still consists solely of Resblocks and a single FC layer.\nTo strengthen the association of each head and improve the prediction accuracy, we still attempt to introduce a sequential connection between the decoding heads. However, we choose to add sequential connections between the Resblocks within the decoding head instead of simply introducing a serial connection between all the decoding heads.\nTo achieve this, we create a special Resblock (depicted as a green square in Figure 4b) within the Cerberus head to receive two hidden states. Specifically, in the i-th decoding head, given the hidden states $h_i$ and the hidden states $h_{i-1}$ from the i-1-th head's Resblock, the computation flow of the special Resblock can be formulated as follows:\n$h = Concat (h_i, h_{i-1})$\n$Output = SiLU ((W \\cdot h + b) + h)$\nEach decoding head is now equipped to receive information not only from the last hidden states but also from the prior decoding heads, by adding this sequential connection to introduce sequential knowledge. Furthermore, since our implementation is inside the decoding heads rather than outside the entire heads, each decoding head can start decoding simultaneously to maintain the parallelism of decoding heads."}, {"title": "Entropy-based Gating Mechanism", "content": "Based on our first key observation (Section 3.1): During the entire decoding process, some simple decoding steps can be effectively solved using parallel decoding, while some challenging decoding steps that confuse the LLM are only suitable for auto-regressive decoding. Our goal is for the LLM to adaptively choose the appropriate decoding approach at each step.\nUnlike existing parallel decoding frameworks, which implement parallel decoding for the entire decoding process. Our framework Cerberus can adaptively decide whether to use auto-regressive decoding or parallel decoding based on the model's confidence level during each decoding step. Only by implementing parallel decoding at the decoding steps is the LLM confident in reducing unnecessary overheads caused by parallel decoding."}, {"title": "Experiments", "content": "In this section, we conduct several experiments to evaluate the Cerberus on various downstream tasks and make a comparison with the Medusa (Cai et al., 2024) and auto-regressive decoding."}, {"title": "Experimental Setup", "content": "Models Both the Cerberus and Medusa will be employed on the Vicuna-7B (Chiang et al., 2023), which is fine-tuned from the LLaMA (Touvron et al., 2023) model.\nDataset We evaluate the Cerberus on the MT-Bench (Zheng et al., 2024), which is a dataset used to evaluate the ability of LLMs in multi-round conversations and instruction adherence. The MT-Bench is composed of eight different types of questions, including writing (WT), roleplay (RP), extraction (ET), reasoning (RS), math (MA), coding (CD), stem (ST), and humanities (HT). For each category, researchers manually designed 10 multi-round questions."}, {"title": "End-to-End Result", "content": "Table 2 shows the detailed comparison among three decoding approaches when the number of tree paths is set to 120. Cerberus achieves the best inference speed in all downstream tasks of MT-Bench.\nWhen regarding the auto-regressive decoding as the baseline, the average acceleration obtained by Cerberus is 10% higher than that of Medusa. In the tasks of Roleplay, Cerberus's acceleration increment can even achieve 30% higher than Medusa's. Furthermore, in terms of generation quality, Cerberus also performs better than Medusa, with only a slight decrease compared to auto-regressive decoding. Overall, Cerberus shows great performance in both inference speed and generation quality.\nTo demonstrate the robustness of Cerberus's performance, we then compare the inference speed of Cerberus, Medusa, and auto-regressive decoding under three types of tree settings. As shown in Figure 6, Cerberus can achieve a 2.12x speedup compared to auto-regressive decoding, surpassing Medusa's 2.06x speedup, when aligned with the original setting of Medusa's technical report (Cai et al., 2024) (i.e., when the number of tree paths is set to 63). Moreover, Cerberus still achieves the best inference speed for the other two tree settings, which can demonstrate Cerberus's robustness in accelerating inference."}, {"title": "Ablation Study", "content": "To obtain a deeper understanding of our approach, we independently conduct a series of ablation experiments on each component of Cerberus.\nAblation on Gating Mechanism To evaluate the effectiveness of the gating mechanism separately, we compared the inference speed of Cerberus and Cerberus without the gating mechanism (Cerberus w/o gating) on MT-Bench. As shown in Table 3, Cerberus performs better than Cerberus w/o gating in all tasks of the MT-Bench. This suggests that the gating mechanism plays an important role in Cerberus.\nAblation on Decoding Heads To evaluate the performance of Cerberus heads separately, we compare the top-k prediction accuracy of Cerberus heads and Medusa heads, which can well reflect the decoding head's ability to predict tokens correctly. As shown in Figure 5, the overall performance of the Medusa heads in top-k accuracy is inferior to that of Cerberus heads. Especially for the prediction of the rear positions like the head3 and head4, the Cerberus heads can show more obvious advantages since the prediction gets harder. This indicates that Cerberus heads capture longer contexts and have better performance than the Medusa heads by introducing sequential knowledge between decoding heads."}, {"title": "Conclusion", "content": "In this paper, we propose Cerberus, an adaptive and effective parallel decoding framework that can be seamlessly integrated into existing LLMs. It incorporates two key components: (1) a novel paradigm for decoding heads that introduces sequential knowledge without compromising execution parallelism, and (2) an entropy-based gating mechanism that enables LLMs to select the most appropriate decoding approach at each step. In experiments on MT-Bench, Cerberus achieves up to a 2.12x speedup compared to auto-regressive decoding and outperforms Medusa in both acceleration"}, {"title": "Limitations", "content": "Although our work provides an efficient parallel decoding framework, there are still several limitations as follows:\n\u2022 Cerberus heads introduce sequential knowledge while also introducing more parameters, which requires more computation resources.\n\u2022 For different LLMs and tasks, different entropy thresholds may be required, hence finding a suitable entropy threshold can be challenging. However, we still rely on manual experimentation to determine the optimal entropy threshold so far. How to adaptively determine a suitable entropy threshold for different LLMs and tasks is still an issue that remains an area for further research."}]}