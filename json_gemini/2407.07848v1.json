{"title": "Uncovering Layer-Dependent Activation Sparsity Patterns in ReLU Transformers", "authors": ["Cody Wild", "Jesper Anderson"], "abstract": "Previous work has demonstrated that MLPs within ReLU Transformers exhibit high levels of sparsity, with many of their activations equal to zero for any given token. We build on that work to more deeply explore how token-level sparsity evolves over the course of training, and how it connects to broader sparsity patterns over the course of a sequence or batch, demonstrating that the different layers within small transformers exhibit distinctly layer-specific patterns on both of these fronts. In particular, we demonstrate that the first and last layer of the network have distinctive and in many ways inverted relationships to sparsity, and explore implications for the structure of feature representations being learned at different depths of the model. We additionally explore the phenomenon of ReLU dimensions \"turning off\", and show evidence suggesting that \"neuron death\" is being primarily driven by the dynamics of training, rather than simply occurring randomly or accidentally as a result of outliers.", "sections": [{"title": "1 Introduction", "content": "In deep networks, hard-zero sparsity can be a useful property-both for understanding the computations being made with more clarity, and for potentially making those computations faster than they could be otherwise [5]. This work focuses specifically on the activation dynamics of the two-layer MLPs in a standard Transformer block [9]. While Li et al. [2] demonstrated that trained networks using ReLU nonlinearities exhibit sparsity on a per-token basis, we believe this is the first work to examine in depth how sparsity evolves over training, both on a per-token level and between tokens in a sequence or batch."}, {"title": "2 Methodology", "content": "While attention mechanisms are the feature that differentiate Transformers from other sequence models, the Multi-Layer Perceptrons (MLPs) within a Transformer block seem to be key to how they learn and store information. MLPs contain the majority of parameters within most modern language models, and it is believed that they are central to how models store factual information[4]. Given this, we believe that rigorous work to better understand the internal machinery of Transformer MLPs is of potentially high value to our understanding of this important class of models.\nPrevious work demonstrated that MLPs that use ReLUs tend to be sparse in their activations - that is, for any given token vector that makes a forward pass through a MLP, only a small fraction of the available hidden units will have values greater than 0 after a ReLU activation is applied [2]. This work additionally focuses on sequence and batch measures of sparsity. We believe these metrics are important both for understanding the degree to which sparsity can or can't be computationally exploited, and also for understanding what sparsity might be able to tell us about features being learned.\nTo investigate this, we define and calculate three core metrics of sparsity - or, inversely, of \"hidden unit use\". (We decided that focusing on the positive measure was more intuitive in this setting, but this is fundamentally just sparsity from a different perspective). These metrics are calculated on the hidden dimension activations between the two dense layers that make up a Transformer MLP, and all of them are calculated independently for each layer in the network.\n1. Per-Token Hidden Unit Use - The number of hidden units that are nonzero post-ReLU, calculated per token, and then averaged over sequence and batch.\n2. Per-Sequence Hidden Unit Use - The number of hidden units that are nonzero post-ReLU anywhere in a sequence. This is fundamentally an \"or\" over all token in a sequence - if a hidden unit is used in even one token, it is included in this count. Experiments in this paper are done with a standard sequence length of 512.\n3. Per-Batch Hidden Unit Use - The number of hidden units that are nonzero post-ReLU anywhere in a batch. This is similar to Sequence Hidden Unit Use in that a hidden unit dimension is counted here if any token in any sequence in the batch has a value greater than"}, {"title": "3 Results", "content": "Our most salient top-level results, as shown in Table 1 are that:\n1. Different layers in a network use very different numbers of hidden units over the course of a batch, and this is not a trivial extension of the per-token value. In fact, there is a moderate (-0.15) negative correlation between token and batch hidden unit use.\n2. Specifically, the first layer in the network (layer 0) consistently uses the fewest of its available hidden units in a batch (only about 13.3%) and the final layer in the network (in this case layer 5) uses the most (95.6%)."}, {"title": "3.1 Sparsity at Initialization", "content": "To understand how sparsity develops during training, we need to first understand the sparsity characteristics of an untrained model. In Table 2, we can see that, at initialization, higher layers of the network use monotonically fewer of their dimensions at in the first training batch. Given that weight initialization is shared across layers, this suggests that the transformations being applied even by a random network shift the inputs to each successive MLP in a way that induces sparsity.\nImportantly, we can see that a hidden unit being zero in the first batch of training does not mean that it never has the possibility to shift outside of that regime - in the \"Turned On\" column of 2 we see that all layers turn on some hidden units that began training unused. The tendency for layers to \"turn on\" hidden units is monotonic in the opposite direction, with higher layers turning on more hidden units. This could be indicative of the model learning features at higher layers that benefit from a larger number of hidden units, but could also mean that, while all layers would benefit from turning on hidden units, later layers in the network are more able to do so because of the distribution shift happening underneath them in the earlier layers."}, {"title": "3.2 How Sparsity Evolves Over the Course of Training", "content": "Beyond converging to different levels of sparsity, there seem to be meaningful patterns of when in training each layer becomes sparse. Figure 2 shows two metrics early in training: the fraction"}, {"title": "4 Discussion", "content": ""}, {"title": "4.1 \"Dead Neurons\" and Unused Hidden Units", "content": ""}, {"title": "4.1.1 When Can We Declare a Neuron Dead?", "content": "If a hidden unit (or, \"neuron\", to echo historical literature, is never nonzero over the 8192 tokens in a batch, is it \"dead\", in the conventional sense of that term? This is an interesting question because the term has historically encompassed two somewhat vaguely-defined properties: that it takes a value of 0 everywhere (or at least everywhere empirically sampled), and thus doesn't provide information to the model, and that it doesn't turn back on because it is on the wrong side of the ReLU.\nBased on the results in this paper, we believe we can usefully disentangle these concepts, and argue that, while is is possible for hidden unit that are empirically dead (i.e. never used in a large batch) to turn back on, there is also a point early in training where, if a hidden unit isn't being used, it has effectively been dropped by the model, in the sense that further training will not use it.\nWe believe this on the strength of a test wherein, at 50,000 steps into training (out of a full training time of 1M steps), we calculate a mask of the hidden units by the current batch. This point was chosen due to it seeming in 1 like a point at which all layers had stabilized how many hidden units they used per batch. We then apply that mask as a zero-one masking on all future training steps, such that any hidden unit not active in the batch where the mask was calculated is constrained to be zero on future steps. We observed no statistical difference in the accuracy achieved over the course of training between runs that did or did not have this masking applied.\nConcretely, by this point in training naturally turned off about 30% (calculated in aggregate over all layers) of its hidden units, and that we were able to perform the final 90% of a training run with all of those hidden units masked, once the model had turned them off within a batch. As a check, we attempted to randomly select the same fraction of hidden units, and found performance collapse. This result suggests that, if you have independent reason to be training a ReLU model, it might be possible to exploit this emergent batch-level sparsity by reshaping your model to exclude unused dimensions from the latter 90% of training. We note that it may be possible to find ways to reduce sparsity and regain the value of that capacity, but we leave that to future work.\nReturning to the question at the beginning of this section: a hidden unit being unused over a batch during the very early stages of training shouldn't mark it as definitively dead, but by slightly later in training it is strong evidence that hidden unit could be removed without cost."}, {"title": "4.2 How do Neurons Die?", "content": "The accepted understanding of how neurons become \u201cdead\u201d, or drop out of active model use, is that the behavior emerges randomly or accidentally, due to outlying batches that contribute a highly negative gradient impulse and knock the dimension into a regime where it stops activating. We believe like these results add complexity to and fundamentally challenge that mental model."}, {"title": "4.3 Feature Specificity", "content": "We hypothesize that some of the sparsity behavior observed here can be explained by something we here call Feature Specificity. To explain the concept, imagine two ways features could be constructed. These are both extremes, and we don't expect either represents the internal behavior of real models, but we find them helpful models to illustrate a spectrum of possibilities.\n1. Continuous Shared Subspace In this paradigm, all tokens have informative (nonzero) values along all dimensions, and information is communicated through the continuous differences in the values along each dimension.\n2. Binary Representation In this paradigm, there are many features, features take on binary values, and the information content of a token is represented by the set of hidden units which are on. In this world, you would expect the number of dimensions that are on (or 1) to differ dramatically from token to token, and would expect most dimensions to only be used in a small fraction of tokens, because if all dimensions were switched on all the time, there would be very little information to differentiate tokens. Intuitively, the lower information content of a 0/1 signal compared to a fully continuous one means that the full set of dimensions available to switch on or off would need to be larger in this paradigm to capture a similar amount of information.\nExamining tables 1 and 3, as you rise higher in the network, features appear to shift from being closer to the continuous model to being closer to the binary model. Layer 0 is the most extreme case - it converges to active use of a substantially smaller set of hidden units (1), but without a commensurate drop in number of hidden units used per token. We can also see in Table 3 that hidden units in layer 0 are used much more frequently over a sequence: the 65th percentile hidden unit activates for at least 31% of tokens in a sequence. The next highest layer on this metric, layer 2, activates for only of only 3.2% of tokens, nearly 10x smaller.\nOn the other extreme, we see layers 4 and 5 behaving in ways that more closely correspond with the binary feature model. They have the highest number of hidden units in active use per-batch (28.2K and 31.3K respectively), but use comparatively few for each token token. Looking at 3, we see that the hidden units in these final two layers activate only very infrequently in the course of a sequence; the 90th percentile hidden units only activate 3.7% and 6.9% of tokens in a sequence respectively. To make this more concrete, for layer 4, out of dimensions that are ever used in a batch, 90% of those dimensions only have nonzero values for 20 tokens or fewer out of a 512-length sequence."}, {"title": "4.4 How Does Sparsity Relate to Model-Usable Capacity?", "content": "On a very simplistic level, all of this batch-level sparsity seems like a waste: capacity that the model could be making productive use of that it appears to be ignoring. In one framing, the number of dimensions the model converges to for each layer could be a sort of revealed preference of capacity - perhaps for this width or depth or dataset, the capacity needs of some layers are bottlenecked by others."}, {"title": "5 Sensitivity to Hyperparameters", "content": "Thus far we have focused single case study model to allow for more in-depth examination. Here we seek to understand how these sparsity patterns generalize across different hyperparameter settings. For all experiments discussed here, plots showing behavior over the course of training can be found in the appendix."}, {"title": "5.1 Sensitivity to Learning Rate", "content": "Our optimizer is structured to use a warm-up period of 5000 steps, up to a peak learning rate, and then cosine decay thereafter. Table 5 examines how the converged Per-Batch Hidden Unit Use of a 6-layer model varies based on peak learning rate. Note that the standard experimental setting of 3e-3 has the highest performance, since it was selected via hyperparameter search. Some things are clearly consistent over these different learning rates: in all cases layer 0 substantially fewer dimensions over the course of a batch compared to any other layer, and the two highest layers of the network still use"}, {"title": "5.2 Sensitivity to Hidden Dimension", "content": "In [2] the authors showed that wider models had generally higher levels of sparsity, and we see see the same directional result here in 3, both for hidden units used per token (the measurement closest to the one used in that paper) and for hidden units used per batch. This would be consistent with the idea that there is a minumum necessary absolute number of hidden units for good performance (and thus a higher fraction of a narrower model), but that after that the marginal hidden unit declines in value."}, {"title": "5.3 Sensitivity to Model Depth", "content": "To explore the sensitivity of our observations to model depth, we trained a 6, 10, and 14 layer model with a hidden dimension of 16384 (so that all models could fit on available hardware) In 4 we can see that, when it comes to depth rather than width, we do not get the same directional result as above: there isn't a clear trend in hidden units used per token, but hidden units per batch tend to increase in a deeper model. One explanation that could be at play here is that as we deepen the network we create new layers that are more similar in their behavior to the later layers of a six layer model. That does seem broadly supported by the results here, though we seem to also see some cases (layer 6, in the 14 layer model) which more closely follow the pattern of layer 0 in using a very small fraction of its hidden units per batch, suggesting that as models get deeper there may be value in additional instances of multiple of the layer archetypes so far discussed, not just those of the later layers."}, {"title": "6 Limitations", "content": "The primary limitations of this work come from the fact that we chose to focus exclusively on small models and ReLU activation functions, both of which pull these conclusions further away from immediate applicability to modern foundation models. The former choice was constrained by the amount of metric iteration and the number of ablations this work entailed, which would have been infeasible to justify with large models. The latter was motivated by the simple fact that ReLUs induce the kind of hard sparsity that is easier to study. Based on the results in Mirzadeh et al. [5], where the authors are able to sparsify models not trained with ReLU, we are hopeful that there are underlying similarities in the features being learned between these model types, but which may be easier to bring into focus in a fully sparse setting. We hope that future work along these lines can identify proposed changes to initialization functions or other aspects of model structure that will allow for more effective use of capacity across both ReLU and more competitive activation functions."}, {"title": "7 Related Work", "content": "Examination of Empirical Sparsity Li et al. [2] empirically examines the layer-by-layer token sparsity dynamics of trained encoder-decoder models, focusing on T5. On a high level, we see low-single-digit fraction of hidden dimensions used per token, which is consistent with their findings, though the fact of choosing to focus on decoder-only transformers and smaller networks means we wouldn't necessarily expect to be able to replicate their work directly. Intrigued by the conclusions of their work, we expand to look not only at the sparsity properties of a fully trained model, but the way sparsity evolves over training. We also widen the scope of this previous work by taking an integrated view of sparsity - both at the token level, and across sequences and batches.\nVoita et al. [10] also explores layer-dependent sparsity, and finds that many neurons \"die\", in the sense of never being used over a broad set of data, predominantly in early layers, but only look at this behavior at the end of training, while we explore the evolution of sparsity over the course of training.\nLu [3] similarly finds that some hidden units consistently have a value of zero post-RELU at the beginning of training, and suggest improvements to initialization to address this. However, their derivations on a simple feed-forward network appear to make assumptions about the inputs to each successive layer that it is unclear would apply to the structure of Transformer blocks with residual connections in between.\nWaleffe and Rekatsinas [11] argues through use of PCA that, in the convolutional image models they study, activations only actually use a subspace of smaller rank than the full available hidden dim, and that this can be used to motivate parameter reductions early in training. This is consistent with our findings in language transformers that many activation dimensions are almost never used from early on in training.\nInduced Sparsity Outside of this, the largest cluster of similar work attempts to induce usable, structured sparsity beyond the amount that natively results from training. Jaszczur et al. [1] argues that it's possible to both sparsify attention and to convert dense MLPs to use a top K operation, with minimal cost to performance. Runwal et al. [7] designs a fine-tuning loss that disincentivizes dense use of dimensions, though that work focuses on encoder-decoder models rather than decoder-only. Szatkowski et al. [8] and Zheng et al. [12] operate in the setting of \"MoEification\" where a normally trained model is converted to function as a MoE, and each proposes different procedures for inducing activation sparsity in the underlying model to improve the results of this process.\nRecent work by Mirzadeh et al. [5] shows that you can take a LLM trained with GELU or SiLU activation - which have no bias towards sparsity - and convert it to use ReLU activation with minimal performance loss through a short period of fine-tuning. Once the model is converted to ReLU, the authors find high degrees of activation sparsity. This is compelling because it suggests a degree of \"latent sparsity\" even in models trained with smooth activation functions, and, in our view, makes it more likely that the sparsity dynamics we find here may also correspond to similar dynamics in smoothly-activated networks, but whose effects are easier to legibly study in the ReLU context."}, {"title": "8 Conclusion", "content": "This work seeks to expand the default understanding of sparsity in transformers, from being understood as a generally shared and roughly uniform property across layers, to one that varies in its characteristics at different depths of a network in ways that strongly appear to be systematic. We show evidence demonstrating that the first and last layers of a network are effective inverses of one another in terms of the sparsity behaviors they exhibit, and suggest that this may be due to underlying differences in the structure of features learned at different layers, with layer 0 learning features more densely distributed across a smaller vector space, and later layers, especially the final layer, learning features that are closer to binary over a larger vector space. On a less scientific and more pragmatic front, we demonstrate that, if you have independent reason to be training a ReLU model, you might be able to prune 30+% of the model's hidden dimension neurons for most of training without meaningful cost to accuracy, and with a potentially large savings on compute."}, {"title": "A Appendix", "content": "The appendices are organized as follows:\n1. The first two sections focus on providing expanded or additional context to the core experimental setting. A.1 showing full training curve plots for cases where only the beginning of training was shown in the main paper. The main body of the paper shows aggregated metrics for measures of how frequently hidden units are used within a sequence, and shows a plot for the median (50th percentile). A.2 expands on this, and shows plots for the 65th, 75th, and 90th percentile metrics over the course of training.\n2. The final three sections focus on providing training curve plots for the hyperparameter ablations discussed in section 5, where only aggregations were shown in the full paper.\n3. The final section in the appendix, A.6, gives pseudocode definitions for the core metrics discussed and used in the paper."}, {"title": "A.1 Full Training Plots for Core Metrics", "content": "These plots show the full behavior over training of metrics for which a truncated training plot was shown in the full paper. The goal here is to demonstrate that interesting convergence behavior appears to be confined to the early stages of training, and justify the choice to focus on those in the main body of the paper."}, {"title": "A.2 Higher Percentile Plots of Hidden Dimension Use Frequency", "content": ""}, {"title": "A.3 Sensitivity to Depth Plots", "content": "This section shows token, sequence, and batch hidden unit use for models of 6, 10, and 14 layers, all of which use a hidden unit size of 16384 so that even 14 layer can fit on available hardware."}, {"title": "A.3.1 6 Layer", "content": ""}, {"title": "A.3.2 10 Layer", "content": ""}, {"title": "A.3.3 14 Layer", "content": ""}, {"title": "A.4 Sensitivity to Hidden Dim Size Plots", "content": "This section shows token, sequence, and batch hidden unit use for 6 layer models of hidden dimensions 16384 and 8192 (32768 is the default for the main paper)."}, {"title": "A.4.1 Hidden Dimension: 16384", "content": ""}, {"title": "A.4.2 Hidden Dimension: 8192", "content": ""}, {"title": "A.5 Sensitivity to Learning Rate Plots", "content": "This section shows token, sequence, and batch hidden unit use for models using a max learning rate of 1e-3 and 2e-3 (3e-3 is the standard used in the main paper)"}, {"title": "A.5.1 Max LR: 1e-3", "content": ""}, {"title": "A.5.2 Max LR: 2e-3", "content": ""}, {"title": "A.6 Metrics Definition Pseudocode", "content": "In the code samples that follow, 'batch' is an array of hidden state activations of size (batch, sequence, hidden), and 'seq' is an array of hidden state activations of size (sequence, hidden). When a metric over sequences is shown, it is calculated by mapping a sequence-level function over the sequences in a batch, and then averaging."}, {"title": "A.6.1 Percentile Metrics", "content": "def get_rescaled_percentile(fract_nonzero: array, desired_percentile: int\n):\nfract_zero = 1 - fract_nonzero\nrescaled_percentile = desired_percentile * fract_nonzero + 100 * fract_zero\nreturn rescaled_percentile\n\ndef percentile_used_dimension_count(seq: array, percentile: int\n) -> array:\nseq_dim = seq.shape[0]\nuse_count_by_dimension = sum(seq, axis=0)\nfrac_dimensions_gt_0 = (\nsum(use_count_by_dimension > 0) / use_count_by_dimension.shape[0]\n)\nnew_percentile = get_rescaled_percentile(frac_dimensions_gt_0, percentile)\nreturn percentile(use_count_by_dimension, new_percentile) / seq_dim"}, {"title": "A.6.2 Hidden Unit Use Metrics", "content": "def sequence_total_dimensions_used(seq: array) -> array:\nsummed_activations_over_sequence = sum(seq, axis=0)\nreturn sum((summed_activations_over_sequence > 0).astype(int32))\n\ndef flatten(batch: array) -> array:\nreturn batch.reshape(batch.shape[0] * batch.shape[1], batch.shape[2])\n\ndef total_batch_dimensions_used(batch: array) -> array:\nreturn sequence_total_dimensions_used(flatten (batch))"}]}