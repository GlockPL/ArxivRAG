{"title": "Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning", "authors": ["Lang Xu", "Quentin Anthony", "Jacob Hatef", "Aamir Shafi", "Hari Subramoni", "Dhabaleswar K. (DK) Panda"], "abstract": "Scaling up Large Language Model(LLM) training involves fitting a tremendous amount of training parameters across a limited number of workers. However, methods like ZeRO-3 that drastically reduce GPU memory pressure often incur heavy communication to ensure global synchronization and consistency. Established efforts such as ZeRO++ use secondary partitions to avoid inter-node communications, given that intra-node GPU-GPU transfer generally has more bandwidth and lower latency than inter-node connections. However, as more capable infrastructure like Frontier, equipped with AMD GPUs, emerged with impressive computing capability, there is a need for investigations on the hardware topology and to develop targeted strategies to improve training efficiency. In this work, we propose a collection of communication and optimization strategies for ZeRO++ to reduce communication costs and improve memory utilization. In this paper, we propose a 3-level hierarchical partitioning specifically for the current Top-1 supercomputing cluster, Frontier, which aims at leveraging various bandwidths across layers of communications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication overhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU when compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for up to 384 GCDs. To the best of our knowledge, our work is also the first effort to efficiently optimize LLM workloads on Frontier AMD GPUs.", "sections": [{"title": "II. INTRODUCTION", "content": "Large Language Models (LLM) have been proven to possess incredible capability in various downstream tasks. Recent models like Claude 3 [1], Gemma [2] and Llama 3 [3] have staged various impressive results in Coding [4], Math [5] and world knowledge [6]. However, these models typically contain billions of parameters, following well-known scaling laws [7] that indicate a strong correlation between model scale and its performance. As more and more billion-parameter models burgeon, there is a growing need to conduct large-scale, efficient training over hundreds and thousands of GPUs to address the high computational demands.\nHigh-Performance Computing (HPC) systems are designed and engineered to support sizeable scientific research and deep learning workloads. These HPC systems typically consist of thousands of nodes equipped with two to four advanced GPUs that maximize floating point operations per second (FLOPS), making them ideal for large-scale data-intensive distributed pre-training of LLMs. Inter- and Intra-node communication play a significant role in accelerating parallel applications. Distributed communication backends usually feature NCCL/RCCL for NVIDIA/AMD GPUs and also GPU-aware MPI libraries [8], [9] that leverage GPUDirect technology to accelerate GPU data transfer. Large-scale supercomputing clusters also feature various inter-node and intra-node interconnect combinations, depending on different GPU vendors. A typical DGX node for NVIDIA GPUs consists of several accelerators connected using NVLink. Mellanox InfiniBand (IB) ports are often used to establish inter-node connections. Communication routines are often provided by NCCL [10]. Giant model training usually adopts such a training stack given the high-speed intra-node and inter-node bandwidth.\nOne primary problem that needs to be tackled is to fit models onto limited GPU memory. The traditional data parallel approach is insufficient in this scenario since one GPU cannot fit an entire model replica. For example, LlaMa-7B requires 112GB of model states, which exceeds the memory capacity of an NVIDIA A100-80GB GPU [11]. DeepSpeed ZeRO optimizer [12] solved this by performing a sharding strategy on training parameters in 3 stages, namely ZeRO-1, ZeRO-2, and ZeRO-3. A full ZeRO-3 will distribute optimizer states, gradients, and model parameters across all the processes and"}, {"title": "A. Motivation", "content": "Recently, we have also witnessed growing attention paid to the AMD compute stack. For example, Frontier [16], the Top-1 supercomputing cluster, is equipped with compute nodes that have four MI250X GPUs, connected using Infinity Fabric within a node and Slingshot 11 [17] across nodes. Communications are conducted through RCCL. System topology is detailed in Section IV. Given the low-bandwidth configuration compared to DGX systems, LLM training on such platforms has been an under-studied area, which leaves room for investigation and improvements.\nZeRO-3 is an ideal choice for enabling billion-parameter model training. However, this method requires frequent All-gather and Reduce-scatter operations to aggregate training parameters onto a process and then re-distribute them after each training step. Such a procedure hampers the overall training compute-communication ratio, especially when hundreds of processes are spread across multiple nodes, and the nodes are equipped with rather low-bandwidth interconnects. Figure 1 illustrates ZeRO-3 across two Frontier compute nodes. Note that forward & backward model parameter Allgather and backward gradient Reduce-scatter are conducted across node boundaries and are among all workers, which is detrimental to training throughput."}, {"title": "B. Problem Statement", "content": "The challenge lies in the following: Although ZeRO++ eliminates the inter-node parameter Allgather, Reduce-scatter on gradients has yet to be considered. ZeRO++ only covers a portion of the communication routines in ZeRO-3, leaving significant room for improvement. 1) How can we apply communication optimization to more collectives in ZeRO-3? Secondly, one Frontier compute node has different bandwidths between GCDs, GPUs, and nodes. 2) How can we efficiently utilize their topology to design effective communication strategies? Additionally, ZeRO++ trades memory for reduced communication by adding secondary partitions within a node, but this leads to a reduction in the maximum model size compared to naive ZeRO-3. 3) What additional memory optimization techniques can be adopted to address such a trade-off?"}, {"title": "C. Proposed Solution", "content": "To address the problems raised, we propose a comprehen- sive solution that tackles communication, topology utilization, and memory optimization. We ported ZeRO++ to AMD GPUs and adopted its quantization-assisted techniques to all col- lective communications, including reduce-scatter operations for gradients. Our approach carefully leverages Frontier's three-level topology (GCD, MI250X, Node) by constraining model weight allgather operations between two GCDs and gradient reduce-scatter within a node. This design stems from a thorough analysis of Frontier's architecture, comparing its GPU-GPU, intra-node, and inter-node bandwidths with those of the DGX-A100 system to determine the most effective com- munication strategy. To mitigate the memory-communication trade-off introduced by ZeRO++'s secondary partitions, we adopted block-based quantization for these partitions, reducing memory pressure on each GCD. This multi-faceted approach optimizes bandwidth utilization, scales operations efficiently, and balances memory constraints, thereby addressing the key challenges in adapting ZeRO++ for Frontier's unique architec- ture."}, {"title": "D. Contributions", "content": "1) We conducted a comprehensive analysis of the hard- ware configurations from various vendors to assess their impact on Large Language Model (LLM) training. Specifically, we examined the system specifications of DGX and Frontier compute nodes, focusing on the"}, {"title": "III. BACKGROUND", "content": "Data Parallelism [19] is a distributed deep learning tech- nique that allocates training data across multiple GPUs, each hosting a replica of the model for parallel training. In this approach, each GPU processes a distinct subset of the dataset, performing a forward pass to compute the loss and a back- ward pass to calculate local gradients. After the backward pass, a global synchronization step ensues, wherein all local gradients are collected, averaged to produce global gradients, and redistributed to each GPU. This ensures that every worker updates their weights using the same gradients. This synchro- nization is typically performed using an Allreduce operation. The efficiency of Data Parallelism largely hinges on this communication step, which is synchronous and constrained by bandwidth as the scale increases. While Data Parallelism can significantly enhance throughput compared to single- node training, as evidenced by various applications [20], it encounters limitations with large, dense neural networks due to memory constraints."}, {"title": "B. ZERO", "content": "The Zero Redundancy Optimizer (ZeRO) [12] addresses the memory complexity inherent in data parallelism, enabling the training of larger models on smaller hardware without approximation. ZeRO is divided into three stages, each with distinct memory and communication complexities, partitioning different aspects of model training. These stages handle the model parameters, gradients, and optimizer states. Typically, model parameters and gradients are stored as FP16 or BF16 elements, while optimizer states include the master parameters in FP32 and the optimizer states (such as the moments and variances in Adam [21]) in FP16 or BF16. Consequently, during data parallel training, the memory requirements can be approximated as $4\\Psi + K\\Psi$ bytes, where $\\Psi$ represents the model size in parameters and K denotes the optimizer state partition size in bytes. For the Adam optimizer, K = 12.\nZeRO-1 partitions the optimizer states of the model across data parallel ranks, reducing the memory footprint while main- taining the same communication volume as standard data par- allelism. Specifically, ZeRO-1 decreases the memory required for the optimizer state partition on each rank to $\\frac{K\\Psi}{N}$, where $N$ is the number of data-parallel ranks. Each data parallel rank is responsible for updating of the optimizer states. After the optimizer step, an Allgather operation synchronizes and replaces the model parameters with the updated values. Consequently, ZeRO-1 training reduces the total memory requirement to $4\\Psi + \\frac{K\\Psi}{N}$, while the overall communication volume remains unchanged. This optimization allows for more efficient utilization of memory resources, enabling the training of larger models without increasing communication overhead.\nZeRO-2 expands upon ZeRO-1 by partitioning both the gradients and optimizer states. In this approach, each data parallel rank computes only $\\frac{1}{N}$ of the gradients and up- dates the corresponding $\\frac{1}{N}$ partition of optimizer states. This further reduces the memory requirements compared to ZeRO- 1, bringing it down to $2\\Psi + 2\\frac{\\Psi}{N} + \\frac{K\\Psi}{N}$, without impacting the communication volume. This enhanced partitioning strategy significantly optimizes memory usage, enabling the training of even larger models on the same hardware while maintaining efficient communication.\nZeRO-3 further enhances the capabilities of ZeRO-2 by partitioning the model parameters in addition to gradients and optimizer states. When the model parameters are needed for forward or backward passes, an Allgather operation is performed. This results in a memory requirement of $\\frac{4\\Psi}{N} + \\frac{K\\Psi}{N}$, but increases the communication volume of data parallel training by 1.5 times. While ZeRO-3 offers speed and memory reductions on some systems, it significantly increases com- munication volume, making it inefficient on low-bandwidth systems or during training with small batch sizes."}, {"title": "C. ZeRO++", "content": "To address the inefficiencies of ZeRO-3, [18] introduced ZeRO++, an enhanced version of ZeRO-3 that leverages communication compression and hierarchical communication to improve efficiency on low-bandwidth systems. The com- munication optimizations in ZeRO++ include a quantization- assisted Allgather for the forward pass, hierarchical weight partitioning for the backward pass, and quantized gradient Reduce-scatter for the weight update. These three optimiza- tions collectively reduce the inter-node communication volume from 3M to 0.75M, where M represents the model size in bytes. ZeRO++ utilizes block-based quantization [22], which quantizes blocks of FP16 data into INT8 or INT4 blocks, significantly enhancing communication efficiency.\nBefore the forward pass in ZeRO-3, the model weights must be gathered on each rank through an Allgather operation, which involves a communication volume of M. ZeRO++ optimizes this process by quantizing the tensors before com- munication, thereby reducing the volume from M to 0.5M. After the forward Allgather, ZeRO++ retains a copy of the model weights within each node's GPU memory, referred to as the secondary partition. When the model weights are required for the backward pass, the backward Allgather operation involves only intra-node communication on the secondary partition, effectively reducing the inter-node communication volume from M to 0. However, maintaining these secondary partitions increases memory pressure within a node, potentially limiting both batch size and model size.\nThe final communication optimization in ZeRO++ is the quantized All-to-All-based Reduce-Scatter for gradient com- munication. In this process, ZeRO++ quantizes the FP16 gradients to INT4 tensors, significantly reducing the com- munication volume. To minimize the accumulated error from repeated quantization and dequantization, ZeRO++ introduces a novel All-to-All-based Reduce-scatter technique. This ap- proach effectively reduces the communication volume from M to 0.25M."}, {"title": "IV. SYSTEM ARCHITECTURE ANALYSIS", "content": "This section presents a comprehensive system architecture analysis of popular large-scale HPC clusters used for train- ing large language models (LLMs). Our comparison focuses on clusters built around different GPU vendors, primarily NVIDIA and AMD. Additionally, we analyze the compute node topology of the current TOP-1 Frontier supercomputing cluster hosted by Oak Ridge National Laboratory. We begin by examining a DGX-A100 node designed for NVIDIA GPUs.\nA widely-used DGX-A100 node (Table I) typically hosts 8 A100 GPUs, which can have either 80GB or 40GB of memory, depending on user requirements. The node con- figuration also includes dual AMD EPYC 7742 processors, providing 128 cores, each with a base clock speed of 2.25GHz and a maximum boost of 3.4GHz. Intra-node connections feature third-generation NVLink between each pair of A100s, achieving up to 600GB/s GPU-to-GPU bandwidth. For inter- node connections, the node is equipped with 8 Mellanox InfiniBand (IB) HDR ports, delivering a total of 200GB/s bandwidth. Please see Figure 2 for details. Apart from DGX- A100, clusters hosted by universities and national laboratories, such as the NCSA Delta at UIUC, combine NVIDIA GPUs with other interconnect vendors like Slingshot. Additionally, variations exist in the intra-node connections between A100 GPUs (SXM versus PCIe), depending on resource constraints and system usage. DGX systems are among the most popular choices for large-scale LLM training due to their superior intra-node and inter-node bandwidth provided by NVLink and the numerous IB ports. This high bandwidth is crucial for low- latency communication across GPUs. It is noteworthy that in this system, inter-node connections are approximately three times slower than intra-node connections.\nNext, we examine Frontier, the current top-ranked HPE Cray EX supercomputing cluster hosted by Oak Ridge National Laboratory. Each compute node in Frontier contains 4 AMD MI250X GPUs, each equipped with 2 Graphic Computing Dies (GCDs) and 128GB of HBM memory with a bandwidth of 1.6TB/s. Within each MI250X are four Infinity Fabric links, providing a total GCD-to-GCD bandwidth of 200GB/s. Each pair of MI250X GPUs is connected by two Infinity Fabric links (100GB/s) for adjacent pairs and one link (50GB/s) for cross pairs. Inter-node connections are established using 4 HPE Slingshot ports, delivering a total bandwidth of 100GB/s. When comparing a DGX-A100 node to a Frontier node, as demonstrated in Figure 3, there is a significant bandwidth dis- parity between the networks. For instance, NVLink provides nearly three times more bandwidth than Infinity Fabric, while inter-node bandwidth on a DGX-A100 is twice as large as that of a Frontier node. This makes cross-process communication less ideal for communication-intensive workloads like ZeRO- 3. Optimizations such as ZeRO++ proposed secondary weight partitioning to avoid inter-node Allgather operations during the backward pass, but this approach does not fully leverage the Frontier node topology. Given the bandwidth differences between GCDs, GPUs, and nodes, a more customized parti- tioning strategy to enhance communication efficiency should be achievable. One of our primary goals is to design an efficient communication reduction strategy tailored to the Frontier system topology."}, {"title": "V. DESIGN", "content": "In this section, we illustrate our 3-level topology-aware hierarchical partitioning strategy. We also explain the design intuition with memory consumption and communication vol- ume analysis. In Table III, we define terms and notations for following memory analysis. Note that our analysis uses the terms GPU and GCD interchangeably. To define an efficient training parameter partitioning protocol for distributed LLM training, we must explicitly define sharding factors for model weights, gradients, and optimizer states. Sharding factors refer to the number of GPUs required to allocate a full data replica. In our design, we split model weights among two GCDs within an MI250X, gradients among eight GCDs within a node, and optimizer states across all GCDs across nodes, similar to ZeRO-3. Note that the larger the number of data-parallel ranks, the more workers are required; thus, more communication is needed to spread and maintain correct context among them. In Table IV, ZeRO stage 1 and ZeRO stage 2 require model weights to fit onto a single worker, which is generally invalid for modern large language models. Optimizer states are distributed across all the workers.\nAnother critical fact to be mindful of when designing effec- tive sharding strategies is that training parameter dependency greatly affects data movement and communication volume. Such relation can be defined as follows, as spotlighted in [11] (N stands for number of nodes for each sharding dimension, P stands for number of GPUs per node, N \u00d7 P represents the final sharding factor for the corresponding model state):\n$N \\geq N_{dp} \\geq N_{os} \\geq N_g \\geq N_w$, $P > P_{dp} \\geq P_{os} > P_g \\geq P_w$\nIn essence, we must ensure that each worker stores only the gradients and optimizer states related to its local parameters. Maintaining surplus optimizer states and gradients with re- spect to corresponding gradients and model weights would incur extra communication volume and waste bandwidth. Our proposed sharding design conforms to the aforementioned dependency rule and maintains the smallest number of primary model weight shards (2 GCDs), followed by 8 GCDs of gradient shards (corresponding to the number of workers within a compute node), and optimizer state shards with a degree equal to the number of data-parallel ranks."}, {"title": "A. Weight Partitioning", "content": "For model parameters that do not fit on a single worker, we create primary weight partitions across the two GCDs within a single MI250X GPU, with each GCD (64GB) hosting half of the model parameters. As illustrated in Figure 4, for each micro-batch, the training orchestration conducts gathering on parameters across primary weight shards before each forward pass and across secondary partitions before the backward pass. This process often involves Allgather operations whenever a module of a transformer layer is encountered during each pass. Gradients are calculated in the backward phase and are distributed back to gradient partitions using Reduce-scatter operations. Our design stores primary partitions in FP16 and secondary partitions in a quantized format. We utilize quan- tizer and dequantizer operators from ZeRO++ for Allgather calls and partitions. The following paragraph will discuss"}, {"title": "On-Device Memory", "content": "We provide the on-device memory cost of our weight partitioning strategy in Table V. With our design, each GCD hosts 1.5$\\Psi$ bytes of weight memory, combining the primary partition and quantized secondary partition. Our approach differs from ZeRO++ and ZeRO-3 in that our memory occupation remains fixed regardless of the number of workers, which is not the case in the other two schemes. In this design, we trade memory for communication efficiency."}, {"title": "B. Gradient Partitioning", "content": "Gradients are another critical component in LLM training, as model weight updates depend heavily on the gradients of the loss function. Gradients are typically calculated and accu- mulated in each step for every micro-batch after the backward pass. Since gradients are calculated for each model param- eter, they consume a considerable amount of memory, even when using FP16 precision. Thus, it is crucial to distribute gradients among workers as well. To distribute gradients, we typically use Reduce-scatter operations [12] to synchronize and disseminate them to their corresponding parameter parti- tions. As illustrated in Figure 5, we shard gradients within a compute node on Frontier, resulting in a gradient shard degree of eight. Note that this strategy also fulfills the previously mentioned dependency rule ($N_g \\geq N_w$, $P_g \\geq P_w$) to avoid data redundancy during communication."}, {"title": "On-Device Memory", "content": "Table VI presents the memory require- ments for our proposed gradient partitioning strategy. Similar to weight shards, gradient shards occupy a fixed amount of memory ($\\frac{2\\Psi}{8}$ for FP16). In contrast, for ZeRO++ and ZeRO- 3, the on-device memory decreases as more workers become available, but at the cost of less efficient communication bandwidth."}, {"title": "C. Optimizer State Partitioning", "content": "Optimizer states refer to the internal variables maintained by an optimizer during the training process. These states help the optimizer keep track of historical information, which it uses to make more informed and effective parameter updates. Dif- ferent optimizers maintain different types of states, depending on their specific algorithms and strategies. However, optimizer states can be very costly in terms of memory. For example, when using AdamW [23], we need to maintain a full-precision copy of the model, momentum, and variance. We perform opti- mizer state sharding similar to ZeRO-3, in which the states are grouped into $N \\times P$ partitions (across all available workers). In our case, the number of partitions equals the number of GCDs. As depicted in Figure 6, each Frontier compute node holds $\\frac{1}{N \\times P}$ of the total optimizer states. It is important to note that, to address efficient usage of device memory and data movement, spreading optimizer states across all devices is crucial in fulfilling training parameter dependency. A device containing optimizer states or gradients irrelevant to its local"}, {"title": "D. Communication Volume Analysis", "content": "In this section, we provide a communication cost analysis of our proposed 3-level hierarchical design. We also consider cases with different secondary quantized weight shard degrees. As shown in Table VII, when we split secondary partitions within the same MI250X GPU, we limit forward and backward Allgather operations to only two GCDs, thus taking advantage of the highest intra-GPU bandwidth (200 GB/s). Most impor- tantly, the number of devices involved does not scale with the number of nodes. As a result, communication latency for backward and forward Allgather operations remains constant regardless of the increasing training scale. Furthermore, with the help of block-based quantization, we are able to halve the communication volume compared to ZeRO-3, since each parameter can be represented using only 1 byte (INT8) instead of 2 bytes (FP16).\nWe detail the communication cost of gradient partitioning in Table VIII. We have adopted the all-to-all-based Reduce- scatter with quantization from ZeRO++. Since our gradient shards are strictly distributed within a node, the Reduce- scatter latency does not degrade as we scale up, in contrast to ZeRO++. Additionally, thanks to INT4 quantization, we reduce communication volume by 4x. After synchronizing among MI250Xs using 1-hop all-to-all based Reduce-scatter, we call Allreduce on all nodes to maintain updated global gradients across replicas.\nFollowing completion of model parameter updates, we conduct an Allgather within the optimizer shards to gather the updated weights. This will incur a communication volume of $\\frac{\\Psi}{N \\times P} \\times d$, where $d = N_{os} \\times P_{os}$."}, {"title": "VI. EVALUATIONS", "content": "Scaling Performance In this section, we apply our hierar- chical partitioning strategy on top of ZeRO++ across various model sizes and scales and report system throughput metrics, including TFLOPS per GPU and samples per second. Note that Frontier treats GCDs as GPUs; in our graphs and tables, GPUs and GCDs refer to the same concept of workers or processes. We implemented our changes through DeepSpeed, and model training is conducted using GPT-NeoX, an open-source large- scale Megatron-DeepSpeed training framework integrated with additional techniques. We also devoted efforts to porting the training stack and ZeRO++ to AMD GPUs. For all experi- ments, we enabled FP16 mixed-precision training and flash attention, and maximized GPU utilization with an appropriate batch size. We treat ZeRO++ as the baseline and demonstrate its performance benefits over naive ZeRO-3 on Frontier AMD GPUs. As shown in Figure 7, we observe a 40.5% increase in TFLOPS per GPU over naive ZeRO-3 on 384 GPUs for a 20B model. We verified that the adaptation of quantization kernels and secondary weight partition, which avoids inter- node Allgather collectives, improved system throughput and scalability. Building on this baseline, we profiled our proposed hierarchical partitioning strategy. Our design demonstrated up to 139.8% and 70.7% increases in TFLOPS per GPU over ZERO-3 and ZeRO++, respectively, for up to 384 AMD GPUs and models of up to 20B parameters.\nModel Convergence We employed block-based quantiza- tion, as described in ZeRO++, for weights gathering, gradients distributing and secondary partition. Block-based quantization improves accuracy by dividing weight tensors into smaller chunks and applying independent quantization scaling coef- ficients to each element [22]. It has been demonstrated in pre- vious works that the final evaluation loss with all optimizations was only off by 1% compared to the baseline. Here, we provide the loss curves with quantization enabled and compare them to standard ZeRO-3 training. Looking at Figure 9 and Figure 10, when enabling our proposed method with quantization, we observe similar loss curves towards ZeRO-3 for GPT-NeoX- 20B and GPT-NeoX-10B. The demonstrated loss curves are collected using the web subset of the Pile Dataset for up to 14B tokens."}, {"title": "VII. DISCUSSION", "content": "It required significant effort to port and optimize the ZeRO++ kernels to AMD GPUs, of which we are the first to perform to the best of our knowledge. Our optimization on top of ZeRO++ offers near-linear scaling on Frontier, while ZeRO-3 and ZeRO++ struggle at large scales due to Frontier's expensive inter-node collective communication via RCCL. Further, storing the secondary partition in the low-precision quantized format saves a small amount of GPU VRAM, which can be more productively used elsewhere in the training pipeline to improve model accuracy (e.g. increasing model parameters) or time to convergence (e.g. batch size). While our proposed approach demonstrates significant potential for large- scale distributed training of language models, it is important to acknowledge several limitations and areas for future work."}, {"title": "A. System-specific Optimization", "content": "The core principles of our approach: quantized weight communication, hierarchical partitioning, and quantized gra- dient communication are theoretically applicable to various high-performance computing (HPC) systems. However, it is important to note that we have not yet validated this method on platforms other than Frontier. The performance gains and efficiency improvements we observed may not directly translate to different HPC environments. Nevertheless, our approach demonstrates how to exploit hierarchical bandwidth structures (GCD-to-GCD, GPU-to-GPU, node-to-node) com- mon in many HPC systems for efficient communication. While optimized for Frontier's specific topology, these core princi- ples can be adapted to other AMD-based deployments with similar hierarchical structures, potentially offering comparable benefits across a range of HPC architectures."}, {"title": "B. Model Size Constraints", "content": "Our current implementation works optimally with models whose parameters fit onto two Graphic Compute Dies (GCDs), which allows for efficient training of models up to approxi- mately 36 billion parameters. While this covers a wide range of current state-of-the-art models, the rapid scaling in the field of LLM development may eventually push beyond this scope. Addressing this limitation to accommodate even larger models without sacrificing the efficiency gains of our method represents an important avenue for future research."}, {"title": "C. Further Evaluations", "content": "Our evaluation of the proposed approach primarily focused on TFLOPS per GPU and scaling efficiency. The main model GPT-NeoX-20B [24] we evaluated is an autoregressive trans- former decoder model that is based on the architecture of GPT-3 [25]. While these metrics and this specific model provided valuable insights into our method's performance, we acknowledge the potential for a more comprehensive evaluation results. To broaden the scope of our results, future work could incorporate additional metrics such as Model FLOPs Utilization (MFU), which offers a more holistic view of computational efficiency. Furthermore, expanding the range of tested models to include diverse architectures like Llama-3 [3] or RWKV [26] would provide a more robust assessment of our approach's versatility across different LLM configurations. However, due to resource constraints, we were unable to conduct such extensive training experiments in this study."}, {"title": "VIII. RELATED WORK", "content": "Numerous efforts have been made to reduce communication costs in large-scale distributed training. PyTorch Fully Sharded Data Parallel (FSDP) [13] introduces hybrid sharding, where model weights and gradients can be sharded fully or in a hybrid fashion. In this approach, the model is sharded across one partition and replicated across another [27]. Increasing the size of the sharded partition incurs a higher communication cost with a lower memory cost, while increasing the size of the replicated partition results in a lower communication cost at a higher memory cost. PyTorch FSDP also introduces a separate stream to issue Allgather operations and enables asynchronous Reduce-Scatter and Allreduce operations in relation to the backward pass. This approach also prefetches Allgather op- erations during both backward and forward passes. However, FSDP does not exploit any benefits from communication compression or quantization.\nMegaScale [29] extends collective prefetching mechanisms to 3D parallelism and implements an optimized Redis-based barrier to significantly reduce synchronization among devices. However, MegaScale does not support hybrid parallelism, and any scaling beyond ZeRO-1 and ZeRO-2 strategies relies on 3D parallelism.\nReducing the size of messages being communicated has also been a focal point. One well-established method is to incorporate compression and quantization. [30] adopts hybrid lossy and lossless compression-assisted MPI collectives to ac- celerate 3D parallelism training. ZeRO++ [18] applies block- based quantization [22] in weight and gradient communication to reduce data volume.\nSeveral works have also aimed at using hierarchical par- titioning to alleviate inter-node bandwidth pressure. MiCS [28] proposes a scale-aware partitioning strategy that groups model states (parameters, gradients, and optimizer states) into smaller partitions. This strategy utilizes high-speed NVLink connections within a node if the model states fit inside a single node, thus avoiding inter-node Allgather operations. However, this solution splits all model states evenly across partitions without considering Frontier's intra-node topology and flexible partition sizes for different training parameters.\nAMSP [11] extends this strategy to support independent partition sizes for parameters, gradients, and optimizer states. AMSP also conducts a detailed investigation into fine-grained communication-computation overlap and supports finding the optimal sharding strategy within a dedicated search space. However, this work does not consider quantization within collectives and has only been tested on NVIDIA GPUs. We compare our ZeRO-topo approach with other related works in Table X."}, {"title": "IX. CONCLUSION", "content": "Recent research efforts to decrease communication overhead in large-scale LLM training have primarily focused on the bandwidth gap between intra-node and inter-node commu- nication, often conducted on NVIDIA platforms. However, with the emergence of more capable infrastructure featuring AMD GPUs, a meticulous co-design of software optimization and underlying hardware topology is essential for achieving greater efficiency. In this work, we propose a dedicated 3- level topology-aware hierarchical partitioning strategy tailored for the Frontier supercomputing cluster, the current Top-1 system. This strategy distributes training parameters across different layers of devices to fully utilize the interconnect bandwidth between GCDs, GPUs, and nodes. We implemented this protocol and validated it across various models and scales, achieving up to a 139.8% increase over ZeRO-3 and 70.7% increase over ZeRO++ in TFLOPS per GPU for a 20B model on 384 GPUs."}]}