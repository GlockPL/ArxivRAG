{"title": "A Comparison of DeepSeek and Other LLMs", "authors": ["Tianchen Gao", "Jiashun Jin", "Zheng Tracy Ke", "Gabriel Moryoussef"], "abstract": "Recently, DeepSeek has been the focus of attention in and beyond the AI community. An interesting problem is how DeepSeek compares to other large language models (LLMs). There are many tasks an LLM can do, and in this paper, we use the task of predicting an outcome using a short text for comparison. We consider two settings, an authorship classification setting and a citation classification setting. In the first one, the goal is to determine whether a short text is written by human or AI. In the second one, the goal is to classify a citation to one of four types using the textual content. For each experiment, we compare DeepSeek with 4 popular LLMs: Claude, Gemini, GPT, and Llama.\nWe find that, in terms of classification accuracy, DeepSeek outperforms Gemini, GPT, and Llama in most cases, but underperforms Claude. We also find that DeepSeek is comparably slower than others but with a low cost to use, while Claude is much more expensive than all the others. Finally, we find that in terms of similarity, the output of DeepSeek is most similar to those of Gemini and Claude (and among all 5 LLMs, Claude and Gemini have the most similar outputs).\nIn this paper, we also present a fully-labeled dataset collected by ourselves, and propose a recipe where we can use the LLMs and a recent data set, MADStat, to generate new data sets. The datasets in our paper can be used as benchmarks for future study on LLMs.", "sections": [{"title": "Introduction", "content": "In the past two weeks, DeepSeek (DS), a recent large language model (LLM) (DeepSeek-AI, 2024), has shaken up the AI industry. Since its latest version was released on January 20, 2025, DS has made the headlines of news and social media, shot to the top of Apple Store's downloads, stunning investors and sinking some tech stocks including Nvidia.\nWhat makes DS so special is that in some benchmark tasks it achieved the same or even better results as the big players in the AI industry (e.g., OpenAI's ChatGPT), but with only a fraction of the training cost. For example,\n\u2022 In Evstafev (2024), the author showed that over 30 challenging mathematical problems derived from the MATH dataset (Hendrycks and et al., 2021), DeepSeek-R1 achieves superior accuracy on these complex problems, compared with ChatGPT and Gemini, among others.\n\u2022 In a LinkedIn post on January 28, 2025, Javier Aguirre (a researcher specialized in medicine and AI, South Korea) wrote: \u201cI am quite impressed with Deepseek. ... Today I had a really tricky and complex (coding) problem. Even chatGPT-01 was not able to reason enough to solve it. I gave a try to Deepseek and it solved it at once and straight to the point\". This was echoed by several other researchers in AI.\nSee more comparison in DeepSeek-AI (2024); Zuo and et al. (2025); Arrieta and et al. (2025). Of course, a sophisticated LLM has many aspects (e.g., Infrastructure, Architecture, Performances, Costs) and can achieve many tasks. The tasks discussed above are only a small part of what an LLM can deliver. It is desirable to have a more comprehensive and in-depth comparison. Seemingly, such a comparison may take a lot of time and efforts, but some interesting discussions have already appeared on the internet and social media (e.g., Ramadhan (2025)).\nWe are especially interested in how accurate an LLM is in prediction. Despite that there are a long list of literature on this topic (see, for example, Friedman et al. (2001)), using LLM for prediction still has advantages: while a classical approach may need a reasonable"}, {"title": "1.1 Authorship classification", "content": "In the past two years, AI-generated text content started to spread quickly, influencing the internet, workplace, and daily life. This raises a problem: how to differentiate AI-authored content from human-authored content (Kreps et al., 2022; Danilevsky et al., 2020).\nThe problem is interesting for at least two reasons. First, the AI-generated content may contain harmful misinformation in areas such as health care, news, and finance (Kreps et al., 2022), and the spread of fake and misleading information may threaten the integrity of online resources. Second, understanding the main differences between human-generated and AI-written content can significantly help improve AI language models (Danilevsky et al., 2020).\nWe approach the problem by considering two classification settings, AC1 and AC2.\n\u2022 (AC1). In the first setting, we focus on distinguishing human-generated text and AI-generated text (i.e., hum vs. AI)."}, {"title": "1.2 Citation classification", "content": "When a paper is being cited, the citation could be significant or insignificant. Therefore, to evaluate the impact of a paper, we are interested in not only how many times it is being cited, but also how many significant citations it has. The challenge is, while it is comparably easier to count the raw citations of a paper (i.e., by Google Scholar, Web of Science), it is unclear how to count the number of 'significant' citations of a paper.\nTo address the issue, note that surrounding a citation instance, there is usually a short piece of text. The text contains important information for the citation, and we can use it to predict the type of this citation. This gives rise to the problem of Citation Classification, where the goal is to use the short text surrounding a citation to predict the citation type. Here, we have two challenges. First, it is unclear how many different types academic citations may have and what these types are. Second, we do not have a ready-to-use data set.\nTo address these challenges, first, after reviewing many literature works and empirical results, we propose to divide all academic citations into four different types:\n\u2022 \"Fundamental ideas (FI)\"\n\u2022 \u201cTechnical basis (TB)\u201d,\n\u2022 \"Background (BG)\",\n\u2022 \u201cComparison (CP)\u201d.\nBelow for simplicity, we encode the four types as \u201c1\u201d, \u201c2\u201d, \u201c3\u201d, \u201c4\u201d. Note that the first two types are considered as significant, and the other two are considered as comparably less significant. See Section 2.2 for details.\nSecond, with substantial efforts, we have collected from scratch a new data set by ourselves, which we call the CitaStat. In this data set, we downloaded all papers in 4"}, {"title": "1.3 Results and contributions", "content": "We have applied all 5 LLMs to the four experiments (AC1, AC2, CC1, CC2), and we have the following observations:\n\u2022 In terms of classification errors, Claude consistently outperforms all other LLM approaches. DeepSeek-R1 underperforms Claude but outperforms Gemini, GPT, and Llama in most of the cases. GPT performs unsatisfactorily for AC1 and AC2, with an error rate similar to that of random guessing, but it performs much better than random guessing for CC1 and CC2. Llama performs unsatisfactorily: its error rates are either comparable to those of random guessing or even larger.\n\u2022 In terms of computing time, Gemini and GPT are much faster than the other three approaches, and DeepSeek-R1 is the slowest (an older version of DeepSeek, DeepSeek-V3, is faster but does not perform as well as DeepSeek-R1).\n\u2022 In terms of cost, Claude is much more expensive for a customer than other approaches. For example, for CC1 and CC2 altogether, Claude costs $12.30, Llama costs $1.2, and the other three methods (DeepSeek, Gemini and GPT) cost no more than $0.3.\n\u2022 In terms of output similarity, DeepSeek is most similar to Gemini and Claude (GPT and Llama are highly similar in AC1 and AC2, but both perform relatively unsatisfactorily).\nOverall, we find that Claude and DeepSeek have the lowest error rates, but Claude is relatively expensive and DeepSeek is relatively slow.\nWe have made the following contributions. First, as DeepSeek has been the focus of attention in and beyond the AI community, there is a timely need to understand how it compares to other popular LLMs. Using two interesting classification problems, we"}, {"title": "2 Main results", "content": "In this section, we describe our numerical results on the two problems, authorship classification and citation classification, and report the performances of all 5 LLMs."}, {"title": "2.1 Authorship classification", "content": "The MADStat (Ji et al., 2022; Ke et al., 2024) contains over 83K abstracts, but it is time-consuming to process all of them.\u00b9 We selected a small subset as follows: First, we restricted to authors who had over 30 papers in MADStat. Second, we randomly drew 15 authors from this pool by sampling without replacement. Each time a new author was selected, we checked if he/she had co-authored papers with previously drawn authors; if so, we deleted this author and drew a new one, until the total number of authors reached 15. Finally, we collected all 15 authors' abstracts in MADStat. This gave rise to a data set with 582 abstracts in total (see Table 2).\nFor each original human-written abstract, we used GPT-40-mini to get two variants.\n\u2022 The AI version. We provided the paper title and requested for a new abstract. The prompt is \u201cWrite an abstract for a statistical paper with this title: [paper title].\u201d\nBoth variants are authored by AI, but they look differently. The AI version is usually significantly different from the original abstract, so the 'human versus AI' classification problem is comparably easier. For example, the left panel of Figure 1 is a comparison of the length of the original abstract with that of its AI version. The length of human-written abstracts varies widely, while the length of AI-generated ones is mostly in the range of 100-200 words. The humAI version is much closer to the original abstract, typically only having local word replacements and mild sentence re-structuring. In particular, its length is highly correlated with the original length, which can be seen in the right panel of Figure 1.\nAs mentioned, we consider two classification problems:\n\u2022 (AC1). A 2-class classification problem of \u2018human versus AI',\n\u2022 (AC2). A 2-class classification problem of \u2018human versus humAI'.\nFor each problem, there are 582 \u00d7 2 = 1164 testing samples, half from each class. We input them into each of the 5 LLMs using the same prompt: \u201cYou are a classifier that determines whether text is human-written or AI-edited. Respond with exactly one word: either \u2018human\u2019 for human-written text or\u2018ChatGPT' for AI-written text. Be as accurate as possible.\u201d\nNote that comparing with classification approaches (e.g., SVM, Random Forest (Friedman et al., 2001)), an advantage of using an LLM for classification is that, we do not need to provide any training sample. All we need is to input the LLM with a prompt."}, {"title": "2.2 Citation classification", "content": "The MADStat only contains meta-information and abstracts, rather than full papers. We created a new data set, CitaStat, by downloading full papers and extracting the text surrounding citations. Specifically, we restricted to those papers published during 1996-2020 in 4 representative journals in statistics: Annals of Statistics, Biometrika, Journal of the American Statistical Association, and Journal of the Royal Statistical Society Series B. We wrote code to acquire PDFs from journal websites, convert PDFs to plain text files, and then extracted the sentence (using SpaCy, a well-known python package for sentence parsing) containing each citation (we call it a 'citation instance'). There are over 367K citation instances in total. We randomly selected n = 3000 of them and manually labeled them using one of the following four categories:\n\u2022 'Fundamental Idea (FI)' (previous work that directly inspired or provided important ideas for the current paper). Example: \"The proposed class of discrete transforma-\n\u2022 (CC1). The 4-class classification problem: Given the textual content of a citation (i.e., the text surrounding the citation), we aim to classify it to one of the four classes.\n\u2022 (CC2). The 2-class classification problem: We re-combine the four categories into two, where \u2018Fundamental Idea\u2019and \u2018Technical Basis' are considered as \u2018Significant (S)', and Background and Comparison are considered as \u2018Incidental (I)'.2 Given the textual content of a citation, we aim to predict whether it is a \u2018Significant (S)' citation.\nFor each of the 5 LLMs, we used prompts to get classification decisions. Unlike the previous authorship classification problem, the class definitions in this problem are not common knowledge and need to be included in the prompt. In the 2-class problem, we use the prompt as in Figure 4. It provides definitions, examples, and how the four classes are re-combined into two, aiming to convey to the LLM as much information as possible. The prompt for the 4-class problem is similar, except that the description of grouping 4 classes into 2 is removed and the requirement for output format is modified (see Figure 4)."}, {"title": "3 Discussion", "content": "Since the release of its latest version on January 20, 2025, DeepSeek has been the focus of the attention in and beyond the AI community. It is desirable to investigate how it compares to other popular LLMs. In this paper, we compare DeepSeek with 4 other popular LLMs (Claude, Gemini, GPT, Llama) with the task of predicting an outcome using a short piece of text. We consider two settings, an authorship classification setting and a citation classification setting. In these settings, we find that in terms of the prediction accuracy, DeepSeek outperforms Gemini, GPT, and Llama in most cases, but consistently underperforms Claude.\nOur work can be extended in several directions. First, it is desirable to compare these LLMs with many other tasks (e.g., natural language processing, computer vision, etc.). For example, we may use the ImageNet data set (Deng and et al., 2009) to compare these LLMS and see which AI is more accurate in classification. Second, for both classification problems we considered in this paper, it is of interest to further improve the performance of the LLM by combining tools in statistics and machine learning. Take the authorship classification for example. We can first use statistical tools to suggest a list of words that are discriminative between AI-generated text and human-generated text. We then construct a new prompt by combining these words with the prompt used earlier in our paper. With the new prompt, we expect that the performance of an LLM can be much improved. See our forthcoming manuscripts (Gao et al., 2024; Jin et al., 2025) for example. Last but not the least, the datasets we generated can be used not only as a platform to compare different approaches, but also as useful data to tackle many interesting problems. For example, the MadStatAI data set can be used to identify the patterns of AI generated documents, and the CitaStat data set can be used to tackle problems such as author ranking or estimating the research interest of an author (see for example Ji et al. (2022) and Ke et al. (2024))."}]}