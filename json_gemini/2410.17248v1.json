{"title": "Hyperspectral ViTs: Fast and Accurate methane detection on-board satellites", "authors": ["V\u00edt R\u016f\u017ei\u010dka", "Andrew Markham"], "abstract": "On-board processing of hyperspectral data with machine learning models would enable unprecedented amount of autonomy for a wide range of tasks, for example methane detection or mineral identification. Methane is the second most important greenhouse gas contributor to climate change, and it's automated detection on-board of satellites using machine learning models would allow for early warning system and could enable new capabilities such as automated scheduling inside constellations of satellites. Classical methods for methane detection suffer from high false positive rates and previous deep learning models exhibit prohibitive computational requirements. We propose fast and accurate machine learning architectures which support end-to-end training with data of high spectral dimension. We evaluate our models on two tasks related to hyperspectral data processing methane leak detection and mineral identification. With our proposed general architectures, we improve the F1 score of the previous methane detection state- of-the-art models by more than 27% on a newly created synthetic dataset and by almost 13% on the previously released large benchmark dataset. We also demonstrate that training models on the synthetic dataset improves performance of models finetuned on the dataset of real events by 6.9% in F1 score in contrast with training from scratch. On a newly created dataset for mineral identification, our models provide 3.5% improvement in the F1 score in contrast to the default versions of the models. With our proposed models we improve the inference speed by 85.19% in contrast to previous classical and deep learning approaches by removing the dependency on classically computed features. Namely, one capture from the EMIT sensor can be processed in only 30 seconds on a realistic proxy hardware used on the ION-SCV 004 satellite.", "sections": [{"title": "I. INTRODUCTION", "content": "In this paper, we introduce HyperspectralViTs, new adap- tations of Transformer based machine learning models for semantic segmentation of hyperspectral data (also refered to as imaging spectroscopy data) for low compute environments. We demonstrate these adaptations on two recent architectures, SegFormer [1] and EfficientViT [2] and for two relevant tasks, methane leak detection and mineral identification. We describe the existing landscape of approaches used to detect methane leaks and minerals in remote sensing data and outline several limitations to these methods. We then use these as motivation to propose changes to the Transformer based architectures. We improve the accuracy and inference speed of the existing methods and benchmark the model in limited compute envi- ronment which serves as a proxy to the hardware available on the satellite.\nMethane detection is an important task, which could lead to the reduction of anthropogenic methane in atmosphere this has been identified as one of the fastest pathways to reduce global warming in United Nations Global Methane Assessment report [3]. In practice, the detection of methane leaks remains to be manual, as the outputs of existing methods can be quite noisy. For large scale automation of this process, reliable and accurate models are needed to sift through the increasing deluge of data from newly deployed hyperspectral sensors.\nMineral identification is a task related to scene decompo- sition into individual endmembers from a spectral library. On its own it could serve as a strong general tasks highly relevant for future Foundation models working with hyperspectral data. We note that one of the example downstream tasks of interest could also be methane detection, as it has been shown that having information about scene background is beneficial for separation of methane leak events from confounders [4].\nAutonomy on-board of satellites is needed to enable rapid response, alerting and automatic scheduling within a constel- lation of satellites [5], [6]. Recent works have shown that detection of events of interest (such as floods) can be done directly in space with on-board machine learning models, reducing the quantity of data for downlinking [7].\nWe note that the proposed architecture is aimed at the general problem of processing hyperspectral data and as such can be used outside of the domain of remote sensing. By framing the problem as an end-to-end semantic segmentation, we disentangle the proposed architecture from the task of methane leak detection and retain generality.\nDeveloping machine learning architecture for hyperspectral data is well timed and necessary for many downstream applica- tions. Hyperspectral remote sensing data is used in mineralogy [8] and agriculture [9]. Sensors with high spectral dimension are deployed on devices used for deep space exploration [10]- [12] and on hand-held devices [13] including mobile phones [14].\nOur main contribution is the proposal of a new family of models called HyperspectralViTs, architectures adapted to handle hyperspectral data without informational losses due to reliance on classically computed intermediate products, or due to informational bottlenecks in the original architectures as illustrated on Figure 1. We analyse the limitations of existing machine learning architectures without adaptations for hyperspectral data and show that we can improve their performance with several simple steps."}, {"title": "A. Literature review", "content": "In this paper we primarily tackle the tasks of methane leak detection, as such most of the relevant literature focuses on this discipline. We however also keep a brief section on methods used for mineral identification.\nThe task of methane leak detection can be more generally described as semantic segmentation of remote sensing images and has recently seen increased interest from the machine learning community [16].\nFigure 2 shows the wavelength range of typically used multispectral and hyperspectral satellites alongside with the target signature of methane (modelled from the HITRAN 2012 database [17]). The multispectral satellite Sentinel-2 has been previously used to monitor large methane leaks in [18], however multi-temporal passes over the same location were needed to detect weaker events. The work of [19] uses data from the WorldView-3 satellite, which has multiple bands in the desired area of interest, but only large methane leak events can be detected and validation requires human expert intervention.\nHyperspectral data provides more information for the task of detecting methane signal, this is due to the improved sampling in spectral ranges where methane is visible (namely between 1600-1800nm and 2100-2500nm). This task has been classically addressed with matched filter approaches [20] which compare the observed data against the target signature of methane. The resulting matched filter product, sometimes called \u201cmethane enhancement\", is quite noisy, as other confounder materials (for example rooftops, roads and solar panels) get detected as well. Iterative matched filter approaches, such as the mag1c filter [21] attempt to improve the clarity of the product. Other works depend on matched filter variants using wider spectral windows [22]. Up until recently, these methane enhancement products were manually inspected, and trained experts were tasked to create final detections.\nMineral detection in remote sensing data has been ap- proached with a wide range of classical methods which are described in the overview of [23], and only a subset of these uses hyperspectral data. Mineral identification in hyperspectral data can be formalised as spectral unmixing, that is identifying which individual endmembers from a spectral library of chem- ical and mineral constituents create the observed signature. One classical approach is the Tetracoder program [24] which uses a cascade of manually derived rules to compare observed spectral feature with a reference spectral library of minerals. It isolates discriminative portions of spectra known for each mineral using continuum removal techniques [25] and then computes degree of fit metrics for each item in the library. It has been described as a simulation of the process a trained mineralogist would do when looking at the hyperspectral data. This task can also be formalised as semantic segmentation, using \u201cmulti-hot\u201d labels (multiple minerals can be present in each pixel).\nWe note that datasets for global mineral identification across the entire globe are missing. Works such as [26] evaluate their methods on just two airborne hyperspectral captures. Works such as [24] compare their methods against several well studied scenes such as the Cuprite or Alunite Hill scenes these have been sparsely in-situ sampled to provide verification data. Note that large scale differences exist when comparing in-situ measurements and space based instruments (where one pixel can cover over e.g. 60x60m area). Therefore, existing works mostly remain local in their scope, and the used datasets are quite small. Machine learning models adapted to handle hyperspectral data such as [27], [28] were typically trained only on very small datasets. With our work we aim to train our proposed models on much larger and more diverse datasets, while using multiple tasks.\nAs has been identified by [29] and several overview papers [30]\u2013[32], research in machine learning for processing hyper- spectral data has been limited mainly due to the lack of large, publicly available datasets with high quality annotations. Many works still use very small datasets such as Indian Pines [33], University of Pavia, Houston or Salinas, all of which contain\""}, {"title": "II. DATA", "content": "In order to train machine learning models, which would be able to generalise globally around the world, we primarily need global and large datasets with high quality labels. As we already noted, many of the typically used hyperspectral datasets are very small. Recent methane leak datasets from [4], [34] are large enough to train deep learning models, however they both use data from the aerial AVIRIS-NG sensor, which is relatively local in scope (e.g. the Four Corners region of USA). Instead, we use the data from the recent hyperspectral sensor Earth Surface Mineral Dust Source Investigation (EMIT), which is deployed on the International Space Station and has near global coverage of the arid regions of the Earth [15]. In total we create three new datasets from EMIT data, their global coverage is demonstrated in Figure 3 and they are described in detail in Table I."}, {"title": "A. Methane leak event simulation", "content": "In one of our datasets, we simulate synthetic methane leak events in the clean hyperspectral datacubes, given the knowledge of the expected signal of methane and plume concentration data from real events. Unlike previous study of [44], which used plume shapes simulated using the expansive Large Eddy Simulation (LES) [45], we use real methane leak events from the similar hyperspectral sensor AVIRIS-NG, to inform the simulation. Namely, we use the STARCOP dataset [34], given it's improved and manually refined labels and high diversity of plume sizes.\nMathematically, we can describe the relation between a datacube with and without methane leak event using the Beer-Lambert absorption law [21] as:\n$L_{simulated}(a, s) = L_{clean} * e^{-a*s}$ (1)\nWhere L refers to at sensor radiances in the hyperspectral datacubes (clean and resulting simulated one), a to the con- centration of methane at given location and s the methane absorption at given wavelength. We note that the quantity a is usually the one we are trying to estimate with matched filter approaches [21]. As such, we can use existing matched filter products as sources for this quantity when simulating synthetic events. For s we query the values of the methane signature at the wavelength of each band as plotted on Figure 2."}, {"title": "B. Mineral aggregation", "content": "To create our new mineral dataset, we adapt the steps described in the Theoretical basis document [46] released alongside the \u201cEMITL2BMIN.001", "EMIT10": "ubset defined in [46]. The aggregation process is shown on Figure 4. We note that we use only the binary maps of three selected mineral classes out of the total 10. We chose the subset M = {Goethite, Hematite, Kaolinite}, as these classes were abundant in the locations sampled in our dataset. We use the released annotations which were computed using the Tetracorder method [24], and as such we can't really use Tetracorder as a classical baseline (as it would unfairly obtain perfect scores). Therefore, we consider the created dataset as having pseudo-ground truth labels, and our results only as initial. We however note, that given any manually validated dataset, our models could easily be retrained with the new data."}, {"title": "C. Final created datasets", "content": "Firstly, we created the \"OxHyperSyntheticCH4\" dataset of synthetically simulated methane leak events using the labels of the STARCOP dataset with newly downloaded near-global hyperspectral datacubes from the EMIT sensor. In total we keep 796 large tiles (of 512x512 px) in the training dataset, 198 in the validation dataset and 200 in the test dataset. Each subset is made from completely non-overlapping sources of data (clean EMIT datacubes and labels from the STARCOP dataset from AVIRIS-NG data). In each subset we uniformly sample the available plume sizes, so that we have similar distribution of event sizes. Half of these tiles contain simulated events while the other half is kept event free to provide negative samples. We note that these large tiles (of 512x512 px) are further tiled when training the models. When using the tile size of 64 pixels with an overlap of 32 pixels, we finally get a training dataset of 174341 samples. The total size of this data is 228 GB.\nSecondly, we create the \u201cOxHyperRealCH4\" dataset of real methane leak events. In total we have 279 large tiles in the training dataset, 91 in the validation dataset and 98 in the test dataset. These have reliable ground truth labels, which we calculated from the original labels provided on the EMIT data portal and consequentially manually checked. We note that this dataset is smaller than the other ones and training a machine learning model from scratch could lead to overfitting and poor generalisation, however it can also be used for model fine-tuning and to test generalisation ability of models trained on other datasets. The total size of this data is 47 GB.\nThirdly, we create \u201cOxHyperMinerals\u201d, a completely new dataset for mineral identification, with labels of 3 selected mineral classes and source annotation for all 381 constituents (separated into two groups). In total we have 796 large tiles in the training dataset, 198 in the validation and finally 200 in the test dataset. These large tiles exactly match the tiles used in the synthetic methane events dataset, however they do not contain the simulated events and contain more bands. The mineral dataset contains all 285 spectral bands in between spectral range of 381-2492nm. The total size of this data is 372 GB.\nFinally, we also use a variant of the STARCOP dataset from [34] with all relevant spectral bands extracted from the source data. We train our models and compare them on this benchmark dataset with the previously reported results.\""}, {"title": "III. METHODOLOGY", "content": "We are using three different semantic segmentation archi- tectures with the created dataset of hyperspectral datacubes the commonly used U-Net model [48] and the newer Seg- Former [1] and EfficientViT [2] models. These models can be understood in terms of the encoder-decoder layout. The U- Net model consists of downsampling convolutional layers in the encoder network, and upsampling layers in the decoder,"}, {"title": "A. Limitations of existing approaches", "content": "while also using skip-connections to communicate across the same spatial resolution of the data. The SegFormer model uses Transformer blocks in its encoder network, concatenates the features from different resolution levels and then uses a MLP block in the decoder network. The EfficientViT models further proposes changes to make the model run faster on limited hardware. In all models the spatial resolution is initially re- duced and then reconstructed in the decoder, while information at different scales is kept with the use of skip-connections, or when concatenating the features together.\nWe are interested in training these architectures in an end- to-end manner on the raw hyperspectral datacubes. In the methane detection task, this allows us to not depend on the classical matched filter product, which can be slow and which contains false detections of confounder materials. Models designed with typical computer vision data in mind (with one to three bands) however suffer from what we call informational bottleneck when they are used with data with large spectral dimensionality. As an example, if we select 86 bands of data which are relevant for our task of methane detection, the first layer in the vanilla versions of the used models reduces the data volume by -95.34% in the case of either the SegFormer, EfficientViT or U-Net (instead of increasing this volume by +33% as is common with RGB data). We claim that this informational bottleneck forces the model to get rid of most of the potentially valuable information right at the entry point to the model, which means that the later layers cannot effectively extract high level compressed information, as this extraction has to occur in the first convolutional layer.\nFurthermore, we note that the SegFormer and EfficientViT architectures reduce the spatial resolution of the input data by the factor of 4, and use non-parametric bilinear interpolation to scale the predicted output to the original resolution. In contrast, the U-Net architecture reduces the spatial dimension of the data only by the factor of 2, and uses a learned Upscaling layer at the end of the model to reconstruct the original resolution."}, {"title": "B. Proposed architecture", "content": "We propose three modular changes to the selected ar- chitectures to allow for better handling of data with high spectral dimension (such as data from hyperspectral satellites) as illustrated on Figure 5.\nFirstly, to aid with the spectral dimensionality of the data, we propose an addition of a special spectral layer into all Transformer blocks throughout the encoder networks. We add a Convolutional layer with kernel size of (1,1), stride of 1 and padding of 0, keeping the output of each layer with the same spectral dimension as on its input. This allows for learning operations that occur merely on the spectral data, on its own these allude to pixel-by-pixel operations of classical methods. While in theory the original convolutional layers with larger kernel sizes could learn similar operations, by restricting the operation to just the 1x1 convolutions, we obtain much sharper outputs. Effectively, this also allows the model to weight different input bands with a mechanism similar to attention, thus selecting only the relevant information for the task of interest. While this is a simple addition to the model, we note that it is quite effective, and it doesn't significantly increase the computational requirements of the model.\nSecondly, we address the reduction of the spatial resolution of the data. We implement a similar Upscaling layer to the one used in the U-Net architecture at the end of the Transformer based models, just before their classification heads. This block is composed out of two repeated 2D convolutional layers with kernel size of (3,3), padding and stride of 1, each followed by a ReLu activation layer and Batch normalisation layer.\nAs an additional solution for the recovery of the lost resolution, we also explore the change to the hyperparameters of the original architectures, namely changing the stride of the first convolutional layer (e.g. in SegFormer from 4 to 2). This change effectively increases the spatial size reduction of the data propagated through the network by the factor of 2. This doesn't change the number of parameters of the resulting model, but as the effect of this change influences the whole model, the impact on the inference speed is larger than with the Upscaling layer.\nThese three proposed changes are modular and can be evaluated in isolation or used all together - the resolution of the output product will either be 1\u00bc, \u00bd, or 1 times the resolution of the input."}, {"title": "C. Baselines", "content": "As a classical baseline approach for methane detection in hyperspectral data, we use the matched filter product (using the iterative variant mag1c [21]) with the threshold of 500 ppmxm. Additionally, we apply an opening morphological filter to remove isolated noisy pixels in the product. As a classical machine learning baseline, we implement the Hyper-STARCOP model based on the U-Net architecture proposed by [34] and train it on our new methane datasets. This model uses the pre-computed matched filter product alongside the RGB bands of the input data and effectively learns to clean the classical methane enhancement product. We note, that if the matched filter product misses a methane leak event, this model has no way of accessing the original information in the hyperspectral datacube to recover the detection.\nFor the task of mineral identification, we can't use the classical method of Tetracorder as a baseline, as we used it to generate our pseudo-ground truth labels. When inspecting other baseline approaches such as for example the spectral angle mapper (SAM) [49], we noted significant disagreement between the two methods. As such, for the task of mineral identification, we chose to only compare different versions of our adapted models against their default out-of-the-shelf version which we treat as a deep learning baseline."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "When training machine learning models on our newly cre- ated datasets, we use the following settings and hyperparame- ters. We train our models on the task of semantic segmentation, in methane detection with two exclusive classes (\"plume\" and \"no plume\") and in mineral detection with a multi-hot label (which allows no detection or potentially multiple active classes of \"Goethite\u201d, \u201cHematite\" and \"Kaolinite\u201d).\nDuring training on the methane datasets, to balance the data distribution, we oversample the instances with plume pixels by using the WeightedRandomSampler provided by the PyTorch library. We use the Adam optimiser with learning rate of 0.001 for the U-Net architectures and 0.00006 for the SegFormer and EfficientViT architectures. U-Net models use the MobileNet-v2 backbone [50] in the encoder, SegFormer uses the BO backbone architecture and EfficientViT the B1 backbone architecture. We use the batch size of 32 with U- Net models and 16 with SegFormer models, due to memory limitations while training.\nOn the \"OxHyperSyntheticCH4\" dataset we train all models for 50 epochs. When training we use the loss on the set aside validation subset to select the best performing models. When fine-tuning the models on the \u201cOxHyperRealCH4\" dataset, we train the models for further 10 epochs from the best reached checkpoint. When training models from scratch on this smaller dataset, we keep the number of epochs to 50. End-to-end models use 86 bands of the EMIT data (RGB and bands between 1573-1699nm and 2004-2478nm).\nTo compare the newly proposed models with the Hyper- STARCOP \u201cRGB+MF\u201d model from [34], we also train them on the AVIRIS-NG STARCOP dataset. Here we train the models for 15 epochs using a batch size of 16 and tile size of 128 px with an overlap of 64 px. Except for the batch size (original was 32), these hyperparameters are kept the same as in the original paper. We also weight the model loss by the MF product, as was done with the original HyperSTARCOP models. As there is no validation subset, we use the model from the last epoch. All end-to-end models use 60 bands of AVIRIS-NG (namely bands between 2104-2400 nm). This was chosen to match the range typically used by the matched filter as shown in Figure 2.\nFinally, when training models on the \"OxHyperMinerals\u201d dataset on the task of mineral detection, we train all models for 20 epochs with tiles of 64 px with 32 px overlap. In this case, trained models use all 285 bands of the available EMIT data (381-2492 nm). In this case, the data loader is not balancing the samples and losses are not weighted.\""}, {"title": "A. Hardware Restrictions", "content": "For training, we use a high performance computing cluster with Tesla V100 GPUs with 32GB GPU memory and the Intel Platinum 8628 (Cascade Lake), 2.90GHz CPU with 384GB RAM. This type of machine is typically much more computationally capable than the hardware later used for deployment.\nTherefore, with several tested model variants, we also conducted experiments measuring the runtime on compute constrained hardware, namely on the flatbed hardware pro- vided by Unibap and D-Orbit [37]. These machines serve as a proxy to match the computational in-space environment of a D-Orbit's ION-SCV004 satellite, which is equipped with: quad-core 1.2GHz, AMD GX-412HC SOC CPU processor, the Intel Movidius Myriad X VPU and 2GB of RAM. Similar compute environments leveraging Myriad chips were used on prior missions such as ESA's PhiSat-1 [51] and PhiSat-2 [52].\nWe have also selected two other devices typically used for Internet-of-Things (IoT) applications, the CPU powered Raspberry Pi boards and the GPU equipped NVIDIA Jetson AGX Xavier board. As has been noted by NASA's guidelines on using Raspberry Pi's in space [53], they are not suggested for deployment in any mission critical tasks due to their lower robustness in extreme environments. They have been however used as a proxies for devices of similar compute power in [54]. We then use the more powerful NVIDIA Jetson AGX Xavier board equipped with an 8-core NVIDIA Carmel ARMv8.2 CPU at 2.265 GHz and a GV10B GPU based on the Volta architecture with 16GB memory (shared between CPU and GPU). We used both ONNX Runtime python libraries and the Tensor RT inference engine (using the trtexec command), as was also used in the work of [2]. While initial tests of the effects of radiation on NVIDIA's Jetson devices was explored in [55] and proposed for space applications in [56], these works also state that a careful examination and future tests are still needed."}, {"title": "V. RESULTS", "content": "In this section, we first compare the results from different used models on the task of methane detection in hyperspectral data from two explored sensors. Secondly, we report the results from using our proposed models on a novel dataset for mineral identification. Finally, we also report measurements of the inference time needed to use these models on a compute constrained hardware of the Unibap flatsats. These will serve as an approximate proxy to the runtime needed on-board of potential satellites."}, {"title": "A. Methane detection on EMIT", "content": "Figure 6 shows the comparison on the selected methods classical baseline, U-Net based model from [34] and our proposed Hyper SegFormer model (using the \u201c1x1 Conv, Up, Stride\" variant). We illustrate the advantage of our proposed solution. The baseline and the U-Net based models depend on the matched filter product (also shown in the first column), and as such, if this product doesn't capture the full extent of the methane plume present in the data, these models aren't able to correctly reconstruct it. Meanwhile, our proposed model has access to the original source data and can extract information which would be lost when using the matched filter product. We show this in the first four rows. The last row demonstrates that if the matched filter product contains false detections (confounder materials such as roads, and building outlines), both learned models can filter these out.\nTable II shows the results of several tested model variants for both SegFormer and EfficientViT architectures compared with classical machine learning approach of [34] and baseline method using [21]. We note that all of the deep learning approaches significantly outperform the classical baseline. Furthermore, our proposed HyperSegFormer (ConvUpStride variant) gains over 43.85% in the IoU score (and 27.88% in F1 and 20.7% in AUPRC) over the HyperSTARCOP MF+RGB model. Our model also achieves over 155% in IoU score (and 96.9% in F1 score) over the classical matched filter baseline.These improvements hold over all measured met- rics. We see increasing scores when implementing proposed adaptations in both the SegFormer (from 60.26 F1 score to 74.27) and EfficientViT (from 68.40 F1 score to 72.26) of our HyperspectralViT models.\nIn Figure 7 we evaluate our adapted SegFormer and Ef- ficientViT models on the OxHyperRealCH4 dataset of real methane leak events captured by the EMIT sensor. Figure 8 shows the qualitative results on this dataset. Here, we use the models pre-trained on the synthetic dataset (which uses data of the same input/output shapes and uses the same data normalisation) and test their zero-shot generalisation on the dataset of real events. Furthermore, we fine-tune these models on the smaller dataset of real events. Finally, we also compare these results with training models from scratch only on the dataset of real events. We note that this experiment resembles a real-world scenario, where we want to train models for a newly deployed sensor, which hasn't yet seen enough of these rare events. We try to answer the question of if pre-training on a synthetically created dataset helps the final models detect events on a real but small dataset.\nInterestingly, when evaluating the models pre-trained on the dataset with synthetic events on a new dataset of real methane leak events collected by EMIT, we observe that the zero-shot generalisation performance is low (only two model variants reach the MF baseline). We however see that with fine-tuning for 10 epochs, we are able significantly outperform both the classical baselines and all model variants trained from scratch. Namely the HyperSegFormer (ConvUpStride variant) improves the performance of the MF baseline by 12.9% in F1 score (and 17.1% in IoU) and the same version of the model trained from scratch by over 6.9% in F1 score (and 8.8% in IoU). The HyperEfficientViT (ConvUp variant) outperforms the MF baselines by more than 23% in F1 (31.5% in IoU) and the same model variant trained from scratch by 19.2% in F1 (and 25.6% in IoU). Interestingly, the ConvUp variant slightly outperforms the ConvUpStride variants on this dataset with the EfficientViT models. Overall, we see, that on the task of detecting real methane leak events in EMIT data, we benefit from pre-training our models on a dataset of synthetically created events (especially in cases similar to ours, where the dataset of real events is relatively small)."}, {"title": "B. Methane detection on AVIRIS-NG", "content": "In Table III we evaluate our newly proposed end-to-end models on the benchmark dataset STARCOP [34] of real methane leak events in data from the aerial AVIRIS-NG. We train our proposed end-to-end models from scratch matching the settings used in the original work of [34].\nWhen training our proposed models on the previously created benchmark dataset of real methane leak events in AVIRIS-NG data, we see some interesting mixed outcomes. While the performance of our models drops on F1 score of strong methane leak events (plume emissions larger than 1000 kg/h) in contrast to the HyperSTARCOP \u201cMF+RGB\u201d model, on the metrics that aggregate all event sizes together, we see improvements. Namely, the HypeSegFormer (ConvUp- Stride variant) outperforms the HyperSTARCOP \u201cMF+RGB\u201d baseline by 12.9% in F1 score and by 18% in IoU score. The threshold-less AUPRC metric is also improved by over 17%. The high performance of previous STARCOP models on strong methane leak events could be explained by the fact that the used matched filter method, mag1c, works better for large plumes (as it is iteratively increasing the contrast of positive pixels - this could however lead to lowered performance for weaker events).\nWe have encountered very large gains on the synthetic and real EMIT datasets, however smaller ones on the benchmark dataset of AVIRIS-NG data. We note that perhaps better versions of the MF product could lead to stronger baselines also in the EMIT data. For example [57] explores scene specific adaptation of MF products, and similarly there could be some EMIT specific improvements for calculating MF. This difference could also be caused by the number of used bands while on the STARCOP dataset the end-to-end models use 60 bands, on the new EMIT datasets we use 86 bands which also demonstrate a wider spectral range coverage."}, {"title": "C. Mineral identification", "content": "In Table IV, we show the results of training the proposed machine learning architectures on the mineral dataset OxHy- perMinerals. We show the individual F1 scores for each of the three mineral classes alongside the aggregated scores (which are support weighted by the number of pixels belonging to each class). In Figure 9 we also show examples of model predictions next to the pseudo-ground truth generated by the Tetracorder system. Note the gradual increase of output resolution depending on the used adaptations.\nWhen exploring the results of models trained on the dataset of minerals, we note smaller but sustained gains in perfor- mance due to the proposed adaptations. Namely the HyperSeg- Former (ConvUpStride variant) improves the F1 score by over 3.5% in contrast to the vanilla model (and precision by 7.9%). The HyperEfficientViT demonstrates weaker performance for both base and adapted versions, however the used adaptations do improve this performance - the ConvUp variant scores 3.4% improvement in the F1 score.\nPerhaps better improvement can be seen on the qualitative comparison between the models on Figure 9, where we show the effect of gradually improving output resolution with the HyperSegFormer model. The model variants using \"Up\" and \"Stride\" adaptations (and their combination) show sharper predictions than the vanilla version of the model (as that one is limited to the output size reduced by the factor of 4).\nWe note that our experiments with mineral identification are initial, as we had to rely on pseudo-ground truth labels generated by Tetracorder."}, {"title": "D. Inference times on low compute hardware", "content": "Table V shows the model inference times needed to process a full EMIT scene capture of 1280x1242 px with our models for methane leak detection. These measurements show the IO time needed to load the hyperspectral datacube and the model inference time - for baselines this also includes the time needed to calculate the MF product.\nWe note that the matched filter computation is the slowest step influencing both the classical baselines and the Hyper-STARCOP models. Our proposed end-to-end model variants avoid this step gaining vastly reduced inference time. Namely, our proposed HyperSegFormer ConvUp variant reduces the required time by over 85%, while the ConvUpStride variant reduces the time by over 41% - both compared against the HyperSTARCOP model and effectively also the classical MF baseline.\nWhen considering other devices, the MF calculation remains to be the bottleneck and the end-to-end models are faster. While using the CPU of the Raspberry Pi 3B+ the runtime is slower than on the Unibap flatbed, Raspberry Pi 4B reaches similar performance. Finally, the most powerful CPU of the NVIDIA's Jetson AGX Xavier board shows the fastest run- times of the tested CPU devices.\nIf we use our proposed HyperSegFormer ConvUp model on the Jetson AGX Xavier GPU, we achieve over 47x times speed-up over the Unibap's machine CPU (e.g. 0.64 instead of 30.1 seconds per capture) and 317x times speed-up over the HyperSTARCOP model (203.3 seconds per capture). Using the GPU of the Jetson AGX Xavier board provides the fastest runtimes from all tested devices. We observed further speedup when using the Tensor RT engine (using the \"trtexec\" tool) and compiling the neural network directly on the device with allowed post-training mixed precision quantization (allowing float32 and float16) - in the case of the HyperSegFormer ConvUp, this is by over 70% in contrast to using the Onnxrun- time library (without compilation and without mixed precision mode).\nWhen using the Tensor RT inference engine, we also experience faster inference times when using the adapted EfficientViT architecture instead of the adapted SegFormer architecture - this is consistent with the fact the EfficientViT was optimised for fast inference speed in [2"}]}