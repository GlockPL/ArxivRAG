{"title": "MATEXPERT: DECOMPOSING MATERIALS DISCOVERY BY MIMICKING HUMAN EXPERTS", "authors": ["Qianggang Ding", "Santiago Miret", "Bang Liu"], "abstract": "Material discovery is a critical research area with profound implications for various industries. In this work, we introduce MatExpert, a novel framework that leverages Large Language Models (LLMs) and contrastive learning to accelerate the discovery and design of new solid-state materials. Inspired by the workflow of human materials design experts, our approach integrates three key stages: retrieval, transition, and generation. First, in the retrieval stage, MatExpert identifies an existing material that closely matches the desired criteria. Second, in the transition stage, MatExpert outlines the necessary modifications to transform this material formulation to meet specific requirements outlined by the initial user query. Third, in the generation state, MatExpert performs detailed computations and structural generation to create new materials based on the provided information. Our experimental results demonstrate that MatExpert outperforms state-of-the-art methods in material generation tasks, achieving superior performance across various metrics including validity, distribution, and stability. As such, MatExpert represents a meaningful advancement in computational material discovery using langauge-based generative models.", "sections": [{"title": "INTRODUCTION", "content": "The discovery and design of new materials are central challenges in modern materials science, driven by the need for materials with tailored properties for applications in energy, electronics, and catalysis. Traditional methods for material discovery, such as high-throughput experiments and density functional theory (DFT) simulations, are computationally expensive and often require significant domain expertise to achieve accurate predictions (Miret et al., 2024). Recent advancements in artificial intelligence (AI), particularly large language models (LLMs), have opened new possibilities for automating and accelerating the materials design process (Miret & Krishnan, 2024; Jablonka et al., 2024; Song et al., 2023a;b; Zhang et al., 2024; Ramos et al., 2024).\nLLMs such as GPT-4 OpenAI (2023) have demonstrated remarkable success in natural language processing tasks and have shown potential for application in scientific problems beyond language, including chemistry and materials science Flam-Shepherd & Aspuru-Guzik (2023); Gruver et al. (2024); Schilling-Wilhelmi et al. (2024); Mirza et al. (2024); Del\u00e9tang et al. (2023). For example, LLMs have been used to generate molecular structures Gruver et al. (2024) and predict material properties from textual descriptions Alampara et al. (2024). In the context of material generation, models like Crystal Diffusion Variational Autoencoders (CDVAE) Xie et al. (2022b), as well as fine-tuned LLMs such as LLaMA-2 Gruver et al. (2024), CrystaLLM Antunes et al. (2023), and LM-AC/LM-CH, have made significant progress in generating crystal structures directly from Crystallographic Information Files (CIFs) Antunes et al. (2023). Together, these models aim to accelerate materials design by predicting stable structures and optimizing key properties, such as energy above hull for stability Riebesell et al. (2023)."}, {"title": "RELATED WORK", "content": "The use of AI and large language models (LLMs) in materials science has gained traction as a means to accelerate material discovery, which traditionally relies on computationally intensive methods like DFT simulations Duval et al. (2023); Govindarajan et al. (2024). Miret & Krishnan (2024) provide an extensive review of the limitations of current LLMs in materials science, particularly their inability to handle multi-modal data and iteratively refine structures based on feedback.\nEarly work, such as the Crystal Diffusion Variational Autoencoder (CDVAE) Xie et al. (2022b), generated crystal structures by incorporating physical stability constraints to produce valid periodic materials. Recent work has expanded on CDVAE's approach by exploring alternative representations for diffusion-based models Jiao et al. (2024); Yang et al. (2024); Zeni et al. (2023), as well as flow-matching models Miller et al. (2024). Further advancements include the fine-tuning of LLMs for materials generation. Gruver et al. (2024) fine-tuned LLaMA-2, improving metastable material generation rates, while CrystaLLM Antunes et al. (2023) employs an autoregressive model trained on CIF files to generate inorganic crystal structures, leveraging Monte Carlo Tree Search for refinement. Flam-Shepherd & Aspuru-Guzik (2023) proposed LM-AC and LM-CH, which explore generating 3D molecular structures directly from CIF, XYZ, and PDB formats, pushing the boundaries of LLM applications in material science. In addition to generative models for materials discovery, as well as the development domain-specific LLMs for diverse language tasks materials science (Song et al., 2023b;a; Xie et al., 2023; Zhang et al., 2024), LLMs are also being used to automate the extraction of structured data from scientific literature, streamlining the creation of datasets for machine learning Hira et al. (2024); Mishra et al. (2024); Schilling-Wilhelmi et al. (2024).\nIn summary, while LLMs show great promise, challenges remain in integrating multi-modal data and refining material designs iteratively. Most prior work has focused on unconditioned material generation, but current methods lack the ability to iteratively refine and reason like human experts. Our MatExpert framework addresses this shortcoming by mimicking an expert-driven workflow, employing multi-stage iterative refinement through retrieval, transition, and generation processes, and achieves remarkable performance compared to state-of-the-art methods, particularly in metrics of distances (see Figure 1)."}, {"title": "METHODOLOGY", "content": "Our MatExpert framework simplifies the complex process of material discovery by emulating human experts through three key stages: retrieval, transition, and generation. First, we retrieve a reference material from a database that closely aligns with the desired properties. Next, using a fine-tuned large language model (LLM) with Low-Rank Adaptation (LoRA) Hu et al. (2022), we generate insights on how to modify the reference material to meet the target specifications. Finally, we generate the new material's structure based on the reference and transition steps. The framework employs a T5-based Raffel et al. (2020) model for the retrieval stage and a fine-tuned LLM for the transition and generation stages. A comprehensive illustration of the MatExpert framework pipeline is shown in Figure 3."}, {"title": "FIRST STAGE: RETRIEVAL", "content": "The retrieval stage is the foundational step of our framework, aimed at identifying the material in the database that best matches the desired property description provided by the user.\nTo bridge the gap between the representation of the desired properties and the relevant material, we apply contrastive learning. Contrastive learning Chen et al. (2020) is a self-supervised method that learns representations by bringing positive pairs closer together while pushing negative pairs farther apart in the embedding space. Contrastive learning was first applied in the domain Radford et al. (2021); Singh et al. (2022), recent works have successfully applied similar methods to domains to materials design, such as molecular Liu et al. (2022) and protein modeling Xu et al. (2023). In the context of material retrieval, we use this method to embed material properties and structures in a shared space, enabling the efficient retrieval of materials that closely match specific queries.\nTo implement this, we employ a contrastive learning framework (see Figure 2), using two parallel T5-based encoders to process and embed both property and structure descriptions. The property descriptions include key characteristics such as composition, space group number, formation energy, band gap, and energy above hull. Structural descriptions are extracted using RoboCrystallographer, which provides detailed linguistic descriptions of the structure for each material.\nThe primary objective is to ensure that the embedding of the property description (qi) is closely aligned with the embedding of the corresponding structure description (m\u2081), while distinctly separating it from embeddings of other materials (mk). To quantitatively enforce this alignment, a contrastive loss function is defined as:\n$L = - log \\frac{exp(sim(qi, mi)/\\tau)}{\\sum_{k=1}^{N} exp(sim(qi, mk)/\\tau)}$ (1)\nHere, sim(qi, m\u2081) denotes the similarity between the property and structure embeddings, \\tau is a temperature parameter that controls the sharpness of the distribution, and N is the size of negative sampling. The output of this stage is the best matching material, which serves as a candidate for further refinement."}, {"title": "SECOND STAGE: TRANSITION", "content": "The transition stage focuses on developing a detailed and viable method for modifying the retrieved material to meet the desired properties. This stage bridges the gap between the existing material and the target material by outlining specific structural or compositional changes required to achieve the desired specifications.\nTo build a model capable of generating transition pathways that describe how to transform the existing reference material into the desired target material, two key steps are necessary: (1) constructing a training dataset consisting of <source material, transition pathways, target material> triples, and (2) fine-tuning a model that can produce transition pathways given an source material as a reference.\nIn this stage, we utilize two LLMs in a sequential process. First, during the training phase, we employ GPT-4 to generate ground-truth transition pathways, which provide comprehensive, step-by-step instructions for transforming the retrieved material to meet the target properties. Subsequently, we fine-tune a second, smaller LLM using these ground-truth pathways. In practical applications, including our experiments, this fine-tuned LLM is deployed to generate the potential transition pathways, offering a more efficient solution while maintaining accuracy.\nGround-Truth Pathway Generation with GPT-4: During the training phase, we utilize GPT-4 to generate detailed transition pathways by providing it with carefully designed prompts. These prompts are constructed to ensure that GPT-4 receives all necessary information about both the source material and the target material, enabling it to produce accurate and comprehensive modification pathways. An example template of such a prompt is:\nI have two materials: <formula_source> and <formula_target>.\nBased on the descriptions and properties of the two materials\nbelow, can you summarize the main reasons for the differences\nin properties when transitioning from <formula_source> to\n<formula_target>? The description of <formula_source>:\n<description_source>. The description of <formula_target>:\n<description_target>.\nFine-Tuning the Second LLM: The ground-truth transition pathways generated by GPT-4 form the basis for the supervised fine-tuning of a second LLM. This model is trained to learn and replicate the detailed modification steps provided by GPT-4, allowing it to generate potential transition pathways in practical applications. An example prompt template used for fine-tuning is:\nI am looking to design a new material with the following\nproperties: <property_list>. The closest existing material\nI found in the database is <formula_source>, which has\nsimilar properties. Below is the structure description of\n<formula_source>. \\n <description_source> \\n How should I modify\n<formula_source> to develop a new material with the desired\nproperties?"}, {"title": "THIRD STAGE: GENERATION", "content": "The generation stage is focused on producing the CIF (Crystallographic Information File) representation of the predicted material. Specifically, we first generate a detailed description of the lattice vectors and atomic coordinates, which we refer to as the ALX representation. This ALX representation is then automatically converted into CIF format using the pymatgen library, which serves as the final output of our framework.\nFine-Tuning: The generation process builds upon the conversation initiated during the earlier stages of the framework. The output from the transition stage serves as input for the generation stage. An additional prompt is introduced to extend the conversation, guiding the model to generate the ALX representation based on the transition pathway provided in the previous stage. The prompt for the generation stage is:\nBased on the information, for the new material, could you generate\na description of the lengths and angles of the lattice vectors\nand then the element type and coordinates for each atom within the\nlattice?\nALX Representation: After fine-tuning, the LLM generates the ALX representation for the material based on the transition pathways (as shown in the final response in Figure 3). This representation includes:\n\u2022 Lattice Vectors: The lengths and angles of the lattice vectors that define the unit cell of the material.\n\u2022 Atomic Coordinates: The precise atomic types and their coordinates within the lattice, ensuring that the generated structure aligns with the desired material properties.\nFinal Output in CIF Format: Once the ALX representation is generated, it is converted into CIF format using the pymatgen library. The CIF file contains the complete structural information of the predicted material and serves as the final output of the generation stage. This file can be utilized for further computational studies, experimental validation, or integration into material databases."}, {"title": "DATA COLLECTION & PRE-PROCESSING", "content": "To rigorously evaluate the performance of MatExpert, we assembled a comprehensive dataset from the NOMAD database Scheidgen et al. (2023), utilizing its API. This extensive dataset serves as the large-scale testbed for assessing the capabilities of our framework.\nData Collection from NOMAD NOMAD (Novel Materials Discovery) Scheidgen et al. (2023) is a vast repository of material data, providing access to millions of entries with detailed structural and property information. For our data collection, we applied specific filters to refine the dataset. We excluded materials containing inert elements to focus on those with more active chemical properties, which are of greater interest in materials science research. Additionally, we restricted the dataset to materials whose structures were simulated using the Vienna Ab initio Simulation Package (VASP) Kresse & Hafner (1993), ensuring high-quality computational data. To further improve computational efficiency during training and analysis, we also excluded materials with more than 30 atoms in their original structures.\nBy applying these filtering criteria, we obtained a dataset of 2, 886, 120 materials. This large-scale dataset provides a rich foundation for training and evaluating our models, enabling us to explore a wide range of material compositions and properties.\nPre-processing Each filtered material was converted into CIF format, ensuring the structural integrity of the original data was preserved. CIF files are widely used in materials science to represent crystal structures and are compatible with various computational tools, facilitating seamless integration into our framework.\nIn addition to structural data, we enriched our dataset with linguistic descriptions of each material, generated using RoboCrystallographer Ganose & Jain (2019), a tool that automatically produces human-readable summaries of crystallographic information. This step allows for the integration of language-based processing within our framework, leveraging the strengths of large language models. We also extracted key material properties\u2014such as formation energy, total energy, and elemental composition\u2014using the M3GNet Chen & Ong (2022) and PyMatgen Ong et al. (2013) libraries in Python. These properties provide essential inputs for condition-based material design and evaluation within MatExpert.\nBy leveraging this meticulously curated dataset, we ensure that MatExpert is evaluated on a robust and diverse set of large-scale materials, facilitating comprehensive assessments of its performance and effectiveness."}, {"title": "EXPERIMENTS", "content": null}, {"title": "DATASETS AND BASELINES", "content": "Material Project: We leverage datasets from the Materials Project database similar to prior studies Xie et al. (2022b); Gruver et al. (2024). The MP-20 dataset (Xie et al., 2022b) for unconditional generation contains 45,231 stable materials, filtered to exclude structures with over 30 atoms per unit cell for computational efficiency."}, {"title": "UNCONDITIONAL GENERATION", "content": "In the unconditional generation task, we aim to assess the ability of MatExpert to produce novel and stable material structures without any specific property constraints. This stage serves as a benchmark for evaluating the intrinsic generative capabilities of the model, focusing on the validity, diversity, and stability of the generated materials. For unconditional generation, we randomly select a material from the database during the first stage of MatExpert.\nThe comparison results are shown in Table 1. As evident from the table, the MatExpert family with LLaMA-3 (Dubey et al., 2024) outperforms all other methods across all metrics except metrics of coverage. Traditional models such as CDVAE, LM-CH, and LM-AC excel in coverage and structural validity. When compared to the state-of-the-art LLM-based model Crystal-LLM, our MatExpert method consistently outperforms Crystal-LLM across all settings, including the same model size and temperature as well as DFT-based stability. Notably, MatExpert achieves remarkable results in distribution metrics, benefiting from the transition from an existing material in the database. While Crystal-LLM may struggle with the hallucinations of LLM, thus generating out-of-distribution materials."}, {"title": "CONDITIONAL GENERATION", "content": "In the conditional generation task, we evaluate MatExpert's ability to generate material structures that meet specific property constraints using the large-scale NOMAD dataset. This task is crucial for practical applications, where researchers often require materials with targeted properties, such as specific band gaps, space groups, and chemical compositions, etc. As shown in Table 2, MatExpert significantly outperforms the baseline Crystal-LLM for all metrics. MatExpert also excels in property distribution, achieving much lower Wasserstein distances, which reflects its ability to generate materials with property distributions closely aligned with the training data. Notably, the stability metric like the energy above the hull is treated as an input property in this task and is not separately evaluated as a performance metric.\nTo rigorously evaluate the effectiveness of MatExpert in satisfying user-defined property constraints, we also conducted additional experiments aimed at quantifying the percentage of generated materials that meet specific property criteria set by users.\nWe selected a set of common property constraints, including:\n\u2022 Formula: We parse the composition from the generated CIF. We show the results in three levels by atom numbers: \u2264 5 atoms, \u2264 10 atoms, and \u2264 15 atoms.\n\u2022 Energy above hull: We use M3GNet Chen & Ong (2022) to estimate the energy above hull. We bin stability as $\u00ca^{hull} < 0.1$ for metastable and $\u00ca^{hull} \\geq 0.1$ for unstable.\n\u2022 Space Group Number: We use PyMatgen's SpacegroupAnalyzer with a precision of 0.2 angstroms. Ong et al. (2013)\nFor each constraint, we generated 10,000 materials using MatExpert and the baselines. We then calculated the percentage of materials that met the specified property criteria. The results of the condition satisfaction experiments are summarized in Figure 4. MatExpert consistently outperformed the baselines across all property constraints, demonstrating its superior capability to generate materials that meet user-defined specifications."}, {"title": "ANALYSIS", "content": null}, {"title": "ANALYSIS OF DIVERSITY AND NOVELTY", "content": "To evaluate the MatExpert framework, we use metrics that assess the diversity and novelty of generated materials. Following Xie et al. (2022b), diversity is calculated as the pairwise distance between samples using a featurization of structure and composition. Novelty is determined by comparing the distance to the nearest element of the training set for each sample, with a sample considered novel if this distance exceeds a threshold. We also assess the overall novelty, defined as having either a new structure or composition. To further understand the gaps between MatExpert and baselines, we illustrate the score of each metric normalized to testing samples in Figure 5. As we can see, MatExpert consistently achieves high-level scores in both structure and composition diversity compared to CDVAE and various Crystal-LLM configurations. This indicates MatExpert's superior ability to explore a broad chemical space. In terms of novelty, MatExpert demonstrates a strong capacity to generate structures and compositions not found in existing databases, as evidenced by its high novelty scores. Notably, Crystal-LLM faces challenges in balancing model size and novelty, with larger models exhibiting lower novelty. In contrast, MatExpert consistently maintains high novelty regardless of model size."}, {"title": "ABLATION STUDIES", "content": "To understand the contribution of different components of our framework, we conduct ablation studies by systematically removing or modifying key elements of MatExpert and observing the impact on performance.\nImpact of Transition (CoT) Stage The transition stage, which involves generating modification pathways using Chain of Thought (CoT) reasoning Wei et al. (2022), plays a crucial role in the overall performance of MatExpert. To evaluate the impact of this component, we conducted experiments where the CoT reasoning was removed entirely. The results in Figure 6 highlight its effectiveness of CoT reasoning.\nImpact of Retrieval Stage We assess the impact of the retrieval stage with the contrastive learning framework. By removing the retrieval stage, we observe a noticeable decline in the quality of the retrieved materials (See Figure 6). This confirms that the retrieval stage is crucial for identifying high-quality reference materials, which are essential for effective subsequent modifications.\nChoices of Encoders in Retrieval Stage In the retrieval stage, we explored various encoder architectures to determine their impact on the quality of material retrieval (See Table 3). We compared T5-based encoders with BERT Devlin et al. (2019). Our findings indicate that T5-based encoders consistently outperform BERT, providing more accurate embeddings for both property and structure descriptions, making it more effective for capturing the most similar material aligned with the desired properties in the database."}, {"title": "CASE STUDY", "content": "To showcase the capabilities of MatExpert, we present a comprehensive example of conditional generation under property constraints in Figure 7. This case illustrates the process of generating a predicted material and its adherence to the specified properties."}, {"title": "CONCLUSION", "content": "In this work, we introduced MatExpert, a novel framework leveraging LLMs and contrastive learning for the material discovery process. By emulating the workflow of a human materials science expert, MatExpert integrates retrieval, transition, and generation stages to design new materials. Our experiments show that MatExpert significantly outperforms state-of-the-art methods in material generation tasks, thereby exhibiting its potential to become a scalable tool for accelerating material discovery with higher-quality crystal generation."}, {"title": "REPRODUCIBLITY", "content": null}, {"title": "ENVIRONMENT", "content": "The experiments were conducted on machines with the following specifications.\nMachine 1 for MatExpert with Llama-2 8B and Llama-3 8B:\n\u2022 Hardware: NVIDIA A5000 GPU (24 GB on-board memory) GPU x 4, AMD Ryzen Threadripper PRO 3975WX CPU, 256 GB RAM\n\u2022 Software: Ubuntu 22.04, Python 3.11, PyTorch 2.3.1+cu121, CUDA 12.4\nMachine 2 for MatExpert with Llama-2 70B and Llama-3 70B:\n\u2022 Hardware: NVIDIA L40S GPU (48 GB on-board memory) x 4, INTEL(R) XEON(R) GOLD 6526Y x 2, 512 GB RAM\n\u2022 Software: Ubuntu 22.04, Python 3.11, PyTorch 2.3.1+cu121, CUDA 12.4\nThe list of environment requirements for the Python library will be made publicly available alongside the release of the open-source code of MatExpert."}, {"title": "IMPLEMENTATION DETAILS", "content": "The proposed MatExprt framework consists of two training stages: 1) Training T5-based encoders of the contrastive learning framework in the stage Retrieval. 2) Finetuning the Llama-2/Llama-3 based LLMs in the stage Transition and stage Generation.\nTraining T5-based encoders In the retrieval stage, two T5-based encoders are employed. The model uses the \"t5-base\" architecture in Huggingface Wolf (2019) as its foundational structure. The training process is governed by a learning rate of 0.0001. It undergoes training for 100 epochs with early stopping. A temperature parameter of 0.1 is utilized, which affects the NT-Xent loss function, a common choice for contrastive learning. The batch size is set to 32 and gradient accumulation occurs over 8 steps, effectively simulating a larger batch size and allowing for more stable updates.\nFinetuning Llama family In the transition and generation stages, the Llama family of models is finetuned. Two variants are used: Llama-3 8B and Llama-3 70B, both with specific architectures designed for instruction-based tasks named \u201cMeta-Llama-3-8B-Instruct\u201d and \u201cMeta-Llama-3-70B-Instruct\" in Huggingface Wolf (2019). The learning rate is set to 0.0001. Each device processes a batch size of 1, with an overall batch size of 4. The 8B model is trained for 70 epochs, while the 70B model undergoes 10 epochs, reflecting differences in their complexity and training needs. The AdamW optimizer is employed for parameter updates. Sequence lengths differ, with the 8B model handling sequences of 2048 tokens, and the 70B model processing 1024 tokens limited by the on-board memory of GPUs. LoRA settings include a rank of 8, an alpha of 16, and a dropout rate of 0.1, facilitating efficient adaptation. A warmup ratio of 0.1 helps stabilize initial training and gradient accumulation is set to 8 steps for both models, ensuring robust learning."}, {"title": "EFFICIENCY", "content": "We utilize the open-source LLM training framework named LLaMA-Factory Zheng et al. (2024) to finetune the LLMs. With the advantages of LoRA Hu et al. (2022) and FlashAttention Dao et al. (2022) mechanism, it takes around 30 and 120 hours to fine-tune Llama-3 8B and Llama-3 70B in MatExpert, respectively.\nThe generation speed of MatExpert on our machines is as follows:\n\u2022 MatExpert with Llama-3 8B: ~40 minutes per 10,000 samples.\n\u2022 MatExpert with Llama-3 70B: ~5 hours per 10,000 samples."}, {"title": "DATASETS DETAILS", "content": "Two datasets are included in our experiments: Material Project and NOMAD for unconditional generation and conditional generation, respectively.\nMaterial Project This dataset also named as MP-20 consisting of 45,231 materials is publicly available and was first proposed in Ong et al. (2013). We follow the data-split setting proposed in Gruver et al. (2023) and Xie et al. (2022a).\nNOMAD This large-scale dataset consisting of 2,886,120 materials is collected by ourselves and will be publicly available. We collect the datasets via NOMAD API as the following script:\nWe filter out the materials with elements in excluded_elements and only include the materials with structures computing by VASP Kresse & Hafner (1993).\nThen we further convert the structure in JSON to CIF files via the following script:"}, {"title": "EVALUATION METRICS", "content": null}, {"title": "VALIDITY", "content": "Validity is evaluated based on the structural and compositional integrity of the generated materials. Following Xie et al. (2022a), a structure is considered valid if the shortest distance between any pair of atoms is greater than 0.5 \u00c5, ensuring atomic stability. Furthermore, the overall charge of the material must be neutral to satisfy chemical feasibility. These criteria ensure that the generated materials conform to realistic physical and chemical constraints."}, {"title": "COVERAGE", "content": "Following the methods outlined in Xie et al. (2022a) and Gruver et al. (2023), we measure coverage using two metrics: COV-R (Recall) and COV-P (Precision). These metrics quantify how well the generated materials resemble the ground truth materials in the test set. COV-R (Recall) measures the percentage of ground truth materials accurately represented by the generated set, while COV-P (Precision) evaluates the quality of the generated materials in replicating the true material diversity. These metrics ensure that the generated materials not only capture the diversity of the test set but also maintain high-quality representations. Both validity and coverage are computed over 10,000 materials randomly sampled from the generated set."}, {"title": "DISTRIBUTION", "content": "The distribution of properties is compared between the generated materials and the test set using the Wasserstein distance. The key properties evaluated are density (\u03c1, measured in g/cm\u00b3) and the number of unique elements (Nel). The property distribution is computed over 1,000 valid materials, randomly sampled from those that pass the validity test. This method ensures that the generated materials exhibit realistic physical properties comparable to those in the test set."}, {"title": "DIVERSITY", "content": "Diversity is assessed by calculating the pairwise distances between samples, based on structural and compositional features as described by Court et al. (2020). This metric quantifies how distinct each generated sample is relative to others in the dataset, offering insights into the variety of structures and compositions. Diversity is specifically measured on samples deemed metastable by M3GNet, as these are more likely to contribute meaningful variation. All diversity values are normalized against corresponding metrics from the test set, providing a clear comparison of the underlying data distributions."}, {"title": "NOVELTY", "content": "Novelty is calculated by comparing each generated sample to its nearest neighbor in the training set. A sample is considered novel if its nearest neighbor exceeds a predefined threshold distance. Specifically, we use a structural distance cutoff of 0.1 and a compositional distance cutoff of 2. Novelty is assessed both in terms of structure and composition, with overall crystal novelty determined by the presence of a new structure or composition. These metrics provide insights into the uniqueness of the generated samples, particularly those identified as metastable. Novelty values are normalized by corresponding values for the test set to effectively convey the characteristics of the data distribution."}, {"title": "STABILITY", "content": "We assess the stability of materials using a combination of machine learning potentials and density functional theory (DFT) to ensure a consistent evaluation framework. Specifically, we employ the M3GNet model Chen & Ong (2022), trained on total energy data from VASP calculations within the Materials Project dataset. This aligns the results with established correction schemes and absolute energy values. For consistency with Materials Project settings and prior works Xie et al. (2022a); Gruver et al. (2023), including the PBE functional and DFT/DFT+U, we perform a single relaxation for each candidate structure using the default MPRelaSet parameters.\nTo determine the percentage of metastable compounds, we first filter out samples that fail basic structural and compositional validity checks. The remaining samples are then relaxed using M3GNet to obtain final relaxation energies. The stability calculation includes the validity rate from initial filtering and the rate of compounds with relaxed hull energy $\u00ca^{hull} < 0.1$. For stable materials, those identified as metastable by M3GNet undergo further DFT relaxation, and the percentage with Ehull < 0.0 is reported. This comprehensive approach integrates both machine learning and traditional DFT methods for a robust stability evaluation."}, {"title": "CASES OF UNCONDITIONAL GENERATION", "content": "Please see Figure 8."}, {"title": "CASES OF CONDITIONAL GENERATION", "content": "Please see Figure 9."}]}