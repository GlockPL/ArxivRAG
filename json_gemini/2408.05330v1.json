{"title": "Neural Machine Unranking", "authors": ["Jingrui Hou", "Axel Finke", "Georgina Cosma"], "abstract": "We tackle the problem of machine unlearning within neural information retrieval, termed Neural Machine UnRanking (NuMuR) for short. Many of the mainstream task- or model-agnostic approaches for machine unlearning were designed for classification tasks. First, we demonstrate that these methods perform poorly on NuMuR tasks due to the unique challenges posed by neural information retrieval. Then, we develop a methodology for NuMuR named Contrastive and Consistent Loss (CoCoL), which effectively balances the objectives of data forgetting and model performance retention. Experimental results demonstrate that CoCoL facilitates more effective and controllable data removal than existing techniques.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine unlearning is the process of selectively removing specific data points from a trained machine-learning model [1, 2]. This task has gained significant attention in recent years as it addresses concerns regarding data privacy and model adaptability [3, 1, 4, 2].\nIn this work, we focus on neural ranking models nowadays used for information retrieval (IR), i.e., on neural IR. In this context, machine unlearning may be needed for\na. addressing data-privacy concerns, e.g., for deleting data of a user who has exercised their 'right to be forgotten' [1, 5];\nb. selectively deleting (e.g. outdated) information [6, 7]. For instance, an IR system querying \u201cWhat are the EU member states?\u201d might need to exclude \u201cUK\u201d from its results post-2020 [8], illustrating a practical application of machine unlearning in IR systems.\nIt is therefore important to design methods for machine unlearning that can effectively deal with neural IR. Prominent existing model- and task-agnostic unlearning methods like Amnesiac Unlearning [9, 10] or Negative Gradient Removal [11, 12] (NegGrad) could be employed. However, these have been primarily designed for classification scenarios where it is typically possible to unlearn a class by deliberately damaging the model accuracy on the samples within that class; and Figure 1 illustrates that such unlearning strategies perform poorly in neural IR in the sense that reducing the performance of these models on the 'forget set' (i.e. on the data to be removed) incurs a severe performance loss on the 'retain set' (i.e. on the remaining data) and on test sets. We conjecture that this is due to strong dependencies in neural IR models, where removing individual data points disrupts learned patterns [13, 11, 14]. Another model-agnostic unlearning method is a teacher-student framework [15, 16] which was likewise originally designed for classification tasks. However, as we discuss in detail in Section II-C, a na\u00efve application of this approach to neural IR fails because the relevance scores generated by neural ranking models cannot easily be normalized."}, {"title": "II. BACKGROUND AND PROBLEM DEFINITION", "content": "In this section, we provide, to our knowledge, the first formalisation of the task of machine unlearning within neural IR. We also explain why a na\u00efve application of the teacher-student framework does not work in this context.\nLet $W \\subset \\mathbb{R}^d$ the parameter space and let $S$ be the universe of possible datasets. Let $M: S \\rightarrow W$ be a learning algorithm which maps a training set $S \\in S$ to a model $w \\in W$. Learning algorithms may be random but we do not make this explicit in the notation. The trained model is then:\n$M_{train} := M(S) := arg \\min_{w \\in W} L_S(w)$,\nwhere, employing an empirical-risk-minimization approach, we typically have $L_S(w) := \\Sigma_{(x,y)\\in S} l(f_w(x), y)$, for some suitable learning loss function $l$.\nGiven the training set $S$, let $F \\subset S$ be the forget set which contains a subset of data points in $S$ to be unlearned; and let $R := S \\setminus F \\in S$ be the retain set which contains the remaining data points. This defines the retrained model\n$M_{retrain} := M(R)$.\nLet $U : W \\times S \\times S \\rightarrow W$ be a (potentially random) unlearning algorithm for $M$ which defines the unlearned model\n$M_{unlearn} := U(M_{train}, F, R)$.\nUnlearning algorithms are normally expected to ensure that the unlearned model closely approximates the retrained model, i.e., $M_{unlearn} \\approx M_{retrain}$, whilst the computational cost of unlearning - starting from $M_{train}$ - should be less than retraining from scratch on R [19, 20]. While mimicking $M_{retrain}$ aligns with Goal a., it does not enable controllable forgetting (Goal b.). Therefore, we will base our unlearning approach on the teacher-student framework (also known as knowledge distillation) from Chundawat et al. [15], which can achieve pre-specified degrees of forgetting by implementing different distillation strategies.\nInformally, the teacher-student framework specifies the unlearning algorithm U as using stochastic gradient-descent - initialised from $M_{train}$ - to minimize (or at least decrease)\n$L_{M_F,F}(w) + L_{M_R,R}(w)$,\nwhere, for any dataset A, the objective $L_{M,A}(w)$ penalises the difference between predictions made by the student model $w \\in W$ (which is unlearning) and some fixed teacher model M on A and is typically specified as follows.\nSince $M_{unlearn}$ should perform similarly to $M_{retrain}$ on R which in turn should perform similar to $M_{train}$ (on R), it is common to take $M_R := M_{train}$ in (1). This can be interpreted as training $w$ to obey the 'competent' teacher model $M_{train}$ on R [21, 22, 15, 16].\nSince $M_{unlearn}$ should achieve controllable forgetting, i.e., achieve a pre-specified performance $\\delta$ that is worse than $M_{train}$ on F, it is common to take\n$L_{M_F,F}: -L_{M_{train}, F}$,\nin (1) which can be viewed as training w to disobey the 'competent' teacher $M_{train}$ on F [16]; and then to stop the gradient-descent iterations when the accuracy on the forget set has dropped to the target level $\\delta$. Alternatively, if the goals is that the unlearned model should perform similarly to $M_{init} := M(\\emptyset)$ on F, one could simply take $M_F : M_{init}$ in (1), which can be viewed as training w to obey the 'incompetent' teacher model $M_{init}$ on F [15]. Of course, $M_{init}$ could be replaced by another model, e.g., by an adversarial model trained on with noisy data [21, 12, 22]."}, {"title": "B. Unlearning in neural information retrieval", "content": "The goal of information retrieval (IR) is to identify and retrieve documents in response to a search query [23]. Let Q be the universe of potential queries and let D be the universe of potential documents. Queries are user inputs or requests for specific information, typically in the form of words, phrases, or questions; documents refer to units of content, such as web pages or articles.\nThen a dataset for (neural) IR $S \\in S$ consists of tuples (x, y), where\n\u2022 x = (q, d) \u2208 Q \u00d7 D is a query-document pair;\n\u2022 y \u2208 R is the ground-truth relevance score of (q, d).\nWhen y exceeds some threshold, d is regarded as a positive document for q, indicating that d is highly relevant to q.\nA neural-ranking model w \u2208 W is then trained to predict a relevance score $f_w(x) \\in \\mathbb{R}$ of some query-document pair x = (q, d). Relevance scores output by neural-ranking models are used to rank documents. Each document associated with a query is sorted by its score in (descending) order, so that higher scores correspond to a higher rank and thus earlier positions in the search results. Neural Machine UnRanking (NuMuR) is then the task that this model unlearns either queries or documents (or both):\n\u2022 Query removal refers to deleting a set of queries $Q'$ (and associated relevance scores) from the dataset. In this case, $F := {((q, d), y) \\in S | q \\in Q'}$.\n\u2022 Document removal refers to deleting a set of documents $D'$ (and associated relevance scores) from the dataset. In this case, $F := {((q, d), y) \\in S | d \\in D'}$.\nOne of the difficulties encountered in NuMuR is that certain queries or documents may appear simultaneously in the retain set R and in the forget set F. For example, assume that the query: \"The best one-week itinerary for a trip to London\" is associated with two recommended itineraries (i.e., documents). If one itinerary's owner recalls their answer, we must unlearn one query-document pair whilst maintaining the other.\""}, {"title": "III. PROPOSED NEURAL MACHINE UNRANKING METHODOLOGY", "content": "In this section, we propose a new teacher-student framework for NuMuR, called Contrastive and Consistent Loss (CoCoL). To address the three challenges discussed at the end of the last section, CoCoL introduces the following elements.\n1) To overcome the problem of using an 'incompetent' teacher model (such as $M_{init}$) in the presence of unnormalized relevance scores, we simply attempt to reduce the relevance scores on the forget set (relative to the trained model, $M_{train}$) whilst seeking to maintain the relevance scores everywhere else.\n2) To enable controllable forgetting despite the fact that the relevance scores are not normalized, we stop the unlearning iterations not when the average relevance score reach some target level but when\n$\\frac{1}{|Q_F|} \\Sigma_{q\\in Q_F} rank_w(q)$\nis approximately equal to some pre-specified target $\\delta > 0$. Here, $Q_F := {q\\in Q | \\exists (\\langle q', d\\rangle, y) \\in F : q' = q}$ is the set of distinct queries in the forget set.\n\u2022 In query removal, $rank_w(q)$ denotes the rank of the first relevant document for query q among all documents allocated to query q for ranking, evaluated by Model w.\nHere, (2) simplifies to the classical mean reciprocal rank (MRR) as described by Craswell [24].\n\u2022 In document removal, $rank_w(q)$ represents the rank of the first document marked for removal. This may differ from the rank of the first relevant document. For example, if Model w ranks the documents for Query q as $[d_1, d_3, d_4, d_2, . . .]$, where $d_1$ is the first relevant but $d_2$ is the first marked for removal, the reciprocal rank is recalculated as 0.25. While this differs from the classical MRR, we retain the 'MRR' notation for consistency in evaluation metrics.\n3) To ensure that reducing the model accuracy on the forget set does not inadvertently damage the model performance on the entangled set, we pair a 'forgetting sample' with a random selection of a sample from the corresponding entangled set, as explained in the next section."}, {"title": "B. Objective", "content": "CoCoL uses gradient steps, started from the trained model $M_{train}$, to decrease an objective of the form\n$L_{M_{F \\cup E},F \\cup E}(w) + L_{M_D,D}(w)$,\nwere $L_{M,A}(w)$ is again some objective which penalises the discrepancy between w and some reference model M on some dataset A. Note that this objective differs from the standard teacher-student framework (1) in that the entangled set is moved into the first component. Note also that we say 'decrease' rather than 'minimize' because the unlearning is simply stopped when a pre-defined level of forgetting has been achieved (see below for details).\nThe components $L_{M_{F \\cup E},F \\cup E}(w)$ and $L_{M_D,D}(w)$ are implicitly defined through update rules which we now specify, where for some query-document pair x = (q, d):\n$\\Delta_{x}^{w,M} := \\frac{af_M(x) - f_w(x) + \\beta}{f_M(x) + f_w(x)}$\nmeasures the discrepancy between the relevance score $f_M(x)$ returned by some fixed reference ('teacher') model M and the relevance score $f_w(x)$ returned by the 'student' model w. Specifically, note that (3) decreases if the relevance score of the teacher model M is much higher than that of the student model w. In (3), $\\alpha > 0$ and $\\beta \\geq 0$ are tuning parameters whose choice will be discussed at the end of this section. The update rules are then as follows.\n1) Contrastive loss: implicit definition of $L_{M_{F \\cup E},F \\cup E}(w)$. We employ a contrastive loss to modify the student model w such that it generates lower relevance scores on the forget set than the trained model Mtrain whilst ensuring that the relevance scores on the entangled set are maintained. Specifically, at each iteration, we randomly select a sample (x, y) = ((q, d), y) \u2208 F from the forget set and a second sample (x', y') = ((q', d'), y') \u2208 $E_{q,d}$, where $E_{q,d} := {(\\langle q\", d\"\\rangle, y\") \\in E | q\" = q or d\" = d}$ contains the samples that are entangled with (x, y), and then take a gradient step which reduces\n$Relu(\\Delta_{w,M_{train}}^{w, M_{train}}(x)) + |\\Delta_{x'}^{1,0 w,M_{train}}(x')|$. Here, ReLu(z) := max(0, z). If $E_{q,d} = \\emptyset$ then we take the second term in (4) to be zero.\n2) Consistent loss: implicit definition of $L_{M_D,D}(w)$. We employ a consistent loss to modify the student model w such that it generates relevance scores on the disjoint set that are similar to those from the trained model Mtrain. Specifically, at each iteration, we randomly select a positive (i.e., relevant) sample $(x^+, y^+) = (\\langle q^+, d^+\\rangle, y^+) \\in D$ and a negative (i.e., irrelevant) sample $(x^-, y^-) = (\\langle q^-, d^-\\rangle, y^-) \\in D$ from the disjoint set and then take a gradient step which reduces\n$|\\Delta_{w,M_{train}}^{1,0}(x^+) | + |\\Delta_{w,M_{train}}^{1,0}(x^-)|$.\nIn summary, our CoCoL unranking approach is illustrated\nThe efficacy of CoCoL depends on the appropriate setting of parameters \u03b1 and \u03b2 in (4). From our empirical studies, we have found that \u03b1 = 1 and \u03b2 = 0 works as a suitable default for most neural ranking models and datasets. However, adjusting \u03b1 to a smaller value and \u03b2 to a larger one can expedite forgetting, with minimal impact on the retain set. Specifically, for pretrained models, we recommend \u03b1 \u2248 1 and \u03b2 as a small integer, such as 5. For methods based on word embeddings, a significant reduction in \u03b1 for example, to 0.01 can be beneficial. Detailed experimental results pertaining to various neural ranking models will be discussed in the subsequent sections."}, {"title": "D. When to stop", "content": "With appropriate settings of \u03b1 and \u03b2 in (4), alternating between (4) and (5) ensures stable performance on both entangled and disjoint datasets, while performance on the forget set will progressively decline. Therefore, the optimal time to stop unlearning is when the performance on the forget set as measured by (2) reaches the pre-specified level \u03b4 > 0."}, {"title": "IV. EXPERIMENTS", "content": "Currently, there are no existing IR datasets specifically designed for machine unlearning research. To address this, we propose curating a dataset derived from established benchmark IR datasets. In selecting the appropriate datasets for NuMuR, our selection criteria focused on datasets that feature extensive one-to-many query-document pairings, essential for evaluating NuMuR methodologies. An in-depth review of resources listed on ir-datasets.com identified two sources that fulfil these requirements: MS MARCO [18] and TREC Complex Answer Retrieval (TREC CAR) [25]. These sources were selected due to their large sample sizes and the presence of overlapping queries and documents. The sample ratio of the forget set, entangled set, and disjoint set is approximately 1 : 1 : 2.\nGiven the limited studies that exist in NuMuR, identifying comparable baselines is challenging. Therefore, the following task- and model-agnostic unlearning methods were selected as baselines:\n1) Amnesiac [9, 10] continues training on Mtrain but with mislabeled samples in the forget set1. To adapt this idea to NuMuR, we intentionally score several (q, d) pairs marked as 'negative' higher than those labelled as 'positive' in the forget set and then keep training Mtrain on the revised forget set and the original entangled set.\n2) NegGrad, short for 'negative gradient', updates a learned model in the reverse direction of the original gradient on forget-set samples [11, 12].\nWe also report results for retrain, i.e., for retraining from scratch on only the retain set [19, 1, 20, 2] as this can be considered the 'idealised' approach (unless 'controllable forgetting' is sought). However, recall that as explained in Section II-A, obtaining Mretrain is typically prohibitively costly."}, {"title": "V. CONCLUSION", "content": "In an era where data privacy and dynamic information landscapes are paramount, this study focuses on the field of machine unlearning, specifically within the context of neural ranking models for information retrieval (IR) systems. This research introduced the concept of Neural Machine UnRanking (NuMuR), presenting a novel method (Contrastive and Consistent Loss (CoCoL)) that effectively balances the delicate trade-off between unlearning specific information and maintaining the overall performance of neural ranking models. CoCoL, particularly effective with pretraining-based neural ranking models, represents an advancement in addressing the unique challenges posed by neural ranking tasks in IR systems."}]}