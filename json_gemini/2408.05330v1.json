{"title": "Neural Machine Unranking", "authors": ["Jingrui Hou", "Axel Finke", "Georgina Cosma"], "abstract": "We tackle the problem of machine unlearning within neural information retrieval, termed Neural Machine UnRanking (NuMuR) for short. Many of the mainstream task- or model-agnostic approaches for machine unlearning were designed for classification tasks. First, we demonstrate that these methods perform poorly on NuMuR tasks due to the unique challenges posed by neural information retrieval. Then, we develop a methodology for NuMuR named Contrastive and Consistent Loss (CoCoL), which effectively balances the objectives of data forgetting and model performance retention. Experimental results demonstrate that CoCoL facilitates more effective and controllable data removal than existing techniques.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine unlearning is the process of selectively removing specific data points from a trained machine-learning model [1, 2]. This task has gained significant attention in recent years as it addresses concerns regarding data privacy and model adaptability [3, 4, 2].\nIn this work, we focus on neural ranking models nowadays used for information retrieval (IR), i.e., on neural IR. In this context, machine unlearning may be needed for\na. addressing data-privacy concerns, e.g., for deleting data of a user who has exercised their 'right to be forgotten' [1, 5];\nb. selectively deleting (e.g. outdated) information [6, 7]. For instance, an IR system querying \u201cWhat are the EU member states?\u201d might need to exclude \u201cUK\u201d from its results post-2020 [8], illustrating a practical application of machine unlearning in IR systems.\nIt is therefore important to design methods for machine unlearning that can effectively deal with neural IR. Prominent existing model- and task-agnostic unlearning methods like Amnesiac Unlearning [9, 10] or Negative Gradient Removal [11, 12] (NegGrad) could be employed. However, these have been primarily designed for classification scenarios where it is typically possible to unlearn a class by deliberately damaging the model accuracy on the samples within that class; and Figure 1 illustrates that such unlearning strategies perform poorly in neural IR in the sense that reducing the performance of these models on the 'forget set' (i.e. on the data to be removed) incurs a severe performance loss on the 'retain set' (i.e. on the remaining data) and on test sets. We conjecture that this is due to strong dependencies in neural IR models, where removing individual data points disrupts learned patterns [13, 11, 14]. Another model-agnostic unlearning method is a teacher-student framework [15, 16] which was likewise originally designed for classification tasks. However, as we discuss in detail in Section II-C, a na\u00efve application of this approach to neural IR fails because the relevance scores generated by neural ranking models cannot easily be normalized."}, {"title": "II. BACKGROUND AND PROBLEM DEFINITION", "content": "In this section, we provide, to our knowledge, the first formalisation of the task of machine unlearning within neural IR. We also explain why a na\u00efve application of the teacher-student framework does not work in this context."}, {"title": "A. Machine unlearning", "content": "Let $W \\subset \\mathbb{R}^d$ the parameter space and let $S$ be the universe of possible datasets. Let $M: S \\rightarrow W$ be a learning algorithm which maps a training set $S \\in S$ to a model $w \\in W$. Learning algorithms may be random but we do not make this explicit in the notation. The trained model is then:\n$M_{train} := M(S) := \\arg \\min_{w \\in W} L_s(w)$,\nwhere, employing an empirical-risk-minimization approach, we typically have $L_s(w) := \\Sigma_{(x,y)\\in s} l_M(f_w(x), y)$, for some suitable learning loss function $l_M$.\nGiven the training set $S$, let $F \\subset S$ be the forget set which contains a subset of data points in $S$ to be unlearned; and let $R := S \\setminus F \\in S$ be the retain set which contains the remaining data points. This defines the retrained model\n$M_{retrain} := M(R)$.\nLet $U : W \\times S \\times S \\rightarrow W$ be a (potentially random) unlearning algorithm for $M$ which defines the unlearned model\n$M_{unlearn} := U(M_{train}, F, R)$.\nUnlearning algorithms are normally expected to ensure that the unlearned model closely approximates the retrained model, i.e., $M_{unlearn} \\approx M_{retrain}$, whilst the computational cost of unlearning - starting from $M_{train}$ - should be less than retraining from scratch on R [19, 20]. While mimicking $M_{retrain}$ aligns with Goal a., it does not enable controllable forgetting (Goal b.). Therefore, we will base our unlearning approach on the teacher-student framework (also known as knowledge distillation) from Chundawat et al. [15], which can achieve pre-specified degrees of forgetting by implementing different distillation strategies.\nInformally, the teacher-student framework specifies the unlearning algorithm U as using stochastic gradient-descent - initialised from $M_{train}$ to minimize (or at least decrease)\n$L_{M_F,F}(w) + L_{M_R,R}(w)$,\nwhere, for any dataset A, the objective $L_{M,A}(w)$ penalises the difference between predictions made by the student model $w \\in W$ (which is unlearning) and some fixed teacher model $M$ on A and is typically specified as follows.\nSince $M_{unlearn}$ should perform similarly to $M_{retrain}$ on R which in turn should perform similar to $M_{train}$ (on R),\nit is common to take $M_R := M_{train}$ in (1). This can be interpreted as training $w$ to obey the 'competent' teacher model $M_{train}$ on R [21, 22, 15, 16].\nSince $M_{unlearn}$ should achieve controllable forgetting, i.e., achieve a pre-specified performance $\\delta$ that is worse than $M_{train}$ on F, it is common to take\n$L_{M_F,F}: -L_{M_{train}, F}$,\nin (1) which can be viewed as training w to disobey the 'competent' teacher $M_{train}$ on F [16]; and then to stop the gradient-descent iterations when the accuracy on the forget set has dropped to the target level $\\delta$. Alternatively, if the goals is that the unlearned model should perform similarly to $M_{init} := M(\\emptyset)$ on F, one could simply take $M_F : M_{init}$ in (1), which can be viewed as training w to obey the 'incompetent' teacher model $M_{init}$ on F [15]. Of course, $M_{init}$ could be replaced by another model, e.g., by an adversarial model trained on with noisy data [21, 12, 22]."}, {"title": "B. Unlearning in neural information retrieval", "content": "The goal of information retrieval (IR) is to identify and retrieve documents in response to a search query [23]. Let Q be the universe of potential queries and let D be the universe of potential documents. Queries are user inputs or requests for specific information, typically in the form of words, phrases, or questions; documents refer to units of content, such as web pages or articles.\nThen a dataset for (neural) IR $S \\in S$ consists of tuples (x, y), where\nx = (q, d) \u2208 Q \u00d7 D i) \u2208 Q \u00d7 D is a query-document pair;\ny \u2208 R is the ground-truth relevance score of (q, d).\nWhen y exceeds some threshold, d is regarded as a positive document for q, indicating that d is highly relevant to q.\nA neural-ranking model w \u2208 W is then trained to predict a relevance score $f_w(x) \u2208 R$ of some query-document pair x = (q, d). Relevance scores output by neural-ranking models are used to rank documents. Each document associated with a query is sorted by its score in (descending) order, so that higher scores correspond to a higher rank and thus earlier positions in the search results. Neural Machine UnRanking (NuMuR) is then the task that this model unlearns either queries or documents (or both):\nQuery removal refers to deleting a set of queries $Q'$ (and associated relevance scores) from the dataset. In this case, $F := {((q, d), y) \u2208 S | q \u2208 Q'}$.\nDocument removal refers to deleting a set of documents $D'$ (and associated relevance scores) from the dataset. In this case, $F := {((q, d), y) \u2208 S | d \u2208 D'}$.\nOne of the difficulties encountered in NuMuR is that certain queries or documents may appear simultaneously in the retain set R and in the forget set F. For example, assume that the query: \"The best one-week itinerary for a trip to London\" is associated with two recommended itineraries (i.e., documents). If one itinerary's owner recalls their answer, we must unlearn one query-document pair whilst maintaining the other."}, {"title": "III. PROPOSED NEURAL MACHINE UNRANKING METHODOLOGY", "content": "In this section, we propose a new teacher-student framework for NuMuR, called Contrastive and Consistent Loss (CoCoL). To address the three challenges discussed at the end of the last section, CoCoL introduces the following elements.\nTo overcome the problem of using an 'incompetent' teacher model (such as $M_{init}$) in the presence of unnormalized relevance scores, we simply attempt to reduce the relevance scores on the forget set (relative to the trained model, $M_{train}$) whilst seeking to maintain the relevance scores everywhere else.\nTo enable controllable forgetting despite the fact that the relevance scores are not normalized, we stop the unlearning iterations not when the average relevance score reach some target level but when\n$\\frac{1}{\\vert Q_F \\vert} \\sum_{q \\in Q_F} \\frac{1}{rank_w(q)}$\nis approximately equal to some pre-specified target $\\delta > 0$. Here, $Q_F := {q \u2208 Q | \u2203((q', d), y) \u2208 F : q' = q}$ is the set of distinct queries in the forget set.\nIn query removal, $rank_w(q)$ denotes the rank of the first relevant document for query q among all documents allocated to query q for ranking. evaluated by Model w."}, {"title": "B. Objective", "content": "CoCoL uses gradient steps, started from the trained model $M_{train}$, to decrease an objective of the form\n$L_{M_FUEFUE}(w) + L_{M_D,D}(w)$,\nwere $L_{M,A}(w)$ is again some objective which penalises the discrepancy between w and some reference model M on some dataset A. Note that this objective differs from the standard teacher-student framework (1) in that the entangled set is moved into the first component. Note also that we say 'decrease' rather than 'minimize' because the unlearning is simply stopped when a pre-defined level of forgetting has been achieved (see below for details).\nThe components $L_{M_FUEFUE}(w)$ and $L_{M_D,D}(w)$ are implicitly defined through update rules which we now specify, where for some query-document pair x = (q, d):\n$\\Delta_w^{w,M}(x) = \\frac{a(f_M(x) - f_w(x)) + \\beta}{f_M(x) + f_w(x)}$\nmeasures the discrepancy between the relevance score $f_M(x)$ returned by some fixed reference ('teacher') model M and the relevance score $f_w(x)$ returned by the 'student' model w. Specifically, note that (3) decreases if the relevance score of the teacher model M is much higher than that of the student model w. In (3), $\u03b1 > 0$ and $\u03b2 \u2265 0$ are tuning parameters whose choice will be discussed at the end of this section. The update rules are then as follows.\nContrastive loss: implicit definition of $L_{M_FUEFUE}(w)$. We employ a contrastive loss to modify the student model w such that it generates lower relevance scores on the forget set than the trained model $M_{train}$ whilst ensuring that the relevance scores on the entangled set are maintained. Specifically, at each iteration, we randomly select a sample (x, y) = ((q, d), y) \u2208 F from the forget set and a second sample (x', y') = ((q',d'), y') \u2208 E\u2039q,d), where E\u3008q,d) := {((q\", d\"), y\") \u2208 E | q\" = q or d\" = d} contains the samples that are entangled with (x, y), and then take a gradient step which reduces\n$ReLu(\\Delta_w^{w,M_{train}}(x)) + (x'))|$\nHere, $ReLu(z) := max(0, z)$. If E\u2039q,d) = (\u00d8 then we take the second term in (4) to be zero.\nConsistent loss: implicit definition of $L_{M_D,D}(w)$. We employ a consistent loss to modify the student model w such that it generates relevance scores on the disjoint set that are similar to those from the trained model $M_{train}$. Specifically, at each iteration, we randomly select a positive (i.e., relevant) sample $(x^+,y^+) = ((q^+,d^+),y^+) \u2208 D$ and a negative (i.e., irrelevant) sample $(x^-,y^-) = ((q^-, d^-), y^-) \u2208 D$ from the disjoint set and then take a gradient step which reduces\n$|\\Delta_w^{w,M_{train}}(x^+) | + |\\Delta_w^{w,M_{train}}(x^-)|.$\nIn summary, our CoCoL unranking approach is illustrated in Figure 3."}, {"title": "IV. EXPERIMENTS", "content": "Currently, there are no existing IR datasets specifically designed for machine unlearning research. To address this, we propose curating a dataset derived from established benchmark IR datasets. In selecting the appropriate datasets for NuMuR, our selection criteria focused on datasets that feature extensive one-to-many query-document pairings, essential for evaluating NuMuR methodologies. An in-depth review of resources listed on ir-datasets.com identified two sources that fulfil these requirements: MS MARCO [18] and TREC Complex Answer Retrieval (TREC CAR) [25]. These sources were selected due to their large sample sizes and the presence of overlapping queries and documents. The sample ratio of the forget set, entangled set, and disjoint set is approximately 1 : 1 : 2. Table I summarises the datasets."}, {"title": "B. Evaluation metrics", "content": "Unlearning performance: To measure the ranking performance, we use the MRR type measure as specified in (2) on the forget set. For evaluating performance on the retain and test sets, MRR is similarly calculated as the average of the reciprocals of the rank positions of the first correctly retrieved document for queries in those sets.\nUnlearning efficiency: Unlearning efficiency is measured by the unlearn and relearn times.\na) Unlearn time: To ensure consistent measurement across different neural ranking models and unlearning methods we report the normalized unlearn epoch duration:\n(normalized unlearn epoch duration)\n:=\n$\\frac{(avg \\;time \\;per \\;unlearning \\;epoch \\;of \\;M_{unlearn})}{(avg \\;time \\;per \\;learning \\;epoch \\;of \\;M_{train})}$\nas well as the total unlearn time:\n(total unlearn time) := (normalized unlearn epoch duration)\nx (no of unlearn epochs).\nb) Relearn time: The relearn time measures the number of epochs required for the model to restore its pre-unlearning performance level. A longer relearn time is indicative of a more thorough unlearning process [13, 12, 22]."}, {"title": "C. NIR models and unlearning baselines", "content": "We demonstrate the effectiveness of the proposed method on multiple neural ranking models including two cutting-edge pretraining-based models, ColBERT [17] and BERT with Dot Productions (BERTdot) [26], along with two sophisticated word-embedding-based models, Duet [27], and MatchPyramid [28]. Table II lists the empirically chosen values of the tuning parameters a and \u1e9e used for each model based on performance."}, {"title": "V. CONCLUSION", "content": "In an era where data privacy and dynamic information landscapes are paramount, this study focuses on the field of machine unlearning, specifically within the context of neural ranking models for information retrieval (IR) systems. This research introduced the concept of Neural Machine UnRanking (NuMuR), presenting a novel method (Contrastive and Consistent Loss (CoCoL)) that effectively balances the delicate trade-off between unlearning specific information and maintaining the overall performance of neural ranking models. CoCoL, particularly effective with pretraining-based neural ranking models, represents an advancement in addressing the unique challenges posed by neural ranking tasks in IR systems."}, {"title": "APPENDIX", "content": "RELEVANCE SCORE DISTRIBUTION COMPARISON\nThis section provides examples showing that relevance score distributions vary across different neural ranking models. Figure 8 illustrates differences in the scale of relevance scores. As shown in Figure 8a, both BERTdot and ColBERT exhibit relatively lower relevance score ranges after training. Using Minit as the 'incompetent' teacher may result in higher relevance scores on forgetting samples. Additionally, Figure 8b shows that the relevance score range between relevant and irrelevant pairs varies across models, making it difficult to determine the extent to which the relevance score of relevant pairs (which we aim to forget) should be reduced.\nWhen a and \u1e9e parameters are optimally adjusted in the experiment groups, there is a noticeable trend of declining forgetting scores, accompanied by stable performance on both entangled and disjoint sets (from Figure 10a to Figure 10l). Even when the forgetting performance significantly undercuts the test performance, CoCoL maintains the stability of retain set performance. In certain scenarios, our CoCoL even enhances the convergence of retain sets throughout the unlearning process (as shown in Figure 10m and Figure 10n). Nevertheless, there are instances where it faces challenges in striking a precise balance between forgetting and retaining performances, evident in Figure 10o and Figure 10p."}]}