{"title": "FairJob: A Real-World Dataset\nfor Fairness in Online Systems", "authors": ["Mariia Vladimirova", "Eustache Diemert", "Federico Pavone"], "abstract": "We introduce a fairness-aware dataset for job recommendation in advertising,\ndesigned to foster research in algorithmic fairness within real-world scenarios.\nIt was collected and prepared to comply with privacy standards and business\nconfidentiality. An additional challenge is the lack of access to protected user\nattributes such as gender, for which we propose a solution to obtain a proxy\nestimate. Despite being anonymized and including a proxy for a sensitive attribute,\nour dataset preserves predictive power and maintains a realistic and challenging\nbenchmark. This dataset addresses a significant gap in the availability of fairness-\nfocused resources for high-impact domains like advertising \u2013 the actual impact\nbeing having access or not to precious employment opportunities, where balancing\nfairness and utility is a common industrial challenge. We also explore various\nstages in the advertising process where unfairness can occur and introduce a method\nto compute a fair utility metric for the job recommendations in online systems\ncase from a biased dataset. Experimental evaluations of bias mitigation techniques\non the released dataset demonstrate potential improvements in fairness and the\nassociated trade-offs with utility.", "sections": [{"title": "Introduction", "content": "The intersection of technology and human dynamics presents both opportunities and challenges,\nparticularly in the realm of artificial intelligence (AI). Despite advancements, persistent biases\nrooted in historical inequalities permeate our data-driven systems, perpetuating unfairness and\nexacerbating societal divides. Historical biases shape data collection, influencing AI model outcomes\nand often amplifying existing inequalities [Bolukbasi et al., 2016, Zhao et al., 2017, Chen et al.,\n2023]. Despite concerns regarding privacy, liability, and public relations, the collection of special\nand sensitive category data is crucial for bias assessments [Andrus et al., 2021]. Moreover, evolving\nlegal frameworks, exemplified by the recent AI Act and General Data Protector Regulation [UK\nInformation Commissioner's Office, 2022], mandate the detection, prevention, and mitigation of\nbiases, while imposing some restrictions on the use of sensitive data.\nRecent advances in fairness often involve computer vision, natural language processing and speech\nrecognition tasks [Gustafson et al., 2023, Andrews et al., 2024, Hall et al., 2024, Schumann et al.,\n2024, Veliche and Fung, 2023], while lacking attention to algorithmic decision-making that involves\ntabular data, where each row represents an individual or an observation, and each column represents\na feature or attribute [Le Quy et al., 2022, Zhang et al., 2021], resulting in a very few benchmark\npapers [Gorishniy et al., 2021, 2022, Grinsztajn et al., 2022, Shwartz-Ziv and Armon, 2022, Matteucci\net al., 2023]. Tabular data is commonly used in various high-risk domains such as finance, healthcare,\nhiring, criminal justice, and advertising [van Breugel and van der Schaar, 2024].\nAlgorithmic discrimination in advertising can be related to sensitive verticals which highlights\nbeneficial employment, financial and housing opportunities, or about who sees potentially less"}, {"title": "", "content": "desirable advertising, such as ads for predatory lending services [Lambrecht and Tucker, 2019].\nWhile unfairness in advertising is not punitive but rather assistive, i.e. fairness consists in providing\nequal access to precious opportunities, it is essential to ensure fairness in advertising practices.\nIn some contexts such as housing or lending, such discrimination is explicitly prohibited by law\u00b2.\nSeveral studies conducted analyses on the fairness in advertising at different stages and observed\ndiscriminating behavior that was not necessarily intended by the ad-services [Speicher et al., 2018,\nLambrecht and Tucker, 2019, Andreou et al., 2019, Ali et al., 2019]. This emphasizes the need for\nbetter mechanisms to audit and prevent bias in ads.\nMost of studies on discriminating behavior in advertising were conducted via creating advertising\ncampaigns and choosing targeted audiences and analysing the data from the user perspective without\naccessing the algorithmic features [Speicher et al., 2018, Lambrecht and Tucker, 2019, Andreou et al.,\n2019, Ali et al., 2019]. The absence of publicly available, realistic datasets leads researchers to publish\nresults based on private data, resulting in non-reproducible claims [Geyik et al., 2019, Andreou et al.,\n2019, Timmaraju et al., 2023, Tang and Yu, 2022]. This poses challenges for critical evaluation and\nbuilding upon previous work in the scientific community. Tang and Yu [2022] highlights the lack of\npublic benchmarking datasets to study the fairness related approaches in advertising.\nIn addition, most of the studies assume that the AI systems have an access to the protected attributes\nwhich is often unrealistic due to privacy constraints or legal restrictions [Holstein et al., 2019, Lahoti\net al., 2020, Molina et al., 2023, Timmaraju et al., 2023]. In online advertising, decision-makers\nusually have access to a log of user interactions with the system, which they can use to guess the\nattributes. However, the level of inaccuracy can be significant, making it difficult to ensure that an\nad campaign reaches a non-discriminatory audience [Gelauff et al., 2020]. This makes it hard to\nmeet fairness requirements [Lipton et al., 2018]. We emphasize the need for thorough research in\nreal-world situations where access to protected attributes is limited.\nContributions. To foster research in fairness within real-world scenarios, we release a large-scale\nfairness-aware dataset for advertising. The dataset contains pseudononymized users' context and\npublisher features that were collected from a job targeting campaign ran for 5 months. The data has\nbeen sub-sampled non-uniformly to avoid disclosing business metrics. Feature names have been\nanonymized for business confidentiality, and their values randomly projected to preserve predictive\npower while making the recovery of the original features or user context (i.e. re-indentification)\npractically impossible, with accordance to the privacy-safety measures\u00b3. Although our dataset does\nnot contain explicit sensitive attributes such as gender, it includes a gender proxy derived from\nnon-protected relevant attributes, which we discuss in detail further.\nThis dataset provides a baseline according to the eligible audience generated by an advertiser's\ntargeting criteria for a specific ad. This ensures that ads are tailored to individuals whom the\nadvertiser can feasibly serve (such as those within a specific geographic region) and who are likely to\nbe interested in their offerings, a practice already governed by policies and standards in Housing,\nEmployment, and Credit verticals. Since advertiser targeting adheres to policy constraints to prevent\ndiscriminatory practices\u2074 (such as prohibiting the use of gender criteria in employment ads), the\nresulting eligible audience remains independent of prediction algorithms, serving as a reasonable\nbaseline metric.\nWith the released dataset we examine the stages in the advertising process where unfairness can occur\nand explore techniques to mitigate such biases. Taking into account possible induced biases, we\npropose an unbiased utility metric that help to analyse different bias mitigation techniques. We also"}, {"title": "", "content": "perform experiments on the released dataset to verify how we can improve fairness and the possible\ntrade-offs with utility."}, {"title": "Related works", "content": "Open-source datasets. A limited availability of publicly available fairness-aware tabular datasets\nchallenges research advancements in algorithmic fairness [Le Quy et al., 2022, Hort et al., 2023]. In\n2022, Le Quy et al. [2022] studied datasets used at least 3 times in research publications on fairness,\nand found out there were only 15 open-source fairness datasets, most of which are criticized for\nbeing too small or far from real-world scenarios, including the most frequently used Adult [Dua and\nGraf, 2017] and COMPAS dataset [Larson et al., 2016]. Even though there is a positive tendency\non addressing this issue by open-sourcing privacy-complying datasets, such as BAF [Jesus et al.,\n2022] for bank fraud detection where the data was obtained via data generation techniques, or\nWCLD [Ash et al., 2024], a curated large-scale dataset from circuit courts to address criminal justice,\nthere is still lack in available datasets in other high-impact areas such advertising. It is important for\nacademic researchers to have access to large datasets to study the problem rigorously [L. Cardoso\net al., 2019, Li et al., 2022, Le Quy et al., 2022]. Large-scale datasets are advantageous as they\nincrease the likelihood of capturing significant performance differences in experiments with new\nmethods. With larger dataset sizes, the variance of metrics decreases, enabling more reliable and\nmeaningful comparisons between different approaches.\nBias mitigation methods. The initial step to enhance model fairness is to exclude the protected\nattribute as a feature during training, a strategy known as fairness through unawareness [Chen et al.,\n2019]. However, this approach alone does not ensure fairness because the model may still learn\ncorrelations between other features and the protected attributes, see Section 3.1 and Figure 2b for\ndetails. To achieve a higher level of fairness, AI systems typically employ one of the additional\nmethods: pre-processing, in-training, or post-processing. We refer to Hort et al. [2023] for the most\nup-to-date and thorough survey.\nFairness without demographics. The information on the protected attribute is often not available\nin practice [Holstein et al., 2019, Hort et al., 2023]. Several works studied limited availability of the\nprotected attribute such as via a proxy [Gupta et al., 2018] or assuming there is a partial access to the\ninformation [Hashimoto et al., 2018, Awasthi et al., 2020, Molina et al., 2023]. Lahoti et al. [2020]\nrelies on the assumption that protected groups are computationally-identifiable. However, if there\nwere no signal about protected groups in the remaining features and class labels, we cannot make any\nstatements about improving the model for protected groups. One of the possible solutions is to get\ndata from secure multi-party computation [Veale and Binns, 2017, Kilbertus et al., 2018, Hu et al.,\n2019] or directly from users [Gkiouzepi et al., 2023]. However, these tools are still to be adapted to\nreal-world situations. In addition, transfer leaning can be useful when there is little available data on\nthe protected attributes [Coston et al., 2019]."}, {"title": "Fairness in advertising", "content": "The aim of ad-tech companies is to deliver the most relevant advertisements to users navigating\npublishers' webpages. By matching users' browsing histories and content preferences with products\nthat align with their interests, targeted advertising creates a mutually beneficial ecosystem [Wang\net al., 2017, Choi et al., 2020]. Advertisers reach relevant audiences, users have access to free\ninformation and services in exchange of seeing ads related to their interests, and platforms profit from\nselling targeted ads.\nAd-tech companies grapple with vast volumes of noisy data, which encapsulate users' past actions.\nLeveraging this data, they predict potential clicks and conversions. However, if the data is biased, the\nalgorithms can inadvertently perpetuate and even amplify these biases [Bolukbasi et al., 2016, Zhao\net al., 2017, Chen et al., 2023]. It is crucial to scrutinize the predictors for bias and devise solutions\nto mitigate it. Failing to do so can result in discrepancies between offline evaluations and online\nmetrics, ultimately harming user satisfaction and trust in the service of online systems [Chen et al.,\n2023]. While advertising commonplace items carries little risk, companies must exercise caution\nwith high-risk verticals like job offers [Speicher et al., 2018, Lambrecht and Tucker, 2019, Andreou"}, {"title": "", "content": "et al., 2019, Ali et al., 2019]. For instance, if managerial positions are disproportionately shown\nto men over women, more men may apply, perpetuating historical biases and exacerbating gender\ndisparities.\nBias can be introduced at several stages in the advertising process, see Figure 1. First, when a user\nvisits a webpage with an ad slot, ad-tech companies participate in a real-time bidding (RTB) auction.\nDuring this auction, companies select a campaign (e.g., job offers or clothing) based on attributes of\nthe publisher and the user, including their log of past interactions such as seen ads, their context, the\nfact of clicks on the ads, see Section 3.2. This auction must be organized in a fair way, respecting\nboth the companies placing bids and the publishers providing ad slots, see Section 3.3. After an\nad-tech company wins the display auction, there is the choice of which product to show (e.g., a senior\nposition job or an assistant job). This selection can also introduce bias with respect to the user, see\nSection 3.4. Ensuring fairness at this stage is critical to preventing the reinforcement of existing\ninequalities."}, {"title": "Fairness definition", "content": "We base our discussion on a counterfactual fairness framework that explains the underlying connec-\ntions between the variables in the system [Kusner et al., 2017]. Let $A$ denote a protected attribute\n(can be a set of protected attributes) of an individual, $X$ denote the other observable attributes of any\nparticular individual, $Y$ denote the outcome to be predicted, and let $\\hat{Y}$ be a predictor. The predictor\ntakes into account the available data from logs of user interactions with the system and product\ndescriptions and estimates the probability of a positive outcome, i.e. click of the user on the product.\nThe system takes into account the prediction and then shows the best product to the user, which\nresults into possible positive outcome. In our analysis, we are interested in understanding how $A$ and\n$X$ influence $Y$ and how well our predictor $\\hat{Y}$ captures these relationships. Our goal is not just to\npredict outcomes accurately but also to ensure fairness and mitigate biases in the predictions with\nrespect to $A$. There random variables have the causal relationships that are modelled on Fig. 2 and\nwe discuss further in detail."}, {"title": "Selection bias in campaign choosing", "content": "In our setting, we are interested in assessing the fairness in specific campaign (e.g., job campaign)\nwith respect to the protected attribute. For instance, we want to ensure that job advertisements\nfor managerial roles are fair with respect to a binary protected attribute $A \\in \\{0,1\\}$ (e.g., gender).\nTypically, the data considered in this framework regards the job advertisements for users which have\nbeen assigned to the job campaign $c$. However, the campaign selection process might introduce\nselection bias, which should be taken in account. In particular, $P(A = 1)$ and $P(A = 0)$ are the\n(internet)-population level of a binary protected attribute. This might be approximated to the census\npopulation frequencies of the protected attribute. Let $C$ be a random variable of choosing a campaign,\nthen $P(A = 1 | C = c)$ and $P(A = 0 | C = c)$ are the frequencies of the protected attribute in the\njob campaign data $c$. These differ from the population levels due to selection bias.\nNote that the recommendation engines predict $P(Y = 1 | A = a,C = c)$ for a product in the\ncampaign $c$. Thus, if we use prediction bias mitigation techniques while considering data at the\ncampaign level, in the best case scenario, we obtain fair predictions while being unfair outside of\ncampaing, $P(\\hat{Y} = 1 | A = 0, C = c) = P(\\hat{Y} = 1 | A = 1, C = c)$ and $P(\\hat{Y} = 1 | A = 0) \\neq$\n$P(\\hat{Y} = 1 | A = 1)$ . Thus, we have to take into account the selection bias to ensure demographic\nparity introduced in Eq. (2):\n$\\frac{P(A = 1)}{P(A = 1 | C = c)} = \\frac{P(A = 0)}{P(A = 0 \\vert C = c)}.$"}, {"title": "Market bias", "content": "Lambrecht and Tucker [2019] found that women are a prized demographic, making them more\nexpensive to advertise to. This implies that ads that are meant to be gender-neutral can be delivered\nin the way that appears to be discriminatory by RTB algorithms that focus on optimizing cost-\neffectiveness. Ali et al. [2019] explained that this is not solely the indication of the ingrained cultural\nbias nor a result of user profiles inputted into ads algorithms, but rather the product of competitive\nspillovers among advertisers. Additionally, the feedback loop mechanism considers imbalanced"}, {"title": "Recommendation bias", "content": "In the ad recommendation system, the goal is to choose best products for a user for a given banner\nthat can have several displays at the same time. The goal is to maximize the number of clicks for a\ngiven banner, meaning that there can be several products clicked. When we have several displays to\nshow to a user, the display rank position becomes important and creates position bias with respect to\na positive outcome. The influence of this bias is hard to estimate, however, it is important to take it\ninto account [Singh and Joachims, 2018, 2019, Morik et al., 2020, Usunier et al., 2022].\nLet $J$ be a random variable denoting the set of banner to be shown to a user, $D$ be a display (chosen\nproduct, i.e. job offer) shown to a user on a banner $J$. Let model $f(x, d)$ predicts the following\npositive outcome: $P(Y = 1|X = x, D = d)$, i.e. the probability of a click for a chosen product $d$\ngiven user features $x$. As discussed above, we have to take into account the display position which\nexpressed via variable rank $R$. However, the influence of the position on the utility is hard to estimate.\nFurther, we suggest utility metrics for ads recommendation and in order to avoid the position bias,\nwe suggest to compute them only on randomized displays, where the position of the products on the\nbanner was chosen randomly.\nClick-rank utility. The users' utility for a given model can be expressed as a positive engagement\nin the following way:\n$U(f) = \\mathbb{E}_{I} \\mathbb{E}_{X,D|I} [I(Y_D = 1) \\text{rank}_I f(X_I, D)],$\nwhere $I(Y_D = 1)$ is the identity function of a positive outcome (e.g. click) for display $D$. The function\n$\\text{rank}_D$ computes the ascending order rank within the set of displays for a banner of impression $I$.\nThis metrics is based on estimation of the positive outcome based on the passed events for chosen\nusers.\nProduct-rank utility for biased data. We notice that the metrics for the algorithm can be biased\ndue to the selection bias discussed in Section 3.2 because the prediction algorithm estimates $P(Y =$\n$1|A = a, C = c)$ instead of $P(Y = 1|A = a)$. Even if we correct the prediction bias in $P(Y =$\n$1|A = a, C = c)$ based on the data provided for given campaign $c$, it does not correct the final bias\nin $P(Y = 1A = a)$ due to selection bias. We can adapt the click-rank utility to include possible\nselection bias into the metric, by explicitly considering that the product utility depends on a chosen\ncampaign. Then, when correcting for the unfairness in the prediction, we might improve the utility\nmetric taken into account the selection bias in the data:"}, {"title": "", "content": "$\\mathbb{U}(f) = \\mathbb{E}_{D \\mathbb{E}_{I |D}} \\frac{P(A = a_X)}{P(A = a_X |C = c)} I (Y_D = 1) \\text{rank}_I f(X_I, D),$ where $a_X$ stands for a gender of a given user $X$. Intuitively, if the prediction is biased with respect to protected attribute $A$, the final prediction $P(Y = 1|A = a)$ is even more biased due to selection bias of with respect to the protected attribute of choosing a campaign $C = c$: $P(C = c | A = a)$. In this case, the prediction model amplifies the existing historical bias. However, we can remove the selection bias by adding weights that correspond to the presence of the protected attribute in the whole population and given the campaign. If the user with protected attribute $A = a$ has lower probability of click, and this group was underrepresented in the campaign $C = c$, i.e. $P(A = a) > P(A = a|C = c)$, then in the utility function, the model's prediction will be higher, by addressing the possible bias due to under-representation in the data. This is our suggested metric to evaluate the recommendation system when the selection bias is present and known such as in the FairJob dataset."}, {"title": "FairJobs dataset", "content": "We introduce FairJobs dataset that contains fairness-aware data from a real-world scenario of\nadvertising. This dataset is intended to learn click predictions models and evaluate by how much\ntheir predictions are biased between different gender groups. The dataset consists of 1,072,226\nrows that were collected during 5 months of a targeted job campaign, each row represents a job\nad and user features: 20 categorical and 39 numerical features; label click (binary, if the ad was\nclicked), protected_attribute (binary, proxy for user gender, see below for more thorough\nexplanation), senior (binary, if the job offer was for a senior position), [user_id, impression_id,\nproduct_id] are unqiue identifiers of user, impression and product (job ad). More details and dataset\nstatistics are referred to Appendix.\nDetails on gender proxy. Since we do not directly access user demographics, we have to find a way\nto get a proxy of relevant attribute. Most of recent works leverage the use of external data or prior\nknowledge on correlations to obtain proxies to relevant attributes [Gupta et al., 2018, Hashimoto et al.,\n2018, Awasthi et al., 2020, Lahoti et al., 2020]. We define a product gender, either given by a client,\neither by a category of the product. This gives us approximately 40% of products gender identified.\nThen, we follow the available statistics and choose the gender proxy based on the dominant gender of\nproducts the user interacts with. This gender proxy identities a behavior of a user, i.e. if a user tends\nto buy female or male products. The gender proxy does not necessarily correlate with the gender,\nas it often happens with the proxy variables [Gelauff et al., 2020]. Verification of the accuracy of\nthese approximations is challenging. Additionally, if there are no signal about protected groups in the\nremaining features and class labels, we cannot make any statements about improving the model for\nprotected groups [Lahoti et al., 2020].\nLimitations and interpretation. We remark that the proposed gender proxy does not give a\ndefinition of the gender. Since we do not have access to the sensitive information, this is the best\nsolution we have identified at this stage to identify bias on pseudonymised data, and we encourage\nany discussion on better approximations. This proxy is reported as binary for simplicity yet we\nacknowledge gender is not necessarily binary. Although our research focuses on gender, this should\nnot diminish the importance of investigating other types of algorithmic discrimination. While this\ndataset provides important application of fairness-aware algorithms in a high-risk domain, there are\nseveral fundamental limitation that can not be addressed easily through data collection or curation\nprocesses. These limitations include historical bias that affect a positive outcome for a given user, as\nwell as the impossibility to verify how close the gender-proxy is to the real gender value. Additionally,\nthere might be bias due to the market unfairness that we explained in Section 3.3. Such limitations"}, {"title": "Empirical observations", "content": "Challenges. The first challenge comes from handling the different types of data that are common\nin tables, the mixed-type columns: there are both numerical and categorical features that have to\nbe embedded [Gorishniy et al., 2021, 2022, Grinsztajn et al., 2022, Shwartz-Ziv and Armon, 2022,\nMatteucci et al., 2023]. In addition, some of the features have long-tail phenomenon and products\nhave popularity bias, see Figure 4. Our datasets contains more than 1,000,000 lines, while current\nhigh-performing models are under-explored in scale, e.g. the largest datasets in Grinsztajn et al.\n[2022] are only 50,000 lines, while in Gorishniy et al. [2021, 2022] only one dataset surpasses\n1,000,000 lines. Additional challenge comes from strongly imbalanced data: the positive class\nproportion in our data is less than 0.007 that leads to challenges in training robust and fair machine\nlearning models [Jesus et al., 2022, Yang et al., 2024]. In our dataset there is no significant imbalances\nin demographic groups users regarding the protected attribute (both genders are sub-sampled with 0.5\nproportion, female profile users were shown less job ad with 0.4 proportion and slightly less senior\nposition jobs with 0.48 proportion), however, there could be a hidden effect of a bias that we discussed\nin Section 3. This poses a problem in accurately assessing model performance [van Breugel et al.,\n2024]. More detailed statistics and exploratory analysis are referred to the supplemental material."}, {"title": "", "content": "Results. We choose two baseline methods: (i) unfair, that uses all attributes for training, including\nthe protected one; and (ii) unaware, that corresponds to fairness through unawareness, i.e. using all\nattributes during training except the protected one. We compare the results of these algorithms with a\n(iii) fair model that is trained without protected attribute with additional \"fairness\" penalty [Kamishima\net al., 2011, Bechavod and Ligett, 2017]. The three models correspond to the situations described\nin Figure 2. We refer all the reproducibility details and additional experiments to the supplemental\nmaterial. The resulted prediction can be visually compared in Figure 5.\nThese findings suggest that the trade-off relationship between accuracy and fairness is context-\ndependent. It highlights the need for further research to better understand the conditions under which\nthe accuracy fairness trade-off arises and identify strategies to mitigate or overcome it.\nAdditionally, we propose to restraint an access to the protected attribute and study the trade-off when\nwe train the model on the whole train set but add fairness penalty only for some percentage of train\nset. In some scenarios, we see improvements in fairness without sacrificing the overall performance.\nThe loss in accuracy due to the imposed fairness constraints is often small as also noted in other\nworks [Celis et al., 2019b]. We explore bias correction techniques tailored to address data limitations\nand preserve utility in large-scale. We demonstrate how prioritizing fairness in AI not only benefits\nusers by fostering inclusivity but also contributes to the long-term success and ethical integrity of\ncompanies. Details and results of the experiments are reported in the supplemental material."}, {"title": "Conclusion", "content": "Addressing bias in AI goes beyond mere compliance with legal frameworks like the AI Act; it\nnecessitates proactive measures to detect, prevent, and mitigate biases. Drawing from real-world\nchallenges faced by industries, we highlight the limitations of existing bias mitigation strategies,\nparticularly in environments where access to sensitive user attributes is restricted. We encourage\nother authors and practitioners to experiment with different AI or Fair AI algorithms on this dataset.\nWe expect that with this work, the quality of evaluation of novel AI methods increases, potentiating\nthe development of the area. Additionally, we hope it encourages other similar relevant datasets to be\npublished from other authors and institutions."}, {"title": "Appendix", "content": "We firstly describe in detail how the dataset FairJob was collected, then provide all the information on\nthe context and features with its statistics. Further, we perform experiments and provide all the steps\nfor the sake of reproducubility. The dataset is hosted at https://huggingface.co/datasets/criteo/FairJob.\nSource code for the experiments is hosted at https://github.com/criteo-research/FairJob-dataset/.\nAuthor statement of responsibility. Authors and Criteo bear all responsibility in case of violation\nof rights and confirmation of the data license."}, {"title": "Dataset information", "content": "As illustrated in Figure 6, the process starts with users navigating Publisher and Advertiser websites\n(typically newspapers and retailer shops respectively). Upon user consent10, user information\nabout the events such as visits or product views\u00b9\u00b9 are collected and identified by means of browser\ncookies. Users are subject to personalized advertising (if the job campaign was chosen and the\ndisplay opportunity was won) until the end of the data collection period. Subsequently, only won\ndisplays coming from Publisher and Advertiser partners are joined by cookie identifier on the AdTech\nplatform to form the raw dataset, dropping cookie ids when they are not needed anymore. To ensure\nconfidentiality, the data has been sub-sampled non-uniformly to avoid disclosing business metrics.\nFeature names have been anonymized, and their values randomly projected to preserve predictive\npower while rendering the recovery of the original features or user context practically impossible12.\nThe dataset does not contain the relevant attributes such as gender; however, it includes a gender\nproxy which we discuss in detail in the main text."}, {"title": "Dataset detailed description", "content": "The dataset contains pseudononymized users' context and publisher features that was collected from\na job targeting campaign ran for 5 months by Criteo AdTech company. Each line represents a product\nthat was shown to a user. Each user has an impression session where they can see several products\nat the same time. Each product can be clicked or not clicked by the user. The dataset consists of\n1072226 rows and 55 columns:\n\u2022 user_id is a unique identifier assigned to each user. This identifier has been anonymized\nand does not contain any information related to the real users.\n\u2022 product_id is a unique identifier assigned to each product", "feature": "if the product title contains words describing managerial role (e.g.\n'president'", "ceo": "and others)", "click": "lower rank means higher position of the product on the display.\n\u2022 displayrandom is a binary feature that equals 1 if the display position on the banner of the\nproducts associated with the same impression_id was randomized. The click-rank metric\nshould be computed on displayrandom = 1 to avoid positional bias.\n\u2022 click is a binary feature that equals 1 if the product product_id in the impression\nim```json\n{"}, {"title": "FairJob: A Real-World Dataset\nfor Fairness in Online Systems", "authors": ["Mariia Vladimirova", "Eustache Diemert", "Federico Pavone"], "abstract": "We introduce a fairness-aware dataset for job recommendation in advertising,\ndesigned to foster research in algorithmic fairness within real-world scenarios.\nIt was collected and prepared to comply with privacy standards and business\nconfidentiality. An additional challenge is the lack of access to protected user\nattributes such as gender, for which we propose a solution to obtain a proxy\nestimate. Despite being anonymized and including a proxy for a sensitive attribute,\nour dataset preserves predictive power and maintains a realistic and challenging\nbenchmark. This dataset addresses a significant gap in the availability of fairness-\nfocused resources for high-impact domains like advertising \u2013 the actual impact\nbeing having access or not to precious employment opportunities, where balancing\nfairness and utility is a common industrial challenge. We also explore various\nstages in the advertising process where unfairness can occur and introduce a method\nto compute a fair utility metric for the job recommendations in online systems\ncase from a biased dataset. Experimental evaluations of bias mitigation techniques\non the released dataset demonstrate potential improvements in fairness and the\nassociated trade-offs with utility.", "sections": [{"title": "1 Introduction", "content": "The intersection of technology and human dynamics presents both opportunities and challenges,\nparticularly in the realm of artificial intelligence (AI). Despite advancements, persistent biases\nrooted in historical inequalities permeate our data-driven systems, perpetuating unfairness and\nexacerbating societal divides. Historical biases shape data collection, influencing AI model outcomes\nand often amplifying existing inequalities [Bolukbasi et al., 2016, Zhao et al., 2017, Chen et al.,\n2023]. Despite concerns regarding privacy, liability, and public relations, the collection of special\nand sensitive category data is crucial for bias assessments [Andrus et al., 2021]. Moreover, evolving\nlegal frameworks, exemplified by the recent AI Act and General Data Protector Regulation [UK\nInformation Commissioner's Office, 2022], mandate the detection, prevention, and mitigation of\nbiases, while imposing some restrictions on the use of sensitive data.\nRecent advances in fairness often involve computer vision, natural language processing and speech\nrecognition tasks [Gustafson et al., 2023, Andrews et al., 2024, Hall et al., 2024, Schumann et al.,\n2024, Veliche and Fung, 2023], while lacking attention to algorithmic decision-making that involves\ntabular data, where each row represents an individual or an observation, and each column represents\na feature or attribute [Le Quy et al., 2022, Zhang et al., 2021], resulting in a very few benchmark\npapers [Gorishniy et al., 2021, 2022, Grinsztajn et al., 2022, Shwartz-Ziv and Armon, 2022, Matteucci\net al., 2023]. Tabular data is commonly used in various high-risk domains such as finance, healthcare,\nhiring, criminal justice, and advertising [van Breugel and van der Schaar, 2024].\nAlgorithmic discrimination in advertising can be related to sensitive verticals which highlights\nbeneficial employment, financial and housing opportunities, or about who sees potentially less"}, {"title": "", "content": "desirable advertising, such as ads for predatory lending services [Lambrecht and Tucker, 2019].\nWhile unfairness in advertising is not punitive but rather assistive, i.e. fairness consists in providing\nequal access to precious opportunities, it is essential to ensure fairness in advertising practices.\nIn some contexts such as housing or lending, such discrimination is explicitly prohibited by law\u00b2.\nSeveral studies conducted analyses on the fairness in advertising at different stages and observed\ndiscriminating behavior that was not necessarily intended by the ad-services [Speicher et al., 2018,\nLambrecht and Tucker, 2019, Andreou et al., 2019, Ali et al., 2019]. This emphasizes the need for\nbetter mechanisms to audit and prevent bias in ads.\nMost of studies on discriminating behavior in advertising were conducted via creating advertising\ncampaigns and choosing targeted audiences and analysing the data from the user perspective without\naccessing the algorithmic features [Speicher et al., 2018, Lambrecht and Tucker, 2019, Andreou et al.,\n2019, Ali et al., 2019]. The absence of publicly available, realistic datasets leads researchers to publish\nresults based on private data, resulting in non-reproducible claims [Geyik et al., 2019, Andreou et al.,\n2019, Timmaraju et al., 2023, Tang and Yu, 2022]. This poses challenges for critical evaluation and\nbuilding upon previous work in the scientific community. Tang and Yu [2022] highlights the lack of\npublic benchmarking datasets to study the fairness related approaches in advertising.\nIn addition, most of the studies assume that the AI systems have an access to the protected attributes\nwhich is often unrealistic due to privacy constraints or legal restrictions [Holstein et al., 2019, Lahoti\net al., 2020, Molina et al., 2023, Timmaraju et al., 2023]. In online advertising, decision-makers\nusually have access to a log of user interactions with the system, which they can use to guess the\nattributes. However, the level of inaccuracy can be significant, making it difficult to ensure that an\nad campaign reaches a non-discriminatory audience [Gelauff et al., 2020]. This makes it hard to\nmeet fairness requirements [Lipton et al., 2018]. We emphasize the need for thorough research in\nreal-world situations where access to protected attributes is limited.\nContributions. To foster research in fairness within real-world scenarios, we release a large-scale\nfairness-aware dataset for advertising. The dataset contains pseudononymized users' context and\npublisher features that were collected from a job targeting campaign ran for 5 months. The data has\nbeen sub-sampled non-uniformly to avoid disclosing business metrics. Feature names have been\nanonymized for business confidentiality, and their values randomly projected to preserve predictive\npower while making the recovery of the original features or user context (i.e. re-indentification)\npractically impossible, with accordance to the privacy-safety measures\u00b3. Although our dataset does\nnot contain explicit sensitive attributes such as gender, it includes a gender proxy derived from\nnon-protected relevant attributes, which we discuss in detail further.\nThis dataset provides a baseline according to the eligible audience generated by an advertiser's\ntargeting criteria for a specific ad. This ensures that ads are tailored to individuals whom the\nadvertiser can feasibly serve (such as those within a specific geographic region) and who are likely to\nbe interested in their offerings, a practice already governed by policies and standards in Housing,\nEmployment, and Credit verticals. Since advertiser targeting adheres to policy constraints to prevent\ndiscriminatory practices\u2074 (such as prohibiting the use of gender criteria in employment ads), the\nresulting eligible audience remains independent of prediction algorithms, serving as a reasonable\nbaseline metric.\nWith the released dataset we examine the stages in the advertising process where unfairness can occur\nand explore techniques to mitigate such biases. Taking into account possible induced biases, we\npropose an unbiased utility metric that help to analyse different bias mitigation techniques. We also"}, {"title": "", "content": "perform experiments on the released dataset to verify how we can improve fairness and the possible\ntrade-offs with utility."}, {"title": "2 Related works", "content": "Open-source datasets. A limited availability of publicly available fairness-aware tabular datasets\nchallenges research advancements in algorithmic fairness [Le Quy et al., 2022, Hort et al., 2023]. In\n2022, Le Quy et al. [2022] studied datasets used at least 3 times in research publications on fairness,\nand found out there were only 15 open-source fairness datasets, most of which are criticized for\nbeing too small or far from real-world scenarios, including the most frequently used Adult [Dua and\nGraf, 2017] and COMPAS dataset [Larson et al., 2016]. Even though there is a positive tendency\non addressing this issue by open-sourcing privacy-complying datasets, such as BAF [Jesus et al.,\n2022] for bank fraud detection where the data was obtained via data generation techniques, or\nWCLD [Ash et al., 2024], a curated large-scale dataset from circuit courts to address criminal justice,\nthere is still lack in available datasets in other high-impact areas such advertising. It is important for\nacademic researchers to have access to large datasets to study the problem rigorously [L. Cardoso\net al., 2019, Li et al., 2022, Le Quy et al., 2022]. Large-scale datasets are advantageous as they\nincrease the likelihood of capturing significant performance differences in experiments with new\nmethods. With larger dataset sizes, the variance of metrics decreases, enabling more reliable and\nmeaningful comparisons between different approaches.\nBias mitigation methods. The initial step to enhance model fairness is to exclude the protected\nattribute as a feature during training, a strategy known as fairness through unawareness [Chen et al.,\n2019]. However, this approach alone does not ensure fairness because the model may still learn\ncorrelations between other features and the protected attributes, see Section 3.1 and Figure 2b for\ndetails. To achieve a higher level of fairness, AI systems typically employ one of the additional\nmethods: pre-processing, in-training, or post-processing. We refer to Hort et al. [2023] for the most\nup-to-date and thorough survey.\nFairness without demographics. The information on the protected attribute is often not available\nin practice [Holstein et al., 2019, Hort et al., 2023]. Several works studied limited availability of the\nprotected attribute such as via a proxy [Gupta et al., 2018] or assuming there is a partial access to the\ninformation [Hashimoto et al., 2018, Awasthi et al., 2020, Molina et al., 2023]. Lahoti et al. [2020]\nrelies on the assumption that protected groups are computationally-identifiable. However, if there\nwere no signal about protected groups in the remaining features and class labels, we cannot make any\nstatements about improving the model for protected groups. One of the possible solutions is to get\ndata from secure multi-party computation [Veale and Binns, 2017, Kilbertus et al., 2018, Hu et al.,\n2019] or directly from users [Gkiouzepi et al., 2023]. However, these tools are still to be adapted to\nreal-world situations. In addition, transfer leaning can be useful when there is little available data on\nthe protected attributes [Coston et al., 2019]."}, {"title": "3 Fairness in advertising", "content": "The aim of ad-tech companies is to deliver the most relevant advertisements to users navigating\npublishers' webpages. By matching users' browsing histories and content preferences with products\nthat align with their interests, targeted advertising creates a mutually beneficial ecosystem [Wang\net al., 2017, Choi et al., 2020]. Advertisers reach relevant audiences, users have access to free\ninformation and services in exchange of seeing ads related to their interests, and platforms profit from\nselling targeted ads.\nAd-tech companies grapple with vast volumes of noisy data, which encapsulate users' past actions.\nLeveraging this data, they predict potential clicks and conversions. However, if the data is biased, the\nalgorithms can inadvertently perpetuate and even amplify these biases [Bolukbasi et al., 2016, Zhao\net al., 2017, Chen et al., 2023]. It is crucial to scrutinize the predictors for bias and devise solutions\nto mitigate it. Failing to do so can result in discrepancies between offline evaluations and online\nmetrics, ultimately harming user satisfaction and trust in the service of online systems [Chen et al.,\n2023]. While advertising commonplace items carries little risk, companies must exercise caution\nwith high-risk verticals like job offers [Speicher et al., 2018, Lambrecht and Tucker, 2019, Andreou"}, {"title": "", "content": "et al., 2019, Ali et al., 2019]. For instance, if managerial positions are disproportionately shown\nto men over women, more men may apply, perpetuating historical biases and exacerbating gender\ndisparities.\nBias can be introduced at several stages in the advertising process, see Figure 1. First, when a user\nvisits a webpage with an ad slot, ad-tech companies participate in a real-time bidding (RTB) auction.\nDuring this auction, companies select a campaign (e.g., job offers or clothing) based on attributes of\nthe publisher and the user, including their log of past interactions such as seen ads, their context, the\nfact of clicks on the ads, see Section 3.2. This auction must be organized in a fair way, respecting\nboth the companies placing bids and the publishers providing ad slots, see Section 3.3. After an\nad-tech company wins the display auction, there is the choice of which product to show (e.g., a senior\nposition job or an assistant job). This selection can also introduce bias with respect to the user, see\nSection 3.4. Ensuring fairness at this stage is critical to preventing the reinforcement of existing\ninequalities."}, {"title": "3.1 Fairness definition", "content": "We base our discussion on a counterfactual fairness framework that explains the underlying connec-\ntions between the variables in the system [Kusner et al., 2017]. Let $A$ denote a protected attribute\n(can be a set of protected attributes) of an individual, $X$ denote the other observable attributes of any\nparticular individual, $Y$ denote the outcome to be predicted, and let $\\hat{Y}$ be a predictor. The predictor\ntakes into account the available data from logs of user interactions with the system and product\ndescriptions and estimates the probability of a positive outcome, i.e. click of the user on the product.\nThe system takes into account the prediction and then shows the best product to the user, which\nresults into possible positive outcome. In our analysis, we are interested in understanding how $A$ and\n$X$ influence $Y$ and how well our predictor $\\hat{Y}$ captures these relationships. Our goal is not just to\npredict outcomes accurately but also to ensure fairness and mitigate biases in the predictions with\nrespect to $A$. There random variables have the causal relationships that are modelled on Fig. 2 and\nwe discuss further in detail."}, {"title": "3.2 Selection bias in campaign choosing", "content": "In our setting, we are interested in assessing the fairness in specific campaign (e.g., job campaign)\nwith respect to the protected attribute. For instance, we want to ensure that job advertisements\nfor managerial roles are fair with respect to a binary protected attribute $A \\in \\{0,1\\}$ (e.g., gender).\nTypically, the data considered in this framework regards the job advertisements for users which have\nbeen assigned to the job campaign $c$. However, the campaign selection process might introduce\nselection bias, which should be taken in account. In particular, $P(A = 1)$ and $P(A = 0)$ are the\n(internet)-population level of a binary protected attribute. This might be approximated to the census\npopulation frequencies of the protected attribute. Let $C$ be a random variable of choosing a campaign,\nthen $P(A = 1 | C = c)$ and $P(A = 0 | C = c)$ are the frequencies of the protected attribute in the\njob campaign data $c$. These differ from the population levels due to selection bias.\nNote that the recommendation engines predict $P(Y = 1 | A = a,C = c)$ for a product in the\ncampaign $c$. Thus, if we use prediction bias mitigation techniques while considering data at the\ncampaign level, in the best case scenario, we obtain fair predictions while being unfair outside of\ncampaing, $P(\\hat{Y} = 1 | A = 0, C = c) = P(\\hat{Y} = 1 | A = 1, C = c)$ and $P(\\hat{Y} = 1 | A = 0) \\neq$\n$P(\\hat{Y} = 1 | A = 1)$ . Thus, we have to take into account the selection bias to ensure demographic\nparity introduced in Eq. (2):\n$\\begin{aligned} \\frac{P(\\hat{Y} = 1 | A = 1, C = c) \\frac{P(A = 1)}{P(A = 1 | C = c)}}{P(\\hat{Y} = 1 | A = 0, C = c) \\frac{P(A = 0)}{P(A = 0 \\vert C = c)}} = 1. \\end{aligned}$"}, {"title": "3.3 Market bias", "content": "Lambrecht and Tucker [2019] found that women are a prized demographic, making them more\nexpensive to advertise to. This implies that ads that are meant to be gender-neutral can be delivered\nin the way that appears to be discriminatory by RTB algorithms that focus on optimizing cost-\neffectiveness. Ali et al. [2019] explained that this is not solely the indication of the ingrained cultural\nbias nor a result of user profiles inputted into ads algorithms, but rather the product of competitive\nspillovers among advertisers. Additionally, the feedback loop mechanism considers imbalanced"}, {"title": "3.4 Recommendation bias", "content": "In the ad recommendation system, the goal is to choose best products for a user for a given banner\nthat can have several displays at the same time. The goal is to maximize the number of clicks for a\ngiven banner, meaning that there can be several products clicked. When we have several displays to\nshow to a user, the display rank position becomes important and creates position bias with respect to\na positive outcome. The influence of this bias is hard to estimate, however, it is important to take it\ninto account [Singh and Joachims, 2018, 2019, Morik et al., 2020, Usunier et al., 2022].\nLet $J$ be a random variable denoting the set of banner to be shown to a user, $D$ be a display (chosen\nproduct, i.e. job offer) shown to a user on a banner $J$. Let model $f(x, d)$ predicts the following\npositive outcome: $P(Y = 1|X = x, D = d)$, i.e. the probability of a click for a chosen product $d$\ngiven user features $x$. As discussed above, we have to take into account the display position which\nexpressed via variable rank $R$. However, the influence of the position on the utility is hard to estimate.\nFurther, we suggest utility metrics for ads recommendation and in order to avoid the position bias,\nwe suggest to compute them only on randomized displays, where the position of the products on the\nbanner was chosen randomly.\nClick-rank utility. The users' utility for a given model can be expressed as a positive engagement\nin the following way:\n$U(f) = \\mathbb{E}_{I} \\mathbb{E}_{X,D|I} [I(Y_D = 1) \\text{rank}_I f(X_I, D)],$\nwhere $I(Y_D = 1)$ is the identity function of a positive outcome (e.g. click) for display $D$. The function\n$\\text{rank}_D$ computes the ascending order rank within the set of displays for a banner of impression $I$.\nThis metrics is based on estimation of the positive outcome based on the passed events for chosen\nusers.\nProduct-rank utility for biased data. We notice that the metrics for the algorithm can be biased\ndue to the selection bias discussed in Section 3.2 because the prediction algorithm estimates $P(Y =$\n$1|A = a, C = c)$ instead of $P(Y = 1|A = a)$. Even if we correct the prediction bias in $P(Y =$\n$1|A = a, C = c)$ based on the data provided for given campaign $c$, it does not correct the final bias\nin $P(Y = 1A = a)$ due to selection bias. We can adapt the click-rank utility to include possible\nselection bias into the metric, by explicitly considering that the product utility depends on a chosen\ncampaign. Then, when correcting for the unfairness in the prediction, we might improve the utility\nmetric taken into account the selection bias in the data:"}, {"title": "", "content": "$\\begin{aligned} \\mathbb{U}(f) = \\mathbb{E}_{D} \\mathbb{E}_{I |D} [I(Y_D = 1) \\frac{P(A = a_X)}{P(A = a_X |C = c)} \\text{rank}_I f(X_I, D)], \\end{aligned}$ where $a_X$ stands for a gender of a given user $X$. Intuitively, if the prediction is biased with respect to protected attribute $A$, the final prediction $P(Y = 1|A = a)$ is even more biased due to selection bias of with respect to the protected attribute of choosing a campaign $C = c$: $P(C = c | A = a)$. In this case, the prediction model amplifies the existing historical bias. However, we can remove the selection bias by adding weights that correspond to the presence of the protected attribute in the whole population and given the campaign. If the user with protected attribute $A = a$ has lower probability of click, and this group was underrepresented in the campaign $C = c$, i.e. $P(A = a) > P(A = a|C = c)$, then in the utility function, the model's prediction will be higher, by addressing the possible bias due to under-representation in the data. This is our suggested metric to evaluate the recommendation system when the selection bias is present and known such as in the FairJob dataset."}, {"title": "4 FairJobs dataset", "content": "We introduce FairJobs dataset that contains fairness-aware data from a real-world scenario of\nadvertising. This dataset is intended to learn click predictions models and evaluate by how much\ntheir predictions are biased between different gender groups. The dataset consists of 1,072,226\nrows that were collected during 5 months of a targeted job campaign, each row represents a job\nad and user features: 20 categorical and 39 numerical features; label click (binary, if the ad was\nclicked), protected_attribute (binary, proxy for user gender, see below for more thorough\nexplanation), senior (binary, if the job offer was for a senior position), [user_id, impression_id,\nproduct_id] are unqiue identifiers of user, impression and product (job ad). More details and dataset\nstatistics are referred to Appendix.\nDetails on gender proxy. Since we do not directly access user demographics, we have to find a way\nto get a proxy of relevant attribute. Most of recent works leverage the use of external data or prior\nknowledge on correlations to obtain proxies to relevant attributes [Gupta et al., 2018, Hashimoto et al.,\n2018, Awasthi et al., 2020, Lahoti et al., 2020]. We define a product gender, either given by a client,\neither by a category of the product. This gives us approximately 40% of products gender identified.\nThen, we follow the available statistics and choose the gender proxy based on the dominant gender of\nproducts the user interacts with. This gender proxy identities a behavior of a user, i.e. if a user tends\nto buy female or male products. The gender proxy does not necessarily correlate with the gender,\nas it often happens with the proxy variables [Gelauff et al., 2020]. Verification of the accuracy of\nthese approximations is challenging. Additionally, if there are no signal about protected groups in the\nremaining features and class labels, we cannot make any statements about improving the model for\nprotected groups [Lahoti et al., 2020].\nLimitations and interpretation. We remark that the proposed gender proxy does not give a\ndefinition of the gender. Since we do not have access to the sensitive information, this is the best\nsolution we have identified at this stage to identify bias on pseudonymised data, and we encourage\nany discussion on better approximations. This proxy is reported as binary for simplicity yet we\nacknowledge gender is not necessarily binary. Although our research focuses on gender, this should\nnot diminish the importance of investigating other types of algorithmic discrimination. While this\ndataset provides important application of fairness-aware algorithms in a high-risk domain, there are\nseveral fundamental limitation that can not be addressed easily through data collection or curation\nprocesses. These limitations include historical bias that affect a positive outcome for a given user, as\nwell as the impossibility to verify how close the gender-proxy is to the real gender value. Additionally,\nthere might be bias due to the market unfairness that we explained in Section 3.3. Such limitations"}, {"title": "5 Empirical observations", "content": "Challenges. The first challenge comes from handling the different types of data that are common\nin tables, the mixed-type columns: there are both numerical and categorical features that have to\nbe embedded [Gorishniy et al., 2021, 2022, Grinsztajn et al., 2022, Shwartz-Ziv and Armon, 2022,\nMatteucci et al., 2023]. In addition, some of the features have long-tail phenomenon and products\nhave popularity bias, see Figure 4. Our datasets contains more than 1,000,000 lines, while current\nhigh-performing models are under-explored in scale, e.g. the largest datasets in Grinsztajn et al.\n[2022] are only 50,000 lines, while in Gorishniy et al. [2021, 2022] only one dataset surpasses\n1,000,000 lines. Additional challenge comes from strongly imbalanced data: the positive class\nproportion in our data is less than 0.007 that leads to challenges in training robust and fair machine\nlearning models [Jesus et al., 2022, Yang et al., 2024]. In our dataset there is no significant imbalances\nin demographic groups users regarding the protected attribute (both genders are sub-sampled with 0.5\nproportion, female profile users were shown less job ad with 0.4 proportion and slightly less senior\nposition jobs with 0.48 proportion), however, there could be a hidden effect of a bias that we discussed\nin Section 3. This poses a problem in accurately assessing model performance [van Breugel et al.,\n2024]. More detailed statistics and exploratory analysis are referred to the supplemental material."}, {"title": "", "content": "Results. We choose two baseline methods: (i) unfair, that uses all attributes for training, including\nthe protected one; and (ii) unaware, that corresponds to fairness through unawareness, i.e. using all\nattributes during training except the protected one. We compare the results of these algorithms with a\n(iii) fair model that is trained without protected attribute with additional \"fairness\" penalty [Kamishima\net al., 2011, Bechavod and Ligett, 2017]. The three models correspond to the situations described\nin Figure 2. We refer all the reproducibility details and additional experiments to the supplemental\nmaterial. The resulted prediction can be visually compared in Figure 5.\nThese findings suggest that the trade-off relationship between accuracy and fairness is context-\ndependent. It highlights the need for further research to better understand the conditions under which\nthe accuracy fairness trade-off arises and identify strategies to mitigate or overcome it.\nAdditionally, we propose to restraint an access to the protected attribute and study the trade-off when\nwe train the model on the whole train set but add fairness penalty only for some percentage of train\nset. In some scenarios, we see improvements in fairness without sacrificing the overall performance.\nThe loss in accuracy due to the imposed fairness constraints is often small as also noted in other\nworks [Celis et al., 2019b]. We explore bias correction techniques tailored to address data limitations\nand preserve utility in large-scale. We demonstrate how prioritizing fairness in AI not only benefits\nusers by fostering inclusivity but also contributes to the long-term success and ethical integrity of\ncompanies. Details and results of the experiments are reported in the supplemental material."}, {"title": "6 Conclusion", "content": "Addressing bias in AI goes beyond mere compliance with legal frameworks like the AI Act; it\nnecessitates proactive measures to detect, prevent, and mitigate biases. Drawing from real-world\nchallenges faced by industries, we highlight the limitations of existing bias mitigation strategies,\nparticularly in environments where access to sensitive user attributes is restricted. We encourage\nother authors and practitioners to experiment with different AI or Fair AI algorithms on this dataset.\nWe expect that with this work, the quality of evaluation of novel AI methods increases, potentiating\nthe development of the area. Additionally, we hope it encourages other similar relevant datasets to be\npublished from other authors and institutions."}, {"title": "Appendix", "content": "We firstly describe in detail how the dataset FairJob was collected, then provide all the information on\nthe context and features with its statistics. Further, we perform experiments and provide all the steps\nfor the sake of reproducubility. The dataset is hosted at https://huggingface.co/datasets/criteo/FairJob.\nSource code for the experiments is hosted at https://github.com/criteo-research/FairJob-dataset/.\nAuthor statement of responsibility. Authors and Criteo bear all responsibility in case of violation\nof rights and confirmation of the data license."}, {"title": "A Dataset information", "content": "As illustrated in Figure 6, the process starts with users navigating Publisher and Advertiser websites\n(typically newspapers and retailer shops respectively). Upon user consent10, user information\nabout the events such as visits or product views\u00b9\u00b9 are collected and identified by means of browser\ncookies. Users are subject to personalized advertising (if the job campaign was chosen and the\ndisplay opportunity was won) until the end of the data collection period. Subsequently, only won\ndisplays coming from Publisher and Advertiser partners are joined by cookie identifier on the AdTech\nplatform to form the raw dataset, dropping cookie ids when they are not needed anymore. To ensure\nconfidentiality, the data has been sub-sampled non-uniformly to avoid disclosing business metrics.\nFeature names have been anonymized, and their values randomly projected to preserve predictive\npower while rendering the recovery of the original features or user context practically impossible12.\nThe dataset does not contain the relevant attributes such as gender; however, it includes a gender\nproxy which we discuss in detail in the main text."}, {"title": "A.2 Dataset detailed description", "content": "The dataset contains pseudononymized users' context and publisher features that was collected from\na job targeting campaign ran for 5 months by Criteo AdTech company. Each line represents a product\nthat was shown to a user. Each user has an impression session where they can see several products\nat the same time. Each product can be clicked or not clicked by the user. The dataset consists of\n1072226 rows and 55 columns:\n\u2022 user_id is a unique identifier assigned to each user. This identifier has been anonymized\nand does not contain any information related to the real users.\n\u2022 product_id is a unique identifier assigned to each product", "feature": "if the product title contains words describing managerial role (e.g.\n'president'", "ceo": "and others)", "click": "lower rank means higher position of the product on the display.\n\u2022 displayrandom is a binary feature that equals 1 if the display position on the banner of the\nproducts associated with the same impression_id was randomized. The click-rank metric\nshould be computed on displayrandom = 1 to avoid positional bias.\n\u2022 click is a binary feature that equals 1 if the product product_id in the impression\nimcontinue generating json\n```"}]}]}