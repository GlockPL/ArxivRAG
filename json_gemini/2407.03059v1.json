{"title": "FairJob: A Real-World Dataset for Fairness in Online Systems", "authors": ["Mariia Vladimirova", "Eustache Diemert", "Federico Pavone"], "abstract": "We introduce a fairness-aware dataset for job recommendation in advertising, designed to foster research in algorithmic fairness within real-world scenarios. It was collected and prepared to comply with privacy standards and business confidentiality. An additional challenge is the lack of access to protected user attributes such as gender, for which we propose a solution to obtain a proxy estimate. Despite being anonymized and including a proxy for a sensitive attribute, our dataset preserves predictive power and maintains a realistic and challenging benchmark. This dataset addresses a significant gap in the availability of fairness-focused resources for high-impact domains like advertising \u2013 the actual impact being having access or not to precious employment opportunities, where balancing fairness and utility is a common industrial challenge. We also explore various stages in the advertising process where unfairness can occur and introduce a method to compute a fair utility metric for the job recommendations in online systems case from a biased dataset. Experimental evaluations of bias mitigation techniques on the released dataset demonstrate potential improvements in fairness and the associated trade-offs with utility.", "sections": [{"title": "1 Introduction", "content": "The intersection of technology and human dynamics presents both opportunities and challenges, particularly in the realm of artificial intelligence (AI). Despite advancements, persistent biases rooted in historical inequalities permeate our data-driven systems, perpetuating unfairness and exacerbating societal divides. Historical biases shape data collection, influencing AI model outcomes and often amplifying existing inequalities [Bolukbasi et al., 2016, Zhao et al., 2017, Chen et al., 2023]. Despite concerns regarding privacy, liability, and public relations, the collection of special and sensitive category data is crucial for bias assessments [Andrus et al., 2021]. Moreover, evolving legal frameworks, exemplified by the recent AI Act and General Data Protector Regulation [UK Information Commissioner's Office, 2022], mandate the detection, prevention, and mitigation of biases, while imposing some restrictions on the use of sensitive data.\nRecent advances in fairness often involve computer vision, natural language processing and speech recognition tasks [Gustafson et al., 2023, Andrews et al., 2024, Hall et al., 2024, Schumann et al., 2024, Veliche and Fung, 2023], while lacking attention to algorithmic decision-making that involves tabular data, where each row represents an individual or an observation, and each column represents a feature or attribute [Le Quy et al., 2022, Zhang et al., 2021], resulting in a very few benchmark papers [Gorishniy et al., 2021, 2022, Grinsztajn et al., 2022, Shwartz-Ziv and Armon, 2022, Matteucci et al., 2023]. Tabular data is commonly used in various high-risk domains such as finance, healthcare, hiring, criminal justice, and advertising [van Breugel and van der Schaar, 2024].\nAlgorithmic discrimination in advertising can be related to sensitive verticals which highlights beneficial employment, financial and housing opportunities, or about who sees potentially less desirable advertising, such as ads for predatory lending services [Lambrecht and Tucker, 2019]. While unfairness in advertising is not punitive but rather assistive, i.e. fairness consists in providing equal access to precious opportunities, it is essential to ensure fairness in advertising practices. In some contexts such as housing or lending, such discrimination is explicitly prohibited by law\u00b2. Several studies conducted analyses on the fairness in advertising at different stages and observed discriminating behavior that was not necessarily intended by the ad-services [Speicher et al., 2018, Lambrecht and Tucker, 2019, Andreou et al., 2019, Ali et al., 2019]. This emphasizes the need for better mechanisms to audit and prevent bias in ads.\nMost of studies on discriminating behavior in advertising were conducted via creating advertising campaigns and choosing targeted audiences and analysing the data from the user perspective without accessing the algorithmic features [Speicher et al., 2018, Lambrecht and Tucker, 2019, Andreou et al., 2019, Ali et al., 2019]. The absence of publicly available, realistic datasets leads researchers to publish results based on private data, resulting in non-reproducible claims [Geyik et al., 2019, Andreou et al., 2019, Timmaraju et al., 2023, Tang and Yu, 2022]. This poses challenges for critical evaluation and building upon previous work in the scientific community. Tang and Yu [2022] highlights the lack of public benchmarking datasets to study the fairness related approaches in advertising.\nIn addition, most of the studies assume that the AI systems have an access to the protected attributes which is often unrealistic due to privacy constraints or legal restrictions [Holstein et al., 2019, Lahoti et al., 2020, Molina et al., 2023, Timmaraju et al., 2023]. In online advertising, decision-makers usually have access to a log of user interactions with the system, which they can use to guess the attributes. However, the level of inaccuracy can be significant, making it difficult to ensure that an ad campaign reaches a non-discriminatory audience [Gelauff et al., 2020]. This makes it hard to meet fairness requirements [Lipton et al., 2018]. We emphasize the need for thorough research in real-world situations where access to protected attributes is limited.\nContributions. To foster research in fairness within real-world scenarios, we release a large-scale fairness-aware dataset for advertising. The dataset contains pseudononymized users' context and publisher features that were collected from a job targeting campaign ran for 5 months. The data has been sub-sampled non-uniformly to avoid disclosing business metrics. Feature names have been anonymized for business confidentiality, and their values randomly projected to preserve predictive power while making the recovery of the original features or user context (i.e. re-indentification) practically impossible, with accordance to the privacy-safety measures\u00b3. Although our dataset does not contain explicit sensitive attributes such as gender, it includes a gender proxy derived from non-protected relevant attributes, which we discuss in detail further.\nThis dataset provides a baseline according to the eligible audience generated by an advertiser's targeting criteria for a specific ad. This ensures that ads are tailored to individuals whom the advertiser can feasibly serve (such as those within a specific geographic region) and who are likely to be interested in their offerings, a practice already governed by policies and standards in Housing, Employment, and Credit verticals. Since advertiser targeting adheres to policy constraints to prevent discriminatory practices\u2074 (such as prohibiting the use of gender criteria in employment ads), the resulting eligible audience remains independent of prediction algorithms, serving as a reasonable baseline metric.\nWith the released dataset we examine the stages in the advertising process where unfairness can occur and explore techniques to mitigate such biases. Taking into account possible induced biases, we propose an unbiased utility metric that help to analyse different bias mitigation techniques. We also"}, {"title": "2 Related works", "content": "Open-source datasets. A limited availability of publicly available fairness-aware tabular datasets challenges research advancements in algorithmic fairness [Le Quy et al., 2022, Hort et al., 2023]. In 2022, Le Quy et al. [2022] studied datasets used at least 3 times in research publications on fairness, and found out there were only 15 open-source fairness datasets, most of which are criticized for being too small or far from real-world scenarios, including the most frequently used Adult [Dua and Graf, 2017] and COMPAS dataset [Larson et al., 2016]. Even though there is a positive tendency on addressing this issue by open-sourcing privacy-complying datasets, such as BAF [Jesus et al., 2022] for bank fraud detection where the data was obtained via data generation techniques, or WCLD [Ash et al., 2024], a curated large-scale dataset from circuit courts to address criminal justice, there is still lack in available datasets in other high-impact areas such advertising. It is important for academic researchers to have access to large datasets to study the problem rigorously [L. Cardoso et al., 2019, Li et al., 2022, Le Quy et al., 2022]. Large-scale datasets are advantageous as they increase the likelihood of capturing significant performance differences in experiments with new methods. With larger dataset sizes, the variance of metrics decreases, enabling more reliable and meaningful comparisons between different approaches.\nBias mitigation methods. The initial step to enhance model fairness is to exclude the protected attribute as a feature during training, a strategy known as fairness through unawareness [Chen et al., 2019]. However, this approach alone does not ensure fairness because the model may still learn correlations between other features and the protected attributes, see Section 3.1 and Figure 2b for details. To achieve a higher level of fairness, AI systems typically employ one of the additional methods: pre-processing, in-training, or post-processing. We refer to Hort et al. [2023] for the most up-to-date and thorough survey.\nFairness without demographics. The information on the protected attribute is often not available in practice [Holstein et al., 2019, Hort et al., 2023]. Several works studied limited availability of the protected attribute such as via a proxy [Gupta et al., 2018] or assuming there is a partial access to the information [Hashimoto et al., 2018, Awasthi et al., 2020, Molina et al., 2023]. Lahoti et al. [2020] relies on the assumption that protected groups are computationally-identifiable. However, if there were no signal about protected groups in the remaining features and class labels, we cannot make any statements about improving the model for protected groups. One of the possible solutions is to get data from secure multi-party computation [Veale and Binns, 2017, Kilbertus et al., 2018, Hu et al., 2019] or directly from users [Gkiouzepi et al., 2023]. However, these tools are still to be adapted to real-world situations. In addition, transfer leaning can be useful when there is little available data on the protected attributes [Coston et al., 2019]."}, {"title": "3 Fairness in advertising", "content": "The aim of ad-tech companies is to deliver the most relevant advertisements to users navigating publishers' webpages. By matching users' browsing histories and content preferences with products that align with their interests, targeted advertising creates a mutually beneficial ecosystem [Wang et al., 2017, Choi et al., 2020]. Advertisers reach relevant audiences, users have access to free information and services in exchange of seeing ads related to their interests, and platforms profit from selling targeted ads.\nAd-tech companies grapple with vast volumes of noisy data, which encapsulate users' past actions. Leveraging this data, they predict potential clicks and conversions. However, if the data is biased, the algorithms can inadvertently perpetuate and even amplify these biases [Bolukbasi et al., 2016, Zhao et al., 2017, Chen et al., 2023]. It is crucial to scrutinize the predictors for bias and devise solutions to mitigate it. Failing to do so can result in discrepancies between offline evaluations and online metrics, ultimately harming user satisfaction and trust in the service of online systems [Chen et al., 2023]. While advertising commonplace items carries little risk, companies must exercise caution with high-risk verticals like job offers [Speicher et al., 2018, Lambrecht and Tucker, 2019, Andreou et al., 2019, Ali et al., 2019]. For instance, if managerial positions are disproportionately shown to men over women, more men may apply, perpetuating historical biases and exacerbating gender disparities.\nBias can be introduced at several stages in the advertising process, see Figure 1. First, when a user visits a webpage with an ad slot, ad-tech companies participate in a real-time bidding (RTB) auction. During this auction, companies select a campaign (e.g., job offers or clothing) based on attributes of the publisher and the user, including their log of past interactions such as seen ads, their context, the fact of clicks on the ads, see Section 3.2. This auction must be organized in a fair way, respecting both the companies placing bids and the publishers providing ad slots, see Section 3.3. After an ad-tech company wins the display auction, there is the choice of which product to show (e.g., a senior position job or an assistant job). This selection can also introduce bias with respect to the user, see Section 3.4. Ensuring fairness at this stage is critical to preventing the reinforcement of existing inequalities."}, {"title": "3.1 Fairness definition", "content": "We base our discussion on a counterfactual fairness framework that explains the underlying connec-tions between the variables in the system [Kusner et al., 2017]. Let A denote a protected attribute (can be a set of protected attributes) of an individual, X denote the other observable attributes of any particular individual, Y denote the outcome to be predicted, and let \u0176 be a predictor. The predictor takes into account the available data from logs of user interactions with the system and product descriptions and estimates the probability of a positive outcome, i.e. click of the user on the product. The system takes into account the prediction and then shows the best product to the user, which results into possible positive outcome. In our analysis, we are interested in understanding how A and X influence Y and how well our predictor \u0176 captures these relationships. Our goal is not just to predict outcomes accurately but also to ensure fairness and mitigate biases in the predictions with respect to A. There random variables have the causal relationships that are modelled on Fig. 2 and we discuss further in detail."}, {"title": "3.2 Selection bias in campaign choosing", "content": "In our setting, we are interested in assessing the fairness in specific campaign (e.g., job campaign) with respect to the protected attribute. For instance, we want to ensure that job advertisements for managerial roles are fair with respect to a binary protected attribute $A \\in \\{0,1\\}$ (e.g., gender). Typically, the data considered in this framework regards the job advertisements for users which have been assigned to the job campaign c. However, the campaign selection process might introduce selection bias, which should be taken in account. In particular, $P(A = 1)$ and $P(A = 0)$ are the (internet)-population level of a binary protected attribute. This might be approximated to the census population frequencies of the protected attribute. Let C be a random variable of choosing a campaign, then $P(A = 1 | C = c)$ and $P(A = 0 | C = c)$ are the frequencies of the protected attribute in the job campaign data c. These differ from the population levels due to selection bias.\nNote that the recommendation engines predict $P(Y = 1 | A = a,C = c)$ for a product in the campaign c. Thus, if we use prediction bias mitigation techniques while considering data at the campaign level, in the best case scenario, we obtain fair predictions while being unfair outside of campaing, $P(\\hat{Y} = 1 | A = 0, C = c) = P(\\hat{Y} = 1 | A = 1, C = c)$ and $P(\\hat{Y} = 1 | A = 0) \\neq P(\\hat{Y} = 1 | A = 1)$. Thus, we have to take into account the selection bias to ensure demographic parity introduced in Eq. (2):\n$\\frac{P(\\hat{Y} = 1 \\vert A = 1, C = c) \\frac{P(A = 1)}{P(A = 1 \\vert C = c)}}{P(\\hat{Y} = 1 \\vert A = 0, C = c) \\frac{P(A = 0)}{P(A = 0 \\vert C = c)}}$"}, {"title": "3.3 Market bias", "content": "Lambrecht and Tucker [2019] found that women are a prized demographic, making them more expensive to advertise to. This implies that ads that are meant to be gender-neutral can be delivered in the way that appears to be discriminatory by RTB algorithms that focus on optimizing cost-effectiveness. Ali et al. [2019] explained that this is not solely the indication of the ingrained cultural bias nor a result of user profiles inputted into ads algorithms, but rather the product of competitive spillovers among advertisers. Additionally, the feedback loop mechanism considers imbalanced information-how recommendation systems expose content influences user behavior, which then becomes the training data for future predictions. This feedback loop not only introduces biases but also amplifies them over time, leading to a 'rich get richer' scenario known as the Matthew effect. [Chen et al., 2023]. Imbalanced data with respect to a protected attribute also effects the learning of a prediction, since an algorithm that receives in real time less data about one group, will learn at different speeds [Lambrecht and Tucker, 2020]. These effects are hard to estimate and should be addressed by the RTB process. Apart from users, advertisers can also be unfairly treated during the RBT auction process [Celis et al., 2019a, Chen et al., 2023] but here we focus solely on the user discrimination."}, {"title": "3.4 Recommendation bias", "content": "In the ad recommendation system, the goal is to choose best products for a user for a given banner that can have several displays at the same time. The goal is to maximize the number of clicks for a given banner, meaning that there can be several products clicked. When we have several displays to show to a user, the display rank position becomes important and creates position bias with respect to a positive outcome. The influence of this bias is hard to estimate, however, it is important to take it into account [Singh and Joachims, 2018, 2019, Morik et al., 2020, Usunier et al., 2022].\nLet J be a random variable denoting the set of banner to be shown to a user, D be a display (chosen product, i.e. job offer) shown to a user on a banner J. Let model $f(x, d)$ predicts the following positive outcome: $P(Y = 1|X = x, D = d)$, i.e. the probability of a click for a chosen product d given user features x. As discussed above, we have to take into account the display position which expressed via variable rank R. However, the influence of the position on the utility is hard to estimate. Further, we suggest utility metrics for ads recommendation and in order to avoid the position bias, we suggest to compute them only on randomized displays, where the position of the products on the banner was chosen randomly.\nClick-rank utility. The users' utility for a given model can be expressed as a positive engagement in the following way:\n$\\mathcal{U}(f) = \\mathbb{E}_{I} \\mathbb{E}_{X,D \\vert I} [I(\\mathbb{Y}_{D} = 1) rank_{I} f(X_{I}, D)],$\nwhere $I(\\mathbb{Y}_{D} = 1)$ is the identity function of a positive outcome (e.g. click) for display D. The function $rank_{D}$ computes the ascending order rank within the set of displays for a banner of impression I. This metrics is based on estimation of the positive outcome based on the passed events for chosen users.\nProduct-rank utility for biased data. We notice that the metrics for the algorithm can be biased due to the selection bias discussed in Section 3.2 because the prediction algorithm estimates $P(Y = 1|A = a, C = c)$ instead of $P(Y = 1|A = a)$. Even if we correct the prediction bias in $P(Y = 1|A = a, C = c)$ based on the data provided for given campaign c, it does not correct the final bias in $P(Y = 1 \\vert A = a)$ due to selection bias. We can adapt the click-rank utility to include possible selection bias into the metric, by explicitly considering that the product utility depends on a chosen campaign. Then, when correcting for the unfairness in the prediction, we might improve the utility metric taken into account the selection bias in the data:"}, {"title": "4 FairJobs dataset", "content": "We introduce FairJobs dataset that contains fairness-aware data from a real-world scenario of advertising. This dataset is intended to learn click predictions models and evaluate by how much their predictions are biased between different gender groups. The dataset consists of 1,072,226 rows that were collected during 5 months of a targeted job campaign, each row represents a job ad and user features: 20 categorical and 39 numerical features; label click (binary, if the ad was clicked), protected_attribute (binary, proxy for user gender, see below for more thorough explanation), senior (binary, if the job offer was for a senior position), [user_id, impression_id, product_id] are unqiue identifiers of user, impression and product (job ad). More details and dataset statistics are referred to Appendix.\nDetails on gender proxy. Since we do not directly access user demographics, we have to find a way to get a proxy of relevant attribute. Most of recent works leverage the use of external data or prior knowledge on correlations to obtain proxies to relevant attributes [Gupta et al., 2018, Hashimoto et al., 2018, Awasthi et al., 2020, Lahoti et al., 2020]. We define a product gender, either given by a client, either by a category of the product. This gives us approximately 40% of products gender identified. Then, we follow the available statistics and choose the gender proxy based on the dominant gender of products the user interacts with. This gender proxy identities a behavior of a user, i.e. if a user tends to buy female or male products. The gender proxy does not necessarily correlate with the gender, as it often happens with the proxy variables [Gelauff et al., 2020]. Verification of the accuracy of these approximations is challenging. Additionally, if there are no signal about protected groups in the remaining features and class labels, we cannot make any statements about improving the model for protected groups [Lahoti et al., 2020].\nLimitations and interpretation. We remark that the proposed gender proxy does not give a definition of the gender. Since we do not have access to the sensitive information, this is the best solution we have identified at this stage to identify bias on pseudonymised data, and we encourage any discussion on better approximations. This proxy is reported as binary for simplicity yet we acknowledge gender is not necessarily binary. Although our research focuses on gender, this should not diminish the importance of investigating other types of algorithmic discrimination. While this dataset provides important application of fairness-aware algorithms in a high-risk domain, there are several fundamental limitation that can not be addressed easily through data collection or curation processes. These limitations include historical bias that affect a positive outcome for a given user, as well as the impossibility to verify how close the gender-proxy is to the real gender value. Additionally, there might be bias due to the market unfairness that we explained in Section 3.3. Such limitations"}, {"title": "5 Empirical observations", "content": "Challenges. The first challenge comes from handling the different types of data that are common in tables, the mixed-type columns: there are both numerical and categorical features that have to be embedded [Gorishniy et al., 2021, 2022, Grinsztajn et al., 2022, Shwartz-Ziv and Armon, 2022, Matteucci et al., 2023]. In addition, some of the features have long-tail phenomenon and products have popularity bias, see Figure 4. Our datasets contains more than 1,000,000 lines, while current high-performing models are under-explored in scale, e.g. the largest datasets in Grinsztajn et al. [2022] are only 50,000 lines, while in Gorishniy et al. [2021, 2022] only one dataset surpasses 1,000,000 lines. Additional challenge comes from strongly imbalanced data: the positive class proportion in our data is less than 0.007 that leads to challenges in training robust and fair machine learning models [Jesus et al., 2022, Yang et al., 2024]. In our dataset there is no significant imbalances in demographic groups users regarding the protected attribute (both genders are sub-sampled with 0.5 proportion, female profile users were shown less job ad with 0.4 proportion and slightly less senior position jobs with 0.48 proportion), however, there could be a hidden effect of a bias that we discussed in Section 3. This poses a problem in accurately assessing model performance [van Breugel et al., 2024]. More detailed statistics and exploratory analysis are referred to the supplemental material.\nResults. We choose two baseline methods: (i) unfair, that uses all attributes for training, including the protected one; and (ii) unaware, that corresponds to fairness through unawareness, i.e. using all attributes during training except the protected one. We compare the results of these algorithms with a (iii) fair model that is trained without protected attribute with additional \"fairness\" penalty [Kamishima et al., 2011, Bechavod and Ligett, 2017]. The three models correspond to the situations described in Figure 2. We refer all the reproducibility details and additional experiments to the supplemental material. The resulted prediction can be visually compared in Figure 5.\nThese findings suggest that the trade-off relationship between accuracy and fairness is context-dependent. It highlights the need for further research to better understand the conditions under which the accuracy fairness trade-off arises and identify strategies to mitigate or overcome it.\nAdditionally, we propose to restraint an access to the protected attribute and study the trade-off when we train the model on the whole train set but add fairness penalty only for some percentage of train set. In some scenarios, we see improvements in fairness without sacrificing the overall performance. The loss in accuracy due to the imposed fairness constraints is often small as also noted in other works [Celis et al., 2019b]. We explore bias correction techniques tailored to address data limitations and preserve utility in large-scale. We demonstrate how prioritizing fairness in AI not only benefits users by fostering inclusivity but also contributes to the long-term success and ethical integrity of companies. Details and results of the experiments are reported in the supplemental material."}, {"title": "6 Conclusion", "content": "Addressing bias in AI goes beyond mere compliance with legal frameworks like the AI Act; it necessitates proactive measures to detect, prevent, and mitigate biases. Drawing from real-world challenges faced by industries, we highlight the limitations of existing bias mitigation strategies, particularly in environments where access to sensitive user attributes is restricted. We encourage other authors and practitioners to experiment with different AI or Fair AI algorithms on this dataset. We expect that with this work, the quality of evaluation of novel AI methods increases, potentiating the development of the area. Additionally, we hope it encourages other similar relevant datasets to be published from other authors and institutions."}, {"title": "A.1 Data collection and use", "content": "As illustrated in Figure 6, the process starts with users navigating Publisher and Advertiser websites (typically newspapers and retailer shops respectively). Upon user consent10, user information about the events such as visits or product views\u00b9\u00b9 are collected and identified by means of browser cookies. Users are subject to personalized advertising (if the job campaign was chosen and the display opportunity was won) until the end of the data collection period. Subsequently, only won displays coming from Publisher and Advertiser partners are joined by cookie identifier on the AdTech platform to form the raw dataset, dropping cookie ids when they are not needed anymore. To ensure confidentiality, the data has been sub-sampled non-uniformly to avoid disclosing business metrics. Feature names have been anonymized, and their values randomly projected to preserve predictive power while rendering the recovery of the original features or user context practically impossible12. The dataset does not contain the relevant attributes such as gender; however, it includes a gender proxy which we discuss in detail in the main text."}, {"title": "License and intended use.", "content": "The data is released under the CC-BY-NC-SA 4.0 license which gives liberty to Share and Adapt this data provided that the respect of the Attribution, NonCommercial and ShareAlike conditions. We focus in this paper on algorithmic fairness analysis as a specific use-case, but it does not have to restricted to this. For example, the dataset can be used as a baseline for improving deep learning models on tabular data and to study methods on embedding creation for numerical and categorical features as done by Gorishniy et al. [2021, 2022], Grinsztajn et al. [2022], Shwartz-Ziv and Armon [2022], Matteucci et al. [2023]. Another possible usage is to explore privacy-preserving techniques for tabular data as in Donhauser et al. [2024]. Additionally, this dataset can be used to improve the generation techniques on tabular data, as described in [Jesus et al., 2022, van Breugel and van der Schaar, 2024]. We compare FairJob dataset details to other commonly used"}, {"title": "A.2 Dataset detailed description", "content": "The dataset contains pseudononymized users' context and publisher features that was collected from a job targeting campaign ran for 5 months by Criteo AdTech company. Each line represents a product that was shown to a user. Each user has an impression session where they can see several products at the same time. Each product can be clicked or not clicked by the user. The dataset consists of 1072226 rows and 55 columns:\n\u2022 user_id is a unique identifier assigned to each user. This identifier has been anonymized and does not contain any information related to the real users.\n\u2022 product_id is a unique identifier assigned to each product, i.e. job offer.\n\u2022 impression_id is a unique identifier assigned to each impression, i.e. online session that can have several products at the same time.\n\u2022 cat0, ..., cat5 are anonymized categorical user features.\n\u2022 cat6, ..., cat12 are anonymized categorical product features.\n\u2022 num13,..., num47 are anonymized numerical user features.\n\u2022 protected_attribute is a binary feature that describes user gender proxy, i.e. female is 0, male is 1. The detailed description on the meaning can be found in the main paper.\n\u2022 senior is a binary feature that describes the seniority of the job position, i.e. an assistant role is 0, a managerial role is 1. This feature was created during data processing step from the product title feature: if the product title contains words describing managerial role (e.g. 'president', 'ceo', and others), it is assigned to 1, otherwise to 0.\n\u2022 rank is a numerical feature that corresponds to the positional rank of the product on the display for given impression_id. Usually, the position on the display creates the bias with respect to the click: lower rank means higher position of the product on the display.\n\u2022 displayrandom is a binary feature that equals 1 if the display position on the banner of the products associated with the same impression_id was randomized. The click-rank metric should be computed on displayrandom = 1 to avoid positional bias.\n\u2022 click is a binary feature that equals 1 if the product product_id in the impression impression_id was clicked by the user user_id."}, {"title": "A.3 Dataset statistics", "content": "Table 1, 2 and 3 demonstrate the available statistics for categorical features cardinalities, index features and binary features respectively. Additionally, we provide statistics for selection bias of the job campaign that comes from the Criteo AdTech company (outside of FairJob dataset) in Table 4, which we take into account further to compute utility metrics.\nTable 4: Statistics for job campaign selection bias: we see that while female (internet)-population is slightly larger than male, the female population in the job campaign is much smaller, leading to selection bias of the job campaign \u2013 male users are almost twice more picked than female users."}, {"title": "B.1 Metrics", "content": "Fairness metrics. To measure fairness of the model, we consider demographic parity on senior job opportunities that computes the average difference of predictions given the protected attribute and the job seniority:\n$\\mathcal{D}P(\\hat{Y} \\vert A) = \\mathbb{E} (\\hat{Y} \\vert A = 1, S = 1) - \\mathbb{E} (\\hat{Y} \\vert A = 0, S = 1).\\qquad(6)$\nDemographic parity requires equal proportion of positive predictions in each group (\"No Disparate Impact\") which in our case can be translated as same proportion of shown senior job ads in each group of the protected attribute. Demographic parity can be thought of as a stronger version of the US Equal Employment Opportunity Commission's \"four-fifths rule\", which requires that the \u201cselection rate for any race, sex, or ethnic group [must be at least] four-fifths (4/5) (or eighty percent) of the rate for the group with the highest rate13.\nEqualized odds [Zafar et al., 2017] ensures that a machine learning model works equally well for different groups. In case of job ads, equalized odds would force the prediction of both positive and negative outcomes to be the same which might generate more false positive predictions for one group versus others, resulting in worse outcome. Equal of opportunity [Hardt et al., 2016] ensures that a machine learning model works equally well only on positive outcomes for different groups which results in capturing the costs of missclassification disparities.\nPerformance metrics. The loss function is log-loss and report it as NLLH (negative log-likelihood). We also report AUC (Area under the ROC Curve) as a description of prediction power on strongly imbalanced data in binary classification problems. Additionally, we consider click-rank utility $\\mathcal{U}$ and product-rank utility for biased data $\\bar{\\mathcal{U}}$, proposed in the main text Section 3.4."}, {"title": "B.2 Models.", "content": "Training regimes. We train models in the following ways:\n\u2022 unfair - the model uses protected attribute as a feature in the data,\n\u2022 unaware - the model does not use the protected attribute in the data, corresponds to fairness through unawareness,\n\u2022 fair - the model does not use the protected attribute in the data and trained with an additional fairness-enforcing penalty in the loss."}, {"title": "Algorithm.", "content": "As baseline results", "regimes": "n\u2022 Dummy - classifier based on a single threshold for positive class probability (unaware)", "2011": "n$\\hat{Y} = arg\\,\\text{min}\\,\\mathcal{L}(\\hat{Y}, Y) + \\lambda \\cdot Penalty(\\hat{Y}, Y, A).\\qquad(7)$\nParameter $\\lambda$, or fairness_multiplier, controls the trade-off between the model's predictive accuracy $\\mathcal{L}(\\hat{Y}, Y)$ and fairness. Adjustments on $\\lambda$ allows to control the importance of fairness relative to accuracy. These methods remove the influence of protected attribute on the model's output without restrictions on the data [Kamishima et al., 2011, Bechavod and Ligett, 2017, Mary et al., 2019"}]}