{"title": "On Discrete Prompt Optimization for Diffusion Models", "authors": ["Ruochen Wang", "Ting Liu", "Cho-Jui Hsieh", "Boqing Gong"], "abstract": "This paper introduces the first gradient-based framework for prompt optimization in text-to-image diffusion models. We formulate prompt engineering as a discrete optimization problem over the language space. Two major challenges arise in efficiently finding a solution to this problem: (1) Enormous Domain Space: Setting the domain to the entire language space poses significant difficulty to the optimization process. (2) Text Gradient: Efficiently computing the text gradient is challenging, as it requires backpropagating through the inference steps of the diffusion model and a non-differentiable embedding lookup table. Beyond the problem formulation, our main technical contributions lie in solving the above challenges. First, we design a family of dynamically generated compact subspaces comprised of only the most relevant words to user input, substantially restricting the domain space. Second, we introduce \"Shortcut Text Gradient\" an effective replacement for the text gradient that can be obtained with constant memory and runtime. Empirical evaluation on prompts collected from diverse sources (DiffusionDB, ChatGPT, COCO) suggests that our method can discover prompts that substantially improve (prompt enhancement) or destroy (adversarial attack) the faithfulness of images generated by the text-to-image diffusion model.", "sections": [{"title": "1. Introduction", "content": "Large-scale text-based generative models exhibit a remarkable ability to generate novel content conditioned on user input prompts (Ouyang et al., 2022; Touvron et al., 2023; Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Ho et al., 2022; Yu et al., 2022; Chang et al., 2023). Despite being trained with huge corpora, there still exists a substantial gap between user intention and what the model interprets (Zhou et al., 2022; Feng et al., 2022; Rombach et al., 2022; Radford et al., 2021; Lian et al., 2023; Ouyang et al., 2022; Ramesh et al., 2022). The misalignment is even more severe in text-to-image generative models, partially since they often rely on much smaller and less capable text encoders (Radford et al., 2021; Cherti et al., 2023; Raffel et al., 2020) than large language models (LLMs). As a result, instructing a large model to produce intended content often requires laborious human efforts in crafting the prompt through trials and errors (a.k.a. Prompt Engineering) (Art, Year; Wang et al., 2022; Witteveen & Andrews, 2022; Liu & Chilton, 2022; Zhou et al., 2022; Hao et al., 2022). To automate this process for language generation, several recent attempts have shown tremendous potential in utilizing LLMs to enhance prompts (Pryzant et al., 2023; Zhou et al., 2022; Chen et al., 2023; Guo et al., 2023; Yang et al., 2023; Hao et al., 2022). However, efforts on text-to-image generative models remain scarce and preliminary, probably due to the challenges faced by these models' relatively small text encoders in understanding subtle language cues.\nThis paper presents a systematic study of prompt optimization for text-to-image diffusion models. We introduce a novel optimization framework based on the following key observations. 1) Prompt engineering for diffusion models can be formulated as a Discrete Prompt Optimization (DPO-Diff) problem over the space of natural languages. Moreover, the framework can be used to find prompts that either improve (prompt enhancement) or destroy (adversarial attack) the generation process, by simply reversing the sign of the objective function. 2) We show that for diffusion models with classifier-free guidance (Ho & Salimans, 2022), improving the image generation process is more effective when optimizing \u201cnegative prompts\" (Andrew, 2023; Woolf, 2022) than positive prompts. Beyond the problem formulation of DPO-Diff, where \"Diff\" highlights our focus on text-to-image diffusion models, the main technical contributions of this paper lie in efficient methods for solving this optimization problem, including the design of compact domain spaces and a gradient-based algorithm."}, {"title": "Compact domain spaces.", "content": "DPO-Diff's domain space is a discrete search space at the word level to represent prompts. While this space is generic enough to cover any sentence, it is excessively large due to the dominance of words irrelevant to the user input. To alleviate this issue, we design a family of dynamically generated compact search spaces based on relevant word substitutions, for both positive and negative prompts. These subspaces enable efficient search for both prompt enhancement and adversarial attack tasks."}, {"title": "Shortcut Text Gradients for DPO-Diff.", "content": "Solving DPO-Diff with a gradient-based algorithm requires computing the text gradient, i.e., backpropagating from the generated image, through all inference steps of a diffusion model, and finally to the discrete text. Two challenges arise in obtaining this gradient: 1) This process incurs compound memory-runtime complexity over the number of backward passes through the denoising step, making it prohibitive to run on large-scale diffusion models (e.g., a 870M-parameter Stable Diffusion v1 requires ~750G memory to run backpropagation through 50 inference steps (Rombach et al., 2022)). 2) The embedding lookup tables in text encoders are non-differentiable. To reduce the computational cost in 1), we provide a generic replacement for the text gradient that bypasses the need to unroll the inference steps in a backward pass, allowing it to be computed with constant memory and runtime. To backpropagate through the discrete embedding lookup table, we continuously relax the categorical word choices to a learnable smooth distribution over the vocabulary, using the Gumbel Softmax trick (Guo et al., 2021; Jang et al., 2016; Dong & Yang, 2019). The gradient obtained by this method, termed Shortcut Text Gradient, enables us to efficiently solve DPO-Diff regardless of the number of inference steps of a diffusion model.\nTo evaluate our prompt optimization method for the diffusion model, we collect and filter a set of challenging prompts from diverse sources including DiffusionDB (Wang et al., 2022), COCO (Lin et al., 2014), and ChatGPT (Ouyang et al., 2022). Empirical results suggest that DPO-Diff can effectively discover prompts that improve (or destroy for adversarial attack) the faithfulness of text-to-image diffusion models, surpassing human-engineered prompts and prior baselines by a large margin. We summarize our primary contributions as follows:\n\u2022 DPO-Diff: A generic framework for prompt optimization as a discrete optimization problem over the space of natural languages, of arbitrary metrics.\n\u2022 Compact domain spaces: A family of dynamic compact search spaces, over which a gradient-based algorithm enables efficient solution finding for the prompt optimization problem.\n\u2022 Shortcut Text Gradients: The first novel computation method to enable backpropagation through the diffusion models' lengthy sampling steps with constant memory-runtime complexity, enabling gradient-based search algorithms.\n\u2022 Negative prompt optimization: The first empirical result demonstrating the effectiveness of optimizing negative prompts for diffusion models."}, {"title": "2. Related Work", "content": "Text-to-image diffusion models. Diffusion models trained on a large corpus of image-text datasets significantly advanced the state of text-guided image generation (Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Chang et al., 2023; Yu et al., 2022). Despite the success, these models can sometimes generate images with poor quality. While some preliminary observations suggest that negative prompts can be used to improve image quality (Andrew, 2023; Woolf, 2022), there exists no principled way to find negative prompts. Moreover, several studies have shown that large-scale text-to-image diffusion models face significant challenges in understanding language cues in user input during image generation; Particularly, diffusion models often generate images with missing objects and incorrectly bounded attribute-object pairs, resulting in poor \"faithfulness\" or \"relevance\" (Hao et al., 2022; Feng et al., 2022; Lian et al., 2023; Liu et al., 2022). Existing solutions to this problem include compositional generation (Liu et al., 2022), augmenting diffusion model with large language models (Yang et al., 2023), and manipulating attention masks (Feng et al., 2022). As a method orthogonal to them, our work reveals that negative prompt optimization can also alleviate this issue.\nPrompt optimization for text-based generative models. Aligning a pretrained large language model (LLM) with human intentions is a crucial step toward unlocking the potential of large-scale text-based generative models (Ouyang et al., 2022; Rombach et al., 2022). An effective line of training-free alignment methods is prompt optimization (PO) (Zhou et al., 2022). PO originated from in-context learning (Dale, 2021), which is mainly concerned with various arrangements of task demonstrations. It later evolves into automatic prompt engineering, where powerful language models are utilized to refine prompts for certain tasks (Zhou et al., 2022; Pryzant et al., 2023; Yang et al., 2023; Pryzant et al., 2023; Hao et al., 2022). While PO has been widely explored for LLMs, efforts on diffusion models remain scarce. The most relevant prior work to ours is Promptist (Hao et al., 2022), which finetunes an LLM via reinforcement learning from human feedback (Ouyang et al., 2022) to augment user prompts with artistic modifiers (e.g., high-resolution, 4K) (Art, Year), resulting in aesthetically pleasing images. However, the lack of paired contextual-aware data significantly limits its ability to follow the user intention (Figure 3).\nTextual Inversion Optimizing texts in pretrained diffusion models has also been explored under \"Textual Inversion\" task (Gal et al., 2022; Wen et al., 2023; Mokady et al., 2023). Textual Inversion involves adapting a frozen model to generate novel visual concepts based on a set of user-provided images. It achieves this by distilling these images into soft or hard text prompts, enabling the model to replicate the visual features of the user images. Since the source images are provided, the training process mirrors that of typical diffusion model training. While some Textual Inversion papers also use the term \"prompt optimization\", it is distinct from the Prompt Optimization considered by Promptist (Hao et al., 2022) and our work. Our objective is to enhance a model's ability to follow text prompts. Here, the primary input is the user prompt, and improvement is achieved by optimizing this prompt to enhance the resulting image. Since the score function is applied to the final generated image, the optimization process necessitates backpropagation through all inference steps. Despite using similar terminologies, these methodologies are fundamentally distinct and not interchangeable. Table 3 further summarizes the key differences in taxonomy.\nEfficient Backpropagation through diffusion sampling steps. Text-to-image diffusion models generate images via a progressive denoising process, making multiple passes through the same network (Ho et al., 2020). When a loss is applied to the output image, computing the gradient w.r.t. any model component (text, weight, sampler, etc.) requires backpropagating through all the sampling steps. This process incurs compound complexity over the number of backward passes in both memory and runtime, making it infeasible to run on regular commercial devices. Existing efforts achieve constant memory via gradient checkpointing (Watson et al., 2021) or solving an augmented SDE problem (Nie et al., 2022), at the expense of even higher runtime."}, {"title": "3. Preliminaries on diffusion model", "content": "Denoising diffusion probabilistic models. On a high level, diffusion models (Ho et al., 2020) is a type of hierarchical Variational Autoencoder (S\u00f8nderby et al., 2016) that generates samples by reversing (backward) a progressive noisification process (forward). Let $x_0 ... x_T = x$ be a series of intermediate samples of increasing noise levels, the forward process progressively adds Gaussian noise to the original image $x_0$:\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI),$ (1)\nwhere $\\beta$ is a scheduling variable. Using reparameterization trick, $x_t$ can be computed from $x_0$ in one step:\n$x_t = \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 - \\bar{a}_t}e,$\nwhere $a_t = 1 - \\beta_t$ and $\\bar{a}_t = \\Pi_{i=1}^{t} A_i,$\n(2)\n(3)\nwhere $e$ is a standard Gaussian error. The reverse process starts with a standard Gaussian noise, $x_T \\sim N(0, I)$, and progressively denoises it using the following joint distribution:\n$p_{\\theta}(x_{0:T}) = p(x_T) \\Pi p_{\\theta}(X_{t-1}/X_{t})$\nwhere $p_{\\theta}(x_{t-1}|X_t) = N(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma)$.\nWhile the mean function $p_{\\theta}(x_t,t)$ can be parameterized by a neural network (e.g., UNet (Rombach et al., 2022; Ronneberger et al., 2015)) directly, prior studies found that modeling the residual error $e(x_t, t)$ instead works better empirically (Ho et al., 2020). The two strategies are mathematically equivalent as $\\mu_{\\theta}(x_t,t) = \\frac{1}{\\sqrt{a}}(x_t - \\frac{\\sqrt{1-a}}{\\sqrt{e_{\\theta}(x, t)}})$."}, {"title": "4. DPO-Diff Framework", "content": "Formulation Our main insight is that prompt engineering can be formulated as a discrete optimization problem in the language space. Concretely, we represent the problem domain $S$ as a sequence of $M$ words $w_i$ from a predefined vocabulary $V$: $S = {W_1,W_2, ...w_m|\\forall i, w_i \\in V}$. This space is generic enough to cover all possible sentences of lengths less than M (when the empty string is present). Let $G(s)$ denote a text-to-image generative model, and $s_{user}, s$ denote the user input and optimized prompt, respectively. The optimization problem can be written as\n$\\min_{s \\in S} L(G(s), s_{user})$ (5)\nwhere L can be any objective function that measures the effectiveness of the learned prompt when used to generate images. Following previous works (Hao et al., 2022), we use clip loss $CLIP(I, s_{user})$ (Crumb, 2022) to measure the instruction-following ability of the diffusion model.\nApplication DPO-Diff framework is versatile for handling not only prompt enhancement but also adversarial attack tasks. Figure 1 illustrates the taxonomy of those two applications. Adversarial attacks for text-to-image generative models can be defined as follows:\nDefinition 4.1. Given a user input $s_{user}$, the attacker aims at slightly perturbing $s_{user}$ to disrupt the prompt-following ability of image generation, i.e., the resulting generated image is no longer describable by $s_{user}$.\nTo modify (5) into the adversarial attack, we can simply add a negative sign to the objective function (L), and restrict the distance between an adversarial prompt (s) and user input ($s_{user}$). Mathematically, this can be written as the following:\n$\\min_{s \\in S} -L(G(s), s_{user}) s.t. d(s, s_{user}) \\leq \\lambda,$ (6)\nwhere $d(s, s_{user})$ is a distance measure that forces the perturbed prompt (s) to be semantically similar to the user input ($s_{user}$)."}, {"title": "5. Compact search spaces for efficient prompt discovery", "content": "While the entire language space facilitates maximal generality, it is also unnecessarily inefficient as it is popularized with words irrelevant to the task. We propose a family of compact search spaces that dynamically extracts a subset of task-relevant words to the user input."}, {"title": "5.1. Application 1: Discovering adversarial prompts for model diagnosis", "content": "Synonym Space for adversarial attack. In light of the constraint on semantic similarity in (6), we build a search space for the adversarial prompts by substituting each word in the user input $s_{user}$ with its synonyms (Alzantot et al., 2018), preserving the meaning of the original sentence. The synonyms can be found by either dictionary lookup or querying ChatGPT (Appendix F.2)."}, {"title": "5.2. Application 2: Discovering enhanced prompts for image generation", "content": "While the Synonym Space is suitable for attacking diffusion models, we found that it performs poorly on finding improved prompts. This is in contradiction to LLMs where rephrasing user prompts can often lead to substantial gains (Zhou et al., 2022). One plausible reason is that contemporary diffusion models often rely on small-scale text encoders (Radford et al., 2021; Cherti et al., 2023; Raffel et al., 2020) that are much weaker than LLMs with many known limitations in understanding subtle language cues (Feng et al., 2022; Liu et al., 2022; Yang et al., 2023).\nAntonym Space for negative prompt optimization. Inspired by these observations, we propose a novel solution to optimize for negative prompts instead a unique concept that rises from classifier-free guidance (Ho & Salimans, 2022) used in diffusion models (Section 3). Recall that negative prompts instruct the diffusion model to remove contents in generated images, opposite to the positive prompt; Intuitively, the model's output image can safely exclude the content with the opposite meaning to the words in the user input, thereby amplifying the concepts presented in the positive prompt. We thereby build the space of negative prompts from the antonyms of each word in the user prompt. The antonyms of words can also be obtained either via dictionary lookup or querying ChatGPT. However unlike synonyms space, we concatenate the antonyms directly in comma separated format, mirroring the practical usage of negative prompts. To the best of our knowledge, this is the first exploratory work on automated negative prompt optimization."}, {"title": "6. A Gradient-based solver for DPO-Diff", "content": "Due to the query efficiency of white-box algorithms leveraging gradient information, we also explore a gradient-based method to solve (5) and (6). However, obtaining the text gradient is non-trivial due to two major challenges. 1) Backpropagating through the sampling steps of the diffusion inference process incurs high complexity w.r.t. memory and runtime, making it prohibitively expensive to obtain gradients (Watson et al., 2021; Nie et al., 2022). For samplers with 50 inference steps (e.g., DDIM (Song et al., 2020)), it raises the runtime and memory cost by 50 times compared to a single diffusion training step. 2) To further compute the gradient on text, the backpropagation needs to pass through a non-differentiable embedding lookup table. To alleviate these issues, we propose Shortcut Text Gradient, an efficient replacement for text gradient that can be obtained with constant memory and runtime. Our solution to (1) and (2) are discussed in Section 6.1.1 and Section 6.1.2 respectively. Moreover, Section 6.2 discusses how to sample from the learned text distribution via evolutionary search."}, {"title": "6.1. Shortcut Text Gradient", "content": "6.1.1. BACKPROPAGATING THROUGH DIFFUSION SAMPLING STEPS\nTo efficiently backpropagate the loss from the final image to intermediate feature at an arbitrary step, our key idea is to trim the computation graph down to only a few steps from both ends, resulting in a constant number of backward passes (Figure 1. To achieve this, three operations are required through the image generation process:\n(1) Sampling without gradient from step T (noise) to t. We disable gradients up to step t, thereby eliminating the need for backpropagation from T to t.\n(2) Enable gradient from t to t \u2013 K. The backward computation graph is enabled for the K step starting at t.\n(3) Estimating $x_0$ directly from $x_{t-K}$. To bypass the final t \u2212 K steps of UNet, a naive solution is to directly decode and feed the noisy image $x_{t-K}$ to the loss function. However, due to distribution shifts, these intermediate images often cannot be properly interpreted by downstream modules such as VAE decoder (Rombach et al., 2022) and CLIP (Dhariwal & Nichol, 2021). Instead, we propose to use the following closed-form estimation of the final image $x_0$ (Song et al., 2020) to bridge the gap:\n$\\hat{x}_{0} = \\frac{1}{\\sqrt{\\bar{a}_{t-K}}}(x_{t-K} - \\sqrt{1 - \\bar{a}_{t-K}}\\hat{e}_{\\theta}(x_{t-K},t- K))$\nThis way, the Jacobian of $\\hat{x}_{0}$ w.r.t. $x_{t-K}$ can be computed analytically, with complexity independent of t. Note that the above estimation of $x_0$ is not a trick it directly comes from a mathematically equivalent interpretation of the diffusion model, where each inference step can be viewed as computing $\\hat{x}_{0}$ and plugging it into $q(x_{t-K}|x_t, \\hat{x}_{0})$ to obtain the transitional probability (See Appendix C for the derivation)."}, {"title": "Remark 1:", "content": "Complexity Analysis With Shortcut Text Gradient, the computational cost of backpropagating through the inference process can be reduced to K-times backward passes of UNet. When we set t = T and K = T, it becomes the full-text gradient; When K = 1, the computation costs reduce to a single backward pass. Remark 2: Connection to ReFL (Xu et al., 2024). ReFL is a post-hoc alignment method for finetuning diffusion models. It also adopts the estimation of $x_0$ when optimizing diffusion model against a scorer, which is mathematically equivalent to the case when K = 1."}, {"title": "6.1.2. BACKPROPAGATING THROUGH EMBEDDINGS LOOKUP TABLE", "content": "In diffusion models, a tokenizer transforms text input into indices, which will be used to query a lookup table for corresponding word embeddings. To allow further propagating gradients through this non-differentiable indexing operation, we relax the categorical choice of words into a continuous probability of words and learn a distribution over them. We parameterize the distribution using Gumbel Softmax (Jang et al., 2016) with uniform temperature (\u03b7 = 1):\n$\\tilde{e} = \\sum_{i=1}^{|V|}e_i * \\frac{exp ((log a_i + g_i)/\\eta)}{\\sum_{j=1}^{|V|} exp ((log a_i + g_i)/\\eta)}$\n(7)\nwhere a (a |V|-dimensional vector) denotes the learnable parameter, g denotes the Gumbel random variable, ei is the embedding of word i, and \u1ebd is the output mixed embedding."}, {"title": "6.2. Efficient sampling with Evolutionary Search", "content": "To efficiently sample candidate prompts from the learned Gumbel \"distribution\", we adopt evolutionary search, known for its sample efficiency (Goldberg, 1989; Wu et al., 2019). Our adaptation of the evolutionary algorithm to the prompt optimization task involves three key steps: (1) Genotype Definition: We define the genotype of each candidate prompt as the list of searched words from the compact search space, where modifications to the genotype correspond to edits the word choices in the prompt. (2) Population Initialization: We initialize the algorithm's population with samples drawn from the learned Gumbel distribution to bias the starting candidates towards regions of high potential. (3) Evolutionary Operations: We execute a standard evolutionary search, including several rounds of crossover and mutation (Goldberg, 1989), culminating in the selection of the top candidate as the optimized prompt. Details of the complete DPO-Diff algorithm, including specific hyperparameters, are available in Algorithm 1 of Appendix D and discussed further in Appendix F.1.\nRemark: Extending DPO-Diff to Blackbox Settings. In cases where the model is only accessible through forward API, our Evolutionary Search (ES) module can be used as a stand-alone black-box optimizer, thereby expanding the applicability of our framework. As further ablated in Section 8.1, ES archives descent results with enough queries."}, {"title": "7. Experiments", "content": "7.1. Experimental Setup\nDataset preparation. To encourage semantic diversity, we collect a prompt dataset from three sources: DiffusionDB (Wang et al., 2022), ChatGPT generated prompts (Ouyang et al., 2022), and COCO (Lin et al., 2014). For each source, we filter 100 \u201chard prompts\u201d with a clip loss higher (lower for adversarial attack) than a threshold, amounting to 600 prompts in total for two tasks. Due to space limit, we include preparation details in Appendix G.1.\nEvaluation Metrics. All methods are evaluated quantitatively using the clip loss (Crowson et al., 2022) and Human Preference Score v2 (HPSv2). HPSv2 is a CLIP-based model trained to predict human preferences on images generated from text. For base models, we adopt Stable Diffusion v1-4. Each prompt is evaluated under two random seeds (shared across different methods). Besides automatic evaluation metrics, we also conduct human evaluations on the generated images, following the protocol specified in Appendix G.2.\nOptimization Parameters. We use the Spherical CLIP Loss (Crumb, 2022) as the objective function, which ranges between 0.75 and 0.85 for most inputs. The K for the Shortcut Text Gradient is set to 1, as it produces effective supervision signals with minimal cost. To generate the search spaces, we prompt ChatGPT (gpt-4-1106-preview) for at most 5 substitutes of each word in the user prompt. Furthermore, we use a fixed set of hyperparameters for both prompt improvement and adversarial attacks. We include a detailed discussion on all the hyperparameters and search space generation in Appendix F."}, {"title": "7.2. Application 1 - Adversarial Attack", "content": "Unlike RLHF-based prompt-engineering methods (e.g. Promptist (Hao et al., 2022)) that require finetuning a prompt generator when adapting to a new task, DPO-Diff, as a train-free method, can be seamlessly applied to finding adversarial prompts by simply reversing the sign of the objective function.\nIn this section, we demonstrate that DPO-Diff is capable of discovering adversarial prompts that destroy the prompt-following ability of Stable Diffusion.\nAs suggested by (6), a successful adversarial prompt must not change the original intention of the user prompt. While we specified this constraint to ChatGPT when building the Synonyms Space, occasionally ChatGPT might mistake a word for the synonyms. To address this, during the evolutionary search phase, we perform rejection sampling to refuse candidate prompts that have different meanings to the user input. Concretely, we enforce their cosine similarity in embedding space to be higher than 0.9 (More on this can be found in Appendix G)."}, {"title": "7.3. Application 2: Prompt Improvement", "content": "In this section, we apply DPO-Diff to craft prompts that improve the prompt-following ability of the generated images. We compare our method with three baselines: (1) User Input. (2) Human Engineered Prompts (available only on DiffusionDB) (Wang et al., 2022). (3) Promptist (Hao et al., 2022), trained to mimic the human-crafted prompt provided in DiffusionDB.\nTable 1 summarizes the result. Among all methods, DPO-Diff achieves the best results under both Spherical CLIP loss and Human Preference Score (HPSv2) score. On the other hand, our findings suggest that both human-engineered and Promptist-optimized prompts do not improve the relevance between generated images and user intention. The reason is that these methods merely add a set of aesthetic modifiers to the original prompt, irrelevant to the semantics of user input. This can be further observed from the qualitative examples in Figure 3, where images generated by Promptist often also do not follow the prompts well.\nHuman Evaluation. We further ask human judges to rate DPO-Diff and Promptist on how well the generated images follow the user prompt. Figure 2 summarizes the win/draw/loss rate of DPO-Diff against Promptist; The result shows that DPO-Diff surpasses or matches Promptist in human rate 79% of times on SD-v1."}, {"title": "7.4. Qualitative analysis of search progression", "content": "To examine the convergence of our search algorithm qualitatively, we plot the progression of optimized images at various evaluation stages. We set the target iterations at 0 (the original image), 10, 20, 40, and 80 to illustrate the changes, and showcase the image with the highest clip loss among all evaluated candidates at each iteration.\nFigure 5 illustrates some example trajectories. In most cases, the images exhibit noticeable improvement in aligning with the user's prompt at as early as the 10th iteration, and continue to improve. Moreover, the progression are surprisingly interpretable. For instance, with the prompt: \"A bunch of luggage in front of a truck,\" the initial image fails to include any luggage, featuring only the truck; However, as the optimization continues, we can see that DPO-Diff incrementally adds more luggage to the scene."}, {"title": "8. Ablation Study", "content": "We conduct ablation studies on DPO-Diff using 30 randomly sampled prompts, 10 from each source. Each search algorithm is run under 4 random seeds."}, {"title": "8.1. Comparison of different search algorithms.", "content": "We compare four search algorithms for DPO-Diff: Random Search (RS), Evolution Prompt Optimization (EPO), Gradient-based Prompt Optimization (GPO), and the full algorithm (GPO + ES). Figure 6 shows their performance under different search budgets (number of evaluations)\u00b9; While GPO tops EPO under low budgets, it also plateaus quicker as randomly drawing from the learned distribution is sample-inefficient. Combining GPO with EPO achieves the best overall performance."}, {"title": "8.2. Negative prompt v.s. positive prompt optimization", "content": "One finding in our work is that optimizing negative prompts (Antonyms Space) is more effective than positive prompts (Synonyms Space) for Stable Diffusion. To verify the strength of these spaces, we randomly sample 100 prompts for each space and compute their average clip loss of generated images. Table 2 suggests that Antonyms Space contains candidates with consistently lower clip loss than Synonyms Space."}, {"title": "9. Discussion on the Search v.s. Learning paradigms for utilizing computatons", "content": "This section elucidates the relationship between two distinct prompt optimization approaches for diffusion models: DPO-Diff (ours) and Promptist. While Promptist represents a pioneering effort, it is important to discuss why DPO-Diff remains essential.\nLimitations of Promptist Promptist utilizes the Reinforcement Learning from Human Feedback (RLHF) (Bain & Sammut, 1995; Christiano et al., 2017; Ouyang et al., 2022) approach to fine-tune a language model to generate improved prompts. RLHF relies on paired data (user_prompt, improved_prompt), which is scarce for diffusion models and challenging to curate. This is primarily because generating the improved prompts requires extensive trial-and-error by human experts, essentially performing what DPO-Diff automates. In fact, the performance limit exhibited by Promptist is exactly caused by this lack of data: The data used by Promptist from DiffusionDB predominantly features aesthetic modifiers that do not alter the semantics of the prompts This limits its effectiveness to aesthetic enhancements and not addressing the core need for semantic accuracy in prompts. Consequently, it struggles with semantic prompt adherence and lacks flexibility in modifying prompts for tasks such as adversarial attacks.\nTwo complementary computational paradigms Promptist and DPO-Diff represent two major paradigms for effectively utilizing computation: learning and searching, respectively (Sutton, 2019). Learning-based approach of Promptist enhances performance through more parameters and larger datasets, whereas the search-based approach of DPO-Diff focuses on maximizing the potential of pretrained models via post-hoc optimization. Although learning-based methods require high quality paired data, they can be efficiently deployed once trained; On the other hand, search-based methods generate high quality prompts, but are much slower to execute. Therefore, as Sutton (2019) highlights, these paradigms are complementary rather than competitive. DPO-Diff can be leveraged to generate high quality dataset offline, which can subsequently train Promptist to reduce inference latency effectively. Together, they pave the way for a comprehensive solution to prompt optimization for diffusion models, positioning DPO-Diff as the first search-based solution to address this problem."}, {"title": "10. Conclusions", "content": "This work presents DPO-Diff, the first gradient-based framework for optimizing discrete prompts. We formulate prompt optimization as a discrete optimization problem over the text space. To improve the search efficiency, we introduce a family of compact search spaces based on relevant word substitutions, as well as design a generic computational method for computing the discrete text gradient for diffusion model's inference process. DPO-Diff is generic - We demonstrate that it can be directly applied to effectively discover both refined prompts to aid image generation and adversarial prompts for model diagnosis. We hope that the proposed framework helps open up new possibilities in developing advanced prompt optimization methods for text-based image generation tasks.\nLimitations To motivate future work, we discuss the known limitations of DPO-Diff in Appendix A."}, {"title": "A. Limitations", "content": "We identify the following known limitations of the proposed method: Search cost Our method requires multiple passes through the diffusion model to optimize a given prompt, which incurs a modest amount of search costs. One promising solution is to use DPO-Diff to generate free paired data for RLHF (e.g. Promptist), which we leave for future work to explore. Text encoder moreover, while DPO-Diff improves the faithfulness of the generated image, the performance is upper-bounded by the limitations of the underlying text encoder. For example, the clip text encoder used in stable diffusion tends to discard spatial relationships in text, which in principle must be resolved by improving the model itself, such as augmenting the diffusion model with a powerful LLM ((Lian et al., 2023; Liu et al., 2022; Feng et al., 2022). Clip loss The clip loss used in DPO-Diff might not always align with human evaluation. Automatic scoring metrics that better reflect human judgment, similar to the reward models used in instruction fine-tuning, can further aid the discovery of improved prompts. Synonyms generated by ChatGPT For adversarial attack task, ChatGPT sometimes generate incorrect synonyms. Although we use reject-sampling based on sentence embedding similarity as a posthoc fix, it is not completely accurate. This may impact the validity of adversarial prompts, as by definition they must preserve the user's original intent. We address this in human evaluation by asking the raters to consider this factor when determining the success of an attack."}, {"title": "B. Benefit of optimizing discrete text prompts over soft prompts", "content": "Optimizing discrete text prompts offers two major advantages over tuning soft prompts, primarily in two areas: (1) Interpretability: The results of discrete prompt optimization are texts that are naturally human interpretable. This also facilitates direct use in fine-tuning RLHF-based agents like Promptist. (2) Simplified Search Space: Our preliminary attempts with continuous text embeddings revealed challenges in achieving convergence, even on toy examples. The reason, we conjecture was that the gradients backpropagated through the denoising process have low info-to-noise ratio; And updating soft prompt using such gradient could be very unstable due to its huge continuous search space. In contrast, discrete prompt optimization effectively narrows the search to a finite vocabulary set, greatly reducing search complexity and improving stability."}, {"title": "C. Derivation for the alternative interpretation of DDPM's modeling.", "content": "Proposition C.1. The original parameterization of DDPM at step t \u2212 K: $\\mu_{\\theta}(x_{t-K},t \u2013 K) = \\frac{\\sqrt{\\bar{a}_{t-1}\\beta_t}}{\\sqrt{\\alpha_t(1-\\bar{a}_{t-1})}}x_0 + \\frac{\\sqrt{\\alpha_{t-1}}(1-\\bar{a}_t)}{1-\\bar{a}_t}Xt = \\frac{1}{\\sqrt{a_{t-K}}}(Xt-K - \\sqrt{1 - \\bar{a}_{t-K}}\\hat{e}_{\\theta}(x_{t-K},t- K))$ can be viewed as first computing an estimate of $x_0$ from the current-step error $\\hat{e}_{\\theta}(x_{t-K},t-K)$:\n$\\hat{x}_0 = \\frac{1}{\\sqrt{\\bar{a}_{t-K}}}(Xt\u2212K - \\sqrt{1 - \\bar{a}_{t-K}}\\hat{e}_0(xt\u2212K,t \u2212 K))$\nAnd use the estimate to compute the transition probability $q(x_{t-K}|x_t, \\hat{x_0})$.\nProof. To avoid clustered notations, we use t instead of t - K for the proof below. Starting from reorganizing (3) to the one step estimation:\n$\\hat{x}_0 = \\frac{1}{\\sqrt{\\bar{a}_t}}(xt - \\sqrt{1 - \\bar{a}_t}\\hat{e}_0(xt,t))$\n(8)\nwhere $\\hat{e}$ is the predicted error at step t by the network. Intuitively this equation means to use the current predicted error to one-step estimate $\\hat{x_0}$. Using the Bayesian Theorem, one can show that\n$q(xt-K|xt, x_0) = N(xt\u22121; \\mu(xt, x0), \\beta_tI)$\n$\\mu(xt, x_0) = \\frac{\\sqrt{\\bar{a}_{t-1}}\\beta_t}{\\sqrt{\\alpha_t(1-\\bar{a}_{t-1})}}x_0 + \\frac{\\sqrt{\\alpha_{t-1}}(1-\\bar{a}_t)}{1-\\bar{a}_t}Xt$\n(9)\n(10)\nIf we plug $\\hat{x_0}$ into the above equation, it becomes:\n$\\mu_o(x_t,t) = \\frac{1}{\\sqrt{a_t}} (x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{a}}}} e(x_t, t))$\n(11)\nwhich is identical to the original modeling of DDPM (Ho et al., 2020)."}, {"title": "7.  Experimental Setup", "content": "Algorithm 1 DPO-Diff solver: Discrete Prompt Optimization Algorithm\nRequire: User Input Suser, diffusion model G(\u00b7), a loss function L(I, s), learning rate lr.\nEnsure: An optimized prompt s*.\n// Building Search Space\nQuery ChatGPT to generate a word-substitutes dictionary for Suser\nInitialize Gumbel parameter a accordingly.\n// Gradient Prompt Optimization\nfor i from 1 to max_iter do\nSample p(w; a) for each word w from Gumbel Softmax.\nCompute mixed embedding: $\\tilde{e}(\u03b1) = \\sum p_{i=1}^{|V|}(w; \u03b1) * C_i$\nCompute text gradient: gs = \u2207QL(G(\u1ebd(a)), s)\nUpdate Gumbel Parameter: a\u2081 = Ai - lr * gsuser\nend for\n// Evolutionary Sampling\nGenerate initial population P ~ Gumbel(a)\nFind the population that minimizes L using genetic algorithm P* = EvoSearch(P, L)\ns*  argmax(G(s \u2208 P*), Suser)"}, {"title": "D. The complete DPO-Diff algorithm", "content": "Algorithm 1 DPO-Diff solver: Discrete Prompt Optimization Algorithm\nRequire: User Input Suser, diffusion model G(\u00b7), a loss function L(I, s), learning rate lr.\nEnsure: An optimized prompt s*.\n// Building Search Space\nQuery ChatGPT to generate a word-substitutes dictionary for Suser\nInitialize Gumbel parameter a accordingly.\n// Gradient Prompt Optimization\nfor i from 1 to max_iter do\nSample p(w; a) for each word w from Gumbel Softmax.\nCompute mixed embedding: $\\tilde{e}(\u03b1) = \\sum p_{i=1}^{|V|}(w; \u03b1) * C_i$\nCompute text gradient: gs = \u2207QL(G(\u1ebd(a)), s)\nUpdate Gumbel Parameter: a\u2081 = Ai - lr * gsuser\nend for\n// Evolutionary Sampling\nGenerate initial population P ~ Gumbel(a)\nFind the population that minimizes L using genetic algorithm P* = EvoSearch(P, L)\ns*  argmax(G(s \u2208 P*), Suser)"}, {"title": "E. Taxonomy of prompt optimization v.s. textual inversion", "content": "Table 3: Comparison of prompt optimization and textual inversion tasks."}, {"title": "F. Implementation details", "content": "F.1. Hyperparameters\nThis section details the hyperparameter choices for our experiments. We use the same set of hyperparameters for all datasets and tasks (prompt improvement and adversarial attack), unless otherwise specified."}, {"title": "F.  Hyparameters", "content": "Implementation detailsAlgorithm 1 DPO-Diff solver: Discrete Prompt Optimization Algorithm"}, {"title": "G.1. Dataset Collection", "content": "The prompts used in our paper are collected from three sources, DiffusionDB, COCO, and ChatGPT.\nDiffusionDB DiffusionDB is a giant prompt database comprised of 2m highly diverse prompts for text-to-image generation. Since these prompts are web-crawled, they are highly noisy, often containing incomplete phrases, emojis, random characters, non-imagery prompts, etc (We refer the reader to its HuggingFace repo for an overview of the entire database.). Therefore, we filter prompts from DiffusionDB by (1). asking ChatGPT to determine whether the prompt is complete and describes an image, and (2) remove emoji-only prompts. We filter a total of 4,000 prompts from DiffusionDB and use those prompts to generate images via Stable Diffusion. We sample 100 prompts with clip loss above 0.85 for prompt improvement, and 0.8 for adversarial attacks respectively. For ChatGPT, we found that it tends to produce prompts with much lower clip score compared with COCO and DiffusionDB. To ensure a sufficient amount of prompts from this source is included in the dataset, we lower the cutoff threshold to 0.82 when filtering its hard prompts for the prompt improvement task.\nCOCO We use the captions from the 2014 validation split of MS-COCO dataset as prompts. Similar to DiffusionDB, we filter 4000 prompts, and further sample 100 prompts with clip loss above 0.85 for prompt improvement, and 0.8 for adversarial attack respectively."}, {"title": "I. Further discussion on Gradient-based Prompt Optimization", "content": "The computational cost of the Shortcut Text Gradient is controlled by K. Moreover, when we set t = T and K = T \u2013 1, it becomes the full-text gradient.\nThe result of remark 2 is rather straightforward: recall that the image generation process starts with a random noise xt and gradually denoising it to the final image x0. Since the gradient is enabled from t to t - K in Shortcut Text Gradient; when t = T and K = T, it indicates that gradient is enabled from T to 0, which covers the entire inference process. In this case, the Shortcut Text Gradient reduces to the full gradient on text."}, {"title": "J. Extra ablation study results.", "content": "J.1. Gradient norm v.s. timestep.\nWhen randomly sampling t in computing the Shortcut Text Gradient, we avoid timesteps near the beginning and the end of the image generation process, as gradients at those places are not informative. As we can see, for both adversarial attack and prompt improvement, the gradient norm is substantially smaller near t = T and especially t = 0, compared with timesteps in the middle. The reason, we conjecture, is that the images are almost pure noise at the beginning, and are almost finalized towards the end. Figure 9 shows the empirical gradient norm across different timesteps.\nJ.2. Extended discussion on different search algorithms\nIn our experiments, we found that Gradient-based Prompt Optimization converges faster at the early stage of the optimization. This result confirms the common belief that white-box algorithms are more query efficient than black-box algorithms in several other machine learning fields, such as adversarial attack (Ilyas et al., 2018; Cheng et al., 2018). However, when giving a sufficient amount of query, Evolutionary Search eventually catches up and even outperforms GPO. The reason, we conjecture, is that GPO uses random search to draw candidates from the learned distribution, which bottlenecked its sample efficiency at later stages. This promotes the hybrid algorithm used in our experiments: Using Evolutionary Search to sample from the learned distribution of GPO. The hybrid algorithm achieves the best overall convergence.\nJ.3. Extended discussion on negative v.s. positive prompt optimization\nAs discussed in the main text, one of our highlighted findings of is that optimizing for negative prompts is more effective than positive prompts in improving the prompt-following ability of diffusion models. This is evidenced by Table 2, which shows that Antonym Space contains a denser population of promising prompts (lower clip loss) than positive spaces. Such search space also allows the search algorithm to identify an improved prompt more easily. We conjecture that this might indicate diffusion models are more sensitive to changes in negative prompts than positive prompts, as the baseline negative prompt is merely an empty string."}, {"title": "A. Limitations", "content": "We identify the following known limitations of the proposed method: Search cost Our method requires multiple passes through the diffusion model to optimize a given prompt, which incurs a modest amount of search costs. One promising solution is to use DPO-Diff to generate free paired data for RLHF (e.g. Promptist), which we leave for future work to explore. Text encoder moreover, while DPO-Diff improves the faithfulness of the generated image, the performance is upper-bounded by the limitations of the underlying text encoder. For example, the clip text encoder used in stable diffusion tends to discard spatial relationships in text, which in principle must be resolved by improving the model itself, such as augmenting the diffusion model with a powerful LLM (continue generating json"}]}