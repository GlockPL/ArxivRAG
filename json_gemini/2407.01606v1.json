{"title": "On Discrete Prompt Optimization for Diffusion Models", "authors": ["Ruochen Wang", "Ting Liu", "Cho-Jui Hsieh", "Boqing Gong"], "abstract": "This paper introduces the first gradient-based framework for prompt optimization in text-to-image diffusion models. We formulate prompt engineering as a discrete optimization problem over the language space. Two major challenges arise in efficiently finding a solution to this problem: (1) Enormous Domain Space: Setting the domain to the entire language space poses significant difficulty to the optimization process. (2) Text Gradient: Efficiently computing the text gradient is challenging, as it requires backpropagating through the inference steps of the diffusion model and a non-differentiable embedding lookup table. Beyond the problem formulation, our main technical contributions lie in solving the above challenges. First, we design a family of dynamically generated compact subspaces comprised of only the most relevant words to user input, substantially restricting the domain space. Second, we introduce \"Shortcut Text Gradient\" an effective replacement for the text gradient that can be obtained with constant memory and runtime. Empirical evaluation on prompts collected from diverse sources (DiffusionDB, ChatGPT, COCO) suggests that our method can discover prompts that substantially improve (prompt enhancement) or destroy (adversarial attack) the faithfulness of images generated by the text-to-image diffusion model.", "sections": [{"title": "1. Introduction", "content": "Large-scale text-based generative models exhibit a remarkable ability to generate novel content conditioned on user input prompts (Ouyang et al., 2022; Touvron et al., 2023; Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Ho et al., 2022; Yu et al., 2022; Chang et al., 2023). Despite being trained with huge corpora, there still exists a substantial gap between user intention and what the model interprets (Zhou et al., 2022; Feng et al., 2022; Rombach et al., 2022; Radford et al., 2021; Lian et al., 2023; Ouyang et al., 2022; Ramesh et al., 2022). The misalignment is even more severe in text-to-image generative models, partially since they often rely on much smaller and less capable text encoders (Radford et al., 2021; Cherti et al., 2023; Raffel et al., 2020) than large language models (LLMs). As a result, instructing a large model to produce intended content often requires laborious human efforts in crafting the prompt through trials and errors (a.k.a. Prompt Engineering) (Art, Year; Wang et al., 2022; Witteveen & Andrews, 2022; Liu & Chilton, 2022; Zhou et al., 2022; Hao et al., 2022). To automate this process for language generation, several recent attempts have shown tremendous potential in utilizing LLMs to enhance prompts (Pryzant et al., 2023; Zhou et al., 2022; Chen et al., 2023; Guo et al., 2023; Yang et al., 2023; Hao et al., 2022). However, efforts on text-to-image generative models remain scarce and preliminary, probably due to the challenges faced by these models' relatively small text encoders in understanding subtle language cues.\n\nDPO-Diff. This paper presents a systematic study of prompt optimization for text-to-image diffusion models. We introduce a novel optimization framework based on the following key observations. 1) Prompt engineering for diffusion models can be formulated as a Discrete Prompt Optimization (DPO-Diff) problem over the space of natural languages. Moreover, the framework can be used to find prompts that either improve (prompt enhancement) or destroy (adversarial attack) the generation process, by simply reversing the sign of the objective function. 2) We show that for diffusion models with classifier-free guidance (Ho & Salimans, 2022), improving the image generation process is more effective when optimizing \u201cnegative prompts\" (Andrew, 2023; Woolf, 2022) than positive prompts. Beyond the problem formulation of DPO-Diff, where \"Diff\" highlights our focus on text-to-image diffusion models, the main technical contributions of this paper lie in efficient methods for solving this optimization problem, including the design of compact domain spaces and a gradient-based algorithm."}, {"title": "2. Related Work", "content": "Text-to-image diffusion models. Diffusion models trained on a large corpus of image-text datasets significantly advanced the state of text-guided image generation (Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Chang et al., 2023; Yu et al., 2022). Despite the success, these models can sometimes generate images with poor quality. While some preliminary observations suggest that negative prompts can be used to improve image quality (Andrew, 2023; Woolf, 2022), there exists no principled way to find negative prompts. Moreover, several studies have shown that large-scale text-to-image diffusion models face significant challenges in understanding language cues in user input during image generation; Particularly, diffusion models often generate images with missing objects and incorrectly bounded attribute-object pairs, resulting in poor \"faithfulness\" or \"relevance\" (Hao et al., 2022; Feng et al., 2022; Lian et al., 2023; Liu et al., 2022). Existing solutions to this problem include compositional generation (Liu et al., 2022), augmenting diffusion model with large language models (Yang et al., 2023), and manipulating attention masks (Feng et al., 2022). As a method orthogonal to them, our work reveals that negative prompt optimization can also alleviate this issue.\n\nPrompt optimization for text-based generative models. Aligning a pretrained large language model (LLM) with human intentions is a crucial step toward unlocking the potential of large-scale text-based generative models (Ouyang et al., 2022; Rombach et al., 2022). An effective line of training-free alignment methods is prompt optimization (PO) (Zhou et al., 2022). PO originated from in-context learning (Dale, 2021), which is mainly concerned with various arrangements of task demonstrations. It later evolves into automatic prompt engineering, where powerful language models are utilized to refine prompts for certain tasks (Zhou et al., 2022; Pryzant et al., 2023; Yang et al., 2023; Pryzant et al., 2023; Hao et al., 2022). While PO has been widely explored for LLMs, efforts on diffusion models remain scarce. The most relevant prior work to ours is Promptist (Hao et al., 2022), which finetunes an LLM via reinforcement learning from human feedback (Ouyang et al., 2022) to augment user prompts with artistic modifiers (e.g., high-resolution, 4K) (Art, Year), resulting in aesthetically pleasing images. However, the lack of paired contextual-aware data significantly limits its ability to follow the user intention (Figure 3).\n\nTextual Inversion Optimizing texts in pretrained diffusion models has also been explored under \"Textual Inversion\" task (Gal et al., 2022; Wen et al., 2023; Mokady et al., 2023). Textual Inversion involves adapting a frozen model to generate novel visual concepts based on a set of user-provided images. It achieves this by distilling these images into soft or hard text prompts, enabling the model to replicate the visual features of the user images. Since the source images are provided, the training process mirrors that of typical diffusion model training. While some Textual Inversion papers also use the term \"prompt optimization\", it is distinct from the Prompt Optimization considered by Promptist (Hao et al., 2022) and our work. Our objective is to enhance a model's ability to follow text prompts. Here, the primary input is the user prompt, and improvement is achieved by optimizing this prompt to enhance the resulting image. Since the score function is applied to the final generated image, the optimization process necessitates backpropagation through all inference steps. Despite using similar terminologies, these methodologies are fundamentally distinct and not interchangeable. Table 3 further summarizes the key differences in taxonomy.\n\nEfficient Backpropagation through diffusion sampling steps. Text-to-image diffusion models generate images via a progressive denoising process, making multiple passes through the same network (Ho et al., 2020). When a loss is applied to the output image, computing the gradient w.r.t. any model component (text, weight, sampler, etc.) requires backpropagating through all the sampling steps. This process incurs compound complexity over the number of backward passes in both memory and runtime, making it infeasible to run on regular commercial devices. Existing efforts achieve constant memory via gradient checkpointing (Watson et al., 2021) or solving an augmented SDE problem (Nie et al., 2022), at the expense of even higher runtime."}, {"title": "3. Preliminaries on diffusion model", "content": "Denoising diffusion probabilistic models. On a high level, diffusion models (Ho et al., 2020) is a type of hierarchical Variational Autoencoder (S\u00f8nderby et al., 2016) that generates samples by reversing (backward) a progressive noisification process (forward). Let $x_0 \\dots x_T$ be a series of intermediate samples of increasing noise levels, the forward process progressively adds Gaussian noise to the original image $x_0$:\n\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI),                                                 (1)$\n\nwhere $\\beta$ is a scheduling variable. Using reparameterization trick, $x_{t+1}$ can be computed from $x_0$ in one step:\n\n$x_t = \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 - \\bar{a}_t}e,                                          (2)$\n\nwhere $\\alpha_t = 1 - \\beta_t$ and $\\bar{a}_t = \\prod_{i=1}^t A_i,                                 (3)$\n\nwhere $e$ is a standard Gaussian error. The reverse process starts with a standard Gaussian noise, $x_T \\sim N(0, I)$, and progressively denoises it using the following joint distribution:\n\n$p_{\\theta}(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_{\\theta}(x_{t-1}|x_t)$\n\nwhere $p_{\\theta}(x_{t-1}|x_t) = N(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma)$.\n\nWhile the mean function $p_{\\theta}(x_t,t)$ can be parameterized by a neural network (e.g., UNet (Rombach et al., 2022; Ronneberger et al., 2015)) directly, prior studies found that modeling the residual error $e(x_t, t)$ instead works better empirically (Ho et al., 2020). The two strategies are mathematically equivalent as $\\mu_{\\theta}(x_t,t) = \\frac{1}{\\sqrt{a_t}}(x_t- \\frac{\\beta_t}{\\sqrt{1 - \\bar{a}_t}}e_{\\theta}(x_t, t))$."}, {"title": "Conditional generation and negative prompts", "content": "The above formulation can be easily extended to conditional generation via classifier-free guidance (Ho & Salimans, 2022), widely adopted in contemporary diffusion models. At each sampling step, the predicted error $\\tilde{e}$ is obtained by subtracting the unconditional signal ($e_{\\theta}(c(\"\"))$) from the conditional signal ($e_{\\theta}(c(s))$), up to a scaling factor $w$:\n\n$\\tilde{e}_{\\theta}(x_t, c(s),t) = (1 + w)e_{\\theta}(x_t, c(s), t) - we_{\\theta}(x_t, c(\"\"), t).                                    (4)$\n\nIf we replace this empty string with an actual text, then it becomes a Negative Prompt (Andrew, 2023; Woolf, 2022), instructing the model what to exclude from the generated image."}, {"title": "4. DPO-Diff Framework", "content": "Formulation Our main insight is that prompt engineering can be formulated as a discrete optimization problem in the language space. Concretely, we represent the problem domain $S$ as a sequence of $M$ words $w_i$ from a predefined vocabulary $V$: $S = \\{w_1, w_2, ...w_m|\\forall i, w_i \\in V\\}$. This space is generic enough to cover all possible sentences of lengths less than $M$ (when the empty string is present). Let $G(s)$ denote a text-to-image generative model, and $s_{\\text{user}}, s$ denote the user input and optimized prompt, respectively. The optimization problem can be written as\n\n$\\min_{s \\in S} L(G(s), s_{\\text{user}})                                                  (5)$\n\nwhere $L$ can be any objective function that measures the effectiveness of the learned prompt when used to generate images. Following previous works (Hao et al., 2022), we use clip loss CLIP(I, $s_{\\text{user}}$) (Crumb, 2022) to measure the instruction-following ability of the diffusion model.\n\nApplication DPO-Diff framework is versatile for handling not only prompt enhancement but also adversarial attack tasks. Figure 1 illustrates the taxonomy of those two applications. Adversarial attacks for text-to-image generative models can be defined as follows:\n\nDefinition 4.1. Given a user input $s_{\\text{user}}$, the attacker aims at slightly perturbing $s_{\\text{user}}$ to disrupt the prompt-following ability of image generation, i.e., the resulting generated image is no longer describable by $s_{\\text{user}}$.\n\nTo modify (5) into the adversarial attack, we can simply add a negative sign to the objective function ($L$), and restrict the distance between an adversarial prompt ($s$) and user input ($s_{\\text{user}}$). Mathematically, this can be written as the following:\n\n$\\min_{s \\in S} -L(G(s), s_{\\text{user}}) \\text{ s.t. } d(s, s_{\\text{user}}) \\leq \\lambda,                                     (6)$\n\nwhere $d(s, s_{\\text{user}})$ is a distance measure that forces the perturbed prompt ($s$) to be semantically similar to the user input ($s_{\\text{user}}$)."}, {"title": "5. Compact search spaces for efficient prompt discovery", "content": "While the entire language space facilitates maximal generality, it is also unnecessarily inefficient as it is popularized with words irrelevant to the task. We propose a family of compact search spaces that dynamically extracts a subset of task-relevant words to the user input."}, {"title": "5.1. Application 1: Discovering adversarial prompts for model diagnosis", "content": "Synonym Space for adversarial attack. In light of the constraint on semantic similarity in (6), we build a search space for the adversarial prompts by substituting each word in the user input $s_{\\text{user}}$ with its synonyms (Alzantot et al., 2018), preserving the meaning of the original sentence. The synonyms can be found by either dictionary lookup or querying ChatGPT (Appendix F.2)."}, {"title": "5.2. Application 2: Discovering enhanced prompts for image generation", "content": "While the Synonym Space is suitable for attacking diffusion models, we found that it performs poorly on finding improved prompts. This is in contradiction to LLMs where rephrasing user prompts can often lead to substantial gains (Zhou et al., 2022). One plausible reason is that contemporary diffusion models often rely on small-scale text encoders (Radford et al., 2021; Cherti et al., 2023; Raffel et al., 2020) that are much weaker than LLMs with many known limitations in understanding subtle language cues (Feng et al., 2022; Liu et al., 2022; Yang et al., 2023).\n\nAntonym Space for negative prompt optimization. Inspired by these observations, we propose a novel solution to optimize for negative prompts instead a unique concept that rises from classifier-free guidance (Ho & Salimans, 2022) used in diffusion models (Section 3). Recall that negative prompts instruct the diffusion model to remove contents in generated images, opposite to the positive prompt; Intuitively, the model's output image can safely exclude the content with the opposite meaning to the words in the user input, thereby amplifying the concepts presented in the positive prompt. We thereby build the space of negative prompts from the antonyms of each word in the user prompt. The antonyms of words can also be obtained either via dictionary lookup or querying ChatGPT. However unlike synonyms space, we concatenate the antonyms directly in comma separated format, mirroring the practical usage of negative prompts. To the best of our knowledge, this is the first exploratory work on automated negative prompt optimization."}, {"title": "6. A Gradient-based solver for DPO-Diff", "content": "Due to the query efficiency of white-box algorithms leveraging gradient information, we also explore a gradient-based method to solve (5) and (6). However, obtaining the text gradient is non-trivial due to two major challenges. 1) Backpropagating through the sampling steps of the diffusion inference process incurs high complexity w.r.t. memory and runtime, making it prohibitively expensive to obtain gradients (Watson et al., 2021; Nie et al., 2022). For samplers with 50 inference steps (e.g., DDIM (Song et al., 2020)), it raises the runtime and memory cost by 50 times compared to a single diffusion training step. 2) To further compute the gradient on text, the backpropagation needs to pass through a non-differentiable embedding lookup table. To alleviate these issues, we propose Shortcut Text Gradient, an efficient replacement for text gradient that can be obtained with constant memory and runtime. Our solution to (1) and (2) are discussed in Section 6.1.1 and Section 6.1.2 respectively. Moreover, Section 6.2 discusses how to sample from the learned text distribution via evolutionary search."}, {"title": "6.1. Shortcut Text Gradient", "content": "6.1.1. BACKPROPAGATING THROUGH DIFFUSION SAMPLING STEPS\n\nTo efficiently backpropagate the loss from the final image to intermediate feature at an arbitrary step, our key idea is to trim the computation graph down to only a few steps from both ends, resulting in a constant number of backward passes. To achieve this, three operations are required through the image generation process:\n\n(1) Sampling without gradient from step $T$ (noise) to $t$. We disable gradients up to step $t$, thereby eliminating the need for backpropagation from $T$ to $t$.\n\n(2) Enable gradient from $t$ to $t - K$. The backward computation graph is enabled for the $K$ step starting at $t$.\n\n(3) Estimating $x_0$ directly from $x_{t-K}$. To bypass the final $t - K$ steps of UNet, a naive solution is to directly decode and feed the noisy image $x_{t-K}$ to the loss function. However, due to distribution shifts, these intermediate images often cannot be properly interpreted by downstream modules such as VAE decoder (Rombach et al., 2022) and CLIP (Dhariwal & Nichol, 2021). Instead, we propose to use the following closed-form estimation of the final image $x_0$ (Song et al., 2020) to bridge the gap:\n\n$x_0 = \\frac{1}{\\sqrt{\\bar{a}_{t-K}}}(x_{t-K} - \\sqrt{1 - \\bar{a}_{t-K}}\\hat{\\epsilon}_{\\theta}(x_{t-K},t- K))$\n\nThis way, the Jacobian of $\\hat{x}_0$ w.r.t. $x_{t-K}$ can be computed analytically, with complexity independent of $t$. Note that the above estimation of $x_0$ is not a trick it directly comes from a mathematically equivalent interpretation of the diffusion model, where each inference step can be viewed as computing $\\hat{x}_0$ and plugging it into $q(x_{t-K}|x_t, \\hat{x}_0)$ to obtain the transitional probability (See Appendix C for the derivation).\n\nRemark 1: Complexity Analysis With Shortcut Text Gradient, the computational cost of backpropagating through the inference process can be reduced to $K$-times backward passes of UNet. When we set $t = T$ and $K = T$, it becomes the full-text gradient; When $K = 1$, the computation costs reduce to a single backward pass. Remark 2: Connection to ReFL (Xu et al., 2024). ReFL is a post-hoc alignment method for finetuning diffusion models. It also adopts the estimation of $x_0$ when optimizing diffusion model against a scorer, which is mathematically equivalent to the case when $K = 1."}, {"title": "6.1.2. BACKPROPAGATING THROUGH EMBEDDINGS LOOKUP TABLE", "content": "In diffusion models, a tokenizer transforms text input into indices, which will be used to query a lookup table for corresponding word embeddings. To allow further propagating gradients through this non-differentiable indexing operation, we relax the categorical choice of words into a continuous probability of words and learn a distribution over them. We parameterize the distribution using Gumbel Softmax (Jang et al., 2016) with uniform temperature ($\\eta = 1$):\n\n$\\tilde{e} = \\sum_{i=1}^{|V|} e_i * \\frac{\\exp ((log \\alpha_i + g_i)/\\eta)}{\\sum_{j=1}^{|V|} \\exp ((log \\alpha_i + g_i)/\\eta)}                           (7)$\n\nwhere $a$ (a $|V|$-dimensional vector) denotes the learnable parameter, $g$ denotes the Gumbel random variable, $e_i$ is the embedding of word $i$, and $\\tilde{e}$ is the output mixed embedding."}, {"title": "6.2. Efficient sampling with Evolutionary Search", "content": "To efficiently sample candidate prompts from the learned Gumbel \"distribution\", we adopt evolutionary search, known for its sample efficiency (Goldberg, 1989; Wu et al., 2019). Our adaptation of the evolutionary algorithm to the prompt optimization task involves three key steps: (1) Genotype Definition: We define the genotype of each candidate prompt as the list of searched words from the compact search space, where modifications to the genotype correspond to edits the word choices in the prompt. (2) Population Initialization: We initialize the algorithm's population with samples drawn from the learned Gumbel distribution to bias the starting candidates towards regions of high potential. (3) Evolutionary Operations: We execute a standard evolutionary search, including several rounds of crossover and mutation (Goldberg, 1989), culminating in the selection of the top candidate as the optimized prompt. Details of the complete DPO-Diff algorithm, including specific hyperparameters, are available in Algorithm 1 of Appendix D and discussed further in Appendix F.1."}, {"title": "Remark: Extending DPO-Diff to Blackbox Settings", "content": "In cases where the model is only accessible through forward API, our Evolutionary Search (ES) module can be used as a stand-alone black-box optimizer, thereby expanding the applicability of our framework. As further ablated in Section 8.1, ES archives descent results with enough queries."}, {"title": "7. Experiments", "content": "7.1. Experimental Setup\n\nDataset preparation. To encourage semantic diversity, we collect a prompt dataset from three sources: DiffusionDB (Wang et al., 2022), ChatGPT generated prompts (Ouyang et al., 2022), and COCO (Lin et al., 2014). For each source, we filter 100 \u201chard prompts\" with a clip loss higher (lower for adversarial attack) than a threshold, amounting to 600 prompts in total for two tasks. Due to space limit, we include preparation details in Appendix G.1.\n\nEvaluation Metrics. All methods are evaluated quantitatively using the clip loss (Crowson et al., 2022) and Human Preference Score v2 (HPSv2). HPSv2 is a CLIP-based model trained to predict human preferences on images generated from text. For base models, we adopt Stable Diffusion v1-4. Each prompt is evaluated under two random seeds (shared across different methods). Besides automatic evaluation metrics, we also conduct human evaluations on the generated images, following the protocol specified in Appendix G.2.\n\nOptimization Parameters. We use the Spherical CLIP Loss (Crumb, 2022) as the objective function, which ranges between 0.75 and 0.85 for most inputs. The $K$ for the Shortcut Text Gradient is set to 1, as it produces effective supervision signals with minimal cost. To generate the search spaces, we prompt ChatGPT (gpt-4-1106-preview) for at most 5 substitutes of each word in the user prompt. Furthermore, we use a fixed set of hyperparameters for both prompt improvement and adversarial attacks. We include a detailed discussion on all the hyperparameters and search space generation in Appendix F."}, {"title": "7.2. Application 1 - Adversarial Attack", "content": "Unlike RLHF-based prompt-engineering methods (e.g. Promptist (Hao et al., 2022)) that require finetuning a prompt generator when adapting to a new task, DPO-Diff, as a train-free method, can be seamlessly applied to finding adversarial prompts by simply reversing the sign of the objective function.\n\nIn this section, we demonstrate that DPO-Diff is capable of discovering adversarial prompts that destroy the prompt-following ability of Stable Diffusion.\n\nAs suggested by (6), a successful adversarial prompt must not change the original intention of the user prompt. While we specified this constraint to ChatGPT when building the Synonyms Space, occasionally ChatGPT might mistake a word for the synonyms. To address this, during the evolutionary search phase, we perform rejection sampling to refuse candidate prompts that have different meanings to the user input. Concretely, we enforce their cosine similarity in embedding space to be higher than 0.9 (More on this can be found in Appendix G)."}, {"title": "7.3. Application 2: Prompt Improvement", "content": "In this section, we apply DPO-Diff to craft prompts that improve the prompt-following ability of the generated images. We compare our method with three baselines: (1) User Input. (2) Human Engineered Prompts (available only on DiffusionDB) (Wang et al., 2022). (3) Promptist (Hao et al., 2022), trained to mimic the human-crafted prompt provided in DiffusionDB."}, {"title": "7.4. Qualitative analysis of search progression", "content": "To examine the convergence of our search algorithm qualitatively, we plot the progression of optimized images at various evaluation stages. We set the target iterations at 0 (the original image), 10, 20, 40, and 80 to illustrate the changes, and showcase the image with the highest clip loss among all evaluated candidates at each iteration."}, {"title": "8. Ablation Study", "content": "We conduct ablation studies on DPO-Diff using 30 randomly sampled prompts, 10 from each source. Each search algorithm is run under 4 random seeds."}, {"title": "8.1. Comparison of different search algorithms", "content": "We compare four search algorithms for DPO-Diff: Random Search (RS), Evolution Prompt Optimization (EPO), Gradient-based Prompt Optimization (GPO), and the full algorithm (GPO + ES). While GPO tops EPO under low budgets, it also plateaus quicker as randomly drawing from the learned distribution is sample-inefficient. Combining GPO with EPO achieves the best overall performance."}, {"title": "8.2. Negative prompt v.s. positive prompt optimization", "content": "One finding in our work is that optimizing negative prompts (Antonyms Space) is more effective than positive prompts (Synonyms Space) for Stable Diffusion. To verify the strength of these spaces, we randomly sample 100 prompts for each space and compute their average clip loss of generated images. Such search space also allows the search algorithm to identify an improved prompt more easily. We conjecture that this might indicate diffusion models are more sensitive to changes in negative prompts than positive prompts, as the baseline negative prompt is merely an empty string."}, {"title": "9. Discussion on the Search v.s. Learning paradigms for utilizing computatons", "content": "This section elucidates the relationship between two distinct prompt optimization approaches for diffusion models: DPO-Diff (ours) and Promptist. While Promptist represents a pioneering effort, it is important to discuss why DPO-Diff remains essential.\n\nLimitations of Promptist Promptist utilizes the Reinforcement Learning from Human Feedback (RLHF) (Bain & Sammut, 1995; Christiano et al., 2017; Ouyang et al., 2022) approach to fine-tune a language model to generate improved prompts. RLHF relies on paired data (user_prompt, improved_prompt), which is scarce for diffusion models and challenging to curate. This is primarily because generating the improved prompts requires extensive trial-and-error by human experts, essentially performing what DPO-Diff automates. In fact, the performance limit exhibited by Promptist is exactly caused by this lack of data: The data used by Promptist from DiffusionDB predominantly features aesthetic modifiers that do not alter the semantics of the prompts This limits its effectiveness to aesthetic enhancements and not addressing the core need for semantic accuracy in prompts. Consequently, it struggles with semantic prompt adherence and lacks flexibility in modifying prompts for tasks such as adversarial attacks.\n\nTwo complementary computational paradigms Promptist and DPO-Diff represent two major paradigms for effectively utilizing computation: learning and searching, respectively (Sutton, 2019). Learning-based approach of Promptist enhances performance through more parameters and larger datasets, whereas the search-based approach of DPO-Diff focuses on maximizing the potential of pretrained models via post-hoc optimization. Although learning-based methods require high quality paired data, they can be efficiently deployed once trained; On the other hand, search-based methods generate high quality prompts, but are much slower to execute. Therefore, as Sutton (2019) highlights, these paradigms are complementary rather than competitive. DPO-Diff can be leveraged to generate high quality dataset offline, which can subsequently train Promptist to reduce inference latency effectively. Together, they pave the way for a comprehensive solution to prompt optimization for diffusion models, positioning DPO-Diff as the first search-based solution to address this problem."}, {"title": "10. Conclusions", "content": "This work presents DPO-Diff, the first gradient-based framework for optimizing discrete prompts. We formulate prompt optimization as a discrete optimization problem over the text space. To improve the search efficiency, we introduce a family of compact search spaces based on relevant word substitutions, as well as design a generic computational method for computing the discrete text gradient for diffusion model's inference process. DPO-Diff is generic - We demonstrate that it can be directly applied to effectively discover both refined prompts to aid image generation and adversarial prompts for model diagnosis. We hope that the proposed framework helps open up new possibilities in developing advanced prompt optimization methods for text-based image generation tasks.\n\nLimitations To motivate future work"}, {"title": "A. Limitations", "content": "We identify the following known limitations of the proposed method: Search cost Our method requires multiple passes through the diffusion model to optimize a given prompt, which incurs a modest amount of search costs. One promising solution is to use DPO-Diff to generate free paired data for RLHF (e.g. Promptist), which we leave for future work to explore. Text encoder moreover, while DPO-Diff improves the faithfulness of the generated image, the performance is upper-bounded by the limitations of the underlying text encoder. For example, the clip text encoder used in stable diffusion tends to discard spatial relationships in text, which in principle must be resolved by improving the model itself, such as augmenting the diffusion model with a powerful LLM (Lian et al., 2023; Liu et al., 2022; Feng et al., 2022). Clip loss The clip loss used in DPO-Diff might not always align with human evaluation. Automatic scoring metrics that better reflect human judgment, similar to the reward models used in instruction fine-tuning, can further aid the discovery of improved prompts. Synonyms generated by ChatGPT For adversarial attack task, ChatGPT sometimes generate incorrect synonyms. Although we use reject-sampling based on sentence embedding similarity as a posthoc fix, it is not completely accurate. This may impact the validity of adversarial prompts, as by definition they must preserve the user's original intent. We address this in human evaluation by asking the raters to consider this factor when determining the success of an attack."}, {"title": "B. Benefit of optimizing discrete text prompts over soft prompts", "content": "Optimizing discrete text prompts offers two major advantages over tuning soft prompts, primarily in two areas: (1) Interpretability: The results of discrete prompt optimization are texts that are naturally human interpretable. This also facilitates direct use in fine-tuning RLHF-based agents like Promptist. (2) Simplified Search Space: Our preliminary attempts with continuous text embeddings revealed challenges in achieving convergence, even on toy examples. The reason, we conjecture was that the gradients backpropagated through the denoising process have low info-to-noise ratio; And updating soft prompt using such gradient could be very unstable due to its huge continuous search space. In contrast, discrete prompt optimization effectively narrows the search to a finite vocabulary set, greatly reducing search complexity and improving stability."}, {"title": "C. Derivation for the alternative interpretation of DDPM's modeling.", "content": "Proposition C.1. The original parameterization of DDPM at step $t - K$: $\\mu_{\\theta}(x_{t-K},t \u2013 K) = \\frac{\\sqrt{a_{t-K}}\\beta_{t-K}}{(1-\\bar{a}_{t-K})}\\hat{x}_0 + \\frac{\\sqrt{\\bar{a}_{t-K}}(1-\\bar{a}_{t-1})}{(1-\\bar{a}_{t-K})}x_{t-K} = \\frac{1}{\\sqrt{a_{t-K}"}]}