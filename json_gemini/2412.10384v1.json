{"title": "Adult learners' recall and recognition performance and affective feedback when learning from an AI-generated synthetic video", "authors": ["Zoe Ruo-Yu Li", "Caswell Barry", "Mutlu Cukurova"], "abstract": "The widespread use of generative AI has led to multiple applications of AI-generated text and media to potentially enhance learning outcomes. However, there are a limited number of well-designed experimental studies investigating the impact of learning gains and affective feedback from AI-generated media compared to traditional media (e.g., text from documents and human recordings of video). The current study recruited 500 participants to investigate adult learners' recall and recognition performances as well as their affective feedback on the AI-generated synthetic video, using a mixed-methods approach with a pre-and post-test design. Specifically, four learning conditions-AI-generated framing of human instructor-generated text, AI-generated synthetic videos with human instructor-generated text, human instructor-generated videos, and human instructor-generated text frame (baseline) were considered. The results indicated no statistically significant difference amongst conditions on recall and recognition performance. In addition, the participant's affective feedback was not statistically significantly different between the two video conditions. However, adult learners preferred to learn from the video formats rather than text materials.", "sections": [{"title": "1. Scope", "content": "This document demonstrates the study analyses and results of a comparative study investigating the recall, recognition, and affective feedback differences of adult learners learning the same content as a text framed as written by a human expert, as a text written by genAI, delivered by a human in video recording, delivered by an AI-generated avatar in a synthetic video. The work is completed as part of the first author's PhD study."}, {"title": "2. Objective", "content": "To explore the differences in learning gains between four conditions, namely, the same teaching content is i) framed as human-generated, ii) framed as AI-generated, iii) delivered by human video recording, and iv) delivered by AI-generated synthetic video. Moreover, to understand the adult learners' affective feedback and perceived experiences in four learning conditions."}, {"title": "3. Background", "content": "A wealth of educational research regarding large language models, such as ChatGPT, has emerged recently (Jeon et al., 2023; Kohnke et al., 2023; Lo, 2023). Several studies indicated that learning with generative AI could facilitate students' learning gains and learning engagement (Alneyadi & Wardat, 2023; Bachiri et al., 2023; Wu & Yu, 2024) and boost behavioural intention from hedonic motivation (Strzelecki, 2023). Moreover, few researchers have embarked on examining learning assisted by innovative AI-generated synthetic videos. Among these explorative studies, one discovered that the different designs of Al virtual characters impacted the learners' motivation (Pataranutaporn et al., 2022); another revealed the learning gains of the adult learners did not have a significant difference between the AI-generated synthetic video and the instructor video (Leiker et al., 2023), indicating that AI-generated synthetic media can be equally effective as human video recordings of lectures.\nFrom the cognitive neuroscience perspective, memory retrieval is one of the crucial themes associated with learning. To illustrate, episodic memory includes separate retrieval processes: recall (or recollection) and recognition (or familiarity) based on the \u201cdual process model\u201d (Curran, 2000; Yonelinas, 2001; Yonelinas et al., 2010). Recall involves remembering specific characteristics of an item, resulting in more information being memorised; by contrast, recognition relies on the strength of undifferentiated messages, which leads to lower memory retrieval (Banich & Compton, 2018). Furthermore, whilst memory decreases with age, the difference in delayed verbal recall and verbal recognition between young and middle-aged adults is insignificant (Grady et al., 2006). Therefore, this study recruited young and middle-aged adults aged 22 to 45 years to control general memory ability.\nWhile earlier studies indicated that there might not be a statistically significant difference between human-instructor content and AI-generated text or media, these studies did not consider if there are differences in participants' retrieval of episodic memory at a higher level, such as recall or a lower level, such as recognition. In addition, limited work investigates learners' affect in AI-generated synthetic videos; however, the affective states may lead to greater demands on memory processing, consequentially impacting learners' learning gains. At last, previous research lacked scaled investigations with a large number of participants providing significant statistical power in the analyses. Thus, the main objective of the current research is to investigate learning gains of memory retrieval and affective feedback when adults learn from Al-generated content and videos in an experimental study involving 500 participants in total."}, {"title": "4. Methodology", "content": ""}, {"title": "4.1. Research structure", "content": "The current study adopted a quantitative approach to data measurement using a set of cognitive and affect surveys through Prolific (an online platform that facilitates research implementation and data collection). The two-by-two experimental design consisted of four learning conditions, shown in Figure 1. The two control groups separately used the human instructor-generated text frame and video. In contrast, the other two experimental groups used the AI-generated text frame and AI-generated synthetic video, respectively. Furthermore, the text content from condition one served as the same source for the other three conditions. In other words, condition three's text was the same as that of condition one; however, the participants were informed that this was generated by the specific trained Al model (a deception method in psychological research). This study then explained the deception at the end of the survey (Boynton et al., 2013). Conditions two and four shared the same content as condition one, while the only difference lay in whether the instructor was human or AI-generated synthetic media."}, {"title": "4.2. Research process", "content": "A similar procedure was conducted for both the control and the experimental groups. The experiment consisted of two phases, the duration of which ensured enough time for memory encoding and retrieval. In this study, the retention interval of 30 minutes was set to divide the first learning phase and the second test phase, namely after the learning material presentation and before the post-test, according to the previous studies (Bader & Mecklinger, 2017; Curran & Friedman, 2004; Desaunay et al., 2020). To illustrate, an information sheet was initially shown, followed by the consent form. Only participants who ticked all the boxes in the form could proceed to the pre-test. The text or video was then presented to the participants independently in the four conditions, in which video conditions were set to watch the full video before they entered the next page. Subsequently, a 30-minute interval with an alarm that reminded the participants to come back between 30 to 35 minutes. Participants could do what they wanted to do during the break. After the interval, the online survey finally delivered the post-test in all conditions, alongside an affective assessment for both video conditions."}, {"title": "4.3. Participants", "content": "Initially, 100 participants were used in a pilot study to test the experimental procedure as well as the instruments used in the study. The pilot study was comprised of the same process as the formal study, with around 25 participants in each condition, which not only served to identify errors in the online surveys but also to modify items and analysis methods. The primary study recruited 400 participants from Prolific, randomly assigning them to four groups with varying learning conditions. Each group had about 100 participants ranging from 22 to 45 years old, namely young and middle-aged adults, based on the research of Grady et al. (2006). Before collecting data, the author set four screeners on Prolific: the age range, employment status of full-time and part-time, primary language as English, and English fluency. Furthermore, 100 participants who attended the pilot test before this study were also excluded by adding to the block list. The potential participants were ultimately identified through the platform pool, where their data was fully anonymised during the research process. The survey remuneration for each participant was around \u00a39 per hour, and the median duration was nearly 1 hour and 34 minutes."}, {"title": "4.4. Material and instrument", "content": ""}, {"title": "4.4.1. Material content", "content": "This study cooperated with Synthesia, a market-leading platform for AI-generated video creation, in the material design. As for the human instructor's text, this study searched for a food hygiene lesson around 7.5 minutes from a YouTube channel: Training Express (https://youtu.be/31abMOmIDoA?si=v8hZIe8mYhw-ft29)."}, {"title": "4.4.2. Instrument development", "content": "The current study created 20 pre- and post-test subject items relevant to the experiment material. The process was divided into two stages. 10 items made up the first stage of identifying the recognition level, while 10 items with cues in the post-test, such as those requesting more information and details, made up the second stage of the recall level. Furthermore, through the pilot study, the researcher maintained the use of yes-no questions in the recognition test but replaced the original short-answer questions with fill-in-the-blank questions in the recognition test to reduce the analysis complexity. The instructional manipulation checks (IMCs) randomly appeared in the recognition test to filter the participants who lacked attention (Oppenheimer et al., 2009). Moreover, the affective survey in this study was an adapted version built by Wiggins et al. (1988), in which Cronbach's a were 0.77 and 0.87 for the dominance and affiliation items, respectively (Knutson, 1996). The 16 affective items were evaluated: outgoing, cunning, unfriendly, friendly, shy, sly, sympathetic, unsympathetic, honest, unaggressive, kindhearted, unsocial, dominant, straightforward, antisocial, and assertive. All the questionnaire items, including memory and affective surveys, were designed and delivered using Qualtrics."}, {"title": "4.5. Data analyses", "content": ""}, {"title": "Step 1. Data pre-processing", "content": "After collecting 400 questionnaires, the author checked the dataset and discarded incomplete or invalid responses (n = 20). The number of valid responses was thus 380, with a high valid rate of 95%. Later, one outlier was identified in the recognition test (Standardised DfFit = -0.37; Covariance ratio = 0.88), and two outliers were extracted in the recall test (Covariance ratio = 1.06; Leverage's h value = 0.04). These outliers (n = 3) were removed before further analyses. Additionally, R Studio was used to perform all the statistical computations in this study."}, {"title": "Step 2. Descriptive statistics", "content": "The descriptive data comprised material learning time and demographic data. This study applied the independent t-test to analyse learning duration if there were differences between conditions. In addition, the selected demographic data provided by Prolific included employment, gender, ethnicity, and nationality. The multiple linear regressions were then tested to see if the data above could explain the post-test scores."}, {"title": "Step 3. Pre-post-test analysis", "content": "Before statistical analysis, each yes-no question in the recognition test was worth 10 marks, of which the total score was 100. In the recall test, each fill-in-the-blank question counted as 10 with a total score of 100, marking 3.33 or 2 points for a blank answer based on the blank numbers in one question. ANCOVA was subsequently employed to identify the post-test performances of the four conditions when controlling the pre-test scores. Multiple hypothesis tests would be checked before ANCOVA, which involved the linear relationship between dependent and covariate variables, homogeneity of regression coefficients, normality of dependent variables, and variation homogeneity. The partial eta squared was then calculated, and the means were adjusted by removing interaction terms."}, {"title": "Step 4. Post-test analysis for recall items", "content": "MANCOVA was performed to compare advanced differences between every item's response in the four conditions. A series of assumption tests were calculated, such as the homogeneity of the covariance matrix (Box's M test), multivariate normality, homogeneity of covariate slopes, residual independence (Durbin-Watson's test), variance homogeneity, and variation inflation factor (VIF test). If the result did not pass the normality test, a non-parametric method, PERMANOVA, was used to supplement the MANCOVA result."}, {"title": "Step 5. Affective assessment analysis", "content": "The mean scores of 16 affective items from both video conditions were computed and presented in a bar chart. If any two means of an item were significantly different, a paired t-test was performed. Moreover, Pearson's correlation was used to analyse the similarity between the two video conditions."}, {"title": "Step 6. Open-ended questions", "content": "The percentages of descriptive statistics were ultimately applied to elaborate on the results of the questions about the participants' preference for learning formats. In addition, the participants' material feedback was also summarised using ChatGPT-40 and then checked by the author to make further comparisons across the four conditions."}, {"title": "5. Results", "content": ""}, {"title": "5.1. Material learning time", "content": "The material learning time was defined as when participants stayed on the same material page to read the text or watch the video. Specifically, the learning durations of both text conditions were shorter than the video conditions, as shown in Table 1. Moreover, while the C2 human instructor-generated video length was 7 minutes 31 seconds and the C4 AI-generated synthetic video was 6 minutes 19 seconds, the video player allowed the pause button, given the mean watching time of 9.7 and 7.71 minutes, respectively. For further assessments, C2 and C4 were significantly different (t = 4.46, p < .001) in watching time; however, the normalised pause time, namely watching time minus the video length and then divided by the watching time, in both conditions was similar (t = 0.6, p = .55)."}, {"title": "5.2. Demographic data", "content": "The pie charts in Figure 3 and the bar chart in Figure 4 illustrate the participants' demographic data, including their employment status, sex, ethnicity, and nationality. In general, most of the participants had full-time positions, with around 80%. There was also an imbalance in gender, with 62.5% females and 37.5% males; similarly, a large proportion of participants were black, with 64.72%, while 21.75% were white, and the remaining 13.53% were other ethnicities. The reason for ethnicity composition may be associated with the participant's nationality, of which 236 people came from South Africa, with nearly 62.59%. Moreover, the multiple regression results were not presented because neither demographic data nor learning time could noticeably explain the post-test."}, {"title": "5.3. Recognition test result", "content": "First, all the dependent variables (DV) passed the normality test, in which skewness and kurtosis were between 3 and \u20133. The linear relationship between the DV, namely the post-test scores, and the covariate, namely the pre-test scores, was established (t = 4.05, p < .001). The regression coefficients also reached homogeneity (F(3,371) = 1.42, p = .24), the same with the variation homogeneity test (Levene's F(3,375) = 0.67, p = .57). Second, since the interaction term (pre-test*condition) was insignificant based on the homogeneity test of the regression coefficient, the original means of the post-test scores from C1 to C4 should be adjusted in the order as follows: 60, 60.1, 62.8, and 63.1, measured in a 0 to 100 scale, as demonstrated in Figure 5. The error bar represents the standard error of the mean in the figure."}, {"title": "5.4. Recall test result", "content": "The recall pre-post-test data passed all ANCOVA hypotheses. To elaborate, DV data showed the normality resulting from the skewness and kurtosis were between \u00b13. The linear relationship between DV and covariate was identifiable (t = 6.43, p < .001). Furthermore, the data did not reject the null hypotheses of homogeneity in regression coefficients (F(3,370) = 1.67, p = .17) and homogeneity of variation (Levene's F(3,374) = 2.41, p = .07). The adjusted means of the post-test scores of four conditions in the order were 56.7, 56.5, 56.9, and 52.3, measured in a 0 to 100 scale, as presented in Figure 6."}, {"title": "5.5. Recall item result", "content": "MANCOVA was performed to explore the differences in conditions per recall item. While passing the Box's M test of covariance matrix homogeneity ($\\chi^2$ (165) = 191.97, p = .07), the data failed the multivariate normality test. Nevertheless, due to the large sample size, the impact on MANCOVA did not affect the test results (Huberty & Morris, 1992). Furthermore, only two items, Q1 and Q7, failed the homogeneity test of covariate slopes. To tackle this problem, these two interaction terms were added to the MANCOVA model. Also, two items did not pass the homogeneity test of variances (Levene's $F_{04}$ (3,374) = 18.92, p < .001; $F_{07}$ (3,374) = 5.98, p < .001), which might limit the explanation of the results. The data passed the independence test of residuals (Durbin-Watson's DW = 2.1, p = .78) alongside the VIF test, in which all VIF values of pre-test items and conditions were lower than 1.5.\nThe MANCOVA results, presented in Table 4, indicated that almost all covariates significantly affected the corresponding post-test scores, which aligned with the study's assumption, except for the pre-test of Q8 (Pillai's Trace = 0.04, F(10,349) = 1.33, p = .21, $\\eta^2$ = .03). Even though accounting for the latent influences of the covariates in the MANCOVA model, the main effect of the conditions remained significant (Pillai's Trace = 0.4, F(30,1053) = 5.4, p < .001, $\\eta^2$ = .13). In other words, the post-test means for each item were notably different across the four conditions. Additionally, due to the failure of the normality test, this study used a non-parametric method, PERMANOVA, to verify MANCOVA. Since PERMANOVA also demonstrated significant differences between all conditions in the item level (F(3,374) = 3.34, p < .01), the report of MANCOVA was warranted."}, {"title": "5.6. Affective assessment result", "content": "Both video conditions demonstrated similar mean scores in 16 affective items, as Figure 9 illustrated, based on the insignificant results of the paired t-tests for all items ($t_{outgoing}$ = 1.22, p = .22; $t_{cunning}$ = -0.91, p = .36; $t_{Unfriendly}$ = -0.24, p = .81; $t_{Friendly}$ = -0.15, p = .88; $t_{shy}$ = 1.18, p = .24; $t_{sly}$ = -0.44, p = .66; $t_{Sympathetic}$ = 0.05, p = .96; $t_{Unsympathetic}$ = -0.63, p = .53; $t_{Honest}$ = 0.45, p = .66; $t_{Unaggressive}$ = 0.07, p = .94; $t_{Kindhearted}$ = 0.23, p = .82; $t_{Unsocial}$ = -0.87, p = .39; $t_{Dominant}$ = 0.68, p = .5; $t_{straightforward}$=-1.3, p = .2; $t_{Antisocial}$ = -0.35, p = .73; $t_{assertive}$ = -0.43, p = .67). Moreover, C2 and C4 presented moderate correlation (Pearson's r = .62, p < .01), demonstrated in Table 5, according to the strength of correlation coefficients (Dancey & Reidy, 2004, 2020)."}, {"title": "5.7. Open-ended questions", "content": "This study had three open-ended questions, including one optional for material feedback. The first question was, \"Do you think the learning material in this text/video format is better than watching/reading it in a video/text format?\" To make equivalent comparisons, the human-generated text in C1 was used to compare separately in both video conditions. In contrast, a one-minute clip of the AI-generated video in C4 compared the text conditions. Overall, the preference in all conditions was video format. Among the text conditions, 39% of participants preferred text reading, while in C3, merely 27% of participants preferred AI text. Most participants in C2 preferred video format, with a high 88%; however, the rate slightly reduced to 77% in C4. Furthermore, five participants in C2 reflected they experienced uncanny or uncomfortable feelings with the AI avatar. Regarding the second question for video conditions only, C2 and C4 achieved the same rate of 94%, which participants loved to engage with and learn from the material videos if their organisations used them as part of their training programs.\nFinally, in the third optional question for participant feedback, most of the responses were positive on the informative, concise, and educational content across all conditions, with 45% (n = 38), 70% (n = 43), 57% (n = 37), and 61% (n = 41), in the order of C1 to C4. 4 participants in the text conditions praised an advantage of self-paced reading compared to the video:\n\"Text material is easy to refer back to it. (C1)\"\n\"Text material has the advantage of being able to read at my own pace till 1 understand everything. (C3)\"\nHowever, 5% of the participants in both text conditions felt the material was boring and hard to hold attention:\n\"Although the text is written in an \u2018easy-going' tone, it is still a block of text, making it boring to read and remember the content. (C1)\"\n\"It was well written, but text alone is dull when it comes to learning material; visuals help with recall. (C3)\"\nAlso, 17% of them provided several edit suggestions, such as visual diagrams, bullet points, or pamphlets:\n\"It was very wordy, and I feel like bullet points or images would have helped. Breaking up the text might also help hold attention. (C1)\""}]}