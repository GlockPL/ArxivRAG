{"title": "Diversity Progress for Goal Selection in Discriminability-Motivated RL", "authors": ["Erik M. Lintunen", "Nadia M. Ady", "Christian Guckelsberger"], "abstract": "Non-uniform goal selection has the potential to improve the reinforcement learning (RL) of skills over uniform-random selection. In this paper, we introduce a method for learning a goal-selection policy in intrinsically-motivated goal-conditioned RL: \"Diversity Progress\" (DP). The learner forms a curriculum based on observed improvement in discriminability over its set of goals. Our proposed method is applicable to the class of discriminability-motivated agents, where the intrinsic reward is computed as a function of the agent's certainty of following the true goal being pursued. This reward can motivate the agent to learn a set of diverse skills without extrinsic rewards. We demonstrate empirically that a DP-motivated agent can learn a set of distinguishable skills faster than previous approaches, and do so without suffering from a collapse of the goal distribution\u2014a known issue with some prior approaches. We end with plans to take this proof-of-concept forward.", "sections": [{"title": "1 Introduction", "content": "Intrinsically-motivated learning has been studied extensively in the reinforcement learning (RL) literature (see recent reviews by Colas et al., 2022; Aubret et al., 2023; Lidayan et al., 2024). Here, we focus on intrinsically-motivated skill acquisition, often referred to as competence-based intrinsic motivations (CB-IMs), an area of control problems requiring multiple skills. How to learn a diverse set of skills is a key subproblem of CB-IMs (Colas et al., 2022, p. 1161). In answer, one class of CB-IMs uses rewards computed as functions of the agent's certainty of following the goal it chose to pursue, that is, the goal's discriminability. It is postulated in existing work that the more discriminable a set of goals is, the more diverse we expect the skills to be in terms of observed behaviour (see Section 2.1). Many such discriminator-based models select goals uniformly during training; yet, learning a distribution over goals has the potential to speed up learning (see Section 3).\nOur contribution is threefold. (1) We present Diversity Progress (DP) (Section 4), a method for learning a goal-selection policy prioritising goals based on the observed learning progress over a set of goals. (2) We complement the formalism with empirical findings (Section 5), suggesting that a DP-motivated agent can improve existing discriminator-based CB-IMs by speeding up the learning of a diverse set of skills. (3) We detail plans for taking this proof-of-concept forward (Section 6)."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Discriminability-motivated RL", "content": "Our method falls within the framework of goal-conditioned reinforcement learning (GCRL), in which a skill is a policy paired with a goal, which can be identified with a goal-conditioned reward function (see Appendix A). We focus on a class of intrinsic rewards used to maximise behavioural mutual information (BMI) (termed by Hansen et al., 2020, p. 2; e.g., Gregor et al., 2016; Warde-Farley et al., 2019; Eysenbach et al., 2019; Baumli et al., 2021; Laskin et al., 2022), though our method could be used with any discriminator-based CB-IM. These rewards approximate the mutual information between the goal-defining variable, g, and some function of the trajectory drawn from the corresponding skill, $f(T_{\\pi_g})$. Typically, f maps a trajectory to a single state, but some formulations map to multiple states (e.g., Gregor et al., 2016, use the initial and final states). Following Hansen et al. (2020, p. 4), the BMI objective can be defined as the maximisation of:\n$I(g; f(T_g)) := H(g) \u2013 H(g | f(T_g))$,\nwhere H represents Shannon entropy. However, mutual information can be notoriously hard to compute (Hjelm et al., 2019, p. 1), and a popular approach is to maximise a variational lower bound:\n$\\hat{I}(g; f(T_g)) \\geq H(g) - E_{g \\sim p(g), T_{\\pi_g} \\sim \\pi(g)} [log q(g | f(T_{\\pi_g}))]$,\nwhere q is an arbitrary variational distribution (Barber and Agakov, 2003, p. 2). Given $f(T_{\\pi_g})$, q defines a probability distribution over goals. That is, a probability model, q, predicts, for each skill, the probability that the skill has induced the observations\u2014thus the model is canonically known as a discriminator\u2014and is typically used to compute the reward. Successfully discriminating skills in the observation space requires the agent to observe distinct regions of the state space, encouraging the agent to learn a set of diverse behaviours. If a goal is not discriminable based on observations, two or more skills are producing overlapping behaviours (and therefore, the skills lack diversity); conversely, if a goal is discriminable, then the corresponding skill is inducing trajectories unique to that skill."}, {"title": "2.2 Learning Progress", "content": "Learning Progress (LP) (e.g., Oudeyer et al., 2007; Schmidhuber, 2010) is designed \u201cto select goals that are of learning-optimal difficulty with respect to [the agent's] current capabilities\" (Lintunen et al., 2024, p. 39). Oudeyer et al. (2007, pp. 270\u2013271) formulate LP as an intrinsic reward measuring how much the agent has improved in some prediction over a window of time. For any decision that affects learning, the LP at time t + 1 can be computed for each option, indexed by n:\n$LP_n(t + 1) := \\bar{e}_n(t + 1 - \\tau) \u2013 e_n(t + 1)$,\ngiven an offset, $\\tau$, of the comparison, in time steps. A smoothing hyperparameter $\\eta$ adds robustness to stochastic fluctuations, so the average error $\\bar{e}_n$ is defined as the mean over the smoothing window:\n$\\bar{e}_n(t + 1 - \\tau) := \\frac{1}{\\eta} \\sum_{i=0}^{\\eta} e_n(t + 1 \u2212 \\tau \u2212 i)$,\nwhere $e_n (t + 1)$ is the difference between some prediction and the true observation made due to choosing option n. Originally, predictions were of state observations (Oudeyer et al., 2007, p. 271). LP has since been extended to measures of competence beyond prediction error (Colas et al., 2019)."}, {"title": "3 Related work", "content": "BMI-maximising CB-IMs In VIC (Gregor et al., 2016), one of the earliest BMI-related approaches, the agent learns a categorical distribution over goals, $g \\sim p(g)$-like our method. VIC reinforces $p(g)$ to maximise the same BMI-based intrinsic reward as is used for learning the skills (p. 4). However, Eysenbach et al. (2019, p. 6) showed that in VIC the probability mass collapses to a handful of skills. In response, Eysenbach et al. fixed p(g) as uniform in DIAYN, so all skills receive, in expectation, equal training signal. A side effect is inefficient use of training samples: the curriculum is not optimised for improving discriminability. DP aims to train more efficiently, via a learned goal-selection policy that could be used with any BMI-maximising CB-IMs. Laskin et al. (2022, Table 2, p. 10) list several discriminator-based approaches, noting key differences in reward formulations.\nLP-based CB-IMs LP has been used to motivate goal selection in a variety of ways (Colas et al., 2022, p. 1185). Learning p(g) can significantly improve the speed of learning a set of skills (Stout and Barto, 2010, pp. 260-261). Our method formulates goal selection as a bandit problem rewarded using LP, much like the approach used by CURIOUS (Colas et al., 2019), one such LP-based CB-IM. This formulation is in contrast with VIC, which sets an RL problem, conditioning its goal-selection policy"}, {"title": "4 Diversity Progress", "content": "Our method motivates the agent to prioritise goals that provide most progress in the learning of the discriminator, over all skills. That is, if pursuing one goal improves the discriminability of multiple skills, it is reflected positively in the goal-selection probabilities. We call this Diversity Progress.\nAt a given time, t + 1, the agent's prediction error for a given goal, $g \\sim p(g)$, is defined by:\n$e_g(t + 1) := \\begin{cases} 1 - q(g | s_{t+1}), & \\text{if g corresponds to the skill being followed} \\\\ q(g | s_{t+1}), & \\text{otherwise,} \\end{cases}$\nwhere $e_g (t + 1)$ is an element of $e(t + 1) \\in \\mathbb{R}^{|\\mathcal{G}|}$, a vector composed of the error, at t + 1, for each goal, and q is a discriminator. Then, the errors over an epoch of fixed time length T form a $T \\times |\\mathcal{G}|$ matrix. Given hyperparameters offset, $\\tau$, and smoothing, $\\eta$, (see Section 2.2), we use Equation 4 to compute the most recent average errors, $\\bar{e}(t + 1)$, and the average errors from $\\tau$ time steps earlier, $\\bar{e}(t + 1 \u2013 \\tau)$. Following Equation 3, we compute the LP for each goal, with goal g corresponding to option n. Then, the LP values are averaged over goals to compute the Diversity Progress:\n$DP(t + 1) := \\frac{1}{|G|} \\sum_{g \\in G} \\bar{e}(t + 1 - \\tau) - \\bar{e}(t + 1)$.\nIn this way, the mean progress over all goals is attributed to the goal being pursued, since that goal determines the actions taken. For each goal, g, DP is updated after executing the corresponding skill. The goal-selection probabilities, p(g), are computed via softmax. For the algorithm see Appendix B."}, {"title": "5 Experiments", "content": "For details of implementation and evaluation see Appendices C and D, respectively.\nRQ1. Does the probability mass of the goal distribution collapse?\nTo learn a diverse set of skills, it is material to train a substantial number of skills. Eysenbach et al. (2019, p. 6) showed that, with VIC's goal-selection method, the effective number of skills (see Appendix D.2) collapses to a handful. The method by Eysenbach et al. (2019), DIAYN, therefore selects goals uniformly. Does DP avoid a collapse like VIC's? Figure 1 shows the effective number of skills over training time for all three methods in three environments (see Appendix D.1). With the fixed p(g) in DIAYN, its effective number of skills is constant\u2014equal to |G|. DP does not collapse, and, additionally, we have control over the effective number of skills via the softmax temperature.\nRQ2. What are the effects of DP on the dynamics of goal selection?\nFigure 2 shows DP values, goal-selection probabilities, and cumulative frequencies (counts of how many times each goal has been selected thus far) for a small number of skills (five). Initially, without evidence on all goals, each goal is selected once. Then, the agent should focus on the \u201ceasiest-to-discriminate\" skills, up until repeated attempts generate less progress than other goals. Goals that are too \"hard\" generate little progress, so should be avoided until other goals are sufficiently mastered.\nConsider the purple and red goals. Red initially provided low DP, so was largely ignored until it was sufficiently easy (the policy is not completely greedy, so red is still updated occasionally despite its difficulty) or others provide less DP in comparison (e.g., purple declining). From the cumulative frequencies of selection, we see the red goal eventually caught up-explained by its increase in DP.\nRQ3. Does DP-guided selection of goals speed up learning distinguishable skills?\nLP should prioritise goals that are providing the most improvement to the discriminator. Therefore, we hypothesise that a DP-motivated agent learns a set of diverse skills faster than a learner sampling goals uniformly. We probe this by visualising trajectories in a dimension-reduced feature space to find out whether trajectories from early training are distinguishable based on their mean observations (Figure 3; details in Appendix D.3). Compared to random skills (no learning) and skills learned with uniform goal selection (DIAYN), the set of skills learned with DP is distinguishable earlier."}, {"title": "6 Conclusion and future work", "content": "We propose DP as a method for learning a goal-selection policy in discriminability-motivated RL, prioritising goals based on overall improvements in discriminability. We have shown in three experiments, that: (1) a DP-motivated agent learns a distribution over goals without the probability mass collapsing; (2) the DP values motivate goal selection with respect to observed LP; and (3) with DP, an agent can learn a diverse set of skills in less training time than with uniform-random selection.\nIn future work, we aim to better understand how different factors affect goal selection. We plan to test other intrinsic rewards combining discriminability with LP, including absolute LP, where the agent also attends to goals that it is forgetting (i.e., skills decreasing discriminability). Different entropy regularisation regimes may benefit diversity in terms of increased state-space coverage but make discrimination of skills harder. Following Eysenbach et al. (2019), the utility of DP can be tested on transfer learning and hierarchical tasks (pp. 6, 7\u20138), comparing with other discriminator-based CB-IMs, and on a range of environments including non-episodic and stochastic ones. However, evaluating open-ended learning is notoriously difficult. When the aim is to learn a diverse set of \u201cinteresting\" skills, what is interesting is typically subjectively defined by the researchers and the community, and not clearly designed to maximise a well-defined objective (Colas et al., 2022, p. 1168). We plan to improve the evaluation of skill diversity by considering different diversity metrics, and use our findings to quantitatively evaluate DP against the current state-of-the-art BMI-maxisiming CB-IMs."}, {"title": "A Goal-conditioned RL", "content": "Goal-conditioned reinforcement learning (for a review see Liu et al., 2022) extends the standard definition of a reward function (Kaelbling, 1993, p. 1) to be conditioned on goals:\n$R_G: S \\times A \\times S \\times G \\rightarrow \\mathbb{R}$,\nwhere S is the set of possible states, A is the set of actions available to the agent, and $g \\in G$, known as a goal, or a goal-defining variable, is a parameter to the reward function (cf., Colas et al., 2022, p. 1165; Aubret et al., 2023, p. 6). Note that $R_G(s, a, s', g)$ is a random variable, even for fixed $s, s' \\in S, a \\in A$, and $g \\in G$. Then, a skill is a policy paired with a goal, optimising for the return according to the reward conditioned on that goal. For example, the goal-defining variable, g, can be an index (e.g., Eysenbach et al., 2019) or an element drawn from a learned distribution (e.g., Nair et al., 2018). Its role is simply to indicate which reward function the agent is aiming to maximise.\nGoals can be viewed as \u201ca set of constraints ... that the agent seeks to respect\" (Colas et al., 2022, p. 1165, emphasis in original). While the most immediate intuition of a goal is often as a desired state for the agent to reach (e.g., Kaelbling, 1993, p. 1; Schaul et al., 2015, p. 2), the formalism allows for a more general set of constraints on behaviour (Lintunen et al., 2024, pp. 22\u201323). In effect, any behaviour that can be defined by attempting to maximise some reward function on the environment can be formulated as a goal-skill pairing."}, {"title": "B Algorithm", "content": ""}, {"title": "C Implementation", "content": ""}, {"title": "C.1 Learning objective", "content": "Our implementation is based on DIAYN, so following Eysenbach et al. (2019, p. 3) we wish to ensure: (1) that states not actions\u2014are used to distinguish skills; and (2) that the agent is maximising the entropy of its policies. This leads to the following instantiation of the BMI objective (see Section 2.1 for BMI; see Eysenbach et al., 2019, Equations 1-2, p. 4, for the full derivation):\n$F(\\theta) := I(S; G) + H(A | S) \u2013 I(A; G | S) = H(A|S,G) \u2013 H(G | S) + H(G)$.\nThe objective function is a variational lower bound on Equation 8 (see Eysenbach et al., 2019, p. 4):\n$F(\\theta, \\phi) := H(A | S, G) + E_{g \\sim p(g),s \\sim \\pi_{\\theta}(g)} [log q_{\\phi}(g | s) \u2013 log p(g)]$,\nwhere $q_{\\phi}$ is a discriminator, specifically a multi-layer perceptron (MLP), parametrised by $\\phi$."}, {"title": "C.2 Optimiser and reward function", "content": "In optimising its policy to maximise Equation 9, the agent relies on soft actor-critic (SAC) (Haarnoja et al., 2018), an off-policy maximum entropy actor-critic algorithm. In maximum entropy RL, where learning a stochastic policy is desirable, the standard RL objective of maximising expected return is augmented with an entropy maximisation term, so the optimiser takes care of the entropy term (a) (cf., Eysenbach et al., 2019, p. 4). The expectation (b) is maximised with the intrinsic reward function\n$R := log q_{\\phi}(g | s) \u2013 log p(g)$.\nThis expresses that the agent is rewarded for its ability to discriminate the skill being followed (see Section 2.1). The distribution over goals, p(g), is: (1) categorical; and (2) learned using DP (we describe the method for learning a goal-selection policy in Section 4). At the beginning of an epoch, the agent samples a skill, $\\pi_{g \\sim p(g)}$, and follows it until termination. That is when p(g) is updated."}, {"title": "C.3 Action-selection policy", "content": "At the action-selection level, the agent learns a goal-conditioned policy, $\\pi_{\\theta}(a | s,g)$. Following DIAYN (mujoco_all_diayn.py, ll. 210\u2013216), the policy is a Gaussian mixture model (GMM) with K components, where an MLP, parametrised by $\\theta$, maps from state-goal pairs to the (log) weight $w_k$, mean vector $\\mu_{\\kappa}$, and vector $\\sigma_{\\kappa}$ of (log) standard deviations, i.e., of the diagonal entries of the covariance matrix $\\Sigma_{\\kappa}$, of each Gaussian component:\n$\\pi_{\\theta}(a | s, g) := \\sum_{k=1}^{K} w_{\\kappa}N(a; \\mu_{\\kappa}, diag(\\sigma_{\\kappa}))$.\nThe weights $w_1,\u00b7\u00b7\u00b7, w_K$ are transformed into probabilities for selecting the component (let the parameters of the choice be indexed by *) from which to sample an action: $a \\sim N_*(a; \\mu_*, diag(\\sigma_*))$."}, {"title": "C.4 Hyperparameters", "content": "We tested various candidates for the DP hyperparameters: smoothing, $\\eta$, offset, $\\tau$, and softmax temperature $T_{SM}$. However, since we currently lack a good diversity metric to evaluate differences in performance, the hyperparameter choices in our experiments were mainly based on qualitative observations. Smoothing ranged 100\u2013250 time steps, offset 250\u2013900, and softmax temperature 0.1-0.75. The hyperparameter values used in producing the figures are listed in Table 1. The most noticeable difference was given by varying $T_{SM}$. To achieve stability in the level of greediness of the goal-selection policy, we normalised the mean LP before computing the mean progress over goals.\nFor reference, in the 2D environment, episodes last for 100 time steps, so a goal is sampled every 10 episodes. In the Half-Cheetah environment, episodes last for 1000 time steps, so a new goal is sampled for every episode. In the Ant environment, episodes last for a maximum of 1000 time steps (there is a potential termination condition before the maximum episode length), so a sampled goal may be used starting from partway through an episode and into another episode. For environment details see Appendix D.1.\nFor clarification, the same K and \u03b1 are used for DIAYN and DIAYN+DP across all our experiments. In Figure 3, the DP-specific hyperparameters (\u03b7, \u03c4, and $T_{SM}$) are not needed for DIAYN without DP."}, {"title": "D Evaluation", "content": ""}, {"title": "D.1 Environments", "content": "We carried out experimental comparisons of DP against DIAYN, our baseline method, in environments where DIAYN has been shown to perform well, following the advice of Patterson et al. (2023, p. 26). This choice eases comparison with related work. Specifically, we used three environments tested by Eysenbach et al. (2019): a slightly modified version of their 2D Navigation environment and two MuJoCo environments (Todorov et al., 2012), namely Half-Cheetah and Ant. The 2D Navigation environment affords diagnostics into the algorithms' function, and with the MuJoCo environments, we represent tasks of increasing complexity in terms of both observation space and degrees of freedom.\n2D Navigation Our 2D environment (Figure 4) is a slightly modified version of the one constructed by Eysenbach et al. (2019). An agent starts in the center of the unit box $s_0 = (0.5, 0.5)$ and observes states s \u2208 [0, 1]2. We modified the action space to $a \\in [-0.05, 0.05]^2$ (whereas the original used larger actions, in [-0.1, 0.1]2). The environment is bounded, in that if an action takes the learner outside of the box, they are projected back to the closest point in the support of the observation space.\nMuJoCo HalfCheetah-v1 and Ant-v1 with no modifications (Todorov et al., 2012)."}, {"title": "D.2 Effective number of skills", "content": "Following Eysenbach et al. (2019, Appendix E.2, p. 18), we compute the effective number of skills by exponentiating the entropy of the goal-selection policy, quantifying the number of skills that the policy is effectively sampling from at the time of selection. For intuition, consider an example of an occasion in which one goal is providing significantly more DP than all other goals. This will decrease the effective number of skills, as the learned distribution over goals is less uniform (favouring that one goal). If the DP is similar for all goals, then the distribution is close to uniform and the number of effective skills is close to the total number of skills being learned. With DP, we have some control over the effective number of skills via the softmax temperature: higher temperatures lead to less emphasis on the differences in DP and thus a more uniform sampling distribution; whereas, with low temperatures, we observe higher variance in the effective number of skills over time. High variance in the effective number of skills is likely desirable, since we wish to favour goals only if they provide more DP in comparison to others, and wish for the learner to sample more uniformly in cases of uncertainty (in terms of small differences between the DP values).\nLike Eysenbach et al. (2019, p. 6), we believe that VIC's goal-selection policy collapses due to the choice of optimiser (REINFORCE; Williams, 1992). But our results, as well as those from the wider LP literature, demonstrate their claim that learning p(g) \u201cresults in learning fewer diverse skills\" (Appendix E.2, p 18) to be overgeneralised."}, {"title": "D.3 Evaluating skill diversity with dimensionality reduction", "content": "To generate Figure 3, we first draw 100 trajectories from each skill, for each: (1) a set of random skills (no learning), (2) skills learned using DIAYN (uniform goal selection), and (3) skills learned using DIAYN with DP motivating goal selection. Then, we compute the multivariate means over each trajectory, and dimension-reduce the feature space to two dimensions using t-distributed stochastic neighbor embedding (t-SNE) with the following hyperparemeters: a perplexity of 30, a learning rate of 10, and running the optimisation for a maximum of 5000 iterations. In the plot, the distances between clusters may mean nothing (Wattenberg et al., 2016).\nThe same random seeds were used for the training of DIAYN and DIAYN+DP. Similarly, the seeds for the random number generators of both the policy simulator and t-SNE algorithm were fixed for the processes of simulating trajectories and dimensionality reduction across all three methods."}]}