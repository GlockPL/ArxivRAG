{"title": "Review-Feedback-Reason (ReFeR): A Novel Framework for NLG Evaluation and Reasoning", "authors": ["Yaswanth Narsupalli", "Abhranil Chandra", "Sreevatsa Muppirala", "Manish Gupta", "Pawan Goyal"], "abstract": "Assessing the quality of Natural Language Generation (NLG) outputs, such as those produced by large language models (LLMs), poses significant challenges. Traditional approaches involve either resource-intensive human evaluations or automatic metrics, which often exhibit a low correlation with human judgment. In this study, we propose Review-Feedback-Reason (ReFeR), a novel evaluation framework for NLG using LLM agents. We rigorously test ReFeR using two pre-existing benchmark datasets on diverse NLG tasks. The proposed framework not only enhances the accuracy of NLG evaluation, surpassing previous benchmarks by ~20%, but also generates constructive feedback and significantly improves collective reasoning. This feedback is then leveraged for the creation of instruction-tuning datasets, which, when used to fine-tune smaller models like Mistral-7B, makes them extremely good evaluators, yielding a better correlation with human evaluations and performance nearly on par with GPT-3.5. We highlight the effectiveness of our methodology through its application on three reasoning benchmarks, where it outperforms most of the state-of-the-art methods, and also outperforms the reasoning capabilities of models like GPT-3.5 Turbo by ~11.67% and GPT-4 by ~1% on an average.", "sections": [{"title": "1 Introduction", "content": "The rapid production of content by Foundation Models (FMs) (Bommasani et al., 2021), including large language models (LLMs), poses a challenge to traditional human-centric evaluation methods and conventional linguistic metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), which often misalign with human judgment and face limitations in assessing creative or nuanced responses.\nRecent developments suggest using LLMs as novel, reference-independent evaluators by assessing text quality based on predicted sequence likelihoods, bypassing the need for direct reference comparisons (Chen et al., 2023). Recently, there has been work (Liu et al., 2023b; Chiang and Lee, 2023) on improving the evaluation capability of individual LLMs. Surprisingly, although an ensemble of multiple LLMs is expected to perform better, there has not been much work on how to align evaluations using multiple LLMs with human judgments.\nWe thus introduce the Review-Feedback-Reason (ReFeR) framework, drawing inspiration from the academic peer review process to enhance the evaluation of NLG outputs. By using LLMs as evaluators and feedback providers in a system akin to academic peer review, ReFeR enables a nuanced and comprehensive evaluation of NLG tasks across various domains, promoting model self-improvement, explainability, and robustness in complex scenarios. The paper outlines ReFeR's methodology, including its unique evaluation schema (that diverges from existing benchmarks as outlined by Liu et al. (2023b) and Chiang and Lee (2023)) and the strategic use of LLM agents in roles parallel to peer reviewers and area chairs, facilitating a multi-dimensional evaluation and generating constructive feedback for model refinement.\nThe framework's efficacy has been validated across two established benchmarks. The ReFeR framework not only surpasses existing methods on benchmarks but also suggests its potential as a universal tool for evaluating NLG text. Furthermore, ReFeR's feedback mechanism has enabled the production of instruction-tuning datasets, which are used to fine-tune smaller models to evaluative standards nearly matching those of at least 25\u00d7 larger models like GPT-3.5, achieving a closer correlation with human evaluation. The framework's review and feedback mechanisms enhance its performance on reasoning tasks despite not being specifically pre-trained or fine-tuned for the purpose.\nTo summarize, the primary contributions of our research are as follows: (1) Introduction of a general purpose NLG evaluation framework, called as ReFeR, inspired by the traditional academic peer review system. (2) Development of a novel evaluation schema, incorporating an evaluation guidelines module alongside a critical comments module. (3) Creation of an automated instruction tuning dataset from the framework's feedback outputs, designed to enhance smaller models to achieve the evaluative prowess of their larger counterparts. (4) Empirical validation of the framework's capability to show enhanced reasoning skills on three benchmarks."}, {"title": "2 ReFeR Methodology for NLG Evaluation", "content": "In this section, we introduce ReFeR, a versatile framework for evaluating NLG tasks by using LLM agents in roles analogous to peer reviewers and area chairs, as found in the academic peer review process."}, {"title": "2.1 ReFeR Framework", "content": "The challenge of evaluating work without a predefined correct answer, such as determining the quality of a research paper, is traditionally addressed in academia through the peer review system. In this process, subject-matter experts called peer reviewers, independently review submissions. Authors then have an opportunity to address any concerns raised. Finally, senior researchers serving as area chairs review the adjusted feedback and make the final decision on whether to accept or reject the submission. Our framework draws inspiration from this process, and aims to evaluate NLG outputs replicating this academic review methodology.\nThe framework is structured into three distinct modules, as depicted in Fig. 1. The first module, the Peer Review Body, consists of three LLM agents. Each agent independently evaluates a specific NLG output, providing a comment and a rating. The following module is the Critic Module (optional), wherein another LLM agent, emulating a critic, assesses the evaluations made by the peer reviewers. The peer reviewers can then revisit these interactions and can adjust their assessments before forwarding their final reviews to the Area Chair Module. The final module features an LLM agent acting as an Area Chair, who considers the conclusive reviews to perform the ultimate evaluation of the NLG output. We Reason using the LLM Agents as peers and Area Chairs, take Feedback of peers and pass it to the area chair and finally give a Review or score to the NLG text. Hence our framework is named as ReFeR."}, {"title": "2.2 Evaluation Schema", "content": "An important aspect of assessing NLG outputs with LLM agents involves crafting prompts that elicit the highest quality evaluations. Prior work G-Eval by Liu et al. (2023b) introduced a structured evaluation schema, which organized the prompt into sections: task introduction, evaluation criteria, steps for evaluation, input presentation, and an evaluation form designed to output a numerical rating. Subsequently, Chiang and Lee (2023) demonstrated that a Chain of Thought (CoT) approach does not consistently yield the most accurate correlations with human judgment. They proposed an adjusted schema named Analyze-Rate, which prioritizes an analytical review followed by the scoring. This method showed improved performance over the G-Eval schema.\nTo further refine this approach, we introduce \"evaluation guidelines\" to enhance the peer reviewer's understanding of the scoring criteria, much like guidelines provided in traditional academic review processes. This modification posits that clear guidelines can improve evaluation accuracy by standardizing the scoring rationale. Evaluation guidelines can be automatically generated by prompting an LLM with examples from the dataset. We call this process 'Auto Prompt'. Another possible way to include evaluation guidelines is to use manually written human annotation guidelines of the dataset. We also changed the evaluation form to include a critical comment or reasoning for the given score. The proposed evaluation schema is shown in Fig. 2. This method has improved the performance, as was previously shown by Chiang and Lee (2023)."}, {"title": "3 Datasets", "content": "Following previous works (Zhong et al., 2022; Liu et al., 2023b; Chiang and Lee, 2023), our meta-evaluations predominantly utilize two datasets (SummEval, TopicalChat), each designed to test distinct evaluation capabilities of our framework for Summarization and Dialog Generation tasks.\nSummEval (Fabbri et al., 2021) provides human assessments on four critical dimensions of summarization quality: fluency, coherence, consistency, and relevance, utilizing the CNN/DailyMail dataset (Hermann et al., 2015) as its foundation. Considering computational budget and time constraints, we selected only 200 samples for our experiments.\nTopicalChat (Gopalakrishnan et al., 2019) establishes a framework for the meta-evaluation of evaluators in dialogue response generation systems, specifically those that incorporate knowledge elements. Our approach adheres to the methodology outlined in Zhong et al. (2022), employing human ratings to assess dialogues on four attributes: coherence, engagingness, groundedness, and naturalness."}, {"title": "4 Experiments and Results for NLG Evaluation", "content": ""}, {"title": "4.1 Baselines", "content": "While the current landscape of models for evaluating NLG responses includes reference-free methods such as BERTScore, GPTScore and UniEval (Zhang et al., 2020; Fu et al., 2023; Zhong et al., 2022), we do not consider these models as baselines given they were clearly surpassed by G-Eval (Liu et al., 2023b) and later works. Given our work primarily proposes a LLM based evaluation, we do a comparative analysis primarily against G-Eval (Liu et al., 2023b) and Analyze-Rate (Chiang and Lee, 2023). G-Eval presents a unique approach by deploying a single LLM agent. This agent employs Auto-CoT (chain of thought) reasoning and a form-filling paradigm to ascertain the quality of NLG outputs, delivering only scores for the specific dimensions under scrutiny. Analyze-Rate builds upon G-Eval, advocating for an enhanced prompt structure. This methodology incorporates a preliminary analysis phase before scoring, aiming to enrich the evaluative process for NLG tasks."}, {"title": "4.2 Experimental Setup", "content": "In response to G-Eval not releasing TopicalChat dataset prompts, Chiang and Lee (2023) crafted new prompts for the TopicalChat benchmark, mirroring the original G-Eval design. Consequently, we used their G-Eval prompts for TopicalChat in our experiments. This approach ensures consistency across evaluations and addresses the significant impact of prompt modifications on outcomes, potentially shifting the narrative.\nOur experimental framework employs GPT-3.5 Turbo (2023-06-13) (OpenAI, 2023), Gemini-Pro (Team et al., 2023), and Mixtral 8x7B (Jiang et al., 2024) as peer evaluators, and GPT-3.5 Turbo (with number of responses generated per prompt n = 20) acting solely as the Area Chair and Critic LLM. Appendix B provides details of LLM hyperparameters. Following (Fu et al., 2023; Liu et al., 2023b), we primarily report Spearman correlations (\u03c1) between the scores generated by our framework and those annotated by humans and use this as the primary differentiator to find the best model."}, {"title": "4.3 Main Results for NLG Evaluation", "content": "We assess ReFeR's performance through a series of experiments, employing a diverse array of LLM agents as peers and an Area Chair.\nTables 1 and 2 show \u03c1 and \u03c4 for SummEval and TopicalChat, respectively. Results are shown for individual performance by each of the 3 peers, and G-Eval and Analyze-Rate (both with GPT-3.5, n=20) as baseline methods. Our framework's results here does not use the Critic LLM Phase but we show results using 4 variations of the ReFeR framework with critic phase in Appendix D.1. By juxtaposing the ReFeR framework's outcomes against those derived from G-Eval, Analyze-Rate and contrasting these findings with individual peers' scores, we get insights into the substantial enhancements by our framework. Specifically, ReFeR surpasses the average Spearman correlation by ~20% on the SummEval dataset and by ~3% on the TopicalChat dataset when compared to best baseline performances.\nTable 1 compares different models based on coherence, consistency, fluency, and relevance on SummEval. Table 2 compares different models based on various aspects of conversation quality like coherence, engagingness, groundedness, and naturalness, and average scores on TopicalChat. We make the following observations for both the tables: (1) ReFeR framework performs better than the baselines in terms of spearman correlation (\u03c1) and kendall-Tau correlation (\u03c4). We give more detailed analysis of the framework's capabilities in Section 4.4. (2) The best baseline is Analyze-Rate, as expected.\nWe chose to report main results without the (optional) critic module because as highlighted by Laban et al. (2024), existing LLMs often exhibit fluctuating stances under scrutiny regarding their response confidence, suggesting a propensity for opinion revision. This observation implies that, for optimal correlation scores, it may be advantageous to bypass the critic module until such foundational issues within LLMs are addressed, at which point its incorporation could yield further benefits. However, for completeness, we report results using multiple critic LLMs in Appendix D.1.\nFurther, prompt sensitivity is a fundamental constraint of LLMs (Sclar et al., 2024; Loya et al., 2023). A poorly constructed prompt can skew results, leading to outcomes that deviate from expected benchmarks. This observation is further validated by the findings of Chiang and Lee (2023) in the automated evaluation domain. When the prompt is not optimized the results can be very misleading and can confuse researchers with the thinking that a certain method is not accurate. So to get any conclusive results, we always need a very well-crafted, manually engineered prompt. This led us to investigating the best prompt that should be used for each LLM Agent. We discuss this in detail in the next subsection."}, {"title": "4.4 Ablations", "content": ""}, {"title": "Prompt Ablations", "content": "To identify the most effective prompt for a task, we designed prompts aimed at achieving the highest correlation. We employed the same three models used in our main experiments (GPT-3.5 Turbo, Gemini-Pro, Mixtral-8x7B) and conducted ablation studies on the SummEval dataset by varying the prompts. We utilized two different prompt schemas for this experiment: Analyze-Rate and Eval Guidelines.  p of all three peers, we see that the Eval Guidelines prompt works better for GPT-3.5 and Mixtral, while the Analyze-Rate prompt is better for Gemini. This leads to the important observation that the best-performing prompt for one model may not be the best for another.\nThe results in Table 3 might suggest that using the Eval Guidelines prompt for both the peers and the Area Chair would yield the best results. However, to verify this, we conducted further experiments by permuting the Analyze-Rate and Eval Guidelines prompts for peers and the Area Chair.\nTable 4 displays the outcomes when the prompts were permuted between the peers and the Area Chair. We found that the optimal combination was not using Analyze-Rate for both the peers and the Area Chair, but rather using Analyze-Rate for the peers and Eval Guidelines for the Area Chair on the SummEval dataset. Although the difference in average correlation across all four metrics is only 0.005, a closer examination of the metric-wise differences reveals that Row 2 outperforms Row 1 in all metrics except for the Relevance metric. This indicates that refining the Eval Guidelines prompt for the Relevance metric could yield even better results. As previously mentioned, identifying the optimal prompt is always challenging. Therefore, we did not further explore improvements to the Eval Guidelines prompt, as this is not the primary focus of our paper.\nOur current conclusion is that prompt modification can enhance scores, and we leave the task of finding the best method for determining the optimal prompt to future research. Another important observation is that just because a prompt works best for a model does not guarantee that using the same prompt in a framework setting would yield better results. This suggests that users might need to perform experiments on their downstream applications to determine the best working prompt for their use case."}, {"title": "Performance Ablations", "content": "To understand the overall percentage gains from different parts of the model, we conducted a performance ablation experiment where we added each component of the framework incrementally and observed the improvement in overall performance.  We first start with the base model, i.e., Single Peer (GPT-3.5 Turbo with n = 1). We check the average Spearman correlation across the four metrics of SummEval (coherence, consistency, fluency, relevance) for all the models listed and then calculate their relative percentage gain with respect to the base model (Row 1). For the second row, we increased the hyperparameter n = 20 and observed a gain of +21.29%, highlighting the importance of this hyperparameter. We suspect, this is the reason for the performance behind G-Eval and Analyze-Rate. Then we use all three peers individually and take the average of the peers, resulting in a decline in performance (1.6%) compared to the base model.\nNext, we add the Area Chair into the framework with (n = 1, Analyze-Rate prompt for both Peers and Area Chair), and we see a percentage gain of +18.05% relative to the base model. This shows the importance of the Area Chair in helping the model reconcile all the evaluations and provide a better overall evaluation. We then further conduct ablation by checking with n = 20 for the Area Chair, which improves the gain to +44.2% relative to the base model. From Table 4, we see that Row 2 performs best, and when used in our ablation, it helps us decide what prompts should be finally used in the current framework to achieve maximum gain. This ablation is represented in the last row of , which shows the overall percentage gain our framework brings."}, {"title": "5 Instruction-Tuning of Small LLMs using Area Chair Outputs", "content": "Utilizing Analysis from larger LLMs (\u201cArea Chairs\"), we enhance smaller LLMs through instruction-tuning, using a dataset crafted from comprehensive evaluations. We use the analysis feedback generated within the ReFeR framework, transforming it into a useful resource for instructional tuning. This fine-tuning significantly improves smaller models performance, enabling them to reach or surpass their larger counterparts in evaluation tasks. We use Mistral-7B, since its cost of operation is much cheaper than GPT-3.5 Turbo. We used the instruction-tuning dataset (final output of Area Chair) of ReFeR framework as the training data by separating same 200 test samples we used for the experiments on SummEval dataset, i.e training data is 1350 samples, 50 samples for development and same 200 samples for testing. For TopicalChat dataset we used 45 samples for test data and remaining as training and development data.\nThis method, by integrating detailed feedback into training, enables smaller models to perform detailed, accurate assessments, demonstrating the efficacy and potential of feedback-driven instructional tuning in NLG evaluation. A case study example is given in Appendix F.\nThe comparative analysis presented  establish the enhanced performance of the fine-tuned model. Notably, the fine-tuned Mistral-7B model achieves an average Spearman correlation of 0.293 and 0.401 for SummEval and TopicalChat respectively, surpassing the baseline established by its non-fine-tuned counterpart, which stands at 0.223 and 0.128. These results not only underscore the effectiveness of fine-tuning in augmenting model performance but also illustrate the competitive edge that fine-tuned, smaller models gain against the considerably larger and more resource-intensive GPT 3.5 Turbo model. Mistral-7B-Finetuned is close with GPT-3.5 despite GPT-3.5's 25-fold larger size and associated operational costs. This finding demonstrates the computational efficiency and cost-effectiveness of employing smaller models enhanced through feedback-driven fine-tuning in lieu of relying on larger models like GPT-4 for equivalent levels of evaluative accuracy.\""}, {"title": "6 Collective Reasoning through ReFeR", "content": "An important inquiry related to the deployment of our proposed framework centers on its capacity to amplify collective reasoning beyond the scope of individual cognitive capabilities. This question is especially important in the context of complex problem-solving and evaluative tasks. To systematically explore this aspect, we aligned our investigation with the methodologies outlined by Chen et al. (2024), focusing our analysis on the reasoning performance of the Graduate School Mathematical (GSM8K) dataset (Cobbe et al., 2021) testing math reasoning, StrategyQA (Geva et al., 2021) testing deductive reasoning and CSQA (Talmor et al., 2019) testing commonsense reasoning.\nIn alignment with our evaluation schema, we crafted prompts that mirrored the structure and objectives of the framework's intended application. This initial step involved the generation of answers and corresponding reasoning by each peer agent, which were subsequently consolidated and presented to an area chair. The ensuing step entailed the formulation of a finalized response to the presented reasoning challenge. Sample reasoning prompts used with the framework is mentioned in the Appendix I.\nThe outcomes of this experimental inquiry are systematically documented in . Results of ReConCile are sourced from (Chen et al., 2024). They have used GPT-3.5 Turbo, Claude-2 and Bard. A direct comparison between our framework and ReConCile is not feasible due to the utilization of distinct peer groups and different test samples in each framework. Through this empirical assessment, we sought to quantify the extent to which the collective intelligence harnessed through our framework surpasses the reasoning limitations of individual agents. Some examples of how our framework improves the collective reasoning of each peer is given in Appendix G.\nWe assess the collective reasoning proficiency of the framework against that of individual peer models and the current state-of-the-art reasoning framework, ReConCile (Chen et al., 2024). Despite not being primarily designed to augment reasoning capabilities, our proposed ReFeR framework astonishingly matches and, surpasses the performance of ReConCile in StrategyQA and CSQA datasets, achieving a collective reasoning score of 87.0 compared to ReConCile's average of 85.3 on GSM8K dataset, 81.0 in comparison to 75.6 on StrategyQA and 80 in comparison to 74.7 on CSQA. The proximity to GPT-4's score of 95.0 on GSM8K and surpassing GPT-4 on StrategyQA and CSQA not only highlights ReFeR's competitive reasoning ability but also illustrates its cost-effectiveness relative to the significantly higher computational and financial demands of operating GPT-4 (OpenAI, 2024)."}, {"title": "7 Related Work", "content": "Evaluation using LLMs. Fu et al. (2023) proposes GPTScore, a framework that evaluates texts with generative pre-training models like GPT-3, assuming that a generative pre-training model will assign a higher probability of high-quality generated text following a given instruction and context. Wang et al. (2023) conducted a preliminary survey of using ChatGPT as an NLG evaluator. Kocmi and Federmann (2023) proposed to use GPT models for evaluating machine translation tasks. Hada et al. (2023) investigate whether LLM-based evaluators can help scale up multilingual evaluation. Liu et al. (2023b) introduced G-Eval, a novel framework using large language models through a chain-of-thoughts (CoT) approach combined with a form-filling methodology to evaluate natural language generation outputs. Chiang and Lee (2023) subsequently demonstrated that G-Eval's implementation of an automated CoT does not consistently align with human evaluations. Furthermore, they highlighted the limitations of restricting LLMs to solely numeric evaluations within G-Eval, prompting our investigation into generating evaluative outputs that include both critical commentary and numerical ratings. Jiang et al. (2023) introduced TIGERScore, an innovative metric designed to offer detailed error analysis (in contrast to the scores) for identifying specific inaccuracies within generated texts, moving beyond mere scoring. This metric is underpinned by the use of Llama-2, which was fine-tuned on a proprietary dataset. Shu et al. (2024) introduced FusionEval, a novel evaluation framework that leverages auxiliary evaluators such as NLI, BLEURT, and SBLEURT to analyze questions for assessment. This analysis is then conveyed to a primary large language model, which assigns the final score. Liu et al. (2023a) developed X-Eval, a two-stage instruction tuning framework designed to evaluate texts across both familiar and novel dimensions, tailored to the specific needs of end-users. Chan et al. (2023) proposed ChatEval, a multi-agent referee system that employs a unique method of autonomous debating among the same agents with different personas to evaluate the quality of generated responses. While their approach shares similarities with our work, it primarily relies on a debate and discussion methodology utilizing the same models under varied personas. In contrast, our method employs diverse models acting as peers and area chairs and incorporates a significantly richer evaluation schema.\nReasoning using Multiple LLMs as Peers. Chen et al. (2024) unveiled ReConcile, a multi-model, multi-agent framework structured akin to a round table conference among various LLM agents. Their findings suggest that LLMs exhibit enhanced reasoning capabilities when engaging in discussions and reaching consensus. Xu et al. (2023) introduced a novel framework aimed at augmenting reasoning abilities, drawing inspiration from the academic peer review process. This approach uniquely emphasizes iterative improvement through feedback from peer evaluations, distinguishing it from our methodology, which does not facilitate direct communication between peers but instead involves an area chair reviewing all peer responses. Pham et al. (2023) advocated for the use of embeddings as a communication medium within multi-agent frameworks to optimize reasoning. Conversely, Du et al. (2023) focuses on using solutions from other peers to enhance an individual's reasoning, employing a repetitive improvement cycle. Lastly, Wang et al. (2024) proposes a strategy for selecting the most coherent response from multiple reasoning chains, offering a different perspective on achieving consensus and enhancing reasoning accuracy."}, {"title": "8 Conclusion", "content": "We introduce an NLG evaluation framework, ReFeR, inspired by the open-review academic peer review process which also improves collective reasoning ability. Our framework comprises three distinct stages of evaluation. Initially, a peer review body of three peers independently assesses the NLG text, this is followed by an optional critic phase, where a single model reassesses the initial reviews. Then the final evaluation by area chairs who utilize the three peer reviews to deliver a conclusive score and detailed analysis for enhancement. This analysis serves as a foundation for creating an instruction-tuning dataset, useful in elevating the performance of smaller LLMs to match that of larger LLM evaluators. Our empirical analysis demonstrates that our framework has a notable 20% average improvement in Spearman correlation across the SummEval dataset and 3% average improvement across the TopicalChat dataset compared to the best baseline respectively.\nUtilizing the derived instruction tuning dataset to fine-tune Mistral-7B led to a remarkable 31% and 213% increase in average Spearman correlation on the SummEval dataset and TopicalChat datasets respectively, indicating a significant leap over Non-finetuned performance. Our framework also introduces an innovative evaluation schema, incorporating two modules: Evaluation Guidelines, providing clear scoring directives to LLMs, and an Explanation section for articulating the rationale behind assigned scores. We do a detailed ablation experiments finding multiple observations and performance gains brought by different parts of the framework. Despite not being primarily designed to augment reasoning capabilities, our framework surpasses the performance of the ReConCile reasoning framework with an accuracy of 87% on the GSM8K, 80% on CSQA and 81% on the StrategyQA reasoning benchmarks. The Framework's collective reasoning also outperformed models like GPT-3.5 Turbo by ~11.67% and GPT-4 by ~1% on an average."}, {"title": "9 Limitations", "content": "This work explores the ReFeR framework with only three peer agents, one critic, and 1 Area Chair. Due to computation and time limitations, we did not extend the work to explore the effects of having more peers and area chairs. Also, we explore only a limited number of LLMs, such as GPT-3.5 Turbo, Gemnini-Pro, and Mixtral-8x7B. Future works can explore other LLMs and some source LLMs like Llama-3. We also limited our experiments to two English datasets; it will be interesting to see other multi-lingual works. Regarding reasoning benchmark experiments, it would be interesting if future works compare our ReFeR work with other SOTA methods apart from ReConCile. One of the most important problems with this type of evaluation using LLM agents is prompt sensitivity. Having a bad prompt will reduce the performance drastically, and it is difficult to find an optimized prompt for any new task. So we did not explore on optimizing all our prompts using state of the art techniques, we leave the work of optimizing prompts for the future. Another vital direction we did not focus on is the method of communication between peers and Area Chairs, i.e., how can we effectively communicate with other LLM Agents."}, {"title": "10 Ethics Statement", "content": "This work adheres to ethical guidelines for research and publishing. All data utilized has been sourced responsibly, with proper acknowledgment of existing works and adherence to the fair use policy. No part of this research involves deception, coercion, or any form of unethical treatment of subjects. We do not foresee any ethical problems using our system except where user tries to manipulate the automatic evaluation to evaluate responses in his favor."}, {"title": "11 Potential Risks", "content": "There is a potential risk of misuse where it could be employed to generate misleading or manipulative evaluations. Ensuring ethical usage of the framework is important to prevent scenarios where the generated feedback and evaluations are used to deceive or unfairly influence decisions. As LLM models keep updating it poses a risk in maintaining consistency in evaluations over time. One of the primary risks is the inherent bias in LLMs used as evaluators."}, {"title": "D Critic Communication Strategies and Discussion", "content": ""}, {"title": "D.1 Our Proposed Communication Strategies", "content": "The Critic module serves as a crucial second step in our ReFeR framework. This module operates by submitting the initial evaluations\u2014comprising both scores and commentary generated by peer agents\u2014to a distinct critic LLM agent. This agent then undertakes the critical decision-making process regarding the necessity of re-evaluating the task at hand. Significantly, the critic LLM's feedback is designed to enable peer agents to refine and enhance their evaluations. And in some strategies, the critic LLM is used to give feedback on the peer reviews to the Areachair. Consequently, this process gives rise to a fundamental question: How can effective communication between peer LLM agents and the critic LLM agent be established?\nHence, we propose multiple communication strategies as follows:\n(1) Individual Peer Evaluation: In this strategy, the peer evaluations are given to the critic module separately. The critic agent is tasked with assessing these responses individually, determining the need for re-evaluation, and suggesting enhancements to bolster the evaluation process. Using this feedback from the critic, the peer LLMs do a re-evaluation of the sample.\n(2) Collective Peer Evaluation: This strategy explores the efficacy of submitting all peer evaluations to the critic simultaneously. The goal was to ascertain how such an aggregate submission impacts the critique process. Feedback from the critic was structured in a JSON format, ensuring clear delineation of comments for each peer and facilitating targeted improvements. This strategy significantly reduces the number of calls made to the critic. This strategy is also a re-evaluation strategy similar to the previous strategy.\n(3) Weighted Feedback: Here, the critic is requested to assign weights to each peer's evaluation. These weighted assessments are intended for subsequent review by an area chair, offering a nuanced perspective on the relative merits of each evaluation. In this strategy, the weights are directly relayed to the Area Chair as feedback along with the peer evaluations.\n(4) Critic Feedback to AreaChair: This strategy also involves the direct transmission of the critic's commentary on the peer evaluations to the area chairs with no re-evaluation. This approach prioritizes the provision of expert insight at the decision-making apex of the evaluative hierarchy."}, {"title": "D.2 Discussion about the Communication Strategies", "content": " compares different models based on coherence, consistency, fluency, and relevance on SummEval. We make the following observations: (1) ReFeR with No Critic provides the best \u03c1 and \u03c4. (2) In terms of \u03c1 and \u03c4, collective peer evaluation strategy is the second best. Individual peer evaluation is worse than collective strategy, even though it incurs thrice the cost of its collective counterpart and the critic gives feedback to the peers individually. (3) Strategies like the Critic Comment Feedback Strategy(where the critic gives comments/feedback on the peer evaluations to Area Chair) involve giving the model too much information through prompt which reduce it's performance significantly.\nTable 10 compares different models based on various aspects of conversation quality like coherence, engagingness, groundedness, and naturalness, and average scores on TopicalChat. We make the following observations: (1) ReFeR with No Critic provides the best \u03c1 and \u03c4. (2) In terms of \u03c1 and \u03c4, ReFeR with No critic performs well for engagingness and groundedness, but ReFeR with Weighted Feedback strategy is the best for coherence and Individual Peer Eval is best for Naturalness. (3) ReFeR with Direct Feedback is the least performing ReFeR model since it obtains the lowest \u03c1, \u03c4 values among the ReFeR models."}, {"title": "F Case Study for Instruction Tuning Experiments", "content": "Article: Paul Merson has restarted his row with Andros Townsend after the Tottenham midfielder was brought on with only seven minutes remaining in his team's 0-0 draw with Burnley on Sunday...\nSummary: Paul merson was brought on with only seven minutes remaining in his team 's 0-0 draw with burnley. Andros townsend scored the tottenham midfielder in the 89th minute . Paul merson had another dig at andros townsend after his appearance. The midfielder had been brought on to the england squad last week . Click here for all the latest arsenal news news .,\nHuman Annotated Scores:\n\u2022 Coherence: 1.33\nMistral-7B's Evaluation (Non Finetuned):\nAnalysis: The given coherence example is about a summary of a news article. The summary is meant to be a concise summary that covers the main points of the news article. If each sentence of the summary is relevant to the main topic and key points mentioned in the news article, then the summary can have a high coherence rating. Rating: 4\nMistral-7B's Evaluation (Finetuned):\nAnalysis: The summary seems to lack coherence as it is disjointed and does not link one idea into the next effectively. The central idea about Merson criticizing Townsend and Townsend scoring for England get mixed up in the middle, lacking a logical flow of information. Furthermore, the sentence The midfielder had been brought on to the England squad last weekseems out of place and does not correlate well with the rest of the content, hence, compromising overall coherence. Rating: 1.4"}, {"title": "G Example of Reasoning improved by Framework", "content": "Example from GSM8k:\nQUESTION: Solo has to read 4 pages from his Science textbook", "ANSWER": 6, "RESPONSE": "To find the total number of pages Solo needs to read", "textbook": 4}, {"RESPONSE": "Total pages to read: 4 (Science) + 20 (Social Studies) + 7 (History) + 8 (Geography) = 40 pages. After subtracting 15 pages already read"}, {"RESPONSE": "Total pages to read: 4 (Science) + 20 (Social Studies) + 7 (History) + 8 (Geography) = 39 pages. Solo needs to read, on average, 9.75 pages per"}]}