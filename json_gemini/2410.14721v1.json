{"title": "The Representation of Meaningful Precision, and Accuracy", "authors": ["A Mani"], "abstract": "The concepts of precision, and accuracy are domain and problem dependent. The simplified numeric hard and soft measures used in the fields of statistical learning, many types of machine learning, and binary or multiclass classification problems are known to be of limited use for understanding the meaningfulness of models or their relevance. Arguably, they are neither of patterns nor proofs. Further, there are no good measures or representations for analogous concepts in the cognition domain. In this research, the key issues are reflected upon, and a compositional knowledge representation approach in a minimalist general rough framework is proposed for the problem contexts. The latter is general enough to cover most application contexts, and may be applicable in the light of improved computational tools available.", "sections": [{"title": "Introduction", "content": "In the context of measurement theory, accuracy and precision are numeric measures of observational error. The former indicates the closeness of the measurements to the ground truth or accepted value, while latter is about the similarity of repeated measurements. A measurement system is valid if it is both accurate and precise.\nA data set of repeated measurements of the same quantity, is typically said to be accurate if their measure of central tendency is close to the ground value of the quantity being measured, while the set is said to be precise if their measure of variability is relatively small. These concepts may be read as generalizations of the corresponding concepts in numerical analysis, where accuracy is the closeness of the recursive approximation to the true value; while precision is again about closeness expressed in terms of significant digits. Precision does not guarantee accuracy in these cases. In statistics, bias is the amount of inaccuracy and variability is the amount of imprecision. These are adapted to machine learning and AI studies with limited or poor justification, and the problem of building alternative frameworks is known to be a challenging one.\nWhile every concept of precision has associated structural properties, only some are justifiably quantifiable in terms of the real numbers. For example, an ancient Egyptian limestone sculpture may be said to be precisely sculpted with great skill using simple tools. The idea of precision here, is embodied by the relation of sculpted shapes of parts to some possible shapes of the parts of the real person or abstract entity depicted. The term precisely sculpted conveys a lot more to the intended viewer such as a cue to look for certain types of possible features to compare with, and confirm that the representation is an aggregate of similarly close approximations of some of them. Note that the mature viewer would have several skills of art appreciation, and a good sculpture would be addressing several viewership skills.\nIn earlier work, solutions to problems of AI and ML through general rough sets are proposed by the present author [13,18,15,16] and others [2]. It is of natural interest to explore the mentioned problem of building reasonable theoretical frameworks with minimal intrusion and contamination [10,11] through general rough sets. In this research, a general rough set based framework for the concepts of precision, and accuracy is proposed. It has the advantage of tracing the meaning of the concepts from a granular compositional perspective, and additionally decide on the validity or meaningfulness of possible numeric measures."}, {"title": "Background", "content": "Some essential concepts are mentioned here for the reader's convenience. For relevant basics of general rough sets, the reader is referred to [17,12].\nAn information table I, is a tuple of the form\n$I = (O, A, \\{Va: a \\in A\\}, \\{fa: a \\in A\\})$\nwith O, A and Va being respectively sets of Objects, Attributes and Values respectively. $fa : O \\rightarrow p(Va)$ being the valuation map associated with attribute $a \\in A$. Values may optionally be denoted by a binary function $v: A \\times O \\rightarrow p(V)$ (V being a set of subsets of values) defined by for any $a \\in A$ and $x \\in O$, $v(a, x) = fa(x)$.\nRelations may be derived from information tables by way of conditions of the following form: For $x, w \\in O$ and $B\\subseteq \\Alpha$, $(x, w) \\in \\sigma$ if and only if $(Qa, b \\in B) \\Phi(\\nu(\\alpha, x), v(b, w),)$ for some quantifier Q and formula $\\Phi$.\nThe relational system $S = (S, \\sigma)$ (with $S = O$) is said to be a general approximation space (S and S will be used interchangeably). In particular if $\\sigma$ is an equivalence relation then S is referred to as an approximation space.\nIn fact, every subset K of attributes determines an equivalence IND(K) defined by IND(K)ab if and only if for every z in K $v(a, z) = v(b, z)$.\nIt should be noted that objects are assumed to be defined (to the extent possible) by attributes and associated valuations.\nIn classical rough sets, on the power set p(S), lower and upper approximations of a subset $A \\in p(S)$ operators, apart from the usual Boolean operations, are defined as per: $A^{l} = U_{[x]\\subseteq a} [x]$, $A^{u} = U_{[x]\\cap A\\neq\\emptyset} [x]$, with [x] being the equivalence class generated by $x \\in S$."}, {"title": "Negations and Implications", "content": "Some essential generalized implications and negations (for more details, see [14,1]) on a bounded quasi ordered set L with top element $T$ and bottom $|$ are mentioned here. Consider the conditions possibly satisfied by a map $n : L\\rightarrow L$:\n$n(1) = T \\& n(T) = 1                                                                             (N1)$\n$(\\forall a, b) (a \\leq b \\rightarrow n(b) \\leq n(a))                                                        (N2)$\n$(\\forall a)n(n(a)) = a                                                                           (N3)$\n$n(a) \\in \\{|, T\\} if and only if a = | or a = T                                              (N4)$\nn is a negation if and only if it satisfies N1 and N2, while n is a strong negation if and only if it satisfies all the four conditions.\nImplications satisfy a wide array of properties as they depend on the other permitted operations. Here some relevant ones are mentioned.\nA function $: L^2 \\leftarrow L$ is an implication if it satisfies (for any a, b, c $\\in$ L) the following:\nIf $a \\leq b$ then $bc < Jac                                                                           (First Place Antitonicity FPA)$\nIf $b \\leq c$ then $Jab < Jac                                                                           (Second Place Monotonicity SPM)$\n$T = T                                                                                            (Boundary Condition 1: BC1)$\n$TT = T                                                                                           (Boundary Condition 2: BC2)$\n$| = |                                                                                            (Boundary Condition 3: BC3)"}, {"title": "Precision and Accuracy", "content": "Extension of ROC curves to multi-class contexts from a statistical learning perspective has received some attention in recent years \u2013 though the goal of intelligible representation is not achieved. Precision-recall plots are claimed to be superior to ROC plots (receiver operating characteristic curves) in [23]. Other evaluation studies are [20,3,22].\nHard classification measures proceed on the assumption that ground truth is available for verification, and are based on the number of observations that are classified correctly or incorrectly:\n\u2022 True Positive(TP): classification result is positive and the ground truth is positive.\n\u2022 False Positive(FP): classification result is positive but the ground truth is negative.\n\u2022 True Negative(TN): classification result is negative and the ground truth is negative.\n\u2022 False Negative (FN): classification result is negative and the ground truth is positive.\n\u2022 Total Positive (ToP): classification result is positive.\n\u2022 Total Negative (ToN): classification result is negative.\nDenoting the corresponding cardinalities in bold, the true positive (TPR), true negative (TNR), false positive (FPR) and false negative (FNR) rates are defined by\n$TPR = \\frac{TP}{ToP},                                         FPR = \\frac{FP}{ToN}$\n$TNR = \\frac{TN}{ToN},                                         FNR = \\frac{FN}{ToP}$\nThe measures TPR (precision or positive prediction value ) and TNR (specificity) provide some information in balanced class problems. For unbalanced class problems, these would favor the majority class, and so weighting schemes are used to combine the four cardinalities. For example, Cohen's Kappa and Matthews correlation coefficient are defined respectively as the geometric and harmonic mean of the quantities\n$\\frac{(TP . TN - FP . FN)}{ToP . (FP + TN)}$ and $\\frac{(TP . TN - FP . FN)}{ToN . (TP + FN)}$\nThe former is a chance corrected measure of accuracy (when accuracy is measured by $\\frac{TP+TN}{ALL}$).\nOne measure of precision is $\\frac{TP}{TP+FP}$, and that of recall or sensitivity is $\\frac{TP}{TP+FN}$.\nTheir harmonic mean is the F-measure.\nIf the class imbalance ratio $r = \\frac{ToN}{ToP}$, then $\\frac{TP}{TP+FP} = \\frac{TPR}{TPR+FPR}$. Thus this measure of precision is affected by class imbalance."}, {"title": "Soft Numeric Measures", "content": "The input for ROC curves may possibly be read as probabilities or subjective probabilities, and thus the curves provide for soft classifications. In statistical learning contexts, the AUC score (value of the area under the curve integral) may be read as the probability of the event that a positive labeled observation will receive a higher probability ranking as compared to negative labeled observations. The popularity of ROCs is due to its visual appeal.\nWhile ROC and its AUC avoid the class imbalance aspect by relying on TPR and FPR, they presume a uniform misclassification cost. Remedial measures are proposed in [20]. The AUC computation can be deceptive for the estimation of model performance because it adds the area under curve with low sensitivity and low recall values. ROC and its AUC do not provide information about precision and recall. High AUC values like 0.9 combined with low values of precision and recall like 0.3 and 0.1 respectively, can actually occur in practice. Precision-recall plots are more informative than ROC plots when evaluating binary classifiers on imbalanced data. In such scenarios, ROC plots may be visually deceptive with respect to conclusions about the reliability of classification performance\nIn information retrieval contexts, precision is the fraction of relevant instances among the retrieved instances, while recall (or sensitivity) is the fraction of relevant instances that were retrieved. Balancing these is often relevant in different contexts for loose decision-making.\nIn summary, in the best case scenario, statistical and machine learning perspectives of the measures may be good enough for indicating the unsuitability of the proposed model(s) for the problem. Stochastic justifications are available for a small number of cases, and then inference is hindered."}, {"title": "Minimalist Rough Framework", "content": "Implementation of AIML algorithms (or computational models) correspond to abstract approximations (or partial approximations). Ideas of precision, accuracy, and related measures are relative to inter-relations between these. So general rough sets are naturally applicable. The exact number of approximation operators that need to be considered in an application context needs to be specified beforehand, and these can be many in number. For simplicity, only three pairs are used in this paper. An alternative would be to rely on large-minded reasoners [18,16].\nThe framework used in this paper is necessitated by the need to have multiple 1-place approximation and sufficient order predicates to express the reasoning about approximation operators. Additionally, 2-place approximation operators, rational, and mereological predicates are relevant; these will be used in extended versions of the framework. In [14], a minimalist model for rough sets called a rough convenience lattice (RCL) is introduced, and it is shown that it is equivalent to RCL aggregation negation algebras, and RCL aggregation implication algebras in a perspective. However, a minimalist model such as those of a rough convenience lattice (RCL) [14] or its well-justified abstract generalizations CRCLANA or"}, {"title": "A partial algebraic system of the form", "content": "of type (1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0) with $(B, \\leq, \\perp, T)$ being a bounded quasi-ordered set will be said to be a precision rough convenience quasi-order (PRCQO) if the following conditions are additionally satisfied ($\\lor$ and $\\land$ being partial weak lattice operations, and the operations $l \\in \\{l_{1}, l_{2}, l_{5}\\}$ and $u \\in \\{U_{1}, U_{2}, u_{s}\\}$ are generalized lower and upper approximation operators respectively):\n$\\forall a, b) a \\leq b = b \\lor a / b = a \\rightarrow a \\leq b                                                   (wl12)$\n$(\\forall a, b, c) (a \\leq b = c \\rightarrow a \\land b = c \\lor c/b = a \\rightarrow a \\leq c)                                                  (wl34)$\n$(\\forall x) x = x^{ll} \\leq x \\leq x^{u} \\leq x^{uu}                                                                (qlul)$\n$(\\forall a, b) (a \\leq b \\rightarrow a^{l} \\leq b^{l}). (a,b)(a \\leq b \\rightarrow a^{u} \\leq b^{u})                                                    (qlu-mo)$\n$a^{l} \\land b^{l} = (a \\land b)^{l} = (a \\lor b)^{l} \\land = (a^{l}) \\lor (b^{l})                                           (qlu23)$\n$T = T \\& | = || = ||                                                                                      (topbot)$\nIf additionally, the quasi order is a lattice order (resp. partial order), then the system will be referred to as a precision rough convenience lattice PRCL (resp. precision rough convenience partial order, PRCPO).\nIn a PRCL B, let for any a, b $\\in$ B\n$a*b: a^{ls} = b^{ls}$ and $a*b = a^{us} = b^{us}$\nThese are interpreted as skeptical aggregation, and co-aggregation, and studied in detail in [14] by the present author."}, {"title": "Two generalized negations are definable on a RCL, B as follows", "content": "$a = inf\\{z: z\\in B \\&az = T\\}                                                              (addneg)$\n$~ a = sup\\{z: z\\in B \\& a\\cdotz = |\\}                                                              (mulneg)$\nThese satisfy the properties specified in the next two theorems."}, {"title": "Theorem 1", "content": "$\\forall a) a < a^{us}                                                                                           (WN3-N)$\n$\\forall a, b) (a \\leq b \\rightarrow -b \\leq (-b)^{us} \\leq (-a)^{us})                                       (WN2-N)$\n$ << T \\& T = |                                                                                           (WN1-N)"}, {"title": "Theorem 2", "content": "$\\forall a, b) (a \\leq b \\rightarrow (\\sim b)^{ls} < (\\sim a)^{ls} < \\sim a)                                       (WN2-S)$\n$ < \\sim T \\& T = \\sim |                                                                                           (WN3-S)$\nThe above means that ~ is a weak negation. In general, a negation n can be used to define binary operations $\\vartheta_{n}$ by conditiions like a $\\oplus$ b := $a^{ls} \\backslash$ $(n(b))^{ls}$. In what follows, n will be taken as ~, and it is provable that"}, {"title": "Proposition 1. For any a, b, c \\in B", "content": "$\\alpha \\vartheta a = |                                                                                              (omi1)$\n$a \\leq b \\qquad a \\vartheta b = |                                                                                              (omi2)$\n$a < b \\rightarrow a c \\leq b c                                                                                              (omi3)"}, {"title": "An Abstract PRCQO Aggregation Implication Algebra", "content": "B = (B, $\\leq$, *, $`,$ , $\\ominus$, $\\oplus$, V, $\\Lambda$, $l_1$, $U_1$, $l_2$, $U_2$, $l_5$, $U_s$, $\\supset$ , $\u00ac \\supset$ )\nthat satisfies for l $\\in$ $\\{l_{1}, l_{2}, l_{s}\\}$ and u $\\in$ $\\{U_{1}, U_{2}, U_{s}\\}$:\n$(B, \\lor, \\land, l_1, U_1, l_2, U_2, l_s, U_s, | ,T)$ is a PRCQO. (rcl)\n$(a^{uu} = a^{u} \\& b^{uu} = b^{u} \\& c^{uu} = c^{u} \\alpha (b \\ominus c) = (a b) \\ominus c)$                                         (wAasso1)\na $\\ominus$ ((b ominus e) $\\ominus$ a) ((a \\ominus b) $\\ominus$ c) $\\ominus$ c                                         (wAsso2)\nsatisfies FPA, SPM, BC3, and IBL                                         (imsc)\nsatisfies FPA, IP, SPM, BC1, BC2, and BC3                                         (inegc)\nwith $\\ominus$ being a commutative, monoidal, order-compatible operation with identity $|$ , and $\\oplus$ additionally being a commutative, order-compatible operation with identity T. If additionally, the quasi order is a lattice order (resp. partial order), then the system will be referred to as a PRCLAI (resp. PRCPOAI).\nA concept of difference between an approximation and a standard ap- proximation can naturally be expected to access its quality relative to precision, recall, and accuracy all understood from a general rough ontological perspective (without involving any statistical assumptions). Related theory of posets with difference are studied in several papers such as [5] (a different adaptation to rough sets is in [9]). Some possibilities for a difference operation \u2013 $\\exists$ (corresponding to $\\leq$), derived difference operation $\\nabla$ that satisfies Prop 1, a sum $\\oplus$ (a commutative monoidal binary operation with identity $|$ ), an object x, and three approximation operators a, b, and c in a PRCLAI are\n$\\Pi(x, a, b, c) = (x - x^{e})$ if defined                                                                                              (amalg)\n$\\nabla(x, a, b, c) = (x^{e} x_{b}) (x^{b} x^{e})$                                                                                              (nabla)\n$\\, l_1, l_5, U_1, U_s) = \\nabla(x, l_1, l_s, u_1) \\nabla (x, l_5, l_1, u_1)$                                                                                              (finv)"}, {"title": "Theorem 3", "content": "The following properties hold in the above context:\n$\\forall (x, a, b, c) = \\nabla(x, b, a, c)                                                                                              (na1)$\n$\\forall (x, a, a, c) = \\nabla(x, b, b, c)                                                                                              (na2)$\nIf for all x, $x^{l_{1}} < xl2$ then $\\nabla(x, l_{1}, l_{2}, u_s) = (xl2 \\oplus xl1)^{us} < x^{us}$                                                                                              (na3+)\n$\\nabla(1, a, b, c) = | = \\nabla(T, a, b, c)                                                                                              (na4+)\n$\\exists(x, l_{1}, l_{s}, U_1, U_s) = \\nabla(x, l_{s}, l_{1}, U_s, U_1)$                                                                                              (na5)"}, {"title": "Extended Classical Rough Sets", "content": "Every subset K of the set of attributes At of an information table I, determines a pair $(l_k, u_k)$ of lower and upper approximations. It is necessary to compare such approximations to different extents to determine attribute reducts (subsets of attributes that provide the same quality of classification) or variants thereof. The defined set-valued measures provide a new and nonequivalent way of determining this.\nCorrectly speaking, a specification of preferences over sets of subsets of attributes amounts to extending classical rough sets. It is not about partial or total orders on the set of attributes (as in POAS [9] or dominance based rough sets). Suppose that K is a preliminary reduct or an apparent reduct (based on possible explanations of the decision process). The associated approximations can be taken as the standard approximations $l_s$ and $u_s$. If $(l_1, u_1)$, and $(l_2, u_2)$, are two other pairs of approximations, then any object x is related to the following measures:\n$\\nabla(x, l_{1}, l_{s}, u_s) = (xl1 \\backslash xls)^{us} \\bigcup (xls xl1)^{us}$                                                                                                                                 (lsymag1sc)\n$\\nabla(x, l_{1}, l_{2}, u_s) = (xl1 \\backslash xl2)^{us} \\bigcup (xl2 xl1)^{us}$                                                                                                                                 (lsymag12sc)\n$\\nabla(x, u_{1}, u_{s}, l_s) = (xu1 \\backslash xus)^{ls} \\bigcup (xus xu1)^{ls}$                                                                                                                                 (usymic1sc)\n(?)\nThese can be interpreted as follows (for any two approximation operators a and b):\n\u2022 $\\nabla(x, a, b, u_s)$ is a representation of the greatest possible difference between $x^a$ and $x^b$ relative to $u_s$\n\u2022 $\\nabla(x, a, b, l_s)$ is a representation of the minimum definite difference between $x^a$ and $x^b$ relative to $l_s$\nNote that if $u_s$ (and therefore $l_s$) is taken as the identity operation, then $\\nabla(x, a, b, u_s)$ is the symmetric difference between $x^a$ and $x^b$.\nIn general, not all objects, granules, and their approximations are of interest, and therefore it may suffice to define $\\nabla$ for a subset H of objects alone. For a specific choice of a, b and $u_s$, dom($\\nabla_H$) = $H \\times \\{a\\} \\times \\{b\\} \\times \\{u_s\\}$. Corresponding to this, the range R($\\nabla_H$) is partially ordered by set inclusion."}, {"title": "When H is the set of all objects O, then R(", "content": "V_O) (R(V) for simplicity) is a lower bounded weak partial lattice. Its elements are definite objects. The same statements hold for R($\\nabla$).\nProof. The lower or upper approximation of an empty set is the empty set. So $\\emptyset \\in R(V)$, and it serves as the lower bound of the induced set inclusion order. The partial weak lattice operations are induced by the union and intersection operation on the power set, and are defined (for any f, g $\\in$ R(V))\n$fVg=\\begin{cases}\\hspace{0.5 cm} SfUg & h \\in R(V) | f \\bigcup g = h \\\\ undefined & otherwise\\end{cases}$\n$f\\land g=\\begin{cases}\\hspace{0.5 cm} Sf\\cap g & h \\in R(V) | f \\bigcap g = h \\\\ undefined & otherwise\\end{cases}$"}, {"title": "The interpretation of R(VH) as a measure of precision or accuracy depends on that of the standard approximations", "content": "If it is read as ground truth, then the partial algebra measures accuracy. Otherwise, it needs to read as a representation of relative accuracy. The concepts of representation of precision and accuracy in the previous section naturally carry over to the present case."}, {"title": "Theorem 5", "content": "The following properties hold in the above context:\n$\\forall (x, a, b, c) = \\nabla(x, b, a, c)                                                                                            (na1)$\n$\\forall (x, a, a, c) = \\nabla(x, b, b, c)                                                                                             (na2)$\nIf for all x, $x^{l_{1}} xl2$ then $\\Pi(x, l_{1}, l_{2}, u_s) = (xl2 \\backslash xl1)^{us} C x^{us}$                                                                                            (na3+)\n$\\nabla(1, a, b, c) = | = \\nabla(T, a, b, c)                                                                                             (na4+)$\n$\\exists(x, l_{1}, l_{s}, U_1, U_s) = \\nabla(x, l_{s}, l_{1}, U_s, U_1)$                                                                                             (na5)"}, {"title": "Real Valued Measures", "content": "Arbitrary cardinality based measures of rough inclusion, quality of classification, accuracy degree of approximation [21,7] and similar measures are known not to correspond to algebraic and logical properties in general [11]. The range of $\\nabla$ and $\\exists$ consist of a number of objects that may admit of valuations or grades (in the order-theoretic sense [4,6,19]). This leads to the natural question: how can algebraically compatible valuations correspond to reasoning?\nThey may not be coherent in all cases, and information is bound to be lost. Assuming a finite universe, the following can be proved for classical rough sets."}, {"title": "Then it is a graded poset", "content": "If R(V) is a down-set bounded above by a finite number of elements of the same rank/grade.\nProof. R(V) is a subset of the graded poset (lattice) $\\delta(O)$ of definite objects. That is, there exists a order preserving map $v : \\delta(O) \\leftarrow \\N$ such that if b covers a, then $v(b) = v(a) + 1$.\nSince, by assumption it is generated by a number of definite objects with the same rank value, it must be a graded poset.\nNote that the specification of $\\delta,$ is related to that of R(V)."}, {"title": "Discussion", "content": "In this research, a new compositional approach to the representation of precision and accuracy from a general and a classical rough set perspective is proposed. The methodology steers clear of stochastic assumptions,"}, {"title": "then the student belongs to category L", "content": "the student belongs to category L. Approximations constructed from such granules would be superior from the perspective of causality and meaning. Numeric precision based granules, on the other hand, naturally conflate the attributes.\nThe methods of the paper keep track of attributes and their subjective/non-subjective valuations. In the absence of granular compositionality, steps in the construction of approximations would be lost.\nNote that, in practice, multiple pairs of approximations are typically relevant, especially when they are specified with insufficient justification.\nExample-2:\nSuppose we have a soft or hard clustering $(C_i, F_i); i = 1, 2, ..., n$ with the cores $C_i$ being mutually disjoint, and disjoint with all frontiers $F_j$, while the frontiers may have nonempty intersection with other frontiers. The pair $(C_i, F_i)$ is read as a soft cluster for each i. Validation indices are known to be unreasonable, and tracing of the meaning of the soft/hard clustering from algorithms works only in a few cases.\nGiven general rough operators l and u, it is always possible to compute a new collection of pairs of the form $(C^l, F^u)$. The concepts of precision and accuracy considered in the paper are natural for understanding the quality of the clustering relative to the rough perspective offered. If the operators are granular, then explanations would be representable algebraically. In my earlier papers, methods of tracing the meaning are already specified\nA key step would about representing the difference between the suggested or discovered general rough clustering and the soft/hard clustering, and the relative quality of the clustering.\nThe paper contributes to the last part.\nExample-3: Most other AI/ML problems (supervised or unsupervised) can be approached from a similar perspective. The paper addresses basic questions of comparison from both granular and non-granular perspectives."}, {"title": "Definition 4", "content": "Let S be a collection of sets (that are subsets of a H), and G be the set of neighborhoods generated by a neighborhood map $n : H \\rightarrow pH$. The version of graded rough sets explored in [24] is based on the following k- approximation operators and derived quantities (for any A $\\in$ p(H) = S and positive integer k):\n$A^{k}$ = $\\{z: n(z) \\in G \\& #(n(z) \\cap A) > k\\}                                        (k-upper1)$\n$A^{k}$ = $\\{z : n(z) \\in G \\& #(n(z) \\backslash A) \\leq k\\}                            (k-lower1)$"}, {"title": "If T is a tolerance on the set H, and B the set of its blocks (maximal subsets B of H that satisfy B2 T)", "content": "If T is a tolerance on the set H, and B the set of its blocks (maximal subsets B of H that satisfy $B^2 \\subset T$), then the standard granular and bited approximations of a subset A of H are defined by [11]\n$x^{l}$ = $\\bigcup\\{A : A \\subseteq X \\& A \\in B\\}                                        (lower)$\n$X^{u}$ = $\\bigcup\\{A : A \\cap X \\neq 0 \\& A \\in B\\}                                         (upper)$\n$X^{\\ddot{u}b}$ = $\\bigcup\\{A : A \\cap X \\neq 0 \\& A \\in B\\} \\backslash (X)^{l}$                (bited-upper)"}, {"title": "Let H = {X1, X2, X3, X4} and T be a tolerance T on it generated by", "content": "Let H = $\\{X_{1}, X_{2}, X_{3}, X_{4}\\}$ and T be a tolerance T on it generated by\n$\\{(X_{1}, X_{2}), (X_{2}, X_{3})\\}$\nDenoting the statement that the granule generated by $x_{1}$ is $(x_{1}, x_{2})$ by $(X_{1}: x_{2})$, let the granules be the set of predecessor neighborhoods:\nG = $\\{(X_{1}:X_{2}), (X_{2}: X_{1}, X_{3}), (X_{3}:X_{2}), (X_{4})\\}$\nThe different approximations (lower (l), upper (u) and bited upper ($\\ddot{u}$)) are then as in Table.1 below. For k = 1, the 1-graded approximations are in the last two columns.\nIt can be checked that $\\nabla(x, l_1, l, u_1)$ for the sixteen subsets in sequence are:\n$\\{\\O, \\O, \\O, A_{4}, \\O, \\O, A_{4}, \\O, A_{4}, A_{4}, \\O, A_{4}, A_{4}, A_{4}, \\O\\}$\nIf $S_.$ is taken as $\\{A_{4}, \\O\\}$, then it is obvious that R(V) = $S_.$. Therefore $l_1$ is an accurate approximation of $l$ relative to $S_.$, and $u_1$.\nFurther, the sequence for $\\nabla (x, l_{1}, l, u_{1})$ is\n$\\{\\O, \\O, \\O, \\O, \\O, \\O, \\O, \\O, \\O, \\O, \\O, \\O, \\O, \\O, \\O, \\O\\}$\nTherefore R(\\exists) = R(V). Thus $l$ is as precise as $l_{1}$.\nThe sequence of values for $\\nabla (x, u, u_{s}, u_{s})$ is\n$\\{A_{11}, \\O, A_{11}, \\O, \\O, \\O, A_{11}, \\O, \\O, A_{11}, \\O, \\O, \\O, \\O, \\O, \\O\\}$\nSince $A_{11}$ is most of set H, it can be said that $u$ is not an accurate approximation of $u_s$."}, {"title": "Difference Operations", "content": "Let S = (S, <) be a poset, a binary partial operation $\\oplus$ is called a difference operation (or partial difference operation) if and only if S satisfies\n\u2022 (a $\\leq$ b \\theta c $\\rightarrow$ b $\\oplus$ a = c)\n\u2022 (a $\\leq$ b $\\rightarrow$ b $\\ominus$ (b $\\oplus$ a) = a, b $\\n\u2022 (a \\leq b \\leq c $\\leftarrow$ (c $\\oplus$ b) $\\leq$ (c $\\ominus$a), (c $\\ominus$a) $\\ominus$ (c $\\ominus$ b) = b $\\ominus$ a)\n(S,, $\\oplus$) is then a poset with difference ($\\pi\\delta$) in symbols). If (S, <) is a bounded poset with 0 and 1, then it is a difference poset. If (S, $\\pi\\delta$) satisfying\n(c a, b a c = b c  a = b)\nthen it is a poset with cancellative difference. It is then possible to define a sum operation via\n(b $\\ominus$a) = c  (a + c = b).\n(S,,+) satisfies\n(a + b) (b a)\n(a + b) c  a (b + c)\n(a + b = a + c b = c)\n a a  a\n((a + a) + b a  a a  a)\nwhere `` means if either side is defined then so is the other and the two are equal. S is thus a cancellative partial abelian semigroup."}, {"title": "A key result is", "content": "Any difference poset is also a poset with cancellative difference. A poset with cancellative difference is called a generalized difference poset. An idealK in a poset with difference (S = (S,+, <) is a subset with the induced order satisfying x $\\in$ S y $\\in$ K(x < y then x $\\in$ K). Any ideal of a difference poset is a generalized difference poset. A key result is:\nProposition 3. Any generalized difference poset is realizable as the ideal of a difference poset."}, {"title": "In terms of the sum operation a difference poset can also be defined as a cancellative partial abelian semigroup with unity 1, such"}]}