{"title": "Creating Artificial Students that Never Existed: Leveraging Large Language Models and CTGANs for Synthetic Data Generation", "authors": ["Mohammad Khalil", "Farhad Vadiee", "Ronas Shakya", "Qinyi Liu"], "abstract": "In this study, we explore the growing potential of AI and deep learning technologies, particularly Generative Adversarial Networks (GANs) and Large Language Models (LLMs), for generating synthetic tabular data. Access to quality students' data is critical for advancing learning analytics, but privacy concerns and stricter data protection regulations worldwide limit their availability and usage. Synthetic data offers a promising alternative. We investigate whether synthetic data can be leveraged to create artificial students for serving learning analytics models. Using the popular GAN model- CTGAN and three LLMs- GPT2, DistilGPT2, and DialoGPT, we generate synthetic tabular student data. Our results demonstrate the strong potential of these methods to produce high-quality synthetic datasets that resemble real students' data. To validate our findings, we apply a comprehensive set of utility evaluation metrics to assess the statistical and predictive performance of the synthetic data and compare the different generator models used, specially the performance of LLMs. Our study aims to provide the learning analytics community with valuable insights into the use of synthetic data, laying the groundwork for expanding the field's methodological toolbox with new innovative approaches for learning analytics data generation.", "sections": [{"title": "1 Introduction", "content": "Student learning data is a core component of three related areas of study and practice: data-driven decision-making, educational data mining, and learning analytics [42]. Learning analytics (LA) relies on data to achieve its goals, which often involve supporting students through various interventions, such as making predictions based on machine learning models and providing personalised learning. However, the field of LA has been engaged in ongoing debates about the ethical and practical challenges of collecting student data to further develop LA tools and models [48]."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Synthetic Data in LA", "content": "Synthetic data generation has been communicated early in education through student simulation [51]. For LA, synthetic data generation has gained attention from the community for not a long time as a method for data augmentation and privacy protection, though empirical work remains scarce. One of the earliest studies on synthetic data in LA was by [5], who argued that it could play a key role in enriching student data. However, it is only in the past five years that the practical application of synthetic data in LA has begun to grow [18]. For instance, [56] conducted a comparative evaluation of different synthetic data generation models, including statistical approaches like Gaussian Copula and deep learning models such as CTGANs. Using a large dataset from three Australian courses, they assessed the performance of the used models based on regression accuracy and Root-Mean-Square Error (RMSE). The findings were promising which suggested that synthetic data generation can enhance the supervised machine learning performance of LA models.\nAnother example by [38], who generated synthetic data using a variety of synthetic data methods and evaluated the results by considering two active learning strategies on three unbalanced datasets. Moles and colleagues [38] showed that synthetic data can address the minority class prediction inaccuracies imposed by limited student data. In [22] observed that a significant challenge in LA fairness research is the scarcity of publicly available, unfair datasets due to privacy concerns. To address this limitation, they employed synthetic data generation techniques to create missing datasets from scratch. Meanwhile, [31] applied synthetic data generation methods to mitigate privacy concerns for LA datasets. In [31] results showed that publishing synthetic data can protect the privacy of real data while addressing the issue of data scarcity in the LA field. The aforementioned literature demonstrates that synthetic data provides a promising method for replicating real data in the field of LA.\nHowever, only high-quality synthetic data can, partially, replace real data. From the literature review, we found that previous studies have been relatively scattered in their evaluation of generating high-quality synthetic data for LA, often focusing on various factors such as privacy; an example of such studies is [31]. Consequently, the exploration of a key area, utility, remains insufficient. This paper aims to address this gap by providing a more comprehensive evaluation of synthetic data for this purpose. Our paper also aims to take a significant step forward in advancing the scalability of LA. By generating high-quality synthetic LA datasets that closely resemble real datasets in terms of their predictive performance, this research aims to address the data scarcity faced by LA practitioners. While previous studies in the LA field have primarily relied on statistical distribution methods and deep learning methods for synthetic data generation, recent advancements in LLMs have demonstrated their potential for generating synthetic tabular data [8]. However, to date, there has been no comprehensive evaluation of LLMs for this specific task within the LA domain."}, {"title": "2.2 Synthetic Data Generation", "content": "Synthetic data is defined as \"data that has been generated using a purpose-built mathematical model or algorithm, with the aim of solving a (set of) data science task(s)\" ([23], p. 5). The main types of synthetic data include tabular, time series, text, images, video, or audio simulation. All descriptions of synthetic data generation in this paper are tabular data.\nThere are two primary approaches to generating synthetic data: statistical distribution and deep learning methods [23]. Statistical distribution methods, such as Bayesian networks, rely on mathematical models to represent probability distribution of data. Deep learning methods, particularly GANs, use neural networks to learn the underlying patterns of the data and generate new samples. GANs have shown promising results in the field of LA. For example, in [38] experiment, the GAN-based method outperformed other methods like SMOTE, achieving nearly a 50% improvement in prediction accuracy on some datasets. Within deep learning methods, Conditional Tabular Generative Adversarial Networks (CTGANs) have emerged as a highly effective method for generating structural tabular data, particularly in scenarios where large-scale data collection is impractical or privacy concerns are paramount [53]. CTGAN provides a scalable method for generating high-quality synthetic datasets, which can significantly improve the performance of machine learning models across LA [6, 31, 56]. Therefore, in this paper, we select CTGAN as a representative GAN-based method for further experiments. It will be used as one of two approaches to generate synthetic data.\nOur second approach to generating synthetic data involves LLMs. LLMs have recently gained substantial attention in both academia and industry, with ChatGPT and Google Gemini being widely recognized examples. Additionally, LLMs have significantly enhanced the performance of various natural language processing (NLP) tasks, opening up new possibilities for automating tasks that humans traditionally did. Bubeck et al [10] predict that the impressive performance of LLMs could potentially lead to the development of Artificial General Intelligence (AGI) in the future.\nIn LA, LLMs have been used, for example, to predict challenging moments from students' discourse [49], provide real-time feedback [41] and as an intelligent tutoring systems to support students in higher education [26].\nIn general, LLMs have been extensively explored for generating synthetic images and text, often outperforming other methods [24]. However, their use for generating synthetic tabular data has been far less common. Recently, though, studies have started to investigate the potential of LLMs in tabular data generation [9, 54]. One notable success is the recently developed framework GReaT (Generation of Realistic Tabular data) by [8], whose experimental results show that LLM performance on various datasets is comparable to, if not better than, leading GAN-based methods such as CTGANS. Breugel & Schaar [9] posit that LLMs could significantly impact the generation of synthetic tabular data, potentially transfiguring how tabular data is utilised in science and machine learning. Breugel & Schaar [9] argue that LLMs can be better equipped to reason about the underlying distribution of variables and generalise to new relationships within tabular datasets. Still, exploring tabular data generation using LLMs remains limited. Breugel & Schaar [9] concluded that further exploration of LLMs and benchmarking specifically designed for synthetic tabular is needed.\nOur review of the literature reveals no comprehensive study in LA or educational data science comparing LLM-based models with GAN-based methods for synthetic data generation. This paper addresses this gap.\nIn our study to use LLMs for synthetic data generation for tabular data, we adopt Borisov et al's GReaT [8]. GReaT utilises autoregressive generative LLMs to sample synthetic and high utility and resemblance tabular data. We selected three relatively lightweight and open-source LLMs to engine the creation of synthetic data, namely GPT2, DistilGPT2, and DialoGPT. Below we provide a brief history of each."}, {"title": "2.3 Synthetic Data Evaluation", "content": "Evaluating synthetic data is essential to determine its usefulness. Since this paper focuses on utility, it is important to select appropriate metrics to assess the quality of synthetic data. The simplest evaluation approach involves using descriptive statistics, such as comparing the mean, median, and standard deviation with those of real data. While similar values may suggest that the synthetic data resembles the real data, this method can be misleading [17]. As shown by [4], datasets with nearly identical descriptive statistics can have very different distributions, meaning descriptive statistics alone may not provide an accurate assessment.\nAnother promising approach to evaluating synthetic data involves measuring its resemblance to real data through mathematical metrics. These metrics have been applied in some LA research (e.g., [31]) and more widely across other fields of synthetic data generation [23, 58]. Among the most frequently used methods are Wasserstein Distance (WD), Jensen-Shannon Divergence (JSD), and the Chi-squared test, which are detailed further in Section 3.4. Each of these metrics provides a different lens through which the alignment of synthetic and real data can be understood.\nMore recently, researchers such as [2] and [45] have introduced innovative measures such as Quality, Detection, and Utility to evaluate the utility of synthetic data. These methods go beyond basic similarity (i.e., resemblance), but also offer deeper insights into the replicability and efficiency of synthetic data to real data.\nThis is particularly important for assessing the utility of synthetic data in machine learning applications. By examining the performance of models trained on synthetic data in comparison to those trained on real data, we can gauge whether the synthetic data adequately captures the underlying structure of the real data. If performance is comparable, it suggests that the synthetic data is not only statistically similar but also functionally useful. This approach has gained attention as one of the most robust ways to evaluate the generalisation potential of synthetic data[23]."}, {"title": "3 Methodology", "content": "To address the two research questions in our study, we develop a methodological pipeline as depicted in Figure 1. This pipeline comprises several key components: datasets description (Section 3.1), environment and setup (Section 3.2), synthetic data generation methods (Section 3.3), and finally, the evaluation phase (Section 3.4)."}, {"title": "3.1 Datasets Description", "content": "We selected five tabular datasets from four diverse sources based on LA systems, each varying in size and scale to enhance representativeness. This selection was further refined by incorporating datasets with varying degrees of balance, as recommended by [19] for synthetic data generation. Notably, two of the datasets (i.e., B1 and B2) are identical in structure but differ in size, allowing us to examine the impact of dataset scale on synthetic data generation.\nPrior to utilising our target datasets for synthetic data generation and subsequent evaluation -where they are partitioned into 70% for training and 30% for testing as shown in Figure 1- a series of preprocessing steps were performed. These include random sampling, computation of inner products, imputation of missing values (NaNs), normalisation of features, encoding of categorical variables, addition of new columns, and feature engineering to optimise the datasets. Brief descriptive information about each dataset after being preprocessed, including its class and source, is provided in Table 1. The following describe the datasets in details:"}, {"title": "3.2 Environment and Setup", "content": "The experiments in this study for the synthetic data generation and evaluation were conducted on Google Colab using two premium accounts to access the A100 GPU. The A100 GPU, part of NVIDIA's Ampere architecture, provided the necessary computational power for generating and evaluating synthetic data. The training of LLMs coupled with the evaluation of synthetic data to assess its structural accuracy and predictive performance requires significant computational resources. The A100 GPU serves invaluable in enabling us to carry out this work.\nWe use the Synthcity library [44] mainly to generate synthetic data and benchmarks. Synthcity is a Python library designed to generate and evaluate synthetic tabular data and can automatically calculate about 25 metrics."}, {"title": "3.3 Synthetic Data Generation Methods", "content": "We generate synthetic data using four generative models: CTGAN, DistilGPT2, DialoGPT, and GPT2. We integrate these models into our experiment setups using the Synthcity library. The generative models learn the statistical patterns from the real data by processing the original dataset as input. For CTGAN, this means directly working with the real dataset's tabular structure, learning the distribution of both categorical and numerical features, as well as the relationships between columns. On the other hand, the GReaT framework transforms each row of the tabular dataset into a text-based representation and feeds it into DistilGPT2, DialoGPT, and GPT2 in order to generate similar data [8].\nWe generate synthetic data by using both the LLMs and CTGAN of the same size (i.e., number of rows and columns) as the real datasets to ensure consistency for downstream tasks. Each column value is synthetically produced based on the patterns learned from the real data which preserve key relationships between the dataset's features while mimicking realistic variability. This implies that augmenting the datasets is feasible; however, for our research work, we aimed to create a consistent number of dataset attributes and records. Our intended approach maintains the dataset's structure, which we believe is essential for evaluating and training predictive models as close as to the real data."}, {"title": "3.3.1 Parameter tuning", "content": "For CTGAN, we use the default plugin values provided by the Synthcity library. The three most important parameters are 2,000 training iterations, 200 Batch sizes, and 0.001 learning rate. These parameters influence the model's ability to learn the data distribution. We maintained these default values to provide a standard implementation of CTGAN for our experiments.\nTo generate synthetic data using the three LLMs, we incorporate the GReaT framework [8] as a plugin into the Synthcity library. The GReaT framework leverages advanced pre-trained transformer language models to produce high-quality synthetic tabular data. GReaT converts each row of the dataset into text and uses it to fine-tune the target LLM. However, the default implementation of GReaT imposes a limit on text length, which poses challenges for datasets with a large number of features that require longer text representations, as is the case in our study. To overcome this limitation, we modified the GReaT framework by implementing our own constructor function. This allows us to fine-tune the LLMs with longer texts. We use the Hugging Face library within GReaT to download and fine-tune the three LLMs.\nWe adjusted the training parameters for each LLM and dataset as the following: For the MOOC dataset (Dataset C), we set the number of epochs (i.e., iterations of the training data) to 30 and the batch size (number of samples) to 32. For the Student Math dataset (Dataset A), we used 100 epochs with a batch size of 32. For the Student Info 10% dataset (Dataset B1), we set the number of epochs to 50 and the batch size to 32, while for the Student Info 30% dataset (Dataset B2), we used 20 epochs with the same batch size. Finally, for the Student Performance and Engagement dataset (Dataset D), we adjusted the value of epochs to 30 and the batch size to 32. These adjustments allowed us to manage computational resources to meet our requirements while ensuring sufficient training for each model."}, {"title": "3.4 Evaluation of Synthetic Data Generation", "content": "In this study we use a variety of evaluation metrics to provide a solid utility examination of synthetic data following the guidelines of [2, 23, 31, 45, 56]. In detail, we use WD, JSD, Chi-squared, Quality, Detection, and Utility metrics tests for evaluating the utility of performance and resemblance for synthetic data. To avoid confusion between the Utility metric proposed by [2] and [45] and the broader concept of utility as a general evaluation of synthetic data for machine learning models, we rename the Utility metric from [2] and [45] to OOD AUCROC.\nTo provide an overview of Quality, Detection, and OOD AUCROC, we introduce the Synthetic Data Integrity Score (SDIS). For further assessment of the performance of synthetic data in machine learning, we generated synthetic datasets of the same size as the real datasets. Classifiers were trained on 70% of the data either real or synthetic and tested on 30% of the real data, ensuring consistent training data volumes for a fair comparison. The evaluation metrics include Accuracy, AUCROC score, and Fl-score. Below is a brief description of each metric:"}, {"title": "4 Findings", "content": "This section presents the results of the generation of the synthetic tabular data as well as the detailed evaluation of the five LA tabular datasets. The results form grounding to answer our research questions (RQ1 and RQ2). Due to limited"}, {"title": "4.1 Evaluation of Utility Performance", "content": "Table 2 compares the four models of CTGAN, DistilGPT2, GPT2, and DialoGPT across the different datasets (A, B1, B2, C, and D) using three metrics: Quality, Detection, and OOD AUCROC.\nCTGAN demonstrates higher Quality scores for the datasets B1, B2, and C which the averages are well above 0.64. In contrast, GPT2 and DialoGPT show strong performance, with GPT2 attaining the highest Quality scores of 0.6259 in Dataset D and DialoGPT with a score of 0.6443 in Dataset A. On the other hand, DistilGPT2 produces lower Quality scores compared to other models, which implies that the reduction in model size may compromise its ability to generate data that mimics real distributions. In [33], it is suggested that scores in this range indicate high-quality synthetic data, positioning our result within a favorable spectrum."}, {"title": "4.2 Evaluation of Utility Resemblance", "content": "On reporting to what extent synthetic data resemble the real data in distribution, we evaluated the statistical distribution of WD, JSD, and Chi-squared test of the synthetic data with reference to the real data (see Fig. 4). The WD figure reveals several interesting observations. Dataset A consistently demonstrates the highest mean WD scores across all generative models which suggest a relatively large divergence from the real data. This could return to the large number of continuous and categorical dimensions of the dataset which makes the distance more sensitive. While most models exhibit similar WD scores, datasets B1 and B2 display notably the lowest distances which as a result indicate a closer structural similarity between the synthetic and real data. Dataset D also shows relatively low mean WD values, except for CTGAN. Our summary of the WD analysis suggests that LLMs can generate synthetic data that is comparable to CTGAN in terms of resemblance.\nWhen evaluating the mean JSD, the analysis shows results that differ slightly from the WD analysis. The mean JSD scores are consistently low, typically below 2% which means the resemblance for the synthetic data is very high. For Dataset A, all generative models exhibited similar performance, with JSD values around 0.0170. However, for datasets B1, B2, and C, CTGAN consistently outperformed the other models. Notably, on Dataset D, CTGAN showed a significant divergence, with the highest JSD value, indicating the worst performance among the four models. In our summary, the JSD analysis suggests that CTGAN is the top performer in most cases (except for Dataset D), with GPT2 closely following. While both CTGAN and GPT2 excel on multiple datasets, it is noteworthy that their distribution similarity can vary depending on dataset characteristics."}, {"title": "4.3 Evaluation of Machine Learning Classification", "content": "To check the performance of the synthetic data in machine learning experiments, we created tasks using a set of supervised learning classifiers, including Random Forest, KNN, XGBoost, and Decision Tree (see Fig. 5). Both the real and synthetic data were split into 70% for training and 30% for testing. For each generative model, we train the classifiers on 70% of the synthetic training data, utilising grid search to identify the optimal parameters. The trained models are then evaluated on the 30% of the real test data. The performance of the models were assessed by calculating AUCROC, Fl-score, and Accuracy which enabled us to determine the quality of predictions of synthetic data in training machine learning models. These results are then compared with those obtained when the models are trained on real data. It is important to note that the classification tasks for datasets A, C, and D are binary, while the classification tasks for datasets B1 and B2 are multi-class.\nFor the synthetic data, in Dataset A, the synthetic data generated by GPT2, DialoGPT and DistilGPT2 are well within the highest performance region when the dataset undergoes classification task using the Random Forest. Under the XGBoost classifier, DistilGPT2 outperformed the real data, achieving an F1-score of around 0.92 and an AUCROC score nearly identical to that of the real dataset. Notably, GPT2 demonstrates the best performance among all generative models in Dataset A, even exceeding the real data when it undergoes classification tasks with the Decision Tree classifier.\nIn datasets B1 and B2, CTGAN and DialoGPT have comparable AUCROC and F1-score, although the overall scores in both datasets are relatively lower compared to the other datasets. DistilGPT2 shows weak performance across both datasets, with AUCROC score below 0.53 and F1-score below 0.32. GPT2 exhibits average performance, while DialoGPT and CTGAN outperform the real data in AUCROC during the XGBoost classification task on Dataset B2.\nIn Dataset C, CTGAN achieves performance as close to the real data across three of the classifiers, both in AUCROC and F1-score. However, under the Decision tree classifier, DialoGPT and GPT2 have higher AUCROC scores than the real data. DistilGPT2 has a low F1-score across all the classifiers, although the AUCROC score exceeds 0.75 for the Random forest and XGBoost classifiers.\nIn Dataset D, CTGAN has higher F1-score than the real data when trained using KNN, XGBoost, and Random forest classifiers. But with Decision Tree the synthetic data generated by almost all the generative models performed poorly. The synthetic data generated by DialoGPT and DistilGPT2 consistently underperforms across all the classifiers, with AUCROC Score below 0.75 and F1-score below 0.70.\nThe results depicted in Fig. 5 highlight the varying performance of the generative models when compared to real data in classification tasks. To provide a more comprehensive overview of the four generative models' performance, we conducted a comparative analysis of their mean Accuracy, AUCROC, and F1-score across the synthetic and real datasets, as illustrated in Fig. 6.\nIn Fig. 6, the three evaluation metrics demonstrate that the four generative models-CTGAN, GPT2, DialoGPT, and DistilGPT2-exhibit relatively strong performance compared to real data, with only marginal differences and few exceptions. While real data consistently outperforms the generative models, the differences across all metrics remain minimal which shows a robustness of the synthetic data generated by the models. Notably from the figure, CTGAN slightly exceeds the other LLMs and delivers performance comparable to real data across all three metrics for datasets B1, B2, C, and D. In certain metrics, such as AUCROC for datasets B1, B2, C, and D, as well as F1-score for datasets B1 and B2, DialoGPT and GPT2 achieved performance comparable to that of CTGAN. Among the three LLMs, DistilGPT2 consistently underperformed, with few exceptions. It is important to note that the predictive tasks for datasets B1"}, {"title": "5 Discussion and Conclusion", "content": "This study investigates the application and potential of synthetic data in LA by addressing two key research questions, RQ1: To what extent can synthetic student data be leveraged and evaluated to real data and predict actual educational outcomes? And RQ2: How do LLMs compare to GAN-based models in terms of generating synthetic data for educational data science? Our findings add novel insights to the current underexplored research on synthetic data for educational data science in general, and LA in particular. The study also contributes to the emerging discourse of examining the utility of synthetic data and exploring the difference between LLMs and GANs for that purpose.\nFollowing, we address the two RQs questions of the study by designing a pipeline (Fig. 1) and conduct experiments that entail synthetic data generation and evaluation. With respect to RQ1, interestingly, the results demonstrate that the synthetic tabular datasets generated align closely with real-world datasets, particularly in terms of statistical similarity. The evaluation of utility resemblance metrics WD, JSD, and Chi-squared test demonstrate that the synthetic tabular data can replicate, to a large extent, the underlying statistical distribution of real educational data. The findings from CTGAN are consistent with recent studies by [56] and [31], who explored the use of GANs for generating synthetic educational tabular data and reported promising results in terms of data similarity. Another key finding that stands out from our study, which is timely and novel, is the potential of LLMs to generate synthetic tabular data. In fact this finding accords with [9] and [37]. The latter study demonstrated a comparable performance of LLMs to CTGAN in generating synthetic tabular datasets that closely resemble real-world distributions, as evidenced by our WD compelling scores. The minimal disparity in resemblance performance between LLMs and CTGAN models can be partially attributed to LLMs respective trained parameters size. The substantially larger number of parameters in LLMs allows them to model more intricate data patterns and dependencies; we evidenced that for DialoGPT.\nThe evaluation also considers three important dimensions, namely Quality, Detection, and OOD AUCROC, which provide a comprehensive assessment of the synthetic tabular data's realism, distinguishability and generalisability. These findings suggest that synthetic data serves as a reliable proxy for real data in certain analytical tasks, though its"}, {"title": "5.1 Study Limitations", "content": "It is important to note that this study is more focused on the quality, utility, and resemblance of the generated synthetic data, and for this reason, we exclude consideration of the privacy evaluation of the generated synthetic data. Our primary limitation is the restricted time and computational resources, which impact several areas. First, fine-tuning the LLMs is challenging which leads us to select hyperparameters like batch size and the number of epochs in a way that allows us to run experiments within our resource limits. Unfortunately, this results in suboptimal accuracy. Second, our evaluation metrics are based on two repetitions, with the mean taken as the final result. We believe that a higher number of repetitions would yield a more accurate assessment. Additionally, in this study, we exclude some popular LLMs. For instance, models like GPT3/4 are not open-source, which conflicts with our intention to use open-source models. Many LLMs, such as Llama 3 require high-end GPUs for fine-tuning, which poses another challenge when"}, {"title": "6 Future Direction", "content": "While this study provides significant insights into the use of synthetic data, several areas remain open for further research. One particularly important direction is the potential of synthetic data to improve fairness in predictive modelling (see for example [29]). Synthetic data might offer a potential to generate more balanced and equitable datasets than real datasets, which could help mitigate biases in model training. Future work is required in LA to explore methods addressing fairness issues for demographic records using synthetic data generation, as well as evaluation metrics to assess their results. Some studies have already explored this in healthtech [7] and demographic studies [1]. Additionally, future studies could focus on improving the efficiency of LLMs for synthetic tabular data generation, such as exploring different open-source LLMs and investigating methods such as zero-shot and few-shot learning to reduce computational demands. As synthetic data gains wider application in various domains, investigating new metrics beyond traditional statistical and predictive evaluations will be important to gain a more comprehensive understanding of synthetic data's impact and utility in the LA field and educational data science.\nOur study's core question, which was inspired by [35] and adapted to our context, asks: Can we create artificial students with profiles so authentic they mirror reality? Can synthetic data achieve such a complex ambition? For years, LA researchers have grappled with log files, often struggling with issues of data quality, privacy, and scarcity, which hampers efforts to produce scalable, portable, and generalizable insights. This ongoing challenge leaves us to wonder if advances in AI and deep learning, particularly with technologies like GANs and LLMs, might offer a solution. We believe this innovation holds immense promise, and we approach it from an optimistic perspective, seeing synthetic data generation potential to expand the horizons of the LA field. Our study sought to travel through this possibility, though we recognise this is just the beginning."}]}