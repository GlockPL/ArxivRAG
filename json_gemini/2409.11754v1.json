{"title": "NPAT: Null-Space Projected Adversarial Training Towards Zero Deterioration of Generalization", "authors": ["Hanyi Hu", "Qiao Han", "Kui Chen", "Yao Yang"], "abstract": "To mitigate the susceptibility of neural networks to adversarial attacks, adversarial training has emerged as a prevalent and effective defense strategy. Intrinsically, this countermeasure incurs a trade-off, as it sacrifices the model's accuracy in processing normal samples. To reconcile the trade-off, we pioneer the incorporation of null-space projection into adversarial training and propose two innovative Null-space Projection based Adversarial Training(NPAT) algorithms tackling sample generation and gradient optimization, named Null-space Projected Data Augmentation (NPDA) and Null-space Projected Gradient Descent (NPGD), to search for an overarching optimal solutions, which enhance robustness with almost zero deterioration in generalization performance. Adversarial samples and perturbations are constrained within the null-space of the decision boundary utilizing a closed-form null-space projector, effectively mitigating threat of attack stemming from unreliable features. Subsequently, we conducted experiments on the CI-FAR10 and SVHN datasets and reveal that our methodology can seamlessly combine with adversarial training methods and obtain comparable robustness while keeping generalization close to a high-accuracy model.", "sections": [{"title": "1. Introduction", "content": "Deep learning models are claimed to be universal function approximator(Hornik et al., 1989) and have shown promising capability in fitness on different tasks. Contrarily, deep learning models can be vulnerable to human unnoticeable disturbance on input and generate completely unexpected outcome (Szegedy et al., 2013)(Biggio & Roli, 2018). Adversarial training methods attempt to leverage model vulnerability under these worst-case attacks. The subtlety is the trade-off between the standard error and robustness error, namely, the error on zero perturbed samples and the error on worst-case perturbed samples. The terminology of the trade-off is interchangeable with generalization and robustness in the literature.\nMany previous works have explained and provided theoretical analysis on this trade-off problem. There are two main theories in the literature with one claiming the standard training objective is fundamentally different from that of the adversarial task (Tsipras et al., 2018)(Zhang et al., 2019)(Fawzi et al., 2018) and the other one arguing that the capacity of the classifier is not large enough for improving robustness while keeping accuracy (Nakkiran, 2019). However, the sample separation of different classes for MNIST (Deng, 2012), CIFAR10 (Krizhevsky et al., 2009) and SVHN (Netzer et al., 2011) have been investigated empirically (Yang et al., 2020) that samples are bound to be classifiable perfectly if these attacks were within a \u025b-ball (lo perturbation) less than the smallest inter-class separation. Yet, there is no promising method mitigating the trade-off of accuracy and robustness, but mainly controlling the level of trade-off."}, {"title": "2. Notation and Preliminaries", "content": null}, {"title": "2.1. Standard error, Robust error, Consistent Perturbation", "content": "Given an n pair of input xstd \u2208 Xstd \u2286 Rn\u00d7d and target y \u2208 Y \u2286 Cm\u00d71 dataset D, a standard training tend to learn a mapping f (\u00b7; (std) : X std \u2192 Y with the lowest standard error Cstd, where C denotes the target set {1, 2, . . ., c}.\nLstd = -E(xstd,y)~D[l(f(xstd), y)]  (1)\nA typical adversarial training method attempts to opti-"}, {"title": "2.2. Null Space Definition", "content": "Definition 2.1. Given a matrix W \u2208 Rd1xd2, the null space of W is defined as Null(W) = {x|Wx = 0}.\nDefinition 2.2. Given matrix W \u2208 Rd1\u00d7d2 and r(W) < min{d1,d2}, \u2203 PNull(W) satisfies that,\nWPNull(W)x = 0, for \u2200x \u2208 Rd (5)\nIf rank of matrix r(W) < min{d1,d2}, the null space projection matrix PNull(W) exists non-zero closed-form solution. The null space projection matrix is defined as,\nPNull(W) = I-W(WTW)-1WT (6)\nThe computation of (WTW)-1 is costly and PNull(W) is typically solved by Singular Vector Decomposition(SVD). The SVD factorizes a matrix W = UEVT \u2208 Rm\u00d7n, where U \u2208 Rm\u00d7m corresponds to orthonormal basis of the column space of W, \u03a3\u2208 Rm\u00d7n is a pseudo-diagonal matrix. The diagonal elements are the singular values of W. VT \u2208 Rnxn is the orthonormal basis of row space of W. The projection of row space of W can be represented as VVT. The projection of null space can be calculated as,\nPNull(W) = I \u2013 VVT (7)"}, {"title": "2.3. A Closer Look at Model Behavior in Standard Training vs. Advesarial Training", "content": "Consider a deep learning model fall under standard training, the output y is computed by L \u2013 1 layer of non-linear"}, {"title": "3. Method", "content": "In this section, we elaborate two implementations for mitigating the trade-off between standard error and robustness error. The overall structure of our adversarial training framework can be found in Figure 2a. The first one is in-line with other adversarial training methods such as PGD-AT (Madry et al., 2017), TRADES (Zhang et al., 2019), where we attempted to generate null space projected samples to train model parameters @adv without affecting generalization performance. The second method is to train the last linear layer, WI, by projecting gradient to the null space, which essentially keep track of the output of f (x; (adv) and f(x; 0std).\nWe have demonstrated a toy sample representation difference between typical adversarial training and null projection-based adversarial training in Figure 3. For typical adversarial training, adversarial perturbation is unconstrained, resulting in sample crossing decision boundary. Whereas for null projection-based adversarial training, the perturbation is constrained to Null(Wstd) which is orthogonal to the space affecting decision boundary."}, {"title": "3.1. Null-space Projected Data Augmentation", "content": "Recall Eq.(4), we can rewrite the objective function in this case as,\n\u03b8\u03b1\u03bb\u03c5 \u03b4\n\u0108robust = min maxl(f(x + 8; 0adv), y)\nwhere f(x; (std) = f(x + 8; 0std) (10)\nEquivalently, we can represent Eq.(10) as,\nCrobust min\n\u03b8\u03b1\u03bb\u03c5 \u03b4\u2206hadv \u2208Null(Wstd)\nmax\nl(f(x + \u03b4; (adv), y) (11)\nThe disturbance incorporates precise parameter gradient information from the current training model, thereby augmenting the model's robustness against adversarial attacks relying on reverse gradients. Furthermore, this perturbation is carefully restricted within the null-space of a well-established model, ensuring that it does not have a negative repercussion on the optimal accuracy for non-disturbed samples.\nHowever, it is tough to directly find a d, which maps to \u2206hadv \u2208 Null(Wstd). Alternatively, we can generate it reversely. Firstly, we can generate derivatives with respect to hL-1 and project it to Null(Wstd) to form a null projected adversarial representation in penultimate layer hL\u22121.\nadv-np\nTadv-np: hadv = hstd + PNull(std) d\n\u03b4\u03b9\ndhL-1\nhstd + PNull(std) d\ny\n= W\u03b4\u03b9(12)\nHaving generated hdp, we can compute \u201cequivalent\u201d adversarial sample by carrying on applying chain-rule.\n\u03b4\u03b9\nTadv-np : xadv-np = xstd + na\ndhL-1\nJhL-1\n\u03b4\u03b9\nJhL-1\nWT\n= dy xstd + dy PNull(std) \u0434\u0443(13)\nL\nThe adversarial sample is then generated as (Madry et al., 2017) iteratively,\nxadv-np\n= Xt\n\u03a0xadv-np + NPNull(std) \u03b4(\u03c7,\u03b5)\nWT TOHL-1 dy(14)"}, {"title": "3.2. Null-space Projected Gradient Descent", "content": "Again, we initiate from our objective function in general form in Eq.(4). This time instead of imposing constraint on \u03b4, we cast constraint on model parameter 0. Since we start the adversarial training with the standard training model, f(\u00b7; 0adv)|t=0 = f(\u00b7; (std), where t denotes the number of epochs trained. We can relax the the objective function in this scenario as,\nCrobust\n= min maxl(f(x + \u03b4; (adv), y)\n\u03b8\u03b1\u03bb\u03c5 \u03b4\ns.t. f(x; 0std) \u2248 f(x; (adv) (15)\nLikewise, the gradient update in the non-linear layers are trivial for us, as we only interested in the last layer to keep track of the constraint term in Eq. (15). In this way, we train Wadu simply by projecting the derivative to the null space,"}, {"title": "4. Experiments & Evaluation", "content": "Experiment Setups: We have adopted CIFAR10 and SVHN to verify the effectiveness of our methods. The backbone model used in the experiment were kept with Pre-Act Resnet in this work. During Training, the adversarial samples were found by iterating 10 steps and the adversarial attack coefficient \u03b7 in each step and pre-defined adversarial bound were 2/255 and 10/255. We have implemented PGD attack and Auto-attack (Croce & Hein, 2020) for testing robustness. The adversarial bound in the test phase for both PGD attack and Auto-attack is 8/255. Notice that we have normalized input data with mean and standard deviation. We set mean to 125.3, 123.0, 113.9 and standard deviation to 63.0, 62.1, 66.7 for CIFAR10 and set both mean and standard deviation to 0.5 for SVHN, which is different from the setting as (Croce et al., 2020) but followed the same experiment setup to (Zhou et al., 2023) for comparison. All our experiments were implemented on a NVidia V100 GPU. We have compared State-Of-The-Art (SOTA) adversarial training methods such as PGD-AT, TRADES, FRL, SCORE, CAAT. (Zhang et al., 2019)(Xu et al., 2021)(Madry et al., 2017)(Zhou et al., 2023) Since our methods can be utilized seamlessly with loss defined by these SOTA methods, we report our results accordingly. PGD was used as adversarial method in all our testing."}, {"title": "4.1. Main Results", "content": "We first report the overall performance of different models evaluated on CIFAR10 and SVHN datasets in Table 1 and a scatter plot of performance of CIFAR10 is shown in Figure 1. The st model and lse st model are two standard training models trained by Cross-Entropy(CE) loss and least-squared error(LSE) with pretrained parameters on ImageNet (Russakovsky et al., 2015). The LSE loss is used in the SCORE method as the classification loss. As TRADES loss splitted the CE loss to a classification loss and boundary loss, it allows us imposing an adversarial coefficient \u1e9e to control the level of trade-off between generalization and robustness, whereas the extent of robustness is arbitrary for CE loss. Thus, for a fair comparison, we used TRADES loss for the baseline adversarial models in most cases except for PGD, CAAT and SCORE.\nTo validate the effectiveness of our model, we have tuned \u03b2 to 0.01 to obtain similar clean error as NPDA and NPGD. Both our null-space projector based methods outperform that of baseline TRADES@\u03b2 = 0.01, showing that our boost in robustness is indeed not a result of controlling the level of adversarial loss by tuning hyper-parameter \u03b2.\nThe standard error of NPDA & NPGD under most of configurations are close to that of standard training, except for the lse st model parameter initialized NPGD with SCORE loss and outperform all adversarial baseline methods. The maximum difference between our null-space projected method and standard model are 1.35% and 0.6% for CIFAR10 & SVHN. In general, we have observed a minor accuracy drop except for NPGD on SVHN. Neither NPDA nor NPGD outperforms each other consistently in both datasets in terms of accuracy.\nWithout losing too much on standard accuracy, NPGD obtained a comparable robustness error. The best robustness errors among all adversarial baselines are from PGD and our method NPGD reached almost the same level for CIFAR10 comparing with the best case of baseline adversarial methods, whereas there was a 35.83% gap for SVHN without hindering the generalization performance.\nWe illustrated the training dynamics for NPDA and NPGD in Figure 4. There is no trade-off between generalization and robustness in terms of losses and accuracy, which evidently show that we obtained extra robustness without sacrificing generalization under the scope of no extra dataset and optimizing model structure."}, {"title": "4.2. Variation of Adversarial Coefficient B", "content": "We then experimented on variation of different adversarial coefficient \u1e9e to see if it is possible to improve robustness error without hurting standard error. The adversarial sample generation method was PGD-AT and the loss used was TRADES for all cases. In Figure 5, the accuracy on both datasets are almost straight lines with negligible drop, when increasing adversarial coefficient B. As a result, we still see a trade-off as we gradually increase \u03b2. The cost in trading off robustness for standard error is considerably low under this scope and the robustness gradually saturates as \u03b2 increases. The detailed experimental result can be found in Appendix D.\nWe plotted the loss landscape of PGD-AT, TRADES, NPDA under different adversarial coefficient \u1e9e and NPGD under different adversarial coefficient 8 with adversarial attack and random attack in Figure 6. From our observation, the adversarial training methods generally produce a smoother landscape and by increasing adversarial coefficient, the loss landscape of NPDA & NPGD become smoother."}, {"title": "4.3. Variation of Hidden Size", "content": "Lastly, we investigated the size of the null space by changing hidden size of penultimate layer. As we attempted to increase the null space for the same standard trained model, we initialized models with same backbone for all ResNet blocks and introduced an extra linear layer with different hidden sizes in testing. In general, we observed better robustness with larger hidden size for SVHN but the robustness fluctuated on CIFAR10 from Table 2, while the generalization on both datasets were around the same level under different hidden sizes."}, {"title": "5. Related Works", "content": "There are many previous works provided thorough analysis and existence of the accuracy-robustness trade-off problem. (Tsipras et al., 2018) claimed that the trade-off is inevitable as the objective of two tasks are fundamentally different. They showed the difference by showing a simplified example composing of a moderately correlated robust feature, and a set of strongly correlated vulnerable features altogether grouped as a \"meta\" feature. The optimum accuracy cannot be reached without utilizing the \"meta\" feature. (Fawzi et al., 2018) proposed a framework of analyzing the trade-off for linear classifier and quadratic classifier. On the contrary, (Nakkiran, 2019) proposed it is the capacity of model that determines the level of robustness. (Croce et al., 2020) have proposed a standard robust bench with promising generalization and robustness, but methods with high rank introduces extra dataset or search for a model capacity by neural architecture search(NAS).\nThe earliest adversarial training method, PGD-AT, was proposed by (Madry et al., 2017) and adversarial training was proven to be the most effective way of improving model robustness by (Athalye et al., 2018). (Zhang et al., 2019) designed a trade-off loss, (aka. TRADES) by splitting the standard loss and adversarial loss. However, it is an over-strong assumption that all robust features can be learned by model, which might not be the case in reality due to the model architecture and the way of training. (Raghunathan et al., 2020) also used a noiseless linear regressor to show effect of parameter error when introducing extra dataset (adversarial samples). They provided three theoretical conditions to avoid the trading-off and proved the effectiveness of Robust Self Training (RST) method. Nonetheless, their conditions are for linear model and difficult to meet for generating adversarial samples. (Pang et al., 2022) declared the trade-off is partially due to the misalignment of learned adversarial estimator po* (y|x) and joint data distribution pa(y|x), and proposed a Self-Consistent Robust Error (SCORE) loss by reformulating adversarial loss.\nThe other source of error is known as unfairness, as there exists disparity of samples among different classes due to unequal variance, priors and noise level. (Xu et al., 2021) attempted to leverage the fairness by continuously estimating the upper bound of boundary error and reweighting sample loss for each class (FRL). Essentially, it forms unequal decision boundaries between classes. Upon FRL, (Zhou et al., 2023) introduced an anti-adversarial sample-based method, CAAT, to cope with issue of noisy-sample. The adversarial training task can be considered as a multi-task learning problem since the extra adversarial samples are under same distribution as original dataset. There is a notorious catastrophic forgetting problem where model performance degrades on previous tasks when learning on new task. (Kirkpatrick et al., 2017) proposed a regularizer-based method, EWC, penalizing large deviation parameters from previous tasks. (Wang et al., 2021) have proposed a null-space projecting optimizer for continual learning, which performs null space estimation based on space of previous parameters and the null projection were deployed to every layer of the model.\n(Ravfogel et al., 2020) proposed an iterative null-projection method for removing sensitive information from the representation and obtaining an exclusive estimator. Our work is greatly inspired by the way of decomposing model in their work."}, {"title": "6. Conclusion", "content": "In this work, we provided theoretical studies of training an adversarial estimator in terms of its non-linear backbone and last linear transformation. We then proposed two methods accordingly with derivation of gradient update in both cases. Finally, we verified our methods under different settings to reveal the effectiveness on CIFAR10 and SVHN datasets."}, {"title": "Appendix", "content": null}, {"title": "A. Null-Space Projector By SVD", "content": "Suppose we have a matrix W \u2208 Rm\u00d7n and r(W) < min(m,n) factorized by SVD, W = UEVT \u2208 Rm\u00d7n, where U \u2208 Rm\u00d7m corresponds to orthonormal basis of the column space of W, \u03a3 \u2208 Rm\u00d7n is a pseudo-diagonal matrix.\nVT \u2208 Rn\u00d7n is the orthonormal basis of row space of W.\nRecall Definition 2.1 & Definition 2.2, there exists a PNull(W) that satisfies WPNull(W)x = 0, for \u2200x \u2208 Rn\u00d71.\nBy factorizing W and substitute closed form solution of PNull(W) from SVD based on Eq.(7), we have,\nWPNull(W)x = UEVT (I \u2013 VVT) \u2022 x\n= (UEVT \u2013 UEVTVVT) \u00b7 x, where VTV = I\n= (UEVT \u2013 UDVT) \u00b7 x\n=\nX\n(19)\n0.x\n= 0"}, {"title": "B. Theoretical Guarantee of NSAT", "content": null}, {"title": "B.1. Proof of Remark 3.1", "content": "Proof. From Eq. (17), we are updating the parameter of last linear layer, W\u0142 in a mini-batch as,\n\u0174s = \u01748\u22121 + \u20acPNull(W)item(s - 1)\n= \u0174s-2 + \u20acPNull(W) (item(s - 1) + item(s - 2))\n=\ns-1\nWstd + EPNull(W) \u2211item(k).\nk=0\nwhere item are partial derivatives computed from each batch of data.\nT\nEventually, we get optimal W\u2081 as,\nstop\nWopt = Wstd + \u20acPNull(W) \u2211 item(i),\ni\n(20)\nwhere the last summation term are mapped to the null space of W, Null(W). Since the Wopt is the last linear layer of fadv (x) trained by NPGD. Thereby, we have\nstop\nfadv (x) = [Wstd + EPNull(W) \u2211 item(i)]H\ni\nstop\n= Wstd H + EPNull(W) \u2211 item(i) H\nstop\n= fotd(x) + EPNull(W) \u2211 item(i) H\n(21)\nRearrange Eq. (21),\nstop\nfadv (x) \u2013 fotd(x) = ePNull(W) \u2211 item(i) H \u2208 Null(W)\ni\n(22)"}, {"title": "C. Gradient Update of NPDA", "content": "The gradients with respect to W\u2081 and HL\u22121 are shown in Eq. (23) and Eq. (24).\n\u03b4\u03b9\n= WE\n\u03b4\u03b9\ndy dhadvVT\nL\u0434\u0443 \u0434\u044b-1 (23)\n\u03b4\u03b9WT\n\u03b4\u03b9 \u0434\u0443 \u03b4\u03b9WT\n= d-1L \u0434\u044b-1 \u0434\u0443(24)\nThe gradient updated for Wf can be represented as in Eq. (25).\nW WE-nadvy(25)\nNext, let us elaborate the gradient updated in each layer. The gradient computed for a particular layer Wn for 0 < n < L \u22121 can be represented as,\n\u03b4\u03b9\nJWT = dy hL-1hnL(26)\nTherefore, the gradient for a particular layer Wn after null space projection is as illustrated in Eq. (27).\n\u03b4\u03b9 \u03b4\u03b9 dyhn-np\n=WE dh-1(27)"}]}