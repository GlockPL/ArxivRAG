{"title": "Agent Laboratory: Using LLM Agents as Research Assistants", "authors": ["Samuel Schmidgall", "Yusheng Su", "Ze Wang", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Jiang Liu", "Zicheng Liu", "Emad Barsoum"], "abstract": "Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages\u2014literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.", "sections": [{"title": "1. Introduction", "content": "Scientists frequently face constraints that limit the number of research ideas they can explore at any given time, resulting in ideas being prioritized based on predicted impact. While this process helps determine which concepts are worth investing time in and how best to allocate limited resources effectively, many high quality ideas remain unexplored. If the process of exploring ideas had less limitations, researchers would be able to investigate multiple concepts simultaneously, increasing the likelihood of scientific discovery.\nIn an effort to achieve this, recent work has explored the capability of LLMs to perform research ideation and automated paper generation, where LLM agents perform the role of human scientists (Baek et al. (2024); Ghafarollahi & Buehler (2024b); Lu et al. (2024a); Swanson et al. (2024)). The work of Baek et al. (2024) introduces ResearchAgent, which automatically generates research ideas, methods, and experiment designs, iteratively refining them through feedback from multiple reviewing agents that mirror peer discussions and leverage human-aligned evaluation criteria to improve the outputs. Lu et al. (2024a) explores fully automated paper generation, where The AI Scientist framework generates novel research ideas, writes code, conducts experiments, and creates a full scientific paper with an automated peer-review system to evaluate the work. Even though these works demonstrate that current LLMs can generate ideas judged to be more novel than those produced by human experts, Si et al. (2024) indicates that LLMs still exhibit weaknesses in feasibility and implementation details, suggesting a complementary rather than replacement role for LLMs in research. Therefore, we aim to design an autonomous agent pipeline that can assist humans toward implementing their own research ideas.\nIn this work, we introduce Agent Laboratory, an autonomous pipeline for accelerating the individual's ability to perform machine learning research. Unlike previous approaches, where agents participate in their own research ideation independent of human input (Baek et al. (2024); Lu et al. (2024b)), Agent Laboratory is designed to assist human scientists in executing their own research ideas using language agents. Agent Laboratory takes as input a human research idea and outputs a research report and code repository produced by autonomous language agents, allowing various levels of human involvement, where feedback can be provided at a frequency based on user preference. A detailed list of our contributions are provided below:\n1. We introduce Agent Laboratory, an open-source LLM agent framework for accelerating the individual's ability to perform research in machine learning. In order to accommodate all users, Agent Laboratory is compute flexible, where various levels of compute can be allocated based on the individual's access to compute resource (e.g., CPU, GPU, memory) and model inference budget.\n2. Human evaluators rated papers generated using Agent Laboratory across experimental quality, report quality, and usefulness, showing that while the o1-preview backend was perceived as the most useful, 01-mini achieved the highest experimental quality scores, and gpt-4o was behind in all metrics.\n3. NeurIPS-style evaluations showed that o1-preview performed best among backends, particularly in clarity and soundness, according to human reviewers. However, a clear gap emerged between human and automated evaluations, with automated scores significantly overestimating quality (6.1/10 vs. 3.8/10 overall). Similar discrepancies were seen across clarity and contribution metrics, suggesting the need for human feedback to complement automated evaluations for more accurate assessments of research quality.\n4. Co-pilot mode in Agent Laboratory was evaluated on custom and preselected topics, showing higher overall scores compared to autonomous mode. Co-pilot papers also saw trade-offs"}, {"title": "2. Background & Related Work", "content": "Large language models The research agents in this paper are built on autoregressive large language models (LLMs), which are trained on extensive text corpora to predict conditional probabilities of token sequences, $p(x_t|x_{<t}; \\theta)$, and generate text completions through sampling, where $x_t \\sim \\text{softmax}(W \\cdot h_t)$, with $h_t$ as the hidden state and $W$ as the learned weight matrix mapping to token probabilities. LLMs utilize transformer architectures (Vaswani (2017)) to capture long-range dependencies in text. These models, such as Claude (Anthropic (2024)), Llama (Dubey et al. (2024); Touvron et al. (2023a,b)), and ChatGPT (Achiam et al. (2023); Hurst et al. (2024); OpenAI (2022)), leverage vast datasets and scaling techniques, thus enabling them to perform a wide array of language-based tasks, such as translation, summarization, and reasoning, by generalizing patterns learned during pretraining to novel inputs Brown (2020).\nLLM Agents While LLMs demonstrate strong understanding and reasoning abilities, they face challenges when executing tasks in real-world scenarios. To overcome these limitations, their capabilities are extended through structured frameworks, enabling them to autonomously and semi-autonomously perform task execution and semi-autonomously perform task execution (Chen et al. (2023b); Li et al. (2023); Qian et al. (2024); Wu et al. (2023)). These systems, referred to as agents, utilize techniques such as chain-of-thought prompting (Wei et al. (2022)), iterative refinement (Shinn et al. (2024)), self-improvement (Huang et al. (2022)), and external tool integration to execute complex workflows (Hao et al. (2024); Qin et al. (2023); Schick et al. (2023)). LLM agents have made remarkable progress in solving tasks of real-world significance, such as software engineering Jimenez et al. (2023); Wang et al. (2024b); Yang et al. (2024)), cybersecurity (Abramovich et al. (2024); Fang et al. (2024); Wan et al. (2024)), and medical diagnosis (McDuff et al. (2023); Schmidgall et al. (2024); Tu et al. (2024)). There has also been progress in applying LLMs agents to embodied problems such as autonomous robotics (Black et al. (2024); Brohan et al. (2022, 2023); Kim et al. (2024)), web tasks (Deng et al. (2024); Gur et al. (2023); He et al. (2024); Putta et al. (2024); Shi et al. (2017)), and game playing (AL et al. (2024); Feng et al. (2024); Wang et al. (2023)). For a broader overview of LLM agents, refer to Wang et al. (2024a)."}, {"title": "3. Agent Laboratory", "content": "Overview. Agent Laboratory begins with the independent collection and analysis of relevant research papers, progresses through collaborative planning and data preparation, and results in automated experimentation and comprehensive report generation. As shown in Figure 2, the overall workflow consists of three primary phases: (1) Literature Review, (2) Experimentation, and (3) Report Writing. In this section, we will introduce these phases in detail along with the corresponding involved agents. Furthermore, in Section 4, we will conduct qualitative and quantitative analyses to demonstrate the strengths of Agent Laboratory and its ability to generate"}, {"title": "3.1. Literature Review", "content": "Literature Review. The literature review phase involves gathering and curating relevant research papers for the given research idea to provide references for subsequent stages. During this process, the PhD agent utilizes the arXiv API to retrieve related papers and performs three main actions: summary, full text, and add paper. The summary action retrieves abstracts of the top 20 papers relevant to the initial query produced by the agent. The full text action extracts the complete content of specific papers, and the add paper action incorporates selected summaries or full texts into the curated review. This process is iterative rather than a single-step operation, as the agent performs multiple queries, evaluates the relevance of each paper based on its content, and refines the"}, {"title": "3.2. Experimentation", "content": "Plan Formulation The plan formulation phase focuses on creating a detailed, actionable research plan based on the literature review and research goal. During this phase, the PhD and Postdoc agents collaborate through dialogue to specify how to achieve the research objective, detailing experimental components needed to complete the specified research idea such as which machine learning models to implement, which datasets to use, and the high-level steps of the experiment. Once a consensus is reached, the Postdoc agent submits this plan using the plan command, which serves as a set of instructions for subsequent subtasks.\nData Preparation. The goal of the data preparation phase is to write code that prepares data for running experiments, using the instructions from the plan formulation stage as a guideline. The ML Engineer agent executes code using Python command command and observes any printed output. The ML Engineer has access to HuggingFace datasets, searchable via the search HF command. After agreeing on the finalized data preparation code, the SW Engineer agent submits it using the submit code command. Before the final submission proceeds, the code is first passed through a Python compiler to ensure that there are no compilation issues. This process will be iteratively executed until the code is bug-free.\nRunning Experiments. In the running experiments phase, the ML Engineer agent focuses on implementing and executing the experimental plan formulated prior. This is facilitated by mle-solver, a specialized module designed to generate, test, and refine machine learning code autonomously. mle-solver begins by producing initial code based on the research plan and insights from the literature review. For the first mle-solver step, the program is empty and must generate a file from scratch, which is used as the top scoring program. The following processes describe the workflow of the mle-solver:\nA. Command Execution. During the command execution phase, an initial program is sampled from a maintained set of top-performing programs, which is represented by a single file during initialization. The mle-solver iteratively refines this program through two operations, REPLACE and EDIT, to better align the output with experimental objectives. The EDIT operation identifies a range of lines, substituting the code between the specified line numbers with newly generated code. In contrast, the REPLACE operation generates a completely new Python file.\nB. Code Execution. After a code command is executed, the new program is passed through a compiler to check for runtime errors. If it successfully compiles, a score is returned and the list of top programs is updated if the score is higher than the existing programs. If the code does not compile, the agent attempts to repair the code for $N_{rep}$ tries ($N_{rep}$=3 in our experiments) before returning an error and moving on to a new code replacement.\nC. Program Scoring. If a code succeeds in compilation, it is sent to a scoring function which determines if it is better than previously implemented experiment code. In order to obtain a program score, we implement a scoring function that uses an LLM reward model to assess the effectiveness of the ML code generated by mle-solver. The reward model, invoked as an LM, scores the program on a scale from 0 to 1 considering the outlined research plan, the produced code, and the observed output to determine how accurately the program adheres to"}, {"title": "3.3. Report Writing", "content": "Report Writing. In the report writing phase, the PhD and Professor agent synthesize the research findings into a comprehensive academic report. This process is facilitated by a specialized module called paper-solver, which iteratively generates and refines the report. The paper-solver aims to act as a report generator, positioning the work that has been produced by previous stages of Agent Laboratory. paper-solver does not aim to entirely replace the academic paper-writing process, but rather to summarize the research that has been produced in a human-readable format so that the researcher using Agent Laboratory understands what has been accomplished. The output follows the standard structure of an academic paper, ensuring it meets conference submission requirements (for the paper scoring phase) while being clear and methodical. The following processes describe the workflow of paper-solver:\nA. Initial Report Scaffold. The first task of the paper-solver is to generate an initial scaffold for the research paper. This scaffold outlines the document structure, dividing it into eight standardized sections: Abstract, Introduction, Background, Related Work, Methods, Experimental Setup, Results, and Discussion. During scaffold creation, placeholders are inserted for each section to categorize future content. This process establishes the framework for subsequent detailed text generation. The scaffold includes necessary formatting for LaTeX compilation, allowing the generated paper to be directly reviewed and refined. Special care is taken to ensure the scaffold aligns with academic conventions, such as appropriate section titles and placeholders that guide content development."}, {"title": "4. Results", "content": "In this section, we present our main findings on the efficacy of Agent Laboratory to produce research. We begin our results by asking how human evaluators perceive papers generated by Agent Laboratory running in end-to-end autonomous mode across five topics. Next, we examine human evaluation when using Agent Laboratory in collaborative co-pilot mode from both allowing the researcher to choose any topic they want and from our set of preselected topics. We then provide a detailed runtime analysis including cost, average time, and success rate by various models. Finally, we conclude with an evaluation of the mle-solver in isolation on MLE-Bench, a set of real-world Kaggle challenges. The details of all surveys are provided in Appendix C."}, {"title": "4.1. Evaluation of quality by language model", "content": "Our first experiment aims to evaluate how human-evaluated quality varies across three axes: experiment quality, report quality, and usefulness. This evaluation was conducted by human participants using three different LLM backends: gpt-40 (Hurst et al. (2024)), 01-mini, and o1-preview (OpenAI (2024)). Research questions were selected from a set of 5 templates:\n1. Do language models exhibit cognitive biases, such as confirmation bias or anchoring bias?\n2. Are image transformers more or less sensitive to pixel noise than convolutional networks?"}, {"title": "4.1.1. Human reviewer scores by language model", "content": "In addition to evaluating paper quality, we also asked human reviewers to assess papers generated by Agent Laboratory according to NeurIPS-style criteria, including quality, significance, clarity, soundness, presentation, and contribution as shown in Figure 6. We evaluated the same papers analyzed in Section 4.1 using the aforementioned metrics and conducted the comparison. We found that the average human scores for the three backends revealed differences in performance, with average overall ratings ranging from 3.5/10 with gpt-40, 3.8/10 with o1-mini, and 4.0/10 with 01-preview.\nFirst, when evaluating quality we find that reviewers rated gpt-40 the lowest at 1.8/4, while 01-mini achieved the highest score of 2.3/4, demonstrating relatively better technical soundness. In terms of significance, all three backends received similar scores between 2.2\u20132.5/4, indicating a modest contribution to advancing research goals. Clarity scores showed slight variability, with gpt-40 receiving 2.6/4 and 01-mini falling slightly lower at 2.1/4 (-0.5), reflecting differences in how well the papers were written. The soundness of the generated outputs, which assesses the robustness of claims, was rated highest for o1-preview at 2.2/4, with o1-mini and gpt-40 at 1.8 (-0.4) and 1.7. Presentation and contribution ratings followed similar trends, with the overall contribution score averaging 2.1/4 across models, highlighting a need for improvement in the originality of the outputs.\nThese scores show a general trend where human reviewers identified 01-preview as producing slightly better-rounded outputs compared to other backends, though significant gaps remain in technical and methodological aspects across all models. We note that the average score of an accepted paper at NeurIPS is 5.9. In this regard, on average, papers produced in autonomous mode are below the acceptance threshold for top ML conferences. These results demonstrate that, in autonomous mode, there is a need for refinement of Agent Laboratory to meet human expectations for high-quality, impactful research papers.\nAutomated Reviews versus Human Reviews. We also explore to what extent the automated reviewer scores align with those of human reviewers. The alignment is graphically illustrated using both tabular data (for all scores) and violin plots (for overall scores) in Figure 6. Our findings suggest that automated reviewers demonstrate notable discrepancies across all metrics compared with human evaluators, with a tendency to highly over-estimate the contribution of self-evaluated work. While the automated reviewers gave an average overall above average NeurIPS paper score of 6.1/10, human reviewers provided a much lower average of 3.8/10 (-2.3 points). Similar gaps are observed for all"}, {"title": "4.2. Evaluation of co-pilot quality", "content": "We next evaluate the use of Agent Laboratory in co-pilot mode, where a human researcher is providing feedback at the end of each subtask (see Section 3.3.1 for more details). We evaluate performance across two measures: (1) the quality of Agent Laboratory as a tool for assisting their research and (2) the quality of generated papers. We first ask researchers to co-pilot Agent Laboratory on a topic of their choice without limitations. We then ask researchers to select a topic from the 5 topics introduced in Section 4.1, resulting in a total of 2 papers per researcher which we refer to as custom and preselected papers respectively. After their papers are generated, we ask researchers to rate their experience using Agent Laboratory during the process of generating custom and preselected papers. We then ask them to self-evaluate the generated papers according to NeurIPS-style criterion. Finally, we ask external researchers to evaluate their paper comparing performance with Agent Laboratory in autonomous mode. All experiments used an o1-mini backbone for all phases except the literature review."}, {"title": "4.2.1. Quality as a tool", "content": "The evaluation of Agent Laboratory as a research tool focuses on understanding its effectiveness in assisting researchers during the co-pilot mode. After generating their papers, participants were asked to reflect on their experiences and assess the tool's utility, usability, and overall satisfaction. We begin our evaluation by asking the following questions:\n* Utility: How useful is Agent Laboratory for assisting your research?\n* Continuation: How likely are you to continue using Agent Laboratory for research?\n* Satisfaction: How much did you enjoy using Agent Laboratory?\n* Usability: How easy was it for you to build a project using Agent Laboratory?\nThe result of answering each question is a score from 1-5, where 1 indicates the lowest agreement and 5 indicates the highest. We find that the overall scores across all experiments are 3.5/5 for utility, 3.75/5 for continuation, 3.63/5 for satisfaction, and 4.0/5 for usability (Figure 7). We also delineate average scores based on custom and preselected topics. For custom experiments, we find overall scores of 3.75/5 for utility, 4.0/5 for continuation, 3.75/5 for satisfaction, and 3.75/5 for usability. For preselected topics, we find overall scores of 3.25/5 for utility, 3.5/5 for continuation, 3.5/5 for satisfaction, and 4.25 for usability. Ratings for preselected topics are lower across all measures compared with custom, except for usability which was -0.5 points lower. From preselected to custom, utility and continuation increased by +0.5 points and satisfaction increased by +0.25 points.\nWe also evaluated across the same questions reported in Section 4.1. We report an average experimental quality rating of 2.38/5, a report quality rating of 3.13/5, and a usefulness rating of"}, {"title": "4.2.2. Evaluation of co-pilot generated papers", "content": "To assess the quality of papers generated by Agent Laboratory in co-pilot mode, we conduct evaluations using two approaches: (1) researchers self-assessed their generated papers based on NeurIPS-style criteria, and (2) external researchers provided evaluations of the same papers. This section aims to understand differences in scores from self-assessment and external assessment, as well as how assessments compare to Agent Laboratory in fully autonomous mode. We use the same NeurIPS criterion introduced in Section 4.1.1."}, {"title": "5. Limitations", "content": "While our results suggest that Agent Laboratory demonstrates strong performance as a research tool, we now turn to a discussion of limitations that could inform future work. While some of these are also limitations of LLMs themselves, others are not, and we nonetheless provide a thorough and critical discussion of our work. We hope that progress in autonomous research will address these limitations."}, {"title": "5.1. Workflow limitations", "content": "Challenges with self-evaluation The paper-solver is being evaluated for quality by using LLMs emulated NeurIPS reviewers. This has two limitations: (1) while the reviewing agents were shown to have high alignment with real reviewers (Lu et al. (2024b)), qualitatively research reports from Agent Laboratory are less satisfying than research papers from The AI Scientist (Lu et al. (2024b)), with ours having lower quality figures, despite Agent Laboratory papers obtaining higher scores overall. (2) The research reports produced by Agent Laboratory are not meant to replace the paper writing process done by humans as it was in The AI Scientist, rather it is meant to provide a report for the human to understand what has been accomplished, so that they can scale up the experiment and write their own research report. However, we nonetheless use NeurIPS reviewer scores as the heuristic for the quality of our presented paper-solver, which aims to evaluate the reports from the perspective of a complete research paper. Additionally, contrasting with Lu et al. (2024b) demonstrate that LLMs perform less reliably for self-evaluation compared with human reviewers, with lower agreement scores (53.3% vs. 56.1%). Although LLMs demonstrate reasonable consistency, this may stem from reliance on superficial patterns rather than robust evaluation criteria, resulting in discrepancies between LLM and human rankings. This limits LLMs in subjective tasks like research idea evaluation, which is the foundation of mle-solver and paper-solver.\nChallenges with automated structure There are also some limitations that present themselves due to the structure enforced in the workflow. For example, paper-solver is encouraged to a organize the paper into a relatively fixed structure (abstract, introduction, etc), which disallows unique paper organizations and section orders. Another limitation is that mle-solver and paper-solver are limited to generating only two figures for the paper. This can be solved in future work, by allowing all of the figures generated by the mle-solver (without restriction) to be incorporated into"}, {"title": "5.2. Common failure modes", "content": "In addition to the limitations outlined in Section 5.1, we also outline common failure modes observed during the runtime of Agent Laboratory. We report a list of the most common failure modes observed below:\n* Many of the more capable models (gpt-40, 01-mini, o1-preview) struggled with instruction-following during the literature review phase, and had a tendency to repeatedly use the summarize command until the maximum phase steps have been reached, leading to a termination.\n* Retrieved papers during the literature review phase had been observed to reach the maximum token limit for some models.\n*  When generating figures for the paper using mle-solver, the figure legends, titles, or often\n* Experiments run by mle-solver sometimes obtain 0% accuracy for all tested methods which is not corrected by the agent by the time mle-solver runs out of solving steps.\n* mle-solver has a tendency to edit line 0 more than other lines in the code, causing to the replace command to more often lead to successful code compiles.\n* Printed output from the data preparation or experimental results can lead to the LLMs reaching their token limit.\n* mle-solver often generated the python exit () command, which terminated the entire process. This had to be detected and removed manually.\n* mle-solver has been observed to run system commands on the host computer using the subprocess.run() command. While nothing problematic has been observed, safeguards should be implemented around this.\n* paper-solver often struggles to search for relevant papers using the arXiv engine. Before a search time-limit was enforced, it could take up to 100 tries for a successful search query to return any papers. A limit of 5 was place thereafter to prevent this cycle."}, {"title": "5.3. Ethical considerations", "content": "Agent Laboratory offers potential to accelerate the field of machine learning research by automating time-intensive tasks and enabling researchers to focus on ideation and experimental design. However, its capabilities also bring ethical challenges that require careful consideration. The ability"}, {"title": "6. Discussion", "content": "In this paper, we introduce Agent Laboratory, an open-source LLM agent framework for accelerating the individual's ability to perform research in machine learning. Unlike fully automated research pipelines that attempt to conceive their own research directions, Agent Laboratory is designed as a co-pilot, enabling a more human-centric mode of scientific exploration. Because of this, we present results from human-centered experiments. Our initial evaluations focused on the quality of generated papers in autonomous mode, assessing human evaluations of experimental and report quality, usefulness, as well as reviewer scores based on standard academic criteria across different language models. We also assessed the effectiveness of Agent Laboratory in co-pilot mode, comparing its performance with autonomous mode, receiving positive feedback from researchers.\nThe findings of this work highlight the variability in performance across LLM backends, with the 01-preview model being rated most useful, while 01-mini demonstrated the highest experimental quality. Autonomous mode outputs, although generally well-received, revealed gaps when evaluated against human expectations for high-quality research papers, particularly in terms of clarity and soundness. We also find that automated reviewer scores do not predict human reviewer scores demonstrating the importance of human evaluations inautomated research. ntegrating human feedback in co-pilot mode overall produced higher-quality outputs than autonomous mode, with higher scores across most metrics. The co-pilot feature in Agent Laboratory is overall found to have high utility and usability when rated by human users, with most participants deciding to continue usage after their experience. Finally, runtime and cost analyses demonstrated the efficiency of the framework, with the gpt-40 backend offering the fastest execution and lowest costs. Finally, evaluations of the mle-solver on MLE-Bench demonstrates improved ability to solve general ML problems over previous methods.\nAgent Laboratory builds upon an emerging trend in the use of language agents for science, where previous works have shown the potential of LLMs to generate research ideas (Baek et al. (2024); Li et al. (2024a); Si et al. (2024)), implement machine learning projects (Chan et al. (2024); Huang et al. (2024); Jing et al. (2024)), and even produce scientific papers (Lu et al. (2024b)). While many of these prior efforts leverage LLMs as tools to be applied at discrete stages, Agent Laboratory integrates these processes into a single, continuous pipeline that can scale and adapt to"}, {"title": "A. Agent Laboratory configuration", "content": "A.1. Hyperparameters\nTable 1 | Hyperparameters for AGENT LABORATORY.\nCategory Hyperparameter Value\nLiterature Review Number of Paper Summaries 5\nFull Text History Decay Steps 3\nAgent temperature 0.8\nData Preparation Experiment Timeout 120s\nRunning Experiments mle-solver steps 3\nCode repair attempts 2\nMaximum top codes 2\nError history length 5\nCode history length 2\nNumber of comparison trials 2\nExperiment Timeout 600s\nScore generation temperature 0.6\nRepair temperature 0.8\nInitial code temperature 1.0\nSolver temperature 1.0\nPaper Writing paper-solver steps 5\nMaximum top papers 1\nPaper history length 10\nNumber of Reviewers 1\nNumber of comparison trials 2\nSolver temperature 1.0\nInitial paper temperature 0.8\nPaper Refinement Number of Reviewers 3\nA.2. Hardware\nAll experiments in this paper were run on a 2023 MacBook Pro with an Apple M3 Max processor and 36 GB of memory."}, {"title": "B. Prompts", "content": "B.1. Base Inference Prompt\nBase System Prompt\nYou are {self.role_description()}\nTask instructions:{self.phase_prompt(phase)}\n{self.command_descriptions(phase)}\nBase Prompt\n{context_prompt}\nHistory: {history_str}\nCurrent Step #{step}\nPhase: {phase}\n{complete_str}\n[Objective] Your goal is to perform research on the following topic:\n{research_topic}\nFeedback: {feedback}\nNotes: {notes_str}\nYour previous command was: {self.prev_comm}. Make sure your new output is different.\nPlease produce a single command below:\nPhase Notes (notes_str)\nNotes for the task objective: {phase_notes}\nComplete String The complete string is typically set to the empty string. However, in the case when the number of steps reaches 70% of the way toward completion, the following is appended to the base prompt to encourage the agent to produce a submission.\nComplete String (complete_str)\nYou must finish this task and submit as soon as possible!\nHistory Line\nStep #{step}, Phase: {phase}, Feedback: {feedback}, Your response: {model_resp}"}, {"title": "B.2. Context Prompts", "content": "Context Prompt\n{sr_str}\n{context_prompt}\nContext Prompt Second Round String (sr_string)\nThe following are results from the previous experiments\nPrevious Experiment code: {self.prev_results_code}\nPrevious Results: {self.prev_exp_results}\nPrevious Interpretation of results: {self.prev_interpretation}\nPrevious Report: {self.prev_report}\n{self.reviewer_response}\nContext Prompt Plan Formulation\nCurrent Literature Review: {self.lit_review_summary}\nContext Prompt Data Preparation\nCurrent Literature Review: {self.lit_review_summary}\nCurrent Plan: {self.plan}\nContext Prompt Results Interpretation\nCurrent Literature Review: {lit_review_sum}\nCurrent Plan: {self.plan}\nCurrent Dataset code: {self.dataset_code}\nCurrent Experiment code: {self.results_code}\nCurrent Results: {self.exp_results}\nContext Prompt Report Refinement\nCurrent Literature Review: {lit_review_sum}\nCurrent Plan: {self.plan}\nCurrent Dataset code: {self.dataset_code}\nCurrent Experiment code: {self.results_code}\nCurrent Results: {self.exp_results}\nCurrent Interpretation of results: {self.interpretation}"}, {"title": "B.3. Agent Phase Descriptions", "content": "B.3.1. PhD Student phase\nPhD Literature Review Phase Prompt\nYour goal is to perform a literature review for the presented task and add papers to the literature review.\nYou have access to arXiv and can perform two search operations: (1) finding many different paper summaries from a search query and (2) getting a single full paper text for an arXiv paper.\nPhD Literature Review Phase Prompt\nYou are a PhD student being directed by a postdoc who will help you come up with a good plan, and you interact with them through dialogue.\nYour goal is to produce plans that would make good experiments for the given topic. You should aim for a very simple experiment that showcases your plan, not a complex one. You should integrate the provided literature review and come up with plans on how to expand and build on these works for the given topic. Your plans should provide a clear outline for how to achieve the task, including what machine learning models to use and implement, what types of datasets should be searched for and used to train the model, and the exact details of the experiment.\nPhD Data Preparation Phase Prompt\nYou are a PhD student directing a machine learning engineer, where the machine learning engineer will be writing the code, and you can interact with them through dialogue.\nYour goal is to help the ML engineer produce code that prepares the data for the provided experiment. You should aim for very simple code to prepare the data, not complex code. You should integrate the provided literature review and the plan and come up with code to prepare data for this experiment.\nPhD Results Interpretation Phase Prompt\nYou are a PhD student being directed by a postdoc who will help you come up with an interpretation for results from an experiment, and you interact with them through dialogue.\nYour goal is to interpret results from experiments that were previously run. You should read through the code and look at the results to understand what occurred. You should then discuss with the postdoc your interpretation and use their feedback to improve your thoughts. You should integrate the provided literature review, code, and plans to come up with an exciting interpretation that could"}, {"title": "B.4. Machine Learning Engineer Phase Descriptions", "content": "ML Engineer Data Preparation Phase Prompt\nYou are a machine learning engineer being directed by a PhD student who will help you write the code, and you can interact with them through dialogue.\nYour goal is to produce code that prepares the data for the provided experiment. You should aim for simple code to prepare the data, not complex code. You should integrate the provided literature review and the plan and come up with code to prepare data for this experiment."}, {"title": "B.5. Postdoc Phase Descriptions", "content": "Postdoc Plan Formulation Prompt\nYou are directing a PhD student to help them come up with a good plan, and you interact with them through dialogue.\nYour goal is to produce plans that would make good experiments for the given topic. You should aim for a very simple experiment that showcases your plan, not a complex one. You should integrate the provided literature review and come up with plans on how to expand and build on these works for the given topic. Your plans should provide a clear outline for how to achieve the task, including what"}, {"title": "B.6. Agent Command Description", "content": "B.6.1. PhD Student Command Description\nPhD Student Literature Review Command Prompt\nTo collect paper summaries", "command": "n```SUMMARY\nSEARCH QUERY\nwhere SEARCH QUERY is a string that will be used to find papers with semantically similar content and SUMMARY is just the word SUMMARY.\nTo get the full paper text for an arXiv"}]}