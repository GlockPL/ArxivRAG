{"title": "Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games", "authors": ["Tong Yang", "Bo Dai", "Lin Xiao", "Yuejie Chi"], "abstract": "Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of applications involving the interaction of a group of agents in a shared unknown environment. A prominent framework for studying MARL is Markov games, with the goal of finding various notions of equilibria in a sample-efficient manner, such as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE). However, existing sample-efficient approaches either require tailored uncertainty estimation under function approximation, or careful coordination of the players. In this paper, we propose a novel model-based algorithm, called VMG, that incentivizes exploration via biasing the empirical estimate of the model parameters towards those with a higher collective best-response values of all the players when fixing the other players' policies, thus encouraging the policy to deviate from its current equilibrium for more exploration. VMG is oblivious to different forms of function approximation, and permits simultaneous and uncoupled policy updates of all players. Theoretically, we also establish that VMG achieves a near-optimal regret for finding both the NEs of two-player zero-sum Markov games and CCEs of multi-player general-sum Markov games under linear function approximation in an online environment, which nearly match their counterparts with sophisticated uncertainty quantification.", "sections": [{"title": "Contents", "content": null}, {"title": "1 Introduction", "content": "Multi-agent reinforcement learning (MARL) is emerging as a crucial paradigm for solving complex decision-making problems in various domains, including robotics, game theory, and machine learning [Busoniu et al., 2008]. While single-agent reinforcement learning (RL) has been extensively studied and theoretically analyzed, MARL is still in its infancy, and many fundamental questions remain unanswered. Due to the interplay of multiple agents in an unknown environment, one of the key challenges is the design of efficient strategies for exploration that can be seamlessly implemented in the presence of a large number of agents\u00b9 without the need of complicated coordination among the agents. In addition, due to the large dimensionality of the state and action spaces, which grows exponentially with respect to the number of agents in MARL, it necessitate the adoption of function approximation to enable tractable planning in modern RL regimes.\nA de facto approach in exploration in RL is the principle of optimism in the face of uncertainty [Lai, 1987], which argues the importance of quantifying the uncertainty, known as the bonus term, in the pertinent objects, e.g., the value functions, and using their upper confidence bound (UCB) to guide action selection. This principle has been embraced in the MARL literature, leading a flurry of algorithmic developments [Liu et al., 2021, Bai et al., 2021, Song et al., 2021, Jin et al., 2021, Li et al., 2022, Ni et al., 2022, Cui et al., 2023, Wang et al., 2023, Dai et al., 2024] that claim provable efficiency in solving Markov games [Littman, 1994], a standard model for MARL. However, a major downside of this approach is that constructing the uncertainty sets quickly becomes intractable as the complexity of function approximation increases, which often requiring a tailored approach. For example, near-optimal techniques for constructing the bonus function in the tabular setting cannot be applied for general function approximation using neural networks.\nTherefore, it is of great interest to explore alternative exploration strategies without resorting to explicit uncertainty quantification, and can be adopted even for general function approximation. Our work is inspired by the pioneering work of Kumar and Becker [1982], which identified the need to regularize the maximum-likelihood estimator of the model parameters using its optimal value function to incentivize exploration, and has been successfully applied to bandits and single-agent RL problems [Liu et al., 2020, Hung et al., 2021, Mete et al., 2021, Liu et al., 2024] with matching performance of their UCB counterparts. However, this strategy of value-incentivized exploration has not yet been fully realized in the Markov game setting; a recent attempt [Liu et al., 2024] addressed two-player zero-sum Markov games, however, it requires asymmetric updates and solving bilevel optimization problems with the lower level problem being a Markov game itself. These limitations motivate the development of more efficient algorithms for the general multi-agent setting while enabling symmetric and independent updates of the players. We address the following question:\nCan we develop provably efficient algorithms for online multi-player general-sum Markov games with function approximation using value-incentivized exploration?"}, {"title": "1.1 Contribution", "content": "In this paper, we propose a provably-efficient model-based framework, named VMG (Value-incentivized Markov Game solver), for solving online multi-player general-sum Markov games with function approximation. VMG incentivizes exploration via biasing the empirical estimate of the model parameters towards those"}, {"title": "2 Two-Player Zero-Sum Matrix Games", "content": "In this section, we start with a simple setting of two-player zero-sum matrix games, to develop our algorithmic framework."}, {"title": "2.1 Problem setting", "content": "Two-player zero-sum matrix game. We consider the (possibly KL-regularized) two-player zero-sum matrix games with the following objective:\n$\\displaystyle \\max_{\\mu \\in \\Delta^m} \\min_{\\nu \\in \\Delta^n} f_{\\mu, \\nu} (A) := \\mu^T A \\nu - \\beta KL(\\mu || \\mu_{\\text{ref}}) + \\beta KL(\\nu || \\nu_{\\text{ref}}),$"}, {"title": "2.2 Algorithm development", "content": "We propose a model-based approach, called VMG, that enables provably efficient exploration-exploitation trade-off via resorting to a carefully-regularized model (i.e., the payoff matrix) estimator without constructing uncertainty intervals. To enable function approximation, we parameterize the payoff matrix by $A_w \\in \\mathbb{R}^{m\\times n}$, where $w\\in \\Omega \\subset \\mathbb{R}^d$ is some vector in the parameter space $\\Omega$.\nThe proposed approach, on a high level, alternates between updating the payoff matrix based on all the samples collected so far, and collecting new samples using the updated policies. Let's elaborate a bit further. At each round $t$, let the current payoff matrix estimate be $A_{w_{t-1}}$, and its corresponding NE be $(\\mu_t, \\nu_t)$.\n*   Value-incentivized model updates. Given all the collected data tuples $\\mathcal{D}_{t-1}$ and the policy pair $(\\mu_t, \\nu_t)$, VMG updates the model parameter $w_t$ via solving a regularized least-squares estimation problem as (7), favoring models that minimizes the squared loss between the model and the noisy feedback stored in $\\mathcal{D}_{t-1}$, and maximizes the value of each player when the other player's strategy is fixed. In other words, the regularization term aims to maximize the duality gap at $(\\mu_t, \\nu_t)$, which tries to pull the model away from its current estimate $A_{t-1}$, whose duality gap is 0 at $(\\mu_t, \\nu_t)$. The regularized estimator thus strikes a balance of exploitation (via least-squares on $\\mathcal{D}_{t-1}$) and exploration (via regularization against the current model $A_{w_{t-1}}$).\n*   Data collection from best-response policy updates. Using the updated payoff matrix $A_{w_t}$, VMG updates the best-response policy of each player while fixing the policy of the other player via (8), resulting in policy pairs $(\\mu_t, \\nu_t)$ and $(\\tilde{\\mu}_t, \\tilde{\\nu}_t)$. Finally, VMG collects one new sample from each of the policy pairs respectively following the oracle (2), and add them to the dataset $\\mathcal{D}_{t-1}$ to form $\\mathcal{D}_t$.\nThe complete procedure of VMG is summarized in Algorithm 1. VMG invokes the mechanism of regularization as a means for incentivizing exploration, rendering it more amenable to implement in the presence of function approximation. In contrast, prior approach [O'Donoghue et al., 2021] heavily relies on explicitly adding an exploration bonus to the estimate of the payoff matrix using confidence intervals, which is challenging to construct under general function approximation. In addition, VMG allows parallel and independent policy execution from both players."}, {"title": "2.3 Theoretical guarantee", "content": "We demonstrate that VMG achieves near-optimal regret, assuming linear function approximation of the payoff matrix. Specifically, we have the following assumption."}, {"title": "3 Multi-player General-sum Markov Games", "content": "We now turn to the more challenging setting of online multi-player general-sum Markov games, which includes the two-player zero-sum Markov game as a special case."}, {"title": "3.1 Problem setting", "content": "Multi-player general-sum Markov game. We consider an $N$-player general-sum episodic Markov game with a finite horizon denoted as $M_p := (\\mathcal{S}, \\mathcal{A}, P, r, H)$, where $\\mathcal{S}$ is the state space, $\\mathcal{A} := \\mathcal{A}^1 \\times \\cdots \\times \\mathcal{A}^N := \\prod_{n=1}^N \\mathcal{A}^n$ is the joint action space for all players, with $\\mathcal{A}^n$ the action space of player $n$, and $H \\in \\mathbb{N}_+$ is the horizon length. Let $\\Delta(\\mathcal{S})$ and $\\Delta(\\mathcal{A})$ denote the set of probability distributions over $\\mathcal{S}$ and $\\mathcal{A}$, respectively. $P = \\{P_h\\}_{h\\in [H]}$ with $P_h : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ is the inhomogeneous transition kernel: at step $h$, the probability of transitioning from state $s$ to state $s'$ by the action $a = (a^1,\\ldots, a^N)$ is $P_h(s'|s,a)$. $r = \\{r^{h,n}\\}_{h\\in [H],n\\in [N]}$ stands for the reward function with $r^n : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$ the reward of the $n$-th player at step $h$.\nMarkov policies. In this paper, we focus on the class of Markov policies, where the policy of each player depends only on the current state, without dependence on the history. We let $\\pi^n : \\mathcal{S} \\times [H] \\rightarrow \\Delta(\\mathcal{A}^n)$ denote the policy of player $n$, and $\\pi^n(\\cdot|s) \\in \\Delta(\\mathcal{A}^n)$ denotes the probability distribution of the action of player $n$ at step $h$ given any state $s$. We let $\\pi = (\\pi^1,\\ldots,\\pi^N) : \\mathcal{S} \\times [H] \\rightarrow \\Delta(\\mathcal{A})$ denote the joint Markov policy (we assume all policies appear in this paper are Markovian, and we let joint policy stands for joint Markov policy), where $\\pi(\\cdot|s) := (\\pi_h^1,\\ldots,\\pi_h^N)(\\cdot|s) \\in \\Delta(\\mathcal{A})$ for all $s \\in \\mathcal{S}$ and $h\\in [H]$. For any joint policy $\\pi$, we let $\\pi^{-n}$ denote the joint policy excluding player $n$. With a slight abuse of notation, we write $\\pi = (\\pi^n, \\pi^{-n})$. In addition, a joint policy is called a product policy if $\\pi^1,\\ldots,\\pi^N$ are executed independently, i.e., under policy $\\pi$, each player takes actions independently. We denote $\\pi = \\pi^1 \\times \\cdots \\times \\pi^N$ for a product policy.\nKL-regularized value function and Q-function. Given a joint policy $\\pi$, the KL-regularized state-value function (value function) $V_{\\pi}^{h,n} : \\mathcal{S} \\rightarrow \\mathbb{R}$ and the KL-regularized state-action value function (Q-function)"}, {"title": "3.2 Algorithm development", "content": "For simplicity, we will focus on the function approximation over the transition kernel of the Markov game assuming the reward function is fixed and deterministic, while it is straightforwardly to also incorporate the reward function approximation. We let $\\mathcal{F}$ denote the function class of the estimators of the transition kernel of the Markov game, and we denote the parameterized transition kernel as\n$\\displaystyle P_f = (P_{f,1},\\ldots, P_{f,H}) \\in \\mathcal{F} = \\mathcal{F}_1 \\times \\cdots \\times \\mathcal{F}_H,$"}, {"title": "3.3 Theoretical guarantee", "content": "We demonstrate that VMG achieves near-optimal regret under the following linear mixture model of the transition kernel for Markov games."}, {"title": "C Extension to the Infinite-horizon Setting", "content": "In this section, we consider the $N$-player general-sum episodic Markov game with infinite horizon denoted as $M_p := (\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$ as a generalization of the finite-horizon case in the main paper, where $\\gamma\\in [0,1)$ is the discounted factor, and $P : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ is the homogeneous transition kernel: the probability of transitioning from state $s$ to state $s'$ by the action $a = (a^1,\\ldots, a^N)$ is $P(s'|s, a)$. For the infinite horizon case, the KL-regularized value function is defined as"}, {"title": "C.1 Algorithm development", "content": "The algorithm for solving the (KL-regularized) Markov game is shown in Algorithm 6, where in (128) we set the loss function at each iteration $t$ as the negative log-likelihood of the transition kernel estimator $f$:"}, {"title": "C.2 Theoretical guarantee", "content": "We first state our assumptions on the function class for Markov game with infinite horizon."}]}