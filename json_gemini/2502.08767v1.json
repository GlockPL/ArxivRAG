{"title": "SELFELICIT: Your Language Model Secretly Knows Where is the Relevant Evidence", "authors": ["Zhining Liu", "Rana Ali Amjad", "Ravinarayana Adkathimar", "Tianxin Wei", "Hanghang Tong"], "abstract": "Providing Language Models (LMs) with relevant evidence in the context (either via retrieval or user-provided) can significantly improve their ability to provide factually correct grounded responses. However, recent studies have found that LMs often struggle to fully comprehend and utilize key evidence from the context, especially when it contains noise and irrelevant information-an issue common in real-world scenarios. To address this, we propose SELFELICIT, an inference-time approach that helps LMs focus on key contextual evidence through self-guided explicit highlighting. By leveraging the inherent evidence-finding capabilities of LMs using the attention scores of deeper layers, our method automatically identifies and emphasizes key evidence within the input context, facilitating more accurate and factually grounded responses without additional training or iterative prompting. We demonstrate that SELFELICIT brings consistent and significant improvement on multiple evidence-based QA tasks for various LM families while maintaining computational efficiency.", "sections": [{"title": "1 Introduction", "content": "Language models have demonstrated remarkable capabilities (OpenAI, 2022, 2023; Kaddour et al., 2023), yet hallucinations and factual inaccuracies remain a significant limitation (Ji et al., 2023; Wang et al., 2023). Furnishing the context with relevant evidence is a popular approach to enhance LM's ability to generate factually correct grounded responses (Ram et al., 2023; Jiang et al., 2023b). However, recent studies find that LMs can fail to properly leverage supporting facts within context, leading to incorrect answers despite available evidence (Shi et al., 2024; Zhao et al., 2024). Such failure largely stems from the noise and irrelevant information in context (Wu et al., 2024; Cuconasu et al., 2024), which is usually inevitable in practice (Gao et al., 2023).\nRecently, improved prompting (Zhou et al., 2023) or decoding (Shi et al., 2024) methods have been proposed to enhance the focus on context. However, they treat the whole context as a single entity, overlooking the fact that not all information provided in the context is important. In this work, we investigate how to leverage contextual information at a finer granularity to help LMs focus more effectively on the key evidence within the context. Our contributions in this work are as follows:\n\u2022 By analyzing the attention scores during response generation, we demonstrate that the LMs have an inherent ability to identify the relevant evidence in the context, regardless of whether they respond correctly or not. This observation holds across various LM families.\n\u2022 Leveraging this inherent ability, we propose an inference time context augmentation approach, SELFELICIT, that highlights the important evidence in the context and improves LM's ability to provide factually correct grounded responses. SELFELICIT is efficient for inference, training-free, and robust to noise and hyper-parameter choice.\n\u2022 In a comprehensive study, we show the significant performance improvement that SELFELICIT brings across model families and benchmarks. We also provide detailed studies for hyper-parameters in SELFELICIT and additional analysis on its ability to elicit relevant evidence robustly from noisy context."}, {"title": "2 Preliminaries", "content": "Problem description.\nGiven an LM \u03a6, question q, context c and QA prompt template $TQA$, we obtain the generated answer g for the question by combining context and question as input: $g \\leftarrow \\Phi(TQA(c, q))$. We use the following template as baseline to test different LMs' base ability in leveraging contextual evidence for QA tasks:"}, {"title": "Notation", "content": "We only outline the notation and details of Transformer models needed for this work, please see (Vaswani et al., 2017) for more detailed exposition on transformer architecture. Given an input sequence with n tokens, a decoder only transformer LM generates next token by adaptively attending to all previous n tokens. Denote the attention probability vector for attention head h and layer l by $a^{(l,h)} \\in R^n$. Then we define\n$\\overline{a}^{(l)} = \\frac{1}{H} \\sum_{h=1}^{H} a^{(l,h)}$ (1)"}, {"title": "3 SELFELICIT", "content": "SELFELICIT is an inference-time context augmentation framework to enhance generative LM's capability to rely on relevant information present in the context in order to reduce hallucinations and provide factually correct grounded response. SELFELICIT requires no additional training, is efficient at inference time and provides significant performance improvement across models and tasks."}, {"title": "3.1 Self-guided Contextual Evidence Eliciting", "content": "The first step in SELFELICIT is to leverage the LM's internal representations to identify the relevant evidence sentences in the context. Consider the context-based QA setup described in Sec. 2. Denote the number of tokens in input (c, q) by n and the number of sentences\u00b9 in the context c by m. Define $S: {S_1, S_2, ..., s_m}$ where $s_i$ denotes i-th sentence. Let $(t_{start}, t_{end})$ denote the start and end token index of i-th sentence. Define sentence-level attention vector $\\overline{\\overline{a}}^{(l)} \\in R^m$ from token level attention $\\overline{a}^{(l)}$ as follows\n$\\overline{\\overline{a}}^{(l)} := [\\overline{a}_{s_1}^{(l)}, \\overline{a}_{s_2}^{(l)}, ..., \\overline{a}_{s_m}^{(l)}] \\in R^m$,\nwhere $\\overline{a}_{s_i}^{(l)} = \\frac{1}{t_{end} - t_{start} + 1} \\sum_{j=t_{start}}^{t_{end}} \\overline{a}_{j}^{(l)}$(2)\nIntuitively $\\overline{a}_{s_i}^{(l)}$ tells the relative importance of each context sentence $s_i$ at layer l. In Fig. 2 we plot how $\\overline{\\overline{a}}$ varies across layers for evidence vs non-evidence sentences in the context when generating the first response token. We can clearly see that, regardless of whether the model responds correctly or not, the deeper layers of the LM pay significantly higher attention to the relevant evidence\u00b2 in the context. This observation holds across models families and datasets, see Appendix B.2.\nWe leverage this insight to define the sentence evidence scores $e_i$ for each sentence $s_i$ in S, which"}, {"title": "3.2 Contextual Evidence Highlighting", "content": "The second step of SELFELICIT is to modify the original context to highlight the evidence sentences identified in Sec. 3.1 and to modify the original instructions in Sec. 2 to guide the model to use the highlighted evidence. Prioritizing simplicity and efficiency, we introduce a prompt augmentation strategy to achieve this. Specifically for highlighting, in the raw context passage c we place text markers \"\" and \"\" before and after each sentence in SSE, resulting in the new context $c*$. Furthermore, we update the task instructions to guide the LM's attention towards the highlighted sentences. The response is generated using the mark-highlighted $c*$ and the updated prompt template TSEQA."}, {"title": "4 Experiments", "content": "We conduct comprehensive experiments across six LMs from different families with varying sizes and on four single- and multi-hop reasoning open-book QA tasks from various domains to investigate:\n\u2022 RQ1: How does SELFELICIT perform in terms of improving answer quality and factuality?\n\u2022 RQ2: How do the evidence scores and the context sentences highlighted by SELFELICIT correlate with relevant evidence?\n\u2022 RQ3: Robustness to noise in the context?\n\u2022 RQ4&5: How do different choices of evidence-reading layers & threshold \u03b1 affect SELFELICIT?"}, {"title": "4.1 Experimental Setup", "content": "Datasets and Metrics. We test 4 datasets: HotpotQA (Yang et al., 2018) and the MRQA version (Fisch et al., 2019) of NewsQA (Trischler et al., 2017), TriviaQA (TQA) (Joshi et al., 2017), and Natural Questions (NQ) (Kwiatkowski et al., 2019). These datasets feature context passages from diverse sources (e.g., web/Wikipedia/news reports), requiring the model to reason over a single or multiple pieces of evidence within the context. This provides a comprehensive test of SELFELICIT in real-world applications. For all datasets, we use the official validation split on HuggingFace for testing. We apply greedy decoding to get deterministic answers. Exact Match (EM) and Token-level F1 scores are used for QA performance evaluation.\nModels and Baselines. We test six open-source instruction fine-tuned models: Llama-3.1 (8B, 70B) (Dubey et al., 2024), Mistral (7B, 12B) (Jiang et al., 2023a), and Qwen2.5 (7B, 32B) (Yang et al., 2024). We compare SELFELICIT to the following prompting/evidence-eliciting approaches:\n\u2022 COT (Wei et al., 2022): Chain-of-thought prompting encourages the model to reason through intermediate steps before reaching a final answer. While CoT promotes step-by-step reasoning based on the evidence in the context, it does not explicitly highlight important information within the context. Therefore, we use it as a natural baseline for validating the advantage of explicit evidence elicitation in SELFELICIT. We implement it by adding the COT prompt \"Think step by step to provide the answer.\" at the end of the instruction.\n\u2022 FULLELICIT: A naive approach that highlights the entire context as important. Comparing with it demonstrates the necessity of fine-grained, sentence-level evidence elicitation.\n\u2022 PROMPTELICIT: This method leverages the LM itself for generative evidence extraction. It involves two steps: first, the LM is prompted to select the most relevant evidence sentences from the context passage that can help answer the question. Then, we highlight the extracted evidence and get new context for QA following Section 3.2. Note that this method involves generative evidence extraction in iterative prompting, which requires the model to generate a large amount of additional tokens. This serves as a strong baseline for evaluating whether the quality of evidence selected by SELFELICIT can match that of evidence extracted through generative approach."}, {"title": "4.2 Main Results", "content": "RQ1: SELFELICIT consistently improves grounded factuality. Table 1 compares SELFELICIT to the other methods. We observe that\n\u2022 As expected FULLELICIT does not provide meaningful and consistent improvement since it doesn't elicit fine-grained evidence.\n\u2022 COT also does not provide a meaningful and consistent gain which highlights the fact that asking the model to reason carefully does not improve its ability to leverage the relevant evidence while generating the response. COT results also show variation across LM families where we don't observe meaningful gains for Llama and Mistral models. For Qwen models COT generates longer answers with intermediate steps. While this results in a slight increase in EM scores, it also leads to a large drop (up to 27.9) in F1 scores due to the inclusion of redundant information, as well as longer inference times (e.g., 171.9% more inference time for Qwen2.5-7B)\n\u2022 SELFELICIT significantly and consistently improves the performance across all datasets and models of different sizes (5.0%-11.7% gain over baseline). Even when compared to computationally expensive (average inference time increase of 878%/939% for Llama3.1-8B/70B) iterative prompting approach PROMPTELICIT, SELFELICIT outperforms for 40 out of 48 model-task metric pairs while incurring a fraction of the computational cost increment (only ~3-5% increase when compared to PROMPTELICIT)."}, {"title": "4.3 Additional Analysis", "content": "RQ3: SELFELICIT is effective in presence of context noise. To explore the impact of real world noise on SELFELICIT we study it's performance for the \"distractor\" variant of HotpotQA dataset containing additional distracting information, retrieved from Wikipedia (Yang et al., 2018), in the context. The distractor setting increases the average length of the context by 1443% by introducing irrelevant information. Figure 3(a) shows that SELFELICIT maintains the advantage over the baseline even in the presence of substantial context noise.\nHowever, similar to the baseline, the performance of SELFELICIT for \"distractor\" variant worsens meaningfully when compared to SELFELICIT performance for the gold context setup, also shown in Figure 3(a). To investigate this we do a deep dive into the evidence elicitation capability of SELFELICIT in the presence of substantial noise. Figure 3(b) contrasts the elicit ratio (i.e., the proportion of elicited evidence within the original context) under gold and distractor settings. With gold context, SELFELICIT tends to select larger proportion as evidence since most contextual information provided is relevant. By contrast, in the distractor setting where most of the context is irrelevant, the proportion of evidence selected decreases substantially (typically <10%) for same \u03b1 = 0.5. Furthermore the proportion of elicited evidence coming from the additional distractor information is only TK% even though the distractor information constitutes a major portion of the context. Therefore, even though SELFELICIT highlights the relevant evidence without being distracted by noise, there is still potential to improve how the LM can utilize this highlighted evidence to maintain similar performance as in the case of gold context, a topic for future research.\nRQ4: Deeper Layers are better for LER. We already saw in Fig. 2 that the deeper layers of all the model families exhibit higher attention scores and a clear ability to distinguish relevant evidence within the context. In Table 4 we verify this further empirically by comparing the evidence elicit accuracy and QA performance of seven different choices of LER for Llama3.1-8B evaluated on HotpotQA with \u03b1 = 0.5. We see that, consistent with the observation in Fig. 2, choosing last 50% of the layers as LER leads to (close to) best metrics in terms of both evidence elicitation and task performance. Based on the qualitative observation in Fig. 2, this choice also looks robust across model families which is then further verified by the results in Table 1 where we see a consistent significant improvement across all model families and tasks for this choice of LER. While there may be other choices of LER which lead to better performance gains for a specific model-dataset pair, finding them requires model and task specific hyper-parameter tuning unlike universally applicable default setting.\nRQ5: Balancing elicitation precision and comprehensiveness in choice of \u03b1. The choice of \u03b1 in SELFELICIT acts as a proxy for trade-off between evidence elicitation precision vs coverage. For example, for \u03b1 = 1 we only select the sentence with highest $e_i$ whereas for \u03b1 = 0 we have the whole context selected, analogue to FULLELICIT. In Figure 4 we show the impact of \u03b1 on the Token F1 score and evidence elicitation ratio for Llama3.1-8B on four QA datasets. We observe\n\u2022 For all datasets the performance quickly rises and the evidence elicitation ratio quickly drops with increasing \u03b1 for smaller values of \u03b1 indicating clearly that evidence elicitation upto a threshold helps all datasets.\n\u2022 Beyond \u03b1 = 0.5 there is relatively minor variation in the performance with change in \u03b1, illustrating robustness of SELFELICIT to choice of \u03b1 \u2208 [0.5, 1] across datasets. The minor variation is dependent on the nature of the dataset. For datasets that require multi-hop reasoning, relying on multiple pieces of relevant evidence, we achieve optimal performance for \u03b1 closer to 0.5 whereas for NQ requiring simpler reasoning we achieve the best performance for \u03b1 = 1."}, {"title": "5 Related Works", "content": "Context-based question answering. Furnishing LMs with relevant context is an effective way of providing up-to-date external and/or private knowledge (Ji et al., 2023; Asai et al., 2023) to help mitigate hallucination and improve response accuracy (Huang et al., 2023). Retrieval-augmented Generation (RAG) is a widely adopted paradigm for this purpose (Gao et al., 2023; Fan et al., 2024). Despite its popularity, recent studies have pointed out that context retrieved from external sources often contains noise and irrelevant information, leading to confusion for LM (Cuconasu et al., 2024; Wu et al., 2024). Motivated by this, we explore how to leverage contextual information more effectively at a finer granularity by highlighting critical information within the context. To the best of our knowledge, we are the first to investigate automated contextual evidence highlighting based on LM internal representations.\nFactuality and internal representation. Recent studies have explored ways to understand QA factuality by analyzing the internal representations to identify important attention layers (Yuksekgonul et al., 2024; Chen et al., 2024b) or heads (Halawi et al., 2024) that are crucial for generation correctness or hallucination. However, they primarily focus on how LMs utilize their parametric knowledge in controlled generation (Yuksekgonul et al., 2024; Halawi et al., 2024) or closed-book QA (Chen et al., 2024b), with little discussion on the role of internal representations when utilizing external non-parametric knowledge in context-based QA. Our work focuses on investigating LM's internal attention patterns for the context and on how to leverage these patterns to identify critical evidence in the context."}, {"title": "6 Conclusion", "content": "This paper proposes a novel method SELFELICIT that boosts LM's ability to utilize context with automatic contextual evidence highlighting. It harnesses the inherent ability of specific attention layers within the LM to differentiate evidence within the context, enabling test-time auto-eliciting that is general, efficient and requires no additional training. Comprehensive experiments on context based QA validate the effectiveness of SELFELICIT, showing a performance improvement of 5.0% to 11.7% across multiple datasets and LMs. Compared to more costly generative evidence extraction, our method achieves better performance with far less additional computational overhead."}, {"title": "Limitations", "content": "We evaluated the effectiveness of SELFELICIT across several open-source LMs. However, we can not assess SELFELICIT on proprietary LMs due to the need to access attention scores. On the computational efficiency front, although SELFELICIT generates only one additional token in the first pass and already demonstrates good computational efficiency in practice, there is potential for further acceleration by avoiding re-encoding prompt. This redundancy arises because the input sequence is encoded twice during the first and second passes, despite the only difference being a few text markers inserted into the context passage for evidence highlighting. Exploring methods for caching and reusing shared input content encoding process, thereby avoiding the repeated encoding of the input content is a promising future direction for further reducing computational overhead."}, {"title": "B.1 Computational efficiency", "content": "Due to space constraints, we report only the average running time per sample across all datasets in Table 1. Table 5 provides a detailed breakdown of efficiency results for each dataset, including the average inference time per sample (i.e., context-question pair) and the total number of tokens generated during inference. We observe that: (i) While FULLELICIT is highly efficient as a naive method, it barely helps with the QA performance, as shown"}, {"title": "B.2 More layer-wise attention visualization", "content": "In Figure 2, we present the differences in attention paid by different layers of the model to evidence versus non-evidence context information on the HotpotQA (Yang et al., 2018) dataset. We chose HotpotQA because it provides human-annotated sentence-level \"supporting_facts\" annotations. In this section, we extend our analysis to other datasets. Since human annotations are unavailable for these datasets, we use a simple rule to distinguish evidence from non-evidence sentences: any sentence containing at least one correct answer is considered evidence. While this is not a rigorous approach for labeling evidence sentences, it still allows us to visualize patterns and roughly validate whether our observations hold across different datasets. As shown in Figure 5, deeper layers of various LMs consistently demonstrate a strong ability to differentiate evidence across datasets."}, {"title": "B.3 Sentence-level versus token-level eliciting", "content": "We now show that sentence-level evidence eliciting holds a significant advantage over token-level eliciting due to its ability to highlight evidence with better semantic coherence. Table 6 presents the experimental results using the llama3.1-8B model across four datasets. We compare the normal version SELFELICIT (sentence) and a new token-level variant SELFELICIT (token) by treating each token as a separate \"sentence\". It is evident that token-level eliciting performs worse than sentence-level eliciting on all datasets. The underlying reason is that tokenization does not ensure each token represents a complete entity or concept, resulting in highlighted evidence that lacks full semantic meaning. For example, we notice that a year within the context, say \"2002\", can be tokenized into two tokens \"200\" and \"2\" and token-level eliciting could end up highlighting only the first token \"200.\". Similar \"partial highlighting\" issues also occur with names, locations, and other uncommon phrases. In contrast, sentence-level eliciting highlights the entire sentence, maintaining semantic continuity in the processed context, which better aids the model in utilizing context information for QA."}, {"title": "B.4 Examples with full context", "content": "Finally, we present the full context of examples from Table 2 to illustrate SELFELICIT's ability to identify supporting facts/evidence within noisy long contexts. The results are shown in Tables 7 and 8. These examples are from HotpotQA's distractor setting, where the context passage contains distracting information retrieved from Wikipedia using the question as a query (Yang et al., 2018). Note that these distractors are not completely irrelevant random noise that can be easily filtered out by retrieval systems. Instead, they appear related to the question but do not actually support answering it, serving as \"hard negatives.\" This scenario is quite common in practice and significantly impacts the performance of retrieval-augmented generation (Wu et al., 2024; Cuconasu et al., 2024; He et al., 2024; Liu et al., 2024b). As shown in Tables 7 and 8, the LM struggles to effectively use contextual evidence to provide correct answers under such a situation. Despite this, SELFELICIT accurately highlights the critical evidence within the noisy long context, helping the model focus on the most relevant facts and thus arrive at the correct answer."}, {"title": "C Additional Discussions", "content": "C.1 Potential Risks\nGenerative AI tools such as language models have an increasing impact on our daily lives in the era of big data and AI (Yan et al., 2024; Xu et al.; Ban et al., 2021; Lin et al., 2024a,b; Guo et al., 2023), such as finance (Chan et al., 2024; Liu et al., 2024d) and healthcare (Ye et al., 2023; Liu et al., 2024c), especially with the recent trends of foundation models (Zheng et al., 2024a; Fu et al., 2024; Zheng et al., 2024b; Li et al., 2024). This study focuses on enhancing language models' ability to effectively utilize information from contextual documents. However, retrieved documents from the internet may contain unethical or discriminatory content, which the model might read and incorporate into its outputs. While addressing ethical concerns in Retrieval-Augmented Generation (RAG) or fact-based question-answering tasks (Liu et al., 2024a, 2022, 2021a) is beyond the scope of this work, such issues can typically be mitigated by using detectors to filter harmful information from the context documents provided to the language model, or other general techniques such as ensemble multi-model answers (Liu et al., 2021b, 2020a,b) or watermarking (Chen et al., 2024a) AI-generated contents.\nC.2 Usage of Artifacts and AI Assistants\nAll models and datasets used in this study are publicly available on HuggingFace, and we adhered to their respective licenses and terms of use, limiting our work to non-commercial academic research. These models and datasets have been reviewed by their developers/creators to minimize the inclusion of personally identifiable information or offensive content and are widely adopted by the research community. The datasets primarily consist of English-language content and focus on fact-based question-answering tasks. We used AI tools to assist with language refinement during the writing process, but the paper contains no AI-generated paragraphs. All material has been carefully reviewed to ensure accuracy and adherence to ethical standards."}]}