{"title": "Representation Learning of Structured Data for Medical Foundation Models", "authors": ["Vijay Prakash Dwivedi", "Viktor Schlegel", "Andy T. Liu", "Thanh-Tung Nguyen", "Abhinav Ramesh Kashyap", "Jeng Wei", "Wei-Hsian Yin", "Stefan Winkler", "Robby T. Tan"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various domains, including healthcare. However, their ability to effectively represent structured non-textual data, such as the alphanumeric medical codes used in records like ICD-10 or SNOMED-CT, is limited and has been particularly exposed in recent research. This paper examines the challenges LLMs face in processing medical codes due to the shortcomings of current tokenization methods. As a result, we introduce the UniStruct architecture to design a multimodal medical foundation model of unstructured text and structured data, which addresses these challenges by adapting subword tokenization techniques specifically for the structured medical codes. Our approach is validated through model pre-training on both an extensive internal medical database and a public repository of structured medical records. Trained on over 1 billion tokens on the internal medical database, the proposed model achieves up to a 23% improvement in evaluation metrics, with around 2% gain attributed to our proposed tokenization. Additionally, when evaluated on the EHRSHOT public benchmark with a 1/1000 fraction of the pre-training data, the UniStruct model improves performance on over 42% of the downstream tasks. Our approach not only enhances the representation and generalization capabilities of patient-centric models but also bridges a critical gap in representation learning models' ability to handle complex structured medical data, alongside unstructured text.", "sections": [{"title": "Introduction", "content": "In medical and healthcare domain, the extensive use of both unstructured text and structured alphanumeric data, such as medical codes, is prevalent for recording patient diagnoses, procedures, and"}, {"title": "Related Work", "content": "In this section, we review the closely related literature on structured data representation, the treatment of LLMs on medical codes and the recent foundation models on medical data and identify their limitations which we address through our work.\nStructured Data Representation. The representation of structured data in healthcare domain, particularly medical codes such as ICD-10, remains a significant challenge due to the hierarchical and compositional nature of these codes, while being elementary in their definition [26, 4, 18]. Even the recent LLMs have struggled to effectively handle the unique requirements of such data [21, 8, 14]. [10] highlight similar issues in the domain of continuous numerical data, where standard tokenization methods like Byte Pair Encoding (BPE) [19] often fail as these would potentially split non-aligned tokens leading poor numerical operations. To address this, [10] propose a custom tokenization strategy that separates digits by spaces, treating each digit as an individual token, which significantly improves model performance in time series forecasting. This points towards a fundamental issue: both structured medical codes and continuous numerical data require specialized tokenization and representation designs to ensure the model's ability to accurately learn and predict complex patterns.\nLLMs on Medical Codes. LLMs like GPTs [3] have shown impressive capabilities in natural language processing but have revealed significant limitations when applied to tasks on medical coding, particularly with structured data like ICDs codes. [8] demonstrate that these models often produce incorrect predictions due to their inability to fully comprehend the hierarchical and compositional aspects of medical codes. This challenge is analogous to the difficulties observed in processing continuous numerical data [10]. Similar limitations are identified in [21, 14] calling the need for a specialized representation of medical codes, which we primarily attend to in this work.\nFoundation Models on Structured Medical Data. Foundation models have been increasingly applied to healthcare domain due to their potential to unify both structured and unstructured data. MedPaLM [20], for instance, is a large-scale pre-trained and fine-tuned LLM specifically trained on unstructured medical text. The EHRSHOT benchmark, introduced by [27], represents the largest publicly available database of longitudinal patient records, serving as a critical evaluation framework for these models. In this context, the CLMBR-T-base model [22] has shown promise in handling structured data, outperforming traditional baselines across multiple EHRSHOT downstream tasks. However, this model processes medical codes as individual elements with a feature processing stage, which limits its ability to effectively capture the co-occurrence and relational dynamics of multiple medical codes that together describe a patient's condition, analogous to how subword representation captures linguistic patterns in LLMs.\nIn this paper, we address the challenges of structured data representation, particularly in patient-centric longitudinal settings where LLMs have been found to underperform. We develop a foundation model that surpasses the capabilities of existing architectures, as in [22, 27], offering enhanced representation of patient structured data."}, {"title": "Proposed UniStruct Foundation Modal", "content": "In this section, we introduce our framework designed to address the challenges associated with representing structured medical data, specifically alphanumeric medical codes like ICD-10 codes [26], SNOMED codes [6] in large language models (LLMs). Our framework integrates structured data (e.g., medical codes) and unstructured data (e.g., clinical text) modalities, with a custom tokenization approach especially for structured medical codes. The tokenization process, as illustrated in Figure 2 is adapted from subword tokenization methods such as BPE [19, 2], allowing it to effectively represent the frequent co-occurrences and relationships inherent in patients' medical visits.\nThis inductive bias concerning the structured data enables the model to efficiently compress and consolidate the representation of a certain medical condition as the codes surrounding a condition has a high likelihood of being assigned together to a patient during a visit, as well as in future visits in conditions with long term occurence. We then pre-train a Transformer neural network [24, 16] on both data modalities constructured as respective sequences on large patient databases, both internal and public, and finetune on downstream evaluation tasks to evaluate the representations learnt.\nTokenization for Structured Data\nA key innovation of our framework lies in the specialized tokenization process we developed for structured medical codes, leveraging the fact that the modeling of statistical co-occurences of textual units in LLMs are fundamental to their represenational power. Unlike the direct use of traditional tokenization methods optimized for natural language processing [14], our approach acknowledges the unique properties of medical codes, which represents a combination of alphanumeric elements and are as atomic as characters in texts.\nThe tokenization process begins by preprocessing the structured data, which includes ICD codes, exam codes, drug codes, SNOMED codes, and other medical identifiers from the patient's history. This data is analyzed to identify frequent co-occurrences and patterns that are critical for accurate representation. We employ Byte Pair Encoding (BPE) to segment the structured data into meaningful subwords or tokens. This method allows the model to capture both individual code elements and their frequent combinations, which are often indicative of common medical conditions. For instance,"}, {"title": "Model Architecture", "content": "Our framework employs a dual-modality transformer network, as shown in Figure 3, to integrate structured medical data with unstructured clinical text in one pipeline. In a data setting where no text records are available, such as on EHRSHOT[27], we only use the structured data Transformer.\nText Modality Transformer. For the text modality, we utilize standard language models, such as BERT-based models [5, 17]. This Transformer is pre-trained on clinical text notes and is responsible for processing unstructured text data, generating embeddings that capture the semantic information of the current visit's medical records. One might question why past clinical texts are not used. The reason is that structured medical codes offer better representations of past visits and compress the information that is comprised in a corresponding past visit.\nStructured Data Modality Transformer. For the structured data modality, we use a separate Transformer which uses the custom tokenizer discussed earlier. It is pre-trained on structured history data using a Causal Language Modeling (CausalLM) objective [16], allowing it to learn the distribution and relationships of these tokenized representations. In other words, analogous to text pre-training, the longitudinal history of a patient can be understood as one sentence, where each visit is a word, and the visit consists of assigned medical codes which are the characters making up the words. As such, the sequential structure of a patient timeline is naturally encoded."}, {"title": "Downstream Prediction", "content": "The embeddings from both transformers are combined to create a comprehensive representation of the patient's medical state, including the current visit as well as past history. This combined representation is then utilized for various downstream tasks, such as predicting new assignments, diagnoses, or operational outcomes [27]. We adopt a logistic regression head for the downstream prediction."}, {"title": "Numerical Experiments", "content": "We now present our numerical experiments to evaluate the proposed UniStruct architecture on (i) a large scale internal dataset, and (ii) a public benchmark of longitudinal patient history, EHRSHOT [27]. We first describe the experimental setup including the details on the datasets, then show the results on various downstream tasks, including a New Medical Code Assignment task on the internal dataset, and 15 downstream tasks on the public EHRSHOT benchmark, and finally discuss our findings.\nDatasets and Setup\nInternal Data. We utilize an internal dataset from Cheng Hsin General Hospital, comprising of over 10 million records of over 765,000 patients multiple visits averaging 16.25 visits per patient. Consequently, each record of a patient, as shown in the de-identified sample below, includes a longitudinal timeline of visits, along with structured data such as ICD-10 codes, drug codes, and other medical identifiers for every visit. Additionally, we source the clinical text associated with these"}, {"title": "Results", "content": "We report the numerical results of our experiments on the internal dataset in Table 1 and Figure 4, while that on the EHRSHOT benchmark in Figures 5 and 6. Table 1 presents the comparison of different model settings on a New Medical Code Assignment task using the internal dataset, highlighting the performance metrics (Recall@5 and Recall@7) for models with and without structured patient"}, {"title": "Discussion", "content": "The results from our internal evaluation highlight several key insights into the effectiveness of our proposed UniStruct model in handling structured medical data, particularly through the inclusion of patient history and the application of a custom tokenization strategy. On the public evaluation on EHRSHOT benchmark, despite the small pre-training corpus used by UniStruct, it outperforms or performs comparably on several downstream tasks.\nInternal Evaluation\n\u2022 Impact of Patient History: The inclusion of patient history in our model has shown a significant improvement over the baseline, which did not incorporate historical data. Specifically, we observed up to a 23% enhancement as reported in Table 1 in absolute\nRecall@K scores for the predicting new medical code assignments. This finding is consistent with prior studies [27] that emphasize the importance of leveraging patient history for better generalization in downstream tasks.\n\u2022 Advantage of Proposed Tokenization: Our comparison between element tokenization, as discussed in Section 3.1 which does not treat multiple medical codes during a condition as one single entity, and BPE tokenization reveals a notable performance gain. The use of a subword tokenization for structured data, which captures the relational dynamics of multiple medical codes representing the same patient condition, resulted in approximately a 2% improvement in performance as illustrated in the same Table 1. This suggests that BPE tokenization more effectively preserves the context and relationships inherent in medical codes compared to treating each code as an isolated element, as seen in models like CLMBR-T-base on the EHRSHOT benchmark.\n\u2022 Cases by Individual Departments: A more detailed analysis of specific cases by individual medical departments, such as those involving long-term heart conditions in Cardiovascular departments, as shown in Figure 4, reveals that our proposed UniStruct model demonstrates a 42% improvement over the baseline, which did not account for patient history modeling. Significant improvements are also observed in departments like Metabolism and Gastroenterology, where patients often have long-term or high-risk factors, making historical context particularly valuable [9]. In contrast, the more moderate improvements in Neurology, Otolaryngology, and Orthopedics suggest that while patient history is somewhat beneficial, its impact may not be as pronounced due to the nature of the diagnoses in these departments. Overall, the results highlight the advantages of our approach for conditions with long-term characteristics. On the other hand, the Emergency Department, which handles patient cases related to emergency visits, shows negligible improvement when patient history is considered. This is understandable, as the triaging process in emergency situations is often preliminary, and a patient's longitudinal timeline may not significantly contribute to predicting future emergency medical code assignments.\nPublic Evaluation - EHRSHOT\nDespite our foundation model being pre-trained on an approximately 1/1000 fraction of the patients compared to the CLMBR-T-base model in [27], i.e., around 2.3K compared to 2.57M patients, our approach either improves or performs comparably (within 0.01 margin) on around 42% of the tasks (6/14) which is the primary group of downstream tasks we target.\n\u2022 Positive Results: Figure 5 presents a performance comparison across various tasks. Among the three operational outcome tasks shown in Figure 5a-ICD Admission prediction, long length of stay (LOS) prediction, and 30-day readmission prediction our proposed model significantly outperforms others on the first two tasks. This validates the efficiency of our UniStruct's tokenization in representing structured data. In Figure 5c, UniStruct surpasses CLMBR-T-base in predicting the assignment of Acute MI (Myocardial Infarction) and Celiac, while performing comparably in predicting Hyperlipidemia and Pancreatic"}, {"title": "Conclusion", "content": "In this paper, we proposed a novel architecture designed to address the challenges associated with representing structured medical data, particularly medical codes, within language modeling-like framework. By integrating structured and unstructured data modalities and employing a custom tokenization approach adapted from subword tokenization methods like Byte Pair Encoding (BPE), we significantly improve the representation and generalization capabilities of patient-centric models. Our experiments, conducted on both an internal dataset and the public EHRSHOT benchmark, demonstrate that our method not only enhances model performance in tasks involving long-term patient conditions but also validates the effectiveness of our tokenization strategy in capturing the relational dynamics of medical codes. Despite limited pre-training data compared to what was used in existing models like CLMBR-T-base, our approach achieved comparable or better results across various downstream tasks on the EHRSHOT benchmark.\nLimitations and Future Directions.\nWe believe the proposed UniStruct architecture, through its promising results on structured data modeling in this work, can advance multiple directions to develop more effective foundation models for multimodal settings with structured data as well as for use in the healthcare domain. First, there is the challenge of further unifying structured and unstructured data modalities, as the integration used in our work maintains two separate representation spaces for the two modalities considered. Next, the transfer of structured knowledge across different databases presents difficulties, particularly when dealing with varying internal structures and compliance requirements, which hinder the effective use of pre-trained tokens. Technically, the transfer of pre-trained knowledge from one database to another involves overcoming stark differences in the unique codes that may occur in individual databases. A recent work by [15] explores zero-shot transfer of tokenization, which can be relevant in such scenarios. Additionally, adapting the capabilities of state-of-the-art LLMs to our structured data representation is another challenge, as current approaches often require separate pre-training for each"}]}