{"title": "Neural Network Plasticity and Loss Sharpness", "authors": ["Max Koster", "Jude Kukla"], "abstract": "In recent years, continual learning-a prediction setting in which the problem environ- ment may evolve over time has become an increasingly popular research field due to the framework's gearing towards complex, non-stationary objectives. Learning such objectives requires plasticity, or the ability of a neural network to adapt its predictions to a different task. Recent findings indicate that plasticity loss on new tasks is highly related to loss landscape sharpness in non-stationary RL frameworks. We explore the usage of sharpness regularization techniques, which seek out smooth minima and have been touted for their generalization capabilities in vanilla prediction settings, in efforts to combat plasticity loss. Our findings indicate that such techniques have no significant effect on reducing plasticity loss.", "sections": [{"title": "1. Introduction", "content": "The traditional supervised learning setup is that in which a model learns a function which is invariant over time and model training occurs once. However, in various learning domains, the function may be non-stationary. In a discrete-time setting, a non-stationary objective consists of training a neural net on some series of related, yet distinct tasks.\nThis is referred to this as a continual learning setting, where the model faces a changing stream of data to learn (Wang, Zhang, Su, & Zhu, 2023). Consider an email spam classifier: the vocabulary of spam emails changes over time, and would thus require re-training from scratch periodically in a train once setting. Instead, in a continual learning setting, the model would continue training and simply adapt to changes in spam trends.\nNaturally, the central objective of a continual learning model is to maintain performance throughout various tasks. The decreased ability of a neural network to perform accurately on later tasks is described as a loss of plasticity. The loss of plasticity in current deep learning models was observed as early as the first deep networks and continues to persist in modern deep learning (Dohare, Hernandez-Garcia, Rahman, Sutton, & Mahmood, 2023).\nRecent works have focused on investigating causes of plasticity loss and to possibly discover various techniques to counteract such loss. One area of interest is the network's loss environment and more specifically the loss sharpness, which can be quantified through the maximal eigenvalue of the loss's Hessian matrix. Lyle et al. found that the maximal eigenvalue increases over tasks; that is, sharpness of minima increases as more tasks are presented to the network (Lyle, Zheng, Nikishin, Pires, Pascanu, & Dabney, 2023).\nIn this paper, we introduce the usage of sharpness regularization techniques to prevent plasticity loss in non-stationary functions. Such techniques bias networks to find 'flatter' minima. These methods have been found to improve generalization ability on traditional supervised learning tasks. Our usage of sharpness regularization techniques is clearly motivated by the results of (Lyle et al., 2023)."}, {"title": "2. Previous Work", "content": "Beyond these recent empirical results, it is also reasonable to posit that if a minima of task A has greater generalization ability, it will be more transferable to task B; thus, we should seek such minima."}, {"title": "2.1 Flat Minima", "content": "The relationship between flat minima and generalization ability has become well-examined in recent years. The actual mathematical formulation of what a 'flat minima' is can differ in various literature, but it is unequivocally thought of as a region where loss remains relatively constant. Here, we present a simple approach via Lipschitz continuity. Recall that a function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ is Lipschitz continuous everywhere if $\\forall x_1, x_2 \\in \\mathbb{R}^n$ there exists a Lipschitz constant K such that:\n$|| f(x_1) - f (x_2)||_2 \\leq K \\cdot ||x_1 - x_2||_2$\nSelection of the Lipschitz constant is typically the minimum such value for which the equality holds. Also, Lipschitz continuity can be described over some local neighborhood $V(x)$. In this case, the value K is an upper bound for the perturbation or change in f(x) in some epsilon neighborhood of x. This is a clear framework to quantify the 'sharpness' of a local minima.\nRecently, there has been progress in developing optimization schema which explicitly seek out flat minima. Sharpness-Aware Minimization (SAM) minimizes loss sharpness by penalizing the loss in some e neighborhood of w (Foret, Kleiner, Mobahi, & Neyshabur, 2020). This is formalized as the following minimization:\n$\\min_w L^{SAM}(w) + \\lambda ||w||_2$\nwhere\n$L^{SAM}(w) = \\max_{||\\epsilon||_2\\leq\\rho} L_s(w + \\epsilon)$\nGradient Norm Penalty (GNP) instead explicitly penalizes the gradient of the loss $L_s(w)$ (Zhao, Zhang, & Hu, 2022):\n$L(w) = L_s(w) + \\lambda ||\\nabla L_s(w)||_2$\nThe reasoning follows from the previously presented Lipshitz definition. We first recall the Mean Value Theorem from real analysis: if f(x) is differentiable on [a, b], then there exists $c\\in [a, b]$ such that $\\nabla f(c)(a \u2013 b) = f(a) \u2212 f(b)$\nNow consider an arbitrary minimum w and some neighborhood V(w). Then $\\exists \\zeta \\in V(w)$ such that $\\forall w_i \\in V_{\\epsilon}(w)$,\n$||\\nabla(\\zeta)(w \u2013 w_i)||_2 = ||f(w) \u2212 f(w_i)||_2$\nThen it follows from the Cauchy-Schwarz inequality that\n$||f(w) \u2212 f(w_i)||_2 \\leq ||\\nabla(\\zeta)||_2 ||w \u2013 w_i||_2$"}, {"title": "2.2 Plasticity", "content": "The factors which cause plasticity of neural networks are still poorly understood; this is an active research area. Some explored potential causes include feature inactivity, increased weight norm, and low rank of features (Lyle et al., 2023). Considerable attention is also given to a multitude of proposed mechanisms to prevent plasticity loss, including layer nor- malization, weight decay, Shrink and Perturb and spectral normalization.\nPast empirical work have introduced spectral normalization techniques to deep reinforce- ment learning problems such as Atari. Said techniques similarly exploit the relation between the gradient norm and Lipshitz continuity. However, rather than modifying the loss func- tion, they use a typical loss function but project parameters into a subspace with a desired spectral radius after each gradient step. (Gogianu, Berariu, Rosca, Clopath, Busoniu, &\nPascanu, 2021) We also note that said technique was solely applied to specific network layers rather than the whole network. To our knowledge, our work represents the first attempt to directly apply neural network sharpness regularization techniques to the continual learning setting."}, {"title": "2.3 Continual Learning", "content": "Continual Learning is a very broad problem setting with many subcategories. We formalize this notation by describing all data $D = \\{(x_1,y_1), (x_2,y_2), ..., (x_n, y_n)\\} \\subset \\mathbb{R}^d \\times Y$ with the total set of labels $y \\in Y$. Similar to (Wang et al., 2023), we define an incoming batch b of a prediction class as $D_b = \\{X_b, Y_b\\}$, where $X_b$ is the input data, $Y_b$ is the data label and $b\\in B_t$ is the batch index. Though many various problem settings are discussed in literature, we now expand on two particularly popular and well-known ones: Class-Incremental & Domain-Incremental Learning."}, {"title": "2.3.1 CLASS-INCREMENTAL LEARNING", "content": "Class-Incremental Learning is a classification problem setting in which additional classes are incrementally added. That is, each new batch $D_b$ introduces or removes some class. each Db is composed of points drawn from some specific subset $S \\subseteq y$, or alternatively $D_b = \\{(x_1, y_1), (x_2,y_2), ..., (x_n, y_n)\\} \\subset \\mathbb{R}^d \\times S$."}, {"title": "2.3.2 DOMAIN-INCREMENTAL LEARNING", "content": "Domain-Incremental Learning is a classification problem setting in which the classes stay the same, but the underlying distribution of a class may shift. That is, for each additional batch we still have $D_b \\subset \\mathbb{R}^d \\times Y$, but each batch is drawn from distinct distributions $P_b(x_i, y_i)$. Our work aims to explore both of these types of continual learning."}, {"title": "3. Experimental Setup", "content": ""}, {"title": "3.1 Data", "content": "The MNIST database is a large set handwritten digits that is widely used in training and testing machine learning systems. It contains 60,000 training images and 10,000 testing images. Each image is 28 \u00d7 28, where each pixel can take on a value from 0 (black) to 1 (white).\nIn order to create a domain-incremental learning problem, we took the original version of this dataset and randomly permuted the pixels ($10^{1930}$ possibilities) to create a single learning \"task,\" preserving the integrity of both the training and test sets with the creation of each task. Here, a randomly permuted dataset is a task.\nIn order to create a class-incremental learning problem, we took the original version of the dataset, and extracted pairwise class data, taking care to maintain the 6:1 train-to-test ratio. A \"task\" here might be distinguishing the digit 1 from the digit 2, or the digit 7 from the digit 3. That is, a dataset containing two classes is a task."}, {"title": "3.2 Classifier", "content": "In both of these learning tasks, we replicate the techniques of Dohare et al. (Dohare et al., 2023) and used a simple feed-forward neural network. Our architecture consists of one 28 \u00d7 28 input layer, followed by three fully connected linear layers with ReLU activation,"}, {"title": "3.3 Results", "content": "We devise a set of 6 training settings in order to evaluate the performance of both SAM and GNP relative to SGD in both learning problems: SGD with \u03b1 = 0.01, SGD with \u03b1 = 0.001, SAM with \u03b1 = 0.01, SAM with \u03b1 = 0.001, GNP with \u03b1 = 0.01, and GNP with \u03b1 = 0.001. Note that we set \u039b = 0.1 for both the SAM and GNP models. We train our network 10 times each in these training settings and report the mean task-specific test accuracy after each task.\nIt is important to note that in both learning problems, t. This is integral to evaluating true plasticity in our network."}, {"title": "3.4 Domain-Incremental Learning", "content": "We plot the task-specific test accuracies (averaged over 10 runs) for each training setting below on 100 randomly selected domain-incremental learning tasks. Additionally, we report the average change in test accuracy over the 100-task training period"}, {"title": "3.5 Class-Incremental Learning", "content": "We again plot the task-specific test accuracies (averaged over 10 runs) for each training setting below on the $\\binom{10}{2} = 45$ possible tasks. Additionally, we report the average change in test accuracy over the 45-task training period"}, {"title": "4. Discussion", "content": "Overall, our results demonstrate that sharpness regularization techniques like SAM and GNP does not decrease-and instead may worsen-plasticity loss in continual learning set- tings.\nIn the domain-incremental learning experiments, all training methods showed to perform with similar accuracy over 100 tasks. SGD with \u03b1 = 0.001 performed the best in terms of maintaining performance, with a slight positive per-task accuracy change on average. SAM and GNP with higher learning rates showed noticeable declines in per-task accuracy over tasks.\nIn the class-incremental learning setting, SGD achieved stable per-task accuracy changes close to zero. In contrast, SAM showed significant declines in per-task accuracy, indicating substantial plasticity loss. GNP proved most effective at maintaining plasticity in this setting, with almost no change in per-task accuracy across the 45 class-incremental tasks."}, {"title": "4.1 Conclusion and Future Work", "content": "Overall, our findings show that applying sharpness regularization alone to promote neural network plasticity will not effectively work. There are several possible explanations for this result. First, there is some question on the relevance of the Permuted MNIST dataset as an adequate example of continual learning (Farquhar & Gal, 2019). Further empirical studies with different continual learning tasks can provide indication on whether this is the case. There is also the possibility that despite the empirical observations (Lyle et al., 2023) of the relation between sharpness and plasticity, this is not causal and there exists some (currently) unknown confounding factor.\nKey areas for future work include testing on more complex benchmark datasets as well as reinforcement learning problems with non-stationary reward functions over time. Adap- tations to the sharpness regularization terms may also further improve continual learning performance\u2014there is still much of the parameter space to be explored."}]}