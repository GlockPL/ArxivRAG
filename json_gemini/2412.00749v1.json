{"title": "MERLIN: Multi-stagE query performance prediction for dynamic paRallel oLap pIpeliNe", "authors": ["Kaixin Zhang", "Hongzhi Wang", "Kunkai Gu", "Ziqi Li", "Chunyu Zhao", "Yingze Li", "Yu Yan"], "abstract": "High-performance OLAP database technology has emerged with the growing demand for massive data analysis. To achieve much higher performance, many DBMSs adopt sophisticated designs including SIMD operators, parallel execution, and dynamic pipeline modification. However, such advanced OLAP query execution mechanisms still lack targeted Query Performance Prediction (QPP) methods because most existing methods target conventional tree-shaped query plans and static serial executors. To address this problem, in this paper, we proposed MERLIN a multi-stage query performance prediction method for high-performance OLAP DBMSs. MERLIN first establishes resource cost models for each physical operator. Then, it constructs a DAG that consists of a data-flow tree backbone and resource competition relationships among concurrent operators. After using a GAT with an extra attention mechanism to calibrate the cost, the cost vector tree is extracted and summarized by a TCN, ultimately enabling effective query performance prediction. Experimental results demonstrate that MERLIN yields higher performance prediction precision than existing methods.", "sections": [{"title": "1 INTRODUCTION", "content": "OnLine Analytical Processing (OLAP) databases are a type of database system designed specifically to support complex queries and large-scale data analysis. With the development of OLAP DBMSs, there were already many advanced execution mechanisms proposed to improve OLAP DBMS's execution speed, such as applying the bulk (chunk) processing to speed up queries used by PostgreSQL and MonetDB [4]. Some advancing OLAP DBMSs such as ClickHouse [25], SparkSQL [18], Databricks [36], and DuckDB [24] pushed the OLAP query processing technology to a new level by introducing a dynamic pipeline instead of using a conventional static query execution engine. Some of them such as ClickHouse and SparkSQL go a step further and use a highly parallel DAG-based pipeline structure instead of the tree-shape physical plan executed by a Volcano-style engine. Such huge and unique modifications to the query plan model and engine lead to many new demands for components like the cost estimator and query optimizer etc. As shown in Table 1, we refer to such OLAP DBMSs support all the mechanisms as high-performance OLAP DBMSs and focus on the learned QPP method toward them."}, {"title": "C.1 SIMD Operators' resource prediction.", "content": "More and more DBMSs [4, 11, 25] use SIMD instructions to implement physical operators, whose cost variance is larger under different input scales and system resource utilization rates, and their resource costs are more difficult to predict but also important to the overall query's performance prediction. Common methods of predicting query performance end-to-end used by most existing approaches cannot capture each operator's cost effectively enough. A new process needs to be proposed to make a more fine-grained estimate of operator resource overhead."}, {"title": "C.2 Dynamic pipeline extraction and expression.", "content": "Some DBMSs [18, 25, 36] employ runtime optimization to alleviate issues such as inaccurate estimation of key information such as cardinality before querying, or changes in data distribution and system load during runtime. Among all these DBMSs, ClickHouse uses the most unique mechanism that allows dynamic modifications on the pipeline itself. For example, ClickHouse will add extra operators during execution, and change the algorithm of operators such as sort and join. In summary, there are significant differences between ClickHouse and other databases, and existing QPP methods are not designed for such a unique framework. Such a mechanism makes it inaccurate to use the pipeline extracted before the query, and it is also difficult to express any dynamic pipeline extracted to a form that can be easily processed by learned QPP methods. It's difficult to extract a low-level runtime dynamic pipeline that is detailed enough to support the QPP task. Also, even if such information can be extracted, it is still difficult to process with low cost by learned QPP methods, especially when the pipeline is in spatiotemporal graph form since the inference on spatiotemporal graph networks is expensive."}, {"title": "C.3 Parallel execution and resource competition.", "content": "DBMSs such as ClickHouse and SparkSQL employ parallel pipeline executors, which introduces more complex resource competition within a single OLAP query. Taking ClickHouse 1 as an example, its parallel execution of ClickHouse consists of two parts, horizontal and vertical parallel. The horizontal parallel execution means that different parts of data are processed in parallel by the same computation. ClickHouse achieves this by widely using SIMD instruction sets like SSE and AVX 2 and creating multiple execution branches (threads) for operators like MergeSorting to speed up queries. Such operators' costs are highly related to the system workload and much harder to predict within an end-to-end model. The vertical parallel execution means that different stages of computation are run in parallel. To fit such a design and speed up queries, different from the widely used volcano model, ClickHouse's query plans are constructed as parallel pipelines\u00b3, resulting in significant resource competition among operators within a single query plan, including competition for CPU, I/O, and memory resources. Existing methods focus on common databases with tree-like query plans, how to capture those resource competition information and calibrate operators' costs still remains to study."}, {"title": "In this paper, we propose a multi-stage QPP method MERLIN", "content": "to tackle the challenges above and give an accurate and efficient prediction, which decouples the operator cost prediction, cost calibration, and query performance prediction. The core idea of MERLIN to achieve high accuracy for high-performance OLAP DBMSs is first to obtain detailed low-level physical pipeline information that includes dynamic pipeline changes and then to use multi-stage models to predict the resource cost of SIMD operators separately, calibrate the resource competition, and summarize the overall pipeline latency. For challenge C.1, MERLIN decouples operator cost prediction from query performance prediction. We establish lightweight cost models (a.k.a operator cost predictors (OCPs)) for each operator to predict the resource and time cost of each operator's execution as Stage 1. These individual operator cost vectors are then aggregated into a graph for subsequent processing. For challenge C.2, MERLIN includes a Runtime Tracker to extract training data and necessary dynamic information for prediction. To collect training data of the OCPS, MERLIN uses a novel serial executor and the tracker runs on the full-collection mode that marks all data chunks by their addresses. For query-level training data of training and prediction, considering high-performance OLAP DBMSs' dynamic parallel execution mechanism and inspired by ClickHouse's probe phase [25], the tracker uses a probe execution mode which only launches a few chunks to obtain operator features and the execution paths, which usually contain more operators than the initial pipeline. For dynamic pipeline expression, MERLIN uses the data-flow tree rather than the pipeline DAG as the backbone of the data structure. MERLIN includes a Graph Constructor to build the resource competition graph with the backbone data-flow tree and corresponding operators' cost vectors based on the execution paths of chunks collected by the Runtime Tracker. Therefore, the chunks' execution information generated by the modified pipeline can be reserved in different execution paths of the data-flow tree without mixing up."}, {"title": "2 RELATED WORKS", "content": "The most common method is to build cost models for different operators before learned methods are introduced into the database system. Such methods employ the CPU cost, I/O cost, etc. of operators to estimate the query cost. Manegold et al. proposed a series of cost functions to different memory access patterns [20]. These functions are parameterized to fit different hardware features. The operators' cost can be inferred from the combination of those cost functions. Feilong Liu et al. proposed a memory I/O cost model [17] to identify good evaluation strategies for complex query plans with multiple hash-based equip-joins over memory-resident data. Wen-tao et al. proposed a cost model [35] that represents the query latency as a function of the selectivities of operators in the query plan as well as the constants that describe the cost of CPU and I/O. They solve the coefficient of the cost model from the observed query information. Wu et al. [33] proposed an offline profiling method to set the coefficients of a cost model for specific hardware and software conditions. Such methods have two main problems, the first problem is that the parameters of the cost models are hard to adjust properly to maximize accuracy. Although there are some methods [30, 35] proposed to adjust or solve the parameters, it's not accurate enough for dynamic workload environments since the ground truth costs change with the hardware workload. The second problem is that some research [13, 27] show that the cardinality estimation results are crucial to those cost models, which is another harder problem that has much higher error usually. One important reason is that compared to learned models, conventional methods' prediction results highly depend on the input scale and lack of robustness like learned methods."}, {"title": "2.2 Learned Methods", "content": "Before the raising of deep learning, some studies used machine learning to predict query performance. Ganapathi et al. take plan-level information into consideration and use KCCA [3] to predict multiple performance metrics for a single query[6]. This method can support both short and long-time-running queries [9]. Li et al. [14] train a boosted regression tree for every operator and estimate the cost of the whole plan by scaling and adding the operators' cost. Akdere et al. proposed two QPP models [1]. The first is a plan-level model which takes the input of the query plan information including numbers and cardinality of each operator and predicts the overall query performance. The second is an operator-level model which predicts the operator's cost based on the feature of the operator itself and the cost of its children operators. Then they combined these two methods by using the plan-level model to deal with the materialization of the sub-query plan and the operator-level model to others, in order to avoid the high error of the operator-level models on the materialization sub-plan. In the area of deep learning-based QPP methods, Marcus et al. proposed NEO [21] and QPPNet [22]. The NEO [21] captures relationships between physical operators and their children operators, they do not effectively capture resource competition among operators across different sub-trees. The QPPNet employs one deep learning-based latency model for each operator. Each model accepts the latency, data, and query features of its children operators and other features and then predicts the latency of its operator. Then it dynamically combines the networks corresponding to all operators based on the structure of the query plan tree and passes data features, query features, and latency information between operators to estimate the overall query latency. It's a flexible method that can adapt to various tree-structured query plans. However, it has a very high cost to build the network for each plan and an insufficient training throughput due to the heterogeneous network architecture makes it hard to take full advantage of GPU's data parallel capabilities. Ji Sun et al. proposed an end-to-end learned cost estimator [27] which includes carefully designed complete query representation and feature engineering by vectorizing encoded physical query plan trees containing predicate information and uses a tree-LSTM model to perform end-to-end prediction of query costs. This method is also designed to tree-shaped physical plan. Yue Zhao et al. proposed QueryFormer [38], a variant of Transformer that can capture tree-structure data modal more effectively, which can be used for tasks like QPP. Lin Ma et al. proposed MB2 [19], a two-level QPP method that considers concurrent operators. They first build a lightweight resource cost model for each operator, then use an interference model that takes inputs of the estimated cost vector of operators and predicts the query performance. Compared to previous methods, this"}, {"title": "3 OVERVIEW", "content": "As introduced in section 1, MERLIN decouples the cost prediction process of operators and the whole query. Besides the OCPs, such workflow requires the method to collect detailed runtime resources-cost, time-cost information, and operators' features to train those OCPs. It also includes a multi-stage model to summarize each operator's cost of the query plan and predict the total query latency. As shown in Figure 1, MERLIN contains three stages: Stage.1 Operator Cost Prediction. As the Stage 1, it responds to estimate each SIMD operator's cost by the OPCs, a set of lightweight neural networks responsible for predicting the corresponding operators' resource costs. Each model is trained on the operator cost data collected by the Runtime Tracker. They take the system overhead and operators' calling information as input and output the predicted cost vector of operators. Note that the cost vector is not limited to latency only but includes the hardware resource overhead the operator is expected to consume without considering the impact of other concurrent operators. For a given database software and hardware environment, compared to end-to-end models, OCPs can provide better generalization performance across different query plans. Finally, the cost vectors and vertice features are mapped by FFN separately and added together for later cost calibration. Stage.2 Cost Calibration. In Stage 2, the Cost Calibrator first uses the Graph Constructor to merge execution paths collected in the probe execution mode of the Runtime Tracker into a data-flow tree as a rich expression of the query plan. Then it uses an attention mechanism to adjust each operator's resource competition weight based on both the meta-competition matrix and the data-flow tree. After that, the Cost Calibrator uses a GAT to calibrate each operator's cost vector based on the DAG and predicted operators' costs during parallel pipeline execution."}, {"title": "Stage.3 Query Performance Prediction.", "content": "In Stage 3, we combine the calibrated cost vectors and the structure of the data-flow tree into a vector tree and use a TCN model to summarize every operator's cost vector from the bottom up and predicate the total latency of the whole query. It should be noted that the entire process during Stages 2 and 3 is differentiable, thus we can pass the gradient back to the GAT network and train the GAT and TCN models together. To support the three stages above, MERLIN also includes a Runtime Tracker, which can collect necessary data for training and inference. The Runtime Tracker made some modifications to the common components like the physical operators, data chunks, and executors. We will introduce it in detail in section 4. In the aspect of the training phase, the Runtime Tracker first collects each operator's features and performance information by executing queries with a serial executor under the full-collection mode, which is either provided by the OLAP DBMS or modified from the OLAP DBMS's parallel executor by the Runtime Tracker. Then it collects the query plan and the whole query's latency with the DBMS's parallel executor under the probe execution mode. Next, we use the collected training data to train the OCPs. The last step of training is to use the predicted cost vectors and other runtime pipeline data to train the Cost Calibrator and the Query Performance Predictor together. In the aspect of the prediction phase, we only need to set the Runtime Tracker to probe execution mode, which uses the DBMS's parallel executor for a fast probe execution to obtain each operator's runtime information, the chunks' execution paths, and the system overhead. Then we use the three stages, i.e., Operator Cost Prediction, Cost Calibration, and Query Performance Prediction, sequentially to predict the query's latency. In the rest of this section, we will introduce MERLIN's Runtime Tracker, and the three stages will be introduced in section 5."}, {"title": "4 RUNTIME TRACKER", "content": "To develop a physical plan-level performance predictor for high-performance OLAP DBMSs, we need to collect detailed information to train and evaluate the predictor. However, such DBMS's parallel and dynamic execution mechanisms prevent us from using tools like 'EXPLAIN' to support query performance prediction for the following reasons. To make an accurate query performance prediction, it is necessary to acquire detailed operators' runtime information, such as their resource cost and runtime calling parameters. It would be hard to match each operator's information with the pipeline structure given by the 'EXPLAIN' SQL command since most of them employ dynamic execution mechanisms that allow inserting and modifying operators including adding new execution paths and changing the algorithm used by operators such as sort and join. Taking ClickHouse as an example, it involves dynamic modifications of the pipeline structure and dynamically decides on physical operators' calling parameters, such as the join algorithm parameter of the Joining Transform operator, which is hard to foresee during performance prediction."}, {"title": "4.1 Motivation", "content": "Figure 2 shows a typical dynamic pipeline modification scenario if a DBMS with a dynamic execution mechanism (a.k.a adaptive query execution in SparkSQL and Databricks) finds that the pipeline is not the optimal one under the current system workload after launching the query, it will dynamically modify the pipeline by two types of actions: (1) changing the algorithm used by the corresponding operator; (2) changing the structure of the pipeline by inserting new operators. For the first type of action, assuming the DBMS finds that it mistakenly selected merge join instead of hash join algorithm, it will actively modify the join algorithm. Some DBMSs such as ClickHouse achieve this by passing different types of algorithm parameters to the Join operator, the instance of the operator remains the same, but the operator will have different features and costs for most of the execution, which cannot be obtained by simply using the static pipeline. Other DBMSs such as SparkSQL change the algorithm by replacing the corresponding physical operator. In both situations, relying solely on the information provided in a static pipeline is unreliable. For the second type of action, taking ClickHouse as an example, assuming that it decides to switch from internal sorting to external sorting, it will involve adding and replacing operators in the pipeline. Other DBMSs like SparkSQL with the AQE mechanism can even adjust the join order during execution. In such a situation, the execution paths of chunks can be completely changed. Therefore, simply extracting static information from the planner is not enough, the runtime information including calling parameters, pipeline modifications, and chunks execution information is necessary for precise performance prediction. However, the pipeline exported by the 'EXPLAIN' statement not only does not contain any of those modification details, but it also cannot provide any runtime information of operators and chunks. Thus, it is difficult to use for QPP. Although it is impossible to obtain all pipeline modifications during the whole execution process while predicting query performance, inspired by ClickHouse's probe phase [25], we developed a probe execution mode to launch, track, and collect the first several chunks' execution information rapidly and obtain the pipeline structure, operators' runtime features and pipeline modifications happened on the starting of the execution. Considering the dynamic pipeline execution mechanism, those chunks can be generated by different execution paths that correspond to different features and costs. Thus, it would be a time-space graph with unaligned time steps. To collect that information, MERLIN either embeds a variable number of chunks' features from unaligned time slices into each operator of the pipeline or separates those chunks' execution paths by constructing and processing a data flow tree. The former solution needs time-space graph convolution on a graph with many padded zero features, which could lead to resource waste and lower accuracy. Therefore, we chose"}, {"title": "4.2 Design of Runtime Tracker and Data Collection Process", "content": "To explain how the Runtime Tracker collects data, we introduce it from the perspective of data collection for different parts of the models. Data for OCPs. The features and execution costs of each single operator that is used to train the OCPs should be collected offline. Since the OCPs aim to predict the cost of each operator without interference from the parallel execution of other operators, its training data should be collected separately by a serial executor. To collect enough data with as few queries as possible, we run Runtime Tracker in full-collection mode, which collects every operator's runtime features and costs on every data chunk. Although this will cause extra data collecting and training costs compared to using the history workload log, the overhead would not be too high because the OCPs are small MLP networks, and they only need a single-time data collection for a deployed DBMS. Taking ClickHouse's code as an example for the convenience of discussion, we wrap the work() function with our Cost Logger to record the operator's calling parameters, system resource utilization, and resource cost during execution. The Cost Logger uses ioctl library to obtain the CPU instructions, CPU cycles, cache references, and cache misses. It uses the cpuinfo file to obtain the CPU frequency and uses proc/stat to obtain the CPU utilization. The Cost Logger also uses the proc/meminfo file of Linux to measure the memory cost and uses the /proc/PID/io file to measure the I/O read and write cost. After applying these modifications, the serial executor can be regarded as degenerating into a volcanic model. Then we execute workloads and collect each operator's features and costs of executing each data chunk. The data collection is efficient because there are dozens to hundreds of data chunks for each operator in one query. Data for Cost Calibrator. As discussed above, MERLIN solves the dynamic modification of pipeline and calling parameters during cost calibration and query performance prediction by expanding the runtime execution paths in the spatial dimension. This is achieved by tracking the execution path of chunks in the probe execution mode. During offline training data collection, MERLIN will set the executor and the tracker to the probe execution mode and make a quick short execution till the first few result chunks are collected. During this procedure, all intermediate chunks' execution paths and operators' calls are traced, and the potential dynamic modifications are also logged. The Graph Constructor will use this information in Stage 2 to construct a data-flow tree. To track and log this information, we modified the code of the chunk and operators. We use the address of each operator instance as its ID to distinguish them. Once chunks are generated or processed, the corresponding operator will append its ID into the transform_addr list of the chunks. During the processing of chunks, Cost Logger would trace the costs and log them. The address of those cost records will also be added to the chunks' record_addr list. By analyzing those records, MERLIN can construct the data-flow tree for cost calibration and query performance prediction. The detailed construction algorithm will be introduced in subsection 5.2."}, {"title": "5 MERLIN'S QPP MODEL", "content": "In this section, we introduce MERLIN's predictor part in the following order: Operator Cost Prediction, Cost Calibration, and Query Performance Prediction."}, {"title": "5.1 Operator Cost Prediction", "content": "In this section, we introduce the Operator Cost Predictor (OCP) in detail. To predict high-performance OLAP DBMSs' query performance, it is necessary to estimate the resource consumption and the current system resource utilization. Knowing that each operator's resource consumption is the precondition for predicting the resource competition between operators and accurate latency prediction since those DBMSs are designed to maximize resource utilization efficiency with a high degree of parallelism. The OCPs are designed to predict the resource consumption of the given operator without the interference of other concurrent operators. Therefore, we can decouple and predict the cost of a single operator and the cost of the whole parallel query pipeline separately. This can improve the prediction accuracy for physical operators with SIMD instructions and make it easier for MERLIN to generalize between different query templates to reduce the accuracy degradation problem caused by workload drift. The inputs of the OCPs consist of two parts: Resource Utilization and Operator Features. \u2022 Resource Utilization: It contains CPU, memory, and I/O utilization before execution, all OCPs have these three features. \u2022 Operator Features: The operator features include the operator type, calling parameters, and data features. For each operator, we select the parameters that have the most significant influence on their cost as a part of the feature. Some of them are determined by knobs, and some are determined by the executor at runtime. The data feature is provided with the input rows (cardinality) and columns. The information on whether an operator is implemented by the SIMD instruction set will be captured by the OCP model. This type of feature varies depending on the operators and the compile settings. There are two types of features, and we encode them separately. For numerical features, we normalize them during encoding to avoid numerical issues. For categorical features including string-type features and other non-numerical features, we use one-hot encoding to represent them. Since the goal of OCPs is to predict each operator's resource cost, their output is defined as the cost vector that includes the cost of CPU, memory, and I/O. The CPU cost includes the elapsed time, CPU time, CPU cycles, CPU instruction numbers, CPU cache references, and CPU cache misses. The memory cost includes the average and maximum memory consumption. The I/O cost includes the disk block read and write numbers. We use shallow MLP networks as OCPs to achieve a good balance between accuracy and inference speed and train them with the MSE loss function."}, {"title": "5.2 Cost Calibration", "content": "Cost Calibration is a key procedure of MERLIN due to high-performance OLAP DBMSs employing a high degree of parallelism design. Taking ClickHouse as an example, it uses horizontal and vertical parallel execution. Horizontal parallel execution includes two parts, the operators implemented by SIMD CPU instructions and the operators created multiple times to deal data in multiple threads, just like the sort operator in Figure 2. The former is deals with MERLIN's decoupling design, and its costs are predicted by OCPs. The latter involves multi-threading which will be represented in the pipeline structure, and its costs should be calibrated by the GAT model introduced in this section. Vertical parallel execution means ClickHouse keeps traversing the pipeline and parallel executes all operators that can be executed. So every operator in the pipeline can be executed anytime once its input port is ready to send data chunks. This design can significantly accelerate execution, but will also cause widespread competition for CPU, memory, and I/O resources in the whole pipeline. To calibrate those operators' costs predicted by the OCPs, MERLIN needs to know the pipeline structure and the resource competition relation among all operators. As we discussed in section 4, the regular 'EXPLAIN' tool is not enough for pipelines with dynamic modification mechanisms, and MERLIN's cost tracker collects more detailed information including tracked execution paths and other physical plan information with probe execution mode. In stage 2 of cost calibration, MERLIN's Graph Constructor fully uses the tracked execution paths of chunks and constructs a data-flow tree instead of using the DAG pipeline. To understand such data-flow tree construction better, taking the Figure 2 as an example, it shows the pipeline and chunks during the probe execution. Taking ClickHouse's official example in their presentations, assuming that ClickHouse dynamically added and replaced some operators such as MergeSort during the probe execution, then all chunks after this modification would be executed through a different path. Suppose that we simply use the modified DAG pipeline as a representation of the query plan. In that case, all execution information before modification has to be dropped since they cannot match the modified pipeline. Such a solution will cause inaccurate estimation. Besides, such a DAG pipeline is also hard to adapt to existing methods for comparison and hard to support"}, {"title": "Expression", "content": "After being processed using all the methods above, MERLIN now gets the calibrated resource cost vector of each operator in the pipeline. MERLIN then converts the calibrated resource vectors back to the pipeline tree structure and summarizes those vectors from the bottom to the top by a TCN to predict the query performance. Note that all learnable weights in this section are trained by QPP tasks together with TCN introduced in the next subsection."}, {"title": "5.3 Query Performance Prediction", "content": "In this section, we introduce how MERLIN predicts the query performance based on calibrated cost vectors and how to train the calibrator and the predictor together. In subsection 5.2, we express the pipeline as the data-flow tree and each operator's predicted cost vector is calibrated according to the DBMS's operator parallelization mechanism. Considering the high-performance OLAP DBMSs' execution mechanisms, simply adding each operator's cost vector as the final predicted performance is too rough. Instead, MERLIN combines those calibrated cost vectors according to the data-flow tree structure to construct a vector tree. We use a tree convolutional network (TCN) [23] to summarize the cost from bottom to top. The TCN has two main advantages. Firstly, it is powerful to capture tree shape query plan's feature with a low inference cost, which is the reason why it has been widely used in many AI4DB methods [12, 21]. Secondly, the paper of TCN proposed a continuous binary tree mode, which native supports trees with multiple children nodes. Since the data-flow tree is constructed from execution paths of a DAG pipeline, many nodes have more than two children, and TCN can handle such a scenario well. The TCN has a triangle shape convolution filter that is made of three weight matrices $W_{leftconv}$,$W_{middleconv}$ ,$W_{rightconv}$. For node $x_i$ in the window of a filter, its weight matrix is denoted as $W_{conv,i}$, which is a linear combination of those three weights above with corresponding coefficients $\\eta_l$ , $\\eta_m$, and $\\eta_r$. Each layer's output of TCN is calculated by the following formulas [23]:\n$d_{i-1} = d_i \\cdot (1 - \\eta_l - \\eta_m) (6)$\n$\\eta_l = (1 - \\eta_l) \\cdot (1 - \\eta_m)(7)$\n$\\eta_r = \tanh (\\sum_{i=1}^{N}\nW_{conv,i},x_i+b_{conv})(8)$\n, where $d_i$ is the depth of node i in the window, and d is the depth of the window. MERLIN's GAT and TCN networks are trained together in a query-driven way since constructing the calibrated cost vector to the cost vector tree is differentiable. They are trained with the MSE loss function together with normalized ground-truth query latency as the label. To make the model converge more stable, all neural networks are connected by residual connection path [7]."}, {"title": "6 EXPERIMENTS", "content": "In this section, we evaluate MERLIN's performance and compare it with other baseline methods. Since ClickHouse supports all the mechanisms as shown in Table 1, we consider it one of the most representative high-performance OLAP DBMSs. Thus, we implement MERLIN on ClickHouse for evaluation."}, {"title": "6.1 Experimental Settings", "content": "Baselines. Since previous works already applied full comparison with conventional methods such as SVM and RBF [2, 15, 22] and proved learned QPP approaches out-performed conventional approaches on average accuracy, we will focus on the deep learning-based methods proposed in recent years. All baselines compared are listed by following: (1) QPPNet [22]: The QPPNet is an operator-level method that builds a cost model with a unified input and output interface for each operator and combines them according to the structure of the query plan tree to predict the latency. This method is considered to be a flexible model that can adapt to all different plan templates."}, {"title": "6.2 QPP Accuracy", "content": "The results of MERLIN's and baselines' accuracy are shown in Table 2 and Figure 5. On both TPC-H and TPC-DS workloads, MERLIN significantly outperforms all baselines. The Q-Error distribution shows consistent performance gains across OLAP workloads of varying size and complexity. The TCN's Q-Error is quite low on TPC-H but increases a lot on TPC-DS, especially the max Q-Error. We also noticed that both GCN and TCN can reach similar accuracy compared to MERLIN on TPC-H, but their Q-Error gains a lot on TPC-DS. This proves the effectiveness of these two widely used methods. But it also indicates that when it comes to complex OLAP workloads on high-performance DBMSs, such methods lose their accuracy due to a lack of targeted design. On the other hand, MERLIN maintains a stable lead on both workloads. The QPPNet's long-tail distribution problem on TPC-H is indeed serious. We have tried everything that we could such as adjusting its hyper-parameters and switching to better loss functions like LeakyReLU and Swish. This is the best result we can get. We also noticed that, although TPC-DS is a much more complex workload, the max Q-Error of QPPNet was significantly reduced compared to TPC-H. We think the problem is that QPPNet is not a resource-aware approach that cannot model parallel pipeline execution well in certain situations. With enough training templates, it may perform better due to the ability to generalize. However, TPC-H is a workload that only has a few templates, 23.5% of templates used for testing is too high for them. Therefore, when there are serious resource competitions in some queries, QPPNet which is trained on limited TPC-H workloads cannot predict the latency accurately. This result provides indirect evidence that traditional learning methods are difficult to adapt to high-performance OLAP DBMSs and the resource-aware mechanism is crucial in OLAP DBMS like ClickHouse. In order to more intuitively compare the error distribution between different methods, we grouped the methods by latency range and plotted the accuracy box plots for each method since the latency can reflect the complexity of the query. The results are shown in Figure 6. The results show that the Q-Error of the QPP task in different query execution time groups does not show an obvious increasing trend. We think this is because the accuracy of query-driven QPP methods is more affected by the generalization between different templates than by the size of the query plan. The long tail distribution of methods such as QPPNet and TCN is concentrated"}, {"title": "Equation", "content": "The definition of Q-Error is listed as follows:\n$Q-Error = \\frac{max \\{prediction, actual\\}}{min \\{prediction, actual\\}} (9)$"}, {"title": "6.3 Performance Comparison", "content": "The training speed, inference latency, and model size are crucial to the practicality of the QPP approach. We report the performance of MERLIN and compare it with baselines as shown in Table 3. Since TCN is a component of MERLIN we didn't include it for comparison."}, {"title": "6.4 Robustness to Cardinality Errors", "content": "The cardinality errors usually have a significant impact on cost estimation and query performance prediction tasks, as the input scale directly decides the cost. Therefore, we evaluate MERLIN's robustness to cardinality errors in this subsection. Due to the complexity of ClickHouse's pipeline builder and dynamic executor, it is hard to adapt multiple cardinality estimation methods to its kernel. Instead, we generate Q-Errors that follow a log-normal distribution and simulate the estimation result by multiplying it with the true cardinality collected by MERLIN's Runtime Tracker. As shown in Figure 8, the Q-Error distribution of generated cardinality is similar to real estimation with a long-tailed distribution. We assess MERLIN's accuracy changing as the changing of the Q-Error distribution on TPC-H workloads. Due to QPPNet's accuracy being significantly lower than other baselines, we did not include it"}, {"title": "7 CONCLUSION AND FUTURE WORKS", "content": "In this paper, we proposed a novel query performance prediction approach MERLIN for high-performance OLAP DBMSs with SIMD operators and dynamic parallel pipeline. By carefully tracking and collecting data with the Runtime Tracker and employing a multi-stage resource-competition-aware model, MERLIN can capture the resource competition and dynamic pipeline modifications better in such DBMSs. We implemented MERLIN for ClickHouse and adapted three baselines for comparison, the experimental results show that MERLIN outperforms all baselines in accuracy, model size, and inference speed, while its training speed and robustness are also practical. We plan to expand MERLIN and create a cross-DBMS query performance prediction plugin that can automatically adapt different DBMSs."}]}