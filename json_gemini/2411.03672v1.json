{"title": "Towards 3D Semantic Scene Completion for Autonomous Driving: A Meta-Learning Framework Empowered by Deformable Large-Kernel Attention and Mamba Model", "authors": ["Yansong Qu", "Zilin Huang", "Zihao Sheng", "Tiantian Chen", "Sikai Chen"], "abstract": "Semantic scene completion (SSC) plays a pivotal role in achieving comprehensive perception for autonomous driving systems. However, existing methods often neglect the high deployment costs of SSC in real-world applications, and traditional architectures such as 3D Convolutional Neural Networks (3D CNNs) and self-attention mechanisms struggle to efficiently capture long-range dependencies within 3D voxel grids, limiting their effectiveness. To address these challenges, we propose MetaSSC, a novel meta-learning-based framework for SSC that leverages deformable convolution, large-kernel attention, and the Mamba (D-LKA-M) model. Our approach begins with a voxel-based semantic segmentation (SS) pretraining task, designed to explore the semantics and geometry of incomplete regions while acquiring transferable meta-knowledge. Using simulated cooperative perception datasets, we supervise the training of a single vehicle's perception using the aggregated sensor data from multiple nearby connected autonomous vehicles (CAVs), generating richer and more comprehensive labels. This meta-knowledge is then adapted to the target domain through a dual-phase training strategy-without adding extra model parameters-ensuring efficient deployment. To further enhance the model's ability to capture long-sequence relationships in 3D voxel grids, we integrate Mamba blocks with deformable convolution and large-kernel attention into the backbone network. Extensive experiments show that MetaSSC achieves state-of-the-art performance, surpassing competing models by a significant margin, while also reducing deployment costs.", "sections": [{"title": "1. INTRODUCTION", "content": "AUTONOMOUS driving (AD) has seen rapid advancements, with research exploring different parts from perception [1], [2], planning [3], [4], localization [5], and control [6], [7], [8]. Among these, perception plays a foundational role, as it enables autonomous vehicles to sense, interpret, and understand their environment in real time. However, the suddenness and variability of dynamic traffic participants, combined with the extensive range and distance of static objects, present substantial challenges for autonomous vehicles in perceiving complex driving scenes. In many urban areas, these challenges are amplified by unpredictable behaviors from other drivers, cyclists, and pedestrians. Additionally, complex intersections, varying traffic signals, road geometry, and weather-induced changes further complicate perception. Autonomous vehicles must continuously analyze their surroundings and respond appropriately within milliseconds, making real-time perception essential for safe and effective operation. These challenges hinder the advancement of higher levels of autonomy, where vehicles need to operate with minimal human intervention.\nAmong the many approaches aimed at improving perception, Scene Semantic Completion (SSC) stands out as a technique that simultaneously reasons about both the geometry and semantics of a driving scene. As shown in Fig.1, unlike traditional perception tasks that rely on individual object detection and tracking, SSC provides a more holistic understanding of the environment by filling in missing information from partial or occluded sensor inputs. This capability is especially critical when sensors like LiDAR or cameras are obstructed by other vehicles or environmental elements, as it helps ensure robust situational awareness. Recent progress in 3D SSC has demonstrated promising results across multiple domains [9], [10], [11], [12], laying the foundation for further research in this area.\nHowever, collecting and labeling large-scale real-world datasets is a bottleneck in autonomous driving development. It"}, {"title": "2. RELATED WORK", "content": ""}, {"title": "2.1 3D semantic scene completion for autonomous driving", "content": "SSC infers both the geometry and semantics of large-scale outdoor environments from incomplete sensor inputs [19] simultaneously. It offers a full understanding of driving scenes and predicts missing elements, which is critical for autonomous driving.\nRold\u00e3o et al. [20] proposed LMSCNet, a multiscale network that combines a 2D U-Net backbone with a 3D segmentation head. This design reduces the computational burden of full 3D convolutions while maintaining competitive performance. Similarly, Yan et al. [21] introduced a multi-task learning framework where Semantic Segmentation (SS) and SSC are trained jointly. By sharing features between both tasks, the model improves both geometry and semantic predictions.\nFurther studies leverage 2D-to-3D feature projections supervised by LiDAR point clouds [22], [23], [24]. These approaches utilize monocular RGB cameras compared to LiDAR, which can lower the cost of deployment. However, transforming 2D RGB inputs into pseudo-voxelized point clouds presents challenges. False features can be introduced during this kind of pixel-to-point conversion in unoccupied areas of the 3D space, degrading model performance.\nTo address these limitations, recent research centers on improving pixel-to-point transformations and refining feature fusion techniques. Some methods incorporate depth estimation into RGB inputs, while others use attention mechanisms to selectively enhance relevant features. Despite these advancements, balancing accuracy and efficiency remains a challenge, especially for autonomous driving."}, {"title": "2.2 Sim2real knowledge transfer", "content": "Simulated datasets provide scalable and diverse environments for testing algorithms, helping overcome the limitations of real-world data collection [25], [26]. For example, Xu et al. [27] introduced the OPV2V dataset, which facilitates the evaluation of multiple object detection methods and fusion strategies for Vehicle-to-Vehicle (V2V) perception in CARLA [13]. Similarly, Li et al. [28] proposed the V2X-SIM datase on Vehicle-to-Everything (V2X) perception for autonomous driving. These datasets have become the benchmarks and essential tools for advancing cooperative perception research in autonomous driving.\nHowever, there is always a significant \"reality gap\" existing between simulated environments and real-world conditions. Models trained on data collected from simulated environments might perform well on simulated environments, but often struggle to perform reliably in real-world settings due to domain gap. To bridge this gap, researchers have explored meta-learning approaches [29], where MAML [15] has gained attention for its ability to leverage prior experiences, enabling rapid adaptation to new tasks by fine-tuning with minimal data of target domain. This makes MAML particularly well-suited for sim-to-real transfer, where models need to generalize quickly to new conditions.\nBuilding on the success of sim-to-real methods [30], [31], [32] and meta-learning frameworks [33], we explore the feasibility of identifying a pretraining task closely aligned with SSC while preserving the focus on incomplete data. Inspired by cooperative perception [34], [35], we design the pretraining task as a cooperative voxel semantic segmentation task, where each ego vehicle uses its sensor data to infer the full scene, and the supervision is provided by the combined sensor data from all Connected Autonomous Vehicles (CAVs) in the simulation. This cooperative framework enhances scene understanding by utilizing meta-knowledge, offering a promising strategy for reducing the deployment cost of perception models in real-world settings."}, {"title": "2.3 Deformable large kernel attention", "content": "There are two primary methods for learning the correlations between different voxels in the SSC task [18]. The first approach utilizes 3D convolutions with large kernels and stacking multiple layers, enabling the model to capture long-range dependencies across the 3D space. However, the computational cost increases exponentially with the number of layers, and the large number of parameters requires more memory and training time. These limitations make it impractical for real-time applications, especially in autonomous driving scenarios, where efficiency is crucial.\nThe second approach utilizes self-attention mechanisms, selectively attending to relevant features. self-attention provides flexibility in modeling relationships between distant voxels. However, self-attention tends to overlook the inherent 3D structure of the scene, treating the input data more like flattened sequences rather than structured spatial information. Besides, self-attention does not dynamically adapt to changes in the channel dimension, limiting its ability to represent complex transformations in driving environments. These limitations, combined with the computational overhead of attention-based models, present challenges for deploying them in resource-constrained systems.\nTo address these issues, researchers have explored deformable convolutions [17], [37], which introduce additional offsets that allow the network to adaptively resample spatial features. This approach enhances the model's ability to handle geometric variations by focusing on the most relevant regions of the input, improving its robustness in complex scenarios. Deformable convolutions have proven effective in tasks where objects undergo non-rigid transformations, making them particularly useful for autonomous driving.\nBuilding on previous work [38], we applied Deformable Large Kernel Attention (D-LKA) to SSC tasks. This hybrid approach aims to combine the strengths of both 3D convolutions and attention mechanisms, enabling the model to capture long-range dependencies while maintaining awareness of the 3D structure [36]. The deformable component ensures adaptability to geometric variations, while the large-kernel attention improves the focus on critical regions of the scene. This combination enhances the model's ability to represent complex driving environments effectively, supporting better SSC performance."}, {"title": "2.4 Mamba on 3D semantic scene completion", "content": "Due to its computational efficiency, Mamba [16] has emerged as a promising successor to the Transformer [39] and has recently gained significant attention [40]. Mamba's streamlined architecture reduces the computational overhead typically associated with Transformers, making it well-suited for applications that require fast inference. It adopts a lightweight design, which replaces the multi-head self-attention mechanism with simpler linear transformations, while still capturing essential relationships between input elements. As a result, Mamba has proven to be an effective model for tasks that involve sequential data, where traditional Transformers can be too resource-intensive.\nFor instance, Zhu et al. [41] developed a generic vision backbone based on Mamba to model the relationships between image patches, demonstrating the potential of Mamba for computer vision tasks. By efficiently encoding the relationships between image regions, Mamba offers a practical alternative to Transformer-based models in visual processing. Furthermore, Mamba could be even more effective in 3D modeling tasks, where the sequence of 3D blocks is substantially longer and more complex than 2D image patches. This insight has encouraged researchers to explore new ways of extending Mamba's capabilities beyond 2D applications.\nOne such effort is OccMamba [42], which introduces a 3D-to-1D re-ordering operation to adapt Mamba for 3D modeling. Specifically, a height-prioritized 2D Hilbert expansion is used to transform 3D voxel grids into linear sequences, enabling the application of Mamba's linear modeling capabilities directly to 3D data. Moreover, several studies [43], [44], [45], [46] have explored combining Transformer architectures with Mamba, suggesting that the two models complement each other. While Mamba excels in efficiency and scalability, Transformers provide robust capacity for capturing global dependencies. Integrating the strengths of both architectures offers new opportunities for enhanced performance among multiple tasks, particularly in 3D modeling."}, {"title": "3. METHODOLOGY", "content": "Research [21], [47] has shown that combining SS and SSC within a multi-task learning framework enhances the performance of both tasks, where SS provides detailed semantic features that complement the geometric understanding captured by SSC, allowing both modules to benefit from shared feature extraction. Also, some approaches increase the density of semantic labels by using historical LiDAR scans as auxiliary supervision. While these methods improve the model's capacity to capture fine-grained semantics, the reliance on historical scans adds computational overhead, making these solutions challenging to deploy in real-time autonomous driving scenarios.\nOur approach differs by treating SS as a pretraining task to learn meta-knowledge for SSC. The pretraining step helps the model generalize better across different domains, preparing it to handle real-world complexities such as occlusion and sensor noise. To further enhance supervision, we aggregate semantic information from nearby CAVs to provide denser labels that extend across greater distances. This aggregated semantic information from multiple vehicles addresses the limitations of individual sensors, which are often constrained by sparse data and occlusion. It allows the model to reason more effectively about incomplete areas, resulting in a more comprehensive scene understanding.\nThis strategy not only improves the performance of SSC but also reduces training costs. By pretraining the model with simulated data and leveraging CAV-based information in real-world scenarios, our approach minimizes the need for large annotated datasets, accelerating deployment. We adopt a dual-phase training strategy based on MAML [15], which consists of"}, {"title": "3.1 Problem formulation", "content": "We define the problem of 3D SSC as follows: Given a sparse 3D voxel grid  $X \\in R^{H\\times W\\times D}$, where H, W, and D denote the height, width, and depth of the driving scene, respectively. Each voxel  $x_{i,j,k}$ in X can be either 0 or 1, indicating the occupancy of objects, where i, j and k are the voxel indexes. The objective of 3D SSC is to learn a model  $f(*)$ that assigns a semantic label to each  $X_{i,j,k}$ in X, resulting in  $C = f(X)$, where  $c_{i,j,k}$ is the label at the corresponding position. These labels belong to the set  $[C_0, C_1,..., C_N]$, with N being the number of semantic classes and  $c_0$ representing a free label."}, {"title": "3.2 Dual-phase training strategy", "content": "Based on MAML [15], the workflow of our proposed method, MetaSSC, is illustrated in Fig. 2 and consists of two main phases: meta-pretraining and adaptation. These phases enable the SSC-MDM model to transfer knowledge from simulated environments to real-world driving scenarios, improving performance on 3D SSC tasks.\nThe meta-pretraining phase (Fig. 2-Part A) aims to prepare the model for generalization across diverse tasks by learning from simulated data. The source datasets, OPV2V [27] and V2XSIM [28], provide a range of V2V and V2X scenarios that help the model develop robust features for dynamic environments. Tasks are sampled from these datasets, each comprising a support set and a query set. The support set is used to optimize task-specific parameters in the inner loop, while the query set evaluates the generalization performance of the model in the outer loop.\nThe meta-learner initializes the SSC-MDM backbone with a set of parameters 0, which are assigned to each task. Given n tasks  $T_1 = S_1 \\cup Q_1, T_2= S_2 \\cup Q_2, ..., T_n= S_n \\cup Q_n$ from source"}, {"title": "3.3 D-LKA-M architecture", "content": "The architecture of D-LKA-M, as shown in Fig. 4, is derived from the D-LKA network [38] with the integration of Mamba blocks [16] to effectively handle the long-sequence modeling of 3D blocks. This design follows a hierarchical structure similar to LMSCNet [20], resembling a U-Net [49] architecture. The hierarchical structure enables multi-scale processing,"}, {"title": "(1) Deformable convolution", "content": "Deformable convolution [17] introduces an offset field to adjust the convolution kernel adaptively, which is especially crucial in autonomous driving, where objects like pedestrians, vehicles, and obstacles often do not conform to rigid shapes or positions. Traditional convolutions with fixed kernels struggle to effectively capture such irregularities, which limits the model's ability to accurately perceive complex driving environments. Deformable convolution addresses this issue by dynamically modifying the receptive field for each input position. The mechanism can be summarized as follows [38]:\n$x_1 = DAttn(LN(x_{in})) + x_{in}$,\nwhere DAttn(*) denotes the deformable attention mechanism, LN(*) denotes the layer normalization.\nIn deformable attention, for any position po in the input feature map x, the learned offset  $\\Delta p_n$ is added to the receptive field, defined as  $p_0 + p_n$. This mechanism allows the model to shift its focus dynamically, extending beyond fixed spatial regions. Here,  $p_n$ enumerates the positions in a regular voxel grid. The deformable convolution output at position po is given by:"}, {"title": "(2) Large-kernel attention", "content": "The large-kernel attention (LKA) proposed by Guo et al. [18] introduces a novel way to capture both local and global contextual information efficiently. Unlike traditional convolutions, which struggle to balance local details and a large receptive field, LKA decomposes a large  $K \\times K$ kernel convolution into multiple stages, each designed to handle different aspects of feature extraction while maintaining computational efficiency. Specifically, the large kernel convolution is decomposed into a  $\\frac{K}{d} \\times \\frac{K}{d}$ depth-wise dilation convolution with dilation d, a (2d \u2212 1) \u00d7 (2d \u2013 1) depth-wise convolution, and a 1 \u00d7 1 channel convolution.\nThis decomposition not only achieves a large receptive field with linear complexity but also provides dynamic processing capabilities, making it well-suited for complex tasks such as 3D SSC in autonomous driving. The mathematical formulation of LKA can be expressed as:\n$A = Conv_{1x1} (DWDConv(DWConv(x)))$,\nwhere x is the input feature, DWConv denotes the depth-wise convolution, DWDConv represents the depth-wise dilation convolution, and Conv1\u00d71 is the channel convolution. The final output of LKA is obtained through an element-wise product between the attention weights A and the input feature x :\n$X_{out} = Ax,$"}, {"title": "3.4 Mamba", "content": "Unlike Vision Mamba [41], which applies Mamba blocks within a purely 2D feature extraction pipeline, our approach directly processes the features learned from the D-LKA block with the Mamba block [16] to enhance long-sequence modeling for 3D voxel grids. This direct integration allows our model to effectively capture both local features from D-LKA and long-range dependencies through Mamba blocks, resulting in more robust scene understanding for autonomous driving. The mathematical formulation of this process is expressed as:\n$x_2 = Mamba(x_1)$,\nwhere  $x_1$ represents the input features extracted from the D-LKA block, and Mamba(*) denotes the Mamba block. The Mamba block's ability to handle sequential data ensures that the model effectively captures the intricate spatial relationships within 3D scenes, which are essential for tasks such as SSC.\nOnce the features are processed by the Mamba block, they are further refined through a feed-forward network (FFN) and a convolution layer. The final output  $x_{out}$ is computed as:\n$X_{out} = Conv(FFN(x_1)) + x_2$,\nwhere Conv(*) represents the convolution layer, and FFN(*) is a feed-forward network.\nIn summary, the integration of D-LKA and Mamba blocks allows our model to perform both local and long-sequence modeling efficiently, and also ensures a balance between local details and global context for accurate decision-making."}, {"title": "4. EXPERIMENTS", "content": "To identify a pretraining task akin to SSC while preserving its focus on exploring incomplete positions, we utilized OPV2V [27] and V2X-SIM [28] as source domains to meta-train the D-LKA-M model. The aim was to create a pretraining process that closely mimics SSC's challenges, such as the incomplete perception of the driving environment, encouraging the model to develop robust completion strategies for real-world scenarios. Since OPV2V does not contain semantic labels in its original datasets, we positioned five semantic LIDARs at the sites of the original cameras and ray-cast LIDAR, replaying the simulations to capture the semantics of driving scenes within the cameras' Field of View (FoV). This adjustment ensures that the necessary semantic information is available for training, replicating the conditions needed for scene completion. For V2X-SIM, we used the originally provided semantic data without modifications. However, both OPV2V and V2X-SIM only capture the ego vehicle's surrounding semantics, neglecting the semantics of other moving objects. To address this, we associated ground-truth labels from all CAVs but continued using the ego vehicle's ray-cast LIDAR as the primary input. This strategy ensures that the model learns to infer a more complete scene understanding from limited input, aligning well with the objective of SSC to explore and predict incomplete positions.\nWe conducted experiments on SemanticKITTI [48], a real-world outdoor dataset with urban scenes, providing 3D voxel grids from semantically labeled scans of the HDL-64E rotating LiDAR [50]. These scans represent real-world driving environments and test the model's ability to transfer knowledge gained from simulated data to real scenarios. The voxel grids are highly sparse, with a shape of 256\u00d7256\u00d732 and a voxel size of 0.2m, presenting challenges in accurately completing scenes from incomplete inputs. This sparsity requires the model to perform efficiently with limited data, mimicking real-world autonomous driving conditions. We used the original SemanticKITTI settings [48] to split the data into training, validation, and testing sets, ensuring consistency with previous studies.\nWe employed the Adam optimizer [51] with an initial learning rate of 0.001 for most experiments; for the meta-training stage, we used Stochastic Gradient Descent (SGD). All experiments were performed on an Ubuntu operating system with Python 3.8, and the code was implemented using PyTorch. All models were trained on a Dell Precision T3650 workstation equipped with a 24 GB Nvidia GeForce RTX 3090 Graphics Card and an Intel 17-10700K CPU."}, {"title": "4.1 Experiment 1: comparison with baseline models", "content": "The baseline models are introduced as follows:\n1) SSCNet\nSSCNet [19] is an end-to-end 3D convolutional network designed to take a single depth image as input and perform scene completion by generating semantic voxel predictions.\n2) LMSCNet\nLMSCNet [20] employs a 2D U-Net backbone with multi-scaled connections to extract features from the input data. It predicts both occupancy and semantic labels simultaneously using 3D segmentation heads, achieving improved scene understanding. The multi-scaled connections provide the model with the ability to capture features at different levels of granularity.\n3) TS3D"}, {"title": "4.3 Experiment 2: ablation analysis on critical techniques", "content": "To further examine the contribution of critical techniques, an ablation analysis was conducted on the validation dataset of SemanticKITTI [48] benchmark. This analysis aims to isolate and assess the impact of key components of the proposed model by comparing different variant architectures. The four variant models, referred to as Multi-scaled, D-LKA, Transfer, and Mamba, are described as follows:\n1) Multi-scaled\nLMSCNet serves as the base model for our analysis. It is a lightweight model that learns features at multiple resolutions, leveraging multi-scaled connections to capture both fine and broad contextual information. We incrementally improve our proposed method starting from this model to test how different components contribute to the final performance."}, {"title": "2) D-LKA", "content": "In this variant, we replace the LMSCNet backbone with the deformable large-kernel attention network [38] to enhance feature extraction. This modification aims to improve the network's ability to predict complex 3D scenes more accurately.\n3) Transfer\nThis variant adopts the dual-phase training strategy discussed earlier to improve model performance and reduce training time. By pretraining on source datasets and fine-tuning on the target dataset, \"Transfer\" leverages knowledge from simulation domains to enhance real-world performance, ensuring faster convergence and improved generalization.\n4) Mamba\nIn this final variant, we integrate the Mamba block into the D-LKA network to handle the long-sequence modeling of 3D blocks. Mamba's strength lies in its ability to process sequential dependencies efficiently, which further enhances the model's understanding of 3D spatial structures for SSC."}, {"title": "4.4 Experiment 3: few-shot capacity of meta-learning", "content": "Since MetaSSC is a meta-learning-based method that facilitates knowledge transfer from source domains to the target domain, we investigated whether this kind of knowledge transfer could benefit the model in few-shot learning scenarios. Few-shot learning is critical in autonomous driving, where data collection is expensive and time-consuming, especially for less frequent events such as accidents or rare weather conditions. As part of this investigation, we also explored several techniques to assess their impact on the model's robustness under data scarcity conditions.\nFor these experiments, we used Scenario 00 and Scenario 01 of the training dataset from the SemanticKITTI [48] benchmark and evaluated the SSC metrics on the validation dataset. This setup allows us to simulate a data-scarce environment by limiting the training data to only two scenarios. Building on the D-LKA model introduced in the previous section, we extended the model with the following components: knowledge transfer, data augmentation on raw inputs, data augmentation on representations, and adversarial samples. Each of these components aims to either improve generalization under limited data or increase the robustness of the model to novel, unseen driving scenes."}, {"title": "6. CONCLUSIONS AND DISCUSSIONS", "content": "This work presents a meta-learning-based framework for tackling the SSC task in autonomous driving, focusing on efficiently transferring knowledge from simulations to real-world applications. By leveraging meta-knowledge acquired from simulated environments, the framework reduces the dependence on large-scale real-world data, which significantly lowers deployment costs and shortens development cycles. This approach not only enhances generalization to new environments but also ensures the model can adapt to diverse and dynamic driving scenarios.\nA key innovation of this framework lies in its integration of LKA mechanisms and Mamba blocks within the backbone model. These components enable the model to effectively extract multi-scaled, long-sequence relationships from the sparse and irregular data provided by 3D voxel grids. LKA mechanisms allow the model to capture both local details and global context by expanding the receptive field without increasing computational complexity. In parallel, Mamba blocks improve the model's ability to process sequential dependencies across 3D blocks, enhancing the SSC task by capturing temporal and spatial relationships within driving scenes.\nOur experimental results demonstrate that the proposed model has superior performance. The success of the dual-phase training strategy, which involves pretraining on simulated data followed by fine-tuning on real-world datasets, has been validated through extensive experiments. This strategy not only accelerates convergence but also improves the model's robustness and transferability to real-world conditions.\nAdditionally, we explored the model's capabilities under few-shot learning scenarios, which are critical for applications where data collection is limited or costly. Our findings indicate that knowledge transfer and data augmentations substantially improve performance in such settings. While the inclusion of"}]}