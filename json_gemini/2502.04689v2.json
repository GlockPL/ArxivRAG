{"title": "ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning", "authors": ["Yuwei Yin", "Giuseppe Carenini"], "abstract": "Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance (\"think step by step\"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Zhao et al., 2023; Min et al., 2023; Minaee et al., 2024) have been a transformative technique in Natural Language Processing (NLP) owing to their excellent text generation and conversation abilities (Hurst et al., 2024; Anthropic, 2024; Team et al., 2024a). Challenging benchmarks for language model evaluation have significantly driven LLM advancements (Chang et al., 2024), with most designed as multiple-choice question-answering (MCQA) tasks (Robinson and Wingate, 2023) requiring answer selection from given options (Clark et al., 2018; Liu et al., 2020; Hendrycks et al., 2021). Recent LLM benchmarks demand extensive commonsense, world knowledge, and complex reasoning (Srivastava et al., 2023; Suzgun et al., 2023; Wang et al., 2024b), posing significant challenges for LLMs. Optimizing LLM performance in QA tasks is increasingly crucial for their continued development.\nRecent advancements have introduced various methods to enhance LLM reasoning abilities (Qiao et al., 2023; Sun et al., 2023), with Chain-of-Thought (CoT) prompting proving effective across various tasks (Li et al., 2024). Key variants include few-shot CoT (Wei et al., 2022), which provides rationale-based exemplars for in-context learning (Brown et al., 2020; Dong et al., 2024), and zero-shot CoT (Kojima et al., 2022), which employs general instructions such as \u201cLet's think step by step.\" Due to its simplicity and effectiveness, zero-shot CoT has gained extensive adoption. By appending this trigger sentence to the original QA prompt, LLMs generate step-by-step reasoning to improve question-answering performance.\nDespite its widespread use, zero-shot CoT prompting provides only generic reasoning guidance. As illustrated in Figure 1, answering complex questions typically involves three key steps: (1) analyzing the question's intent (Adams, 1986; Mele, 1989; Mele and Moser, 1994) to obtain a thorough"}, {"title": "2 Related Work", "content": "context understanding, a clear problem-solving target, and a purposeful planning guide, (2) retrieving relevant information from context, external sources, or memory for supportive reference (Jones and Steinhardt, 2022; Shi et al., 2023), and (3) systematically applying inductive and deductive reasoning (Clark, 1969; Johnson-Laird, 1999; Heit, 2000; Hayes and Heit, 2018). Therefore, we hypothesize that an effective prompt should direct LLMs to complete these steps. To verify this hypothesis, we propose a refined zero-shot prompting method, ARR, which explicitly incorporates these three elements: Analyzing, Retrieving, and Reasoning. Specifically, ARR employs the answer trigger sentence: \"Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.\" This structured approach is expected to enhance the performance across diverse QA tasks and various models, akin to the improvements observed with zero-shot CoT upon its first introduction (Kojima et al., 2022).\nTo evaluate the effectiveness of the proposed ARR method, we test the performance (accuracy) of open-weights LLMs (Dubey et al., 2024) on 10 multiple-choice QA datasets, covering reading comprehension (Clark et al., 2019; Liu et al., 2020), commonsense reasoning (Talmor et al., 2019; Sap et al., 2019), world knowledge (Welbl et al., 2017; Mihaylov et al., 2018; Clark et al., 2018), and multitask understanding (Suzgun et al., 2023; Hendrycks et al., 2021; Wang et al., 2024b). Compared to the Baseline method without a specific trigger sentence and the zero-shot CoT method with general prompts, ARR consistently improves QA performance across all datasets, demonstrating its effectiveness and superiority. Additionally, ablation studies show that each individual component of ARR-Analyzing, Retrieving, and Reasoning-outperforms both the Baseline and CoT methods, confirming their positive contributions. Notably, the Analyzing-only setting yields the largest performance gain on average, highlighting the critical role of intent analysis in question answering. Beyond quantitative results, we provide qualitative case studies to reveal problems in the Baseline and CoT methods such as intent misunderstanding, context misuse, and faulty reasoning.\nFurthermore, we conduct extensive experiments across various settings to assess the generalizability of the proposed ARR method. ARR consistently outperforms alternatives across different model sizes, LLM series (architectures), generation temperatures, and few-shot scenarios. These comprehensive experiments and analyses further solidify its effectiveness, robustness, and adaptability. The key contributions of this work are as follows:\n1. This paper proposes ARR, an intuitive, general, and effective zero-shot prompting method to improve LLM performance in various question-answering tasks.\n2. Comprehensive experiments across diverse QA tasks demonstrate that ARR consistently outperforms the Baseline and CoT methods. Ablation and case studies further validate the positive contributions of each component.\n3. Additional extensive experiments on various settings solidify the effectiveness and generalizability of ARR across different model sizes, LLM series, and generation configurations."}, {"title": "2.1 LLM Prompting", "content": "Recent large language models (LLMs) (Dubey et al., 2024; Lambert et al., 2024; Liu et al., 2024) are pre-trained on large-scale text corpora curated from the Internet (Soldaini et al., 2024; Penedo et al., 2024; Weber et al., 2024). Their advanced text understanding and generation capabilities (Hurst et al., 2024; Anthropic, 2024; Team et al., 2024a) have significantly revolutionized the field of natural language processing (NLP). Consequently, the NLP paradigm is shifting toward a framework comprising pre-training, post-training, and prompting (Liu et al., 2023), with post-training focusing on aligning models with human preferences (Ouyang et al., 2022; Bai et al., 2022; Rafailov et al., 2023) rather than fine-tuning for specific downstream tasks (Devlin et al., 2019). After the training stages, LLMs can generate satisfactory responses to natural language instructions and questions, highlighting the growing importance of prompt design (White et al., 2023; Giray, 2023; Sahoo et al., 2024). In this work, we propose an intuitive, general, and effective prompting method to enhance LLM performance in question-answering."}, {"title": "2.2 LLM Reasoning", "content": "Recent LLM research increasingly emphasizes reasoning abilities (Qiao et al., 2023; Sun et al., 2023). Chain-of-Thought (CoT) is a prompting strategy that enhances problem-solving by guiding LLMs to generate intermediate reasoning steps."}, {"title": "3 Question Answering with LLMs", "content": "This section presents a formally defined multiple-choice question-answering workflow using large language models. Our pipeline combines ideas from the two-step prompting introduced by Kojima et al. (2022) and the multiple-choice selection method proposed by Robinson and Wingate (2023)."}, {"title": "3.1 Question Answering Data", "content": "In this work, we consider multiple-choice question-answering (MCQA) tasks with one correct answer, where the model is asked to answer the question by selecting an option from a list of choices. Formally, let $D = {X, Y}$ be a MCQA dataset, where $X = {X_1, X_2, ..., X_n}$ is the input information, $Y = {Y_1, Y_2, ..., Y_n}$ is the corresponding correct-choice label ($y_i \\in R$), and $n$ is the number of instances in $D$.\nIn closed-book QA tasks, $X_i = {q_i, o_i}$, where $q_i$ is the $i$-th question, and $o_i = {o^i_j}_{j=1}^m$ is the option list with $m$ choices. In open-book QA tasks, $X_i = {p_i, q_i, o_i}$, where $p_i$ is the $i$-th passage provided by the task. Then, we obtain the input prompt $x_i$ for LLMs as follows:\n$X_i =\\begin{cases}\nP(p_i, q_i, o_i), & \\text{Open-book QA}\\\\\nP(q_i, o_i), & \\text{Closed-book QA}\n\\end{cases}$ (1)\nwhere $P(.)$ denotes the prompt function which concatenates the string objects in $X_i$ using line breaks as the delimiter ($\\Delta$=\u201c\\n\u201d). Thus, $P(p_i, q_i, o_i)$ is:\n$p_i \\Delta q_i \\Delta o^i_1 \\Delta o^i_2 \\Delta ... \\Delta o^i_m \\Delta \\phi_i$\nThe answer trigger sentence $\\phi$ is the only difference between the proposed ARR method and"}, {"title": "3.2 Multiple-Choice Question Answering", "content": "baseline methods in each experiment. Let $\\bar{z_i}$ be the tokenized representation of text $x_i$. The decoder-only Transformer-based (Vaswani et al., 2017; Radford et al., 2018) LLM M takes $\\bar{z_i}$ as input and generate a new token after each timestep. The model freely generates the text response given by\n$r_i = M(x_i)$, (2)\nwhere $r_i$ may contain the analysis, reasoning, and answer. Then, we combine the original text input $x_i$, the generated response $r_i$, and each choice $o_j^i$ in the option list $o_i$ as follows:\n$z^i_j = P(x_i, r_i, o^i_j)$. (3)\nLet $\\bar{z^i_j} = [t^{i}_{j;1}, t^{i}_{j;2}, \\ldots, t^{i}_{j;L}] \\in R^{L}$ be the tokenized $z^i_j$, where $L$ is the number of effective tokens that are not used for word masking or sequence padding. To select an option, we feed the model M and obtain the cross entropy loss (Shannon, 1948, 1951; Jurafsky and Martin, 2025) of each $\\bar{z^i_j}$ as follows:\n$C^i_{j} = -\\log Pr(t^i_{j;k}|t^i_{j;<k}; \\Theta), (4)\nwhere $\\Theta$ is the parameters of $M$, $t^i_{j;k}$ is the $k$-th token, and $t^i_{j;<k}$ denotes all the previous tokens before $t^i_{j;k}$. Hence, for each option $o^i_j$ in $o_i = {o^i_j}_{j=1}^m$, we have a corresponding cross-entropy loss $C^i_{j}$. Then, the option with the lowest loss value is selected, i.e.,\n$y_i = \\arg \\min_{j \\in \\{1, 2, ..., m\\}} {\\{ C^i_j \\}}_{j=1}^m$ (5)\nThus, the overall accuracy is calculated by\n$\\alpha = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(y_i = \\hat{y}_i)$, (6)\nwhere $\\alpha \\in [0, 1]$ and the indicator function $\\mathbb{I}(.)$ returns 1 if $y_i = \\hat{y}_i$ or 0 otherwise."}, {"title": "4 Experimental Setup", "content": "This section introduces the experimental setup, including datasets, models, and evaluation settings."}, {"title": "4.1 Datasets", "content": "As mentioned in \u00a7 3.1, we consider 10 multiple-choice QA tasks with questions $q_i$ and options $o_i$. Reading comprehension tasks (Chen, 2018) explicitly provide passages $p_i$ to base on. The model M is asked to answer the question by choosing one from the option list. We consider a wide range of QA benchmarks to evaluate the capabilities of M in different aspects, including reading comprehension, commonsense reasoning, world knowledge, and multitask understanding. The dataset statistics are presented in Table 1."}, {"title": "4.1.1 Reading Comprehension", "content": "BoolQ (Clark et al., 2019) is a question answering dataset for yes/no questions. It evaluates the performance of M on reading comprehension.\nLogiQA (Liu et al., 2020) is a reading comprehension dataset that requires M to have logical reasoning for question-answering."}, {"title": "4.1.2 Commonsense Reasoning", "content": "examines M on commonsense question-answering problems constructed using information from ConceptNet (Speer et al., 2017).\nSocialIQA (Sap et al., 2019) is a large-scale QA benchmark for commonsense reasoning about social situations, which probes emotional and social intelligence in everyday situations."}, {"title": "4.1.3 World Knowledge", "content": "provides scientific supports for M to answer the multiple-choice science questions."}, {"title": "4.1.4 Multitask Understanding", "content": "BIG-Bench Hard (Suzgun et al., 2023) is a suite challenging tasks filtered from BIG-Bench (Srivastava et al., 2023). Solving these problems often requires multi-step reasoning. In this work, 4 (out of 27) subtasks in BBH are discarded as they are not multiple-choice QA tasks.\n MMLU (Hendrycks et al., 2021) comprehensively measures the multitask accuracy of M on 57 tasks including elementary mathematics, history, computer science, and more.\n MMLU-Pro (Wang et al., 2024b) extends the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options."}, {"title": "4.2 Models", "content": "Our experiments adopt open-weights, decoder-only, and Transformer-based (Vaswani et al., 2017) LLMs. We mainly employ LLaMA3-8B-Chat (Dubey et al., 2024), an instruction-following LLM with 8 billion model parameters, and use the model implementation and checkpoints provided by Hugging Face Transformers (Wolf et al., 2020). In generalizability experiments, we also explore LLaMA3-Chat models of different sizes in \u00a7 6.1 and 7B-Chat models of different LLM series in \u00a7 6.2, i.e., Qwen2.5 (Yang et al., 2024), Gemma (Team et al., 2024b,c), and Mistral (Jiang et al., 2023)."}, {"title": "4.3 Evaluation", "content": "To evaluate the QA performance of LLMs, we apply a two-step process including reasoning generation and option selection, as mentioned in \u00a7 3.2. First, we let the model freely generate text responses that may include their analysis, reasoning, and answer choice. Then, we concatenate the input and output in the first stage with each choice from the given option list, pass each concatenation to the model, and select the option with the lowest cross-entropy loss. The loss corresponds to the perplexity of language models: A lower loss means a lower perplexity and a higher confidence. Length normalization is not applied because the options are mostly in the A/B/C/D, Yes/No, or True/False format. As the datasets in our experiments are all multiple-choice QA tasks, we adopt accuracy as the evaluation metric, which is calculated by Eq. 6."}, {"title": "5 Main Experiments", "content": "The main experiments test the zero-shot QA performance of LLaMA3-8B-Chat (Dubey et al., 2024) on various multiple-choice QA datasets. The only difference between Baseline, zero-shot CoT (Kojima et al., 2022), and ARR is the answer trigger sentence $\\phi$ shown in Figure 2. The results in Table 2 demonstrate that our ARR method boosts the Baseline method by a large margin, with an improvement of +4.1% on average. In addition, ARR consistently outperforms zero-shot CoT prompting across all QA datasets, highlighting its universal superiority in various task types including reading comprehension, commonsense reasoning, world knowledge, and multitask understanding. Moreover, the \"w/o Reason\" method, which directly selects options without relying on rationales ($r_i$ in Eq. 2), performs significantly worse, emphasizing the benefits of our two-stage QA approach."}, {"title": "5.2 Ablation Study", "content": "To better understand the performance gains shown in Table 2, we conduct an ablation study to explore the efficacy of each component of the ARR method, i.e., analyzing, retrieving, and reasoning. Specifically, we test the model's QA performance using the five different answer trigger sentences $\\phi$ in Table 3. Table 4 reports the accuracy scores of LLaMA3-8B-Chat under different ablation cases, where \u2460 is the full version of ARR and \u2464 is equivalent to the \"Baseline\" method in Table 2. In \u2461, \u2462, and \u2463, $\\phi$ only contains one single component, i.e., analyzing, retrieving, and reasoning, respectively.\nWe observe that all the single-component ARR settings (\u2461, \u2462, and \u2463) outperform the Baseline method (\u2464) by a large margin, which verifies that each ARR component contributes positively. In addition, \u2461 outperforms the CoT method in Table 2, which means the \"Reasoning\" prompting of our ARR method (i.e., \u201canswer the question with step-by-step reasoning\u201d) is better than zero-shot CoT prompting (i.e., \"think step by step\") for QA tasks. Furthermore, the complete ARR method (\u2460) has a higher accuracy score than the Retrieving-only (\u2462) and Reasoning-only (\u2463) methods, meaning the intent analysis benefits the other two \u201cR\u201d parts.\nNotably, the Analyzing-only setting (\u2461) brings the greatest improvement gain, suggesting the significance of analyzing the intent of the question. \u2461 even outperforms the full ARR version (\u2460) on average, mainly because it performs excellently on the"}, {"title": "5.3 Case Study", "content": "dataset (Mihaylov et al., 2018) drawn from the main experiments, where our ARR method correctly answers the question but the Baseline and CoT methods fail. We can observe that the baseline method's reasoning is incorrect: After stating that \u201cthe summer solstice is in June,\" it wrongly concludes that \u201cthe summer solstice is four months before July.\u201d The CoT method misunderstands the question, resulting in counting four months before June instead of after June. In contrast, our ARR method identifies the question's intent clearly, leading to a correct reasoning path and final answer."}, {"title": "6 Generalizability", "content": "The main experiments in \u00a7 5 have validated the effectiveness of our ARR method quantitatively and qualitatively. To verify the generalizability of ARR, we conduct additional extensive experiments under different configurations on three challenging, reasoning-intense, and multitask benchmarks introduced in \u00a7 4.1.4: BBH, MMLU, and MMLU-Pro."}, {"title": "6.1 Model Sizes", "content": "We evaluate the LLaMA3-Chat models of different sizes, i.e., 1B, 3B, and 8B (default) parameters, on multiple-choice QA tasks. As the accuracy scores (%) shown in Table 6, our ARR method brings solid performance gains over the Baseline method and consistently outperforms zero-shot CoT. For the 1B model, ARR slightly under-"}, {"title": "6.2 LLM Series", "content": "To verify the effectiveness of our ARR method on models other than LLaMA3 (Dubey et al., 2024), we conduct experiments on 7B-Chat LLMs of different series: Qwen2.5 (Yang et al., 2024), Gemma (Team et al., 2024b,c), and Mistral (Jiang et al., 2023). The results in Table 7 exhibit a consistent superiority of the proposed ARR method over the Baseline and CoT methods. This is similar to the findings in the main experiments (Table 2), solidifying the efficacy and generalizability of ARR."}, {"title": "6.3 Generation Temperatures", "content": "For reproducibility, we set the generation temperature to 0 by default, as this setting makes the generation process deterministic. However, a higher temperature brings a more diverse output, which may lead to a different QA accuracy. To study the effect of this key factor, we report the QA accuracy (%) of the LLaMA3-8B-Chat model using different temperatures during the reasoning generation stage: 0.0 (default), 0.5, 1.0, and 1.5.\nAs shown in Table 8, our ARR method surpasses the Baseline and CoT methods with different temperatures, demonstrating a strong robustness of ARR. In addition, we observe that the model generally performs better when the temperature is lower."}, {"title": "6.4 Few-shot Generation", "content": "For each subtask in a QA dataset, we randomly pick 10 examples from the training or validation set if they exists. If a subtask only has the test set, 10 test examples are held out for few-shot usage, slightly reducing the number of items for evaluation. For each raw example, we construct the CoT and ARR rationales using GPT-40 (Hurst et al., 2024). Specifically, the input prompts provided to GPT-40 match those used in the evaluation experiments under CoT/ARR settings. The model's output is extracted as CoT/ARR rationales. In few-shot examples, these rationales, along with correct answers, are appended to the answer trigger sentence $\\phi$. For the Baseline setting, few-shot examples include correct answers for in-context learning (ICL) (Brown et al., 2020; Dong et al., 2024) but exclude rationales."}, {"title": "7 Conclusion", "content": "In this work, we introduce ARR, an intuitive, simple, and general prompting method that effectively enhances the question-answering performance of LLMs by integrating three key steps: analyzing the question's intent, retrieving relevant information, and reasoning step by step. Extensive experiments across diverse QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms the CoT prompting method. Ablation and case studies further validate the positive contributions of each component, with intent analysis proving particularly crucial. In addition, evaluations across various model sizes, LLM series, and generation configurations confirm the effectiveness, robustness, and generalizability of the proposed ARR method."}, {"title": "Limitations", "content": "We did not explore variations or paraphrases of the proposed ARR prompts, opting instead for the straightforward expressions presented in this paper. While certain phrasings may further enhance performance, the core idea remains the same.\nIn addition, resource constraints limited our focus to open-weights LLMs with no more than 8B parameters. However, the results from model size experiments (\u00a7 6.1) align with the scaling laws for language models (Kaplan et al., 2020), demonstrating the potential and generalizability of our ARR method when applied to larger models.\nLastly, we observe that some generated rationales in the Reasoning Generation stage are repetitive and redundant. A dynamic stopping strategy or post-processing to filter out redundancies can reduce the computational cost and potentially further boost the final QA accuracy."}]}