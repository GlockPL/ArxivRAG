{"title": "SEGMENT ANYTHING MODEL 2: AN APPLICATION TO 2D AND 3D MEDICAL IMAGES", "authors": ["Haoyu Dong", "Hanxue Gu", "Yaqian Chen", "Jichen Yang", "Maciej A. Mazurowski"], "abstract": "Segment Anything Model (SAM) has gained significant attention because of its ability to segment a variety of objects in images given a prompt. The recently developed SAM 2 has extended this ability to video inputs. This opens an opportunity to apply SAM to 3D images, one of the fundamental tasks in the medical imaging field. In this paper, we provide an extensive evaluation of SAM 2's ability to segment both 2D and 3D medical images. We collect 18 medical imaging datasets, including common 3D modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and positron emission tomography (PET) as well as 2D modalities such as X-ray and ultrasound. We consider two evaluation pipelines of SAM 2: (1) multi-frame 3D segmentation, where prompts are provided to one or multiple slice(s) selected from the volume, and (2) single-frame 2D segmentation, where prompts are provided to each slice. The former is only applicable to 3D modalities, while the latter applies to both 2D and 3D modalities. We learn that SAM 2 exhibits similar performance as SAM under single-frame 2D segmentation, and has variable performance under multi-frame 3D segmentation depending on the choices of slices to annotate, the direction of the propagation, the predictions utilized during the propagation, etc.", "sections": [{"title": "Introduction", "content": "Medical image segmentation is a crucial task for multiple clinical applications such as disease diagnosis and clinical analysis [17, 23, 26, 32]. Despite the advancements in medical imaging technologies, segmentation remains a challenging task due to the labor-intensive nature of data annotation and the complexity of medical images [4, 20, 35].\nSegment Anything Model (SAM) addresses these challenges in multiple directions. On one hand, SAM has demonstrated impressive zero-shot segmentation performance with prompt inputs, significantly reducing the need for extensive manual data annotation [6, 20]. On the other hand, several works fine-tune SAM to specific tasks and demonstrate improvements over standard segmentation techniques [10, 7, 18, 22], such as nn-UNet [14]. Despite these advancements, SAM's limitation to 2D images restricts its applicability to scenarios that require three-dimensional understanding [27].\nExisting work addresses this challenge by introducing extra components to SAM to enable its 3D segmentation capability. For example, SAM3D [5] combines the SAM encoder with a lightweight 3D CNN decoder; 3DSAM-A [9] modifies the original prompt encoder and mask decoder to operate in 3D; SAM-Med3D [33] introduces an additional 3D convolution before the image encoder and replaces 2D positional encoding layers with 3D one. On the other hand, the recently introduced SAM 2 [27] solves this limitation fundamentally by extending the backbone of SAM to 3D. Specifically, SAM 2 proposes a memory bank that retains information from past predictions and allows it to make predictions on slices without prompts based on the information. This feature motivates us to examine SAM 2's ability to segment 3D medical images since video segmentation can be transferred to 3D segmentation seamlessly, i.e., each slice is treated as a frame. Note that we will use the terms \u201cslice\u201d and \u201cframe\u201d interchangeably throughout the paper.\nIn this paper, we extend the previous evaluation experiments on SAM [20] to SAM 2, aiming to explore the model's effectiveness in a more complex, three-dimensional context. Specifically, we consider two evaluation pipelines: multi-"}, {"title": "Methods", "content": "In this section, we discuss the two evaluation pipelines, single-frame 2D segmentation and multi-frame 3D segmentation, in detail. Intersection over Union (IoU) is the evaluation metric throughout the paper. To have a comparable performance between 2D and 3D segmentation, IoU is only computed over non-empty slices.\n2.1 Evaluation of Single-Frame 2D Model Testing on 2D Images\nDuring single-frame 2D segmentation, SAM 2 exhibits the same behavior as SAM in segmenting the object of interest based on prompts. For datasets with 2D modalities, we run SAM 2 naturally at the image level. For datasets with 3D modalities, we simulate prompts for each slice of the volume. Following previous work [20], we design single-frame 2D segmentation in a non-iterative manner in which all prompts are determined without feedback from any prior predictions. Specifically, the following four 2D prompting modes (P-Mode) are used:\n1. P-Mode 1: One point prompt placed at the center of the largest connected region of the object of interest.\n2. P-Mode 2: One point prompt placed at each separate connected region of the object (up to three points).\n3. P-Mode 3: One box prompt placed at the center of the largest connected region of the object of interest.\n4. P-Mode 4: One box prompt placed at each separate connected region of the object (up to three boxes).\nThese strategies cover the most efficient and straightforward annotation strategy that provides one point or box prompt to each connected region of the object of interest.\n2.2 Evaluation of Multi-Frame 3D Model Testing on 3D Volumes\nSAM 2 differs from SAM mostly in its new ability to segment videos, which can be seamlessly transferred to the 3D image segmentation task. In this section, we are mostly interested in SAM 2's semi-supervised segmentation ability, where we only provide prompts for one or a few frames in advance and use SAM 2 to predict other frames.\nInitial Frame Selection. In video segmentation, the initial frame to be annotated is typically the first frame of video as it arrives first in the time stream and the object of interest usually does not change shape or size dramatically between"}, {"title": "Dataset", "content": "Consistent with the previous experimental study on SAM [20], this study utilizes 18 diverse medical datasets to evaluate the performance of SAM 2. Specifically, all datasets are evaluated during single-frame 2D segmentation, and twelve datasets with 3D modalities (MRI, CT, and PET) are evaluated during multi-frame 3D segmentation. The \u201cUS-Nerve\u201d dataset[2], which was included in the previous SAM evaluation paper[20], has been removed from this project due to licensing regulations. We also updated the MRI-Breast dataset to include more annotated volumes. The pre-processing steps are consistent with the previous study, except that for SAM 2 3D mode, we have to convert the input images to JPG format instead of PNG. We also provided visual representations of the annotations masks for each dataset in Fig. 2.\n3.1 2D Datasets\nWe included 6 2D datasets, 2 X-rays, and 4 ultrasounds, covering 7 different anatomical objects. Specifically, the X-ray datasets cover chest and hip joint segmentation, and the ultrasound datasets encompass a broader range of regions, including breast, kidney, muscle, and ovarian tumor segmentation. Detailed information on these datasets can be found in Table 1. To keep the format consistent with 3D datasets, the images of 2D datasets are also converted to JPG using the same pipeline.\n3.2 3D datasets\nWe included 12 3D datasets, 5 MRI, 6 CT, and 1 PET-CT, covering 20 different anatomical objects. The structures of the datasets are modified to fit the input requirements of SAM 2 3D, where each individual volume has its own folder, and each slice is numbered according to its position within the volume."}, {"title": "Experimental Results", "content": "4.1 Results of SAM 2 on Single-frame 2D Segmentation testing compared with previous SAM\nIn this section, we present the performance of SAM 2 under single-frame 2D segmentation scenario across four prompting modes (refer to Section 2.1). Fig. 3 and Fig. 4 show the results for 2D and 3D datasets, respectively. The findings indicate that SAM 2's single-frame 2D segmentation capability is comparable with that of SAM (a comparison of single-frame 2D segmentation results between SAM and SAM 2 is provided in Fig. 5). Similar to SAM, the performance of SAM 2 varies significantly across different datasets. For instance, SAM 2 achieves an impressive IoU of 0.907 on the Xray-Hip dataset for ilium but performs poorly with an IoU of 0.277 on the MRI-Spine dataset for GM.\nComparing the performance for different prompting modes, both the results on SAM and SAM 2 highlight the superiority of box prompts over point prompts. Moreover, prompts that indicate each separate part of the object individually are generally superior to those that indicate only one part or outline all parts in a single box, which is also consistent with the findings in previous SAM evaluations [20].\n4.2 Results compared with 3D model's Prediction Selection Strategy\nWe investigate the impact of prediction selection strategies across three initial frame selection modes: F-Mode 1, F-Mode 2, and F-Mode 3, all within the single point click setting (P-Mode 1) and using the front-to-end propagation direction (D-Mode 1). The results, depicted in Figure 6, demonstrate that for the annotated slice, opting for the first channel's prediction achieves performance that is comparable to or better than SAM 2's default method of choosing the most confident slice. Notably, for the MRI-Spine: SC dataset, selecting the first channel leads to an approximate 4% improvement in DSC when starting from the largest slice, and a 14% improvement when starting from the center slice and 17% when starting from 3 uniform slices.\nThus, we find that selecting the first channel from the annotated slices is a better choice compared to the highest confidence selection. Consequently, we will primarily focus on this selection method in the following sections.\n4.3 Results compared with different Initial frame selection and Prompt simulation strategies\nAs shown in Figure 7, we compare the results of providing prompts on the largest slice, the center slice, and three uniformly distributed slices. We find that providing only one prompt for the entire volume results in unsatisfactory performance. For the front-to-end propagation (subplot row 1, first two plots), selecting the largest slice yields an average DSC of 6% across all datasets, while selecting the center slice yields an average DSC of 14%. For the bidirectional propagation (subplot row 2, first two plots), these numbers are 8% and 19%, respectively. These results show a dramatic decrease in performance compared to the previous 2D evaluation results across all datasets.\nFor different prompt modes, we do not observe a significant performance change. Providing one or more prompts (as shown by the different colors of the bars in each plot) yields consistent results. This is expected because additional prompts are only provided when there is more than one disconnected region. In most medical image datasets, the target object usually appears as a single connected region."}, {"title": "Impact of the propagation mode", "content": "Comparing the first and second rows in Figure 7, we find that applying bi-directional propagation can slightly improve the performance. When selecting the initial frame with F-Mode 1, i.e., the slice with the largest object of interest, the performance of SAM 2 improves around 2% IoU; and under F-Mode 2, i.e., the middle slice of the object of interest, SAM 2's performance improves around 5% IoU.\nThese findings suppose that, though SAM 2 supports prompting on any frame as a condition to predict new frames, it is better to start propagating from adjacent slices as they are first added to the memory bank. Though the original SAM 2 was mainly examined on 1-N propagation mode in their experimental settings, the performance of reverse propagation (saying from K to K-1) can still work well given that the order of medical images is not relevant to the time order."}, {"title": "Conclusion", "content": "In this work, we conduct an initial investigation of the performance of SAM 2 in the medical imaging field. In addition to evaluating the 2D segmentation task, we investigate its ability to do the 3D segmentation task thanks to SAM 2's ability to segment videos. We find that under single-frame 2D segmentation:\n1. SAM 2 exhibits similar performance to SAM.\nUnder multi-frame 3D segmentation, we observe that:\n1. When deciding the initial frame selection, selecting the center slice of the object of interest is better than selecting the slice with the largest object of interest.\n2. Providing 1 or 3 points to the selected frame has little effect on the final performance.\n3. During propagation, it is better to utilize the first predicted mask instead of the one with the largest confidence.\n4. Propagating predictions bidirectionally, starting from the annotated slice, works better than propagation from beginning to end.\n5. The best average performance of SAM 2 in the 3D segmentation is 0.19 IoU when only one point prompt is provided to one volume."}]}