{"title": "PROACTIVE DEPOT DISCOVERY: A GENERATIVE FRAMEWORK\nFOR FLEXIBLE LOCATION-ROUTING", "authors": ["Site Qu", "Guoqiang Hu"], "abstract": "The Location-Routing Problem (LRP), which combines the challenges of facility (depot) locating\nand vehicle route planning, is critically constrained by the reliance on predefined depot candidates,\nlimiting the solution space and potentially leading to suboptimal outcomes. Previous research on LRP\nwithout predefined depots is scant and predominantly relies on heuristic algorithms that iteratively\nattempt depot placements across a planar area. Such approaches lack the ability to proactively\ngenerate depot locations that meet specific geographic requirements, revealing a notable gap in\ncurrent research landscape. To bridge this gap, we propose a data-driven generative DRL framework,\ndesigned to proactively generate depots for LRP without predefined depot candidates, solely based\non customer requests data which include geographic and demand information. It can operate in two\ndistinct modes: direct generation of exact depot locations, and the creation of a multivariate Gaussian\ndistribution for flexible depots sampling. By extracting depots' geographic pattern from customer\nrequests data, our approach can dynamically respond to logistical needs, identifying high-quality\ndepot locations that further reduce total routing costs compared to traditional methods. Extensive\nexperiments demonstrate that, for a same group of customer requests, compared with those depots\nidentified through random attempts, our framework can proactively generate depots that lead to\nsuperior solution routes with lower routing cost. The implications of our framework potentially\nextend into real-world applications, particularly in emergency medical rescue and disaster relief\nlogistics, where rapid establishment and adjustment of depot locations are paramount, showcasing its\npotential in addressing LRP for dynamic and unpredictable environments.", "sections": [{"title": "1 Introduction", "content": "The Location-Routing Problem (LRP) is a critical optimization challenge in the urban logistics industry, combining two\ninterdependent decisions: selecting depot locations where vehicles commence and conclude their tasks, and planning\nvehicle routes for serving customers. This integration is crucial as the depot locations can directly affect the vehicle\nroute planning, thereby impacting overall costs [1]. The LRP can be formally defined as [2]: Given a set of customers\nwith specific location and quantity of demands, and a set of potential depot candidates each with a fleet of vehicles\nfeaturing fixed capacity, aiming to properly select a subset of depots and plan routes for vehicles departing from these\nchosen depots to meet customers\u2019 demands, while minimizing both depot-related and route-related costs, without\nviolating specific constraints.\nIn this traditional problem configuration, solving LRP have relied on a predefined set of depot candidates [3, 4, 5, 6]\ninstead of directly generating desired optimal depot locations, thereby limiting the solution space and potentially leading\nto suboptimal outcomes. This constraint is particularly pronounced in scenarios where the optimal depot locations are\nnot included in the candidates set, or when the problem configuration demands a high degree of flexibility in depot\nplacement, requiring quickly establish and adjust depot locations. The real-world application that underscores the\nnecessity of generating depots without predefined candidates is medical rescue and disaster relief logistics: In the\naftermath of a natural disaster, such as an earthquake or flood, the existing infrastructure may be severely damaged,\nrendering previously established depots unusable. In such scenarios, the ability to dynamically generate new depot\nlocations based on current needs and constraints is crucial for efficient and effective relief operations."}, {"title": "1.1 Related Work", "content": "Methods for LRP with Predefined Depot Candidates: In addressing the LRP with Predefined Depot Candidates,\ntraditional methods have predominantly employed exact and heuristic approaches. Exact methods, such as Mixed Integer\nProgramming (MIP) models enhanced by branch-and-cut [10, 11] or column generation techniques [3], offer precision\nbut often struggle with scalability in larger and complex scenarios due to an exponential increase in binary variables.\nThis limitation has pivoted attention towards heuristic methods, which are categorized into: matheuristic approaches\n[12, 13, 14] that blend heuristic rules with exact methods, learning-aided heuristics [15, 4] that leverage learning-based\nalgorithms to refine heuristic operations, and pure meta-heuristic algorithms. Among pure meta-heuristics, cluster-based\nheuristics [16, 17] and iterative methods [9, 5, 18] have been notable. However, the cluster-based heuristics, which focus\non geographically clustering customers, exhibit limitations in handling additional constraints like customer-specific\ntime windows, while the iterative methods present insufficient conjugation between the two stages of depot-selecting\nand route-planning. Besides, these methods typically require initiating a new search process for each case, leading to\ninefficiencies when even minor alterations occur to current problem instance.\nThe advancements in DRL have shown promise in addressing routing problems, both in \u201clearn-to-construct/generalize\"\n[19, 20, 21, 22] and \u201clearn-to-improve/decompose\u201d [23, 24, 25]. However, its application in LRP, which integrates the\nchallenges of facility locating with routing problem, still remains notably underexplored due to their inherent limitations\nin problem formulation for scenario involving multiple depots and the inability to organically integrate depot-selecting\nwith route-planning. The works [26, 27, 28] focus on resolving routing problems involving multiple depots, but without\nconsidering the depot-relate cost, which technically confine them as multi-depot VRP, instead of LRP which considers\nboth route-related cost and depot-related cost. The work [6] considers depot-related cost but adopts a two-stage process,\nclustering customers with an assigned depot location first and planning routes second, thereby separating depot selection\nfrom route planning, which fails to capture the interdependencies between these two critical aspects, while also lacking"}, {"title": null, "content": "verification on standard LRP setup align with real-world datasets. Most importantly, all these methods are constrained\nto the predefined depot candidates, falling short in dealing with LRP without predefined depot choices.\nExploration of LRP without Predefined Depot Candidates: Only a scant number of studies explore the LRP without\npredefined depot candidates, predominantly employing heuristic strategies for attempting new depots across a planar\narea devoid of predefined depot choices. The work [7, 8] concentrate on single-depot scenario, where a single and\nuncapacitated depot is to be selected from a planar area. Specifically, [8] extends the cluster-based method in [7],\nproposing a learning-aided heuristic method to recurrently initiate new depot. Based on the same single-depot scenario,\nthe work [29] proposes a hierarchical heuristic method to iteratively update candidate circle to select new depot and\nthen plan routes based on this depot. Furthermore, [9] extends the iterative heuristic method to explore multi-depot\nscenario, but only manages to deal with cases with up to two depots.\nIt is notable that, compared with our method's endeavors on actively and directly generating the recommended depots,\nthese works employ heuristic algorithms to iteratively attempt new depot and then decide if it is a better one by\nre-planning routes based on it, limited to single or double-depot scenarios. Moreover, all these works lack ability on\nconsidering specific location constraints for depots, highlighting the necessity for a more adaptable and flexible solution."}, {"title": "2 Methodology", "content": "Overview: Chain-of-Thoughts Solely based on the customer requests data which include geographic and goods\ndemand information within an area, in pursuit of a solution that proactively generates high-quality depots satisfying\nspecific location requirements, and subsequently plans optimized vehicle routes from these generated depots to efficiently\nserve customers, we propose the generative DRL framework, which is overviewed in Fig. 1.\nThe Depot Generative Model (DGM) takes in the customer requests information including the positions and specific\nquantity of demands, generating the required amount of depots in two distinct modes: the exact depot locations or a\nmultivariate Gaussian distribution for flexible depots sampling.\nTraining DGM for generating desired depots hinges on a robust evaluation mechanism to assess the quality of the\ngenerated depot set, i.e., based on the same group of customer requests and identical route-planning strategies,\ndetermining which set of depot can lead to solution routes with lower routing cost. This necessitates a critic model to\nscore the generated depot locations or the distribution during training. Additionally, to facilitate an efficient training\nprocess, this critic model must be able to instantly provide scores for the generated depot set, and also has to operate\nbatch-wisely."}, {"title": null, "content": "In this pursuit, we modify the Attention Model [19] to accommodate the multi-depot scenario, introducing the Multi-\ndepot Location-Routing Attention Model (MDLRAM) as a critic model placed after the DGM, constituting the entire\nframework. By taking in the customer requests and the generated depots from DGM, MDLRAM outputs the LRP\nsolution routes, associated with a minimized objective, providing score to rate the generated depot locations or the\ndistribution.\nBecause MDLRAM serves as a critic model for DGM, it should be robust enough to provide a reliable score for\nassessing the generated depots from DGM. That means, the score is expected to solely reflect the quality of the generated\ndepots, ruling out the influence of LRP routing solution's quality per se as much as possible. To achieve this capability,\nthe MDLRAM should be pre-trained to be able to provide the LRP routing solution with minimized overall cost based\non given requests and depots, and then set fixed to participate in the training of DGM. In this manner, during training\nthe DGM, the different sets of depots generated by DGM for a same group of customer requests will get different scores\nfrom MDLRAM, solely reflecting the influence of depot locations, thereby facilitating a robust training process for\nDGM."}, {"title": "2.1 Critic Model: MDLRAM", "content": "As a critic model for DGM, the MDLRAM takes in the customer requests and the generated depots, aiming to output\nthe integrated LRP routing solution with minimized objectives including both route-related and depot-related cost.\nMDLRAM-Configuration: In alignment with the conventional setting [10], the configuration is defined on an\nundirected graph G = (V, E), where the V = {VD\u2081,\u06f0\u06f0\u06f0,VDm,VS1,..., vs } denote the vertices set comprising n\ncustomers and m depots. Specifically, vDr signifies the coordinates (XDk, YDk) for depot Dk, where k \u2208 {1, ...,m},\nand vs represents the coordinates (xse, yse) for customer Se, where e \u2208 {1, ..., n}. The Euclidean edge set is defined\nas E C V \u00d7 V, with dij representing the Euclidean distance from vi to vj.\nEach customer vs. has a specific quantity of demands for goods denoted as qe. Each depot VD is characterized by two\nattributes: (i) the maximum supply Mk (soft constraint), indicating the desired maximum total goods dispatched from\nthis depot; (ii) the fixed opening cost Ok, indicating the expense for using this depot facility. Regarding the vehicles,\nwe operate a homogeneous fleet, with each vehicle having the same maximum capacity Q (hard constraint) indicating\nthe maximum vehicle load during service, and a setup cost U for using this vehicle in service. (More details for LRP\nconfiguration are available in Appendix A.1).\nMDLRAM-Objective Function: In the LRP scenario, a feasible solution is essentially a set of routes, simultaneously\nexecuted by multiple vehicles starting and ending at their designated depots. To utilize DRL model to output solution\nroutes, it's crucial to formulate the solution routes into a Markov Decision Process (MDP) as the output format,\nrepresenting iterative decisions to construct the solution. To mathematically formulate the LRP solution into MDP, a\ntuple (S, A, P, R, \u03b3) is defined, with each decision step t associated with a tuple (st, at, Pt, rt, t). The st represent\ncurrent state, encompassing information of: the current route's depot, the current serving customer and remaining\ncapacity on current vehicle; The action at denotes the next visit point, subject to the vehicle's remaining load; The\npt and the rt correspond to the transition probability and cost associated with action at, respectively. Along with\nthe generation of MDP, in each decision step t, the current state is dynamically updated upon serving a customer or\nreturning to a depot. (See Appendix A.2 for detailed MDP formulation proposed for LRP scenario.)\nFollowing this construction, a feasible LRP solution is formulated, associated with an objective (cost) function expressed\nas Eq. (1). Apart from the step-wisely accumulated transit distance length \u2211trt along with the MDP generating\nprocess, other costs, which depict solution's overall performance are also integrated into the total cost with a respective\ndiscount, including: (i) the opening cost for used depots; (ii) the setup cost for dispatched vehicles; (iii) penalty of\nexceeding depot desired maximum supply. In Eq. (1), \u03b7\u03ba \u2208 {0, 1} represents whether depot Dk is opened, Xk records\nthe number of vehicles dispatched from depot Dk, and \u03b1, \u03b2, \u03b4 are coefficients.\n$L_{Sel}(A) = \\sum_{t} r_t + \\alpha \\cdot \\sum_{k=1}^{m} O_k \\cdot \\eta_k + \\beta \\cdot \\sum_{k=1}^{m} U \\cdot \\chi_k + \\delta \\cdot \\sum_{k=1}^{m} max[(\\sum_{e} q_e)_k \u2013 M_k, 0]$ (1)\nMDLRAM aims to minimize the expectation of this loss associated with the LRP solution, defined as E[LSel(A)],\nwhere Lsel(A) is expressed in Eq. (1).\nMDLRAM-encoder: As is shown in the red block of Fig. 1, two streams of information are fed into MDLRAM as\ninput: the depot candidates and the customer requests data. For each depot candidate VD\u2081 where k \u2208 {1, ..., m}, it\nis represented by its coordinates gDk = [XDk,YDk]T. For each customer vs where e \u2208 {1,..., n}, it is depicted\nby a vector concatenating its coordinates and specific demands, in form of gse = [xse,Yse, qe]T. By respectively\nimplementing different learnable linear projections, these depot candidates information vectors and customers in-\nformation vectors are embedded into a high-dimensional space with same dimension, deriving the node features"}, {"title": null, "content": "{hD\u2081,\u2026\u2026\u2026,hDm, hs1,..., hsn}. These node features undergo N standard attention modules, encoded as the final node\nembeddings (h)....,h), h),..., h} for downstream decoding.\nDm,\nMDLRAM-decoder: With the encoded node embeddings, the decoder operates iteratively to construct feasible solution\nroutes in form of vertices' permutation as an MDP. Each decoding step necessitates a context embedding h depicting\ncurrent state st, and a mask finalizing point selection domain through filtering out the current infeasible points, both\nupdated step-wisely.\n(i) Context embedding: We design the context embedding ht to depict current state, concatenating four elements:\nh = W[ha||h(t) ||hD(t) ||Qt] + b, where ha = (1m+n\\sum_{k=1}^{m+n} h_k^g) is the global information; h(t) is\nthe node embedding of the point where current vehicle is situated, while Qt is the remained load on current vehicle.\nNotably, hD(t) is the node embedding of the depot which current route belongs to.\n(ii) Mask mechanism: In each decoding step, guided by the context embedding ht, the decoder produces the correspond-\ning probabilities for all the feasible points within the selection domain, while infeasible points-determined by vehicle\nremained load and tasks completion state-are masked. To efficiently handle batch processing of problem instances,\nwe employ a boolean mask tailored for the LRP scenario, allowing for batch-wise manipulation on selection domains,\navoiding repeated operation for each individual instance. (See step-wise update pseudo code in Appendix A.3)\nUpon finalizing the boolean mask for current decoding step, the context embedding is applied to conduct Multi-head\nAttention (MHA) with the node embeddings filtered by the mask. This yields an intermediate context embedding \u0125\nincorporating the glimpse information on each feasible point. Then, \u0125 participates in Single-head Attention (SHA) with\nthe filtered node embeddings, yielding the corresponding probabilities for all the feasible points in its selection domain,\nwhere a feasible point, as an action at, can be selected with an associated pt. This decoding process is delineated as:\nh = FF(MHA(h, mask{h) ....hh ,h}))\n(N) hat = argmax(softmax[- FF (query) (h) \u2022 FF (key) (mask{hy),..., ho, h(N) h ....,h)})])\n2.2 Dual-mode Depot Generation: DGM\n(2)\nAs depicted in the purple block of Fig. 1, the DGM is designed to only take in the customer requests data and generate\nthe depots in two distinct modes based on preference: exact depot locations or a multivariate Gaussian distribution for\nflexible depot sampling.\nD1\nDGM-Configuration: The configuration for depot generation is also defined on an undirected graph G = (V, E),\nwhere the V = {VS\u2081,..., vs} only including customer requests. A solution set incorporating m depots is pending\nto be generated. During depot generation, the distances among generated depots are expected to be within the range\n[lmin, Imax], which means the depots being excessively close or distant with each other will both incur violation penalty.\n(N) MDLRAM DmS1\nSn\n\u2713 dim\n(N) DM S1\nSn"}, {"title": null, "content": "DGM-Objective Function: As the main task of depot generation, the depots with desired properties are expected to be\ngenerated. According to the problem configuration for DGM, for the solution set of generated depots, denoted as D, its\nloss can be defined as Eq. (3), where LMDLR is the route length derived by MDLRAM based on the DGM generated\ndepots, \u03bb, \u03b5 are coefficients for penalty of the depots being too distant or close with each other:\n$L_{Gen} (D) = L_{MDLR} + \\sum_{i=1}^{m} \\sum_{j=i}^{m} [\\lambda \\cdot max(d_{ij} - l_{max}, 0) + \\varepsilon \\cdot max(l_{min} \u2013 d_{ij}, 0)]$ (3)\nDGM aims to minimize the expectation of this loss associated with generated depot set, defined as E[LGen(D)], where\nLGen (D) is formed as Eq. (3).\nDGM-encoder: The DGM solely processes the customer requests, each characterized by a vector gse = [xSe, YSe, qe]T\nconcatenating location and demands. Following the similar encoding process with MDLRAM, these requests are\nencoded as node embeddings {hs_1,...hs_n} based on which a global embedding is finalized as: hserve =\n\u03a3 hsi for downstream depot generation.\nDGM-generator in Multivariate Gaussian distribution mode: In this mode, the DGM aims to generate a multivariate\nGaussian distribution where depots can be flexibly sampled. Since m depots are pending to be identified, the generated\nmultivariate Gaussian distribution should exhibit 2m dimensions, with each pair of dimensions denoting the coordinates\n(XDk, YDk) for depot Dk, where k \u2208 {1, ..., m}. To achieve this, we define this multivariate Gaussian distribution,\npending to be generated, as: Xdepot ~ N (\u03bc, \u03a3), where any randomly sampled 2m-dimensional vector Xdepot =\n(X1, X2,..., X2m)T represents the coordinates for a depot set including m depots.\n{h(N) Si} for downstream depot generation.\ni=1"}, {"title": null, "content": "To generate such distribution, two essential components are: the mean vector \u03bc\u2208 R2m and covariance matrix\n\u03a3\u2208R2m\u00d72m. Hence, the output of DGM should be a vector hdepot as below, where the first 2m dimensions represent\nthe mean vector, followed by the second 2m dimensions denote corresponding variance of each coordinate, with\nthe remaining C2m dimensions as the covariance of any two coordinates. Therefore, the hdepot \u2208 R2m+2m+C2m_is\narranged as Eq. (4).\nh_\\text{depot} = (\\overbrace{h_1,..., h_{2m}}^{\\text{mean}},\\overbrace{..., h_{4m}}^{\\text{variance}},\\overbrace{..., h_{4m+C_{2m}}}^{\\text{covariance}})^T (4)\nTo output this hdepot for constructing the 2m-dimensional Gaussian distribution, we employ a layer module featuring\ntwo fully connected layers to process the global embedding hserve derived by encoder, expressed as: hdepot =\ntanh (FFmultig (tanh(FF(hserve)))).\nWhen utilizing the hdepot to construct the multivariate Gaussian distribution, it is critical to ensure that the variances\nremain positive. Hence, before constructing, we process the second 2m dimensions' elements as below: var =\n1 + elu((h2m+1,..., h4m)T). Besides, when sampling the depot set Xdepot which records the depot coordinates, to\nadhere to the configuration, Xdepot should be mapped within unit square [0, 1] \u00d7 [0, 1] to standardize the depot set:\nDmultig = sigmoid(Xdepot).\nDGM-generator in Exact position mode: In this mode, DGM aims to directly generate the exact positions for a set\nof depots based on the global embedding hserve derived by encoder. To this end, we retain m as the depot number,\nthen the DGM's output should be a vector hdepot \u2208 R2m in which every two dimensions represent the coordinates\n(XDk, YDk) for a depot Dk.\nh_\\text{depot} = (\\overbrace{h_1,h_2}^{\\text{D1}},..., \\overbrace{h_{2m-1}, h_{2m}}^{\\text{Dm}})^T (5)\nTo output this hdepot, a layer module, encompassing two fully connected layers, is employed: hdepot =\nFFexactP (tanh (FF(hserve))). Also, to satisfy the configuration, we map the hdepot within the unit square [0, 1] \u00d7 [0, 1]\nto standardize the depot set: DexactP = sigmoid(hdepot)."}, {"title": "3 Training Strategy", "content": "Step I: Pre-training of MDLRAM: MDLRAM concurrently processes a batch of problem instances randomly sampled\nfrom the configuration, thereby generating a batch of corresponding MDPs as their respective feasible solutions.\nEach MDP involves a permutation of actions, denoted as MDP(A) = {a1,a2,...}. Because each action at is\nassociated with a probability pt for selecting the corresponding point, the entire MDP's probability is manifested as:\n\u0420\u03b8\u2081 (A) = \u03a0t Pt = \u03a0t P(St+1|St, at), which is parameterized by 0\u2081, denoting the MDLRAM's parameters that require\ntraining. Based on a batch of such MDPs, each associated with a probability po\u2081 (A) and a cost Lsel (A) in Eq. (1), the\nMDLRAM is trained by REINFORCE gradient estimator with greedy rollout baseline [19] to minimize the expectation\nof cost, as depicted in Eq. (6), where the baseline B is established through a parallel network mirroring the structure of\nMDLRAM, persistently preserving the best parameters attained and remaining fixed. (See Appendix A.4 for pseudo\ncode and details.)\n\u25bdL(\u03b81) = Epo\u2081 (A) [(Lsel(A) \u2013 B)\u2207log po\u2081 (A)] (6)\nStep II: Dual-mode training of DGM: As depicted in Fig. 2, DGM can be trained in two modes, with the pre-trained\nMDLRAM serving as a fixed sub-solver. For the record, the 011, denoting DGM's parameters that requires training, is\nappended as footnote only to those variables which are parameterized by DGM.\n(i) Multivariate Gaussian distribution mode: In this mode, as depicted in left side of Fig. 2, the DGM takes in a main-\nbatch (Batchsize: Bmain) of randomly sampled graphs: {Gb|b = 1, 2, ..., Bmain}, each with a group of customer requests,\nand then outputs a main-batch of corresponding multivariate Gaussian distributions: {N\u2081 (\u03bc, \u03a3) |b = 1, 2, ..., Bmain}.\nThus, training DGM involves ensuring that the depots sampled from these distributions yield favorable expectations for\nthe cost LGen (D).\nTo achieve this, from each distribution N\u2081 (\u03bc, \u03a3) within the main-batch, we sample a sub-batch (Batchsize: Bsub)\nsets of depots. Each depot set is represented as Dmultig, associated with their probabilities pon (DmultiG) and costs\nLGen (DmultiG) in Eq. (3). In this way, a main-batch (Bmain) of cost expectations, each corresponding to a multivariate\nGaussian distribution, can be derived. This entire process is shown in left part of Fig. 2. We employ following optimizer\nto train the DGM in this distribution mode\nB_\\text{main} \\nabla L(\\theta_\\Pi) = \\frac{1}{B_\\text{main}} \\sum_{b=1}^{B_\\text{main}} E\n{\\mathbb P_{\\theta_{\\Pi}}(D_{\\text{multiG}})} \\left[ L_{Gen} (D_{\\text{multiG}}, G_b) \\cdot \\nabla \\text{Vlog } \\mathbb P_{\\theta_{\\Pi}}(D_{\\text{multiG}})\\right] (7)"}, {"title": null, "content": "(ii) Exact position mode: In this mode, as depicted in right side\nof Fig. 2, the DGM still ingests a main-batch (Bmain) of randomly\nsampled graphs: {Gb/b = 1,2,..., Bmain}, each with a group of\ncustomer requests, but directly generates the corresponding sets of\ndepots: {Dexactp/b = 1,2, ..., Bmain}. For each set of depots DexactP,\nthe cost LGen (Dexactp) can be derived by Eq. (3) whose first part\nis obtained by pre-trained MDLRAM. Below optimizer guides the\nDGM's training in exact mode:\n$\\nabla L(\\theta_\\Pi) = \\frac{1}{B_{\\text{main}}} \\sum_{b=1}^{B_{\\text{main}}} L_{Gen} ((D_{\\text{exactP}})_b, G_b)$ (8)\nIt is crucial to differentiate that, for different modes, the DGM's\nparameters 01\u2161 are tracked in different variables. In multivariate\nGaussian distribution mode, what has been parameterized by O is\nthe probability for each sampled Dmultig, whereas in exact position mode, what has been parameterized by O is\nthe Dexactp. Therefore, the gradients in these two modes are respectively backpropagated to parameters 01 through\nPon (DmultiG) and (DexactP)01"}, {"title": "4 Experimental Results and Discussion", "content": "Training Setup: To ensure comparability with prior methods, we establish the training dataset following the setup\noutlined in prior routing studies [19]. As for the depot-related setting, we adopt the data format prevalent in real-world\nLRP benchmark datasets which are conventionally employed in pertinent studies [10, 15]. Every single problem\ninstance in the training dataset is defined on a unit square [0, 1] \u00d7 [0, 1], where the customers' requests are uniformly\nscattered, with their corresponding demands uniformly sampled from [0, 10].\nThe problem instances for MDLRAM's pre-training are from three problem scales: n = 20,50,100 customers,\nrespectively coupled with m = 3, 6, 9 depot candidates. Corresponding to each scale: (1) The vehicle's maximum\ncapacity Q is selected as 30, 40, 50 respectively; (2) The vehicle's setup cost U is set as 0.3; (3) The depot's desired\nmaximum supply Mk is uniformly selected from [50, 80], [80, 120], [120, 170]; (4) The depot's opening cost Ok is\nuniformly selected from [2, 5], [2, 5], [12, 19]; The coefficients in objective function Eq. (1) are defined as a = 1, \u03b2 =\n1,8 = 2; For each scale, we train MDLRAM on one A40 GPU for 100 epochs with 1,280,000 problem instances\ngenerated on the fly as training dataset, which can be split into 2,500 batches with batchsize of 512 (256 for scale 100\ndue to device memory limitation).\nAs for DGM's problem instance, only including customer requests, we also consider three problem scales: n =\n20, 50, 100 customers. The expected distance among the generated depots ranges within [0.2, 0.7]. The coefficients in\nobjective function Eq. (3) are specified as X = 10, \u03b5 = 10. Correspondingly, for each scale, we train DGM on one A40\nGPU for 100 epochs. Within each epoch, 2,500 main-batches of problem instances are generated on the fly as training\ndataset and iteratively fed into DGM. In multivariate Gaussian distribution mode, the main-batch size Bmain is set as\n32 (16 for scale 100), and the sub-batch size Bsub for sampling in each distribution is selected as 128, 64, 32 for scale\n20, 50, 100 respectively. In exact position mode, where no sampling is performed, we set main-batch size as 512 (256\nfor scale 100)."}, {"title": "4.1 Results Analysis for Critic Model - MDLRAM", "content": "We first assess the efficacy of the pre-trained MDLRAM. As critic model, it is expected to instantly provide optimized\nLRP solution for serving customers based on generated depots, in batch-wise manner. Therefore, we test it on both\nSynthetic Dataset to highlight the instant batch-wise solving ability, and Real-world Benchmark Dataset to evaluate its\ngeneralization performance by comparing with SOTA results, which, so far, are all achieved by specifically designed\nheuristic methods.\n(i) Testing on Synthetic dataset: For each problem scale, the synthetic testing dataset include 10,000 problem instances\nrandomly sampled from the same configuration in training process, capable of being divided into batches to facilitate\nbatch-wise testing. Given the lack of existing DRL method specifically designed for standard LRP scenario setup,\nwe compare results from various enhanced classic heuristic methods, which are commonly applied to solve routing\nproblems, even though heuristic methods are not suitable for batch-wise usage considering its solving manner and\nunstable inference time. The parameters are tuned to align LRP setup to report the best performance."}, {"title": "4.2 Results Analysis for DGM", "content": "As for the DGM, based on a same group of customer requests devoid of predefined depot candidates, it is expected\nto generate a depot set D which can lead to lower total cost than the randomly attempted depots. Thus, we arrange\nan experiment to evaluate which of the following three strategies can identify the best depot set for a same group of\ncustomer requests: (i) Generating the depot set in DGM-Exact mode; (ii) Generating the depot set in DGM-Gaussian\nmode; (iii) Randomly attempting the depot set in batches. The quality of the depot set is judged by LGen (D) in Eq. (3).\nFor each scale n \u2208 {20, 50, 100}, we randomly sample 8,000 problem instances as testing dataset. Each instance only\nincludes a group of customer requests. The results are reported in two ways, respectively evaluating the average-level\nand best-level of the solution depot set generated by each method: (i) Average test: For each problem instance, these\nmethods respectively generate 512 solution sets of depots to serve the same group of customers accordingly. The mean\nof these 512 cost values is reported as the corresponding result for this problem instance. Then, the average of these\n8,000 mean costs are obtained as final result. (ii) Sampling test: Similarly, for each problem instance, these methods\nrespectively generate 512 solution sets of depots to serve the same group of customers, but only the lowest cost is\nreported as the result for that problem instance. The final result is derived as the average of these 8,000 lowest cost\nvalues. Notably, DGM's Exact mode directly generate deterministic solution depot set for given problem instance,\nthereby yielding identical outcomes for its both testing ways."}, {"title": null, "content": "All three methods undergo testing on same dataset,\nand their results are respectively decomposed and\ncompared in Table 3 with two testing ways: (1) Av-\nerage test aims at comparing the average level of\nthe solution depot set that each method can generate.\nObservations reveal that both DGM's two modes\ncan identify superior solution depot set than ran-\ndomly attempting, while the Exact mode exhibits\nbetter performance over the Gaussian mode; (2)\nSampling test further compares the best level of so-\nlution depot set that each method can achieve. The\nresults demonstrate that, when compared with \"ran-\ndomly attempting\" on the same problem instance,\nDGM's Gaussian mode can find better solution de-\npot set within the same sampling timeframe. As\nfor the DGM's Exact mode, its specific solution\nset consistently outperform those of the Gaussian\nmode, whereas the Gaussian mode can offer more\nflexibility with its sampling ability. Only one ex-\nception is observed in small scale (n = 20), where\n\"randomly attempting\u201d achieves better solution set without the aid of DGM, which may not be replicable at larger\nscales. It's worth noting that, the superior performance achieved by the generated depot set D not just simply reflects\non the total cost LGen(D) which is the sum of route length and the violation of distance range among depots, but\nalso respectively reflected on each individual cost items. This reflects DGM's ability on both identifying good depot\npositions and satisfying the location requirements, instead of only focusing on minimizing the violation of distance\nrange among depots, while neglecting the route length, to achieve \u201csuperior performance\u201d."}, {"title": "5 Conclusion and Future Work", "content": "In this study, we propose a generative DRL framework for depot generation without predefined candidates. Based\non customer requests data, the DGM proactively generates depots, while the MDLRAM efficiently plans routes from\nthese generated depots, demonstrating flexibility and cost reductions, especially in scenarios requiring quick depot\nestablishment and flexible adjustments. This modular framework"}]}