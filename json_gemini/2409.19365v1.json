{"title": "Conditional Image Synthesis with Diffusion Models: A Survey", "authors": ["Zheyuan Zhan", "Defang Chen", "Jian-Ping Mei", "Zhenghe Zhao", "Jiawei Chen", "Chun Chen", "Siwei Lyu", "Can Wang"], "abstract": "Conditional image synthesis based on user-specified requirements is a key component in creating complex visual content. In recent years, diffusion-based generative modeling has become a highly effective way for conditional image synthesis, leading to exponential growth in the literature. However, the complexity of diffusion-based modeling, the wide range of image synthesis tasks, and the diversity of conditioning mechanisms present significant challenges for researchers to keep up with rapid developments and understand the core concepts on this topic. In this survey, we categorize existing works based on how conditions are integrated into the two fundamental components of diffusion-based modeling, i.e., the denoising network and the sampling process. We specifically highlight the underlying principles, advantages, and potential challenges of various conditioning approaches in the training, re-purposing, and specialization stages to construct a desired denoising network. We also summarize six mainstream conditioning mechanisms in the essential sampling process. All discussions are centered around popular applications. Finally, we pinpoint some critical yet still open problems to be solved in the future and suggest some possible solutions.", "sections": [{"title": "I. INTRODUCTION", "content": "Image synthesis is an essential generative AI task. It is more useful when incorporating user-provided conditions to generate images that meet diverse user needs through precise control. Early works have made significant break-throughs in various conditional image synthesis tasks, such as text-to-image generation [37, 41, 156, 159, 239], image restoration [87, 88, 125, 210], and image editing [1, 10, 107]. However, the performance of conditional image synthesis with early deep learning-based generative models such as generative adversarial networks (GANs) [49, 131], variational auto-encoders (VAEs) [81, 185], and auto-regressive models (ARMs) [199, 200] is unsatisfactory due to their intrinsic limitations: GANs are vulnerable to mode collapse and unstable training [49]; VAEs often generate blurry images [81]; and ARMs suffer from sequential error accumulation and huge time consumption [200].\nIn recent years, diffusion models (DMs) have emerged as state-of-the-art image generation models due to their strong generative capabilities and versatility [20, 57, 71, 184, 191]. In DMs, images are synthesized from Gaussian noise through iterative denoising steps guided by the predictions of a de-noising network. This distinctive multi-step sampling process enables DMs to achieve remarkable generative performance characterized by stable training, diverse outputs, and exceptional sample quality. It also gives DMs a unique advantage in facilitating conditional integration compared to one-step generative models. These benefits have made DMs the tool of choice for conditional image synthesis, leading to rapid growth in the research on Diffusion-based Conditional Image Synthesis (DCIS) over the past few years [25, 45, 56, 75, 118, 160, 167, 168, 209, 242, 247]. \nThe rapidly expanding body of works, the numerous variations in model architectures, training methods, and sampling techniques, along with the broad scope of potential conditional synthesis tasks, make it challenging for researchers to grasp the full landscape of DCIS. This complexity can be particularly overwhelming for newcomers to the field. What is needed is a systematic survey that offers a comprehensive yet structured overview of this growing research area.\nThere exist several surveys on specific conditional image synthesis tasks, such as image restoration [238], text-to-image synthesis [103], and image editing [64], or classifying works in computer vision according to their target conditional synthesis tasks [32, 149]. While these task-oriented surveys provide valuable insights into approaches for their respective target tasks, they do not include the commonalities in model frameworks across different conditional synthesis tasks in terms of model architectures and conditioning mechanisms. Two recent surveys [14, 182] provide overview on DM-based works for a wide range of tasks in the field of conditional image synthesis. However, their scope remains limited as they primarily focus on DCIS works built on T2I backbones, neglecting earlier works that integrate conditioning into uncon-ditional denoising networks or involve training task-specific conditional denoising networks from scratch. These earlier efforts are foundational for the current advancements in DCIS using T2I backbones and are still widely applied in low-level tasks such as image restoration. Besides, [182] focuses most of its attention on the DM-based image editing framework and lacks systematic analysis on the unified framework for other tasks in this field while [14] does not delve deeper into the design choices in model architecture and detailed conditioning mechanisms for sampling process. This leads to a lack of systematization in their taxonomies and the omission of crucial related works in the field of DCIS.\nIn contrast, this survey aims to provide a comprehensive and structured framework that covers a wide range of current DCIS works by offering a taxonomy based on the mainstream techniques for condition integration in DCIS frameworks. We present a clear and systematic breakdown of the compo-nents and design choices involved in constructing a DCIS framework with condition integration. Specifically, we review and summarize existing DCIS methods by examining how conditions are integrated into the two fundamental compo-nents of diffusion modeling: the denoising network and the sampling process. For the denoising network, we break down the process of establishing a conditional denoising network into three stages. For the sampling process, we categorize six mainstream in-sampling conditioning mechanisms, detailing how control signals are integrated into various components of the sampling process. The objective is to give readers a high-level and accessible overview of existing DCIS works across diverse tasks, equipping them to design conditional synthesis frameworks for their own desired tasks, including novel tasks that have yet to be explored.\nThe remainder of this survey is organized as follows: we first introduce the background of diffusion models and the conditional image synthesis task in Sec. II. Next, we sum-marize methods for condition integration within the denoising network in Sec. III, and for the sampling process in Sec. IV. Finally, we explore potential future directions in Sec. V. \n illustrates the DCIS taxonomy proposed in this survey."}, {"title": "II. BACKGROUNDS", "content": "Diffusion-based generative modeling adopts a forward diffusion process of gradually adding noise into clean data and learns a denoising network to predict the added noise. In the sampling process, data is synthesized by reversing the forward process from Gaussian noise based on the prediction of a denoising network. We first introduce the core concepts of discrete-time and continuous-time diffusion modeling in Sec. II-A. Then, we discuss the model architecture in Sec. II-B and highlight representative DCIS tasks in Sec. II-C."}, {"title": "A. The Formulation of Diffusion Modeling", "content": "1) Discrete-Time Formulation: The discrete-time diffusion model was initially proposed in [184]. It constructs a forward Markov chain to transform clean data into noise by progressively adding small amounts of Gaussian noise so that a parameterized denoising network can be learned to predict the added noise in each forward step. Once the denoising network is trained, images can be generated from Gaussian noise by reversing the diffusion process. This idea gained popularity through an important follow-up work known as denoising diffusion probabilistic models (DDPMs) [57]. This work led to a substantial improvement in the quality of synthesized images and increased resolutions, from 32 \u00d7 32 [184] to 256 \u00d7 256, sparking a rapidly growing interest in diffusion models. Next, we adopt the notation from DDPM [57], which is widely used in the literature to describe discrete-time diffusion models [75, 160, 186].\nThe forward Markov chain is parameterized based on a pre-defined schedule $\u03b2_1,...,\u03b2_T$, where $\u03b2_t$ is the noise variance in each step and the total number of steps T is usually large, e.g., 1,000. Given the clean data sampled from the training dataset $x_0 \u223c p_{data}(x)$, the transition kernel is $q(x_t | x_{t-1}) = N(x_t; \\sqrt{1 - \u03b2_t}x_{t-1}, \u03b2_tI)$, or, $q(x_t | x_0) = N(x_t; \\sqrt{\\bar{\u03b1_t}}x_0, (1 - \\bar{\u03b1_t})I)$, where $x_1,...,x_T$ are latent vari-ables, $\u03b1_t = 1 \u2212 \u03b2_t$, $\\bar{\u03b1_t} = \u03a0_{i=1}^t \u03b1_i$, and $\\bar{\u03b1_T} \u2192 0$. By progressively adding Gaussian noise to the clean data, this Markov chain transforms the data distribution to an approximate nor-mal distribution, i.e., $\u222b q(x_T|x_0)p_{data}(x_0)dx_0 \u2248 N(0, I)$.\nIn the training phase, DDPM [57] learns a denoising network with parameter \u03b8 by minimizing the KL divergence between the transition kernel $p_\u03b8(x_{t-1}|x_t)$ and the posterior distribution $q(x_{t-1} | x_t, x_0)$. In practice, DDPM [57] is trained on the following re-parameterized loss function to improve the training stability and sample quality:\n$E_{x_0,\u03f5,t} [||\u03f5 - \u03f5_\u03b8(\\sqrt{\\bar{\u03b1_t}}x_0 + \\sqrt{1 - \\bar{\u03b1_t}}\u03f5, t)||^2]$, (1)\nwhere $\u03f5_\u03b8(x_t,t)$ is a noise-prediction network to estimate the added noise $\u03f5 = \\frac{x_t - \\sqrt{\\bar{\u03b1_t}}x_0}{\\sqrt{1 - \\bar{\u03b1_t}}}$ in each step. For the conditional generation that performs denoising steps conditioned on con-trol signal c, the conditional denoising network $\u03f5_\u03b8(x_t, t, c)$ can be trained on a loss function similar to Eq. 1.\nIn the sampling process, DDPM gradually generates clean data from Gaussian noise by computing the reverse transition kernel $p_\u03b8$ with the learned network $\u03f5_\u03b8$, i.e.,\n$x_{t-1} = \\frac{1}{\\sqrt{\u03b1_t}}(x_t - \\frac{1 - \u03b1_t}{\\sqrt{1 - \\bar{\u03b1_t}}} \u03f5_\u03b8) + \\sqrt{\\frac{1 - \u03b1_{t-1}}{1 - \\bar{\u03b1_t}}} \u03b2_t\u03f5_t$, (2)\nwhere $\u03f5_t \u223c N(0, I)$ is the standard Gaussian noise independent of $x_t$. The following work DDIM [186] proposed a family of sampling processes sharing the same marginal distribution $p(x_t)$ with the above sampling process, which are written as\n$x_{t\u22121} = \\sqrt{\\bar{\u03b1_{t\u22121}}} \u00b7 f_\u03b8(x_t) + \\sqrt{1 \u2212 \\bar{\u03b1_{t\u22121}} \u2212 \u03c3_t^2} \u03f5_\u03b8 + \u03c3_t\u03f5_t$, (3)\nwhere $f_\u03b8(x_t) = \\frac{x_t - \\sqrt{1-\\bar{\u03b1_t}}} \u03f5_\u03b8}{\\sqrt{\\bar{\u03b1_t}}}$ denotes the predicted $x_0$ at time step t. For simplicity, we will refer to $f_\u03b8(x_t)$ as the intermediate denoising output $x_{0|t}$ hereafter. Each choice of $\u03c3_t$ represents a specific sampling process in DDIM [186]. It is identical to the DDPM generative process in Eq. 2 when $\u03c3_t = \\sqrt{(1 \u2212 \\bar{\u03b1_{t\u22121}}) / (1 \u2212 \\bar{\u03b1_t})} \\sqrt{1 \u2212 \\bar{\u03b1_t}/\u03b1_{t-1}}$ and becomes a deterministic process when $\u03c3_t = 0$.\n2) Continuous-Time Formulation: Song et al [191] proposed to formulate a diffusion process ${x_t \u223c p_t(x)}_{t=0}^{T}$ with the continuous time variable $t \u2208 [0, T]$ as the solution of an It\u00f4 stochastic differential equation (SDE) $dx = f(x, t)dt + g(t)dw_t$, where $w_t$ denotes the standard Wiener process, and $f(x, t)$ and $g(t)$ are drift and diffusion coefficients, respectively [19, 143]. This diffusion process smoothly transforms a data distribution into an ap-proximate noise distribution $p_T$ and its specific discretization recovers the forward process of DDPM [57]. There exists a probability flow ordinary differential equation (PF-ODE) $dx = [f(x, t) \u2212 g(t)^2\u2207_xlog p_t(x)] dt$, sharing the same marginal distribution with the reverse SDE $dx = [f(x, t) \u2212 g(t)^2\u2207_xlog p_t(x)] dt + g(t)dw$ [20, 71, 191, 243]. Therefore, we can learn a time-dependent score-based denoising network $s_\u03b8(x_t, t)$ to estimate the score function $\u2207_xlog p_t(x)$ with a sum of denoising score matching [123, 202] objectives weighted by $\u03bb(t)$:\n$E_t [\u03bb(t)E_{x_0,x_t} [||s_\u03b8(x_t, t) \u2212 \u2207_{x_t}log p_t(x_t | x_0)||^2]]$. (4)\nWhen the score-based denoising network $s_\u03b8(x_t, t)$ is trained, we can employ general-purpose numerical methods such as Euler-Maruyama and Runge-Kutta methods to solve the reverse SDE or PF-ODE and recover clean data $x_0$ from $x_T$.\nIn the following sections, unless otherwise specified, we will use notation $\u03f5_\u03b8$ to represent the denoising network."}, {"title": "B. Architecture of the Denoising Network", "content": "Pioneering works adopted U-Net [161] as the denoising network architecture [57, 186, 189, 190]. A U-Net typically consists of an U-shaped encoder-decoder structure with skip connections. The encoder leverages a stack of residual layers and downsampling convolutions to reduce the spatial data dimension and the decoder upsamples the compressed data back to the original dimension. The U-Net architecture is advantageous for diffusion models due to its exceptional feature extraction, contextual understanding, precise segmen-tation, and dimensionality preservation property, which enables accurate modeling of complex data distributions for high-quality synthesis. Many followed-up works improved the vanilla U-Net architecture by incorporating multi-head attention [36, 140, 191], normalization [36, 57, 140], or cross-attention layers[160, 167]. Recently, transformers emerged as an alternative for denoising networks because of its capability in capturing long-range dependencies [39, 96], and have achieved success in DM-based works for many tasks including class-conditional generation [229], text-to-image generation [6, 51, 96, 176, 194], layout generation [16], and medical image generation [144]. In the following sections, unless oth-erwise specified, we assume the architecture of the denoising network adopts a U-Net structure."}, {"title": "C. Conditional Image Synthesis Tasks", "content": "A conditional image synthesis task T generates target image x by sampling from a conditional distribution:\n$x \u223c p_T(x|c), c \u2208 D_T$, (5)\nwhere $D_T$ is the domain of conditional input c, and $p_T$ is the conditional distribution defined by the task T. Based on the form of conditional inputs and the correlation between the con-ditional input and the target image formulated as conditional distribution $p_T(x|c)$, we classify representative conditional image synthesis tasks into seven categories."}, {"title": "III. CONDITION INTEGRATION IN DENOISING NETWORKS", "content": "The denoising network is the crucial component in the diffusion model (DM)-based synthesis framework, which estimates the noise added in each forward step to reverse the initial Gaussian noise distribution back into the data distribution. In practice, the most straightforward way to achieve conditional control in DM-based synthesis framework is incorporating the conditional inputs into the denoising network. In this section, we divide the condition integration in denoising network into three stages: (a) training stage: training a denoising network on paired conditional input and target image from scratch, (b) re-purposing stage: re-purposing a pre-trained denoising network to conditional synthesis scenarios beyond the task it was trained on, (c) specialization stage: Performing testing-time adjustments on denoising network based on user-specified conditional input. \nNext, we first review the fundamental conditional DMs modeled in training stage in Sec. III-A. We then summarize the architecture design choices and condition injection approaches in re-purposing stage in Sec. III-B. Finally, we introduce the works performing condition integration in specialization stage in Sec. III-C."}, {"title": "A. Condition Integration in the Training Stage", "content": "The most straightforward way to integrate the conditional control signal c into the denoising network is performing supervised training from scratch with the following loss function:\n$E_{c,x\u223cp(x|c),\u03f5,t} [||\u03f5 - \u03f5_\u03b8(x_t, t, c)||^2]$, (6)\nwhere c and x denote the paired conditional inputs and target image. Thereby, the learned conditional denoising network $\u03f5_\u03b8(x_t, t, c)$ can be employed to sample from $p(x|c)$.\nNext, we introduce the existing conditional denoising networks trained from scratch, focusing their model architec-tures, conditioning mechanisms, which are crucial for creating the connection between the conditional inputs and its cor-responding image. Because of the conditioning architectures and mechanisms are designed based on the target scenarios, we categorize these works based on the their applications, represented by text-to-image and image restoration."}, {"title": "B. Condition Integration in the Re-purposing Stage", "content": "Currently, diffusion models (DMs) are employed in in-creasingly diverse and complex conditional synthesis scenarios [11, 100, 179, 209, 225, 232, 242]. Simply training denoising networks from scratch for each conditional synthesis scenario would place a heavy burden on computational resources. Fortunately, pre-trained text-to-image (T2I) DMs associate text embedding with its corresponding image, which serves as a semantic powerful backbone for a wide range of conditional synthesis tasks beyond the T2I. Studies design task-specific denoising network based on T2I backbone and performing fine-tuning on paired conditional inputs and image to re-purpose the base T2I denoising network to the target task. In practice, the re-purposed denoising network can be divided into three key modules: (a) Conditional encoder: The module to encode the task-specific conditional inputs into feature embedding, (b) Condition injection: The module to inject task-related feature embedding into T2I backbone, (c) Backbone: The T2I backbone that can stay frozen or be fine-tuned during the re-purposing stage. In the re-purposing stage, conditional fine-tuning can be performed in each of these components for condition integration. Subsequently, we will summarize the design choice for these modules among current works performing condition integration in the re-purposing stage."}, {"title": "C. Condition Integration in the Specialization Stage", "content": "Although theoretically we can incorporate any form of conditional inputs c into the denoising network $\u03f5_\u03b8(x_t, t, c)$ during the training and re-purposing stages, for complicated conditional synthesis scenarios, incorporating such control signals into the conditional space of denoising network faces challenges in collecting annotated training dataset and mod-eling the complicated correlation between conditional inputs and desire results. This limits the model capability to deal with zero-shot or few-shot conditional inputs.\nA straightforward idea to remedy these issues is to align the given conditional inputs with the conditional space of a general T2I backbone through a specialization stage. \nIn practice, works perform condition integration in specialization stage are mainly targeted to image editing and customization tasks to achieve desired edits on user-specified visual subjects including source images(image editing) and personal objects(customization) while preserving the charac-teristics and details in these visual subjects [45, 75, 163]."}, {"title": "IV. CONDITION INTEGRATION IN THE SAMPLING PROCESS", "content": "In DM-based image synthesis frameworks, the sampling process iteratively reserve noisy latent variable into desired image with the prediction of the denoising network. As men-tioned in Sec. III, integrating the conditional control signals into the denoising network always requires time-consuming training, fine-tuning or optimization. To ease the burden for conditioning the denoising network, numerous works perform condition integration in the sampling process to ensure the consistency between synthesized image and given conditional input without computational intensive supervised-training or fine-tuning [25, 36, 56, 73, 111, 193]."}, {"title": "A. Inversion", "content": "In diffusion model (DM)-based image synthesis, the starting latent variable controls the spatial structure and semantics of synthesized result. Inversion process provides an effective way to encode the given source image back into its corresponding starting latent variable and effectively preserve the image structure and semantics for further editing. In this section, we firstly summarize the inversion approaches in Sec. IV-A1. Next, we will discuss the applications of inversion in various conditional synthesis scenarios in Sec. IV-A2."}, {"title": "B. Attention Manipulation", "content": "After determining the starting point for the sampling process via sampling from Gaussian distribution or inversion methods, the sampling process is performed by iterative denoising steps. As pointed out in E4T [46], the attention layers in the denoising network have the greatest influence on the predicted noise in each denoising step and thereby control the structure and layout of synthesized image. Therefore, a branch of works resort to design task-specific manipulation to the attention layers in denoising network to achieve more accurate control over the spatial layout and geometry [56, 118, 148, 196]. Different from the works [100, 232] performing fine-tuning on modified attention module in re-purposing stage, approaches in this category manipulates the attention layers via tuning-free replacement or localization during sampling process."}, {"title": "C. Noise Blending", "content": "Noise blending process fuses noises predicted by different (conditional) DMs to perform single sampling process controlled by multiple conditional signals."}, {"title": "D. Revising Diffusion Process", "content": "Most of in-sampling conditioning mechanisms such as Guidance, Conditional Correction and Attention Manipulation performs modification on the standard formulation of the de-noising step, which leads to deviations from the predetermined sampling trajectory and results in artifacts in synthesized images. Therefore, a branch of works prefer to incorporate the conditional control signals into the denoising step via revising the formulation of standard diffusion process to adapt the conditional synthesis task [73, 122, 212, 236]. Thereby, the conditional control signals can be incorporated into the corresponding reverse diffusion step of the revised diffusion process without deviations from the diffusion formulation.\nBased on the revision on diffusion process, these works can be divided into two categories: (a) mean-reverting SDEs, which revise the diffusion process to preserve the information in conditional inputs in image restoration, (b) decomposition-based noise redefinition, which incorporate a sequence of additive noises in the sampling process on spectral space to revise the noise-level mismatch in noisy linear problem."}, {"title": "E. Guidance", "content": "In the field of conditional image synthesis, an intuitive idea to sample from the conditional distribution $p(x|c)$ is approximating the conditional score function $\u2207_{x_t}log p_t(x_t | c)$ with conditional denoising network $\u03f5_\u03b8(x_t, t, c)$. Guidance provides another pathway to approximate the conditional score function without time-consuming conditional training, since the conditional score function can be decomposed into an unconditional score function and the gradient of log likelihood as follows:\n$\u2207_{x_t}log p_t(x_t | c) = \u2207_{x_t}log p_t(c|x_t) + \u2207_{x_t}log p_t(x_t)$ (9)\nwhere the score function $\u2207_{x_t}log p_t(x_t)$ can be estimated by an unconditional denoising network $\u03f5_\u03b8(x_t, t)$. Guidance-based methods design task-specific guidance loss function to reflect the consistency between intermediate latent variable $x_t$ and conditional inputs c at each time step t, which serves as the estimation for the log likelihood $log p_t(c | x_t)$.\nFor multiple conditional inputs, guidance can also be em-ployed to perform conditional control for part of the conditional inputs. In practice, we can split the conditional inputs c into components $c_0$ and $c_1$ which are incorporate into the diffusion synthesis framework with conditional denoising network and guidance respectively. In this case, the conditional score function can be written as $\u2207_{x_t}log p_t(x_t | c_0, c_1) = \u2207_{x_t}log p_t(c_1|x_t, c_0) + \u2207_{x_t}log p_t(x_t | c_0)$. In this formu-lation, $\u2207_{x_t}log p_t(x_t | c_0)$ can be estimated by a denois-ing network conditioned on $c_0$ and the log likelihood $log p_t(c_1 | x_t, c_0)$ can be estimated with the guidance loss.\nCurrently, guidance-based methods are employed in a wide range of conditional synthesis scenarios with designed task-specific guidance loss. Subsequently, we categorize these approaches based on the target applications."}, {"title": "F. Conditional Correction", "content": "In some conditional synthesis scenarios, the synthesized im-ages are controlled by the constrains specified by conditional inputs c (such as the formulation of inverse problems). To ensure the synthesized result to be consistent to the inputs c, conditional correction-based methods perform a correction operator on the intermediate diffuse output $x_t$ (or $x_{0|t}$), which directly projects the current diffuse output onto the data manifold satisfying the constrain imposed by given conditional control signal c. Subsequently, this corrected latent variable will be pass into next denoising step."}, {"title": "V. CHALLENGES AND FUTURE DIRECTIONS", "content": "Although DM-based conditional image synthesis has made remarkable progress in generating high-quality images aligned with various user-provided conditions, there remains a significant disparity between academic advancements and practical needs for conditional image synthesis. In this section, we summarize several main challenges in this field and identify potential solutions to address them in the future."}, {"title": "A. Sampling Acceleration", "content": "The time-consuming sampling process often creates a bot-tleneck of diffusion-based image synthesis, and its acceleration will facilitate the model deployment in practice [102, 254]. Early works on sampling acceleration are devoted to re-ducing the number of sampling steps with better numerical solvers [20, 116, 117, 186, 258] or distilling pre-trained diffusion models to build short-cuts that enable faster sam-pling [19, 129, 169, 188]. However, too few denoising steps with the distilled model may compromise the effectiveness of in-sampling condition integration. One feasible solution is to first train a model to approximate the conditional denoising outputs along the sampling process equipped with in-sampling conditioning mechanisms, and then perform distillation on this model [129]. Another important type of existing works reduces the computational cost of each denoising step by decreas-ing model parameters using techniques such as knowledge distillation [17, 18] and architecture search [77, 102, 254]."}, {"title": "B. Artifacts Caused by In-sampling Conditioning Mechanisms", "content": "In-sampling condition mechanisms summarized in Sec. IV allows for flexible condition integration in DM-based image synthesis without performing time-consuming condition inte-gration for the denoising network. However, these conditioning mechanisms introduce modification to the standard sampling process in diffusion framework and lead to deviations from the modeled data distribution, which resulting in artifacts in synthesized images [5, 120, 147, 234]. The vast majority of works resort to complex adjustment mechanisms to address the artifact issue caused by in-sampling condition integration. This includes time-step rolling back for guidance [234], localization for attention map [15, 118] and diffusion process revision for restoration tasks [73, 122]. However, these methods are highly customized based on specific application scenarios. A feasible future direction for developing more generic solution is to perform lightweight fine-tuning on the denoising net-work with the diffusion loss based on the intermediate latent variables in the sampling process equipped with in-sampling conditioning mechanisms. This tends to smooth out artifacts under in-sampling conditioning mechanisms and synthesize desire images in a lower computational cost comparing to perform condition integration in denoising network ."}, {"title": "C. Training Datasets", "content": "Among the various conditioning mechanisms, the most fundamental and effective pathway for condition integration is still the supervised learning on pairs of conditional input and image. Although training datasets are relatively sufficient for conditional synthesis tasks involving single modality con-ditional inputs, such as text-to-image [171, 172], restoration [2, 72, 139], and visual signal to image [13, 104, 257], gathering enough data for tasks with complex, multi-modal conditional inputs like image editing, customization, and composition remains challenging. With the advancement of training and efficient fine-tuning techniques for large language models, various types of large models are constantly being developed with powerful multi-modal representation learning [12, 93, 94] and content generation abilities [56, 196], making it possible to leverage these pre-trained models to automatically produce desired training datasets. We may also consider self-supervised or weakly supervised learning to reduce the demand for a large amount of high-quality training data [222, 246, 250]."}, {"title": "D. Robustness", "content": "Due to the lack of objective task-specific evaluation datasets and metrics in some complex tasks, studies for these tasks prefer to compare models based on a set of self-defined conditional inputs, making the performance appear overly optimistic. In fact, many renowned text-to-image models [155, 160, 167] have been found to produce unsatisfactory synthesized results for certain specific categories of text prompts, as demonstrated by the shortcomings of Imagen [167] in generating facial images. Training dataset augmentation, carefully designed architecture of conditional encoders, and improved conditioning formulation for fine-grained control are promising directions for enhancing robustness.\nHere we point out some pathways to address issues of robustness. First, for conditional inputs where the model performs poorly, augmenting the training dataset is a direct approach. Second, the difficulties to handle conditional inputs in a certain category may be due to the insufficient capability or unsuitability of the conditional encoder with this category of data. In this case, incorporating encoder architectures tailored for this data category into the conditional encoder, or designing more capable compound conditional encoders, becomes a preferable choice. Besides, performing specialization for given conditional inputs is also an effective pathway to provide robust results at the cost of time-consuming fine-tuning or optimization. Finally, employ sampling process conditioning mechanisms, such as guidance, conditional correction and attention manipulation, to achieve more detailed control can also prevent undesired synthesis results."}, {"title": "E. Safety", "content": "The developments in AI-generated content (AIGC) propelled by the superior performance of diffusion-based conditional synthesis and their downstream applications lead to severe safety concerns in aspects of bias and fairness, copy-right, and the risk of exposure to harmful content. Safety-oriented DM-based conditional image synthesis is dedicated to mitigating these issues by embedding watermarks that are easily reproducible in DM-generated images to detect copyright infringement [33, 217, 235], and reducing bias by increasing model's orientation towards minority groups in basic unconditional or text-conditioned synthesis via classic conditioning mechanisms, such as fine-tuning [175], guidance [197], and conditional correction [91]. Efforts have also been made in preventing harmful contents in the text-to-image task via harmful prompt detection [160], prompt engineering [91] and safety guidance [170]. The current safety-focused efforts mainly concentrate on basic unconditional or text-conditioned synthesis. We believe that for more complex conditional synthesis scenarios, safety-oriented efforts in this area can be focused on four main aspects: (a) detecting harmful conditional inputs, (b) filtering and removing bias from the training dataset, (c) providing safety-focused guidance for the sampling process, and (d) implementing safety-focused fine-tuning of the denoising network."}, {"title": "VI. CONCLUSION", "content": "This survey presents a thorough investigation of DM-based conditional image synthesis, focusing on framework-level con-struction and common design choices behind various condi-tional image synthesis problems across seven representative categories of tasks. Despite the progress made, efforts are still needed in the future to handle challenges in practical applications. Future researches should focus on gathering and creating sufficient high-quality and unbiased task-specific datasets, carefully designed conditional encoder architectures and in-sampling conditioning mechanisms for effective and robust conditional modeling to synthesize stable and flawless results. Trade-off between fast sampling and synthesization quality and is also a key issue for practical deployment. Finally, as a popular AIGC technology, it is necessary to fully consider the safety issues and legitimacy it brings."}]}