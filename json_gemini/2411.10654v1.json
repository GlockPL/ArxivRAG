{"title": "Pluralistic Alignment Over Time", "authors": ["Toryn Q. Klassen", "Parand A. Alamdari", "Sheila A. McIlraith"], "abstract": "If an AI system makes decisions over time, how should we evaluate how aligned it\nis with a group of stakeholders (who may have conflicting values and preferences)?\nIn this position paper, we advocate for consideration of temporal aspects including\nstakeholders' changing levels of satisfaction and their possibly temporally extended\npreferences. We suggest how a recent approach to evaluating fairness over time\ncould be applied to a new form of pluralistic alignment: temporal pluralism, where\nthe AI system reflects different stakeholders' values at different times.", "sections": [{"title": "1 Temporal Aspects of Pluralistic Alignment", "content": "In this paper we consider aligning an AI system's decisions with the values or preferences of multiple\nstakeholders, a topic that has been recently drawing attention (e.g., Sorensen et al., 2024; Conitzer\net al., 2024), in the context of sequential decision making. Sorensen et al. wrote that\n[W]e need systems that are pluralistic, or capable of representing a diverse set of\nhuman values and perspectives.\nMotivated by our context of sequential decision making, which takes place over time, we focus on\ntemporal aspects of alignment. There are a variety of ways that time may factor into evaluating how\naligned an AI system is, including (as illustrated in Figure 1)\n1. preference change over time,\n2. temporally extended preferences,\n3. and how pluralistic alignment may only be realizable over time, through acting to achieve\ndifferent stakeholder's interests at different times.\nEach of these points will be elaborated upon in the following subsections (and we encourage further\nresearch on them), but the third will then be the focus of the rest of this paper. We will adapt our\nrecently introduced framework for temporally extended fairness (Alamdari et al., 2024) to pluralistic\nalignment over time."}, {"title": "1.1 Preference change over time", "content": "The preferences of a society or group may change over time. One of the ways of operationalizing\npluralistic alignment suggested by Sorensen et al. (2024) (for LLMs) was Overton pluralism, where\nthe LLM responds to a query by giving all answers in the Overton window. However, the Overton\nwindow is often spoken about as shifting in reality (e.g., Astor, 2019). Furthermore, actions taken by\nAI systems could potentially influence such shifts, e.g., by changing enough individual preferences\n(Carroll et al., 2024) or by influencing demographic changes. Less diverse preferences might be\neasier to satisfy, which could give an AI system an incentive for manipulation."}, {"title": "1.2 Temporally extended preferences, norms, goals, and rewards", "content": "People may also have explicitly temporal preferences, i.e., preferences about how the state evolves\nover time. (There is a body of work in AI planning about satisfying (typically one agent's) temporally\nextended goals and preferences (e.g., Bacchus and Kabanza, 1998; Son and Pontelli, 2004; Baier\net al., 2009; Bienvenu et al., 2011).)\nTo illustrate, one might prefer that dessert be served after the main course, while someone else might\nwant to eat dessert first (Figure 1b). What order should the meal's courses be served in? For this case,\nthere may also be social norms or conventions to consider. Zhi-Xuan et al. (2024) have suggested\nthat AI systems should be aligned with norms rather than aggregated preferences. Norms may also\nbe temporally extended (e.g., Porfirio et al., 2018; Kasenberg et al., 2018; Malle et al., 2023).\nIn automated sequential decision making, it's common\n(for example, when using Markov Decision Processes)\nto represent preferences or other things to be opti-\nmized using a reward function that assigns a numeric\nvalue $R(s_t, a_t, s_{t+1})$ to each transition from a state $s_t$\nto another state $s_{t+1}$ using action $a_t$. However, it's\nalso possible to define a non-Markovian reward func-\ntion $R(T)$ that maps a whole trajectory of alternating\nstates and actions $T = s_1, a_1, s_2,\uff65\uff65\uff65,a_T,s_{T+1}$ to a\nreal number. This allows for rewarding temporally\nextended behaviors.\nNon-Markovian reward functions have been describ-\ning using temporal logics including LTL (e.g., Bac-\nchus et al., 1996; Thi\u00e9baux et al., 2006; Camacho et al.,\n2017) and reward machines (Toro Icarte et al., 2018,\n2022; Camacho et al., 2019). An example reward ma-\nchine is shown in Figure 2. In the context of alignment,\nZhi-Xuan et al. (2024) recently encouraged the con-\nsideration of temporal logics and reward machines to\nbetter represent human preferences.\nAnother temporal aspect of preferences is temporal discounting, where future rewards are valued\nless than current ones. Pitis (2023) considers aggregating the preferences of multiple stakeholders\nwho each have a Markovian reward function, but possibly different discount factors, and argues that\nin general the aggregation should be a non-Markovian reward function. Finally, we note that the\ndynamic reward MDPs used by Carroll et al. (2024) to model changing preferences also resemble\nreward machines."}, {"title": "1.3 Pluralistic alignment over time", "content": "Pluralistic alignment requires consideration of a collection of human preferences, but such preferences\nmay be conflicting, such that after any one decision, alignment is only with a subset of humans.\nIn a sequential decision making setting, such disparities can be mitigated by future actions. More\ngenerally, the decisions made by AI (or human) systems will often cause different people to be better\noff at different times. How should such temporal tradeoffs be evaluated to determine how good the"}, {"title": "2 A framework for pluralistic alignment over time", "content": "In this section we adapt the framework for temporally extended fairness from our previous work\n(Alamdari et al., 2024)\u00b9 to apply to the problem of pluralistic alignment.\nNotation When writing a trajectory of states and actions $T_T = s_1, A_1, s_2,..., a_T, s_{T+1}$, the\nsubscript T on \u03c4 (if present) indicates the number of actions in the trajectory. Given $\u03c4_t$, we can write\n$\u03c4_i$ (where i < T) for the prefix of $\u03c4_T$ ending with $s_{i+1}$.\nWhat we want is to be able to evaluate how well a trajectory of states and actions reflects the\npreferences of a collection of stakeholders. We previously had the notion of a fairness scheme\n(Alamdari et al., 2024, Def. 4.5), which we adopt as a temporal pluralism scheme:\nDefinition 1 (Temporal pluralism scheme). Given a state space S and action space A, a temporal\npluralism scheme for n agents is a tuple (U, Wex, B) where\n\u2022 $U : (S \u00d7 A)^* \u00d7 S \u2192 R^n$ is the stakeholder status function.\n\u2022 $Wex: (R^n)^* \u2192 R$ is the extended aggregation function.\n\u2022 $B : (S \u00d7 A)^* \u00d7 S \u2192 {0, 1}$ is the filter function.\nU's output, a vector of length n, is meant to measure how \"good\" the input (a trajectory $\u03c4$ of states and\nactions) has been for each of n stakeholders. For example, the measure could be, for each stakeholder,\nthe (discounted) sum of that stakeholder's rewards over the trajectory. Alamdari et al. assumed the\nith stakeholder has a (Markovian) reward function $R_i(s, a, s')$, but we note that a non-Markovian\nreward function $R_i(\u03c4)$ is just as compatible with the approach."}, {"title": "Definition 2 (Temporal pluralism score)", "content": "Given a trajectory $\u03c4_T = s_1,A_1,s_2,...,a_T,s_{T+1}$, the\ntemporal pluralism score of $\u03c4_T$ according to the temporal pluralism scheme (U, Wex, B) is\n$Wex (U(\u03c4_{t_1}), U (\u03c4_{t_2}),..., U (\u03c4_{t_k}))$\nwhere $(t_1, t_2,..., t_k)$ is the subsequence of (1, 2, . . ., T) for which $B(\u03c4_{t_i}) = 1$ for each i.\nThe extended aggregation function can be thought of as a temporally extended social welfare, which\nlooks at how well each stakeholder is doing at each given point in time.\nFor a simple illustration, consider a scenario (inspired by Lackner (2020)) where an AI assistant books\nrestaurants for the frequent joint dinners of a group of five friends with diverse culinary preferences.\nWe could evaluate the AI using a temporal pluralism scheme with the following components:\n\u2022 U(\u03c4) includes, for each stakeholder (member of the friend group), how often they went to a\nrestaurant of their preferred type over the course of the trajectory \u03c4.\n\u2022 $Wex (U_1, U_2,..., U_k) = Nash(U_{11},..., U_{1n}, U_{21}, ..., U_{2n}, . . ., U_{k1},..., U_{kn})$ where Nash\nis Nash welfare, a standard social welfare function (whose value is just the product of its\ninputs) and $u_{ij}$ is the jth entry of the vector $u_i$. That is, we compute the Nash welfare as\nthough each temporal version of each stakeholder were another individual.2\n\u2022 $B(\u03c4) = 1$ only on those time steps on which another ten restaurants have been visited.\nThe idea is to give higher scores to trajectories on which not only have a variety of restaurants been\nvisited in the long term, but also during the process (for each ten restaurants).\nAs Alamdari et al. noted, temporal pluralism schemes allow for long-term, periodic, and anytime\nevaluations.\nLong-term The score given by a long-term temporal pluralism scheme to a trajectory $\u03c4_T$ only varies\nwith the last stakeholder status U($\u03c4_T$). This ignores the evolution before the end (except\ninsofar as the stakeholder status captures it).\nPeriodic A periodic temporal pluralism scheme (with period p) ignores or filters out times\nthat are not a multiple of p, so that the temporal pluralism score is equal to\n$Wex(U(\u03c4_p), U(\u03c4_{2p}),..., U(\u03c4_{[T/p]p}))$. This might be desirable, for example, in a situa-\ntion where a robot is distributing goods to a group of people, and it takes a while for each\nround of deliveries to be completed (before the end of a round, it might seem like some\npeople are being ignored, and so that the robot is not aligned with them).\nAnytime An anytime temporal pluralism scheme is a periodic scheme with period 1. Depending on\nthe extended aggregation function Wex, achieving a high temporal pluralism score with such\na scheme may be difficult or impossible."}, {"title": "3 Conclusion", "content": "We have argued for further consideration of temporal aspects of alignment with multiple stakeholders,\nand suggested that in some cases it may only be possible to achieve pluralistic alignment, reflecting a\ndiversity of preferences or values, over time. We further suggested adapting the approach to temporally\nextended fairness from Alamdari et al. (2024) to the problem of pluralistic alignment. Further work\nis needed to investigate what specific temporal pluralism schemes would be most appropriate for\naggregating people's preferences. Frameworks that people use to organize the satisfaction of their\ninterests over time, like turn-taking and queuing, may be informative in making this choice."}]}