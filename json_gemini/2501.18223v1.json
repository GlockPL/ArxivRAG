{"title": "Exploring Large Protein Language Models in Constrained Evaluation Scenarios within the FLIP Benchmark", "authors": ["Manuel F. Mollon", "Joaquin Gonzalez-Rodriguez", "Alicia Lozano-Diez", "Daniel Ramos", "Doroteo T. Toledano"], "abstract": "In this study, we expand upon the FLIP benchmark-designed for evaluating protein fitness prediction models in small, specialized prediction tasks\u2014by assessing the performance of state-of-the-art large protein language models, including ESM-2 and SaProt on the FLIP dataset. Unlike larger, more diverse benchmarks such as ProteinGym, which cover a broad spectrum of tasks, FLIP focuses on constrained settings where data availability is limited. This makes it an ideal framework to evaluate model performance in scenarios with scarce task-specific data. We investigate whether recent advances in protein language models lead to significant improvements in such settings. Our findings provide valuable insights into the performance of large-scale models in specialized protein prediction tasks.", "sections": [{"title": "1. Introduction", "content": "Protein fitness prediction, the process of forecasting the functional impact of mutations on protein behavior, is a critical task in computational biology. Accurate prediction of protein fitness has significant implications for various fields, including drug design, protein engineering, and directed evolution. Directed evolution [1], in particular, relies on the ability to predict how mutations will influence protein properties, enabling the design of proteins with desired traits. However, traditional methods of protein fitness prediction, such as early machine learning (ML)-based approaches often require large datasets, limiting their application to certain scenarios.\nTo address this challenge, the FLIP (Functional Landscape of Interacting Proteins) [2] benchmark was developed as a specialized dataset tailored to predict protein fitness under constrained conditions. Unlike larger datasets, such as ProteinGym, which encompass a wide array of protein-related tasks, the FLIP dataset focuses on smaller, more specific tasks. FLIP uses data splits like two vs many and low vs high, providing a unique opportunity to study model performance in scenarios with lower mutation levels during training and higher mutation levels during testing. This differs from the ProteinGym [3] approach, which involves random sampling across tasks. The FLIP setup is important for real-world applications where models must generalize well to more complex, higher-mutation scenarios. This structure makes overfitting a bigger concern in FLIP compared to random sampling scenarios, as the model is trained on lower mutations and tested on higher mutations. While this does not inherently make one approach better or worse, it is a crucial factor to consider when analyzing training and evaluation metrics in this paper, especially for models with limited data and a need for generalization across diverse protein functions.\nRecent advancements in state-of-the-art models like ESM-2 [4] and SaProt [5] have shown promising results in addressing the challenge of requiring large datasets by using self-supervised pretraining. These models are trained on vast amounts of unlabeled protein sequence data, enabling them to capture patterns and structural properties of proteins without needing labeled datasets. This allows them to generalize well in low-data scenarios or zero-shot tasks. These models are increasingly being explored for protein fitness prediction tasks, where their ability to generalize from limited training data is highly valuable. In this study, we expand on the FLIP baseline by testing the performance of these recent high-performance models within the context of the FLIP benchmark, aiming to understand how they perform in small, task-specific datasets."}, {"title": "2. Dataset", "content": "Within the FLIP benchmark, several activities provide the foundational tasks for protein fitness prediction. Among them, three key activities are GB1, Meltome, and AAV. These three tasks used in this paper and summarized in this section represent distinct challenges in protein fitness prediction,"}, {"title": "2.1. AAV", "content": "The AAV task focuses on predicting the efficiency of adenovirus-associated virus (AAV) vectors used in gene therapy. The task involves predicting how mutations in the AAV capsid affect its ability to deliver genetic material to target cells. This is an important application in gene therapy, as optimizing the viral vector's efficiency can lead to better therapeutic outcomes. The AAV dataset consists of mutations that either enhance or reduce the viral vector's transduction efficiency, and the challenge is to predict these effects based on sequence data."}, {"title": "2.2. Meltome (Thermostability)", "content": "The Meltome activity involves predicting the melting temperature ($T_m$) of proteins, which serves as a proxy for protein stability. The melting temperature is an important characteristic that reflects the protein's ability to maintain its functional conformation under different conditions. The Meltome dataset includes a wide range of mutations across various proteins, and the task is to predict how these mutations will shift the $T_m$ value, indicating changes in protein stability. This task is particularly relevant for protein design, where maintaining stability is often a key goal."}, {"title": "2.3. GB1", "content": "The GB1 task focuses on the prediction of the stability of a single-domain antibody, GB1. This is a critical task for understanding antibody engineering. The dataset for GB1 contains mutations that are known to either stabilize or destabilize the protein, and the challenge lies in determining how these mutations influence the overall structural integrity and binding affinity."}, {"title": "3. Evaluated Models", "content": "In this study, we extend the evaluation of the FLIP benchmark to include additional state-of-the-art models and enhance training procedures to improve performance. Below, we summarize each model, including modifications made to ensure a comprehensive and fair assessment of model capabilities on the dataset."}, {"title": "3.1. ESM-10 [6]", "content": "The ESM 1v model was previously used in the FLIP baseline; however, in this study, we make several improvements. The training process was extended by increasing the number of epochs and employing a learning rate (LR) scheduler to dynamically adjust the learning rate based on model performance, with early stopping applied via a patience parameter to prevent overfitting. These modifications aim to optimize the performance of the model in protein fitness prediction tasks."}, {"title": "3.2. ESM-2", "content": "We evaluate three configurations of the ESM-2 model: a 6, 33, and 48 layer variant. Table 2 provides details regarding the parameter count for each model. The 6-layer and 48-layer models were selected to assess the smallest and largest ESM-2 configurations, allowing us to analyze how model size impacts learning capabilities and generalization power. Additionally, we included the 33-layer model as an intermediate option to compare the performance of ESM-2 with its predecessor, ESM-1v, since both have approximately the same number of parameters.\nESM-2, one of the latest iteration in the ESM model family, introduces architectural improvements aimed at enhancing depth and representational capacity, thereby improving performance on protein-related tasks. By benchmarking these ESM-2 configurations, we seek to evaluate the impact of model depth on fitness prediction tasks, particularly in terms of generalization, overfitting, and robustness across different dataset splits. This analysis not only updates benchmark results to reflect the capabilities of the latest ESM models, but also provides insights into the trade-offs between model complexity and performance, offering valuable guidance for selecting the most appropriate model for specific tasks in protein engineering."}, {"title": "3.3. SaProt", "content": "SaProt, a structure-aware model, is designed to incorporate structural information into the prediction of protein fitness. Specifically, it focuses on sequence patterns associated with both functional and structural protein attributes. By including SaProt in the benchmark, we aim to evaluate how well models can leverage structural information to enhance their predictions.\nThe pipeline used to implement SaProt consists of several steps. First, protein structure is predicted using ESMFold [4], an efficient alternative to AlphaFold2 (AF2). ESMFold generates predicted protein structures as atomic coordinate files in the Protein Data Bank format (\".pdb\"), which are then processed using FoldSeek [7] to convert these files into structure characters corresponding to each amino acid. This conversion allows the generation of a structure-aware sequence, enhancing the model's understanding of the protein's function in the context of its 3D conformation.\nIn our experiments, we use pLTTD (predicted Local True Template Distance) values as a measure of structural accuracy. In the first experiment, we use unmasked sequences, including all amino acids in their native forms. In a second experiment, we masked amino acids with a pLTTD lower than seventy. This allows us to assess the effect of masking less confident structural predictions on the model's ability to use structural information effectively. This approach provides insights into how SaProt's performance is impacted by varying levels of structural accuracy, highlighting the importance of structural quality for enhancing predictions in protein fitness tasks."}, {"title": "3.4. Considerations for a Fair Benchmark", "content": "To ensure fairness in model comparison, a strict distinction was maintained between training, validation, and test sets across all models. These splits were predefined and remain consistent across all evaluations, ensuring that no model sees the same sequence in both training and testing phases. Sequence-aware models were allowed to leverage the sequences to inform structural prediction, but no explicit structural information about the proteins was added to the model inputs beyond the sequences themselves. This consistency is critical for fair assessment, as it ensures that all models are evaluated on an equal footing, with no additional prior information about the proteins influencing the predictions."}, {"title": "4. Evaluation Framework", "content": ""}, {"title": "4.1. Training Details", "content": "For training, the models were used as embedding extractors. These resulting embeddings were then used to train a lightweight model designed to process input tensors, where each sequence is represented by a set of embeddings. This model was borrowed from the one used in the FLIP paper code. The 'Attention1d\u2018 layer computes attention weights over the sequence dimension, generating a weighted representation that effectively summarizes the entire sequence into a single tensor. This summarized representation is passed through a fully connected layer, followed by a ReLU activation to introduce non-linearity. Finally, the transformed embedding is fed into another linear layer, which outputs a scalar value used for the downstream classification task. For details on the model refer to Figure 1 and for pipeline implementation refer to Figure 2.\nThe lightweight models were fine-tuned for a maximum of 500 epochs, starting with an initial learning rate of 0.001. A learning rate scheduler was used to dynamically adjust the learning rate based on the validation loss, and early stopping with a patience of 20 epochs was implemented to prevent overfitting on the validation set. These settings were carefully chosen to promote consistent and stable convergence across all models, enabling a fair comparison of their performance under identical fine-tuning conditions. To evaluate consistency and variability in convergence, each model was trained over 10 independent experiments, using the same set of 10 random seeds for reproducibility.\nHowever, it is important to highlight that these measures do not completely prevent the model from overfitting to the specific distribution of mutations present in the training set. Such overfitting can negatively impact the model's ability to generalize to test samples with higher numbers of mutations."}, {"title": "4.2. Evaluation Metrics: Mean Squared Error and Spearman Rank Correlation", "content": "Mean Squared Error (MSE) and Spearman rank correlation were chosen as evaluation metrics to assess both predictive accuracy and rank reliability. MSE serves as a standard metric to gauge the absolute error in predictions, highlighting the model's overall accuracy by penalizing large errors. Meanwhile, Spearman rank correlation, a non-parametric measure of rank correlation, assesses how well the predictions preserve the order of protein fitness scores.\nIn protein fitness landscape applications, Spearman rank correlation ($\\rho$) is particularly valuable because it evaluates the model's ability to capture the relative ordering of fitness levels rather than absolute values, which are often less critical. This metric is widely used in fitness landscape modeling as it can effectively reflect whether a model correctly ranks sequences according to their relative fitness. High Spearman correlation values indicate that the model accurately captures the underlying fitness landscape, enabling researchers to predict protein stability and activity across sequence variations effectively. Given the typical challenges in protein modeling, including small sample sizes and noisy fitness measurements, the use of Spearman rank correlation provides a robust assessment of a model's generalization and consistency in ranking protein fitness across variations."}, {"title": "4.3. Implementation", "content": "The FLIP repository was used as the starting point for training and evaluating the ESM models. However, several modifications were made to adapt it to our specific requirements. These included adjusting the training settings to incorporate early stopping and a learning rate scheduler, as previously mentioned, as well as adding functionality to calculate embeddings for the newer ESM models. For SaProt, we utilized the data splits provided by the FLIP repository but developed new code for structure prediction using ESM-Fold and FoldSeek, along with implementations for training, evaluating, and metric summarization. The ESMFold and SaProt repository were used for structure and embedding prediction, respectively."}, {"title": "4.4. Computational Resources", "content": "For running experiments and accessing computational resources, we utilized the CCC (Centro de Computaci\u00f3n Cient\u00edfica - UAM). Specifically, we performed our tasks using four NVIDIA A100 80GB PCIe GPUs. For more information, please refer to the CCC website\u00b9."}, {"title": "5. Results", "content": "In this section, we present the results of each model's performance on the protein fitness prediction tasks, evaluated using mean squared error (MSE) and Spearman's rank correlation coefficient. The results shown represent the median values of ten experiments run per split, as the median is more robust to outliers compared to the average. Detailed visualizations of these metrics for each split can be found in the Appendix, where violin plots illustrate the distribution of MSE and Spearman's rank coefficients across splits and experiments.\nTables 3 through 11 present the evaluation results across different models and splits. The \u2018esm1v' results were not computed by us; rather, they were directly taken from the supplementary material of the original FLIP paper [2]. The results highlight the variations in predictive performance between models, providing valuable insights into the strengths and weaknesses of each architecture for protein fitness prediction. A general trend observed is a slight improvement in both performance and generalization ability with the newer ESM-2 models compared to their ESM1 counterparts of the same size, along with a more significant improvement in the larger ESM-2 models. Overall, SaProt and the ESM-2 model with 33 layers achieve the most consistent results. However, caution is needed, as they are more prone to overfitting than smaller or simpler models."}, {"title": "5.1. Meltome", "content": "The evaluation of the models on the Meltome datasets reveals several interesting trends and provides insights into the trade-offs between training"}, {"title": "5.3.1. Impact of data in training", "content": "To further assess how variations in the training data affect model performance, we focus on the results from the GB1 splits, where the splits clearly reflect the number of mutations in the training set. Figure 3 illustrate how different splits in the training set impact performance. From left to right, each split includes progressively more data, as well as information from a higher number of mutations per sequence. It is evident that, with each richer split, performance improves for all models and the gap between training and evaluation narrows. However, from the plots we cannot clearly indicate whether the improvement is primarily due to the increased amount of data or the additional information about the number of mutations per sequence. A more detailed analysis is needed to determine with greater confidence whether a higher number of mutations per sequence in the training dataset contributes to improved evaluation across a broader range of mutations. The results show that both the quantity of data and the diversity of mutations per sequence positively impact model performance, with richer training splits leading to improved results across all models."}, {"title": "6. Conclusions and future work", "content": "In this study, we extended the FLIP benchmark by evaluating newer and larger models, including ESM-2 (in various sizes) and SaProt. By leveraging FLIP's unique framework-training on low-mutation data and testing on high-mutation data\u2014we assessed the ability of these models to generalize across different mutation landscapes. Our findings reveal that strong performance on low-mutation training and validation sets does not always translate to better generalization on high-mutation test sets. This highlights the critical need to prioritize generalization performance when applying these models in real-world scenarios.\nAnother important consideration in model selection is computational efficiency. While ESM-2 models were straightforward to implement, SaProt required a more complex pipeline due to its structure-aware design. Despite this, SaProt achieved superior correlation in the training set, whereas ESM-2 excelled in maintaining performance on the test set. These differences underscore the impact of training strategies and fine-tuning decisions. For example, reducing training duration or deprioritizing validation loss could mitigate overfitting and improve generalization to data with higher mutation rates.\nLooking ahead, future research should focus on understanding how mutation density affects generalization loss in large protein models. By addressing this challenge, we can better tailor these models for practical applications, enhancing their robustness and effectiveness in real-world scenarios."}]}