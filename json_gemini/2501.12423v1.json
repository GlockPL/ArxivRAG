{"title": "FREYR: A FRAMEWORK FOR RECOGNIZING AND EXECUTING YOUR REQUESTS", "authors": ["Roberto Gallotta", "Antonios Liapis", "Georgios N. Yannakakis"], "abstract": "Large language models excel as conversational agents, but their capabilities can be further extended through tool usage, i.e.: executable code, to enhance response accuracy or address specialized domains. Current approaches to enable tool usage often rely on model-specific prompting or fine-tuning a model for function-calling instructions. Both approaches have notable limitations, including reduced adaptability to unseen tools and high resource requirements. This paper introduces FREYR, a streamlined framework that modularizes the tool usage process into separate steps. Through this decomposition, we show that FREYR achieves superior performance compared to conventional tool usage methods. We evaluate FREYR on a set of real-world test cases specific for video game design and compare it against traditional tool usage as provided by the Ollama API.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have become an essential part of numerous applications, ranging from domain-specific chatbots [1, 2, 3] to code generation [4, 5, 6, 7] and content creation [8, 9, 10]. While their ability to generate text is remarkable, LLMs can be extended beyond these interactions through tool usage. Tools allow LLMs to execute functions, run code, or interact with external systems, enabling them to not only produce responses but also perform computations, generate content, or interface with specialized domains. This capability makes LLMs that support tool usage significantly more versatile, allowing them to reason and act directly [11, 12, 13].\nNot all LLMs however support tool usage. Current approaches to enabling tool usage often fall into two main categories. The first involves instruction prompting, where a description of how to format a response is given to the LLM. The response is then parsed to execute different predefined tools. This method is model-specific and lacks versatility across different tools and scenarios, but can handle a dynamic set of tools presented to the model at runtime. The second category involves fine-tuning the model to condition the response format to interact with a predefined set of tools. This method improves performance but requires significant resources to fine-tune the model and reduces the model's generalizability. Moreover, both approaches face challenges when a large number of tools is given to the model at once. As tools are described to the LLM as text, they must fit in the model's input context size. Open-source models often are unable to fully utilize their full context length [14], and even large proprietary model's performance starts degrading after 10 to 20 tools [15].\nTo address these limitations, we propose a \"Framework for Recognizing and Executing Your Requests\u201d (FREYR). FREYR falls into the first category of approaches for tool usage, however it modularizes the pipeline for generating responses via tools into discrete, explicit steps. By decomposing the pipeline into its core phases, FREYR enhances the efficiency and reliability of tool-enabled LLMs. This modular approach allows for greater flexibility in adapting to new tools and reduces reliance on model-specific or resource-intensive methods. Furthermore, FREYR leverages consumer-level open-source LLMs, ensuring compatibility with local environments and avoiding third-party dependencies. This makes FREYR an accessible solution for a broader range of users.\nWe evaluate FREYR on the LLMaker test set, a content creation domain where the LLM is tasked to iteratively refine video game assets following a designer's requests. We compare FREYR performance against readily-available tool usage provided by the popular Ollama API [16].\nWe release the full implementation of FREYR on GitHub at https://github.com/gallorob/freyr. This open-source release includes the full codebase to reproduce and evaluate our results, and allows extensions of the base framework."}, {"title": "2 Related Work", "content": "In this work we introduce a novel approach to allow LLMs to use tools (Section 2.1) that leverages intent recognition (Section 2.2). We test our approach on the test cases first introduced in LLMaker (Section 2.3)."}, {"title": "2.1 Tool Usage", "content": "The ability of LLMs to use external tools has recently garnered much attention. Tool usage allows LLMs to integrate external knowledge into their responses [17, 18], or even perform tasks that go beyond pure text generation [19]. Enabling LLMs to use tools initially required collecting a large data set of human annotated tool interactions, which was resource intensive and time consuming. This was the approach used in Toolformer [20]. More recent work instead focused on generating such datasets using other LLMs, such as in GPT-4Tools [21].\nOne prominent application of tool usage for LLMs is generating and executing API calls, which are functionalities provided to clients by a server or application. This can be problematic for LLMs as there can be thousands of possible functions to choose from [17, 22], requiring smart ways to only provide a handful of tools to the LLM, so as not to exceed the LLM's context length. Gorilla [18], for example, leveraged a retrieval-augmented generation (RAG) approach to select relevant tools, whereas ToolPlanner [23] clustered tools by similarity.\nThese existing frameworks however are limited because they rely on static fine-tuned models or heuristic-driven prompt templates. The popular Ollama API platform instead enables tool usage by ad-hoc prompting each of the officially supported LLMs\u00b9. ReAct [24] instead prompted LLMs to generate interleaved verbal reasoning traces between tool calls, which improved tool usage accuracy. However these approaches still struggle to correctly generate the correct response when presented with a large number of diverse tools. This is because they require the full description of the tools. The framework we propose is instead more scalable, as the choice of tools to execute requires a shorter description to include in the prompt. Additionally, we target existing off-the-shelf LLMs to correctly execute tool calls without the need for a RAG system, further reducing the need for resources both on train and deployment of the system."}, {"title": "2.2 Intent Recognition", "content": "Humans are very good at expressing the same concept in a myriad different ways [25], which makes it very difficult to build computing systems that are able to correctly identify and process requests. The core idea behind intent recognition is learning a mapping between human expressions and grounded, distinct intents. This mapping can be learned by deep learning models [26], which however can suffer from lack of adaptability and robustness to unseen tools. More recent advances improved intent recognition using relationship meta-features [27], and leveraging the popular transformer architecture [28, 29]. These methods excel in structured environments but often require substantial pre-training or, as black-box models, lack explainability.\nWith the surge in popularity of LLMs, recent work has demonstrated their efficacy in the task of intent recognition. LLM agents can predict intents from a pre-defined set [30], or leveraging external knowledge bases [31]. Fine-tuning LLMs for intent detection specifically has also been explored with promising results [32]. Acting upon detected intents also improves LLMs explainability, as highlighted by Wang et al. [33].\nIn this work, we rely on intent detection to interpret user requests, leveraging the LLM's commonsense implicit knowledge and providing a set of possible intents in the prompt. This type of in-context learning requires less resources than fine-tuning a model, and can overcome more rigid classification system and heuristic-driven intent mappings."}, {"title": "2.3 LLMaker", "content": "LLMaker [34] is a mixed-initiative content creation tool that utilizes LLMs to assist human users in designing video game levels for the reverse dungeon crawler game Dungeon Despair. Inspired by Darkest Dungeon (Red Hook Studios, 2016), the game features heroes battling enemies within rooms and corridors of a dungeon.\nIn their introductory paper, Gallotta et al. [19] compared different prompting methods to edit the game level, evaluated on GPT-3.5-Turbo 1106. Using a set of test cases that mimicked a designer interacting with the tool, Gallotta et al. determined that function calling was the best performing approach. At its core, LLMaker relies on a predefined set of functions that are called through natural language instructions provided by the human user. During design, the LLM uses context and domain constraints to generate function names and necessary arguments for each call. These functions impact the game's level by adding, editing, or removing rooms, corridors, enemies, traps, or treasures. Once a call is completed, the LLM provides a brief summary of the changes to the user. If a function fails to execute, a functional error [35] is instead returned to the LLM, which can then decide whether to retry calling the function with different parameters or inform the user of the issue."}, {"title": "3 A Framework for Recognizing and Executing Your Requests", "content": "As introduced in Section 2.1, LLMs can use tools by generating a response that contains the tool name and the required parameters. Responses generated via tools require at least two LLMs inference passes: the first to generate the tool call parameters, and the second to report to the user the result of the tool call. However, this implies that all tools properties are available to the LLM at once, significantly increasing the number of tokens present in the prompt. While this approach is ideal for use cases where a small number of tools is available to the LLM, it does not scale well with multiple tools, requiring larger and larger context windows.\nIn this work we instead suggest that the tool calling pipeline can be split into separate steps, requiring at best only one additional inference step. Let us imagine that an LLM receives a request from the user and has only one tool at its disposal: the first thing it must do is decide whether a tool should be used. This first step has been explored explicitly for RAG applications, letting the model decide whether it should retrieve information from a knowledge base [36]. If not, the LLM generates a response in natural language. If instead a tool should be used, the LLM must generate the parameters for the tool. It then receives the tool execution result, and provides a summary response in natural language to the user. FREYR integrates this pipeline explicitly with four separate modules: one to determine whether a tool should be used (LLMintent), one to generate the tool parameters (LLMparameters), one to summarize the tool outputs (LLMsummary), and one to process conversations (LLMchat). An overview of the entire FREYR pipeline is presented in Algorithm 1. We note that we do not enforce a structured generation of the outputs, instead specifying a more free-form response format (comma-separated or bulleted list) that we parse afterwards\u00b2. This \u201cdivide and conquer\" approach allows us to select up to four different LLMs for each sub-task, giving great flexibility to the overall system and allowing for specific optimizations to take place. For example, LLMchat can be a very small LLM that is very good for conversation, while employing a larger LLM for the LLMparameters role that is more suitable for generating the parameters required by the tool. This is in direct contrast with current approaches, where the same LLM is used for the entire tool calling pipeline."}, {"title": "4 Experimental Protocol", "content": "In this work we assess the performance of the proposed framework, evaluating it on the existing LLMaker test set [19]. The test set is comprised of five test cases (T1 to T5) that mimic a designer's interaction with the tool. The test cases have an increasing number of requests and grow in complexity, with T5 being the hardest. The tool set is comprised of a total of 16 functions to add, edit, or remove rooms, corridors, enemies, treasures, traps, and enemy actions. As baseline comparison we run the same tests with the models that support tool usage via the Ollama API (henceforth, \"Tools\"). The JSON schema of the functions is approximately 3933 tokens as tokenized by GPT-24. We include in Table 1 the full test case 5 of the set of requests as an example.\nIn this work, we set to answer the following research questions:\nRQ1 How does FREYR perform compared to Tools?\nWe are interested in comparing the performance of FREYR against Tools. For each of the test cases, we count the number of steps that lead to a successful edit (i.e.: an edit that matches the designer's request). In their initial LLMaker paper, Gallotta et al. [19] considered different LLM failure modes in level editing, including parser, domain, and design failures. As they were concerned with editing levels in a way that would match the designer's request, they would terminate the execution of a test case in case of a parser or domain failure, as any further change to the level would lead to an invalid output. We instead do not terminate the test case as soon as an error is generated. We execute each step of the test case, providing a predefined starting level for each step to properly assess the edits performed by the LLM. These hand-authored levels are always valid and ensure a consistent evaluation. After each step, we verify whether the LLM's changes satisfy both domain and design requirements, which are predefined based on Gallotta et al. [19]. A step is considered successful if it updates the level in a way that passes both domain and design validity checks. For example, for the second step in T5, \"Add a goblin archer in the first room\" (see Table 1), the initial level for this step contains three empty rooms (one set in \u201cRome\u201d, one in \u201cParis\u201d, and one in \u201cBarcelona\u201d) and the step is considered successful if the only change is that the first room (the one in \"Rome\") now contains an enemy.\nRQ2 Does FREYR require fewer tokens overall than Tools?\nLLMs operate by attending to tokens in input and generating a sequence of tokens as outputs. The input tokens generally consist of the system prompt provided to the LLM and the conversation messages. In Tools mode, the full description of each function is passed in JSON format in the system prompt, and additional commands are injected in the system prompt to condition the LLM to generate valid tool calls if needed. The number of input tokens then scales with the number of tools available to the LLM, which can lead to degraded performance. FREYR, instead, introduces an overhead because of multiple roles. Each role has a specific system prompt with its role's task definition. While this impacts the number of input tokens, each role also results in at least one output each, which affects the number of generated tokens. In this work we keep track of both the number of tokens in input (Tokensin) and output (Tokensout), as measured by the Ollama API, summing them across roles. Keeping the counts separate allows us to better understand where the overhead introduced by FREYR affects performance and gives a clearer indication of the scaling capabilities of either approach with a larger number of tools available.\nRQ3 IS FREYR slower than Tools?\nAs introduced in Section 3, our framework assigns different roles to different LLMs that result in multiple responses being generated before a final output is returned by the system. Similarly, Tools also generates the tool calls before evaluating them. We are interested in comparing the overhead introduced by FREYR against Tools, as both systems suffer from a longer response time depending on the number of tool calls executed and their complexity. In this work we measure the cumulative time required to obtain a response from the system. For FREYR particularly, this time is the sum of time taken to generate a response by each role. For both modes, FREYR and Tools, we measure time in seconds even when a response results in an error. In case the error is generated internally, for example when a parameter for a function is not generated, both modes are instructed to try again and regenerate their response. The time it takes to regenerate a response until a valid one is produced is also tracked. If no valid response is generated after the maximum number of allowed retries, the test case step is marked as failed, but we still include that test case step's time in our results. In general, we want to obtain responses quickly to allow for real-time content editing. Here, we can also compare our results against the GPT-3.5 Turbo performance reported by Gallotta et al. [19]."}, {"title": "5 Results", "content": "In this section we report the results of FREYR and tool usage via Ollama API (Tools) approaches on the LLMaker test set (Section 5.1). We then analyze the effects of models choice in the different blocks of FREYR (Section 5.2). Finally, we comment on the quality of the generated content from a designer perspective (Section 5.3).\n5.1 Performance of FREYR against Tools\nWe evaluated the performance of FREYR and compared it against Tools on the LLMaker test set. We present our results in Table 2. A Wilcoxon signed-rank test with Bonferroni correction (p < 0.05) was conducted to determine statistical difference between results.\nFinding 1 FREYR consistently outperforms Tools in handling requests.\nWhen focusing on the number of steps completed successfully per test case (Steps (%)), we can see that FREYR outperforms its counterpart in Tools in 13 out of 15 cases, completing more steps. The two cases where FREYR underperforms are with Llama 3.1 and Qwen 2.5 on T5, which is the most difficult test case. We can see that, regardless of the model, the Tools approach only achieves at most 47% and as low as 17% of completed steps. FREYR can almost (98%) complete entire test cases (T2) or reach 80% to 90% completion rates (T1 and T3). The generally best performing model is Command-R, which is also the largest model. Except in T1, where Llama 3.1 scores higher, Command-R dominates in all other test cases, with Qwen 2.5 close behind (T2 and T3). However, understanding failures is of equal importance as highlighting success. Reading through the results logs, we find that most of the Tools failures are due to repeated wrong tools parameters being generated. This type of failure is handled by querying the LLM to regenerate the response, providing it with details on the previous error. While we found that sometimes errors would be fixed in one or two retries, most instances would fail to solve the error, resulting in a failure due to exceeding the maximum number of retries allowed. For FREYR, instead, we highlight the failure case of Llama 3.1 in T5, which is the only case where Tools outperforms FREYR. Here, the model fails by generating too many intents, which triggers an early termination. The problem of token repetitions in LLM responses is well known [37], and can generally be avoided with careful parameter tuning.\nFinding 2 FREYR consumes less tokens than Tools, but tends to generate more tokens.\nWhen looking at tokens, we find that FREYR uses fewer tokens in input (Tokensin) than Tools, ranging from 57% of the tokens in the case of Llama 3.1 in T3 to 20% in the case of Qwen 2.5 in T3. This trend is consistently found across all models tested on all test cases. Qwen 2.5 notably uses the fewest tokens in input on all test cases. For the output tokens (Tokensout), results show a different trend. While Qwen 2.5 in FREYR generates fewer output tokens than its Tools counterpart on four out of five test cases, this difference is significant only in two test cases (T2 and T3). Command-R and Llama 3.1 tend to generate more output tokens with FREYR than with Tools, although the differences are not significant. Command-R also generates the fewest output tokens in Tools mode for T4 and T5, though again not significantly. We will revisit whether this behavior leads to better responses in Section 5.3. We highlight two particularities found in T5, the hardest test case. First, Qwen 2.5 in FREYR generates more than double the output tokens of its Tools counterpart, which is in stark contrast with its performance on the other test cases. Secondly, Llama 3.1 in FREYR also generates more than double the output tokens than its Tools counterpart. In both cases we note that the results are extremely noisy, likely due to the random initialization of the models. However, this should not be a surprise: as mentioned above, the failure of Llama 3.1 on this test case is due to the generation of too many intents, which impact the number of tokens generated. For Qwen 2.5, instead, we find from the results logs that this higher-than-usual number of output tokens is due to a failure of self-correcting ill-formed tool calls. This also explains the fewer successful responses.\nFinding 3 FREYR has consistently and significantly lower response times than Tools.\nFinally, we can look at the average step time elapsed across test cases. As the speed at which a step is completed is tightly related to the speed at which prompts are processed and responses are generated, it comes at no surprise that the models with fewer input and output tokens also achieve faster response times. We find that Qwen 2.5 in FREYR is significantly faster than its Tools counterpart, with elapsed time savings that range from as little as 52% on T5 to as much as 70% on T3. Llama 3.1 instead achieves comparable times in either modes, with a significant difference only on T1 and T4. The most interesting outcome of these results is however Command-R. We find that this model in FREYR consistently outperforms its Tools counterpart, with times ranging from approximately 11% (T2 and T5) to 16% (T1, T3, and T4) of the elapsed time in Tools. These time reductions are more impressive than the ones for any other model tested. However, we note that the actual time taken by Command-R in FREYR mode may be too much for a real-time application, as it is too close to the upper bound of 10 seconds. Its Tools counterpart is simply unusable in real-time applications, as it can take up to a minute before a step is completed. Qwen 2.5 and Llama 3.1 in FREYR instead generate responses in between 2 to 5 seconds for Qwen2.5, and between 3 to 8 seconds for Llama 3.1. Their percentage of steps completed exceeds their Tools counterparts in all test case except T5. While completing less steps than Command-R in FREYR, the trade-off between speed and accuracy make both of these two models great candidates for real-time applications\nWe can also compare FREYR with GPT-3.5 Turbo, based on the results reported by Gallotta et al. [19]. In their paper, they report that tool usage with GPT-3.5 Turbo consistently solved all test cases with no failures. Interestingly, the reported time per request on each test case ranges from approximately 5 to 10 seconds, which are in line with the time taken by the open-source models in Tools mode. However, with FREYR, we achieve lower average response times than GPT-3.5 Turbo, though we do not achieve the same completion rates.\n5.2 Models Sensitivity in FREYR\nOne of the key strengths in FREYR is its separate modules, that allow for easy drop-in replacement and combinations of LLMs to take place. While in the previous section we analyzed the performance of FREYR when using the same LLM for both intent recognition and parameters generation steps, here we instead focus on the effects that different models have when playing different roles. We also evaluate the performance of FREYR using a model that is not supported directly for tool usage via the Ollama API. Here, we use Gemma 2 (7B). We include the full table with results in Appendix A.\nFinding 4 The choice of different LLMs for different roles does not significantly affect performance in FREYR.\nWe find that employing separate models for intent detection (LLMintent) and parameter generation (LLMparameters) results in a modest performance improvement compared to using the same model for both roles. For instance, Qwen 2.5 alone achieves only 89% of completed steps on T2, but it successfully completes the test case when paired with either Command-R or Gemma 2 as LLMparameters. Similarly, Llama 3.1's performance on T3 increases dramatically from 60% to 99% when Gemma 2 is used as LLMparameters. However, not all configurations lead to improvements: in T4, Qwen 2.5's performance drops from 52% when handling both roles to 51% when paired with Command-R as LLMparameters. Despite occasional drops, performance gains outweigh losses overall. The largest improvement is a 33% increase by Llama 3.1 as LLMintent and Gemma 2 as LLMparameters on T3, while the largest decrease is a 17% drop by Command-R as LLMintent and Llama 3.1 as LLMparameters on T2.\nWhen focusing on configurations where Gemma 2 serves as LLMparameters, we find that these pairings achieve some of the highest percentages of completed steps across test cases, often nearing or reaching 100% (86% on T1, 100% on T2, 100% on T3, 73% on T4, and 55% on T5). However, some combinations are slower to generate responses. For example, in T3 and T5, Qwen 2.5 as LLMintent is the second slowest configuration (13.7 seconds and 15.1 seconds for T3 and T5, respectively). Gemma 2 also performs exceptionally well when paired with itself for both LLMintent and LLMparameters, consistently achieving near-perfect completion rates (e.g., 100% in T3 and T5). Additionally, this configuration is faster than Qwen 2.5, making it a strong choice for time-sensitive tasks without compromising accuracy.\nThat said, Gemma 2's standalone performance shows variability. For example, on T2, it completes 11% fewer steps than the best-performing configuration, which uses Qwen 2.5 as LLMintent and Gemma 2 as LLMparameters. Similarly, on T5, it completes 17% fewer steps compared to the best configuration, which pairs Command-R as LLMintent with Gemma 2 as LLMparameters. These results highlight trade-offs between performance and efficiency. For instance, using Llama 3.1 as LLMintent with Qwen 2.5 as LLMparameters achieves the fastest response times in four out of five test cases (except T5), but ranks last in terms of completed steps.\nThe combination of Gemma 2 as LLMintent and Llama 3.1 as LLMparameters resulted in the lowest input token usage in two out of five test cases (T1 and T5). Notably, the configuration that processes the fewest input tokens also tends to generate the fewest output tokens across test cases. This trend differs from the results shown in Table 2, where, for instance, in T5, Command-R generates 252 tokens compared to Qwen 2.5's 794 tokens, despite attending to nearly 2,000 more tokens. Additionally, certain model combinations further optimized token usage. For example, in T4, the pairing of Llama 3.1 as LLMintent and Command-R as LLMparameters processed only 60% and 55% of the input tokens, respectively, and generated just 39% and 45% of the output tokens compared to their standalone performance.\nOverall, the results indicate that while there are exceptions and nuances based on specific configurations and test cases, the choice of different LLMs for different roles can lead to minor improvements. As there is no specific combination that performs better across all metrics on all test cases, the configuration must be selected in an ad-hoc fashion based on specific performance criteria.\n5.3 Creativity: Beyond Validity\nSo far we have been concerned with the response and the performance of the framework. In LLMaker, however, the response provided by the system (generated by LLMsummary in FREYR) is only half the feedback to the designer. The application generates content as well, setting its properties that ultimately drive the creative process. One interesting aspect of the generated content that we did not evaluate objectively is its quality. If the model produces uninteresting content, it would lead to a less appealing interaction for the user. If instead the model generates serendipitous content, the user would be more prone to explore different design choices. There are many and different measures of creativity, each with its own limitations [38]. Here however we just aim to give a qualitative evaluation, according to the authors' judgment. Table 3 reports two example contents edited during a test case step per model sampled at random from all runs and test cases.\nFinding 5 Content generated via FREYR is more creative than that generated via Tools.\nBased on Table 3, we can see that Tools seem to reuse heavily any detail introduced in the query: for example, Command-R in Tools sets the description of the new room as \"filled with ancient runes on the floor\" verbatim, whereas the FREYR version of the same model generates a more complex description while adhering to the description provided in the query. Similarly, we find that Tools simply gives numbers to differentiate content of the same type (\"Gob 1\" and \u201cGob 2\u201d), while FREYR occasionally fails to generate fully distinct content within the same type (\u201cSwampySnake\u201d and \u201cEnemy SwampySnake\u201d). These failures often require users to intervene manually. For instance, the user may request to rename entities to avoid ambiguity. Such interruptions can slow down the design process and risk frustrating the user, particularly if these issues occur frequently.\nNonetheless, we find that FREYR generates more creative content. We believe that this creativity is is able to emerge because the generation of parameters in FREYR occurs in its separate step, which lets the LLM focus on generating more freely. With Tools, instead, this generation occurs in a more restrictive frame, as the LLM is requested to produce a valid JSON as response."}, {"title": "6 Discussion", "content": "From the presented results, we find that our proposed framework, FREYR, consistently outperforms the existing approach to enable LLMs to use tools via the Ollama API. We explored the impact on performance of assigning tasks to different LLMs in our framework, and demonstrated how FREYR can enable LLMs for tool usage even if they do not natively support it, with no degradation of quality. While not explored, the framework can be further tuned to different needs by tinkering with the prompts assigned to the different roles (e.g.: LLMintent). For example, if explainability is of interest, LLMintent can be set to also produce an explanation for each detected intent. This level of information however would not be as informative as an entire thought process as available with more recent finetuned LLMs [39]. While we developed FREYR with open-source and full control of the flow of information in mind, the framework can be applied to closed-source and proprietary models (such as OpenAI's GPT-4 or Anthropic's Claude 3.5), as it relies only on prompt engineering techniques. The lack of control, replicability, and environmental impact of these massive models is however a concern [40], regardless of how well they might perform on these tasks.\nThis work has multiple limitations: mainly, we evaluated FREYR only on a single domain. While the LLMaker test cases proved challenging for existing methods already, it does not provide any guarantee on generalizability of results. Secondly, we only evaluate a small subset of all possible existing LLMs, focusing on models with medium context lengths. While long-context LLMs would probably be better suited when many tools are provided, they are also known to struggle with in-context learning tasks [41]. However, more recent models can handle longer context without degradation in performance [41, 42]. Another limitation we highlight in this work is that FREYR enforces a simple yet structured format in the responses (comma-separated list for intents, and bulleted list for the parameters). The appeal of instead enforcing more structured outputs, such as JSON or XML, in the generation of responses by the LLM is sought after by industry professionals as it would speed up prompt-based development efficiency, satisfy requirements, and improve user experience [43]. There are multiple approaches that allow for such structured generation via controlled grammar decoding [44, 45, 46], where the schema of the response is defined by the user and enforce in the response. In preliminary tests for this paper using the Outlines library [47], however, we found that such approaches could lead to unstable responses and sometimes would cause the entire generation infinitely repeat the same sequence of tokens.\nOne last limitation of the presented work is that we do not focus on the quality of the generated content. While we do give a qualitative overview of the results in Section 5.3, we do not formally evaluate it, either automatically or via human evaluations. Creativity in LLMs is still a hot topic in the computational creativity community [48], and we believe FREYR could benefit from a user study. In concordance to existing literature in the field, FREYR generates seemingly creative content by setting a non-zero temperature [49], not requesting the LLM to \"just be creative\" [50], and using prompts that allow for creative writing [51]. However, LLMs are known to only be capable of valuablee creativity and a weak version of novelty, possibly due to their autoregressive nature [38]."}, {"title": "7 Conclusions", "content": "In this work, we introduced FREYR, a novel framework designed to allow tool usage with any LLM by modularizing the tool usage process. Through comprehensive experimentation on the real-world test case specific to video game design, LLMaker, we demonstrated that FREYR consistently outperforms the traditional approach via the Ollama API. By decomposing the tool usage process into distinct steps, FREYR enables LLMs to effectively utilize tools without requiring model-specific adaptations or fine-tuning, thereby overcoming key limitations of existing methodologies. By also releasing the code for this work publicly, we hope that FREYR will inspire more research in the area of flexible, innovative, and effective tool usage for LLMs."}]}