{"title": "N-Version Assessment and Enhancement of Generative AI", "authors": ["Marcus Kessel", "Colin Atkinson"], "abstract": "Generative AI (GAI) holds great potential to improve software engineering productivity, but its untrustworthy outputs, particularly in code synthesis, pose significant challenges. The need for extensive verification and validation (V&V) of GAI-generated artifacts may undermine the potential productivity gains. This paper proposes a way of mitigating these risks by exploiting GAI's ability to generate multiple versions of code and tests to facilitate comparative analysis across versions. Rather than relying on the quality of a single test or code module, this \u201cdifferential GAI\u201d (D-GAI) approach promotes more reliable quality evaluation through version diversity. We introduce the Large-Scale Software Observatorium (LASSO), a platform that supports D-GAI by executing and analyzing large sets of code versions and tests. We discuss how LASSO enables rigorous evaluation of GAI-generated ar- tifacts and propose its application in both software development and GAI research.", "sections": [{"title": null, "content": "By automatically generating software artifacts, including code, from small, natural language prompts, generative AI (GAI) has the potential to dramatically boost the productivity of software en- gineers. However, while the creativity of GAI systems is essentially boundless, the \"quality\" of the artifacts they generate is not, significantly limiting their trust- worthiness in engineering domains where low quality is costly. Effectively judging, and if necessary enhancing, the quality of GAI-generated artifacts will therefore be critical to its success in practical software engineering (SE).\nSoftware quality can be defined by various prop- erties, but measuring some of the most critical quality attributes - specifically semantic (i.e., behavioral) prop- erties - remains undecidable in most cases (cf. Rice's theorem\u00b9). As a result, it is impossible, in general, to develop a tool that can automatically verify non- trivial semantic properties of arbitrary code modules, including their functional correctness relative to spec- ifications or previous versions. This limitation is a key reason why modern software projects often allocate over 50% of their total effort to verification and valida- tion (V&V) techniques such as testing.\u00b2 Consequently, any potential productivity gains from using GAI could be significantly diminished or even negated - by the additional V&V work required to mitigate its inherent unreliability.\nAt first glance, it might seem feasible to address the V&V challenges posed by GAI by using it to au- tomatically generate additional V&V artifacts, such as tests. However, these artifacts \u2013 like all GAI-generated outputs are also of uncertain quality. This creates a dilemma: verifying code modules of uncertain quality with verification artifacts (e.g., tests) that are equally unreliable. The only practical way to break this cycle and achieve genuine confidence in the quality of GAI- generated code, without resorting to excessive manual effort, is to generate multiple, diverse versions of both code modules and tests\u00b3 and conduct extensive com- parative analysis of their executions.\nThe concept of comparing multiple executions of similar versions of code modules has a long his- tory across various branches of SE. It appears in approaches such as differential testing for software V&V, N-version programming for enhancing software reliability, mutation testing for assessing test quality, and the use of \"cross-checking\u201d oracles in automated test generation. While these methods differ in their ob- jectives and the stages of a software artifact's lifecycle in which they are applied, they all share a common core: comparing the executions of multiple tests on multiple versions of code modules expected to deliver specific functionality. This comparative approach is also valuable in experimentally evaluating tools used to create or modify these versions, including tools for code refactoring, program repair, code recommenda- tion, and GAI for code generation.\nThe premise of this paper is that the future success of GAI in mainstream SE projects will be significantly enhanced by large-scale, \"comparative\" or \"differential\" testing of GAI-generated artifacts. However, current testing frameworks are not suited for this purpose, as they are designed to test individual code modules. Even existing differential testing tools offer limited user feedback on discrepancies. Moreover, these tools lack the decision-making capabilities and data-driven in- sights of code recommenders, which take into account users' weighted preferences for functional and non- functional properties to select the best code modules or tests based on comprehensive criteria.\u2077\nIn this paper, we explain how the \"differential GAI\" (D-GAI) approach to code and test synthesis could significantly accelerate GAI-driven SE in both devel- opment and research. To illustrate this, we introduce the Large-Scale Software Observatorium (LASSO), a new software observation platform designed to support the D-GAI approach. LASSO provides domain-specific languages, data structures, and an execution \u201carena\u201d that facilitates the execution of large numbers of tests across numerous code module versions. By analyzing the resulting observational data, functional and non- functional differences and patterns can be identified, leading to improved code quality and reliability - re- gardless of how the versions or tests were generated. LASSO can also be used to conduct large-scale, reproducible studies on the effectiveness of GAI mod- els, with the observational data serving as valuable training input to further enhance GAI systems. After presenting a D-GAI scenario based on a typical code generation task, we outline how a D-GAI engine could be realized using a platform like LASSO. Finally, we explore how D-GAI can improve the utility of GAI in practical SE projects while also providing a means to assess and enhance GAI systems themselves."}, {"title": "EXAMPLE D-GAI SCENARIO", "content": "As an illustration of a D-GAI scenario, suppose a team of software developers wants to use one of the latest GAI code generation models to help ac- celerate their project. Some of the most well-known code models currently available include GPT-3.5/-4 Turbo, Codex (GitHub CoPilot), CodeGen and InCoder (see), which are all based on the Transformer-driven LLM technology popularized by ChatGPT. These can perform a variety of SE tasks, but one of the most canonical is synthesizing code modules from natural language descriptions of the desired functionality. In the GAI community this is usually referred to as a code completion \u201cproblem\u201d, \u00b9\u2070 but essentially provides a service called \"code recommendation\" in the traditional software reuse community.\u2077\nTo get such a code generation model (i.e., code model for short) to recommend a code module, devel- opers have to provide it with a \"prompt\" that describes the desired software functionality in some way. For example, the following prompt is for a method that calculates the greatest common divisor (GCD) of two integers. After identifying the signature of the method (here in Python syntax), the prompt provides a short natural language description of the desired function- ality as well as two tests a correct implementation is expected to pass.\ndef gcd(a: int, b: int) -> int:\n\"\"\"\nReturn a greatest common divisor\nof two integers a and b\n>>> gcd (3, 7)\n1\n>>> gcd(10, 15)\n5\"\"\"\n(body to be generated)\nOn receiving such a prompt, a code model re- turns a recommended implementation of the desired functionality. State-of-the-art code models are able to provide correct recommendations of this size a sur- prisingly high proportion of times. Recent experiments (e.g., MultiPL-E,\u00b9\u2070 EvalPlus\u00ba) and leaderboards\u00b9 have achieved significant success rates on popular bench- marks, with some exceeding 80%. However, while these rates are high from a GAI research point of view, they are not particularly high for practical software de- velopment (cf.\u00b9\u00b9). Thus, in order to be able to actually use a recommended code module in a real project, developers would probably need to test it extensively.\nIn the D-GAI version of this scenario, developers would receive the same basic service, driven by the same prompt, but the \"quality\" of the recommended code module would usually be much higher. This is achieved by means of a special tool or platform (i.e., a D-GAI engine) that automatically performs the code recommendation request multiple times, possibly using multiple code models, to generate N versions of the de- sired code module, and then automatically compares them using multiple, automatically generated tests. The D-GAI engine then returns the best version based on"}, {"title": null, "content": "specific ranking criteria. Applying such an N-version approach has numerous important benefits\nResults Aggregation - the D-GAI engine is able to aggregate the results of many GAI code recommendation requests, from one or more code models. Even if one code model is used, this is advantageous because they are inherently non-deterministic in their behavior, and thus can give different results for the same prompt. When more code models are available, obviously the best results can be selected. Since it aggregates results, a D-GAI engine will always deliver a result that is at least as good as a single-use realization of the code recommendation service.\nSemantic Awareness A major benefit of the D-GAI approach is that by actually executing all multiple candidates (i.e., N versions), a re- sult can be returned that genuinely passes the required tests. The users of the recommenda- tion service can then have confidence that the returned result, if there is one, truly has the behavior demanded by the tests. This not only increases confidence in the results, but obviates the need for further testing with the prompt tests. Code models themselves cannot give a guarantee that the recommended code passes the prompt tests, because they are unaware of the true behavior of software code. \u00b9\u00b2 This idea corresponds to the notion of test-driven search in the software reuse community. \u00b9\u00b3\nObservational Measurement Another major benefit of executing the candidates is that their comparison can include dynamic code metrics, such as performance and coverage, as well as static code metrics (e.g., measurements ob- tained by applying GQM\u00b9\u2074). Code models are unable to consider the former because they are unaware of the true execution properties of the code modules they generate. They also do not apply classic static metrics in the traditional sense, which a D-GAI engine could easily do.\nOf course, the extra testing and analysis involved in D-GAI comes at the cost of significantly lower response times. However, considering the widespread adoption of test-driven development and continuous integration practices in modern SE, we envision GAI code recom- mendations being primarily used in an asynchronous, offline mode that will not disrupt developers' work- flows. As such, D-GAI will likely become a natural extension of the offline capabilities offered by powerful continuous integration platforms, further streamlining the development process. Moreover, even for direct use, with a powerful D-GAI engine, the code recom- mendation response times are reasonable, depending on the number of module versions explored. In a recent experiment, \u00b9\u00b3 average response times for 1.000 code modules harvested from code repositories were ~4 minutes on modern hardware.\nWhile employing code models may introduce costs due to additional token generation from multiple mod- els or repeated iterations on a single model to obtain N versions of the code modules and tests, the D-GAI approach offers a rapid feedback loop reminiscent of agile methodologies. This helps developers to making informed, data-driven decisions, avoiding the pitfalls of blindly integrating generated code. By minimizing tech- nical debt and potential risks, this approach can lead to significant long-term cost savings through reduced rework, debugging, and maintenance."}, {"title": "TOWARDS A D-GAI ENGINE", "content": "In this section, we delve deeper into the architecture of a D-GAI engine and outline key components necessary for its implementation. To facilitate the discussion, we employ our LASSO platform\u00b2, which was designed specifically to support the N-version comparison layer required to realize D-GAI on top of standard code models. While LASSO is not exclusively focused on D- GAI, it provides a suitable foundation for implementing the D-GAI approach. As well as a powerful testing platform and large code repository, LASSO offers several dedicated languages and data structures for N-Version comparison.\nFigure 1 illustrates how the GCD code recommen- dation scenario described earlier can be efficiently implemented using LASSO. The top left section of the diagram introduces LASSO's dedicated \"sequence sheet\" notation, used to store stimulus-response pairs for sequences of method invocations. A sequence sheet functions similarly to a method with a corre- sponding signature, but instead of using traditional code for its body, it uses a table format. Each row in this table represents an individual method invocation, while the columns represent different components: column B designates the called method, columns C to E hold the input parameters, and column A captures the out- put. Users can either represent their test components directly as sequence sheets or have them generated automatically from the traditional prompt format shown earlier.\nOn receiving a code recommendation prompt, LASSO can generate a so-called stimulus matrix (SM) of the form shown on the top right-hand side of Figure 1 and populate it with multiple implementations of the desired functionality (as columns) and multiple tests of the desired functionality (as rows). The N versions can be obtained from a variety of sources, including from LASSO's own code corpus, but in the D-GAI scenario they would be generated by multiple code generation requests to one or more code models. If only one code model is available, N versions can be obtained either (a) by invoking it many times since the code synthesis behavior is non-deterministic, or (b) by varying the various model parameter choices like the temperature parameter to fine-tune the level of generation behavior. To populate the SM with tests, any tests provided in the prompt are converted into sequence sheets and invocations, as shown by the first two rows in the example. More tests can also be added automatically through code models using generated prompts, or by applying standard unit test generation tools like EvoSUITE\u00b9\u2075 or RANDOOP.\nOnce the SM has been populated it can be input to LASSO's dedicated test engine, the so-called \"arena\", where all the tests are executed on all N versions of the desired functionality. To cope with the high volume of executions that can be involved, the arena is implemented as a parallel, distributed architecture with load balancing and sandboxed (controllable and secure) code execution environments based on con- tainerization. Once all the testing has been performed, the results are output as a stimulus response matrix (SRM), which has the same shape as the input SM, but contains all the runtime data observed during the exe- cution process (i.e., outputs, execution time, methods called, branches covered etc.). As shown in the bottom right-hand side of Figure 1, the core advantage of using sequence sheets to define tests is that it allows SRMs to store the responses of the different module versions within the sequence sheets that provoked them. For example, the sequence sheet at the bottom left of the diagram shows that the invocation of GCD module version V1 with the values 3 and 7 (the first test in the prompt) returned the value 1. SRMs therefore provide a persistent, analyzable record of the behavior of each version in response to each test.\nOne other important LASSO feature represented in Figure 1 is the pipeline script used to drive the work flow steps involved in a D-GAI code recommendation request. This allows engineers to write new D-GAI services in a dedicated DSL.\nFinally, the LASSO platform has a large underly- ing repository of executable software code, harvested from large open source code repositories, that can be accessed using classic code search technologies. \u00b9\u00b3 Although the use of this repository is not essential"}, {"title": "D-GAI USE CASES & SERVICES", "content": "The LASSO data structures described above are not essential for building a D-GAI engine. However, using them offers several advantages. Firstly, treating tests as \"first-class\" citizens in the differential comparison process allows them to be assessed and enhanced using D-GAI alongside the N module versions. Sec- ondly, storing and analyzing execution observations offline with analysis tools like Python/Pandas, enables researchers and practitioners to assess, benchmark, and enhance the underlying code models across mul- tiple coding problems.\nFigure 2(a) presents a comprehensive overview of the range of use cases and services that could be supported by a D-GAI engine of the kind outlined previously. It categorizes the services into three main tiers. The root-level distinction is between services aimed at assisting developers in building concrete software applications and those intended to aid re- searchers in evaluating or enhancing code models. Ap- plication Engineering services are further divided into Development-time and Runtime services. The latter focus on improving the quality of deployed applications during execution time as in classic N-version program- ming.\u2075 Research services comprise GAI code model evaluation services for benchmarking and comparing different code models, as well as code model enhance- ment services for training and refining GAI models themselves. Within the Development-time application engineering category, subservices help developers to evaluate, create and enhance code and tests. In the remainder of this section, we will delve deeper into these leaf cases."}, {"title": "Code Recommendation", "content": "Code recommendation is the use case exemplified in the second section, and is one of the core use cases of D-GAI for software development. The basic idea resembles the test-driven search technology provided by some code search engines, \u00b9\u00b3 and offers the same important benefit of improving the user's confidence in the correctness of the recommended module (or modules), since it has been verified to pass the tests supplied in the prompt. Code models themselves do not give a guarantee that the recommended code passes the prompt tests, because they are unaware of the true behavior of software code. \u00b9\u00b2 The basic difference is that whereas code search engines retrieve human-written code modules from code repositories, a D-GAI code recommendation service selects the most suitable code module from a set of GAl-synthesized modules. Executing the candidates also allows the final ranking of the candidates to include both dynamic metrics and static metrics. Code models can enhance code recommendations services not only by synthesiz- ing large numbers of alternative module versions, but also by synthesizing additional tests."}, {"title": "Differential Testing", "content": "Differential testing is a widely practiced testing tech- nique in modern SE projects which aims to improve the quality of a particular code module under development (i.e., \"base version\u201d) by comparing its behavior to that of one or more alternative versions to identify discrep- ancies that may reveal faults or functional differences.\u2074 It is most commonly applied in the form of regression testing where the alternative versions are older ver- sions of the code module. This is because obtaining a diverse set of alternative versions of the base version (i.e., that are not simply clones) is a challenging task."}, {"title": "Test Quality Evaluation & Enhancement", "content": "Test quality evaluation is perhaps the simplest of the test-oriented D-GAI services in which the prompt in- cludes a set of tests and a base code module of the desired functionality, and the output is one or more test quality metrics. Code models are used only to generate new versions of the base version, which play the same role as the mutations in mutation testing.\u00b2 After executing all the tests on all the versions, the D-GAI engine can calculate a version kill score, analogous to a mutation score to capture the quality of the supplied test sets.\nTest enhancement services take this one step fur- ther by not only determining the version kill score but also leveraging this information to produce a minimal subset of input tests (i.e., test set minimization) that achieves the same (or a comparable) level of test effectiveness."}, {"title": "Test Recommendation", "content": "This service resembles the code recommendation ser- vice, but recommends new tests, or more precisely test sets. Here the prompt would include the base module the developer wishes to have tests for, rather than the tests the developer wishes to have a code module for. As in the previous cases, the code model would be asked to synthesize new tests as well as new versions of the supplied code whose execution outputs would then be compared for differences. The new versions would be used in the same way as in the previous two services to identify effective sets and return the minimum number needed to kill the largest possible number of versions."}, {"title": "Test Oracle Recommendation", "content": "Obtaining oracles to define correct behavior has tradi- tionally been a big obstacle to test automation, and is still typically performed by hand. D-GAI can help by facilitating the creation of multiple \"verdicts\" on correct output values from N versions of the code modules. There are two basic strategies for doing this, illustrated at the bottom of Figure 2(b) for the GCD example. The first approach, test-based voting, identifies the oracle values by selecting the most common result for each test case across all N versions, while the second approach, cluster-based voting, identifies the oracle values based on behavioral clustering over common outputs over all the tests. The former is essentially the approach taken in the original N-version programming approach.\u2075 However, in the example in Figure 2(b), test-based clustering is inconclusive, since none of the N versions match the oracle for all outputs, thus no variant is functionally correct and deemed to be of the highest quality according to these criteria. In contrast, the clustering-based approach leads to two versions being judged to as functionally correct, and thus of the highest quality - namely V4 and V5 which are in the large (green) cluster. In the figure, outputs that deviate from the cluster-based voting oracle are shown in red font."}, {"title": "Research", "content": "To evaluate GAI-synthesized code artifact quality, re- searchers conduct experiments comparing generated artifacts in various coding problems. D-GAI facilitates this by aggregating SRMs for multiple problems and offering oracle-based quality assessments (cf.\u2078). As the number of code modules and tests grows, results generalizability and statistical power improve, allowing more precise reliability estimates. Additionally, com- bining D-GAI services mirrors existing benchmark en- hancement approaches like EvaLPLUS.\u00ba Furthermore, SRMs offer valuable observational data that can be used to train and improve code models, leading to better performance of GAl-synthesized code modules and tests."}, {"title": "CONCLUSION", "content": "This paper has introduced Differential GAI (D-GAI), a practical approach to improving the quality of genera- tive Al (GAI)-produced software artifacts through the comparative analysis of multiple versions. By incor- porating \"true\" semantic information derived from the execution of multiple versions, D-GAI enables more reliable quality assessments without depending on the trustworthiness of any single artifact.\nOur key contribution is the introduction of the Large- Scale Software Observatorium (LASSO), a platform designed to support large-scale execution and analysis of code versions and tests. LASSO offers develop- ers and researchers a powerful tool for assessing GAI-generated artifacts. By comparing different execu- tions, LASSO facilitates the identification of behavioral patterns, functional discrepancies, and non-functional variations across versions, thereby enhancing the reli- ability and trustworthiness of GAI outputs."}]}