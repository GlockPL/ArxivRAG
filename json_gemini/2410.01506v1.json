{"title": "LEGO: LEARNABLE EXPANSION OF GRAPH OPERATORS FOR MULTI-MODAL FEATURE FUSION", "authors": ["Dexuan Ding", "Lei Wang", "Liyun Zhu", "Tom Gedeon", "Piotr Koniusz"], "abstract": "In computer vision tasks, features often come from diverse representations, domains (e.g., indoor and outdoor), and modalities (e.g., text, images, and videos). Effectively fusing these features is essential for robust performance, especially with the availability of powerful pre-trained models like vision-language models. However, common fusion methods, such as concatenation, element-wise operations, and non-linear techniques, often fail to capture structural relationships, deep feature interactions, and suffer from inefficiency or misalignment of features across domains or modalities. In this paper, we shift from high-dimensional feature space to a lower-dimensional, interpretable graph space by constructing relationship graphs that encode feature relationships at different levels, e.g., clip, frame, patch, token, etc. To capture deeper interactions, we use graph power expansions and introduce a learnable graph fusion operator to combine these graph powers for more effective fusion. Our approach is relationship-centric, operates in a homogeneous space, and is mathematically principled, resembling element-wise relationship score aggregation via multilinear polynomials. We demonstrate the effectiveness of our graph-based fusion method on video anomaly detection, showing strong performance across multi-representational, multi-modal, and multi-domain feature fusion tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Imagine preparing a fruit salad (see Figure 1). Initially, we slice fruits like apples, bananas, and oranges into distinct pieces, each retaining its unique flavor. This is analogous to features in multi-modal data, sourced from different modalities such as text, images, or videos. Combining these fruit slices resembles traditional early fusion methods in computer vision, where features are concatenated but remain largely independent of each other (Snoek et al., 2005; Gadzicki et al., 2020; Barnum et al., 2020). Next, we might cut the fruit into smaller pieces and mix them further, but the distinct flavors persist. This reflects late fusion methods, which combine outputs from separately trained models on different modalities (Snoek et al., 2005; Bodla et al., 2017; Wang et al., 2019a). While some integration occurs, the deeper interactions between the features are still missing, just as the flavors in the salad remain separate. Finally, we use a fruit mixer. This tool thoroughly blends\nTraditional fusion techniques like concatenation, element-wise operations, or attention mechanisms (Dai et al., 2021) often capture shallow or superficial interactions. These approaches typically overlook deeper, structural relationships between feature elements (Atrey et al., 2010; Feng et al., 2019), limiting their ability to align features across different modalities or domains. Furthermore, they often suffer from inefficiencies in computation and alignment. Our motivation for this work arises from the limitations of current methods, which struggle to blend and enhance feature relationships meaningfully. We propose a paradigm shift from high-dimensional feature spaces to lower-dimensional, interpretable graph spaces. Instead of relying on raw features, our approach emphasizes the fusion of relationships between features, similar to how a fruit mixer blends distinct flavors into a cohesive whole. Specifically, we introduce relationship graphs, such as similarity graphs, as intermediary representations that encode the relationships between entities like frame-, patch-, or token-level features from videos. These graphs provide a more compact and interpretable representation of the data (Mai et al., 2020). In a similarity graph, nodes correspond to entities (e.g., video clips, frames, or patches), while edges represent their relationships, such as cosine similarity. To further capture complex interactions, we leverage graph power expansions to model multi-hop connections, revealing structural insights often overlooked by traditional fusion methods.\nWe also introduce a learnable weight matrix, the graph fusion operator, which combines different powers of relationship graphs. Unlike simple concatenation or addition that treats all feature components equally, our learnable mechanism dynamically weights the contributions of different graph powers, resulting in better fusion performance across various modalities, domains, and representations. Our graph-based fusion operates in a lower-dimensional, homogeneous space, offering several advantages. First, by representing relationships instead of individual features, it reduces dimensionality and computational costs. Second, the homogeneous space allows consistent fusion across domains and modalities, aligning features into a common structure. Third, it provides better interpretability by focusing on relationships rather than abstract features. The use of graph powers reveals multi-hop, deeper feature interactions. Lastly, the learnable fusion mechanism adapts to specific tasks via learning objective functions, improving both performance and efficiency.\nFurthermore, our approach can be connected to multilinear polynomials with learnable coefficients, where element-wise relationship scores are aggregated across graph power sequences. This mathematically grounded framework generalizes simple linear operations, capturing more complex interactions between graph powers. Our main contributions are as follows:\ni. We propose a novel graph-based feature fusion framework, termed LEGO fusion, which effectively captures multi-representational, multi-modal, and multi-domain relationships through relationship graphs, thereby enriching feature representations.\nii. We introduce a learnable graph fusion operator that dynamically integrates different graph powers, facilitating deeper interactions among features and balancing self-relationships with inter-feature relationships.\niii. We establish a theoretical connection between our graph fusion approach and multilinear polynomials, providing insights into feature interactions. We empirically validate our method in video anomaly detection, demonstrating improvements in both performance and interpretability over traditional feature-level fusion techniques."}, {"title": "2 RELATED WORK", "content": "Traditional feature fusion. Traditional fusion methods in multi-modal learning (D'mello & Kory, 2015) typically rely on simple operations such as concatenation, element-wise addition, or multiplication (Chen et al., 2023a). Early Fusion techniques (Snoek et al., 2005), for example, combine features from different modalities, such as text, images, and videos, into a single high-dimensional feature vector. While straightforward, this approach often leads to overfitting and increased computational complexity due to the high dimensionality of the concatenated features. Moreover, early\nfusion tends to amplify noise from heterogeneous data sources, negatively impacting performance in complex tasks (Liu et al., 2016). Late Fusion, on the other hand, merges the outputs of independently trained models from different modalities (Snoek et al., 2005). This approach alleviates some dimensionality issues, allowing each model to focus on learning modality-specific features before integration. However, it fails to capture deep interactions between modalities, limiting its effectiveness. More advanced techniques, such as attention mechanisms (Vaswani, 2017; Dai et al., 2021), have been introduced to dynamically weigh features based on importance. Nonetheless, these methods often overlook structural relationships between features, restricting their ability to model complex interactions. Neural network-based non-linear fusion introduces learnable layers to the process (Wang et al., 2019a; Wang & Koniusz, 2021), but these models often lack transparency, making it difficult to interpret how individual features contribute to predictions. Aligning features from disparate modalities remains a challenge, further complicating fusion of non-comparable data.\nGraph-based fusion. To overcome the limitations of traditional methods, recent work has turned to graph-based approaches (Liao et al., 2013; Feng et al., 2019; Iyer et al., 2020; Chen & Zhang, 2020; Mai et al., 2020; Zhang et al., 2024), which emphasize the relationships between features rather than the features themselves. In these methods, data is represented as a graph, with nodes corresponding to entities such as frames, patches, or tokens, and edges capturing similarities or interactions (Iyer et al., 2020). This structured representation allows for more interpretable and context-aware feature fusion. Graph Convolutional Networks (GCNs) have been widely used for modeling relationships across modalities (Zhang et al., 2020), particularly in video understanding tasks (Huang et al., 2020; Gkalelis et al., 2021), where capturing temporal and spatial relationships is critical. GCNs aggregate information from neighboring nodes, enabling the modeling of context-dependent interactions. Despite their effectiveness, most graph-based methods capture only first-order relationships, limiting their ability to model complex, multi-step dependencies between features (Chen & Zhang, 2020).\nOur work addresses this limitation by using graph power expansions, which capture higher-order relationships through multi-hop connections. This enables our method to model richer and more nuanced structural interactions, revealing insights that conventional fusion techniques often miss.\nInterpretable and efficient fusion. As machine learning models grow in complexity, there is increasing demand for fusion methods that balance interpretability and computational efficiency. Traditional approaches like concatenation and neural-based fusion often operate in high-dimensional spaces, which can lead to inefficiencies and hinder transparency. Recent advancements have focused on designing more interpretable fusion strategies (Ma et al., 2016). For example, Capsule Networks (Sabour et al., 2017) and attention mechanisms (Vaswani, 2017) aim to provide greater insight into the fusion process by highlighting important features. However, these methods still suffer from the computational burdens associated with high-dimensional data, particularly in large-scale, multi-modal tasks.\nOur approach offers a different solution by shifting from feature-level fusion to relationship-centric fusion, operating in a lower-dimensional graph space. This transition improves both interpretability"}, {"title": "3 APPROACH", "content": "This section presents our proposed method, Learning Expansion of Graph Operators (LEGO fusion), beginning with key notations and then detailing the construction of our relationship graph, graph power expansion, and fusion strategy.\nNotations. Let $\\mathbb{I}_{T}={1,2, ..., T}$ represent the index set. Scalars are denoted by regular fonts, e.g., $x$; vectors by lowercase boldface, e.g., $\\mathbf{x}$; matrices by uppercase boldface, e.g., $\\mathbf{X}$; and tensors by calligraphic letters, e.g., $\\mathcal{X}$."}, {"title": "3.1 LEGO: LEARNING EXPANSION OF GRAPH OPERATORS", "content": "Relationship graph of unit-level features. Text, images, and videos can be used to extract various unit-level features (see definition in Appendix A), ranging from word- and paragraph-level to patch-, clip-, frame-, cube-, or token-level, using pre-trained models. These heterogeneous features are then transformed into a homogeneous graph space by modeling pairwise relationships among unit-level features, such as similarities, distances, or other relevant metrics. Since distances and similarities are inversely related, meaning high similarity corresponds to low feature distance (see proof in Appendix B), similarity scores are particularly effective for encoding local relationships among units, helping to identify which feature points are close or similar. In the resulting relationship graph, e.g., similarity graph, each unit feature point is represented as a node, and the graph structure captures the local neighborhood of these unit-level features.\nTo show this process, we consider extracting unit-level feature representations from multiple pre-trained models or multi-modal sources. We denote these feature representations as $\\mathbf{F}^{(1)} \\in \\mathbb{R}^{N \\times d_{1}}, \\mathbf{F}^{(2)} \\in \\mathbb{R}^{N \\times d_{2}}, ..., \\mathbf{F}^{(T)} \\in \\mathbb{R}^{N \\times d_{T}}$. Here, $\\mathbf{F}^{(m)} \\in \\mathbb{R}^{N \\times d_{m}}$ ($m \\in \\mathbb{I}_{T}$) denotes the feature set from the $m$-th model or modality, where $N$ corresponds to the number of unit-level features, and each feature exists in $d_{m}$ dimensions. We begin by computing pairwise relationships between the unit-level features within each model or modality as follows:\n$S_{i,j} = r(f_{i}, f_{j})$\nwhere $r(\\cdot, \\cdot)$ is a relationship function, e.g., cosine similarity, Gaussian kernel, or another distance metric. The term $s_{i,j}$ represents the relationship score between unit-level features $f_{i}$ and $f_{j}$ from the feature set $\\mathbf{F}$ (the model or modality index is omitted for simplicity). Using these pairwise relationships, we build a relationship graph represented by the matrix:\n$\\mathbf{R} = [S_{i,j}]_{(i,j) \\in \\mathbb{I}_{N} \\times \\mathbb{I}_{N}}$\nwhere $\\mathbf{R} \\in \\mathbb{R}^{N \\times N}$ captures the pairwise relationships between unit-level features. A value of 1 in the matrix indicates a strong relationship, e.g., two unit-level features are identical in visual or textural concepts, while a value of 0 means no relationship. This matrix serves as the adjacency matrix for the graph.\nHowever, while similarity- or distance-based graphs are useful, they often fail to capture the global structure of the data, as they predominantly rely on local information. To address this limitation, we propose a graph power expansion approach, which enhances the graph's representation by reflecting multi-hop relationships between nodes.\nGraph powers and graph power expansion. Graph powers refer to the repeated multiplication of a graph's adjacency matrix, which shows multi-step connections between nodes. For a graph represented by the adjacency matrix $\\mathbf{R}$, the $k$-th power of the graph, denoted as $\\mathbf{R}^{k}$, uncovers relationships between nodes that are $k$ steps apart. Specifically, each element $\\mathbf{R}_{i,j}^{k}$ indicates the cumulative influence of all paths of length $k$ between nodes $i$ and $j$. This capability is particularly"}, {"title": "Graph fusion operator", "content": "We introduce a novel graph fusion operator, denoted as $\\otimes$, designed to integrate information from different modalities by merging their relationship graphs across multiple power levels. This operator enhances the model's capacity to learn complex representations by incorporating both direct and higher-order relationships within the data. Mathematically, the graph fusion is expressed as follows:\n$\\mathbf{G} = \\mathcal{G}^{(a)} \\otimes \\mathbf{A} \\otimes \\mathcal{G}^{(b)}$\n$= \\sum_{q=0}^{Q} \\sum_{p=0}^{P} \\mathbf{R}^{(a)^{p}} \\mathbf{A}_{p,q} \\mathbf{R}^{(b)^{q}}$\n$=\\sum_{q=0}^{Q} \\sum_{p=0}^{P} a_p b_q \\left(\\mathbf{R}^{(a)^{p}} \\mathbf{R}^{(b)^{q}}\\right),$\nwhere $\\mathbf{a} = [a_p]_{p \\in \\mathbb{I}_{(P+1)}}$ and $\\mathbf{b} = [b_q]_{q \\in \\mathbb{I}_{(Q+1)}}$ are the modality graph power selectors, and $\\mathbf{A} = \\mathbf{a} \\otimes \\mathbf{b} \\in \\mathbb{R}^{(P+1) \\times (Q+1)}$, with $\\otimes$ representing the outer product. We also propose an advanced variant:\n$\\mathbf{G} = \\sum_{q=0}^{Q} \\sum_{p=0}^{P} \\mathbf{A}_{p, q} \\left(\\mathbf{R}^{(a)^{p}} \\mathbf{R}^{(b)^{q}}\\right),$\nwhere $\\mathbf{A} \\in \\mathbb{R}^{(P+1) \\times (Q+1)}$ is a learnable weight matrix that modulates the fusion process. The operator $\\odot$ denotes element-wise multiplication. In Appendix C, we provide a detailed explanation of our graph fusion process, including the derivation of equation 5, and the relationship between equation 4 and equation 5. The fused relationship graph $\\tilde{\\mathbf{G}} \\in \\mathbb{R}^{N \\times N}$ is the result of this integration, enabling the model to optimize the combination of graph powers through backpropagation, thus improving the fusion of features across different levels.\nWe observe that $\\mathbf{R}^{(a)^{p}} \\mathbf{R}^{(b)^{q}}$ in equation 4 and equation 5 captures all possible combinations of selected graph powers, while $\\mathbf{A}$ provides the appropriate weights for the fusion process. Incorporating the 0-th power, e.g., $\\mathbf{R}^{0}$, in the fusion process is critical; it preserves self-connections and maintains original feature information. This mechanism allows for adaptive weighting that balances the influence of direct relationships against multi-hop interactions, ensuring the model can emphasize the most relevant connections tailored to the specific task."}, {"title": "3.2 CONNECTING TO MULTILINEAR POLYNOMIALS", "content": "Our graph fusion process is intrinsically connected to multilinear polynomials, which provide a robust mathematical framework for aggregating multiple powers of relationship scores between two relationship graphs. Specifically, each entry in the fused graph $\\mathbf{G}$ is a function of the element-wise relationship scores between the two input graphs. These scores are then combined in a manner that parallels how terms in a multilinear polynomial are constructed. This allows us to capture both linear and nonlinear interactions between modalities.\nWe can express each entry in the fused relationship graph, $G_{i,j}$, as a multilinear combination of the relationship scores from the two graphs, $\\mathcal{G}^{(a)}$ and $\\mathcal{G}^{(b)}$, at different powers. Rewriting equation 4 or equation 5 based on equation 1, we get:\n$G_{i,j} = \\mathbf{A}_{0,0} + \\mathbf{A}_{0,1}s^{(b)}_{i, j} + \\mathbf{A}_{0,2}s^{(b)^2}_{i, j} + ... + \\mathbf{A}_{P,Q}s^{(a)^P}_{i, j}s^{(b)^Q}_{i, j}$ \n$= \\sum_{p=0}^{P} \\sum_{q=0}^{Q} \\mathbf{A}_{p,q}s^{(a)^p}_{i, j}s^{(b)^q}_{i, j}$ \nThe term $\\mathbf{A}_{p,q}$ serves as a learnable coefficient that controls the relative importance of the interaction between the $p$-th power of the relationship score from modality $a$ and the $q$-th power from modality $b$. This general form is closely related to multilinear polynomials, where the powers of the relationship scores (i.e., $s^{(a)^p}_{i, j}$ and $s^{(b)^q}_{i, j}$) represent the different degrees of interaction between the modalities."}, {"title": "Interpretation of multilinear polynomial terms", "content": "The structure of this multilinear polynomial offers valuable insights into the fusion process. Each individual term, such as $s^{(a)^p}_{i, j}s^{(b)^q}_{i, j}$, represents interactions between higher-order relationships in the graphs $\\mathcal{G}^{(a)}$ and $\\mathcal{G}^{(b)}$. These higher-order terms capture increasingly complex dependencies between the two graphs, allowing the model to learn not only from direct pairwise relationships (as seen in the linear terms) but also from more subtle, nonlinear relationships that emerge from specific combinations of scores.\nLinear terms like $\\mathbf{A}_{0,1}s^{(b)}_{i, j}$ or $\\mathbf{A}_{1,0}s^{(a)}_{i, j}$ represent simple linear combinations of relationship scores from the two graphs. These terms capture first-order interactions, essentially weighting how much each modality's direct relationship contributes to the fused graph. Quadratic and higher-order terms like $\\mathbf{A}_{1,1}s^{(a)}_{i, j}s^{(b)}_{i, j}$ or $\\mathbf{A}_{2,2}s^{(a)^2}_{i, j}s^{(b)^2}_{i, j}$ capture cross-modality interactions that go beyond simple weighting. These terms allow the model to learn relationships in which one modality's relationship score influences the contribution of another modality, enabling more sophisticated fusion strategies. For instance, if two modalities appear only weakly related initially, higher-order terms can help reveal deeper latent connections. This formulation shows that our graph fusion is, in fact, a more general and flexible version of graph-based fusion methods, capable of modeling complex interactions between different modalities."}, {"title": "3.3 MULTI-MODAL VIDEO ANOMALY DETECTION", "content": "We present our approach to video anomaly detection for several key reasons:\ni. Multi-modality fusion. Robust video anomaly detection inherently requires the integration of multiple data modalities, such as video, audio, and text (Wu et al., 2020; Chen et al., 2023a; Wu et al., 2024). These modalities can be easily obtained, e.g., by using pre-trained video captioning models to generate accompanying text data. This enables us to explore and evaluate the efficacy of multi-modality feature fusion, where combining complementary information across modalities can enhance anomaly detection performance.\nii. Multi-representational fusion. Current state-of-the-art video anomaly detection methods typically rely on pre-trained action recognition or motion-based models for feature extraction (Zhu et al., 2024). These models, such as I3D (Carreira & Zisserman, 2017) and C3D (Tran et al., 2015), offer distinct perspectives on the same data modality, e.g., videos, extracting different features that represent various aspects of motion, appearance, or temporal dynamics. Our proposed feature fusion approach is well-suited to this setting, allowing us to combine features from different representations within the same modality, thus enriching the representational capacity of the model and potentially boosting detection accuracy.\niii. Multi-domain fusion. Existing video anomaly detection datasets often represent a single domain or scenario, such as videos captured from specific locations like streets or university campuses (Zhu et al., 2024). This limitation offers an opportunity for us to explore multi-domain feature fusion, where features from different environments or scenarios can be integrated to create a more generalized and robust anomaly detection framework. This cross-domain learning can enhance the model's adaptability and performance across diverse settings.\niv. Binary classification as a foundational task. Video anomaly detection is commonly framed as a binary classification problem, where the objective is to distinguish between normal and anomalous events. This straightforward approach facilitates intuitive visualization and analysis of the model's performance when implementing our fusion technique, enabling us to gain deeper insights into the impact of feature fusion on detection accuracy. By starting with a binary task, we establish a robust foundational benchmark that allows for iterative refinement and testing of our fusion strategies. Furthermore, once our framework is validated in this context, extending it to more complex multi-class classification tasks becomes a natural next step.\nDegree variance regularization. Regularization plays a crucial role in enhancing our fused graph representation, which captures the intricate relationships between nodes. In this context, each entry in the fused graph represents the weight of the edge connecting two nodes, providing a quantitative measure of their interconnections. To assess the connectivity of each node, we calculate the sum of the relationship scores for each row in the fused graph. This results in a single value for each node, representing the total weight of all edges linked to it; this value is commonly referred to as the weighted degree. A weighted degree of zero indicates that a node is isolated with no connections,"}, {"title": "4 EXPERIMENTS", "content": "Datasets. We select the following datasets for our evaluation: (i) UCSD Ped2 (Ped2) features 16 training and 21 testing videos of pedestrians, with anomalies like cyclists, skateboarders, and cars on paths. (ii) ShanghaiTech (ShT) has 330 training and 107 testing videos across 13 campus scenes, with 130 abnormal events such as cyclists and fights. (iii) CUHK Avenue (Avenue) includes 16 training and 21 testing videos, with 47 anomalies like running, walking in the wrong direction, and object throwing. (iv) Street Scene (Street) comprises 46 training and 35 testing videos of a two-lane street, capturing 205 anomalies like jaywalking, U-turns, and car ticketing.\nFeatures. We use I3D (Carreira & Zisserman, 2017) (with a ResNet-50 backbone), pretrained on Kinetics-400 (Kay et al., 2017), as our feature extractor, obtaining 2048-dimensional visual features per 16-frame segment. We also extract 4096-dimensional visual features using C3D (Tran et al., 2015). For text feature extraction, we apply SwinBERT (Lin et al., 2022), pretrained on VATEX (Wang et al., 2019b), to generate dense video captions for every 64-frame segment. We then use supervised SimCSE (Gao et al., 2021) to obtain 768-dimensional text embeddings.\nMetrics. Following common practice (Chen et al., 2023a), we consider Area Under the ROC curve (AUC) which is widely used for evaluation in video anomaly detection. Similar to existing methods like (Tian et al., 2021; Chen et al., 2023b;a), which evaluate frame-level performance by repeating snippet-level predictions (e.g., 16 times) to fit frame-level labels, we adopt a snippet-by-snippet evaluation method as we do not have access to frame-level features. To obtain snippet-level labels, we derive them from the frame-level labels: if any anomaly occurs within a 16-frame snippet, we label the snippet as abnormal; otherwise, it is labeled as normal. For simplicity, we set N to 32."}, {"title": "4.2 EVALUATION", "content": "Discussion on hyperparameter evaluations. Fig. 4 shows our results. First, the cut-off threshold a for filtering weak relationships varies by dataset: higher values (e.g., 0.8 and 0.9) work better for Ped2 and Street, while a lower a is needed for Avenue, likely due to more background motion in the latter. Second, the optimal k for selecting maximum degree values of normal nodes in the abnormal relationship graph is similar across ShT (11), Ped2 (17), and Avenue (15). This similarity may arise from the datasets being captured on campus and featuring comparable anomalies, like cyclists. Interestingly, the optimal k for Street is just 1, which may reflect its complexity due to diverse anomalies in a two-lane street setting. Additionally, the optimal regularization penalty parameter $\\lambda$ for Street is low (1e-4), suggesting a minor effect of regularization on its feature fusion. In contrast, a larger $\\lambda$ (e.g., 1) yields the best performance on Ped2.\nA closer look at the learnable graph operator. We set both $P$ (for I3D visual features) and $Q$ (for SimCSE text embeddings) in equation 5 to range from 1 to 10 and conduct a grid search to evaluate their impact on our LEGO fusion. We evaluate the framework on all four individual anomaly detection datasets, as well as on a combination of ShT and Ped2, as shown in Fig. 5. The results indicate that $P$ and $Q$ significantly influence LEGO fusion performance, suggesting"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce a novel graph-based fusion framework, termed LEGO fusion, which enhances feature integration. By using relationship graphs on raw features, our approach captures deeper interactions through graph power expansions and a learnable fusion operator. Our framework not only reduces dimensionality and computational costs but also improves interpretability by emphasizing feature relationships. Experiments in video anomaly detection show that our method outperforms traditional fusion techniques, underscoring the potential of relationship-driven fusion approaches in using multi-representational, multi-modal, and multi-domain features. We believe our contributions will inspire further research into understanding complex feature interactions, ultimately leading to more robust model performance across diverse applications."}, {"title": "A UNIT-LEVEL REPRESENTATION", "content": "Unit-level refers to the fundamental, granular components of data that can be segmented and analyzed independently. Depending on the data modality, unit-level entities include words or paragraphs in text, patches in images, frames, clips, or cubes in videos, and tokens in sequential data. These units represent the smallest meaningful segments of the data, serving as essential building blocks for deeper analysis and processing.\nUnit-level features are the extracted representations of these granular data components. Derived from individual units, such as words or paragraphs in text, patches in images, frames, cubes, or clips in videos, or tokens in sequences, unit-level features capture the distinctive characteristics of each unit. These features are widely used in recent advances like large language models (LLMs) and vision-language models (VLMs), where the ability to capture fine-grained details at the unit level has significantly improved tasks such as text generation, image classification, object recognition, and multimodal understanding. By using unit-level features, modern models achieve superior performance in higher-level tasks, including classification, recognition, prediction, and even cross-modal tasks, where detailed analysis is crucial for accurate and contextually rich outcomes."}, {"title": "B RELATIONSHIP BETWEEN FEATURE DISTANCE AND SIMILARITY", "content": "A widely used feature distance measure is the Euclidean distance. When we use Euclidean distance to measure the similarity between two network-encoded, L2-normalized features from two images, we obtain the following expression:\n$\\|\\phi(\\mathbf{X}) - \\phi(\\mathbf{Y})\\|^2 = (\\phi(\\mathbf{X}), \\phi(\\mathbf{X})) - 2(\\phi(\\mathbf{X}), \\phi(\\mathbf{Y})) + (\\phi(\\mathbf{Y}), \\phi(\\mathbf{Y}))$\n$= 2 - 2(\\phi(\\mathbf{X}), \\phi(\\mathbf{Y}))$\n$= 2 - 2k(\\phi(\\mathbf{X}), \\phi(\\mathbf{Y}))$\nIn this equation, $\\phi(\\mathbf{X})$ and $\\phi(\\mathbf{Y})$ represent the feature maps of images $\\mathbf{X}$ and $\\mathbf{Y}$, respectively, while $k(\\cdot,\\cdot)$ denotes various types of similarity measures. These measures can include dynamic"}, {"title": "C DERIVATION OF GRAPH FUSION OPERATOR IN LEGO FUSION", "content": "We begin by applying learnable weights", "follows": "n$\\mathbf{G} = \\mathcal{G}^{(a)} \\otimes \\mathbf{A} \\otimes \\mathcal{G}^{(b)} = \\sum_{q=0}^{Q} \\sum_{p=0}^{P} \\mathbf{R}^{(a)^{p}} a_{p} \\otimes \\mathbf{R}^{(b)^{q}} b_{q} = \\sum_{q=0}^{Q} \\sum_{p=0}^{P} a_{p}b_{q} \\left(\\mathbf{R}^{(a)^{p}} \\otimes \\mathbf{R}^{(b)^{q}}\\right)$\n$= \\sum_{q=0}^{Q} \\sum_{p=0}^{P} (a \\otimes b) \\odot (\\mathcal{G}^{(a)} \\otimes \\mathcal{G}^{(b)})$,\n$\\qquad\\begin{aligned}\n&= \\sum_{q=0}^{Q} \\sum_{p=0}^{P}  \\begin{bmatrix}\n  a_{0} \\\\\n  a_{1} \\\\\n  \\\\\\n  a_{P}\n\\end{bmatrix}\n\\begin{bmatrix}\n  b_{0}, b_{1},..., b_{Q}\n\\end{bmatrix} \\otimes \\begin{bmatrix}\n \\mathbf{R}^{(a)^{0}}\\mathbf{R}^{(b)^{0}}, \\mathbf{R}^{(a)^{0}}\\mathbf{R}^{(b)^{1}}, ..., \\mathbf{R}^{(a)^{0}}\\mathbf{R}^{(b)^{Q}}\\\\\n \\mathbf{R}^{(a)^{1}}\\mathbf{R}^{(b)^{0}}, \\mathbf{R}^{(a)^{1}}\\mathbf{R}^{(b)^{1}}, ..., \\mathbf{R}^{(a)^{1}}\\mathbf{R}^{(b)^{Q}}\\\\\n \\vdots \\\\\\\\n \\mathbf{R}^{(a)^{P}}\\mathbf{R}^{(b)^{0}}, \\mathbf{R}^{(a)^{P}}\\mathbf{R}^{(b)^{1}}, ..., \\mathbf{R}^{(a)^{P}}\\mathbf{R}^{(b)^{Q}}\n\\end{bmatrix}\n\\end{aligned}\n\\\\\n&= \\sum_{q=0}^{Q} \\sum_{p=0}^{P} \\begin{bmatrix}\n    a_{0}b_{0} & a_{0}b_{1} & a_{0}b_{2} & ... & a_{0}b_{Q} \\\\\n    a_{1}b_{0} & a_{1}b_{1} & a_{1}b_{2} & ... & a_{1}b_{Q} \\\\\n    \\vdots & \\vdots & & & \\vdots \\\\\n    a_{P}b_{0} & a_{P}b_{1} & a_{P}b_{2} & ... & a_{P}b_{Q}\n\\end{bmatrix} \\otimes \\begin{bmatrix}\n    \\mathbf{R}^{(a)^{0}}\\mathbf{R}^{(b)^{0}} & \\mathbf{R}^{(a)^{0}}\\mathbf{R}^{(b)^{1}} & \\mathbf{R}^{(a)^{0}}\\mathbf{R}^{(b)^{2}} & ... & \\mathbf{R}^{(a)^{0}}\\mathbf{R}^{(b)^{Q}} \\\\\n    \\mathbf{R}^{(a)^{1}}\\mathbf{R}^{(b)^{0}} & \\mathbf{R}^{(a)^{1}}\\mathbf{R}^{(b)^{1}} & \\mathbf{R}^{(a)^{1}}\\mathbf{R}^{(b)^{2}} & ... & \\mathbf{R}^{(a)^{1}}\\mathbf{R}^{(b)^{Q}} \\\\\n    \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n    \\mathbf{R}^{(a)^{P}}\\mathbf{R}^{(b)^{0}} & \\mathbf{R}^{(a)^{P}}\\mathbf{R}^{(b)^{1}} & \\mathbf{R}^{(a)^{P}}\\mathbf{R}^{(b)^{2}} & ... & \\mathbf{R}^{(a)^{P}}\\mathbf{R}^{(b)^{Q}}\n\\end{bmatrix},\n\\\\\n&= \\sum_{q=0}^{Q} \\sum_{p=0}^{P} \\mathbf{A} \\odot \\begin{bmatrix}\n    \\mathbf{R}^{(a)^{0}}\\mathbf{R}^{(b)^{0}} & \\mathbf{R}^{(a)^{0}}\\mathbf{R}^{(b)^{1}} & \\mathbf{R}^{(a)^{0}}\\mathbf{R}^{(b)^{2}} & ... & \\mathbf{R}^{(a)^{0}}\\mathbf{R}^{(b)^{Q}} \\\\\n    \\mathbf{R}^{(a)^{1}}\\mathbf{R}^{(b)^{0}} & \\mathbf{R}^{(a)^{1}}\\mathbf{R}^{(b)^{1}} & \\mathbf{R}^{(a)^{1}}\\mathbf{R}^{(b)^{2}} & ... & \\mathbf{R}^{(a)^{1}}\\mathbf{R}^{(b)^{Q}} \\\\\n    \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n    \\mathbf{R}^{(a)^{P}}\\mathbf{R}^{(b)^{0}} & \\mathbf{R}^{(a)^{P}}\\mathbf{R}^{(b)^{1}} & \\mathbf{R}^{(a)^{P}}\\mathbf{R}^{(b)^{2}} & ... & \\mathbf{R}^{(a)^{P}}\\mathbf{R}^{(b)^{Q}}\n\\end{bmatrix}.\n\\\\\n\n\nIn this formulation, $\\otimes$ represents the outer product, $\\odot$ denotes element-wise multiplication, and $\\circledast$ is an operation we define that functions similarly to the outer product but uses element-wise multiplication for the fusion process.\nAs shown in equation 9, instead of using two independent vectors a and b that result in $\\mathbf{A} = a \\otimes b$, we can make $\\mathbf{A}$ a fully learnable matrix. This allows the model to explore a more efficient and flexible fusion process by learning the weights directly"}]}