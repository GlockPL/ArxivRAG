{"title": "Statistical Framework for Clustering MU-MIMO Wireless via Second Order Statistics", "authors": ["Roberto Pereira", "Xavier Mestre"], "abstract": "This work explores the clustering of wireless users by examining the distances between their channel covariance matrices, which reside on the Riemannian manifold of positive definite matrices. Specifically, we consider an estimator of the Log-Euclidean distance between multiple sample covariance matrices (SCMs) consistent when the number of samples and the observation size grow unbounded at the same rate. Within the context of multi-user MIMO (MU-MIMO) wireless communication systems, we develop a statistical framework that allows to accurate predictions of the clustering algorithm's performance under realistic conditions. Specifically, we present a central limit theorem that establishes the asymptotic Gaussianity of the consistent estimator of the log-Euclidean distance computed over two sample covariance matrices.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern wireless communication systems rely on multiple- antenna radio access technologies to enhance the overall connectivity and spectral efficiency of the network. The use of multi-antenna technology allows to introduce an additional orthogonality dimension that can be combined with time and/or frequency multiplexing to enhance the total achievable rates in a wireless setting. When the total number of user equipments (UE) is larger than the number of antennas at the base station (BS), spatial multiplexing alone is not able to completely cancel out interference among transmissions. In this type of setting, several advanced non-orthogonal techniques have been proposed in the literature, which try to manage this residual interference. Examples of this type of approach are non- orthogonal multiple access (NOMA) [1], joint spatial division multiplexing (JSDM) [2] and rate splitting (RSMA) [3] to name a few. All these methods need an effective partitioning of receivers into clusters according to spatial proximity, so that specific signal processing is applied to separately treat inter-cluster and intra-cluster interference.\nTypically, one may use spatial precoding to separate clusters of users in the angle domain. In this case, the similarity among principal channel subspaces can be taken as a relevant measure of angle proximity [4]\u2013[6]. One may therefore use orthogonal multiplexing to separate users in the same angular cluster.\nHowever, this is not enough in heavily loaded systems, where one may additionally exploit the power domain to separate users within the same angular characteristics. The idea here is that simultaneous transmission of signals with very different power may be possible with the use of sequential interference cancellation [2], [7] so that one may cluster users according to their spatial proximity (both in terms of their angles of arrival and distance to the BS). Inter-cluster separability can be attempted by spatial multiplexing and sequential interfer- ence cancellation, whereas intra-cluster separability can be achieved by orthogonal techniques. This previously requires clustering the channels of the different UEs according to their spatial proximity. Since wireless channels experiment rapid fluctuations while multiplexing schemes need to be fixed for long periods of time, it seems convenient to carry out this clustering by relying on the second order statistics of the channels, rather than the channel realizations themselves. In order to perform this clustering, it seems convenient to rely on distances between spatial covariance matrices.\nRecent approaches have shifted focus from the traditional Euclidean distance [8], [9] to the study of distances that exploit the fact that covariance matrices belong to the Riemmannian manifold of positive definite matrices [10]-[14]. This is the case of the (square) log-Euclidean distance [15] which, given two covariance matrices $R_1$, $R_2$, is defined as\n$\\d_M(1,2) = \\frac{1}{M} \\text{tr} \\left[ (\\log R_1 - \\log R_2)^2 \\right]$\nwhere the logarithm is applied matrix-wise (i.e. to the eigen- values). The log-Euclidean metric was originally derived by endowing the manifold of positive definite matrices with an appropriate Lie group structure, together with a logarithmic scalar multiplication that gives the essential properties of a vector space [16]. Contrary to other alternatives, e.g., affine- invariant metric [12], [17], the log-Euclidean distance is more amenable from the computational complexity, has a closed form solution for its (Fr\u00e9chet) mean and always yields a positive definite Gaussian kernel [18]. Hence, it is often the choice when comparing covariance matrices [17]-[20].\nUnfortunately, in practical scenarios, the BS only has access to the instantaneous channel of each transmitter and has to estimate the distance between spatial covariance matrices using raw channel data. The idea here is that the BS can periodically estimate the channel of each UE (e.g. by the"}, {"title": "II. PRELIMINARIES", "content": "Let us consider a wireless scenario where a base station (BS) equipped with M antennas simultaneously communicates to K single-antenna user equipments (UEs) over an uplink channel. We assume that the BS has access to $N_j$ independent realizations of the jth user's channel matrix and denote as $Y_j \\in \\mathbb{C}^{M \\times N_j}$ the matrix containing these $N_j$ realizations as columns, for $j = 1,..., K$. Moreover, we consider a correlated a Rayleigh Fading channel model such that the channel matrix of the jth user is generated as\n$Y_j = R_jX_j$\nwhere $X_j$ is an $M \\times N_j$ matrix of i.i.d. entries with zero mean and unit variance associated to the small-scale fading whereas $R_j \\in \\mathbb{C}^{M \\times M}$ is the channel covariance matrix at the BS related to the jth user. Specifically, $R_j$ models the channel's spatial correlation and large-scale fading $\\beta_j = M^{-1}\\text{tr}(R_j), j = 1,...,K$, which depend on the angular position and distance between the UE and the BS, respectively.\nFollowing the discussion above, we are interested in clus- tering the different transmitters such that UEs with similar covariance matrices belong to the same cluster. Unfortunately,"}, {"title": "A. Estimators of log-Euclidean Distance", "content": "the BS only has access to an estimator of these covariance matrices, namely the SCM\n$\\hat{R}_j = \\frac{1}{N_j} Y_jY_j^H, j = 1,..., K$\nassociated to each of the UEs. In this context, for a pair of UEs $(r, s)$, $r \\neq s, r, s = 1, ..., K$, the BS may attempt to estimate $d_M(r, s)$ by plugging the SCMs $\\hat{R}_r$, $\\hat{R}_s$ into (1) resulting in the traditional plug-in estimator, hereafter, defined as\n$\\hat{d}_M(r, s) = \\frac{1}{M} \\text{tr} \\left[ (\\log \\hat{R}_r - \\log \\hat{R}_s)^2 \\right]$\nFor clarity, in this section, we will assume $K = 2$ and therefore $r = 1, s = 2$. In the next section, we will explore the more general case where $K > 2$. Furthermore, we will omit the indices $(1, 2)$ whenever it is clear from the context and write, for instance, $d_M = d_M(1,2)$.\nAs discussed above, in the wireless communications con- text, where the number of channel samples $N_1, N_2$ is compa- rable to the number of antennas at the BS, the plug-in estimator often fails to correctly approximate $d_M$. In this setting, it is more reliable to use another form of distance estimator, which turns out to be consistent even if $N_1, N_2$ increase with $M$, see further [9]. Let us now introduce some formal mathematical assumptions that will guarantee the consistency of the proposed estimator.\nWe make the following assumptions on the covariance matrices $R_j$, $j = 1, ..., K$:\n(As1) The different eigenvalues of $R_j$ are denoted $0 < \\lambda_1^{(j)} < ... < \\lambda_{M_j}^{(j)} < \\Lambda$ for $j = 1,..., K$, and have multiplicity $K_1^{(j)},..., K_{M_j}^{(j)}$, where $M_j$ is the total number of distinct eigenvalues. All these quantities may vary with $M$ but we always have $\\inf_M M_j > 0$ and $\\sup_M \\lambda_M^{(j)} < \\infty$.\n(As2) The quantities $N_j$, $j = 1,..., K$ depend on $M$, that is $N_j = N_j(M)$. Furthermore, when $M \\rightarrow \\infty$ we have, for $j = 1,...,K$, $N_j(M) \\rightarrow \\infty$ in a way that $N_j \\neq M$ and $M/N_j \\rightarrow c_j$ for some constant $0 < c_j < 1$.\nUnder the above conditions, it follows from [9], that $\\hat{d}_M - d_M \\rightarrow 0$ with probability one, where the consistent estimator $\\hat{d}_M$ can be expressed as follows. Let us denote the eigenvalues and the associated eigenvectors of the SCM $\\hat{R}_j$ as $\\hat{\\lambda}_1^{(j)} < ... < \\hat{\\lambda}_M^{(j)}$ and $\\hat{\\mathbf{e}}_1^{(j)}, ..., \\hat{\\mathbf{e}}_M^{(j)}$ , respectively. The consistent estimator $\\hat{d}_M$ can be expressed as\n$\\hat{d}_M = \\alpha_M^{(1)} + \\alpha_M^{(2)} - \\sum_{k=1}^{M} \\sum_{m=1}^{M} \\left( \\hat{\\mathbf{e}}_k^{(2)H} \\hat{\\mathbf{e}}_m^{(1)} \\right)^2 \\left[ \\beta_{k,m}^{(2)} \\right]^2$\nwhere (for $j = 1, ..., K$) the coefficients $\\beta_k^{(j)}$ and $\\alpha_M^{(j)}, k = 1,..., M$, are defined as\n$\\beta_k^{(j)} = \\left[ 1 + \\frac{1}{N_j} \\sum_{m=1}^{M} \\frac{\\hat{\\lambda}_m^{(j)}}{\\hat{\\lambda}_k^{(j)} - \\hat{\\lambda}_m^{(j)}} \\right] \\log \\hat{\\lambda}_k^{(j)} - \\log \\hat{\\lambda}_k^{(j)} + 1 - \\frac{1}{N_j} \\sum_{r=1 \\atop r \\neq k}^{M} \\frac{\\hat{\\lambda}_r^{(j)}}{\\hat{\\lambda}_k^{(j)} - \\hat{\\lambda}_r^{(j)}} \\log \\hat{\\lambda}_r^{(j)}$"}, {"title": "III. ASYMPTOTIC FLUCTUATION OF \u00ceM", "content": "The results in the previous section were derived for the comparison of two SCMs. In practical scenarios, however, one is usually interested in considering multiple distances among several covariances and how they relate to each other. In this context, let $\\mathbf{d}_M$ denote an R-dimensional column vector con- taining the consistent estimator of the log-Euclidean distances between R different pairs from a total of K transmitters, that is\n$\\mathbf{\\hat{d}}_M = [\\hat{d}_M(i_1, j_1), ..., \\hat{d}_M(i_R, j_R)]^T$\nwhere $1 < i_r, j_r < K, i_r \\neq j_r, r = 1,..., R$ is the distance between two different UEs (indexed by $i_r$ and $j_r$) among the R considered pairs. Likewise, we will denote by $\\mathbf{d}_M$ the R-dimensional column vector containing the actual distances between these covariances, so that (by the above discussion), $|\\mathbf{\\hat{d}}_M - \\mathbf{d}_M| \\rightarrow 0$ almost surely. In this section, we will show that the random vector $\\zeta_M = M(\\mathbf{\\hat{d}}_M - \\mathbf{d}_M)$ of size $R \\times 1$ asymptotically fluctuates according to a multivariate Gaussian distribution with zero mean and certain covariance.\nLet us introduce the expression of the asymptotic covariance matrix here. To that effect, let us consider the resolvent matrix $Q_j(\\omega) = (R_j - \\omega I_M)^{-1}$. The asymptotic covariance matrix $\\Sigma_M$ is defined as an $R \\times R$ matrix with $(r, s)$th entry given by\n$[\\Sigma_M]_{r,s} = \\frac{1}{(2 \\pi j)^4} \\oint_{C_{\\omega_{i_r}}} \\oint_{C_{\\omega_{j_r}}} \\oint_{C_{\\omega_{i_s}}} \\oint_{C_{\\omega_{j_s}}} (\\log(\\omega_{i_r}) - \\log(\\omega_{j_r}))^2 (\\log(\\omega_{i_s}) - \\log(\\omega_{j_s})) \\times \\bar{\\sigma}_{i_r,j_r,i_s,j_s}(\\omega_{i_r}, \\omega_{j_r}, \\omega_{i_s},\\omega_{j_s}) d\\omega_{i_r}d\\omega_{j_r}d\\omega_{i_s}d\\omega_{j_s}$\nand where $C_{\\omega_j}$ is a simple contour enclosing all the eigenvalues of $R_j$ and not {0}, and where $\\bar{\\sigma}_{i,j,m,n} (\\omega_i, \\omega_j, \\tilde{\\omega}_m, \\tilde{\\omega}_n)$ is defined as\n$\\bar{\\sigma}_{i,j,m,n} (\\omega_i, \\omega_j, \\tilde{\\omega}_m, \\tilde{\\omega}_n) = Q_{i,j} (\\omega_i, \\omega_j, \\tilde{\\omega}_m, \\tilde{\\omega}_n) \\delta_{i=m} \\delta_{j=n} + Q_{i,j}(\\omega_i, \\omega_j, \\tilde{\\omega}_\\eta, \\tilde{\\omega}_m) \\delta_{i=\\eta} \\delta_{j=m} + \\bar{\\sigma}_{i,j}^1 (\\omega_i, \\tilde{\\omega}_m; Q_j (\\omega_j), Q_n (\\tilde{\\omega}_n)) \\delta_{i=m} + \\bar{\\sigma}_{i,j}^2 (\\omega_j, \\tilde{\\omega}_n; Q_i (\\omega_i), Q_m(\\tilde{\\omega}_m)) \\delta_{j=n} + \\bar{\\sigma}_{i,j}^3 (\\omega_i, \\tilde{\\omega}_n; Q_j (\\omega_j), Q_m (\\tilde{\\omega}_m)) \\delta_{i=n} + \\bar{\\sigma}_{i,j}^4 (\\omega_j, \\tilde{\\omega}_m; Q_i (\\omega_i), Q_n(\\tilde{\\omega}_n)) \\delta_{j=m}$\nwith the following additional definitions. The functions $\\sigma_{\\xi}^j (\\omega, \\tilde{\\omega}; A, B)$ for $j = 1, ..., K$ are defined as\n$\\sigma_{\\xi}^j (\\omega, \\tilde{\\omega}; A, B) = \\frac{\\Gamma_j(\\omega, \\tilde{\\omega}) \\text{tr} \\left[ A \\Gamma_j(\\omega, \\tilde{\\omega}) B \\right]}{N_j (1 - \\Gamma_j(\\omega, \\tilde{\\omega}))} + \\frac{1}{N_j^2} \\frac{\\text{tr} \\left[ R_j Q_j (\\omega) A \\right] \\text{tr} \\left[ R_j Q_j (\\omega) B \\right]}{\\Gamma_j(\\omega) \\Gamma_j(\\tilde{\\omega})}$\nand where we have introduced the quantities\n$\\Gamma_j(\\omega, \\tilde{\\omega}) = \\frac{1}{N_j} \\text{tr} \\left[ R_j Q_j (\\omega) Q_j (\\tilde{\\omega}) \\right]$\n$\\Gamma_j(\\omega) = \\frac{1}{N_j} \\text{tr} \\left[ R_j Q_j (\\omega) Q_j (\\omega) \\right]$\nFinally, the functions $Q_{i,j} (\\omega_i, \\omega_j, \\tilde{\\omega}_i, \\tilde{\\omega}_j)$ are defined as\n$Q_{i,j} (\\omega_i, \\omega_j, \\tilde{\\omega}_i, \\tilde{\\omega}_j) = \\frac{\\text{tr}^2 \\left[ R_i Q_i (\\omega_i) Q_i (\\tilde{\\omega}_i) R_j Q_j (\\omega_j) Q_j (\\tilde{\\omega}_j) \\right]}{N_i N_j (1 - \\Gamma_i (\\omega_i, \\tilde{\\omega}_i)) (1 - \\Gamma_j (\\omega_j,\\tilde{\\omega}_j))}$\nHaving introduced the terms above, we are now in the position to formulate a central limit theorem.\nTheorem 3.1: In addition to (As1)-(As2), assume that the observations are complex circularly symmetric Gaussian dis- tributed and that the minimum eigenvalue of $\\Sigma_M$ is bounded away from zero. Then, the random vector\n$\\Sigma_M^{-1/2} \\left[ M(\\mathbf{\\hat{d}}_M - \\mathbf{d}_M) \\right]$\nconverges in law to a multivariate standard Gaussian."}, {"title": "IV. NUMERICAL VALIDATION", "content": "In order to validate the results presented above, we con- sider the scenario where K distinct transmitters are each uniquely assigned to one of three groups (g = 1,2,3) based on their spatial locations. More specifically, we fix three cluster centroids with coordinates (in meters) at $(x,y) = (20, 60)\\tau, (70, 70)\\tau, (80,30)\\tau$, where $\\tau > 0$ is a certain scaling parameter that effectively controls how close these clusters are. Each transmitter within a group is randomly placed within a 15 meter radius from its group's centroid location, while ensuring that no two transmitters are placed at the same location.\nWe follow a similar model as the one detailed in [24] used to model 3GPP channels, e.g. the Urban Microcell model. Hence, we model the large-scale fading coefficient, measured in decibels dB for each transmitter, as\n$\\beta_j = -30.5 - 36.7\\log_{10}(d_j) + S_j$\nwhere $d_j, j = 1,...,K$ is the three-dimensional physical distance (in meters) between the jth transmitter to the BS. The shadowing is designed as a random variable with zero mean and is correlated among different users by $E[s_j s_k] = 42^{-18jk}$, where $d_{jk}$ is the distance between the jth and kth UE (see [24, Chapter 5] for details on the fading modeling). Additionally, we consider an uplink communication scenario utilizing a 20MHz bandwidth, with the total receiver noise power of -94dBm. We assume the BS to be located at (0,0) and to be elevated 12 meters above the ground, whereas all the transmitters are placed 2 meters above from the ground.\nFinally, we consider non-line-of-sight communication only and model the channel's covariance matrices $R_j, j=1...,K$ as the contribution of multi-path signals impinging over a M dimensional uniform linear array, namely\n$R_j = \\beta_j \\int_{-\\pi}^{\\pi} \\mathbf{a}(\\theta) \\mathbf{a}^H(\\theta) g_j(\\theta) d\\theta$\nwhere $\\mathbf{a}(\\theta)$ is the array response at the base station's an- tenna array at angle $\\theta$ and $g_j(\\theta)$ represents the probabil- ity density function of a Gaussian distribution with mean $\\theta_j = \\arctan(y_j/x_j)$ (i.e., depending on the user's location) and variance 30\u00b0 (fixed for all transmitters). We assume for simplicity of exposition that the number of channel samples $N_g, g = 1,2,3$ are the same among all the UEs associ- ated to the same cluster centroid, so that we will denote $c_g = M/N_g$ to represent the ratio between the number of antennas at the BS and the number of samples at groups g = 1, 2, 3.\nWe start by comparing the asymptotic distribution described in Theorem 3.1 with the empirical distribution of the esti- mated distances with finite dimensions."}, {"title": "A. Clustering of Multiple UEs", "content": "We now consider the clustering of multiple SCMs in the wireless communications context. Specifically, we explore the advantages of using the consistent estimate over the traditional plug-in estimator to cluster multiple UEs based on their SCMs. In this context, we examine the wireless simulation scenario outlined above and illustrated in Figure 1, with three groups (G = 3) and different values of K. For clarity, in the figures, the traditional plug-in estimator is denoted as \u201cTRAD\u201d, while the consistent estimator is labeled as \"IMP\".\nWe consider that the clustering using a specific distance is successful for a specific set of channel realizations when the inter-cluster distance is always higher than the largest intra- cluster distance. We can therefore evaluate the probability of clustering success by conditioning on each of the three clusters achieving the maximum intra-cluster distance. Let $\\mathcal{J}_{inter}$ denote the set of pairs of indexes of UEs belonging to different clusters and $\\mathcal{J}_{intra(g)}$ the set of pairs of indexes of UEs belonging to the g cluster. The probability of success can be mathematically expressed as\n$\\mathcal{P}_{succ} = \\sum_{g=1}^3 \\prod \\mathbb{P} \\{\\hat{d}_M(i, j) > \\underset{(k,l) \\in \\mathcal{J}_{intra(g)}}{\\text{max}} \\hat{d}_M(k, l) \\| A_g \\} \\mathbb{P} [A_g]$\nwhere we have denoted as $A_g$ the event that the gth cluster achieves the maximum intra-cluster distance.\nEach of these probabilities can in turn be written as $\\mathbb{P} (A \\hat{\\mathbf{d}}_M < 0)$ where $A \\mathbf{d}_M$ is a column vector that contains all the distances as in (5) and A is a selection matrix with all the entries equal to zero except for one +1 and one -1 for each row, corresponding to the selection of distances to conform the different events that come into play into each argument of the above expression. Since $\\mathbf{\\hat{d}}_M$ is asymptotically Gaussian distributed, so is the resulting column vector $A \\hat{\\mathbf{d}}_M$. In fact, the transformed vector will also be asymptotically approximated as Gaussian distributed, with mean $A \\mathbf{d}_M$ and covariance $A(\\Sigma_M/M^2)A^T$. Hence, each of the probabilities above can be evaluated by a multivariate Gaussian cumulative distribution function evaluated at zero. To evaluate the empir- ical probability of accurate clustering (referred to as \"Prob. Clustering\" in the figures), we will utilize $10^3$ simulations. This clustering success rate is then defined as the percentage of realizations where the all estimated intra-cluster distances are lower than the smallest estimated inter-cluster distance."}, {"title": "V. CONCLUSION", "content": "In this work, we have proposed a statistical framework to estimate the probability of correct clustering multiple UEs via their channels' sample covariance matrices using a consistent estimator of the log-Euclidean distance of two SCMs. Numerical simulations confirm that the consistent estimator can increase the quality of clustering solutions even for relative low number of channel samples. Finally, we show that the proposed framework not only enhances the clustering accuracy but also provides a reliable predictive methodology for assessing the performance of these clustering algorithms in realistic MU- MIMO settings."}]}