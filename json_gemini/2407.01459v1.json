{"title": "On Implications of Scaling Laws on Feature\nSuperposition", "authors": ["Pavan Katta"], "abstract": "Using results from scaling laws, this theoretical note argues that the following two\nstatements cannot be simultaneously true:\n1. Superposition hypothesis where sparse features are linearly represented across\na layer is a complete theory of feature representation.\n2. Features are universal, meaning two models trained on the same data and\nachieving equal performance will learn identical features.", "sections": [{"title": "Introduction", "content": "Scaling laws for language models give us a relation for a model's macroscopic properties such as\ncross entropy loss L, Amount of Data D used and Number of non-embedding parameters N in the\nmodel (Kaplan et al., 2020).\n\nL(N.D) = \\left(\\frac{N_c}{N}\\right)^{\\alpha_N} + \\left(\\frac{D_c}{D}\\right)^{\\alpha_D}\n\nwhere Nc, Dc, \u03b1N, and \u03b1D are constants for a given task such as language modeling.\nThe scaling laws are not mere empirical observations and can be seen as predictive laws on limits of\nlanguage model performance. During training of GPT-4, OpenAI was able to predict the final loss of\nGPT-4 early in the training process using scaling laws with high accuracy (OpenAI, 2023).\nAn important detail is that the relation is expressed in terms of the number of parameters. It's natural\nto think of a model's computational capacity in terms of parameters, as they are the fundamental\nindependent variables that the model can tune during learning. The amount of computation that a\nmodel performs in FLOPs for each input is also estimated to be 2N (Kaplan et al., 2020).\nLet's compare this with Interpretability, where the representation of a feature is defined in terms of\nneurons or groups of neurons. At first glance, it might seem unnecessary to distinguish between\ncomputational capacity and feature representational capacity, as parameters are connections between\nneurons after all. However, we can change the number of neurons in a model while keeping the\nnumber of parameters constant. Kaplan et al. found that Transformer performance depends very\nweakly on the shape parameters nlayer (number of layers), nheads (number of attention heads), and\ndff (feedforward layer dimension) when we hold the total non-embedding parameter count N fixed\n(Kaplan et al., 2020). The paper reports that the aspect ratio (the ratio of number of neurons per layer\nto the number of layers) can vary by more than an order of magnitude, with performance changing by\nless than 1\nIn this paper, we assume the above to be true and consider the number of parameters to be the true\nlimiting factor, and we can achieve similar model performance for a range of aspect ratios. We then"}, {"title": "Case study on changing Aspect Ratio", "content": "Let's consider two Transformer models, Model A and Model B, having the same macroscopic\nproperties. Both have an equal number of non-embedding parameters, are trained on the same\ndataset, and achieve similar loss according to scaling laws (Kaplan et al., 2020). However, their shape\nparameters differ. Using the same notation as Kaplan et al., let's denote the number of layers as\nNlayer, and number of neurons per layer as dmodel. Model B has twice the number of neurons per\nlayer compared to A. As the number of parameters is approximated by dmodelNlayer, Model B must\nhave 1/4 the number of layers to maintain the same number of parameters as Model A. This means\nModel B has 8 times the aspect ratio (\\frac{d_{model}}{N_{layer}}) of A which falls under the reported range in Kaplan et\nal.\nThe total number of neurons in a model is calculated by multiplying the number of neurons per layer\nby the number of layers. As a result, Model B has half the total number of neurons compared to\nModel A.\nNow, let's apply the superposition hypothesis, which states that features can be linearly represented\nin each layer. Since both models achieve equal loss on the same dataset, it's reasonable to assume\nthat they have learned the same features (Olah et al., 2020). Let's denote the total number of features\nlearned by both models as F.\nThe above three paragraphs are summarized in Table 1.\nThe average number of features per neuron is calculated by dividing the number of features per layer\nby the number of neurons per layer. In Model B, this value is twice as high as in Model A, which\nmeans that Model B is effectively compressing twice as many features per neuron, in other words,\nthere's a higher degree of superposition. However, superposition comes with a cost of interference\nbetween features, and a higher degree of superposition requires more sparsity."}, {"title": "Discussion", "content": "To recap, we started with the postulate that model performance is invariant over a wide range of aspect\nratios and arrived at the inconsistency between superposition and feature universality. Though we\nframed the argument through the lens of superposition, the core issue is that the model's computational\ncapacity is a function of parameters whereas the model's representational capacity is a function of\ntotal neurons.\nA useful, though non-rigorous analogy, is to visualize a solid cylinder of radius dmodel and height\nnlayer. The volume (parameters) of the cylinder can be thought of as computational capacity whereas\nfeatures are represented on the surface (neurons). We can change the aspect ratio of the cylinder while\nkeeping the volume constant by stretching or squashing it. This changes the surface area accordingly.\nThough this analogy doesn't include sparsity, it captures the essentials of the argument in a simple\nway.\nComing to solutions, I do not have one that's consistent with scaling laws, superposition hypothesis,\nand feature universality, but will speculate on what a possible one might look like."}, {"title": "Schemes of Compression Alternative to Superposition", "content": "A crude and simple way to convert the total number of features into a function of parameters is to add\na square term to compressed sensing bounds so it becomes n = m^2\\cdotf(1 \u2013 S). But this would require"}, {"title": "Cross Layer Superposition", "content": "Previously, we used to look for features in a single neuron (Radford et al., 2017), now we extended it\nto a group of neurons in a layer. A natural progression is to look for features localizing to neurons\nacross multiple layers. But Model B from the above section, has half the number of neurons as A and\nthe same inconsistencies would arise if features grow linearly on the number of neurons. Number of\nfeatures represented across two or more layers by cross-layer superposition should grow superlinearly\nif Model B were to compensate for fewer neurons and still have the same representational capacity."}]}