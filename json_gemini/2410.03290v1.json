{"title": "GROUNDED-VIDEOLLM: SHARPENING FINE-GRAINED TEMPORAL GROUNDING IN VIDEO LARGE LANGUAGE MODELS", "authors": ["Haibo Wang", "Zhiyang Xu", "Yu Cheng", "Shizhe Diao", "Yufan Zhou", "Yixin Cao", "Qifan Wang", "Weifeng Ge", "Lifu Huang"], "abstract": "Video Large Language Models (Video-LLMs) have demonstrated remarkable capabilities in coarse-grained video understanding, however, they struggle with fine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM, a novel Video-LLM adept at perceiving and reasoning over specific video moments in a fine-grained manner. We identify that current Video-LLMs have limitations for fine-grained video understanding since they lack effective temporal modeling and timestamp representation. In light of this, we sharpen our model by incorporating (1) an additional temporal stream to encode the relationships between frames and (2) discrete temporal tokens enriched with specific time knowledge to represent timestamps. To optimize the training of Grounded-VideoLLM, we employ a multi-stage training scheme, beginning with simple video-captioning tasks and progressively introducing video temporal grounding tasks of increasing complexity. To further enhance Grounded-VideoLLM's temporal reasoning capability, we also curate a grounded VideoQA dataset by an automatic annotation pipeline. Extensive experiments demonstrate that Grounded-VideoLLM not only excels in fine-grained grounding tasks such as temporal sentence grounding, dense video captioning, and grounded VideoQA, but also shows great potential as a versatile video assistant for general video understanding.", "sections": [{"title": "INTRODUCTION", "content": "Multi-modal Large Language Models (MLLMs) have made remarkable progress in image-level understanding (Liu et al., 2023; Dai et al., 2023; Li et al., 2023a). However, extending their capabilities to the video domain poses distinct challenges. Unlike static images, the temporal nature of videos challenges models to process not only visual content but also the sequence and timing of events. While current Video-LLMs (Xu et al., 2024a; Li et al., 2024; Zhang et al., 2023b; Lin et al., 2023) are capable of capturing global visual semantics and generating coarse-grained captions for short clips, they struggle with fine-grained video understanding (Liu et al., 2024b; Wang et al., 2024d), which requires decomposing the video along the temporal axis to accurately perceive and reason over specific moments, such as subtle actions, transitions, and events that unfold over time.\nPrevious research efforts (Ren et al., 2024; Huang et al., 2024a; Qian et al., 2024a; Huang et al., 2024b; Guo et al., 2024) have explored temporal grounding to improve fine-grained video understanding. However, two main challenges impede their potential for achieving effective fine-grained temporal grounding: (1) Models like Video-ChatGPT (Maaz et al., 2024b), P-LLaVA (Xu et al., 2024a), and Video-LLAMA (Zhang et al., 2023b) typically sample multiple frames from a video and encode each frame independently using an image encoder, followed by a feature projector (e.g., sliding Q-former (Ren et al., 2024), slot-based token compression (Guo et al., 2024), or visual adapter (Huang et al., 2024a)). This approach focuses primarily on spatial details while"}, {"title": "MODEL ARCHITECTURE", "content": "Given that current MLLMs already exhibit strong image-understanding capabilities, our architecture aims to sharpen temporal awareness by capturing motion dynamics across frames, which serve as a vital supplement to spatial content. As shown in Figure 2, we develop Grounded-VideoLLM upon a well-established MLLM for spatial comprehension and integrate an expert video encoder for temporal comprehension. Additionally, to avoid tokenizing numerical texts, we incorporate temporal tokens into the LLM's vocabulary for efficient and unified timestamp representation."}, {"title": "Two-STREAM ENCODING", "content": "To effectively model the long-range temporal structure, given a video V with T frames, we divide it into K segments and employ a segment-wise encoding strategy (Wang et al., 2016). Due to the inherent redundancy of consecutive frames in videos, each segment can be naturally represented from two perspectives: spatial and temporal. The spatial representation of each segment is derived from an individual keyframe, capturing the primary appearance semantics, while the temporal representation is learned from multiple frames depicting the motion evolution within the segment. We discuss the details of our two-stream segment encoding as follows.\nSpatial Stream. We sample the middle frame from each segment as the keyframe and extract its spatial features using the original image encoder from the MLLM (Radford et al., 2021), resulting in spatial features $F_s \\in \\mathbb{R}^{H_s \\times W_s \\times D_s}$, where $H_s$, $W_s$, $D_s$ denote the height, width and dimension of the spatial features. Since dense frames are crucial for fine-grained temporal grounding, an appropriate pooling strategy is required to reduce token length for efficient computation. As indicated by Xu et al. (2024a) and Yao et al. (2024) that a 2D average pooling is both efficient and robust for spatial downsampling, we employ a 2D pooling kernel with a size $\\sigma_s \\times \\sigma_s$ over the feature map and gets $F_s \\in \\mathbb{R}^{N_s \\times D_s}$ as the feature for spatial stream, where $N_s = \\frac{H_s \\times W_s}{\\sigma_s \\times \\sigma_s}$.\nTemporal Stream. Traditional two-stream networks (Simonyan & Zisserman, 2014; Feichtenhofer et al., 2016) typically encode the optical flow as the temporal stream. However, given the scale of data and parameters involved in MLLMs, extracting optical flow is computationally expensive and impractical. Consequently, we resort to a strong and well pre-trained video encoder, InternVideo2 (Wang et al., 2024b), to extract motion representations for each segment, using a lower resolution but more frames. Specifically, we input each segment, containing frames, into the video encoder to obtain the segment-level features $F_T\\in \\mathbb{R}^{k\\times H_T\\times W_T\\times D_T}$, where $H_T$, $W_T$, $D_T$ denote the height, width and dimension of each frame feature. Similar to the spatial stream, we apply a 2D average pooling strategy to downsample $F_T$. However, as the temporal stream focuses primarily on temporal modeling, we retain the complete temporal information by only pooling along the spatial dimensions. Specifically, we aggressively downsample $F_T$ using a kernel with a larger size of $\\sigma_T \\times \\sigma_T$, resulting in the compressed $F_T \\in \\mathbb{R}^{\u310a\\times N_T \\times D_T}$ for temporal stream, where $N_T = \\frac{H_T \\times W_T}{\\sigma_T \\times \\sigma_T}$."}, {"title": "UNIFIED TEMPORAL TOKENS", "content": "Given a text span depicting a particular video clip and its associated timestamps, we employ a relative time representation that converts continuous timestamps into a sequence of discrete temporal tokens. For a video V with a duration of L seconds, we evenly divide V into M equal-length chunks, and then define M + 1 anchor points (ranging from <0> to <M>) across V, representing relative temporal positions. Each anchor point corresponds to a specific timestamp of V and is encoded as a temporal token. For instance, <0> denotes the very start of V while <M> indicates the end. These M + 1 tokens are added to the LLM's vocabulary to enable unified modeling alongside text. A specific continuous timestamp + can be easily converted to a temporal token <t> and vice verse:\n$t = Round(M \\cdot \\frac{\\tau}{L})$, \n$\\tau = L \\cdot \\frac{t}{M}$\nWhile this may introduce minor quantization errors, these can be minimized by selecting an appropriate M. We then organize the text span and its corresponding temporal tokens in a unified format. Both text tokens and temporal tokens are mapped to embeddings through the extended word embedding layer of LLM. For example, one input representation is as follows:\n<s><video>Fvid</video> <grounded> From <0> to <6>, a baby is crying. From\n<7> to <16>, a man is coming and picking up the baby. From <20> to <25>, the baby is\neating an apple. From <27> to <35>, the baby is smiling happily.</s>\nwhere <s> and </s> indicate start- and end-of-sequence, <video> and </video> represent\nthe beginning and end of encoded video representations. <grounded> is a special token to tell\nthe model should output the grounded timestamps. This strategy avoids the need to tokenize and\nprocess numerical values, which has been identified as a limitation of LLMS (Schwartz et al., 2024),\nand it greatly simplifies the representation of timestamps within the unified embedding space of\nLLMs. Consequently, text and timestamps can be jointly decoded as a single sequence, following\nthe objective outlined in Eq. (2)."}, {"title": "PROGRESSIVE TRAINING", "content": "Different from previous methods (Zhang et al., 2023b; Lin et al., 2023) that train models from scratch using mixed image and video datasets, we start with a pre-trained image-based MLLM (Microsoft, 2024) and progressively enhance its fine-grained temporal grounding capabilities. This strategy can be applied to any off-the-shelf MLLM and is more efficient. Table 1 enumerates the datasets used at different training stages, and Table 9 lists the prompts for different tasks."}, {"title": "Grounded VideoQA Dataset Generation", "content": "Grounded VideoQA requires the model to not only answer questions but also identify relevant video moments that support predicted answers, thereby demonstrating the model's temporal reasoning abilities. The NEXT-GQA dataset (Xiao et al., 2024) was manually developed by extending NEXT-QA (Xiao et al., 2021) with temporal labels for start and end timestamps. However, annotating these temporal labels is labor-intensive and time-consuming, which has limited NEXT-GQA only to QA pairs for the validation and test sets. To create a scalable training dataset, we utilized OpenAI GPT-4 (Achiam et al., 2023) to assist in constructing training sets for the grounded VideoQA task. These sets were built on public datasets that already contain temporal labels, such as ActivityNet-"}, {"title": "EXPERIMENTS", "content": ""}, {"title": "EXPERIMENT SETTING", "content": "Implementation Details. We select the Phi3.5-Vision-Instruct-3.8B (Microsoft, 2024) as the base MLLM for our Grounded-VideoLLM. For temporal stream encoding, we adopt InternVideo2-1B (Wang et al., 2024b) as the video encoder. Each video is sampled as a sequence of T = 96 frames, which are evenly divided into K = 12 segments. We set the pooling size $\\sigma_s$ = 2 for the spatial stream while $\\sigma_T$ = 4 for the temporal stream respectively, which results in $N_s$ = 144 tokens per frame for the spatial stream while $N_T$ = 16 tokens per frame for the temporal stream. Moreover, we introduce M = 300 temporal tokens into the LLM's vocabulary for timestamp representation. More implementation details can be found in Appendix A.1.\nTasks and Benchmarks. To thoroughly evaluate Grounded-VideoLLM in fine-grained temporal grounding, we assess it across three video temporal grounding tasks: Temporal Sentence Grounding, Dense Video Captioning, and Grounded VideoQA, utilizing datasets such as Charades-STA (Gao et al., 2017), ActivityNet-Captions (Caba Heilbron et al., 2015), and NExT-GQA (Xiao et al., 2024). We also show its reasoning capability by the task of Open-Ended VideoQA with datasets in-cluding MSVD-QA, MSRVTT-QA (Xu et al., 2017), and ActicityNet-QA (Yu et al., 2019). Additionally, to evaluate the model's general video understanding capabilities, we benchmark Grounded-VideoLLM against existing models using modern Video-LLM benchmarks including VCG-Bench (Maaz et al., 2024b) and MVBench (Li et al., 2024).\nEvaluation Metrics. For temporal sentence grounding, we report the metric of Intersection over Union (IoU) (Gao et al., 2017) between the timestamps predicted by the model and the ground truth, including Recall at IoU thresholds of {0.3, 0.5, 0.7} and their mean IoU. For dense video captioning, we use metrics including SODA_c (Fujita et al., 2020) which is specifically tailored for the video's storyline, and METEOR score (Banerjee & Lavie, 2005), which is the average of"}, {"title": "MAIN RESULTS", "content": "Temporal Sentence Grounding requires the model to identify the precise time interval corresponding to a given query sentence. As shown in Table 2, Grounded-VideoLLM achieves performance on \"mIoU\" with 36.8 for the Charades-STA (Gao et al., 2017) and 36.1 for ActivityNet-Grounding (Caba Heilbron et al., 2015) respectively, surpassing previous SoTA end-to-end Video-LLMs, i.e., HawkEye (Wang et al., 2024c), by a significant margin (+3.4). It is worth mentioned that the promising performance of \u201cmIoU\" are largely attributed to the signigicant gains in terms of \"R@0.7\" compared with other thresholds, demonstrating that Grounded-VideoLLM is more advanced in localizing specific moments within videos with finer granularity.\nDense Video Captioning involves generating descriptions for all events in a video, along with their corresponding start and end timestamps. We evaluated Grounded-VideoLLM on the ActivityNet-Captions (Caba Heilbron et al., 2015), and the results in Table 2 show that our method achieves the highest SODA_c score (6.0), which demonstrates that, thanks to the Temporal Token Alignment training stage, Grounded-VideoLLM is highly effective in identifying the multi-event structure of the video and capturing complete storylines. The highest METEOR score (6.8) also indicates that Grounded-VideoLLM provides more detailed event descriptions compared with other Video-LLMs.\nNEXT-GQA (Xiao et al., 2024) is quite challenging since it requires the model to not only correctly answer questions but also provide timestamps that support the answers, highlighting the temporal reasoning capability. According to Table 3, Grounded-VideoLLM achieves the highest Acc@GQA (26.7, +2.4) and delivers comparable IoU and IoP scores to models such as SeViLA (Yu et al., 2023) and LLoVi (Zhang et al., 2023a), which use specialized grounding modules or rely on proprietary"}, {"title": "IN-DEPTH ANALYSIS", "content": "Two-stream Encoding. We conduct ablations to our two-stream encoding. Specifically, we set two variants by removing the temporal stream while only retaining the spatial stream, where all frame embeddings are concatenated as the video representation: (1) w/o temporal-stream (dense) feeds T = 96 frames with a pooling size os = 4 (36 tokens per frame), resulting in a total of 36 \u00d7 96 =\n3456 tokens. (2) w/o temporal-stream (sparse) feeds T = 24 frames with a pooling size $\\sigma_s$ = 2"}, {"title": "CONCLUSION", "content": "We present Grounded-VideoLLM, a Video-LLM capable of fine-grained perception and reasoning over specific video moments. This is achieved through a novel model architecture that incorporates two-stream encoding for effective temporal modeling, along with the temporal tokens for efficient timestamp representation. We employ a multi-stage training scheme, starting with an image-based MLLM and progressively equipping it with fine-grained temporal grounding capabilities. Additionally, we curate a grounded-VideoQA dataset to further enhance the model's temporal reasoning ability. Extensive experiments demonstrate that Grounded-VideoLLM not only excels in video temporal grounding tasks but also performs strongly on general video understanding benchmarks."}, {"title": "APPENDIX", "content": ""}, {"title": "MORE IMPLEMENTATION DETAILS", "content": "Phi3.5-Vison-Instruct (Microsoft, 2024) consists of a CLIP style ViT image encoder (Radford et al., 2021), an MLP projector f(\u00b7), and the large language model Phi3.5-mini-3.8B (Abdin et al., 2024). Each video is sampled as a sequence of T = 96 frames, which are evenly divided into K = 12 segments. For the spatial stream encoded by the ViT in Phi3.5-V, we adopt a higher resolution 336\u00d7336, but a lower resolution 224\u00d7224 for the temporal stream encoded by InternVideo-2.\nWe set the pooling size os to be 2 while \u03c3\u03c4 to be 4 respectively. For the spatial stream, each frame takes up 24 \u00d7 24 = 576 tokens before while 12 \u00d7 12 = 144 tokens after pooling. For the temporal stream, each frame takes up 16 \u00d7 16 = 256 tokens before while 4 \u00d7 4 = 16 tokens after pooling. Therefore, we have an overall of K \u00d7 (144 + $\\frac{T}{K}$ \u00d7 16) = 3264 tokens in total.\nDuring training, we use the AdamW optimizer with a cosine learning rate decay and set the learning rate as 2e-5 and 1e-3 for projector f(\u00b7) and g(\u00b7) in stage-1. During stage-2 and stage-3, we set the learning rate for both projectors and word embeddings as 2e-5, while 2e-4 for LoRA parameters (r\n= 128 and a = 256). All experiments are conducted on 8 NVIDIA A100/H800 GPUs."}, {"title": "INSTRUCTIONS FOR EACH TASK", "content": "The quality and diversity of instructions are essential in the training process. We manually write well-designed instructions for some tasks, combined with some templates in Time-IT (Ren et al.,\n2024). Table 9 lists the prompts for different tasks."}, {"title": "EVALUATION PROCESS", "content": "For the evaluation of the temporal sentence grounding task, we directly input the prompt [\"At which time interval in the video can we see < query > occurring?\"] in\nTable 9 to get the response [\"From < start > to < end >\"], and then calculate the predicted timestamps with the Equation (3) to get the IoU metrics.\nFor the evaluation of the dense video captioning task, we directly input the prompt [\"Detect\nand report the start and end timestamps of activity events in the\nvideo, along with descriptions.\"] in Table 9 to get the response [\"From\n< start\u2081 > to < end\u2081 >, < caption\u2081 >. From < start2 > to < end2 >,\n<caption2 >. ... \"], and then calculate the SODA_c (Fujita et al., 2020) and Meteor scores\n(Banerjee & Lavie, 2005).\nFor the evaluation of the visually-grounded VideoQA task, we adopt a two-round conversation eval-uation as follows:\nRound-1:\nUSER: Question: < question >. Options: < options >\nASSISTANT: Answer: < answer >.\nRound-2:\nUSER: Provide the timestamps that correspond to your answer.\nASSISTANT: From < start > to < end >.\nIn the first round, we input the question and options into the model and get the answer. In the second round, we input the question, options, and predicted answer as the contexts, together with the prompt [\"Provide the timestamps that correspond to your answer.\"], into the model to get the predicted timestamps. With both the predicted answers and timestamps, we can calculate the metrics including IoU, IoP, and Acc@GQA (Xiao et al., 2024).\nFor the evaluation of the Open-ended VideoQA and VCG-Bench, we employed GPT-3.5 turbo to\njuxtapose model outputs with ground truth data as introduced by Video-ChatGPT (Maaz et al.,\n2024b), subsequently computing both accuracy and a score. To ensure a fair and consistent compar-"}, {"title": "VISUALIZATION PROCESS", "content": "We visualize the attention weights from the last layer of the LLM during the generation of a new temporal token. Since the full video representation consists of a total of K \u00d7 (Ns + $\\frac{T}{K}$ \u00d7 NT) tokens-where T, K, Ns, and NT denote the number of frames, number of segments, number of tokens per frame for the spatial stream, and number of tokens per frame for the temporal stream, respectively we obtain an attention weight vector with the shape [K \u00d7 (Ns + $\\frac{T}{K}$ \u00d7 NT), 1]. First, we discard the spatial stream portion, focusing only on the temporal information, which results in a new vector with the shape [K \u00d7 $\\frac{T}{K}$ \u00d7 NT, 1]. We then reshape this vector to the form [T, NT, 1] and average it along the spatial dimension, yielding [T, 1], which represents the final attention weights corresponding to each frame when generating a new token."}, {"title": "DISTRIBUTION OF TEMPORAL TOKENS", "content": "We visualize the embeddings of the M = 300 temporal tokens to investigate their distribution in embedding space. We employ PCA (Abdi & Williams, 2010) to reduce the dimensionality of the temporal tokens to 1D, 2D, and 3D representations. For all reductions, we use the reduced values as coordinates, incorporating a gradient color scheme in which the color of the data points changes progressively with the token index, as illustrated in Figure 5. Our observations reveal that temporal tokens with similar indices tend to cluster together, exhibiting a continuous transition from tokens with smaller indices (light colors) to those with larger indices (darker colors)."}]}