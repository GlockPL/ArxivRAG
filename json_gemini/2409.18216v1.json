{"title": "MMMT-IF: A CHALLENGING MULTIMODAL MULTI-TURN INSTRUCTION FOLLOWING BENCHMARK", "authors": ["Elliot L. Epstein", "Kaisheng Yao", "Jing Li", "Xinyi Bai", "Hamid Palangi"], "abstract": "Evaluating instruction following capabilities for multimodal, multi-turn dialogue is challenging. With potentially multiple instructions in the input model context, the task is time-consuming for human raters and we show LLM based judges are biased towards answers from the same model. We propose MMMT-IF, an image based multi-turn Q&A evaluation set with added global instructions between questions, constraining the answer format. This challenges models to retrieve instructions dispersed across long dialogues and reason under instruction constraints. All instructions are objectively verifiable through code execution. We introduce the Programmatic Instruction Following (PIF) metric to measure the fraction of the instructions that are correctly followed while performing a reasoning task. The PIF-N-K set of metrics further evaluates robustness by measuring the fraction of samples in a corpus where, for each sample, at least K out of N generated model responses achieve a PIF score of one. The PIF metric aligns with human instruction following ratings, showing 60 percent correlation. Experiments show Gemini 1.5 Pro, GPT-40, and Claude 3.5 Sonnet, have a PIF metric that drops from 0.81 on average at turn 1 across the models, to 0.64 at turn 20. Across all turns, when each response is repeated 4 times (PIF-4-4), GPT-4o and Gemini successfully follow all instructions only 11% of the time. When all the instructions are also appended to the end of the model input context, the PIF metric improves by 22.3 points on average, showing that the challenge with the task lies not only in following the instructions, but also in retrieving the instructions spread out in the model context. We plan to open source the MMMT-IF dataset and metric computation code.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite the significant success of Large Foundation Models (LFMs) (Team et al., 2024), (Open AI, 2024), (Anthropic, 2024), (OpenAI et al., 2024) instruction following is still a challenging task (Zhou et al., 2023a). This challenge becomes more pronounced when there are multiple instructions spread out over several turns in a chat setting between a user and a LFM, where the model needs to reason over various turns of the conversation. While there are several instruction following evaluation datasets, for example (Zhou et al., 2023a), (Zhang et al., 2024), these evaluations are usually single turn and most often use text input. Another key challenge is developing objective evaluation criteria for instruction following. In collecting human annotated reference answers for our evaluation dataset, annotators reported that, at each answer turn, rewriting the answer to follow all given instructions took 10 minutes on average, highlighting that human evaluation is time intensive. Recent developments have suggested using LLMs as judges of answer quality, but we found that there was a bias in the LLM judge to favor responses coming from the same model.\nA new development has been to create tasks where model answers can be programmatically checked, in the domains of coding (Yang et al., 2023), data science (Huang et al., 2022), and text (Dong et al., 2024), ensuring an objective evaluation. Among these, (Dong et al., 2024) also focus on instruction"}, {"title": "2 DATASET", "content": "This section describes the MMMT-IF evaluation dataset, as well as the human data we collect to create reference answers and preference ratings."}, {"title": "2.1 INSTRUCTION FOLLOWING EXTENSION", "content": "The instruction following extension of the MMDU dataset, to create the MMMT-IF evaluation set was described in the Introduction 1. Note that the extension makes the task also require more long context abilities in the models, as instructions needs to be retrieved from multiple parts of the input model context."}, {"title": "2.2 HUMAN WRITTEN REFERENCE LABELS", "content": "We collect human labels for a reference response that both answers the questions correctly and follows all the constraints from the given instructions. In addition, the human annotators were asked to rate the answer accuracy from 1 to 10, the instruction following accuracy from 1 to 10 and give a pairwise preference score between each of the models (Gemini 1.5 Pro, GPT-40, and Claude 3.5 Sonnet) in our evaluation set. The full set of instructions given to the human annotators is in the Appendix G."}, {"title": "2.3 DATA FILTERING", "content": "The initial evaluation set, had a total of 1342 turns, from 98 chats, the data was filtered down to 990 turns, with 71 full chats, based on the following criteria:\n1. Removing chats where some image is corrupted: 23 chats\n2. Removing chats with more than 5 images: 3 chats\n3. Removing chats containing skipped turns due to model error or content filters: 1 chat\n4. Truncating chats to have a maximum length of 20 turns."}, {"title": "3 EVALUATION METRICS", "content": "This section introduces the PIF and PIF-N-K metrics, and provides a rationale for their use."}, {"title": "3.1 PROGRAMMATIC INSTRUCTION FOLLOWING (PIF) METRIC", "content": "At each question turn, in a chat, either an instruction is added or not, with up to six instructions in each chat. Please refer to the Introduction 1 for details of how the instructions were added. Given model input context X, and model response Y, we can define the PIF metric for that response to be\n$$PIF(X, Y) = \\frac{\\text{# Instructions given in input context X that are followed in response Y}}{\\text{# Instructions given in input context X}}$$\nNote that the PIF considers whether the response follows all given instructions in previous turns, not just the instruction given at the current turn. The PIF metric does not take into account if the question was answered correctly, but rather, it focuses on if the instructions given to constrain the answer were followed.\nGiven a dataset {Xi, Yi}21 we overload the notation and define the corpus level (mean) PIF score as\n$$PIF(\\lbrace X_i, Y_i \\rbrace_{i=1}^M ) = \\frac{1}{M} \\sum_{i=1}^M PIF(X_i, Y_i)$$\nFor our evaluation set, we have M chats, and chat m\u2208 {1, ..., M} have Nm turns. This gives us our evaluation set: D = {(Xi,j, Yi,j)}=1=1, where Xi,j is the input model context for chat i at turn j, and Yi,j is the model response for chat i at turn j. The corpus level Programmatic Instruction Following Score conditioned on turn j, is given by\n$$PIF(D|turn = j) = PIF(\\lbrace (X_{i,j}, Y_{i,j}) \\rbrace_{i=1}^M )$$\nwhere chats with less than j turns are excluded. It will be clear from the context whether we refer to the corpus or sample PIF metric.\nThe PIF metric captures the following aspects:\n1. The ability for a model to retrieve several pieces of information from different parts of an input text context and reason over them\n2. The ability for the model to follow objective instructions\nOf these, we think the most important is the first, as this is a very common scenario for real use-cases, and it's a feature that single-turn based metrics are not capturing as well."}, {"title": "3.2 CONSISTENCY METRICS", "content": "In addition to having a high average score, we want models to consistently produce the same high quality results. We develop a metric to capture this intuition.\nWe propose a metric where for each turn N responses are sampled, and PIF-N-K will then denote the fraction of samples where at least K samples have PIF score 1.\nThus the sample level PIF-N-K, for input model context X, and sampled responses Y\u2081, . . . YN is\n$$PIF-N-K(X, Y_1, ..., Y_N) = \\begin{cases} 1, & \\text{if } \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}(PIF(X,Y_i)=1) \\geq K \\\\ 0, & \\text{otherwise} \\end{cases}$$\nThe intuition is that we want to measure how consistently the models can follow all the instructions correctly. We overload notation and define the corpus level (mean) PIF-N-K for a dataset with M turns, D = {Xi, Y1,i,..., YN,i}=1 as\n$$PIF-N-K(D) = \\frac{\\sum_{i=1}^M PIF-N-K(X, Y_{1,i},..., Y_{N,i})}{M}$$\nWith this definition it holds that, for any dataset D,\n$$PIF-N-i(D) < PIF-N-j(D)$$\nwhen i > j."}, {"title": "4 EVALUATED MODELS", "content": "This section describes the models evaluated, and provides an analysis of the answer lengths of the models."}, {"title": "4.1 MODEL ENDPOINTS", "content": "We access Gemini 1.5 Pro (abbreviated as Gemini) through the Vertex AI API, using the following model version: 'Gemini-1.5-pro-preview-0514'. We access Claude 3.5 Sonnet (abbreviated as Sonnet) through the Anthropic Vertex API, with the model version 'claude-3-5-sonnet@20240620'. We access GPT-40 from the OpenAI API with the model version \u2018gpt-4o-2024-05-13'. The hyperparameters for all models are the default settings. The default temperature for all models in 1. The safety filters for all models are the default settings. We don't see questions that are marked as unsafe with the default setting for the models."}, {"title": "4.2 CONTEXT LENGTHS", "content": ""}, {"title": "5 EVALUATION RESULTS", "content": "The section describes the results from the evaluation experiments, starting with results for the PIF metric, then considering similarities with the needle in a haystack experiment, results for the PIF-N-K metric, before finally considering human and LLM-as-a-judge evaluation results."}, {"title": "5.1 PIF METRIC", "content": "Figure 4 shows the PIF conditional on question turn. We note that, as expected, the PIF metric decreases with the question turn. The 95% confidence bounds for the PIF metric are done on a per-turn basis, using a Bernoulli confidence interval approximation. This gives conservative confidence bounds as the Bernoulli distribution is the distribution that for a given mean maximizes the variance among all distributions on [0,1].\nFigure 6 shows the empirical cumulative distribution function for the PIF metric. The interpretation of the left graph in Figure 6 is that at turn 2, the programmatic instruction following score can be 0, 0.5, or 1. For Gemini 1.5 Pro, it's 0 with probabilty 18%, while for GPT-40 it's zero with probability around 10%. The probability that the programmatic instruction following score is less than 1 (i.e 0.5 or 0) is around 35% for GPT-40, 52% for Gemini and 10% for Sonnet."}, {"title": "5.2 NEEDLES IN A HAYSTACK?", "content": "This experimental setup has several similarities and differences with a needle in a haystack experiment. In our proposed set up, the complex reasoning across the needles (given instructions) is important, in addition to the retrieval of the needles. To understand the impact of where in the input model context the instructions are located, we run the following ablation: In addition to having in-"}, {"title": "5.3 PIF-N-K METRIC", "content": "We now consider the results for the PIF-N-K, measuring the robustness for following all given instructions correctly. In our experiments we set N = 4. Figure 7 shows the results. As expected PIF-4-4, meaning the fraction of turns where all N = 4 sampled turn answer candidates got all the instructions correct is quite low, for both Gemini and GPT-40 it's 11%, highlighting that this is a very challenging metric with significant headroom for model improvement. However, note that also for Sonnet 3.5, the model with the strongest performance, the metric rapidly becomes more challenging as we move from PIF-4-1 to PIF-4-4. This points to a significant robustness issue with the models we have studied in this work, as if the model always had the same percentage of instructions followed in its responses, we would not see a decrease in the PIF-N-K metric."}, {"title": "5.4 HUMAN EVALUATION", "content": "As described in Section 2.2 we collect human evaluations of instruction following, chat accuracy and pairwise preferences. In Figure 8 with human evaluations, we observe that Gemini underperforms GPT-40 and Sonnet, and Sonnet and GPT-40 are broadly similar. We also find that the correlation between the human instruction rating and the PIF metric, the results are shown in Table 4."}, {"title": "5.4.1 HoW HARD IS THE TASK FOR HUMAN RATERS?", "content": "Starting with a reference answer from the original MMDU dataset, human raters were instructed to rewrite the responses to both be correct and to follow all the given instructions. The human raters had access to the LLM model responses, the original reference answer for the MMDU dataset, as well as a list of all instructions given in the chat, so they did not have to look at the chat history to find the instructions. The raters reported that it took on average 10 minutes to write the answer and reported that the hardest instructions to satisfy where the constrains on the sentence start word and the constrains on the sentence lengths. The programmatic instruction following scores for the human raters have an average of 0.94, significantly higher than both Gemini and GPT-40 with all instructions in the end of the input context, but actually lower than Sonnet 3.5 in the setting with all instructions added at the end of the input model context, at a mean PIF score of 0.97. This highlights that while the task is challenging, the human raters are able to complete it with great proficiency, indicating that there is headroom for models to improve. The raters reported that having access to the model answers helped speed up the rewriting process by giving inspiration to ways to follows the given constrains. The raters also noted that sometimes the original reference answers from the MMDU dataset were incorrect and had to be adjusted in addition to ensuring that all instructions were followed."}, {"title": "5.5 AUTORATER", "content": ""}, {"title": "5.5.1 AUTORATER METRICS", "content": "We use an LLM based autorater to rate the chats on 8 metrics: Creativity, Richness, Visual Perception, Logical Coherence, Answer Accuracy, Image Relationship Understanding, Instruction Following and Overall Quality, each on a scale from 1 to 10. The autorater is given the chat history, model response, and input and outputs a dictionary with the score for each attribute. We run experiments"}, {"title": "5.5.2 PAIRWISE COMPARISONS", "content": "For each of the 8 metrics, a score in the range 1-10 is given by the auto-rater model. For the programmatic instruction following a score in the range 0 - 1 is given. We create a final response score by taking a weighted average of all the autorater scores and the programmatic instruction following score, where the programmatic instruction following score has a weight of 20, as it's scored on a more narrow range, and it's the metric that is most objective. Using this weighted average, we can compare the responses for Gemini 1.5 Pro, GPT-40 and Sonnet 3.5 for each chat turn.\nFrom the preference scores in Figure 9 we see that Gemini significantly lags behind both Sonnet and GPT-40 on the task, and in particular against Sonnet, the win rate is only around 28%.\nAutorater bias From Table 5 we make an interesting, but not surprising observation: With GPT-4o as judge, GPT-40 performs better, and with Gemini as a judge, Gemini has a better performance. The Gemini based autorater gets the relative order of the instruction following correct (as measured by the programmatic instruction following metric) whereas the GPT-40 judge ranks the GPT-40 as having better instruction following than Sonnet. In addition, we see that the human rater scores are in general more conservative. For the accuracy ratings, the GPT-40 judge has the same relative ranking as the human raters, which the Gemini judge does not."}, {"title": "6 CONCLUSION", "content": "In this work we proposed the MMMT-IF instruction following evaluation set for multi-modal, multi-turn dialogue, along with several challenging metrics for the current LFMs. All the metrics are objective and verifiable by code execution, ensuring an unbiased evaluation. Our analysis shows that the main difficulty of the task lies not within the instruction following, but rather to retrieve the instructions from different parts of the input context and then reason over them. We find that Gemini 1.5 Pro consistently, and to a lesser extent also GPT-40 and Claude 3.5 Sonnet, have a significant PIF metric degradation on the turns in the evaluation set with many instructions. We also find that all examined models have low PIF-N-K scores, indicating that they fail to robustly follow all given instructions correctly. We hope that the PIF and the PIF-N-K metric can serve more broadly for practitioners wishing to create other evaluation benchmarks for multimodal, multi-turn instruction following.\nPossible directions for future work include creating training datasets for Reinforcement Learning from Execution Feedback (RLEF) with the PIF and PIF-N-K as reward signals. Another possible extension include creating dependent instructions, such as having instructions that modify previously given instructions. This will make the task further more challenging."}, {"title": "D ADDITIONAL METRICS", "content": ""}, {"title": "D.1 PIF-IQR-N METRIC", "content": "Here we define an additional metric for robustness that focus on overall robustness rather than robustness for correctly following all instructions as in PIF-N-K. For a given input model context X, we sample responses Y\u2081, . . ., Yn, to get PIF scores PIF(X, Y\u2081), . . ., PIF(X, Y\u2163). Now we define PIF-IQR-N as\n$$PIF-IQR-N(X, Y_1, ..., Y_N) = IQR(\\lbrace PIF(X, Y_1), ..., PIF(X, Y_N) \\rbrace )$$\nwhere IQR denotes the interquartile range. This measure has the property to in expectation to be independent of N."}, {"title": "D.1.1 ELO COMPUTATION", "content": "Another way to use the pairwise preference ratings from the weighted sum of the autorater and the PIF metric is to compute the ELO score between the three models. We also compute an ELO score for each of the models, where the ELO is initialized at 1000. We follow the procedure in (Chiang et al., 2024).\nWe get that Gemini has ELO 950, GPT-4o has an ELO of 994, and Sonnet has an ELO of 1055. This ELO ranking suggests a win probability of 35% between Gemini and Sonnet, 44% between Gemini and GPT-40 and 59% between Sonnet and GPT-40, which is broadly aligned with the observed win-rates, although Sonnet has a higher winrate compared to the human preference ratings."}, {"title": "E EXAMPLE CHAT", "content": ""}, {"title": "F ERROR ANALYSIS", "content": "In this section we explore a few chats and the errors made. While the average performance of the PIF metric for human raters was at 0.94, the lowest observed PIF score for a chat turn was 0.4, the chat turn is shown in Table 8. Note that the word 'like' is not in the response, and that sentence 7 is both longer than 18 words and start with the letter T, hence the PIF score of 0.4. Notice that the answer looks broadly correct and it requires a careful view to spot the errors.\nGemini repeats the same answer One error pattern noticed for Gemini 1.5 Pro was that it's repeating the same answer to multiple questions, see row 1-5 in Table 9, which are consecutive turns in a conversation. In row 5, Gemini follows 0 out of the 4 given instructions."}, {"title": "G HUMAN ANNOTATOR INSTRUCTIONS", "content": "In this section the instructions given to human annotators are presented. There were 8 human annotators working on the MMMT-IF dataset. The work was split so that all turns in a chat were rated and rewritten by the same person. Below are the full set of instructions given to human annotators:\nRequirement For each sample (total 71) based on the MMDU benchmark (MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs), several images (up to 5) are shown and multiple questions and instructions (average 14 questions and 6 instructions be sample) related to the images are asked based on this. Answers from several models are provided for each question, following the instructions and answering the question. In total there"}, {"title": "A LITTERATURE REVIEW", "content": ""}, {"title": "A.1 LONG CONTEXT RETRIEVAL", "content": "There have been several works focusing on the effect of long input context on model performance on downstream tasks, including (Liu et al., 2023), (Levy et al., 2024), and (An et al., 2023). Similar to the Lost-in-the-middle paper Liu et al. (2023), our paper examines the effect of where in the input context information is located. The results in (Levy et al., 2024) are also complementary, as both observe performance degradation with input context length increases. Our evaluation set can also be viewed as a task similar to multiple needles in the haystack (a task where several tokens needs to be retrieved from a long input context), where each needle is an instruction that the model needs to reason over."}, {"title": "A.2 INSTRUCTION FOLLOWING EVALUATION", "content": "There a many instruction following evaluation benchmarks, several are, like our work, focusing on instructions related to answer constrains on a Q&A task, (Xia et al., 2024), (Zhou et al., 2023a), (Zhang et al., 2024), (Tam et al., 2024) and (Sun et al., 2024a). Compared with these works, we focus on multiple instructions spread out over a long context, testing not only instruction following but also retrieval and complex reasoning from the input context. There have been a long range of other instruction following evaluation sets (Chen et al., 2024c), (Zhou et al., 2023b), (Adlakha et al., 2024), (Sun et al., 2024b), (Yan et al., 2024), (Jiang et al., 2024), (Chia et al., 2023), (Skopek et al., 2023), and (Qin et al., 2024), but their focus in not on multiple instructions spread out in the input context for multi-modal multi-turn chat."}, {"title": "A.3 PROGRAMMATIC INSTRUCTION FOLLOWING", "content": "There have been several previous papers that use program execution to determine instruction following capability, for code (Yang et al., 2023), Data science (Huang et al., 2022), and text (Dong et al., 2024). Our work is most related to (Dong et al., 2024), but we fix a set of instructions, and instead of a single instruction use case, we focus on multiple instructions, over multiple turns of multi-modal question answering."}, {"title": "A.4 MULTI-MODAL EVALUATION DATASETS", "content": "There have been several benchmarks suggested for multi-modal models, for the multi-turn chat use case, we have (Liu et al., 2024c) and (Liu et al., 2024a). However, while the datasets are multi-turn, the chat turns can be independently answered, thus making it less relevant for long context models. By introducing given at several locations throughout the chat, we introduce long range dependencies in the data needed to answer questions. Our evaluation set is an extension of the MMDU dataset (Liu et al., 2024c). Other work for evaluating multi-modal models include (Yue et al., 2024), (Liu et al., 2024b), (Srinivasan et al., 2021), (Yu et al., 2023), (Xu et al., 2023), (Chen et al., 2024b) and (Wang et al., 2024). None of these focus on multi-turn instruction following."}, {"title": "A.5 LLM JUDGES", "content": "There have been several previous works on using LLMs to judge quality of other LLM responses, including (Dubois et al., 2024b), (Zheng et al., 2023), (Chen et al., 2024a), (Dubois et al., 2024a), (Zeng et al., 2024), and (Liu et al., 2024c). Compared with these works, our LLM judge approach is different in that we combine an LLM evaluation with a programmatic evaluation through a weighed sum to create final model evaluations."}, {"title": "B ADDITIONAL EXPERIMENTS", "content": ""}, {"title": "B.1 PERFORMANCE ON SPECIFIC INSTRUCTIONS", "content": "In Figure 10 the PIF score conditional on an instruction having been given is shown. We note that Gemini 1.5 Pro has a hard time following an instruction to end sentences with a question mark, and GPT-40 has some issues with following instructions related to outputting even or odd numbers in its responses. The definition of the categories are presented in Table 6 in the Appendix."}, {"title": "B.2 ANALYSIS OF DATASET QUESTIONS", "content": "In Figure 12 we display the model capabilities targeted in each question turn, where the classification is done by GPT-40. We manually reviewed the classifications to ensure they were aligned with human categorizations."}, {"title": "B.3 PIF METRIC AND HUMAN ACCURACY SCORES", "content": "While the PIF score is an important metric for instruction following, it's also important to answer the image based questions in the MMMT-IF dataset correctly, not only following the answer constraints. Figure 14 shows a scatter plot with PIF score on the y axis and human accuracy score for each"}]}