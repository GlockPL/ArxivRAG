{"title": "Phoneme-Level Feature Discrepancies: A Key to Detecting Sophisticated Speech Deepfakes", "authors": ["Kuiyuan Zhang", "Zhongyun Hua", "Rushi Lan", "Yushu Zhang", "Yifang Guo"], "abstract": "Recent advancements in text-to-speech and speech conver-sion technologies have enabled the creation of highly con-vincing synthetic speech. While these innovations offer nu-merous practical benefits, they also cause significant securitychallenges when maliciously misused. Therefore, there is anurgent need to detect these synthetic speech signals. Phonemefeatures provide a powerful speech representation for deep-fake detection. However, previous phoneme-based detectionapproaches typically focused on specific phonemes, over-looking temporal inconsistencies across the entire phonemese quence. In this paper, we develop a new mechanism fordetecting speech deepfakes by identifying the inconsisten-cies of phoneme-level speech features. We design an adap-tive phoneme pooling technique that extracts sample-specificphoneme-level features from frame-level speech data. Byapplying this technique to features extracted by pre-trainedaudio models on previously unseen deepfake datasets, wedemonstrate that deepfake samples often exhibit phoneme-level inconsistencies when compared to genuine speech. Tofurther enhance detection accuracy, we propose a deepfake detector that uses a graph attention network to model the tem-poral dependencies of phoneme-level features. Additionally,we introduce a random phoneme substitution augmentationtechnique to increase feature diversity during training. Exten-sive experiments on four benchmark datasets demonstrate thesuperior performance of our method over existing state-of-the-art detection methods.", "sections": [{"title": "Introduction", "content": "In today's digital age, advanced machine-learning models have made it increasingly easy to manipulate digital con-tent, raising concerns about the reliability of speech record-ings (M\u00fcller et al. 2022). Speech deepfakes are syntheticrecordings that closely mimic a person's speech, making itchallenging to verify the authenticity of information (Tanet al. 2024). Advancements in deep learning technologiesfor generating realistic speech have made it increasinglydifficult to detect these forgeries with conventional meth-ods (Zhang et al. 2024).\nDeep-learning audio synthesizers typically employ neuralnetworks to replicate the vocal process, often using encoder-"}, {"title": "Related Work", "content": "Speech Synthesis can be produced through Text-to-speech (TTS) (Casanova et al. 2022) or voice conversion (VC) (Qi et al. 2024) technologies. TTS methods convert text into speech, while VC methods modify existing speech to changeits style. Despite using different inputs, both approachesshare a similar encoder-decoder framework.\nThe encoder processes the input text or speech into em-beddings that capture the unique characteristics of the in-puts. The decoder then takes these embeddings as input andoutputs corresponding speech. In many existing TTS andVC methods (Oord et al. 2016; Guan et al. 2024), the de-coder is further divided into a Mel spectrogram generatorand a vocoder. The Mel spectrogram generator produces aMel spectrogram from the embedding, while the vocoderconverts the Mel spectrogram into a synthesized audio wave-form. In contrast, those methods (Tan et al. 2024; Kim,Kong, and Son 2021) based on conditional variational au-toencoder can directly synthesize the waveform from theembeddings.\nThe rapid development of speech synthesis tools poses in-creasing challenges to information security.\nPhoneme-based Deepfake Speech Detection\nThe authors in the work (Dhamyal et al. 2021) employed a2D self-attention model to distinguish spoofed and bonafidespeech signals using transformed spectrogram features. Byanalyzing the attention weights, they discovered that the de-tection model primarily focuses on only six phonemes in theASVSpoof2019 Logical Access (LA) Dataset, and the de-tection performance remains effective even when only thetop 16 most-attended phonemes are used as input. Similarly,the work (Blue et al. 2022) decomposed speech into pairsof phonemes (bigrams) and applied fluid dynamics to esti-mate the arrangement of the human vocal tract from thesebigrams. The whole-sample detection was then conductedby comparing the distribution of bigram features to identifybonafide samples. The authors found that using only 15.3%phoneme bigrams was sufficient to achieve over 99% accu-racy in their constructed dataset.\nWhile these methods demonstrate the effectiveness of us-ing phonemes for detection, they have certain limitations.They require precise recognition and timestamp labeling of"}, {"title": "Phoneme-level Feature Analysis", "content": "We first pre-train a phoneme recognition model to recognize phonemes for audio frames and then utilize this pre-trained model to generate phoneme-level features for visualization. Note that We only focus on mono audio in this paper. We assume that the input audio is with the shape of T \u00d7 1, where T denotes the audio length and the sampling rate is 16k HZ.\nPretraining Phoneme Recognition Model\nGenerating phoneme-level features requires phoneme labels for each audio frame. In realistic scenarios, phoneme anno-tations and timestamps are rarely available in audio data, es-pecially for deepfake datasets. Moreover, Deepfake speech is widely available in different language domains. Considering these facts, we train a multilingual phoneme recognitionmodel to recognize phonemes.\nModel Architecture In this work, we use a pre-trained audio model, e.g., Wav2Vec2.0 (Baevski et al. 2020) and WavLM (Chen et al. 2022), as the backbone, which is specif-ically trained on large-scale audios (English) and can be finetuned on various downstream tasks. As shown in Fig. 3, the pretraining audio model consists of a feature extractor, a feature projector and a Transformer encoder.\nThe feature extractor and projector employ a 1D CNN to initially extract audio features Finit with a shape of T' \u00d7 h, where T' is the number of audio frames. The Transformer encoder takes Finit as input and uses self-attention lay-ers to capture the dependencies and correlations in audio frames. We denote the output of the Transformer encoder as the frame-level feature and employ a prediction head for phoneme classification.\nTraining We adopt the multi-language Common Voice 6.1 corpus to train our phoneme recognition model. Specifi-cally, we select approximately 375k speech samples in 9 lan-guages: English (EN), German (DE), Spanish (ES), French (FR), Polish (PL), Russian (RU), Ukrainian (UK), and Chi-nese (ZH). Due to computational equipment and time con-straints, our final trained multilingual phoneme recognition"}, {"title": "Deepfake Speech Detector", "content": "Fig.4 illustrates the overview model architecture of our detection model, which consists of a frozen pre-trained phoneme recognition model, a copied Transformer encoder, and a GAT module (Veli\u010dkovi\u0107 et al. 2017). Given the in-put audio sample, the frozen pre-trained phoneme recog-nition model first uses a feature extractor and projector to extract the initial speech feature Finit, following utilizes a Transformer encoder to learn frame-level speech feature Ff, and finally employs a phoneme classification head to predict phonemes. Note that all the parameters in the pre-trained phoneme recognition model are set to be untrainable. To detect the deepfake label, we copy and finetune the Trans-former encoder to learn frame-level speech feature F\u02b9 from Finit. Based on the predicted phonemes, we apply average phoneme pooling to F' to obtain the phoneme-level feature Fp. We then employ predicted phonemes to generate edges and utilize the GAT to learn temporal dependency. Finally, we append a classification head for deepfake classification.\nGraph Attention Module\nWe employ the GAT module to capture the temporal depen-dencies of phoneme-level features $F_p \\in \\mathbb{R}^{T'\\times C}$. To do so, we construct edges between consecutive phonemes to modelthe transition between phonemes. Concretely, assuming $F_p$composes T' phoneme vectors ${f_1, f_2, ..., f_{T'} }, f_i \\in \\mathbb{R}^{C}$,we build maximum N edges for each phoneme with itsneighborhood phonemes behind: for the i-th phonemewhere i < T' \u2013 1, we add N \u2013 1 edges {i \u2192 min(i +1,T'), i \u2192 min(i + 2,T'),\u2026\u2026 ,i \u2192 min(i + N,T')}.\nFor the T' phoneme vectors in Fp, a graph attention layer(GAL) computes the importance between phoneme vectorsusing the attention mechanism. The attention coefficients aijbetween phoneme i and each of its neighboring phonemesj\u2208 Ni, i.e., the importance of f; to fi, is calculated as:\n$\\alpha_{ij} = \\frac{\\exp \\left(\\text { LeakyReLU }\\left(a^{\\top} \\left[W f_{i} \\|\\| W f_{j}\\right]\\right)\\right)}{\\sum_{k \\in N_{i}} \\exp \\left(\\text { LeakyReLU }\\left(a^{\\top} \\left[W f_{i} \\|\\| W f_{k}\\right]\\right)\\right)}$"}, {"title": "Random Phoneme Substitution Augmentation", "content": "We propose the RPSA technique to improve the feature di-versity during training. Specifically, for the extracted fea-ture Finit of i-th sample in the input batch, we randomlysubstitute phonemes in Finit with the same phonemes ofother samples. For instance, if the k-th phoneme in Finitspans nk frames, it could be replaced by the same phonemewith a possible different number of frames from F init. Eachphoneme in the sample has a probability of p to be sub-stituted. After obtaining the substituted samples, we collectthem into a new batch and feed them into the copied Trans-former encoder and GAT to obtain classification results \u0177'.Note that the label for all the samples in this augmentedbatch is fake."}, {"title": "Loss Function", "content": "We employ the contrastive language-image pre-training(CLIP) (Wu et al. 2022) loss to increase the semantic simi-larity between frame-level features $F_f$ and $F'_f$. This enablesthe deepfake detector to also focus on the phoneme pre-diction task, forming a multi-task learning schedule. Con-cretely, the CLIP loss is defined as follows:\n$L_{\\text {CLIP }}=\\frac{1}{N} \\sum_{i=1}^{N} \\log \\left(\\frac{\\exp \\left(s\\left(g\\left(F^{\\prime} f^{i}\\right), F_f^{i}\\right) / T\\right)}{\\sum_{k=1}^{N} \\exp \\left(s\\left(g\\left(F^{\\prime} f^{i}\\right), F_f^{k}\\right) / T\\right)}\\right)$", "content_latex": ["L_{\\text {CLIP }}=\\frac{1}{N} \\sum_{i=1}^{N} \\log \\left(\\frac{\\exp \\left(s\\left(g\\left(F^{\\prime} f^{i}\\right), F_f^{i}\\right) / T\\right)}{\\sum_{k=1}^{N} \\exp \\left(s\\left(g\\left(F^{\\prime} f^{i}\\right), F_f^{k}\\right) / T\\right)}\\right)"]}, {"title": "Experiment Setting", "content": "Implementaion Details\nWe utilize the WavLM as the backbone of our phoneme recognition model. The number of edges in GAT is setto 10. The substitution probability p in RPSA is set to0.2. We train our detection model using the AdamW opti-mizer (Loshchilov and Hutter 2019), where the learning rateof the copied Transformer is set to 5e-5 and that of otherlearnable parameters is set to le-4.\nWe introduce two data augmentation strategies and anearly-stopping technique for every detection approach.Specifically, the data augmentation involves adding randomGaussian noise and applying random pitch adjustments tothe audio samples. The early-stopping technique will ter-minate the training of models if there's no improvement inthe area under the Receiver Operating Characteristic (ROC)Curve (AUC) score after three training epochs. All tests werecarried out on a computer equipped with a GTX 4090 GPU,using the PyTorch programming framework."}, {"title": "Conclusion", "content": "Current deepfake detection methods are increasingly chal-lenged by the rapid advancements in deepfake audio gen-eration. In response, our work introduces a novel approachto deepfake speech detection by focusing on inconsisten-cies in phoneme-level speech features. We begin by usingvisualization tools to demonstrate the effectiveness of thephoneme-level feature and subsequently design a deepfakedetector based on it. Specifically, by employing adaptivephoneme pooling and a GAT, we effectively capture andanalyze phoneme-level features to identify deepfake sam-ples. Additionally, our proposed RPSA technique enhancesfeature diversity in training. Experimental results demon-strate that our method consistently outperforms state-of-the-art baselines across multiple deepfake speech datasets."}]}