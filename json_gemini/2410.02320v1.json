{"title": "Post-edits Are Preferences Too", "authors": ["Nathaniel Berger", "Stefan Riezler", "Miriam Exel", "Matthias Huck"], "abstract": "Preference Optimization (PO) techniques are currently one of the state of the art techniques for fine-tuning large language models (LLMs) on pairwise preference feedback from human annotators. However, in machine translation, this sort of feedback can be difficult to solicit. Additionally, Kreutzer et al. (2018) have shown that, for machine translation, pairwise preferences are less reliable than other forms of human feedback, such as 5-point ratings.\nWe examine post-edits to see if they can be a source of reliable human preferences by construction. In PO, a human annotator is shown sequences 81 and 82 and asked for a preference judgment, while for post-editing, editors create 81 and know that it should be better than 82. We attempt to use these implicit preferences for PO and show that it helps the model move towards post-edit-like hypotheses and away from machine translation-like hypotheses. Furthermore, we show that best results are obtained by pre-training the model with supervised fine-tuning (SFT) on post-edits in order to promote post-edit-like hypotheses to the top output ranks.", "sections": [{"title": "1 Introduction", "content": "The current state of the art methods for training large language models offline on human preference data are Direct Preference Optimization (DPO) (Rafailov et al., 2023) or Identity Preference Optimization (IPO) (Gheshlaghi Azar et al., 2024). Instead of training a separate reward model and then performing reinforcement learning, these methods train directly on the collected preference data by deriving a directly optimizable loss function from the preference model.\nHowever, in some domains, the pairwise preference annotations required for using these methods have been found to be less reliable than other annotation schemes. Kreutzer et al. (2018) find that inter-rater reliability for pairwise ranking of machine translation outputs to be less than that of 5-point rating. In the field of translation, there are many different dimensions on which one translation may be better than another, e.g. fluency, faithfulness, formality, terminology, etc. (Lommel et al., 2013). This poses a problem for human annotators when they are presented with two plausible translations.\nWe propose using the data generated by post-editing to yield reliable preferences by construction. The current generative process for preference data is that two sequences 81 and 82 are given, and a preference judgment 81 > 82 is sought, yielding the generative process 81 \u2192 81 > 82 \u2190 $2. We propose using data generated by the following process: Given 82 and the implicit preference that 81 > 82, create s1, yielding the generative process $1 > $2 \u2192 $1 \u2190 82 (see Fig. 1).\nPost-editing is already a common practice in the translation community to clean up raw-MT outputs before publishing. Post-editors create new sequences that they prefer with regards to the quality expected in their domain. Typically, the original raw-MT output is discarded and the post-edit is published. If this data is used for training, the post-edit is treated as a new reference for supervised fine-tuning (SFT). This ignores, however, the fact that the post-edit is not just a new reference translation but also a quality judgment of what in the raw-MT was erroneous. Using PO objectives allows us to fine-tune an LLM to translate in a way that is more in line with the post-editors' implicit preferences. However, PO does not necessarily promote the preferred sequence to become the argmax output of the model, but rather re-ranks sequences within the model's probability space. If the two sequences are both unlikely under the model's output distribution, they will remain unlikely but their relative probability will respect the preferences. We show that best results are obtained by pre-training the model on post-edits with SFT, promoting post-edits to the top ranks, followed by fine-tuning with a PO loss. This combined training teaches the model to prefer and promote post-edits such that reference-like translations are produced but also dispreferred machine-translation-like hypotheses are avoided."}, {"title": "2 Related Work", "content": "Kreutzer et al. (2018) gather human feedback on machine translation outputs in the form of 5-point ratings and as pairwise preferences. They then use this feedback to train two reward models, one that is trained on the 5-point ratings and is trained with a regression loss to directly predict a reward value and one that is trained on pairwise preferences by fitting a Bradley-Terry model (Bradley and Terry, 1952) to the preferences as had been done by Christiano et al. (2017). These reward models are then used to train machine translation models. Kreutzer et al. (2018) find that ratings are more reliable than rankings and that reinforcement learning with a ratings-trained reward estimator yields better results than using rankings-trained reward estimates.\nBerger et al. (2023) fine-tune a pre-trained NMT model on post-editing data by presenting the model with both the post-edit and the current MT hypothesis. At each epoch, the NMT model being trained generates translations for all training data. These generated outputs are then compared to the original post-edits with a token-level diff. Both sequences are then used as training examples for the NMT system. However, tokens that appear in the hypothesis but not the post-edit are given a negative weight in the loss function. On examples where the two sequences differ, the model gets both negative feedback, where the probability of that token is to be decreased, and positive feedback, where the probability should be increased.\nXu et al. (2024b) similarly present the model with a positive and negative example of machine translation outputs during training but use a modified version of the DPO (Rafailov et al., 2023) loss to optimize it. Their change to DPO adds an SFT term. The SFT term promotes the preferred sequence to be the argmax output of the model while the DPO part of the loss establishes the distance between the two sequences in log-probability space. The MT hypotheses that they generate come from two different LLMs; ALMA-13B-LORA (Xu et al., 2024a) and GPT-4 (OpenAI et al., 2024). Additionally, they use reference translations from the original dataset. The preferences that they use are predicted by open-source quality estimation models KIWI-XXL (Rei et al., 2023) and XCOMET (Guerreiro et al., 2023)."}, {"title": "3 Preference Optimization", "content": "Using reinforcement learning with human feedback (RLHF) has recently re-emerged as a method for training LLMs to generate outputs that are preferred by human annotators (Ziegler et al. (2019), Ouyang et al. (2022), Bai et al. (2022), inter alia) without requiring handwritten demonstrations of preferred behavior which would be required for supervised fine-tuning (SFT). The general recipe is as follows: pre-train an LLM on in-domain data; generate multiple completions y for a single input x (or prompt); have human annotators rank or rate the completions; train a reward model to predict rankings or ratings given inputs and completions; use the trained reward model to predict rewards for reinforcement learning, frequently with proximal policy optimization (Schulman et al., 2017). Training a separate model to predict rewards for reinforcement learning is known as an actor-critic method.\nThe reward model in the previous works is structured as a Bradley-Terry model (Bradley and Terry, 1952), where the probability of preferring y\u2081 over Y2 is given by\n$P(Y_1 > Y_2|x) = \\sigma(r_\\theta(x,y_1) - r_\\theta(x,y_2))$\nwhere \u03c3 is the logistic function and $r_\\theta$ is the reward model that is trained on the pairwise rankings to give the preferred sequence a higher value. The reward model can then be used to estimate rewards for outputs sampled during online training.\nThis process requires training an additional model and hiring human annotators to perform ranking. DPO (Rafailov et al., 2023) is a technique that obviates the need for a secondary model by instead giving the model both the preferred and dispreferred sequences and optimizing a distance between the two sequence in log-probability space. If handwritten demonstrations of preferred sequences are available, then SFT would typically be performed. The goal of SFT is to maximize the probability of the demonstrations under the model. For text generation, this is done by minimizing the negative log-probability of each token given all previous tokens in the sequence.\n$L_{SFT}(y) = -\\sum_{i=0}^{|y|} log(\\pi(y_i | y_{0:i-1}))$\nMinimizing this loss promotes the sequence y to be the argmax output of the model, while reinforcement learning increases or decreases the probability of a sequence with regard to the magnitude of its reward."}, {"title": "3.2 PO Objectives", "content": "The DPO loss (Rafailov et al., 2023) is based on the Bradley-Terry model of human preferences but, unlike actor-critic reinforcement learning techniques, it does not train a separate reward model. Instead they rewrite the reward function r in terms of the optimal policy and the baseline model. They notice that the theoretically optimal policy \u03c0r, with a KL-divergence constraint, is equal to the baseline model with its output distribution re-weighted according to the reward function\n$\\pi_r(y|x) = \\frac{1}{Z(x)} \\pi_{ref}(y|x) exp(\\frac{1}{\\beta} r(x,y))$\nwhere Z is the partition function, which normalizes the function to be a proper probability distribution. This formula can also be solved for the reward function r, such that rewards are expressed as the difference between two models' probability ratios. If this r is then inserted back into the Bradley-Terry model, it becomes\n$P(Y_\\omega > Y_\\iota | x) = \\frac{\\pi^*(Y_\\omega|x)}{\\pi^*(Y_\\omega|x) + \\pi^*(Y_\\iota|x)} = \\sigma(\\beta log(\\frac{\\pi^*(Y_\\omega|x)}{\\pi_{ref}(Y_\\omega|x)}) - \\beta log(\\frac{\\pi^*(Y_\\iota|x)}{\\pi_{ref}(Y_\\iota|x)}))$\nwhere yw and y\u0131 denote the preferred and dispreferred completion, respectively, and \u03c0* is now the model we are training to be optimal under the reward function. This probability, $p(Y_\\omega > Y_\\iota|x)$ can be optimized by minimizing the negative log-probability. With regards to output probability, the loss monotonically decreases as yw becomes more probable than y\u0131. Increasing the difference between the two always decreases the loss.\nGheshlaghi Azar et al. (2024) re-derive a similar loss with some theoretical advantages. Instead of training the model to be optimal under the Bradley-Terry derived reward function, they train the model to separate the two outputs by a fixed difference in log-probability space.\n$L_{IPO}(Y_\\omega, Y_\\iota, x) = \\frac{1}{2\\beta}((log(\\frac{\\pi^*(Y_\\omega|x)}{\\pi_{ref}(Y_\\omega|x)}) - log(\\frac{\\pi^*(Y_\\iota|x)}{\\pi_{ref}(Y_\\iota|x)})) - (2\\beta)^{-1})^2$\nBecause this loss function is minimized when the log-probability ratio difference is exactly $(2\\beta)^{-1}$, and will increase when the outputs move further apart in log-probability space, the authors claim an advantage for deterministic preferences, where the same preferences are seen multiple times during training. Because the preferences that we use are deterministic, we opt for the IPO paradigm of PO.\nA follow-up work to DPO, focused specifically on machine translation, additively combines the DPO loss and the SFT loss (Xu et al., 2024b), which the authors call Contrastive Preference Optimization (CPO). Additionally, they perform an ad-hoc modification of the DPO loss by dropping the normalizer $\u03c0_{ref}$ in the DPO loss so as to not perform a second forward pass on the reference model.\n$L_{CPO}(Y_\\omega, Y_\\iota, x) = -log(\\sigma(\\beta log(\\pi^*(Y_\\omega|x)) - \\beta log(\\pi^*(Y_\\iota|x))))$\nWe use a reformulation of the CPO loss with the IPO training objective for our experiments because our preferences are deterministic. Additionally, we keep the normalizers in the IPO loss because these can be pre-computed in advance, instead of in a second forward pass, and incur only a negligible memory and speed penalty.\nOur modified variant of the CPO loss, which we call dDPO for the deterministic preferences involved in post-editing, is\n$L_{dCPO}(Y_\\omega, Y_\\iota, x) = -log(\\pi^*(Y_\\omega|x)) + log(\\sigma(\\beta(\\frac{\\pi^*(Y_\\omega|x)}{\\pi_{ref}(Y_\\omega|x)} - log(\\frac{\\pi^*(Y_\\iota|x)}{\\pi_{ref}(Y_\\iota|x)}))^{-1}))$\nwhich is the SFT objective added to the IPO objective. When we refer to dCPO later in this paper, we are referring to this modified version of the CPO objective."}, {"title": "4 Experiments", "content": "We fine-tune an LLM for the task of machine translation under five different conditions: SFT, IPO, dCPO, and pre-training with SFT followed by either IPO or dCPO, denoted as SFT\u2192IPO and SFT\u2192dCPO, respectively, and evaluate with the neural metrics XCOMET (Guerreiro et al., 2023) and MetricX (Juraska et al., 2023). Rafailov et al. (2023) pre-train their large language models on in-domain data such that they are already able to perform the requested task to begin with. For the task single-turn dialogue, they use the Anthropic Helpful and Harmless dialogue dataset but because no pre-training data is available, they perform SFT on the helpful answers as a pre-training step. This is similar to our conditions SFT\u2192IPO and SFT\u2192dCPO.\nThe LLM that we choose to fine-tune is Tower-Base by Alves et al. (2024). We make this choice because it is a multi-lingual LLM pre-trained on all languages we intend to work with and because the size of the model is still small enough to perform a full fine-tune with our resources\u00b9. We opt for Tower-Base instead of Tower-Instruct because Tower-Instruct has been instruction fine-tuned for various down-stream tasks and not just for machine translation. Using Tower-Base instead allows us to perform a SFT step on our own. We fine-tune in all scenarios with a minimal prompt \"Translate English to German.\\nEnglish: {Source}\\nGerman:\" for our German examples. Our Russian examples use a prompt with the language name changed. This prompt is used for both SFT and PO training objectives.\nOur post-edits come from WMT Automatic Post-Editing (APE) shared tasks of previous years. These datasets contain triples of source, machine-translation (MT), and post-edit (PE). We focus on the language pairs En\u2192De from 2020 and En\u2192Ru from 2019. The En\u2192De source data comes from Wikipedia and is translated by a black-box NMT system (Chatterjee et al., 2020). The En\u2192Ru data comes from the information technology domain from Microsoft Office localization work and was translated by Microsoft's production NMT system (Chatterjee et al., 2019). The En\u2192Ru data contains base64 encoded data and sequences long enough to cause out of memory errors. We therefore filter out sequences with fewer than 4 tokens, more than 128 tokens, or more than 500 characters from the En Ru training data, leaving 9290 (source, mt, pe) triples for training. The En\u2192De training data was already clean and all 7000 (source, mt, pe) triples were kept for training.\nWe train with fully-sharded data parallelism (FSDP) in PyTorch using Accelerate (Gugger et al., 2022) across four GPUs with an effective batch size of 256 sequences. When possible, we shared hyperparameters across all runs and datasets. For example, both IPO and dCPO have \u03b2 set to 0.1. Full hyper-parameters can be found in the Appendix A."}, {"title": "5 Results", "content": "Our XCOMET metric results are shown in Table 2. The evaluation shows that Tower Base is already competent at performing zero-shot translation for English to German, achieving reference-free XCOMET-XL and -XXL scores that are above the MT hypotheses contained in our dataset; 94.33 and 94.75, respectively. This is in spite of the fact that the model has not yet been instruction fine-tuned to perform zero-shot translation.\nThe lack of instruction fine-tuning is made obvious in the English to Russian results, where the model is unable to translate well prior to fine-tuning. Specifically, the Tower Base model frequently translated its instructions to Russian and ignored the source text, yielding lower scores. XCOMET-XL and -XXL seem to react differently to these non-translations with XCOMET-XXL punishing them more severely than XCOMET-XL.\nSupervised fine-tuning is able to reach the level of the post-edits contained in the APE datasets when evaluating with reference-free evaluation. SFT surpasses the post-edits only with XCOMET-XL on the En\u2192De data but this improvement is not significant. Here, we are evaluating the post-edits included in the dataset as if they were hypotheses for the source sentences.\nIPO and dCPO are able to improve XCOMET-XL and -XXL scores for En\u2192De above what the post-edits achieve, but only for -XXL is this improvement significant, as evaluated by pairwise bootstrap resampling implemented in the COMET package. For En\u2192Ru, only dCPO is able to surpass post-edits and even then only for XCOMET-XL.\nHowever, once we initialize the PO methods with the SFT model, we find our best results. SFT\u2192dCPO is significantly better than both the MT and PE data from the dataset, the Tower Base model, and the SFT model for both En\u2192De and Ru; while for just En\u2192Ru, it is better than all other systems.\nResults with references do not differ drastically and can also be found in Table 2. We also evaluate with MetricX 23 XL (Juraska et al., 2023) and show our results in Table 3. The relations follow those of XCOMET and reinforce our conclusions."}, {"title": "6 Analysis", "content": "In addition to evaluating the fine-tuned models with neural metrics, we analyze the behavior of the models after training to see how the log-probabilities of the two sequences change compared to the baseline model. Additionally, we use the log-probabilities as a measure for model preferences. If one sequence is more probable, it is preferred by the model.\nIn our analysis, we remove machine translation and post-edit pairs where the post-edit remains un-edited. This is because we are looking for differences in model behavior between machine translations and post-edits, which can not be done when they are the same sequence."}, {"title": "6.1 Log Probability Changes", "content": "Figures 2 and 3 are split violin plots showing the difference between log-probabilities before and after training, for German and Russian respectively. The left side of each violin shows the post-edit sequences' change from the baseline model's while the right side shows the machine translations' difference. This way we can examine how each training method affects the two sequence types individually. Additionally, we also measure the difference between the post-edits and machine translations after training in Tables 4 and 5 for German and Russian, respectively.\nAs we see in Figure 2, if we perform SFT on post-edits, as would typically be done when treating post-edits as new references, both the post-edits and the MT outputs become more likely under our fine-tuned model. Because the post-edits and MT outputs are highly correlated, they likely reside very close to each other in the model's hidden representation. This means, that with a smooth mapping from hidden representations to outputs, increasing the probability of the PE will also increase the probability of the MT sequence.\nFor the En\u2192De IPO and dCPO runs, we see the post-edits stay close to the baseline while the MT is pushed further down in log-probability space. Additionally, the distance between the two sequences increases under PO compared to SFT. As shown in Table 4, the average distance that PEs are above MT outputs doubles after PO compared to SFT.\nAfter the IPO training, both sequences become less likely as seen in the split violin plot for IPO in Figure 2. This method does not have the upwards pressure on the preferred sequences that SFT or dCPO does, so we hypothesize that the downward pressure on the MT output also drags the PE sequence down as well; similar to how SFT increases the probability of MT without training on it. Alternatively, it could be that in order to establish a greater distance between the sequences, probability mass has to be re-allocated to other possible sequences.\nWith the En Ru data, we see that the MT sequences benefit more from training than the PE sequences do, even though they remain unseen, as shown by the violin plot for SFT in Figure 3. This corresponds to the smaller difference between PE and MT that we see for SFT when compared to Tower Base in Table 5. The need for more fine-tuning of the Tower Base model is also visible in the SFT, SFT-initialized, and dCPO models' larger displacement from the Tower Base log-probabilities. IPO remains close to the 0 point for both sequences because the only pressure for each sequence is for them to move further apart; which is more difficult with the large overlap between the machine-translations and post-edits.\nEn Ru appears similar for IPO, where both sequences are moved down in log-probability space, however the violin plot for dCPO and the SFT initialized models have a displacement from the baseline similar to SFT. This is because the baseline model was unable to perform zero-shot translation for En Ru and, since the dCPO loss includes an SFT term, it learned how to translate which moved all sequences upwards. Unlike SFT, post-edits benefit more than machine translations after dCPO training.\nFinally for the En\u2192De SFT initialized models, we see in Figure 2 that post-edits increase in probability over the baseline while machine translation outputs are held close to or below the baseline. The difference between PE and MT is increased here compared to the PO only conditions.\nWe find that this behavior generalizes also to the development and test sets as shown in Table 4. For En\u2192Ru, the SFT IPO model and the dCPO model both have post-edits and machine translation increase in likelihood compared to the baseline. This is again due to the baseline model being unable to perform zero-shot translation and both sequences become more likely after it is able to do so. SFT\u2192dCPO appears similar but far more stretched out and with MT moved below the baseline. This model trained for much longer before reaching its early stopping criterion ( SFT\u2192dCPO stopped after 10 epochs, compared to SFT IPO stopping after 2)."}, {"title": "6.2 Preference Changes", "content": "Changes in log-probabilities from the baseline model do not necessarily indicate whether the models' preferences have changed. It could be that, in (mt, pe) pairs where it is already the case that pe > mt, the distances between pe and mt increased, but examples where mt > pe did not have their ordering changed. To that end, we also examine the baseline model's preference in terms of sequence probability\u2014if a sequence's average log probability is strictly greater than that of another sequence, it is preferred. We plot preferences across all data splits for En\u2192De in Figure 4 and for En Ru in Figure 5. The exact values with corresponding confidence intervals are in Tables 6 and 7, respectively.\nFor both language pairs, we find that the Tower Base model does not have strong preferences. On the En De data set, it prefers post-edits to machine translation 57.80% of the time on the test set while for En\u2192Ru this preference occurs 59.49% of the time. For En\u2192De, SFT significantly improves this preference on the training data but not on the development or test data. SFT actually seems to change the preferences in favor of machine translations on the En Ru data; which also coincides with a decrease in the average distance between sequences and machine translations increasing in probability more.\nWhen we train with IPO and dCPO on En\u2192De, we find that both improve the preference for post-edits up to 68.27% on test data. The improvements above SFT are significant for both models on the train set while for dev, the confidence intervals overlap, and for test only IPO is significantly better. On En\u2192Ru, we see a similar improvement in preferences but only on the training set are they significant.\nInitializing with SFT and then training with PO on En De yields the best improvements with 69.87% on test. Both SFT IPO and SFT-dCPO are significantly better than SFT across all data splits. Again, En\u2192Ru shows similar behavior with only the change on the training set being significant.\nAcross all data splits on En\u2192De, IPO methods seem to establish a slightly stronger preference for post-edits which seems to be accounted for by increase in difference between the two sequence types as shown in Table 4. For En\u2192Ru, dCPO is better at establishing this preference which also coincides with the increase in differences from Table 5."}, {"title": "7 Conclusion", "content": "Post-editing is part of common translation workflows before publishing to clean up raw-MT outputs. If the post-edits are used for training purposes, they are treated simply as new references and the MT output is treated as a by-product. Post-edits are created with an implicit preference in mind, that the PE should be better than the MT. We find that keeping both the PE and MT allows us to perform preference optimization techniques and improve translation quality with data that would otherwise be discarded.\nWe find that performing supervised fine-tuning using post-edits as references also increases the likelihood of the machine translations which remained unseen by the system. However, because the original machine translations were erroneous (in order to need correction), it is disadvantageous to increase their likelihood as well. Using PO techniques allows the model to establish a larger margin between the post-edit sequence and the machine translation sequence in log-probability space.\nIncreasing this margin coincides with significant improvements in neural translation metrics. We additionally find that we can measure the models' preferences in terms of sequence probability\u2014if one sequence is more likely it is preferred. Models trained with SFT do not have a significant change in preferences compared to the baseline models but using PO teaches the model to prefer the post-edits above the machine translations.\nIn future work, we would like to examine the effect of the distance between post-edit and machine translation sequence probabilities. Currently, IPO sets a single distance for all sequence pairs but this may be sub-optimal when the sequences are correlated to different degrees. For example, if a post-edit and machine translation share a large prefix, the rest of the tokens in the sequences must account for the distance, while for non-overlapping sequences all tokens contribute to the distance between the log-probabilities."}]}