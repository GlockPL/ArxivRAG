{"title": "Out-of-Distribution Detection with Attention Head Masking for Multimodal Document Classification", "authors": ["Christos Constantinou", "Georgios Ioannides", "Aman Chadha", "Aaron Elkins", "Edwin Simpson"], "abstract": "Detecting out-of-distribution (OOD) data is crucial in machine learning applications to mitigate the risk of model overconfidence, thereby enhancing the reliability and safety of deployed systems. The majority of existing OOD detection methods predominantly address uni-modal inputs, such as images or texts. In the context of multi-modal documents, there is a notable lack of extensive research on the performance of these methods, which have primarily been developed with a focus on computer vision tasks. We propose a novel methodology termed as attention head masking (AHM) for multi-modal OOD tasks in document classification systems. Our empirical results demonstrate that the proposed AHM method outperforms all state-of-the-art approaches and significantly decreases the false positive rate (FPR) compared to existing solutions up to 7.5%. This methodology generalizes well to multi-modal data, such as documents, where visual and textual information are modeled under the same Transformer architecture. To address the scarcity of high-quality publicly available document datasets and encourage further research on OOD detection for documents, we introduce FinanceDocs, a new document AI dataset. Our code\u00b9 and dataset\u00b2 are publicly available.", "sections": [{"title": "1 Introduction", "content": "Out-of-distribution (OOD) detection presents a significant challenge in the field of document classification. When a classifier is deployed, it may encounter types of documents that were not included in the training dataset. This can lead to mishandling of such documents, causing additional complications in a production environment.\nEffective OOD detection facilitates the identification of unfamiliar documents, enabling the system to manage them appropriately which allows the classifier to maintain its reliability and accuracy in real-world applications.\nThis has heightened the focus on OOD detection, where the primary objective is to determine if a new document belongs to a known in-distribution (ID) class or an OOD class. A significant challenge lies in the lack of supervisory signals from the unknown OOD data, which can encompass any content outside the ID classes. The complexity of this problem increases with the semantic similarity between the OOD and ID data (Fort et al., 2021).\nA number of approaches have been developed to differentiate OOD data from ID data, broadly classified into three categories: (i) confidence-based methods, which focus on softmax confidence scores"}, {"title": "2 Related Work", "content": "Learning embedding representations that generalize effectively and facilitate better differentiation between ID and OOD data is a well-recognized challenge in the field of machine learning (Zhou et al., 2023). To tackle this challenge, various studies have focused on specialized learning frameworks aimed at optimizing intra-class compactness and inter-class separation (Ye et al., 2021). Building on the principles of contrastive representation learning, researchers such as Chen et al. (2020) and Li et al. (2021) introduced prototypical learning (PL). This approach leverages prototypes derived from offline clustering algorithms to enhance unsupervised representation learning. Furthermore, Ming et al. (2024) integrated PL into their OOD learning framework, HYPO, achieving effective separation between ID and OOD data. This line of research was further advanced by Lu et al. (2024), who introduced the concept of multiple prototypes per cluster and employed a maximum likelihood estimation (MLE) loss to ensure that sample embeddings closely align with their corresponding prototypes. Additionally, approaches such as VOS (Du et al., 2022) and NPOS (Tao et al., 2023) have focused on regularizing the decision boundary between ID and OOD data by generating synthetic OOD samples, while Lin and Gu (2023) utilized open-source data as an OOD signal.\nIn our proposed methodology, we similarly aim to enhance the distinction between ID and OOD data through improved embedding representations. However, unlike previous studies that explore customized learning frameworks diverging from the standard cross-entropy loss, we concentrate on feature regularization during inference using our proposed attention head masking methodology. Our approach deliberately avoids altering the network's training procedure, thereby mitigating potential negative impacts on performance and preventing increased training costs. By focusing on inference rather than training modifications, our method ensures robust and cost-effective OOD detection.\nOther inference-based methods, such as Avg-Avg (Chen et al., 2022) and Gnome (Chen et al., 2023), have also sought to enhance OOD detection through innovative techniques. Avg-Avg operates by averaging embeddings across both sequence length and different layers of a fine-tuned model, while Gnome combines embeddings from both a pre-trained and a fine-tuned model. These"}, {"title": "3 Method", "content": "The proposed AHM method, focuses on the feature extraction mechanisms inherent in transformer models, specifically the self-attention mechanism (Vaswani et al., 2017). Based on the premise that OOD data exhibit less semantic similarity to ID data, our goal is to generate embedding features that enhance the separation between ID and OOD data. The embeddings are then used in distance or density-based OOD detection methods, such as the Mahalanobis (Lee et al., 2018) or kNN+ (Sun et al., 2022). Our method is provided in Algorithm 1 (cf. Appendix A.1 for the theoretical framework) and the masking step is summarised in Figure 1."}, {"title": "4 Results and Discussion", "content": "We utilized two datasets in our experiments: Tobacco3482 and FinanceDocs. The Tobacco3482 dataset (Kumar et al., 2014) comprises 10 classes: Memo (619), Email (593), Letter (565), Form (372), Report (261), Scientific (255), Note (189), News (169), Advertisement (162), and Resume (120). As a subset of IIT-CDIP (et al., 2006), it was further processed to remove blank and rotated pages, preserving the rich textual and image modalities essential for a multi-modal system. Despite these efforts, some instances exhibit poor OCR quality due to the low-quality scans.\nWe present FinanceDocs (cf. Appendix A.5 for per-category details and A.6 for dataset samples), a newly created dataset comprising 10 classes derived from open-source financial documents, including SEC Form 13 (663), Financial Information (360), Resumes (287), Scientific AI Papers (267), Shareholder Letters (256), List of Directors (188), Company 10-K Forms (181), Articles of Association (176), SEC Letters (141), and SEC Forms (121). Unlike Tobacco3482, FinanceDocs consists of high-quality digital PDFs (Annual Reports; SEC EDGAR Database; Companies House Service; ACL Anthology; Resume Dataset). The FinanceDocs dataset was labeled through the following process: a PDF parsing package (PyPDF2) was used to extract content from the original PDF documents. Each page was then visualized individually by a human annotator, who determined the relevance of the page to the collected classes and assigned the appropriate class label (cf. Appendix A.4 for annotator training and validation).\nWe employ two widely recognized OOD metrics to assess the performance of our proposed AHM method in comparison to other OOD benchmarks (Yang et al., 2024): AUROC, which measures the area under the ROC curve (higher values indicate better performance), and FPR, the false positive rate at a 95% true positive rate. A higher AUROC signifies better discrimination, while a lower FPR indicates greater robustness in rejecting OOD data.\nFor our experiments, we utilize LayoutLMv3 (Huang et al., 2022), a transformer-based multi-modal model with 125.92 million parameters. We conduct both cross-dataset and intra-dataset OOD experiments. In cross-dataset OOD, the model is"}, {"title": "4.3 Current Benchmarks", "content": "We evaluated the peformance of various OOD detection methods, comparing them with our proposed methods, knnAHM, mahAHM, and mahAvgAvg_AHM, which apply k-Nearest Neighbor (kNN) and Mahalanobis methods to dense embeddings generated by AHM. mahAvgAvg_AHM is similar to mahAHM but uses the AvgAvg embedding aggregation method (Chen et al., 2022).\nAs shown in Table 1, for the Tobacco3482 dataset with ADVE as the OOD class, our proposed mahAHM outperformed other methods, achieving an AUROC of 0.985 and an FPR of 0.071. The high AUROC indicates that our method significantly enhances the Mahalanobis distance-based approach in distinguishing between ID and OOD samples. The notably lower FPR compared to previous methods like vim and residual (FPRs of 0.147 and 0.149, respectively) demonstrates the robustness of mahAHM in correctly rejecting OOD samples.\nFor the FinanceDocs dataset, with Resumes as the OOD class, both knnAHM and mahAHM achieved superior performance, with AUROCs of 0.975 and 0.978, and FPRs of 0.114 and 0.099, respectively. Our mahAvgAvg_AHM method also improved performance over mahAvgAvg, highlighting the effectiveness of our approach in creating more separable embeddings between ID and OOD data. This is further evidenced by cross-dataset results in Table 1, where mahAvgAvg_AHM consistently outperformed mahAvgAvg, notably reducing the FPR by 5% on FinanceDocs and achieving an AUROC of 0.99 with an FPR of 0.0001 on Tobacco3482. This performance surpasses the respective method mahAvgAvg without AHM applied. In fact, across all methods tested mahAvgAvg, Mahalanobis and knn, the application of our AHM technique consistently resulted in improved performance.\nOverall, the AHM technique significantly enhances the performance of kNN, Mahalanobis, and mahAvgAvg, resulting in superior outcomes for knnAHM, mahAHM, and mahAvgAvg_AHM, as evidenced by higher AUROCs and lower FPRs across intra-dataset and cross-dataset experiments, demonstrating strong generalizability across diverse datasets and methods."}, {"title": "5 Conclusion", "content": "In this study, we present the AHM technique for OOD detection in transformer-based document classification. Our methods, knnAHM, mahAHM and mahAvgAvg_AHM, demonstrated significant improvements in AUROC and FPR metrics across various datasets. These results underscore the effectiveness of optimizing attention mechanisms to enhance feature separation between ID and OOD"}, {"title": "6 Limitations", "content": "While AHM techniques significantly reduced FPR in most cases, the improvements were marginal in cross-dataset scenarios where the Tobacco dataset served as the OOD data. This suggests a potential dependency on specific datasets. Additionally, AHM is a technique limited to attention-based DNN architectures that employ multi-head self-attention. Future research should aim to broaden the range of datasets explored."}, {"title": "A Appendix", "content": "This section provides supplementary material in the form of dataset examples, implementation details, etc. to bolster the reader's understanding of the concepts presented in this work."}, {"title": "A.1 Proposed Methodology and Theoretical Framework", "content": "The central hypothesis underlying the proposed solution is predicated on the assumption that ID data should exhibit greater similarity in their feature representations when compared to OOD data. Consequently, we posit that when considering a pair of data points from two similar ID classes (denoted as Pair A) and a pair consisting of one ID and one OOD data point (denoted as Pair B), the application of a masking procedure on input features (whether textual or visual) would result in a more pronounced divergence in the feature space for Pair B as compared to Pair A. Initial experiments were conducted with random masking of input features. For textual data, this involved replacing tokens randomly with the '[MASK]' token. For visual data, random image patches were set to zero, effectively splitting the image into patches and nullifying selected segments. These preliminary experiments revealed two critical factors influencing the final feature embeddings used in distance-based OOD detection methods, such as the Mahalanobis distance: (a) the input tensors provided to the model, and (b) the feature extraction mechanism employed by the model, specifically the attention mechanism. Although the early experiments primarily focused on input masking, achieving a consistent masking strategy proved challenging. While a consistent mask could be established for visual data by dividing images into uniformly sized chunks and consistently masking specific segments, such consistency was elusive for textual features. The variability in sequence length across different tokens complicated the masking process, often leading to strategies that involved masking padding tokens rather than meaningful data.\nIn light of these challenges, our focus shifted from input masking to the feature extraction process itself, particularly the attention mechanism within the model. We discovered that consistent masking could be achieved by selectively masking attention heads within different layers of the encoder. These heads are responsible for learning different representations and capturing different as-"}, {"title": "A.2 Description of Algorithm 1", "content": "As detailed in Algorithm 1, we begin with a fine-tuned model and proceed by randomly initializing various attention head masks based on a masking hyperparameter p. This hyperparameter represents the percentage of attention heads H set to zero within each attention layer N of the model. For each random mask, we extract dense hidden representations from both the training and evaluation datasets. The objective is to identify which of these randomly generated attention head masks minimizes the divergence between the representations of the evaluation and training data in the feature space. This is accomplished by calculating the average similarity score among the top K nearest neighbors for each evaluation data point. The attention head masks are then ranked based on these aggregated similarity scores. Finally, we select the top F masks with the highest similarity scores between the evaluation and training data and use them to generate new feature representations. These features are then ensembled (i.e., averaged) and subsequently utilized in a distance-based OOD detection method, such as the Mahalanobis distance."}, {"title": "A.3 Hyperparameter Tuning", "content": "Table 2 summarizes the hyperparameters for model training. The model was trained using a carefully selected set of hyperparameters to optimize its performance. The training batch size per device was set to 32, while the evaluation batch size was configured at 8, ensuring efficient computation throughout the process. To stabilize updates, gradient accumulation was performed over 8 steps. The learning rate was set at 5 \u00d7 10\u22125, with no weight decay applied, to prevent the risk of overfitting.\nThe Adam optimizer was configured with parameters \u03b2\u2081 = 0.9, \u03b22 = 0.999, and an epsilon value of 1 \u00d7 10-8 to ensure effective convergence. To maintain stability during training, the maximum gradient norm was capped at 1.0. The model underwent training for 65 epochs, with evaluations delayed by 5 steps to monitor progress at appropriate intervals, allowing for a well-tuned and stable learning process.\nThe hyperparameters chosen for the proposed"}, {"title": "A.4 Annotator Training and Validation", "content": "To maintain high-quality annotation in line with ethical standards, we enlisted three postgraduate students fluent in English. They received instruction and participated in sessions with finance professionals to address any task-related questions. The annotation process spanned about four months, involving 90 training sessions, with breaks scheduled every 45 minutes. The students were compensated"}, {"title": "A.5 Dataset description of FinanceDocs", "content": "The FinanceDocs dataset comprises a diverse collection of financial and legal documents sourced from various reliable platforms, offering a comprehensive view of corporate disclosures, shareholder communications, and regulatory filings. Each document type serves a distinct purpose, providing insights into different aspects of corporate governance, financial performance, and regulatory compliance, as detailed below:\n\u2022 SEC form documents: These documents were collected from the Securities Exchange Commission (SEC) website. These forms are statements of changes in beneficial ownership.\n\u2022 Shareholder letter documents: These documents were collected from annual reports. A shareholder letter in an annual report provides a summary of the company's financial performance, highlighting key achievements, strategic initiatives, and market conditions over the past year. It offers leadership's perspective on successes and challenges while outlining future goals and potential risks. The letter also emphasizes the company's commitment to corporate governance, social responsibility, and long-term growth.\n\u2022 SEC letter documents: These documents were collected from the SEC website. These are letters from companies to the SEC about various company disclosures.\n\u2022 SEC-13 form documents: These documents were collected from the SEC website. These forms disclose significant information about an entity's ownership or control over securities, typically required for investors with large holdings.\n\u2022 10k form documents: These documents were collected from annual reports. These represent the 10k forms of an annual report\n\u2022 Financial info documents: These documents were collected from annualreports (Annual Reports). They consist of various financial information, including the income statement,"}]}