{"title": "Large Language Models to Diffusion Finetuning", "authors": ["Edoardo Cetin", "Tianyu Zhao", "Yujin Tang"], "abstract": "We propose a new finetuning method to provide pre-trained large language models (LMs) the ability to scale test-time compute through the diffusion framework. By increasing the number of diffusion steps, we show our finetuned models achieve monotonically increasing accuracy, directly translating to improved performance across downstream tasks. Furthermore, our finetuned models can expertly answer questions on specific topics by integrating powerful guidance techniques, and autonomously determine the compute required for a given problem by leveraging adaptive ODE solvers. Our method is universally applicable to any foundation model pre-trained with a cross-entropy loss and does not modify any of its original weights, fully preserving its strong single-step generation capabilities. We show our method is more effective and fully compatible with traditional finetuning approaches, introducing an orthogonal new direction to unify the strengths of the autoregressive and diffusion frameworks.", "sections": [{"title": "1. Introduction", "content": "The scalability of autoregressive large language models (LMs) is a pivotal component of the current generation of foundation models. However, despite their unprecedented capabilities, LMs inherently lack many valuable properties that could be expected of an \"artificial general intelligence,\u201d such as the ability to scale computation for their most critical decisions. Efforts to address this limitation primarily focused on eliciting more nuanced responses through prompting and targeted searches over the space of possible completions, anchoring the reasoning process in the space of generated tokens.\nIn this work, we aim to unite the strengths of these frameworks by introducing LM to Diffusion (L2D): a new finetuning method powering pre-trained LMs with the scaling properties of diffusion. Rather than learning a diffusion model from scratch, our method harnesses the large amount of \"system 1\" understanding efficiently acquired during autoregressive pre-training by casting LMs as single-step diffusions. Then, by introducing a small fraction of new parameters \u2013 comparable to modern parameter-efficient approaches \u2013 we imbue the model with a new set of multi-step \u201creasoning\u201d skills, the ability to scale computation on-demand, and the potential to incorporate powerful guidance techniques , all without compromising its original single-step capabilities.\nIn summary, our technical contributions are the following:\n\u2022 We introduce L2D, a new finetuning method to power LMs with the scaling properties of diffusion, combining key strengths from these two frameworks.\n\u2022 We show that L2D significantly improves performance across four different LMs on math, coding, and a variety of reasoning tasks; and that its benefits are both superior and complementary to traditional finetuning.\n\u2022 We demonstrate that L2D allows to scale performance with additional compute and opens the door to LMs equipped with autonomous per-token scaling and powerful diffusion guidance techniques.\nWe will share our full code to facilitate future advances in developing new scalable foundation models with diffusion."}, {"title": "2. Gaussian Diffusion for LM Finetuning", "content": "In this section, we describe the key components of our L2D framework. In particular, we provide details about the considered diffusion formulation, together with our designed training and inference approaches. Although each of the following subsections offers a concise introduction to the concepts and modern practices of diffusion and language modeling, we refer to recent work and Section 5 for more comprehensive resources. We conclude the section explaining how our design decisions make L2D a natural extension to modern language modeling aimed to complement rather than supersede the autoregressive framework."}, {"title": "2.1. Gaussian Diffusion", "content": "Gaussian diffusion decomposes the problem of generating new samples from a target unknown distribution p* from a source distribution q := N(0, I) over multiple \u201csimpler\" steps, reusing the intermediate information computed in the model's previous attempts. These subsequent diffusion steps can be seen as a discretization of a continuous \"denoising\" process from t = 0 to time t = 1, over which the model is tasked to transform samples from q to p*. All intermediate distributions along the denoising process are defined by a corresponding corruption process, mixing target data points X1 ~ p* with noise from q to produce xt ~ Pt:\n$Xt = Atx1 + Btxo, where xo ~ N(0, I)$.  (1)\nHere, the schedules at and \u00dft are defined as monotonic functions with ao = \u03b2\u2081 = 0 and \u03b1\u2081 = \u03b2\u03bf = 1, satisfying the constraints such that po := q and p\u2081 := p*.\nNeural networks (NNs) in single-step generative modeling solely rely on an external source of pure randomness to generate new samples from scratch. In contrast, the goal of diffusion is to learn a neural network fe conditioned on samples from each pt and tasked with solving the simpler problem of generating new samples from lower nearby noise levels Pt+\u25b3t. Thus, effectively splitting the challenge of learning and generating new samples in multiple steps, which can be scaled based on computational availability."}, {"title": "2.2. L2D Parametrization and Training Formulation", "content": "An effective choice of loss to train diffusion models is simply to predict the values in the uncorrupted target datapoints from p\u2081 (i.e., p*) given the partial information contained at each corruption level \u00ee = fo(xt, t). When p\u2081 is a distribution over a continuous domain, this is commonly done by using a simple mean squared regression loss on all timesteps t, as popularized by the DDPM algorithm:\n$LL2(0) = Et,x0,x1 [||X1 - fo(xt,t)||2] $.  (2)\nAnother key design decision for diffusion is the choice of schedules at and \u00dft, which define the diffusion path that fo will be learning. This is one of the most significant choices for continuous diffusion models, affecting all aspects of both training and inference dynamics . In our work, we employ the schedules at = t and \u00dft = (1 \u2212 t)o, where o is a hyper-parameter linearly scaling the signal-to-noise ratio for all timesteps between p\u2081 and po within the samples xt ~ Pt. This choice is closely tied to the rectified flow matching schedules , which have been shown to possess particularly desirable properties for \"straightening\" the diffusion path  and have been widely adopted in the recent diffusion literature . To ease our notation and make this connection explicit, we absorb the hyper-parameter \u03c3 in the standard deviation of our base distribution po, which simplifies the schedules to at = t and \u1e9et = (1-t).\nUnlike for the continuous case, language modeling operates over a target distribution p\u2081 defined on a finite vocabulary table V, where to each index y \u2208 1, . . ., |V| there corresponds a token embedding x \u2208 Rd. This key difference is one of the main reasons that diffusion in language modeling is yet to have a predominant recipe with several recent approaches even exploring alternative diffusion formulations over the discrete space of vocabulary indices y . In this work, we choose to still diffuse over the token embeddings x, as in standard continuous diffusion, but do not employ an MSE loss as done by Li et al. (2022). Instead, we learn our diffusion model with a simple cross-entropy loss, establishing a direct connection to traditional single-step language modeling. In particular, given a token x\u2081 indexed by label y sampled"}, {"title": "2.3. L2D Inference Formulation", "content": "To generate new samples with a traditional continuous diffusion model, an effective approach is to use the predictions 2 from fo(xt, t) to construct an ODE that preserves the marginal distribution pt at each timestep t. While many such valid ODEs exist for a single diffusion process, we adopt the formulation from Liu et al. (2022), which is designed to yield a constant expected velocity along the denoising path at each timestep t:\n$dxt = -xt / (1 - t)$.  (4)\nGiven access to these velocity predictions, the denoising process can start at t = 0 by simply drawing xt from pure noise, and be then performed over a sequence of steps. At each step, previous predictions are reused to bring xt to a lower noise level at t + At by following the direction dxt. In the simplest case, this process practically amounts to Euler integration where Xt+\u25b3\u2081 = xt + \u2206t \u00d7 dxt. However, any ODE solver can be employed with either constant or adaptive costs given by fixed discretization levels At or adaptive accuracy requirements.\nGiven our parameterization of fe, outputting categorical probabilities over the vocabulary tokens, meaning its pre-"}, {"title": "2.4. LMs as Single-step Diffusion Models", "content": "Our choices in designing L2D establish a clear connection with the traditional LM framework. As detailed above, training a diffusion model with Equation 3 can be interpreted as standard next-token prediction where the model is provided with an additional \u201cdiffusion token\" xt containing some amount of knowledge about the target y, ranging from no information (t = 0) to perfect information (t = 1). Therefore, LMs are essentially trained with an equivalent prediction objective to L2D's when t = 0, where xt is entirely uncorrelated with the target y. Similarly, inference following Algorithm 1 involves iteratively sampling increasingly accurate next tokens \u00ee from the model's logits up to a sampling budget T. Thus, traditional LM inference can be again viewed as a special case of this procedure with T = 1, where only the model's first sample is used to predict y.\nThe purpose of these design choices is that L2D aims to extend pre-trained LMs via a finetuning approach, rather than learning new models from scratch. While fully adopting diffusion training from the start might appear more general, we argue this risks losing some of the training scalability and powerful inductive biases inherent to traditional autoregressive modeling which led to their wide establishment in the language domain . Furthermore, L2D directly enables leveraging the extensive \u201csystem 1\u201d understanding already encoded in open foundation models. In fact, by building on their existing capabilities we avoid the prohibitive costs required in past attempts to match their performance with diffusion."}, {"title": "3. L2D Implementation", "content": "We design our L2D implementation as a modular extension for pre-trained transformers to efficiently harness the"}, {"title": "3.1. Diffusion Path Parametrization", "content": "Structure and initialization. We process the diffusion tokens xt within a separate parallel path to the LM's original architecture (denoted for and for). This choice allows us to optimize only a subset of the model's parameters without losing its original ability to process the sequence of past \"uncorrupted\" tokens in the context c. We implement the diffusion path fed with a transformer architecture and the same number of blocks as the main path fo\u2081, each comprising a subset of its layers. Moreover, to make the most of the pre-trained LM's knowledge, all layers in the diffusion path are also initialized with the weights from \u03b8\u03b9, in a similar spirit as. In practice, we find this initialization enables fast and inexpensive training, allowing us to optimize the diffusion path with simple low-rank adaptation. Furthermore, this approach greatly minimizes L2D's memory overhead, as it requires us only to store the small LoRA modules by reusing the LM's original weights in both \u03b8\u03b1 and \u03b8\u03b9.\nDiffusion path components. The transformer blocks in the diffusion path comprise a sequence of residual MLP and cross-attention modules. While the MLP modules follow the same structure as the corresponding modules in for, the cross-attention modules exclusively parameterize query and output linear layers. In particular, during cross-attention, the diffusion token \u00e6 for target token yk attends over all previous keys and values already computed from the corresponding self-attention module in for. We only integrate the information processed in fe back to the main path after all blocks, right before the LM's linear head. Specifically, we merge the two paths with an element-wise weighted sum for + wafea where the rescaled latents of diffusion token xfare added to the latents of the previous token xk\u22121.\nProperties and advantages. Our design choices have several key advantages over prior diffusion architectures targeted for multi-token generation . During inference, by saving the latent representation from for together with the KV cache, we only need to compute the output of the main path once for each generated token, no matter the number of diffusion steps. Furthermore, as the diffusion token for the k-th target only affects the main path at the previous position, we can fully parallelize training across the sequence batch dimension, sampling timesteps t\u2081...tk and diffusion tokens x...x independently. By doing this, we greatly mitigate the variance of the diffusion optimization objective, efficiently obtaining independent diffusion losses for all K sequence positions for each sampled input context x\u00ba . . . xK-1 in the data batches."}, {"title": "3.2. L2D Conditioning", "content": "Diffusion space vocabulary. To condition fea, we construct the vocabulary containing the discrete set of token embed-"}, {"title": "4. Experimental Results", "content": "In this section, we provide descriptions for the implementation specifics, training, and evaluation of our new L2D method. Then, we present comprehensive quantitative results, evaluating the benefits of L2D across state-of-the-art LMs of different sizes from the Llama 3 and Qwen 2.5 families . Lastly, we focus on Llama 3.2 1B Instruct to study the properties of L2D in greater depth \u2013 showing its complementarity to traditional finetuning and further pushing performance with advances specific to the diffusion literature, such as adaptive ODE solvers and classifier-free guidance.\nTo complement this section, we refer to Appendices A and B for a full set of hyper-parameters, further implementation details, and comprehensive descriptions of our datasets and tasks. Furthermore, we refer to Appendix C for thorough ablations of L2D and our baselines, together with Appendix D for results on additional benchmarks, analyses of additional extensions, and detailed per-task performance tables."}, {"title": "4.1. Implementing, Training, and Evaluating L2D", "content": "As described in Section 2, our main L2D implementation adapts the frozen pre-trained model parameters with LORA, efficiently reusing them in the diffusion path. We employ \u03c3 = 64 for the standard deviation of the base distribution po, as the discrete nature of language makes token classification trivial for low noise levels and we want to regularize against the model's most influential diffusion steps being concentrated early on during inference. Similarly to related work, we employ a small diffusion dimension d = 256 and rescale the inputs for fe\u300f such that the standard deviation of each component of xt has expectedly unit variance at all timesteps t. In all main results, we perform multi-step inference with a midpoint solver and 8 discretization levels, resulting in only 15 evaluations of fea.\nTypical applications of modern LMs involve processing a large fixed context of tokens before tackling the target task, such as user-provided prompts or fetched background re-"}, {"title": "4.2. L2D Across Modern Large Language Models", "content": "In Table 1, we provide quantitative results after training L2D on top of four different LMs spanning different model families and scales. L2D yields consistent improvements particularly evident in the math and coding tasks, the focus of our targeted training dataset, while optimizing a small fraction of the original weights (less than 6% for Llama 1B and 3.5% for Llama 8B). Although expectedly more limited, we still find some benefits in general knowledge tasks, indicating that the inductive bias from multi-step inference might also allow the model to better extract pre-acquired knowledge even beyond the finetuning corpus. Overall, we believe these results highlight the generality and effectiveness of L2D, allowing LMs to go beyond pure autoregression and harness some of the scaling properties of the diffusion framework in line with our primary goal.\nTo disentangle the benefits of our method from our choice of data, we directly compare L2D with both LoRA and full weight finetuning baselines. However, these traditional strategies appear to yield lower overall improvements with even frequent performance drops for the Llama instruct models on the coding problems. In Appendix D, we show that finetuning the base versions of Llama does not experience similar drops but fails to achieve competitive performance, suggesting that the private datasets employed in the instruction finetuning phases of these models might be superior to our public sources. Nonetheless, the concrete perfor-"}, {"title": "4.3. Analysis and Extensions", "content": "Inference-time diffusion scaling. In Figure 1, we show the performance of L2D while simply scaling the number of diffusion steps performed during inference. Moreover, in Figure 3, we show how performance varies within the L2D diffusion process as a function of t. In both cases, we expectedly observe a monotonic increase in overall LM performance, clearly analogous to the scaling properties of the diffusion framework for image modeling. Furthermore, comparing the scores of the highest and our default choice of 15 evaluations in Figure 1 shows that over 90% of the performance boost can be retained without excessive overhead costs. These results evidence that the efficiency benefits of diffusion formulations based on rectified flows empirically transfer to the language domain, allowing effective generation in a handful of steps .\nAdaptive diffusion process. In the first section of Table 2, we evaluate scaling compute using L2D with an adaptive second-order Runge-Kutta ODE solver , running inference for 118.33 steps on average. Remarkably, this extension allows the Llama 1B model to exceed the highest previous results obtained with the midpoint solver and a fixed number of 127 steps \u2013 notably showing the effectiveness of adaptively tuning compute based on the diffusion errors for each generated token. In line with these observations, as illustrated in Figure 4, we find the number of steps to visibly vary between different tasks. For instance, when dealing with the challenging MATH and coding benchmarks (whose performance is provided in the pass@10 regime) the"}, {"title": "5. Related Work", "content": "A popular approach for bringing diffusion models to the language domain has been to derive new generalizations of"}, {"title": "6. Discussion and Future Work", "content": "In this work, we provide concrete steps toward a new generation of autoregressively-trained LMs with the scaling capabilities of diffusion. We show how, after a small fine-tuning phase, L2D enables trading test-time compute for performance, beyond what possible with further training-time optimizations. Additionally, we demonstrate how our new method provides LMs with the key properties of diffusion models, enabling effective adaptive computation and domain guidance expertise specific to user demands. We hope this work provides new inspirations for unifying the strengths of the autoregressive and diffusion paradigms, powering some of the greatest milestones achieved in AI."}, {"title": "Algorithm 1 Diffusion language modeling predictions", "content": "Input diffusion model fe, context c, budget T\nInitialize t\u2190 0, \u2206t \u2190 1/(T \u2013 1)\nSample xt ~ \u039d(0, \u03c3\u00b2\u0399)\nfor i = 1, 2, ..., T \u2013 1 do\nSample yt ~ fo(xt, t, c)\nSet 2 - Vyt\nCompute dxt = -xt /\nUpdate t t + \u2206t, Xt \u2190 Xt + At \u00d7 dxt\nend for\nReturn y~ fo(x1, 1, c)"}]}