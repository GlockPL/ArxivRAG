{"title": "QVD: Post-training Quantization for Video Diffusion Models", "authors": ["Shilong Tian", "Hong Chen", "Chengtao Lv", "Yu Liu", "Jinyang Guo", "Xianglong Liu", "Shengxi Li", "Hao Yang", "Tao Xie"], "abstract": "Recently, video diffusion models (VDMs) have garnered significant attention due to their notable advancements in generating coherent and realistic video content. However, processing multiple frame features concurrently, coupled with the considerable model size, results in high latency and extensive memory consumption, hindering their broader application. Post-training quantization (PTQ) is an effective technique to reduce memory footprint and improve computational efficiency. Unlike image diffusion, we observe that the temporal features, which are integrated into all frame features, exhibit pronounced skewness. Furthermore, we investigate significant inter-channel disparities and asymmetries in the activation of video diffusion models, resulting in low coverage of quantization levels by individual channels and increasing the challenge of quantization. To address these issues, we introduce the first PTQ strategy tailored for video diffusion models, dubbed QVD. Specifically, we propose the High Temporal Discriminability Quantization (HTDQ) method, designed for temporal features, which retains the high discriminability of quantized features, providing precise temporal guidance for all video frames. In addition, we present the Scattered Channel Range Integration (SCRI) method which aims to improve the coverage of quantization levels across individual channels. Experimental validations across various models, datasets, and bit-width settings demonstrate the effectiveness of our QVD in terms of diverse metrics. In particular, we achieve near-lossless performance degradation on W8A8, outperforming the current methods by 205.12 in FVD.", "sections": [{"title": "1 INTRODUCTION", "content": "The diffusion model has experienced vigorous development in vision generation tasks due to its high controllability, photorealistic generation, and impressive diversity. Recently, research on video tasks based on diffusion models has gained increasing attention, driving the emergence of numerous attractive applications including, but not limited to, text-to-video [5, 18, 20, 65], image-guided video generation [4, 18, 25, 44], video editing [14, 41, 59], and other conditionally guided video generation tasks [53, 54, 60].\nDespite the remarkable effects of diffusion models, their relatively slow inference speed and substantial memory usage have hindered their broader application, particularly in video tasks. The core reasons for these limitations include: 1) The denoising procedure encompasses several hundred iterations, and 2) The extension of frame dimensions results in a notable escalation in memory utilization compared with image diffusion models. There are primarily two strategies to overcome such bottlenecks: minimizing the number of iteration steps, including [37, 51], and optimizing the efficiency of individual denoising operations through techniques like pruning [16, 32], distillation [30, 39], or quantization [33, 47]. The former only focuses on the first issue while ignoring the significant memory consumption. In this work, we mainly study the quantization of video diffusion models.\nModel quantization is a widely adopted and practical approach for reducing model footprint and accelerating inference by mapping floating-point values into low-bit integers. Among various quantization methods, post-training quantization (PTQ) requires no retraining or fine-tuning of the model and incurs minimal overhead, making it more practical in deployment. PTQ methods have been extensively studied in image diffusion models [19, 27, 33, 47, 49]. However, these methods exhibit significant performance degradation when directly applied to video diffusion models. We discover that the rationale lies in two aspects, i.e., the introduction of the frame dimensions and temporal attention modules in video diffusion models. Specifically, based on the following two observations, we explore the difficulty of quantization for VDMs:\nObservation 1: Highly reliant on discriminable temporal features. As illustrated in Figure 1(a), in a single denoising iteration, the features of all frames rely on the same temporal feature, which implies that disturbances arising from the quantization of temporal features will impact the generative quality of all frames. Furthermore, we observe that the uniform quantizer leads to the homogenization of temporal features, demonstrating a significant performance gap compared to those utilizing full-precision temporal features. Upon analyzing the distribution of temporal features, we observe a pronounced skewness near zero, where outliers often exceed thousands of times the magnitude of regular values, making conventional uniform quantizers unsuitable for temporal features.\nObservation 2: Inter-channel variations reduce the coverage of quantization levels. As depicted in Figure 1(b), in the image diffusion model, activation across different channels tends to be concentrated, with the range of activation for individual channels closely approximating the range of overall activation. Each channel covers nearly all quantization levels, indicating low quantization difficulty. In contrast to the image diffusion model, the activation values of the temporal attention module in video diffusion models are discrete and asymmetric across channels. The range of individual channels is significantly narrower than the overall activation. As a result, each channel accesses only a tiny fraction of quantization levels, posing a new challenge for PTQ.\nTo tackle these obstacles, we propose QVD, the first post-training quantization scheme for video diffusion models. Figure 2 shows the overall pipeline of the QVD. To mitigate the problem in observation 1, we introduce the High Temporal Discriminability Quantization (HTDQ), which contains a High Discriminability Temporal Quantizer (HiDi-TQ) to prioritize numerous near-zero values and retains high identifiability of time, and the Temporal Discriminability score (TDScore) to evaluate the similarity of adjacent temporal features. To settle the issue in observation 2, we introduce a Scattered Channel Range Integration (SCRI) method, which employs a per-channel integration operation to enhance the coverage of quantization levels by individual channels, therefore handle the discrete range of activations. We conduct comprehensive experiments to validate the superiority and versatility of our QVD.\nIn summary, our contributions are as follows:\n\u2022 We propose QVD quantization framework, which, to our knowledge, is the first PTQ method tailored explicitly for video diffusion models.\n\u2022 We identify the critical inter-channel variations issue in video diffusion models and highlight the significance of accurate and discriminable temporal features for video generation.\n\u2022 We introduce SCRI to improve the coverage of quantization levels across individual channels, TDScore to quantify temporal similarity, and the HiDi-TQ quantizer to keep high discriminability of temporal features.\n\u2022 Extensive experiments on various models and datasets demonstrate the superiority of QVD, which results in a 257.9 decrease in the FVD for the W6A8 PTQ of video diffusion models compared to existing methods in the image domain."}, {"title": "2 RELATED WORK", "content": "Recently, Diffusion Probabilistic Models [22, 50] have overtaken Generative Adversarial Networks (GANs) [13] as the leading approach in generative modeling, establishing a new benchmark for the field. Following the success of image diffusion techniques, video diffusion has also received widespread interest. VDM [23] becomes the pioneer in video generation and adopts 3D U-Net [12] structure. Some text-to-video (T2V) methods, such as MagicVideo [64], LVDM [21] utilize the Latent Diffusion Model (LDM) [46] and plug temporal modeling technique to it. Subsequent T2V schemes [5, 55, 55, 62] extend the single-stage to multi-stages. Image-to-video (I2V) methods, as another promising scheme, generate the video from a conditional image. Initially, LaMD [26] focuses on training an autoencoder to isolate motion information contained in videos. Stable Video Diffusion leverages text-to-image pretraining, video pretraining, and high-quality video finetuning to produce high-resolution videos. AnimateDiff [18] integrates the LORA [24] and avoids the time-consuming retraining. Other conditions, such as pose [29, 38], motion [7, 61], sound [31, 36] are also proposed. Substantial efficient solutions, including retraining-free sampler [2, 35, 37] and retraining-based methods [51, 63]. The first aims to decrease the number of sampling steps and the second is time-consuming. However, there is a gap in the research on video diffusion, and our work is the first to undertake a study in this area.\nQuantization has achieved substantial advancements in the domain of neural network acceleration, as corroborated by numerous scholarly investigations [6, 10, 28, 34, 40, 42, 56, 58]. Mainstream quantization schemes can be briefly classified into two categories: quantization-aware training (QAT) [9, 15] and post-training quantization (PTQ) [6, 10, 40]. QAT aims to retrain the network on the whole dataset, while PTQ only requires a small amount of unlabeled datasets for calibration. Several classical quantization methods, such as MinMax [28], Percentile [58], LSQ [15], PACT [9] are proposed successively for the convolutional neural networks. In recent years, the quantization of diffusion models [19, 27, 33, 47] has garnered widespread attention within the academic community. PTQ4DM [47] first discovers the difficulty of multiple-step activation distribution and generates the calibration data from a kew-normal distribution. Q-diffusion [33] introduces the uniform sampling calibration and split shortcut quantization for the bimodal activation distribution of the shortcut layers. PTQD [19] decomposes the quantization noise into interrelated and residual parts. TFMQ-DM [27] addresses the temporal feature disturbance and optimizes them separately. However, these existing methods mainly focus on image diffusion. In comparison, video diffusion necessitates significantly greater computational resources and storage. To the best of our knowledge, our work is the first to conduct the quantization for video diffusion models."}, {"title": "3 PRELIMINARIES", "content": "Diffusion models employ a sophisticated approach to image generation, relying on the application of Gaussian noise through a Markov chain in a forward process and a learned reverse process to generate high-quality images. Beginning with an initial data sample X0 ~ q(x) from a real distribution q(x), the forward diffusion process incrementally adds Gaussian noise over T steps:\nXT ~ q (xt/xt-1) = N (xt; \u221a1 - \u03b2ext-1, \u03b2\u03b5I),\n(1)\nwhere t is an arbitrary timestep and {ft} is the variance schedule. The reverse process, in contrast, aims to denoise the Gaussian noise N(0, I) into the target distribution by estimating q (xt-1|xt). In every step of the reverse process, marked by t, the model estimates the conditional probability distribution using a network ee(xt, t), which incorporates both the timestep t and the prior output xt as its inputs:\nXt-1~Po(xt-1|Xt) =\nNXt-1;\n1\nVat\n\u221a1-at\nXt\n\u221a1-\u0101t ee(xt, t), \u03a3tI\n(2)\nwhere at = 1 \u2212 \u1e9et and \u1fb6t = \u03a0\u2081=1&i. When extending the image diffusion to video diffusion, the latent noise adds a new dimension K which denotes the length of the video frames.\nModel quantization represents a technique for model compression, vital in optimizing neural networks for resource-constrained environments. Quantization transforms the network's weights and activations from a floating-point to a low-bit representation, thereby reducing memory footprint and computational intensity. This transformation is quantitatively described as follows:\n2b - 1\nwq = clamp (W1+z, 0, 261),\n(3)\nw = s \u00b7 (wq \u2212 z) \u2248 w,\n(4)\nwhere w and \u0175 denote the original and de-quantized weights or activations, wq is the quantized integer representation, s represents the scaling factor, z is the zero point, and b is the bit precision corresponding to 2b quantization levels. The uniform quantization here has equal intervals between each level. Quantization introduces an approximation (quantization) and a subsequent reconstruction (de-quantization) of network parameters.\nExpanding upon uniform quantization, our research incorporates logarithmic quantizer that also aligns well with the hardware-oriented aspects. For instance log2 quantization, used primarily on positive activation values, is succinctly represented as:\nwq = clamp (L-log21, 0, 26-1),\n(5)\nws2 Wa \u2248 w.\n(6)\nThis method, like uniform quantization, involves the scaling factor s but introduces a logarithmic approach to the quantization process. It offers rapid bit-shifting operations, making it a strategic choice for efficiently implementing models on hardware platforms. The integration of log2 quantization into our model framework further exemplifies our commitment to enhancing computational efficiency while maintaining fidelity in the intricate process of diffusion-based video generation.\nIn the video diffusion model, the time step t is encoded by the function h() into a temporal encoding, which is then mapped to temporal features by the embedding function f (\u00b7). These temporal features are channel-adjusted by the function g (\u00b7) in every Resnetblock3D of the noise estimation network and fused with all frame features. Formally, for the i-th Resnetblock3D, this process can be described using the following equation:\nFt = F + gi(f(h(t))),\n(7)\nwhere Ft represents the frame feature fused with the projected temporal feature. We denote the temporal feature at time-step t as T :\nTemb = f(h(t)).\n(8)"}, {"title": "4 MODIFICATIONS", "content": "As previously discussed, video diffusion models introduce a frame dimension, which enables the model to predict the noise for N frame features in each inference, a concept illustrated in Figure 2. In contrast, in image diffusion models, the frame count remains fixed at one. Structurally, temporal features are integrated into the features of each frame, and the quantization noise consequently spreads across all frames. Features from different frames are further fused within the temporal attention blocks, which results in the effects of quantization being compounded. As indicated in Table 4, omitting the quantization of temporal features leads to a significant reduction in the FVD by 160.82 compared to the uniform baseline. These findings highlight the critical importance of precise temporal features for video diffusion models.\nTo further explore this, we investigate the temporal features to explain why uniform quantizers fail to function effectively. The distribution of temporal features demonstrates a pronounced skewness, with a majority of the values aggregating near zero, and outliers are several orders of magnitude greater than the typical values observed as depicted in Figure 4(a). Even with a 10-bit uniform quantizer, dense intervals utilize only one of the 1024 quantization levels, as depicted in Figure 4(b). This causes most values in the temporal features to collapse to a single value, as shown in Figure 3(b), severely impairing their distinguishability. The log2 quantizer, as shown in Figure 4(c), allocates more quantization levels to dense intervals, preserving the distribution of small values in the temporal features and thus their discriminability. As indicated in Table 4, the log quantizer reduces FVD by 131.63 compared to the linear quantizer, demonstrating its effectiveness. However, we note that despite the improved FVD with the log quantizer, it incurs a greater L2 quantization loss compared to the uniform quantizer, as shown in Figure 5. We hypothesize that L2 loss prioritizes the impact of larger values while neglecting smaller values, which is inappropriate given the unique distribution.\nWe conduct comparative experiments to further validate the contribution of minor values to the discriminability of temporal features. Specifically, in setting 1, we zero the interval [-0.5min, 0.5max], and in setting 2, we apply a noise mask ranging from [-1.5, 1.5] to scale values within the [0.9max, max] interval, where max and min denote the maximum and the minimum for the temporal feature, respectively. As detailed in Table 4, the model exhibited robustness to disturbances in large values, while homogenization of small values lead to a collapse in model performance, underscoring the critical importance of minor values in preserving the discriminability of temporal features.\nThis analysis indicates the necessity of designing a quantizer specifically for the unique distribution of temporal features and developing a metric to measure the discriminability of temporal features. Driven by the above motivations, we propose the Temporal Discriminability Score (TDScore) and High Discriminability Quantizer for the Temporal Feature (HiDi-TQ)."}, {"title": "4.1.1 Temporal Discriminability Score", "content": "The TDScore evaluates the similarity between the current temporal feature and several adjacent temporal features. We denote the discriminability score of the t-th temporal feature as TDScoret. We initially apply a logarithmic function to Tt to enhance focus on minor values within the temporal feature as defined in Equation 9:\nemb\nTt' = sign(Temb) \u00b7 |log2 |Temb||.\n(9)\nSubsequently, we compute the mean cosine similarity between the feature and its n contiguous time steps following Equation 10:\ni=t+n\nTDScoret =\n1\n- cos_sim(Tembe\u0442\u044c).\nn\ni=t+1\n(10)\nA lower TDScore indicates higher discriminability of the temporal feature. We can employ the TDScore to assess the efficacy of quantization precisely."}, {"title": "4.1.2 High Discriminability Quantizer for Temporal Feature", "content": "As aforementioned, the quantization of temporal features not only necessitates minimizing quantization loss but also preserving distinctions between time steps. We evaluate quantizers calibrated using min-max or mean squared error (MSE) calibration strategy, as illustrated in Figure 5. These quantizers minimize quantization loss but concurrently diminish the distinguishability of temporal features (TDScores are near 1). We introduce the high discriminability quantizer considering the unique distribution of temporal features. This quantizer is based on a logarithmic quantizer which is non-uniform. It allocates more quantization levels to values concentrated around zero, unlike uniform quantizers. Conversely, sparse distributions of large values are allocated fewer quantization levels. However, the vanilla logarithmic quantizer maps both the positive part and the negative part of temporal features to the same positive interval. This causes the data concentration regions of positive and negative intervals to overlap, exacerbating data concentration. Consequently, this reduces the utilization of available quantization levels. To address this issue, a relaxation coefficient \u1e9e is introduced, as follows:\n2b - 1). (11)\nSclip emb\u03b2\nTZ clipTemb-Bemb - \u03b2\n[emb = sign(Temb) \u00b7 clipembS \nThe quantization process is initiated by a \u1e9e shift in the temporal features, thereby reducing the clustering of absolute values. Temb represents the quantized temporal features. The scaling factor s is typically set to be greater than the maximum absolute value of Temb to ensure that the scaled activation values fall within the range [0, 1]. Additionally, s can be adjusted to modify the data concentration, where increasing the value of s narrows the range of activation values and makes it more concentrated.\nAs illustrated in Figure 5, although the logarithmic quantizer ensures foundational temporal discrimination (low TDScore), it also incurs a considerable L2 loss (MSE loss). To balance the precision of quantization and temporal discrimination, we utilize a composite metric K as defined in Equation 12, which incorporates both TDScore and L2 loss, to search the optimal s and \u1e9e:\nK =\nT\nTDScorei +\nT\n(Tembemb)2.\n(12)\nThe TDScore allows K to prioritize smaller values, while the L2 component ensures attention to losses in larger values. To mitigate truncation errors, We constrain the value of s to the interval\nmax (|Temb\u2758), min (emb+ eps\n(13)\nThis typically spans a very large range, to enhance search efficiency, we employ exponential step sizes where si = max |Temb\u2758 \u00d7 20.05xi."}, {"title": "4.2 Scattered Channel Range Integration", "content": "Compared to the image diffusion models, we observe significant inter-channel variations in the video diffusion model. As depicted in Figure 1(b), we plot box plots for the activation sampled from the image diffusion model and output of the temporal attention block in the video diffusion model. It is evident that the activations generated by the temporal attention block exhibit discrete and asymmetric characteristics, referred to as inter-channel variations. The narrow range of individual channels leads to minimal overlap in the activation ranges across channels, resulting in low coverage of quantization levels per-channel, as illustrated in Figure 6, this diminishes the performance of the quantized model.\nUnlike the issues previously identified in large language models (LLMs), the problem of inter-channel variations is particularly pronounced in video diffusion models. This issue differs significantly from the outliers extensively studied in LLMs. In LLMs, as indicated by OS+ [57], outliers manifest as extreme shifts in specific channels. These shifts are consistent across samples, which allows for the straightforward identification and adjustment of an accurate shift amount to align the channel midpoints. Such a method is effectively utilized in LLMs, where techniques like OS+ [57] align and scale channel midpoints in the activation of the LayerNorm layer through a channel-wise shift to address these outliers. However, in video diffusion models, the challenge of inter-channel variations requires distinct approaches due to its more complex and variable nature compared to the channel-specific shifts observed in LLMs. Specifically, in video diffusion models, most channels exhibit varying degrees of shift that change with each sample, making it challenging to calculate an accurate shift for aligning the channel midpoints.\nTo address this issue, we propose the method of Scattered Channel Range Integration (SCRI). Formally, SCRI is a straightforward approach, as illustrated in Equation 14:\nX = X s, s =\nXcmax\nt\n(14)\nwhere X denotes the activations with C channels, while s, a vector of length C similar to that used in OS+ [57], is employed to adjust the range of activation values. Xcmax represents a vector composed of the maximum activation values within each channel, and t serves as an adaptive parameter.\nThe SCRI, through meticulous design, can amplify the range of activation values for each channel, reducing discreteness and increasing overlap. However, an excessively large activation range is not conducive to improving the coverage of quantization levels for individual channels. To strike a balance between the activation range and the coverage of quantization levels, identifying the optimal value for s is essential. Specifically, through a forward pass, we use a calibration set to determine the maximum activations values Xcmax. Concurrently, we determine the optimal t using a grid search approach, confining our search within [min(\u0425\u0441\u0442\u0430\u0445), \u0442\u0430\u0445 (\u0425\u0441\u0442\u0430\u0445)]. The optimization criterion during this search is the minimization of the MSE loss, comparing the output of the quantized model block against that of the full-precision model, as follows:\narg min ||F (W, X) \u2212 FQ (W, X; t)||, (15)\nt\nwhere F represents the mapping function for the layers following LayerNorm in temporal attention modules and self-/cross-attention modules in the video diffusion model and Fo denotes the mapping function corresponding quantized module.\nSpecifically, we identify several layers to apply SCRI: the Feed Forward Network (FFN), the projection layer, typically a linear layer or a convolution layer, and the attention layer subsequent to the LayerNorm. Similar to OS+ [57], we implement SCRI by making equivalent transformations to LayerNorm and subsequent layers, which incurs no additional overhead during inference. As depicted in Figure 6, SCRI significantly enhances the coverage of quantization levels by individual channels."}, {"title": "5 EXPERIMENTS", "content": "Video synthesis experiments are conducted on two cutting-edge models, MagicAnimate [60] and AnimateDiff [17], utilizing the TED-talks [48], FS-COCO [11] and COCO Captions [8] datasets. Both the input and output layers are consistently set at an 8-bit representation, whereas all remaining convolutional and linear layers are quantized in accordance with the predetermined target bit-width. To calibrate the models accurately, samples from all time steps of application (25 in this work) are collected to form a calibration set, corresponding to one video consisting of 16 consecutive frames. This process ensures that the models are finely tuned for generating high-quality video sequences, establishing a benchmark in the domain of video synthesis.\nEvaluation Metrics. In our experimental framework, we report the spatiotemporal fidelity of video synthesis using the FID-VID [1] and FVD [52] metrics, combining image quality assessment through Fr\u00e9chet Inception Distance with temporal coherence evaluation using Fr\u00e9chet Video Distance. Following AnimateDiff [18], semantic integrity is evaluated using the CLIP metric [45], which leverages a sophisticated language-image pretraining model to measure the alignment between generated animations and reference imagery. This involves calculating the cosine similarity between CLIP image embeddings of each animation frame and reference images, thereby evaluating domain similarity (Domain). Furthermore, our assessment is enhanced by examining the similarity between the prompt embeddings and individual frames to assess text alignment (Text) and by analyzing the similarity between consecutive frames to evaluate motion smoothness (Smooth). For computational performance, we calculate Bit Operations (BOPs) [3, 19] per video diffusion model during each forward pass, balancing efficiency with processing demands."}, {"title": "5.2 Main Results", "content": "In this section, we apply our quantization method to the task of Human Image Animation, utilizing the MagicAnimate [60] framework on the TED-talks dataset [48]. MagicAnimate uses a motion sequence and a reference image to generate a video clip where the sequence directs the person's movements. We use the plain round-to-nearest Linear Quantization [43], PTQ4DM [47] and Q-Diffusion [33] as our baselines. As shown in table 1, our QVD consistently outperforms other methods by a large margin. Actually, these methods specifically designed for image diffusion, namely PTQ4DM and Q-Diffusion, do not achieve significant performance improvements in video diffusion quantization. On the contrary, at W8A8, our QVD gains a FID-VID reduction of 25.78 ( 75.16 49.38 compared with Q-Diffusion) and a FVD reduction of 205.12 (590.89 385.77 compared with PTQ4DM) on TED-Talks, which is almost lossless. When the bit-width decreases (i.e., W6A8 and W6A6), other methods drop drastically while our QVD maintains the performance to a great extent, merely introducing a 1.56 upswing in FID-VID and a 0.7 upswing in FVD.\nTo demonstrate the superiority and versatility of our QVD, we further conducted experiments on AnimateDiff [18]. AnimateDiff utilizes both a visual modality (image or sketch) and a text prompt as inputs to generate videos. Our quantitative comparison mainly focuses on text alignment, domain similarity, and motion smoothness by employing the CLIP metric. Specifically, for the text alignment, our approach achieves a pioneering breakthrough on the FS-COCO dataset with Text-CLIP, surpassing 29 for the first time. For the domain similarity, Our QVD method leads the Q-Diffusion by 6.51 on the Domain-CLIP at W6A8 when evaluated on the COCO Captions dataset. For the motion smoothness, our method exhibits extraordinary video coherence on W8A8, remarkably achieving a score of only 0.05 lower than that of full precision (Smooth-CLIP)."}, {"title": "5.3 Ablation Studies", "content": "In our ablation studies, we meticulously dissect the impact of each component in our QVD. These experiments are performed using the TED-talks dataset with W8A8 quantization, aiming to elucidate the individual contributions of different components to the overall effectiveness of the model. Linear quantization served as the baseline, adopting the MinMax calibration strategy for weight quantization and the MSE calibration strategy for activation quantization. Our analysis involves examining the effects of SCRI and HTDQ, as detailed in the following.\nOverall Effect of HTDQ and SCRI. Table 3 outlines the collective impact of these components. The baseline model with linear quantization scores an FID-VID of 80.70 and an FVD of 618.33. Incorporating HTDQ remarkably improves these metrics to 61.22 and 483.99, respectively. Similarly, adding SCRI yields an improvement, achieving scores of 67.57 (FID-VID) and 529.70 (FVD). The combination of both HTDQ and SCRI (our complete Magic-animate model) further reduces these scores to 49.38 (FID-VID) and 385.77 (FVD), underlining the benefit of integrating both components.\nTo dissect the influence of HTDQ, we consider two stages in its integration (Table 4): the log quantization and our enhanced HTDQ. The experimental data presented in Table 4 clearly indicates the superiority of our enhanced HTDQ over the log quantization. When comparing the SCRI+Log2 Quant method and that with full SCRI+HTDQ, there is a noticeable improvement in both FID-VID and FVD scores, dropping from 50.43 to 49.38 for FID-VID, and from 398.07 to 385.77 for FVD. This demonstrates that our HTDQ, with its additional refinements, effectively enhances the quality of the quantized animations, offering a more faithful representation compared to the baseline.\nIn our ablation study detailed in Table 5, we compare the impact of different scaling methods on quantization quality. When the shift operation is removed from the OS+ [57] method, we see a modest improvement in both FID-VID (reduced from 51.21 to 50.48) and FVD (lowered from 407.93 to 405.16). Further refining the scale computation in our SCRI method yields even better results, with a notable decrease in FID-VID to 49.38 and FVD to 385.77, demonstrating the effectiveness of our adjustments in scale calculation for quantization.\nTo enhance the efficiency of the search process of SCRI, we investigate the impact of varying the number of frames and time steps on the calibration outcomes. As shown in Table 6, a calibration setup using 16 frame features and 1 time-step achieves a balance between performance and time efficiency. All the experiments are conducted based on this setup."}, {"title": "5.4 Comparison of Visualization Results", "content": "Figure 7 shows the W8A8 qualitative results on FS-COCO and TED-talks datasets. When the reference is sketch (top), Linear Quantization and Q-Diffusion exhibit grid-like artifacts in their prediction results, while QVD successfully eliminates this issue and more closely approximates the performance of a full-precision model. When the reference is the motion video (bottom), Q-Diffusion generates noise and disordered backgrounds in its outputs. Moreover, QVD demonstrates superior detail retention for moving objects, exemplified by the hand of the speaker in the lower right corner."}, {"title": "6 CONCLUSION", "content": "In this work, we explore the application of post-training quantization in video diffusion models. We identify the significance of distinctive features for high-quality video generation and introduce the HTDQ method to preserve the discriminability of temporal features after quantization. Additionally, we observed a severe inter-channel variation issue in video diffusion models. To address this, we proposed the SCRI method to integrate activations across channels, thereby enhancing the performance of the quantized model. Our proposed QVD quantization framework is the first to quantize video diffusion models to 8-bit without significant performance degradation."}, {"title": "A QUANTIZED INFERENCE OF HIDI-TQ", "content": "Here, we provide a detailed formulation and derivation of the quantized inference for the HiDi-TQ. As defined in Equation 1,\n1), (1)\nTemb = clip -log2 Temb - p1092\nwhere sT is a scalar. We denote the quantized temporal feature\nas Thus the dequantized temporal feature can be defined as\nEquation 2:\nTemb = sign(Temb) sT 2 -Temb + \u00df, (2)\nthen, the computation of the quantized time projection layer is\ndepicted as the Equation 3:\nWTTemb = WT Temb\nj=w\nBitShift(W\u2081, -T2mb) + W2 (3)\n= (SW) [sign(Temb) ST2 -Temb + \u00df] + BW\n\u00dfw*\n= (sss) [sign(Temb) ST 2 -Temb] + SB*W*\n=s \u00b7 W12 Temb + W2\n= s BitShift(W\u2081, -T2mb) + W2\nFor brevity, we utilize symmetric weight quantization. The term\nWT signifies the transposed weight matrix, while W represents\nits quantized counterpart. The symbols sw and correspond to\nthe quantizer parameters for weights and activations, respectively.\nThe expression W2 is defined as s \u00b7 \u00df \u00b7 WT. Ultimately, the matrix\nmultiplication operation is approximated by BitShift and addition,\nthereby enhancing the speed of inference and reducing the demand\nfor memory consumption.\nThe temporal feature is actually derived from the repetition\nof the same vector, hence, the sign(Temb) can be simplified to a\nsingle vector, which is then integrated into Was Equation 4:\nW00 W01 Wlw\nW10 W11 W1w sign(Temb)\nW = hlw\nW00S W0151 WowSw\nW10S\nW11S WhwS (4)\nhoso Wnlsl WhwSw\nW W W\nw Whl\nEquation 5 defines the detailed calculation process of the BitShift\noperation,\nj=w\nBitShift(W1,- Tembj,j) = \u03a3 \u03a3 wij Tembj (5)"}, {"title": "B MORE EXPLORATION ON TEMPORAL FEATURES", "content": "Outliers of temporal features. The temporal features exhibit pronounced skewness, with a handful of outliers exceeding the magnitude of regular values by several-fold. A naive approach might involve discarding these outliers to compress the activation range. Motivated by this rationale, we execute an experiment, the specifics of which are delineated in Table 1. The results reveal that truncating even a scant 5% of the largest data values induces a drastic degradation in model performance. In conjunction with prior experiments, this outcome suggests that despite the robustness of outliers to quantization noise, their retention is critical for the integrity of temporal features.\nSkewness in temporal features of different models. To explore whether the extreme distribution of temporal characteristics is a common occurrence, we conduct further analysis on the currently popular video diffusion model, SVD [?]. As shown in Figure 1, we present histograms of additional temporal features. Besides AnimateDiff and MagicAnimate, extreme distributions of temporal features are similarly observed in SVD.\nCombination with TFMQ. TFMQ [?] introduces the Finite set calibration (FSC) for temporal features, assigning quantization parameters to each temporal feature. We attempt to transfer this approach to the HiDi-TQ and conduct experiment on the TED-talks dataset with a setting of W8A8, the results shows that FID-VID decrease to 58.38 while FVD decrease to 420.79. Figure 2 indicates that using FSC for quantization improves the TDScore and compromises the distinctiveness of the temporal features. To analyze the reasons, we plot the temporal features obtained from the two quantization methods. As shown in Figure 3, employing the FSC method in the HiDi-TQ results in the quantized temporal features covering fewer quantization levels.\nThe potential of HiDi-TQ. To explore the potential of HiDi-TQ, we investigate the performance of the model on the TED-talks dataset with varying bits. As shown in Table 3, HiDi-TQ still exhibits no significant performance degradation even at a 5-bit setting."}, {"title": "C MORE IMPLEMENTATION DETAILS", "content": "COCOCaption and FSCOCO are image-text pair datasets where the text describes the contents of the images but lacks descriptions of potential actions within the images. To better utilize these datasets for testing video generation models", "descriptions": "Please add several motion elements to the following prompt", "following": 1.0, "2```json": {"title": null, "content": "Table 1: Results of discarding varying amounts of outliers\nPercentile\nFID-VID\nFVD\n0.85\n306.20\n2791.01\n0.9\n306.57\n2791.53\n0.95\n305.65\n2782.77\nEquation 5 defines the detailed calculation process of the BitShift operation,\nj=w\nBitShift(W1, -Tembi,j) = \u03a3 \u03a3 wij > Tembi,j\nj=0\nTable 3: Results of discarding varying amounts of outliers.\nbits\nFID-VID\nFVD\n5\n51.06\n387.57\n6\n50.93\n384.52\n7\n50.72\n384.56\n8\n49.38\n385.77\nTable 2: Comparison between the original prompts and the enhanced prompts.\nOriginal Prompts\nEnhanced Prompts\nsome elephants and one is by some water\nSome elephants, with one drinking by the water\nA zebra all by itself in the green forest\nA zebra walking alone in the green forest\nA group of people who are standing together\nA group of people standing together, talking and moving around\nthere are two large boats that are in the water\nTwo large boats sailing in the water"}}]}