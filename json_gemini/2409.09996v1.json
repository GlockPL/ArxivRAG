{"title": "FreeMark: A Non-Invasive White-Box Watermarking for Deep Neural Networks", "authors": ["Yuzhang Chen", "Jiangnan Zhu", "Yujie Gu", "Minoru Kuribayashi", "Kouichi Sakurai"], "abstract": "Deep neural networks (DNNs) have achieved significant success in real-world applications. However, safeguarding their intellectual property (IP) remains extremely challenging. Existing DNN watermarking for IP protection often require modifying DNN models, which reduces model performance and limits their practicality. This paper introduces FreeMark, a novel DNN water-marking framework that leverages cryptographic prin-ciples without altering the original host DNN model, thereby avoiding any reduction in model performance. Unlike traditional DNN watermarking methods, FreeMark innovatively generates secret keys from a pre-generated watermark vector and the host model using gradient descent. These secret keys, used to extract watermark from the model's activation values, are securely stored with a trusted third party, enabling reliable watermark extraction from suspect models. Extensive experiments demonstrate that FreeMark effectively resists various watermark re-moval attacks while maintaining high watermark capacity.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural network (DNN) watermarking is a typical approach for protecting the intellectual property of DNN models. It allows the model owner to embed a predetermined watermark into the model and later prove ownership by extracting it. Watermarking techniques are broadly divided into two categories: black-box and white-box, depending on whether the model's internal details are available.\nBlack-box watermarking, which operates without access to the model's internal details, typically relies on backdoor attacks to embed and extract watermarks using specifically designed trigger data, such as pattern-based triggers [1], ODD-based triggers [1], and perturbation-based triggers [2]. However, black-box watermarks tend to be less effective against watermark removal attacks [3].\nIn contrast, white-box watermarking, which has access to the model's internal details, is more robust [4]. In this setting, watermarks are embedded into host DNN models using techniques such as parameter regularization [5], adversarial learning [6], residuals of parameters [7], probability density functions [3], hidden memory states [8], and directly modifying the model's weights [9].\nHowever, all existing DNN watermarking methods require modifying the host models, which significantly reduces model performance and limits their practical-ity. Although efforts have been made to minimize this impact, significant limitations still remain in their real-world applications. Naturally, this raises the question: Is it possible to watermark DNNs without any decline in model accuracy?\nIn this paper, we provide an affirmative answer, demonstrating that DNNs can be watermarked without altering the models. Specifically, we propose a novel watermarking framework, termed FreeMark, which leverages cryptographic principles without modifying the original models, thereby avoiding any reduction in model performance (see Fig. 1). Moreover, our method FreeMark offers three key advantages.\n(i) Non-invasive. FreeMark ingeniously integrates watermark with the intrinsic features of the host model into secret keys stored on a trusted third party (TTP), without making any modifications to the model, thereby eliminating the risk of performance degradation during watermark embedding.\n(ii) Robustness and security. Our FreeMark demonstrates strong resistance to watermark removal attacks, such as pruning and fine-tuning. Even under extreme scenarios, FreeMark accurately distinguishes the host model from suspect models.\n(iii) Efficiency. FreeMark is computationally simple, offering a lightweight and convenient framework for watermark embedding, extraction, and verification."}, {"title": "II. PRELIMINARIES", "content": "The following notations will be used throughout this paper.\n\u2022 b: a watermark vector of length N, i.e., b\u2208 {0,1}N.\n\u2022 b: an extracted watermark vector, also b \u2208 {0,1}N\n\u2022 \u0393: a trigger set for watermarking, which is a subset of the training dataset.\n\u2022 fi: the average activation value vector over the trigger set \u0393 in the l-th layer with M neurons, i.e., f\u2208RM.\n\u2022 (A, d): a pair of secret keys, where A \u2208 RN\u00d7M and d\u2208 RM.\n\u2022 \u03b8: a predetermined threshold, where \u03b8 \u2208 [0, 1]."}, {"title": "III. PROPOSED METHOD", "content": "Our FreeMark method consists of three components: watermark embedding, watermark extraction, and watermark verification, as described below."}, {"title": "A. Watermark Embedding", "content": "Preparation. For a pretrained host model H, the model owner selects a trigger set \u0393 that contains at least one sample from each label in the training data. By using the trigger set \u0393 as input to the host model H, the owner can obtain the average activation value vector f. The model owner also generates a binary watermark vector b of length N.\nDefine a non-linear function \u2206(\u00b7) as\n$\\triangle(z) \\triangleq \\text{Thresholding} \\Big(\\frac{1}{1+e^{-z}}\\Big)$"}, {"title": "Algorithm 1 Generate Secret Key A via Gradient Descent", "content": "Input: Watermark vector b; an auxiliary vector \u03bc; learning rate \u03b7; iteration times T.\nOutput a secret matrix A\nInitialize A(0) with random values and t \u2190 0.\nwhile t < T do\nCalculate the current loss Lt = \u2206(A(t) \u00b7 \u03bc) \u2013 b.\nUpdate A(t) by A(t+1) = A (t) \u2212 \u03b7\u22c5\u2207JA(t)\nend while\nreturn A = A(T)."}, {"title": "where", "content": "Thresholding(z) =\n{\n0 if z < 0.5\n1 if z\u2265 0.5.\nFor a vector x = (x1,...,xn), denote\n\u2206(x) = (\u2206(x1), ..., \u2206(xn)).\nGenerate secret keys. In [5], DeepSigns used a predetermined watermark vector b and a secret matrix A to modify the host model H such that\nb\u2248 \u2206(A \u00b7 f\u00ed)\nvia fine-tuning, by adding an additional term that minimizes the distance between b and \u2206(A\u00b7 f\u00ed) to the overall loss function. Here f' is the activation value vector in the l-th layer of the modified model H'. Similarly, other existing watermarking methods also require modifying the host model, e.g. [3], [6]\u2013[9]. However, all of these DNN watermarking methods that involve modifying the host model reduce the accuracy and limit their applications."}, {"title": "Algorithm 2 Key Generation Algorithm", "content": "Input: Host model H; trigger set \u0413; target layer l; an auxiliary vector \u03bc; threshold \u03b8.\nOutput Secret key (A, d) and scaling factor \u03b1\nCalculate the average activation value f\u2081 over the trigger set \u0393 in the l-th layer of the host model H.\nGenerate secret key A via Algorithm 1.\nCalculate secret key d = af\u0131 \u2013 \u03bc, where a satisfies (1).\nreturn (A, d) and \u03b1."}, {"title": null, "content": "Our motivation is to design a DNN watermarking mechanism that does not require modifying the host models, thereby preserving model accuracy.\nTo that end, we first randomly generate an auxiliary vector \u03bc ~ N(0,1) of length M from the standard normal distribution. Next, by using the watermark b and the auxiliary vector \u00b5, we aim to generate a secret key A\u2208RN\u00d7M such that\nb = \u2206(\u0391\u00b7 \u03bc).\nTo achieve this, we introduce Algorithm 1 via gradient descent method, which can effectively address the non-linear multivariable optimization problem of deriving A. Then we establish another secret key d\u2208 RM by\nd = \u03b1\u03be\u03b9- \u03bc\nwhere a \u2208 R is a scaling factor such that\n||\u2206(A\u00b7 d) \u2013 b||1 \u2265 \u03b8N"}, {"title": null, "content": "to ensure the security, integrity, and robustness of the watermark.\nThe overall secret key generation pipeline is illustrated in Algorithm 2. The generated secret key pair (A, d) such that\nb = \u2206 (A \u00b7 (af\u0131 \u2013 d))\nis uploaded to a TTP, which ensures the secure storage of the secret keys, prevents unauthorized access, and provides a trusted environment for watermark extraction from a suspect model in the later process."}, {"title": "B. Watermark Extraction", "content": "Algorithm 3 demonstrates the watermark extraction procedure from a suspect model, which involves two primary steps conducted by the TTP.\nFirst, by inputting the trigger set \u0393 into the suspect model S, the average activation value vector fi over \u0393 in the 1-th layer of S is obtained. Second, by using the secret key pair (A, d), the TTP computes the extracted watermark information b = \u2206(A\u00b7 (af\u0131 \u2013 d))."}, {"title": "Algorithm 3 Watermark Extraction", "content": "Input: Suspect model S; trigger set \u0413; secret key pair (A, d); target layer l; scaling factor \u03b1.\nOutput Extracted watermark b\nCalculate the average activation value f\u2081 over the trigger set \u0393 in the l-th layer of the suspect model S.\nCalculate b = \u2206(A\u00b7 (af\u0131 \u2013 d)).\nreturn the extracted watermark b."}, {"title": "C. Watermark Verification", "content": "To verify whether the suspect model S is a copy of the host model H, we compare the predetermined watermark b with the extracted watermark b from S using the metric of bit-error-rate (BER), where\nBER =\n||b-b||1\nN"}, {"title": null, "content": "We use a predetermined threshold \u03b8\u2208 [0,1] as a criterion. If BER < \u03b8, the suspect model S is regarded as a copy of the host model H. Conversely, if BER > \u03b8, S is not regarded as a copy of H."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "Setting. We conduct experiments using MNIST [10], CIFAR-10, and CIFAR-100 [11] datasets for image classification. For the host model, we use Lenet-5 [12], VGG16 [13], Resnet20 [14], CNN2, and WRN-28-10 [15].\nWe set the watermark length to N = 512, which indicates a significantly larger watermark capacity than existing white-box DNN watermarking schemes [3], [5].\nSince our method FreeMark does not modify the host model itself, there is no decline in model accuracy during watermarking. Instead, we focus on verifying the following three aspects of the watermarking scheme.\nSecurity. The watermark b can be accurately extracted using the secret key (A, d), while a third party without the secret key cannot accurately extract the watermark.\nIntegrity. The watermark cannot be extracted from a third party's unmarked model, indicating that our FreeMark method has a low false-positive rate.\nRobustness. The watermark can still be correctly extracted even after the host model undergoes watermark removal attacks such as fine-tuning and model pruning."}, {"title": "A. Experiments on Security", "content": "Watermark extraction with correct keys. We conduct experiments to verify that the watermark b can indeed be extracted from the host model H using the correct secret key pair (A, d). As shown in Table I, the BER in this scenario is consistently 0, indicating the security of FreeMark.\nWatermark extraction with forged keys. We verify that the predetermined watermark cannot be correctly extracted from the host model H using forged keys. To do this, we generate forged keys A' and d' using a standard normal distribution. Additionally, we randomly generate 200 forged key pairs for each model to conduct the experiments. The BER results in this scenario are expected to be as high as possible."}, {"title": "B. Experiments on Integrity", "content": "We verify that in FreeMark, watermark information will not be erroneously extracted from a third party's model. To strengthen this argument, we conduct experiments in the following two most extreme scenarios:\n(i) The third party's model has the same architecture, the same tasks, and uses the same training data as the host model, but is trained with different hyperparameters.\n(ii) The third party's model has the same structure, the same tasks, and uses the same training hyperparameters as the host model, but is trained on different data.\nAs shown in Table I, even in the most extreme sce-narios, our method FreeMark can accurately distinguish between the host model and a third party's unmarked model, demonstrating that FreeMark has a low false-positive rate."}, {"title": "C. Experiments on Robustness", "content": "To verify the robustness of FreeMark, we conduct experiments on watermark extraction from the host model subjected to three typical watermark removal attacks, as described below. In this scenario, even if the host model is attacked, the watermark is expected to be correctly extracted.\nFine-tuning. For each model, we fix a portion of the model's weights and fine-tune the remaining weights using the original training data. As shown in Table I, fine-tuning increases the accuracy of all models except for the WRN. Moreover, we can still accurately extract the watermark from the models that underwent fine-tuning.\nPruning. A model pruning attack removes all weights below a threshold \u03b7 from the model. As shown in Table I with \u03b7 = 0.1, the model accuracy decreases, but the watermark can still be correctly extracted.\nFurthermore, Fig. 3 illustrates the model performance for different pruning thresholds. The results indicate that even after substantial performance degradation due to pruning, the watermark can still be accurately extracted with BER = 0.\nOverwriting. A watermark overwriting attack adds multiple watermarks to the model, thereby overriding"}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a novel DNN watermarking approach, termed FreeMark. It ingeniously integrates the predetermined watermark and model characteristics into secret keys stored on a TTP, without modifying the model, thereby preserving its original accuracy. Comprehensive experiments demonstrate that FreeMark exhibits superiority in security, integrity, and robustness against various attacks. This work offers new insights into DNN watermarking and opens up possibilities for watermarking DNNs without model modification."}]}