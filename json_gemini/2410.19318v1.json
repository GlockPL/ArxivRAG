{"title": "TWO ARE BETTER THAN ONE: Context WINDOW EXTENSION WITH MULTI-GRAINED SELF-INJECTION", "authors": ["Wei Han", "Pan Zhou", "Soujanya Poria", "Shuicheng Yan"], "abstract": "The limited context window of contemporary large language models (LLMs) remains a huge barrier to their broader application across various domains. While continual pre-training on long-context data is a straightforward and effective solution, it incurs substantial costs in terms of data acquisition and computational resources. To alleviate this issue, we propose SharedLLM, a novel approach grounded in the design philosophy of multi-grained context compression and query-aware information retrieval. SharedLLM is composed of two short-context LLMs such as LLaMA-2, termed upper model and lower model. The lower model functions as a compressor while the upper model acts as a decoder. The upper model receives compressed, multi-grained context information from the lower model and performs context-aware modeling on the running text. Information transfer between the compressor and decoder occurs only at the lowest layers to refrain from long forward paths in the lower model and redundant cross-attention modules in the upper model. Based on this architecture, we introduce a specialized tree-style data structure to efficiently encode, store and retrieve multi-grained contextual information for text chunks. This structure, combined with a search algorithm, enables rapid encoding and retrieval of relevant information from various levels of the tree based on the input query. This entire process, wherein the sender and receiver are derived from the same LLM layer, is referred to as self-injection. In our evaluation on long-context modeling and understanding tasks, SharedLLM achieves superior or comparable results to several strong baselines, striking an effective balance between efficiency and performance. Meanwhile, with the aforementioned design choices, SharedLLM can greatly reduce memory consumption, and demonstrates substantial speed-ups over other advanced baselines (2\u00d7 over streaming, 3\u00d7 over encoder-decoder architectures). The code is available at https://github.com/Clement25/SharedLLM.", "sections": [{"title": "1 INTRODUCTION", "content": "Since the release of GPT-3 , the rapid advancement of large language models (LLMs) has revolutionized the NLP research community and transformed various workflows. Pretrained on trillions of tokens, LLMs exhibit remarkable abilities, such as completing unfinished text or code and following human instructions to perform designated tasks after minimal supervised fine-tuning .\nDespite their impressive capabilities, several factors limit their broader application. One major constraint is the context window size , which refers to the maximum number of tokens an LLM can process smoothly in a single input. The length of context window is typically set during pretraining-for example, LLaMA and LLaMA-2 have context windows of 2,048 and 4,096"}, {"title": "2 RELATED WORK", "content": "Long-context Language Models. There are two prevalent routines to build LLMs that are capable of processing extremely long text: directly pretraining on large corpus of targeted context length from scratch  or adapting short context-window LLMs to longer context lengths via combined various techniques . The former approach consumes tremendous data and computational resources, while the latter allows for more convenience and flexibility for researchers and developers to explore potential optimization to the default settings . The core idea behind these adaptations is to mimic short input scenarios (i.e., length within the model's text window) when the input length exceeds window size. Attention map manipulation is the most common approach for this goal, which can be realized via positional encoding (PE) rescaling, such as ALiBi , positional interpolation (PI) and YaRN , or positional index rearranging . Both directly or indirectly adjust attention scores to be similar as the short-input scenarios so that the model can handily deal with. Another line of works compress past tokens sequentially into dense representations  as input at the next step or store them in an external retrievable memory  to reduce the input lengths. Yen et al. (2024) utilizes small model such as RoBERTa  for context encoding to boost speed and enable higher parallelism. However, this heterogeneous architecture necessitates meticulous task design for the extra pretraining and warmup stages to stabilize the fine-tuning process. In contrast to these works, our method directly tunes off-the-shelf models to compress context into structural representations for query-aware retrieval. Powered by efficient architecture design and a fast-forwarding mechanism, the whole procedure can be fully paralleled online without excessive memory usage, which greatly cuts down the latency during inference time.\nEfficient Methods for Long-context Modeling. In vanilla self-attention, the space and time complexity grows quadratically (O(L2)) with the input sequence length L, which can cause out-of-memory (OOM) issues on GPU clusters. A straightforward solution is to add parameter efficient fine-tuning (PEFT) modules  to shrink the size of gradient tensors during backward propagation. Many works strive to reduce the memory footprint of attention computation to enhance computational efficiency. Longformer  introduces a hybrid attention pattern to capture local and global semantic features concurrently. Katharopoulos et al. (2020) designs linearized attention that merely demands O(L) space to accomplish attention computation. FlashAttention and PagedAttention  maximize the memory efficiency from system's perspective. More recently, Xiao et al. (2024b) discovers the \u201cattention sink\u201d phenomenon and proposes streaming-llm to address high perplexity issue in generation under window-attention. Our work basically follows the efficient design principle in three aspects: 1) lightweight architecture through lower layer self-injection; 2) compact structural representations via structural information extraction and compression; 3) efficient construction and retrieval algorithm based on the proposed context tree structure."}, {"title": "3 METHOD", "content": "In this section, we first introduce the overall architecture of our proposed SharedLLM in Sec. 3.1, and then elaborate on its two main components, lower model and upper model in Sec. 3.2 and 3.3."}, {"title": "3.1 OVERVIEW", "content": "As illustrated in Figure 1, SharedLLM adopts a hierarchical architecture, akin but not identical to classical encoder-decoder models. The lower model, or the \u201ccompressor\u201d, breaks down the long input context Xc into smaller chunks that can be processed within limited GPU memory. It then uses the same LLM model to compress each context chunk into compact and structured representations in parallel. The upper model, or the \"decoder\u201d, takes the rear part of the input text (the running context, such as questions) as input, then integrates the compressed information from the lower model, and finally predicts future tokens in an auto-regressive manner."}, {"title": "3.2 LOWER MODEL", "content": "The lower model is a small pretrained LLM, implemented as the first M shallow layers of LLaMA-2. It independently encodes and compresses each past context chunk Ci from the set of chunks {Ci}n i=1,\nand constructs a context tree that stores multi-grained information across various levels. The encoding for all chunks {Ci}n i=1 is fully paralleled to boost the speed. Below, we detail the context tree structure and its efficiency-enhanced query-dependent dynamic construction, and the tree search process.\nContext Tree. The motivation to build the context tree is intuitive and problem-driven. Given a text chunk Ci and a task-specific query, the task-related information is often distributed unevenly across the chunk of text. For instance, to summarize a given passage, one should pay more attention to the topic sentences, collect messages from them and rephrase to produce the answer, rather than focuses much on narrative details. Whereas in the task of passkey finding, detailed relations are more important than theme paragraphs. To this end, we aim for the contextual representations to capture fine-grained details for the relevant portions of the text, while encoding only coarse-grained information for the less relevant parts. The tree structure is the best fit to simulate this process: the spltting of nodes resembles splitting larger text chunks into smaller ones, from which we can get more fine-grained information.\nIn the context tree, its root node contains the entire chunk Ci = {xs, ..., xt} where xp (s < p < t) denotes a token, s and t are the start and end index of that chunk; and each other node consists of a sub-sequence of the chunk Ci. Then we introduce how to build the child nodes from a parent node. Specifically, for any non-leaf node that contains l tokens {xu+1,..., xu+1}, at training phase, we split it into two sub-sequences for constructing its left child and right child as:\nCparent = {xu+k}k=1, Cleft = {xu+k}k=1, Cright = {xu+k}k=b+1. (1)\nHere we adopt a random splitting by setting b = [l/2 - ] and ~ N(0, \u03c3\u00b2) where \u03c3 is a predefined hyperparameter, since random lengths can slightly improve the performance as concluded in Zhang et al. (2024a). At test time, the noise is fixed to zero. One can continue this process until arriving at the limited tree depth. Next, building upon this static tree, we construct a more efficient query-dependent dynamic tree.\nQuery-Dependent Dynamic Tree Construction and Search. A task-specific query is typically highly relevant to certain tree nodes while being less relevant to others. For highly relevant nodes, further expansion is necessary to extract fine-grained information. However, for less relevant nodes, expansion is unnecessary. Thus, instead of constructing an entire static context tree as aforementioned, we build a query-dependent dynamic tree that expands only the relevant nodes, as shown in Figure 2, significantly saving both GPU memory and time.\nStarting from the root node, we perform a depth-first splitting and search process. Each node sequence is first split into two subsequences according to Eq. (1). We then use a non-parametric policy \u03c0 to decide the next selected node based on the two subsequences, xleft and xright, and a query sequence y:\n\u03c0((Xleft, Xright), y) \u2192 left or right,\nHere the policy \u03c0 determines whether the left or right child of the node will be selected. The unselected sibling node is marked as \u201cpreserved\u201d and will not be expanded further. Note, the root node is always selected to ensure expansion. For policy \u03c0, it is task-specific. Specifically, regarding language modeling task, since there are no explicit queries (i.e., y = (\u00d8), we simply set \u03c0 to be deterministic:\n\u03c0((Xleft, Xright), y) = right.\nFor instruction-following tasks, such as question-answering, where queries like questions are available, \u03c0 selects the node with higher similarity to the query in the hidden space:\n\u03c0((Xleft, Xright), y) = argmax (sim(h, hy)),\nE {left, right}\nwhere sim(,) represents the cosine similarity between two vectors. The hidden vector h at the last position of a sequence is embedded by either the lower or upper model. Specifically, this involves a short forward pass through one self-attention layer in the lower model for h\u00e6 and the upper model for hy. Once the selected node is determined, the process continues with that node, repeating the procedure until reaching leaf nodes. At this point, both the left and right child are marked as \"preserved\"."}, {"title": "3.3 UPPER MODEL", "content": "The upper model mainly inherits from the LLaMA architecture, which consists of N (32 for LLaMA-2-7B) self-attention layers with slight modifications. As illustrated in Figure 1, for each one of the M shallow layers, we add a cross-attention module on the top of the vanilla self-attention layer for information fusion.\nPosition-aware Cross-attention on the Context Tree. In Section 3.2, we can obtain a sequence of tree-structural representations S\u2032 = {S\u2032 1, ..., S\u2032 n} for n chunks {Ci}n i=1,\nwhere S\u2032 i = {K, V} stands for the representations of chunk Ci. Since the sequence of chunk keys K = {K1, ..., K\u2032 n}\nis produced from ordered chunks {C1, ..., Cn}, their positional information should be aware at chunk level by the query. We assign the following chunk-level positional indices to Q and K:\nPQ = {|XD| , |XD| , ..., |XD| }, PK = { 0, 0, ..., 0, |C1|/\u03b2 , |C1|/\u03b2 , ..., |C1|/\u03b2 , |Cn|/\u03b2 \u2013 1, |Cn|/\u03b2 \u2013 1, ..., |Cn|/\u03b2 \u2013 1}. (2)\nHere we view the upper model's query Q as one chunk and endow with the largest positional index, because Q is encoded from XD which is behind all context chunks Xc in the raw input sequence X. We will show in Section 4.4 that this setting also facilitates chunk-level extrapolation and answer text production in downstream tasks.\nWe then conduct cross attention between the query Q and concatenated KVs to integrate their carried context information into running context for more coherent language modeling:\nO = cross_attn(Q, concat([K1; ...; K\u2032 n]), concat([V1; ...; V\u2032 n])). (3)\nTraining We use the standard language modeling loss during training, which maximizes the log probability of the ground-truth tokens in the target sequences Xtar, conditioned on the context Xc and all preceding tokens x<t from XD:\nL = - log P(xt|Xc;x<t).\nXt\u2208Xtar\nFor language modeling data, Xtar = XD, i.e., the target tokens are all tokens in XD, excluding the first token. For instruction-following data, XD includes both the instruction Xinst and the annotated response Xres. In this case, we set Xtar = Xres, meaning that we optimize only for the response tokens, while the instruction text is masked during loss calculation."}, {"title": "4 CONCLUSION", "content": "In this work, we present SharedLLM, which leverages a self-injection mechanism to adapt a pair of short-context LLMs for efficient long-context modeling. By integrating the operations of context compression and key information retrieval into a dedicated binary-tree structure, SharedLLM excels in language modeling and various downstream instruction-following tasks, while maintaining excellent memory and time efficiency. Besides, SharedLLM is directly trained from off-the-shelf LLMs, eliminating the need for additional feature alignment steps and making implementation easier. We hope this learning paradigm can be generalized to other short-context LLMs, offering a scalable approach for a context-window extension to an arbitrary length.\nLimitations. While SharedLLM demonstrates superior performance on both language modeling and long-context benchmarks, as well as high efficiency in terms of time and memory, there are still some limitations. First, although this work strikes a relatively good balance between efficiency and performance at the model architecture level, further improvements could be achieved by optimizing at the system and hardware levels. Second, while a simple and effective retrieval mechanism is implemented in this work, more advanced retrieval techniques, such as BM25 and Graph-RAG , were not explored and may further enhance performance. We aim to pursue these improvements in future research."}, {"title": "A IMPLEMENTATION DETAILS", "content": ""}, {"title": "A.1 TRAINING CONFIGURATIONS", "content": "We list more training configurations that are not specified in the main text in Table 6. The sequential values of \u03b1 are level-wise compression ratios, from level 1 to level 3."}, {"title": "A.2 ONLINE SPLIT-AND-SEARCH ALGORITHM", "content": "We provide the pseudo code for the online split-and-search algorithm introduced in Section 3.2, from the splitting of root node till collecting all key-value states for all preserved nodes and all M layers. The code snippet in the entire model.py file can be found in the supplementary material."}, {"title": "A.3 DATASET STATISTICS", "content": "Downsampled Redpajama. We follow Yen et al. (2024) and Touvron et al. (2023b) to prepare our training set. The proportions of data regarding seven domains in the resulted training set are listed in Table 7. Note that for books domain we have excluded S3 due to copyright issues, as we highlighted in Section 4.\nMixed Dataset in SFT. This dataset is directly loaded based on the code from Zhang et al. (2024a), which is a mixed version of RedPajama and LongAlpaca (Chen et al., 2024). We follow Zhang et al. (2024a) to only filter samples whose lengths range from 1K to 8K. The distribution of sample lengths is below."}, {"title": "A.4 BASELINE METHODS", "content": "We briefly introduce the baseline methods for comparison in our experiments below.\nStreamingLLM  observes the \u201cattention sink\u201d phenomenon during inference time when inputs are extremely long. The authors then devise a training strategy by prepending a special token as dedicated attention sink during pretraining. The model can inference in a streaming manner by holding a constant KV cache, which greatly cuts down the time and memory cost.\nAutoCompressor  compresses past text segments into smaller and compact sequences. The compressed sequences are then added to the next segments as prefix to continue the forward pass.\nLongAlpaca  introduces a new sparse attention pattern (S2-Attn) to approximate the full-attention performance. They further design a lora-based finetuning strategy to further reduce the memory cost during training time.\nLongLlama is built upon the foundation of OpenLLaMA and fine-tuned using the Focused Transformer (FoT) method , which contrasts the context representations from the same document to those from others. Trained from the CodeLLaMA checkpoint, LongLlama successfully extends the context length from 2K to 256K."}, {"title": "B OVERHEAD ANALYSIS", "content": "In section 4.3, we have explained the outstanding efficiency of our model by comparing the memory usage and inference speed with other competitors. In this section, we give a more comprehensive analysis towards the inherent factors that may impact model's efficiency, including compression ratio B, tree height h, the number of shared layers M and the retrieval-based policy which requires an additional short forward pass.\nWe rerun our experiments to measure the forward time and memory cost from language modeling on 8K tokens, adjusting one variable at a time while keeping others at their default values. The results are shown in Table 9, 10 and 11. Among these factors, the number of injection layers, M, has the most significant impact on both speed and memory: both memory and latency grows as M increases. As an opposite, compression ratio \u03b2 and tree height h produces nuances effect on both metrics. For example, if we decreases \u03b2 from 64 to 1 (preserve all KVs), the inference time increases by 6.7% while memory increases by 3%. A similar trend is observed on experiments with tree height h. We speculate that the reason behind these outcomes are partly from the internal optimization in FlashAttention, which efficiently computes attention blockwisely. When the configuration meets its requirement for block size and hidden dimension (e.g., length is divisible by 256), We further investigate the potential overhead caused by the extra short forward path in query-aware splitting-and-search algorithm. As shown in Table 12, we observe that the extra forwarding incurs around 15% overhead in both time and space. We believe this type of overhead can be further eliminated with more careful optimization to the implementation details."}]}