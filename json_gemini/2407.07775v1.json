{"title": "Mobility VLA: Multimodal Instruction Navigation\nwith Long-Context VLMs and Topological Graphs", "authors": ["Hao-Tien Lewis Chiang", "Zhuo Xu", "Zipeng Fu", "Mithun George Jacob", "Tingnan Zhang", "Tsang-Wei Edward Lee", "Wenhao Yu", "Connor Schenck", "David Rendleman", "Dhruv Shah", "Fei Xia", "Jasmine Hsu", "Jonathan Hoech", "Pete Florence", "Sean Kirmani", "Sumeet Singh", "Vikas Sindhwani", "Carolina Parada", "Chelsea Finn", "Peng Xu", "Sergey Levine", "Jie Tan"], "abstract": "An elusive goal in navigation research is to build an intelligent agent that can understand\nmultimodal instructions including natural language and image, and perform useful\nnavigation. To achieve this, we study a widely useful category of navigation tasks we\ncall Multimodal Instruction Navigation with demonstration Tours (MINT), in which\nthe environment prior is provided through a previously recorded demonstration video.\nRecent advances in Vision Language Models (VLMs) have shown a promising path\nin achieving this goal as it demonstrates capabilities in perceiving and reasoning about\nmultimodal inputs. However, VLMs are typically trained to predict textual output\nand it is an open research question about how to best utilize them in navigation. To\nsolve MINT, we present Mobility VLA, a hierarchical Vision-Language-Action (VLA)\nnavigation policy that combines the environment understanding and common sense\nreasoning power of long-context VLMs and a robust low-level navigation policy based\non topological graphs. The high-level policy consists of a long-context VLM that takes\nthe demonstration tour video and the multimodal user instruction as input to find the\ngoal frame in the tour video. Next, a low-level policy uses the goal frame and an offline\nconstructed topological graph to generate robot actions at every timestep. We evaluated\nMobility VLA in a 836m\u00b2 real world environment and show that Mobility VLA has\na high end-to-end success rates on previously unsolved multimodal instructions such\nas \"Where should I return this?\" while holding a plastic bin.", "sections": [{"title": "1 Introduction", "content": "Robot navigation has come a long way. Early work relied on users specifying physical coordinates in\npre-mapped environments [1, 2, 3, 4, 5, 6, 7]. Object goal and Vision Language navigation (ObjNav\nand VLN) [8, 9, 10, 11, 12, 13, 14] are a giant leap forward in robot usability as they allow the use of\nopen-vocabulary language to define navigation goals, such as \"Go to the couch\". To make robots truly\nuseful and ubiquitous in our daily lives, we propose another leap forward by lifting ObjNav and VLN's\nnatural language space onto the multimodal space, meaning that the robot can accept natural language\nand/or image instructions simultaneously. For example, a person unfamiliar with the building can ask\n\"Where should I return this?\" while holding a plastic bin (Figure 1, upper left), and the robot guides\nthe user to the shelf for returning the box based on verbal and visual context. We call this category of\nnavigation tasks Multimodal Instruction Navigation (MIN).\nMIN is a broad task consisting of environment exploration and instruction guided navigation. However,\nin many scenarios one can bypass exploration by leveraging a demonstration tour video that fully traverses\nthe environment. The demonstration tour has several benefits: 1) It is easy to collect: users can teleoperate\nthe robot or simply record a video on a smartphone while walking in the environment. There also exists\nexploration algorithms [9, 14] that can be used to create the tour. 2) It aligns with user common practice:\nwhen a user gets a new home robot, it is natural for them to show the robot around in their home, and\nthey can verbally introduce locations of interest during the tour. 3) In certain circumstances, restricting\nthe robot's motion in a pre-defined zone is desirable due to safety and privacy purposes [15]. To that end,\nin this paper, we introduce and study this category of tasks called Multimodal Instruction Navigation with\nTours (MINT), which leverages demonstration tours and focuses on fulfilling multimodal user instructions.\nRecently, large Vision-Language Models (VLMs) [16, 17, 18] have shown great potential in solving\nMINT thanks to their impressive capabilities in language and image understanding and common-sense\nreasoning [19], all critical pieces to achieve MINT. However, VLMs alone struggle to solve MINT due to:\n1) The number of input images for many VLMs are highly limited due to context-length limitation. This\nseverely limits the fidelity of environment understanding in large environments. 2) Solving MINT requires\ncomputing robot actions. Queries to solicit such robot actions are typically out-of-distribution from what\nVLMs are (pre)trained with. As a result, the zero-shot navigation performance is often unsatisfactory (we\nshow this in Section 5.3).\nTo solve MINT, we present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigation\npolicy that combines the environment understanding and common sense reasoning power of long-context\nVLMs and a robust low-level navigation policy based on topological graphs. Specifically, the high-level\nVLM uses the demonstration tour video and the multimodal user instruction to find the goal frame in\nthe tour video. Next, a classical low-level policy uses the goal frame and a topological graph (constructed\noffline from tour frames) to generate robot actions (waypoints) at every timestep. The use of long-context\nVLMs addressed the environment understanding fidelity problem, and the topological graph bridges the\ngap between VLM's training distribution and the robot actions required to solve MINT."}, {"title": "2 Related Work", "content": "Classical Navigation Classical navigation methods typically focus on moving the robot from point-to-\npoint, where goals are specified in metric coordinates [20]. These systems usually rely on pre-built or dynam-\nically generated maps and utilize path-planning algorithms like D* [21] to generate fine-grained navigation\ncommands (e.g., twist drive velocity) to achieve collision-free movement. Similar to previous works, the low-\nlevel controller used in this work combines a visual SLAM algorithm, COLMAP [22], and an iLQR based\nModel Predictive Control (MPC) method [23] to track desired waypoints obtained from high-level VLMs.\nObject and Visual Navigation While classical navigation methods typically exhibit robust behavior,\nthey do not leverage semantically meaningful information for specifying navigation targets. In contrast,\nobject and image goal navigation techniques [24, 25, 26] utilize rich input modalities. These include\nobject categories [27, 28, 29, 11], natural language instructions [30, 31], dialogue [32], goal image\nconditions [33, 34], and even multimodal inputs combining language and images [35].\nMost of these approaches involve an active exploration phase because the robot operates without prior\nknowledge of the environment. Our work distinguishes itself by leveraging environment priors provided\nin the form of a previously collected video tour. In this regard, our work shares similarities with [14],\nwhere semantic information is obtained from past explorations, and [36, 37, 38] which utilizes memory\nto improve mapping and planning. However, a key difference lies in the absence of explicit semantic scene\nrepresentation graphs [39, 40] in our approach, thanks to the capabilities of VLMs to process raw videos.\nVision-language models Prior to the emergence of large VLMs, researchers typically needed to\npretrain their own visual representations for navigation tasks [41, 42, 43, 44], although some leveraged\nexisting pretrained multimodal embeddings [35, 14]. Recent breakthroughs in large language models\n(LLMs) [45, 46] and VLMs [17, 18], trained on web-scale data, have paved the way for zero or few-shot\nnavigation capabilities. This potential has been explored in various studies [47, 48, 49, 50], showcasing\nthe diverse applications of LLMs and VLMs in navigation. These models have demonstrated the ability\nto: Provide navigation preferences, e.g., \"stay close to marked pavements\" [48]; Construct high-level\nmotion plans, e.g., \u201cmove past the hallway towards the bedroom\" [49]; Substitute object detectors, i.e.,\nrecognizing landmarks [50]; In some cases directly output trajectories [51]. Our work is most similar\nto [49] in the sense that a large VLM (Gemini Pro 1.5 [17]) is used to generate high-level navigation\nplans for the robot, but differs from the the previous work in that our VLM directly outputs the navigation\ngoal for the low-level controller to consume and generate navigation commands."}, {"title": "3 MINT Problem Formulation", "content": "The MINT task considered in this paper takes as input a demonstration tour video and a multimodal user\ninstruction. The robot must navigate to certain goal location(s) to satisfy the user's instruction.\nUnder this setting, the demonstration tour video consists of a sequence of first-person view image frames\nF = {fi|fi \u2208 RH\u00d7W\u00d73,i = 1,2,...,k} taken during a tour of the environment, where k is the number\nof frames in the video. In addition, optional natural language narratives can be added to certain frames"}, {"title": "4 Mobility VLA", "content": "Mobility VLA is a hierarchical navigation policy (Figure 1) with online and offline components. In\nthe offline phase, a topological graph G was generated from the demonstration tour (N, F). Online,\nthe high-level policy takes the demonstration tour and the multimodal user instruction (d,I) to find the\nnavigation goal frame index g, which is an integer corresponding to a specific frame of the tour. Next,\nthe lower-level policy utilize the topological graph, the current camera observation (O) and g to produce\na waypoint action (a) for the robot to execute at each timestep.\n\\(g=h(F,N,d,I)\\)\n\\(\\pi(\u03b1|O,F,N,d,I)=l(a|G,O,g)\\)\nwhere h and I are the high and low-level policies."}, {"title": "4.1 Demonstration Tour and Offline Topological Graph Generation", "content": "Mobility VLA utilizes a demonstration tour of the environment to solve MINT. This tour can be given\nby a human user via teleoperation, or by simply recording a video on a smartphone while walking in the\nenvironment.\nMobility VLA then constructs a topological graph G = (V, E) offline, where each vertex vi \u2208 V\ncorresponds to the frame fi from the demonstration tour video (F, N). We use COLMAP [22, 52], an\noff-the-shelf structure-from-motion pipeline to determine the approximate 6-Degree-of-Freedom camera\npose for each frame and store it in the vertex (see Section 7.1 for details). Next, a directed edge is added\nto G if the target vertex is \"in front of' the source vertex (less than 90 degrees away from source vertex's\npose) and is within 2m.\nCompared to traditional navigation pipelines (e.g., map the environment, identify traversable areas and\nthen construct a PRM [53]), the topological graph approach significantly simpler as it captures the general\nconnectivity of the environment based on the tour trajectory."}, {"title": "4.2 High-Level Goal Finding with Long-Context Multimodal VLMs", "content": "During online execution, the high-level policy leverages the common sense reasoning ability of VLMs to\nidentify a navigation goal from the demonstration tour that satisfies a wide range of multimodal, colloquial\nand often ambiguous user instructions. To this end, we prepare a prompt P(F,N,d, I) consisting of\ninterleaving text and images. A concrete example of P for the multimodal user instruction \"Where should\nI return this?\""}, {"title": "4.3 Low-level Goal Reaching using Topological Graphs", "content": "Once the goal frame index g is identified by the high-level policy, the low-level policy (Algorithm 1) takes\nover and produces a waypoint action at every timestep.\nAt every timestep, we use a real-time hierarchical visual localization system (described briefly below,\nplease see Section 7.1 for more details) to estimate the pose of the robot T and the closest start vertex\nvs \u2208G using the current camera observation O. This localization system finds k-nearest candidate\nframes in G w.r.t a global descriptor [54], and then computes T through PnP [55]. Next, the shortest path\nS on the topological graph between vs and the goal vertex vg (the vertex corresponding to g) is identified\nby Dijkstra's algorithm (line 9). Finally, the low-level policy returns a waypoint action which is simply\nthe \u0394x, \u0394y, \u0394\u03b8 of the next vertex v\u2081 in S relative to T (line 10)."}, {"title": "5 Experiments", "content": "To demonstrate the performance of Mobility VLA and gain further insights into key designs, we design\nexperiments to answer the following research questions (RQs):\nRQ1: Does Mobility VLA perform well in MINT in the real world?\nRQ2: Does Mobility VLA outperform alternatives thanks to the use of long-context VLM?\nRQ3: Is the topological graph necessary? Can VLMs produce actions directly?\nWe highlight the key experimental setup below and leave details to Section 7.\nEnvironments. We evaluate Mobility VLA for MINT in an real office environment occupied by humans\n(Figure 2a). It is 836m\u00b2 and cluttered with everyday items such as shelves, desks and chairs.\nRobot. We use a wheel-based mobile manipulator (Figure 2b) to evaluate Mobility VLA. The robot uses a\nMPC-based algorithm [23] to execute the waypoint action (\u0394x, \u0394y, \u0394\u03b8 in the robot-centric frame) while\navoiding obstacles.\nDemonstration Tour. We collect the demonstration tour by teleoperating the robot with a gamepad. All\ncorridors are traversed twice from opposite directions. The resulting tour is roughly 16 minutes long (948\nframes @ 1Hz) and we add narratives during the tour \"Temp desk for everyone\" and \"Lewis' desk\" to\nframe 5:28 and 7:14 respectively to enable personalized navigation.\nMultimodal User Instructions. We crowd-sourced 57 user instructions in 4 categories. This includes:\n20 Reasoning-Free (RF), 15 Reasoning-Required (RR), 12 Small Objects (SO), and 10 Multimodal\n(MM) instructions (Examples are in Table 1, full list in Section 7.7). Importantly, \"Reasoning Required\"\ninstructions do not mention the specific object or location the robot needs to navigate to, and the destination\nof Multimodal instructions are nearly impossible to infer without the image modality in the instruction.\nAs far as we know, prior works were not designed for or evaluated against these two categories of tasks,\nand they are the key differentiator between MINT and ObjNav and VLN."}, {"title": "5.1 RQ1: Mobility VLA's robust high end-to-end performance in the wild", "content": "To evaluate Mobility VLA in MINT in the real world, we randomly select 5 user instructions per category\nand evaluate Mobility VLA's performance from 4 random starting poses (location and yaw) that are at\nleast 20 m away. We use Gemini 1.5 Pro [17] as our long-context multimodal VLM.\nHigh end-to-end success rate. Table 2 shows that Mobility VLA has a high end-to-end navigation\nsuccess rate in most user instructions categories, including previously infeasible Reasoning-Required and\nMultimodal instructions. However, the success rate is significantly lower in the Small Object category.\nThis is not unexpected given the limited tour video resolution. Mobility VLA also has a reasonable\nSPL (Success Rate weighted Path Length), indicating that the topological graph does not incur a high\npath length penalty. Lastly, Mobility VLA successfully incorporated the personalization narratives in the\ndemonstration tour. It correctly navigated to different locations when responding to essentially the same\ninstructions, but from different users (moved to frame 7:14 when asked \u201cI'm Lewis, take me to a temp\ndesk please.\" and moved to frame 5:28 when asked \"Hi robot, I'm visiting, can you take me to a temp\ndesk?\"). See the supplementary video for examples.\nRobust low-level goal reaching. Table 2 also shows the robustness of Mobility VLA's low-level goal\nreaching policy (100% success rate) in the real world, with the demonstration tour recorded months prior\nto experiments when many objects, furniture, and lighting conditions had been different."}, {"title": "5.2 RQ2: Long-context VLM outperforms alternatives on high level goal finding", "content": "We investigate how well alternative methods perform compared to Mobility VLA to answer whether using\nlong-context multimodal VLMs is the key to solve MINT. Concretely, we compare the following baselines:\nCLIP-based retrieval: We reproduce the high-level goal finding module of NLMap [14] by\nadopting OWL-ViT [57] for region proposal and CLIP [58] for sub-regions and full-images\nembeddings extraction for tour frames. We then perform goal frame retrieval using CLIP\nembeddings of the instruction language and image. State-of-the-art work like ESC [26],\nZSON [35], and CLIP-on-Wheels [59] also fall into the category of this baseline.\nText-Only Mobility VLA: Similar to [60], where the multimodal demonstration tour is captioned\nby a VLM frame-by-frame to form a \"text tour\". An LLM (Gemini 1.5 Pro [16]) then uses the\ntext tour to produce the goal frame index.\nMobility VLA outperforms comparisons. Table 3 shows that high-level goal finding success rates of\nMobility VLA are significantly higher than comparison methods. Given the 100% low-level success rate,\nthis high-level goal finding success rates are representative of end-to-end success rates.\nProcessing high frame rate tour videos with long-context VLMs is critical for success. Feeding a\nfull demonstration tour of a large environment into non-long-context VLMs is challenging since each"}, {"title": "5.3 RQ3: Topological graphs are critical for success", "content": "Mobility VLA uses a hierarchical architecture to harness long-context VLM's reasoning capability and\nuses a topological graph to produce waypoint actions. Is this necessary? Can we prompt the VLM to\noutput waypoint actions directly?\nTopological graphs are critical for navigation success. Table 5 shows the end-to-end performance of\nMobility VLA in simulation compared to prompting the VLM to output waypoint actions directly (prompt\nand details in Section 7.6). The 0% end-to-end success rate shows that Gemini 1.5 Pro is incapable of\nnavigating the robot zero-shot w/o the topological graph. Empirically, we found that Gemini almost always\noutputs the \u201cmove forward\u201d waypoint action regardless of the current camera observation. In addition, the\ncurrent Gemini 1.5 API requires the upload of all 948 tour images at every inference call, resulting in a\nprohibitively expensive 26s per-step running time for the robot to move just 1m. On the other hand, Mobility"}, {"title": "6 Discussion", "content": "In this paper, we present Mobility VLA, a new paradigm of navigation policy for solving MINT.\nMobility VLA achieved 86% and 90% end-to-end success rates on previously infeasible navigation tasks\ninvolving complex reasoning and multimodal user instructions in a large real world environment. We also\ndemonstrated a leap forward in how easily users can interact with the robot, where a user records a video\nwalkthrough in a home environment with a smartphone and then asks \u201cWhere did I leave my coaster?\"\nLimitation: Lack of exploration. The current version of Mobility VLA relies on a demonstration tour, and\ndoes not explore the environment automatically. However, existing exploration mechanisms such as frontier\nexploration or diffusion-based exploration [43] can be easily integrated during the demonstration tour.\nLimitation: Long VLM inference time impedes natural user interactions. The inference time of\nhigh-level VLMs is round 10-30 seconds, resulting in users awkwardly waiting for the robot to respond.\nHowever, it is possible to cache the demonstration tour, which takes up roughly 99.9% of the input tokens,\nin order to significantly improve inference speed.\nFuture Work. Mobility VLA can be easily deployed on different robot embodiments as the onboard\ncompute requirement is light (VLMs run on clouds) and only requires RGB camera observations. We plan\nto deploy Mobility VLA on more embodiments. In addition, we found preliminary evidence that Gemini 1.5\nPro is capable of generating high-level action plans to fulfill compound user multimodal instructions beyond\nnavigation such as \u201cDo they have my favorite drink today?\" from a user with lots of Coke cans on their desk\nasks. It knows that the robot should navigate to the fridge, inspect if there are Cokes, and then return to the\nuser to report the result. We included this result in the supplementary video and plan to investigate further."}, {"title": "7 Appendix", "content": ""}, {"title": "7.1 Structure-from-Motion and Hierarchical Localization", "content": "We use COLMAP [22], an off-the-shelf structure-from-motion pipeline to estimate the pose of the robot\nfor each frame in the tour (i.e. reference images), 3D point landmarks in the environment (see Figure 4)\nand their corresponding 2D projections across all reference images (i.e. 2D-3D correspondences).\nThe poses are used to build a fully connected topological graph. The tour frames F, 3D landmarks and 2D\nfeatures are used in our own implementation of a real-time hierarchical localizer. The method is hierarchical\nsince it divides localization of the observed image O into two steps: a global search to determine a set\nof candidate reference images close to O followed by local feature matching and pose estimation.\nIn the global search, the candidate set C C F of k-nearest (w.r.t. the 12-norm of a global image\ndescriptor [54]) tour frames to O is determined. 2D features [62] in O are matched to the 2D features of\neach frame in C. Using the pre-computed 2D-3D correspondences, we establish correspondences between\n2D features in O and 3D landmarks observed in the tour.\nGiven the set of 2D-3D correspondences for each frame in C, the pose of O is computed by solving the\ncorresponding Perspective-n-Point problem [55]. The pose with the most inlier 2D-3D correspondences\nis selected as To.\nWhen To is used to determine the closest vertex on G, the scale-ambiguity characteristic of monocular\nstructure-from-motion systems is inconsequential to the high-level goal-finding policy. However, when\ncomputing the waypoint action for low-level navigation (see Algorithm 1), the scale factor is utilized to\ngenerate metrically accurate actions."}, {"title": "7.2 Home-Like Environment Experiment Setup", "content": "A handheld Pixel 6 smartphone tour of a home-like environment was collected. The tour\nis 75 seconds long and contains 224 frames (3 Hz).\nOnce the topological graph was built from the tour, Mobility VLA was evaluated end-to-end with 5\ninstructions and 4 random start points.\nEven though the images from the camera are significantly different (see Figure 7) from the robot's camera\nwith a collection trajectory independent of robot motion, we achieved 100% success rate with an SPL\nof 0.87."}, {"title": "7.3 Additional Experiments", "content": "We also investigate if strictly multimodal user instructions (instructions that are nearly impossible to answer\nwithout the image) can be answered by the text modality alone. To this end, we replace the image part of\nthe multimodal user instructions with its caption.\nMultimodal user instructions requires multimodal demo tour and image instructions.\nthat the success rate is much higher when multimodal demo tour and image instructions are fed to the\nVLM (lower right corner). Replacing the image with its caption significantly reducess success rate."}, {"title": "7.4 Large-Scale end-to-end sim details", "content": "To further investigate Mobility VLA's end-to-end performance, we use simulations to conduct large-scale\nevaluations. To that end, we created a high fidelity reconstruction of the Office environment in simulation\nusing NeRF\nThe Office environment was reconstructed in simulation using ZipNeRF [63]. To accomplish this, we\ncapture 3,244 images of the office environment using a Sony A7 IV camera with a Rokinon 12mm\nwide-angle lens. The capture is taken uniformly along the walkways of the office facing in all directions.\nCOLMAP [22] is then used to determine the image poses and camera intrinsics. We train a NEural\nRadiance Field (NeRF) using the technique described in ZipNeRF [63]. This NeRF is used to derive\nthe assets needed for simulation. To generate the collision mesh, we distill a mesh from the NeRF using\nthe technique described in [64]. We then use the distilled mesh as a reference to construct the collision\nmesh by hand from mesh primitives using Blender. To render realistic camera images, we align the NeRF\nwith the simulated environment and render using the same camera intrinsics as the real robot's cameras.\nThis ensures that the image distributions of the simulator and the real environment match as closely as\npossible. Figure 8 shows a side-by-side comparison of a simulated render using NeRF and an image from\nthe robot's camera at the same location in the real environment.\nWe then evaluate Mobility VLA in simulation by randomly picking 10 instructions in both Reasoning-Free\nand Reasoning-Required categories each with 50 random starting poses. Unfortunately we cannot directly\nuser instructions in the real Office environment as the NeRF reconstruction was done a few months prior\nfrom when the demonstration tour in real was collected. We omit the Small Objects and Multimodal\ninstructions due to limited visual fidelity and challenges in including humans in NeRF"}, {"title": "7.5 Full VLM intermediate output in Figure 3", "content": ""}, {"title": "7.6 Prompt for direct VLM waypoint actions output", "content": "You are a robot operating in a building and your task is to respond to the user\ncommand about going to a specific location by finding the closest frame in the\ntour video to navigate to.\nThese frames are from the tour of the building last year.\n[Frame 1 Image f1]\nFrame 1. (Frame narrative n\u2081]\n[Frame k Image fk]\nFrame k. (Frame narrative nk]\nThis image is what you see now. You may or may not see the user in this image.\n[Image Instruction I]\nThe user says: Where should I return this?\nCould you select and answer the most appropriate action to take now among"}, {"title": "7.7 User Instructions tested in the Office environment", "content": "Reasoning-Free Instructions.\n1. Can you take me to the building map?\n2. Where can I find a ladder?\n3. Take me to the exit\n4. Where can I find some paper cups?\n5. gray trash can.\n6. Take me to robot number 109.\n7. Take me to a blue area.\n8. I want to borrow my friend's scooter, can you take me to it?\n9. Take me to a conference room with a double door.\n10. I need a tripod, where can I find it in this office?\n11. Take me to a whiteboard.\n12. Where are the gray cabinets again?\n13. I heard there's a cool dark-backgrounded poster, where is it?\n14. where can I find a long wooden bench?\n15. Take me to a two-paned door\n16. I'm Lewis, take me to a temp desk please.\n17. Hi robot, I'm visiting, can you take me to a temp desk?\n18. Take me to a white shelf\n19. Take me to a plant\n20. where can I find a moving box?\nReasoning-Required Instructions.\n1. There is a fire, where should I find tools to fight the fire?\n2. I'm thirsty.\n3. I'm here to water things, please guide take me to them.\n4. Help me dispose of this cardboard box.\n5. Take me to a room with a closed door.\n6. I want to store something out of sight from public eyes. Where should I go?\n7. I left my drink on a cart, can you take me to it?\n8. Can you take me upstairs?\n9. I need to charge my phone, please help.\n10. I heard there is a place to see lots of robots?\n11. I need to sit down.\n12. Can you take me somewhere to lie down?\n13. Where can I find something cold?\n14. I'm tired. Where can I rest?\n15. I want to draw something."}, {"title": "Small Objects Instructions.", "content": "1. Where is the Jackery portable power station?\n2. where is the bench with a bag of chips on it?\n3. where can I find a fire extinguisher?\n4. Where can I borrow a hand sanitizer?\n5. I heard there is a cute tiny traffic cone, where is it?\n6. I need a xbox controller.\n7. Did you see my white water bottle?\n8. Where can I find a fire alarm switch?\n9. Can you help me find my cat mask?\n10. My friend told me to get his moving box under his desk, can you help me find it?\n11. take me to the tombstone I heard so much about.\n12. where can I find a toy cart?\nMultimodal Instructions. See Table 10 below."}]}