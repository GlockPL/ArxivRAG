{"title": "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "authors": ["Melissa Roemmele", "Andrew S. Gordon"], "abstract": "LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper we consider LLMs as authors of commonsense assessment items. We prompt LLMs to generate items in the style of a prominent benchmark for commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine the outcome according to analyses facilitated by the LLMs and human annotation. We find that LLMs that succeed in answering the original COPA benchmark are also more successful in authoring their own items.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) can perform complex writing tasks ranging from paraphrasing sentences to composing long-form stories. Because success on these open-ended authoring tasks is hard to measure quantitatively, NLP researchers tend to focus on more constrained tasks when judging the abilities of LLMs. Many of these assessment tasks, or benchmarks, are conceptually connected with capabilities relevant to authoring, but they use a specific data design to support quantitative evaluation. For instance, assessments of commonsense reasoning often involve multiple-choice question answering, and accuracy on these questions is a proxy indicator for a model's ability to write coherently. New LLMs have excelled on these types of evaluations, but how this success relates to the LLMs' authoring capabilities is still unclear.\nCreating these assessments is itself a skilled writing task, which up to this point has been performed by human authors. Given their success in answering assessments, in this paper we examine whether LLMs can author assessment items as well. We prompt LLMs to generate items in the style of one well-known benchmark for commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine the outcome in terms of the LLMS' responses to their own generated items as well to what degree the items meet the benchmark authoring standards as judged by human raters. We show that an LLM's authoring success is associated with its success in answering the original benchmark."}, {"title": "2 COPA", "content": "The Choice of Plausible Alternatives (COPA) (Roemmele et al., 2011) is an English-language benchmark that assesses the task of commonsense causal reasoning. As shown by the examples in Table 1, each item in COPA consists of a premise, a question specifying a causal direction (forwards or backwards), and two alternatives. One alternative is considered more plausible than the other with regard to the question. For the forwards direction, the question elicits the alternative that is the more plausible effect (result) of the premise, whereas for the backwards direction the question elicits the more plausible cause of the premise. Human performance on this benchmark is considered 100%. The order of the alternatives is balanced across the test set such that random guessing yields 50% accuracy.\nFrom an authoring perspective, creating a COPA item involves writing a collection of one-sentence narrations of events. The author writes two events that have a clear cause-and-effect relation, which respectively become the premise and the more plausible alternative (mpa). The author also writes an event that does not have a clear causal relation to the premise, and this becomes the less plausible alternative (lpa). As described in Roemmele et al. (2011), composing the lpa is particularly challenging. This is because the benchmark is intended to assess models' ability to isolate causal relations in text separate from more generic associations that are captured by simple lexical co-occurence statistics. The author is expected to write an lpa that has some semantic and/or temporal relation to the premise, but no salient causal relation.\nUntil only recently, COPA was considered a difficult benchmark. When first presented as a shared task at SemEval-2012, submitted systems achieved accuracy in the 60-65% range (Gordon et al., 2012). When COPA was incorporated into the well-known SuperGLUE ensemble of benchmarks in 2019, the best system reached almost 85% (Wang et al., 2019). Today some LLMs obtain near-perfect accuracy, as we observe in this work. This reflects the life cycle of most benchmarks, where existing ones are continuously replaced by harder new ones once maximal performance is obtained."}, {"title": "3 Research Questions", "content": "In this work, we apply a common LLM interaction paradigm (prompting with exemplars) to facilitate LLMs to generate COPA-style items. Moving forward in this paper, we refer to the original COPA items as Orig-COPA and the LLM-authored items as Gen-COPA. When existing research refers to the COPA task, it means the task of predicting the answer (i.e. more plausible alternative) to each item. Here, we more specifically refer to this task as answering COPA, in order to distinguish it from the task of generating COPA items.\nWe are interested in the following research questions. First, can LLMs author items with the design of COPA? In particular, how often do Gen-COPA items meet the same authoring standards as the original benchmark? Second, when an LLM produces its own Gen-COPA set, does it then answer its own items correctly? In other words, does the LLM behave consistently during generation and answering in what it deems as the more plausible alternative? Third, how does an LLM's success in authoring Gen-COPA items relate to its ability to correctly answer Orig-COPA items? Are LLMs that do well on Orig-COPA also better at authoring new COPA items?"}, {"title": "4 Selected LLMs", "content": "To investigate these questions, we employ a variety of notable open-source LLMs. We consider eleven models from six different families, where models within the same family are distinguished by size (number of parameters).\nBLOOM-7B & BLOOM-176B: The Big-Science Large Open-science Open-access Multi-lingual Language Model (BLOOM) family was developed through a large cross-team research collaboration (Workshop et al., 2022). It was trained on a collection of publicly available datasets that together comprise 1.61 terabytes of text spanning a wide variety of natural and programming languages (Lauren\u00e7on et al., 2022). Its performance on various benchmarks in SuperGLUE was competitive with other open-source models.\nFALCON-7B & FALCON-40B: The FALCON family (Almazrouei et al., 2023) developed by the Technology Innovation Institute was trained on 1-1.5 trillion tokens, primarily the RefinedWeb corpus derived from CommonCrawl (Penedo et al., 2023). The 40B-sized model obtained top performance on several benchmarks upon its release.\nLLAMA2-7B, LLAMA2-13\u0432, &\nLLAMA2-70B: The LLAMA2 family developed by Meta was trained on a collection of publicly available datasets comprising 2 million tokens (Touvron et al., 2023). It has outperformed FALCON and MPT (below) on several benchmarks.\nMISTRAL-7B & MISTRAL-7B (Jiang et al., 2023), developed by Mistral AI, was trained on open web data. Upon its release, it was presented as a key competitor of the LLAMA2 models. It outperformed the much larger LLAMA2-13B on various knowledge and reasoning benchmarks.\nMPT-7B & MPT-30B: The MosaicML Pre-trained Transformer (MPT) family developed by MosaicML was trained on 1 billion tokens from a mix of web text and curated data (MosaicML NLP Team, 2023). It has performed favorably compared with other open-source LLMs on benchmarks including COPA.\nPHI-2: PHI-2 (Javaheripi and Bubeck, 2023), developed by Microsoft, is a 2.7B parameter model"}, {"title": "5 Orig-COPA Answering Performance", "content": "We first consider how well these LLMs answer the original COPA benchmark. We used a few-shot prompting approach to elicit answers for COPA items. In particular, we selected four items from the COPA development set as task exemplars to include in all prompts for the 500 items in the test set. Each exemplar ends with a sentence specifying the numerical label of the correct (more plausible) alternative, and the target item includes the prefix of this sentence in order to elicit a corresponding prediction from the model.\nWe applied the LLMs outlined in Section 4 to these prompts for the COPA test set. We used greedy decoding (i.e. selecting the maximum probability token at each generation step) to ensure the predictions were deterministic. We generated outputs with a maximum length of four tokens, then post-processed each output with a regular expression to detect the presence of \u201c1\u201d or \u201c2\u201d as the predicted answer. Our policy was to randomly select one of these numbers if neither was present in the output, but in our experiments this was not applied since all outputs contained a 1 or 2.\nTable 3 shows the accuracy of these predictions according to the benchmark. For families with models of varying sizes, the larger models outperform the smaller ones within each family, which is an expected pattern (e.g. Kaplan et al., 2020). However, size does not fully account for the overall ranking of model performance. For example, BLOOM-176B is the largest model but ranks only the third lowest in accuracy, and PHI-2 is the smallest model but ranks the third highest.\nEven with the wide spread in performance between different models, the near-perfect accuracy from LLAMA2-70B confirms that COPA is no longer as challenging as it previously was and will soon become outdated. We note that because the COPA test set is available online, it is possible the LLMs observed these items during training. While we expect that developers of these LLMs adhered to the standard best practice of excluding test data from training sets, we cannot strictly assume this."}, {"title": "6 LLM Authoring of Gen-COPA", "content": "Next, we examined to what degree these LLMs can generate their own items in the design of COPA. We again used a few-shot prompting approach to facilitate this task. Each prompt consisted of three items of one particular causal direction (forwards or backwards) randomly sampled from the COPA development set. Table 4 shows the format: instead of using the labels \u201cAlternative 1\u201d and \u201cAlternative 2\" as in the answering task, each exemplar directly refers to the \"More Plausible Alternative\" and \"Less Plausible Alternative\u201d for a given premise and question, and the models are expected to generate segments with these same identifiers.\nWe assembled 500 prompts for each causal direction, so each LLM was run on 1000 prompts total. Each of these prompts had a unique set of exemplars. To further promote diversity in the outputs, we used random sampling during decoding, in particular top-p (nucleus) sampling with p=0.9 and temperature=1.0. We generated outputs with a maximum length of 200 tokens.\nWe parsed each LLM output with the template shown in Table 5, which captures the premise, more plausible alternative (mpa), and less plausible alternative (lpa) segments for each item of a predefined direction. We refer to these resulting variables for a single output as a schema. We automatically classified an output as failed if it could not be parsed according to the template or if at least one segment in the item exactly matched another one (e.g. the two alternatives were the same). We also considered whether the generated outputs were duplicates of Orig-COPA items. We failed any output whose tokens were all contained in a single item from the Orig-COPA dev or test set.\n6.1\nConsistency\nOur first analysis of the resulting Gen-COPA items assessed the LLMs' consistency between generation and answering. In particular, when an LLM is presented with an item that the LLM itself generated, is the alternative it predicts as more plausible the same one it originally authored as the mpa? To determine this, we transformed each LLM's passable schemas into the design of the benchmark items by randomly assigning the labels of \u201cAlternative 1\u201d and \u201cAlternative 2\" to the mpa and lpa in each schema. We ensured each resulting Gen-COPA set was balanced such that always guessing Alternative 1 as the answer yielded 50% accuracy (or trivially above 50% for sets with an odd number of items). We used the same 4-shot prompt design we applied to Orig-COPA (Section 5) in order to elicit answers from an LLM for its own Gen-COPA set. We quantify consistency as the LLM's answering accuracy on this set: it measures how often the LLM's predicted answer for an item is the same alternative that LLM originally designated as the mpa in the generated schema for that item.\nTable 6 shows that consistency varies widely between different LLMs. The least consistent models"}, {"title": "6.2 Validity", "content": "The consistency analysis does not indicate whether the Gen-COPA items are valid according to a standard other than the LLM itself. Here, we consider an item valid if the alternative deemed more plausible by a consensus of humans is the same as the mpa in the generated schema. This is analogous to the development of the original COPA benchmark, where an item was considered valid if two human judges both selected the answer designated as the mpa by the author of the item.\nTo assess the validity of the Gen-COPA items, we employed a two-stage process. In the first stage, an internally employed expert rater reviewed each schema to judge if the mpa was indeed more plausible than the lpa. If so, the rater marked the schema as conditionally-valid, otherwise they marked it as invalid. We then set aside the invalid schemas and converted the conditionally-valid schemas to items with the randomized \"Alternative 1\" and \"Alternative 2\" labels. We presented each of these items to two external raters on the Prolific data collection platform. The raters observed the premise, question, and alternatives for each item and selected from two options indicating either Alternative 1 or 2 as the more plausible answer to the question.\nUltimately, an item was considered fully-valid if both Prolific raters selected the alternative that was also designated as the mpa in the schema for that item. Items where raters disagreed on the answer were marked as invalid. Each rater responded to 50 items and was paid $6 for an expected completion time of no more than 30 minutes.\nWe randomly sampled 300 schemas per LLM for this validity assessment, a total of 3300 items. The expert rater flagged a few items to be withheld from the external raters due to potentially offensive content. We resampled a new item to replace each of these (0-5 per LLM, as shown in Table 7). 1033 items were classified as conditionally-valid in the first stage of annotation. In the second stage, the two raters agreed on the mpa for 914 of them. Their agreement in terms of Cohen's $\\kappa$ was .79, indicating substantial agreement.\nTable 7 shows the proportion of items for each LLM that were categorized as valid through this process. The validity rate varies significantly between models. The least successful LLM is BLOOM-7B, with only 10% of items marked valid, while LLAMA2-70B has the most success with \u224846% of its items marked valid. Just like consistency, validity is strongly correlated with answering accuracy on Orig-COPA ($r_s$ = .87, p < .001). Thus, models that perform well on the original benchmark are more likely to generate valid items."}, {"title": "6.3 Gen-COPA Answering Performance", "content": "The consistency results in Section 6.1 are affected by both the LLM's answering performance as well as the quality of its Gen-COPA items. For instance, BLOOM-7B has the lowest consistency but also the lowest Gen-COPA validity. Its poor performance in answering its own items might be due to how often the mpa is invalid. We considered whether the results would differ if we isolated only the valid Gen-COPA items and measured the LLMs' answering accuracy specifically on these sets.\nGiven the valid schemas for a single LLM, we used the same process described in Section 6.1 to randomly map the mpa and lpa in each schema to the numerical labels, balancing them across the set so that always selecting Alternative 1 would yield 50% accuracy. For this analysis, we ensured an even number of items in each set, so we downsampled one item in sets with an odd number of valid Gen-COPA items. We then applied the same 4-shot prompt format to these items that we used for the experiments in Section 5 and Section 6.1.\nTable 9 shows each LLM's answering accuracy on all valid Gen-COPA sets. The bolded numbers on the diagonal indicate each model's consistency on its own items. For all eleven Gen-COPA sets, the accuracies obtained by the LLMs on the set are strongly correlated with their accuracies on the Orig-COPA test set ($r_s$ \u2265 .89, p < .001 for all columns of Table 9). This shows that LLMs' answering performance on COPA also generalizes to LLM-generated versions of COPA. Moreover, the fact that the models that do particularly well on Orig-COPA (i.e. LLAMA2-70B, MISTRAL-7B, and PHI-2) also do well on answering Gen-COPA suggests their success on the former is not just an illusion resulting from rote memorization of the Orig-COPA test set. Since we verified the Gen-COPA items are not duplicates of Orig-COPA items, we know the answers to the Gen-COPA items have not been memorized by the LLMs during training.\nAccuracy on the valid Gen-COPA sets tends to be higher for all LLMs compared with their accuracy on Orig-COPA. In particular, examining the final column in Table 9 where the results are aggregated over all Gen-COPA sets, the accuracy of each LLM is between 1 and \u224811 percentage points higher on Gen-COPA than on Orig-COPA, with exception to BLOOM-7B whose accuracy is low on both groups of items. This suggests that the valid subset of Gen-COPA items is easier to answer than Orig-COPA. This may reflect the mechanism behind model collapse, where synthetically generated data contains only the most common patterns in the original data distribution, losing the diverse signal in the distribution tail (Shumailov et al., 2024).\nThese results also confirm that LLMs do not necessarily answer their own Gen-COPA items consistently even if the items are valid. For example, FALCON-7B performs exactly at the level of random guessing on its own valid Gen-COPA set. This finding aligns with increasing evidence demonstrating LLM inconsistency in question answering performance: in particular, changing the order and/or"}, {"title": "6.4 Composition Quality", "content": "Just like Orig-COPA items, valid Gen-COPA items each have an agreed-upon alternative that is considered the correct answer. However, even items with a correct answer may not meet all composition quality standards reflected in the original benchmark. As with most benchmarks, while there are some explicit authoring guidelines for COPA, many of the quality standards are only implicitly defined. Thus, we enlisted one of the authors of Orig-COPA items to assess the intrinsic authoring quality of the valid Gen-COPA items. A item rated as high-quality is one the rater deems they would accept as an additional item in the benchmark.\nTable 10 shows the proportion of items categorized as high-quality among the set of valid Gen-COPA items produced by each LLM. Though the association is more moderate than that of validity, the rate of high-quality items is also correlated with answering accuracy on Orig-COPA ($r_s$ = .76, p = .007). This again suggests that LLMs that correctly answer the original benchmark also have better authoring ability.\nTo illuminate the characteristics underlying these ratings, Table 11 describes and exemplifies the most common reasons that an item failed to receive a high-quality rating, while Table 12 lists examples of items that were marked as high-quality. Notably, in these high-quality items, the lpa is highly related to the premise, but its causal relation is unclear compared with the mpa. Consequently, these items are challenging to answer without discerning causal relatedness separately from coarse semantic relatedness. This is a prominent feature of Orig-COPA items and was a key reason the benchmark remained unbeaten for so long. All Gen-COPA items with their validity and quality annotations are publicly available for further analysis."}, {"title": "7 Related Work and Outlook", "content": "Researchers are increasingly turning to LLMs to replace human effort in developing and evaluating NLP systems. In particular, LLMs are being used to synthesize labeled data in order to train or fine-tune models (Choi et al., 2024; He et al., 2022; Li et al., 2023). LLMs are also being applied to score the output quality of other models (Chiang and Lee, 2023; Kocmi and Federmann, 2023; Wang et al., 2023). Our work aligns with the above endeavors in exploring the use of LLMs as an alternative to human authoring of evaluation items.\nOur particular goal in using LLMs for this assessment authoring is to better understand their capabilities, rather than to produce a novel benchmark that is valuable for evaluating future LLMs. However, there are some emerging demonstrations of using LLMs to facilitate the creation of evaluation data that is unique in its design and scope. For instance, Anthropic (2023) described prompting an LLM to derive multiple-choice Q&A sets from short sections of text, which are then used to evaluate the same LLM's ability to answer those questions when the relevant information is provided in one long document. As another example, Tian et al. (2024) utilized an LLM as an interactive tool combined with human feedback in the authoring process for a novel challenge set focused on physical problem-solving.\nGiven the short lifecycle of benchmarks, rapid creation of new ones is pivotal to capturing further progress in NLP. Additionally, because they do not serve a broader purpose outside of research, the authoring standards for benchmarks are not always clearly understood by the human authors tasked with their creation, and these standards are not necessarily well-anchored to other real-world writing tasks. The current paradigm of LLM interaction via few-shot prompting suggests that LLMs can perform sophisticated authoring tasks in the absence of explicit guidelines just by observing representative examples. As current benchmarks quickly age and new benchmarks become more complex in what they aim to measure, LLMs are likely to take on a sanctioned role in their development."}, {"title": "8 Conclusion", "content": "This paper looks at a notable assessment of commonsense reasoning, COPA, as an authoring task performed by LLMs. The results indicate that models that answer COPA items correctly (both LLM-authored and human-authored ones) are also better at writing COPA items. In future work, we will investigate how this extends to other benchmarks.\nOur work bears upon the trend of widening generalizability of NLP models across tasks. Previously, models designed for text generation were not considered directly applicable to commonsense question answering benchmarks, because this answering task was presumed to require a distinct model architecture supporting answer label prediction. This is no longer a constraint in the new paradigm of LLMs. Increasingly, we are observing models demonstrate the same core knowledge across highly diverse task representations."}, {"title": "Limitations", "content": "The current paradigm of LLMs has some key limitations that are reflected in this work. First, there has been limited transparency into the development process behind most LLMs, even the open-weight models used here. With exception to BLOOM which had transparency as an explicit goal of its development, not all details of the LLMs in this work regarding their training data, model architecture, and optimization techniques have been clearly documented. Given this as well as the complexity of these details, it is not easy to interpret why exactly different types of LLMs perform differently on the same benchmarks. So while we conclude there is a positive association between answering and generating COPA, we do not propose an explanation for why certain models (e.g. the LLAMA2 family) excel on these tasks more than other others (e.g. the BLOOM family).\nSecond, LLM behavior is highly sensitive to prompt design, such that different prompts representing the same task and input features can yield different outputs. As a result, prompt optimization has become a significant focus of utilizing LLMs. Finding tractable solutions to this optimization process is an ongoing research endeavor (e.g. Zhou et al., 2023). Meanwhile, this is typically done manually without any guarantee of optimality. In particular, researchers often write a few different prompt variations based on their knowledge of the task and select the one that performs best on the task evaluation. In this work, we took a principled rather than data-driven approach to selecting the prompts for answering and generating COPA, in which the prompt design matches the conceptual design of the benchmark defined in Roemmele et al. (2011). It is possible that varying the language of the prompt as well as the number of exemplars (shots) would result in better performance in terms of answering accuracy or Gen-COPA validity for some LLMs, which could yield a different view of how they compare to one another."}, {"title": "Ethical Considerations", "content": "The business of data annotation has grown dramatically with the expansion of NLP systems and LLMs in particular. There are serious concerns about the ethics of this business when it comes to fair compensation of workers (e.g. Perrigo, 2023). For the annotation in this work, the expert rater was compensated as part of their normal job role. The external raters we employed on Prolific were paid at the rate of $12/hour, which meets Prolific's recommended universal standard for fair pay (Prolific, 2023).\nLLMs have a well-known risk of generating offensive content (e.g. Gehman et al., 2020). We anticipated this risk would emerge in some of the Gen-COPA items and sought to reduce exposure to these items to people outside our internal team. As described in Section 6.2, the expert rater consented to conduct an initial review of all Gen-COPA items. They assigned a \u201ccontent warning\" to items they deemed potentially offensive or harmful, and these items were consequently not shown to external raters on Prolific. While we cannot guarantee that the external raters were not offended by items that were not flagged in the initial review, we did not receive any report of this from raters.\"\n    }"}]}