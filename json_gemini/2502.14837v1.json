{"title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs", "authors": ["Tao Ji", "Bin Guo", "Yuanbin Wu", "Qipeng Guo", "Lixing Shen*", "Zhan Chen", "Xipeng Qiu", "Qi Zhang", "Tao Gui"], "abstract": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by DeepSeek, designed to ensure efficient and economical inference by significantly compressing the Key-Value (KV) cache into a latent vector. Compared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its variants such as Grouped-Query Attention (GQA) exhibit significant cost disadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA without pre-training from scratch is both meaningful and challenging. This paper proposes the first data-efficient fine-tuning method for transitioning from MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE, we remove ROPE from dimensions of queries and keys that contribute less to the attention scores, for low-rank approximation, we introduce joint SVD approximations based on the pre-trained parameters of keys and values. These carefully designed strategies enable MHA2MLA to recover performance using only a small fraction (3% to 6%) of the data, significantly reducing inference costs while seamlessly integrating with compression techniques such as KV cache quantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%, with only a 0.5% drop in LongBench performance.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of large language models (LLMs) has significantly accelerated progress toward artificial general intelligence (AGI), with model capabilities scaling predictably with parameter counts (Kaplan et al., 2020). However, these gains come at a steep cost: escalating computational demands for training and degraded inference throughput, resulting in substantial energy consumption and carbon emissions (Strubell et al., 2019).\nAs downstream tasks grow increasingly complex, long-context processing and computationally intensive inference have become central to LLM applications (An et al., 2024). A key bottleneck lies in the memory footprint of the Key-Value (KV) cache inherent to the Multi-Head Attention (MHA, 2017) mechanism, which scales linearly with sequence length and model size. To mitigate this, variants like Grouped-Query Attention (GQA, 2023) and Multi-Query Attention (MQA, 2019) have been explored. However, these methods reduce not only the KV cache size but also the number of parameters in the attention, leading to performance degradation. The DeepSeek introduces Multi-Head Latent Attention (MLA, 2024), an attention mechanism equipped with low-rank key-value joint compression. Empirically, MLA achieves superior performance compared with MHA, and meanwhile significantly reduces the KV cache during inference, thus boosting the inference efficiency.\nA critical yet unexplored question arises: Can LLMs originally well-trained for MHA be adapted to enabling MLA for inference? The inherent architectural disparities between MHA and MLA render zero-shot transfer impractical, while the prohibitive cost of pretraining from scratch makes this transition both technically challenging and underexplored in existing research. To address this gap, we propose the first carefully designed MHA2MLA framework that maximizes parameter reuse from pre-trained MHA networks while aligning the KV cache storage and inference process with MLA's paradigm (Figure 1). Our framework features two pivotal technical innovations: partial rotary position embedding (partial RoPE) and low-rank approximation. The primary objective of MHA2MLA is to achieve data-efficient performance recovery - restoring architecture-induced capability degradation using minimal fine-tuning data.\nThe inherent incompatibility between MLA's"}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 Multi-Head Attention (MHA)", "content": "Given an input sequence ${x_1,...,x_l} \\in \\mathbb{R}^{l\\times d}$, standard MHA (Vaswani et al., 2017) projects each token $x_i$ into queries $q_i^{(h)} = x_iW_q^{(h)}$, keys $k_i^{(h)} = x_i W_k^{(h)}$, and values $v_i^{(h)} = x_i W_v^{(h)}$, where $W_q^{(h)}, W_k^{(h)}, W_v^{(h)} \\in \\mathbb{R}^{d \\times d_h}$ for each head $h\\in \\{1,...,n_h\\}$. The Rotary positional encoding (RoPE, 2024) is applied to queries and keys (e.g., $q_{i, rope}^{(h)}= ROPE(q_i^{(h)})$), followed by scaled dot-product attention:\n$o_i^{(h)} = Softmax(\\frac{(q_{i,rope}^{(h)})^T k_{j,rope}^{(h)}}{\\sqrt{d_h}})v_j^{(h)}$\n$MHA(x_i) = [o_i^{(1)},...,o_i^{(n_h)}] W_o$\nwhere $W_o \\in \\mathbb{R}^{(n_h d_h) \\times d}$ and $[\\cdot, \\cdot]$ means vector concatenate. During autoregressive inference, MHA stores the KV cache $\\{k_{i, rope}^{(h)}, v_i^{(h)}\\}_{i=1}^{l}$ of size"}, {"title": "2.2 Multi-Head Latent Attention (MLA)", "content": "MLA (DeepSeek-AI et al., 2024) introduces a hybrid architecture that decouples PE from latent KV compression. For each head h, the input $x_i$ is projected into two complementary components:\nPosition-Aware Component A subset of dimensions retains PE to preserve positional sensitivity:\n$q_{i, rope}^{(h)}, k_{i, rope}^{(h)} = ROPE(x_i W_{dq}^{(h)} W_{qr}^{(h)}, x_i W_{kr}^{(h})$\nwhere $W_{dq} \\in \\mathbb{R}^{d \\times d_q}$, $W_{qr}^{(h)} \\in \\mathbb{R}^{d_q \\times d_r}$, $W_{kr} \\in \\mathbb{R}^{d \\times d_r}$ project queries/keys into a RoPE-preserved component of dimension $d_r$.\nPosition-Agnostic Component The remaining dimensions $d_c$ are stripped of PE (i.e., NoPE), $k_{i, nope}^{(h)}$ and $v_i^{(h)}$, and compressed into a shared latent vector $c_{i,kv}^{(h)}$:\n$q_{i, nope}^{(h)} = x_i W_{dq} W_{qc}^{(h)}$,\n$c_{i,kv} = x_i W_{dkv}^{(h)}$,\n$k_{i, nope}^{(h)}, v_i^{(h)} = c_{i,kv} W_{uk}^{(h)}, c_{i,kv} W_{uv}^{(h)}$,\nwhere $W_{qc}^{(h)} \\in \\mathbb{R}^{d_q \\times d_c}$, $W_{dkv} \\in \\mathbb{R}^{d \\times d_{kv}}$, $W_{uk}^{(h)} \\in \\mathbb{R}^{d_{kv} \\times d_c}$, $W_{uv}^{(h)} \\in \\mathbb{R}^{d_{kv} \\times d_h}$. Note that $d_r + d_c = d_h$. The attention output of MLA combines both components:\n$o_i^{(h)} = Softmax\\left(\\frac{(q_{i,rope}^{(h)})^T k_{j,rope}^{(h)} + (q_{i,nope}^{(h)})^T k_{j,nope}^{(h)}}{\\sqrt{d_h}}\\right)v_j^{(h)}$\n$MLA(x_i) = [o_i^{(1)},...,o_i^{(n_h)}] W_o$"}, {"title": "3 MHA2MLA", "content": ""}, {"title": "3.1 Partial-ROPE", "content": "To enable migration from standard MHA to MLA, we propose partial-RoPE finetuning, a strategy that removes ROPE from a targeted proportion of dimensions and converts them into NoPE. Critically, while prior work has explored training LLMs with partial-RoPE from scratch (achieving marginally better perplexity than full-RoPE (Black et al., 2021; Barbero et al., 2024)), no existing method addresses how to efficiently adapt pre-trained full-ROPE models (e.g., Llama) to partial-RoPE without costly retraining. Our work bridges this gap by systematically evaluating partial-RoPE variants to identify the most data-efficient fine-tuning protocol for recovering model performance post-adaptation."}, {"title": "3.2 Low-rank Approximation", "content": "After transitioning from full RoPE to partial ROPE, we obtain the first component of the KV cache in MLA, represented as: $k_{i,rope} = R_i^{[2k,2k+1]} (\\theta_k) k_j^{[2k,2k+1]}$ Our next goal is to derive the second component, $c_{i,kv} \\in \\mathbb{R}^{d_{kv}}$, which serves as a low-rank representation of $k_{i,nope}$ and $v_i$.\nGiven the keys $k_i = x_i W_k$ and values $v_i = x_i W_v$ in MHA, we first extract the subspace of $W_k$ corresponding to $k_{i,nope}$, i.e., the dimensions not included in S, yielding: $k_{i,nope} = x_i W_{k,nope}$. We propose two Singular Value Decomposition (SVD)-based strategies (Illustrated in Figure 4) to preserve pre-trained knowledge while achieving rank reduction:\nDecoupled SVD (SVDsplit) Separately decompose $W_{k, nope}$ and $W_v$ into truncated SVDs, allocating $d_{kv}/2$ dimensions to each:\n$W_{k, nope} = U_k \\Sigma_k V_k^T, W_v = U_v \\Sigma_v V_v^T$,\nwhere $U_k, U_v, V_k, V_v\\in \\mathbb{R}^{d_h \\times \\frac{d_{kv}}{2}}$, $\\Sigma_k, \\Sigma_v \\in \\mathbb{R}^{\\frac{d_{kv}}{2} \\times \\frac{d_{kv}}{2}}$. The down-projection matrices $W_{d*}$ and up-projection matrices $W_{u*}$ become:\n$W_{dk} = U_k \\Sigma_k^{\\frac{1}{2}}, W_{uk} = \\Sigma_k^{\\frac{1}{2}}V_k^T$,\n$W_{dv} = U_v \\Sigma_v^{\\frac{1}{2}}, W_{uv} = \\Sigma_v^{\\frac{1}{2}}V_v^T$.\nThe low-rank representation $c_{i,kv}$ can be constructed using $c_{i,kv} = [x_i W_{dk}, x_i W_{dv}]$.\nJoint SVD (SVDjoint) To preserve interactions between $k_{nope}$ and V, we jointly factorize the concatenated matrix:\n$[W_{k,nope}, W_v] = U_{kv} \\Sigma_{kv} V_{kv}^T$,\nwhere $U_{kv}, V_{kv} \\in \\mathbb{R}^{d_h \\times d_{kv}}$, $\\Sigma_{kv} \\in \\mathbb{R}^{d_{kv} \\times d_{kv}}$. The latent projection is then:\n$W_{dkv} = U_{kv} \\Sigma_{kv}^{\\frac{1}{2}}$\n$W_{uk} = \\Sigma_{kv}^{\\frac{1}{2}}V_{kv}[:,: \\frac{-dv}{2}], W_{uv} = \\Sigma_{kv}^{\\frac{1}{2}}V_{kv}[:,dv :]$\nThis jointly optimizes the latent space for both keys and values, i.e., $c_{i,kv} = x_i W_{dkv}$, retaining cross-parameter dependencies critical for autoregressive generation . Section 4.3 shows SVDjoint outperforming SVDsplit, validating that joint factorization better preserves pre-trained knowledge."}, {"title": "4 Experiment", "content": "We evaluate our method on LLMs of varying scales (SmolLM-135M/360M/1B7, Llama2-7B) pre-trained with MHA or GQA. We chose the SmolLM-series because its pretraining data and framework are both open-source, which can minimize the gap in fine-tuning data and processes. We chose Llama2-7B because it is one of the widely used open-source LLMs (but its pretraining data is not open-source, there is a potential gap in fine-tuning data)."}, {"title": "4.1 Commonsense Reasoning Tasks", "content": "Main Results As shown in Table 1, our method achieves efficient architectural migration across four model scales (135M to 7B) under varying KV cache compression ratios (via latent dimension $d_{kv}$). First, when comparing the performance of our fine-tuning approach with the original LLM, we observe only minor changes in performance across the four base models: a -0.25% decrease on the 135M, +0.03% on the 360M, +0.03% on the 1B7, and +0.37% on the 7B. This suggests that the fine-tuning data does not significantly degrade or improve the performance of the original model, providing an appropriate experimental setting for the MHA2MLA framework.\nNext, as $d_{kv}$ decreases (e.g., from 32 to 16 to 8), the KV cache reduction increases (i.e., from -68.75% to -81.25% to -87.5%), but the performance loss becomes more challenging to recover through fine-tuning. Figure 5 shows the fine-tuning loss curves of 135M (representing GQA) and 7B (representing MHA) under different compression ratios. As the compression ratio increases, the loss difference from the baseline becomes larger. Additionally, we observe that the fluctuation trends of the loss curves are almost identical, indicating that our architecture migration does not significantly harm the model's internal knowledge.\nWe also find that larger models experience less performance degradation when transitioning to the MLA architecture. For example, with compression down to 18.75%, the performance drops by 2.41% for 135M, 2.69% for 360M, 1.28% for 1B7, and 0.61% for 7B, revealing the potential scaling law of MHA2MLA. Finally, from the 135M model to the 7B model, the number of tokens required for fine-tuning is only about 0.3% to 0.6% of the pre-training tokens, demonstrating the data efficiency of our method.\nOverall, whether using GQA2MLA or MHA2MLA, the architecture transition is achieved with minimal cost, resulting in efficient and economical inference."}, {"title": "4.2 Long Context Tasks", "content": "Settings To evaluate the generative capabilities of the model, we adopt LongBench (Bai et al., 2024) as the benchmark for generation performance. All models are tested using a greedy decoding strategy. The context window size is determined based on the sequence length used during model fine-tuning. We use HQQ (Badri and Shaji, 2023) and Quanto to set caches with different levels of precision to evaluate the performance of the original model as the baseline. Since our method is compatible with KV cache quantization, we also conduct additional experiments to assess the combined effect of both approaches.\nMain Results As evidenced in Table 2, MHA2MLA achieves competitive or superior efficiency-accuracy profiles compared to post-training quantization methods on LongBench. While 4-bit quantization incurs modest degradation (-0.2% to -0.4%) at comparable compression ratios, aggressive 2-bit quantization suffers severe performance collapse (-6.2% to -9%) despite 87.5% KV cache reduction. In contrast, MHA2MLA alone attains 87.5% compression (at $d_{kv}$ = 16) with only 3% accuracy loss, and further synergizes with 4-bit quantization to reach 92.19%/96.87% compression ($d_{kv}$=64/16+Int4HQQ) while limiting degradation to -0.5%/-3.2%, outperforming all 2-bit baselines. This highlights that MHA2MLA's latent space design remains orthogonal to numerical precision reduction, enabling compound efficiency gains"}, {"title": "4.3 Ablation Study", "content": "Four Partial-RoPE strategies: $S_{high}, S_{low}, S_{uniform}, S_{2-norm}$ Table 3 presents the results of four strategies for converting full-RoPE to partial-RoPE. First, when comparing the four strategies with full-RoPE, we observed that the low-frequency retention strategy, $S_{low}$, incurred the greatest performance loss (a reduction of -6.49%@135M and -1.21%@1B7), whereas the high-frequency retention strategy, $S_{high}$, experienced significantly less degradation (a reduction of -0.85% @135M and -0.76%@1B7), underscoring the importance of high-frequency subspaces. Both $S_{uniform}$ and $S_{2-norm}$ yielded better performance, the $S_{uniform}$ preserves subspaces across the frequency spectrum, while the $S_{2-norm}$ retains subspaces based on their contribution to the attention scores. We choose $S_{2-norm}$ as the default configuration because the removed subspaces (i.e., NoPE) are more suitable for the (SVD-based) low-rank approximation.\nTwo SVD-based low-rank approximations: SVDsplit, SVDjoint The last two rows of each group in Table 3 compare the effects of the two SVD methods. We observe that, on both LLMs, the SVDjoint method consistently outperforms SVDsplit, yielding an average performance improvement of 0.92% on the 135M model and 0.74% on the 1B7 model. It indicates that SVDjoint emerges as the"}, {"title": "5 Related Work", "content": "Efficient Attention Architectures The standard Multi-Head Attention (MHA, 2017) mechanism's quadratic complexity in context length has spurred numerous efficiency innovations. While MHA remains foundational, variants like Multi-Query Attention (MQA) and Grouped-Query Attention (GQA, 2023) reduce memory overhead by sharing keys/values across heads\u2014albeit at the cost of parameter pruning and performance degradation. Parallel efforts, such as Linear Transformers (Guo et al., 2019; Katharopoulos et al., 2020; Choromanski et al., 2021), RWKV (Peng et al., 2023), and Mamba (Gu and Dao, 2023), replace softmax attention with linear recurrences or state-space models, but struggle to match the expressiveness of standard attention in autoregressive generation.\nMulti-Head Latent Attention (MLA, 2024) distinguishes itself by compressing KV caches into low-rank latent vectors without pruning attention parameters. Our work bridges MLA with mainstream architectures (MHA/GQA), enabling seamless migration via data-efficient fine-tuning. Notably, while many linear attention variants abandon softmax query-key interactions (e.g., through kernel approximations), architectures preserving a query-key dot product structure\u2014even in factorized forms\u2014remain compatible with our MHA2MLA framework."}, {"title": "Economical Key-Value Cache", "content": "The memory footprint of KV caches has become a critical bottleneck for long-context inference. Recent advances fall into three categories:\nInnovative Architecture methods like MLA (DeepSeek-AI et al., 2024), MiniCache (Liu et al., 2024a), and MLKV (Zuhri et al., 2024) share or compress KV representations across layers or heads. While effective, cross-layer sharing risks conflating distinct attention patterns, potentially harming task-specific performance. Only MLA has been successfully validated in Deepseek's LLMs.\nQuantization techniques such as GPTQ (Frantar et al., 2022), FlexGen (Sheng et al., 2023), and KIVI (Liu et al., 2024b) store KV caches in low-bit formats (e.g., 2-bit), achieving memory savings with precision loss.\nDynamic Pruning approaches like A2SF (Jo and Shin, 2024) and SnapKV (Li et al., 2024) prune \"less important\" tokens from the KV cache. However, token pruning risks discarding critical long-range dependencies, while head pruning (e.g., SliceGPT (Ashkboos et al., 2024), Sheared (Xia et al., 2024), and Simple Pruning (Sun et al., 2024)) irreversibly reduces model capacity.\nOur MHA2MLA method achieves the migration of standard Transformer-based LLMs to the more economical MLA architecture and has demonstrated its ability to integrate with KV quantization techniques to realize a ~97% cache saving. It is also theoretically compatible with other methods like pruning."}, {"title": "6 Conclusion", "content": "This work addresses the critical challenge of adapting pre-trained MHA-based LLMs (or variants) to the KV-cache-efficient MLA architecture. By introducing MHA2MLA with contribution-aware partial-RoPE removal and SVD-driven low-rank projection, we achieve near-lossless compression of KV cache (up to 96.87% size reduction for Llama2-7B) while requiring only 3% to 6%of training data. The framework demonstrates strong compatibility with existing compression techniques and maintains commonsense reasoning and long-context processing capabilities, offering a practical pathway for deploying resource-efficient LLMs without sacrificing performance. Our results underscore the feasibility of architectural migration for LLMs through targeted parameter reuse and data-efficient fine-tuning."}, {"title": "Limitations", "content": "Verification on More LLMs Considering that MHA2MLA can significantly reduce inference costs, it is worthwhile to validate it on larger and more diverse open-source LLMs. However, constrained by our computation resources, models like Llama3 require fine-tuning on a 128K context length to mitigate performance degradation from continued training, so we did not perform such experiments. Furthermore, since Deepseek has not yet open-sourced the tensor-parallel inference framework for MLA, it is currently challenging to explore models larger than 7B. This will be addressed in our future work.\nParameter-Efficient MHA2MLA Fine-tuning This paper primarily focuses on the data efficiency of MHA2MLA. Since the architectural transformation does not involve the Feed-Forward (FFN) module, future work could explore parameter-efficient MHA2MLA fine-tuning, for example by freezing the FFN module and/or freezing the parameters in the queries and keys that correspond to the retained ROPE. This could further reduce the cost of the MHA2MLA transition."}, {"title": "A The Calculation of 2-norm Score", "content": "To compute the 2-norm scores for each attention head, we selected 1,024 samples from the training dataset. The proportions of the subsets and sequence length used during the 2-norm computation are consistent with those used during fine-tuning. First, we calculate the query vectors and key vectors for each head. Then, for each rotational subspace of the vectors, we compute the 2-norm scores. Finally, the 2-norm scores of the query and key vectors are aggregated within each subspace. If the model employs Grouped-Query Attention (GQA), the 2-norm scores are averaged within each GQA group, and the scores are shared between the groups."}, {"title": "B Inference Process of MHA2MLA", "content": "During inference in the MHA2MLA model, our input includes the hidden representation $x_i$ of the i-th token, as well as the previously stored $k_{i,rope}^{(h)}$ and $c_{i,kv}^{(h)}$ in the KV cache for the first i \u2013 1 tokens.\nDuring the inference, our goal is to compute the h-th head's dot product of these two parts $q_{i, rope}^{(h)} k_{j,rope}^{(h)T}$ and $q_{i, nope}^{(h)} k_{j,nope}^{(h)T}$. For the RoPE part, we can easily extract $W_{q,rope}^{(h)}$ and $W_{k,rope}^{(h)}$ from the pre-trained parameter matrices $W_q^{(h)}$ and $W_k^{(h)}$ (i.e., the rows corresponding to the subspace that retains RoPE) and then obtain the result through a linear transformation:\n$k_{rope}^{(h)} = x W_{q,rope}^{(h)}$\n$k_{rope}^{(h)} = x W_{q,rope}^{(h)}$\nNote that $k_{rope}^{(h)}$ is already stored in the KV cache and can be directly retrieved.\nFor the NoPE part, $q_{i,nope}^{(h)}$ can still be easily obtained through a linear transformation $W_{q,nope}^{(h)}$ which extracted from the pre-trained parameter matrix $W_q^{(h)}$ by separating the rows corresponding to the subspace with RoPE removed. However, $k_{i, nope}^{(h)}$ requires two linear transformations: a dimensionality reduction transformation using $W_{dkv}$, and a dimensionality expansion transformation using $W_{uk}^{(h)}$. Note that $W_{dku}$ is shared across all heads in the current layer, and both $W_{dku}$ and $W_{uk}^{(h)}$ are"}, {"title": "C The Details of Fine-tuning", "content": "Data We fine-tune our model using the pretraining corpus from SmolLM10. The dataset consists of fineweb-edu-dedup, cosmopedia-v2, python-edu, open-web-math, and StackOverflow. The first three datasets are part of the smollm-corpus\u00b9\u00b9 curated by HuggingFaceTB. Fineweb-edu-dedup is a high-quality dataset filtered by HuggingFaceTB from education-related webpages. Similarly, HuggingFaceTB filtered Python code snippets from The Stack to construct the python-edu dataset. Cosmopedia-v2 is a high-quality dataset generated by a model based on 34,000 topics defined by BISAC book classifications. Additionally, open-web-math12 and StackOverflow13 are sourced from high-quality mathematical texts available online and posts from StackOverflow, respectively.\nHyperparameters The fine-tuning hyperparameters for models of all sizes are listed in Table 4. The training process employs a warmup phase followed by a decay strategy. A 1-sqrt decay strategy is applied to ensure a smooth and gradual reduction."}, {"title": "D Ablation Study on Partial-RoPE Dimensions", "content": "To better determine the strategy and dimensionality for partial-RoPE, we conducted an ablation study on the number of RoPE dimensions using the 135MSmolLM model. The experimental results are presented in Table 5. By comparing the performance of four different strategies in varying dimensionalities, we observed that the low-frequency strategy, Slow, suffered significant performance degradation (-14.7%) when the dimensionality was relatively low (\u2264 4). In contrast, both Suniform and S2-norm consistently demonstrated superior performance regardless of dimensionality. Furthermore, increasing the dimensionality from 4 to 8 provided negligible performance gains. Based on these findings, we selected a dimensionality of 4 for partial-ROPE."}, {"title": "E Detailed Results", "content": "In this section, we present the detailed results.\nDetailed LongBench evaluation is reported in Table 6.\nDetailed ablation experiment is reported in Table 7."}]}