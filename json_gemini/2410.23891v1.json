{"title": "AllClear: A Comprehensive Dataset and Benchmark for Cloud Removal in Satellite Imagery", "authors": ["Hangyu Zhou", "Chia-Hsiang Kao", "Cheng Perng Phoo", "Utkarsh Mall", "Bharath Hariharan", "Kavita Bala"], "abstract": "Clouds in satellite imagery pose a significant challenge for downstream applications. A major challenge in current cloud removal research is the absence of a comprehensive benchmark and a sufficiently large and diverse training dataset. To address this problem, we introduce the largest public dataset - AllClear for cloud removal, featuring 23,742 globally distributed regions of interest (ROIs) with diverse land-use patterns, comprising 4 million images in total. Each ROI includes complete temporal captures from the year 2022, with (1) multi-spectral optical imagery from Sentinel-2 and Landsat 8/9, (2) synthetic aperture radar (SAR) imagery from Sentinel-1, and (3) auxiliary remote sensing products such as cloud masks and land cover maps. We validate the effectiveness of our dataset by benchmarking performance, demonstrating the scaling law - the PSNR rises from 28.47 to 33.87 with 30x more data, and conducting ablation studies on the temporal length and the importance of individual modalities. This dataset aims to provide comprehensive coverage of the Earth's surface and promote better cloud removal results.", "sections": [{"title": "1 Introduction", "content": "Satellite image recognition enables environmental monitoring, disaster response, urban planning [Pham et al., 2011, Wellmann et al., 2020], crop-yield prediction [Doraiswamy et al., 2003], and many more applications, but is held back significantly due to occlusion by clouds. Roughly 67% of the Earth's surface is covered by clouds at any given moment [King et al., 2013]. The limited availability of cloud-free captures is especially problematic for time-sensitive events like wildfire control [Kyzirakos et al., 2014, Thangavel et al., 2023] and flood damage assessment [Rahman and Di, 2020]. Consequently, developing effective cloud removal techniques is crucial for maximizing the utility of remote sensing data in various domains.\n\nA major challenge holding back research into cloud removal is the lack of comprehensive datasets and benchmarks. A survey of publicly available datasets for cloud removal (Table 1) reveals several problems. First, most existing datasets are sampled from a small set of locations and thus have limited geographical diversity [Ebel et al., 2020, Huang and Wu, 2022, Ebel et al., 2022], impacting both the effectiveness of training and the rigor of evaluation. Second, many existing datasets filter out very cloudy images (e.g., more than 30% cloud coverage), thus preventing trained models from tackling practical situations with extensive cloud cover [Sarukkai et al., 2020, Requena-Mesa et al., 2021] (Figure 1). Third, some existing benchmarks use ground-truth cloud-free images captured at a very different time point from the time the input images are captured [Sarukkai et al., 2020, Ebel et al., 2022]. This means that many changes may have occurred on the ground between the"}, {"title": "2 Background", "content": "2.1 Existing Cloud Removal Datasets\n\nAdvances in cloud removal research for satellite imagery have led to the development of several datasets with unique characteristics and limitations. STGAN introduced two cloud removal datasets and established the multi-temporal task format of using three images as input [Sarukkai et al., 2020]. However, the dataset discards all image crops with more than 30% cloud cover, leading to only"}, {"title": "3 Dataset", "content": "3.1 Regions-of-Interest Selection\n\nWe choose our ROIs to satisfy two objectives: (a) coverage of most of the land surface and (b) a balanced sampling of land cover types. This balanced sampling in particular ensures that smaller but more popular locations like cities are as well represented as the large swathes of wilderness. To get these ROIs, we follow a two-step procedure: curating a pool of ROI candidates and then building train/benchmark subgroups balanced across land cover types, as shown in Figure 1. This ensures both the benchmark and the training sets contain a sufficient amount of data representing various land cover types.\n\nFor curating the ROI pool, unlike previous work that followed random ROI selection [Sarukkai et al., 2020, Huang and Wu, 2022, Ebel et al., 2020, 2022, Xu et al., 2023], we use grid sampling to select an ROI every 0.1\u00b0 latitude and every 0.1\u00b0cos(\u03b8) longitude, where \u03b8 is the latitude, from 90\u00b0S to 90\u00b0N. The intuition behind this approach is that the same 0.1\u00b0 longitude can represent 11.1 km at the equator and 4.35 km at 67\u00b0 latitude. This weighting provides a simple yet effective method for not over-sampling high-latitude areas. By excluding ocean areas using the GeoPandas package, we select a total of 1,087,947 ROIS.\n\nNext, we select ROIs from the pool to achieve a more balanced dataset over land-cover use while considering the natural imbalance of land cover distribution on the earth's surface. We leverage the land cover data from the Dynamic World product [Brown et al., 2022] from Google Earth Engine,"}, {"title": "3.2 Data Preparation", "content": "AllClear contains three different types of open-access satellite imagery made available by the Google Earth Engine (GEE) platform [Gorelick et al., 2017]: Sentinel-2A/B [Drusch et al., 2012], Sentinel-1A/B [Torres et al., 2012], and Landsat 8/9 [Williams et al., 2006]. For Sentinel-2, we collected all thirteen bands of Level-1C orthorectified top-of-atmosphere (TOA) reflectance product. For Sentinel-1, we acquired the S1 Ground Range Detected (GRD) product with two polarization channels (VV and VH). As for Landsat 8/9, we collected all twelve bands of Collection 2 Tier 1 calibrated TOA reflectance product. All the raw images in AllClear were resampled to 10-meter resolution. We follow the default GEE preprocessing steps during all the downloading process. In addition, we include the Dynamic World Land Cover Map for all the Sentinel-2 imagery [Brown et al., 2022]. For each selected ROI, our goal is to collect all 2.56 \u00d7 2.56 km\u00b2 patches in 2022 with a spatial resolution of 10 meters. We adopt the Universal Transverse Mercator (UTM) coordinate reference system (CRS), following Ebel et al. [2020, 2022], Zhao et al. [2023], which divides the Earth into 60 zones, each spanning 6 degrees of longitude, to ensure minimal distortion, especially along the longitude axis. Since satellite imagery do not necessarily conform to the boundaries of UTM zones, gaps (NaN values) can occur where the tile data does not cover the entire ROI. In such cases, we exclude all images containing NaN values to maintain data quality.\n\nData Preprocessing. For Sentinel-1, following Ebel et al. [2022], we clip the values in the VV channel of S1 to [-25; 0] and those of the VH channels to [-32.5, 0]. For Sentinel-2 and Landsat 8/9, we clip the raw values to [0, 10000] [Ebel et al., 2022, Huang and Wu, 2022]. The values are then normalized to the range of [0, 1].\n\nCloud and Shadow Mask Computation. The cloud and shadow masks are indispensable to this dataset as they are used for guiding evaluation metric computation by masking out regions where there are clouds and shadows in the target images. To obtain the cloud mask, we use the S2 Cloud Probability dataset available on Google Earth Engine. This dataset is built by using S2cloudless [Zupanc, 2017], an automated cloud-detection algorithm for Sentinel-2 imagery based on a gradient boosting algorithm, which shows the best overall cloud detection accuracy on opaque clouds and semi-transparent clouds in the Hollstein reference dataset [Hollstein et al., 2016, Skakun et al., 2022] and the LCD PixBox dataset [Paperin et al., 2021, Skakun et al., 2022].\n\nAs for the shadow mask, ideally the cloud shadows can be estimated using the sun azimuth and cloud height but the latter information cannot be obtained. We therefore proceed with curating the shadow mask following documentation in Google Earth Engine [jdbcode, 2023]. The shadow is estimated by computing dark pixels and projecting cloud regions. For the dark pixels, we use the Scene Classification Map (SCL) band values from Sentinel-2 to remove water pixels, as water pixels can resemble shadows. We then threshold the NIR pixel values with a threshold of le-4 to create a map of dark pixels. Finally, we take the intersection of the dark pixel map and the projected cloud regions to obtain the cloud shadow masks."}, {"title": "3.3 Benchmarking Task Setup and Evaluation", "content": "For evaluation, we construct a sequence-to-point task using our AllClear dataset with train, validation, and test splits of 278,613, 14,215, and 55,317 samples, respectively. Each instance contains three input images (u1, u2, u3), a target clear image (v), input cloud and shadow masks, target cloud and shadow masks, timestamps, and metadata such as latitude, longitude, sun elevation angle, and sun azimuth. Sentinel-2 images are considered the main sensor modality, while sensors such as Sentinel-1"}, {"title": "4 Experiments", "content": "We next evaluate the usefulness of our dataset for both evaluation and training.\n\n4.1 Benchmarking prior methods on the AllClear test set\n\nSelection of SoTA model architecture. We choose the state-of-the-art pre-trained models Un-CRtainTS [Ebel et al., 2023], U-TILISE [Stucker et al., 2023], CTGAN [Huang and Wu, 2022], PMAA [Zou et al., 2023a], and DiffCR [Zou et al., 2023b] to benchmark on our AllClear dataset. For the evaluation, all models receive three images as input. Specifically, they receive both Sentinel-2 and Sentinel-1 images concatenated along the channel dimension.\n\nSimple baselines To better contextualize model performance, we follow previous works [Ebel et al., 2022, 2023] and include two simple baselines: \"Least Cloudy\u201d and \u201cMosaicing\". The former simply uses the input image with the least cloud and shadow coverage as the output. \u201cMosaicing\u201d operates in the following way: for each image coordinates in the input images if only one image is clear, we directly copy its pixel value; if more than one clear images exist, we take the average of these clear pixel values; if there is no clear image, we fill the gap with 0.5.\n\nResults. The quantitative and qualitative results are shown in Table 2 and Figure 8, respectively. We first notice that simple baselines least cloudy and mosaicing perform well on the dataset. UnCRtainTS performs slightly better than these simple baselines in terms of SSIM and SAM. On the other hand, the U-TILISE model falls short of reaching the performance of the simple baselines. Since U-TILISE is a sequence-to-sequence model, we adopt it for sequence-to-point evaluation by choosing the image from the output sequence with the lowest MAE score as the model output. Notably, the training of U-TILISE involves adding sampled cloud masks to the cloud-free images as inputs, and it is trained to recover the original cloud-free sequence. The model is evaluated in a similar manner. The distribution disparity between the sampled cloud masks and the real clouds may contribute to the low score of U-TILISE in the real scenario. For the good performance of least cloudy and mosaicing, we conjecture that this is because of the small temporal gap in AllClear between input and target images, so simply averaging or choosing from the input images is likely to yield good results.\n\nNotably, models pre-trained on STGAN and Sen2_MTC datasets (specifically CTGAN, PMAA, and DiffCR) performed below simple baselines. Due to insufficient documentation of imagery specifications and pre-processing protocols in these datasets, we excluded these pre-trained models from subsequent analysis."}, {"title": "4.2 Training on AllClear", "content": "We next evaluate the benefits of training on AllClear. For this purpose, we use UnCRtainTS given its good performance on prior benchmarks [Ebel et al., 2023]. To evaluate if there is any domain difference between AllClear and the previous SEN12MS-TR-CS dataset, we first run an equal-training-set-size comparison. We train UnCRtainTS on a subset of AllClear that is of the same size as the training set size used in UnCRtainTS training, which is 10,167 data points. We also follow the training hyperparameters as in the original paper to avoid extra tuning."}, {"title": "4.3 Stratified evaluations", "content": "We use the available land-cover type labels in AllClear to conduct a stratified evaluation across land-cover types. As shown in Figure 4, we find that both PSNR and SSIM metrics are generally much worse for both water bodies and snow cover. Water bodies have transient wave patterns, and snow cover is also often transient, which may explain the difficulty of predicting these classes. Snow may also be confused with cloud.\n\nFollowing past work [Ebel et al., 2022], we also perform a stratified evaluation of accuracy relative to the extent of cloud cover and shadows (Figure 5). For cloud cover, generally performance decreases with cloud percentage, which is expected. Training on a larger dataset (AllClear) substantially improves accuracy for low and medium cloud cover, but not for fully clouded regions. Note that the striped pattern is because of fully cloudy images as explained in the Appendix. Shadows are generally less of a problem, and shadow percentage seems to be uncorrelated with performance."}, {"title": "4.4 Effect of various temporal spans", "content": "We next use our benchmark to see whether the common practice of using 3 input images is sufficient. We compare two models, one using 3 images and the other using all 12 images captured at that location. Both models are trained on a 10k subset of AllClear. The results, shown in Table 5, suggest"}, {"title": "4.5 Ablation study on multi-modality preprocessing strategies", "content": "Here we explore the integration of multiple sensors into the input data. As described above, we concatenate multi-spectral Sentinel-2 images with Sentinel-1 and Landsat images to create an input with multiple channels. However, due to the differing revisit intervals of these satellites, there can be gaps in the input sequences, meaning that some Sentinel-2 images may not have corresponding Sentinel-1 or Landsat-8/9 images.\n\nTo address these gaps, we experimented with different preprocessing strategies, as shown in Table 6. We discovered that filling the gaps with different constant values significantly impacts the results. Specifically, filling with zeros yielded better performance compared to filling with ones. Also, we provided additional experiments adding an extra input dimension called the \"availability mask,\" which is filled with zeros if there is no paired Sentinel-1 image and ones otherwise, but this approach did not improve results. Additionally, while outcomes regarding using extra Landsat images were inconsistent, filling gaps with zeros for Landsat gave the best results, albeit still lower than using only Sentinel-1 and Sentinel-2 alone. This might be due to the low-resolution of Landsat imagery; we suggest a model redeisgn to fully exploit Landsat images."}, {"title": "4.6 Experiments on spatial correlation", "content": "To investigate the role of spatial correlation in model generalization, we conducted a geographical hold-out experiment. We trained and validated the model for 16 epochs using a modified version of the AllClear dataset that excluded all North American regions of interest (ROIs). The model was evaluated on two distinct test sets: one containing 2,887 North American ROIs (combined from original train, validation, and test splits) and another containing only non-North American ROIs from the original test set. The results shown in Table 8 suggest that training solely on the held-out dataset does not guarantee spatial generalization. This experiment highlights the importance of addressing spatial generalization in future research. Further investigation of the spatial correlation is presented in Appendix C.4"}, {"title": "5 Limitations and Future Work", "content": "While AllClear advances the state of cloud removal in satellite imagery, we acknowledge several important limitations. First, the largest limitation of AllClear is the lack of ground truth annotations. The cloud labels are derived from existing cloud masks computed using s2cloudless algorithm (offered by Google Earth Engine), not manually annotated. Second, we are using Google Earth Engine product level-1C, which is not atmospherically corrected. The main reason is for consistency with the previous largest cloud removal dataset and the derived pre-trained models.\n\nFor future work, our findings suggest several promising directions: (1) development of hybrid approaches combining algorithmic and manual cloud annotations, (2) investigation of atmospheric correction's impact on cloud removal performance, and (3) extension to other satellite platforms and spatial resolutions."}, {"title": "6 Conclusion", "content": "This paper has introduced AllClear, the most extensive and diverse dataset available for cloud removal research. The larger training set significantly advances state-of-the-art performance. Our dataset also enables stratified evaluation on cloud coverage and land cover, and ablations of the sequence length and sensor type. We hope that future research can build on this benchmark to advance cloud removal, for instance by exploring the dynamics between SAR and multispectral images."}, {"title": "A Overview", "content": "In this supplementary material we present more information about the dataset (including a datasheet for the dataset) and extensive results that could not fit in the main paper. In Sec. B we present more details about our dataset such as dataset specifications. In Sec. C we present additional ablation studies. In Sec. D we include a datasheet for our dataset, author statement, and hosting, licensing, and maintenance plan.\n\nThe data and pre-trained models are publicly available at https://allclear.cs.cornell.edu. Our code for accessing the dataset, benchmark result reproduction can be found at https://github.com/Zhou-Hangyu/allclear.\n\nThe Croissant metadata tool was not used because it does not support the metadata format we used in our dataset. Specifically, we use a hierarchical structure with dictionaries of lists to store the file path and corresponding timestamp for each image within each sample. The Croissant framework currently does not support parsing such a format. We will provide Croissant metadata file once support for this format is available in the future."}, {"title": "B Dataset Curation", "content": "We define a sample (i.e., an instance) from the AllClear dataset using an ordered pair <I1, I2, I3, T, M1, M2, M3, MT, DW, metadata>. Specifically, there are three input cloudy images I1, I2, I3 and a single target image T, each of spatial size R256\u00d7256. The number of channels is 13 for Sentinel-2, 2 for Sentinel-1, and 11 for Landsat-8/9. We set Sentinel-2 to be the main sensor (i.e., we evaluate models' performance on reconstructing Sentinel-2 images) and use the other satellites as auxiliary ones. The cloud and shadow masks for input M1, M2, M3 and target MT are all the same size as the inputs, with the number of channels being 5. These channels represent the cloud probability, binary cloud mask, and binary shadow mask with dark pixel thresholds of 0.2, 0.25, and 0.3. Notably, the cloud and shadow masks are paired with and derived from Sentinel-2 input images only. The DW indicates the land cover and land use maps derived from Dynamic World (DW) V1 algorithm [Brown et al., 2022], which have the same spatial size and resolution, with nine classes representing water, trees, grass, flooded vegetation, crops, shrub and scrub, built-up areas, bare land, and snow and ice. The metadata includes geolocation (latitude and longitude), sun elevation and azimuth, and timestamps.\n\nFor the benchmark dataset, we ensured that every target image have a corresponding land cover map generated by Dynamic World to enable stratified evaluation. After removing instances without corresponding land cover maps, we found that 98 out of 3,796 original test ROIs were disqualified, so we moved them to the training split to maintain benchmark dataset quantity and quality. For benchmark evaluation, we notice that some ROIs can provide over 30 test instances while some ROIs only have single test instance, and thus we decide to sample one instance for each ROI to avoid oversampling, resulting in 3,698 benchmark instances. Future works can include more test instances as an alternative to gain a more comprehensive evaluation on model performance. The statistics of our dataset are based on the final version after these adjustments."}, {"title": "C Experiments", "content": "C.1 Correlation between Cloud Removal Quality and Cloud and Shadow Coverage\n\nWe illustrate the relationship between qualitative performance and cloud and shadow coverage in Figure 6. From the left to the right columns, we quantify the cloud and shadow mask using (1) average cloud coverage, (2) average shadow mask coverage, (3) consistent cloud coverage, and (4) consistent shadow coverage. Specifically, consistent cloud (shadow) coverage refers to the percentage of pixels in the input images that are always covered by clouds (shadows). This shows a consistent trend where higher cloud coverage correlates with decreased quality of the target images, consistent with previous observations. The strips in the subplots, especially in the left column at x-axis values of 0.33, 0.67, and 1.0, are due to the fact that some images are fully clouded, resulting in more data points in particular positions in those subplots. During shadow mask synthesis, we discard regions of shadow masks that overlap with cloud masks. Thus images with low shadow percentage may have extremely high or extremely low cloud coverage. This explains the high variance of model performance in the low shadow percentage region.\n\nC.2 Evaluation of cloud removal preprocessing model's contribution for downstream tasks\n\nIt is important to evaluate if cloud removal preprocessing can be beneficial for downstream tasks. To address this, we consider the scenarios of having partially cloudy images and want to infer the land use segmentation map. The goal is to see if models trained on AllClear dataset yield the best downstream segmentation result. To this end, we have conducted additional experiments using our AllClear dataset to create a land use segmentation task.\n\nFor dataset curation, we built a land use segmentation dataset using the existing AllClear test set. The dataset contains 2000 training and 400 test images, each 256x256 pixels. Each multispectral image is paired with a corresponding land use map (9 classes). For model training, we trained a 2-D UNet model on the paired training dataset until convergence.\n\nWe prepared several versions of the test set images:\n\n\u2022 UpperBound: Original clear images (ground truth for cloud removal)\n\n\u2022 UnCRtainTS output: Based on 3 cloudy images, using the pre-trained model to yield the predicted clear images for downstream tasks.\n\n\u2022 10% AllClear model output\n\n\u2022 100% AllClear model output\n\n\u2022 LeastCloudy: Selecting the least cloudy image from the three input images"}, {"title": "C.3 Evaluation of reliance on cloud mask", "content": "As our source for cloud mask comes from s2cloudless, we discuss the reliance on s2cloudless for evaluation. Specifically, to analyze the impact of imperfect cloud masks on our analysis, we considered two scenarios: (1) False positives: Adding extra masks with jitter noise (uniformly random sampling 10% of pixels, with fixed random seeds), and (2) False negatives: Removing existing cloud masks entirely. Then, we evaluated the similarities between each trained model's performance and the ground truth test images concerning these corrupted masks. This approach simulates the way cloud masks are used to exclude pixels during evaluation.\n\nAs shown in Table 12, when cloud masks were corrupted PSNR decreased and SAM increased for all models. However, the decrease in performance is relatively minor, indicating that our evaluation is relatively robust. Additionally, despite the mask corruption, the relative ranking of model performance remained consistent, suggesting that the general trend does not significantly influence comparative performance."}, {"title": "C.4 Further discussion on spatial correlation", "content": "To assess the impact of spatial autocorrelation on cloud removal performance, we compute the correlation between models' performance and the distance of each test ROI to its nearest training ROI. We explore the correlation on the UnCRtainTS model trained on 100% of AllClear on the test set. As shown in Figure 7, we find little correlation between model performance and the distance to training ROIs, suggesting that models do not appear to utilize the spatial autocorrelation nature of satellite images and can generalize to unseen regions."}, {"title": "D Datasheet", "content": "We include a datasheet for our dataset following the methodology from \"Datasheets for Datasets\" Gebru et al. [2021]. In this section, we include the prompts from Gebru et al. [2021] in blue, and in black are our answers.\n\nD.1 Motivation\n\nFor what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\n\nThe dataset was created to facilitate research development on cloud removal in satellite imagery. The task we include allows a trained model to output a clear image given three (or more) cloudy satellite images. Specifically, our task is more temporally aligned than previous benchmarks.\n\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\n\nThe dataset was created by Hangyu Zhou, Chia-Hsiang Kao, Cheng Perng Phoo, Utkarsh Mall, Bharath Hariharan, and Kavita Bala at Cornell University.\n\nWho funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.\n\nThis work was funded by the National Science Foundation (IIS-2144117 and IIS-2403015).\n\nAny other comments?\n\nWe specify the bands we collect for Sentinel-1, Sentinel-2, and Landsat-8/9. All images are sampled at 10-meter spatial resolution.\n\nD.2 Composition\n\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\n\nAn individual instance in the benchmark dataset is a set of input images, target (clear) images, cloud and shadow masks, land use and land cover maps, and metadata. The input images primarily consist of Sentinel-2 images, while auxiliary sensor information such as Sentinel-1 and Landsat 8/9 may be included if specified in the arguments. Additionally, the number of timestamps for the input images can be 3, 6, or 12, indicating that the inputs contain images from different time frames, typically covering approximately 30 days of image collection, given the average revisit time for Sentinel-2 is 5 days. The cloud and shadow masks are binary spatial maps for each input and target Sentinel-2 image. The land use and land cover maps correspond to the target images. The metadata includes geolocation information such as latitude and longitude, as well as timestamps, sun elevation, sun azimuth, and precomputed cloud coverage.\n\nHow many instances are there in total (of each type, if appropriate)?\n\nThere are 278,613 training instances, 14,215 validation instances, and 55,317 benchmarking instances.\n\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\n\nThe dataset contains all instances from 23,742 ROIs (Regions of Interest) for the year 2022. It does not include all ROIs around the world, but it is a representative subset. We believe the samples are representative of the larger geographic coverage, as the ROI selection was balanced using land use and land cover maps."}, {"title": "D.3 Collection Process", "content": "How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\n\nThe dataset is built upon the publicly available Sentinel-2, Sentinel-1, and Landsat-8/9 satellite imagery.\n\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?\n\nThe raw satellite images were collected using Google Earth Engine APIs.\n\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\n\nThe dataset is not a sample of a larger dataset.\n\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\n\nThe first authors are involved in the data collection process.\n\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.\n\nThe dataset is built with satellite imagery in the year 2022. The image captured time stamps for each image in each instance are explicitly labeled.\n\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation."}, {"title": "D.4 Preprocessing/cleaning/labeling", "content": "Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.\n\nWe preprocessed the Sentinel-2 and Landsat-8/9 images with value clipping and normalization. Detailed steps are depicted in Section 3.2.\n\nWas the \"raw\" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the \"raw\" data.\n\nWe do not do extra pre-processing of the downloaded image dataset. The preprocessing steps are done on the fly.\n\nIs the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point.\n\nNot applicable.\n\nAny other comments?\n\nNone."}, {"title": "D.5 Uses", "content": "Has the dataset been used for any tasks already? If so, please provide a description.\n\nThe dataset presented a novel task and has not been used for any tasks yet."}, {"title": "D.6 Distribution", "content": "Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.\n\nYes, the dataset is publicly available on the internet.\n\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\n\nThe dataset can be downloaded from Cornell's server at https://allclear.cs.cornell.edu. The dataset currently does not have a DOI, but we are planning to get one.\n\nWhen will the dataset be distributed?\n\nThe dataset is available (since June 2024).\n\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\n\nThe dataset is available under Creative Commons Attribution-NonCommercial 4.0 International License.\n\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.\n\nSince our dataset is derived from Sentinel-2, Sentinel-1, and Landsat-8/9 images. Please also refer to Sentinel terms of service and Landsat terms of service."}, {"title": "D.7 Maintenance", "content": "Who will be supporting/hosting/maintaining the dataset?\n\nThe dataset is hosted and supported by web servers at Cornell. The CS department at Cornell will be maintaining the dataset.\n\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)?\n\nHangyu and Chia-Hsiang can be contacted via email (hz477@cornell.edu, and ck696@cornell.edu). More updated information can be found on the dataset webpage.\n\nIs there an erratum? If so, please provide a link or other access point.\n\nNo.\n\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub)?\n\nThe updates to the dataset will be posted on the dataset webpage.\n\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.\n\nOur dataset does not contain information about individuals.\n\nWill older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how."}]}