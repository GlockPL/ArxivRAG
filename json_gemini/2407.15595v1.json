{"title": "Discrete Flow Matching", "authors": ["Itai Gat", "Tal Remez", "Neta Shaul", "Felix Kreuk", "Ricky T. Q. Chen", "Gabriel Synnaeve", "Yossi Adi", "Yaron Lipman"], "abstract": "Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions: (i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser (x-prediction) and noise-prediction (e-prediction); (iii) practically, focusing on specific probability paths defined with different schedulers considerably improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on_1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.", "sections": [{"title": "1 Introduction", "content": "Despite the remarkable success of diffusion and flow models in generating continuous spatial signals such as images (Ho et al., 2020; Rombach et al., 2022; Esser et al., 2024) and videos (Singer et al., 2022; Blattmann et al., 2023), their performance still falters when applied to discrete sequential data compared to autoregressive models. Recent progress in adapting diffusion and flow models to the discrete setting has been made via mostly two approaches: embedding the discrete data in continuous space and applying continuous diffusion (Dieleman et al., 2022; Stark et al., 2024) or designing diffusion or flow processes over discrete state spaces (Austin et al., 2021a; Campbell et al., 2022).\nIn this paper, we pursue the discrete flow approach of Campbell et al. (2024) and introduce Discrete Flow Matching (FM), a theoretical framework and algorithmic methodology for discrete flow models that yields a state-of-the-art discrete non-autoregressive generative approach. Surprisingly, Discrete FM exhibits similarities with the continuous Flow Matching (Lipman et al., 2022) approach proposed for continuous signals. Notably, its generating probability velocity, employed in the sampling algorithm, is identical in form to its continuous counterpart. Additionally, Discrete FM offers the following advancements and simplifications over prior methods: It encompasses a more comprehensive family of probability paths transforming source (noise) distributions into target (data) distributions, accommodating arbitrary source-target couplings and time-dependent schedulers. Furthermore, it provides a unified formulation for the generating probability velocity directly expressed in terms of the learned posteriors and schedulers, along with a unified and general theory and algorithm for corrector sampling and iterations. In practice, we observe that path and corrector schedulers are pivotal, and their proper tuning leads to substantial improvements in generation quality. We have trained a 1.7B parameter Discrete FM model on the same data mix as in Llama-2 (Touvron et al., 2023) and CodeLlama (Roziere et al., 2023), achieving 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks; Figure 1 shows some code generation examples. In conditional text generation our model produces texts with a generated perplexity score of 9.7 as measured by the Llama-3 8B model, surpassing a 1.7B autoregressive model that achieves 22.3 and not far from the Llama-2 7B model that achieves 8.3 in perplexity score. We strongly believe that Discrete FM represents a"}, {"title": "2 Discrete Flow Matching", "content": "In discrete sequence modeling, we denote a sequence x as an array of N elements $(x^1,x^2,...,x^N)$. Each element, or token, within this sequence is selected from a vocabulary of size d. Consequently, the entire set of possible sequences is given by $D = [d]^N$, where $[d] = {1, ..., d}$. A random variable taking values in the space D is denoted by X and its corresponding probability mass function (PMF) is $P(X = x)$. For simplicity, throughout the paper, we sometimes omit the random variable X and use p(x) to denote the PMF.\nTo describe marginalization properties, we denote $p(x^i)$ the $x^i$ marginal of p, i.e., $p(x^i) = \\sum_{x^{-i}} p(x)$, where $x^{-i} = (..., x^{i-1},x^{i+1}, ...) \\in [d]^{N-1}$ are all the arguments excluding i. Similarly, $p(x^{i}) = \\sum_{x_{i}} p(x)$, and $x^{i} \\in [d]$.\nA useful PMF is the delta function, $\\delta_{y}$, $y \\in D$, which is defined by\n$\\delta_{y}(x) = \\prod_{i=1}^{N} \\delta_{y_i}(x^i)$, where $\\delta_{y_i}(x^i) =\\begin{cases}1 & x^i = y_i \\\\0 & x^i \\neq y_i\\end{cases}$\nWith the marginal notation $\\delta_{y}(x^{-i}) = \\delta_{y^i}(x^{-i})$ and $\\delta_{y}(x^{i}) = \\delta_{y^i}(x^{i}) = \\prod_{j \\neq i} \\delta_{y_j}(x^j)$ which simplifies notation."}, {"title": "2.2 Source and target distributions", "content": "In discrete generative models our goal is to transform source samples $X_0 \\sim p$ to target samples $X_1 \\sim q$. Our training data, consist of pairs $X_0$ and $X_1$ that are sampled from a joint distribution $\\pi(x, y)$, satisfying the marginals constraints $p(x) = \\sum_{y\\in D} \\pi(x, y)$, $q(y) = \\sum_{x\\in D} \\pi(x, y)$, i.e.,\n$(X_0, X_1) \\sim \\pi(X_0, X_1)$.\nIn the simplest case, the training pairs $X_0$ and $X_1$ are sampled independently from the source and target distributions respectively,\n$(X_0, X_1) \\sim p(X_0)q(X_1)$.\nExample: source and couplings. Common instantiations of source distribution p are: (i) adding a special token value often referred to as a 'mask' or 'dummy' token, denoted here by m, and setting the source distribution"}, {"title": "2.3 Probability paths", "content": "We follow the Flow Matching approach (Lipman et al., 2022; Liu et al., 2022; Albergo and Vanden-Eijnden, 2022) that uses a predefined probability path $p_t$ interpolating p and q, i.e.,\n$p_0 = p$ and $p_1 = q$\nto train the generative model taking a source sample $X_0 \\sim p$ to a target sample $X_1 \\sim q$. We use arbitrary coupling of source and target (Pooladian et al., 2023; Tong et al., 2023), $\\pi(x_0, x_1)$, and the symmetric FM path (Albergo and Vanden-Eijnden, 2022) to define the marginal probability path,\n$p_t(x) = \\sum_{x_0,x_1 \\in D} p_t(x|x_0, x_1)\\pi(x_0, x_1)$, where $p_t(x|x_0,x_1) = \\prod_{i=1}^{N} p_t(x^i|x_0, x_1)$,\nand $p_t(x^i|x_0, x_1)$ is a time-dependent probability on the space of tokens [d] conditioned on the pair $x_0, x_1$, and satisfying $p_0(x^i|x_0,x_1) = \\delta_{x_0}(x^i)$ and $p_1(x^i|x_0,x_1) = \\delta_{x_1}(x^i)$. If the conditional path $p_t(x^i|x_0,x_1)$ satisfies these boundary conditions then the marginal path $p_t(x)$ satisfies equation 6.\nIn developing the framework, we would like to consider as general as possible set of probability paths that are also tractable to learn within the Flow Matching framework. We consider conditional probability paths as a convex sum of m conditional probabilities $\\omega^j(x^i|x_0, x_1)$, i.e.,\n$p_t(x^i|x_0, x_1) = \\sum_{j=1}^{m} \\kappa_t^j\\omega^j(x^i|x_0, x_1)$,\nwhere $\\sum_j \\kappa_t^j = 1$ and $\\kappa_t^j \\geq 0$ are collectively called the scheduler. Note that the scheduler can be defined independently for each location in the sequence $i \\in [N]$ or uniformly for all tokens, $\\kappa_{i,j} = \\kappa^j$.\nA simple yet useful instance of these conditional paths is reminiscent of the continuous Flow Matching paths formulated as convex interpolants,\n$p_t(x^i|x_0, x_1) = (1 - \\kappa_t)\\delta_{x_0}(x^i) + \\kappa_t\\delta_{x_1}(x^i)$,\nwhere the scheduler $\\kappa_t$ satisfies $\\kappa_0 = 0$, $\\kappa_1 = 1$, and monotonically increasing in t. Another interesting instantiation of equation 8 is adding uniform noise with some probability depending on t,\n$p_t(x^i|x_0,x_1) = \\kappa_t^1\\delta_{x_1}(x^i) + \\kappa_t^2p_u(x^i) + \\kappa_t^3\\delta_{x_0}(x^i)$,\nwhere $\\kappa_0^3 = 0$, $\\kappa_1^1 = 1$, $\\kappa_0^1 = \\kappa_1^3 = 0$ (remembering that $\\sum_j \\kappa_t^j = 1$)."}, {"title": "2.4 Generating Probability Velocities", "content": "Continuous generating velocity. Sampling in continuous FM is performed by updating the current (continuous) sample $X_t \\in R^N$, t \u2208 [0, 1), according to a learned generating velocity field u(Xt), i \u2208 [N]. Euler sampling follows the (deterministic) rule\n$X_{i+h} = X_i + hu(X_t)$,\nwhere h > 0 is a user-defined time step. Note that equation 11 is updating separately each of the sample coordinates, $X_i, i \\in [N]$, see e.g., Figure 2, left. The velocity u(Xt) can be either directly modeled with a neural network, or parameterized via the denoiser (a.k.a. x-prediction) or noise-prediction (a.k.a. $\\epsilon$-prediction), see left column in Table 1. If, for all t \u2208 [0,1), starting at $X_t \\sim p_t$ and sampling with equation 11 provides $X_{t+h} \\sim p_{t+h+o(h)}$ then we say that ut generates pt.\nGenerating probability velocity. For defining FM in the discrete setting, we follow Campbell et al. (2024) and consider a Continuous-Time discrete Markov Chain (CTMC) paradigm, namely the sample $X_t$ is jumping between states in D, depending on a continuous time value t \u2208 [0,1]. Similar to the continuous FM setting described above, we focus on a model that predicts the rate of probability change of the current sample $X_t$ in each of its N tokens, see Figure 2, middle-left. Then, each token of the sample $X_t \\sim p_t$ is updated independently by\n$X_{i+h} \\sim \\delta_{X_t(i)} + hu_t(\\cdot, X_t)$,\nwhere we call ut the probability velocity as reminiscent of the velocity field in continuous Flow Matching, and as in the continuous case, we define:\nDefinition 1. Probability velocity ut generates the probability path pt if, for all t \u2208 [0,1) and given a sample $X_t \\sim p_t$, the sample $X_{t+h}$ defined in equation 12 satisfies $X_{t+h} \\sim p_{t+h} + o(h)$.\nAlgorithm 1 formulates a basic sampling algorithm given a generating probability velocity ut. In order for the r.h.s. of equation 12 to define a proper PMF for sufficiently small h > 0, it is necessary and sufficient that the probability velocity satisfies the conditions\n$\\sum_{x^i \\in [d]} u(x^i, z) = 0$, and $u(x^i, z) \\geq 0$ for all $i \\in [N]$ and $x^i \\neq z$.\nNow the main question is how to find a probability velocity ut that generates the probability path defined in equations 7 and 8? A key insight in Flow Matching (Lipman et al., 2022) is that ut can be constructed as a marginalization of conditional probability velocities, $u(x^i, z|x_0,x_1)$, generating the corresponding conditional probability paths $p_t(x^i|x_0,x_1)$. This can also be shown to hold in the discrete CTMC setting (Campbell et al., 2024), where a reformulation in our context and notation is as follows."}, {"title": "Theorem 2.", "content": "Given a conditional probability velocity $u(x^i, z|x_0, x_1)$ generating a conditional probability path $p_t(x|x_0,x_1)$, the marginal velocity defined by\n$u(x^i, z) = \\sum_{x_0,x_1 \\in D} u_x(x^i, z|x_0, X_1)p_t(x_0, X_1|z)$\ngenerates the marginal probability path pt(x), where by Bayes' rule\n$p_t(x_0, X_1|z) = \\frac{p_t(z|x_0, X_1)\\pi(x_0, X_1)}{p_t(z)}$\nFor completeness we provide a simple proof of this theorem in Appendix E.2. The proof, similar to the continuous FM case, shows that ut and pt satisfy the (discrete version of the) Continuity Equation."}, {"title": "The Continuity Equation.", "content": "To provide the mathematical tool for showing that a probability velocity ut does indeed generate the probability path pt, and also to further highlight the similarities to the continuous case, we next formulate the Kolmogorov Equations, which describe the state probability rate pt(x), x \u2208 D, in CTMC as a Continuity Equation (CE). The Continuity Equation, similarly to Kolmogorov Equations, describes pt(x), x \u2208 RN in the continuous case, and is formulated as the Partial Differential Equation (PDE)\n$\\dot{p_t(x)} + div_x(p_tu_t) = 0$,\nwhere the divergence operator $div_x(v)$ applied to a vector field $v : R^N \\to R^N$ is defined by\n$div_x(v) = \\sum_{i=1}^{N} \\partial_x v_i (x)$,\nand intuitively means the total flux leaving x, see Figure 2 (middle-right). This gives an intuitive explanation to the Continuity Equation: the rate of the probability pt(x) of a state x \u2208 RN equals the total incoming probability flux, p_tut, at x. In the discrete case (CTMC) the Continuity Equation (equation 16) holds as is, once the discrete divergence operator is properly defined, i.e., to measure the outgoing flux from a discrete state. In more detail, given some vector field, which in the discrete case is a scalar-valued function over pairs of states, v: D \u00d7 D \u2192 R, the discrete divergence is\n$div_x (v) = \\sum_{z \\in D} [v(z, x) - v(x, z)]$,\nwhere v(z, x) represents the flux x \u2192 z and v(x, z) represent the opposite flux z \u2192 x; see Figure 2, right. Now, in our case (see Figure 2, middle-left), the probability flux at a state x \u2208 D involves all sequences with at most one token difference from x, i.e., the probability flux p_tut at x takes the form $v(x, z) = p_t(z)u(x^i, z)$ and $v(z,x) = p_t(x)u_i(z^i, x)$ for z and x that differ only in the i-th token, $v(x, x) = \\sum_{ i = 1}^N u(x^i, x)$, and v(x, z) = 0 for all other (z,x) \u2208 D \u00d7 D. A direct calculation now shows (see Appendix E.1):\n$div_x (p_tu_t) = - \\sum_{z \\in D} p_t(z) \\sum_{i=1}^{N} \\delta_{z^i}(x^i)u(x^i, z)$.\nChecking that a probability velocity ut generates a probability path pt (in the sense of Definition 1) amounts to verifying the Continuity Equation (equation 16). Indeed, using arguments from Campbell et al. (2024) and the discrete divergence operator, the PMF of $X_{t+h}$ defined by sampling according to equation 12 is\n$E_{X_t} \\prod_{i=1}^{N} [\\delta_{x_i}(x^i) + hu(x^i, X_t)] = E_{X_t} \\delta_x(x) + h \\sum_{i=1}^{N} \\delta_{x_i( )}(x^i)u(x^i,X_t) + o(h)$\n$= p_t(x) - hdiv_x (p_tu_t) + o(h) = p_t(x) + h\\dot{p_t}(x) + o(h) = p_{t+h}(x) + o(h)$,"}, {"title": "Theorem 2", "content": "Given a conditional probability velocity $u(x^i, z|x_0, x_1)$ generating a conditional probability path $p_t(x|x_0,x_1)$, the marginal velocity defined by\n$u(x^i, z) = \\sum_{x_0,x_1 \\in D} u_x(x^i, z|x_0, X_1)p_t(x_0, X_1|z)$\ngenerates the marginal probability path pt(x), where by Bayes' rule\n$p_t(x_0, X_1|z) = \\frac{p_t(z|x_0, X_1)\\pi(x_0, X_1)}{p_t(z)}$\nFor completeness we provide a simple proof of this theorem in Appendix E.2. The proof, similar to the continuous FM case, shows that ut and pt satisfy the (discrete version of the) Continuity Equation."}, {"title": "Theorem 3", "content": "A generating probability velocity for the conditional paths $p_t (x|x_0, x_1)$ defined in equations 7 and 8 is\n$u(x^i, X_t|x_0, x_1) = \\sum_{j=1}^{m} a_t^{i,j}\\omega^j(x^i|x_0, x_1) + b_t^{i,l}\\delta_z(x^i)$,\nwith $a_t^{i,j} = - \\kappa_t^j / \\kappa_t^l$, and $b_t^{i,l} = \\kappa_t^{i,l} / \\kappa_t^{i,l}$ where $l = arg min_{j \\in [m]} [\\kappa_t^j / \\kappa_t^j]$."}, {"title": "4 Experiments", "content": "We evaluate our method on the tasks of language modeling, code generation, and image generation. For language modeling, we compare the proposed method against prior work considering the widely used generative perplexity metric. We scale the models to 1.7 billion parameters and present results on coding"}, {"title": "5 Conclusions and future work", "content": "We introduce Discrete Flow Matching, a generalization of continuous flow matching and discrete flows that provides a large design space of discrete non-autoregressive generative models. Searching within this space we were able to train large scale language models that produce generated text with an improved generative perplexity compared to current non-autoregressive methods and able to solve coding tasks at rates not achievable before with non-autoregressive models, as far as we are aware. While reducing the number of network evaluations required to generate a discrete sample compared to autoregressive models, Discrete FM still does not achieve the level of sampling efficiency achieved by its continuous counterpart, flagging an interesting future work direction. Another interesting direction is to explore the space of probability paths in equation 8 (or a generalization of which) beyond what we have done in this paper. We believe discrete non-autoregressive models have the potential to close the gap and even surpass autoregressive models as well as unlock novel applications and use cases. As our work introduces an alternative modeling paradigm to discrete sequential data such as language and code, we feel it does not introduce significant societal risks beyond those that already exist with previous large language models."}, {"title": "Appendix", "content": "We provide here some more details on relevant related works.\nContinuous diffusion and flows. Another line of works has been exploring the use of continuous space diffusion for discrete data, typically operating in the logits space (Dieleman et al., 2022; Li et al., 2022; Han et al., 2022; Lin et al., 2022; Chen et al., 2022). An additional body of work has been focusing on the adoption of latent diffusion-like modeling (Lovelace et al., 2022; He et al., 2022). Stark et al. (2024) proposed to learn a continuous Flow Matching on the probability simplex with Dirichlet paths.\nAutoregressive modeling. Autoregressive models have been a significant area of focus in recent years, particu- larly in the context of natural language processing and machine learning (Zhao et al., 2023). Autoregressive modeling, in its most fundamental form, utilizes the chain rule to learn the joint sequence probability by breaking it down into next-token conditional probabilities. GPT-2 (Radford et al., 2019), showcased the power of autoregressive language models in generating coherent and contextually relevant text over long passages. Its successor, GPT-3 (Brown et al., 2020), further pushed the boundaries, demonstrating impressive performance across a range of tasks without task-specific training data. Later models were adapted to other domains such as, code (Roziere et al., 2023; Li et al., 2023; Chen et al., 2021), biology (Zhang et al., 2024; Ferruz and H\u00f6cker, 2022; Madani et al., 2023), math (Romera-Paredes et al., 2024; Imani et al., 2023; Ahn et al., 2024), audio (Kreuk et al., 2022; Copet et al., 2024; Hassid et al., 2024) and more.\nMasked generative modeling. Masked generative modeling proposes to mask a variable portion of the input sequence and training a model to predict this masked section. Ghazvininejad et al. (2019) proposed Mask- Predict, a masked language modeling with parallel decoding. Savinov et al. (2021) extended the mask-modeling approach by employing an additional loss term that incorporates rolling model predictions. MaskGIT (Chang et al., 2022) followed a similar path, for the task of class-conditioned image synthesis, Chang et al. (2023) extended this approach to high-quality textually guided image generation over low-resolution images followed by a super-resolution module. Recently, Ziv et al. (2024) proposed a text-to-music method, which relies on the MaskGIT foundations while observing that span masking boosts the quality of the generated sequence significantly."}, {"title": "B Further implementation details", "content": "Safe sampling. When sampling according to Algorithm 1 using the generating probability velocity in equation 22, an arbitrary step size h > 0 can make some probabilities in $\\delta_{x(.)} + hu_t(\\cdot, X_t)$ negative and consequently require clamping and injecting further error into the sampling process that can in turn accumulate to a non-negligible global sampling error. A simple fix that guarantees a valid probability distribution while keeping the o(h) sampling error at the relatively manageable price of potentially more function evaluations is using the following adaptive step size in Algorithm 1: at time t \u2208 [0, 1) use\n$h_{adaptive} = min\\big[h, min_{i,l} {\\frac{\\kappa_t^{i,l}}{\\dot{\\kappa_t^{i,l}}}, min_{i,l} {\\frac{\\bar{\\kappa_t^{i,l}}}{\\bar{\\dot{\\kappa_t^{i,l}}}}} \\big]$\nAs can be verified with the general probability velocity formula in equation 22, the above choice for $h_{adaptive}$ guarantees $\\delta_{x(.)} + hu_t(\\cdot, X_t)$ is a valid PMF. As mostly used in this paper, for the probability denoiser parameterization (equation 24) the adaptive step is\n$h_{adaptive} = min[h, \\frac{1-\\kappa_t}{\\dot{\\kappa_t}}]$"}, {"title": "Conditioning.", "content": "In our unconditional coupling (U-coupling), see equation 5, we define the conditioning pattern based on prefixes of random length $N_0 < N$, i.e.,\n$\\zeta = (\\underbrace{1, ..., 1}_{N_0}, \\underbrace{0, ..., 0}_{N-N_0})$\nDuring the training phase, we sample $N_0 \\sim U(0, N)$ and adjust the input sequence in accordance with the mask $\\zeta$.\nDuring conditional sampling with Algorithm 1 we replace, after each update step, the relevant tokens with the conditioned ones, i.e., $X = \\zeta \\odot Y + (1 - \\zeta) \\odot X$, where X is the current sample, Y is the condition, and $\\zeta$ is the condition's mask.\nNFE bound. For mask modeling, i.e., $p = \\delta_m$, we have seen that the probability denoiser is time-independent (see Proposition 6). Consequently, when sampling with Algorithm 1 and ut from equation 24 without corrector sampling one is not required to recompute the forward pass $p_{1|t}(\\cdot|X_t)$ if $X_t$ is identical to $X_{t-h}$ (i.e., no m has been unmasked). This means that the NFE of Algorithm 1 in this case is bounded by the number of tokens N.\nPost training scheduler change. For a trained posterior $\\hat{w}_t(x^i|z)$ of a conditional probability path as in equation 9 with a scheduler $\\kappa_t$, the velocity is given by equations 24 or 25, where $\\hat{w}_t(x^i|z)$ is either $p_{1|t}(x^i z)$ or $p_{0|t}(x^i z)$ respectively. In this case, we can apply the velocities in equations 24 and 25 for sampling with any scheduler $\\kappa^\\prime_t$, using the change of scheduler formula for posteriors,\n$\\omega^{\\prime}_t(x^i|z) = \\hat{\\omega}_t(x^i|z)$,\nwhere $\\omega^{\\prime}_t(x^i|z)$, is the posterior of the scheduler $\\kappa_t^{\\prime}$, $t^{\\prime} = \\kappa[\\kappa^{-1}]^{-1}$, and $\\kappa^{-1}$ is the inverse of $\\kappa$. The scheduler change formula in equation 32 is proved in Proposition 8. We note that by Proposition 6, for mask modeling, i.e., $p = \\delta_m$, the posterior $\\omega_t (x^i|z)$ is time independent. Hence, in that case, the posterior is not affected by a scheduler change."}, {"title": "C Code infilling", "content": "We additionally evaluate the proposed method considering the task of code infilling. In which, we are provided with an input prompt that contains various spans of masked tokens, and our goal is to predict them based on the unmasked ones. See Figure 1 (middle and right sub-figures) for a visual example. Notice, this evaluation setup is the most similar to the training process.\nFor that, we randomly mask tokens with respect to several masking rations, $p \\in {0.0, 0.1, 0.2, ..., 1.0}$, from HumanEval and report both pass@1 and compiles@1 metrics. For the purpose of this analysis, we provide the oracle length for each masked span. In other words, the model predicts the masked to- kens for already given maks length. Results for the 1.5B parameters models can be seen in Figure 4. As expected, both pass@1 and compiles@1 keep im- proving as we decrease the level of input masking."}, {"title": "D Ablations", "content": "Train and sampling path scheduler choice (kt). We study how the choice of the probability path scheduler affects the model performance. For that, we consider a parametric family of cubic polynomial with parameters a, b:\n$\\kappa_t = -2t^3 + 3t^2 + a(t^3 - 2t^2 + t) + b(t^3 - t^2)$.\nNote that $\\kappa_0 = 0$ and $\\kappa_1 = 0$ and a and b are setting the derivative of $\\kappa_t$ at t = 0 and t = 1, respectively. We visualize this kt with choices of a, b \u2208 {0,1,2} in Figure 5a.\nTo test the effect of path schedulers in training we have trained 150M parameters models for all choices of a, b \u2208 {0,1,2,3}. We then generate 1000 samples from each model. The samples are computed using Algorithm 1 with the path scheduler the model was trained on, and with temperature levels \u03c4\u2208 {0.8,0.9, 1}, where temperature is applied via\n$\\mathcal{P}_{1|t}(x^i|X_t) = r^{-1} log p_{1|t}(x^i|X_t)$.\nWe then evaluate the generative perplexity of these samples with GPT-2. Figure 6 shows the results. The graphs indicate that, in the context of text modality, the cubic polynomial scheduler with a = 0, b = 2 (equivalent to a square function) achieves the highest performance. Consequently, we exclusively used this scheduler for the language models.\nCorrector scheduler. In our experiments we only applied corrector sampling to our large models (U-coupling and C-coupling; 1.7B parameters). We used the optimal path schedulers from previous section and considered the following parametric family of schedulers for the corrector sampling:\n$\\alpha_t = 1 + a t^b(1 - t)^c$,"}, {"title": "E Theory and proofs", "content": "We present the computation of the discrete divergence in equation 18, i.e.,\n$div_x (p_tu_t) = - \\sum_{z \\in D} p_t(z) \\sum_{i=1}^{N} \\delta_{z^i}(x^i)u(x^i, z)$\nComputing the discrete divergence (equation 18) of the flux p_tut at a state x amounts to adding outgoing flux from x and subtracting the incoming flux into x. Using the fact that $d_z(x^i) = 1$ if and only if z = x or z"}, {"title": "Theorem 2", "content": "Given a conditional probability velocity $u(x^i, z|x_0, x_1)$ generating a conditional probability path $p_t(x|x_0,x_1)$, the marginal velocity defined by\n$u(x^i, X_t) = \\sum_{x_0,x_1 \\in D} u_x(x^i, X_t|x_0, X_1)p_t(x_0, X_1|X_t)$\ngenerates the marginal probability path pt(x), where by Bayes' rule\n$p_t(x_0, X_1|X_t) = \\frac{p_t(X_t|x_0, X_1)\\pi(x_0, X_1)}{p_t(X)}$\nProof (Theorem 2). We start by taking the time derivative of the marginal probability path, pt(x) = $\\sum_{x_0,x_1} p_t(x^i|x_0, X_1)\\pi(x_0,x_1)$, as follows,\n$\\dot{p_t(x)} = \\sum_{x_0,x_1} \\dot{p_t(x|x_0, X_1)}\\pi(x_0, X_1)$\n$\\dot{p_t(x)} = -div_x (p_tu_t)$"}, {"title": "Theorem 3", "content": "A generating probability velocity for the conditional paths"}]}