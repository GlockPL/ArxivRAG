{"title": "Discrete Flow Matching", "authors": ["Itai Gat", "Tal Remez", "Neta Shaul", "Felix Kreuk", "Ricky T. Q. Chen", "Gabriel Synnaeve", "Yossi Adi", "Yaron Lipman"], "abstract": "Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions: (i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser (x-prediction) and noise-prediction (e-prediction); (iii) practically, focusing on specific probability paths defined with different schedulers considerably improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on_1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.", "sections": [{"title": "1 Introduction", "content": "Despite the remarkable success of diffusion and flow models in generating continuous spatial signals such as images (Ho et al., 2020; Rombach et al., 2022; Esser et al., 2024) and videos (Singer et al., 2022; Blattmann et al., 2023), their performance still falters when applied to discrete sequential data compared to autoregressive models. Recent progress in adapting diffusion and flow models to the discrete setting has been made via mostly two approaches: embedding the discrete data in continuous space and applying continuous diffusion (Dieleman et al., 2022; Stark et al., 2024) or designing diffusion or flow processes over discrete state spaces (Austin et al., 2021a; Campbell et al., 2022).\nIn this paper, we pursue the discrete flow approach of Campbell et al. (2024) and introduce Discrete Flow Matching (FM), a theoretical framework and algorithmic methodology for discrete flow models that yields a state-of-the-art discrete non-autoregressive generative approach. Surprisingly, Discrete FM exhibits similarities with the continuous Flow Matching (Lipman et al., 2022) approach proposed for continuous signals. Notably, its generating probability velocity, employed in the sampling algorithm, is identical in form to its continuous counterpart. Additionally, Discrete FM offers the following advancements and simplifications over prior methods: It encompasses a more comprehensive family of probability paths transforming source (noise) distributions into target (data) distributions, accommodating arbitrary source-target couplings and time-dependent schedulers. Furthermore, it provides a unified formulation for the generating probability velocity directly expressed in terms of the learned posteriors and schedulers, along with a unified and general theory and algorithm for corrector sampling and iterations. In practice, we observe that path and corrector schedulers are pivotal, and their proper tuning leads to substantial improvements in generation quality. We have trained a 1.7B parameter Discrete FM model on the same data mix as in Llama-2 (Touvron et al., 2023) and CodeLlama (Roziere et al., 2023), achieving 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks; Figure 1 shows some code generation examples. In conditional text generation our model produces texts with a generated perplexity score of 9.7 as measured by the Llama-3 8B model, surpassing a 1.7B autoregressive model that achieves 22.3 and not far from the Llama-2 7B model that achieves 8.3 in perplexity score. We strongly believe that Discrete FM represents a"}, {"title": "2 Discrete Flow Matching", "content": "2.1 Setup and notations\nIn discrete sequence modeling, we denote a sequence x as an array of N elements (x1,x2,...,xN). Each element, or token, within this sequence is selected from a vocabulary of size d. Consequently, the entire set of possible sequences is given by D = [d]N, where [d] = {1, ..., d}. A random variable taking values in the space D is denoted by X and its corresponding probability mass function (PMF) is P(X = x). For simplicity, throughout the paper, we sometimes omit the random variable X and use p(x) to denote the PMF.\nTo describe marginalization properties, we denote p(xi) the xi marginal of p, i.e., p(xi) = \u2211xi p(x), where xi = (..., xi\u22121,xi+1, . . .) \u2208 [d]N\u22121 are all the arguments excluding i. Similarly, p(xi) = \u2211xi p(x), and xi \u2208 [d]. A useful PMF is the delta function, \u03b4y, y \u2208 D, which is defined by\n$\\delta_y(x) = \\prod_{i=1}^N \\delta_{y^i}(x^i), \\text{where } \\delta_{y^i}(x^i) = \\begin{cases} 1 & x^i = y \\\\ 0 & x^i \\neq y \\end{cases}$   (1)\nWith the marginal notation \u03b4y(xi) = \u03b4yi (xi) and \u03b4y(xi) = \u03b4yi(xi) = \u03a0j\u2260i \u03b4yi (xi) which simplifies notation.\n2.2 Source and target distributions\nIn discrete generative models our goal is to transform source samples Xo ~p to target samples X1 ~ q. Our training data, consist of pairs Xo and X1 that are sampled from a joint distribution \u03c0(x, y), satisfying the marginals constraints p(x) = \u2211y\u2208D \u03c0(x, y), q(y) = \u03a3x\u2208D \u03c0(x, y), i.e.,\n(\u03a7\u03bf, \u03a71) ~ \u03c0(\u03a7\u03bf, \u03a71).   (2)\nIn the simplest case, the training pairs Xo and X1 are sampled independently from the source and target distributions respectively,\n(X0, X1) ~ p(\u0425\u043e)q(X1).   (3)\nExample: source and couplings. Common instantiations of source distribution p are: (i) adding a special token value often referred to as a 'mask' or 'dummy' token, denoted here by m, and setting the source distribution"}, {"title": "2.3 Probability paths", "content": "We follow the Flow Matching approach (Lipman et al., 2022; Liu et al., 2022; Albergo and Vanden-Eijnden, 2022) that uses a predefined probability path pt interpolating p and q, i.e.,\nPo = p and P1 = q   (6)\nto train the generative model taking a source sample Xo ~p to a target sample X1 ~ q. We use arbitrary coupling of source and target (Pooladian et al., 2023; Tong et al., 2023), \u03c0(xo, x1), and the symmetric FM path (Albergo and Vanden-Eijnden, 2022) to define the marginal probability path,\n$p_t(x) = \\sum_{x_0,x_1 \\in D} p_t(x | x_0, x_1) \\pi(x_0, x_1), \\text{ where } p_t(x | x_0, x_1) = \\prod_{i=1}^N p_t(x^i | x_0, x_1),$   (7)\nand pt (xi|xo, x1) is a time-dependent probability on the space of tokens [d] conditioned on the pair xo, x1, and satisfying po(xi|xo,x1) = \u03b4xo (xi) and p1(xi|xo,x1) = \u03b4x1(xi). If the conditional path pt(xi|xo,x1) satisfies these boundary conditions then the marginal path pt(x) satisfies equation 6.\nIn developing the framework, we would like to consider as general as possible set of probability paths that are also tractable to learn within the Flow Matching framework. We consider conditional probability paths as a convex sum of m conditional probabilities wi (xi|xo, x1), i.e.,\n$p_t(x^i | x_0, x_1) = \\sum_{j=1}^m \\kappa_t^j \\omega^i(x^i | x_0, x_1),$   (8)\nwhere \u2211j \u03baj = 1 and \u03baj > 0 are collectively called the scheduler. Note that the scheduler can be defined independently for each location in the sequence i \u2208 [N] or uniformly for all tokens, \u03baj = \u03baj.\nA simple yet useful instance of these conditional paths is reminiscent of the continuous Flow Matching paths formulated as convex interpolants,\n$p_t(x^i | x_0, x_1) = (1 - \\kappa_t) \\delta_{x_0}(x^i) + \\kappa_t \\delta_{x_1}(x^i),$   (9)\nwhere the scheduler kt satisfies \u043a\u043e = 0, \u03ba1 = 1, and monotonically increasing in t. Another interesting instantiation of equation 8 is adding uniform noise with some probability depending on t,\n$p_t(x^i | x_0, x_1) = \\kappa_t^1 \\delta_{x_1}(x^i) + \\kappa_t^2 p_u(x^i) + \\kappa_t^3 \\delta_{x_0}(x^i),$   (10)\nwhere \u03ba0 = 0, \u03ba1 = 1, \u03ba2 = \u03ba3 = 0 (remembering that \u2211i \u03bai = 1)."}, {"title": "2.4 Generating Probability Velocities", "content": "Continuous generating velocity. Sampling in continuous FM is performed by updating the current (continuous) sample Xt \u2208 RN, t \u2208 [0, 1), according to a learned generating velocity field u(Xt), i \u2208 [N]. Euler sampling follows the (deterministic) rule\nXi+h = X + hu(Xt),   (11)\nwhere h > 0 is a user-defined time step. Note that equation 11 is updating separately each of the sample coordinates, Xi, i \u2208 [N], see e.g., Figure 2, left. The velocity u(Xt) can be either directly modeled with a neural network, or parameterized via the denoiser (a.k.a. x-prediction) or noise-prediction (a.k.a. \u025b-prediction), see left column in Table 1. If, for all t \u2208 [0,1), starting at Xt ~ pt and sampling with equation 11 provides Xt+h ~ Pt+h+o(h) then we say that ut generates pt.\nGenerating probability velocity. For defining FM in the discrete setting, we follow Campbell et al. (2024) and consider a Continuous-Time discrete Markov Chain (CTMC) paradigm, namely the sample Xt is jumping between states in D, depending on a continuous time value t \u2208 [0,1]. Similar to the continuous FM setting described above, we focus on a model that predicts the rate of probability change of the current sample Xt in each of its N tokens, see Figure 2, middle-left. Then, each token of the sample Xt ~ pt is updated independently by\nXi+h ~ \u03b4xi(.) + hu(, Xt),   (12)\nwhere we call ut the probability velocity as reminiscent of the velocity field in continuous Flow Matching, and as in the continuous case, we define:\nDefinition 1. Probability velocity ut generates the probability path pt if, for all t \u2208 [0,1) and given a sample Xt ~ pt, the sample Xt+h defined in equation 12 satisfies Xt+h ~ Pt+h + o(h).\nAlgorithm 1 formulates a basic sampling algorithm given a generating probability velocity ut. In order for the r.h.s. of equation 12 to define a proper PMF for sufficiently small h > 0, it is necessary and sufficient that the probability velocity satisfies the conditions\n$\\sum_{x^i \\in [d]} u(x^i, z) = 0, \\text{ and } u(x^i, z) \\ge 0 \\text{ for all } i \\in [N] \\text{ and } x^i \\neq z.$   (13)\nNow the main question is how to find a probability velocity ut that generates the probability path defined in equations 7 and 8? A key insight in Flow Matching (Lipman et al., 2022) is that ut can be constructed as a marginalization of conditional probability velocities, u(xi, z|xo,x1), generating the corresponding conditional probability paths pt(xi|x0,x1). This can also be shown to hold in the discrete CTMC setting (Campbell et al., 2024), where a reformulation in our context and notation is as follows."}, {"title": "Theorem 2", "content": "Given a conditional probability velocity u(xi, z|xo, x1) generating a conditional probability path Pt(x|xo,x1), the marginal velocity defined by\n$u(x^i, z) = \\sum_{x_0, x_1 \\in D} u_x(x^i, z | x_0, X_1) P_t(x_0, X_1 | z)$   (14)\ngenerates the marginal probability path pt(x), where by Bayes' rule\n$P_t(x_0, X_1 | z) = \\frac{P_t(z | x_0, X_1) \\pi(x_0, X_1)}{P_t(x)}$   (15)\nFor completeness we provide a simple proof of this theorem in Appendix E.2. The proof, similar to the continuous FM case, shows that ut and pt satisfy the (discrete version of the) Continuity Equation.\nThe Continuity Equation. To provide the mathematical tool for showing that a probability velocity ut does indeed generate the probability path pt, and also to further highlight the similarities to the continuous case, we next formulate the Kolmogorov Equations, which describe the state probability rate pt(x), x \u2208 D, in CTMC as a Continuity Equation (CE). The Continuity Equation, similarly to Kolmogorov Equations, describes pt(x), x \u2208 RN in the continuous case, and is formulated as the Partial Differential Equation (PDE)\n$\\dot{p_t}(x) + div_x (p_t u_t) = 0,$   (16)\nwhere the divergence operator divx(v) applied to a vector field v : RN \u2192 RN is defined by\n$div_x (v) = \\sum_{i=1}^N \\partial_{x^i} v(x),$   (17)\nand intuitively means the total flux leaving \u00e6, see Figure 2 (middle-right). This gives an intuitive explanation to the Continuity Equation: the rate of the probability pt(x) of a state x \u2208 RN equals the total incoming probability flux, ptut, at x. In the discrete case (CTMC) the Continuity Equation (equation 16) holds as is, once the discrete divergence operator is properly defined, i.e., to measure the outgoing flux from a discrete state. In more detail, given some vector field, which in the discrete case is a scalar-valued function over pairs of states, v: D \u00d7 D \u2192 R, the discrete divergence is\n$div_x (v) = \\sum_{z \\in D} [v(z, x) - v(x, z)],$   (18)\nwhere v(z, x) represents the flux x \u2192 z and v(x, z) represent the opposite flux z \u2192 x; see Figure 2, right. Now, in our case (see Figure 2, middle-left), the probability flux at a state x \u2208 D involves all sequences with at most one token difference from x, i.e., the probability flux ptut at x takes the form v(x, z) = pt(z)u(xi, z) and v(z,x) = pt(x)ui(zi, x) for z and x that differ only in the i-th token, v(x, x) = \u2211i = 1 u(xi, x), and v(x, z) = 0 for all other (z,x) \u2208 D \u00d7 D. A direct calculation now shows (see Appendix E.1):\n$div_x (p_t u_t) = - \\sum_{z \\in D} p_t(z) \\sum_{i=1}^N \\delta_z(x^i) u(x^i, z).$   (19)\nChecking that a probability velocity ut generates a probability path pt (in the sense of Definition 1) amounts to verifying the Continuity Equation (equation 16). Indeed, using arguments from Campbell et al. (2024) and the discrete divergence operator, the PMF of Xt+h defined by sampling according to equation 12 is\n$E_{X_t} \\left[ \\prod_{i=1}^N [\\delta_{x^i}(x^i) + h u(x^i, X_t)] \\right] = E_{X_t} \\left[ \\delta_x(x) + h \\sum_{i=1}^N \\delta_{x_i}(x^i) u(x^i, X_t) + o(h) \\right]$   (20)\n$= p_t(x) - h div_x (p_t u_t) + o(h) = p_t(x) + h \\dot{p_t}(x) + o(h) = P_{t+h}(x) + o(h),$"}, {"title": "Conditional and marginal generating velocities", "content": "We provide the probability velocities generating the conditional probability paths pt(x|x0,x1) defined in equations 7 and 8. Then, using the marginalization formula in equation 14 we end up with a closed-form marginal velocity for the probability paths pt(x). In Appendix E.3 we show\nTheorem 3 (Probability velocity of conditional paths). A generating probability velocity for the conditional paths pt(x|x0,x1) defined in equations 7 and 8 is\n$u(x^i, z | x_0, x_1) = \\sum_{j=1}^m a_t^{i,j} \\omega^i (x^i | x_0, x_1) + b_t^{i, l} \\delta_z(x^i),$   (21)\n$\\text{with } a_t^{i,j} = - \\frac{\\kappa_t^j}{\\kappa_t^l} \\text{ and } b_t^{i, l} = \\frac{\\dot{\\kappa_t^l}}{\\kappa_t^l} \\text{ where } l = \\arg \\min_{j \\in [m]} \\frac{\\dot{\\kappa_t^j}}{\\kappa_t^j}.$   \nNow, computing the marginal probability velocity using equation 14 applied to the conditional probability velocity in equation 21 gives\n$u(x^i, z) = \\sum_{j=1}^m a_t^{i,j} \\overline{\\omega^i} (x^i, z) + b_t^{i, l} \\delta_z(x^i),$   (22)\nwhere the posteriors w\u00afi of wi (that are later shown to be tractable to learn) are defined by\n$\\overline{\\omega^i} (x^i, z) = \\sum_{x_0, x_1 \\in D} \\omega^i (x^i | x_0, x_1) P_t(x_0, X_1 | z),$   (23)\nwhere pt (x0, X1|z) (defined in equation 15) is the posterior probability of xo, X1 conditioned on the current state Xt = z. A useful instantiation of the general velocity in equation 22 is when considering the path family in equation 9, for which w1 (xi|x0,x1) = \u03b4x1 (xi), w2 (xi|xo,x1) = \u03b4xo (xi), \u03ba1 = \u03bat, \u03ba2 = 1 - \u03bat, \u03ba1 = kt, kt \u2265 0 (i.\u0435., monotonically non-decreasing in t) and in this case equation 22 reads as\n$u(x^i, z) = \\frac{\\dot{\\kappa_t}}{1 - \\kappa_t} [P_{1 | t}(x^i | z) - \\delta_z(x^i)]$   (24)\nwhere we use the notation P1|t(xi|Xt) = \u2211x0,x1\u03b4x1(xi)pt(x0,x1|Xt) for the probability denoiser.\nSampling backward in time. We can also sample backwards in time by following the sampling rule Xi\u2212h ~ \u03b4x(\u00b7) \u2212 hu(\u00b7, Xt). In this case \u2212u(xi, z) should satisfy equation 13. A (backward-time) generating probability velocity can then be achieved from equation 22 with the simple change to the coefficients ai and bi, see Appendix E.4. For pt defined with equation 9 the generating velocity is"}, {"title": "2.5", "content": "$u(x^i, z) = \\frac{\\dot{\\kappa_t}}{\\kappa_t} [\\delta_z(x^i) - P_{0 | t}(x^i | z)],$   (25)\nwhere in this case P0|t(xi|z) = \u2211x0,x1\u2208D \u03b4x0 (xi)pt(x0, x1|z) is the probability noise-prediction.\nRemarkably, the generating velocity fields in 24 and 25 take the exact same form as the generating (a.k.a. marginal) velocity fields in continuous flow matching when parameterized via the denoiser or noise-prediction parameterizations and using the same schedulers, see Table 1 and Appendix E.9 for explanation of the continuous case. In Appendix E.4 we provide the backward-time version of Theorem 3.\nCorrector sampling. Combining the forward-time \u00fbt (eq. 24) and backward-time \u016dt (eq. 25), i.e.,\n$u(x^i, z) = a_t \\hat{u}(x^i, z) - \\beta_t \\check{u}(x^i, z),$   (26)\nprovides a valid forward-time probability velocity field (i.e., satisfies equation 13) for t \u2208 (0,1) as long as at, \u03b2t > 0. This velocity field can be used for two types of corrector sampling: (i) When at - \u03b2t = 1 sampling with \u016bt leads to corrector sampling where intuitively each step moves 1+ at forward in time and -at backwards, which allows reintroducing noise into the sampling process; and (ii) when at = \u03b2t = 0 sampling with ut when fixing t \u2208 (0,1) leads to corrector iterations where limit samples distribute according to pt. In Appendix E.6 we prove:\nTheorem 4. For perfectly trained posteriors and at, \u03b2t > 0, t \u2208 (0,1), \u016bt in equation 26 is a probability velocity, i.e., satisfies equation 13, and: (i) For at \u2212 \u03b2t = 1, \u016bt provides a probability velocity generating pt; (ii) For at = \u03b2t = 0, repeatedly sampling with \u016bt at fixed t \u2208 (0,1) and sufficiently small h is guaranteed to converge to a sample from pt.\nOne simplification to equation 26 can be done in the case of paths constructed with conditional as in equation 9, independent coupling \u03c0(xo, x1) = p(xo)q(x1), and i.i.d. source p(xo) = \u03a0Ni=1P(xi), e.g., p(x) is uniform over [d] or dm(x). In this case, the backward-time formula in equation 25 take an equivalent simpler form\n$u(x^i, z) = \\frac{\\dot{\\kappa_t}}{\\kappa_t} [\\delta_z(x^i) - p(x^i)],$   (27)\nwhich does not require estimation of the posterior Polt. See Appendix E.5 for the derivation.\nTraining. Equation 22 shows that for generating samples from a probabilty path pt(x) we require the posteriors wi(xi|Xt). Training such posteriors can be done by minimizing the loss\n$L(\\theta) = - \\sum_{j \\in [m], i \\in [N]} E_{t, (X_0, X_1), X_t, Y} \\log \\omega^i(Y | X_t; \\theta),$   (28)\nwhere t is sampled according to some distribution in [0,1] (we used uniform), (X0, X1) ~ \u03c0(\u03a7\u03bf, X1), Xt ~Pt(Xt|Xo, X1), and Y ~ wi (Y|X0, X1); \u03b8 \u2208 R\u03b8 denotes the learnable parameters. In the common case we use in this paper of learning a single posterior, i.e., the probability denoiser P1|t, the loss takes the form\n$L(\\theta) = - \\sum_{i \\in [N]} [E_{t, (X_0, X_1), X_i} \\log P_{1 | t}(X^i | X_t).$ "}, {"title": "3 Related work", "content": "In the section we cover the most related work to ours; in Appendix A we cover other related work.\nDiscrete Flows (Campbell et al., 2024) is probably the most related work to ours. We build upon their CTMC framework and offer the following generalizations and simplifications over their original formulation: Campbell et al. (2024) define a rate matrix (equivalent to our probability velocity) for generation, expressed as an expectation over the posterior, Exi~p1\t(X}\\X\u2081) R+ (X+, xi | Xi) where R+ is defined using the conditional probability path. This formulation requires computing an expectation during sampling in the general case (see their Algorithm 1) or a particular path-dependent derivation. We show, for a more general class of probability"}, {"title": "4 Experiments", "content": "We evaluate our method on the tasks of language modeling, code generation, and image generation. For language modeling, we compare the proposed method against prior work considering the widely used generative perplexity metric. We scale the models to 1.7 billion parameters and present results on coding"}, {"title": "5 Conclusions and future work", "content": "We introduce Discrete Flow Matching, a generalization of continuous flow matching and discrete flows that provides a large design space of discrete non-autoregressive generative models. Searching within this space we were able to train large scale language models that produce generated text with an improved generative perplexity compared to current non-autoregressive methods and able to solve coding tasks at rates not achievable before with non-autoregressive models, as far as we are aware. While reducing the number of network evaluations required to generate a discrete sample compared to autoregressive models, Discrete FM still does not achieve the level of sampling efficiency achieved by its continuous counterpart, flagging an interesting future work direction. Another interesting direction is to explore the space of probability paths in equation 8 (or a generalization of which) beyond what we have done in this paper. We believe discrete non-autoregressive models have the potential to close the gap and even surpass autoregressive models as well as unlock novel applications and use cases. As our work introduces an alternative modeling paradigm to discrete sequential data such as language and code, we feel it does not introduce significant societal risks beyond those that already exist with previous large language models."}, {"title": "A Related works, continuation", "content": "We provide here some more details on relevant related works.\nContinuous diffusion and flows. Another line of works has been exploring the use of continuous space diffusion for discrete data, typically operating in the logits space (Dieleman et al., 2022; Li et al., 2022; Han et al., 2022; Lin et al., 2022; Chen et al., 2022). An additional body of work has been focusing on the adoption of latent diffusion-like modeling (Lovelace et al., 2022; He et al., 2022). Stark et al. (2024) proposed to learn a continuous Flow Matching on the probability simplex with Dirichlet paths.\nAutoregressive modeling. Autoregressive models have been a significant area of focus in recent years, particularly in the context of natural language processing and machine learning (Zhao et al., 2023). Autoregressive modeling, in its most fundamental form, utilizes the chain rule to learn the joint sequence probability by breaking it down into next-token conditional probabilities. GPT-2 (Radford et al., 2019), showcased the power of autoregressive language models in generating coherent and contextually relevant text over long passages. Its successor, GPT-3 (Brown et al., 2020), further pushed the boundaries, demonstrating impressive performance across a range of tasks without task-specific training data. Later models were adapted to other domains such as, code (Roziere et al., 2023; Li et al., 2023; Chen et al., 2021), biology (Zhang et al., 2024; Ferruz and H\u00f6cker, 2022; Madani et al., 2023), math (Romera-Paredes et al., 2024; Imani et al., 2023; Ahn et al., 2024), audio (Kreuk et al., 2022; Copet et al., 2024; Hassid et al., 2024) and more.\nMasked generative modeling. Masked generative modeling proposes to mask a variable portion of the input sequence and training a model to predict this masked section. Ghazvininejad et al. (2019) proposed Mask-Predict, a masked language modeling with parallel decoding. Savinov et al. (2021) extended the mask-modeling approach by employing an additional loss term that incorporates rolling model predictions. MaskGIT (Chang et al., 2022) followed a similar path, for the task of class-conditioned image synthesis, Chang et al. (2023) extended this approach to high-quality textually guided image generation over low-resolution images followed by a super-resolution module. Recently, Ziv et al. (2024) proposed a text-to-music method, which relies on the MaskGIT foundations while observing that span masking boosts the quality of the generated sequence significantly."}, {"title": "B Further implementation details", "content": "Safe sampling. When sampling according to Algorithm 1 using the generating probability velocity in equation 22, an arbitrary step size h > 0 can make some probabilities in dx(\u00b7) + hu\u2084(\u00b7, X\u2081) negative and consequently require clamping and injecting further error into the sampling process that can in turn accumulate to a non-negligible global sampling error. A simple fix that guarantees a valid probability distribution while keeping the o(h) sampling error at the relatively manageable price of potentially more function evaluations is using the following adaptive step size in Algorithm 1: at time t \u2208 [0, 1) use\n$h_{\\text{adaptive}} = \\min \\left\\{ h, \\min_i \\min \\left\\{ \\frac{\\kappa_t^i}{\\dot{\\kappa_t^i}} \\right\\} \\right\\}$   (29)\nAs can be verified with the general probability velocity formula in equation 22, the above choice for hadaptive guarantees dx(\u00b7) + hu(, X\u2081) is a valid PMF. As mostly used in this paper, for the probability denoiser parameterization (equation 24) the adaptive step is\n$h_{\\text{adaptive}} = \\min \\left\\{ h, \\frac{1 - \\kappa_t}{\\dot{\\kappa_t}} \\right\\}$   (30)"}, {"title": "Conditioning", "content": "With the corrector sampling (equations 26 and 51) we have the adaptive step:\n$h_{\\text{adaptive}} = \\min \\left\\{ h, \\frac{A_t \\kappa_t}{\\dot{\\kappa_t}} + \\frac{B_t \\kappa_t}{\\dot{\\kappa_t}} \\right\\}$   (31)\nConditioning. In our unconditional coupling (U-coupling), see equation 5, we define the conditioning pattern based on prefixes of random length No < N, i.e.,\n$\\iota = (\\underbrace{1, ..., 1}_{N_0}, \\underbrace{0, ..., 0}_{N - N_0}).$\nDuring the training phase, we sample No ~ U(0, N) and adjust the input sequence in accordance with the mask 1.\nDuring conditional sampling with Algorithm 1 we replace, after each update step, the relevant tokens with the conditioned ones, i.e., X = [ \u2299 Y + (1 \u2212 1) \u2299 X, where X is the current sample, Y is the condition, and [ is the condition's mask.\nNFE bound. For mask modeling, i.e., p = dm, we have seen that the probability denoiser is time-independent (see Proposition 6). Consequently, when sampling with Algorithm 1 and ut from equation 24 without corrector sampling one is not required to recompute the forward pass P1|t(\u00b7|Xt) if Xt is identical to Xt\u2212h (i.e., no m has been unmasked). This means that the NFE of Algorithm 1 in this case is bounded by the number of tokens N.\nPost training scheduler change. For a trained posterior \u0175t(xi|z) of a conditional probability path as in equation 9 with a scheduler Kt, the velocity is given by equations 24 or 25, where \u0175t(xi|z) is either P1|t(xi|z) or P0|t(xi|z) respectively. In this case, we can apply the velocities in equations 24 and 25 for sampling with any scheduler \u03bat, using the change of scheduler formula for posteriors,\n$\\overline{\\omega_t}(x^i | z) = \\hat{\\omega}_{t'}(x^i | z),$   (32)\nwhere w\u00aft(xi|z), is the posterior of the scheduler \u03bat, t' = \u03ba\u22121\u03bat, and \u03ba\u22121 is the inverse of \u03ba. The scheduler change formula in equation 32 is proved in Proposition 8. We note that by Proposition 6, for mask modeling, i.e., p = dm, the posterior wt (xi\u2758z) is time independent. Hence, in that case, the posterior is not affected by a scheduler change."}, {"title": "C Code infilling", "content": "We additionally evaluate the proposed method considering the task of code infilling. In which"}, {"title": "Discrete Flow Matching", "authors": ["Itai Gat", "Tal Remez", "Neta Shaul", "Felix Kreuk", "Ricky T. Q. Chen", "Gabriel Synnaeve", "Yossi Adi", "Yaron Lipman"], "abstract": "Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions: (i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser (x-prediction) and noise-prediction (e-prediction); (iii) practically, focusing on specific probability paths defined with different schedulers considerably improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on_1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.", "sections": [{"title": "1 Introduction", "content": "Despite the remarkable success of diffusion and flow models in generating continuous spatial signals such as images (Ho et al., 2020; Rombach et al., 2022; Esser et al., 2024) and videos (Singer et al., 2022; Blattmann et al., 2023), their performance still falters when applied to discrete sequential data compared to autoregressive models. Recent progress in adapting diffusion and flow models to the discrete setting has been made via mostly two approaches: embedding the discrete data in continuous space and applying continuous diffusion (Dieleman et al., 2022; Stark et al., 2024) or designing diffusion or flow processes over discrete state spaces (Austin et al., 2021a; Campbell et al., 2022).\nIn this paper, we pursue the discrete flow approach of Campbell et al. (2024) and introduce Discrete Flow Matching (FM), a theoretical framework and algorithmic methodology for discrete flow models that yields a state-of-the-art discrete non-autoregressive generative approach. Surprisingly, Discrete FM exhibits similarities with the continuous Flow Matching (Lipman et al., 2022) approach proposed for continuous signals. Notably, its generating probability velocity, employed in the sampling algorithm, is identical in form to its continuous counterpart. Additionally, Discrete FM offers the following advancements and simplifications over prior methods: It encompasses a more comprehensive family of probability paths transforming source (noise) distributions into target (data) distributions, accommodating arbitrary source-target couplings and time-dependent schedulers. Furthermore, it provides a unified formulation for the generating probability velocity directly expressed in terms of the learned posteriors and schedulers, along with a unified and general theory and algorithm for corrector sampling and iterations. In practice, we observe that path and corrector schedulers are pivotal, and their proper tuning leads to substantial improvements in generation quality. We have trained a 1.7B parameter Discrete FM model on the same data mix as in Llama-2 (Touvron et al., 2023) and CodeLlama (Roziere et al., 2023), achieving 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks; Figure 1 shows some code generation examples. In conditional text generation our model produces texts with a generated perplexity score of 9.7 as measured by the Llama-3 8B model, surpassing a 1.7B autoregressive model that achieves 22.3 and not far from the Llama-2 7B model that achieves 8.3 in perplexity score. We strongly believe that Discrete FM represents a"}, {"title": "2 Discrete Flow Matching", "content": "2.1 Setup and notations\nIn discrete sequence modeling, we denote a sequence x as an array of N elements (x1,x2,...,xN). Each element, or token, within this sequence is selected from a vocabulary of size d. Consequently, the entire set of possible sequences is given by D = [d]N, where [d] = {1, ..., d}. A random variable taking values in the space D is denoted by X and its corresponding probability mass function (PMF) is P(X = x). For simplicity, throughout the paper, we sometimes omit the random variable X and use p(x) to denote the PMF.\nTo describe marginalization properties, we denote p(xi) the xi marginal of p, i.e., p(xi) = \u2211xi p(x), where xi = (..., xi\u22121,xi+1, . . .) \u2208 [d]N\u22121 are all the arguments excluding i. Similarly, p(xi) = \u2211xi p(x), and xi \u2208 [d]. A useful PMF is the delta function, \u03b4y, y \u2208 D, which is defined by\n$\\delta_y(x) = \\prod_{i=1}^N \\delta_{y^i}(x^i), \\text{where } \\delta_{y^i}(x^i) = \\begin{cases} 1 & x^i = y \\\\ 0 & x^i \\neq y \\end{cases}$   (1)\nWith the marginal notation \u03b4y(xi) = \u03b4yi (xi) and \u03b4y(xi) = \u03b4yi(xi) = \u03a0j\u2260i \u03b4yi (xi) which simplifies notation.\n2.2 Source and target distributions\nIn discrete generative models our goal is to transform source samples Xo ~p to target samples X1 ~ q. Our training data, consist of pairs Xo and X1 that are sampled from a joint distribution \u03c0(x, y), satisfying the marginals constraints p(x) = \u2211y\u2208D \u03c0(x, y), q(y) = \u03a3x\u2208D \u03c0(x, y), i.e.,\n(\u03a7\u03bf, \u03a71) ~ \u03c0(\u03a7\u03bf, \u03a71).   (2)\nIn the simplest case, the training pairs Xo and X1 are sampled independently from the source and target distributions respectively,\n(X0, X1) ~ p(\u0425\u043e)q(X1).   (3)\nExample: source and couplings. Common instantiations of source distribution p are: (i) adding a special token value often referred to as a 'mask' or 'dummy' token, denoted here by m, and setting the source distribution"}, {"title": "2.3 Probability paths", "content": "We follow the Flow Matching approach (Lipman et al., 2022; Liu et al., 2022; Albergo and Vanden-Eijnden, 2022) that uses a predefined probability path pt interpolating p and q, i.e.,\nPo = p and P1 = q   (6)\nto train the generative model taking a source sample Xo ~p to a target sample X1 ~ q. We use arbitrary coupling of source and target (Pooladian et al., 2023; Tong et al., 2023), \u03c0(xo, x1), and the symmetric FM path (Albergo and Vanden-Eijnden, 2022) to define the marginal probability path,\n$p_t(x) = \\sum_{x_0,x_1 \\in D} p_t(x | x_0, x_1) \\pi(x_0, x_1), \\text{ where } p_t(x | x_0, x_1) = \\prod_{i=1}^N p_t(x^i | x_0, x_1),$   (7)\nand pt (xi|xo, x1) is a time-dependent probability on the space of tokens [d] conditioned on the pair xo, x1, and satisfying po(xi|xo,x1) = \u03b4xo (xi) and p1(xi|xo,x1) = \u03b4x1(xi). If the conditional path pt(xi|xo,x1) satisfies these boundary conditions then the marginal path pt(x) satisfies equation 6.\nIn developing the framework, we would like to consider as general as possible set of probability paths that are also tractable to learn within the Flow Matching framework. We consider conditional probability paths as a convex sum of m conditional probabilities wi (xi|xo, x1), i.e.,\n$p_t(x^i | x_0, x_1) = \\sum_{j=1}^m \\kappa_t^j \\omega^i(x^i | x_0, x_1),$   (8)\nwhere \u2211j \u03baj = 1 and \u03baj > 0 are collectively called the scheduler. Note that the scheduler can be defined independently for each location in the sequence i \u2208 [N] or uniformly for all tokens, \u03baj = \u03baj.\nA simple yet useful instance of these conditional paths is reminiscent of the continuous Flow Matching paths formulated as convex interpolants,\n$p_t(x^i | x_0, x_1) = (1 - \\kappa_t) \\delta_{x_0}(x^i) + \\kappa_t \\delta_{x_1}(x^i),$   (9)\nwhere the scheduler kt satisfies \u043a\u043e = 0, \u03ba1 = 1, and monotonically increasing in t. Another interesting instantiation of equation 8 is adding uniform noise with some probability depending on t,\n$p_t(x^i | x_0, x_1) = \\kappa_t^1 \\delta_{x_1}(x^i) + \\kappa_t^2 p_u(x^i) + \\kappa_t^3 \\delta_{x_0}(x^i),$   (10)\nwhere \u03ba0 = 0, \u03ba1 = 1, \u03ba2 = \u03ba3 = 0 (remembering that \u2211i \u03bai = 1)."}, {"title": "2.4 Generating Probability Velocities", "content": "Continuous generating velocity. Sampling in continuous FM is performed by updating the current (continuous) sample Xt \u2208 RN, t \u2208 [0, 1), according to a learned generating velocity field u(Xt), i \u2208 [N]. Euler sampling follows the (deterministic) rule\nXi+h = X + hu(Xt),   (11)\nwhere h > 0 is a user-defined time step. Note that equation 11 is updating separately each of the sample coordinates, Xi, i \u2208 [N], see e.g., Figure 2, left. The velocity u(Xt) can be either directly modeled with a neural network, or parameterized via the denoiser (a.k.a. x-prediction) or noise-prediction (a.k.a. \u025b-prediction), see left column in Table 1. If, for all t \u2208 [0,1), starting at Xt ~ pt and sampling with equation 11 provides Xt+h ~ Pt+h+o(h) then we say that ut generates pt.\nGenerating probability velocity. For defining FM in the discrete setting, we follow Campbell et al. (2024) and consider a Continuous-Time discrete Markov Chain (CTMC) paradigm, namely the sample Xt is jumping between states in D, depending on a continuous time value t \u2208 [0,1]. Similar to the continuous FM setting described above, we focus on a model that predicts the rate of probability change of the current sample Xt in each of its N tokens, see Figure 2, middle-left. Then, each token of the sample Xt ~ pt is updated independently by\nXi+h ~ \u03b4xi(.) + hu(, Xt),   (12)\nwhere we call ut the probability velocity as reminiscent of the velocity field in continuous Flow Matching, and as in the continuous case, we define:\nDefinition 1. Probability velocity ut generates the probability path pt if, for all t \u2208 [0,1) and given a sample Xt ~ pt, the sample Xt+h defined in equation 12 satisfies Xt+h ~ Pt+h + o(h).\nAlgorithm 1 formulates a basic sampling algorithm given a generating probability velocity ut. In order for the r.h.s. of equation 12 to define a proper PMF for sufficiently small h > 0, it is necessary and sufficient that the probability velocity satisfies the conditions\n$\\sum_{x^i \\in [d]} u(x^i, z) = 0, \\text{ and } u(x^i, z) \\ge 0 \\text{ for all } i \\in [N] \\text{ and } x^i \\neq z.$   (13)\nNow the main question is how to find a probability velocity ut that generates the probability path defined in equations 7 and 8? A key insight in Flow Matching (Lipman et al., 2022) is that ut can be constructed as a marginalization of conditional probability velocities, u(xi, z|xo,x1), generating the corresponding conditional probability paths pt(xi|x0,x1). This can also be shown to hold in the discrete CTMC setting (Campbell et al., 2024), where a reformulation in our context and notation is as follows."}, {"title": "Theorem 2", "content": "Given a conditional probability velocity u(xi, z|xo, x1) generating a conditional probability path Pt(x|xo,x1), the marginal velocity defined by\n$u(x^i, z) = \\sum_{x_0, x_1 \\in D} u_x(x^i, z | x_0, X_1) P_t(x_0, X_1 | z)$   (14)\ngenerates the marginal probability path pt(x), where by Bayes' rule\n$P_t(x_0, X_1 | z) = \\frac{P_t(z | x_0, X_1) \\pi(x_0, X_1)}{P_t(x)}$   (15)\nFor completeness we provide a simple proof of this theorem in Appendix E.2. The proof, similar to the continuous FM case, shows that ut and pt satisfy the (discrete version of the) Continuity Equation.\nThe Continuity Equation. To provide the mathematical tool for showing that a probability velocity ut does indeed generate the probability path pt, and also to further highlight the similarities to the continuous case, we next formulate the Kolmogorov Equations, which describe the state probability rate pt(x), x \u2208 D, in CTMC as a Continuity Equation (CE). The Continuity Equation, similarly to Kolmogorov Equations, describes pt(x), x \u2208 RN in the continuous case, and is formulated as the Partial Differential Equation (PDE)\n$\\dot{p_t}(x) + div_x (p_t u_t) = 0,$   (16)\nwhere the divergence operator divx(v) applied to a vector field v : RN \u2192 RN is defined by\n$div_x (v) = \\sum_{i=1}^N \\partial_{x^i} v(x),$   (17)\nand intuitively means the total flux leaving \u00e6, see Figure 2 (middle-right). This gives an intuitive explanation to the Continuity Equation: the rate of the probability pt(x) of a state x \u2208 RN equals the total incoming probability flux, ptut, at x. In the discrete case (CTMC) the Continuity Equation (equation 16) holds as is, once the discrete divergence operator is properly defined, i.e., to measure the outgoing flux from a discrete state. In more detail, given some vector field, which in the discrete case is a scalar-valued function over pairs of states, v: D \u00d7 D \u2192 R, the discrete divergence is\n$div_x (v) = \\sum_{z \\in D} [v(z, x) - v(x, z)],$   (18)\nwhere v(z, x) represents the flux x \u2192 z and v(x, z) represent the opposite flux z \u2192 x; see Figure 2, right. Now, in our case (see Figure 2, middle-left), the probability flux at a state x \u2208 D involves all sequences with at most one token difference from x, i.e., the probability flux ptut at x takes the form v(x, z) = pt(z)u(xi, z) and v(z,x) = pt(x)ui(zi, x) for z and x that differ only in the i-th token, v(x, x) = \u2211i = 1 u(xi, x), and v(x, z) = 0 for all other (z,x) \u2208 D \u00d7 D. A direct calculation now shows (see Appendix E.1):\n$div_x (p_t u_t) = - \\sum_{z \\in D} p_t(z) \\sum_{i=1}^N \\delta_z(x^i) u(x^i, z).$   (19)\nChecking that a probability velocity ut generates a probability path pt (in the sense of Definition 1) amounts to verifying the Continuity Equation (equation 16). Indeed, using arguments from Campbell et al. (2024) and the discrete divergence operator, the PMF of Xt+h defined by sampling according to equation 12 is\n$E_{X_t} \\left[ \\prod_{i=1}^N [\\delta_{x^i}(x^i) + h u(x^i, X_t)] \\right] = E_{X_t} \\left[ \\delta_x(x) + h \\sum_{i=1}^N \\delta_{x_i}(x^i) u(x^i, X_t) + o(h) \\right]$   (20)\n$= p_t(x) - h div_x (p_t u_t) + o(h) = p_t(x) + h \\dot{p_t}(x) + o(h) = P_{t+h}(x) + o(h),$"}, {"title": "Conditional and marginal generating velocities", "content": "We provide the probability velocities generating the conditional probability paths pt(x|x0,x1) defined in equations 7 and 8. Then, using the marginalization formula in equation 14 we end up with a closed-form marginal velocity for the probability paths pt(x). In Appendix E.3 we show\nTheorem 3 (Probability velocity of conditional paths). A generating probability velocity for the conditional paths pt(x|x0,x1) defined in equations 7 and 8 is\n$u(x^i, z | x_0, x_1) = \\sum_{j=1}^m a_t^{i,j} \\omega^i (x^i | x_0, x_1) + b_t^{i, l} \\delta_z(x^i),$   (21)\n$\\text{with } a_t^{i,j} = - \\frac{\\kappa_t^j}{\\kappa_t^l} \\text{ and } b_t^{i, l} = \\frac{\\dot{\\kappa_t^l}}{\\kappa_t^l} \\text{ where } l = \\arg \\min_{j \\in [m]} \\frac{\\dot{\\kappa_t^j}}{\\kappa_t^j}.$   \nNow, computing the marginal probability velocity using equation 14 applied to the conditional probability velocity in equation 21 gives\n$u(x^i, z) = \\sum_{j=1}^m a_t^{i,j} \\overline{\\omega^i} (x^i, z) + b_t^{i, l} \\delta_z(x^i),$   (22)\nwhere the posteriors w\u00afi of wi (that are later shown to be tractable to learn) are defined by\n$\\overline{\\omega^i} (x^i, z) = \\sum_{x_0, x_1 \\in D} \\omega^i (x^i | x_0, x_1) P_t(x_0, X_1 | z),$   (23)\nwhere pt (x0, X1|z) (defined in equation 15) is the posterior probability of xo, X1 conditioned on the current state Xt = z. A useful instantiation of the general velocity in equation 22 is when considering the path family in equation 9, for which w1 (xi|x0,x1) = \u03b4x1 (xi), w2 (xi|xo,x1) = \u03b4xo (xi), \u03ba1 = \u03bat, \u03ba2 = 1 - \u03bat, \u03ba1 = kt, kt \u2265 0 (i.\u0435., monotonically non-decreasing in t) and in this case equation 22 reads as\n$u(x^i, z) = \\frac{\\dot{\\kappa_t}}{1 - \\kappa_t} [P_{1 | t}(x^i | z) - \\delta_z(x^i)]$   (24)\nwhere we use the notation P1|t(xi|Xt) = \u2211x0,x1\u03b4x1(xi)pt(x0,x1|Xt) for the probability denoiser.\nSampling backward in time. We can also sample backwards in time by following the sampling rule Xi\u2212h ~ \u03b4x(\u00b7) \u2212 hu(\u00b7, Xt). In this case \u2212u(xi, z) should satisfy equation 13. A (backward-time) generating probability velocity can then be achieved from equation 22 with the simple change to the coefficients ai and bi, see Appendix E.4. For pt defined with equation 9 the generating velocity is"}, {"title": "2.5", "content": "$u(x^i, z) = \\frac{\\dot{\\kappa_t}}{\\kappa_t} [\\delta_z(x^i) - P_{0 | t}(x^i | z)],$   (25)\nwhere in this case P0|t(xi|z) = \u2211x0,x1\u2208D \u03b4x0 (xi)pt(x0, x1|z) is the probability noise-prediction.\nRemarkably, the generating velocity fields in 24 and 25 take the exact same form as the generating (a.k.a. marginal) velocity fields in continuous flow matching when parameterized via the denoiser or noise-prediction parameterizations and using the same schedulers, see Table 1 and Appendix E.9 for explanation of the continuous case. In Appendix E.4 we provide the backward-time version of Theorem 3.\nCorrector sampling. Combining the forward-time \u00fbt (eq. 24) and backward-time \u016dt (eq. 25), i.e.,\n$u(x^i, z) = a_t \\hat{u}(x^i, z) - \\beta_t \\check{u}(x^i, z),$   (26)\nprovides a valid forward-time probability velocity field (i.e., satisfies equation 13) for t \u2208 (0,1) as long as at, \u03b2t > 0. This velocity field can be used for two types of corrector sampling: (i) When at - \u03b2t = 1 sampling with \u016bt leads to corrector sampling where intuitively each step moves 1+ at forward in time and -at backwards, which allows reintroducing noise into the sampling process; and (ii) when at = \u03b2t = 0 sampling with ut when fixing t \u2208 (0,1) leads to corrector iterations where limit samples distribute according to pt. In Appendix E.6 we prove:\nTheorem 4. For perfectly trained posteriors and at, \u03b2t > 0, t \u2208 (0,1), \u016bt in equation 26 is a probability velocity, i.e., satisfies equation 13, and: (i) For at \u2212 \u03b2t = 1, \u016bt provides a probability velocity generating pt; (ii) For at = \u03b2t = 0, repeatedly sampling with \u016bt at fixed t \u2208 (0,1) and sufficiently small h is guaranteed to converge to a sample from pt.\nOne simplification to equation 26 can be done in the case of paths constructed with conditional as in equation 9, independent coupling \u03c0(xo, x1) = p(xo)q(x1), and i.i.d. source p(xo) = \u03a0Ni=1P(xi), e.g., p(x) is uniform over [d] or dm(x). In this case, the backward-time formula in equation 25 take an equivalent simpler form\n$u(x^i, z) = \\frac{\\dot{\\kappa_t}}{\\kappa_t} [\\delta_z(x^i) - p(x^i)],$   (27)\nwhich does not require estimation of the posterior Polt. See Appendix E.5 for the derivation.\nTraining. Equation 22 shows that for generating samples from a probabilty path pt(x) we require the posteriors wi(xi|Xt). Training such posteriors can be done by minimizing the loss\n$L(\\theta) = - \\sum_{j \\in [m], i \\in [N]} E_{t, (X_0, X_1), X_t, Y} \\log \\omega^i(Y | X_t; \\theta),$   (28)\nwhere t is sampled according to some distribution in [0,1] (we used uniform), (X0, X1) ~ \u03c0(\u03a7\u03bf, X1), Xt ~Pt(Xt|Xo, X1), and Y ~ wi (Y|X0, X1); \u03b8 \u2208 R\u03b8 denotes the learnable parameters. In the common case we use in this paper of learning a single posterior, i.e., the probability denoiser P1|t, the loss takes the form\n$L(\\theta) = - \\sum_{i \\in [N]} [E_{t, (X_0, X_1), X_i} \\log P_{1 | t}(X^i | X_t).$"}, {"title": "3 Related work", "content": "In the section we cover the most related work to ours; in Appendix A we cover other related work.\nDiscrete Flows (Campbell et al., 2024) is probably the most related work to ours. We build upon their CTMC framework and offer the following generalizations and simplifications over their original formulation: Campbell et al. (2024) define a rate matrix (equivalent to our probability velocity) for generation, expressed as an expectation over the posterior, Exi~p1\\t(X}\\X\u2081) R+ (X+, xi | Xi) where R+ is defined using the conditional probability path. This formulation requires computing an expectation during sampling in the general case (see their Algorithm 1) or a particular path-dependent derivation. We show, for a more general class of probability"}, {"title": "4 Experiments", "content": "We evaluate our method on the tasks of language modeling, code generation, and image generation. For language modeling, we compare the proposed method against prior work considering the widely used generative perplexity metric. We scale the models to 1.7 billion parameters and present results on coding"}, {"title": "5 Conclusions and future work", "content": "We introduce Discrete Flow Matching, a generalization of continuous flow matching and discrete flows that provides a large design space of discrete non-autoregressive generative models. Searching within this space we were able to train large scale language models that produce generated text with an improved generative perplexity compared to current non-autoregressive methods and able to solve coding tasks at rates not achievable before with non-autoregressive models, as far as we are aware. While reducing the number of network evaluations required to generate a discrete sample compared to autoregressive models, Discrete FM still does not achieve the level of sampling efficiency achieved by its continuous counterpart, flagging an interesting future work direction. Another interesting direction is to explore the space of probability paths in equation 8 (or a generalization of which) beyond what we have done in this paper. We believe discrete non-autoregressive models have the potential to close the gap and even surpass autoregressive models as well as unlock novel applications and use cases. As our work introduces an alternative modeling paradigm to discrete sequential data such as language and code, we feel it does not introduce significant societal risks beyond those that already exist with previous large language models."}, {"title": "A Related works, continuation", "content": "We provide here some more details on relevant related works.\nContinuous diffusion and flows. Another line of works has been exploring the use of continuous space diffusion for discrete data, typically operating in the logits space (Dieleman et al., 2022; Li et al., 2022; Han et al., 2022; Lin et al., 2022; Chen et al., 2022). An additional body of work has been focusing on the adoption of latent diffusion-like modeling (Lovelace et al., 2022; He et al., 2022). Stark et al. (2024) proposed to learn a continuous Flow Matching on the probability simplex with Dirichlet paths.\nAutoregressive modeling. Autoregressive models have been a significant area of focus in recent years, particularly in the context of natural language processing and machine learning (Zhao et al., 2023). Autoregressive modeling, in its most fundamental form, utilizes the chain rule to learn the joint sequence probability by breaking it down into next-token conditional probabilities. GPT-2 (Radford et al., 2019), showcased the power of autoregressive language models in generating coherent and contextually relevant text over long passages. Its successor, GPT-3 (Brown et al., 2020), further pushed the boundaries, demonstrating impressive performance across a range of tasks without task-specific training data. Later models were adapted to other domains such as, code (Roziere et al., 2023; Li et al., 2023; Chen et al., 2021), biology (Zhang et al., 2024; Ferruz and H\u00f6cker, 2022; Madani et al., 2023), math (Romera-Paredes et al., 2024; Imani et al., 2023; Ahn et al., 2024), audio (Kreuk et al., 2022; Copet et al., 2024; Hassid et al., 2024) and more.\nMasked generative modeling. Masked generative modeling proposes to mask a variable portion of the input sequence and training a model to predict this masked section. Ghazvininejad et al. (2019) proposed Mask-Predict, a masked language modeling with parallel decoding. Savinov et al. (2021) extended the mask-modeling approach by employing an additional loss term that incorporates rolling model predictions. MaskGIT (Chang et al., 2022) followed a similar path, for the task of class-conditioned image synthesis, Chang et al. (2023) extended this approach to high-quality textually guided image generation over low-resolution images followed by a super-resolution module. Recently, Ziv et al. (2024) proposed a text-to-music method, which relies on the MaskGIT foundations while observing that span masking boosts the quality of the generated sequence significantly."}, {"title": "B Further implementation details", "content": "Safe sampling. When sampling according to Algorithm 1 using the generating probability velocity in equation 22, an arbitrary step size h > 0 can make some probabilities in dx(\u00b7) + hu\u2084(\u00b7, X\u2081) negative and consequently require clamping and injecting further error into the sampling process that can in turn accumulate to a non-negligible global sampling error. A simple fix that guarantees a valid probability distribution while keeping the o(h) sampling error at the relatively manageable price of potentially more function evaluations is using the following adaptive step size in Algorithm 1: at time t \u2208 [0, 1) use\n$h_{\\text{adaptive}} = \\min \\left\\{ h, \\min_i \\min \\left\\{ \\frac{\\kappa_t^i}{\\dot{\\kappa_t^i}} \\right\\} \\right\\}$   (29)\nAs can be verified with the general probability velocity formula in equation 22, the above choice for hadaptive guarantees dx(\u00b7) + hu(, X\u2081) is a valid PMF. As mostly used in this paper, for the probability denoiser parameterization (equation 24) the adaptive step is\n$h_{\\text{adaptive}} = \\min \\left\\{ h, \\frac{1 - \\kappa_t}{\\dot{\\kappa_t}} \\right\\}$   (30)"}, {"title": "Conditioning", "content": "With the corrector sampling (equations 26 and 51) we have the adaptive step:\n$h_{\\text{adaptive}} = \\min \\left\\{ h, \\frac{A_t \\kappa_t}{\\dot{\\kappa_t}} + \\frac{B_t \\kappa_t}{\\dot{\\kappa_t}} \\right\\}$   (31)\nConditioning. In our unconditional coupling (U-coupling), see equation 5, we define the conditioning pattern based on prefixes of random length No < N, i.e.,\n$\\iota = (\\underbrace{1, ..., 1}_{N_0}, \\underbrace{0, ..., 0}_{N - N_0}).$\nDuring the training phase, we sample No ~ U(0, N) and adjust the input sequence in accordance with the mask 1.\nDuring conditional sampling with Algorithm 1 we replace, after each update step, the relevant tokens with the conditioned ones, i.e., X = [ \u2299 Y + (1 \u2212 1) \u2299 X, where X is the current sample, Y is the condition, and [ is the condition's mask.\nNFE bound. For mask modeling, i.e., p = dm, we have seen that the probability denoiser is time-independent (see Proposition 6). Consequently, when sampling with Algorithm 1 and ut from equation 24 without corrector sampling one is not required to recompute the forward pass P1|t(\u00b7|Xt) if Xt is identical to Xt\u2212h (i.e., no m has been unmasked). This means that the NFE of Algorithm 1 in this case is bounded by the number of tokens N.\nPost training scheduler change. For a trained posterior \u0175t(xi|z) of a conditional probability path as in equation 9 with a scheduler Kt, the velocity is given by equations 24 or 25, where \u0175t(xi|z) is either P1|t(xi|z) or P0|t(xi|z) respectively. In this case, we can apply the velocities in equations 24 and 25 for sampling with any scheduler \u03bat, using the change of scheduler formula for posteriors,\n$\\overline{\\omega_t}(x^i | z) = \\hat{\\omega}_{t'}(x^i | z),$   (32)\nwhere w\u00aft(xi|z), is the posterior of the scheduler \u03bat, t' = \u03ba\u22121\u03bat, and \u03ba\u22121 is the inverse of \u03ba. The scheduler change formula in equation 32 is proved in Proposition 8. We note that by Proposition 6, for mask modeling, i.e., p = dm, the posterior wt (xi\u2758z) is time independent. Hence, in that case, the posterior is not affected by a scheduler change."}]}]}