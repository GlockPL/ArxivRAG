{"title": "Learning to Complement and to Defer to Multiple Users", "authors": ["Zheng Zhang", "Wenjie Ai", "Kevin Wells", "David Rosewarne", "Thanh-Toan Do", "Gustavo Carneiro"], "abstract": "With the development of Human-AI Collaboration in Clas-sification (HAI-CC), integrating users and AI predictions becomes chal-lenging due to the complex decision-making process. This process hasthree options: 1) AI autonomously classifies, 2) learning to complement,where AI collaborates with users, and 3) learning to defer, where AIdefers to users. Despite their interconnected nature, these options havebeen studied in isolation rather than as components of a unified sys-tem. In this paper, we address this weakness with the novel HAI-CCmethodology, called Learning to Complement and to Defer to Multi-ple Users (LECODU). LECODU not only combines learning to comple-ment and learning to defer strategies, but it also incorporates an esti-mation of the optimal number of users to engage in the decision process.The training of LECODU maximises classification accuracy and min-imises collaboration costs associated with user involvement. Comprehen-sive evaluations across real-world and synthesized datasets demonstrateLECODU's superior performance compared to state-of-the-art HAI-CCmethods. Remarkably, even when relying on unreliable users with highrates of label noise, LECODU exhibits significant improvement overboth human decision-makers alone and AI alone. Code is available athttps://github.com/zhengzhang37/LECODU.git .", "sections": [{"title": "1 Introduction", "content": "Human-AI collaborative classification (HAI-CC) optimally combines human andAI strengths in training and testing, offering superiority over standalone AI mod-els and manual decision processes [14, 56]. For instance, HAI-CC tends to bemore accurate, interpretable, and engaging than standalone AI models [14,56].Compared with manual processes, HAI-CC can enhance efficiency and decisionconsistency, and mitigate human errors [14, 56]. Thus, HAI-CC is increasingly"}, {"title": "2 Related work", "content": "Human-AI Collaborative Classification (HAI-CC) explores the strengthsof expert users and AI by offering improved accuracy, interpretability, and ethicalconsiderations compared to standalone AI models, while also providing increasedefficiency, data-driven insights, and consistency in decision-making compared tousers without AI assistance [11, 34, 54, 65, 71]. HAI-CC is motivated by recentstudies [27,51,53] which suggest that previous AI classification approaches havepredominantly focused on optimising AI systems in isolation, neglecting theinfluence of human-AI collaborative classification. To address this gap, manymethods have been proposed to harmonize and enhance the collaboration be-tween humans and AI [1,4,44, 60, 67, 68].A prevalent HAI-CC strategy is Learning to Defer (L2D), which is optimisedto decide if the expert user or the AI will make the classification. Originatedfrom the learning to reject strategy [13], which considered the impact of otheragents in the decision-making process [35], L2D was addressed by studies ondeferral and abstention through optimising different surrogate loss functions ofthe classification loss [7, 8, 37-40, 45, 59]. Mozannar et al. [38] proposed a cost-sensitive softmax cross-entropy consistent surrogate loss, while Verma et al. [59]suggested a one-vs-all classifier as another consistent surrogate to mitigate un-derfitting issues. Disagreement on disagreements (DoD) [8] provided an activelearning algorithm that is able to train a classifier-rejector pair by minimallyquerying the human on selected points. Furthermore, Mozannar et al. [37] pro-vided a realizable-consistent surrogate loss function to address learning with de-ferral using halfspaces. A common limitation across these studies is their focus"}, {"title": "3 Method", "content": "Let $D = \\{x_i, M_i\\}_{i=1}^{|D|}$ be the noisy-label multi-rater training set, where $x_i \\in \\mathbb{X} \\subset \\mathbb{R}^{H \\times W \\times R}$ denotes an input image of size $H \\times W$ and $R$ channels, and $M_i = \\{m_{i,j}\\}_{j=1}^M$ represents the $M$ experts' noisy annotations for image $i$, with $m_{i,j} \\in \\mathcal{Y} \\subset \\{0,1\\}^{|Y|}$ being a one-hot label. A SOTA LNL AI model $[16, 61, 79]$ $f_\\theta: \\mathbb{X} \\rightarrow \\Delta^{|Y|-1}$ has been pre-trained with the training set (the noisy label per image $x_i$ is randomly selected as one of the experts' annotations in $M_i$), where $\\Delta^{|Y|-1}$ denotes the $|Y|$-dimensional probability simplex, and $\\theta \\in \\Theta$ is the"}, {"title": "4 Experiments", "content": "4.1 Datasets\nCIFAR-10 Dataset. CIFAR-10 [30] has 50K training images and 10K test-ing images of size 32 \u00d7 32, with each image belonging to one of 10 classes. Toleverage real-world human annotations on CIFAR-10, we utilize CIFAR-10N [64]for training and CIFAR-10H [43] for testing. CIFAR-10N extends the CIFAR-10training set with three noisy labels for each image, enriching the dataset withreal-world annotation variability. CIFAR-10H extends the CIFAR-10 test set byproviding around 51 noisy labels per image. Following LECOMH setting [74],in the testing phase, a maximum of three users are randomly selected from thethe pool of 51 users in CIFAR-10H to engage in collaboration. Secondly, weconduct experiments with multi-rater CIFAR10-IDN [69], which harnesses syn-thesised annotations with multi-rater instance-dependent label noise. The labelnoise rates range from 0.2 to 0.5 for both training and testing sets. Three distinctnoisy labels are produced for each noise rate to enable the simulation of diversehuman predictions with similar error rates.\nChaoyang Dataset. The Chaoyang dataset comprises 6,160 colon slide patches,each with a resolution of 512 \u00d7 512 [79], where each patch has three noisy labelsproduced by real pathologists, with each image belonging to one of 4 classes. Inthe original Chaoyang dataset setup, the training set has patches with multi-rater noisy labels, while the testing set only contains patches that all users agreeon a single label. To ensure that both training and testing sets contain multiple"}, {"title": "4.2 Implementation Details", "content": "Architecture. All methods are implemented in Pytorch [42] and run on NVIDIARTX A6000. For CIFAR-10N and CIFAR-10H, we pre-trained ProMix [61] withtwo PreAct-ResNet-18 as the LNL AI model using the Rand1 [64] annotation.For the multi-rater CIFAR10-IDN experiments, we pre-trained InstanceGM [16]with two PreAct-ResNet-18 as the LNL AI model. For Chaoyang, followingNSHE [79], two ResNet-34 are pre-trained using the label_A annotation, andthe best-trained network is selected for the LNL AI model. All the above mod-els were selected because of their SOTA performance in the respective datasets.\nFor the Human-AI Selection Module, we utilise the same backbone of the pre-trained models. The Collaboration Module comprises a two-layer MLP with512-dimensional hidden layers with a ReLU activation function. The pre-trainedPromix on CIFAR-10N reaches 97.41% accuracy on the CIFAR-10 test set. Onthe multi-rater CIFAR-10 IDN, the pre-trained InstanceGM reaches test accu-racy of 96.64%, 96.52%, 96.33% and 95.90% for noise rates 0.2, 0.3, 0.4 and 0.5.\nThe pre-trained NSHE reaches 82.44% test accuracy on Chaoyang.\nTraining and evaluation details. LECODU is trained for 200 epochs usingSGD with a momentum=0.9 and a weight decay=0.0005. The batch size is 1024for CIFAR and 96 for Chaoyang, with an initial learning rate of 0.05. The GumbelSoftmax temperature parameter of the Human-AI Selection Module is 5. Also,to ensure consistent data range, the AI predictions are normalised by GumbelSoftmax with temperature parameter of 0.5 before the concatenation with userannotations. To prevent bias, the order of human annotations is randomly shuf-fled during both training and testing, drawn from the human label pool for eachimage. Ground truth training labels are determined using CROWDLAB's con-sensus label. Evaluation is based on test accuracy (%) relative to collaborationcost, measured by the number of labels used from the human label pool for thewhole testing set.\nHAI-CC Baselines. Following [37,74], we compare LECODU with single-expert L2D (SEL2D) methods, such as cross-entropy surrogate [38] (CE), one-vs-all-based surrogate [59] (OVA), selective prediction that thresholds classifier con-fidence for the rejector [37] (SP), the confidence method [45] (CC), differentiabletriage [40] (DIFT), mixture of experts [35] (MoE), and Realizable Surrogate(RS) [37]. For training SEL2D methods, we rely on randomly sampled annota-tions or aggregated (majority voting) annotations to simulate a single expertfrom the human annotation pools, while for testing, we use randomly sampledannotations. To compute SEL2D's collaboration cost, we sort the testing im-ages based on their rejection scores and then adjust the threshold for annotatingthese testing cases by users [37]. We also compare LECODU with multi-expert"}, {"title": "4.3 Results", "content": "For CIFAR-10, the total cost computation is governed by Eq. 4, where the pa-rameter \u03bb in Eq. 2 is adjusted during the training phase to influence LECODU'scost considerations. This results in a minimum cost of 0 (all testing cases pre-dicted by AI alone) and a maximum cost of 30000 (all testing cases predicted byAI + 3 users or deferred to 3 users) for 10K test images. In contrast, for Chaoyangdataset, the cost scale is normalized from the original span of [0,3 \u00d7 1435] to abroader range of [0,3 \u00d7 10000] to enable an easier comparison across differentdatasets. Single-expert methods have a total cost in [0, 10000] as only one userper image is allowed. LECODU assesses accuracy within the cost range of [0,10000], and for multi-expert methods, to maintain consistency in comparativeanalysis, if cost > 10000, the accuracy plot is truncated at cost=10000.\nFig. 3 illustrates the cost-accuracy curves for LECODU and other Human-AI Collaborative Classification (HAI-CC) strategies, with detailed numericalresults in Tab. 4 in the supp. material. The user accuracy (\u2018User': dashed lines)reflects the inherent noise level of the benchmark, such as around 80% and 50%accuracy for IDN20 and IDN50, respectively. Note that in CIFAR-10H, the LNLAI model (with accuracy \u2248 97%) surpasses human accuracy (\u2248 95%), while inChaoyang, the LNL AI model's accuracy is slightly inferior (\u2248 82%) comparedto that of pathologists (users have accuracy from 86% to 99%). Within the IDNbenchmarks, the LNL AI consistently outperforms humans.\nLECODU shows higher classification accuracy than all competing HAI-CCmethods for all collaboration costs in all benchmarks. For the majority of com-peting methods (except LECOMH and CET), the accuracy at cost = 0 is mostlyfrom the LNL pre-trained model. Then this accuracy increases, reaching a peakfor some cost < 10000, followed by a decrease until reaching the accuracy of usersat cost = 10000. On the other hand, MEL2C methods (LECOMH and CET)tend to always improve accuracy with increasing collaboration cost, which is alsotrue for our LECODU. Note that when the AI model outperforms humans (e.g.CIFAR-10H), both our method and competing methods can show more accuratepredictions than humans or AI alone. In contrast, if humans are more accuratethan AI (e.g. Chaoyang), the accuracy of SEL2D methods is limited by expert"}, {"title": "4.4 Ablation study", "content": "In Fig. 4, we analyse the influence of some key points of LECODU, namely: 1)engaging multiple users for collaboration during training and testing; 2) train-"}, {"title": "5 Conclusion", "content": "In this paper, we introduced a novel MEHAI-CC method LECODU. LECODUcombines learning to defer and learning to complement strategies to make threekey decisions: when to complement with expert decisions, when to defer decisionsto experts, and how many experts should be engaged in the decision process.LECODU not only maximises classification accuracy but also minimises collab-oration costs. Comprehensive evaluations across real-world and synthesized mul-tiple noisy label datasets demonstrate LECODU's superior accuracy to SOTAhuman-AI collaborative classification methods. Even in medium and high noiserate scenarios where user annotations are unreliable, LECODU has a discernibleimprovement over human decision-makers and AI alone.\nOne potential weakness of LECODU is that we assume all users are the same,which helps the training process in two ways. First, it mitigates the combina-torial explosion of the number of different combinations of specific users andAI model in L2D and L2C scenarios. Second, we do not need to identify users,which prevents potential privacy concerns. However, such assumption is unreal-istic since users are different and have distinct classification accuracy that needto be taken into account for training and testing. We plan to propose new AIsystems that collaborate with users with different competencies. Another issueof LECODU is that it can promote over-reliance on AI in human-AI collabora-tion, leading to a reduction in the decision-making skills of humans. We plan tointroduce methods to mitigate such deskilling problem."}]}