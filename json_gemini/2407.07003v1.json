{"title": "Learning to Complement and to Defer to Multiple Users", "authors": ["Zheng Zhang", "Wenjie Ai", "Kevin Wells", "David Rosewarne", "Thanh-Toan Do", "Gustavo Carneiro"], "abstract": "With the development of Human-AI Collaboration in Classification (HAI-CC), integrating users and AI predictions becomes challenging due to the complex decision-making process. This process has three options: 1) AI autonomously classifies, 2) learning to complement, where AI collaborates with users, and 3) learning to defer, where AI defers to users. Despite their interconnected nature, these options have been studied in isolation rather than as components of a unified system. In this paper, we address this weakness with the novel HAI-CC methodology, called Learning to Complement and to Defer to Multiple Users (LECODU). LECODU not only combines learning to complement and learning to defer strategies, but it also incorporates an estimation of the optimal number of users to engage in the decision process. The training of LECODU maximises classification accuracy and min-imises collaboration costs associated with user involvement. Comprehensive evaluations across real-world and synthesized datasets demonstrate LECODU's superior performance compared to state-of-the-art HAI-CC methods. Remarkably, even when relying on unreliable users with high rates of label noise, LECODU exhibits significant improvement over both human decision-makers alone and AI alone.", "sections": [{"title": "1 Introduction", "content": "Human-AI collaborative classification (HAI-CC) optimally combines human and AI strengths in training and testing, offering superiority over standalone AI models and manual decision processes. For instance, HAI-CC tends to be more accurate, interpretable, and engaging than standalone AI models. Compared with manual processes, HAI-CC can enhance efficiency and decision consistency, and mitigate human errors. Thus, HAI-CC is increasingly recognized as a valuable classification strategy, particularly for complex problems in high-stake real-world scenarios, such as breast cancer classification from mammograms, risk assessment endeavors, and the identification of incorrect or misleading information generated by large language models. HAI-CC approaches aim to maximise accuracy and minimise the human-AI collaboration costs through learning to defer and learning to complement techniques. In learning to defer, HAI-CC determines when to classify with the AI model alone or defer to users, while learning to complement combines AI and user classifications. In a single expert HAI-CC (SEHAI-CC) setting, only one user is included, while multiple experts HAI-CC (MEHAI-CC) methods explore strategies to complement with or defer to multiple experts. Despite the interconnected nature of learning to defer and learning to complement, prior HAI-CC methods have mainly studied them in isolation. This oversight represents a missed opportunity to adopt a more comprehensive approach that could improve HAI-CC efficacy. Additionally, most HAI-CC methods assume the presence of clean labels in the training set, which may be impractical, especially in multi-rater training sets where clean labels may be unavailable or impossible to recover. Among HAI-CC techniques, LECOMH stands out as an exception that does not depend on clean labels, relying on noisy-label learning techniques to handle multiple noisy labels during training. Nevertheless, LECOMH is a learning to complement approach that does not consider the learning to defer option.\nIn this paper, we propose a novel HAI-CC approach called Learning to Complement and to Defer to Multiple Users (LECODU). LECODU integrates learning to defer and learning to complement to multiple users, leveraging training sets containing multiple noisy-label annotations. More specifically, LECODU is designed to make three decisions: 1) when to collaborate with experts, 2) when to defer to experts, and 3) how many experts should be engaged in the decision process. These three decisions target the highest classification accuracy at the minimum collaboration cost, where cost is measured by the number of experts engaged in the decision process. Our main contributions are:\nThe novel LECODU model featuring a selection module and a collaboration module that combines learning-to-defer and learning-to-complement strategies; and\nA new training algorithm that leverages a training set containing multiple noisy labels per image to minimise the costs and maximise the accuracy of LECODU.\nWe assess LECODU's effectiveness against state-of-the-art (SOTA) HAI-CC methods using real-world and synthesized multi-rater noisy label benchmarks (CIFAR-10N, CIFAR-10H, multi-rater CIFAR10-IDN, and Chaoyang). Across all benchmarks, LECODU consistently outperforms the competition, showing higher accuracy for equivalent collaboration costs, as measured by the number of experts engaged in classification. Even in scenarios with medium and high noise rates where user annotations are unreliable, LECODU exhibits remarkable improvement over both human decision-makers and AI alone."}, {"title": "2 Related work", "content": "Human-AI Collaborative Classification (HAI-CC) explores the strengths of expert users and AI by offering improved accuracy, interpretability, and ethical considerations compared to standalone AI models, while also providing increased efficiency, data-driven insights, and consistency in decision-making compared to users without AI assistance. HAI-CC is motivated by recent studies which suggest that previous AI classification approaches have predominantly focused on optimising AI systems in isolation, neglecting the influence of human-AI collaborative classification. To address this gap, many methods have been proposed to harmonize and enhance the collaboration between humans and AI.\nA prevalent HAI-CC strategy is Learning to Defer (L2D), which is optimised to decide if the expert user or the AI will make the classification. Originated from the learning to reject strategy, which considered the impact of other agents in the decision-making process, L2D was addressed by studies on deferral and abstention through optimising different surrogate loss functions of the classification loss. Mozannar et al. proposed a cost-sensitive softmax cross-entropy consistent surrogate loss, while Verma et al. suggested a one-vs-all classifier as another consistent surrogate to mitigate underfitting issues. Disagreement on disagreements (DoD) provided an active learning algorithm that is able to train a classifier-rejector pair by minimally querying the human on selected points. Furthermore, Mozannar et al. provided a realizable-consistent surrogate loss function to address learning with deferral using halfspaces. A common limitation across these studies is their focus on single-expert scenarios, overlooking the complexities of multi-user collaboration within human-AI teams. Recently, several works have focused on learning to defer with multiple users. Hemmer et al. introduced a model with ensemble prediction combining AI and human predictions, but collaboration cost optimization is lacking. On the other hand, Mozannar et al. proposed an L2D method capable of deferring to one of multiple users without combining AI and human predictions. Mao et al. propose a two-stage H-consistent surrogate loss for learning to defer to multiple experts.\nAnother important HAI-CC strategy, learning to complement (L2C), has been designed to maximise the expected utility of the human-AI decision. Steyvers et al. proposed a Bayesian framework for modeling human-AI complementarity, while Kerrigan et al. proposed to combine human and model predictions via confusion matrices and model calibration. Zhang et al. proposed LECOMH, integrating learning to complement with learning with noisy label (LNL) and multi-rater learning (MRL). Bansal et al. focus on optimising the expected utility of decision-making for human-AI teams, diverging from traditional model optimization centered on accuracy. Liu et al. leverage perceptual differences through post-hoc teaming, showing that human-machine collaboration can be more accurate than machine-machine collaboration.\nWe reiterate that L2D and L2C methods have similar goals, but have been developed in isolation, even though they should be considered parts of a unified system. This is a gap we are addressing in this paper with LECODU.\nLearning with Noisy Labels (LNL). Except for LECOMH, previous HAI-CC methods do not rely on any technique to handle label noise in the training set, which is critical for dealing with real-world data that commonly contains multiple noisy labels per training sample. In this subsection, we provide a short review of LNL methods that can be used in the HAI-CC context. LNL approaches have explored many techniques, including: robust loss functions, co-teaching, label cleaning, semi-supervised learning (SSL), iterative label correction, meta-learning, and graphical models. Existing LNL SOTA methods predominantly rely on SSL techniques. For instance, DivideMix integrates MixMatch and Co-teaching to harness the SSL potential. Adhering to this paradigm, several LNL studies employ MixUp within SSL. Furthermore, InstanceGM introduces graphical models to work together with SSL methods, while Promix optimises the utility of clean samples through a matched high-confidence selection technique. However, LNL is inherently an ill-posed problem that needs constraints to become identifiable. One such constraint is the presence of multiple noisy labels per training sample, which is naturally present in multiple expert HAI-CC (MEHAI-CC). Therefore, investigating LNL techniques in MEHAI-CC appears to be a promising exploration avenue. Such techniques are known as multi-rater learning (MRL) and are described in more detail next.\nMulti-rater Learning (MRL) aims to train a classifier with noisy crowdsourced labels from multiple annotators, where the key is how to extract the \"clean\" label from the imperfect crowdsourced labels. Previous methods focus"}, {"title": "3 Method", "content": "Let $\\mathcal{D} = \\{x_i, M_i\\}_{i=1}^{|\\mathcal{D}|}$ be the noisy-label multi-rater training set, where $x_i \\in \\mathcal{X} \\subset \\mathbb{R}^{H\\times W\\times R}$ denotes an input image of size $H \\times W$ and $R$ channels, and $M_i = \\{m_{i,j}\\}_{j=1}^{M}$ represents the $M$ experts' noisy annotations for image $i$, with $m_{i,j} \\in \\mathcal{Y} \\subset \\{0,1\\}^{|\\mathcal{Y}|}$ being a one-hot label. A SOTA LNL AI model $f_\\theta : \\mathcal{X} \\rightarrow \\Delta^{|\\mathcal{Y}|-1}$ has been pre-trained with the training set (the noisy label per image $x_i$ is randomly selected as one of the experts' annotations in $M_i$), where $\\Delta^{|\\mathcal{Y}|-1}$ denotes the $|\\mathcal{Y}|$-dimensional probability simplex, and $\\theta \\in \\Theta$ is the model parameter. The consensus label for training our LECODU is obtained via the SOTA MRL method CROWDLAB that takes the training images and experts' labels $(x, M) \\in \\mathcal{D}$, together with the AI classifier's predictions $\\hat{y} = f_\\theta(x)$ for each sample in $\\mathcal{D}$ to produce a consensus label $\\hat{y} \\in \\mathcal{Y}$ and a quality (or confidence) score $a$. The consensus label dataset is formed with:\n$\\mathcal{D}^c = \\{(x_i, \\hat{y}_i, M_i)|(x_i, M_i) \\in \\mathcal{D}, (\\hat{y}_i, a_i) = CrowdLab(x_i, f_\\theta(x_i), M_i), a_i > 0.5\\}.\n$\nOur proposed LECODU is shown in Fig. 2, which contains a Human-AI Selection Module and a Collaboration Module. The Human-AI Selection Module, denoted as $g_\\varphi: \\mathcal{X} \\rightarrow \\Delta^{2M}$, is designed to predict a categorical distribution reflecting the probability distribution over several collaborative scenarios, which allows for an integration of AI prediction with human experts' predictions across these different collaborative scenarios. These scenarios include the standalone AI prediction (1st dimension), the learning to complement prediction that integrates the AI with one or multiple experts (2nd dimension: AI +1 user, ..., M+1st dimension: AI + M users), and the learning to defer prediction that leaves the prediction to be done collectively by one or multiple experts (M+2nd dimension: 1 user, ..., 2M+1st dimension: M users). The Collaboration Module $h_\\psi: \\Delta^{|\\mathcal{Y}|-1} \\times ... \\times \\Delta^{|\\mathcal{Y}|-1} \\rightarrow \\Delta^{|\\mathcal{Y}|-1}$ has the AI prediction alone as its first input, the next inputs, indexed by 2 to M + 1, represent the collaborative pre-diction integrating AI with 1 to M users, and inputs M + 2 to 2M + 1 denote the collective predictions of 1 to M users. To be unbiased to any specific expert, we randomly selected users from the pool of users during training and testing for both the learning to complement and learning to defer scenarios.\nThe training for the Human-AI Selection Module and the Collaboration Module relies on the following optimisation:\n$\\begin{aligned} \\varphi^*, \\psi^* = \\arg \\min_{\\Phi, \\Psi} \\frac{1}{|\\mathcal{D}^c|} \\sum_{(x_i,\\hat{y}_i, M_i) \\in \\mathcal{D}^c} l (y_i, h_\\psi (p (g_\\varphi(x_i), f_\\theta(x_i), shf(M_i)))) + \\lambda \\times cost(g_\\varphi(x_i)), \\end{aligned}$\nwhere $l(.)$ is the cross-entropy (CE) loss, $\\lambda$ is a hyper-parameter that weights the cost function,\n$\\begin{aligned} p (g_\\varphi(x), f_\\theta (x), shf(M)) = \\begin{cases} [f_\\theta(x), m_{i,1},...,m_{i,1}] & \\text{if } max_j g^{(j)}(x) = g^{(1)}(x)\\\\ [f_\\theta(x), f_\\theta(x), m_{i,1}, ..., m_{i,M}] & \\text{if } max_j g^{(j)}(x) = g^{(2)}(x) \\\\ ... \\\\ [f_\\theta(x), m_{i,1}, ..., m_{i,M}] & \\text{if } max_j g^{(j)}(x) = g^{(M+1)}(x), \\\\ [0_{|\\mathcal{Y}|}, m_{i,1}, ..., m_{i,1}] & \\text{if } max_j g^{(j)}(x) = g^{(M+2)}(x) \\\\ ... \\\\ [0_{|\\mathcal{Y}|}, m_{i,1}, ..., m_{i,M}] & \\text{if } max_j g^{(j)}(x) = g^{(2M+1)}(x) \\end{cases} \\end{aligned}$\nwith $g^{(j)}(.)$ denoting the $j^{th}$ output from the Human-AI Selection Module and $shf(M)$ representing a function that shuffles the users' annotations (so the train-"}, {"title": "4 Experiments", "content": "4.1 Datasets\nCIFAR-10 Dataset. CIFAR-10 has 50K training images and 10K testing images of size 32 \u00d7 32, with each image belonging to one of 10 classes. To leverage real-world human annotations on CIFAR-10, we utilize CIFAR-10N for training and CIFAR-10H for testing. CIFAR-10N extends the CIFAR-10 training set with three noisy labels for each image, enriching the dataset with real-world annotation variability. CIFAR-10H extends the CIFAR-10 test set by providing around 51 noisy labels per image. Following LECOMH setting, in the testing phase, a maximum of three users are randomly selected from the the pool of 51 users in CIFAR-10H to engage in collaboration. Secondly, we conduct experiments with multi-rater CIFAR10-IDN, which harnesses synthesised annotations with multi-rater instance-dependent label noise. The label noise rates range from 0.2 to 0.5 for both training and testing sets. Three distinct noisy labels are produced for each noise rate to enable the simulation of diverse human predictions with similar error rates.\nChaoyang Dataset. The Chaoyang dataset comprises 6,160 colon slide patches, each with a resolution of 512 \u00d7 512, where each patch has three noisy labels produced by real pathologists, with each image belonging to one of 4 classes. In the original Chaoyang dataset setup, the training set has patches with multi-rater noisy labels, while the testing set only contains patches that all users agree on a single label. To ensure that both training and testing sets contain multiple noisy labels and no patient data overlaps between the two sets, we follow the LECOMH data partition, where the new training set has 862 patches with 2 out of 3 consensual labels and 3862 patches with 3 out of 3 consensual labels. The new testing set includes 449 patches with 2 out of 3 consensual labels and 986 patches with 3 out of 3 consensual labels.\n4.2 Implementation Details\nArchitecture. All methods are implemented in Pytorch and run on NVIDIA RTX A6000. For CIFAR-10N and CIFAR-10H, we pre-trained ProMix with two PreAct-ResNet-18 as the LNL AI model using the Rand1 annotation. For the multi-rater CIFAR10-IDN experiments, we pre-trained InstanceGM with two PreAct-ResNet-18 as the LNL AI model. For Chaoyang, following NSHE, two ResNet-34 are pre-trained using the label_A annotation, and the best-trained network is selected for the LNL AI model. All the above models were selected because of their SOTA performance in the respective datasets. For the Human-AI Selection Module, we utilise the same backbone of the pre-trained models. The Collaboration Module comprises a two-layer MLP with 512-dimensional hidden layers with a ReLU activation function. The pre-trained Promix on CIFAR-10N reaches 97.41% accuracy on the CIFAR-10 test set. On the multi-rater CIFAR-10 IDN, the pre-trained InstanceGM reaches test accuracy of 96.64%, 96.52%, 96.33% and 95.90% for noise rates 0.2, 0.3, 0.4 and 0.5. The pre-trained NSHE reaches 82.44% test accuracy on Chaoyang.\nTraining and evaluation details. LECODU is trained for 200 epochs using SGD with a momentum=0.9 and a weight decay=0.0005. The batch size is 1024 for CIFAR and 96 for Chaoyang, with an initial learning rate of 0.05. The Gumbel Softmax temperature parameter of the Human-AI Selection Module is 5. Also, to ensure consistent data range, the AI predictions are normalised by Gumbel Softmax with temperature parameter of 0.5 before the concatenation with user annotations. To prevent bias, the order of human annotations is randomly shuffled during both training and testing, drawn from the human label pool for each image. Ground truth training labels are determined using CROWDLAB's consensus label. Evaluation is based on test accuracy (%) relative to collaboration cost, measured by the number of labels used from the human label pool for the whole testing set.\nHAI-CC Baselines. Following, we compare LECODU with single-expert L2D (SEL2D) methods, such as cross-entropy surrogate (CE), one-vs-all-based surrogate (OVA), selective prediction that thresholds classifier confidence for the rejector (SP), the confidence method (CC), differentiable triage (DIFT), mixture of experts (MoE), and Realizable Surrogate (RS). For training SEL2D methods, we rely on randomly sampled annotations or aggregated (majority voting) annotations to simulate a single expert from the human annotation pools, while for testing, we use randomly sampled annotations. To compute SEL2D's collaboration cost, we sort the testing images based on their rejection scores and then adjust the threshold for annotating these testing cases by users. We also compare LECODU with multi-expert"}, {"title": "4.3 Results", "content": "For CIFAR-10, the total cost computation is governed by Eq. 4, where the parameter $\\lambda$ in Eq. 2 is adjusted during the training phase to influence LECODU's cost considerations. This results in a minimum cost of 0 (all testing cases predicted by AI alone) and a maximum cost of 30000 (all testing cases predicted by AI + 3 users or deferred to 3 users) for 10K test images. In contrast, for Chaoyang dataset, the cost scale is normalized from the original span of [0,3 \u00d7 1435] to a broader range of [0,3 \u00d7 10000] to enable an easier comparison across different datasets. Single-expert methods have a total cost in [0, 10000] as only one user per image is allowed. LECODU assesses accuracy within the cost range of [0, 10000], and for multi-expert methods, to maintain consistency in comparative analysis, if cost > 10000, the accuracy plot is truncated at cost=10000.\nLECODU shows higher classification accuracy than all competing HAI-CC methods for all collaboration costs in all benchmarks. For the majority of competing methods (except LECOMH and CET), the accuracy at cost = 0 is mostly from the LNL pre-trained model. Then this accuracy increases, reaching a peak for some cost < 10000, followed by a decrease until reaching the accuracy of users at cost = 10000. On the other hand, MEL2C methods (LECOMH and CET) tend to always improve accuracy with increasing collaboration cost, which is also true for our LECODU. Note that when the AI model outperforms humans (e.g. CIFAR-10H), both our method and competing methods can show more accurate predictions than humans or AI alone. In contrast, if humans are more accurate than AI (e.g. Chaoyang), the accuracy of SEL2D methods is limited by expert information and cannot improve significantly. However, ME{L2D,L2C} methods generally outperform SEL2D approaches as the collaboration cost increases, but at the lower cost level, they all fall short of LECODU's accuracy.\nIn IDN benchmarks, CET (w. LNL) consistently incurs a fixed high cost of 10,000, showing higher accuracy than SEL2D approaches, but lagging behind LECODU. The performance of Multi_L2D (w. LNL) aligns closely with that of SEL2D methods. Furthermore, LECOMH is often less accurate than LECODU, suggesting that a strategy focusing solely on MEL2C might fail in some cases where L2D would be a better option. Furthermore, LECOMH shows comparable results to LECODU at higher collaboration costs but significantly lags at low-cost levels (e.g., CIFAR-10H, Chaoyang, and IDN20), underlining LECODU's superior adaptability and efficiency in optimising human-AI collaboration for enhanced decision-making accuracy.\nIn high noise rate scenarios (e.g., IDN{30,40,50}), where user annotations are unreliable, the efficacy of HAI-CC reduces, leading to a limited improvement in accuracy by all methods. SEL2D methods show a pronounced degradation in performance as collaboration costs increase, where the top accuracy happens at cost = 0, when the pre-trained LNL method is running alone without any human collaboration. For ME{L2D,L2C} methods, CET and LECOMH exhibit superior accuracy to SEL2D, but they still fall short of LECODU, while Multi_L2D also shows similar results as SEL2D methods. Although LECODU aligns with LECOMH on IDN50, we can notice a discernible improvement on IDN30 and IDN40, suggesting LECODU can be effective even in highly noisy environments."}, {"title": "4.4 Ablation study", "content": "In Fig. 4, we analyse the influence of some key points of LECODU, namely: 1) engaging multiple users for collaboration during training and testing; 2) training with CROWDLAB's consensus labels; and 3) pre-training with an LNL AI model. The comparison between collaborations involving a single user (LECODU w. SH-aggregation or random, where training uses a single label from aggregation through consensus or randomly selected, respectively, and testing relies on a randomly selected label) and multiple users (LECODU) highlights the significance of the latter. Multiple users collaboration mitigates the bias inherent in single-user collaborations, a benefit that is particularly pronounced in low noise rate problems (CIFAR-10H, Chaoyang, IDN20), where multi-user approaches consistently surpass single-user approaches at all cost levels, verifying that the lack of multiple users collaboration procedure has a negative impact in such scenarios. However, in scenarios with high noise rates, there is little difference between single and multiple users collaborative classification.\nNext, we analyse the differences among MRL approaches to emphasize the importance of generating reliable labels during training. Substituting CROWD-LAB's consensus labelling with simpler schemes like majority voting or random labelling (LECODU w. aggregation or random label, respectively) reveals the detrimental effect of relying on unreliable training labels across varying noise levels. Such an impact is severe in high noise rates scenarios (e.g., IDN50), but even in lower-noise rates settings (e.g., IDN20, Chaoyang), resorting to random labels significantly degrades model performance. To highlight even further the significance of CROWDLAB, we evaluate the accuracy of training set consensus labels generated by other MRL methods, such as majority voting and SOTA UnionNet-B, as displayed in Tab. 2. Compared to majority voting, UnionNet-B can improve the consensus labels in medium and high noise rate scenarios (e.g., IDN30, IDN40 and IDN50), but lags behind CROWDLAB. The results reflect CROWDLAB's superior capability in producing more reliable and accurate consensus labels. Furthermore, we conduct experiments without LNL AI model pre-training (LECODU w/o LNL) in Fig. 4, which confirms that LNL pre-training is crucial, particularly in situations where the LNL AI model's performance exceeds that of the original human annotations (e.g., CIFAR-10H and IDN{20,50}). The absence of LNL pre-training is detrimental in high-noise en-"}, {"title": "5 Conclusion", "content": "In this paper, we introduced a novel MEHAI-CC method LECODU. LECODU combines learning to defer and learning to complement strategies to make three key decisions: when to complement with expert decisions, when to defer decisions to experts, and how many experts should be engaged in the decision process. LECODU not only maximises classification accuracy but also minimises collaboration costs. Comprehensive evaluations across real-world and synthesized multiple noisy label datasets demonstrate LECODU's superior accuracy to SOTA human-AI collaborative classification methods. Even in medium and high noise rate scenarios where user annotations are unreliable, LECODU has a discernible improvement over human decision-makers and AI alone.\nOne potential weakness of LECODU is that we assume all users are the same, which helps the training process in two ways. First, it mitigates the combinatorial explosion of the number of different combinations of specific users and AI model in L2D and L2C scenarios. Second, we do not need to identify users, which prevents potential privacy concerns. However, such assumption is unrealistic since users are different and have distinct classification accuracy that need to be taken into account for training and testing. We plan to propose new AI systems that collaborate with users with different competencies. Another issue of LECODU is that it can promote over-reliance on AI in human-AI collaboration, leading to a reduction in the decision-making skills of humans. We plan to introduce methods to mitigate such deskilling problem."}]}