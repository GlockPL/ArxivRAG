{"title": "AutoFlow: Automated Workflow Generation for Large Language Model Agents", "authors": ["Zelong Li", "Wenyue Hua", "Hao Wang", "Shuyuan Xu", "Balaji Rama", "He Zhu", "Kai Mei", "Om Raheja", "Yongfeng Zhang"], "abstract": "Recent advancements in Large Language Models (LLMs) have shown significant progress in understanding complex natural language. One important application of LLM is LLM-based AI Agent, which leverages the ability of LLM as well as external tools for complex-task solving. To make sure LLM Agents follow an effective and reliable procedure to solve the given task, manually designed workflows are usually used to guide the working mechanism of agents. However, manually designing the workflows requires considerable efforts and domain knowledge, making it difficult to develop and deploy agents on massive scales. To address these issues, we propose AutoFlow, a framework designed to automatically generate workflows for agents to solve complex tasks. AutoFlow takes natural language program as the format of agent workflow and employs a workflow optimization procedure to iteratively optimize the workflow quality. Besides, this work offers two workflow generation methods: fine-tuning-based and in-context-based methods, making the AutoFlow framework applicable to both open-source and closed-source LLMs. Experimental results show that our framework can produce robust and reliable agent workflows. We believe that the automatic generation and interpretation of workflows in natural language represent a promising paradigm for solving complex tasks, particularly with the rapid development of LLMs. The source code of this work is available at https://github.com/agiresearch/AutoFlow.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) have demonstrated substantial progress in understanding and processing complex natural language. These developments have opened up a wide array of applications, among which the deployment of LLM-based AI agents stands out. These agents leverage the capabilities of LLMs along with external tools to tackle intricate tasks, ranging from data analysis [7], software development [23, 35], scientific research [2], travel planning [46] to many other decision-making processes in various domains.\nOne of the critical aspects of ensuring that LLM-based AI agents operate effectively and reliably is the design of workflows that guide their task-solving procedures. For example, an LLM-based agent for fake news detection may execute under the following workflow designed by information and"}, {"title": "2 Related Work", "content": "AI agent is an autonomous entity capable of making decisions and executing actions in a given environment to effectively handle various complex tasks [30, 8, 38, 45]. Recently, with the rapid advancement of Large Language Models (LLMs), LLM-based AI agents have become an important type of agent for complex task solving [7, 23, 35], such as reasoning, planning and coding.\nReasoning: LLMs typically break down complex tasks into a series of steps, constituting a chain of reasoning [40]. Approaches such as Chain of Thought (CoT) and its derivatives [40, 21], including tree [49] and graph structures [1], are commonly used. The self-consistency method [39] samples multiple reasoning paths and selects the most consistent outcome through voting.\nPlanning: Planning tasks require LLMs to generate a sequence of actions to achieve specific goals [9]. Recent studies have designed platforms to test LLMs' planning abilities in areas such as expert model integration [7], travel task planning [46], and tool usage [52]. However, a known issue is that LLMs may generate non-executable, invalid or grammatically wrong plans, such as using a piece of text as input to an image-processing tool. To solve the problem, some studies [7, 52] use post-processing method to extract a chain of tools from the generated texts, which use LLM itself as a parser to post-process the generated text. Further, recent attempts integrate finite state machines into LLMs to enhance human's controllability of LLM in planning [25, 44]. The ReAct approach [50] also uses external tools such as search engines to improve LLM planning. In this work, we build on these ideas to enhance the executability of the generated frameworks.\nCoding: LLMs can generate code to solve complex tasks, reducing the need for manual programming [29, 47, 15, 27, 5, 34, 31, 3]. However, the generated code may contain errors or fail to meet user requirements. To mitigate these issues, workflow-based methods have been proposed, including manually designed and automatically generated workflows [16, 43, 53]. Another research direction involves using LLMs for natural language programming, leveraging their strong natural language understanding abilities. A notable example is the CoRE language [47], which unifies natural language programming, pseudo-code programming, and workflow programming under the same framework using LLM as interpreter. Our work follows the workflow concept in natural language programming and develops an automated workflow generation framework to reduce human labor."}, {"title": "2.2 Automated Machine Learning", "content": "Automated Machine Learning (AutoML) aims to reduce human labors in designing and deploying machine learning techniques, simplifying the application of ML in real-world problems. There are three main types of AutoML techniques [48, 26]:\nAutomated Model Selection: Tools such as Auto-sklearn [6] and Auto-WEKA [22] automatically select the best machine learning model from a library of models and hyper-parameter settings.\nAutomated Feature Engineering: Tools such as Data Science Machine [17], ExploreKit [18], and VEST [4] generate or select useful features without manual intervention, since feature engineering significantly impacts model performance in many applications.\nNeural Architecture Search (NAS): Methods such as ENAS [33], DARTS [28], NASH [37], GNAS [12], and AmoebaNet-A [36] discover effective neural network architectures for specific tasks without manual design. Experiments show that networks generated through NAS can match or even outperform human-designed architectures across various tasks.\nAutoML systems typically involve two main components for training: a controller, which is a machine learning model responsible for sampling model selections, and a child model, which comprises the parameters of the machine learning model to be created and used for the task at hand. In our work, we follow this training paradigm, using a workflow generator LLM as the controller, and the generated workflow along with a workflow interpreter LLM as the child model. More details of the proposed technique are introduced in Section 4."}, {"title": "3 Preliminary and Background", "content": "In this section, we introduce how to use natural language programs as a representation of workflows. Specifically, we will use the Code Representation and Execution (CoRE) system [47] as an example to show how to construct workflows as natural language programs and how the LLM Agent follows the workflow by executing the natural language program.\nThe CoRE language defines four components to organize workflows as natural language instructions.\n\u2022 Step Name is used to uniquely identify each step of the workflow.\n\u2022 Step Type defines the type of instruction for each step. There are three different types of steps:\nProcess: The process step transitions to the next specified step after executing the current step.\nDecision: Similar to conditional statements (e.g., \u201cif-else\u201d), the decision step is used for branch- ing the program flow based on evaluated conditions.\nTerminal: The terminal step represents the end of the program.\n\u2022 Step Instruction is a natural language instruction to be executed in the step.\n\u2022 Step Connection points to the next step, which establishs the program execution flow."}, {"title": "3.1.2 LLM as Interpreter for Workflow Execution", "content": "To process and execute the workflow in the CoRE language, the system uses an LLM as an interpreter. The LLM interpreter executes instructions step by step. Concretely, the execution of one step can be divided into four procedures in the CoRE system.\n\u25cf First, the LLM decides which information from memory may be needed to execute the current step and retrieves the relevant information from memory. After obtaining the relevant information, the system integrates the information with the instruction of that step into a structured prompt, which the LLM processes to generate a response. To extend LLM's capability, the system may use external tools to analyze the initial response of each step. According to the initial response to the current step, the LLM determines whether external tools are required. If tool usage is confirmed, LLM will decide the tool name and tool arguments, then execute the external tool, and finally incorporate the results into the memory. After the execution of the current step, LLM will decide which is the next step to execute based on the output of the current step."}, {"title": "3.2 Motivation", "content": "The CoRE system enables users to write workflows in natural language, which unifies natural language programming, pseudo-code programming, and workflow programming. Although the entry barrier is lower than coding in programming languages, constructing workflows in natural language"}, {"title": "4 The AutoFlow Framework", "content": "In this section, we introduce the two methods of applying the AutoFlow framework to the workflow generator LLM, i.e., the fine-tuning method for open-source LLMs and the in-context learning method for closed-source LLMs."}, {"title": "4.1 Fine-tuning Method for Workflow Generation with Open-source LLMs", "content": "We use LoRA adapter [11] for fine-tuning open-souced LLMs as workflow generators. The training process is shown in Figure 2a.\nFirst, the workflow generator LLM receives a few-shot example workflow and a description of the task from users as the input query. Although the CoRE language has minimal grammar requirements and the instructions are written in natural language, which can be well learned and generated by LLMs, an example workflow can help the workflow generator LLM better understand the grammar of the CORE language. The natural language description of the task is to help the generator LLM understand the application scenarios of the workflow to be generated. Take the text and image processing tasks in OpenAGI benchmark [7] as an example, the task description could be \"Provide a workflow with several steps. The workflow can guide the LLM to design plans for a type of complex tasks realted to text and image processing using the provided tools\".\nSecond, the next step is to generate an executable workflow based on the input query. For closed- source LLMs such as GPT-4, the model can directly generate a grammatically valid workflow given the few-shot example. However, open-source LLMs such as Mixtral-8x7B cannot consistently generate grammatically valid workflow even if few-shot example workflows are provided. To solve the problem, we follow the post-processing strategy in previous work [7, 52] and use GPT-4 as a parser to revise the output workflow into a grammatically valid one."}, {"title": "4.2 In-context Learning Method for Workflow Generation with Closed-source LLMS", "content": "As for closed-source LLMs such as GPT-4, we use in-context learning to avoid fine-tuning the parameters. As shown in Figure 2b, the AutoFlow framework also requires an example workflow and a description of the task, and feeds them as the input query to the workflow generator LLM. After the GPT-4 generates the workflow, we do not use a parser to revise the flow since GPT-4 can well follow the CoRE grammar demonstrated by the example workflow. Then, the interpreter LLM executes the workflow to evaluate its performance on the validation dataset as the reward, which is the same process as the fine-tuning method. The difference is that, in the next step, the AutoFlow framework directly includes the reward value in the query and prompts the generator LLM to generate a new workflow given the performance of the previously generated workflow, such as \u201cThe execution performance of the previous workflow is 0.6415. Provide a new workflow that can gain a better performance\". The whole process is demonstrated in Figure 2b.\nWe will show in the experimentation that closed-source LLMs such as GPT-4 can well utilize the reward values in the prompt to refine the workflow and finally obtainn the optimal workflow by using the in-context learning method."}, {"title": "5 Experiments", "content": "We conduct experiments on both closed-source and open-source LLMs:\n\u2022 GPT-4 [32] (Closed-source) is a generative pre-trained transformer of OpenAI. In this work, we use the GPT-4-1106-preview version.\n\u2022 Mixtral-8x7B [14] (Open-source) is a pre-trained generative Sparse Mixture of Experts with 46.7 billion parameters.\nIn our experiment, we apply these two types of LLMs for both workflow generator LLM and interpreter LLM. Thus, there are four combinations in total."}, {"title": "5.2 Planning Schema of LLMS", "content": "We adopt the following LLM-based agent planning schema:\n\u2022 Zero-shot Learning (Zero) directly inputs the query to the LLM.\n\u2022 Chain-of-Thought (CoT) [40] induces the LLM to generate a coherent language sequence that serves as a meaningful intermediate step bridging the input query and the output answer.\n\u2022 Few-shot Learning (Few) presents a set of high-quality demonstrations in the prompt, each consisting of both input and desired output on the target task.\n\u2022 CORE [47] uses a manually designed workflow with LLM as an interpreter.\n\u2022 AutoFlow is our proposed framework that can automatically generate workflows."}, {"title": "5.3 Benchmark Datasets", "content": "We conduct experiments on a benchmark dataset, OpenAGI [7]. The OpenAGI benchmark tasks are categorized based on their output type and ground-truth label type (Task 1, 2, and 3). Then, based"}, {"title": "5.4 Implementation Details", "content": "Our framework and all baselines are implemented by PyTorch, an open-source library. We follow the implementation setting of the OpenAGI platform [7] for Zero-shot and few-shot learnings. We leverage the DSPy framework [19, 20] to apply the CoT strategy to the OpenAGI platform. We also tried Program-of-Thought [5] and ReAct [51] strategies on the OpenAGI platform. However, the ReAct strategy requires text observation, which is unsuitable for our OpenAGI task since some observations are in image format, and Program-of-Thought cannot generate executable codes. Thus, we did not include them as the baselines.\nFor the hyper-parameter setting of the AutoFlow framework, we set the number of iterations for the workflow generator LLM as 30. For the open-source LLM, Mixtral, as the generator LLM, we use the REINFORCE [41] as the core reinforcement learning (RL) algorithm for the generator LLM, with the average score on the training dataset as the reward. We use Adam as the optimizer with the learning rate at 0.001 for RL. Also, we apply Low-Rank Adaptation (LoRA) [11] with the rank equal to 8 to Mixtral for efficient fine-tuning."}, {"title": "5.5 Experimental Analysis", "content": "We conduct the experiments on the OpenAGI [7] benchmark dataset. For a fair comparison, we show the results using the same workflow interpreter LLM in a table. Specifically, the results of using the open-source LLM, Mixtral, as the LLM interpreter is shown in Table 1; and the results of using the closed-source LLM, GPT-4, as the LLM interpreter is shown in Table 2. Each row stands for a type of task, each column represents the planning schema of an LLM interpreter. From these two tables, we can see that, after applying our AutoFlow framework, the average score over tasks is significantly better than the baselines. Compared to the best baseline, CoRE, AutoFlow has over 40% improvement when using Mixtral as the LLM interpreter, and over 5% improvement when using GPT-4 as the interpreter LLM. For the score of each type of task, our AutoFlow also reaches the highest one. Thus, the experiment results validate that AutoFlow is effective and can generate a workflow with better performance than manually designed ones.\nAn interesting observation is that, the best average score when using Mixtral as the LLM interpreter, is AutoFlow with GPT-4 as the workflow generator; and the best average score when using GPT-4 as the LLM interpreter, is AutoFlow with Mixtral as the workflow generator. This observation suggests that the combination of different systems (Mixtral and GPT-4) for the LLM interpreter and workflow generator might lead to a kind of synergistic effect where the strengths of one system complement the weaknesses of the other, which helps to better solve complex multi-step tasks."}, {"title": "6 Conclusions and Future Work", "content": "In this study, we introduce the AutoFlow framework to use Large Language Models (LLMs) for automatically generating effective workflows for agents. We propose two learning methods for AutoFlow, the fine-tuning method when using open-source LLM as workflow generator, and the in-context learning method when using closed-source LLM as workflow generator. Compared to manually designed workflows, automatically generated workflows can reach better performance and significantly reduce the human labor, leading to a higher degree of automation.\nAlthough AutoFlow demonstrates promising results, there is still space for improvement. For example, the learning process for the workflow generator LLM uses reinforcement learning, which may not be the most efficient compared to some gradient-based methods or few-shot learning methods. Future studies may try to evaluate the efficacy of other learning methods. Another example in the AutoFlow framework is that, the workflow generator and interpreter LLMs work together using a collaborative learning paradigm. Instead, we may try other learning paradigms such as the teacher-student paradigm or the adversarial learning paradigm."}]}