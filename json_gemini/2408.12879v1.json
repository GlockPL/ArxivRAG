{"title": "Frequency-aware Feature Fusion for Dense Image Prediction", "authors": ["Linwei Chen", "Ying Fu", "Lin Gu", "Chenggang Yan", "Tatsuya Harada", "Gao Huang"], "abstract": "Dense image prediction tasks demand features with strong category information and precise spatial boundary details at high resolution. To achieve this, modern hierarchical models often utilize feature fusion, directly adding upsampled coarse features from deep layers and high-resolution features from lower levels. In this paper, we observe rapid variations in fused feature values within objects, resulting in intra-category inconsistency due to disturbed high-frequency features. Additionally, blurred boundaries in fused features lack accurate high frequency, leading to boundary displacement. Building upon these observations, we propose Frequency-Aware Feature Fusion (FreqFusion), integrating an Adaptive Low-Pass Filter (ALPF) generator, an offset generator, and an Adaptive High-Pass Filter (AHPF) generator. The ALPF generator predicts spatially-variant low-pass filters to attenuate high-frequency components within objects, reducing intra-class inconsistency during upsampling. The offset generator refines large inconsistent features and thin boundaries by replacing inconsistent features with more consistent ones through resampling, while the AHPF generator enhances high-frequency detailed boundary information lost during downsampling. Comprehensive visualization and quantitative analysis demonstrate that FreqFusion effectively improves feature consistency and sharpens object boundaries. Extensive experiments across various dense prediction tasks confirm its effectiveness.", "sections": [{"title": "1 INTRODUCTION", "content": "Dense image prediction encompasses various computer vision tasks that involve labeling each pixel in an image with a predefined class. These tasks include object detection [1], semantic segmentation [2], [3], instance segmentation [4], and panoptic segmentation [5]. They are crucial for scene understanding and are important for real-world applications such as autonomous driving [6], [7], medical imaging [8], [9], and robotics [10]. These tasks require robust category information for object classification and detailed spatial boundary information for object location.\nModern models [12]\u2013[14] typically use a hierarchical design with multiple downsampling operations to progressively reduce feature size. This process often results in the loss of detailed boundary information, which is essential for precise dense pixel-wise predictions. To solve this, feature fusion [15] is widely used [2], [9], [16]\u2013[18] to combine higher-level coarse features from deep layers with lower-level but high-resolution features. Empirically, higher-level features tend to carry more category information, while lower-level features provide more boundary details [15], [19], [20]. During standard feature fusion, coarse features are simply upsampled via nearest neighbor or bilinear interpolation and then added or concatenated with high-resolution features.\nNonetheless, standard feature fusion exhibits two issues that significantly impact dense prediction, namely, intra-category inconsistency and boundary displacement, as illustrated in Figure 1. One main reason for intra-class inconsistency arises from the substantial differences between various parts of the same object [21]. For instance, the wheel of a car may exhibit more texture and darkness, while the car window appears smooth and shiny. But standard feature fusion [16] falls short in addressing these inconsistent features. Instead, simple bilinear upsampling, commonly employed in it, may worsen the problem by upsampling a single inconsistent feature to multiple pixels, exacerbating intra-category inconsistency. Additionally, prior studies [22], [23] have observed that simple interpolation often overly smooths features, resulting in boundary displacement.\nTo quantify these problems, we employ feature similarity analysis, as illustrated in Figure 2. Intra-category inconsistency can be assessed through intra-category similarity, which measures the similarity between the feature vector and the category-wise averaged feature, i.e., category center [24]. Similarly, we can evaluate inter-class similarity, allowing us to calculate the similarity margin. The boundary displacement can be characterized by low intra-category similarity and similarity margin of boundary areas. As illustrated in Figure 1, the inconsistent features in the interior bus and truck exhibit low intra-category similarity, and the boundaries also manifest low and diminishing intra-category similarity. Given that the classification score is determined by computing the similarity between learned category-aware fixed weights and features [25], features with low intra-class similarity and low similarity margin lead to low classification scores for the corresponding category and result in misclassifications.\nIn this paper, we observe the presence of rapid changes or variations in feature values within the object, i.e., disturbed high-frequency in the feature leads to low intra-category similarity [11], resulting in intra-category inconsistency. Furthermore, the blurred boundary exhibits a lack of accurate high frequency, leading to boundary displacement, as shown in Figure 1.\nBuilding upon these observations, we propose frequency-aware feature fusion (FreqFusion), a method designed to enhance features during the process of feature fusion. FreqFusion consists of three key components: Adaptive Low-Pass Filter (ALPF) generator, offset generator, and Adaptive High-Pass Filter (AHPF) generator. The ALPF generator predicts spatially-variant low-pass filters, aiming to reduce intra-class inconsistency by attenuating high-frequency components within objects and smoothing the features during upsampling. The offset generator predicts offsets to resample feature pixels and replaces features with low intra-category similarity with nearby features exhibiting high intra-category similarity, thereby refining both the interior and boundaries. The AHPF generator extracts high-frequency details from lower-level features that cannot be recovered after downsampling, resulting in more accurate boundary delineation. These three components work collaboratively to recover fused features with consistent category information and sharp boundaries.\nSpecifically, the ALPF generator applies low-pass filters to smooth and upsample coarse high-level features, thereby reducing the disparity between pixel values and minimizing feature inconsistency. To prevent blurring at boundaries, inspired by [26], it predicts spatially variant low-pass filters for each upsampled feature coordinate instead of using the fixed kernel in the conventional interpolation [27]. Through feature similarity analysis, we reveal that smooth features with spatial-variant low-pass filters can largely reduce overall feature inconsistency. It increases intra-category similarity and leads to a higher similarity margin, thereby enhancing the consistency and discriminative nature of the features. Consequently, it benefits dense prediction tasks.\nWhile the use of smooth features with the ALPF generator increases overall intra-category similarity, it may not excel in rectifying large areas of inconsistent features or fine boundaries. Expanding the size of low-pass filters is beneficial for addressing large areas of inconsistent features but can be detrimental to thin and boundary areas. Conversely, reducing the size of low-pass filters benefits thin and boundary areas but hinders the correction of large areas with inconsistent features To address this contradiction, we introduce the offset generator. It is motivated by the observation that low intra-category similarity features often have neighbors with high intra-category similarity, as shown in Figure 1. The offset generator first calculates local similarity and then predicts an offset in the direction of high similarity for resampling. This approach allows for resampling features with high intra-category similarity to replace features with low intra-category similarity. Thus, the offset generator can rectify inconsistent features in both large areas and thin boundary regions.\nAlthough the ALPF generator and offset generator effectively recover upsampled high-level features with high intra-class consistency and refined boundaries, the detailed boundary information in lower-level features lost during downsampling cannot be fully restored in high-level features. According to the Nyquist-Shannon Sampling Theorem [28], [29], frequencies higher than the Nyquist frequency, which is equivalent to half of the sampling rate, are permanently lost during downsampling. For instance, frequencies above  becomes aliased during a 2\u00d7 downsampling operation (e.g., a 1\u00d71 convolution layer with a stride of 2 has a sampling rate of ). To address this limitation, we introduce the AHPF generator, which extracts detailed boundary information by predicting and applying spatially variant high-pass filters to low-level features, thereby enhancing the high-frequency power above the Nyquist frequency and sharping the boundary. Frequency analysis demonstrates an improvement in high-frequency power, resulting in finer visualized dense prediction results.\nQualitative results showcase the effectiveness of FreqFusion in recovering high-resolution features with discriminative category information and clear boundaries. Quantitative analysis reveals significant improvements in intra-category similarity and similarity margin. This, in turn, leads to substantial performance enhancements across various tasks, including semantic segmentation, object detection, instance segmentation, and panoptic segmentation, outperforming previous state-of-the-art methods. Specifically, 1)"}, {"title": "2 RELATED WORK", "content": "We begin by reviewing dense prediction tasks and techniques for feature fusion and aggregation. Subsequently, we introduce advancements in frequency domain learning."}, {"title": "2.1 Dense Image Prediction.", "content": "Dense image prediction tasks encompass various challenges, such as object detection [1], [35], semantic segmentation [2], [3], [36], [37], instance segmentation [4], [38]\u2013[40], and panoptic segmentation [5]. The advancements in dense prediction primarily hinge on a few seminal deep network architectures. For instance, since the advent of fully convolutional networks (FCNs) in semantic segmentation [2], the field has evolved with foundational segmentation architectures like U-Net [9], SegNet [41], and DeepLab [3], [42]. Similarly, in object detection, models such as R-CNN [1] and YOLO [43] have dominated the landscape in recent years. Subsequently, typical network architectures like Feature Pyramid Networks (FPNs) [16] have been widely used in other dense prediction tasks, including instance segmentation and panoptic segmentation.\nDifferent from image processing or generation tasks [44]\u2013[55], in these representative architectures for dense prediction, feature fusion is an essential component. This is because most backbone architectures involve multiple downsampling stages [13], [14], [56], while the expected output often requires high-resolution information for accurate object classification and detailed spatial boundaries for precise object localization. Feature fusion offers a simple solution by combining low-level, high-resolution features with coarse, high-level features. In this work, we further propose an effective feature fusion method named FreqFusion, to obtain high-quality fused features with consistent category information and clear boundaries. The proposed FreqFusion seamlessly integrates with state-of-the-art architectures, from CNNs (e.g., SegNeXt [31]) to Transformers (e.g., SegFormer [30]), providing a performance boost with minimal additional parameters and computational cost.\nWe select a few representative models as baselines in different tasks, including SegNeXt [31], SegFormer [30], and Mask2Former for semantic segmentation, Faster R-CNN for object detection, Mask R-CNN [4] for instance segmentation, Panoptic FPN [5] for panoptic segmentation). We demonstrate how FreqFusion is applied to these tasks and how it improves upon these baselines."}, {"title": "2.2 Feature Fusion and Aggregation.", "content": "Feature fusion is a process to fuse lower-level high-resolution features and higher-level coarse features to obtain details and semantic information [15]. While feature aggregation aggregates features from different network stages, i.e., it consists of a series of feature fusion operations.\nThere are two most common feature aggregation architectures, one is top-down, e.g., SegNet [41], U-Net [9], RefineNet [57], and DeepLabv3+ [3], which fuse features from low resolution to high resolution. The other is bottom-up, e.g., FRRN [58], DLA [59], and SeENet [60], which aggregate features from high resolution to low resolution. Owing to the gap between resolution and semantic levels, a simple fusion strategy is less effective [15].\nNext, we introduce recent methods to improve feature fusion, which can be categorized into two types: 1) kernel-based, and 2) sampling-based.\nKernel-based. To fuse low and high-resolution features, feature upsampling is needed to upscale the low-resolution features. Traditional upsampling operations, e.g., nearest neighbor and bilinear interpolation use fixed/hand-crafted kernels that are defined by the relative distance between pixels. Though kernel parameters of deconvolution [61], Pixel shuffle [62], and DUpsampling [63] are learnable, the upsampling kernels are fixed and spatially-invariant once learned. The importance of dynamic property has been emphasized recently. As a hand-crafted operator, un-pooling [41] has dynamic upsampling behavior, in other words, upsampled positions are conditioned on max pooling indices. CARAFE [64] dynamically reassembles the local decoder features in a context-aware manner. Similar to CARAFE, IndexNet [65] and A2U [66] also only consider the assets of encoder features for dynamic upsampling. SAPA [22] and FADE [23] further consider the assets of both encoder and decoder features for generating upsampling kernels. In summary, the essence of these operators lies in the data-dependent upsampling kernels, whose parameters are predicted by a sub-network. This underscores a promising avenue for exploring better feature upsampling.\nSampling-based. Recently, a series of methods aim to improve feature fusion by adjusting sampling coordinates. GUM [67] learns the guidance offsets and applies those offsets to upsampled feature maps. SFNet [19] warps coarse high-level features with predicted offsets for alignment. AlignSeg [68] further predicts offsets for aligning multi-resolution features and context modeling. FaPN [69] utilizes deformable convolution [70] to align features from the coarsest resolution to the finest resolution progressively. IFA [20] aligns multi-resolution features with implicit offsets by using implicit neural representation. Dysample [71] learn to upsample by learning to sample the coarse high-level feature dynamically. These methods apply explicit or implicit spatial offsets to align the low and high-resolution features. Moreover, recent works [15], [21], [72] employ channel attention or gates to combine lower and higher-level features, using adaptive weights conditioned on higher-level features rather than equal weights. The Mask2Former utilizes a deformable attention module [73], which applies both spatial offsets and adaptive attention weights to fuse multi-scale features.\nOur work is closely related to both kernel-based and dynamic sampling-based methods. While previous studies empirically observe the problem in standard feature fusion, they lack clear definitions supported by quantitative measurements. In contrast, we clearly identify and define the issues of intra-category inconsistency and boundary displacement, measuring them through feature similarity analysis. The proposed FreqFusion effectively addresses these issues with the goal of achieving simultaneous feature consistency and boundary sharpness."}, {"title": "2.3 Frequency Domain Learning", "content": "Frequency-domain analysis, as a fundamental tool [74], [75], has long been proven to be an effective tool for traditional signal processing. Recently, a series of works have introduced frequency analysis to deep learning.\nIn this context, they are employed to examine the optimization strategies [76] and generalization capabilities [77] of Deep Neural Networks (DNNs). Rahaman et al. [78] and Xu et al. [79] find the effective target function for a deeper hidden layer bias towards lower frequencies during training, thus these networks prioritize learning the low-frequency modes, this phenomenon is called spectral bias/frequency principle. Zhang et al. [80] investigate how frequency aliasing impacts the shift-invariance of modern models, and subsequently, AdaBlur [26] applied content-aware low-pass filters during downsampling for anti-aliasing. Additionally, FLC [81] also demonstrated that frequency aliasing degrades the robustness of models. Qin et al. [82] and Magid et al. [83] explore utilizing more frequency components obtained from discrete cosine transform coefficients for channel attention mechanisms. Huang et al. [84] employ the conventional convolution theorem in DNNs, demonstrating that adaptive frequency filters can efficiently serve as global token mixers. A series of frequency-domain techniques have also been seamlessly integrated into DNN architectures, facilitating the learning of non-local features [84]\u2013[88]. Chen et al. [38] demonstrate how the low-pass filter suppress high frequency feature noise caused by noise in the images, effectively addressing the challenges of instance segmentation in low-light conditions. Many works demonstrate adversarial attack can be achieved by manipulate the high frequency components [11], [76], [89] Luo et al. [11] demonstrate that perturbing high frequencies leads to a large reduction in intra-category similarity, thereby degrading feature representations.\nIn this work, we consider intra-category inconsistency as the presence of disturbed high frequency, significantly reducing intra-category similarity, as observed in [11]. Boundary displacement is regarded as a lack of high frequency, as noted in [83]. Thus, we employ adaptive low-pass filters to reduce feature inconsistency and high-pass filters to enhance useful high-frequency details and sharpen boundaries. This demonstrates an innovative application of frequency-domain techniques in addressing intra-category inconsistency and boundary displacement in feature fusion, benefiting various fundamental computer vision tasks."}, {"title": "3 FEATURE SIMILARITY ANALYSIS METRICS", "content": "We begin by introducing metrics for feature similarity analysis. These metrics aim to quantify both intra-category inconsistency and boundary displacement issues that emerge during the process of feature fusion. This establishes a solid foundation for developing and analyzing effective feature fusion techniques.\nFeature similarity is widely used for assessing the quality of extracted features [24], [90]\u2013[92]. Typically, features within the same category should show high similarity, ensuring high intra-category similarity. On the other hand, features from different categories should exhibit low similarity, resulting in low inter-category similarity. A large gap between intra-category and inter-category similarities, referred to as similarity margin, is crucial for preventing misclassification.\nTo facilitate a quantitative assessment of intra-category inconsistency and boundary displacement, as well as to assess the quality of the fused features, we introduce metrics encompassing intra-category similarity, similarity margin, and similarity accuracy. These metrics offer a comprehensive framework for evaluating the discriminative power of extracted feature maps.\nIntra-category & inter-category similarity. Intra-category similarity is computed by first deriving the category center through the averaging of features within each category. Subsequently, we calculate the cosine similarity between the category center and the feature vector belonging to the same category. This is expressed as:\n$IntraSim(Y_{i,j}=1) = CosSim(Y_{i,j}=1, \\frac{1}{|O_{cls=1}|} \\sum_{i,j \\in O_{cls=1}}Y_{i,j})$.\nHere, we consider binary segmentation that has two categories, $cls = 1$ denotes the ground truth category of feature vector $Y_{i,j}$, $O_{cls=1}$ represents the area belonging to category 1, and CosSim is the cosine similarity. Similarly, the inter-category similarity is calculated using the same method, with the distinction that the category center and feature vector are from different categories.\n$InterSim(Y_{i,j}=1) = CosSim(Y_{i,j}=1, \\frac{1}{|O_{cls=0}|} \\sum_{i,j \\in O_{cls=0}}Y_{i,j})$.\nHere, $cls=0$ indicates the area that belong to category from $Y_{i,j}$.\nSimilarity margin. Consequently, the similarity margin is determined by subtracting the inter-category similarity from the intra-category similarity\n$SimMargin(Y_{i,j}) = IntraSim(Y_{i,j}) \u2013 InterSim(Y_{i,j})$.\nSimilarity accuracy. To comprehensively assess the risk of mis-classification rate resulting from intra-category inconsistency and"}, {"title": "4 FREQUENCY-AWARE FEATURE FUSION", "content": "In this section, we introduce FreqFusion as shown in Figure 3. It consists of three essential components: the Adaptive Low-Pass Filter (ALPF) generator, the offset generator, and the Adaptive High-Pass Filter (AHPF) generator, as illustrated in Figure 4.\nFreqFusion operates through two primary stages, i.e., initial fusion and final fusion. Prior to the final fusion step, it is necessary to compress and fuse both low-level and high-level features to serve as input for the three generators, ensuring efficiency in the final fusion stage. We first introduce how we enhance initial fusion, elucidating its significance within the FreqFusion framework. Subsequently, we provide detailed insights into the functioning of each of the three generators, thereby offering a comprehensive understanding of their roles in the fusion process."}, {"title": "4.1 Overview of FreqFusion", "content": "We begin by presenting the widely-used standard feature fusion approach, followed by an overview of the design of FreqFusion.\nStandard feature fusion. Generally, a common way of feature fusion can be formulated as [15], [16], [93]:\n$Y^l = FUP(Y^{l+1}) + X^l,$\nwhere $X^l \\in R^{C \\times 2H \\times 2W}, Y^{l+1} \\in R^{C \\times H \\times W}$ represent the l-th features generated by the backbone and the fused feature at the l-th level, respectively. We assume they have the same number of channels; if not, a simple projection function like a 1 \u00d7 1 convolution can ensure this [16], which we omit for brevity. The term $FUP$ denotes upsampling, for example, 2\u00d7 nearest neighbor or bilinear interpolation [16], [17].\nAlthough widely used, this straightforward approach to feature fusion manifests two issues that detrimentally impact dense prediction, i.e., intra-category inconsistency and boundary displacement. Standard fusion falls short in rectifying these inconsistent features, and simple interpolation within it may even worsen the problem by upscaling a single inconsistent feature to multiple inconsistent pixels. Furthermore, as observed in various prior works [2], [22], [65], outputs from simple interpolation often lean towards excessive smoothness, resulting in boundary displacement. Additionally, the detailed boundary information in lower-level features is not fully utilized.\nDesign of FreqFusion. As shown in Figure 3, the proposed FreqFusion can be formally written as:\n$\\begin{aligned} & \\hat{Y}^{l+1}_{i,j} = F_{LP}(\\hat{Y}^{l+1}) + (u, v), \\\\ & \\Upsilon^{l}_{i,j} = Y^{l}_{i,j}+X^{l}_{i,j}\\\\ &\\hat{\\Upsilon}^{l+1} = F_{UP}(F_{LP} (Y^{l+1})), \\\\ &\\hat{X}^{l} = F_{HP} (X^{l}) + X^{l}.\\end{aligned}$"}, {"title": "4.2 Enhancing Initial Fusion", "content": "The three generators rely on the initially fused compressed feature $Z^l$ to predict adaptive filters and resampling offsets. However, the simple initial fusion presented in Equation (6) exhibits two sub-optimal aspects, which can adversely affect the subsequent three"}, {"title": "4.3 Adaptive Low-Pass Filter Generator", "content": "The Adaptive Low-Pass Filter (ALPF) generator is designed to predict dynamic low-pass filters, aiming to effectively smooth high-level features to mitigate feature inconsistency [11] and subsequently upsample the high-level feature. To achieve high-quality adaptive low-pass filters, it is crucial to leverage the advantages of both high-level and low-level features [23]. Thus, the ALPF generator takes the initially fused $Z^l$ as input and predicts spatial-variant low-pass filters. It comprises a 3 \u00d7 3 convolutional layer followed by a softmax layer, which is represented as:\n$\\Upsilon^{l}_{i,j} = Y^{l}_{i,j}+X^{l}_{i,j},$\n$W = Softmax(V^{l}_{i,j}) = \\frac{exp(p,q)}{\\sum_{p,q \\in \\Omega }exp(V^{l}_{i,j})}$,\nwhere $V^l \\in R^{K^2 \\times 2H \\times 2W}$ represents spatially-variant filter weights, where $K$ indicates the kernel size of the low-pass filter. After reshaping, $W^l$ contains $K \\times K$ filters for each position. Here, $\\Omega$ denotes a size of $K \\times K$. Upon passing through a kernel-wise softmax to constrain the filters to be all positive and sum to one, the results are smooth and low-pass filters in $W \\in R^{K^2 \\times 2H \\times 2W}$ [26].\nNext, we upscale $Y^{l+1} \\in R^{C \\times H \\times W}$ using a sub-pixel upsampling technique [62]. Specifically, we reshape $W^l$ in a pixel unshuffle way [62], reducing the height and width by half and expanding the channel by 4\u00d7. We then divide the channels into 4 groups, with each group having a spatially-variant low-pass filter denoted as $W^{l,g} \\in R^{K^2 \\times H \\times W}$, where $g \\in {1,2,3,4}$ indicates the group. Consequently, we obtain 4 groups of low-pass filtered features, represented as $\\hat{Y}^{l+1,g} \\in R^{C \\times H \\times W}$, which are then rearranged to form a 2\u00d7 upsampled feature $\\hat{Y}^{l+1} \\in R^{C \\times 2H \\times 2W}$ as:\n$\\hat{Y}^{l+1}_{i+p,j+q} = \\sum_{p,q \\in \\Omega} g \\hat{Y}^{l+1,9},$\n$\\hat{Y}^{l+1} = PixelShuffle(\\hat{Y}^{l+1,1},\\hat{Y}^{l+1,2}, \\hat{Y}^{l+1,3}, \\hat{Y}^{l+1,4})$.\nAs illustrated in Figure 6, the ALPF generator adaptively predicts spatially variant low-pass filters based on the feature"}, {"title": "4.4 Offset Generator", "content": "While the ALPF generator smooths features to enhance overall intra-category similarity, it may fall short in rectifying extensive areas of inconsistent features or refining thin and boundary areas. Increasing the size of low-pass filters proves beneficial for addressing large inconsistent regions but can adversely impact thin and boundary areas. Conversely, reducing the filter size aids in preserving thin and boundary areas but may hinder the correction of extensive areas with inconsistent features.\nTo address this dilemma, we propose the offset generator. Motivated by the observation that neighboring features with low intra-category similarity often exhibit features with high intra-category similarity. The offset generator begins the process by computing local cosine similarity:\n$S^{p,q}_{i,j} = \\frac{\\sum^{C}_{c=1}Z^c_{i,j}Z^c_{i+p,j+q}}{\\sqrt{(\\sum^{C}_{c=1}Z^c_{i,j})^2}\\sqrt{(\\sum^{C}_{c=1}(Z^c_{i+p,j+q})^2)}}$,\nwhere $S \\in R^{8 \\times H \\times W}$ contain the cosine similarity between each pixel and its 8 neighbor pixels, which encourage the offset generator to sample towards features with high intra-category similarity, thereby reducing the ambiguity in boundary or intra-category inconsistent areas, as depicted in Figures 8 and 9.\nSpecifically, the offset generator takes the $Z^l$ and $S$ as input and predicts offsets. It consists of two 3 \u00d7 3 convolutional layers to predict the offset direction and offset scale, represented as:\n$\\begin{aligned} & O\u2019 = D^{l} \u00b7 A^{l}, \\\\ & D^{l} = Conv3 \u00d7 3(Concat(Z^{l}, S^{l})), \\\\ & A^{l} = Sigmoid(Conv3 \u00d7 3(Concat(Z^{l}, S^{l}))), \\end{aligned}$"}, {"title": "4.5 Adaptive High-Pass Filter Generator", "content": "Although the ALPF generator and offset generator effectively recover upsampled high-level features with high intra-class consistency and refined boundaries, the detailed boundary information present in lower-level features, lost during downsampling, cannot be fully restored in high-level features.\nAccording to the Nyquist-Shannon Sampling Theorem [28], [29], frequencies higher than the Nyquist frequency, which is equivalent to half of the sampling rate, are permanently lost during downsampling. For example, when the high-level feature is downsampled by a factor of 2 compared to the low-level feature to be fused (e.g., using a 1\u00d71 convolution layer with a stride of 2 for downsampling, resulting in a sampling rate of ), frequencies above  become aliased during the process.\nTo elaborate, we transform the feature map $X \\in R^{C \\times H \\times W}$ into the frequency domain using the Discrete Fourier Transform (DFT), denoted as $X_F = F(X)$, expressed as:\n$X_F(u, v) = \\frac{1}{HW} \\sum^{H-1}_{h=0} \\sum^{W-1}_{w=0} X(h, w)e^{-2\\pi j(\\frac{uh}{H}+\\frac{vw}{W})}$,\nwhere $X_F \\in R^{C \\times H \\times W}$ represents the output array of complex numbers from the DFT. H and W denote its height and width. h, w indicate the coordinates of feature map X. The normalized frequencies in the height and width dimensions are given by |u| and |v|. Consequently, the set of high frequencies larger than the Nyquist frequency $H_+ = {(u,v) | |k| > \\frac{H}{2} or |\\frac{1}{4} > \\frac{W}{2} }$ is aliased and permanently lost in downsampled high-level features.\nTo address this limitation, we employ the AHPF generator to enhance detailed boundary information lost during downsampling. Specifically, the AHPF generator takes the initially fused $Z^l$ as input and predicts spatial-variant high-pass filters. It consists of a 3 \u00d7 3 convolutional layer followed by a softmax layer and a filter inversion operation, represented as:\n$\\begin{aligned} & V^{l} = Conv3 \u00d7 3(Z^{l}), \\\\ & \\hat{W}^{p,q}_{i,j} = E - Softmax(V_{i,j}^{l})\\\\ & = E - \\frac{exp(V_{i,j}^{p,q})}{\\sum_{p,q \\in \\Omega} exp(V_{i,j}^{p,q})}, \\end{aligned}$"}, {"title": "5 EXPERIMENTAL RESULTS", "content": "We first showcase the universality of the proposed FreqFusion across four typical dense prediction tasks, including semantic segmentation, object detection, instance segmentation, and panoptic segmentation. Following the setting [22], [71] we set the kernel size in Deconvolution and Pixel Shuffle [62] as 3. For CARAFE [94], we adhere to its default configuration. We utilize the 'HIN' version of IndexNet [95] and the 'dynamic-cs-d\u2020' version of A2U [66]. In the interest of stability across all dense prediction tasks, we opt for FADE [23] without a gating mechanism and SAPA-B [22]."}, {"title": "5.1 Semantic Segmentation", "content": "Semantic segmentation necessitates the prediction of per-pixel class labels, ensuring that pixel groups belonging to the same object class are appropriately clustered. Typically, the decoder of a segmentation model employs a stage-by-stage upsampling and fusion architecture [17], [30], highlighting the crucial role of feature fusion in this process.\nGiven the significance of feature fusion, FreqFusion is particularly well-suited to justify its behaviors in the context of semantic segmentation tasks. The inherent requirements of this task involve the precise clustering and separating of pixels for distinct object classes. This necessitates both low intra-category inconsistency and low boundary displacement, underscoring the importance of effective feature fusion mechanisms, such as those employed by FreqFusion."}, {"title": "5.1.1 Experimental Settings", "content": "Datasets. We evaluate our methods on several popular challenging datasets including Citysacpes [96", "33": "and COCO-Stuff [97", "96": "contains 19 semantic categories for semantic segmentation tasks and consists of 5,000 finely annotated images of 2048 \u00d7 1024 pixels, its training, validation, and test set have 2,975, 500, and 1,525 samples, respectively. We only use the training set for learning. ADE20K [33", "97": "is a challenging benchmark, which contains 172 semantic categories and 164k images in total. i.e., 118k for training, 5k for validation, 20k for test-dev and 20k for the test-challenge.\nMetrics. In line with previous works such as Segformer [30", "32": "and SegNext [31"}]}