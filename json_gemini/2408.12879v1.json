{"title": "Frequency-aware Feature Fusion for Dense Image Prediction", "authors": ["Linwei Chen", "Ying Fu", "Lin Gu", "Chenggang Yan", "Tatsuya Harada", "Gao Huang"], "abstract": "Dense image prediction tasks demand features with strong category information and precise spatial boundary details at high resolution. To achieve this, modern hierarchical models often utilize feature fusion, directly adding upsampled coarse features from deep layers and high-resolution features from lower levels. In this paper, we observe rapid variations in fused feature values within objects, resulting in intra-category inconsistency due to disturbed high-frequency features. Additionally, blurred boundaries in fused features lack accurate high frequency, leading to boundary displacement. Building upon these observations, we propose Frequency-Aware Feature Fusion (FreqFusion), integrating an Adaptive Low-Pass Filter (ALPF) generator, an offset generator, and an Adaptive High-Pass Filter (AHPF) generator. The ALPF generator predicts spatially-variant low-pass filters to attenuate high-frequency components within objects, reducing intra-class inconsistency during upsampling. The offset generator refines large inconsistent features and thin boundaries by replacing inconsistent features with more consistent ones through resampling, while the AHPF generator enhances high-frequency detailed boundary information lost during downsampling. Comprehensive visualization and quantitative analysis demonstrate that FreqFusion effectively improves feature consistency and sharpens object boundaries. Extensive experiments across various dense prediction tasks confirm its effectiveness.", "sections": [{"title": "1 INTRODUCTION", "content": "DENSE image prediction encompasses various computer vision tasks that involve labeling each pixel in an image with a predefined class. These tasks include object detection [1], semantic segmentation [2], [3], instance segmentation [4], and panoptic segmentation [5]. They are crucial for scene understanding and are important for real-world applications such as autonomous driving [6], [7], medical imaging [8], [9], and robotics [10]. These tasks require robust category information for object classification and detailed spatial boundary information for object location.\nModern models [12]\u2013[14] typically use a hierarchical design with multiple downsampling operations to progressively reduce feature size. This process often results in the loss of detailed boundary information, which is essential for precise dense pixel-wise predictions. To solve this, feature fusion [15] is widely used [2], [9], [16]\u2013[18] to combine higher-level coarse features from deep layers with lower-level but high-resolution features. Empirically, higher-level features tend to carry more category information, while lower-level features provide more boundary details [15], [19], [20]. During standard feature fusion, coarse features are simply upsampled via nearest neighbor or bilinear interpolation and then added or concatenated with high-resolution features.\nNonetheless, standard feature fusion exhibits two issues that significantly impact dense prediction, namely, intra-category in-consistency and boundary displacement, as illustrated in Figure 1. One main reason for intra-class inconsistency arises from the substantial differences between various parts of the same ob-ject [21]. For instance, the wheel of a car may exhibit more texture and darkness, while the car window appears smooth and shiny. But standard feature fusion [16] falls short in addressing these inconsistent features. Instead, simple bilinear upsampling, commonly employed in it, may worsen the problem by upsampling a single inconsistent feature to multiple pixels, exacerbating intra-category inconsistency. Additionally, prior studies [22], [23] have observed that simple interpolation often overly smooths features, resulting in boundary displacement.\nTo quantify these problems, we employ feature similarity analysis, as illustrated in Figure 2. Intra-category inconsistency can be assessed through intra-category similarity, which measures the similarity between the feature vector and the category-wise averaged feature, i.e., category center [24]. Similarly, we can evaluate inter-class similarity, allowing us to calculate the similarity margin. The boundary displacement can be characterized by low intra-category similarity and similarity margin of boundary areas. As illustrated in Figure 1, the inconsistent features in the interior bus and truck exhibit low intra-category similarity, and the boundaries also manifest low and diminishing intra-category similarity. Given that the classification score is determined by computing the similarity between learned category-aware fixed weights and features [25], features with low intra-class similarity and low similarity margin lead to low classification scores for the corresponding category and result in misclassifications.\nIn this paper, we observe the presence of rapid changes or variations in feature values within the object, i.e., disturbed high-frequency in the feature leads to low intra-category similarity [11], resulting in intra-category inconsistency. Furthermore, the blurred boundary exhibits a lack of accurate high frequency, leading to boundary displacement, as shown in Figure 1.\nBuilding upon these observations, we propose frequency-aware feature fusion (FreqFusion), a method designed to enhance features during the process of feature fusion. FreqFusion consists of three key components: Adaptive Low-Pass Filter (ALPF) generator, offset generator, and Adaptive High-Pass Filter (AHPF) generator. The ALPF generator predicts spatially-variant low-pass filters, aiming to reduce intra-class inconsistency by attenuating high-frequency components within objects and smoothing the features during upsampling. The offset generator predicts offsets to resample feature pixels and replaces features with low intra-category similarity with nearby features exhibiting high intra-category similarity, thereby refining both the interior and bound-aries. The AHPF generator extracts high-frequency details from lower-level features that cannot be recovered after downsampling, resulting in more accurate boundary delineation. These three components work collaboratively to recover fused features with consistent category information and sharp boundaries.\nSpecifically, the ALPF generator applies low-pass filters to smooth and upsample coarse high-level features, thereby reduc-ing the disparity between pixel values and minimizing feature inconsistency. To prevent blurring at boundaries, inspired by [26], it predicts spatially variant low-pass filters for each upsampled feature coordinate instead of using the fixed kernel in the con-ventional interpolation [27]. Through feature similarity analysis, we reveal that smooth features with spatial-variant low-pass fil-ters can largely reduce overall feature inconsistency. It increases intra-category similarity and leads to a higher similarity margin, thereby enhancing the consistency and discriminative nature of the features. Consequently, it benefits dense prediction tasks.\nWhile the use of smooth features with the ALPF generator increases overall intra-category similarity, it may not excel in rectifying large areas of inconsistent features or fine boundaries. Expanding the size of low-pass filters is beneficial for addressing large areas of inconsistent features but can be detrimental to thin and boundary areas. Conversely, reducing the size of low-pass filters benefits thin and boundary areas but hinders the correction of large areas with inconsistent features To address this contradiction, we introduce the offset generator. It is motivated by the observation that low intra-category similarity features often have neighbors with high intra-category similarity, as shown in Figure 1. The offset generator first calculates local similarity and then predicts an offset in the direction of high similarity for resampling. This approach allows for resampling features with high intra-category similarity to replace features with low intra-category similarity. Thus, the offset generator can rectify inconsistent features in both large areas and thin boundary regions.\nAlthough the ALPF generator and offset generator effectively recover upsampled high-level features with high intra-class con-sistency and refined boundaries, the detailed boundary information in lower-level features lost during downsampling cannot be fully restored in high-level features. According to the Nyquist-Shannon Sampling Theorem [28], [29], frequencies higher than the Nyquist frequency, which is equivalent to half of the sampling rate, are permanently lost during downsampling. For instance, frequencies above become aliased during a 2\u00d7 downsampling operation (e.g., a 1\u00d71 convolution layer with a stride of 2 has a sampling rate of). To address this limitation, we introduce the AHPF generator, which extracts detailed boundary information by pre-dicting and applying spatially variant high-pass filters to low-level features, thereby enhancing the high-frequency power above the Nyquist frequency and sharping the boundary. Frequency analysis demonstrates an improvement in high-frequency power, resulting in finer visualized dense prediction results.\nQualitative results showcase the effectiveness of FreqFusion in recovering high-resolution features with discriminative category information and clear boundaries. Quantitative analysis reveals significant improvements in intra-category similarity and similar-ity margin. This, in turn, leads to substantial performance enhance-ments across various tasks, including semantic segmentation, ob-ject detection, instance segmentation, and panoptic segmentation, outperforming previous state-of-the-art methods. Specifically, 1)"}, {"title": "2 RELATED WORK", "content": "We begin by reviewing dense prediction tasks and techniques for feature fusion and aggregation. Subsequently, we introduce advancements in frequency domain learning.\n2.1 Dense Image Prediction.\nDense image prediction tasks encompass various challenges, such as object detection [1], [35], semantic segmentation [2], [3], [36], [37], instance segmentation [4], [38]\u2013[40], and panoptic segmen-tation [5]. The advancements in dense prediction primarily hinge on a few seminal deep network architectures. For instance, since the advent of fully convolutional networks (FCNs) in semantic segmentation [2], the field has evolved with foundational segmen-tation architectures like U-Net [9], SegNet [41], and DeepLab [3], [42]. Similarly, in object detection, models such as R-CNN [1] and YOLO [43] have dominated the landscape in recent years. Subsequently, typical network architectures like Feature Pyramid Networks (FPNs) [16] have been widely used in other dense prediction tasks, including instance segmentation and panoptic segmentation.\nDifferent from image processing or generation tasks [44]\u2013[55], in these representative architectures for dense prediction, feature fusion is an essential component. This is because most backbone architectures involve multiple downsampling stages [13], [14], [56], while the expected output often requires high-resolution information for accurate object classification and detailed spatial boundaries for precise object localization. Feature fusion offers a simple solution by combining low-level, high-resolution features with coarse, high-level features. In this work, we further propose an effective feature fusion method named FreqFusion, to obtain high-quality fused features with consistent category informa-tion and clear boundaries. The proposed FreqFusion seamlessly integrates with state-of-the-art architectures, from CNNs (e.g., SegNeXt [31]) to Transformers (e.g., SegFormer [30]), providing a performance boost with minimal additional parameters and computational cost.\nWe select a few representative models as baselines in different tasks, including SegNeXt [31], SegFormer [30], and Mask2Former for semantic segmentation, Faster R-CNN for object detection, Mask R-CNN [4] for instance segmentation, Panoptic FPN [5] for panoptic segmentation). We demonstrate how FreqFusion is applied to these tasks and how it improves upon these baselines.\n2.2 Feature Fusion and Aggregation.\nFeature fusion is a process to fuse lower-level high-resolution features and higher-level coarse features to obtain details and semantic information [15]. While feature aggregation aggregates features from different network stages, i.e., it consists of a series of feature fusion operations.\nThere are two most common feature aggregation architectures, one is top-down, e.g., SegNet [41], U-Net [9], RefineNet [57], and DeepLabv3+ [3], which fuse features from low resolution to high resolution. The other is bottom-up, e.g., FRRN [58], DLA [59], and SeENet [60], which aggregate features from high resolution to low resolution. Owing to the gap between resolution and semantic levels, a simple fusion strategy is less effective [15].\nNext, we introduce recent methods to improve feature fusion, which can be categorized into two types: 1) kernel-based, and 2) sampling-based.\nKernel-based. To fuse low and high-resolution features, feature upsampling is needed to upscale the low-resolution features. Tra-ditional upsampling operations, e.g., nearest neighbor and bilinear interpolation use fixed/hand-crafted kernels that are defined by the relative distance between pixels. Though kernel parameters of deconvolution [61], Pixel shuffle [62], and DUpsampling [63] are learnable, the upsampling kernels are fixed and spatially-invariant once learned. The importance of dynamic property has been emphasized recently. As a hand-crafted operator, un-pooling [41] has dynamic upsampling behavior, in other words, upsampled positions are conditioned on max pooling indices. CARAFE [64] dynamically reassembles the local decoder features in a context-aware manner. Similar to CARAFE, IndexNet [65] and A2U [66] also only consider the assets of encoder features for dynamic upsampling. SAPA [22] and FADE [23] further consider the assets of both encoder and decoder features for generating upsampling kernels. In summary, the essence of these operators lies in the data-dependent upsampling kernels, whose parameters are predicted by a sub-network. This underscores a promising avenue for exploring better feature upsampling.\nSampling-based. Recently, a series of methods aim to improve feature fusion by adjusting sampling coordinates. GUM [67] learns the guidance offsets and applies those offsets to upsampled feature maps. SFNet [19] warps coarse high-level features with predicted offsets for alignment. AlignSeg [68] further predicts off-sets for aligning multi-resolution features and context modeling. FaPN [69] utilizes deformable convolution [70] to align features from the coarsest resolution to the finest resolution progressively. IFA [20] aligns multi-resolution features with implicit offsets by using implicit neural representation. Dysample [71] learn to upsample by learning to sample the coarse high-level feature dynamically. These methods apply explicit or implicit spatial offsets to align the low and high-resolution features. Moreover, recent works [15], [21], [72] employ channel attention or gates to combine lower and higher-level features, using adaptive weights conditioned on higher-level features rather than equal weights. The Mask2Former utilizes a deformable attention module [73], which applies both spatial offsets and adaptive attention weights to fuse multi-scale features.\nOur work is closely related to both kernel-based and dynamic sampling-based methods. While previous studies empirically ob-serve the problem in standard feature fusion, they lack clear definitions supported by quantitative measurements. In contrast, we clearly identify and define the issues of intra-category in-consistency and boundary displacement, measuring them through feature similarity analysis. The proposed FreqFusion effectively addresses these issues with the goal of achieving simultaneous feature consistency and boundary sharpness.\n2.3 Frequency Domain Learning.\nFrequency-domain analysis, as a fundamental tool [74], [75], has long been proven to be an effective tool for traditional signal processing. Recently, a series of works have introduced frequency analysis to deep learning.\nIn this context, they are employed to examine the optimization strategies [76] and generalization capabilities [77] of Deep Neural Networks (DNNs). Rahaman et al. [78] and Xu et al. [79] find the effective target function for a deeper hidden layer bias towards lower frequencies during training, thus these networks prioritize learning the low-frequency modes, this phenomenon is called spectral bias/frequency principle. Zhang et al. [80] investigate how frequency aliasing impacts the shift-invariance of modern mod-els, and subsequently, AdaBlur [26] applied content-aware low-pass filters during downsampling for anti-aliasing. Additionally, FLC [81] also demonstrated that frequency aliasing degrades the robustness of models. Qin et al. [82] and Magid et al. [83] ex-plore utilizing more frequency components obtained from discrete cosine transform coefficients for channel attention mechanisms. Huang et al. [84] employ the conventional convolution theorem in DNNs, demonstrating that adaptive frequency filters can efficiently serve as global token mixers. A series of frequency-domain techniques have also been seamlessly integrated into DNN archi-tectures, facilitating the learning of non-local features [84]\u2013[88]. Chen et al. [38] demonstrate how the low-pass filter suppress high frequency feature noise caused by noise in the images, effectively addressing the challenges of instance segmentation in low-light conditions. Many works demonstrate adversarial attack can be achieved by manipulate the high frequency components [11], [76], [89] Luo et al. [11] demonstrate that perturbing high frequencies leads to a large reduction in intra-category similarity, thereby degrading feature representations.\nIn this work, we consider intra-category inconsistency as the presence of disturbed high frequency, significantly reducing intra-category similarity, as observed in [11]. Boundary displacement is regarded as a lack of high frequency, as noted in [83]. Thus, we employ adaptive low-pass filters to reduce feature inconsistency and high-pass filters to enhance useful high-frequency details and sharpen boundaries. This demonstrates an innovative application of frequency-domain techniques in addressing intra-category in-consistency and boundary displacement in feature fusion, benefit-ing various fundamental computer vision tasks."}, {"title": "3 FEATURE SIMILARITY ANALYSIS METRICS", "content": "We begin by introducing metrics for feature similarity analysis. These metrics aim to quantify both intra-category inconsistency and boundary displacement issues that emerge during the process of feature fusion. This establishes a solid foundation for develop-ing and analyzing effective feature fusion techniques.\nFeature similarity is widely used for assessing the quality of extracted features [24], [90]\u2013[92]. Typically, features within the same category should show high similarity, ensuring high intra-category similarity. On the other hand, features from different categories should exhibit low similarity, resulting in low inter-category similarity. A large gap between intra-category and inter-category similarities, referred to as similarity margin, is crucial for preventing misclassification.\nTo facilitate a quantitative assessment of intra-category incon-sistency and boundary displacement, as well as to assess the qual-ity of the fused features, we introduce metrics encompassing intra-category similarity, similarity margin, and similarity accuracy. These metrics offer a comprehensive framework for evaluating the discriminative power of extracted feature maps.\nIntra-category & inter-category similarity. Intra-category sim-ilarity is computed by first deriving the category center through the averaging of features within each category. Subsequently, we calculate the cosine similarity between the category center and the feature vector belonging to the same category. This is expressed as:\n$IntraSim(Y_{i,j}=1) = CosSim(Y_{i,j}=1, \\frac{1}{|O_{cls=1}|}\\sum_{i,j\\in O_{cls=1}}Y_{i,j})$.\nHere, we consider binary segmentation that has two categories, $cls = 1$ denotes the ground truth category of feature vector $Y_{i,j}$, $O_{cls=1}$ represents the area belonging to category 1, and CosSim is the cosine similarity. Similarly, the inter-category similarity is calculated using the same method, with the distinction that the category center and feature vector are from different categories.\n$InterSim(Y_{i,j}=1) = CosSim(Y_{i,j}=1, \\frac{1}{|O_{cls=0}|}\\sum_{i,j\\in O_{cls=0}}Y_{i,j})$.\nHere, $O_{cls=0}$ indicates the area that belong to category from $Y_{i,j}$.\nSimilarity margin. Consequently, the similarity margin is deter-mined by subtracting the inter-category similarity from the intra-category similarity\n$SimMargin(Y_{i,j}) = IntraSim(Y_{i,j}) \u2013 InterSim(Y_{i,j})$.\nSimilarity accuracy. To comprehensively assess the risk of mis-classification rate resulting from intra-category inconsistency and"}, {"title": "4 FREQUENCY-AWARE FEATURE FUSION", "content": "In this section, we introduce FreqFusion as shown in Figure 3. It consists of three essential components: the Adaptive Low-Pass Filter (ALPF) generator, the offset generator, and the Adaptive High-Pass Filter (AHPF) generator, as illustrated in Figure 4.\nFreqFusion operates through two primary stages, i.e., initial fusion and final fusion. Prior to the final fusion step, it is necessary to compress and fuse both low-level and high-level features to serve as input for the three generators, ensuring efficiency in the final fusion stage. We first introduce how we enhance initial fu-sion, elucidating its significance within the FreqFusion framework. Subsequently, we provide detailed insights into the functioning of each of the three generators, thereby offering a comprehensive understanding of their roles in the fusion process.\n4.1 Overview of FreqFusion\nWe begin by presenting the widely-used standard feature fusion approach, followed by an overview of the design of FreqFusion.\nStandard feature fusion. Generally, a common way of feature fusion can be formulated as [15], [16], [93]:\n$Y^{l} = FUP(Y^{l+1}) + X^{l},$\nwhere $X^{l} \\in \\mathbb{R}^{C\\times2H\\times2W}, Y^{l+1} \\in \\mathbb{R}^{C\\times H\\times W}$ represent the l-th features generated by the backbone and the fused feature at the l-th level, respectively. We assume they have the same number of channels; if not, a simple projection function like a 1 \u00d7 1 convolution can ensure this [16], which we omit for brevity. The term FUP denotes upsampling, for example, 2\u00d7 nearest neighbor or bilinear interpolation [16], [17].\nAlthough widely used, this straightforward approach to feature fusion manifests two issues that detrimentally impact dense pre-diction, i.e., intra-category inconsistency and boundary displace-ment. Standard fusion falls short in rectifying these inconsistent features, and simple interpolation within it may even worsen the problem by upscaling a single inconsistent feature to multiple inconsistent pixels. Furthermore, as observed in various prior works [2], [22], [65], outputs from simple interpolation often lean towards excessive smoothness, resulting in boundary displace-ment. Additionally, the detailed boundary information in lower-level features is not fully utilized.\nDesign of FreqFusion. As shown in Figure 3, the proposed FreqFusion can be formally written as:\n$\\begin{aligned}\n &\\hat{Y}^{l+1}_{i,j} = FUP(F_{LP}(Y^{l+1}_{i,j})), \\tilde{X}^{l}_{i,j} = F_{HP}(X^{l}_{i,j}) + X^{l}_{i,j}.\\\\ &Y^{l}_{i,j} = \\hat{Y}^{l+1}_{i+u,j+v} + \\tilde{X}^{l}_{i,j}.\n\\end{aligned}$\nwhere $F_{LP}$ denotes the low-pass filters predicted by the ALPF generator, $(u, v)$ indicates the offset values predicted by the offset generator for the feature coordinates at $(i, j)$, and $F_{HP}$ represents the high-pass filters predicted by the AHPF generator, respectively. They address category inconsistency and boundary displacement by adaptively smoothing the high-level feature with spatial-variant low-pass filters, resampling nearby category-consistent features to replace inconsistent features in the high-level feature, and enhanc-ing the high-frequency boundary details of lower-level features.\nTo efficiently generate the low-pass filters $F_{LP}$, offset values $(u, v)$, and high-pass filter $F_{HP}$, it is necessary to first compress $X^{l}$ and $Y^{l+1}$ and fuse them for input into the three generators, a process we refer to as initial fusion. A simple initial fusion [19], [23], [68] can be formally expressed as:\n$Z^{l} = FUP(Conv_{1\\times1}(Y^{l+1}))) + Conv_{1\\times1}(X^{l}).$\nwhere $Z^{l} \\in \\mathbb{R}^{C/r\\times2H\\times2W}$ indicates the fused compressed feature, and the $r$ is the channel reduction rate for reducing the following computational cost of three generators. The 1 \u00d71 convolutional layer is utilized for channel compression. Next, we proceed to explain how we enhance the initial fusion, followed by describing the details of the three generators\n4.2 Enhancing Initial Fusion\nThe three generators rely on the initially fused compressed feature $Z^{l}$ to predict adaptive filters and resampling offsets. However, the simple initial fusion presented in Equation (6) exhibits two sub-optimal aspects, which can adversely affect the subsequent three"}, {"title": "4.3 Adaptive Low-Pass Filter Generator", "content": "The Adaptive Low-Pass Filter (ALPF) generator is designed to predict dynamic low-pass filters, aiming to effectively smooth high-level features to mitigate feature inconsistency [11] and sub-sequently upsample the high-level feature. To achieve high-quality adaptive low-pass filters, it is crucial to leverage the advantages of both high-level and low-level features [23]. Thus, the ALPF generator takes the initially fused $Z^{l}$ as input and predicts spatial-variant low-pass filters. It comprises a 3 \u00d7 3 convolutional layer followed by a softmax layer, which is represented as:\n$V^{l} = Conv_{3\\times3}(Z^{l}),$\n$\\begin{aligned}\n\\hat{W}^{l}_{p,q} = Softmax(\\bar{V}^{l}_{p,q}) = \\frac{exp(\\bar{V}^{l}_{p,q})}{\\sum_{p,q \\in \\Omega} exp(\\bar{V}^{l}_{p,q})}\n\\end{aligned}$\nwhere $\\bar{V}^{l} \\in \\mathbb{R}^{K^{2}\\times2H\\times2W}$ represents spatially-variant filter weights, where K indicates the kernel size of the low-pass filter. After reshaping, $\\hat{W}^{l}$ contains K \u00d7 K filters for each position. Here, $\\Omega$ denotes a size of K \u00d7 K. Upon passing through a kernel-wise softmax to constrain the filters to be all positive and sum to one, the results are smooth and low-pass filters in $\\hat{W}^{l} \\in \\mathbb{R}^{K^{2}\\times2H\\times2W}$ [26].\nNext, we upscale $Y^{l+1} \\in \\mathbb{R}^{C\\times H\\times W}$ using a sub-pixel upsampling technique [62]. Specifically, we reshape $\\hat{W}^{l}$ in a pixel unshuffle way [62], reducing the height and width by half and expanding the channel by 4\u00d7. We then divide the channels into 4 groups, with each group having a spatially-variant low-pass filter denoted as $\\hat{W}^{l,g} \\in \\mathbb{R}^{K^{2}\\times H\\times W}$, where $g \\in \\{1,2,3,4\\}$ indicates the group. Consequently, we obtain 4 groups of low-pass filtered features, represented as $\\hat{Y}^{l+1,g} \\in \\mathbb{R}^{C\\times H\\times W}$, which are then rearranged to form a 2\u00d7 upsampled feature $\\hat{Y}^{l+1} \\in \\mathbb{R}^{C\\times2H\\times2W}$ as:\n$\\hat{Y}^{l+1,g}_{i,j} = \\sum_{p,q \\in \\Omega} \\hat{W}^{l,g}_{p,q} Y^{l+1}_{i+p,j+q},\\\\ \\hat{Y}^{l+1} = PixelShuffle(\\hat{Y}^{l+1,1}, \\hat{Y}^{l+1,2}, \\hat{Y}^{l+1,3}, \\hat{Y}^{l+1,4})$.\nAs illustrated in Figure 6, the ALPF generator adaptively predicts spatially variant low-pass filters based on the feature content to smooth and enhance feature consistency. To provide deeper insights, visualized results are presented in Figure 7. The findings depicted in Figure 7(a) reveal that the commonly used bilinear upsampled feature in standard feature fusion exhibits sig-nificant intra-category inconsistency and boundary displacement. For instance, the car's interior shows low intra-category similarity, and the boundary appears blurred, indicating severe displacement.\nIn contrast, Figure 7(b) demonstrates improved features char-acterized by enhanced interior consistency, which can be attributed to the introduction of the ALPF generator. This component ef-fectively mitigates intra-category inconsistency, resulting in more cohesive features. Additionally, there is a noticeable improvement in boundary sharpness.\nQuantitative analysis, as presented in Table 1, corroborates these observations. Standard feature fusion techniques exhibit relatively low intra-category similarity, similarity margin, and similarity accuracy, thus increasing the risk of misclassification. However, the incorporation of the ALPF generator within the FreqFusion framework yields notable improvements. Specifically, there is a substantial increase in overall intra-category similarity (0.727\u21920.799), similarity margin (0.245\u21920.297), and similarity accuracy (0.918\u21920.941). In summary, the ALPF generator plays a pivotal role in enhancing feature consistency, thereby bolstering the effectiveness of the FreqFusion approach."}, {"title": "4.4 Offset Generator", "content": "While the ALPF generator smooths features to enhance overall intra-category similarity, it may fall short in rectifying extensive areas of inconsistent features or refining thin and boundary areas. Increasing the size of low-pass filters proves beneficial for ad-dressing large inconsistent regions but can adversely impact thin and boundary areas. Conversely, reducing the filter size aids in preserving thin and boundary areas but may hinder the correction of extensive areas with inconsistent features.\nTo address this dilemma, we propose the offset generator. Motivated by the observation that neighboring features with low intra-category similarity often exhibit features with high intra-category similarity. The offset generator begins the process by computing local cosine similarity:\n$\\begin{aligned}\nS^{p,q}_{i,j} = \\frac{\\sum_{c=1}^{C}Z^{l}_{i,j}Z^{l}_{i+p,j+q}}{\\sqrt{\\sum_{c=1}^{C}(Z^{l}_{i,j})^{2}}\\sqrt{\\sum_{c=1}^{C}(Z^{l}_{i+p,j+q})^{2}}}\n\\end{aligned}$\nwhere $S \\in \\mathbb{R}^{8\\times H\\times W}$ contain the cosine similarity between each pixel and its 8 neighbor pixels, which encourage the offset generator to sample towards features with high intra-category similarity, thereby reducing the ambiguity in boundary or intra-category inconsistent areas, as depicted in Figures 8 and 9.\nSpecifically, the offset generator takes the $Z^{l}$ and $S$ as input and predicts offsets. It consists of two 3 \u00d7 3 convolutional layers to predict the offset direction and offset scale, represented as:\n$O' = D^{l} \\cdot A^{l},$\n$D^{l} = Conv3\\times 3(Concat(Z^{l}, S^{l})),$\n$A^{l} = Sigmoid(Conv3 \\times 3(Concat(Z^{l}, S^{l}))),$\nwhere $D^{l} \\in \\mathbb{R}^{2G\\times H\\times W}$ represents the direction of offsets, $A^{l} \\in \\mathbb{R}^{2G\\times H\\times W}$ aims to control the magnitude of offsets, and $O \\in \\mathbb{R}^{2G\\times H\\times W}$ is the final predicted offsets for each pixel of the high-level feature. G denotes the number of offset groups; we strategically divide the feature into distinct groups, assigning unique spatial offsets for a more granular resampling. This approach allows for resampling features with high intra-category similarity to replace features with low intra-category similarity. In this way, the offset generator can address large areas of inconsistent features and refine the boundary.\nAs shown in Figure 9, at the inner boundaries of buses and cars, our offset generator strategically directs offsets toward interior locations where features exhibit higher consistency and clarity. Conversely, at the outer boundaries, we observe the offsets being strategically directed in the opposite direction, enriching the boundary regions with enhanced clarity. This intentional divergence in offset direction serves to accentuate the object boundaries. Consequently, as shown in Figure 7(c), the offset gen-erator contributes to achieving more consistent features and more accurate boundary delineations. Quantitative analysis in Table 1 reveals that it enhances intra-category similarity (0.760\u21920.799) and enhances similarity accuracy both overall (0.925\u21920.941) and at the boundary (0.720\u21920.728). This suggests that the offset gen-erator provides benefits in addressing intra-category inconsistency and boundary displacement issues."}, {"title": "4.5 Adaptive High-Pass Filter Generator", "content": "Although the ALPF generator and offset generator effectively recover upsampled high-level features with high intra-class con-sistency and refined boundaries", "28": [29], "as": "n$X_{F"}, "u, v) = \\frac{1}{HW}\\sum^{H-1}_{h=0}\\sum^{W-1}_{w=0} X(h, w)e^{-2\\pi j(\\frac{uh}{H}+\\frac{vw}{W})},$\nwhere $X_{F} \\in \\mathbb{R}^{C\\times H\\times W}$ represents the output array of complex numbers from the DFT. H and W denote its height and width. h, w indicate the coordinates of feature map X. The normalized frequencies in the height and width dimensions are given by |u| and |v|. Consequently, the set of high frequencies larger than the Nyquist frequency $H_{+} = \\{(u,v) | |\\frac{u}{H}| > \\frac{1}{2} or |\\frac{v}{W}| > \\frac{1}{2} \\}$ is aliased and permanently lost in downsampled high-level features.\nTo address this limitation, we employ the AHPF generator to enhance detailed boundary information lost during downsampling. Specifically, the AHPF generator takes the initially fused $Z^{l}$ as input and predicts spatial-variant high-pass filters. It consists of a 3 \u00d7 3 convolutional layer followed by a softmax layer and a filter inversion operation, represented as:\n$V^{l} = Conv_{3\\times3}(Z^{l}),$\n$\\begin{aligned}\n\\hat{W}^{l}_{p,q} = E - Softmax(\\bar{V}^{l}_{p,q}) = E - \\frac{exp(\\bar{V}^{l}_{p,q})}{\\sum_{p,q \\in \\Omega} exp(\\bar{V}^{l}_{p,q})}\n\\end{aligned}$\nwhere $\\hat{U}^{l} \\in \\mathbb{R}^{K^{2}\\times H\\times W}$ contains initial kernels at each location (i, j). K indicates the kernel size of high-pass filters. To ensure the final generated kernels $\\hat{W}^{l}$ are high-pass, we follow [83"], "as": "n$\\tilde{X}^{l}_{i,j} = X^{l}_{i,j} + \\sum_{p,q \\in \\Omega} \\hat{W}^{l}_{p,q} X^{l}_{i+p,j+q}$.\nIn Figure 10, the effectiveness of the AHPF generator in en-hancing detailed boundary information is evident. For instance, the original feature lacks clarity in delineating the outline of the bus and the details of a person's head. However, with the incorporation of the AHPF generator, these boundary details"}