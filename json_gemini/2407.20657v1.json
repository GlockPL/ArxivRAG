{"title": "Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks", "authors": ["Hunmin Yang", "Jongoh Jeong", "Kuk-Jin Yoon"], "abstract": "Recent vision-language foundation models, such as CLIP, have demonstrated superior capabilities in learning representations that can be transferable across diverse range of downstream tasks and domains. With the emergence of such powerful models, it has become crucial to effectively leverage their capabilities in tackling challenging vision tasks. On the other hand, only a few works have focused on devising adversarial examples that transfer well to both unknown domains and model architectures. In this paper, we propose a novel transfer attack method called PDCL-Attack, which leverages the CLIP model to enhance the transferability of adversarial perturbations generated by a generative model-based attack framework. Specifically, we formulate an effective prompt-driven feature guidance by harnessing the semantic representation power of text, particularly from the ground-truth class labels of input images. To the best of our knowledge, we are the first to introduce prompt learning to enhance the transferable generative attacks. Extensive experiments conducted across various cross-domain and cross-model settings empirically validate our approach, demonstrating its superiority over state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Regarding the training of deep neural networks, it is typically assumed that the training and testing data are independent and identically distributed. This common principle may impair the generalization of trained models in situations involving domain shifts, which are frequently encountered in real-world deployments. To address this issue, numerous approaches have been proposed in the fields of domain adaptation and generalization. They fundamentally share a common objective: to extract domain-invariant semantic features that prove to be effective for target domains and downstream tasks.\nAnother related but distinct area of research delves into the transferability aspect of adversarial examples. While these examples inherently possess somewhat transferable characteristics, na\u00efvely crafted adversarial examples are often"}, {"title": "2 Related Work", "content": "Generative model-based attack. Generative attacks leverage adversarial training, using a pre-trained surrogate model as a discriminator to generate adversarial perturbations effective across entire data distributions. This approach is computationally efficient and advantageous, as it can simultaneously generate diverse forms of perturbations across multiple images. CDA seeks to enhance the training of the generator by incorporating both the cross-entropy (CE) and the relativistic CE loss. The work initially introduced domain-agnostic perturbations as well as model-agnostic ones. LTP and BIA utilize features extracted from the mid-level layers of the surrogate model, which have been examined to contain a higher degree of shared information among various model architectures. FACL-Attack exploits frequency domain manipulations to boost the attack transferability. Several studies have utilized vision-language models and object-centric features to acquire effective features for attacking multi-object scenes. Our work further enhances the training by employing a vision-language model and prompt learning.\nVision-language foundation model. Recent advancements in large vision-language models (VLM) have demonstrated superior capabilities in learning generic representations. This significant progress has been made possible by leveraging enormous web-scale training datasets. In particular, Contrastive Language Image Pre-training (CLIP) employs 400 million image-text pairs for contrastive pre-training of image and text encoders within a joint vision-language embedding space. The zero-shot CLIP model of ViT-L/14 exhibits impressive image classification performance, achieving a top-1 accuracy of 76.2% on ImageNet-1K, on par with that of a fully-supervised ResNet101. CLIP demonstrates superior domain generalization capability compared to supervised trained models. For ImageNet-Sketch, zero-shot CLIP achieves 60.2%, while the fully-supervised ResNet101 only attains 25.2%. Furthermore, context optimization has further improved the few-shot performance of the employed CLIP model. Interestingly, this trained context enhances CLIP's robustness to distribution shifts, as highlighted in [62]. Inspired by this line of research, our work incorporates CLIP and prompt learning into the generative attack framework to enable more effective transfer attacks.\nText-driven feature guidance. As both image and text features are mapped into a joint vision-language embedding space as in CLIP [41], various vision tasks focused on generalization or adaptation have benefited from text-driven feature guidance. LADS introduced a method for extending the trained model to new domains based solely on language descriptions of distribution shifts. PODA introduced a straightforward feature augmentation method for zero-shot domain adaptation, guided by a single textual description of the target domain. Prompt-Styler proposed a text-driven style generation method to enhance the domain generalization performance. From the perspective of utilizing generic text features to guide model training, our work aligns with these efforts. We aim to explore an effective text-driven feature contrastive method to enhance the training of the perturbation generator towards a more robust regime."}, {"title": "3 Proposed Approach: PDCL-Attack", "content": "Problem definition. Transfer attacks aim to craft adversarial examples which are transferable in black-box settings. This task aims to create adversarial examples that are not just effective against a single surrogate model but also capable of deceiving other victim models, even those with different architectures or trained on different data distributions. To that end, crafting transferable adversarial examples involves identifying vulnerabilities that are shared among diverse models and domains, effectively addressing the challenge of generalization. Our goal is to devise an effective transfer attack method that can train a robust perturbation generator model, denoted as $G_{\\theta}(\\cdot)$, to generate adversarial examples which are transferable to arbitrary domains and model architectures."}, {"title": "3.1 Phase 1: Prompt Context Training", "content": "The objective of Phase 1 is to train learnable context words used in Prompter($\\cdot$), facilitating the extraction of generalizable text features in Phase 2. To fully harness the capabilities of CLIP [41] model, recent studies have shown the effectiveness of leveraging context optimization. These methods have demonstrated enhanced performance of the CLIP model across various distribution-shifting scenarios. We adhere to the recent protocol outlined in [61,62] and integrate it to enhance our attack framework. We utilize the same training dataset (i.e., ImageNet-1K [43] which contains diverse distribution shifts) as in Phase 2.\nOptimal context training. We model the context words used in Prompter(.) via learnable continuous vectors $[V_1][V_2]\u2026 [V_M]$ as follows:\nPrompter($\\cdot$) = $[V_1][V_2]\u2026 [V_M][\\cdot ]$, (1)\nwhere each $[V_m]$ ($m \\in \\{1, ..., M\\}$) is a trainable 512-dimensional float vector with the same word embedding size of CLIP [41], and $M$ is the number of context words. The training is exclusively focused on these word vectors, whereas the weights of the CLIP image encoder CLIPimg($\\cdot$) and text encoder CLIPtxt($\\cdot$) remain unchanged. The training objective is to optimize $[V_1][V_2]\u2026[V_M]$ by minimizing the cross-entropy loss computed using pairs of input images and GT class labels through few-shot learning. When the input images $x_s$ are passed through CLIPimg($\\cdot$), the produced image features can be represented as $\\phi_s$. With $K$ class labels, we can obtain text features $\\{T_i\\}_{i=1}^K$ by feeding the class labels into CLIPtxt(). Then, the prediction probability can be computed as:\n$p(y = i|x_s) = \\frac{exp(cos(T_i, \\phi_s)/\\lambda)}{\\sum_{j=1}^K exp(cos(T_j, \\phi_s)/\\lambda)}$, (2)\nwhere $\\lambda$ is the temperature parameter learned by CLIP [41], and cos($\\cdot$,$\\cdot$) denotes the standard cosine similarity."}, {"title": "3.2 Phase 2: Perturbation Generator Training", "content": "This phase aims to train a perturbation generator model $G_{\\theta}(\\cdot)$ capable of crafting transferable perturbations as described in Algorithm 1. We randomly initialize the generator $G_{\\theta}(\\cdot)$. Given a batch of clean images $x_s$ with a batch size of $n$, $G_{\\theta}(\\cdot)$ generates unbounded adversarial examples. Subsequently, these are constrained by the perturbation projector $P(\\cdot)$ within a predefined budget of $e$.\nSurrogate model loss $L_{surr}$. The generated adversarial images $x_s'$ and clean images $x_s$ are passed through the surrogate model $f_k(\\cdot)$, where we extract k-th mid-layer features. We define the surrogate model loss $L_{surr}$ as follows:\n$L_{surr} = cos(f_k(x_s'), f_k(x_s))$, (4)\nwhere cos($\\cdot$,$\\cdot$) denotes the standard cosine similarity."}, {"title": "Prompt-driven contrastive loss $L_{PDCL}$", "content": "As CLIP [41] employs contrastive learning to learn representations in a joint vision-language embedding space, we design a contrastive learning based attack loss for effectively harnessing CLIP features. Our loss design differs from GAMA since we separate heterogeneous surrogate and CLIP features, and utilizes representative text features extracted from ground-truth (GT) class labels of input images. Additionally, our method leverages CLIP's perturbed image features, which can provide effective loss gradients. Given clean images $x_s$ of batch size $n$, we first feed $x_s$ into the CLIP image encoder CLIPimg($\\cdot$) for acquiring clean image features $\\phi_s$. We input $x_s$ into $G_{\\theta}(\\cdot)$ followed by $P(\\cdot)$ and craft adversarial images by $x_s' = P(G_{\\theta}(x_s))$. We then pass $x_s'$ through CLIPimg($\\cdot$), and CLIP's adversarial image features $\\phi_s'$ can be obtained as follows:\n$\\phi_s' = CLIPimg(x_s')$. (5)\nSince we utilize a large-scale ImageNet-1K [43] dataset for training, we have corresponding GT class labels $y_s$ for each $x_s$. If there are $K$ classes in the training dataset (e.g., $K = 1,000$ for ImageNet-1K), we can obtain the corresponding set of $K$ text features using the CLIP text encoder CLIPtxt($\\cdot$). In contrast to GAMA [2], we pass $y_s$ as input to the pre-trained Prompter($\\cdot$), which yields more effective text-driven prompt inputs for CLIPtxt(). Then, text features $T_s$ extracted from the GT class labels of input images can be computed as follows:\n$T_s = CLIPtxt(Prompter(y_s))$. (6)\nWith adversarial image features $\\phi_s'$ as the anchor point, we employ adversarial text features $\\tau_s'$ as the positives, which are computed by identifying the least similar text features compared to the input image features $\\phi_s$. Specifically, we compute a set of text features denoted as $T = [T_1, T_2,\u2026, T_K]$ using the class names of all $K$ classes, where each $T_i$ corresponds to CLIPtxt(Prompter($y_i$)). For each batch, we randomly select $n = 16$ candidates from $T$ for computational efficiency and identify the least similar candidate, using its label as $y_s'$. We define the adversarial text features $\\tau_s'$ as follows:\n$\\tau_s' = CLIP txt (Prompter(y_s'))$. (7)\nWe use $\\phi_s'$ from Eq. (5) as the anchor, $T_s$ from Eq. (6) as the negatives, and $\\tau_s'$ from Eq. (7) as the positives to constitute our contrastive loss. Finally, our prompt-driven contrastive loss can be formulated as follows:\n$L_{PDCL} = ||\\phi_s' - \\tau_s'||_2^2 + max(0, \\alpha - ||\\phi_s' - T_s||_2)^2$, (8)\nwhere $\\alpha$ denotes the desired margin between $\\phi_s'$ and $T_s$. All the features are $l_2$-normalized before the loss calculation. As a result, $L_{PDCL}$ facilitates more robust training of the generator $G_{\\theta}(\\cdot)$ by leveraging the prototypical semantic characteristics of text-driven features, boosted by the frozen robust Prompter($\\cdot$).\nTotal loss $L$. Our generator $G_{\\theta}(\\cdot)$ is trained by minimizing $L$ as follows:\n$L = L_{surr} + L_{PDCL}$, (9)"}, {"title": "3.3 Phase 3: Inference using the Frozen Generator", "content": "Since our perturbation generator $G_{\\theta}(\\cdot)$ has been trained robustly, we freeze it for the final inference stage of crafting adversarial examples. With the frozen $G_{\\theta}(\\cdot)$, we can generate adversarial examples by applying it to arbitrary image inputs, even if they belong to target data distributions different from the source domain. We assess the performance of the trained $G_{\\theta}(\\cdot)$ in black-box scenarios across diverse domains and model architectures. The generated adversarial examples are expected to exhibit superior cross-domain and cross-model transferability."}, {"title": "4 Experiments", "content": "4.1 Experimental Setting\nDatasets and attack scenarios. Since our goal is to generate adversarial examples which show high transferability across various domains and models, we carry out experiments in challenging black-box scenarios, namely cross-domain and cross-model settings. Building upon a recent work that demonstrated remarkable attack transferability by employing a large-scale training dataset (i.e., ImageNet-1K ), we also leverage ImageNet-1K to train our perturbation generator and prompter. The efficacy of the trained generator is assessed by conducting evaluations on three additional datasets: CUB-200-2011 , Stanford Cars , and FGVC Aircraft . Specifically for the cross-domain setting, we evaluate our method on unknown target domains (i.e., CUB-201-2011, Stanford Cars, FGVC Aircraft) and victim models distinct from both the source domain (i.e., ImageNet-1K) and the surrogate model. For the cross-model setting, we evaluate our method against black-box models with varying architectures, maintaining a white-box domain setup using ImageNet-1K.\nSurrogate and victim models. The perturbation generator is trained on ImageNet-1K against a pre-trained surrogate model of VGG-16 [44]. For the cross-domain setting, we employ fine-grained classification victim models trained by DCL framework. These models are based on three different backbones: ResNet50 (Res-50) , SENet154, and SE-ResNet101 (SE-Res101) . For the cross-model setting, we explore various different model architectures such as Res-50, ResNet152 (Res-152) , DenseNet121 (Dense-121), DenseNet169 (Dense-169) , Inception-v3 (Inc-v3) , MNasNet , and ViT . \nImplementation details. We adhere closely to the implementations employed in recent generative model-based attacks to ensure a fair comparison. The mid-layer from which we extract features from the surrogate model (i.e., VGG-16 ) is \u041c\u0430xpool.3. For the CLIP model, we use ViT-B/16 for the image encoder and Transformer [48] for the text encoder, consistent with the configurations used in GAMA . We train the perturbation generator using"}, {"title": "4.2 Main Results", "content": "Cross-domain evaluation results. We compare our method with the state-of-the-art generative model-based attacks on various black-box domains with black-box models. In the training stage, we utilize ImageNet-1K [43] as the source domain to train a perturbation generator model against a pre-trained surrogate model of VGG-16 [44]. In the inference stage, we evaluate the trained perturbation generator on various unknown target domains, namely CUB-200-2011 , Stanford Cars , and FGVC Aircraft [33], using different victim model architectures. Specifically, we employ several fine-grained classification models that have been trained using the DCL framework . These victim models are based on three different backbones: Res-50 , SENet154 and SE-ResNet101 (SE-Res101) . We assess the effectiveness of our trained perturbation generator in this challenging black-box attack scenario.\nAs shown in Table 2, our method exhibits superior attack effectiveness with significant margins on most cross-domain benchmarks, which are also cross-model. This highlights the robust and potent transferability of our crafted adversarial examples, enabled by prompt-driven feature guidance and prompt learning to maximize its effectiveness. We conjecture that the remarkable generalization ability of PDCL-Attack might be attributed to the synergy between our two proposed methods: harnessing the superior representation power of CLIP's text features and improving it further with a prompt learning method to produce generalizable text features. In essence, our approach indeed enhances the perturbation generator's capability to generalize across various black-box domains and state-of-the-art model architectures.\nCross-model evaluation results. While our method demonstrates the effectiveness in enhancing the attack transferability in the strict black-box scenario as shown in Table 2, we further conducted investigations in a controlled"}, {"title": "5 Limitations and Broader Impacts", "content": "Limitations. Since our method employs a vision-language foundation model, it entails several limitations. Firstly, the attack effectiveness depends on the quality of the pre-trained vision-language model. This could be alleviated with the"}, {"title": "6 Conclusion", "content": "In this paper, we have introduced a novel generator-based transfer attack method that harnesses the capabilities of a vision-language foundation model and prompt learning. We design an effective attack loss by incorporating image and text features extracted from the CLIP model and integrating these with a surrogate model-based loss. We find that text feature guidance from ground-truth labels and adversarial image-driven loss gradients enhance the training. Extensive evaluation results validate the effectiveness of our approach in black-box scenarios with unknown distribution shifts and variations in model architectures."}]}