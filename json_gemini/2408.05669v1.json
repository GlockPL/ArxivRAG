{"title": "Stealth Diffusion: Towards Evading Diffusion Forensic Detection through Diffusion Model", "authors": ["Ziyin Zhou", "Ke Sun", "Zhongxi Chen", "Huafeng Kuang", "Xiaoshuai Sun", "Rongrong Ji"], "abstract": "The rapid progress in generative models has given rise to the critical task of AI-Generated Content Stealth (AIGC-S), which aims to create AI-generated images that can evade both forensic detectors and human inspection. This task is crucial for understanding the vulnerabilities of existing detection methods and developing more robust techniques. However, current adversarial attacks often introduce visible noise, have poor transferability, and fail to address spectral differences between Al-generated and genuine images. To address this, we propose StealthDiffusion, a framework based on stable diffusion that modifies AI-generated images into high-quality, imperceptible adversarial examples capable of evading state-of-the-art forensic detectors. StealthDiffusion comprises two main components: Latent Adversarial Optimization, which generates adversarial perturbations in the latent space of stable diffusion, and Control-VAE, a module that reduces spectral differences between the generated adversarial images and genuine images without affecting the original diffusion model's generation process. Extensive experiments show that StealthDiffusion is effective in both white-box and black-box settings, transforming Al-generated images into high-quality adversarial forgeries with frequency spectra similar to genuine images. These forgeries are classified as genuine by advanced forensic classifiers and are difficult for humans to distinguish.", "sections": [{"title": "1 Introduction", "content": "In recent years, generative models, particularly diffusion-based image synthesis techniques [19], have made significant progress in deep learning and excelled at generating highly realistic images. As these generative technologies become increasingly widespread, it is crucial to develop techniques that can create Al-generated images indistinguishable from genuine ones by both human eyes and forensic detectors. This will help identify the limitations and weaknesses of current detection methods and contribute to the development of more robust detection models. We refer to this as the AI-Generated Content Stealth (AIGC-S) task, which aims to generate Al-generated images that can evade detection by both human perception and AI-based algorithms. The goal of this task is to apply carefully crafted perturbations to existing AI-generated images, making them indistinguishable from genuine images while maintaining their visual quality."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 AI-Generated Content Stealth", "content": "The goal of AI-Generated Content Stealth (AIGC-S) task is to transform Al-generated images into forms that can evade detection algorithms without introducing visible adversarial noise. Using traditional adversarial algorithms capable of generating adversarial perturbations can misleading the target model [8, 16, 34]. However, these adversarial noises do not meet our stealth criteria.\nWith the advent of diffusion methods, new adversarial attack techniques have been developed that use diffusion models to create more natural-looking perturbations than traditional gradient-based methods [4, 56]. Chen et al. [4] manipulate the latent space of diffusion models with semantic labels to produce adversarial examples targeting the Imagenet database [9]. Similarly, Xue et al. [56] employ a method that iteratively adds adversarial perturbations, reconstructing them through a diffusion model at each step to create more realistic adversarial images. However, since diffusion is an Al-generated method, it might increase the chance of these images being detected by forensics detectors."}, {"title": "2.2 Image Forensics Detection", "content": "Image Forensics Detection has gained considerable attention from researchers in order to prevent the misuse of AI-generated images. Some works [5, 24, 31, 32, 35, 36, 44-48, 61] focus on detecting Deep-fake images. Zhuang et al. [61] uses Vision Transformers to detect forged regions without pixel-level annotations. Miao et al. [36] introduces a Hierarchical Frequency-assisted Interactive Network to explore frequency-related forgery cues. Miao et al. [35] presents a High-Frequency Fine-Grained Transformer network with Central Difference Attention and High-frequency Wavelet Sampler to capture fine-grained forgery traces in spatial and frequency domains. Wang et al. [52] addressed it as a binary classification problem, training deep learning networks with fake images generated by [23] and genuine images from the LSUN dataset [57]. Recent approaches, such as [14, 49], have focused on improving detection's generalization and accuracy by extracting features from images instead of using the images themselves as the training dataset. Tan et al. [49] employed a GAN-based discriminator to convert images into gradient maps for detection. Specialized methods also exist for detecting GAN-generated fake faces [2, 33]. These studies demonstrate the effectiveness of simple supervised image forensics classifiers in detecting GAN-generated images. However, as diffusion-based generation techniques continue to advance, previous GAN-based detection methods can not adequately generalize to diffusion images. Consequently, there is growing research interest in detecting generated images produced by diffusion [39, 53], which has shown promising results in effective detection."}, {"title": "3 Methodology", "content": "This section presents the overall framework of our proposed Stealth-Diffusion, as shown in Fig. 2. The workflow begins by applying a Projected Gradient Descent (PGD) adversarial attack to an input image, followed by processing through a Variational Autoencoder (VAE) to extract its latent representation. Within the diffusion framework, the latent image is refined through noise addition and strategic denoising using a UNet. An adversarial loss function optimizes these features to evade detection by a surrogate classifier. To minimize spectral artifacts and reduce detectability, a Control-VAE module, trained to align the spectral frequency of the reconstructed image with genuine images, is integrated during the decoding phase via skip connections."}, {"title": "3.1 Preliminaries", "content": "Our optimization framework commences with a preprocessing phase designed to streamline the optimization challenges encountered in subsequent stages. This is particularly crucial for operations within the latent space during the stable diffusion process, which may amplify the likelihood of generated image detection. Drawing inspiration from [20], we implement the Projected Gradient Descent (PGD) technique to inject nuanced and effective adversarial perturbations into the initially generated images $x_0$. This strategic enhancement bolsters the images' ability to evade detection in the later stages of our framework. The application of these perturbations is guided by a surrogate forensic model, which is explicated in the equation that follows:\n$X_{t+1} = Clip(x_t + \\eta sign(\\nabla_x L(S(x_t), Y_{true})))$, (1)\nwhere $x_0$ is the initial adversarial image and $x_{t+1}$ represents its evolution after iteration t. The clipping function Clip ensures that the perturbations do not exceed the imperceptibility threshold determined by $\\epsilon$. $\\nabla_x L$ signifies the gradient of the loss function L, considering the true label $y_{true}$ and the surrogate forensic classifier S. This PGD preprocessing not only primes the images for robustness but also reduces the complexity of subsequent optimization within the diffusion process, thereby enhancing the model's ability to evade forensic detection with greater efficiency."}, {"title": "3.2 Latent Adversarial Optimization", "content": "Building on the robust foundation provided by the preprocessing stage, the adversarially perturbed image $x_{t+1}$ is transformed into a latent representation through the encoding capabilities of a Variational Autoencoder (VAE) encoder, denoted by E. This crucial step compresses the perturbed image into a latent format within a lower-dimensional space, optimally preparing it for the sophisticated denoising and refinement processes of the Denoising Diffusion Probabilistic Models (DDPM). The VAE encoder plays a pivotal role in this phase, distilling the essential features of the image and setting the stage for the complex operations characteristic of the subsequent DDPM-based adversarial optimization.\nWe then proceed to a meticulous adversarial optimization process within the latent space. This phase is vital as it exploits the inherent structural properties of the latent space to enable precise and controlled refinement of the image. Employing the strengths of Stable Diffusion Models, we conduct a series of forward and backward operations that systematically enhance the latent variables. This detailed manipulation allows us to carefully craft the adversarial features into configurations that are more resistant to forensic detection, all while maintaining the image's integrity. Denoising Diffusion Probabilistic Models (DDPM) employ a series of forward and reverse operations to iteratively refine the latent"}, {"title": "3.3 Control-VAE Module", "content": "Motivation Analysis. While Latent Adversarial Optimization enhances the resistance of diffusion-generated images to detection techniques by optimizing latent variables, we identified that this optimization fails to alter the distinctive spectral signatures intrinsic to generated images. However, numerous studies [3, 13, 14, 39, 43, 52, 60] have observed significant differences between the spectral signatures of generated images and those of genuine images. These studies have identified that the spectral discrepancies primarily originate from the high-frequency components. Consequently, some research [6, 7, 41] has employed specifically designed filters to remove the content of generated images, effectively isolating most of the low-frequency components. This process yields the noise residuals of generated images, thereby providing a more intuitive demonstration of the differences in the spectral signatures between generated and genuine images. Eliminating these spectral patterns in forged images has been proven to be an important method to evade detection by recognition models [3, 11, 20, 22, 25, 28, 54]. To address and further analyze these spectral disparities, we investigated the frequency spectra produced by various diffusion methods. Specifically, for three generative methods including ADM [10] for Denoising Diffusion Probabilistic Model (DDPM), BigGAN [1] for Generative Adversarial Network (GAN), Stable_Diffusion versions 1.4 and 1.5 [42] for Latent Diffusion Model (LDM), we selected a random set of one thousand images ${x_i}$. we employ a commonly used denoising network [58] as our filter, which is also the filter utilized in [6, 7], to extract these noise residuals:\n$r_i = x_i - f(x_i)$, (5)\nWe calculated the average Fourier amplitude spectra of these residuals, as visualized in Fig. 3. Our analysis indicated that images generated by the Denoising Diffusion Probabilistic Model (DDPM) tend to display spectra that closely mimic those of genuine images, with minimal detectable flaws. In contrast, the Latent Diffusion Model (LDM) spectra still exhibit a subtle grid-like pattern, characterized by high frequencies that are akin to those observed in GAN-generated images.\nThis phenomenon could be ascribed to the decoder module's repetitive upsampling process in LDM, which inadvertently introduces high-frequency artifacts as a result of spectrum replication [3, 12, 29]. Unlike Latent Diffusion Models (LDM), traditional Denoising Diffusion Probabilistic Models (DDPM) solely utilize a Unet architecture with residual connections and do not employ a Variational Autoencoder (VAE) architecture to embed images into the latent space. Although the Unet architecture includes upsampling mechanisms, the integration of downsampling maps from the encoder with upsampling maps in the decoder through residual connections effectively mitigates artifacts introduced by upsampling."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setups", "content": "Dataset. We evaluated our method on the GenImage dataset [60], which consists of 1.35 million generated images and 1.33 million genuine images from ImageNet [9]. The dataset encompasses sub-datasets generated by seven diffusion methods (ADM [10], Glide [38], Midjourney [37], SDv1.4 & 1.5 [42], VQDM [17], Wukong [55]), and one GAN method (BigGAN [1]). The dataset's large quantity of images and diverse generation methods allow for comprehensive analysis, making it a suitable choice for our experiments.\nSurrogate Forensic Detector. We employed the classification evidence method proposed in [52], using EfficientNet-B0 [50], ResNet-50 [18], DeiT(Base) [51], and Swin-T(Base) [30] as backbone models."}, {"title": "4.2 Attack on Surrogate Forensic Detector", "content": "We evaluated our proposed attack method on four detectors with different backbones under white-box and black-box settings. Due to space constraints, we only present the transfer attack success rates of Swin-T (S) against EfficientNet-B0 (E), ResNet-50 (R), and DeiT (D) in Tab. 1. Our method outperformed all baselines in average attack success rate across all datasets, demonstrating strong transfer attack performance and generalization across different AI-generated methods. Notably, it achieved over 90% success against the commercial AI content generator Midjourney. The complete table is in the supplementary materials."}, {"title": "4.3 Attack on Universal Forensic Detector", "content": "To further demonstrate our method's attack capability, we evaluated the transferability of attacks from the Surrogate Forensic Detector to the Universal Forensic Detector, targeting two forensic classifiers for detecting images generated by Diffusion methods [39, 53]. The"}, {"title": "4.4 Analysis", "content": ""}, {"title": "Image Quality Analysis", "content": "To further demonstrate the image quality of our adversarial generation method, we conducted both qualitative and quantitative analyses of the adversarial samples produced by the baseline attack method and our proposed method. Fig. 5"}, {"title": "Image Spectral Analysis", "content": "To further analyze the spectral characteristics of different attack methods, we conducted both qualitative and quantitative analyses of the spectral properties of adversarial"}, {"title": "4.5 Ablation Study", "content": "In this section, we will conduct a series of ablation experiments on the proposed attack method.\nCore Component Analysis. We only generate attacks using Swin-T [30], with results for other backbones in the supplementary materials. Tab. 5 shows results for different variants of our method. The baseline method excludes Control-VAE and Latent Adversarial Optimization (LAO). Using either Control-VAE or LAO improves ASR metrics for both backbones, and combining them yields even better performance. For example, attacks generated by ResNet50 [18] showed success rate improvements of 42.5%, 11.12%, and 10.57% on EfficientNet-B0 [50], DeiT [51], and Swin-T [30] detection models, respectively. Additionally, our preprocessing stage introduces minimal adversarial noise, enhancing the success rate of transfer attacks. Omitting this stage results in noticeable performance degradation, supporting our rationale for its inclusion.\nEffect of Noise Prototype Loss in Control-VAE. In Tab. 6, we compared the use of NP Loss and non-use of NP Loss in the Control-VAE module in ASR. Adding NPL to the Control-VAE achieves better performance with respect to all the metrics. In Tab. 7, we demonstrate the probability of these reconstructed images being detected as genuine. The results show that using Control-VAE and NPL can minimize the probability of reconstructed genuine images being detected as generated as much as possible."}, {"title": "5 Conclusion", "content": "The paper proposes a framework called StealthDiffusion to enhance the robustness of diffusion model-generated images in forensic detection. StealthDiffusion adds adversarial perturbations on the latent space of stable diffusion to generate high-quality synthetic images that are resistant to detection. To further reduce the spectral differences between genuine and generated images, we introduce the Control-VAE module to improve the effectiveness of the attack. Experimental evaluations on different forensic detectors demonstrate the success and superiority of the proposed attack method compared to baseline methods."}]}