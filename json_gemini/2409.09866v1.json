{"title": "Constructing a Singing Style Caption Dataset", "authors": ["Hyunjong Ok", "Jaeho Lee"], "abstract": "Singing voice synthesis and conversion have emerged as significant subdomains of voice generation, leading to much demands on prompt-conditioned generation. Unlike common voice data, generating a singing voice requires an understanding of various associated vocal and musical characteristics, such as the vocal tone of the singer or emotional expressions. However, existing open-source audio-text datasets for voice generation tend to capture only a very limited range of attributes, often missing musical characteristics of the audio. To fill this gap, we introduce S2Cap, an audio-text pair dataset with a diverse set of attributes. S2Cap consists of pairs of textual prompts and music audio samples with a wide range of vocal and musical attributes, including pitch, volume, tempo, mood, singer's gender and age, and musical genre and emotional expression. Utilizing S2Cap, we suggest an effective novel baseline algorithm for singing style captioning. Singing style captioning is a relative task to voice generation that generates text descriptions of vocal characteristics, which we first suggested. First, to mitigate the misalignment between the audio encoder and the text decoder, we present a novel mechanism called CRESCENDO, which utilizes positive-pair similarity learning to synchronize the embedding spaces of a pretrained audio encoder to get similar embeddings with a text encoder. We additionally supervise the model using the singer's voice, which is demixed by the accompaniment. This supervision allows the model to more accurately capture vocal characteristics, leading to improved singing style captions that better reflect the style of the singer. The dataset and the codes are available at https://github.com/HJ-Ok/S2cap.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in text-to-speech modeling [1]\u2013[3] have led to tremendous success in the task of generating speech conditioned on specific styles [4], specified by the text prompt given to the model [5]\u2013[8]. In response to this progress, the speaking style captioning task has also received much attention [9]\u2013[11], which aims to extract para-/non-linguistic information from speech and represent it as a textual prompt. In a sense, these tasks are dual to each other; generating speech from the given style prompt and generating a prompt that can generate the speech in the desired style.\nIn this work, we consider a dual of the prompt-conditioned singing voice synthesis and conversion, which have recently gained popularity [12]\u2013[14]. Precisely, we propose a novel task of singing style captioning, which generates style de-scriptions from the singing voices; to our knowledge, this is the first specialized attempt for singing voices. Singing style captioning is much more challenging to address than standard speech captioning, as the task requires understanding the vocal and musical characteristics of the given audio, such as the vocal tone of the singer or emotional expressions. However, no speech captioning benchmark dataset exists that contains annotations on such a detailed set of attributes.\nTo fill this gap, we present S2Cap: Singing Style Captioning dataset. S2Cap considers a total of nine attributes, which is much larger than previous speech captioning works, e.g., [14], with only three attributes. Unlike prior works, S2Cap includes vocal and musical attributes, such as tempo, timbre, and genre, which are typically not present in general speech data. This rich set of information enables the trained model to understand detailed and diverse style captions for singing voices.\nBased on S2Cap, we establish a novel and robust base-line for the singing style captioning. Our framework uti-lizes a pretrained audio encoder, text encoder, and text de-coder, together with two novel techniques. The first tech-nique, called CRESCENDO (Connect Representations for Efficient Sequence-Level Captioning with an End-to-End Transformer) aims to resolve the potential misalignment be-tween the independently pretrained audio encoder and text decoder; CRESCENDO matches the representation spaces by performing positive-pair similarity learning between audio and text encoders. The second technique encourages the model to focus on the vocal track of the given audio rather than musical accompaniments for generating captions. This is done by leveraging vocal demixing methods, which may act as an additional means of supervision to the model.\nOur contributions can be summarized as follows:"}, {"title": "II. METHODS", "content": "In this section, we first introduce the data generation pipeline of the S2Cap dataset (Sec.II-A). Then, we give a general overview of the proposed baseline framework for the singing style captioning (Sec.II-B). Finally, we describe the novel techniques used in our framework: CRESCENDO (Sec.II-C) and the demixing supervision (Sec.II-D).\nA. S2Cap\nWe introduce S2Cap, a singing style captioning dataset. Similar to recent works [15], [16], S2Cap is generated using a large language model (LLM) based pipeline based on an existing audio dataset. For S2Cap, we use the Melon playlist dataset [17] as the base dataset.\nThe overall generation pipeline is as follows (Fig 2). We first obtain additional metadata on the audio clips through web scraping. We have collected Last.fm tags and the singer's demographic information, such as age and gender. As the Melon playlist dataset consists of only the low-resolution mel-spectrogram for each music, we have additionally collected the corresponding audio tracks from YouTube. To ensure the quality and relevance of the dataset, we have utilized the song metadata from Last.fm and filtered out audio files with missing entries or mismatched metadata. We have also filtered out the songs that mismatch in duration between song metadata and YouTube videos and from singers born before 1970, as their songs tend to be frequently missing on YouTube.\nFurthermore, we have processed the audio tracks by utiliz-ing the demixing model, called Demucs [18], to separate the vocal parts of each track. On these isolated vocals, we employ a speaker-diarization model to extract 5\u201330 seconds segments. These extracted segment intervals are then spliced from the original audio and kept as input features for the S2Cap dataset.\nFor caption generation, we leverage the pretrained Qwen-2 Audio [19] to generate descriptive text corresponding to the extracted vocal segments such as gender, timber, mood, and tempo. Additionally, we have utilized a spoken language understanding model to extract acoustic features such as pitch and volume. We classify both elements in three categories, \"low,\" \"medium,\" and \"high,\" utilizing the amplitude root mean square for volume and the average Fo for pitch clas-sification. The resulting textual descriptions containing Melon Playlist Dataset metadata, information from web scraping, and results of various models were then input into an LLM to generate final captions that reflect the singer's style. We utilize the Qwen2-72B-Instruct-AWQ [20] as our LLM.\nThe resulting S2Cap dataset consists of 71,215 captions derived from 12,105 music tracks. We have partitioned the dataset into training, development, and test sets, allocating 70%, 10%, and 20% of the samples, respectively. Detailed statistics are shown in Table 2. To avoid a licensing issue, we provide the code and URLs to download the wav files instead"}, {"title": "B. Overall structure", "content": "We propose a simple framework for singing style captioning that combines an AST [21] audio encoder with a BART [22] text decoder (see Fig 3). We have selected to use AST, as it empirically outperforms other pretrained audio encoders; detailed results are given in Sec. IV.\nAST encoder: Consider a mel-spectrogram $X \\in R^{T\\times F}$, with T denoting the temporal frames and F representing the frequency bins. The AST encoder maps this representation into a feature space, which we denote by:\n$Z = E_{AST}(X)$                                                                                                                                 (1)\nThe resulting feature set Z is then sent to the BART decoder.\nBART decoder: Given the feature representation Z, the BART decoder transforms these features into a textual se-quence $Y = [Y_1, Y_2,\\cdots, Y_N]$, denoted by:\n$Y = D_{BART}(Z)$                                                                                                                                (2)\nOptimization criteria: To train the model, we use the standard cross-entropy loss and adopt the teacher-forcing strategy. In other words, the model receives the true output token from the previous time step as an input for the current time rather than its generated output. More concretely, the loss can be written as:\n$\\mathcal{L}_{CE} = - \\sum_{n=1}^{N} log P(y_n|y_1,..., Y_{n-1}, Z)$                                                                                                                                               (3)"}, {"title": "C. CRESCENDO", "content": "While the proposed framework with a pretrained audio encoder and a text decoder is intuitive and powerful, a sig-nificant challenge emerges: embedding misalignment. More specifically, as the BART decoder is trained to be paired with only the BART encoder, a misalignment of the representation space may be present when paired with an audio encoder. To tackle this issue, we propose a sequence-level similarity learning approach to match the embeddings of the AST encoder with the BART encoder, related to works in both image and text domains that do not need negative examples [23]\u2013[25]."}, {"title": "D. Demixing supervision", "content": "Singing style captioning necessitates focusing on the vocal track of a song rather than the musical accompaniment to generate meaningful captions. To address this challenge, we additionally utilize demixing supervision, which leverages the results of vocal demixing algorithms. We implement this supervision by incorporating an auxiliary loss function based on the Kullback-Leibler (KL) divergence between the embeddings of the demixed vocal audio and the original song. Let $E_{AST}(X_v)$ and $E_{AST}(X_o)$ denote the embeddings of the demixed vocal audio and the original song, respectively, obtained from the AST encoder. The KL divergence loss $\\mathcal{L}_{KL}$ is defined as:\n$\\mathcal{L}_{KL} = D_{KL}(E_{AST}(X_v)||E_{AST}(X_o))$,                                                                                                          (7)"}, {"title": "III. EXPERIMENTS", "content": "Evaluation metrics: To evaluate our proposed meth-ods, we employ metrics that are widely used in captioning tasks such as BLEU [30], METEOR [31], ROUGE-L [32], CIDEr [33], SPICE [34], and SPIDEr [35]. BLEU is a mod-ified n-gram precision metric incorporating a brevity penalty, while ROUGE-L calculates an F-measure based on the longest common subsequence. METEOR enhances the evaluation by considering several factors like stem- and word-level overlap and synonymy. CIDEr employs a geometric mean of n-gram and cosine similarity scores. SPICE focuses on semantic content by parsing scene graphs from captions, and SPIDEr is the average score of SPICE and CIDEr. While the above metrics are valuable for assessing captioning systems, inspired by recent findings [36]\u2013[38] they have limits in capturing the semantic meaning in generated captions. So we supplement our evaluation with Sentence-BERT [39], metrics tailored for improved semantic relatedness, which produces embeddings for calculating sentence-level similarity.\nImplementation details: We use SpecAugment [40] for data augmentation, opting for a 16 kHz sampling rate and 128-dimensional mel-spectrograms for feature extraction. Our architecture combines the AST\u00b3 encoder with a BART-base4 decoder. For CRESCENDO, we train for 10 epochs with a batch size of 128, a weight decay of 0.01, and a learning rate of $1 \\times 10^{-4}$. Then the model is finetuned over 20 epochs with a batch size of 32, accumulation steps of 2, a weight decay of 0.01, and a learning rate of $2 \\times 10^{-5}$. We deploy a beam search at the inference stage with a beam size of 5 to generate captions and report the average score from 4 runs with different random seeds for each setting."}, {"title": "IV. RESULTS", "content": "Results comparison from various encoders: In designing our approach for singing style captioning, we first conducted an exploratory analysis to identify an appropriate pretrained audio model to serve as our audio encoder. The models evalu-ated in this study included well-known architectures pretrained with specific domain datasets: audio, music, and speech. Specifically, we considered AST [21] from audio, MERT [26] from music, Wav2vec 2.0 [27], WavLM [28], and HUBERT [29] from speech. A detailed exposition of the performance metrics for each model is in Table II. Accordingly, we used AST as our audio encoder, outperforming the other models.\nAblation study: As in Table III, we progress the abla-tion study in the S2Cap dataset. Adding CRESCENDO and demixing supervision led to an increase in metrics, affirming its role in enhancing the model's performance."}, {"title": "V. CONCLUSIONS", "content": "We propose a novel task, singing style captioning, which aims to generate textual prompts describing the vocal charac-teristics of singers from given song inputs. For singing style captioning, we developed S2Cap, a comprehensive dataset reflecting diverse vocal attributes. Additionally, we established a robust baseline for singing style captioning by introducing the CRESCENDO method, which addresses encoder-decoder misalignment and demixing supervision to enhance vocal characteristics against musical accompaniment. These contri-butions provide a solid baseline for future research in this emerging field."}]}