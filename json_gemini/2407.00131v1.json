{"title": "RepAct: The Re-parameterizable Adaptive Activation Function", "authors": ["Xian Wu", "Qingchuan Tao", "Shuang Wang"], "abstract": "With the ever-increasing demand and application of Internet of Things (IoT) technology in the real world, artificial intelligence applications in edge computing have attracted more and more attention. However, due to the limitations in computational resources on edge devices, the design of neural networks at the edge often must consider aspects such as lightweight structure and logical reasoning capabilities. While there has been considerable research on lightweight neural network architectures for the edge, little attention has been paid to how to enhance the reasoning capabilities of neural networks within the constraints of limited model parameters. To address this issue, we propose RepAct (Re-parameterizable Adaptive Activation Function), a simple yet effective adaptive activation function that can be re-parameterized, aimed at fully utilizing model parameter capacity while enhancing the inference and understanding abilities of lightweight networks. Specifically, RepAct adopts a multi-branch activation function structure to exploit different features information through various activation branches. Furthermore, to improve the understanding for the different activation branches, RepAct integrates each branch across different layers of the neural network with learnable adaptive weights. RepAct, by training a variety of power-function-based activation functions like HardSwish and ReLU in multi-branch settings, has been validated on a range of tasks including image classification, object detection, and semantic segmentation. It demonstrated significant improvements over the original lightweight network activation functions, including a 7.92% Top-1 accuracy increase on MobileNetV3-Small on ImageNet100. At the same time, with computational complexity at inference stage approximating HardSwish, RepAct approached or even surpassed the task accuracy achieved by various mainstream activation functions and their variants in datasets like Cifar100, Cifar10, VOC12-Detect and VOC12-Segment within MobileNetV3, proving the effectiveness and advanced nature of the proposed method.", "sections": [{"title": "1 Introduction", "content": "Since the inception of the neural network resurgence with AlexNet[1], the design of activation functions has garnered continuous attention [2]. Activation functions introduce non-linearity into the networks and play a critical role in feature extraction. In traditional neural network designs, the selection and design of activation functions for different layers and networks are typically based on manual design experience [3, 4] or adapted through NAS (Neural Architecture Search) [5, 6]. The emergence of adaptive activation function [6, 7, 8] effectively improves network performance. Subsequently, various dynamic adaptive parameters were introduced from different dimensions of the feature graph [9, 10, 11]. Although the introduced parameters and computation amount were small, the memory cost of element by element operation often formed a bottleneck of lightweight network reasoning [12].\nWhen deploying lightweight networks on resource-constrained edge devices, real-time performance requirements impose strict limitations on model parameters, computational power, and memory operations [12, 13, 14, 15]. Convolutional neural networks exhibit sparsity in their activations [16], which prevents lightweight networks from fully utilizing model capacity to learn features from task data. We have noticed that re-parameterizable convolutional structures [17, 18, 19, 20, 21], by virtue of their multi-branch architecture during training, enhance the network's feature capturing ability. At inference stage, these networks leverage the linear computational characteristics to revert to a single-branch model for deployment, balancing network performance with inference speed, thereby rejuvenating various classical network architectures. Inspired by this, we propose a series of re-parameterizable adaptive activation functions, RepAct, which, through multi-branch activation during training and single-branch activation during inference, enhance the learning capabilities of lightweight networks in various tasks without additional memory access and computational overhead. RepAct adaptively adjusts the weights of each branch within different layers of the network according to gradient descent. We adopted common activation functions from lightweight networks to form a multi-branch activation structure, thus enhancing the feature extraction capability of the current network layer. Also, we introduce an Identity linear branch that avoids the destruction of feature information caused by nonlinear activations, balancing the linear and nonlinear flow of feature information, making it easier for features to transmit across network layers. This was analyzed and verified from the perspective of inter-layer feature propagation and gradient scaling. In addition, we have further explored RepAct-II with a degradable soft gating mechanism (Softmax-type) and RepAct-III with global degradable information (BN-type) to adapt to different types of task scenarios.\nWe substituted the original activation functions in the backbone of lightweight networks (ShuffleNet, MobileNet, and lightweight ViT) with the RepAct series of activation functions and validated their performance on image classification, object detection, and segmentation tasks against the original networks. After substituting with the RepAct series, a considerable improvement in task accuracy was observed compared to the original and other adaptive activation functions, which verified the capability enhancement of the RepAct series for feature extraction in lightweight networks. Moreover, by utilizing GradCAM and RepAct visualization, we analyzed the reasons for the improvement in network feature extraction ability from forward inference and gradient back-propagation.\nThis paper addresses the performance degradation issue in lightweight networks by first proposing a plug-and-play, re-parameterizable adaptive activation function that significantly improves the learning abilities of various lightweight networks in their respective task training without almost any increase at the inference stage. Secondly, we validated and analyzed the characteristics of RepAct in feature transmission and gradient solution, deeply observing and elucidating its mechanism of action, providing a theoretical basis for network optimization. We designed RepAct-II and RepAct-III with a degradable soft gating mechanism and degradable global information, respectively, suitable for different tasks and network scenarios, hence enhancing the model's flexibility and universality."}, {"title": "2 RELATED WORKS", "content": "2.1 Activation function\nDifferent types of activation functions significantly affect the feature extraction capabilities and gradient propagation of neural networks, such as ReLU [1], SoftPlus [22], ELU [23], Mish [24], and their variants [8, 9, 25, 26, 27]. These activation functions often focus on discussions about continuity, differentiability, and the saturation of the upper and lower bounds. In the design of network structures, we have a substantial and excellent library of activation functions at our disposal. However, the design of activation functions for different network structures in various datasets still relies on manual experience [3, 4] or NAS searches [5, 6]. Each attempt comes with considerable cost; thus, how to design an activation function suitable for the current data distribution and network structure remains a long-standing problem in network design.\nMobileNetV2 [3] discusses the harms of overly strong activation functions when the feature map channel dimensions are narrow in an inverted residual structure, confirming that the realms of activation layer and network architectural design complement each other. VallinaNet [11] greatly enhances the feature extraction ability of shallow networks by introducing additional stacking calculations of activation functions. Networks composed solely of ReLU can be equivalent to a 3-layer shallow network during inference [28], but it is still worth pondering which activations should be chosen during training to optimize the feature extraction and flow between different network layers, enabling the network to reach the optimum via gradient descent.\nLearnable activation functions can be involved in backward propagation as network parameters, endowing the network with a stronger learning capacity. PReLU [7], as an adaptive improvement of LReLU, introduces a learnable slope parameter to participate in training and achieves human-level performance on the ImageNet dataset [29] with virtually no additional inference cost. FReLU [10] uses a 2D funnel-like spatial gating mechanism to enable dynamic spatial modeling activation capability. DYReLU [9] activates after dynamically calculating the current feature map through a module similar to SE [30], which is particularly effective for lightweight networks. AReLU [8] acknowledges the gradient amplification of the activation layer as a vital characteristic for the rapid convergence of neural networks. ACON [31], by adaptively choosing active neurons (dynamically), performs well in terms of precision on lightweight networks and presents a series of designs with different granularity and dynamic-static combinations. Sheng [32] explored gated, mixed, and hierarchical combinations of linear and exponential activation functions and designed combinatory adaptive activation functions. Although most dynamic adaptive activation functions effectively improve the task accuracy of networks, compensate for the insufficient model capacity and weak learning ability of lightweight networks, the computational complexity of the exponential type is too high, and dynamic adaptiveness relies on additional memory operations. Access to the feature map causes redundant serial memory overhead due to traversal or fragmented access, hindering the fusion of computational modules and activation operators, disastrously impacting inference speed on edge devices [12].\n2.2 Reparameterable structure\nRe-parameterizable neural network structures [17, 18, 20, 33, 34] and re-parameterizable convolutional structures separate the multi-branch network architecture during training from the single-branch during deployment. They aim to capture as much information as possible during the training phase, while capitalizing on linear characteristics to merge branches during the deployment phase. Apple's MobileOne [19] re-parameterizes the MobileNet architecture to achieve a backbone network with a phone inference duration at the millisecond level. RepViT [21] undertakes re-parameterization from a ViT perspective, surpassing existing state-of-the-art lightweight ViTs. RepOptimizer [35] applies gradient re-parameterization to the VGG [36] style models by modifying gradients based on specific hyperparameters, incorporating model-specific prior knowledge into the optimizer, focusing on effective training. However, to date, the design of re-architecturable structures has concentrated on dense linear computational modules and has not fully explored the re-parameterizable form of activation functions, which constitute an essential part of the non-linear operations in a network model. Thus, exploring the re-parameterizable form of activation functions is crucial for enhancing the capability of networks to extract features."}, {"title": "3 RepAct", "content": "The RepAct proposed in this article is a re-parameterizable adaptive activation function. It utilizes a combination of common activation functions for multi-branch training during the training stage, where each branch's weight factor is adaptively adjusted through gradient descent. In the inference stage, these branch weight factors are re-parameterized in different segments to revert to a single-branch structure. The paper analyzes how the adaptively scaled adjustments of RepAct I to the forward features and the backward gradients enhance the network's learning capability. Furthermore, it proposes RepAct III-Softmax with a degradable soft gating mechanism, and RepAct III-BN with degradable global information.\n3.1 RepAct\nThe RepAct I structure is a universal type that combines common activation functions into a multi-branch setup, summing computed feature maps across each branch. This achieves a fusion of feature information mapped after activation under different nonlinear branches, thereby enhancing the network's feature extraction capability.\nThe RepAct I structure presents two distinct paradigms: a multi-branch structure during training and a re-parameterized single-branch structure during deployment. When the selected branch activation functions are all of the power-function type, the degree of re-parameterization for each branch segment is higher. Hence, this paper opts for power-function type activation functions that are suitable for deployment in lightweight networks. In terms of activation function segmentation, the empirical selection inherits the union of segments from each branch. After re-parameterization to a single branch, the memory access consumption during inference remains the same as for non-dynamic calculation class activation functions, equating to the size of the input feature map. The computational complexity is approximated to the complexity required for the RepAct's largest branch, as detailed in equation 1.\n\\(O(RepAct) \u2248 max(O(RepAct_n))\\)\n(1)\nO(RepAct_n) Is the computational complexity of the Nth brahch of RepAct\nIn some network architectures, nonlinear operations can disrupt the transmission of feature information between layers. MobileNetV2 [3] mentions that in certain layers, the nonlinear behavior of activation functions may damage the feature structure, resulting in a loss of feature information and a decrease in network performance. Therefore, in the RepAct structure, the identity mapping is also included as one of the branches in the multi-branch framework. Through learnable coefficients for each branch, the structure effectively decouples linear and nonlinear features and adaptively balances the nonlinear feature expressiveness and linear feature retention capabilities across different layers of the network.\nWhen selecting Identity, ReLU, PReLU, and HardSwish-type activation functions for the power function as the components of RepAct I multi-branch structure, the activation function representation of RepAct I during the training phase is expressed as equation 2.\n\\(X' = a_0Identity(X)+a_1ReLU(X)+a_2PReLU(X)+a_3HardSwish(x)\\)\n(2)\nDuring the network training phase, the weight factors of each branch are learnable adaptive parameters. During the network inference phase, they are re-parameterized and merged as static constants. Thus, the RepAct I activation function at the inference stage can be simplified to equation 3, restoring it to a single-branch power function form with segmented sections. In the combination, the single-branch segment interval of RepAct I after re-parametrization becomes the union of branch segment intervals, and \\(a_n\\) is re-parametrized as \\(\\delta_n\\) to serve as a static network parameter.\n\\(X' = RepAct(X) = \\begin{cases} x * \\delta_1, 3 \\leq x \\\\ x * x * (\\alpha_3/6) + x * (\\delta_0 + \\alpha_1 + \\alpha_2 + \\alpha_3/2), 0 < x < 3 \\\\ x * x * (\\alpha_3/6) + x * (\\delta_0 + (PReLU.T * \\alpha_2) + \\alpha_3/2),-3 < x < 0\\\\ x * (\\delta_0 + (PReLU.T * \\alpha_2) + 0),x < -3  \\end{cases}\\)\n\\({\\delta_1 = \\alpha_0 + \\alpha_1 + \\alpha_2+ \\alpha_3}\\)\nIn the formula \\({\\delta_2 = \\alpha_3/6}\\)\n\\({\\delta_3 = \\alpha_0 + \\alpha_1 + \\alpha_2+ \\alpha_3/2}\\)\n\\({\\delta_4 = \\alpha_0 + \\alpha_1 + (PReLU.T * \\alpha_2)+ \\alpha_3/2}\\)\n\\({\\delta_5 = \\alpha_0 + \\alpha_1 + (PReLU.T * \\alpha_2) + 0}\\)\nAfter re-parameterization of RepAct I, the number of model parameters has increased by only five parameters in each layer implementing RepAct. Within the various branches of RepAct, the maximum computational complexity function (O(RepAct_n) corresponds to HardSwish, as shown in equation 4. Compared to the maximum complexity of (O(RepAct_n), the computation of RepAct is increased by only one if operation and two multiplication operations.\n\\(HardSwish (X) = \\begin{cases} x,3 \\leq x \\\\ x * x * x *,-3 < x < 3 \\\\ 0,x < -3 \\end{cases}\\)   \\(RepAct I(X) = \\begin{cases} x * \\delta_1,3 \\leq x \\\\ x * x * \\delta_2 + x * \\delta_3,0 < x < 3 \\\\ x * x * \\delta_2+ x * \\delta_4, -3 < x < 0\\\\ x * \\delta_5,x < -3 \\end{cases}\\)\n(4)\nDuring the deployment phase, in GPUs or AI-accelerated edge devices, activation functions are often fused with convolution computations to reduce frequent memory access. Convolution processing is typically parallel or partially parallel, so for the fused segmented power series class activation functions, the bottleneck of inference speed is the segment with the maximum computation among all segments. After re-parameterization of RepAct I, the computational complexity of the segment with the maximum computation remains as O(ax\u00b2 + bx), consistent with max(O(RepAct_n), which allows the approximation O(RepAct) \u2248 max(O(RepAct_n)).\nDuring the initialization of RepAct I's branch weight factors, a simple approach is to evenly distribute the branch weights so that their sum equals one. This ensures that during the initial stages of network training, the sum of features and gradients are not too small to vanish, enabling the effective transmission of different features across each branch. Following initialization, the activation function of RepAct I and its first-order derivative are as illustrated in figures 2(a), (b), and (c), respectively showing each branch's activation mapping and first-order derivatives within the [0,1] range.\n3.2 Adaptive scaling of forward feature and reverse gradient\nForward reasoning:\n\\(a^{(L)} = \\sigma^{(L)} (z^{(L)})\\)\n(5)\n\\(z^{(L)} = w^{(L)}a^{(L-1)}+b^{(L)}\\)\n(6)\n\\(C_0 = LOSS = - [ylog(\\hat{y}) + (1 -y) log(1 - \\hat{y})]\\)\n(7)\n(a(L) is the active value of the current layer,\\(\\sigma^{(L)}\\) is the current layer activation function,\\(w^{(L)}\\) and \\(b^{(L)}\\) they are weight and bias,C0 is a loss function)\n\\({\\partial C_0}\\)\nInverse gradient calculation:\n\\({\\partial w^{(L)}}\\)\n\\(\\frac{\\partial C_0}{\\partial w^{(L)}}= \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\frac{\\partial C_0}{\\partial a^{(L)}}\\)\n(8)\n\\({\\partial C_0}\\)\n\\({\\partial a^{(L)}}\\) = loss_derivative\n(9)\n\\({\\partial a^{(L)}}\\)\n\\({\\partial z^{(L)}}\\) = \\({\\sigma^{(L)} } \\) (z(L))\n(10)\n\\({\\partial z^{(L)}}\\)\n\\({\\partial w^{(L)}}\\) = a(L-1)\n(11)\nDuring the calculation of gradients for co at layer L, the scaling effect of \\({\\sigma^{(L)} }\\) directly influences the derivative of \\({\\partial C_0}\\).In the forward process, there is an indirect impact on \\({\\partial z^{(L)}}\\) through the effect on a(L).As a(L-1) is subject to scaling through \\({\\sigma^{(L-1)} }\\) in the forward direction, \\({\\partial C_0}\\) is also indirectly affected by \\({\\sigma^{(L-1)} }\\).Therefore, the scaling action of the activation function will affect the feature values being propagated and the gradients during the backward propagation for the current layer as well as the subsequent layers, forming a hierarchical gradient scaling mechanism.\nIn CNNs, the scaling effect of activation functions on features and gradients is similar. As shown in Figure 4, each convolutional layer is typically followed by a Batch Normalization (BN) layer [12, 37, 38], which adaptively normalizes forward features and backward gradients. The BN layer ensures that the scaling effect of the activation function in the current layer does not cause an explosion or vanishing of features or gradients in more distant feature or gradient updates [38]. Hence, when updating the weights of each layer in a CNN, the gradient scaling caused by the activation function is only significantly effective for that particular layer. For the shallow layers of the network, after BN processing, they carry back more category information to the shallow layers; therefore, RepAct can enhance the learning ability of various lightweight CNNs.\nOn the other hand, CNN feature maps usually exhibit sparse activations [16], suggesting that after CNN training is complete, the network can be compressed and pruned in different dimensions [39, 40, 41, 42]. This indicates that sparse activations in CNNs lead to redundant network weights, preventing the full utilization of the total weight information capacity for task learning. RepAct continuously adjusts the weight factors of each channel branch, adaptively capturing and scaling features during forward inference and scaling the gradient magnitude in each layer during backward propagation. Thus, RepAct forms a gradient scaler that not only adapts to network layer structures but also to the inter-layer feature distribution, which is more conducive to the network's efficient use of its information capacity for learning.\nIn Figure 5, within RepAct, the Identity and ReLU functions represent linear and nonlinear mappings, respectively, each with their own branch weight factors. During the backward propagation of the network, after re-parameterization of RepAct to a single branch, the partial derivative of the weight coefficient \\(k_1\\) for X > 0 will be further decoupled. Utilizing the prior segmentation experience of Identity and ReLU, an update is formulated for the decoupled linear and nonlinear mappings, which together affect \\(k_1\\).\nRepact = \\({\\begin{cases} k_1x,x > 0  \\\\ k_2x,x < 0  \\end{cases}}\\)\n(12)\n\\(k_1 = a_1+a_2, k_2 = a_2\\)\n\\(k_1 = k_1 - l(da_1 + da_2),k_2 = k_2 - l(da_2)\\)\n(13)\n3.3 RepAct II competition and III cooperation\n3.3.1 Repact-II-Softmax - Competition\nWhen RepAct I \\(\\sum \\alpha_i > 1\\), feature information passing through the RepAct layer is magnified during the forward propagation, amplifying the feature itself. Similarly, during backpropagation, the feature gradient is also magnified due to the chain rule, which can facilitate the learning of network parameters. However, this may also lead to the network easily capturing noise and potentially causing overfitting. Therefore, it becomes necessary to impose constraints on the adaptive weight factors of the branches.\nWe further propose the RepAct II-Softmax model, which features a degradable soft gating adaptive factor. The adaptive factors a of each branch are mapped through a Softmax function, resulting in \\(\\sum \\alpha' = 1\\) after the mapping for each branch. This approach reduces the magnifying effect of the activation layer, creating a limited-resource competition among the branch activation functions, which in turn helps reduce the network's tendency to overfit.\n\\(\\alpha' = \\frac{e^{\\alpha_i}}{\\sum e^{\\alpha_i}} \\\\), \\(\\sum \\alpha' = 1\\)\n(14)\n(ais the branch i weight before the mapping,\\(\\alpha'_i\\)is the weight of branch i after mapping)\nDuring the inference stage,\\(\\alpha'\\) is retained after the Softmax mapping, and the system reverts back to the RepAct I form. This process involves re-parameterization of the various branches to restore the single-branch activation function, consistent with equation (3).\n3.3.2 RepAct III-BN - Cooperation\nIntuitively, we expect that aside from the backward gradient descent adaptive adjustments, the single-branch factor \\(\\alpha_i\\) should also cooperate synergistically with the factors of other branches to modulate the branch weights, allowing the fused feature maps after branching to aggregate more efficient feature information. However, we do not wish to introduce additional auxiliary memory access operations during the inference stage, as this would slow down network inference speeds.\nConsequently, we further propose the RepAct III-BN cooperative type, which possesses degradable global information. This design allows the branch weight factors to make use of the global information from the feature maps after branch fusion to readjust during the BN fusion stage.\nRepAct III-BN first overlays the feature maps activated by each branch, resulting in a fused feature map. In the second stage, this fused feature map is treated as an integrated whole and fed into a single-channel BN layer where it undergoes an affine transformation using the global information of the mean and variance of the fused feature map and the learnable parameters. The final feature map after the BN layer is then the final output feature map of RepAct III-BN.\nDuring the branch fusion phase, the linear properties of the BN layer are first leveraged to merge it with the factors from each branch of RepAct. The fusion of RepAct III-BN is formalized as equation 15. This means that in the inference phase, RepAct III-BN degrades to the form shown in Figure 7, RepAct III-BN (b), where each branch factor \\(\\alpha'_i\\) after BN linear degradation is as described in equation 16. After the degradation of BN, the branch factor a' of RepAct III-BN reverts to the form of RepAct I. The factor \\( \\beta '\\)complements or is amalgamated into the segments as a zero-power coefficient after the fusion of RepAct I branches, completing the training structure from multi-branch structure to single-branch inference of RepAct III-BN. The re-parameterization post-training is as depicted in Figure 7, RepAct III-BN (c). As such, RepAct III-BN, through two stages of re-parameterization degradation, enables each branch's adaptive factor to possess the global feature information of the post-fusion feature maps.\n\\(Y_{final} = BN(\\alpha_0x_0+\\alpha_1x_1 + ... + \\alpha_nx_n) = \\gamma (\\frac{\\alpha X}{\\sqrt{\\sigma^2+1e-6}})+  (\\beta - \\frac{\u03b3\u03bc}{\\sqrt{\\sigma^2+1e-6}})\\) = \\( \\varepsilon X + \\beta '\\)\n(15)\nIn the equation 15,\\(\\alpha_0x_0 + \\alpha_1x_1 + ... + \\alpha_nx_n = X_n, ; \\varepsilon = \\frac{\u03b3}{\\sqrt{\\sigma^2+1e-6}}; \\beta ' = \\beta - \\frac{\u03b3\u03bc}{\\sqrt{\\sigma^2+1e-6}}\\)\n\\(\\alpha' = \\varepsilon \\alpha_i = \\frac{\u03b3}{\\sqrt{\\sigma^2+1e-6}} \\alpha_i\\)\n(16)"}, {"title": "4 Experiments", "content": "To comprehensively evaluate the performance of RepAct in classical lightweight network architectures and various tasks, we conducted a thorough validation across multiple datasets, including Imagenet100, Cifar100, VOC12-Detect, and VOC12-Segment. With the use of GradCAM for visualization, we intuitively demonstrated the enhancement of the feature attention areas in lightweight networks by RepAct. This provided an in-depth analysis of the working mechanism behind the enhancement of network learning capabilities from a gradient perspective.\n4.1 Image Classification\nUtilizing the ImageNet100 dataset (composed of 100 categories extracted from the ImageNet2012-1k) [29] and the Cifar100 dataset, we conducted ablation experiments on classic lightweight networks [12, 37] and small ViT models [43, 44] by simply replacing the original network activation functions with the RepAct series. For the training, we used image sizes of 224 pixels (ImageNet100) and 32 pixels (Cifar100) as inputs, to perform network learning using the conventional image classification training approach (detailed settings of the training hyperparameters can be reviewed in the appendices, with all tasks having a fixed random seed of 0).\n4.1.1 RepAct and gradient visual analysis"}, {"title": "4.2 RepAct commonality", "content": "In other tasks where various lightweight networks act as the backbone, RepAct easily replaces the original activation function to enhance the backbone network's task learning capabilities and feature extraction abilities. Using the VOC12 dataset for object detection and segmentation [46], we divide the data following the original VOC12 division method for training and testing to demonstrate the universality of RepAct across different tasks.\n4.2.1 Target detection\n4.2.2 Semantic segmentation"}, {"title": "5 Summary and discussion", "content": "This research introduces a series of plug-and-play, re-parameterizable, adaptive RepAct activation functions, taking full advantage of the characteristics of multi-branch training and single-branch inference. Without introducing extraneous parameters and additional memory computations, we have successfully enhanced the learning ability of lightweight networks by relying solely on power-function type RepAct. The mechanism behind RepAct was validated and analyzed from multiple perspectives including adaptive branch selection, forward feature scaling, and backward gradient propagation across different tasks and classical lightweight networks. However, there are still issues that necessitate further experimental exploration:\n1.The selection of types of branches for RepAct, the balance of their quantities, and the degrees of re-parameterization still require further verification and experimentation. This paper has attempted re-parameterizable combinations of activation functions with power-function forms, and the exploration of combinations involving exponential-level activation functions remains to be explored.\n2.For network structures with outstanding gradient design and learning capabilities, such as ResNet[52], using RepAct may lead to rapid convergence and overfitting. Therefore, how to balance the learning capabilities of RepAct or effectively regularize it remains a question that needs to be addressed."}]}