{"title": "The ISCSLP 2024 Conversational Voice Clone (CoVoC) Challenge: Tasks, Results and Findings", "authors": ["Kangxiang Xia", "Dake Guo", "Jixun Yao", "Liumeng Xue", "Hanzhao Li", "Shuai Wang", "Zhao Guo", "Lei Xie", "Qingqing Zhang", "Lei Luo", "Minghui Dong", "Peng Sun"], "abstract": "The ISCSLP 2024 Conversational Voice Clone (CoVoC) Challenge aims to benchmark and advance zero-shot spontaneous style voice cloning, particularly focusing on generating spontaneous behaviors in conversational speech. The challenge comprises two tracks: an unconstrained track without limitation on data and model usage, and a constrained track only allowing the use of constrained open-source datasets. A 100-hour high-quality conversational speech dataset is also made available with the challenge. This paper details the data, tracks, submitted systems, evaluation results, and findings. The challenge's official website is https://www.magicdatatech.com/iscslp-2024.", "sections": [{"title": "1. Introduction", "content": "Text-to-speech (TTS) aims to generate speech that sounds as natural and human-like as possible. Recent advancements in neural speech synthesis have significantly enhanced the quality and naturalness of generated speech [1, 2, 3], leading to widespread applications of TTS systems in real-world scenarios. A notable breakthrough in the field is witnessed in zero-shot TTS, driven by expanded datasets [4] and new approaches [5] (e.g., decoder-only paradigms), attracting extensive attention from academia and industry. However, these advancements haven't been sufficiently investigated to address challenges in spontaneous [6, 7] and conversational [8] contexts. Specifically, the primary challenge lies in effectively managing prosody details in the generated speech, which is attributed to the diverse and intricate spontaneous behaviors that differentiate spontaneous speech from read speech.\nLarge-scale TTS systems yield promising outcomes in zero-shot generation due to in-context learning ability. However, a prevalent challenge in the field of large-scale zero-shot TTS is the lack of consistency in training and testing datasets, along with a standardized evaluation benchmark. This issue hinders direct comparisons and makes it challenging to accurately assess various systems' performance.\nWe launch the Conversational Voice Clone Challenge (CoVoC) to promote the development of expressive spontaneous-style speech synthesis in the zero-shot scenario. Besides the existing 10,000-hour WenetSpeech4TTS [9] dataset and 180 hours of Mandarin conversational speech data \u00b9, we also release a new 100-hour high-quality conversational dataset. Furthermore, we also conduct a standardized testing dataset accompanied by carefully designed text which aims to establish a comprehensive benchmark. This paper presents the data details, track design, submitted systems, evaluation results, and key findings."}, {"title": "2. Challenge Design", "content": "The goal of the CoVoC is to conduct a benchmark for zero-shot voice cloning with conversational speech, aiming to evaluate and compare the performance of different systems in generating conversational speech."}, {"title": "2.1. Track Setting", "content": "The CoVoC challenge has two tracks: a constrained track and an unconstrained track. In the constrained track, only the specified dataset can be used for model training, while pre-trained open-source models are allowed. The details of training datasets are as follows:\n\u2022 WenetSpeech4TTS [9]: A multi-domain corpus derived from the open-sourced WenetSpeech [10] dataset. Tailored for TTS tasks, we refined WenetSpeech by adjusting segment boundaries, enhancing the audio quality, and eliminating speaker mixing within each segment. Following a more accurate transcription and quality-based data filtering process, the obtained corpus contains 12,800 hours of paired audio-text data.\n\u2022 MAGICDATA (Conversational Speech Corpus): A 180-hour speech dataset recorded with various mobile devices. A total of 663 speakers were invited to participate in the recording. Recordings were conducted in a quiet indoor environment. All speech data were manually labeled and professional inspectors proofed the transcriptions to ensure the labeling quality.\n\u2022 HQ-Conversations: A 100-hour dataset featuring 200 speakers, including 75 males and 125 females. The content consists of segmented conversations, which closely reflect daily life scenarios, characterized by natural and expressive language rather than a scripted or read-aloud"}, {"title": "2.2. Test Datasets", "content": "We carefully selected 20 speech samples from 20 speakers as the target speaker's prompt during the testing phase. The target speakers are categorized into two types based on their speaking style: 8 speakers with an ordinary reading style and 12 speakers with a conversational spontaneous style. This setup is intended to see how systems perform on different prompts in zero-shot conversational speech generation. The lengths of the audio prompts ranged from 5 seconds to over 20 seconds, and the distribution of audio lengths is shown in Table 2.\nThe text test sets for the CoVoC challenge include seven distinct types of text: conversational texts, colloquial texts, stammer texts, rhotic accent texts, tone sandhi texts, polyphonic character texts, and long-form audiobook texts. To ensure impartiality and prevent specialized handling, the exact categories of texts are not specified in the test set. The distribution of these text types is detailed in Table 3. Each participating team is required to synthesize audio for all test texts across 20 target speakers, resulting in a total of 8,600 synthetic audio samples."}, {"title": "3. Metrics", "content": "The evaluation of CoVoC includes both subjective and objective evaluations. All audio samples are evaluated using objective metrics, and a selected subset of 100 audio samples is used for subjective evaluations. During the objective evaluation, all audio samples are resampled to a uniform rate of 16 kHz."}, {"title": "3.1. Objective Metrics", "content": "The objective evaluation consisted of two aspects: pronunciation accuracy and timbre similarity. We employ Character Error Rate (CER) and cosine similarity for evaluation:\n\u2022 Character Error Rate: CER is computed between the ground truth transcript and the recognized transcript. We use an open-source paraformer-large model [18] to recognize the synthesized speech into the corresponding transcription.\n\u2022 Speaker Similarity (SIM): We employ the Resemblyzer tool [19, 20] to extract speaker embedding and compute the cosine similarity between the reference speech and generated speech."}, {"title": "3.2. Subjective Metrics", "content": "For subjective evaluation, we conducted mean opinion score (MOS) tests to assess speech in four aspects: speech quality, speech naturalness, speaker similarity, and speech spontaneous style.\n\u2022 Speech Naturalness (SN): Evaluate the naturalness of the generated speech. Consider whether the pronunciation is correct, if there is any ambiguity, if there are tone changes, and if the pauses sound natural. Assign a score from 1 (completely unnatural) to 5 (highly natural).\n\u2022 Speech Quality (SQ): Evaluate the quality of the generated speech. Determine if there is any electronic distortion and if the voice is clear. Assign a score from 1 (very poor) to 5 (excellent).\n\u2022 Speaker Similarity (SS): Evaluate the similarity between the generated speech and the target speaker, focusing on timbre and speaking style. As the speaker's audio contains only one sentence, the style may be somewhat uniform, so it is less crucial to focus on style similarity. Assign a score from 1 (not similar at all) to 5 (highly similar).\n\u2022 Speech Spontaneous Style (SSS): Evaluate the colloquial characteristics of the speech. Consider whether the pronunciation of colloquial words such as \"um,\" \"uh,\" and \"ah\" sounds natural, whether laughter and non-rhythmic pauses are normal, and if the stress and rhythm resemble those of a real person. Assign a score from 1 (completely unnatural) to 5 (highly natural).\nWe invite 10 professional raters to listen to the generated samples and give a score for each audio sample. The subjective listening tests used the original audio submissions from the competition teams without any additional processing. The final score (FS) shown in Table 1 is computed by a weighted sum of the 4 MOS scores across four aspects:\nFS = 0.25 \u00d7 SN + 0.25 \u00d7 SQ+0.25 \u00d7 SS + 0.25 \u00d7 SSS"}, {"title": "4. Submitted systems", "content": "A total of 11 teams submitted final results: 5 in the constrained track and 7 in the unconstrained track. These teams consisted of 7 from industry and 4 from academia. Detailed information about each team's system is presented in Table 1.\nAs outlined in Section 2, the training data for the constrained track systems was limited to specified datasets, although open-source models were permitted. Only two systems reported training dataset sizes ranging from 10,000 to 300,000 hours in the unconstrained track.\nRegarding acoustic models, most teams employed autoregressive (AR) text-to-semantics models, with only three opting for non-autoregressive (NAR) structures. Various methods were used at the waveform generation stage, including Codec [21] decoders, VITS [3] decoders, and Flow Matching [16], with no significant differences observed in objective and subjective evaluation outcomes. Each team's final score was determined by a weighted sum of four scores from subjective evaluations as detailed in Section 3.2.\nAlthough not considered in the ranking, the challenge organizer also submitted a system with the highest score in the final subjective listening test. This system utilized a TorToise-like model framework and employed Single-Codec [22] for speech tokenization. It featured frequency band expansion in the vocoder to improve audio quality and incorporated DSP-GAN [23] to upscale a 16k mel-spectrogram to a 48k high-fidelity waveform. In the final inference stage, in-context learning (ICL) was employed, prepending the target speaker's text and audio to the input text, thus treating speech synthesis as a continuation task."}, {"title": "5. Results and Analysis", "content": "We conducted a systematic comparative analysis of the submission results from all teams. In terms of objective metrics, we analyzed the relationship between CER and text type, as well as the relationship between SIM and the length of the target speaker's audio. In terms of subjective metrics, we found that the spontaneous style of speech is closely related to the speaking style of the target speaker."}, {"title": "5.0.1. Analysis between CER and Text Type", "content": "We calculated the average CER of each team on different types of test texts. The results are shown in Figure 1. All participating teams performed well on the tone sandhi test set, while all teams showed an increased CER on the rhotic accent test set. This indicates that current systems learned well for the tone sandhi patterns with a large amount of paired training data, but their performance is relatively limited on the less frequent rhotic accent data.\nAdditionally, teams Cl-we_are_NPC, C3-THU-HCSI, C4-ViveTTS, and C5-SMIIP_TTS showed a significant increase in CER on the long-form audiobook test set, partly due to the small proportion of audiobook text types in the constrained track training and partly due to unresolved instability issues in AR models.\nFor the long texts, we found that the spectrogram is quite clear in the intervals between two consecutive sentences where there should be a pause in some submissions. We argue that these teams segmented the long text into smaller sentences for synthesis and then concatenated the short segments as the final results for submission. Despite this, CER still showed a significant positive correlation with text length.\nWe plotted the curve of the CER with text length across some typical AR and NAR systems as shown in Figure 2. It can be seen that three NAR teams (U1-MASTER-TTS, U3-Orion, U7-hySoundClone) in the unconstrained track obtained low CER and were less affected by text length."}, {"title": "5.0.2. Analysis between SIM and Prompt Duration", "content": "In terms of the objective metric of speaker similarity, we found that as the duration of the target speaker's audio increases, the speaker similarity (SIM) tends to rise, as shown in Figure 3. This trend is particularly noticeable when the target speaker's audio duration increases from the range of 0 to 5 seconds to the range of 5 to 10 seconds."}, {"title": "5.0.3. Analysis between SSS and Prompt Style", "content": "The MOS scores in 4 dimensions are listed in Table 4. As we can see, the scores of SN and SQ are generally higher than those of SS and SSS across all participating teams, indicating the naturalness and quality of the current speech synthesis systems have achieved significant advancements.\nComparing the speech spontaneous style (SSS) scores between two types of target speakers, i.e., the speakers with ordinary audio prompts and dialog prompts, we found that the scores for conversational spontanous style target speakers were consistently higher than those for ordinary reading style target speakers, as shown in Figure 4.\nIn addition, we find that some teams, such as U1, U5, and U7, suffer significant degradation when the prompt is an ordinary speaking style. This indicates that the synthesized results of these systems strongly depend on prompt speech and exhibit poor stability. Furthermore, the two top-final-scoring autoregressive teams (U2, U4) perform well in the spontaneous style, suggesting that autoregressive models still have some advantages in modeling specific speaking styles."}, {"title": "6. Conclusion", "content": "The CoVoC Challenge has successfully established a comprehensive benchmark for evaluating zero-shot voice cloning in spontaneous conversational contexts. Participation from various teams across academia and industry has highlighted significant advancements in generating high-quality, spontaneous-style speech. The release of standardized datasets, such as WenetSpeech4TTS and HQ-Conversations, has facilitated consistent training and evaluation, addressing a critical gap in the field of TTS research. The challenge's results highlight the effectiveness of both autoregressive and non-autoregressive models in different aspects of speech synthesis. Objective evaluations revealed that while non-autoregressive models generally achieve lower character error rates, autoregressive models often excel in maintaining speech spontaneous style over varying prompt styles. Through detailed analysis, we identified key areas for improvement, such as handling long texts and rare speech patterns like rhotic accents. The strong performance of non-autoregressive models on text length indicates potential pathways for future research. In summary, the CoVoC Challenge has established a robust foundation for future studies in zero-shot TTS, promoting the development of models capable of generating natural, spontaneous speech. Continued efforts in dataset expansion, model innovation, and evaluation methodologies will further enhance the capabilities of conversational voice cloning systems."}]}