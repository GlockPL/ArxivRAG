{"title": "A Framework for testing Federated Learning algorithms using an edge-like environment", "authors": ["Felipe Machado Schwanck", "Marcos Tomazzoli Leipnitz", "Joel Lu\u00eds Carbonera", "Juliano Araujo Wickboldt"], "abstract": "Federated Learning (FL) is a machine learning paradigm in which many clients cooperatively train a single centralized model while keeping their data private and decentralized. FL is commonly used in edge computing, which involves placing computer workloads (both hardware and software) as close as possible to the edge, where the data is being created and where actions are occurring, enabling faster response times, greater data privacy, and reduced data transfer costs. However, due to the heterogeneous data distributions/contents of clients, it is non-trivial to accurately evaluate the contributions of local models in global centralized model aggregation. This is an example of a major challenge in FL, commonly known as data imbalance or class imbalance. In general, testing and assessing FL algorithms can be a very difficult and complex task due to the distributed nature of the systems. In this work, a framework is proposed and implemented to assess FL algorithms in a more easy and scalable way. This framework is evaluated over a distributed edge-like environment managed by a container orchestration platform (i.e. Kubernetes).", "sections": [{"title": "1. Introduction", "content": "Federated Learning (FL) is a machine learning solution designed to train machine learning models while keeping the data private and decentralized [1]. The main idea of FL is for each client to train its local model using its data and, afterward, upload the generated local model to a single centralized server, where the regional models of the participant clients will be aggregated and weighted to create a global model. FL is commonly used in edge computing, which involves placing computer workloads (both hardware and software) as close as possible to the edge, where the data is being created and where actions are occurring, enabling faster response times, greater data privacy, and reduced data transfer costs [2]. Thus, FL can be used with a variety of clients, such as smartphones, sensors, IoT devices, and data silos (distributed databases that need to keep their data private from the outside world). However, as seen in [3], in general, the data distribution of the mobile systems and other similar settings is imbalanced, which can increase the bias of the model and impact in a negative way its performance. Different approaches have been proposed, such as Deep Reinforcement Learning (DRL) [4], as a solution for this problem.\nAlthough regular centralized machine learning may outperform FL in prediction performance [5], the entire dataset must be shared. Its first application was in Google GBoard [6], which learns from every smartphone using Gboard without sharing user data. Since then, FL applicability has advanced to various fields such as autonomous vehicles, traffic prediction and monitoring, healthcare, telecom, IoT, pharmaceutics, industrial management, industrial IoT, and healthcare and medical AI [7]. Moreover, the number of academic publications with FL as main subject has increased significantly since its conception [1].\nThe increase in IoT devices has greatly enabled FL. The total number of IoT connections will reach 83 billion by 2024, rising from 35 billion connections in 2020, a growth of 130% over the next four years. The industrial sector has been identified as a critical driver of this growth. Expansion will be driven by the increasing use of private networks that leverage cellular network standards [8]. The evolution of the IoT device's computational power has also enabled FL; since an edge computer can process data locally, its sensors (e.g. cameras) could collect samples (e.g. images or frames) at a higher resolution and a higher frequency (such as frame rate) than would be possible if the data had to be sent to the cloud for processing [9].\nOne of the many challenges that have come up with the advance of FL is dealing with data imbalance and heterogeneity, as seen in [7]. In FL, using their local data, each edge node trains a shared model. As a result, data distribution from those edge devices is based on their many uses. Imagine a scenario, for example, of the distribution of cameras in a surveillance system. Compared to cameras located in the wild, cameras in the park, for example, capture more photographs of humans. Also, the size of the dataset that each one of those cameras will have to train their local models might differ by a large magnitude since a park might have much more data to input than a camera in the wild. Furthermore, an approach that has been proposed in many studies to dynamically address weight for the local models of clients participating in the FL global model and, therefore, deal with the heterogeneous data is DRL [10, 11, 12, 13].\nDRL has gathered much attention recently. Recent studies have shown impressive results in activities as diverse as autonomous driving [14], game playing [15], molecular recombination [16], and robotics [17]. In all those applications, it has been used in computer programs to teach them how to solve complex problems, for example, how to fly model helicopters and perform aerobatic maneuvers, and, in some applications, it has already outsmarted some of the most skilled humans, such as in Atari, Go, poker and StarCraft.\nTesting and evaluating FL algorithms can be a very difficult task to accomplish. FL intrinsically creates complex distributed systems with non-trivial interactions among its participants. Therefore, this work proposes and implements a framework for testing FL algorithms that enables users to easily create different training scenarios by simply changing configuration parameters. These include computing and data distributions, datasets, global model aggregations, local client models, and server/client training parameters. The framework is also capable of collecting and visualizing training results and resource usage metrics (e.g., CPU, memory, etc.). Experiments have been conducted over a realistic edge-like environment managed by a Kubernetes container orchestration platform, with varied parameters on top of the proposed framework to demonstrate its capabilities.\nThe remainder of this work is organized as follows. Section 2 presents a literature review for edge computing and FL, respectively. Sections 3 and 4 present the conceptual framework architecture proposed and the tools and frameworks used to implement the PoC solution. Section 5 details the PoC solution developed. Sections 6, 7, and 8 present the experimental setup, the results obtained, and how resource usage and network traffic can be monitored, respectively. Finally, Section 9 concludes the work with final remarks and a perspective on future work."}, {"title": "2. Literature Review", "content": "This section presents the main topics of this work and provides an overview of the current state of the art of edge computing, federated learning, and deep reinforcement learning."}, {"title": "2.1. Edge Computing", "content": "Data is increasingly produced at the edge of the network; therefore, processing the data at the network's edge would be more efficient. With the advancement of telecommunication services and the increase of the necessity for low-latency computing, the edge computing paradigm has been motivated. In edge computing, instead of having computer workloads (both hardware and software) centralized in a data center (cloud), we have them as close as possible to the edge, where the data is being created and where actions are occurring, thus benefiting lower latency, greater data privacy, and reduced data transfer costs. For [18], edge computing refers to the enabling technologies allowing computation to be performed at the edge of the network, on downstream data on behalf of cloud services, and upstream data on behalf of IoT services. Therefore, edge devices can be any device with Internet access, such as smartphones, smart cars, or other IoT devices.\nMulti-access Edge Computing (MEC) [19] is proposed as a critical solution that enables operators to open their networks to new services and IT ecosystems and leverage edge-cloud benefits in their networks and systems since it places storage and computation at the network edge. The proximity of the end users and connected devices provides low latency and high bandwidth while minimizing centralized cloud limitations such as delay, access bottlenecks, and single points of failure.\nMEC use cases can be seen in real-time traffic monitoring [20] and autonomous vehicles [21]. In traffic monitoring, real-time and accurate video analysis is critical and challenging work, especially in situations with complex street scenes; therefore, edge computing-based video pre-processing is proposed to eliminate the redundant frames edge devices need to process since a considerable amount of vehicle video data is generated. Also, the decentralized and highly available nature of multi-access edge computing is taken advantage of to collect, store, and analyze city traffic data in multiple sensors. For autonomous vehicles, a large amount of real-time data processing from different sensors at high speed is needed to guarantee driver safety."}, {"title": "2.2. Federated Learning", "content": "Federated learning (FL) is a distributed form of machine learning proposed by Google [1] to train models at scale while allowing the user data to be private. In Federated Averaging (FedAvg), the server aggregates the model updates using simple averaging. It returns the new model parameters to the client devices, which continue training using the updated model parameters. Google's proposal provided the first definition of federated learning, as well as the Federated Optimization [22] approach to improve these federated algorithms further. Advanced Federated Optimization [23] is a variant of the Stochastic Gradient Descent (SGD) algorithm commonly used in centralized training. In FedOpt, each local node applies SGD to its local data to compute the gradients and then sends them to a central server. The server then aggregates the gradients from all the nodes to update the global model. Adaptive Federated Optimization with Yogi (FedYogi) incorporates a momentum-based optimizer called Yogi after the central server aggregates the model updates using the FedAvg algorithm to improve the convergence rate. Federated Averaging with Momentum [24] incorporates a momentum-based optimizer, similar to the Yogi optimizer in FedYogi. The critical difference is that it uses a combination of the gradients from the current iteration and the gradients from the previous iteration to update the model parameters. This allows the optimizer to maintain a direction of movement even when the gradient changes direction, which helps to smooth out noisy gradients and accelerate convergence.\nIn the \"Federated Learning: Collaborative Machine Learning without Centralized Training Data\" blog post [25], Google explains how FL is enabling mobile phones to collaboratively learn a shared prediction model while keeping all the training data on the device, decoupling the ability to do machine learning from the need to store the data in the cloud. In addition, the current use of FL to predict keyboard words in Google's Gboard [6] and how it can be used for photo ranking and further improving language models.\nFL usually deals with data distributed across multiple devices. In such settings, data is usually non-independently and identically distributed (i.e., non-IID). One of the main challenges in FL is dealing with the heterogeneity of the data distribution among the parties since data distribution from those edge devices is based on their many uses [7].\nFurthermore, many use cases in FL have data samples distributed among multiple devices, which are not always synchronized and may have limited connectivity. Thus, it cannot train these devices in parallel and directly aggregate them, as it cannot guarantee device availability or data homogeneity. Understanding how to properly select clients and weigh each client's contributions in the global model remains an open problem in FL. An example of a practical scenario of data imbalance and heterogeneity in which DRL was used in FL as a solution can be seen in recent work proposed to deal with blade icing detection in distributed wind turbines [26]. Wind turbines closer to the sea experience windy and snowy weather, while those closer to the continent deal with windy and rainy conditions. This heterogeneity introduces a bias in the local models since one might be more susceptible to icing than another. Therefore, since our objective is to identify icing, clients who experience more icing should have a different weight assigned to their contribution to the global model than others without."}, {"title": "3. Conceptual Framework", "content": "The main objective of the proposed framework is to enable FL testing in a platform where users can easily change parameters to create different scenarios in a distributed computing environment. These include different computing and data distributions, datasets, global model aggregations, client models, and server and client training parameters. The framework should also be capable of collecting training results and resource usage metrics. Figure 1 presents an overview of the proposed framework main components in a conceptual architecture.\nThe framework is designed in these layers to have the highest level of independence within them. This enables easier development and segregation of functions, so if improvements are done in one layer, it does not affect the others. The following subsections detail each one of the components of the layers."}, {"title": "3.1. Distributed Infrastructure Layer", "content": "This bottom layer contains all the infrastructure needed to create a distributed computing environment. It is responsible for scaling computing, storage, and networking from a pool of resources to meet user input parameters. The user parameters will reflect how the resource layer will be laid out to create the scenario desired for testing. For example, let's say a user wants to configure a scenario of FL using ten clients, whereas each client has a specific computing requirement of four CPU cores and 4GB of RAM per client. The distributed infrastructure manager will then configure the resource layer from its pool of resources (computing, storage, and networking) with ten clients, each with the specified specifications to meet user requirements. Having an independent infrastructure from the resource and application layers enables the usage of different computational devices since the application only sees the resource layer. This will be fundamental to allow different scenarios where other edge computing devices can be used as clients."}, {"title": "3.2. Resource Layer", "content": "The resource layer can be found on top of the distributed infrastructure layer. As mentioned before, it will match the configuration the user sets to set the FL training scenario. We can categorize the distribution of resources into four major categories: server resources, client resources, monitoring resources, and experiment results resources. The server resource will contain all the necessary resources to run the FL server matching the user configuration, and the same can be said for the client resources. The monitoring resource will include all the resources needed to run the monitoring server, which will monitor the distributed infrastructure layer and record the usage of the resources. The experiment results resource will have all the resources necessary to run an application to display the results of the experiments run in the framework."}, {"title": "3.3. Application Layer", "content": "The application layer will run on top of the resource layer in each correspondent resource. It contains all the applications necessary to run an FL algorithm in the framework, such as the FL server application, FL client application, monitor agent application, monitor server application, and the experiment results visualization application. Subsections 3.3.1 to 3.3.4 detail each of the mentioned applications."}, {"title": "3.3.1. FL Server Application", "content": "The FL Server application can be divided into five major components that enable FL testing in a distributed environment.\nServer is responsible for communicating with the clients and controlling the entire FL learning process, which includes selecting available clients to start training; control of the number of FL server rounds and round timeout; global model parameters aggregation and distribution and retrieval of model parameters.\nModel is responsible for server-side initialization of the global model parameters since some global model aggregation strategies need it to start FL learning training and enable the usage of different models to learn weight balancing of client models to distribute parameters.\nStrategies selects and configures supported global model aggregation strategies from the user-specified configuration.\nDataset is a collection of datasets supported by the framework used by the application to handle the dataset configuration in memory and to properly obtain the raw data to create data distributions via Storage Manager.\nStorage Manager is responsible for distributing the raw data of the configured dataset throughout the distributed storage infrastructure. For example, suppose a user wants to test an FL algorithm in a dataset using an unbalanced non-iid data distribution with ten clients. The storage manager will separate the data into ten unbalanced and biased non-iid data parts. The storage manager of the server application is also responsible for managing where each experiment will write its data and where the client will output their results data in the system."}, {"title": "3.3.2. FL Client Application", "content": "The FL client application can be divided into three major components.\nClient: is responsible for the training and testing algorithms run on the client side and the connection with the server.\nModel: is responsible for selecting the desired model configured by the user to be used by the client to train it.\nStorage Manager: is responsible for loading the distributed data for training in the client and handling the experiments and test results of the client in the storage."}, {"title": "3.3.3. Monitoring Application", "content": "The Monitoring Server Application is responsible for gathering resource usage data of the distributed infrastructure and storing it for further visualization and analysis by the Monitoring Visualization Application. Each Monitoring Agent is responsible for gathering local resource usage data from the distributed resources and sending the metrics to the Monitoring Server Application."}, {"title": "3.3.4. Experiment Visualization Application", "content": "This application is responsible for querying data from already run experiments and enabling the user to visualize the data for each step of the FL training experiment in customizable dashboards."}, {"title": "4. Tools and Frameworks", "content": "This section provides an overview of the tools and frameworks considered to implement the conceptual framework described in Section 3."}, {"title": "4.1. Orchestration, Deployment, and Building", "content": "This section presents tools related to orchestration, deployment, and building of containers."}, {"title": "4.1.1. Docker", "content": "Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security allow many containers to run simultaneously on a host. Containers provide a robust solution for bundling software and its dependencies into a transportable unit that can run seamlessly across diverse computing environments. A containerized application encapsulates all its libraries, settings, and tools within an isolated environment. Developers can avoid the arduous task of addressing compatibility issues with varied hardware and software and concentrate solely on the application's functionality. This inherent portability of containers eradicates the need to reconfigure applications for distinct environments, ensuring uniformity and dependability."}, {"title": "4.1.2. Kubernetes", "content": "Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, which facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. A Kubernetes cluster consists of a set of worker machines, called nodes, which run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods, which are the components of the application workload. Heterogeneous edge computing devices can be easily integrated and managed by Kubernetes as worker nodes. This also facilitates the deployment of different FL algorithms and the creation of varying testing scenarios in a distributed infrastructure environment."}, {"title": "4.2. Libraries and Frameworks", "content": "This section approaches the utilized libraries and frameworks to run federated learning (FL)."}, {"title": "4.2.1. PyTorch", "content": "PyTorch is a popular open-source machine learning library that was created by Meta's AI research team. It is used to develop and train deep learning models and is written in Python, which makes it easy to use and integrate with other Python libraries. PyTorch was the main library for training and testing the FL models used in this work."}, {"title": "4.2.2. Poetry", "content": "Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on, and it will manage (install/update) them for you. Poetry offers a lock file to ensure repeatable installs and can build your project for distribution. It was used in this work to create the packages of the FL client and server applications."}, {"title": "4.2.3. Flower", "content": "As discussed in this work, the concept of federated learning emerged in response to the need to leverage data from multiple devices while ensuring its privacy. However, federated learning introduces two additional challenges not present in traditional machine learning: scaling to various clients and dealing with data heterogeneity.\nTo address these challenges, as proposed in [27], Flower (flwr) has been developed as an open-source framework for building federated learning systems. Flower provides two primary interfaces: the client and the server. These interfaces enable the decentralization of standard centralized machine learning solutions by implementing the necessary methods, making building and deploying federated learning systems easier. In Flower's architecture each edge device in the training process runs a Flower client containing a local machine-learning model. Flower provides a transparent connection via the Edge Client Proxy using an RPC protocol such as gRPC to ensure connectivity between the clients and the server."}, {"title": "4.3. Monitoring and Visualization", "content": "Finally, monitoring, storage, and data visualization tools are presented in this section."}, {"title": "4.3.1. Prometheus", "content": "Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. Since its inception in 2012, many companies and organizations have adopted this tool, and the project has a very active developer and user community. It is now a standalone open-source project maintained independently of any company. Prometheus was used as the primary application for monitoring the usage of resources in the PoC solution of this work."}, {"title": "4.3.2. Rook Ceph", "content": "Rook Ceph is a storage solution that combines the capabilities of the Rook storage orchestrator with the Ceph distributed storage system. Rook is an open-source tool for managing storage systems on Kubernetes, while Ceph is a distributed object and file storage system that provides scalability, reliability, and performance. Rook Ceph was the persistent storage for the client and server application."}, {"title": "4.3.3. Grafana", "content": "Grafana is a data visualization tool commonly used with Prometheus that allows you to query, visualize, alert on, and understand your metrics. It was used as the primary tool to visualize Prometheus's resource usage."}, {"title": "5. Proof-of-Concept Implementation", "content": "Previous sections detailed the conceptual framework proposed and the tools considered for its implementation. This section details the current PoC implemented to run fully functional FL scenarios in a distributed infrastructure with multiple clients and different data distributions. The source code of the implementations of the software components described in this section, as well as configuration files used to allow the deployment and execution of the experiments discussed in the following sections, are available at our GitHub repository.\nThe PoC consists of an end-to-end edge-like environment solution using edge computing devices orchestrated by Kubernetes to run the FL applications, the server and the clients, and the applications for monitoring resource usage. Figure 2 shows the complete diagram of the PoC solution implemented. The distributed infrastructure layer comprises the Kubernetes cluster deployed at the Institute of Informatics, which serves as the foundation for each resource. The application layer can be visualized on top of the resources as containerized Docker images of each application running in the cluster pods. Further sections will go through more details about the layers, including how they were assembled with the tools and frameworks presented in Section 4, the underlying infrastructure of the Kubernetes cluster, and how the components of the PoC work."}, {"title": "5.1. Distributed Infrastructure Layer", "content": "The Kubernetes cluster at the Institute of Informatics includes a varied number of computing resources, with 352 CPUs, 544 GiB of RAM, and 6.3 TiB of disk space spread across 43 distinct nodes, as indicated in Table 1. These nodes are conveniently categorized into three classifications: \"computer\", \"edge\" and \"server\" enabling experimentation with pods that are constrained to specific machines with hardware that closely matches the real-life devices being simulated. For instance, the \"edge\" label encompasses a total of twenty-three Raspberry Pi devices (three Raspberry Pi 4s and twenty Raspberry Pi 3s) with less powerful hardware specifications relative to the \"computer\" and \"server\" machines.\nA network switch interconnects the cluster nodes and has distributed storage configured using Rook Ceph Filesystem (subsection 4.3.2) to build a layer on top of the storage resources. This enables the mounting of Persistent Volumes (PV) from the storage pool via a Persistent Storage Claim (PVC) into the container images that can be shared between applications to enable data distribution by the server and saving experiment results from each client. When a pod requests storage resources, it creates a PVC that specifies the amount of storage required and any other requirements, such as access mode and storage class. The Kubernetes scheduler then looks for an available PV that matches the requirements specified in the PVC. If a matching PV is found, it is bound to the PVC, and the pod can use the storage resource provided by the PV. The advantage of using PVs and PVCs is that they provide a level of abstraction between the pod and the underlying storage infrastructure. This allows pods to request storage resources without having to know the details of the underlying storage infrastructure."}, {"title": "5.2. Resource Layer", "content": "Kubernetes utilizes labels to organize objects. They are key-value pairs that can be attached to Kubernetes objects such as pods, services, nodes, and deployments and can be used to identify and group related objects. This is a powerful mechanism for selecting and manipulating subsets of objects based on specific criteria and can also be used to manage nodes in a Kubernetes cluster. Nodes are the worker machines that run containerized applications and services in a Kubernetes cluster. By attaching labels to nodes, we can assign specific roles or attributes to them and use those labels to manage and schedule workloads on those nodes.\nFor example, one can label nodes based on their hardware characteristics, such as CPU or memory capacity, and then use those labels to schedule workloads that require specific hardware requirements. The Kubernetes Cluster of the Institute of Informatics uses the \"node-type\" label to identify if a node is a \"computer\", \"server\" or \"edge\" type of computational device.\nFinally, in the PoC solution, the resource layer used is a total reflex of the configuration set in the Kubernetes deployment file. Labels were used to assign where each containerized application would run and how many node resources would be available for them to use."}, {"title": "5.3. Application Layer", "content": "The application layer of the PoC solution is composed of all the Docker images built by the author, such as the FL server, client, and experiment results images, which contain all the implemented logic for FL algorithm testing. For monitoring purposes, Prometheus monitoring installed in the Kubernetes cluster was used to capture resource usage by the applications, and Grafana was used to visualize the data in charts.\nFigure 3 shows how an experiment starts from the beginning to the end in the PoC solution from a high-level overview. In the blue boxes, we can see the actions of the Kubernetes cluster, the yellow ones from the FL Server application, and the orange ones from the FL clients. From a high-level perspective, the user configures a scenario in Lens for deployment, and Kubernetes scales the necessary resources and deploys the applications. Afterward, the FL server handles the data distribution of the dataset and starts the FL training algorithm. The client waits for the server to connect with it, runs local training rounds, and returns the model parameters to the server. The server receives those parameters, aggregates them using the configured aggregation method by the user in the experiment layout, and distributes them back to the clients, who will start local training rounds again with the new parameters. After all server rounds are done, the FL server will save the results in the correct output folder of the experiment, and the experiment will be over.\nThe GitHub repository used in the development of the solution was organized into isolated packages containing all necessary code, data, or declarations of each component to maintain independence and enable better re-usability. The FL server and client applications were also developed to run in bare-metal environments. In addition, to allow easy application testing during development, you can run a docker-compose environment to deploy the application locally using the built-in docker images. Further subsections will detail each one of the applications and their role in the PoC solution."}, {"title": "5.3.1. Server Application", "content": "The server is a containerized Python application with the FlowerML server framework as a dependency. The server storage manager is responsible for initializing the experiment root path in the distributed storage, where the results and logs of the currently deployed run of each client will be saved. It will also be responsible for receiving the models trained by the clients, averaging the received parameters using the selected strategy, and then updating the clients' models with the averaged parameters. The connection is done to the clients through a gRPC connection with SSL encryption. The algorithm is outlined in Algorithm 1.\nThe server address, number of server rounds, dataset, data distribution, global aggregation strategy, client local rounds, and minimum number of connected clients are parameterized for the server application and can be changed in each deployment. From the folder structure and source code files available at our GitHub repository, it is easy to correlate each part of the application with the conceptual framework since each source code file encapsulates its corresponding component, as we explain in the following.\ndataset.py contains all the implemented classes of the supported datasets by the framework used by the application to handle the dataset configuration in memory and how to obtain the raw data of the dataset properly. At the time of this work, the CIFAR-10, CIFAR-100, and FMNIST datasets were implemented. Further development of this class can enable any dataset to be compatible with the framework. Section 6.1 will detail each one of the currently supported datasets.\nmain.py contains the main program loop and the server component of the application, which is responsible for communicating with the clients and controlling the entire FL learning process, which includes selecting available clients to start training; control of the number of FL server rounds and round timeout; global model parameters aggregation and distribution and retrieval of model parameters. All the implemented packages are included here, and the central server program runs as demonstrated in Algorithm 1.\nmodel.py contains the supported models and is responsible for server-side initialization of global model parameters since some global model aggregation strategies need it. At the time of this work, the current used models for testing are a simple CNN, googlenet [28] and resnet [29]. These models were chosen due to the simplicity and popularity. Further development of classes can enable any models to be compatible with the framework.\nstorage.py contains the storage manager class, which is responsible for distributing the raw data of the configured dataset throughout the distributed infrastructure storage to achieve user requirements for testing. The storage manager of the server application is also responsible for managing where each experiment will write its data and where the client will output their results data in the system. At the time of this work, the storage manager can create data distributions by changing the following parameters:\nBalance boolean parameter specifies if the data should be balanced between clients. If true, the dataset is balanced, meaning the data batches have the same number of samples. Otherwise, if false, the number of data samples differs between them.\nNon-IID boolean parameter specifies whether the data should be Non-IID or IID between clients. If true, the dataset is Non-IID, meaning there is a class imbalance between the clients. Otherwise, if false, the dataset has a balanced class distribution.\nDistribution this parameter specifies how data should be distributed regarding the dataset classes if $Non \u2013 IID = true$. If set to pat, it will generate a pathological scenario of class imbalance, whereas each client will have only a subset of classes. If set to dir:, it will use a heterogeneous unbalanced Dirichlet distribution of the classes, similar as seen in [30], an a parameter can modify that to change the distribution aspects. Further development of this class can enable more data distributions to be compatible with the framework.\nFurther development of the storage manager class can enable more types of data distributions to be compatible with the framework.\nstrategies.py contains the strategy for supported global model aggregation strategies from the user-specified configuration. At the time of this work, the supported strategies implemented are FedAvg, FedAvgM, FedYogi, and FedOpt. Further development of this class can enable more strategies to be compatible with the framework."}, {"title": "5.3.2. Client Application", "content": "The client waits for the server storage manager to initialize the experiment path in the distributed storage, connects to the server through a gRPC connection encrypted with SSL, and performs the pre-defined number of epochs received from the server, which is the number of times that the data set passes through the neural network. When multiple clients are running, an individual client is oblivious to the existence of the other clients - it can only communicate with the server. The algorithm for the client application can be seen in Algorithm 2.\nThe client will only upload the model when the server requests the models, and the loop will end when the server finishes its rounds. From our folder structure at GitHub, it is easy to correlate each part of the application with the conceptual framework since each source code file encapsulates its corresponding component. The client's models and the address they will connect are parameterized for the client application and can be changed in each deployment.\nclient.py contains the client classes with the utilized training and testing algorithms. At the time of this work, it would only support one PyTorch training algorithm.\nmain.py contains the client application main loop described in Algorithm 2. It is responsible for initializing the environment variables used to set parameters, such as the local model utilized for training, and common variables, such as the experiment path, to save results.\nmodel.py contains all the supported models described in the server application section.\nstorage.py contains the storage manager class of the client, which is responsible for loading the distributed data batch for training and testing the distributed storage and saving the experiments test results of the client in the storage."}, {"title": "5.3.3. Monitoring Application", "content": "Prometheus uses the Kubernetes API to discover the various resources it needs to monitor in the cluster, such as pods, services, deployments, nodes, and more. It does this by querying the Kubernetes API server for information about the desired resources and then collecting metrics data from these resources.\nFor example, to monitor a Kubernetes pod, Prometheus will query the Kubernetes API server to get information about the pod's name, namespace, labels, and other metadata. Prometheus will then use this information to collect metrics data from the pod, such as CPU and memory usage, network traffic, and other metrics. Grafana queries the data from the Prometheus database to enable real-time visualization of resource usage in dashboards. Some sample charts of these dashboards are presented in Section 8."}, {"title": "5.3.4. Experiments Results Application", "content": "To retrieve the experiment results from the Kubernetes cluster, a container running an Ubuntu base image from Docker was used to mount the used PVs and access the results. Since the results are stored in files, we can copy them to the local machine to read and interpret them. Figure 4 illustrates how the experiments that run in the PoC are saved in the distributed storage.\n.temp contains the partial results while running the application. This directory is deleted after the server application storage manager has moved the finalized run into its correct directory.\ndata contains each client's training and testing data batches used in the experiment.\nruns contains the results for each experiment run. The logs subfolder of a run holds all the logs from the server and client applications, and the results subfolder contains the evaluation matrix for each local epoch run in the clients. A copy of the configuration used in the experiment run is saved to enable visualization of how the data was distributed in each client, how the classes were distributed inside each data batch, and which model and global aggregation strategy was used.\nconfig.json contains the last configuration used in the experiment. This enables testing of the same data distribution using several parameters, strategies, and models."}, {"title": "6. Experimental Setup", "content": "This section presents the experimental setup used to evaluate the solution proposed in Section 3. Three experiments were conducted to demonstrate the framework's capabilities. The adjusted parameters are the dataset, global model aggregation, local client model, client epochs, and server rounds. Each experiment was run with ten clients - named client-0 through client-9 and taken from the set of computational devices labeled computer in the cluster - running with different data distributions. Also, in the Flower framework, the fraction fit was set to 1, and the minimum available clients was set to 10 to ensure running every server round with all the clients participating. The specific clients involved in a given experiment may vary from one run to another, showing how the framework can help reproduce real-life scenarios effectively."}, {"title": "6.1. Datasets", "content": "The datasets considered for the experiments were the following:\nCIFAR-10 Consists of 60,000 32x32 color images divided into ten classes, with 6"}]}