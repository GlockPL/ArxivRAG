{"title": "A Framework for testing Federated Learning algorithms using an edge-like environment", "authors": ["Felipe Machado Schwanck", "Marcos Tomazzoli Leipnitz", "Joel Lu\u00eds Carbonera", "Juliano Araujo Wickboldt"], "abstract": "Federated Learning (FL) is a machine learning paradigm in which many clients cooperatively train a single centralized model\nwhile keeping their data private and decentralized. FL is commonly used in edge computing, which involves placing computer\nworkloads (both hardware and software) as close as possible to the edge, where the data is being created and where actions are\noccurring, enabling faster response times, greater data privacy, and reduced data transfer costs. However, due to the heterogeneous\ndata distributions/contents of clients, it is non-trivial to accurately evaluate the contributions of local models in global centralized\nmodel aggregation. This is an example of a major challenge in FL, commonly known as data imbalance or class imbalance. In\ngeneral, testing and assessing FL algorithms can be a very difficult and complex task due to the distributed nature of the systems.\nIn this work, a framework is proposed and implemented to assess FL algorithms in a more easy and scalable way. This framework\nis evaluated over a distributed edge-like environment managed by a container orchestration platform (i.e. Kubernetes).", "sections": [{"title": "1. Introduction", "content": "Federated Learning (FL) is a machine learning solution de-\nsigned to train machine learning models while keeping the data\nprivate and decentralized [1]. The main idea of FL is for each\nclient to train its local model using its data and, afterward, up-\nload the generated local model to a single centralized server,\nwhere the regional models of the participant clients will be ag-\ngregated and weighted to create a global model. FL is com-\nmonly used in edge computing, which involves placing com-\nputer workloads (both hardware and software) as close as pos-\nsible to the edge, where the data is being created and where\nactions are occurring, enabling faster response times, greater\ndata privacy, and reduced data transfer costs [2]. Thus, FL can\nbe used with a variety of clients, such as smartphones, sensors,\nIoT devices, and data silos (distributed databases that need to\nkeep their data private from the outside world). However, as\nseen in [3], in general, the data distribution of the mobile sys-\ntems and other similar settings is imbalanced, which can in-\ncrease the bias of the model and impact in a negative way its\nperformance. Different approaches have been proposed, such\nas Deep Reinforcement Learning (DRL) [4], as a solution for\nthis problem.\nAlthough regular centralized machine learning may outper-\nform FL in prediction performance [5], the entire dataset must\nbe shared. Its first application was in Google GBoard [6], which\nlearns from every smartphone using Gboard without sharing\nuser data. Since then, FL applicability has advanced to vari-\nous fields such as autonomous vehicles, traffic prediction and\nmonitoring, healthcare, telecom, IoT, pharmaceutics, industrial\nmanagement, industrial IoT, and healthcare and medical AI [7].\nMoreover, the number of academic publications with FL as\nmain subject has increased significantly since its conception [1].\nThe increase in IoT devices has greatly enabled FL. The to-\ntal number of IoT connections will reach 83 billion by 2024,\nrising from 35 billion connections in 2020, a growth of 130%\nover the next four years. The industrial sector has been identi-\nfied as a critical driver of this growth. Expansion will be driven\nby the increasing use of private networks that leverage cellular\nnetwork standards [8]. The evolution of the IoT device's com-\nputational power has also enabled FL; since an edge computer\ncan process data locally, its sensors (e.g. cameras) could collect\nsamples (e.g. images or frames) at a higher resolution and a\nhigher frequency (such as frame rate) than would be possible if\nthe data had to be sent to the cloud for processing [9].\nOne of the many challenges that have come up with the ad-\nvance of FL is dealing with data imbalance and heterogeneity,\nas seen in [7]. In FL, using their local data, each edge node\ntrains a shared model. As a result, data distribution from those\nedge devices is based on their many uses. Imagine a scenario,\nfor example, of the distribution of cameras in a surveillance sys-\ntem. Compared to cameras located in the wild, cameras in the\npark, for example, capture more photographs of humans. Also,\nthe size of the dataset that each one of those cameras will have\nto train their local models might differ by a large magnitude\nsince a park might have much more data to input than a camera\nin the wild. Furthermore, an approach that has been proposed in\nmany studies to dynamically address weight for the local mod-\nels of clients participating in the FL global model and, there-\nfore, deal with the heterogeneous data is DRL [10, 11, 12, 13].\nDRL has gathered much attention recently. Recent stud-\nies have shown impressive results in activities as diverse as au-\ntonomous driving [14], game playing [15], molecular recombi-\nnation [16], and robotics [17]. In all those applications, it has\nbeen used in computer programs to teach them how to solve\ncomplex problems, for example, how to fly model helicopters"}, {"title": "2. Literature Review", "content": "This section presents the main topics of this work and pro-\nvides an overview of the current state of the art of edge com-\nputing, federated learning, and deep reinforcement learning."}, {"title": "2.1. Edge Computing", "content": "Data is increasingly produced at the edge of the network;\ntherefore, processing the data at the network's edge would be\nmore efficient. With the advancement of telecommunication\nservices and the increase of the necessity for low-latency com-\nputing, the edge computing paradigm has been motivated. In\nedge computing, instead of having computer workloads (both\nhardware and software) centralized in a data center (cloud), we\nhave them as close as possible to the edge, where the data is\nbeing created and where actions are occurring, thus benefiting\nlower latency, greater data privacy, and reduced data transfer\ncosts. For [18], edge computing refers to the enabling tech-\nnologies allowing computation to be performed at the edge of\nthe network, on downstream data on behalf of cloud services,\nand upstream data on behalf of IoT services. Therefore, edge\ndevices can be any device with Internet access, such as smart-\nphones, smart cars, or other IoT devices.\nMulti-access Edge Computing (MEC) [19] is proposed as a\ncritical solution that enables operators to open their networks\nto new services and IT ecosystems and leverage edge-cloud\nbenefits in their networks and systems since it places storage\nand computation at the network edge. The proximity of the\nend users and connected devices provides low latency and high\nbandwidth while minimizing centralized cloud limitations such\nas delay, access bottlenecks, and single points of failure.\nMEC use cases can be seen in real-time traffic monitor-\ning [20] and autonomous vehicles [21]. In traffic monitoring,\nreal-time and accurate video analysis is critical and challeng-\ning work, especially in situations with complex street scenes;\ntherefore, edge computing-based video pre-processing is pro-\nposed to eliminate the redundant frames edge devices need to\nprocess since a considerable amount of vehicle video data is\ngenerated. Also, the decentralized and highly available nature\nof multi-access edge computing is taken advantage of to col-\nlect, store, and analyze city traffic data in multiple sensors. For\nautonomous vehicles, a large amount of real-time data process-\ning from different sensors at high speed is needed to guarantee\ndriver safety."}, {"title": "2.2. Federated Learning", "content": "Federated learning (FL) is a distributed form of machine\nlearning proposed by Google [1] to train models at scale while\nallowing the user data to be private. In Federated Averaging\n(FedAvg), the server aggregates the model updates using sim-\nple averaging. It returns the new model parameters to the client\ndevices, which continue training using the updated model pa-\nrameters. Google's proposal provided the first definition of\nfederated learning, as well as the Federated Optimization [22]\napproach to improve these federated algorithms further. Ad-\nvanced Federated Optimization [23] is a variant of the Stochas-\ntic Gradient Descent (SGD) algorithm commonly used in cen-\ntralized training. In FedOpt, each local node applies SGD to\nits local data to compute the gradients and then sends them to\na central server. The server then aggregates the gradients from\nall the nodes to update the global model. Adaptive Federated\nOptimization with Yogi (FedYogi) incorporates a momentum-\nbased optimizer called Yogi after the central server aggregates\nthe model updates using the FedAvg algorithm to improve the\nconvergence rate. Federated Averaging with Momentum [24]\nincorporates a momentum-based optimizer, similar to the Yogi\noptimizer in FedYogi. The critical difference is that it uses\na combination of the gradients from the current iteration and\nthe gradients from the previous iteration to update the model\nparameters. This allows the optimizer to maintain a direction\nof movement even when the gradient changes direction, which\nhelps to smooth out noisy gradients and accelerate convergence.\nIn the \"Federated Learning: Collaborative Machine Learning\nwithout Centralized Training Data\" blog post [25], Google ex-\nplains how FL is enabling mobile phones to collaboratively\nlearn a shared prediction model while keeping all the training\ndata on the device, decoupling the ability to do machine learn-\ning from the need to store the data in the cloud. In addition,\nthe current use of FL to predict keyboard words in Google's\nGboard [6] and how it can be used for photo ranking and fur-\nther improving language models.\nFL usually deals with data distributed across multiple de-\nvices. In such settings, data is usually non-independently and"}, {"title": "3. Conceptual Framework", "content": "The main objective of the proposed framework is to enable\nFL testing in a platform where users can easily change parame-\nters to create different scenarios in a distributed computing envi-\nronment. These include different computing and data distribu-\ntions, datasets, global model aggregations, client models, and\nserver and client training parameters. The framework should\nalso be capable of collecting training results and resource usage\nmetrics. Figure 1 presents an overview of the proposed frame-\nwork main components in a conceptual architecture.\nThe framework is designed in these layers to have the high-\nest level of independence within them. This enables easier de-\nvelopment and segregation of functions, so if improvements are\ndone in one layer, it does not affect the others. The following\nsubsections detail each one of the components of the layers."}, {"title": "3.1. Distributed Infrastructure Layer", "content": "This bottom layer contains all the infrastructure needed to\ncreate a distributed computing environment. It is responsible\nfor scaling computing, storage, and networking from a pool of\nresources to meet user input parameters. The user parameters\nwill reflect how the resource layer will be laid out to create\nthe scenario desired for testing. For example, let's say a user\nwants to configure a scenario of FL using ten clients, whereas\neach client has a specific computing requirement of four CPU\ncores and 4GB of RAM per client. The distributed infrastruc-\nture manager will then configure the resource layer from its\npool of resources (computing, storage, and networking) with\nten clients, each with the specified specifications to meet user\nrequirements. Having an independent infrastructure from the\nresource and application layers enables the usage of different\ncomputational devices since the application only sees the re-\nsource layer. This will be fundamental to allow different scenar-\nios where other edge computing devices can be used as clients."}, {"title": "3.2. Resource Layer", "content": "The resource layer can be found on top of the distributed\ninfrastructure layer. As mentioned before, it will match the\nconfiguration the user sets to set the FL training scenario. We\ncan categorize the distribution of resources into four major cate-\ngories: server resources, client resources, monitoring resources,\nand experiment results resources. The server resource will con-\ntain all the necessary resources to run the FL server matching\nthe user configuration, and the same can be said for the client re-\nsources. The monitoring resource will include all the resources\nneeded to run the monitoring server, which will monitor the\ndistributed infrastructure layer and record the usage of the re-\nsources. The experiment results resource will have all the re-\nsources necessary to run an application to display the results of\nthe experiments run in the framework."}, {"title": "3.3. Application Layer", "content": "The application layer will run on top of the resource layer\nin each correspondent resource. It contains all the applications\nnecessary to run an FL algorithm in the framework, such as the\nFL server application, FL client application, monitor agent ap-\nplication, monitor server application, and the experiment results\nvisualization application. Subsections 3.3.1 to 3.3.4 detail each\nof the mentioned applications."}, {"title": "3.3.1. FL Server Application", "content": "The FL Server application can be divided into five major\ncomponents that enable FL testing in a distributed environment.\nServer is responsible for communicating with the clients and\ncontrolling the entire FL learning process, which includes\nselecting available clients to start training; control of the\nnumber of FL server rounds and round timeout; global model\nparameters aggregation and distribution and retrieval of model\nparameters.\nModel is responsible for server-side initialization of the global\nmodel parameters since some global model aggregation strate-\ngies need it to start FL learning training and enable the us-\nage of different models to learn weight balancing of client\nmodels to distribute parameters.\nStrategies selects and configures supported global model ag-\ngregation strategies from the user-specified configuration.\nDataset is a collection of datasets supported by the framework\nused by the application to handle the dataset configuration\nin memory and to properly obtain the raw data to create data\ndistributions via Storage Manager.\nStorage Manager is responsible for distributing the raw data\nof the configured dataset throughout the distributed storage\ninfrastructure. For example, suppose a user wants to test\nan FL algorithm in a dataset using an unbalanced non-iid"}, {"title": "3.3.2. FL Client Application", "content": "The FL client application can be divided into three major\ncomponents.\nClient: is responsible for the training and testing algorithms\nrun on the client side and the connection with the server.\nModel: is responsible for selecting the desired model con-\nfigured by the user to be used by the client to train it.\nStorage Manager: is responsible for loading the distributed\ndata for training in the client and handling the experiments and\ntest results of the client in the storage."}, {"title": "3.3.3. Monitoring Application", "content": "The Monitoring Server Application is responsible for gath-\nering resource usage data of the distributed infrastructure and\nstoring it for further visualization and analysis by the Monitor-\ning Visualization Application. Each Monitoring Agent is re-\nsponsible for gathering local resource usage data from the dis-\ntributed resources and sending the metrics to the Monitoring\nServer Application."}, {"title": "3.3.4. Experiment Visualization Application", "content": "This application is responsible for querying data from al-\nready run experiments and enabling the user to visualize the\ndata for each step of the FL training experiment in customiz-\nable dashboards."}, {"title": "4. Tools and Frameworks", "content": "This section provides an overview of the tools and frame-\nworks considered to implement the conceptual framework de-\nscribed in Section 3."}, {"title": "4.1. Orchestration, Deployment, and Building", "content": "This section presents tools related to orchestration, deploy-\nment, and building of containers."}, {"title": "4.1.1. Docker", "content": "Docker provides the ability to package and run an applica-\ntion in a loosely isolated environment called a container. The\nisolation and security allow many containers to run simulta-\nneously on a host. Containers provide a robust solution for\nbundling software and its dependencies into a transportable unit\nthat can run seamlessly across diverse computing environments.\nA containerized application encapsulates all its libraries, set-\ntings, and tools within an isolated environment. Developers can\navoid the arduous task of addressing compatibility issues with\nvaried hardware and software and concentrate solely on the ap-\nplication's functionality. This inherent portability of containers\neradicates the need to reconfigure applications for distinct envi-\nronments, ensuring uniformity and dependability."}, {"title": "4.1.2. Kubernetes", "content": "Kubernetes is a portable, extensible, open-source platform\nfor managing containerized workloads and services, which fa-\ncilitates both declarative configuration and automation. It has\na large, rapidly growing ecosystem. Kubernetes services, sup-\nport, and tools are widely available. A Kubernetes cluster con-\nsists of a set of worker machines, called nodes, which run con-\ntainerized applications. Every cluster has at least one worker\nnode. The worker node(s) host the Pods, which are the compo-\nnents of the application workload. Heterogeneous edge com-\nputing devices can be easily integrated and managed by Ku-\nbernetes as worker nodes. This also facilitates the deployment\nof different FL algorithms and the creation of varying testing\nscenarios in a distributed infrastructure environment."}, {"title": "4.2. Libraries and Frameworks", "content": "This section approaches the utilized libraries and frame-\nworks to run federated learning (FL)."}, {"title": "4.2.1. PyTorch", "content": "PyTorch is a popular open-source machine learning library\nthat was created by Meta's AI research team. It is used to de-\nvelop and train deep learning models and is written in Python,\nwhich makes it easy to use and integrate with other Python li-\nbraries. PyTorch was the main library for training and testing\nthe FL models used in this work."}, {"title": "4.2.2. Poetry", "content": "Poetry is a tool for dependency management and packag-\ning in Python. It allows you to declare the libraries your project\ndepends on, and it will manage (install/update) them for you.\nPoetry offers a lock file to ensure repeatable installs and can\nbuild your project for distribution. It was used in this work to\ncreate the packages of the FL client and server applications."}, {"title": "4.2.3. Flower", "content": "As discussed in this work, the concept of federated learning\nemerged in response to the need to leverage data from multiple\ndevices while ensuring its privacy. However, federated learning\nintroduces two additional challenges not present in traditional\nmachine learning: scaling to various clients and dealing with\ndata heterogeneity.\nTo address these challenges, as proposed in [27], Flower\n(flwr) has been developed as an open-source framework for\nbuilding federated learning systems. Flower provides two pri-\nmary interfaces: the client and the server. These interfaces\nenable the decentralization of standard centralized machine learn-\ning solutions by implementing the necessary methods, making\nbuilding and deploying federated learning systems easier. In\nFlower's architecture each edge device in the training process\nruns a Flower client containing a local machine-learning model.\nFlower provides a transparent connection via the Edge Client\nProxy using an RPC protocol such as gRPC to ensure connec-\ntivity between the clients and the server."}, {"title": "4.3. Monitoring and Visualization", "content": "Finally, monitoring, storage, and data visualization tools are\npresented in this section."}, {"title": "4.3.1. Prometheus", "content": "Prometheus is an open-source systems monitoring and alert-\ning toolkit originally built at SoundCloud. Since its inception\nin 2012, many companies and organizations have adopted this\ntool, and the project has a very active developer and user com-\nmunity. It is now a standalone open-source project maintained\nindependently of any company. Prometheus was used as the\nprimary application for monitoring the usage of resources in\nthe PoC solution of this work."}, {"title": "4.3.2. Rook Ceph", "content": "Rook Ceph is a storage solution that combines the capabil-\nities of the Rook storage orchestrator with the Ceph distributed\nstorage system. Rook is an open-source tool for managing stor-\nage systems on Kubernetes, while Ceph is a distributed object\nand file storage system that provides scalability, reliability, and\nperformance. Rook Ceph was the persistent storage for the\nclient and server application."}, {"title": "4.3.3. Grafana", "content": "Grafana is a data visualization tool commonly used with\nPrometheus that allows you to query, visualize, alert on, and\nunderstand your metrics. It was used as the primary tool to\nvisualize Prometheus's resource usage."}, {"title": "5. Proof-of-Concept Implementation", "content": "Previous sections detailed the conceptual framework pro-\nposed and the tools considered for its implementation. This\nsection details the current PoC implemented to run fully func-\ntional FL scenarios in a distributed infrastructure with multiple\nclients and different data distributions. The source code of the\nimplementations of the software components described in this\nsection, as well as configuration files used to allow the deploy-\nment and execution of the experiments discussed in the follow-\ning sections, are available at our GitHub repository.\nThe PoC consists of an end-to-end edge-like environment\nsolution using edge computing devices orchestrated by Kuber-\nnetes to run the FL applications, the server and the clients, and\nthe applications for monitoring resource usage. Figure 2 shows\nthe complete diagram of the PoC solution implemented. The\ndistributed infrastructure layer comprises the Kubernetes clus-\nter deployed at the Institute of Informatics, which serves as the"}, {"title": "5.1. Distributed Infrastructure Layer", "content": "The Kubernetes cluster at the Institute of Informatics in-\ncludes a varied number of computing resources, with 352 CPUs,\n544 GiB of RAM, and 6.3 TiB of disk space spread across\n43 distinct nodes, as indicated in Table 1. These nodes are\nconveniently categorized into three classifications: \"computer\",\n\"edge\" and \"server\" enabling experimentation with pods that\nare constrained to specific machines with hardware that closely\nmatches the real-life devices being simulated. For instance, the\n\"edge\" label encompasses a total of twenty-three Raspberry Pi\ndevices (three Raspberry Pi 4s and twenty Raspberry Pi 3s) with\nless powerful hardware specifications relative to the \"computer\"\nand \"server\" machines.\nA network switch interconnects the cluster nodes and has\ndistributed storage configured using Rook Ceph Filesystem (sub-\nsection 4.3.2) to build a layer on top of the storage resources.\nThis enables the mounting of Persistent Volumes (PV) from the\nstorage pool via a Persistent Storage Claim (PVC) into the con-\ntainer images that can be shared between applications to en-\nable data distribution by the server and saving experiment re-\nsults from each client. When a pod requests storage resources,\nit creates a PVC that specifies the amount of storage required\nand any other requirements, such as access mode and storage\nclass. The Kubernetes scheduler then looks for an available\nPV that matches the requirements specified in the PVC. If a\nmatching PV is found, it is bound to the PVC, and the pod can\nuse the storage resource provided by the PV. The advantage of\nusing PVs and PVCs is that they provide a level of abstrac-\ntion between the pod and the underlying storage infrastructure.\nThis allows pods to request storage resources without having to\nknow the details of the underlying storage infrastructure."}, {"title": "5.2. Resource Layer", "content": "Kubernetes utilizes labels to organize objects. They are key-value pairs that can be attached to Kubernetes objects such as\npods, services, nodes, and deployments and can be used to iden-\ntify and group related objects. This is a powerful mechanism for\nselecting and manipulating subsets of objects based on specific\ncriteria and can also be used to manage nodes in a Kubernetes\ncluster. Nodes are the worker machines that run containerized\napplications and services in a Kubernetes cluster. By attach-\ning labels to nodes, we can assign specific roles or attributes to\nthem and use those labels to manage and schedule workloads\non those nodes.\nFor example, one can label nodes based on their hardware\ncharacteristics, such as CPU or memory capacity, and then use\nthose labels to schedule workloads that require specific hard-\nware requirements. The Kubernetes Cluster of the Institute of\nInformatics uses the \"node-type\" label to identify if a node is a\n\"computer\", \"server\" or \"edge\" type of computational device.\nFinally, in the PoC solution, the resource layer used is a to-\ntal reflex of the configuration set in the Kubernetes deployment\nfile. Labels were used to assign where each containerized ap-\nplication would run and how many node resources would be\navailable for them to use."}, {"title": "5.3. Application Layer", "content": "The application layer of the PoC solution is composed of\nall the Docker images built by the author, such as the FL server,\nclient, and experiment results images, which contain all the im-\nplemented logic for FL algorithm testing. For monitoring pur-\nposes, Prometheus monitoring installed in the Kubernetes clus-\nter was used to capture resource usage by the applications, and\nGrafana was used to visualize the data in charts.\nFigure 3 shows how an experiment starts from the beginning\nto the end in the PoC solution from a high-level overview. In\nthe blue boxes, we can see the actions of the Kubernetes cluster,\nthe yellow ones from the FL Server application, and the orange\nones from the FL clients. From a high-level perspective, the\nuser configures a scenario in Lens for deployment, and Ku-\nbernetes scales the necessary resources and deploys the applica-\ntions. Afterward, the FL server handles the data distribution of\nthe dataset and starts the FL training algorithm. The client waits\nfor the server to connect with it, runs local training rounds, and\nreturns the model parameters to the server. The server receives\nthose parameters, aggregates them using the configured aggre-\ngation method by the user in the experiment layout, and dis-\ntributes them back to the clients, who will start local training\nrounds again with the new parameters. After all server rounds\nare done, the FL server will save the results in the correct output\nfolder of the experiment, and the experiment will be over.\nThe GitHub repository used in the development of the solu-\ntion was organized into isolated packages containing all neces-\nsary code, data, or declarations of each component to maintain\nindependence and enable better re-usability. The FL server and\nclient applications were also developed to run in bare-metal en-\nvironments. In addition, to allow easy application testing dur-\ning development, you can run a docker-compose environment\nto deploy the application locally using the built-in docker im-\nages. Further subsections will detail each one of the applica-\ntions and their role in the PoC solution."}, {"title": "5.3.1. Server Application", "content": "The server is a containerized Python application with the\nFlowerML server framework as a dependency. The server stor-\nage manager is responsible for initializing the experiment root\npath in the distributed storage, where the results and logs of the\ncurrently deployed run of each client will be saved. It will also\nbe responsible for receiving the models trained by the clients,\naveraging the received parameters using the selected strategy,\nand then updating the clients' models with the averaged param-\neters. The connection is done to the clients through a gRPC\nconnection with SSL encryption. The algorithm is outlined in\nAlgorithm 1.\nThe server address, number of server rounds, dataset, data\ndistribution, global aggregation strategy, client local rounds,\nand minimum number of connected clients are parameterized\nfor the server application and can be changed in each deploy-\nment. From the folder structure and source code files available\nat our GitHub repository, it is easy to correlate each part of the\napplication with the conceptual framework since each source\ncode file encapsulates its corresponding component, as we ex-\nplain in the following.\ndataset.py contains all the implemented classes of the sup-\nported datasets by the framework used by the application to\nhandle the dataset configuration in memory and how to ob-\ntain the raw data of the dataset properly. At the time of this\nwork, the CIFAR-10, CIFAR-100, and FMNIST datasets"}, {"title": "5.3.2. Client Application", "content": "The client waits for the server storage manager to initial-ize the experiment path in the distributed storage, connects to\nthe server through a gRPC connection encrypted with SSL, and\nperforms the pre-defined number of epochs received from the\nserver, which is the number of times that the data set passes\nthrough the neural network. When multiple clients are running,\nan individual client is oblivious to the existence of the other\nclients - it can only communicate with the server. The algo-\nrithm for the client application can be seen in Algorithm 2.\nThe client will only upload the model when the server re-\nquests the models, and the loop will end when the server fin-\nishes its rounds. From our folder structure at GitHub, it is easy\nto correlate each part of the application with the conceptual\nframework since each source code file encapsulates its corre-\nsponding component. The client's models and the address they\nwill connect are parameterized for the client application and can\nbe changed in each deployment.\nclient.py contains the client classes with the utilized training\nand testing algorithms. At the time of this work, it would\nonly support one PyTorch training algorithm.\nmain.py contains the client application main loop described\nin Algorithm 2. It is responsible for initializing the envi-\nronment variables used to set parameters, such as the local\nmodel utilized for training, and common variables, such as\nthe experiment path, to save results.\nmodel.py contains all the supported models described in the\nserver application section.\nstorage.py contains the storage manager class of the client,\nwhich is responsible for loading the distributed data batch\nfor training and testing the distributed storage and saving\nthe experiments test results of the client in the storage."}, {"title": "5.3.3. Monitoring Application", "content": "Prometheus uses the Kubernetes API to discover the vari-\nous resources it needs to monitor in the cluster, such as pods,\nservices, deployments, nodes, and more. It does this by query-\ning the Kubernetes API server for information about the desired\nresources and then collecting metrics data from these resources.\nFor example, to monitor a Kubernetes pod, Prometheus will\nquery the Kubernetes API server to get information about the\npod's name, namespace, labels, and other metadata. Prometheus\nwill then use this information to collect metrics data from the\npod, such as CPU and memory usage, network traffic, and other\nmetrics. Grafana queries the data from the Prometheus database\nto enable real-time visualization of resource usage in dashboards.\nSome sample charts of these dashboards are presented in Sec-tion 8."}, {"title": "5.3.4. Experiments Results Application", "content": "To retrieve the experiment results from the Kubernetes clus-\nter, a container running an Ubuntu base image from Docker was\nused to mount the used PVs and access the results. Since the re-\nsults are stored in files, we can copy them to the local machine\nto read and interpret them. Figure 4 illustrates how the experi-\nments that run in the PoC are saved in the distributed storage.\n.temp contains the partial results while running the applica-\ntion. This directory is deleted after the server application"}, {"title": "6. Experimental Setup", "content": "This section presents the experimental setup used to evalu-\nate the solution proposed in Section 3. Three experiments wereconducted to demonstrate the framework's capabilities. The ad-justed parameters are the dataset, global model aggregation, lo-cal client model, client epochs, and server rounds. Each experi-ment was run with ten clients - named client-0 through client-9 and taken from the set of computational devices labeled com-puter in the cluster - running with different data distributions.Also, in the Flower framework, the fraction fit was set to 1, andthe minimum available clients was set to 10 to ensure runningevery server round with all the clients participating. The spe-cific clients involved in a given experiment may vary from onerun to another, showing how the framework can help reproducereal-life scenarios effectively."}, {"title": "6.1. Datasets", "content": "The datasets considered for the experiments were the fol-lowing:\nCIFAR-10 Consists of 60,000 32x32 color images divided intoten classes, with 6,000 images per class. There are 50,000training images and 10,000 test images [31].\nCIFAR-100 Similar to CIFAR-10, except it has 100 classescontaining 600 images each. There are 500 training imagesand 100 testing images per class. The 100 classes in theCIFAR-100 are grouped into 20 superclasses. Each imagecomes with a fine label (the class to which it belongs) and acoarse label (the superclass to which it belongs) [31].\nFashion-MNIST A dataset of Zalando'sarticle images con-sisting of a training set of 60,000 examples and a test setof 10,000 examples. Each example is a 28x28 grayscaleimage, associated with a label from 10 classes. We in-tend Fashion-MNIST to be a direct drop-in replacement forthe original MNIST dataset [32] for benchmarking machinelearning algorithms. It shares the exact image size and struc-ture of training and testing splits."}, {"title": "6.2. Performance Metrics", "content": "The results from each experiment are saved in performance\nscore matrices and written to log files, as described in Sec-tion 5.3.4. The F1 score is a standard metric for evaluating\nclassification models and is particularly useful in cases with\nimbalanced data. It is the harmonic mean of two competing\nscores: precision and recall. Precision is the proportion of true\npositives out of all predicted positives (true positives + false\npositives), measuring how often the model correctly predicts a\npositive class, and recall is the proportion of true positives out\nof all actual positives (true positives + false negatives), measur-ing how well the model can detect positive classes. Thus, forbinary classification problems, the F1 score is calculated by\n$F1 score = 2 \\times \\frac{precision \\times recall}{precision + recall}$\nGiven the F1 score of each class, as defined by (1), we can\naggregate them into different metrics typically used for evalu-ating the performance of multi-class classification models: the\nmicro, macro, and weighted average F1 scores. These metrics\nare computed for each client and saved to their log files. Inshort, the performance metrics analyzed were the following:"}, {"title": "7. Results", "content": "The results were collected directly from the output folder\nand aggregated into visualization charts to ease analysis. In to-tal, 143 containers (server and clients) were run in the cluster\nnodes, resulting in 143 log files containing performance mea-sures through each server round. Each experiment run gener-ates 11 log files, one from the server's container and 10 fromthe clients' containers. As the number of output files can growexponentially, we considered only the last run of each experi-ment, i.e., 33 files were analyzed to generate the results.\nWe used external commercial tools to manipulate the data\ncollected from all log files and show the results through mean-ingful charts. As a future work, the framework can include au-tomatic chart generation from the performance measures gen-erated from the experiments."}, {"title": "7.1. Experiment 1", "content": "Table 2 details the parameters used for the first experimentand the data distribution across clients. The FMNIST datasetwas used with a non-IID and unbalanced data distribution. Also,the classes were distributed using a Dirichlet distribution with$\n\\alpha = 0.1$. The global model aggregation used was FedOpt, andthe experiment ran with ten local epochs and ten server rounds.Figure 5a shows each client's accuracy through the serverrounds. The x-axis indicates the server round number, and they-axis indicates the computed accuracy. It is important to no-tice that accuracy measures the percentage of correctly classi-fied cases regardless of their classes, assigning the same im-portance to all instances. The figures indicate that clients withlarge amounts of data had a relatively high initial accuracy inthe first rounds. In contrast, the accuracy of clients with littledata was initially low. Over the server rounds, the chart revealsthat the aggregate model had a subtly negative impact on the ac-curacy of clients with more data available while benefiting theaccuracy of clients with less data.\nFigure 5b shows the macro F1 score of each client throughthe server rounds. Notice that macro F1 assigns the same im-portance to each class of the dataset, which means this metrictends to exhibit better results as the performance of all classesin the dataset improves evenly. The results suggest that the per-formance may seem unacceptable when focusing on averagingthe performance of different classes with the same weight, de-spite some clients having a reasonable accuracy, as shown inFigure 5a. Even so, as a general trend, the chart reveals perfor-mance increases over the server rounds.\nAnother way to evaluate the model's performance is to mea-sure the clients' weighted F1 score, i.e., the average F1 scoreweighted by the number of instances in each class, as exem-plified in Figure 5c. The chart shows that some clients (5, 6,and 8) had a relatively high initial performance with a slightdecrease over time. Contrasting with the results in Figure 5b,we can note that the performance of these clients in the ma-jority classes (with more instances) is considerably higher inthe initial rounds. In these majority classes, however, the ag-gregated model worsened the performance over the server runs,even though the overall performance considering all classes im-proved. When observing client-6 in Figure 5b, for example,there is a performance leap between rounds 4 and 5, suggestingthat the aggregated model improved the model's overall perfor-mance despite reducing performance for the majority class.\nFigure 5d shows the distributed accuracy over all clients andthe distributed loss of the FedOpt aggregation strategy in eachserver round. The x-axis indicates the server round number, theleft y-axis indicates the accuracy, and the second y-axis indi-cates the losses. We can see that the overall performance of theFL algorithm converged to an accuracy between 60% and 70%with less than 100 losses.\nFurther analysis could be done to understand better how thedataset's class distribution impacted the results, as the frame-work saves this information in the configuration file of eachexperiment run. Recall that the purpose of this work is notto present a comprehensive evaluation of FL algorithms but todemonstrate how the proposed framework can aid in doing so.Therefore, further analysis can be subject of future work."}, {"title": "7.2. Experiment 2", "content": "As shown in Table 3, the second experiment was run usingthe CIFAR-10 dataset with non-IID and unbalanced data dis-tribution, similar to the first experiment. However, we used apathological class distribution for two classes per client and theFedAvg global model aggregation. The experiment ran withfifteen local epochs and ten server rounds. The results analysisfollows the same approach used in Section 7.1.\nFigure 6a shows the accuracy of all clients through the serverrounds. We can see that clients 0 and 1 can initially classify agood part of the test instances, but most clients cannot clas-sify any cases. As more server rounds are run, however, theperformance of clients with high accuracy decreases while theperformance of clients with low accuracy increases. As a result,the overall accuracy stabilizes in a medium range between 30%and 60%, i.e., the aggregation model was able to distribute theperformance among clients.Figure 6b presents the macro F1 score of all clients throughthe server rounds. It shows that clients 0 and 8 initially man-aged to obtain an excellent performance in the two classes fromwhich they had data. However, the aggregation drastically harmedtheir performance, reducing it to less than 20%. On the otherhand, the aggregation positively affected clients who performedquite poorly initially. Nevertheless, the general performanceof all clients, considering all their classes equally important,turned out to be very low.\nIn contrast, Figure 6c shows the weighted F1 score, suggest-ing a more erratic behavior over the server rounds. We can notethat clients 0 and 8, which had a noteworthy decline in perfor-mance in the first round when all classes had the same weight,as shown in Figure 6b, had a different behavior when consider-"}, {"title": "7.3. Experiment 3", "content": "The last experiment was run using the CIFAR-100 dataset,which also had non-IID but a balanced data distribution. Sim-ilar to the second experiment, a pathological distribution oftwenty classes per client was used. The global model aggre-gation used was FedYogi, and the experiment ran with ten localepochs and fifteen server rounds. Table 4 shows the parametersand the balanced data distribution used in the experiment.\nThe experiment deals with a classification problem that isfairly difficult in itself because there are more classes to pre-dict, and originally, there were relatively few instances per classavailable. Furthermore, in the distributed context, each clienthas access to only 20 of the classes and less than half of theoriginal dataset samples per class. Due to this, we expect poorperformance in this scenario.\nThe chart in Figure 7a shows that the proportion of pre-dicted instances generally increases for all clients, with someexceptions (see between rounds 9 and 13), demonstrating that,in general, aggregation benefits clients over time.\nFigure 7b shows the f1-macro measure. Thus, consideringthe performance for all classes and those with equal importance,the general performance of all clients is poor, not even reaching10% of the macro f-measure. However, verifying the generalbeneficial effects of aggregation throughout the rounds is pos-sible, with some exceptions."}, {"title": "8. Resource Usage Monitoring", "content": "The PoC solution uses the Grafana application to monitorthe use of computing and networking resources during the ex-periments, allowing the visualization of the data collected bythe Prometheus. To demonstrate some of the capabilities ofthe PoC solution on monitoring, an analysis of CPU usage andnetwork traffic was made in Experiment 3 (subsection 7.3). Itis important, once again, to emphasize the focus on the demon-stration of the capabilities and not on the achieved performance.Similar analyzes can be done for all the experiments run sinceall the metrics are saved in the Prometheus server database."}, {"title": "8.1. CPU Usage", "content": "Figure 8 shows each client and server's CPU resource usagein the third experiment. The X-axis indicates the timestamp ofthe metric, and the Y-axis indicates the CPU usage in percent-age, and each data sample has a 10-second interval between oneanother.\nWe can notice that the figure is quite chaotic since all theten clients and the server are plotted in it. However, we cansee the execution of each of the fifteen server rounds ran in theexperiment and that some of the clients had a CPU usage of 20-30% while others stayed between 1-5%. This can be explainedby the client's CPU processing power heterogeneity since someclients run on different hardware profiles. We also noticed thatthe workload was distributed evenly across the clients since thedata distribution was balanced.\nThe choice of using Grafana for experiment data visualiza-tion allows us to dynamically select which data points to plotin the chart. So, to better analyze the assumption about the firstchart, we can choose some clients of specific hardware and oth-ers of another specification to compare them. Figure 9 demon-strates this selection using the same chart with now only client-5, client-4, and the server CPU usage being plotted. It becomesevident that client-4 (Intel Core i5-3210M CPU @ 2.50GHz)presented higher CPU resource usage and needed more timeto process the same number of samples as client-5 (Intel Corei7-5500U CPU @ 2.40GHz).\nAs for the server (orange line in Figure 9), the peak of CPUusage is noticeable at the beginning of the experiment as a briefbump (near 1%) before the first training round. This can beexplained by the fact that the data distribution created by theserver is done at the beginning of the experiment. We can alsosee that the server's CPU resource usage is almost negligiblecompared to any client's. The server only uses the CPU inbetween training rounds to aggregate the parameters receivedfrom the clients, which is a pretty simple task compared to thetraining done by the clients, thus explaining this phenomenon.The hardware profile of the server node is the same as client-5 (Intel Core i7-5500U CPU @ 2.40GHz), thus the amount ofCPU resources needed for client and server tasks can be ana-lyzed and understood in perspective.\nTo demonstrate that the analysis done in experiment 3 couldbe done in other experiments, we present a sample chart fromexperiment 1. Figure 10 shows the same chart of figure 8 plot-ted with data from the first experiment. It is interesting to seehow the data distribution affects the CPU usage of the clients.We can visualize the gaps in between them due to the size of thesamples being different for each client, even for those runningin similar hardware profiles. Further analysis can be done inthe Grafana dashboard to understand the behavior of each oneof the clients individually and in any time frame specified. Wewill not present an exhaustive use of interactions with Grafanacharts in this paper for the sake of brevity."}, {"title": "8.2. Network Traffic", "content": "Similar to the CPU usage charts presented in the previ-ous subsection, the same type of analysis can be done to vi-sualize the network traffic between the clients and the server.Prometheus gathers information on traffic flowing in and out ofPods, collecting metrics such as, packets in/out, bytes in/out,and errors/drops, whereas Grafana enables interactive visual-ization and filtering of these metrics in different types of charts.To demonstrate this we created a simple dashboard with linecharts containing bytes in/out of all Pods participating in theexperiments. Two of these charts are presented in the followingwith data from the third experiment over the same time framepreviously used in Figures 8 and 9.\nFigure 11 shows the received (inward) traffic for each oneof the clients involved in the third experiment as stacked areas.The X-axis indicates the timestamp of the metric, and the Y-axisindicates the network traffic in Megabytes per second (MBps).Each data sample is also collected with a 10-second intervalbetween them. The first aspect to notice is that peaks of trafficare presented at the beginning and end of each round, which isexpected since that is when communication between client andserver should occur. The aggregated traffic peak considering all10 clients was around 80 MBps. We did not show traffic sent(outward) from each client Pod because the communication fol-lows a client-server paradigm, i.e., all the traffic flowing out ofclients goes into the server and vice-versa.\nTo better understand client and server behavior, we can filteronly one client and the server in the same chart. For example,Figure 12 shows the traffic received (inward) in the server andclient-9 for the third experiment. One can see that the com-munication pattern is always the server receiving data and, af-terward, the client. This is expected since, initially, the clientsconnect to the server, and the server accepts the connection.During training, the pattern is maintained since the server re-ceives the parameters from the clients, and the client receives"}, {"title": "9. Conclusion", "content": "FL is an ML paradigm that is constantly being adopted due\nto its distributed approach, the necessity of data privacy, and\nthe vast increase in IoT devices. Heterogeneous data distri-\nbutions/contents of clients proved to be a real challenge and\ngained great notoriety over the last few years. Proposals, suchas DRL algorithms to dynamically learn the weight of the con-\ntributions of each client at each round, continue to be developedand will be fundamental to increasing the accuracy and usabil-ity of FL in more applications. Testing and assessing this FLalgorithm can be a challenging and complex task due to the sys-tems' distributed nature and the possible scenarios. To addressthis complexity, this work proposed a conceptual frameworkto facilitate testing federated learning scenarios in distributedcomputing environments with different types of data distribu-tions. This work achieves the intended proposal by proposing aconceptual framework for testing federated learning scenariosand demonstrating an implementation of those concepts in thePoC solution. The solution developed shows that creating anedge-like FL testing framework that can scale to several differ-ent types of real-life scenarios using distributed heterogeneouscomputing and other data distributions is possible, inspiring fur-ther development of the concepts and improvement of the PoCsolution.\nTo prove the capabilities of the PoC solution, three experi-ments with three different FL scenarios were conducted. Theresults showed how it is possible to analyze the impacts ofclass and data imbalance in a real-life distributed system of FLthrough the framework via the f1-measures outputted in the ex-periment results. It was also possible to see the resource usageof the applications via the monitoring solution and to demon-strate the impact of the underlying heterogeneous infrastructureused.\nThe critical point taken from this work is that designing asolution from the beginning that has independence between in-frastructure and applications has shown to be very efficient andeffective during the development phase. Containers enabled re-usability, isolation, and easy testing of the applications despitethe environment in which they were deployed, either locallyor in the distributed cluster. Also, this allowed the system toscale horizontally by adding more nodes to the cluster from aninfrastructure perspective or by creating more application repli-cas from an application point of view."}]}