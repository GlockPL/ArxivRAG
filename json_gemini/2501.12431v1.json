{"title": "Modality Interactive Mixture-of-Experts for Fake News Detection", "authors": ["Yifan Liu", "Yaokun Liu", "Zelin Li", "Ruichen Yao", "Yang Zhang", "Dong Wang"], "abstract": "The proliferation of fake news on social media platforms disproportionately impacts vulnerable populations, eroding trust, exacerbating inequality, and amplifying harmful narratives. Detecting fake news in multimodal contexts-where deceptive content combines text and images-is particularly challenging due to the nuanced interplay between modalities. Existing multimodal fake news detection methods often emphasize cross-modal consistency but ignore the complex interactions between text and visual elements, which may complement, contradict, or independently influence the predicted veracity of a post. To address these challenges, we present Modality Interactive Mixture-of-Experts for Fake News Detection (MIMOE-FND), a novel hierarchical Mixture-of-Expert framework designed to enhance multimodal fake news detection by explicitly modeling modality interactions through an interaction gating mechanism. Our approach models modality interactions by evaluating two key aspects of modality interactions: unimodal prediction agreement and semantic alignment. The hierarchical structure of MIMOE-FND allows for distinct learning pathways tailored to different fusion scenarios, adapting to the unique characteristics of each modality interaction. By tailoring fusion strategies to diverse modality interaction scenarios, MIMOE-FND provides a more robust and nuanced approach to multimodal fake news detection. We evaluate our approach on three real-world benchmarks spanning two languages, demonstrating its superior performance compared to state-of-the-art methods. By enhancing the accuracy and interpretability of fake news detection, MIMOE-FND offers a promising tool to mitigate the spread of misinformation, with potential to better safeguard vulnerable communities against its harmful effects.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the rise of online social networks has enabled users to freely express their opinions and emotions on the Web. However, this shift has also contributed to the proliferation of online fake news, defined as news content intentionally manipulated to spread disinformation or misinformation [25]. The rapid spread of fake news through social networks seriously threatens public knowledge and societal trust [30, 44], especially affecting vulnerable populations who are more susceptible to its influence [11]. For example, alcohol-related deaths among young adults in the U.S. increased by 25% in 2020 [2], fueled by COVID misinformation on social media that claimed consuming concentrated alcohol could kill the virus [14].\nTo combat this growing threat, automatic Fake News Detection (FND) has emerged as a critical research topic, aiming to develop solutions to safeguard online information integrity [40]. In FND research, textual cues (e.g., word choices and sentence sentiment [1, 4, 23]) and visual cues (e.g., image quality [8] and image semantics [18]) provide complementary insights. To leverage both modalities for improved fake news detection, recent studies increasingly focus on multimodal FND [40]. Earlier works directly concatenate and fuse feature vectors from different modalities for classification [28, 29, 32, 41]. However, these methods do not guide the model to prioritize specific unimodal representations or their shared cross-modal representations. For instance, in cases where the textual content is misleading but paired with an authentic image, the model might overly rely on the visual representation, leading to an incorrect prediction [7]. To address this challenge, prior work evaluates cross-modal consistency by leveraging similarity measures or incorporating auxiliary image-text matching tasks [3, 31, 33, 37, 39]. Nevertheless, cross-modal consistency evaluation has limitations in fully capturing the nuanced interplay between modalities. First, existing cross-modal consistency evaluation methods are under the assumption that real multimodal news (e.g., image-text pairs) are always semantically aligned [31, 33, 37]. Second, prior works ignore the agreement level of unimodal feature-only detections, which reflects whether cues from one modality agree with those from another. The above limitations hinder the effective characterization of the synergistic interactions between modalities.\nSpecifically, we observe that in many online posts, images may not align or agree with the text content, yet the image-text pairs can still convey truthful or deceptive information, even when there are significant dissimilarities between the modalities. For instance, an image of a crowded hospital waiting room paired with text claiming a health crisis caused by vaccine side effects illustrates how mismatched visuals and text can craft a deceptive narrative to influence public opinion. We refer to such scenarios in multimodal FND as modality interactions [20]. To the best of our knowledge, our work is the first to investigate modality interactions in the multimodal FND task while incorporating distinct modules to address the the variability of the interactions within the framework design.\nIn our approach, we focus on two key factors that define the interactions between modalities in a multimodal FND task: unimodal prediction agreement and semantic alignment. Specifically, unimodal prediction agreement refers to the consistency between predictions of the veracity of the news (real or fake) based solely on unimodal features, while semantic alignment captures the degree to which textual and visual content convey coherent or related meanings.\nThe combination of these two factors leads to four distinct types of modality interactions in FND, illustrated in Figure 1: 1. Agreed Misalignment (AM): semantically misalign, but unimodal predictions agree. For example, Figure 1a shows a celebrity image followed by a mismatched entertainment news. Despite the lack of semantic connection between the image and text, both unimodal predictions independently classify the instance as real news, emphasizing the challenge of extracting a meaningful relationship between unimodal predictions in scenarios where fake news can consist of both a true image and true text. 2. Agreed Alignment (AA): semantically align, and unimodal predictions agree. In Figure 1b, the image and text both depict an incident of a large yellow duck toy, and their unimodal predictions both indicate fake news. This scenario highlights that similar signals across modalities can reinforce each other, generating a more confident detection. 3. Disagreed Misalignment (DM): semantically misalign, and unimodal predictions disagree. Figure 1c shows a car crash image paired with unrelated text describing a new garbage incineration facility, with true image prediction and fake text prediction, highlighting the difficulty of reconciling contradictory signals from different modalities when they lack semantic overlap. 4. Disagreed Alignment (DA): semantically align, but unimodal predictions disagree. As shown in Figure 1d, the text and image both indicate that Miley Cyrus and Liam Hemsworth starting a family, yet the image-only prediction estimates the post as fake news, while the text-only prediction identifies it as true news. This example reflects the challenge of resolving conflicts even with semantic alignment, introducing additional synergistic effects in semantic reasoning.\nTo address the limitations of prior approaches that overlook modality interactions (AM, AA, DM, DA), we propose Modality Interactive Mixture-of-Experts for Fake News Detection (MIMOE-FND), a novel framework that explicitly models modality interactions through a gating mechanism, enabling tailored fusion strategies for improved detection accuracy. Overall, our method incorporates a hierarchical Mixture-of-Experts (MoE) architecture to dynamically route input data to different \"experts\" [24]. In our hierarchical approach, we first design improved MoE blocks for feature refinement and multimodal fusion through a token attention based gating, which enhance the model's ability to capture task-relevant information in different channels. At the upper level hierarchy of the MIMOE-FND architecture, we introduce a modality interaction gating module that dynamically routes input image-text pairs to distinct fusion expert modules. These fusion experts are trained to address different modality interaction challenges as discussed above, enabling a more tailored multimodal fusion for FND. To assess the efficacy of MIMOE-FND, we evaluate its performance on three real-world multimodal FND datasets in both English and Chinese, benchmarking it against state-of-the-art approaches. The results demonstrate significant improvements across four evaluation metrics, highlighting its superior accuracy and robustness. Our contributions are as follows:\n\u2022 We are the first to investigate and model modality interactions in the multimodal FND task. Our approach categorizes four distinct types of modality interactions based on semantic alignment and unimodal prediction agreement.\n\u2022 We propose MIMOE-FND, a hierarchical MoE framework for multimodal FND task with adaptive multimodal fusion guided by modality interactions. Our model learns to dynamically route news instances to their corresponding fusion experts based on the evaluated modality interactions.\n\u2022 We validate the effectiveness of MIMOE-FND through extensive experiments on three widely used multimodal FND benchmarks across two different languages, where our scheme shows significant performance gains compared to the state-of-the-art baselines."}, {"title": "2 RELATED WORK", "content": "2.1 Modality Interactions\nModality interaction is a research topic that studies how elements in different modalities interact with each other to increase information for task inference [20]. For a multimodal downstream task, the effective task completion could require dynamic features extracted from both unimodal and multimodal inputs [34]. To utilize information from different data modalities, factorized contrastive learning emerges as a framework accounting for different modality interactions in multimodal classification tasks with an information theoretical formulation [19, 27]. More recently, Yu et al. propose to model the modality interactions using a gating network [38]. It highlights that different modality interactions could be better captured by separate modelings. In the realm of multimodal fake news detection, the detection model is expected to utilize both unique and shared information of different modalities to account for different modality interactions. However, existing multimodal fake news detection works do not explicitly model semantic-level modality interactions, resulting in suboptimal multimodal fusion. To this end, our work explicitly considers the modality interactions in multimodal FND through a gating mechanism supervised by a unimodal prediction agreement and semantic alignment.\n2.2 Multimodal Fake News Detection\nOver the past few years, multimodal fake news detection has gained a significant amount of attention in research community [40]. Jin et al. proposes to utilize the attention mechanism to enhance LSTM model for effective modality fusion to detect online rumors [15]. MVAE uses variational auto-encoders to learn a shared embedding space to account for both text and image data distribution in order to achieve a better multimodal fusion and more accurate FND [17]. SpotFake leverages pretrained XLNet and ResNet for feature extraction and perform FND based on the concatenated text-image representations, benefiting from the rich features provided by these large pretrained models. However, despite the high-quality features these models offer for different data modalities, the misalignment of cross-modal features can reduce the FND performance.\nTo effectively align the text and visual representations in multimodal fake news detection, a number of prior works introduce extra modeling designs to account for modality-wise consistency to better guide the feature alignment heuristically [3, 31, 39]. SAFE models cross-modal inconsistency by calculating the similarity between text and visual information in news articles [39]. MCAN uses co-attention layers to obtain fused feature from both visual and text inputs [35]. MCNN models the modality-wise consistency by calculating the cosine similarity of visual and text representations after a weight sharing scheme [36]. CAFE takes a probabilistic modeling approach by introducing two VAEs to model text and visual distributions, followed by a cross-modal ambiguity evaluation using Kullback-Leibler (KL) divergence [3]. Similarly, CMC proposes to implicitly learn cross-modal correlation through knowledge distillation guided by a soft target [33]. COOLANT utilizes a cross-modal contrastive learning phase followed by a similar ambiguity-aware fusion as proposed in CAFE [31]. Likewise, with cross-modal ambiguity evaluation, BMR introduces a Mixture-of-Experts module to dynamically bootstrap multi-modal representations [37]. More recently, FND-CLIP uses pretrained CLIP feature representations to guide the fusion process of multi-modal FND, which brings a more semantic level cross-modal consistency evaluation [42].\nThe majority of prior work utilizing cross-modal ambiguity-guided fusion relies on weighted aggregation across modalities [3, 31, 37]. While this method provides a mechanism to adaptively fuse different modality representations, it heavily focuses on statistical features due to the distribution modeling of cross-modal ambiguity. To this end, we propose to guide the fusion of multimodal representations using a gating mechanism supervised by both unimodal prediction agreement and CLIP-guided semantic alignment. Our method designs specialized fusion expert modules to account for different semantic/unimodal agreement scenarios, allowing flexibility for challenging cases in multimodal FND."}, {"title": "3 METHOD", "content": "As shown in Figure 2, our approach has a hierarchical MoE architecture, structured for three primary phases of our multimodal FND pipeline: 1. Feature Extraction & Refinement, 2. Modality Interaction Gating, 3. Multimodal Fusion and Detection. In the lower-level hierarchy of MIMOE-FND, we utilize adapted MoE blocks for both feature refinement and multimodal feature fusion. These adapted MoEs are defined and elaborated as improved MoE blocks (iMoE) in Section 3.1. For the upper-level hierarchy, we utilize a modality interaction gating network alongside distinct iMoE-based feature fusion experts to perform tailored multimodal fusion for samples based on their modality interactions (AM, AA, DM, DA). Overall, the hierarchical design of MIMOE-FND ensures more effective handling of diverse multimodal interactions in multimodal FND.\nFor the multimodal Fake News Detection (FND) task, we formulate it as a binary classification problem, where the positive class corresponds to fake news and the negative class corresponds to real news. The entire pipeline is trained in an end-to-end manner, taking inputs drawn from the dataset D, where D = {xi | xi = (xtext, ximg) }N i=1. Here, each data point x \u2208 D is a text-image pair, with xtext representing the text input and ximg representing the image input. The output of the pipeline is a binary detection result \u0177 indicating whether or not the input news is a fake news.\n3.1 iMoE Block\nWe first outline our proposed improved Mixture-of-Experts block (iMoE) which is tailored to perform dynamic adaptive pooling along the token dimension of input. Similar to an MoE layer defined in [24], an iMoE block contains a gating network G and n expert networks E1, E2, ..., En. However, inspired by the Squeeze-and-Excitation Network (SENet) [12], we additionally introduce an attention vector, attx, to enable the expert networks to flexibly focus on different tokens within an input vector x consisting of N tokens, each with d dimensions. Specifically, as shown in Figure 2, we compute attx by squeezing input x along the token dimension using an attention network. The attention vector weighted input (attx \u2297 x) is then passed to the gating network G to obtain gate output, which we denote as a dispatch vector. Finally, the output o of an iMoE block is given by a dispatch vector weighted feature aggregation from all expert networks:\no = \u03a3 Gi(attx \u2297 x)Ei (x), (1)\nIn the iMoE block, both the attention network and the gating network are composed of two fully connected layers, with a Sigmoid activation function applied between them. For each expert network E within the iMoE, we utilize a pre-defined transformer block derived from the Vision Transformer [6].\nCompared to the gating mechanism used in MoE [24], the token attention based gating of iMoE enables a dynamic and context-aware pooling, thus enhancing model's ability to capture task-relevant information in different tokens. When used for modality feature fusion, iMoE can also perform modality-wise pooling by stacking multimodal feature vectors as \"tokens\", which results in a more flexible fusion than commonly adopted weighted sum aggregation [3, 39]. Different from prior work that applies the Multi-gate Mixture-of-Experts (MMoE) to fuse feature components across multiple modality branches [37], the iMoE block focuses on leveraging the token attention guided gating mechanism to improve the flexibility and adaptability of feature pooling. Furthermore, we highlight that our proposed iMoE block can be used both for feature refinement and multimodal fusion.\n3.2 Feature Extraction & Refinement\nIn the feature extraction & refinement phase of MIMOE-FND pipeline, we utilize pretrained BERT [5] to extract text representations and MAE [9] to extract image representations for a given input x = (xtext, ximg). More specifically, we denote the extracted unimodal features as ut \u2208 RNt\u00d7DBERT for text and ui \u2208 RNi\u00d7DMAE for image.\nWhile BERT and MAE provide rich semantic features, directly combining image and text representations can be challenging due to the discrepancies between text and image pretrained feature spaces. To mitigate this issue, we use iMoE blocks to refine the unimodal representations and perform adaptive pooling. We construct three feature branches for unimodal text representation ut, unimodal image representation ui and multimodal representation um = concat(ut, ui). For each feature branch, we pass the feature vector to an iMoE block to obtain the pooled and refined feature vector. We denote the refined feature vectors as et, ei, em for unimodal text, unimodal image and multimodal representations respectively.\n3.3 Modality Interaction Gating\nTo route news instances with different modality interactions to their respective feature fusion experts, we introduce a modality interaction gating mechanism. The modality interaction gating module is supervised by evaluations of unimodal prediction agreement and semantic alignment, ensuring that the gating process effectively leverages modality interactions.\n3.3.1 Modality Interactions. To quantitatively evaluate the modality interaction of an instance, we define unimodal prediction divergence and semantic alignment as follows:\nDefinition 3.1 (Unimodal Prediction Divergence). For a given input news x, we define unimodal prediction agreement to be:\n\u03b4(x) = d(\u0177text, \u0177img) (2)\nwhere \u0177j represents the fake news detection result of the news instance based solely on the information from modality j (e.g., text or image), and d : Y \u00d7 Y \u2192 R+ is a distance function measuring the divergence between unimodal predictions.\nDefinition 3.2 (Semantic Alignment). For a given input news x, we define the semantic alignment as the semantic level similarity between the text and image. Let atext and aimg be two vector representations fully capturing semantic features in a shared feature space of text and image. The semantic alignment is defined as:\np(x) =  xtext \u00b7 aimg / || atext || || aimg ||  (3)\nTo evaluate the unimodal prediction divergence d(x), we attach classification heads ftext, fimg for each of the refined unimodal feature branch to extract \u0177text = ftext(et) and \u0177img = fimg(ei). The classification heads are trained as a separate part of the pipeline, which is not used in our final prediction. We use the standard cross-entropy loss to guide the unimodal classification heads:\nLuni (\u0177text, \u0177img) = 1/2(LCE(\u0177text, y) + LCE(\u0177img, y)) (4)\nWe then evaluate \u03b4(x) to be the Jensen-Shannon (JS) divergence [21] between unimodal predictions, defined as:\n\u03b4(x) = DJs (\u0177text, \u0177img) = 1/2DKL (\u0177text||M) + DKL (\u0177img||M), (5)\nwhere M = (\u0177text + \u0177img)/2 represents the mean of the unimodal prediction distributions, DKL denotes the Kullback-Leibler (KL) divergence and \u0177j is the unimodal prediction distribution of modality j (e.g., text or image).\nFor the evaluation of semantic alignment, we pass the raw text-image pair to a pretrained CLIP model to extract feature vectors mt and mi for text and image in a joint feature space. CLIP is pretrained on a variety of internet-sourced text-image pairs, which enables them to capture semantic relationship between texts and images. The calculation of \u03c1(x) in equation 3 is thus computed as the cosine similarity of CLIP text and image embeddings, which is also referred to as the CLIP score [10].\nAdditionally, we define an agreement threshold \u03b8agr and an alignment threshold \u03b8sem, which are set to fixed constants in all our experiments empirically. Specifically, we categorize a data instance to be semantically aligned and unimodal prediction agreed based on 1(\u03c1(x) > \u03b8sem) and 1(\u03b4(x) < \u03b8agr) respectively, where 1(\u00b7) is an indicator function with binary outputs (0 or 1). We categorize four modality interactions (AM, AA, DM, DA) in a multimodal FND task according to the permutations of outputs of the binary indicators.\n3.3.2 Interaction Gating Module. We introduce an interaction gating module that learns to dispatch news instances according to their modality interactions while also optimizing task performance. The interaction gating module takes the refined unimodal feature vectors (et, ei) and the CLIP embeddings (mt, mi) as inputs, and outputs a dispatch vector \u0177d with a size corresponds to the number of feature fusion experts. Similar to the gating network illustrated in the iMoE block (section 3.1), the interaction gating module contains an attention network and an interaction gating network. The attention network is used to calculate a modality-level attention vector. With the modality-level attention weighted input, the interaction gating module computes a softmax-normalized dispatch vector which determines the routing process. During the modality interaction routing, only the fusion expert corresponding to the highest value in the dispatch vector is activated, ensuring that multimodal features are processed by the most relevant fusion expert.\nTo guide the training of interaction gating network to route the data instances according to modality interactions, we use 1(\u03c1(x) > \u03b8sem) and 1(\u03b4(x) < \u03b8agr) as our training target for interaction gating by expanding the two binary outputs to 4 different classes corresponding to modality interactions (AM, AA, DM, DA), where we obtain the target modality interaction yint = 2 \u00b7 1(\u03b4(x) < \u03b8agr) + 1(\u03c1(x) > \u03b8sem). The modality interaction gating process is then formulated as a classification task, where the model output is \u0177d and the target label is yint. Given a dispatch vector output \u0177d from the gating network, we define a modality interaction loss as follows:\nLd = LCE(\u0177d, yint) (6)\n3.3.3 Gating Regularization. In our experiments, we observe that the modality interaction gating mechanism can lead to imbalanced training of the feature fusion experts, causing suboptimal performance in the final detection task. We hypothesize that this issue arises from an imbalance in the training data distribution across different modality interactions. To ensure effective training of interaction gating modules and the associated fusion experts, we apply a router-Z loss to penalize extreme values in interaction gating network outputs and a balance loss to ensure load balancing of fusion experts [43]. For a dispatch vector \u0177d, the router-Z loss Lz and the balance loss Lb are calculated as following:\nLz = (log \u03a3 exp(od)i) ) 2 , Lb = \u03a3 (\u0177di \u2212 t(i))2  (7)\nwhere \u0177d denotes the softmax normalized dispatch vector, od denotes the raw output of interaction gating network and t(i) denotes the probability from a uniform distribution t of data instance being assigned to expert i. Overall, apart from the task loss of multimodal FND, the interaction gating module is supervised by a combination of modality interaction loss Ld and regularization loss Lreg = \u03b7Lz + \u03b3Lb:\nLint = Ld + Lreg (8)\nwhere we empirically set the regularization weights \u03b7 = 0.01 and \u03b3 = 0.1 respectively in our experiments.\n3.4 Multimodal Fusion & Detection\nIn the multimodal fusion and detection phase, we first apply adaptive normalization that adjusts the unimodal feature vectors et, ei with learnable mean and variance [13] to further reduce the feature space discrepancy for better fusion, where we denote the normalized unimodal features as e\u02dct and e\u02dci. For each data instance, we then perform fusion by passing concatenated the adaptively normalized feature vectors (e\u02dct, em, e\u02dci) to the dispatched fusion expert.\nIn MIMOE-FND, we use four fusion experts corresponding to modality interactions (AM, AA, DM, DA). Each fusion expert is an iMoE block performing modality-level aggregation. The final feature vector is obtained from the target fusion expert guided dispatch vector \u0177d. In the detection phase, the final feature vector is fed into a classification head to obtain a detection result \u0177, which is then used to calculate the task loss as binary cross-entropy loss.\nLtask = LBCE(\u0177, y) (9)\nOur overall objective function is formulated as a weighted sum of modality interaction gating module loss, task loss and unimodal prediction loss, which can be summarized as:\nL = Ltask + \u03b1 \u00b7 Luni + \u03b2 \u00b7 Lint (10)\nwhere the \u03b1 and \u03b2 are hyper-parameters adjusting the different learning speeds for the components. The whole pipeline is then trained in an end-to-end manner with a separate optimizer to update parameters for the interaction gating module and an optimizer for all other parameter updates."}, {"title": "4 EXPERIMENTS", "content": "To evaluate our approach, we benchmark it against several strong multimodal FND baselines on three widely used datasets: Weibo [16], Weibo-21 [22], and GossipCop [26]. The detailed experimental settings and the list of baseline methods are provided in Appendix A.\n4.1 Performance Comparison\nThe performance comparisons between our approach and other baselines are reported in Table 1. We report accuracy, precision, recall and F1-score. We observe that our method is able to consistently perform the best in terms of detection accuracy across all datasets with the accuracy of 0.928, 0.895 and 0.956 for Weibo, GossipCop and Weibo-21 respectively. Our method is also able to out-perform most of the baselines on other reported metrics for both the fake news and real news. However, we do observe several outliers in the results. For Weibo, we notice that SpotFake has better fake news recall and F1-score. However, it has the worst performance for the real news prediction. Such a severely imbalanced prediction indicates that the model has not been generalized well to the data distribution and is instead overfitting to the characteristics of fake news, failing to correctly identify real news.\nFor GossipCop, BMR has a very close performance compared to our approach. Nevertheless, we notice our method is able to perform better in all metrics for fake news detection while tie with BMR in terms of real news precision and F1-score. This suggests that our method can detect fake news more effectively while keeping the same real news prediction performance as BMR. To further understand the performance of our approach, we closely scrutinize GossipCop dataset and find the majority data collected from Gossip-Cop are well-edited entertainment news with celebrity faces as the accompanied image, which can only partially reflect the modality interactions. On the other hand, Weibo and Weibo-21 contain more noisy user-generated posts, thus reflecting a more complex set of modality interactions, where we observe that our method is able to obtain larger performance gains.\n4.2 Ablation Study\nTo understand the effectiveness of each components in our approach, we perform ablation study on input modalities and modality interaction gating as shown in Table 2. Specifically, we remove components centered around the interaction gating module and detail the settings after the removals as follows:\n\u2022 image-only: We use only image input followed by a feature refinement component and an MLP classification head.\n\u2022 text-only: We use only text followed by the same feature refinement component and classification head.\n\u2022 w/o reg.: We remove the regularization losses Lz and Lb in the loss function of interaction gating network while keep all other components and hyper-parameters the same.\n\u2022 w/o sem.: We remove the semantic alignment (sem.) target in the training of interaction gating network.\n\u2022 w/o agr.: We remove the unimodal prediction agreement (agr.) target in the training of interaction gating network.\n\u2022 w/o int.: We remove the modality interaction (int.) supervision (both sem. and agr.) and use only final task loss together with regularization losses to guide the training of interaction gating network."}, {"title": "4.3 Parameter Analysis", "content": "In this section, we analyze the sensitivity of the training process for modality interaction gating to two hyper-parameters: the modality interaction supervision weighting (\u03b2) and the learning rate (\u03bb) for the interaction gating module. The parameter \u03b2 determines the contribution of modality interaction supervision in the final loss function, while \u03bb controls the update speed of the interaction gating module. For each parameter value, all other hyper-parameters are fixed according to the best configuration detailed in Appendix A.2. Additional parameter analysis is presented in Appendix B.\nAs shown in Figure 3, we train MIMOE-FND using with different modality interaction loss weights \u03b2\u2208 {0.1, 0.3, 0.5, 0.7, 0.9} on all three datasets. Across all datasets, most models with nonzero \u03b2 outperform the baseline with no interaction supervision (\u03b2 = 0). The robustness to a wide range of \u03b2 values underscores the adaptability of MIMOE-FND and its ability to effectively leverage modality interaction supervision for enhanced fake news detection across all datasets. We also analyze the effect of varying the learning rate \u03bb\u03b5 {10\u22122, 10\u22123, 104, 105, 10-6} for the interaction gating module, as presented in Figure 3. In Weibo-21, mid-range \u03bb values (10-3 to 10-4) yield optimal performance, achieving the best trade-off between accuracy and F1 scores for both fake and real news. Conversely, extreme \u03bb values (10\u22127 and 10\u2212\u00b9) result in performance degradation, likely due to insufficient or overly aggressive updates to the interaction gating network."}, {"title": "4.4 Gating Mechanism Case Study", "content": "To analyze the behavior of the modality interaction gating mechanism under modality interaction supervision and task supervision, we conduct case studies using an English dataset to ensure broader relevance. Specifically, we select four instances, each dispatched to one of the fusion experts, illustrating the four modality interaction scenarios. In Agreed Misalignment (AM; Figure 4a), both modalities align while providing complementary and unique contributions. Due to semantic misalignment, the combined prediction (\u0177 = 0.73) is reversed compared to the individual predictions, highlighting that the fact that the AM expert synthesizes complementary cues from both modalities, leveraging synergistic information to address semantic misalignment and make the correct prediction. In Agreed Alignment (AA; Figure 4b), both modalities produce aligned predictions with high semantic alignment, as indicated by a dispatch weight of 0.7. The final prediction closely mirrors the text-only prediction, demonstrating redundancy from both text and image. For Disagreed Misalignment (DM; Figure 4c), the DM expert effectively detects the fake news by reconciling the contradictory signals even with low semantic alignment. Finally, in Disagreed Alignment (DA; Figure 4d), semantic information aligns despite unimodal prediction disagreements, resulting in a synergistic prediction that achieves a higher confidence score than either modality alone. Here the DA expert is able to examine the conflict between image and text predictions when they are semantically aligned and derives the final detection based on semantic-level reasoning. These case studies demonstrate that MIMOE-FND effectively utilizes modality-specific contributions and their interplay, showcasing robust multimodal inference capabilities."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce MIMOE-FND, a novel hierarchical MoE framework for multimodal fake news detection. Our approach models modality interactions through unimodal prediction agreement and semantic alignment. MIMOE-FND employs unimodal prediction divergence and CLIP-based semantic evaluation to supervise an interaction gating network that dynamically routes data instances to specialized fusion experts for precise multimodal fusion. These experts leverage a modality-wise attention mechanism within an adapted MoE model to selectively attend to key modality features. Experimental results on three widely used multimodal FND datasets demonstrate that MIMOE-FND achieves superior performance compared to prior methods."}, {"title": "A EXPERIMENT SETUP", "content": "A.1 Dataset\nSince our method focuses on the modality interaction modeling, we select datasets with sufficient number of training data that has images and texts both accessible. Specifically, we consider three multimodal FND benchmark datasets commonly used by prior works, which are Weibo [16], Weibo-21 [22] and GossipCop [26]. All three datasets are collected from social media with binary labels indicating the veracity of the multimodal news. Both Weibo and Weibo-21 are Chinese datasets containing image and text pairs collected from social media platform Weibo. Weibo has 3749 real news and 3783 fake news in training set, 1000 fake news and 996 real news in the test set. Weibo-21 was created in 2021, where more recent social posts were collected with 4640 real news and 4487 news in total. GossipCop is a dataset collected from fact verification website, which contains 7974 real news and 2036 fake news for training and 2285 real news and 545 fake news for testing. For Weibo and GossipCop we keep the train-test splits provided by original datasets in our evaluation. For Weibo-21, where train-test split is not provided by original dataset, we keep the same train-test split at a ratio of 9:1 of BMR [37].\nA.2 Implementation Details\nIn our implementation, we use the pretrained checkpoint 'mae-pretrain-vit-base'\u00b9 as our image encoder. For the Chinese datasets (Weibo and Weibo-21), we use 'bert-base-chinese'\u00b2. as our text encoder, and for GossipCop, we use 'bert-base-uncased'\u00b3. For CLIP embedding extractions, we utilize 'clip-vit-base-patch16'4 for GossipCop and 'chinese-clip-vit-base-patch16'5 for the Chinese datasets.\nA.3 Baselines\nWe compare our method with a number of strong multimodal FND baselines, where we use their publicly available source code/pretrained model. We list the details of our baselines as follows:\n\u2022 EANN [32] is a multimodal FND solution that utilizes an event adversarial network to prevent model to overfit to a specific event.\n\u2022 SAFE [39] is a method utilizing a similarity calculation between different modality representations to guide the multimodal FND. In our experiments, we adopt the official code released by the author. For the Chinese datasets which are not considered by the official implementation, we use the same text encoder as our approach and a pretrained ViT-GPT image captioning model6."}, {"title": "B ADDITIONAL PARAMETER ANALYSIS", "content": "For \u03b2 values under the setting decribed in Section 4.3, as shown in Figure 3 and Figure 5, the performance gains are more pronounced and consistent in Weibo and Weibo-21, where all tested \u03b2 values improve detection accuracy over the baseline. The consistent improvements in Weibo datasets indicates a stronger sensitivity to interaction supervision, which is likely due to the diversity of observed modality interactions in social media posts. For GossipCop, on the other hand, we observe two \u03b2 values where the performance drop below the ablated baseline. The performance drop of these \u03b2 values show that with lower diversity of modality interactions, partially enforcing modality interaction supervision can introduce routing instability, disrupting the model's ability to leverage the dominant unimodal signals effectively. Overall, the robustness to a wide range of \u03b2 values underscores the adaptability of MIMOE-FND and its ability to effectively leverage modality interaction supervision for enhanced fake news detection across all datasets.\nFor \u03bb values, same as Weibo-21 results, mid-range \u03bb values (10-3 to 10-4) yield optimal performance. Notably, the GossipCop dataset shows higher sensitivity to \u03bb variations compared to Weibo and Weibo-21, which is likely attributed to the imbalance in the number of samples across modality interactions."}, {"title": "C QUALITATIVE ANALYSIS", "content": "In Figure 7, we extract feature vectors before classification heads for text-only, image-only and MIMOE-FND and visualize it using t-SNE visualizations. We observe that our method is able to achieve clear decision boundaries in all datasets. We also observe that compared to unimodal representations, MIMOE-FND features are more closely clustered, facilitating a more confident prediction."}, {"title": "D COMPLEXITY ANALYSIS", "content": "Complexity Analysis of iMoE Block: First, the token attention vector attx is computed using a two-layer MLP, with complexity O(Nd\u00b2) for an input sequence of shape (N, d). The attention vector performs a weighted aggregation over N tokens, adding O(Nd) complexity. The aggregated vector is then passed through another two-layer MLP to obtain the gate output, with complexity O(d2). Each of the ne expert networks applies a Vision Transformer block, with complexity O(N\u00b2. d + N \u00b7 d\u00b2) per expert, resulting in O(ne (N2d+Nd\u00b2)) for all experts. Finally, the gated the expert outputs aggregation adds a complexity of O(ned).\nSumming these terms, the overall complexity of the iMoE block is O(N\u00b7 d\u00b2 + d\u00b2 + ne \u2022 (N\u00b2 \u2022 d + N \u00b7 d\u00b2) + ne \u00b7 d), which simplifies to O(ne (N2. d + N \u00b7 d\u00b2)). This highlights dependencies on N (number of tokens), d (dimension of each token"}]}