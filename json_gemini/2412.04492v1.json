{"title": "Socio-Emotional Response Generation: A Human Evaluation Protocol for LLM-Based Conversational Systems", "authors": ["Lorraine Vanel", "Ariel Ricardo Ramos Vela", "Alya Yacoubi", "Chlo\u00e9 Clavel"], "abstract": "Conversational systems are now capable of producing impressive and generally relevant responses. However, we have no visibility nor control of the socio-emotional strategies behind state-of-the-art Large Language Models (LLMs), which poses a problem in terms of their transparency and thus their trustworthiness for critical applications. Another issue is that current automated metrics are not able to properly evaluate the quality of generated responses beyond the dataset's ground truth. In this paper, we propose a neural architecture that includes an intermediate step in planning socio-emotional strategies before response generation. We compare the performance of open-source baseline LLMs to the outputs of these same models augmented with our planning module. We also contrast the outputs obtained from automated metrics and evaluation results provided by human annotators. We describe a novel evaluation protocol that includes a coarse-grained consistency evaluation, as well as a finer-grained annotation of the responses on various social and emotional criteria. Our study shows that predicting a sequence of expected strategy labels and using this sequence to generate a response yields better results than a direct end-to-end generation scheme. It also highlights the divergences and the limits of current evaluation metrics for generated content. The code for the annotation platform and the annotated data are made publicly available for the evaluation of future models.", "sections": [{"title": "I. INTRODUCTION", "content": "New, powerful Large Language Models (LLMs) have widely democratised the use of text generation systems, spurring the field of Natural Language Processing toward a new era marked by attempts at reducing the gap between academic progress and day-to-day applications. Such use cases include motivational interviews [1], customer service [2] or assistance in psychotherapy sessions [3]. However, as these models are currently data-driven and generate textual content in a fully end-to-end manner [4], it is unsure how the social and emotional aspects of the responses formulated by these models, such as informing or sympathising, are planned and regulated. This work aims to join in the effort of building more trustworthy conversational systems."}, {"title": "II. RELATED WORK", "content": null}, {"title": "A. LLMs for Planning socio-conversational Response Gener-ation", "content": "Response planning is a crucial aspect of building effective and engaging dialogue systems, as it directly impacts the system's ability to maintain natural and contextually coherent interactions. Although end-to-end Large language models (LLMs) have demonstrated impressive skills, particularly in generating fluent text responses, they encounter difficulties with planning tasks. Fully end-to-end approaches such as [5] rely on the generation of data controlled by knowledge bases to fine-tune end-to-end models to implicitly integrate socio-emotional strategies. This type of approach gives no visibility or control over the socio-emotional strategy underlying the response, which raises questions of transparency. This is why we have chosen to focus on approaches that provide greater visibility by adding an explicit planning stage.\nNumerous research works have been undertaken for plan-ning socio-emotional strategies either by prompting LLMs or"}, {"title": "B. Socio-Conversational System Evaluation", "content": "In their survey, [11] lists the most used metrics for evalu-ating Empathetic Conversational systems and shows that Perplexity (PPL) is the most popular metric, closely followed by BLEU. Other approaches, such as n-gram-based or sentence-embedding similarity metrics are also commonly used. The survey also insists on the importance of human evaluation to properly evaluate specific user perception-related metrics [6], [7].\n[12] boasts a high number of labels and seems to provide a comprehensive evaluation of a response's overall quality. However, their protocol lacks nuance and detail when it comes to what we seek to study which is social and emotional consistency. Evaluating social and emotional content is vast; many aspects can be modelled and defined as evaluation criteria. Studies have focused on the evaluation of fluency [9], [13], relevance [9], [14] and empathy (defined as emotion appropriateness) [9], [15]. We decided to select a set of criteria inspired by these works, which include consistency (derived from the relevance criterion), fluency, and emotion adequacy (derived from the empathy criterion). We also define a new criterion based on social aspects: social adequacy.\nAfter defining the evaluation criteria comes the question of the evaluation method, particularly how to make the results reusable for comparison with future papers. Different methods exist to evaluate a response. Pair-wise or multiple choice testing [16] allows for a strong comparison between available models but makes it hard to compare with future models not considered during the ranking. Alternatively, rating-based systems such as a binary scale or a Likert scale [13], [17] are easier to benchmark, with Likert scales providing a more nuanced evaluation.\nHowever, for these annotations to be reliable, a good amount of data must be annotated, preferably by more than one human annotator, which amounts to an ever-growing evaluation cost. Evaluation costs also take into account the annotators' cognitive workload during the task. A popular method to decrease such costs is the use of semi-automatic annotation, which can for example entail training a classification model to pre-fill or assist the evaluation [18], [19]."}, {"title": "III. PROPOSED ARCHITECTURE FOR THE CONDITIONING BY SOCIO-EMOTIONAL STRATEGIES", "content": "The architecture we propose in this paper is composed of two modules, as illustrated on the right of Figure I: a first model is dedicated to predicting the sequence of socio-emotional strategies that the agent is expected to follow in the next speaker turn. Then, in the second module, this sequence is fed to a generative LLM to condition the selection of a final response from a set of generated candidate answers."}, {"title": "A. First Module: Next Strategies Prediction", "content": "Our goal is to develop a planning module to condition and control the next response generation for more socially relevant answers in dialogue. In particular, we are interested in the planning of two specific aspects of conversational strategies we will now refer to as socio-emotional strategies [19], [20]: Emotion-based strategies (i.e expressing happiness or anger) refer to approaches that involve the expression of emotion in response to a user's emotional state [21], [22]. Dialogue strategies (i.e informing, questioning) are a set of actions and behaviours used to express a conversational intent or goal [19], [23].\nWe consider the dialogue history of a conversation $C = (C_i)_{i\\in[1,t]}$, $C_t$ the current speaker turn, and $SE$ the list of socio-emotional labels. We are interested in predicting the succession of the socio-emotional labels expected to be displayed in the speaker turn $C_{t+1}$. We thus want to predict the following sequence: $Y_{t+1} = (Y_{t+1})_j)_{j\\in[1,l_{t+1}]}$ where $Y_{t+1} \\in SE$ and $l_{t+1}$ is the length of the sequence. To determine what model to use, we compared various prompt-based and fine-tuning approaches. We opted to use a fine-tuned BART Base model as it provided the best results on the Daily Dialog dataset (see Appendix A). It predicts on average 1.15 labels per utterance (min: 1 label, max: 3 labels), against the dataset ground truth's 1.20 labels per utterance on average."}, {"title": "B. Second Module: Socio-Emotional Response Generation", "content": "Once obtained, this sequence of labels $Y_{t+1}$ is used to condition the generation of the next speaker turn. Two types of methods are investigated here: i) A prompt-based approach where LLMs are instructed to generate a response given a 3-turn dialogue history and the expected sequence of socio-emotional strategies; ii) a reranking approach such as in [8]. For each test sample, a generative model receives the last 3 turns of the dialogue history as input and generates multiple alternative answers (N = 10). To identify the labels present in the generated candidate speaker turns, we train a BERT classifier\u00b2 on the Daily Dialog dataset.\u00b3 Each candidate is fed to this BERT classifier and the resulting list of labels, $l_k$, is compared to the sequence of expected labels, $Y_{t+1}$. For each candidate k, we use the Normalised Levenshtein Similarity (NLS) to obtain a similarity score between the socio-emotional labels k predicted by the BERT classifier and the expected labels $Y_{t+1}$ predicted by the first module using the context history. The candidate with the highest similarity score is selected as the final response: $argmax_kNLS(l_k, Y_{t+1})$. The conditioning of the response is meant to guarantee that the model generates adequate content that is consistent with both the interaction's context and the social and emotional context of the user."}, {"title": "IV. EXPERIMENTAL PROTOCOL", "content": "We design an experimental protocol to answer our research question: Does conditional generation improve the quality of the response? To that end, we use generative models to compare responses generated without conditioning (no-CD, i.e., considering the first most probable speaker turn outputted by the generative model) and those planned with socio-emotional strategies: i) CD-pred using the labels predicted by the first module; ii) CD-GT using the same labels as the ones of the ground truth (that are the ones of the human speaker turn in the test set). For each context, the models first generate 10 responses. Then, we follow the method described in Section III to rerank the set of candidates and select the one that best matches the expected socio-emotional labels as our final response."}, {"title": "A. Experimental Setting", "content": "a) Models: We compare the following models\u2074 (details in Appendix B):\nGPT-2 We fine-tune both GPT-2 Small (117M parameters) and GPT-2 Medium (345M parameters) [24].\nDialoGPT We fine-tune both DialoGPT Small (124M parameters) and DialoGPT Medium (355M parameters) [25] to generate an answer given a dialogue context.\nBART Like in the first experiment, we consider both BART Base (140M parameters) and BART Large (406M parameters).\nBeluga Lastly, we use Beluga (13B parameters) for the prompt-based alternative. We try two approaches: i) Beluga R (Reranking): We instruct Beluga to generate N = 10 responses for each test sample. This is to test Beluga's generation on the reranking approach, comparable to the other models. ii) Beluga PB (Prompt-Based): We directly instruct Beluga to generate a response to the 3-turn context using a certain tone conditioned"}, {"title": "B. Automated Evaluation", "content": "To evaluate the quality of the generated responses, we use various metrics implemented in the HuggingFace evaluate library. We use string-based metrics (Sacrebleu [27], Rouge [28] and chrf [29]), as well as embedding-based metrics (BERTscore [30] (between the generated candidate and the reference)) to measure the quality of the generated content. Both approaches are based on a comparison of the generated content to the dataset reference. We also look at reference-free metrics: i) the BERTScore to measure the distance between the generated candidate and the context history (BERTscore context), ii) Perplexity (PPL) [31] that measures how well a language model predicts a text sample."}, {"title": "C. Human Evaluation", "content": "While these automated metrics are convenient and easily accessible, most of them are dependent on the reference which makes them obsolete when it comes to evaluating tasks such as response generation: to the same context, many responses can be appropriate, even if they are very different from the ground truth.\nTo obtain dataset-independent results that reflect this fact, we perform a human evaluation on a randomly selected sample of 300 contexts extracted from the test set. After running each {model, conditioning} combination over our test dataset, a list of 23 generated responses per context is obtained, to which the human reference found in the dataset is added. Since the CD-GT and CD-pred conditioning methods rely on a reranking approach based on the same pool of 10 generated candidates, they can often select the same candidate. For each context, the duplicates are thus removed.\nThe annotation process is divided into three steps, to reduce the workload for the annotators. First, the responses associated with the same context are divided into those that are consistent and those that are not. Second, the best responses among the consistent ones are selected by the annotators. Third, once only the \"best\" responses remain, they will then be annotated with more precise criteria."}, {"title": "V. RESULTS & DISCUSSION", "content": "Table I summarises the results discussed in this section. We observe that conditioning yields slightly better results on both automated and human evaluation metrics. Then, we see that conditioning on predicted labels does not seem to induce a significant decrease in results compared to the \"ideal\" ground-truth conditioning.\na) Results of the evaluation of the consistency criteria (Evaluation Steps 1-2): Out of the 24 available responses, there is an average of 19 considered responses once we have removed the duplicate answers to the same context.\nSTEP 1: Human evaluation eliminated on average 10 can-didates per context, to retain 9. The human reference is consistently better and is deemed as \"relevant\" 87% of the time. When we look at the generated responses, we notice that only GPT-2 Medium and DialoGPT Medium, as well as BELUGA PB CD-pred, are saved more than 50% of the time.\nSTEP 2: For the top 3, as two annotators judged each set of responses, the overlap shows that the size of the union of the selected top-3 responses is 4, while the average intersection size of the two top-3 is 1.6. The human reference is chosen as part of the top-3 best responses 61% of the time. CD models tend to do better, with BELUGA CD-pred significantly outperforming the other generative approaches. Some models obtain very low results on this task, namely the Beluga R models, but also the Base / Small models. This second step allows us to mark the gap between the better models (Beluga PB and NO-CD, GPT-2 Medium, DialoGPT Medium) and the rest, highlighting the difference in quality that might not have been as obvious after the first consistency filter in Step 1.\nb) Results of the socio-emotional criteria evaluation (Evaluation Step 3): The results of the annotation of fine-grained socio-emotional criteria seem to show that both CD and NO-CD responses, once filtered by consistency, tend to be of equally good quality across all three axes: logical, emotional and social. It is important to keep in mind that for this step, only 59 contexts were annotated out of the 300 considered in the previous steps (around 250 individual responses), so the sample is quite smaller than for the previous task. Beluga R models are not represented as they were very seldom selected in the annotators' top-3.\nAs specified previously, the socemo score combines both the logical, emotional and social consistency ratings, as well as the frequency with which the model was selected as one of the top-3 best responses to a context amidst the 24 available responses. Overall, Beluga PB CD-GT is the model, apart"}, {"title": "VI. CONCLUSION", "content": "This paper is the first to tackle the task of jointly predicting explicit dialogue and emotion-based strategies to condition response generation using LLMs. This novel approach requires the release of new resources, which is why we propose a dual contribution: First, we propose an architecture to condition the response generation by a set of dialogue and emotion-based strategies. Then, to properly evaluate our approach, we describe a new human evaluation protocol for socio-emotional response generation and introduce a novel criterion for social adequacy. This protocol aims to reduce the annotation costs without sacrificing the evaluation's depth and precision and is validated by a satisfactory inter-annotator agreement. The details are presented in Appendix D and both the code for the evaluation interface as well as the data (samples of the Daily Dialog dataset) annotated by our team are shared for comparison of future models.\nThe evaluation leads to two main results: i) Conditioning improves the quality of the generated response, both on the general consistency of the answer, as well as the finer-grained social and emotional criteria. Conditioning on dataset labels is often equivalent to conditioning on predicted labels, which means that even if the intermediate step does perfectly predict the sequence of labels, it is close enough to obtain results similar to the ideal dataset-assisted scenario; ii) Contrasting automated metrics and the human results show that while automated metrics manage to pick up some general trends of the quality evaluation, they are still unable to capture important information, especially when it comes to social behaviours. Current automated metrics do not suffice to properly evaluate the quality of a response.\nOur future work includes exploring new LLM-based con-ditional generation approaches and comparing them to the baseline established in this paper, to develop a dialogue system able to generate responses that are both context-relevant as well as socially and emotionally consistent. We also mean to investigate the influence of planning emotions and dialogue strategies individually to explore their individual contribution to the socio-emotional quality of the response."}, {"title": "VII. ACKNOWLEDGMENT", "content": "This work was partially funded by the ANR-23-CE23-0033-01 SINNet project. We thank our team of conversation analysts at Zaion for their diligence and hard work in carrying out the evaluation of the responses, allowing us to provide insight and reliable results in this study."}, {"title": "VIII. ETHICAL IMPACT STATEMENT", "content": "This study features an evaluation carried out by a team of human linguists, and it is important to note that annotation includes biases. They can be related to the personal and cultural experiences of the annotators, which may influence their perception of emotions and interactions. Thus, the data we provide in this study may contain some biases on how emotion is perceived and labelled, but communication and reference materials were shared between all annotators to curb these differences as much as possible.\nOn another note, modular architectures allow for a more explicit selection of dialogue policies, which is not the case in end-to-end approaches. The current NLP trends seem to favour end-to-end approaches, especially as large LLMs have proved their proficiency, but it often comes at the price of transparency. This research seeks to find a middle ground between the more rigid architecture of modular systems with the computation power or larger end-to-end solutions while trying not to compromise transparency.\nWe aim to develop a system that can accurately generate a response that matches the social and emotional tone of the user's utterances as well as the context, but we want to do so in the most transparent way possible, with a model able to justify its output with understandable arguments. It is also important to note that in the realm of conversational Al agents equipped with social and emotional capabilities, a noteworthy emerging risk lies in their potential to sway consumers towards making purchases or believing misinformation.\nTo our knowledge, this paper is the first to propose an approach that combines explicit planning with LLMs. It also involves jointly predicting emotion-based strategies as well as dialogue strategies, which we haven't seen being done in the literature. To coordinate these two novel concepts into a single architecture, we first opted for a system with simple, explicit modules to supervise the two steps of the process (planning, and then generating), before we can move on to more complex alternatives. We are also aware that it is crucial to study cultural differences when it comes to social interactions. However, the lack of resources that include both emotion and dialogue acts annotations as well as culturally rich dialogues currently does not allow us to provide a reliable generalisability on this aspect.\nWhile we are working on this subject to contribute to the scientific community and to improve the quality of the service that agents offer by being more tuned to the users' emotional and social situations, we are aware that it could be used in defective ways. We believe that communicating and informing the users of such systems is crucial to developing their awareness of such potential risks, as well as protecting them for their future interactions with Al systems."}, {"title": "APPENDIX A: CHOICE OF THE NEXT LABEL PREDICTION MODEL", "content": "In this first step, we aim to evaluate the performance of various models on the task of predicting a sequence of labels that models the social and emotional behaviours that are expected to be displayed in a generated response to a conversational context. In other words, we want to test the first step of our approach and determine the most suitable model to use as the planning module.\na) Data Preprocessing: We work with the Daily Dialog dataset. For each speaker turn, we consider 3 dialogue turns as the \"context\" and pair them with the label(s) of the following utterance to constitute a training sample. The model thus learns how to predict the labels of the next speaker turn. Our resulting train/validation/test splits are made up of 76052 / 7070 / 6740 samples.\nb) Models: All the models we present were trained using a single GPU (NVIDIA RTX 8000, 48GB memory), with the hyper-parameters described in the Appendix ??. We describe the models used in Experiment 1 below:\nc) BERT - Multilabel Classification: BERT Base (110M parameters) and BERT Large (340M parameters) [1] are trained on a multi-label classification task. We set the con-fidence threshold at 0.7 for BERT Base and 0.5 for BERT Large.\nd) BART Sequence Generation: BART Base (140M parameters) and BART Large (406M parameters) [2] are fine-tuned on the task of generating the next labels sequence.\ne) Beluga - Prompt-Based Generation: We use Beluga (13B parameters), a Llama2 model [3] fine-tuned on an Orca style dataset, to generate the sequence of the next labels using few-shots prompt-based generation. Beluga was prompted to generate the sequence of labels associated with the following speaker turn, given a dialogue utterance. The prompt used is: Predict the sequence of labels associated with the utterance that follows the given dialogue.\nf) Metrics: As a sub-task of the response generation process, label sequence prediction is a one-to-many problem: many sequences can match a same context. However, effi-ciently evaluating the relevance of a sequence of labels to a context remains a challenging task due to the lack of suitable metrics. Thus, to evaluate this experiment, we must rely on comparing the pairs of sequences: the generated or predicted sequence, and the expected sequence."}, {"title": "APPENDIX B: DETAILS ON THE MODELS USED FOR CONDITIONAL GENERATION", "content": "Here are the main hyper-parameters used to train each model presented in this paper. Each model was trained using a single GPU (NVIDIA RTX 8000, 48GB memory).\nA. Beluga: Prompts for Conditional Response Generation\nMultiple prompts were tested to optimise the results and here are the final instructions used to train Beluga for the two experiments. Here, N is the number of sequences to be generated. In this paper, we used N = 10. Element refers to the dialogue history considered, we use a window of 3 utterances of context. We set the dialogue history in the format: SPEAKER A: utt1 SPEAKER B: utt2 SPEAKER A: utt3.\na) Beluga F&R: For the generation of a single response, 'NO-CD' task, the prompt used is: Generate the response following the given context.\nb) Beluga PB: In this case, 'element' still stands for the 3-turn context, and 'labels' is the sequence of expected labels. The expected labels can either come from the dataset (task CD-GT) or from the prediction of a BART generative model (task CD-pred). The prompt used is: Generate the response following the given context : + element The tone of the response must be + labels"}, {"title": "APPENDIX C: GENERATION EXAMPLE", "content": "Let's consider the following dialogue:\nSPEAKER A: Good morning. What's the matter with you?\nSPEAKER B: Good morning, Doctor. I have a terrible headache.\nSPEAKER A: All right, Young man. Tell me how it got started."}, {"title": "APPENDIX D: COMPLETE FORM FOR HUMAN EVALUATION STEP 3", "content": "In this Appendix, we present the details and reference materials that were provided to the human judges during the evaluation task. Steps 1 and 2 relied on the definitions for Consistency and Specificity given in the paper. For Step 3, the annotators first had to tag each response with dialogue responses. Daily Dialog uses a system of 4 dialogue acts:"}, {"title": "V. APPENDIX E: DETAILED RESULTS OF HUMAN EVALUATION", "content": "In Table V, you will find the details of all scores obtained from the human evaluation we carried out on Daily Dialog. While the socemo score is weighted by the number of responses by the model in the annotated sample, the logical, emotional and social ratings are unweighted. We weigh the fluency score similarly to the socemo score to compare it to the Perplexity metric."}, {"title": "VI. APPENDIX F: RESULTS ON NEW DAILY DIALOG DATASET", "content": "Instead of using the huggingface dataset, which was re-ported to have a significant overlap between the test and train sets, we use the splits provided in Daily Dialog's original paper, which do not display the same duplicate issue. In our original experiments, we had not fine-tuned our Beluga models (inference only), so those results are unaffected by the test-train set data overlap. We reran our code on the remaining models - BART, DialoGPT and GPT2 - using the same GPU and hyper-parameters as in the main paper). These results, available in Table VI are similar to those obtained with the test-train sets duplicates. While we do not claim that using the huggingface splits displaying duplicates did not have any negative impact on the training, this new set of results seems to indicate that this impact might not be too significant or invalidate the results shown in this study."}]}