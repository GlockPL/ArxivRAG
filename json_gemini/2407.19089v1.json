{"title": "Many-Shot In-Context Learning for Molecular Inverse Design", "authors": ["Saeed Moayedpour", "Alejandro Corrochano-Navarro", "Faryad Sahneh", "Shahriar Noroozizadeh", "Alexander Koetter", "Ji\u0159\u00ed Vym\u011btal", "Lorenzo Kogler-Anele", "Pablo Mas", "Yasser Jangjou", "Sizhen Li", "Michael Bailey", "Marc Bianciotto", "Hans Matter", "Christoph Grebner", "Gerhard Hessler", "Ziv Bar-Joseph", "Sven Jager"], "abstract": "Large Language Models (LLMs) have demonstrated great performance in few-shot In-Context Learning (ICL) for a variety of generative and discriminative chemical design tasks. The newly expanded context windows of LLMs can further improve ICL capabilities for molecular inverse design and lead optimization. To take full advantage of these capabilities we developed a new semi-supervised learning method that overcomes the lack of experimental data available for many-shot ICL. Our approach involves iterative inclusion of LLM generated molecules with high predicted performance, along with experimental data. We further integrated our method in a multi-modal LLM which allows for the interactive modification of generated molecular structures using text instructions. As we show, the new method greatly improves upon existing ICL methods for molecular design while being accessible and easy to use for scientists.", "sections": [{"title": "1. Introduction", "content": "Molecular design is fundamentally about crafting novel molecules that possess specific properties tailored to address particular scientific or industrial challenges (Schneider &\nFechner, 2005). This process involves not only creating\nnew molecular structures, but also refining and improving existing ones to more efficiently meet specific criteria. The\nrefinement helps in converging a broad range of properties\nthat the molecules need to satisfy, including affinity for\nparticular biological targets, favorable pharmacokinetics and\npharmacodynamics, or non-toxicological profiles, making\nthem suitable for further development and eventual clinical\ntrials. This is typically achieved through iterative cycles of\nsynthesis, testing, and analysis, with each iteration aiming\nto bring candidates closer to the optimal properties.\nEvery factor to be considered introduces additional layers\nof complexity and uncertainty, making the lead optimiza-\ntion and molecular design a resource-intensive and time-\nconsuming task. Artificial intelligence (AI) emerges as\na transformative tool capable of identifying and learning\nfrom complex patterns that can guide on the design of new\nmolecules with optimized features. Furthermore, it can\nautonomously propose novel compounds that fulfill spe-\ncific criteria, reducing the cycle time in iterative design and\nsynthesis, and accelerating the development of new drugs\n(Langevin et al., 2020; Loeffler et al., 2024; Sauer et al.,\n2023).\nGenerative AI models are trained to understand and approx-\nimate the real distribution of the data they are fed with.\nEssentially, these models internally create a representation\nof the distribution such that the outcomes they produce ad-\nhere to the underlying rules governing the nature of the\ndata (Bilodeau et al., 2022). In the context of chemistry,\nthis 'data' refers to chemical space, which encompasses\nall possible chemical compounds and their configurations.\nChemical space is vast and complex, containing an almost\ninfinite variety of potential molecules with diverse physical\nand biological properties. Generative models can thus be\nused to propose novel molecules in an efficient and effective\nmanner (Meyers et al., 2021).\nMolecules can be represented using textual symbols cor-\nresponding to their structures. The most common descrip-\ntion language for small molecules is SMILES (Simplified\nMolecular-Input Line-Entry System). Similar to other lan-\nguages (including English and Amino Acids) several as-\npects of the SMILES language can be adequately captured"}, {"title": "2. Methods", "content": ""}, {"title": "2.1. Datasets", "content": "We used several datasets containing molecular activity data\nagainst 15 different pharmaceutically relevant proteins tar-\ngets (Sauer et al., 2023). Molecules were filtered by keeping\nonly those compounds with specific activity data, i.e., IC50,\nKi, and posterior filtering by pCHEMBL values.\nNext, molecules for each dataset were clustered and sorted\nbased on activity using similarity of circular fingerprints. A\nproperty profiling of each cluster is conducted by including\nphysicochemical and substructure properties, SA Score (Ertl\n& Schuffenhauer, 2009) to estimate synthetic accessibility,\nor ligand efficiency among others. Top scoring cluster was\nselected and the 20 most active analogs from pCHEMBL\n(best20) were chosen for experimental testing samples. In\naddition, the 50 most active compounds from the remaining\nclusters were used as lead structures for generative design\n(pool50). An additional pool (allminus20) is created con-\ntaining all compounds belonging to the same target except\nfor those being part best20.\nEach pool contains the molecular structures represented\nas SMILES together with their corresponding activities,\nmolecular weights, SA score, topological polar surface area\n(tPSA), and lipophilicity (logP)."}, {"title": "2.2. Molecular generation and evaluation details", "content": "In order to perform many-shot in-context learning (ICL)\nexperiments and design new molecules, we used the Claude\n3 Sonnet model (Anthropic, 2024) with a 200k context win-"}, {"title": "2.3. Few-shot to many-shot ICL", "content": "To assess the capability of LLMs for generating molecules\nwith specific properties and to evaluate the performance\ngain by transitioning from few-shot to many-shot settings,\nwe conducted several experiments. In the first experiment,\nwe included 5 to 500 molecular SMILES strings and activ-\nity examples from the training data in the prompt to allow\nthe LLMs to learn chemical patterns and substructures cor-\nrelated with the activity. The provided examples in each\nsetting were obtained by sorting the training data based on\ntheir activities and selecting the top performers. The model\nwas then tasked with modifying a given lead molecule to\ngenerate new molecules with high activities and output them\nin a specific JSON format. Figure 7 illustrates the structure\nof the input prompt in a multi-objective setting with several\nconditions on desired properties."}, {"title": "2.4. Iterative inclusion of self-generated data for ICL", "content": "The training dataset contains a limited number of highly\nactive molecules. In a many-shot setup with 100 training\nexamples, the model generates novel and diverse molecules\nwith high activities. However, when moving to larger sample\nsize (500 molecules) the added shots, which make up the\nmajority of examples, have low activity and the distribution\nof activities for generated molecules shifts toward lower\nactivity regions.\nTo address this issue and utilize the full benefits of many-\nshot ICL, we explored an approach for the iterative inclusion\nof high-performing generated molecules and their predicted activities in the context, in addition to the experimental\nmolecules and labels. For the predictive activity models, we\ntrained several tree-based boosting models using three dif-\nferent types of features, namely circular fingerprints, RDKit\ndescriptors, and Mol2Vec (Jaeger et al., 2018) molecular\nfeatures.\nCircular Fingerprints are generated using a hash function\nthat transforms the local environment E of each atom into a\nbit in a fixed-size vector. For each atom, the environment\nis defined by the type of atom and its neighbors up to a\nspecified radius r:\n\\(X_{cf}[i] =  \\bigoplus_{atom j\\in Atoms(s)} Hash(E(j, r))\\)\nHere, \\(X_{cf}[i]\\) is the i-th bit of the fingerprint vector, \\(\\bigoplus\\) de-\nnotes a bitwise OR operation across all atoms, and E(j, r)\nrepresents the substructure around atom j within the given\nradius. Hash is a function that maps each unique substruc-\nture to a bit position.\nRDKit computes a set of predefined descriptors for each\nmolecule. These descriptors are functions of the molecular\nstructure, such as molecular weight, logP, and the count of\nvarious types of bonds and atom types. The generation of a\ndescriptor vector can be mathematically represented as:\n\\(X_{rd} = [f_1(s), f_2(s), ..., f_n(s)]\\)\nWhere \\(f_k(s)\\) is the k-th descriptor function applied to the\nSMILES string s, and n is the total number of descriptors\ncalculated by RDKit.\nMol2Vec converts each molecule into a vector by first tok-\nenizing the SMILES string into meaningful chemical frag-\nments and then mapping each fragment to a vector in a\npre-trained embedding space:\n\\(X_{mv} = \\frac{1}{m} \\sum_{k=1}^{m} Vec(Token_k)\\)\nIn this equation, m is the number of tokens in the SMILES\nstring s, Tokenk is the k-th token, and Vec(Tokenk) is the\nvector representation of Tokenk from a pre-trained embed-\nding model. The final feature vector Xmv is the average of\nall token vectors, providing a dense representation of the\nmolecule.\nEach feature set Xcf, Xrd, and Xmv is then used as input\nto separate gradient boosting models, which are trained to\npredict molecular activity a' based on these distinct repre-\nsentations:\n\\(M(X) \\rightarrow a'\\)"}, {"title": "2.5. Semi-Supervised training for few shot learning", "content": "To increase the number of high active samples we designed\nan iterative process in which we included newly generated\nmolecules whose predicted activity from all three activity\nprediction models was higher than a cutoff. As discussed\nabove, while there is a small overlap in the input features\nused by the three methods their are also unique features\nused by each one. For the selected generated molecules,\nthe average predicted activity was used as the final activity\nlabel in the context. We define M as an LLM model that\ngenerates new SMILES strings based on the current context\nS and corresponding activities A. The function M operates\nas follows:\n\\(M(S, A) \\rightarrow S'\\)\nThe initial context, So, consists of 500 experimental\nmolecules represented by SMILES strings along with a\nlist of their corresponding activities A0:\n\\(S_0 = {s_{0,1}, s_{0,2},..., s_{0,500}}\\)\n\\(A_0 = {a_{0,1}, a_{0,2},..., a_{0,500}}\\)\nDuring each iteration i, the context S is expanded by in-\ncluding newly generated SMILES strings S' with predicted\nactivities A' that exceed a cutoff C value determined by the\n80th percentile of activities from the current context:\n\\(S_{i+1} = S_i \\cup {s' \\in S' | a' > C_i  s' = M(S_i, A_i)}\\)\n\\(A_{i+1} = A_i \\cup {a' | a' > C_i  a' = M(S_i, A_i)}\\)\nUsing this approach, the context expands by LLM gener-\nated molecules with high predicted activity. We observed\na improvement in the distribution of activities of generated\nmolecules within a few iterations. The results are further\npresented and discussed in section 3.1."}, {"title": "2.6. ICL for multi-objective molecular design", "content": "To further verify the capabilities of LLMs for learning the re-\nlationships between chemical structures and specified prop-\nerties, especially in multi-objective settings, we performed\nexperiments where additional property conditions on molec-\nular weight, synthetic accessibility (SA) score, tPSA, and\nlogP were imposed in the context in addition to activity."}, {"title": "2.7. ICL for property prediction", "content": "We investigated the ability of SMILESs based LLMs to\nlearn quantitative structure-activity relationships (QSAR)\nthrough in-context learning (ICL) for activity prediction\ntasks. To evaluate this, we performed cross-validated ex-\nperiments where we compared the performance of LLMs\nfor activity prediction tasks through many shot ICL with\ndifferent CatBoost regression models. These models were\ntrained using described input features, including circular fin-\ngerprints, RDKit descriptors, and Mol2vec features. To use\nLLMs for predicting activities, we provided the SMILES\nand activity examples from the training datasets and asked\nthe model to predict the activity of a certain SMILES in the\ntest datasets. We repeated this query for every individual\nSMILES in the test dataset, as well as all train/test datasets\nof the cross-validation folds. The results showcasing the\ncapability of LLMs for QSAR and predicting activities from\nSMILES are discussed in Section 3.3."}, {"title": "2.8. Interactive molecular design", "content": "Our proposed ICL workflow can successfully generate novel\ncandidate molecules with multiple target properties. How-\never, the generated molecules are not always ideal. In many\ninstances, a generated molecule with high predicted perfor-\nmance may be potentially appealing to an expert chemist,\nbut could be significantly enhanced by a minor modifica-\ntion, particularly in terms of synthesizability. In such cases,\ndirectly modifying the SMILES string, especially for large\ncomplex molecules, can pose challenges. Following our ob-\nservations of the profound chemical understanding of LLMs,\nwe have developed an online interactive design module that\nallows for seamless modification of generated molecules\nusing text instructions. This module can take the SMILES"}, {"title": "3. Results and Discussion", "content": ""}, {"title": "3.1. Iterative many-shot ICL with self generated data\nfor molecular design", "content": "Figure 1A compares the distribution of molecular activi-\nties against the MMP8 protein target in the lead molecules\ndataset (Lead) and the utilized subset of training datasets in\nthe 5 to 500 shot ICL experiments, which include the top 5\nto 500 highly active molecules. As can be seen, there is only\na limited number of highly active molecules. This leads\nto performence decline when using more than the top 10\nmolecules (Figure 1B). The upper quartile of the distribution\nof generated molecules still increases with more molecules,\nbut even the top ones deteriorate when including more than\n100 shots (Figure 1B). In contrast, for the iterative design\nlearning that we developed results continue to improve even\nafter 10 iterations and 1125 molecules. In addition, the\nFCD distances of generated molecules from the pool of\nmolecules significantly increase in the iterative procedure\n(Figure 1C), demonstrating the generation of more novel\ncandidates due to the model's access to a wider variety of\nchemical fragments in the context.\nTo utilize the full potential of many-shot ICL for gener-\nating novel molecules with high predicted activities, we\nperformed iterative ICL experiments. In each iteration, in\naddition to 500 shots of experimental data, we added gen-\nerated molecules with high predicted activity from the pre-\nvious iteration. The selected generated molecules have a\npredicted activity above a cutoff (80th percentile of activities\nin the training data) by all three trained activity prediction\nmodels. We observed a shift in the distribution of activi-\nties of generated molecules toward the high activity region\nwithin a few iterations, as shown in Figure 1D."}, {"title": "3.2. Multi-objective molecular design with many-shot\nICL", "content": "Figure 2 presents the distribution of different properties in\nthe 500-shot ICL generation experiments where, in addi-\ntion to molecular activities, additional property conditions\nand labels were introduced in the context. All generation\nexperiments shown in Figure 2 include a condition on gen-\nerating highly active molecules (activity above 10, with\nhigher values being better) and also a secondary condition\non molecular weight between 320-420 (Figure 2A), low syn-"}, {"title": "3.3. Molecular property prediction with many-shot ICL", "content": "Our results demonstrate that Large Language Models\n(LLMs) can effectively learn the quantitative structure-\nactivity relationships through ICL. The performance com-\nparison of activity prediction with MMP8 protein target for\nLLMs and CatBoost models, which were trained with dif-\nferent input features, is presented in Table 1. Additionally,\nFigure 1 provides visual representation of the scatter plots\nof predicted activities obtained by different models against\nthe experimental values of activity for the same protein tar-\nget. Although LLMs did not surpass the performance metrics\nof the CatBoost models, their ability to learn structure activ-\nity relationships was evident. These results are in agreement\nwith their capability for generating molecules with target\nproperties through many-shot ICL, showing that LLM can\nleverage the learned structure-activity relationships to gen-\nerate new highly active molecules."}, {"title": "3.4. Interactive molecular design", "content": "We developed an interactive user interface that utilizes\nLLMs to perform structural modifications. These modi-\nfications include adding, removing, or replacing functional\ngroups, as well as more general physiochemical changes\nin steric hindrance, polarity, and hydrophobicity of differ-\nent molecular fragments. Figure 5 shows some example\ninput molecules, instructions, and the generated modified\nmolecules. This tool not only facilitates the design and mod-"}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Technical details", "content": "Our workflow utilizes the LiteLLM package, which allows for seamless usage of a wide variety of deployed LLMs through\nBedrock, Hugging Face, OpenAI, etc., as the molecular generation engine. For interactive modification of generated\nmolecules using expert instructions, we explored both the Claude 3 Sonnet and OpenAI GPT-4 models. Although further\nextensive benchmarks are needed, we empirically observed a deeper understanding of chemical structure and a better\ncapability for following instructions for structural modification by GPT-4."}, {"title": "A.2. Comparison to other SOTA molecular design tools", "content": "We compared the performance of our framework with REINVENT 4 which is a modern open-source generative A\u0399\nframework for the molecular design. Reinvent4 uses a reinforcement learning for molecular design using a SMILES as the\nrepresentation of the molecules. The algorithm for molecular optimization implemented in the package utilizes Mol2Mol - a\ntransformer-based conditional prior model. It was trained by systematic and exhaustive exploration of chemical space while\nbeing regularized on the molecular (Tanimoto) similarity. Therefore, the similarity can be directly to the corresponding\nnegative log-likelihood of the generated molecular representation. In coarse of molecular optimization, the likelihood of\ngenerated molecule is additionally biased by a scalar score evaluating the targeted chemical properties as well as diversity"}]}