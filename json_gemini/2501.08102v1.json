{"title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media", "authors": ["Wenlu Fan", "Yuqi Zhu", "Chenyang Wang", "Bin Wang", "Wentao Xu"], "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using two open-source models: Gemma and Llama. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic similarity between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: Gemma shows a tendency toward negative emotion amplification, particularly anger, while maintaining certain positive emotions like optimism. Llama demonstrates superior emotional preservation across a broader spectrum of affects. Both models systematically generate responses with attenuated emotional intensity compared to human-authored content and show a bias toward positive emotions in response tasks. Additionally, both models maintain strong semantic similarity with original texts, though performance varies between continuation and response tasks. These findings provide insights into LLMs' emotional and semantic processing capabilities, with implications for their deployment in social media contexts and human-AI interaction design.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) represent one of the most significant yet controversial technological advancements in recent years. These models demonstrate unprecedented and expanding human-like capabilities, particularly in text generation, enabling diverse applications including text summarization (van Schaik and Pugh 2024), translation (Sung et al. 2024), and news writing (Mu\u00f1oz-Ortiz, G\u00f3mez-Rodr\u00edguez, and Vilares 2024). Consequently, LLM-based applications have proliferated across domains, from conversational agents (Dam et al. 2024) to educational assistants (Liu, Jiang, and Wei 2025).\nDespite their advantages, LLMs raise significant concerns regarding potential negative implications. These include content fabrication, commonly termed \"hallucination,\" which contributes to misinformation propagation (Huang et al. 2023). Furthermore, research indicates that LLM-generated content perpetuates societal biases encountered during training, potentially exacerbating AI fairness issues (Gallegos et al. 2024; Ayoub et al. 2024). Additionally, LLMs can influence human decision-making processes, potentially leading to unintended consequences through emotional manipulation or deception (Park et al. 2024). Given their widespread deployment, careful evaluation of LLMs' text generation capabilities becomes imperative.\nLLMs exhibit both task-specificity and context-sensitivity, with performance varying across different applications and contextual settings (Sung et al. 2024; Li, Zhang, and Sun 2023). Consequently, evaluating their text generation capabilities within realistic, socially relevant contexts becomes crucial. Social media platforms, serving as extensive networks for information exchange, provide valuable digital artifacts for such investigations.\nIn social media contexts, LLM text generation manifests in two primary forms: response tasks (e.g., replies) and continuation tasks (e.g., summarization and dialogue). The generated content influences public perception and engagement on social media platforms. Emotion embedded within text plays a crucial role as it can be rapidly activated and disseminated through extensive social networks, potentially facilitating emotional contagion (Kramer, Guillory, and Hancock 2014). Consequently, emotion serves as a strategic tool for engagement and persuasion in social media environments (Stieglitz and Dang-Xuan 2013; Hamby and Jones 2022).\nPrevious investigations of emotional effects on social media have employed real-life digital experiments through content manipulation (Kramer, Guillory, and Hancock 2014). However, such methodologies raise ethical concerns regarding unauthorized manipulation of user content and have generated public discomfort (Boyd 2016). LLMs offer a more ethically sound approach to examining emotional dynamics in social contexts. Given their increasingly sophisticated human-like capabilities, LLMs are extensively employed in simulating social interactions (Gao et al. 2024a). This LLM-based simulation methodology presents two key advantages:"}, {"title": "", "content": "(1) AI agents can serve as safer substitutes for human participants in extreme or sensitive scenarios, and (2) they enable more controlled experimental conditions, facilitating precise examination of relevant variables.\nWith the widespread adoption of LLMs in generating human-like content, it becomes imperative to understand the consistency of LLM-generated text and its potential societal impact. Accordingly, this study investigates LLM text generation tasks (response generation and continuation) through systematic analysis of emotional consistency and semantic similarity. By examining these dynamics within climate change communication\u2014a highly polarized and emotionally charged domain-this research addresses the following questions:\nRQ1: How consistent are the emotions expressed in text generated by LLMs on social media?\nRQ2: How does the emotional intensity of text generated by LLMs compare to text on social media?\nRQ3: To what extent do LLMs demonstrate semantic similarity between generated text and text on social media?\nBy answering the research questions, this study has the following contributions:\n\u2022 Although LLMs demonstrate remarkable human-like capabilities in text generation, the mechanisms underlying their outputs remain insufficiently understood. Through analysis of emotional consistency between human- and LLM-generated text in social media contexts (response and continuation tasks), this study provides a deeper, contextualized understanding of LLMs' text generation performance. The identified distinctions between human- and LLM-generated content illuminate how these models navigate social media interactions, which prompts critical ethical considerations regarding Al's role in human interaction.\n\u2022 Emotion represents a fundamental behavioral response and crucial element in social media discourse. While previous research has predominantly focused on human-human communication, raising ethical concerns (Ferrara and Yang 2015), this study implements an LLM-based simulation approach. This methodology replicates human-AI agent interactions while addressing ethical limitations inherent in traditional research approaches, offering an innovative and ethically sound framework for emotion research.\n\u2022 Through analysis of real-world social media datasets concerning controversial scientific topics such as climate change, this study simulates scenarios where LLM-enabled tools participate in public discourse, presenting both opportunities and challenges (Feng et al. 2024). These findings enhance our understanding of communication dynamics and catalyze discussions regarding LLMs' role in emotional guidance within controversial scientific discourse on social media platforms."}, {"title": "Related Works", "content": "Evaluation of LLMs generated text\nThe evaluation of LLM-generated text originates from natural language generation (NLG), defined as the process of computationally producing human-comprehensible text (Sai, Mohankumar, and Khapra 2022). Given the widespread deployment of AI models in text generation, extensive research has explored effective evaluation frameworks for NLG (Sai, Mohankumar, and Khapra 2022). Traditional evaluation metrics, primarily focused on quantifying content overlap between system outputs and references (Gao et al. 2024b), such as BLEU (Papineni et al. 2002) and ROUGE (Lin 2004), have served as standard metrics for automatically assessing output quality in machine translation and summarization tasks. However, these metrics demonstrate limitations when applied to complex, context-dependent tasks, particularly in the current generative AI paradigm (Gao et al. 2024b). Consequently, researchers have developed novel benchmarks for task-specific LLM evaluation (e.g., (Que et al. 2024)), while recent studies have proposed methodologies leveraging LLMs themselves for evaluation purposes (see (Gao et al. 2024b) for a comprehensive review).\nThe evaluation of LLM-generated text consistency with human behavior represents a fundamental approach to assessing model performance. Alignment with human behavior and response patterns remains a central objective in artificial intelligence development (Russell and Norvig 2016). Consistency is crucial for operational reliability and safety of LLMs, ensuring they can generate contextually appropriate and relatable outputs. Additionally, semantic similarity serves as an established metric for quantifying textual consistency (Chandrasekaran and Mago 2021). Researchers have evaluated LLM output consistency through semantic similarity measures and developed enhancement strategies to improve human alignment (Yang et al. 2024; Raj et al. 2023).\nExisting literature predominantly examines distinctive characteristics between LLM- and human-generated text. For instance, (Herbold et al. 2023) conducted comparative analyses of human-written versus ChatGPT-generated essays across dimensions including topical coverage, logical structure, vocabulary usage, and linguistic constructions through human assessment. Beyond manual annotation, (Guo et al. 2023) implemented a mixed-methods approach to analyze LLM/human-generated responses across linguistic dimensions, revealing that LLM outputs demonstrate enhanced logical coherence, comprehensive detail, and reduced bias. (Mu\u00f1oz-Ortiz, G\u00f3mez-Rodr\u00edguez, and Vilares 2024) employed quantitative analysis to compare human- and LLM-authored news content across morphological, syntactic, psychometric, and sociolinguistic dimensions. Through automated analysis, (Zanotto and Aroyehun 2024) identified distinctive linguistic patterns in text length, variability, syntactic complexity, and lexical diversity."}, {"title": "Text generation on social media context", "content": "In the social media environment, LLM text generation offers significant applications, including AI-powered social bots for online discourse participation, discussion summarization tools, and related applications (Li et al. 2024). However, ensuring generated text consistency requires careful consideration of contextual factors and interaction objectives. Social media interactions encompass both response generation (e.g., comment replies) and content continuation (e.g., social bot engagement). While existing research provides empirical evidence comparing human and LLM-generated content, the evaluation of social media-specific tasks, particularly responses and continuations, warrants comprehensive evaluation to understand LLM text generation in dynamic social media contexts.\nAlthough emotion serves as a crucial factor in social media engagement and persuasion, its utilization as an evaluative feature for text generation remains insufficiently explored. Current comparative studies of human and LLM-generated text focus predominantly on static contexts, overlooking emotional dynamics. For instance, comparative analysis of human-written versus LLM-generated news content revealed stronger negative emotional expression in human-authored texts (Mu\u00f1oz-Ortiz, G\u00f3mez-Rodr\u00edguez, and Vilares 2024). Similarly, while (Guo et al. 2023) examined response differences through multilingual sentiment classification, this approach presents limitations for comprehensive emotional analysis (e.g., distinguishing between joy and sadness). Given the dynamic nature of social media interactions, evaluation of emotional consistency in information exchange becomes crucial.\nEmotional content permeates social media discourse and functions as a crucial determinant in shaping public opinion (Naskar et al. 2020). Emotion demonstrates high susceptibility to influence and serves as a critical factor in controversial and uncertain social agendas, including epidemics (Lu and Hong 2022), disasters (Chu et al. 2024), and polarizing social issues such as climate change (Brady et al. 2017). In these contexts, emotional responses can exert both beneficial and detrimental effects on public discourse. Climate change discourse, in particular, represents an extensively studied yet remains highly polarized domain, characterized by persistent denialism and skepticism (Treen, Williams, and O'Neill 2020; Whitmarsh 2011). These misconceptions frequently leverage emotional appeals, particularly fear, to influence public perception (Martel, Pennycook, and Rand 2020). Clinical psychology research has established correlations between anger, elevated cortisol responses to stress, and increased vulnerability to misinformation (Sharma, Wade, and Jobson 2023).\nAbove all, evaluating emotional patterns across response and continuation tasks within climate change discussions on social media provides a crucial framework for comparing LLM and human-generated content in dynamic, real-world scenarios."}, {"title": "Methodology", "content": "Experimental Design\nFigure 1 illustrates the overall setup of the experimental design. In our experiment, we used two open-source large language models: Gemma\u00b9 and Llama\u00b2, developed by Google and Meta, respectively. Specifically, we chose Gemma2-27B-Instruct-Q8 and Llama3-70B-Instruct, respectively, both of which excel in both performance and robustness. We utilized Ollama\u00b3 as a framework to enable the two open source models to run on our local server.\nOllama supports various functions such as model creation, content generation, chat and embedding calculation. In our study, we mainly used the chat and embedding calculation. For the response task, we called its chat function directly, and unlike normal content generation functions, it allows the large language model to talk directly to the input content, which means there is no limitation of prompt words. This enables the most intuitive observation of the state of the large language model as an interlocutor, yielding more realistic and direct data. When it comes to the continuation task, we used the content generation function, which needs a prompt to ask the LLMs to expand the text it received. Again, here we made our prompts as concise as possible to minimize the impact of the prompts on the model. We tell the LLMs that \"Assuming you are the author of this text, stand in your shoes and continue to expand the passage as you understand it.\""}, {"title": "Dataset", "content": "This study utilized climate change corpora collected from Twitter (now X) and Reddit. We collected data using the Twitter Search API by querying relevant keywords, including climate change\", climate science\", climate manipulation\", climate Engineering\", climate Hacking\", climate modification\", Global Warming\u201d, carbon footprint\u201d, and \u201cThe Paris Agreement\u201d. For Reddit, we used data maintained by Pushshift from https://the-eye.eu/redarcs/. The Pushshift Reddit dataset consists of two sets of files: submissions and comments(Baumgartner et al. 2020). The same keywords were applied to filter Reddit data, and to compare the differences in emotions, we collected both posts and comments from both platforms.\nWith the keywords, we obtained 5,768,822 Reddit comments and 76,596,654 tweets from Twitter. We used histograms to understand the basic distribution of data (Figure 2). To ensure temporal representation and minimize the impact of special events, we employed proportional sampling, selecting 200 rows per month systematically from Twitter data and 100 rows per month from Reddit data. This sampling strategy yielded a final dataset comprising 12,200 rows of Twitter data and 10,900 rows of Reddit data."}, {"title": "Emotion Labeling", "content": "In this study, we developed a methodology to analyze emotions in cross-platform social media data using a deep neural network-based model. We employed the twitter-roberta-base-emotion-multilabel-latest\u2074 model from Hugging Face to examine the emotional content of both original texts and content generated by large language models. This model, built upon the ROBERTa-base architecture, is a fine-tuned version of \"cardiffnlp/twitter-roberta-base-2022-154m\" optimized on the SemEval 2018 Task 1 dataset-Affect in Tweets.\nThe twitter-roberta-base-emotion-multilabel-latest model identifies eleven distinct emotion categories: anticipation, joy, love, optimism, surprise, trust, anger, disgust, fear, pessimism, and sadness. The model outputs probability scores for each category, which serve as quantitative measures of emotional content for subsequent analysis. While previous studies used sentiment analysis (negative, positive, and neutral) for evaluating differences between human- and LLMs-generated text (Guo et al. 2023), our emotion-based approach provides a more granular and nuanced understanding of the underlying emotional states in posts."}, {"title": "Semantic Similarity", "content": "To assess both the comprehension capabilities of LLMs and their content generation accuracy, we employed cosine similarity as a quantitative measure of semantic alignment between LLMs' outputs and human-generated texts. This methodology also enables the detection of semantic aberrations, commonly referred to as \"hallucinations\", which frequently manifest in LLM outputs (Breum et al. 2023).\nCosine similarity, a fundamental metric in natural language processing, has demonstrated its utility across various text mining applications, including text classification, summarization, information retrieval, and question answering systems (Li and Han 2013). The mathematical foundation of this metric ensures robust comparison of textual similarities, making it particularly suitable for our analysis.\nDue to the architectural differences between the two models, we implemented separate embedding calculations for LLM-generated and human-authored texts using both Gemma and Llama. The semantic similarity computations were performed using the cosine similarity function from the sklearn library. For each platform, we derived four distinct similarity metrics quantifying the semantic relationships among four text pairs: Gemma's continuation text, Gemma's response text, Llama's continuation text, and Llama's response text, each compared against the original human text. To evaluate the comparative performance across different tasks and models, we employed the Mann-Whitney U-test to determine the statistical significance of variations in cosine similarity scores."}, {"title": "Results", "content": "Emotion Dynamics of the Original Text in Downstream Tasks In this study, we examined the emotional transitions between human-generated text and LLM outputs in downstream tasks. We categorize 11 kind emotions into those that are positively oriented and those that are negatively oriented as followed(Robinson 2008) :\nPositive emotions: anticipation, joy, love, optimism, surprise, trust(Vaillant 2008);\nNegative emotions: anger, disgust, fear, pessimism, sadness\nAnalysis of Figure 3a reveals that 62% of texts initially labeled as angry maintained their emotional valence in Gemma's continuations. In contrast, only 11% of originally optimistic texts maintained their optimistic valence in the continuations. Other emotional categories, including anticipation, disgust, fear, joy, and sadness, predominantly shifted toward anger in the continuations, with transition rates of 29%, 44%, 30%, 39%, and 31%, respectively. Notably, optimism and surprise exhibited distinct patterns: 43% of optimistic texts preserved their emotional valence in the Gemma task, while texts expressing surprise demonstrated consistent emotional preservation. These findings suggest that the Gemma model exhibits a systematic tendency to transform diverse emotional expressions into anger during continuation tasks, indicating a bias toward negative emotional content, particularly anger. However, its performance with optimism and surprise demonstrates capacity for emotional preservation, suggesting selective ability to maintain certain emotional contexts throughout the generation process. In the analysis of emotional transitions during Gemma's response task, we observed a significant shift in emotional valence, with more than 50% of initially angry texts transitioning toward anticipation and optimism in the responses.\nOur analysis reveals that over 50% of texts with negative emotional valence transitioned toward positive expressions, particularly anticipation and optimism, in the response tasks. Texts with initial positive valence consistently preserved their affective characteristics, manifesting as anticipation, joy, or optimism. This pattern highlights Gemma's systematic bias toward positive affect during response tasks. Significantly, a subset of original emotions still transitioned to anger in the responses, suggesting Gemma's persistent sensitivity to anger-related content across both response and continuation tasks.\nExamination of Llama 3's performance, presented in Figure 3c and Figure 3d, demonstrates enhanced capability in emotional recognition and preservation during continuation tasks, maintaining original emotional valence with greater consistency.\nThe underlying mechanisms of emotional preservation in LLM-generated text appear to involve multiple factors: emotion-specific lexical choices, syntactic structures, and contextual integration during the generation process, coupled with the model's capacity for sustaining emotional patterns. Emotions characterized by prominent semantic features (e.g., intense affective vocabulary) appear more readily preserved by the model.\nQuantitative analysis of emotional transitions reveals that 21% of texts originally expressing anticipation, 48% expressing joy, and 17% expressing optimism transitioned to anger expressions. This pattern can be attributed to two primary factors: first, the semantic prominence of anger, characterized by explicit and intense features (e.g., critical, adversarial language) that are more readily recognized by the model during generation tasks; second, the contextual fragility of positive emotions, which typically require more complete contextual support, whereas LLM-generated content appears to favor more salient negative emotional expressions.\nConsistent with Gemma's response patterns, Llama's response task demonstrated that, excluding fear, transitions to anticipation and optimism exceeded 50% across emotional categories. This consistent positive bias in response tasks across both models suggests a systematic emotional bias embedded during training, particularly evident in interactive contexts.\nIn addition to Reddit, we analyzed data from Twitter, which exhibits distinct discourse patterns surrounding climate change. Figure 4 illustrates the emotional transitions between original Twitter content and LLM-generated responses. In Gemma's continuation and response tasks, we observed that regardless of the original emotional valence-whether positive (anticipation, joy) or negative (anger, disgust)-the generated content predominantly expressed anger and anticipation. Among these transitions, anger represented the highest proportion, while anxiety emerged as the predominant emotion, followed by anticipation. A notable distinction between the two tasks was the increased frequency of responses transitioning toward anticipation. In Llama's continuation task, the original emotional content similarly demonstrated a predominant shift toward anger. In contrast, Llama's response task exhibited a primary shift toward anticipation. The performance patterns of both Gemma and Llama models across continuation and response tasks reveal two key insights: first, Gemma's heightened sensitivity to anger-related content, and second, the models' systematic bias toward positive affect when functioning in interactive dialogue contexts."}, {"title": "Resources of LLMs' Generated Content Emotions", "content": "We analyze the emotional sources of LLM-generated content by examining the relationship between input and output emotions. In Gemma's continuation task, Figure 5a reveals that positive emotions in generated content primarily derive from positive emotional sources. For instance, joy-labeled content originates from anticipation (32.04%), joy (25.24%), and optimism (13.59%). Conversely, content expressing negative emotions predominantly stems from negative emotional sources, with anger-labeled content derived primarily from original anger (61.8%) and disgust (13.03%). This pattern suggests Gemma's tendency to maintain emotional valence consistency during content continuation.\nFigure 5c illustrates Llama's continuation task results, which differ from Gemma's pattern. Most emotional content, except for optimism and joy, originates predominantly from negative emotional sources. While the preservation of negative emotional sources aligns with Gemma's behavior, Llama distinctively generates positive emotional content primarily from negative emotional sources, potentially indicating an inherent positive affect bias.\nFigure 5b,d illustrate the response tasks for Gemma and Llama, respectively. In Gemma's responses, most positive emotional content, except joy, originates from predominantly negative emotional sources. Negative emotional content maintains its source valence, with anger-labeled content derived 69.55% from original anger expressions. Similarly, Llama's responses demonstrate consistent transformation of negative emotions into positive ones, exemplified by anticipation-labeled content originating 37.23% from anger. However, negative emotional content maintains its original valence.\nThese findings demonstrate LLMs' systematic transformation of negative emotions into positive ones during response tasks, while simultaneously exhibiting some degree of negative emotional preservation in their responses."}, {"title": "Comparative Analysis of Emotional Intensity between LLMs and Human Text", "content": "Beyond examining emotional transitions, we investigated the quantitative differences in emotional intensity between LLM-generated and human-authored content. This analysis specifically focused on determining whether LLM-generated content exhibits higher or lower emotional intensity compared to human expressions. To quantify emotional content, we employed a probabilistic model that assigns normalized scores (0 to 1) to each emotional category within the text. These probability values were interpreted as emotional intensity scores (Miyazaki et al. 2024). We categorized emotional expressions into five distinct intensity groups based on these scores. Statistical analysis comprised an ANOVA test to evaluate differences in emotional intensity across groups, followed by Tukey's post-hoc test to identify significant pairwise variations(Elnaggar, Mohamed, and Gehan 2024). Analysis of variance (ANOVA) results presented in Table 11 indicate statistically significant differences (P < 0.05) across the five groups in emotional intensity values for anger, anticipation, disgust, optimism, fear, joy, and sadness on Twitter. Similar significant variations were observed in the Reddit dataset for anger, anticipation, disgust, optimism, fear, and sadness. Tukey's post-hoc analysis identified several significant differences across three comparison categories: within-model, between-model, and model-to-human comparisons.\nOn Twitter discussions of climate change, within-model analyses revealed that Gemma's continuation content exhibited significantly lower anticipation values compared to its response content. Similarly, Llama's continuation content showed significantly lower anticipation values than its response content.\nBetween-model comparisons demonstrated that Gemma's continuation content expressed significantly higher anticipation values than Llama's continuation content. Additionally, Gemma's response content showed significantly higher anticipation values compared to Llama's responses. In model-to-human comparisons, both models' generated content (continuation and response) demonstrated significantly lower anticipation values compared to the original human text.\nThese findings demonstrate that: (1) both models express higher anticipation values in response tasks compared to continuation tasks; (2) Gemma consistently generates content with higher anticipation values compared to Llama across both tasks; and (3) both models generate content with significantly reduced anticipation values compared to human-authored text, indicating a systematic reduction in emotional intensity during the generation process.\nFurther analysis of additional emotional dimensions revealed that Tukey's post hoc test results indicate significant differences (P < 0.05) between LLM-generated and original texts across multiple emotional categories. Both Gemma and Llama's generated content, in both continuation and response tasks, exhibited significantly lower intensities of positive emotions, particularly joy and optimism. A nuanced pattern emerged in the expression of negative emotions. The continuation texts generated by both models demonstrated attenuated levels of sadness, anger, disgust, and fear compared to their respective response texts and the original human content. Notably, Gemma's response text exhibited significantly lower intensities of anger, disgust, and fear compared to Llama's responses. These findings suggest two key insights: first, LLMs demonstrate systematic suppression of certain negative emotions, particularly in continuation tasks; second, the response task appears to operate under distinct generative mechanisms, resulting in differential emotional expression patterns. Furthermore, the consistent reduction in optimism across all LLM-generated texts relative to human-authored content indicates a systematic constraint in LLMs' capability to fully capture and convey positive emotional states."}, {"title": "Semantic Coherence Analysis between LLM-Generated and Human-Authored Content", "content": "In LLM-human interactions, beyond examining emotional congruence and intensity patterns demonstrated in our previous experiments, we investigated the models' capacity to maintain topical coherence and generate contextually relevant responses. To quantify this relationship, we employed cosine similarity as a metric to assess semantic alignment between generated and original content. This approach provides a quantitative framework for evaluating the semantic fidelity of LLM-generated responses across different interaction contexts.\nTable 4 presents the Mann-Whitney U test results across the four experimental conditions. The analysis reveals that both Gemma and Llama maintained substantial semantic alignment with the original text in their continuation and response tasks. Figure 6a illustrates that semantic similarity values on the Twitter platform predominantly exceeded 0.5, indicating strong semantic coherence between LLM-generated content and input text. This suggests the models' capability to comprehend and generate contextually relevant responses while maintaining topical coherence. Similar patterns emerged in the Reddit dataset, as shown in Figure 6b, where all four sets of cosine similarities demonstrated values above 0.5.\nThe Mann-Whitney U test (Table 4) revealed significant within-model and cross-task variations in semantic similarity. On the Twitter platform, Gemma's continuation task demonstrated significantly higher semantic similarity compared to its response task, while Llama exhibited the opposite pattern, with responses showing significantly higher similarity (P < 0.01). Cross-model analysis revealed task-specific differences: Gemma's continuation task demonstrated significantly higher semantic similarity compared to Llama's continuation task (P < 0.01), while Llama achieved higher similarity scores in the response task. On Reddit, Llama's continuation content exhibited significantly higher semantic similarity compared to its response content (P < 0.01). Furthermore, Gemma's generated content demonstrated significantly higher semantic similarity compared to Llama's in both continuation and response tasks (P < 0.01)."}, {"title": "Discussion", "content": "This study investigated the consistency between LLM-generated content and human input, with particular focus on emotional content transmission. Given that text often encodes substantial emotional information, we hypothesized that LLMs may demonstrate diverse emotional patterns. As LLMs increasingly integrate into human social contexts through chatbots and social agents, understanding their semantic consistency and emotional transmission patterns becomes crucial for human-machine interaction. We decomposed the fundamental question of LLM emotional capability into three specific research questions, based on our empirical findings. Several key insights emerged from our analysis.\nIn the continuation task, Gemma demonstrated a systematic transformation of most original emotions toward anger, indicating a bias toward negative emotional expression, particularly in anger intensification. However, Gemma exhibited capacity for emotional preservation, specifically for optimism and surprise, suggesting ability to perceive and maintain certain emotional valences through the continuation process. Conversely, the Llama model demonstrated enhanced capability in preserving original emotional states during continuation tasks, particularly for anger, anticipation, fear, optimism, and sadness, with relatively lower emotional transformation rates, indicating superior emotional continuity (RQ1).\nWe posit that during continuation tasks, LLMs employ emotion-specific lexical markers, syntactic patterns, and contextual cues to maintain emotional consistency with source texts. Emotions characterized by prominent semantic features (such as anger, anticipation, and sadness) typically manifest through distinct emotional vocabulary and strong affective markers, facilitating model recognition and continuation. Moreover, the semantic salience of anger may enhance its recognition and preservation in generative tasks, while positive emotions like optimism or joy may require more robust contextual support, making them susceptible to disruption by LLM-generated negative content.\nIn the response task, both Gemma and Llama models exhibited a systematic bias toward positive emotions, predominantly converting original emotions into anticipation and optimism. This pattern suggests an embedded bias from the training process favoring positive affect. However, the persistent presence of anger in a subset of responses demonstrates these models' susceptibility to negative emotional content during the response generation process.\nOur experimental findings indicate that both Gemma and Llama models exhibit emotional capabilities and can effectively recognize and maintain human emotional states in continuation tasks. This emotional recognition capability is fundamental for LLMs, as it underlies their ability to comprehend both user requirements and emotional context.\nRegarding emotional intensity (RQ2), our results demonstrate that LLM-generated content exhibits significantly attenuated intensity across most emotional dimensions compared to human-authored texts. This suggests that LLMs exhibit more moderated emotional expression, maintaining more constrained emotional ranges across both positive and negative affects. Furthermore, the consistently lower emotional intensity values in both continuation and response tasks, compared to original texts, indicates potential limitations in LLMs' capacity to fully capture and convey emotional depth.\nAnalysis of emotional types reveals distinct patterns in LLM-generated content. Regarding positive emotions, LLM-generated texts demonstrate significantly lower intensities of joy and optimism compared to human-authored texts, suggesting limitations in the models' capacity to fully comprehend and express positive emotional states characteristic of human experience. Similarly, for negative emotions, LLM-generated content exhibits attenuated intensities of sadness, anger, disgust, and fear, indicating systematic suppression of negative emotional expression in downstream tasks.\nIn the context of global challenges such as climate change, which transcends temporal, spatial, and geographical boundaries and impacts long-term human development, public discourse exhibits diverse emotional responses. As social media platforms rapidly evolve, they become crucial channels for public communication during climate-related events, such as the Australian bushfires. The effective management of public emotions during critical periods following such events represents a key component of public opinion governance. Recent advances in Artificial Intelligence Generated Content (AIGC), driven by Generative AI (GAI) technology, have garnered attention beyond computer science (Cao et al. 2023). Given the increasing integration of LLMs into daily life, their emotional characteristics significantly influence opinion leadership, as emotional content shapes public perception and discourse framing.\nOur experimental results (Figure 3 and Figure 4) demonstrate that LLMs systematically transition toward positive emotions during response tasks. This positive bias suggests potential utility in public opinion events, offering constructive responses and mitigating negative public sentiment. However, the observed patterns of negative emotional transmission pose potential societal risks. LLMs' capability to generate contextually relevant content could be exploited for malicious purposes. Beyond concerns regarding misinformation dissemination, the potential intensification of emotional polarization presents a significant challenge in LLM deployment.\nFrom the perspective of human-computer interaction, emotional support serves as a fundamental component in enhancing social interactions, facilitating psychological interventions, and improving customer service outcomes through addressing emotional needs. The quality of emotional support and user understanding significantly impacts long-term user engagement and trust in LLM interactions (Schneider, Flores, and Kranz 2024).\nAddressing our third research question, we analyzed embedding representations of generated and original content, employing cosine similarity as a metric for semantic proximity. Our findings indicate that both Gemma and Llama consistently demonstrate high cosine similarity values, suggesting robust capability in capturing and reproducing semantic features. Models have shown a remarkable ability to represent, comprehend, and generate human-like text. Compared to prior Natural Language Processing (NLP) approaches, one of the most striking advances of LLMs is their ability to generalize their \"knowledge\" to novel scenarios, contexts, and tasks(Peters and Matz 2024) Future implications suggest that while LLMs demonstrate competence in generating semantically coherent content, opportunities exist for enhancing alignment between generated outputs and nuanced human context. Future research directions should explore mechanisms for refining LLMs' comprehension of implicit meaning and contextual subtleties, thereby enhancing user experience and expanding application domains."}, {"title": "Limitation and Future Work", "content": "This study contributes to understanding the emotional dynamics of human-AI interactions, while suggesting several avenues for future research based on current limitations.\nFirst, regarding experimental design, our study was limited to data from Reddit and Twitter platforms, potentially under-representing the broader social media ecosystem. Other platforms such as YouTube, Instagram, and TikTok exhibit distinct user behaviors and content structures (Hilde A. M. Voorveld and Bronner 2018). Furthermore, our analysis was bounded by the capabilities of open-source emotion models (Llama and Gemma), which may demonstrate different performance characteristics compared to proprietary commercial models like ChatGPT or Google's Bard. Future research could expand the scope by incorporating data from additional social media platforms and evaluating commercial models to enhance generalizability.\nSecond, our experimental scope, focused on English-language climate change discourse, provides limited insight into model performance across diverse topics and languages. The emotional expression patterns of LLMs may vary significantly across different subject domains. Future research directions could incorporate multilingual datasets to identify cross-linguistic semantic variations (Zhao et al. 2020) and investigate potential biases within LLMs across isolated or intersecting semantic spaces (Hassan et al. 2018).\nThird, while we utilized social media to obtain human-authored content, the dataset potentially includes automatically generated content and social bot interactions that may not accurately represent genuine human expression. Future research should implement more rigorous data validation protocols to enhance dataset quality and control for confounding variables."}, {"title": "Conclusion", "content": "This study examined the semantic and emotional consistency of content generated by LLMs. Recognizing the emotional information embedded in text and the way LLMs handle these emotions is essential as they become more involved in human social contexts.\nOur findings revealed that Gemma demonstrated a tendency to amplify negative emotions, particularly anger. Despite this bias, Gemma effectively perceived and transferred the original emotional tone of the text. On the other hand, the Llama model displayed stronger emotional retention across a broader spectrum, including anger, expectation, fear, optimism, and sadness, with fewer transitions to other emotions. For the semantic problem, both models show better performance for both continuation and response. LLMs can understand human input to some extent. These observations highlight distinct emotional dynamics between LLMs, offering insights for enhancing their design and application in emotion-sensitive contexts."}]}