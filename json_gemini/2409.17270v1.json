{"title": "PROOF OF THOUGHT : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning", "authors": ["Debargha Ganguly", "Srinivasan Iyengar", "Vipin Chaudhary", "Shivkumar Kalyanaraman"], "abstract": "Large Language Models (LLMs) have revolutionized natural language processing, yet they struggle with inconsistent reasoning, particularly in novel domains and complex logical sequences. This research introduces PROOF OF THOUGHT, a framework that enhances the reliability and transparency of LLM outputs. Our approach bridges LLM-generated ideas with formal logic verification, employing a custom interpreter to convert LLM outputs into First Order Logic constructs for theorem prover scrutiny. Central to our method is an intermediary JSON-based Domain-Specific Language, which by design balances precise logical structures with intuitive human concepts. This hybrid representation enables both rigorous validation and accessible human comprehension of LLM reasoning processes. Key contributions include a robust type system with sort management for enhanced logical integrity, explicit representation of rules for clear distinction between factual and inferential knowledge, and a flexible architecture that allows for easy extension to various domain-specific applications. We demonstrate PROOF OF THOUGHT's effectiveness through benchmarking on StrategyQA and a novel multimodal reasoning task, showing improved performance in open-ended scenarios. By providing verifiable and interpretable results, our technique addresses critical needs for AI system accountability and sets a foundation for human-in-the-loop oversight in high-stakes domains.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have revolutionized the field of AI and enabled a wide range of applications. However, as these models are increasingly deployed to process unstructured data and perform complex tasks autonomously, their inconsistent reasoning capabilities remain a critical limitation [Marcus, 2020]. This inconsistency manifests in variable performance across out-of-domain reasoning, negation understanding, and extended logical chains, suggesting a reliance on superficial heuristics [Bender et al., 2021]. The implications are far-reaching, particularly in high-stakes domains where reliable and transparent decision-making is crucial [Rudin, 2019]. Errors or biases in these contexts could have severe consequences, underscoring the urgent need for more dependable and interpretable AI systems.\nRecent advances in prompt engineering have shown promise in addressing these challenges. Techniques such as Chain-of-Thought (CoT) [Wei et al., 2022], Self-Consistency with CoT (CoT-SC)"}, {"title": "2 Related Work", "content": "Early Integration Attempts laid the groundwork for neuro-symbolic AI. Works like EBL-ANN [Towell and Shavlik, 1994], KBANN [Towell et al., 1990], and C-ILP [d'Avila Garcez et al., 2009] incorporated propositional formulae into neural networks. While pioneering, these approaches struggled with scalability and expressiveness. Knowledge Graph integration advanced the field further. Methods proposed by Chen et al. [2020a] and Kampffmeyer et al. [2019] showed promise in leveraging structured knowledge for improved reasoning. However, maintaining interpretability and explicit rule incorporation remained challenging. Differentiable Logic Programming frameworks like DeepProbLog [Manhaeve et al., 2018] and Scallop [Li et al., 2023] demonstrated the potential of integrating probabilistic logic programming with neural networks. These approaches enable end-to-end training of neuro-symbolic systems but face limitations in handling complex reasoning tasks and diverse logical formalisms. Gupta and Kembhavi [2023] showed how compositional visual reasoning can be done by program generation without training.\nLarge Language Models have opened new avenues for neuro-symbolic reasoning. Techniques such as Chain-of-Thought [Wei et al., 2022], Tree-of-Thoughts [Yao et al., 2024], and Graph-of-Thoughts [Besta et al., 2024] have shown impressive results in complex reasoning tasks. However, these methods often produce inconsistent intermediate steps and struggle with out-of-domain reasoning. Interpretable Concept-based Models, as explored by Kim et al. [2018] and Chen et al. [2020b], aim to increase trust in deep learning models. However, state-of-the-art approaches often rely on high-dimensional concept embeddings that lack clear semantic meaning, limiting their interpretability."}, {"title": "3 PROOF OF THOUGHT: A Neurosymbolic Reasoning Framework", "content": "In this section, we introduce PROOF OF THOUGHT (PoT), a novel framework that bridges NLP with formal logical reasoning to enhance the interpretability and verifiability of LLM outputs. We first outline the foundational concepts and notations that underpin our approach."}, {"title": "3.1 Background Concepts & Notation from LLM Reasoning Literature", "content": "Let p\u03b8 denote a pre-trained language model (LM) with parameters \u03b8. In a conversation, user messages (prompts) and LM replies (thoughts) are exchanged. We use lowercase letters x, y, z, ... to indicate LM thoughts, where the definition of a \"thought\" is use-case specific (e.g., a paragraph, document, or code block). The simplest Prompting Approach approach is Input-Output (IO) where an LM directly transforms an input sequence x into output y without intermediate steps. Chain-of-Thought (CoT) introduces intermediate thoughts a1, a2, between x and y, enhancing performance on tasks like mathematical reasoning.\nMultiple CoTs generalizes CoT by generating k independent chains and selecting the best output based on a prescribed scoring metric. This approach, introduced as Self-Consistency with CoT (CoT-SC), allows exploration of different reasoning paths. Tree of Thoughts (ToT) further enhances CoT-SC by modeling reasoning as a tree of thoughts. Each node represents a partial solution. For a given node, k new nodes are generated, then evaluated using an LM or human scores. The tree expansion is guided by search algorithms like BFS or DFS. Finally, Graph of Thoughts (GoT) extends"}, {"title": "3.2 Framework Overview", "content": "\"All our knowledge begins with the senses, proceeds then to the understanding, and ends with reason. There is nothing higher than reason.\"\nImmanuel Kant, in Critique of Pure Reason\nPROOF OF THOUGHT models the LLM's reasoning process as a structured transformation from natural language input to formal logical expressions that can be verified using theorem proving techniques. The framework consists of three primary components :\n\u2022 Logical Representation Generator G: Maps input x to a logical representation L using p\u03b8.\n\u2022 Interpreter I: Parses L and constructs formal logical expressions \u00a2 in first-order logic (FOL).\n\u2022 Theorem Prover T: Verifies the validity of \u00a2 and provides proofs or counterexamples.\nThe PoT reasoning process can thus be formalized as:\n$L = G(x; p_\\theta); \\phi = I(L); Verification Result = T(\\phi)$\nGuarantees: By using theorem proving, we rely on the principle of logical consequence, where conclusions are guaranteed to be true if the premises and inference rules, depicted in the logical representations (both of which are human readable, allowing interpretability and verifiability). These logical representations are the ultimate arbiter of truth and validity. The guarantees are what dis-tinguishes PoT from other forms of reasoning. Inductive reasoning (drawing general conclusions from specific observations) can be useful, but doesn't offer the same level of certainty. In contrast, guarantees with theorem proving allow us to establish precise arguments with mathematical truths with absolute confidence, elevating the reasoning process beyond mere conjecture or intuition.\nOur PROOF OF THOUGHT framework architecture (shown in Fig 1) introduces a JSON-based Domain Specific Language (DSL) along with the associated interpreter. Next, we discuss the design choices for these in detail."}, {"title": "3.3 Design of the JSON-Based Domain-Specific Language (DSL)", "content": "The core of our logical reasoning system is built upon a carefully designed JSON-based Domain-Specific Language (DSL) that serves as an intermediate representation for translating reasoning tasks into formal logic. This DSL was created with the primary challenge of being general-purpose enough to accommodate a wide range of reasoning problems. The choice of JSON as the underlying format was deliberate, leveraging its widespread use, human readability, and ease of parsing. This design decision ensures that the logical representations are both machine-parseable and accessible to users who may not have expertise in formal logic or programming. Moreover, JSON's compatibility with structured outputs from AI service providers like OpenAI and Google, which offer guaranteed outputs matching particular schemas, makes it an ideal choice for our system that doesn't rely on retraining models. The key components of the DSL are:\n1. Sorts (S) define the domains or types used in the logic. Let S = {S1, S2, . . ., Sn} be the set of sorts, where each Si represents a specific domain. The inclusion of sorts in our system is a key differentiator. It allows for reasoning over high-level, human-understandable concepts, which is crucial for bridging the gap between natural language problem descriptions and formal logical representations. The sort definition in Fig 2 allows for more intuitive problem representation. For instance, instead of reasoning about abstract entities, we can now reason about Persons, Equipment, Tasks, Locations, and Time. This makes it easier to translate natural language problems into formal logic. The type-safe reasoning catches semantic errors early. For example, if we tried to apply a function meant for Equipment to a Person, the system would catch this error before any reasoning takes place. Moreover, this\n2. Functions (F) definitions in our system go beyond simple predicates, allowing for rich, typed relationships between sorts. Let f : S1 \u00d7 S2 \u00d7... \u00d7 Sk \u2192 Sr represent a function, where S1, S2,..., Sk, Sr \u2208 S. For predicates, Sr = Bool.\nFor example, in Fig 2, the function definition allows for complex domain modeling. For instance, 'assigned_to\u02bb represents a relationship between Tasks and Persons, 'location_of\u02bb transforms a Person into a Location, and 'skill_level' represents a property of a Person in relation to a specific Task. The type-checking ensures that functions are only applied to arguments of the correct sort, reducing logical errors. For example, trying to find the 'skill_level' of an Equipment item for a Task would be caught as a type error. In future work, we hope that these function definitions also open up the possibility of incorporating external algorithms. For instance, the 'duration' function could be linked to an external scheduling algorithm that calculates task durations based on various factors.\n3. Constants (C) with associated sorts provide grounding for abstract reasoning in concrete entities. Let ci: Si denote that constant ci is of sort Sj. In Fig 2 constant declaration grounds the abstract sorts in concrete entities. It allows for easy integration of domain-specific knowledge - for instance, we know that \"alice\", \"bob\", and \"charlie\" are Persons in our system. In future work, we believe this structure has the potential for linking with external databases or knowledge graphs. For example, the \"equipment\" constants could be linked to an external database containing detailed specifications for each piece of equipment.\n4. Variables (V) enable clear scoping rules for quantifiers and type-safe substitutions in logical formulas. Let xi : Sj denote that variable xi ranges over sort Sj (see Fig 2).\n5. Knowledge Base (KB) contains axioms or facts assumed to be true within the logical system. Let KB = {\u03c61,\u03c62,...,\u03c6m} where each \u03c6i is a well-formed formula in first-"}, {"title": "3.4 Design of the Interpreter's Facilities and Capabilities", "content": "This section provides an in-depth description of the interpreter's facilities, detailing how it constructs and manipulates logical expressions.\nType System, Sort Management: The interpreter implements a robust type system, managing sorts and ensuring type safety across all expressions. It supports a variety of Z3 compatible sorts, including primitive sorts like Bool, Int, and Real, which form the foundation of the type system. User-defined sorts, known as declared sorts, allow for the representation of specific domains such as Person or Equipment. For situations requiring a finite set of elements, enumerated sorts are available. The type system also accommodates composite sorts, constructed using type constructors, which enable the creation of function sorts or tuple sorts. Throughout its operations, the interpreter rigorously enforces\ntype consistency, ensuring that functions and predicates are applied only to arguments of the correct sorts.\nSymbol Table, Scope Management: Central to the interpreter's functionality is a symbol table that maintains mappings from identifiers to their definitions, including variables, constants, and functions. This table is crucial for scope management, particularly when dealing with quantified variables in logical expressions. The parsing process is another key component, where the interpreter builds abstract syntax trees (ASTs) that represent the structure of expressions. This process handles a wide range of logical constructs, from atomic formulas (basic predicates applied to terms) to complex formulas constructed using logical connectives and quantifiers. The interpreter pays special attention to quantifiers, carefully managing bound variables to ensure correct scoping. Additionally, it supports substitution of terms for variables, an essential operation in applying inference rules.\nPre-processing: While the bulk of reasoning is handled by the theorem prover, the interpreter applies basic inference and simplification rules to optimize expressions before passing them on. This includes simplification processes that reduce expressions using logical identities, such as eliminating double negations. Normalization is another crucial step, converting expressions into a standard form (like prenex normal form) to facilitate theorem proving. The interpreter also performs early error detection, identifying contradictory statements or type mismatches before they can cause issues in later stages of processing.\nFeedback Loop: Adequate error handling and diagnostics are paramount in the interpreter's design. It provides detailed error messages to assist the LLM in identifying and correcting issues with its programs. These diagnostics cover a range of potential problems, including type errors that indicate inconsistencies or mismatches in the type system, alerts for undefined symbols when functions, predicates, or constants are used without proper definition, and syntax errors that highlight issues in the structure of logical expressions.\nFuture Proofing: The interpreter's architecture emphasizes extensibility and customization. Users have the flexibility to extend its capabilities in several ways. They can add new sorts to define additional domains of discourse, expanding the system's ability to represent complex scenarios. The logical language can be enhanced by defining new functions and predicates, allowing users to capture more intricate relationships within their domain of interest. Furthermore, the modular design of the interpreter facilitates integration with different theorem provers or logic systems, enhancing its versatility and applicability to various problem domains."}, {"title": "4 Results", "content": ""}, {"title": "4.1 StrategyQA - Complex Natural Language Reasoning", "content": "Task Setup: StrategyQA presents a significant challenge in natural language processing, testing a model's ability to perform multi-hop, implicit reasoning across diverse scenarios. This boolean question answering benchmark requires models to infer unstated reasoning steps, mirroring complex human cognitive processes. For example, \"Did Aristotle use a laptop?\" requires the implicit chain: \"When did Aristotle live? When was the laptop invented? Do these time periods overlap?\" This level of abstraction surpasses simple fact retrieval or explicit reasoning tasks in other benchmarks like BoolQ or Twenty Questions (20QA). While state-of-the-art language models have shown impressive performance on StrategyQA (e.g., PaLM-2 achieving 90.20% accuracy with few-shot Chain of Thought and Self Consistency [Anil et al., 2023]), they lack transparency and verifiability in their reasoning process. Our PROOF OF THOUGHT (PoT) framework addresses this limitation by providing complete, explicit, and verifiable reasoning chains. PoT breaks down implicit reasoning steps into explicit logical representations, defines the knowledge base used, and ensures each inference is provable through a theorem prover.\nPoT Results: We evaluated PoT on a sample of 1000 questions from the StrategyQA dataset, focusing on the framework's ability to generate syntactically correct programs, produce provable reasoning chains, and match outputs with correct answers. The system, with the inclusion of a 3 step feedback loops (i.e., initial prompt, +2 attempts at resolving), successfully compiled and executed 82.4% of the 1000 processed questions, marking a significant improvement from runs with lower feedback loops. This increase in compilation success rate underscores the effectiveness of our feedback mechanism in addressing and resolving issues in generated logical representations. The system demonstrated\nstrong recall at 91.40%, indicating its proficiency in identifying true positive cases. The F1 score of 71.13% suggests a good balance between precision and recall, though precision (58.22%) presents an area for potential enhancement. The high recall, coupled with a false positive rate of 53.98%, indicates a tendency for the system to overpredict positive cases. This observation points to a need for future refinements in discriminating between positive and negative instances more accurately. While the compilation success rate is encouraging, the 17.6% of questions that failed to compile highlight an area for further improvement. Enhancing the robustness of the code generation process through improved prompting techniques, fine-tuning, and expansion of the feedback loop mechanism could potentially reduce this failure rate in future iterations."}, {"title": "4.2 Multimodal Reddit-OSHA Benchmark", "content": "Task Setup: We curated 103 samples from the r/OSHA subreddit, featuring individuals in extremely hazardous situations. This dataset represents long-tail, low-probability scenarios, mirroring challenging real-world deployment conditions for health and safety applications. The images encompass a wide range of problems with varied lighting, scene setups, visual clutter, and resolutions.\nBaselines: We implemented four reasoning strategies using GPT-4 as the underlying language model: Chain of Thought (CoT), Chain of Thought with Self-Consistency (CoT-SC), Tree of Thought (ToT), and Graph of Thought (GoT). Each baseline processes base64-encoded images and uses a consistent system prompt instructing the model to act as a safety inspector. CoT encourages step-by-step reasoning, concluding with a binary hazard decision. CoT-SC extends this by generating 5 independent reasoning paths per image, with the final decision determined by majority voting. ToT explores multiple reasoning paths in a tree-like structure with a maximum depth of 3 and a breadth of 2 at each node. GoT implements an iterative reasoning process with 3 iterations per image, building upon previous analyses. Evaluation metrics include win rate (proportion of correctly identified hazards) and reasoning richness (number of sentences in model responses).\nBaseline Results: All reasoning strategies demonstrated high performance, with CoT achieving a 99.03% win rate and CoT-SC, ToT, and GoT all achieving perfect 100% win rates. This suggests that advanced reasoning strategies can correct errors made by simpler approaches. Reasoning richness varied significantly, with ToT producing the most detailed responses (often over 1000 sentences per image) and CoT the most concise (25-30 sentences).\nPoT Results: Our PROOF OF THOUGHT framework showed remarkable improvements on this dataset with the inclusion of a 3 step feedback loop (i.e., initial prompt, +2 attempts at resolving). Notably, we reduced compilation errors from 14.6% to 0%, demonstrating the effectiveness of our feedback and error correction mechanisms. The win rate on compiled programs increased from 72% to 81.55%, indicating both more reliable code generation and more accurate logical reasoning."}, {"title": "5 Discussion and Future work", "content": "Future research directions include expanding PoT to handle more complex logical structures, that extend past boolean SAT & UNSAT, using one versus all setups, and developing more sophisticated feedback mechanisms to further reduce compilation errors. We intend to explore JSON-like represen-tations for non-boolean responses. Additionally, exploring ways to make the logical representations more accessible to non-expert users and investigating the scalability of PoT to larger, more diverse datasets will be important next steps.\nFurther, integrating PoT with other techniques such as model updates using reinforcement learning, or supervised fine-tuned models on synthetically generated syntactically correct PoT programs might unlock at-scale \"System 2\" thinking."}, {"title": "6 Conclusion", "content": "PROOF OF THOUGHT bridges the gap between language models' flexibility and formal logic's rigor, offering a promising solution for trustworthy reasoning in vision-language models. By enhancing interpretability and providing reasoning guarantees, PoT addresses critical challenges in AI system accountability and reliability. Our results demonstrate its potential in both natural language and"}, {"title": "7.5 A Qualitative Analysis of Generated DSL Programs and Reasoning Patterns", "content": "The generated DSL programs across both the StrategyQA dataset and the OSHA dataset illustrate how formal logical representations can be used to model complex reasoning tasks. In both cases, the structured use of sorts, functions, rules, and verifications ensures that the questions posed are systematically decomposed into logical assertions that can be verified by a theorem prover such as Z3. Here, we analyze key aspects of these programs and how they contribute to effective reasoning."}, {"title": "7.5.1 Sorts and Function Definitions as the Backbone of Logical Modeling", "content": "The use of DeclareSort, BoolSort, RealSort, and other basic sorts in the DSL programs serves as the foundation for defining the domains of discourse. For example, in the StrategyQA question involving Javier Sotomayor and a giraffe, the Person and Animal sorts allow the definition of relationships between humans and animals in terms of measurable attributes (e.g., jump height and height). Similarly, in the OSHA-related examples, Person, Equipment, and SafetyGear sorts model the entities relevant to workplace safety.\nBy defining functions like jump_height, height, Wearing, and Using, we map the relationships between entities and their properties. These functions serve as predicates that are later used in verifi-cations or rule implications. In these cases, the functions provide critical context for understanding the state of the world and the conditions under which certain outcomes (e.g., compliance with safety regulations or reaching a height) hold true."}, {"title": "7.5.2 Knowledge Base and Its Role in Establishing Ground Truth", "content": "The knowledge_base section plays a vital role in grounding the reasoning process by introducing factual information, such as the jump height of Javier Sotomayor (2.45 meters) or the height of an average giraffe (5.5 meters). This knowledge is essential for theorem proving because it establishes the foundational truths that the logical system will work with. Similarly, in the OSHA examples, the knowledge base specifies whether a worker is using certain equipment, wearing protective gear, or working at height.\nIn both datasets, the knowledge base is used to capture the known facts that are assumed to be true at the start of reasoning. This helps set the initial conditions for the logical rules to be applied."}, {"title": "7.5.3 Rules as Key Drivers of Logical Implication", "content": "The rules section formalizes the relationships between entities based on conditional logic. These rules encapsulate the domain knowledge and drive the reasoning process. For instance, the \"Hard Hat Rule\" in the OSHA examples states that if a person is using equipment, they should also be wearing a hard hat, while the \"Harness Rule\" mandates that a harness should be worn when using certain equipment. In the StrategyQA example, no explicit rules are needed beyond the basic comparison of heights.\nThese rules introduce a level of generalization that allows the reasoning process to handle not just specific instances but also classes of entities. For example, the rule:"}, {"title": "7.5.4 Verifications as the Core of Decision-Making", "content": "The verifications section in these programs forms the basis of decision-making by checking whether the conditions specified in the knowledge base and rules hold. In the StrategyQA case, the verification checks if Javier Sotomayor's jump height is greater than or equal to the height of an average giraffe. The outcome of this check (UNSAT) indicates that it is false, thus the predicted answer matches the correct answer.\nIn the OSHA-related examples, verifications are used to ensure compliance with safety rules. For example, the program checks if the worker is wearing a hard hat or harness while using a ladder. These verifications serve as the final step in determining whether the conditions needed for safety are met or not. The results of these verifications provide a direct SAT (true) or UNSAT (false) outcome, which can then be used to assess compliance or answer the given question."}, {"title": "7.5.5 Patterns in Error Detection and Resolution", "content": "The DSL framework helps detect inconsistencies or non-compliance in the input data. For example, the OSHA programs can flag situations where workers are using unsafe equipment or failing to follow safety protocols. This is achieved by systematically comparing the facts provided in the knowledge base with the rules and verifications, ensuring that errors are caught before any conclusions are drawn.\nAdditionally, the explicit use of logical operators like And, Not, and Implies makes it easy to trace the reasoning path when an outcome is SAT (true) or UNSAT (false). This traceability allows users to understand why a particular result was obtained, making the reasoning process more transparent and interpretable."}, {"title": "7.5.6 Overall Analysis and Utility of Generated DSL Programs", "content": "Across both the StrategyQA and OSHA datasets, the use of DSL programs enables structured, logical reasoning that is verifiable and interpretable. The modularity of the DSL-where different aspects of\nthe reasoning process (entities, relationships, rules, and verifications) are clearly separated ensures that the programs remain adaptable to a variety of problem domains.\nThe reasoning traces provided by these DSL programs offer significant benefits:\n\u2022 Interpretability: The modular structure makes it easy to follow the logical steps leading to a conclusion.\n\u2022 Error Detection: The use of formal logic allows for early detection of contradictions or violations of safety rules.\n\u2022 Scalability: The DSL framework can handle increasingly complex scenarios by adding new sorts, functions, rules, and verifications.\n\u2022 Generalization: Rules written in a generalized form (e.g., using ForAll) can be applied across different entities and scenarios, making the system more flexible."}, {"title": "7.6 Exploring the possibilities: Satisfiable Neurosymbolic Programs", "content": "Our DSL designed to be very expressive, and future proof for additional scenarios. In this subsection we present some example problems that can be expressed and solved to be found SAT."}, {"title": "1. Simple Arithmetic Verification : Verify that there exists an integer x such that x + 2 = 5.", "content": ""}, {"title": "2. Basic Safety Equipment Rule : Ensure all workers are wearing hard hats.", "content": ""}, {"title": "3. Parent-Child Relationship : Define a family tree and verify that a grandparent relationship holds.", "content": ""}, {"title": "4. Transitive Relation Verification : Verify that a transitive property holds in a relation.", "content": ""}, {"title": "5. Scheduling Without Conflicts : Ensure two tasks are scheduled at different times.", "content": ""}, {"title": "6. Graph Coloring Problem : Assign colors to nodes such that adjacent nodes have different colors.", "content": ""}, {"title": "7. Health and Safety Scenario : Verify that all workers at heights above 6 feet are wearing safety harnesses.", "content": ""}, {"title": "8. Electrical Safety Scenario: Ensure workers using energized equipment above 250V are wearing insulated gloves.", "content": ""}, {"title": "9. Chemical Handling Safety : Ensure workers handling corrosive chemicals are wearing gloves and goggles.", "content": ""}, {"title": "10. Resource Allocation Optimization: Allocate tasks to workers while minimizing total cost.", "content": ""}, {"title": "7.7 Exploring the possibilities: Unsatisfiable Neurosymbolic Programs", "content": "Our DSL designed to be very expressive, and future proof for additional scenarios. In this subsection we present some example problems that can be expressed and solved to be found UNSAT."}, {"title": "1. Pigeonhole Principle: A classic unsatisfiable problem where more pigeons than holes cannot be assigned uniquely.", "content": ""}, {"title": "2. Non-Three-Colorable Graph : A complete graph with four nodes cannot be colored with only three colors without adjacent nodes sharing the same color.", "content": ""}, {"title": "3. Contradictory Mathematical Constraints", "content": ""}, {"title": "4. An Unsatisfiable Boolean formula in conjunctive normal form (CNF).", "content": ""}, {"title": "5. Mutual Exclusivity: Defining two constants that cannot be equal, but also constrained to be equal.", "content": ""}, {"title": "6. Inconsistent Equations", "content": ""}, {"title": "7. Unsolvable Scheduling Conflict : Tasks that must occur at the same time and also at different times.", "content": ""}]}