{"title": "DATA-CENTRIC AI GOVERNANCE: ADDRESSING THE\nLIMITATIONS OF MODEL-FOCUSED POLICIES", "authors": ["Ritwik Gupta", "Leah Walker", "Rodolfo Corona", "Stephanie Fu", "Suzanne Petryk", "Janet Napolitano", "Trevor Darrell", "Andrew W. Reddie"], "abstract": "Current regulations on powerful AI capabilities are narrowly focused on \"foun-\ndation\" or \"frontier\" models. However, these terms are vague and inconsistently\ndefined, leading to an unstable foundation for governance efforts. Critically, pol-\nicy debates often fail to consider the data used with these models, despite the clear\nlink between data and model performance. Even (relatively) \"small\" models that\nfall outside the typical definitions of foundation and frontier models can achieve\nequivalent outcomes when exposed to sufficiently specific datasets. In this work,\nwe illustrate the importance of considering dataset size and content as essential\nfactors in assessing the risks posed by models both today and in the future. More\nbroadly, we emphasize the risk posed by over-regulating reactively and provide\na path towards careful, quantitative evaluation of capabilities that can lead to a\nsimplified regulatory environment.", "sections": [{"title": "1 THE SHORTCOMINGS OF TODAY'S AI GOVERNANCE", "content": "The past decade has seen a rapid burst of commercial AI products, such as Google Translate and\nOpenAI's ChatGPT, delivering new capabilities into the hands of the public. As AI has made its way\nto wider audiences, it has continued its rapid pace of development, giving everyday users what were\npreviously highly specialized computing tools and capabilities. This raises questions for govern-\nments, academics, and commercial labs about whether certain AI capabilities or behaviors should\nbe deemed as too \u201crisky\u201d for public access (Dragan et al., 2024).\nToday's AI governance efforts have coalesced around the terms \u201cfrontier\u201d, \u201cfoundation\u201d, \u201cdual-use\u201d,\nand \"general purpose\" to describe the largest and most capable of these models. In policy papers\nand legislation, models described by these terms are subject to additional scrutiny and regulatory\ninterest. These terms are usually synonymous with the most cutting-edge models of today including\nOpenAI's ChatGPT (Brown et al., 2020; OpenAI, 2024c), Meta's LLaMA (Touvron et al., 2023),\nand Google's Gemini (Gemini Team, 2024b). Although there is general agreement for the types\nof AI-accelerated risks that regulations aim to curtail, there is much less clarity and consensus in\nconcrete definitions for such models. In an effort to define the characteristics of these large, capable\nmodels, a number of policy documents have focused on parameter counts and/or FLOPs, measures\nof model size and compute requirement (European Union, 2024).\nWe argue that this approach is short-sighted for three reasons. First, there is no consistent definition\nof \"frontier\", \"foundation\u201d, \u201cdual-use\", and \"general purpose\" models with regards to FLOPs or\nparameter count. As discussed below in Section 2.1, this lack of definitional clarity has led to a\nregulatory and governance landscape with varying ceilings for what constitutes a covered capability.\nSecond, as machine learning advances, models are becoming more efficient, requiring fewer param-\neters and FLOPs to achieve the same tasks. This means that the next generation of models could\nbe more capable while falling below regulatory ceilings. Finally, the focus on the largest and most\ncompute-intensive models ignores the fact that smaller models can be just as capable in niche, and\npotentially risky, areas as their larger counterparts. These factors culminate in inadvertent loopholes\nthat powerful capabilities can slip through, rendering expensive regulatory efforts not only useless\nbut potentially detractory from beneficial uses of AI technologies.\""}, {"title": "2 DEFINITIONAL CHALLENGES AND FLAWED LIMITS IN AI GOVERNANCE", "content": "Over time, the capabilities of digital systems have improved in tandem with hardware advance-\nments (Hilbert & L\u00f3pez, 2011). This is also true of machine learning models; simple two-parameter\nlogistic regressions have evolved to deep neural networks with trillions of parameters, enabled by the\nadvent of graphics processing units (GPUs) and high-speed memory\u00b9 (Moore, 1998). Yet, Moore's\nLaw has weakened significantly. We have observed periods of stagnation, and even reversal, in ag-\ngregate computing trends despite overall progress in terms of effectiveness of outcomes (Leiserson\net al., 2020). The trend of plateauing is also observed in machine learning, and policies aiming to\nregulate machine learning models solely as a function of continuous growth are flawed, as demon-\nstrated in Section 2.2.\nIn addition, much of the conversation around AI regulation has centered itself around the prevention\nof behaviors that are deemed to be \"harmful\" or otherwise detrimental to society (Dafoe, 2018;\nHoffman & Frase, 2023). The mention of \u201charm\" is too often unqualified and does not address the\ncapabilities of existing technologies that may already be capable of much of the malicious behavior\ndiscussed in AI policy circles today. For example, AI for biological agent design is widely cited as\na potential harm (Callaway, 2024), yet computational drug discovery has been the norm since the\n1980s and has enabled the discovery of drugs such as ritonavir, a medication critical in treating both\nHIV and COVID-19 (Van Drie, 2007). The conversation surrounding the use of AI to further societal\nharms must contextualize the additional marginal risk posed by these methods when compared to\nexisting technologies such as search engines or statistical inference algorithms.\nThe AI ecosystem's difficulty in defining and identifying harm extends into inconsistent efforts to\nidentify \"harmful\u201d or \u201crisky\u201d models and regulate them. In the following sections, we demonstrate\nthe shortcomings and inconsistencies of these model-focused AI governance efforts, while identify-\ning key drivers of AI risk that are currently overlooked in modern AI policy."}, {"title": "2.1 AN UNSTABLE DEFINITION FOUNDATION", "content": "The use of the terms \u201cfoundation"}, {"title": "2.2 CAPABILITY AND MODEL SIZE ARE NOT STRICTLY CORRELATED", "content": "Today's AI governance efforts regularly seek to define frontier models by their size or the amount\nof computation required to train them. As reflected in some of the governance documents analyzed\nabove, a common strategy is to set a regulatory threshold on the number of parameters included in\na model. The rationale behind this approach is a set of experiments that demonstrate that models\nwith larger numbers of parameters, with all other factors held constant, suddenly perform drastically\nbetter on downstream tasks they are not explicitly trained for (Wei et al., 2022). This phenomenon\nwas termed \"emergence\" and drove fears that sufficiently large models, by default, can perform well\non tasks that pose risks to public safety.\nParameter. A variable within a model that is learned from the training data. Parameters\ndefine how the model makes predictions by influencing the model's internal structure and\ndecision-making process.\nDiscussions prioritizing model size as a viable threshold fixate on a superficial, easy-to-obtain quan-\ntity that is ultimately a red herring. In reality, model capacity and generalizability represent char-\nacteristics that are innately difficult to quantify and measure. Not only are current generalization\nbenchmarks lacking in accurate definitions for model capabilities (Raji et al., 2021), but it is exceed-\ningly common for smaller, more task-focused models to perform better than large, broad-purpose\nmodels on specific downstream tasks, as demonstrated below.\nDownstream tasks. Applications that take model outputs as input, re-purposing model\nknowledge for a new problem that it was not explicitly trained for. The model parameters\nmay optionally be updated via further training using additional data from these tasks.\nWe use the task of image segmentation as an example where smaller models can outperform their\nlarger counterparts. Examples of image segmentation include both civil and national security appli-\ncations such as building damage assessment or object targeting (Gupta et al., 2019; 2024). Specif-\nically, we examine RefCOCO (Kazemzadeh et al., 2014), a common image segmentation dataset\nused to train vision-language models (VLMs), and two models which attain near-state-of-the-art\nperformance on it, PaliGemma (Beyer et al., 2024) and UniLSeg (Liu et al., 2023). PaliGemma is\na large VLM released openly by Google consisting of 3.0 \u00d7 10\u00ba parameters (Google, 2024). On\nthe other hand, UniLSeg, released by Tsinghua University, ByteDance, and the University of Hong\nKong, consists of only 1.7 \u00d7 108 parameters\u2014an order of magnitude smaller than PaliGemma. Yet,\nUniLSeg achieves a mean intersection-over-union of 81.7 versus PaliGemma's 73.4 on RefCOCO,\nwhich is a massive gain of ~11.3% in performance. Figure 1 additionally demonstrates the per-\nformance of two more near-state-of-the-art models, UNINEXT (Yan et al., 2023) and HIPIE (Wang\net al., 2023), on RefCOCO for completeness.\nImage segmentation. A task in which an image is split into regions which each represent a\nspecific object type or concept.\nMean intersection-over-union. A measure of how accurately and precisely, on average, an\nimage segmentation model performs. This ranges from zero to one."}, {"title": "2.3 A MISPLACED FOCUS ON FLOPS", "content": "FLOPs. Floating point operations. The cumulative number of floating point operations (e.g.,\n3.5 x 7.1) used during model training. Not to be confused with floating point operations per\nsecond (FLOPS, with a capital S).\nDefinitions of foundation and frontier models (see Table 1) include regulatory thresholds defined by\ncumulative training FLOPs. These numbers have no basis in outcomes or technical reality, as we\ndemonstrate in the following section. 1026 FLOPs appeared as an arbitrary FLOPs threshold in the\nOctober 2023 Biden Administration Executive Order.\nSome of the largest models in existence today are sufficient to employ in harmful activities (OpenAI,\n2024b;a), yet all fail to meet American FLOPs thresholds (see Table 2), raising questions about the\nthreshold's usefulness. These same models are covered under the the EU's proposed threshold of\n1025 for AI models. However, a fractured environment in which a model regulated in France might\nnot be subject to the same regulations in the United States will lead to confusion.\nThese thresholds further exacerbate the perception that frontier capabilities can only arise from large\nmodels trained with a large amount of computation on larger datasets. As we further demonstrate in\nthis section, even smaller models trained with fewer resources on smaller datasets can set a capability\nfrontier. In fact, research incentives necessitate the creation of methods that reduce computational\nneeds for model training-a trend that is be contrary to regulatory assumptions.\nOptimizations reverse trends. One way to visualize the futility of FLOPs thresholds is via recent\nworks such as those on efficient sparse training (Chen et al., 2021) (Figure 2) or other architectural\nimprovements (Zhu et al., 2024). They demonstrate that model performance can, in some cases,\nbe decoupled from computational cost-models can train faster and more accurately with fewer\nparameters and FLOPs. Further research demonstrates decoupling in the opposite direction, i.e., ef-\nficient training can occur in compute-constrained environments. Models distributed across multiple"}, {"title": "3 DATA IS MISSING FROM THE CONVERSATION", "content": "Machine learning capabilities are not singularly determined by their model architecture. Rather, ma-\nchine learning capabilities are defined by both the model and the data provided. We define \"data\" as\nany information a model is exposed to, whether it is during training or deployment. This paper aims\nto center data in AI governance conversations. We suggest that models alone are not harmful; rather,\nthe unique combination of models exposed to specific datasets (whether during training or inference)\nand subsequently being used for specific purposes may pose a risk to public safety (Baldridge et al.,\n2024)."}, {"title": "3.1 BIG DATA TO USABLE INFORMATION", "content": "The rapid rise of AI since approximately 2010 can largely be attributed to (1) advancements in\ncomputational hardware in accordance with Moore's Law, and (2) a focus on large quantities of\ndata. Models are useless without data, and the availability of \"foundational\" datasets, such as Im-\nageNet (Deng et al., 2009) and Common Crawl, brought modern machine learning capabilities to\nbear. Today, AI datasets are often orders of magnitude larger, created by scraping content across the\ninternet.\nDataset size is a key component in \u201cscaling laws,\" or predictions of performance within a family of\nmodels as a function of variables in a training recipe. Research in this area finds strong relationships\nbetween model performance and amount of training data, amount of computation, and model param-\neters (Kaplan et al., 2020; Hoffmann et al., 2022a; Zhai et al., 2022; Google, 2023). Additionally,\nboth Hoffmann et al. (2022a) and Google (2023) find that model and optimal dataset size scale at\nequal proportions as training compute increases.\nHowever, even an optimal training recipe with an appropriate amount of data, parameters, and com-\npute does not necessarily produce a useful model. The dataset content is a crucial factor. A model\n\"trained on the internet\u201d can unsurprisingly exhibit the same bias (Fleisig et al., 2024) and toxic-\nity (Liang et al., 2023) present in the data and also fall short in other areas: it may fail at logical\nreasoning (Berglund et al., 2023), algebraic computation, or following a user's instructions, to name\na few examples. In a limiting argument, a multi-trillion parameter model trained only on Shake-\nspeare novels may never be able to reason about chemical weapon design.\nTo address this, models are fine-tuned on higher-quality, curated data. Popular techniques that rely\non high-quality data include instruction tuning (e.g., reinforcement learning from human feedback,\nor RLHF (Ouyang et al., 2022)), training models to use tools or act as agents (Schick et al., 2023),\nor supervised fine-tuning for a specific task, such as generating images in a particular artistic style."}, {"title": "4 DATA-CENTRISM OPENS NEW ANALYTIC FRONTIERS", "content": "Modern machine learning methods are useful beyond traditional data querying and correlation tools\nsuch as search engines in part due to their ability to retrieve, compile, and organize data even when\ngiven unspecific queries. Below we outline two distinct features that are uniquely enabled by the\ncombination of ML models and data: (1) retrieval, where a model outputs information retrieved\ndirectly from its data, and (2) derivation, where a model compiles or synthesizes items from provided\ndata to generate new information. These features enable new ways to interact with complex data that\nwould otherwise be difficult to manage, offering potential benefits as well as risks, which we explore\nfurther below."}, {"title": "4.1 RETRIEVAL", "content": "As our ability to collect and maintain digital information has soared over the last few decades,\nretrieving the right results for a certain query has become a core technological focus. Billions of\ndollars have been spent towards developing efficient data representations for search engines (Brin &\nPage, 1998; Dean & Ghemawat, 2008) and databases (Corbett et al., 2012; Shvachko et al., 2010),\nand towards creating the algorithms to find and retrieve these results. Now, AI models trained on\nlarge amounts of data have become both capable encoders and retrievers of data (in addition to\ngenerators, as we describe in the section on derivation). This becomes a problem when a model has\nbeen exposed to specific data points that would be considered sensitive if directly retrieved, such\nas credit card numbers or classified information. The retrieval itself could occur either through (1)\na model memorizing and then reproducing points in training data, or (2) retrieving from a large\namount of data provided at test time, such as a company's internal database. Below, we describe\nthese two cases in more detail.\nRetrieval. A task requiring models to accurately locate and return a requested piece of infor-\nmation.\nRetrieval from training data. Datasets contaminated with outliers have historically relied on\ndataset volume to dilute outlier effects. This leads to the misconception that a small quantity of\n\"harmful\" data points can be negated by massive amounts of otherwise commonplace data. Unfor-\ntunately, this intuition does not translate to modern machine learning methods. Large models are\nknown to memorize some parts of training data and reproduce them if queried correctly (Carlini\net al., 2022). Therefore, large models can retrieve, and therefore utilize, harmful data even if it is\npresent in a negligible quantity.\nHowever, memorization does not occur across data equally: prior work shows that \"average\" training\nsamples are less likely to be memorized, whereas outlierse and duplicated data points are more likely\nto be memorized (Feldman & Zhang, 2020; Feldman, 2020; Carlini et al., 2022). As certain types of\ndata, such as child sexual abuse material, are outliers on the Internet (Thiel, 2023), memorization of\nsuch data poses an inherent risk in the downstream usage of affected models, especially combined\nwith the powerful retrieval abilities of current models. Nasr et al. (2023) is another example of\nwork where ChatGPT was used to retrieve training data which comprised personally identifiable\ninformation of dozens of individuals.\nRetrieval from previously unseen data. An AI system may also be exposed to entire new do-\nmainsof data during inference that were not present during training. Models can utilize new, unseen\ndata through prompting or integration with external databases. Models' ability to effectively inter-"}, {"title": "4.2 DERIVATION", "content": "As AI capabilities increase, a growing concern is the generation of original or derivative information\nthat is more revealing that the data provided to the model initially. For example, if a system is given\ntwo entry-level textbooks in physics and chemistry, respectively, and uses independent concepts\nfrom either to build a toy rocket, we would call the process of arriving at the toy rocket instructions\n\"derivation.\"\nThis feature is especially present in modern machine learning methods when compared to technolo-\ngies such as databases due to their ability to synthesize unrelated pieces of information on the fly.\nWhile retrieved content is often straightforward to recognize and check-i.e., it may be quickly\nobvious that a generated phone number is real, and possible to check if a particular image was con-\ntained within training or deployment-time data-derived content is more nuanced and difficult to\nmeasure, and thus may present a greater concern.\nDerivation. When a model compiles disparate pieces of data to infer novel piece of informa-\ntion not explicitly contained in any of its components.\nUnder this category, multiple pieces of otherwise mundane information could be compiled to form\ninformation that is now sensitive. For instance, a language model trained for code generation could\nbe provided a description of a vulnerability and be used to generate code for exploiting it. Models\nhave already begun to present synthesis capabilities in different arenas, such as for code generation\nof programming languages with low data availability (Mora et al., 2024) and the generation of\nMathematics Olympiad-level geometric proofs as part of larger pipelines (Trinh et al., 2024).\nThe maximal extent to which current models are capable of derivation is not yet clear as methodolo-\ngies for inducing such capabilities are constantly evolving. For example, although modern language\nmodels have shown nascent indicators of capability to generate novel research ideas in fields such\nas natural language processing, the ideas they generate lack diversity and may not be tractable (Si\net al., 2024). Modern image generation models struggle to synthesize images precisely adhering\nto descriptions of unique combinations of objects and their attributes previously unseen in training\ndata (Huang et al., 2023). Our intent in this section is not to establish a measure for models' deriva-\ntion capability but rather to bring attention to derivation as a unique capability offered by modern\nML models."}, {"title": "5 ASSUMPTIONS AND LIMITATIONS", "content": "Given the rapid pace of AI development, we acknowledge the limits of our core analytic assump-\ntions, grounded in the current state-of-art in the field, that drive the analysis and recommendations"}, {"title": "Assumption 1: Powerful models are unable to reason without memorizing information.", "content": "Large\nmodels can perform well by both learning generalizable semantics over their training data, but also\nthrough the rote memorization of data or concepts. Currently, there exists no class of powerful\nmachine learning models which are able to \"reason\" about the world without having memorized any\ndata during its training period. Put another way, there are no reasoning agents that are derived in\na manner that is completely detached from data. One can argue that such a model, should it exist,\nwould fit the definition of \u201cartificial general intelligence\" as it could generalize to any new set of\ndata without inherent data priors."}, {"title": "Assumption 2: Dataset distillation methods are still over the horizon.", "content": "The field's understand-\ning of the amount of data points needed for a model to achieve proficiency on specific tasks is still\nevolving. This area of research is termed \u201cdataset distillation\" and aims to reduce the number of data\npoints necessary to achieve target metrics (Wang et al., 2020; Zhao et al., 2021). Further, it remains\nunclear what exactly constitutes a \u201cdata point,\" especially with modern methods like transformers,\nwhich rely on tokens, the amount of which varies with different tokenization methods (Sennrich\net al., 2016; Schuster & Nakajima, 2012). We aim to establish one rigorous definition of \"data\npoint\" in future work, as well as analysis of how many data points define emergent capability.\nIn a limiting argument, should data distillation methods improve to the point where models can learn\ngeneralizable knowledge without any data at all, this work would need to be revisited.\""}, {"title": "6 AVENUES FOR DATA-FORWARD REGULATION", "content": "Given our analysis above, the inclusion of data in nascent AI governance conversations can sim-\nplify the regulatory overhead by enabling the use of existing legal frameworks and the creation and\nexecution of novel, data-backed evaluation schemes. Specifically, there are numerous policies and\nlaws surrounding the appropriate use of data in contexts that are deemed to be of risk to the public.\nInstead of reinventing these policies using a new set of definitions that are model-specific, expand-\ning and modifying them to account for the use of data by powerful models might offer a simpler\npath towards effective evaluation frameworks in areas where definitions alone are vague, leading to\nsimpler regulations."}, {"title": "6.1 APPLYING EXISTING DATA-FOCUSED LEGAL AND REGULATORY APPROACHES", "content": "Significant work has been and continues to be done to mitigate malicious model outputs or behav-\niors. Thus far, model creators have relied on identifying malicious outputs or behaviors through red\nteaming and safety training (Ganguli et al., 2022; Wei et al., 2023).\nHowever, some classes of outputs or behaviors that are deemed risky could more easily be stemmed\nby careful curation of datasets. Unique information such as the relationship between a person and\ntheir social security number, or specific instances of child sexual abuse material, is extremely un-\nlikely to be generated if that data is never provided to a model.\nThere exists a range of legal and regulatory frameworks that cover many categories of model outputs\nthat are of greatest concern, including personal identifiable information, child sexual abuse mate-\nrial, and classified content. Data-centrism prevents models from acquiring the capacity for harmful\nbehaviors prior to the expenditure of computation. Since existing regulations can be applied, AI\ngovernance can be achieved without the need for new regulatory frameworks."}, {"title": "6.2 TECHNOLOGICAL LEVERS FOR DATA-FORWARD REGULATION", "content": "Although research has shown that certain model capabilities emerge once sufficient model size and\ncompute are attained (Wei et al., 2022), establishing regulatory thresholds is ill-defined given just\nthese two metrics. As discussed in Section 3, models provided with the right data can perform\ncomparably to, if not better than, larger and more compute intensive alternatives. Further, a model\nmust first be paired with sensitive information for it to make use of it. That is, the model does not"}, {"title": "6.3 INCENTIVIZING DATA GOVERNANCE TOOLS AND PRACTICES", "content": "Just as existing policies and regulations advocate for the standardization of model documentation,\nsuch as model or system cards (Mitchell et al., 2019), data-centrism motivates the standardization\nof dataset documentation. Comprehensive approaches for doing so have been proposed already,"}, {"title": "7 CONCLUSION: EVOLVING AI GOVERNANCE ALONGSIDE AI\nTECHNOLOGY", "content": "Despite rapid growths in both model and dataset sizes in recent years, AI policies have hinged on\nthresholds, definitional concepts, and qualifiers that limit their medium-to-long term liability. For a\ntechnology that will be with us for the foreseeable future, we can, and should, approach governance\nin a more deliberate manner, with a clear understanding of what enables these capabilities to be\npowerful in the first place.\nSimilar to how an arbitrarily large engine, no matter how specifically quantified, would be useless\nwithout defining the kind of fuel used with it, the AI policy landscape mistakenly focuses on a small\nset of model-based thresholds, particularly FLOP and parameter counts. Neither fully define how\npowerful a machine learning model may be without an understanding of the data that accompanies\nthem. Furthermore, the lack of definitional clarity with what constitutes a \"frontier\", \"foundation\",\n\"dual-use\", or \"general purpose\u201d model complicates governance efforts. More generally, these two\ntrends in governance further propagate the outdated idea that the largest, most compute intensive\nmodels are those which drive AI risk. As we reach a point where smaller models, when paired with\nlarge, foundational datasets or small, high-quality datasets, can perform as well as larger models,\nthis narrow approach creates loopholes and unfairly penalizes otherwise beneficial technologies.\nCentering data offers a more durable approach to AI governance, particularly as trends in quantifi-\nable measures of model capability are difficult to predict. A focus on data also provides an opportu-\nnity to better research, define, and respond to benefits and risks posed by AI, a debate that remains\nnebulous in both policy and technical circles. Centering data also provides avenues for existing reg-\nulations surrounding sensitive types of data to apply while also clearing the way for new evaluation\nmethods to quantify the use of data and models together. Expanding model-based regulations to\nfocus additionally on their paired data builds a stronger foundation that is less prone to collapse.\nWhile a pivot in the governance landscape may be daunting, a focus on data provides the oppor-\ntunities and incentives for government, academic researchers, civil society, and the private sector\nto develop new tools and approaches that lead to meaningful policies. This paper is the first of the\nFrontier Data Initiative which seeks to focus governance efforts on the combination of data and mod-\nels. Future papers in this series will focus on novel technical approaches to benchmarking dataset\ncapabilities, auditing data and conducting data forward red teaming. These technical papers will be\njoined by policy papers centering data-forward AI governance opportunities."}]}