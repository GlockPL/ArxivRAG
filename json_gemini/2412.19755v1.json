{"title": "\"Did my figure do justice to the answer?\" : Towards Multimodal Short Answer Grading with Feedback (MMSAF)", "authors": ["Pritam Sil", "Bhaskaran Raman", "Pushpak Bhattacharyya"], "abstract": "Personalized feedback plays a vital role in a student's learning process. While existing systems are adept at providing feedback over MCQ-based evaluation, this work focuses more on subjective and open-ended questions, which is similar to the problem of Automatic Short Answer Grading (ASAG) with feedback. Additionally, we introduce the Multimodal Short Answer grading with Feedback (MMSAF) problem over the traditional ASAG feedback problem to address the scenario where the student answer and reference answer might contain images. Moreover, we introduce the MMSAF dataset with 2197 data points along with an automated framework for generating such data sets. Our evaluations on existing LLMs over this dataset achieved an overall accuracy of 55% on Level of Correctness labels, 75% on Image Relevance labels and a score of 4.27 out of 5 in correctness level of LLM generated feedback as rated by experts. As per experts, Pixtral achieved a rating of above 4 out of all metrics, indicating that it is more aligned to human judgement, and that it is the best solution for assisting students.", "sections": [{"title": "1 Introduction", "content": "Personalized feedback plays an important role in a student's learning process (Deeva et al., 2021). Moreover, a corrective, motivational and informative feedback can drastically speed up a student's learning process and help the student develop an innate curiosity to learn more. Thus, this helps in the student's overall growth and fuels a student's passion to learn a particular topic. However, in real-life scenarios, it is often difficult to provide personalized feedback to each and every student present in a classroom. Thus, this led the development of Intelligent Tutoring Systems (ITSs) which are capable of providing personalized feedback to each and every student. Moreover, these systems can easily be customised to cater to the needs of each and every individual students.\nWith advances in Natural Language Processing (NLP), ITSs are currently capable of delivering learning experiences through conversations (Paladines and Ram\u00edrez, 2020), personalized feedback and others. Presently, most such systems make use of MCQs to judge the student's learning progress. Recent works address various aspects of MCQ generation ( (Hwang et al., 2024), (Dutulescu et al., 2024), (Feng et al., 2024)), and even feedback generation is based on the students' responses to such MCQs (McNichols et al., 2024). However, MCQs often limit the student's creative abilities and are limited to selecting one or more correct options out of a set of options. On the other hand, we have subjective or often open-ended questions which encourages the student to answer a particular question using their creative abilities. That is, the student has to write fluent and grammatically correct answers with images. Note that, in particular, we deal with those subjective questions where the answers are short answers, that is, ranging from a few lines to a single paragraph. Interestingly, the problem involving grading such questions and providing feedback based on that is known as the problem of Automatic Short Answer Grading (ASAG) with Feedback.\nThe problem is as follows: given a question and a reference answer, the task is to grade a reference answer along with proper feedback as to why a particular grade is assigned. To develop a solution to this problem, researchers needed access to high-quality datasets. Filligreha et al. (Filighera et al., 2022) was a pioneer in introducing a bilingual short answer feedback dataset which had a short answer to questions pertaining to various topics such as communication networks and others. The dataset contained a total of 4519 submissions ranging over 22 English short answer questions. However, the dataset only had about 2k responses, which were in"}, {"title": "2 Multimodal Short Answer Grading with Feedback (MMSAF) Problem", "content": "This paper introduces the Multimodal Short Answer Grading with Feedback (MMSAF) Problem. The problem is formulated as follows, given the Question (Q) and Reference Answer (RA), the task is to grade the Student Answer (SA) by assigning a Level of Correctness (LC), Image Relevance (IR) and a Feedback (F) which explains why LC and IR were assigned to it (Figure 1). We further divided this problem into a classification problem of assigning the LC and IR values to the student's answer and a single reasoning problem of providing feedback that compares the question and reference answer to the student's answer."}, {"title": "2.1 Classification of Level of Correctness and Image Relevance", "content": "The problem of determining the level of correctness can be formulated as a classification problem where, given the SA and RA, the goal is to assign a level of correctness to it. Hence it is formulated as:\n$(Q, SA, RA)_M \\rightarrow \\{Correct, Partially Correct, Incorrect\\}$,\nwhere Correct (C), Partially Correct (PC) and Incorrect (I) denote the three levels of correctness and M is the model used.\nSimilarly, the problem of determining the image relevance is also a classification problem where, given the Q, SA and RA, the task is to determine whether the image present in the student answer is Relevant (R) or Irrelevant (IRel) to the question and RA. It was formulated as :\n$(Q,SA, RA)_M \\rightarrow \\{Relevant, Irrelevant\\}$, where M is the model used. This often depends on the multimodal reasoning capabilities of the model."}, {"title": "2.2 Feedback Generation", "content": "Next, the problem of feedback generation involves the concept of comparative reasoning. Comparative Reasoning (Yu et al., 2023) is a fundamental cognitive ability that describes the process of comparing different entities or concepts to draw certain"}, {"title": "3 Multimodal Short Answer Grading with Feedback (MMSAF) Dataset", "content": "The Multimodal Short Answer Grading with Feedback (MMSAF) dataset is a benchmark to the MMSAF problem. It is built using a total 181 question-answer pairs taken from high school-level physics, chemistry and biology questions. The corresponding student answers to these questions were synthetically generated to obtain a total of 2197 datapoints in the whole dataset. Each data point in the dataset is a tuple of 5 elements namely - Question (Q), Reference Answer (RA), Student Answer (SA), Level of correctness (LC), Image Relevance (IR), Sample Feedback (F) and Rubrics used to evaluate any generated feedback (FR).\nAs per the MMSAF problem, the model has to generate feedback given a tuple of (Q, RA, SA), which comments on the correctness of the student's answer as well as the image relevance of the student's answer compared to the question and reference answer. Since the student answers are synthetically generated, we record the errors present in them as rubrics under the FR column so that the generated feedback can be evaluated based on these rubrics. Hence, the quality of feedback can be mea-"}, {"title": "3.1 Generation of Textual and Image segments of Student Answers", "content": "The first step towards generating the student answers is to extract the question and reference answer pairs (Step 1 in Figure 2). To do so, 160 such instances have been scraped from openly available high school textbooks, and the remaining 21 instances have been generated by Gemini and verified by a Subject Matter Expert (SME). The next step is to generate the student responses (Step 2 in Figure 2). Note that a single student response has both a textual part and a supporting image. Both are generated separately from the correct answer and then combined based on a correctness matrix, which will be described later in this section.\nFirst, we deal with the textual part of the student answer. We generate a single correct answer for each question by using a prompt (as in Appendix A) along with the question and reference answer, barring 26 numerical questions for whom the correct response is the same as the correct answer. However, the partially correct and incorrect answers have been generated by using the termite strategy by Ashita et al. (Saxena, 2024) to introduce hallucinations in the reference answer. In particular, we introduce either factual inconsistency or factual fabrication in it. The prompts used in generating the partially correct responses and incorrect responses have been added in Appendix B and Appendix C,"}, {"title": "3.2 Generation of Level of Correctness, Image Relevance and Rubrics", "content": "Now, given the correct/partially correct/incorrect textual answer and images, we generate the final set of student answers using the correctness matrix (Table 4 in Appendix D). This resulted in the construction of the final dataset, which is 2197 data points. Note that the level of correctness for each student answer is defined by the output label in Ta-"}, {"title": "4 LLMs in Consideration", "content": "The MMSAF problem consists of multiple images along with text that must be evaluated as a whole. Thus, to solve it, we need such LLMs, which are multimodal and capable of reasoning over multiple text segments and images. Due to this reason, we primarily select 4 LLMs - ChatGPT, Gemini, Pixtral, and Molmo. For Molmo and Pixtral, the huggingface library\u00b9 has been used as they are open sourced while for ChatGPT2 and Gemini\u00b3 we use their respective APIs."}, {"title": "4.1 ChatGPT", "content": "ChatGPT is a well-known multimodal and multilingual model from OpenAI. As per their website4, GPT-40 mini has shown strong performance on the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark and has beaten Gemini Flash and Claude Haiku. This makes it a strong candidate for solving the MMSAF problem as it heavily relies on the multimodal reasoning capabilities of the LLM. Apart from this, gpt-4o-mini is a lighter version of the GPT-40 model which means it will be faster and easier to deploy."}, {"title": "4.2 Gemini", "content": "Gemini, developed by Google, has shown a similar performance to that of ChatGPT when it comes to multimodal reasoning benchmarks such as MMMU. As per their website5, the latest Gemini model, Gemini Ultra, has beaten GPT-4V on the MMMU benchmark. Hence, this also makes it a strong candidate for solving the MMSAF problem owing to its excellent multimodal reasoning abilities. However, for this set of experiments, we have used gemini-1.5-flash since the API is free to use and can be used by any developer without thinking about cost."}, {"title": "4.3 Pixtral", "content": "Since both Gemini and ChatGPT are restricted models, Mistral.ai recently launched its own multimodal model called Pixtral 12B. It (Agrawal et al., 2024) was able to beat some well-known open source LLMs such as Qwen, LLaVA, Phi, Claude-3 and even Gemini-1.5 Flash on multimodal benchmarks such as MMMU. However, GPT-4o still performed better than Pixtral. As a result, we also explore Pixtral as a possible solution for solving the MMSAF problem as it is an open model and has shown significant performance in multimodal reasoning benchmarks."}, {"title": "4.4 Molmo", "content": "While ChatGPT, Gemini and Pixtral have been trained on a vast array of multimodal data, none of them have been specifically trained only"}, {"title": "5 Evaluation of LLM Generated Feedback", "content": "\nThe goal of this evaluation was to quantify the zero-shot performance of existing LLMs on this dataset and grade their capabilities. To do so, 221 data points (130 from biology, 56 from chemistry and 35 for physics) were sampled randomly and fed to different LLMs using Prompt 1. Note, the complete prompt can be found in Appendix E. The corresponding level of correctness, image relevance and feedback values generated were collected and then analyzed in the following subsections."}, {"title": "5.1 Analysis of Correctness and Relevance levels", "content": "To assess how well the LLMs performed in predicting the level of correctness and image relevance labels of the student answers, we make use of met-"}, {"title": "5.2 Evaluation Task for Experts", "content": "The 221 data points mentioned earlier were provided to the LLMs along with a prompt (as in Appendix E), and their feedback, level of correctness and image relevance values were recorded and presented to three Subject Matter Experts (SMEs), each being an expert in their own domain namely, physics, chemistry and biology. Any relevant details about SMEs are mentioned in Appendix F. The SMEs were instructed to score each feedback on a scale of 1 to 5 based on the following parameters -\n1. Fluency and Grammatical Correctness (FGE)\n2. Emotional Impact (EI)\n3. Level of Feedback Correctness (LC)\n4. Error mitigation in feedback (EM)\n5. Correctness of additional information provided by LLM (CAD)\n6. Relevance of additional information provided by LLM (RAD)\n7. Rubric Based Feedback (T/F values)\nFluency and Grammatical Correctness: This metric denotes the fluency and grammatical correctness (FGE) of the LLM-generated feedback. The idea is to check if the LLM-generated sentences are grammatically correct or not. A score of 1 denotes that the FGE level of the feedback is extremely poor while a score of 5 indicates that it is excellent.\nEmotional Impact: This metric is to check whether the LLM-generated feedback will have a positive impact on the student or not, that is, whether the feedback is more encouraging and assistive for the student or not. A score of 1 denotes negative impact, while a score of 5 denotes a positive impact.\nLevel of Feedback Correctness: This metric is to determine whether the feedback has properly captured all the errors present in the student answer. A score of 1 denotes no error has been captured in the feedback, while a score of 5 denotes that all the errors have been captured in the feedback.\nError Mitigation in Feedback: This metric evaluates whether the feedback has properly addressed each and every error present in the student answer and suggested ways to correct them. A score of 1 denotes no such error mitigation has been done, while a score of 5 denotes that all the ways necessary to correct all errors are present.\nCorrectness of additional information provided by LLM: The LLMs had also been prompted to gen-"}, {"title": "5.3 Analysis of Expert Evaluation", "content": "Once the SMEs had completed their respective evaluation tasks, all the scores for each metric were collected and averaged out to present the final data. Table 3 summarizes the average ratings assigned by annotators to the feedback generated by different LLMs over various criteria mentioned in Section 5.2. Out of all the metrics, ChatGPT performs the best in terms of FGE, while Molmo performs the worst. SMEs also pointed out that Molmo has a tendency to generate Chinese while generating its feedback, and hence, they penalized Molmo in FGE for this reason. Again, we see that Pixtral and Gemini perform similarly in FGE. Regarding ChatGPT, SMEs again pointed out that the feedback provided was shallow and required more depth. One key observation of ChatGPT was that it was successful in detecting any calculation mistake present in the student's answer.\nApart from that, it is seen that Pixtral beats other models in terms of other parameters such EI, LC, EM, CAD and RAD. As pointed out by SMEs, Pixtral and Gemini did a great job in structuring the different parts of the feedback. However, Pixtral stood out as it broke down the feedback into different segments, each addressing a different concept associated with the question and the reference answer and also a separate section for the level of correctness and image relevance.\nAll SMEs agreed that Molmo performed the worst, and the feedback generated by it is too direct and consists of large and complex sentences. This will in turn have a negative impact on the student and adding to it, it has a tendency to label correct answers as incorrect. Some feedback provided by it in chemistry was also irrelevant. However, when it comes to rubric-based detection, Gemini performed the best. The following subsections analyze how well these LLMs performed at detecting the rubrics pertaining to errors in text and images."}, {"title": "5.4 Analysis of Rubric based Feedback", "content": "The synthetically generated student answers contain both text and image parts, which might contain errors. Thus, the generated rubrics can either be used to evaluate errors present in the textual part or in the image part. We first analyzed how well the LLMs performed in detecting the textual error, followed by their performance in the case of image errors. Note this data was again annotated by the SMEs as stated in section 5.2. The error detection accuracy (ACED) for textual or image errors is computed as\n$ACED = \\frac{Number \\,of \\,errors \\,detected}{Total \\, number \\, of \\,errors}$"}, {"title": "5.4.1 Text Error based Rubrics", "content": "First, we analyze the performance of LLMs when it comes to detecting errors present in the textual part of the student answer. Figure 3 contains the ACED values pertaining to the textual error of each LLM across each subject and all the subjects as a whole. It can be seen that Pixtral has performed the best when it comes to detecting textual errors across"}, {"title": "5.4.2 Image Error based Rubrics", "content": "Lastly, we analyze whether the LLMs were capable of detecting errors present in the image part of the student's answer as well. Figure 4 contains the $ACED$ pertaining to the image errors of each LLM on each subject and on all subjects as a whole. If we consider all subjects, Gemini performed the"}, {"title": "6 Conclusion", "content": "This paper introduces the MMSAF problem along with a dataset of 2197 data points. The MMSAF dataset contains physics, chemistry and biology questions from high school textbooks. Additionally, we provide an automated framework to generate similar datasets, given a set of questions, reference answers, and annotated images. We also establish a baseline using 4 models, namely ChatGPT, Gemini, Pixtral and Molmo.\nOur evaluations show that while Gemini performed the best in generating the correctness labels and ChatGPT excelled in generating the image relevance labels, human evaluation proved that Pixtral was more aligned towards human judgement and values. Since the feedback should have positive feedback on the student and should be similar to that provided by a human, we conclude that Pixtral will be the most suitable candidate for this problem. Additionally, it is to be noted that Pixtral needs to be improved to generate the LC and IR labels to be deployed in an ITS. Thus future work also involves exploring other solutions, including Retrieval augmented generation (RAG) based approaches to add more insight and conceptual depth to the feedback."}, {"title": "Limitations", "content": "This work introduces the Multimodal Short Answer Grading with Feedback (MSMAF) problem along with a dataset of 2197 data points. It is to be noted that the number of partially correct student responses compared to correct and incorrect student responses is significantly higher and follows a ratio of 1:2:1. This results in a class imbalance problem for models that will be trained on this data. Also, the dataset is currently restricted to physics, chemistry, and biology at the high school level. Questions from other subjects and university level courses can be included in this dataset to increase its complexity level and develop a more robust dataset that will cater to multiple domains at the same time. Apart from this, the partially correct images currently have to be manually annotated, and this leads to a scalability issue. This problem can be automated using simple text manipulation operations performed via OpenCV or by generating diagrams using solutions such as DiagrammerGPT (Zala et al., 2024) once their code is released."}, {"title": "Ethical Considerations", "content": "The questions and reference answers have been extracted from the National Council of Educational Research and Training (NCERT) textbooks for 10th standard science and 11th standard and 12th standard physics, chemistry and biology. NCERT is an autonomous organization established by the Government of India that provides guidance and recommendations to both central and state governments on policies and initiatives to improve the quality of school education. The textbooks have been downloaded from https://www.ncrtsolutions.in/. We adhere to NCERT guidelines, which states that \"NCERT books can also be downloaded free of cost from our website for non-commercial purposes.\""}, {"title": "I Dataset Split Statistics", "content": "The data has been split into the train,test and validation sets in the ratio of 3:1:1 and the statistics is as follows -"}, {"title": "JExperimental Setup", "content": "The system specifications used for LLM Inference are as follows -\n\u2022 CPU : Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz with 112 CPUs\n\u2022 RAM: 503 GB\n\u2022 GPU: NVIDIA A100-SXM GPU with 80 GB of memory (Only 1 out of 4 used)\nThe relevant models used from huggingface and OpenAI and Gemini APIs have already been mentioned in Section 4. All inference operations uses default hyperparameters."}]}