{"title": "HAND-OBJECT RECONSTRUCTION VIA\nINTERACTION-AWARE GRAPH ATTENTION MECHANISM", "authors": ["Taeyun Woo", "Tae-Kyun Kim", "Jinah Park"], "abstract": "Estimating the poses of both a hand and an object has become an important area of research due to the growing need for advanced vision computing. The primary challenge involves understanding and reconstructing how hands and objects interact, such as contact and physical plausibility. Existing approaches often adopt a graph neural network to incorporate spatial information of hand and object meshes. However, these approaches have not fully exploited the potential of graphs without modification of edges within and between hand- and object-graphs. We propose a graph-based refinement method that incorporates an interaction-aware graph-attention mechanism to account for hand-object interactions. Using edges, we establish connections among closely correlated nodes, both within individual graphs and across different graphs. Experiments demonstrate the effectiveness of our proposed method with notable improvements in the realm of physical plausibility.", "sections": [{"title": "1. INTRODUCTION", "content": "Hand-object pose estimation is a promising research field for VR/AR [1], human-computer interaction [2], and robotics [3]. The primary goal is to understand and make the hand interact with its surrounding objects. Previous approaches [4, 5, 6, 7, 8, 9, 10, 11, 12] estimated hand and object poses from images under hand-object interaction (HOI) scenarios. At an early stage, these poses are estimated as a skeletal chain by connecting hand joints. However, it is too sparse to represent HOI, such as contact regions on the hand and the object's surface. For this reason, recent methods [5, 6, 8, 9, 10, 13] predicted 3D shapes that convey dense surface points.\nIn the context of HOI reconstruction, such as contacts and physical plausibility, the representation should be high-dimensional i.e. dense surface points or meshes, since HOI occurs on the surfaces of the instances. In addition, physically feasible hand-object shapes and poses are essential to improve usability and stability for practical applications. Several approaches [6, 9, 10, 11, 12] have incorporated a feature fusion scheme that combines the latent features of the hand and the object. This scheme enables accurate estimation of hand and object poses, considering their interaction through jointly combined features. Furthermore, it can improve physical plausibility, e.g. minimizing penetration volume. Typically, there are two methods for feature fusion: (1) latent feature fusion and (2) graph-based feature fusion.\nThe latent feature fusion schemes [6, 9, 10] utilize two separate encoders to extract each latent feature of the hand and object from an input image. Subsequently, two features are fused, and hand- and object-poses are estimated from the combined feature. This technique improves physical plausibility compared to those estimating the hand and object independently. However, it is hard to explicitly control the fusion of distinct features within the network.\nTo address this limitation, some methods [10, 11, 12] adopted graph neural networks (GNNs) [14, 15]. This technique is advantageous when dealing with data structures that involve spatial information and connectivity, such as meshes. Although graph-based methods show improvements over latent feature fusion, the fusion of hand and object features typically occurs during the graph initialization [10] or node initialization stages [12], limiting the consideration of node connectivity. An alternative approach by Zhang et al. [11] modified the hand and object graph by adding new edges. However, their method is limited to estimating sparse hand-object poses: hand joints and the object bounding box.\nPrevious graph-based approaches focused on incorporating appropriate features into the mesh-structured graph nodes of the hand graph and object graph, neglecting the consideration of node connectivity. On the other hand, our approach aims to enhance physical plausibility by introducing node-connecting schemes that link highly correlated nodes. We define two types of edges: common relation edges $E_c$ and attention-guided edges $E_a$. These edges link not only intra-class (each hand or object separately) nodes but also inter-class (hand-object) nodes. This distinguishes our method from previous approaches [10], which focused solely"}, {"title": "2. RELATED WORKS", "content": "In earlier days, hand and object poses were predicted in the form of hand joints and a 3D bounding box of the object. However, these representations are too sparse to adequately illustrate the hand-object interaction, since they do not fully capture the hand-object shape. Instead of estimating sparse poses, numerous approaches [5, 6, 8, 9, 10] have emerged to estimate hand-object 3D shapes. Hasson et al. [5] first suggested both hand- and object-mesh estimation techniques. They estimated the hand mesh with MANO [17], a parameterized hand model; and the object mesh is acquired through AtlasNet [18]. We adopted their framework to predict the initial hand and object meshes.\nPrevious methods estimate the hand and object meshes in a separate branch independently. However, these methods are inherently limited to the consideration of HOI. The hand and object themselves give key insights into each other: the hand pose is highly correlated with the object's shape and features, and vice versa. In this respect, [6, 8, 10, 12] proposed a feature fusion technique between a hand and an object where each hand and object is estimated from the combined feature of the hand and object.\nLatent feature fusion. Chen et al. [9] reconstructed hand-object meshes from an image using latent feature fusion on top of the network architecture of Hasson et al. [5]. In contrast to Hasson et al., Chen et al.'s network estimated hand and object meshes by fusing the features of hand and object through an LSTM module. This fusion technique reduces the inter-penetration volume between the hand and the object. Liu et al. [6] introduced the attention [19] based feature fusion scheme to improve the feature of an object by the feature of a hand. In this work, we consider hand-object interaction, connecting highly correlated nodes to incorporate each feature of the hand and object. We employ a graph-based feature fusion rather than the latent feature fusion scheme.\nGraph-based feature fusion. Graph-based feature fusion [4, 10, 11, 12] combines hand and object features using a GNN on a graph structure, while the latent feature fusion scheme integrates features in the latent space. Most methods in this domain estimate initial hand and object meshes (or joints), and construct graphs whose structures are the same as the estimated individual meshes. Given the two sub-graph structures, the feature fusion is done by injecting both features to each node of graphs. Subsequently, GNN layers operate on these graphs, and it outputs the feature vector which is used to refine the initially estimated hand object meshes. For instance, Tse et al. [10] refined the initial meshes with the image feature and the graph feature generated by the GNN layers. Wang et al. [12] constructed the hand and object graphs, and refined the initial meshes through cross-attention between the hand and object graph nodes.\nMost methods, however, maintain a static graph structure, solely relying on the initially estimated hand and object meshes without structural changes. Though Tse et al. have used a different graph from the initial meshes by newly connecting the nodes, it only involves the modification between intra-class nodes (i.e. within each hand or object), not inter-class nodes. While Zhang et al. [11] proposed edge modifications connecting both intra-class and inter-class nodes, they estimated hand joints and an object bounding box, limiting their understanding of hand-object interactions. In contrast, our method incorporates not only mesh-structured edges but also additional edges between intra-class nodes and inter-class nodes. We demonstrate the improvement in physical plausibility between the hand and the object via our interaction-aware graph-attention mechanism."}, {"title": "3. INTERACTION-AWARE GRAPH ATTENTION", "content": "The proposed method estimates hand and object meshes from a single image, and fuses both the hand and object features with an interaction-aware graph attention mechanism. Our network architecture is shown in Fig. 1. In the following, we describe each part of our method in detail."}, {"title": "3.1. Initial mesh estimation", "content": "The hand mesh and object mesh are predicted in independent branches, and predicted meshes are on a hand-wrist coordinate, where the root of the hand (wrist) is the origin. Note that we follow Hasson et al.'s [5] architecture design to estimate the initial meshes.\nHand mesh estimation. From an input image, ResNet-18 [20] extracts a hand feature vector $S_{hand}$. The hand feature is fed into the MANO [17] differential layer [5], and outputs the MANO pose and shape parameters. The hand mesh $m_{hand}$ is reconstructed from these parameters.\nObject mesh estimation. As in Hasson et al. [5], we first pre-train the AtlasNet [18] with normalized point clouds. ResNet-18 extracts an object feature vector $S_{obj}$ and the feature is fed into AtlasNet. Then, AtlasNet estimates the normalized object mesh $m_{obj}^{'}$ Since all object meshes have different num-"}, {"title": "3.2. Graph initialization", "content": "Graph initialization is two-fold: node initialization and edge initialization. These are then utilized in the graph refinement stage.\nNotations. A graph $G$ is defined as $G = (N, E)$ where $N$ is a set of nodes $n$, and $E$ is a set of edges $e$. Let us define the hand graph $G_{hand} = (N_{hand}, E_{hand})$ and the object graph $G_{obj} = (N_{obj}, E_{obj})$. Within the graphs, each node is represented by $n_i^{hand} \u2208 N_{hand}$ and $n_i^{obj} \u2208 N_{obj}$, where $i$ denotes the node index.\nNode initialization. To each node, the corresponding vertex coordinate $v$ of the estimated mesh $m$ and the extracted image feature $\\zeta$ are injected. Especially, for each node $i$ of hand and object graphs, the hand node feature $x_i^{hand}$ and object node feature $x_i^{obj}$ are defined as follows: $x_i \\leftarrow v \\oplus \\zeta ; \\zeta\u2208 {hand, obj}, where $\\leftarrow$ refers to the injection of features and $\\oplus$ represents the concatenation. The nodes' feature sets are denoted as $X_{hand}$ and $X_{obj}$ for the hand and the object.\nEdge initialization. We connect nodes to consider hand-object interactions with two types of edges: common relation edges $E_c$ and attention-guided edges $E_a$. Both types of edges link nodes not only within the same class (hand-hand and object-object) but also across different classes (hand-object and object-hand). $E_c$ is constructed by the spatial distance between the graph nodes, representing common adjacent behaviors. However, relying solely on common relation edges may miss contextual or global features of hand-object interactions. Therefore, we introduce $E_a$ based on the distance in a latent space defined by the attention mechanism [19].\nFor both $E_c$ and $E_a$, there are four types of edges: $E^{hh}$, $E^{oo}$, $E^{ho}$, and $E^{oh}$. The upper subscript denotes the type of node, where $h$ represents a hand and $o$ represents an object; $ho$ indicates edges from a hand node to an object node. Additionally, $E^{hh}$ and $E^{oo}$ are homogeneous undirected edges, while $E^{ho}$ and $E^{oh}$ are heterogeneous directed edges. During the graph refinement stage, the combination of these two edge sets (from $E_a$ and $E_c$) is considered as the final graph's edge set.\nCommon relation edges $E_c$. There are four kinds of common relation edges: $E^{hh}$, $E^{oo}$, $E^{ho}$, and $E^{oh}$. The edge sets $E^{hh}$ and $E^{oo}$ represent the mesh faces $f_{hand}$ and $f_{obj}$, respectively.\nIn contrast, $E^{ho}$ and $E^{oh}$ are initialized, connecting the edges between the nearest node pairs from hand to object nodes, and vice versa. This scheme looks similar with Zhang et al. [11], however, we do not connect all possible edges between every closest pair of nodes each other. There are two reasons behind this design. First, Zhang et al. estimated hand joints and an object bounding box that is a sparse representation than meshes. Hence, if we connect all pairs of dense mesh nodes, it computationally costs. The other reason is that we aim to connect only relevant nodes. Therefore, we selectively connect pairs of nodes based on the contact prior. This strategy ensures computational efficiency and captures meaningful interactions between the hand and object.\nFor $E^{ho}$, we connect the edges from designated hand nodes to their nearest object nodes. These designated hand nodes are frequently in contact with an object in ObMan [5] dataset. Unlike hands, objects do not have specific contact regions on their surfaces. To address this, we simply reverse the edges of $E^{ho}$ as $E^{oh}$. We also adopt this strategy to DexYCB [16] training."}, {"title": "3.3. Refinement stage", "content": "Graph refinement stage gets hand node features $X_{hand}$ and object node features $X_{obj}$, and outputs vertex displacements through four graph convolution (GC) blocks. Each GC block consists of four GCN layers, operating on the final graph design ($G^{hh}, G^{oh}, G^{ho}, and G^{oo}$). The last GC block estimates vertex displacements which are added to corresponding vertices of hand mesh and object mesh for the refinement. The structure of GC block is illustrated in Fig. 2.\nPreliminary. The graph convolution [15] operates by updating node features through a message passing scheme. This scheme consists of two steps: aggregation and update. The aggregation gathers the node features of one node $i$ and its neighboring nodes $\\mathcal{N}(i)$, and encapsulated into a message $msg_i^{(k)}$ along node $i$ as follows:\n$msg_i^{(k)} = AGGREGATE^{(k)}(n_p^{(k-1)}, p\u2208(\\mathcal{N}(i) \u222a {i})))$.\nHere, $n_p^{(k-1)}$ represents the feature of node $p$ from the $(k-1)$-th layer, and $\\mathcal{N}(i)$ denotes the set of neighboring nodes of node $i$. The aggregate function $AGGREGATE$ combines the features of neighboring nodes by multiplying each node's feature with its corresponding edge weight, and then summing them. Following the aggregation step, the update stage modifies the node feature at the $k$-th layer as follows:\n$n_i^{(k)} = UPDATE^{(k)}(n_i^{(k-1)}, msg_i^{(k)})$.\nThe update function $UPDATE$ changes the node feature to the aggregated message at the current layer. This message passing scheme allows information to flow between neighboring nodes, enabling the graph convolution layer to capture relational dependencies and refine node features accordingly.\nGraph convolution block. As in Fig. 1, we employed four graph convolution blocks, and each layer consists of four GCNs. Each GCN layer receives the corresponding node features and operates on the final graph structure: $G^{hh}, G^{oh}, G^{ho}, and G^{oo}$. Except for the final layer, the other layers get a previous stage's hand node features $X_h^{(k-1)}$ and object node features $X_o^{(k-1)}$, then output updated node features $X_h^{(k)}$ and $X_o^{(k)}$. Whereas, the final layer gets all previous node features from the preceding layers. This design choice mitigates the graph over-smoothing [21], where all nodes in the graph tend to converge to a single value as more graph convolution layers are stacked. As a result, the final GC block outputs vertex displacements, and the displacements are added to the corresponding vertices of hand mesh and object mesh respectively."}, {"title": "3.4. Training losses", "content": "Hand loss $\\mathcal{L}_{hand}$. The hand loss $\\mathcal{L}_{hand}$ comprises three components: hand mesh vertices loss $\\mathcal{L}_{v}$, the $L2$ term for hand joints loss $\\mathcal{L}_{J}$, and a regularization loss term for shape parameters $\\mathcal{L}_{\\beta}$. $\\mathcal{L}_{v}$ and $\\mathcal{L}_{J}$ guide the estimated hand mesh vertices $V_{hand}$ and hand joint $J_{hand}$ to be similar to the ground truth values by L2 distance. The regularization loss $\\mathcal{L}_{\\beta} = ||\\beta||^2$ incorporates shape parameter $\\beta$ to be close to the average shape in the MANO hand set. The hand reconstruction loss is a summation of these three terms: $\\mathcal{L}_{hand} = \\mathcal{L}_{v} + \\mathcal{L}_{J} + \\mathcal{L}_{\\beta}$.\nObject pre-training loss $\\mathcal{L}_{mobj}$. $\\mathcal{L}_{mobj}$ is utilized to pre-train the AtlasNet with normalized point clouds. It consists of the mean symmetric chamfer distance loss $\\mathcal{L}_{CD}$, edge regularizing loss $\\mathcal{L}_{e}$, and Laplacian smoothing loss $\\mathcal{L}_{L}$. $\\mathcal{L}_{CD}$ ensures the estimated object mesh aligns with the ground truth. $\\mathcal{L}_{e}$ encourages the uniform edge lengths in the object mesh, and $\\mathcal{L}_{L}$ guides the reconstructed mesh faces to be smooth, and $\\mathcal{L}_{mobj}$ is the sum of these terms with weights $\\lambda_e = 2$ and $\\lambda_L = 0.1$ as in the previous work [5]: $\\mathcal{L}_{mobj} = \\mathcal{L}_{CD} + \\lambda_e \\mathcal{L}_{e} + \\lambda_L \\mathcal{L}_{L}$.\nObject training loss $\\mathcal{L}_{obj}$. Unlike pre-training, the final object mesh is defined on the hand-wrist coordinate, hence, we have to consider the coordinate-aligned ground truth. We additionally estimate the scale and translation to modify the normalized mesh. As we adopted Hasson et al.'s [5] framework, we followed their loss settings. In summary, the object branch is trained by object training loss $\\mathcal{L}_{obj} = \\mathcal{L}_{mobj} + \\mathcal{L}_{trans} + \\mathcal{L}_{scale}$.\nRefinement loss $\\mathcal{L}_{refine}$. Refinement loss $\\mathcal{L}_{refine}$ enforces the refined hand mesh and refined object mesh similar with the ground truth. $\\mathcal{L}_{refine}$ is the summation of $\\mathcal{L}_{hand}$ and $\\mathcal{L}_{obj}$, while $\\text{\\'}$ denotes the loss between the refined pose and the ground truth pose. Unlike $\\mathcal{L}_{hand}$, $\\mathcal{L}_{hand}$ does not contain $\\mathcal{L}_{\\beta}$, as we did not utilize MANO during the refinement stage."}, {"title": "4. EXPERIMENTS", "content": "We present experimental details on the implementation and datasets. Subsequently, comparisons with the baselines and results of the ablation study are presented."}, {"title": "4.1. Experimental details", "content": "Implementation details. We trained the AtlasNet with normalized object point clouds at a learning rate $10^{-4}$ for 200 epochs. During ObMan dataset training, the network is trained at a learning rate $10^{-4}$ for 200 epochs, and it decreases to $10^{-5}$ until the next 100 epochs are finished. During DexYCB dataset training, the network is trained at a learning rate $10^{-4}$ for 20 epochs. We employed the Adam optimizer [22] to all experiments. When we trained AtlasNet alone, we supervised the normalized object estimation with $\\mathcal{L}_{mobj}$. The final network is supervised with $\\mathcal{L}_{obj} + \\mathcal{L}_{hand} + \\mathcal{L}_{refine}$.\nDatasets. We employed two datasets for training and evaluation: ObMan [5] and DexYCB [16]. ObMan is a synthetic dataset that illustrates when a hand grasps an object. Our model is trained with 141K images training split of ObMan, and evaluated on 6.2K images in the test split. DexYCB is a real-world dataset that is captured when the hand grasps an object. We used the official split 's0' to train and evaluate our model. As the baseline [10], we filtered DexYCB images in which the hand and object are 1cm apart, naming as DexYCB. There are 300K training images in DexYCB\u00af.\nEvaluation metrics. Our method is evaluated on four metrics with baselines: hand error (H.E.), object error (O.E.), maximum penetration (M.P.), and intersection volume (I.V.). Hand error measures the average error between ground truth hand joints and estimated hand joints. The mean symmetric Chamfer distance (mm) between ground truth object mesh and estimated object mesh is used as object error. For physical plausibility, we adopt maximum penetration (mm) and intersection volume (cm\u00b3). The maximum penetration measures the maximum penetrated depth from hand mesh vertices to the object mesh's surface when a collision occurs. If not, this value becomes 0. The intersection volume measures intersected volume between the voxelized hand and object with a voxel size of 0.5cm. Additionally, we employ hand mesh error which measures the mean error between the hand meshes for self-evaluation."}, {"title": "4.2. Main results", "content": "Our quantitative and qualitative results are presented in Table 1 and Fig. 3, respectively. Significantly, our approach surpasses existing baseline methods [5, 10] in terms of maximum penetration depth and intersection volume metrics, while our method achieves better than Hasson et al. [5] across all evaluation metrics.\nRegarding the hand error, ours is comparable to Tse et al. [10], yet the hand error in the ObMan dataset degenerates slightly. This limitation stems from our vertex displacements breaking the MANO hand mesh structure, leading to significant deviations in the positions of the hand joints, particularly at the root position. There is a gap between the differences of the object error in ObMan [5] and DexYCB [16], compared to Tse et al. Since ObMan contains 2.7K distinct object meshes, whereas, DexYCB dataset contains fewer available object meshes (20 object meshes) and a scarcity of unique scenarios. For this reason, our model can learn the objects' shape better with ObMan than DexYCB.\nMoreover, the annotated hand meshes and object meshes in ObMan do not intersect each other, as ObMan is generated by a physical simulator. However, annotations of DexYCB contain an intersection error between hand meshes and object meshes (M.P.: 4cm / I.V.: 4.3cm\u00b3), since DexYCB is collected in the real-world. Hence, our method struggles to learn the hand-object interaction from the DexYCB, as our graph refinement strategy can capture the correlation between a hand and an object.\nAs mentioned above, ensuring physical plausibility is important for practical applications [1, 2, 3], as the instability by the physical implausibility between the hand and object harms the usability than the decline of usability by less accurate hand-object pose. Hence, our method can be more effective than the baselines [5, 10] for practical applications.\nFurthermore, we evaluate the effectiveness of our refinement stage by comparing between initial estimates and their refined meshes, as illustrated in Table 2 and Fig. 4. Notably, there is a significant improvement in penetration after the refinement stage."}, {"title": "4.3. Ablation study", "content": "To prove the effect of the common relation edges $E_c$ and the attention-guided edges $E_a$, we trained our network without $E_c$ and $E_a$, respectively. The quantitative and qualitative results are given in Table 3 and Fig. 5, respectively. In some metrics, using one of the edges achieves higher results, such as hand joint error in w/o $E_c$. However, when we see the qualitative results in Fig. 5, the refined hand-object poses without $E_c$ and $E_a$ are not visually favorable."}, {"title": "5. CONCLUSION", "content": "We proposed an interaction-aware graph attention mechanism aimed at improving the physical plausibility of hand-object pose estimation. This mechanism incorporates common relation edges and attention-guided edges, fostering connections within intra-class nodes and between inter-class nodes. We have shown that our approach enhances the physical plausibility of estimated hand and object meshes, particularly in addressing issues related to penetration and intersection between them. By improving physical plausibility of the estimated hand and object, both usability and stability are guaranteed for practical applications that need realistic hand-object interaction, such as a hand simulator and imitation learning."}]}