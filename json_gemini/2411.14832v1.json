{"title": "VisGraphVar: A BENCHMARK GENERATOR FOR ASSESSING VARIABILITY IN GRAPH ANALYSIS USING LARGE VISION-LANGUAGE MODELS", "authors": ["Camilo Chac\u00f3n Sartori", "Christian Blum", "Filippo Bistaffa"], "abstract": "The fast advancement of Large Vision-Language Models (LVLMs) has shown immense potential. These models are increasingly capable of tackling abstract visual tasks. Geometric structures, particularly graphs with their inherent flexibility and complexity, serve as an excellent benchmark for evaluating these models' predictive capabilities. While human observers can readily identify subtle visual details and perform accurate analyses, our investigation reveals that state-of-the-art LVLMs exhibit consistent limitations in specific visual graph scenarios, especially when confronted with stylistic variations. In response to these challenges, we introduce VisGraphVar (Visual Graph Variability), a customizable benchmark generator able to produce graph images for seven distinct task categories (detection, classification, segmentation, pattern recognition, link prediction, reasoning, matching), designed to systematically evaluate the strengths and limitations of individual LVLMs. We use VisGraphVar to produce 990 graph images and evaluate six LVLMs, employing two distinct prompting strategies, namely zero-shot and chain-of-thought. The findings demonstrate that variations in visual attributes of images (e.g., node labeling and layout) and the deliberate inclusion of visual imperfections, such as overlapping nodes, significantly affect model performance. This research emphasizes the importance of a comprehensive evaluation across graph-related tasks, extending beyond reasoning alone. VisGraphVar offers valuable insights to guide the development of more reliable and robust systems capable of performing advanced visual graph analysis.", "sections": [{"title": "1 Introduction", "content": "Graphs stand or fall by their choice of\nnodes and edges.\nWatts and Strogatz\nTo prove their relevance, new technologies must face challenging scenarios. Such trials reveal their true\npotential and distinguish meaningful innovations from superficial hype, often lacking technical depth.\nThe rise of foundation models presents an unprecedented opportunity to test this principle. Large\nVision-Language Models (LVLMs) significantly extend the capabilities of foundation models [60], enabling\nmore advanced multimodal interactions and applications [57]. Building on the progress of Large Language\nModels (LLMs)\u2014including Transformer-based architectures like GPT [36] and BERT [9], as well as hybrid\nsystems like BLIP [23] and Florence [58]\u2014LVLMs seamlessly integrate visual analysis into problem-solving,"}, {"title": "2 Related work", "content": "Automating the interpretation of images without dependence on labeled training data has long been a central\nchallenge in computer vision. Traditional vision-language models like VisualBERT [24], VilBERT [29], and\nLXMERT [43] (2019-2020) required task-specific labeled datasets for operations such as Visual Question\nAnswering and Image Captioning. CLIP (Contrastive Language Image Pretraining) changed this paradigm\nas the first model to enable large-scale image interpretation without task-specific labels [38]. Through\nits dual transformer-based architecture [11, 47], combining image and text encoders, CLIP can interpret\nimage-text relationships in a zero-shot manner, forming novel connections without needing prior examples.\nHowever, not all images pose the same difficulty for a model to interpret them.\nThe emergence of advanced multimodal LVLMs like Gemini Pro-1.5 [45], GPT-40 [55], Llama-3.2 [12], and\nClaude-3.5-Sonnet [44] has sparked interest in testing their capabilities with complex geometric structures\nlike graphs. Recent benchmarks evaluate these models' graph understanding abilities: VisionGraph [27]\nfocuses on algorithmic tasks like cycle detection and shortest path finding using few-shot and chain-of\nthought prompting. At the same time, GITA [53] offers an end-to-end framework that leverages visual graph\nrepresentations to enable general graph reasoning. However, it is crucial to recognize that visual graph\nanalysis encompasses a much broader scope of challenges beyond the reasoning tasks that GITA addresses.\nWilliams et al. [54] have also proposed a benchmark, though with a more limited task scope and LVLM\ncoverage. Although DynaMath [64] is not exclusively dedicated to graph evaluation, it incorporates graph\nanalysis as one component of its assessment methodology. Nevertheless, analogous to GITA, its primary\nfocus remains on visual reasoning tasks. All these benchmarks, though significant, examine only a narrow\nrange of graph visualization formats, ignoring key visual aspects of 2D graph representations."}, {"title": "3 VisGraphvar: A benchmark generator", "content": "We present VisGraphVar, a customizable benchmark generator that produces evaluation datasets designed\nto assess the robustness of LVLMs in multimodal graph analysis tasks. This benchmark generator offers\na novel approach to evaluating a critical aspect of graph images: the ability to interpret representational\nvariability. This variability is measured through seven independent graph-analysis tasks designed to test\nmodel performance. This allows us to pinpoint each model's strengths and weaknesses across a range of\ndifferent tasks, providing a comprehensive, independent assessment of LVLM performance."}, {"title": "3.1 A Custom Synthetic Dataset", "content": "VisGraphVar was implemented in Python 3.11 using the NetworkX library for graph image generation. The\ncapacity for customization and extensibility constitutes a fundamental requirement in benchmark design [46].\nOur generator fully implements these capabilities through its modular architecture. The graph visualization\npipeline supports multiple parameterization options:\n1. Layout Selection and Its Impact. Previous graph image benchmarks have largely overlooked\nthe importance of graph layout, typically resorting to aesthetically pleasing arrangements or using\nstandard library defaults (e.g., NetworkX in Python). However, since their training data influences\nLLVMs' inferences, the spatial arrangement of nodes and edges can significantly impact their ability\nto detect evident visual patterns. In general, layout selection is crucial in many graph applications\nbecause the inherent flexibility of graph structures allows us to emphasize specific characteristics\nover others. For example:\n\u2022\nThe spring layout effectively highlights nodes with high connectivity centrality\n\u2022\nA circular layout excels at visualizing disconnected components by maintaining their visibility\nwhile keeping them organized separately from the main graph\n\u2022\nA spectral layout effectively reveals community structures and node clusters by positioning\nnodes based on global connectivity patterns\nThe choice of layout, therefore, depends on which graph properties need emphasis [3]. Consequently,\nif an LLVM fails to perform consistently across different layouts, its practical applications could be\nsignificantly limited. This layout sensitivity becomes particularly critical when the same graph\nstructure, presented in different spatial arrangements, yields varying inference results.\n2. Configurable Graph Element Parameters. The ability to adjust node quantities and edge\nconnection probabilities enables evaluation of how visual complexity affects LLVM performance.\nThis configurability allows us to assess, for example, whether increasing the number of elements in\nthe image degrades LLVM response quality.\n3. Color Schemes and Text Labels. Existing benchmarks often overlook the significance of color\nvariations and label presence in graph visualizations. This oversight is particularly concerning since\nreal-world graph applications heavily emphasize stylistic elements. The ability of LLVMs to capture\nthese subtle visual attributes so far remains uncertain.\nAlthough GITA [53] makes use of a dataset with visual graph attributes such as layout selection, node shape,\noutline style, and edge thickness, the presented evaluation remains limited to reasoning tasks. VisGraphVar\nenables a more comprehensive approach by broadening the range of visual variations and structuring the\nanalysis with multiple tasks, creating a robust evaluation framework capable of assessing both current and\nfuture LVLMs."}, {"title": "3.2 Tasks", "content": "A benchmark that only covers limited aspects of a technology may overlook potential shortcomings. In\nresponse to this challenge, VisGraphVar implements seven comprehensive evaluation tasks (see Figure 1), each\ndesigned to assess distinct aspects of LVLM graph comprehension capabilities while maintaining compatibility\nwith all aforementioned customization options. Below is an explanation of each task and the reasoning behind\nthe choices made."}, {"title": "3.2.1 Task 1: Node and Edge Detection", "content": "A proficient LVLM for graph analysis should be able to accurately detect and count existing elements before\nprogressing to more complex reasoning tasks. This basic capability is a prerequisite for more sophisticated\nanalyses. For this reason, we have incorporated a variety of stylistic variations into the generation options\nfor this task, as shown in Figure 2.\nThese variations include not only the graph layout but also the use of arrows, labels, and distinct node\ncolors. An effective LVLM should be able to follow the instructions provided by means of the prompt. For\ninstance, when counting elements, attributes such as node labels, directed or undirected edges, or varying\ncolors should not influence the count. However, as we will explore in the next section, not all LVLMs meet\nthese criteria.\nVisual benchmarks often assume that visual representations are flawless [53, 27]. In reality, visual imperfec-\ntions are common, especially as networks grow larger, and can occur either intentionally or unintentionally.\nCreating visualizations may be error-prone. Since humans naturally adapt to and work around these vi-\nsual defects, VisGraphVar deliberately incorporates and sometimes introduces such imperfections into the\ngenerated graph images.\nNode and/or edge overlap is a prime example of these real-world visual challenges. In our evaluation\napproach, we expect LVLMs to provide counts that closely approximate the actual number of elements, even\nwhen some are overlapping. For instance, if an image contains 10 nodes with three overlapping ones, an\nLVLM that detects 9 nodes demonstrates better performance than one that only identifies 8. This mirrors\nhuman visual processing\u2014we can typically make reasonable estimates of overlapping elements, even when"}, {"title": "3.2.2 Task 2: Classification", "content": "In addition to mastering the task of detecting and counting nodes and edges, LVLMs should be able\nto classify a graph in the sense of determining its type. This task presents greater complexity as it\nrequires global image analysis, considering edge positions and directions. Such capabilities are crucial for\ncomprehensive graph analysis.\nThis classification task is essential, as shown already in related applications. LVLMs have been used, for\nexample, to interpret and analyze various types of charts [32, 18], including boxplots, pie charts, etc. This\nprocess begins with classifying the detected chart type, a crucial step before conducting any detailed anal-\nsis. Furthermore, classification is essential in real-world applications, such as the classification of medical\nimages [16]. Thus, classification\u2014whether for visual graphs, charts, or real-world applications\u2014is a funda-\nmental task for any foundational vision model.\nTo evaluate an LVLM's proficiency in understanding these higher-order relationships between nodes and\nedges, we examine its ability to classify seven fundamental graph types (see Figure 4):\n1. Acyclic graphs: graphs without loops, that is, without any path starting from a node and returning\nto that node.\n2. Cyclic graphs: graphs containing one or more loops.\n3. Bipartite graphs: the set of nodes is partitioned into two distinct node sets with edges only\nbetween these two sets.\n4. Complete graphs: every node connects to all others.\n5. Meshs: regular grid-like structures.\n6. Planar graphs: graphs that can be drawn without edge crossings.\n7. Tree: Hierarchical graphs branching from a root.\nEach graph type presents unique analytical challenges. The selection included graphs that are clearly distinct\nfrom each other, such as trees and complete graphs, as well as graphs that are similar but with subtle\ndifferences, such as cyclic and acyclic graphs. This variety augments the thoroughness of the LVLM's\nanalysis. For example, identifying cycles requires the model to trace edge paths and detect closed loops, while\nrecognizing bipartite structures demands understanding node groupings and their interconnection patterns.\nSimilarly, tree classification requires comprehending hierarchical relationships and directional flow from root\nto leaves. A LVLM should possess these analytical capabilities to accurately classify graph structures and\nunderstand their inherent properties. This understanding forms a crucial foundation for more complex visual\nreasoning tasks.\nThe VisionGraph benchmark [27], for example, includes a classification task within its reasoning tasks,\nspecifically to detect the presence of a cycle in a graph. In contrast, our benchmark generator offers six"}, {"title": "3.2.3 Task 3: Segmentation", "content": "Beyond the classification of graphs, LVLMs must identify critical regions within graph images, such as\ncut-edges (or bridges). A cut-edge is an edge whose removal increases the graph's number of connected\ncomponents effectively segmenting the graph into two or more disconnected subgraphs [13, 2]. In the\nintroduction to their article, Camilus and V K [4] stated the following:\nImage segmentation can simply result in an image partition composed of relevant regions.\nThe task of dividing a graph into relevant regions (or segments) has numerous practical applications, includ-\ning analyzing graph connectivity [49], optimizing routing [35], and identifying potential failure points and\ndependencies [20].\nOne of the main challenges is accurately detecting cut-edges (bridges). As shown in Figure 5, when the num-\nber of nodes increases, finding the cut-edge becomes even more difficult due to inevitable element overlaps.\nThis difficulty is present even for the human eye. Therefore, having a quick visual detection method that\nidentifies which nodes form a bridge would greatly aid in the analysis.\nAn LVLM capable of correctly detecting a bridge suggests that it can analyze the graph not only at a global\nlevel (considering all connections between nodes) but also identify the specific two nodes whose removal\nwould increase the number of connected components in the graph. This capability demonstrates a nuanced\nunderstanding of both the graph's structure and critical points that impact connectivity."}, {"title": "3.2.4 Task 4: Pattern Recognition", "content": "Identifying a cut-edge requires precise, targeted global analysis, but recognizing and counting specific patterns\nwithin a graph introduces an even higher level of complexity. Task 4 involves more than a surface-level scan;\nthe LVLM must recognize each unique pattern formed by nodes and edges, classify them accurately, and\nthen provide a count (see Figure 6). This step builds upon Task 2 (classification, Section 3.2.2) by requiring\nan additional depth of pattern recognition. Such analysis is especially useful for disconnected graphs, where\nunderstanding the isolated cluster structures is crucial."}, {"title": "3.2.5 Task 5: Link Prediction", "content": "Apart from memory retention and pattern recognition, there are other crucial tasks in graph analysis. A\nvital aspect concerns reasoning, with the example of link/edge prediction. When examining a graph, its\nstructure and the positioning of nodes and edges (determined by the layout) may potentially suggest missing\nconnections. By analyzing the overall topology, we can identify nodes with similar structural patterns that\ncould logically be connected. This concept, known as link prediction, emerges when nodes sharing similar\ntopological characteristics are not yet connected but show potential for connection [31]."}, {"title": "3.2.6 Task 6: Reasoning", "content": "If link prediction already requires the model to perform a certain level of reasoning, asking it to apply an\nalgorithm to the graph displayed in the image significantly increases the difficulty. This is no longer just\na visual task that can be inferred from the position of nodes and edges. An example is the problem of\nfinding shortest paths, where the model must identify nodes, detect edge directions, store edge weights in\nmemory, and then determine the shortest path between a source and destination. This task will test both\nthe reasoning and memorization capabilities of the LVLMs.\nMoreover, this type of task requires applying numerical analysis to visual information. Generally, LLMs\nstruggle with reasoning because their inferences are purely statistical and based on their training data [17].\nThis challenge is very complex for LVLMs because, in addition to processing text input, the model must\nreason about an image containing many visual elements, each of which can play a different role and change\nthe \"direction\" of the reasoning process. In contrast to LLMs that generally only receive textual data to\nmake inferences, an LVLM has to simultaneously process and reason about visual elements in an image,\nunderstand how they relate to each other, and integrate this understanding with any textual input\u2014making"}, {"title": "3.2.7 Task 7: Matching", "content": "Determining whether or not two given graphs match is an important task in graph theory [5]. For this\npurpose, the LVLM must identify the topology of the two graphs and align elements-nodes and edges-to\nmap relationships accurately. This process has applications in a range of fields, including neuroscience [34],\ncomputational biology [8], computer vision [42], and machine learning [26].\nUnlike the previous graph tasks, matching involves analyzing two graphs simultaneously to determine their\nstructural similarity. When nodes with the same labels share the same connection patterns, the graphs are\nsaid to match in this paper. Otherwise, they do not match, even though they might still be isomorphic, for\nexample; see the example on the right-hand side of Figure 9. The core verification steps for graph matching\nby an LVLM should include:\n\u2022 Analyzing the overall structure of both graphs\n\u2022 Identifying and comparing the number of nodes and edges\n\u2022 Verifying if connected nodes are equivalent (by checking their labels)"}, {"title": "3.3 Dataset Configuration and Statistics", "content": "Next, VisGraphVar was used to generate a diverse dataset of graph images whose detailed statistical break-\ndown is shown in Table 1. This dataset comprises a total of 990 images, each measuring 600x600 pixels. The\nimage count varies through tasks and depends on the settings chosen in VisGraphVar, as detailed below.\n\u2022\nTask 1 (Detection) includes 560 images, considerably more than other tasks. This is due to enhanced\nstylistic diversity (node colors, arrow types, layouts, and node labels). All graphs in these images\ncontain 10 nodes, and the number of edges is determined by the probability percentage (2%) of\nadding an edge between any pair of nodes. Remember, that both parameters are configurable in\nVisGraphVar.\n\u2022\nTask 2 (Classification) consists of 70 images, representing seven graph types (tree, planar, mesh,\ncyclic, complete bipartite, and acyclic). The number of nodes and edges depends on the type of\ngraph. In our dataset, the number of nodes does not exceed 10, and the number of edges varies\nbased on the type of graph.\n\u2022\nTask 3 (Segmentation) contains 30 images with varying node counts (10, 20, and 30). The number\nof edges is calculated based on node count, creating a graph where two subgraphs connect via a\nsingle cut-edge.\n\u2022\nTask 4 (Pattern Recognition) has 210 images with three pattern types (chain, clique, and star) in\nvarying quantities (2, 3, and 4 patterns per image). The number of edges is automatically calculated\nbased on node count and desired pattern.\n\u2022\nTask 5 (Link Prediction) includes 30 images, each with a graph containing a unique node count (4,\n5, and 6 nodes). Complete graphs are created, but with one edge removed (the one to be predicted).\n\u2022\nTask 6 (Reasoning) has 30 images, featuring graphs with 5, 6, or 7 nodes. By default, a random\nlayout is used. Similar to Task 1, the number of edges is determined by a probability percentage\n(3%) for adding an edge between any pair of nodes. The edge weights are random values between 1\nand 10, which is also configurable in VisGraphVar.\n\u2022\nTask 7 (Matching) includes 60 images, showing pairs of graphs that match and others that do not\nmatch. The graphs in these images have 4, 5, or 6 nodes. Like in Tasks 1 and 6, edges are added\nbased on a 4% probability of being included between any pair of nodes.\nFor each task, 10 images were generated for each variation regarding the graph's visual style. This allows\nfor an average calculation of each metric (see Section 3.4). For example, in Tasks 3, 5, and 6, where graphs\nare of three different sizes regarding the number of nodes, there are a total of 30 images, corresponding to\nthe three unique visual variations.\nAs managing Task 1 well is a prerequisite for all other tasks, we decided to test various visual styles in this\ncontext, creating a dataset designed to pose a challenge for any LVLM."}, {"title": "3.4 Metrics", "content": "As described in the previous section, each visual combination is evaluated using a set of 10 images. We\nemploy three different metrics, which differ by task, so not all tasks share the same evaluation criteria. Each\nmetric is normalized on a scale from 0 to 1, where 1 indicates optimal performance (complete alignment with\nthe ground truth), and 0 indicates the lowest performance (no alignment with the ground truth). Further\ndetails are provided below.\nMean absolute error (MEA). In Task 1, we used the MAE because we are interested in knowing the\ndegree of error in predicting the number of nodes and edges that the model infers from the image.\n$MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$\n$NMAE = \\min\\left(1, \\frac{MAE}{Range'}\\right)$, where\n$Range' = \\begin{cases} \\max(y_i) - \\min(y_i) & \\text{if } \\max(y_i) \\neq \\min(y_i) \\\\ 1 & \\text{if } \\max(y_i) = \\min(y_i) \\end{cases}$\nHereby, $y_i$ is the actual number of elements (either nodes or edges) that appear in the image, while $\\hat{y}_i$ is\nthe value predicted by the model. Moreover, NMAE is the normalization of MAE-using Range to ensure\nthat predictions closer to the true values approach a value of 1, while those further away approach 0.\nAccuracy. Following Zou et al. [64], we assess model responses in tasks 2-5, and 7 using an accuracy\nmetric, but we extended it to account for both exact and partial matches. The accuracy is calculated as:\n$Accuracy = \\frac{1}{n} \\sum_{i=1}^n m_i$, where\n$m_i = \\begin{cases} 1 & \\text{if } y_i = \\hat{y}_i \\\\ 0.5 & \\text{if } y_i \\text{ partially matches } \\hat{y}_i \\\\ 0 & \\text{otherwise} \\end{cases}$"}, {"title": "3.5 Prompt design", "content": "Like all multimodal models, LVLMs require both an image and a text prompt to operate. We will be\ntesting two different prompting approaches: (1) zero-shot, where the model makes direct predictions without\nexamples) [21]; and (2) chain-of-thought, where the model explains its reasoning step by step [52]. Vatsal\nand Dubey [48] identified up to 39 prompting strategies available for natural language processing tasks;\nhowever, many of these are derivatives of the zero-shot and chain-of-thought approaches, which justifies our\nselection. Thus, this approach requires two distinct prompts for each task, resulting in 14 prompts in total.\nTo streamline this process, we developed a single basic prompt per task and then used an LLM to create\ntwo versions of each one using the zero-shot format and another using the chain-of-thought format. We\nuse these two prompt versions alongside each image to test all considered LVLMs. Moreover, we request all\nmodel responses in JSON format (as specified in the prompts) to facilitate a later analysis. The original\ngeneric prompts (defined by hand) and the LLM-generated versions used for dataset evaluation are included\nin Appendix A."}, {"title": "4 Experiments and Evaluation", "content": "In this section, we evaluate six state-of-the-art LVLMs across seven tasks using the dataset described before\nand generated by VisGraphVar. More specifically, Section 4.1 details our experimental setup, execution\nenvironment, and the rationale behind our LVLM selection. Finally, we present our quantitative analysis in\nSection 4.2, followed by qualitative observations in Section 4.3."}, {"title": "4.1 Environment Setup and LVLM Configuration", "content": "For the evaluation, we used the 990 images from our dataset categorized across seven tasks (see Section 3.3).\nModel evaluation was conducted through the OpenRouter API\u00b9, which facilitates efficient multi-model ex-\necution. We selected LVLMs based on their performance ranking in the Chatbot Arena LLM Leaderboard\n(vision)2, specifically: GPT-40-2024-08-06 [55], Gemini-Pro-1.5 [45], Claude-3.5-Sonnet [44], Llama-3.2-90B-\nVision-Instruct [12], Qwen-2-VL-72B-Instruct [56], and Pixtral-12B [1] as of October 2024. The Chatbot\nArena, developed by researchers at UC Berkeley SkyLab and LMSYS, is an open-source platform for AI\nevaluation through human preference data. Its LVLM evaluation framework has garnered substantial user\nengagement, with over 130,000 votes across 38 different LVLMs. As mentioned above, for each image, we\nemployed two prompting strategies: zero-shot (0-shot) and chain-of-thought (CoT). This resulted in 1980\nevaluations per model (990 images per 2 prompting strategies), totaling 11,880 evaluations across all six\nmodels. For all models, we used OpenRouter's default parameter settings (e.g., temperature) without any\nmodifications or tuning."}, {"title": "4.2 Results", "content": "In this section, we present a detailed comparative analysis of the obtained results. These results are shown\nregarding the utilized metrics (see Section 3.4). However, note that, for the sake of a better understanding,\nmetric scores are shown in terms of percentages. Figures 12 and 13 provide summarized results to easily\nidentify the leading models across all tasks. Additionally, Figure 14 shows the impact of prompt strategies\non the results."}, {"title": "4.2.1 Task-Specific Performance Analysis", "content": "In Figure 10, we observe that Claude-3.5-Sonnet, Gemini-Pro-1.5, and GPT-40 perform similarly across most\ntasks (70%-80% overall accuracy rate). However, significant differences are observed in Task 7 and Task 2,\nwhere Claude-3.5-Sonnet and GPT-40 outperform Gemini-Pro-1.5. In contrast, Gemini-Pro-1.5 and GPT-40\nexhibit a slightly lower performance in Task 1 and Task 3 when compared to Claude-3.5-Sonnet. Notably,\nGemini-Pro-1.5 clearly outperforms all other models in Task 3 and shows a slight advantage in Task 1. These\nresults highlight the general advantage of proprietary models over open-weight alternatives.\nAn interesting observation is that all models show a rather high performance on Task 4 when compared to\ntheir performance on other tasks. Moreover, another eye-catching finding is Qwen-2-vl-72B's performance in\nTask 6, where it nearly matches the top three models. Another significant observation concerns Llama3.2-\n90B, which, despite having substantially more parameters than Qwen-2-VL-72B and especially Pixtral-12B,\nexhibits markedly lower performance than both models and ranks below all other tested models. This\noutcome confirms the suspicion that simply increasing the number of model parameters does not necessarily\nlead to improved performance in visual analysis tasks; in fact, as noted by McKenzie et al. [33], inverse\nscaling may occur due to flaws in the training objective and data."}, {"title": "4.2.2 Performance Distribution Analysis by Task", "content": "Figure 11 employs violin plots to visualize the performance distribution patterns across our evaluated models\nfor all seven tasks. The visualization combines two key elements: individual points representing each model's\naverage performance score, and varying plot widths that indicate the density of scores at each performance\nlevel. This dual representation enables comprehensive analysis of both individual model performance and\nthe overall distribution patterns across different tasks.\nThe green violin plots for Tasks 1, 5, 6, and 7 exhibit a narrow and condensed shape, with an approximate\nscore distribution height of ~ 20% on the y-axis. This indicates that the six models performed consistently\nand with a lower variation on these tasks. The concentrated distribution suggests that the models' responses"}, {"title": "4.2.3 Aggregate Performance Evaluation", "content": "In Figure 12, it is shown that multimodal models like Claude-3.5-Sonnet, Gemini-Pro-1.5, and GPT-40\nexhibit similar performance across all tasks, but with a slight advantage of Claude-3.5-Sonnet. Moreover,\nthere is a significant performance gap of 30.47% between the top model, Claude-3.5-Sonnet, and the model\nwith the weakest performance, Llama3.2-90B. This trend aligns with the fact that all three top models are\nproprietary models, further confirming that, at present, these models outperform open-weight models in\nvisual tasks. This confirms the findings of [14], which demonstrated that there is currently no way for an\nopen-weight model to match the performance of a proprietary model without improvements to its underlying\nbase language model.\nFigure 13 shows that Claude-3.5-Sonnet exhibits a mixed performance across tasks. It excels in Task 4 but\nshows a relatively lower performance in Tasks 1 and 6. Only for Tasks 4 and 5 an average accuracy of over\n80% performance is obtained. These results indicate that, except for Task 4, future LVLMs have significant\nroom for improvement, in particular for what concerns detection (Task 1), Reasoning (Task 6), and Matching\n(Task 7)."}, {"title": "4.2.4 Prompting Strategy Impact Analysis", "content": "In Figure 14, we observe that different prompting strategies, in general, only slightly affect model perfor-\nmance. Llama3.2-90B benefits from CoT prompting across most tasks, with two notable exceptions: in Task\n2, where 0-shot prompting performs better, and in Task 3, where both strategies yield comparable results.\nClaude-3.5-Sonnet, Gemini-Pro-1.5, and GPT-40 demonstrate consistent performance across all tasks, show-\ning minimal variation between different prompting strategies. Similarly, Pixtral-12B and Qwen-2-VL-72B\ngenerally exhibit little variation between prompting methods. However, notable exceptions emerge in the\nanalysis: Pixtral-12B demonstrates enhanced performance with CoT prompting compared to 0-shot ap-\nproaches, achieving a significant 16% improvement in Task 5. Conversely, in Task 3, we observe a 10%\nperformance advantage when using 0-shot over CoT prompting.\nOverall, and somewhat surprisingly, we do not observe any single prompt strategy consistently outperforming\nthe other. Since 0-shot prompts are easier to create than CoT prompts, we recommend beginning with 0-shot\ntesting; if the results are unsatisfactory, then CoT prompts can be attempted. Other benchmarks in the field\nsupport these findings regarding prompting strategies. VisionGraph [27] similarly fails to demonstrate clear\nsuperiority of either prompting approach. GITA [53] takes a different approach, focusing solely on 0-shot\nprompting and fine-tuned LLMs, while omitting CoT evaluation entirely."}, {"title": "4.3 Observations", "content": "This section analyzes three cases that clarify key aspects of LVLM behavior on our dataset created with\nVisGraphVar. We first examine Figures 15 and 16, which reveals crucial insights about spectral layout\ninterpretation in Task 1 (Detection). We then explore Figure 17, highlighting distinctive characteristics in\nPixtral-12B's performance on Task 7 (Matching). Finally, Figure 18 demonstrates the influence of node\nlabeling on model performance and accuracy."}, {"title": "4.3.1 The Striking Case of Spectral Layout", "content": "The bar plot in Figure 15 shows the average score of the best-performing LVLM (Claude-3.5-Sonnet) on\nthe images of Task 1 (Detection). The scores are separately shown for graphs displayed in different layouts.\nInterestingly, we observe a substantial decline in performance for the spectral layout (48,82%) compared to\nother layouts."}, {"title": "4.3.3 The Impact of Node Labels on Model Performance", "content": "In the following, we present an intriguing observation that affects all models. In particular, this observation\nconcerns possible performance differences the models show for images with node-labeled graphs and images\nwith unlabeled graphs. Figure 18 shows that in Task 1 (Detection), images of graphs with labeled nodes\ngenerally result in higher model performance than images with unlabeled graphs.\nTo explain this, consider the case of GPT-40, which demonstrates the largest performance gap of ~ 9%. In\nFigure 19, the graphs in (a) and (b) both have 10 nodes and 16 edges. However, GPT-4o's responses produce\nthe following discrepancies:"}, {"title": "5 Discussion and Open Questions", "content": "The multimodal capabilities of LLMs have greatly expanded their potential, with LVLMs standing out\nas a prime example. With just an API and at a low cost, one can now obtain detailed image analyses,\nwhether for object detection or generating comprehensive image captions. However, tasks that involve vision\nand inference remain inherently difficult and have a long history in computer vision. To properly evaluate\nemerging LVLMs, they must be tested against visually complex tasks. This is where flexible geometric\nstructures like graphs become crucial. Graphs can introduce significant complexity by simply altering the\nspatial arrangement of nodes and edges, or modifying their visual representation.\nOur study introduced VisGraphVar, a benchmark generator designed to challenge current LVLMs and to\nserve as a testing platform for future models focused on visual graph inference. Below, we present open\nquestions from our study that warrant further exploration:\n1. Which visual style changes truly impair a model's prediction? Through the dataset of\ngraph images produced with VisGraphVar, we provided evidence showing that visual changes af-\nfect inference performance, for example, by adding node labels or modifying the layout. However,"}, {}]}