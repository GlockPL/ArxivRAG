{"title": "DRL-Based RAT Selection in a Hybrid Vehicular Communication Network", "authors": ["Badreddine Yacine YACHEUR", "Toufik AHMED", "Mohamed MOSBAH"], "abstract": "Cooperative intelligent transport systems rely on a set of Vehicle-to-Everything (V2X) applications to enhance road safety. Emerging new V2X applications like Advanced Driver Assistance Systems (ADASs) and Connected Autonomous Driving (CAD) applications depend on a significant amount of shared data and require high reliability, low end-to-end (E2E) latency, and high throughput. However, present V2X communication technologies such as ITS-G5 and C-V2X (Cellular V2X) cannot satisfy these requirements alone. In this paper, we propose an intelligent, scalable hybrid vehicular communication architecture that leverages the performance of multiple Radio Access Technologies (RATs) to meet the needs of these applications. Then, we propose a communication mode selection algorithm based on Deep Reinforcement Learning (DRL) to maximize the network's reliability while limiting resource consumption. Finally, we assess our work using the platooning scenario that requires high reliability. Numerical results reveal that the hybrid vehicular communication architecture has the potential to enhance the packet reception rate (PRR) by up to 30% compared to both the static RAT selection strategy and the multi-criteria decision-making (MCDM) selection algorithm. Additionally, it improves the efficiency of the redundant communication mode by 20% regarding resource consumption.", "sections": [{"title": "I. INTRODUCTION", "content": "Vehicle-to-everything applications are classified into three categories according to their requirements. Day 1 V2X applications are for anticipatory driving. Day 2 V2X applications aim to develop collaborative perception and awareness, and Day 3 V2X applications tend to reach fully automated accident-free driving. New generation technologies such as IEEE 802.11bd, 3GPP C-V2X, and 5G NR are being developed to meet the requirements of these last two categories. However, none of them could meet the required QoS [1]. In vehicular networks, the observed performance of a technology typically changes dynamically over time due to frequently changing channel states arising from high node mobility. Moreover, each technology has its weaknesses and strengths, even with ITS-G5 and LTE-V2X PC5 technologies with similar functionalities. For example, in [2], a study shows that LTE-V2X PC5 achieves higher reliability at considerable communication distances, and ITS-G5 is better at close communication distances. Studies from paper [3] show that the improvement gain of LTE-V2X PC5 over 802.11p is minor in the case of a short platoon (e.g., up to five tracks). Nevertheless, LTE-V2X PC5 is more performant with long platoons due to a better link budget. So, getting the required performance to lunch Day 2 and Day 3 applications with only one technology is challenging.\nThis paper proposes a hybrid vehicular network architecture that emphasizes using ITS-G5 and LTE-V2X technologies simultaneously. RAT selection is made with a DRL-based algorithm that considers the channel quality and application requirements to determine the optimal communication mode. Furthermore, we deal with a high mobility communication nature in VANETs where the broadcast success is difficult to estimate, and the link quality is variable. So, based only on local observations, the feedback of the Reinforcement Learning (RL) environment is hard to define. Thus, to get a better estimate of the feedback of each agent, we use the multi-channel communication aspect of ITS-G5 to get the reception acknowledgment [4]. To get seamless connectivity and transparency aspects of the architecture, we follow the same architecture pattern as [1] by adding a hybrid communication management layer to the ITS-station architecture. The remaining of this paper is structured as follows; in Section II, we debrief some related works on hybrid vehicular networks and the use of RL in decision problems. Section III discusses the proposed hybrid vehicular architecture and the DRL-based communication mode selection algorithm. Later in Section IV, we present the simulation setup and discuss the performance results. Finally, Section V concludes the paper and highlights future work."}, {"title": "II. RELATED WORKS", "content": "Works on hybrid vehicular networks are often addressed to enable applications that are part of the Day 3 V2X applications. Paper [1] introduced a protocol stack with a new layer, the Hybrid Communications Management (HCM) layer. The HCM determines a dynamic mapping between the application requirements and the communication technologies. They also investigated single-technology and multi-technology networks to increase the reliability or the network's throughput. However, no concrete evaluation was realized regarding multi-technology selection. In the paper [5], another hybrid vehicular network architecture approach was proposed. It combines Dedicated Short-Range Communication (DSRC) technology-enabled ad-"}, {"title": "III. HYBRID VEHICULAR COMMUNICATION ARCHITECTURE AND THE DRL-BASED RAT SELECTION", "content": "This section presents the hybrid vehicular communication architecture and its RAT selection strategy. We adopt a flat hybrid architecture [10] where each vehicle is equipped with ITS-G2 and C-V2X. Hybrid communication is done using three communication modes. The first is a single communication mode, where only one communication link is used to transmit the message, as is usually done in a legacy vehicular communication network. The second communication mode is a redundant mode where a message is duplicated to achieve higher reliability [1]. Where the third communication mode is the division mode. The message is sent through two RATs as two independent transmissions to enhance the transmission throughput [1]. Furthermore, our hybrid vehicular communication architecture is scalable to other RAT standards, as depicted in Fig. 1, such as IEEE 802.11bd, LTE-V2X UU, and 5G NR. Each technology has a dedicated Radio Resource Management (RRM) entity in the management layer to monitor information about the channel quality. The Access Layer monitors the communication interface and provides information on the network state to the RRM, such as frame transmission statistics, Signal to Noise plus Interference Ratio (SNIR), and other channel load indicators. The hybrid communication layer encompasses all control operations, including communication mode selection, communication technology configuration, and message processing before and after the transmission and reception of a packet. With this layer, hybrid communication is transparent to the facilities layer.\nWe use Deep Reinforcement Learning (DRL) to determine the appropriate communication mode based on network quality and application needs. The objective of RL is to teach the vehicle how to deal with the stochastic nature of the vehicular communication network in order to fulfill the application's requirements. Markov Decision Process (MDP) is generally used as a formal means for RL. However, we cannot solve our problem using an MDP because the communication mode selection problem does not have a finite number of states. To this end, we are using DRL. In our DRL depiction, each vehicle is considered an agent. The environment state in the state space S is composed of six elements:\n$S_t = [SNIR_{ITS-G}, SNIR_{LTE}, PRR_{ITS-G}, PRR_{LTE}, L, R]$\nWhere $SNIR_{ITS-G}$ and $SNIR_{LTE}$ are the measured SNIR of each RAT network. $PRR_{ITS-G5}$ and $PRR_{LTE}$ are the calculated packet reception ratio of each RAT. L and R represent the service's latency and reliability requirements, respectively. These parameters cannot be obtained before sending a message via the transmitter [1]. The only way to get the value of these parameters is to get them at the reception of a message. So, we consider three different environment states to have accurate state representations. $S_t$ as the actual environment state, $s'_t$ as the state of the environment after the vehicle receives the first message from another vehicle in the environment. And the next state is the state of the environment after the vehicle receives the latest message before it sends a new one $S_{t+1}$."}, {"title": null, "content": "Using these three representations, an agent chooses an action based on $s_t$. It receives an evaluation of its action based on the earliest link quality measurement, which is the one measured at $s'_t$. And it updates the current state with the values of $S_{t+1}$. The relation between each state is depicted in Fig. 2. Our discrete action space is given as follows:\n$A = \\begin{cases}\n0, & \\text{Single}_{ITS-G5} \\\\\n1, & \\text{Single}_{LTE} \\\\\n2, & \\text{Hybrid}_{Redandant} \\\\\n3, & \\text{Hybrid}_{Division}\n\\end{cases}$\nThe reward is composed of three parts as follows:\n$R_{vi} = \\begin{cases}\n\\frac{1}{n} \\sum_{j=0}^{n} ReceptionState[V][V_i], i \\neq j \\\\\n+ \\alpha \\\\frac{PS}{2} \\\\\n+ \\beta \\frac{LQ}{2}\n\\end{cases}$\nThe first part concerns the message reception evaluation, which checks if all surrounding vehicles have received the message successfully. This kind of acknowledgment is delivered using the multi-channel aspect of ITS-G5 to avoid congesting the channel with message reception acknowledgments as used in [4]. Every 100 milliseconds, an agent sends a message. And knowing that the delay\u012f (End-to-End latency), represented in Fig. 2, is always less than 100 milliseconds. Each agent can receive an acknowledgment from its surrounding vehicles before sending another message. These acknowledgments are represented as a vector named the reception evaluation vector, which is updated every time a message is received. The vector contains reception reports and a success reception (SR) counter. If a message is received twice by a vehicle, a value of \"two\" is reported, and if it was not received, a value of \"zero\" is reported. Otherwise, the message is considered perfectly received, and a value of \"one\" is reported. The SR counter represents the number of perfectly received messages, which is incremented by one if all reception reports in the vector are equal to \"one.\" The second part of the reward function is about performance satisfaction (PS). It guides the agent in choosing the best communication mode to satisfy the application's requirements. The third part gives feedback to the agent about the link quality (LQ) enhancement. It compares the current state ($s_t$) SNIR values with the earlier next state ($s'_t$) SNIR values.\nDouble deep Q-learning algorithm combines Q-learning and deep learning. It employs a neural network known as the behavior network, which approximates the Q-value Q ($S_t, a_t, w$) using its parameters (\u03c9). These parameters are constantly adjusted to match the optimal strategy \u03c0* by learning from uncorrelated experiences data. The algorithm also uses a second neural network with the same structure and initial parameters w' as the approximation neural network (a.k.a., target network). During the learning process, explained in Algorithm 1, we randomly sample a minibatch of tuples and updates the neural network parameters according to a variant of the stochastic gradient descent (SGD) method, named mini-batch SGD. This mini-batch is picked from a replay buffer (aka., experience replay) that collects and stores < $S_t, A_t, r_t, S_{t+1}$ > tuple in every iteration. The mini-batch is used to update w, thus minimizing the following loss function:\n$Loss(w) = [r_t + \\gamma \\underset{a'}{max} Q(s_t, a', \\omega') \u2013 Q(s_t, a_t, w)]^2 / M$ (1)\nWhere M is the size of the sampled mini-batch and Q(St, at, w') is calculated using the Bellman optimality equation [11] (2) and the target network output.\n$Q_{\\pi^*}(S_t, a_t, \\omega) = E[r_t + \\gamma \\underset{a'}{max} Q_{\\pi^*}(s'_t, a'_t, w) | S_t, A_t]$ (2)"}, {"title": "IV. PERFORMANCE EVALUATION", "content": "To assess our hybrid communication architecture, we simulate the ITS-G5 and LTE-V2X PC5 technologies using the OMNET++ framework, SUMO traffic simulator, Artery Framework [12], and SimuLTE [13]. We compare our DRL-based selection approach with a simple legacy approach (i.e., a single RAT vehicular network, ITS-G5, or LTE-V2X), a static RAT selection strategy, and an MCDM mode selection strategy that leverages TOPSIS, as used in [6].\nA. Simulation scenario\nThe ITS station (ITS-S) architecture supports multiple RATs in the Access layer. However, hybrid vehicular communication is not supported by the OMNET++ simulator. So, to use multiple RATs simultaneously, we implemented the hybrid communication layer using simple OMNET++ modules, as depicted in Fig. 3. Next, to assess our architecture, we lunch a platooning service [14] within a five vehicles platoon. We consider a two-way highway scenario with two lanes in each direction and a length of two kilometers. Two base stations have been deployed as infrastructure to support the LTE-V2X PC5 (mode 3) standard. There are no structures in the vicinity of the highway. The spacing between vehicles in the platoon is 10 meters, and each vehicle follows the Cooperative Adaptive Cruise Control (CACC) car following model as established by the SUMO simulator. The maximum speed limit for platoon vehicles is 10 meters per second and 20 meters per second for all other vehicles. Each vehicle is equipped with ITS-G5 and LTE-V2X PC5, as described in Section III. All parameters are summarized in TABLE I.\nB. Double deep Q-learning algorithm parameters\nThe OMNET++ simulator is implemented using C++. So, to make the interaction between the DRL model and the simulation of the vehicular communications, we implemented the Double deep Q-learning algorithm using the libtorch library. Libtorch is the C++ interpretation of the well-known python machine learning framework Pytorch. The behavior and the target networks are composed of four layers: an input layer with six neurons, two hidden layers with 256 neurons, and an output layer with three neurons. The activation function in each layer is a ReLu function. The ADAM optimizer and the MSE loss function update the neural network parameters. The learning rate is set to 0.0005, y is set to 0.99, and the epsilon decay is set to 10-5. The maximum size of the replay buffer is set to 106, and the mini-batch size is set to 64. In our experiments, a game ends when the SR counter reaches the target of 100 received messages, not to have long games and to give the agent the time to learn. In a game, one step is equivalent to sending a V2V message. The PRR is calculated using equation (3). Where the $N_{sent messages}$ is the number of sent messages to reach 100.\n$PRR_{game} = \\frac{SR_{target}}{N_{sent messages}}$ (3)\nC. Simulation results\nWe employ three performance metrics to evaluate the performance of our hybrid vehicular communication network and to assess our work. The average PRR, the percentage of duplicate messages when redundant mode is enabled. Moreover, to evaluate the convergence performance of our proposed selection algorithm, we present the average expected reward per game to show the convergence of the algorithm. First, we present the evaluation of our DRL-based communication mode selection algorithm. Fig. 4 represents the agent behavior in a medium-congested network. Fig. 4 (a) shows that the reward converges at game number 600. This convergence is followed by an excellent reliability improvement, as depicted in Fig. 4 (b). We notice that in the first 400 games, the reliability of the transmission goes from 59% to nearly 80%. Then it reaches 95% by the end of the simulation. We noticed in Fig. 5 that reward convergence takes more than 800 games in a highly congested network. Experiments with various congestion levels make us realize the need to employ a hybrid communication mode. As illustrated in Fig. 6, when on a highly congested highway, the redundant communication mode is selected more frequently than on a less congested highway. This frequency goes from 3% to 20% with more congestion. Fig. 7 shows a comparison study between four RAT selection strategies. We can see that hybrid communication with a static RAT selection enhances the PRR in both traffic flow densities compared to the baseline performances of ITS-G5 and LTE-V2X."}, {"title": "V. CONCLUSION", "content": "This paper implemented and assessed a hybrid vehicular communication architecture with a DRL-based communication mode selection strategy to satisfy recent Day 3 applications' requirements. Performance findings demonstrate that the DRL-based communication mode selection technique improves reliability near 98% while reducing the proportion of duplicate messages to preserve radio resources. We did our best to mimic the network channel access operations of ITS-G5 and LTE-V2X. In future work, we will assess the performance of this selection algorithm using real equipment and explore the coexistence between ITS-G5 and C-V2X in the 5.9 GHz band."}]}