{"title": "Test-driven Software Experimentation with LASSO: an LLM Benchmarking Example", "authors": ["Marcus Kessel"], "abstract": "Empirical software engineering faces a critical gap: the lack of standardized tools for rapid development and execution of Test-Driven Software Experiments (TDSEs) that is, experiments that involve the execution of software subjects and the observation and analysis of their \"de facto\" run-time behavior. In this paper we present a general-purpose analysis platform called LASSO that provides a minimal set of domain-specific languages and data structures to conduct TDSEs. By empowering users with an executable scripting language to design and execute TDSES, LASSO enables efficient evaluation of run-time semantics and execution characteristics in addition to statically determined properties. We present an example TDSE that demonstrates the practical benefits of LASSO's scripting capabilities for assessing the reliability of LLMs for code generation by means of a self-contained, reusable and extensible study script. The LASSO platform is freely available at: https://softwareobservatorium.github.io/, and a demo video is available on YouTube: https://youtu.be/tzY9oNTWXzw.", "sections": [{"title": "I. INTRODUCTION", "content": "Test-Driven Software Experiments (TDSEs) are experiments that involve controlled testing of software subjects (i.e., code modules) under various conditions, revealing important prop-erties of the code's run-time behavior that cannot be predicted solely through static analysis, because of Rice's theorem [1]. TDSEs are a widely used and effective methodology in soft-ware engineering. Researchers conduct TDSEs to empirically validate tools and techniques that involve the execution of software subjects, such as benchmarking tools and techniques (e.g., test generation and program synthesis). Practitioners use TDSEs to evaluate tools and code, making informed decisions about their adoption and integration (e.g., code recommenda-tion [2]). And educators utilize TDSEs to support teaching activities, for example by offering test-driven exercises in programming courses with rapid feedback.\nDespite the benefits of TDSEs, designing, executing and evaluating them remains a labor-intensive and ad-hoc process that presents significant technical challenges. This is due to the inherent intricacy of setting up TDSEs, including making software subjects testable, specifying reusable tests for them, and collecting run-time observations in a controlled manner so that the behavior of code modules can be compared.\nEmpirical software engineering research, therefore, cur-rently lacks specialized tools to support the rapid development of executable study designs that are self-contained, reusable, interoperable and extensible. However, current approaches often require substantial manual effort, limiting their scalability and reusability, and especially their reproducibility.\nTo overcome the technical complexities of designing and executing TDSEs in practice, we developed LASSO, a Large-Scale Software Observatorium [1], [3]. Unlike traditional ad-hoc approaches that rely on general-purpose languages and custom scripting solutions, leading to significant manual ef-forts, LASSO provides a unified platform for creating auto-mated, reproducible TDSEs at scale. By offering a streamlined set of domain-specific languages (DSLs) and data structures tailored specifically for building executable study pipelines, LASSO empowers users to quickly develop complex work-flows.\nWith LASSO's scripting language, LSL, users can effi-ciently evaluate critical software properties, including dynamic properties such as run-time semantics, and static properties derived from traditional analysis approaches like size-based metrics.\nLASSO's executable study designs have been successfully employed to achieve two primary objectives: (1) providing a set of analysis services built on top of the platform, including test generation [4] and test-driven code recommendation [2], and (2) conducting thorough TDSEs, including the assessment of behavior sampling methods [2], [5], and the replication of MultiPL-E's HumanEval benchmark on code LLMs [1].\nBuilding on the insights gained from the replicated bench-marking study, this paper highlights the practical benefits of the LASSO platform and its unified scripting language by demonstrating their application in a TDSE focused on evaluating the reliability of code LLMs for code generation tasks. Specifically, we showcase how the TDSE's study design is seamlessly translated into an executable LSL script that captures all essential steps and parameters, providing a clear and reproducible example of the platform's capabilities.\nThe outline of this paper is as follows. In Section II, we present a concrete TDSE scenario using an example study script written in LSL, which serves as the foundation for exploring key features and data structures within the LASSO platform. In Section III, we delve into various extension points of the study script, highlighting opportunities for customiza-tion and extension to illustrate how users can adapt and refine their studies to meet specific needs. Finally, in Section IV, we summarize our findings with some concluding remarks."}, {"title": "II. EXAMPLE TDSE \u2013 ASSESSING CODE LLMS", "content": "In this section we provide an overview of the LASSO platform and its key features from the perspective of a po-tential user who wants to use LASSO's scripting language to create and conduct a TDSE. At its core, LASSO provides a scalable workflow engine that can be driven via LSL scripts. Based on the Groovy language, LSL is an efficient impera-tive/declarative domain-specific language with a minimal set of DSL commands inspired by languages for build management, continuous integration and data mining. LSL empowers users to define modular study pipelines that encompass all necessary analysis steps in TDSEs. An in-depth discussion of LASSO's concepts, languages and data structures is provided in [1], [3].\nA. Overview of Study Design\nSuppose the goal of the user is to conduct a basic TDSE to assess the reliability of 3 code LLMs for the task of code gen-eration (i.e., natural-language-to-code task) using differential testing [6]. In our example, the user wants to sample 5 Java code solutions from each model for a single coding problem here Base64 encoding which is described in natural language in terms of a prompt. Code reliability is evaluated by determining whether the generated code solutions exhibit the desired behavior for the coding problem at hand, similar to the criteria used by existing benchmarks to evaluate LLMs' ability to generate code. To do so, we define representative test inputs to execute the generated code solutions, and oracle values to verify their functional correctness.\nB. Translating the Study Design into LSL\nThe initial step in creating a study pipeline for our scenario is to pinpoint the core analysis steps required to conduct the TDSE. This example has two core steps: (a) prompting the code LLMs to generate code solutions for the given coding problem, and (2) assessing whether these generated coding solutions are functionally correct by employing a set of two tests to observe their run-time behavior.\nThese two steps translate directly into generate and observe action blocks in the LSL script. The two actions are defined within a study block and depend on each other. The first action encompasses the use of 3 code models to generate 5 code solutions, while the second action involves LASSO's special \u201carena\u201d test engine, which observes and stores the run-time behavior exhibited by these code solutions in response to two tests to establish functional correctness at a later stage (i.e., through data-driven analysis in an external data analytics tool).\nLSL study pipelines leverage the concept of actions to represent reusable and composable analysis steps that possess a well-defined life cycle . This structure enables the underlying analysis steps to be simplified and easily nested.\nWhile there is no strict categorization of action types based on granularity, we identify two primary categories: (1) actions related to selecting and incorporating existing code modules, and (2) actions that analyze and observe the behavior of these modules (e.g., executors like the arena test engine). In the former case, this example pipeline directly samples code solutions from the code models and automatically adds them to LASSO's executable corpus. This corpus is a substantial collection of executable code modules which includes code sourced from major repositories such as Maven Central. The executable corpus enables textual as well as test-driven code retrieval to select existing code modules for a variety of other scenarios (e.g., code recommendation [2]).\n1) Defining Run-time Behavior: The prompts for code LLMs often comprise a diverse set of ingredients, includ-ing high-level, informal descriptions of desired functionality, concrete (usage) examples, and the required signature of the interface to be invoked (e.g., Java method signature). In the example scenario, the prompt consists of a natural language description of the coding problem at hand \u2013 Base64 encoding which involves encoding a sequence of characters into the Base64 alphabet. Here, we explicitly request a code module implementation that does not implement the padding feature of \u201cfilling\u201d blocks with '='.\nHigh-level descriptions of desired functionality for coding problems are referred to as functional abstractions in LASSO. In LSL scripts, actions can create and process \u201cabstraction containers\u201d that link functional abstractions to its code module candidates. Like coding problems in existing code LLM benchmarks, functional abstractions are typically described using the required interface signature and a set of tests. Actions can create abstraction containers as their output, which then flow to other actions as their inputs, allowing specific actions to depend on them (e.g., action observe depends on Base64Encode from action generate).\na) Interface Signatures: In LASSO, the interface sig-nature is specified using a concise and expressive language called LQL (LASSO Query Language). This allows for a clear definition of input and output parameter types, making it easy to understand what is expected from each module. To ensure testing compatibility and to facilitate systematic comparison of behavior, tests are written against these defined interface signatures, rather than the actual interface exposed by the generated code modules. The LASSO test engine takes this into account by automatically attempting to identify compat-ible interface mappings (i.e., creating adapters for the code modules), which helps maintain consistency across different module implementations.\nb) Stimulus-Response Data Structures: We utilize three data structures \u2013 sequence sheets, stimulus-response matrices (SRMs), and stimulus-response hypercubes (SRHs) \u2013 to define and represent the run-time behavior of code modules in a"}, {"title": "III. EXTENSIBILITY", "content": "The self-contained nature of LSL scripts, as demonstrated in the previous section, offers several advantages beyond reproducibility. Firstly, their explicit encoding of study designs and assumptions in executable code facilitates seamless re-execution, making them highly reusable and modifiable. The ability to redefine important parameters, such as the 3 code LLMs under study and the number of code solutions to sample, as global variables within the script (ideally located in the header section of the script) allows users to effortlessly modify study designs. The same applies for different sets of parameters used for code LLMs (e.g., temperature parameter).\nSecondly, the flexibility of LSL scripts extends to defining new coding problems in an ad-hoc manner by simply adding more abstraction blocks. Alternatively, as showcased in the replicated study in [1], existing datasets and benchmarks can be leveraged to load pre-defined coding problems.\nThirdly, users can choose to expand the study design by incorporating additional actions within the existing pipeline. This modular structure allows users to facilitate the integration of new methods and to refine their experiments, thereby maximizing the efficiency of their studies. By combining these features, LSL scripts offer a powerful tool that fosters reproducibility, reusability, and modularity in TDSEs.\nResearchers interested in exploring the rate of code clones in samples of code generations, for example, may extend the pipeline by adding an action that detects code clones. Practitioners, on the other hand, may decide to rank code solutions by their degree of functional similarity (i.e., number of passing tests). Finally, users can directly extend the platform by integrating their own custom actions, next to the default set of actions which is provided by the platform (note existing actions are documented in LASSO's dashboard). This is made possible through a well-defined Actions API and the provision of tools via Docker containers, providing a seamless and flexible way to tailor LASSO to specific TDSE scenarios."}, {"title": "IV. CONCLUSION", "content": "This paper has demonstrated the benefits of the LASSO plat-form and the scripting language (LSL) in facilitating the cre-ation of reusable, automated test-driven software experiments (TDSEs) that is, experiments that involve the execution of software subjects. By encoding TDSE study designs into executable scripts written in LSL, users can analyze results, including observational data (i.e., \u201cde facto\u201d behavior in terms of serialized outputs), in a data-driven manner using external analytics tools like Jupyter/Pandas. The example TDSE for as-sessing code LLMs with respect to code generation reliability showcases the versatility of this approach. Further, we have discussed how executable study designs written in LSL can be extended in a variety of ways.\nIn the future, we plan to extend LASSO's test engine to support additional programming languages other than Java, including Python. We believe that the open-source nature of the LASSO platform and its growing community will lead to a more vibrant ecosystem around TDSEs, effectively leading to repositories of TDSEs that are shared amongst users.\nThe platform is implemented in Java using Spring Boot. It can be deployed on a single machine using Java or Docker and provides a dashboard and web service for submitting LSL scripts. Setting up a distributed LASSO cluster across multiple machines requires more advanced configurations."}]}