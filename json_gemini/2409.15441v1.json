{"title": "STEWARD: Natural Language Web Automation", "authors": ["Brian Tang", "Kang G. Shin"], "abstract": "Recently, large language models (LLMs) have demonstrated exceptional capabilities in serving as the foundation for AI assistants. One emerging application of LLMs, navigating through websites and interacting with UI elements across various web pages, remains somewhat underexplored. We introduce Steward, a novel LLM-powered web automation tool designed to serve as a cost-effective, scalable, end-to-end solution for automating web interactions. Traditional browser automation frameworks like Selenium, Puppeteer, and Playwright are not scalable for extensive web interaction tasks, such as studying recommendation algorithms on platforms like YouTube and Twitter. These frameworks require manual coding of interactions, limiting their utility in large-scale or dynamic contexts. Steward addresses these limitations by integrating LLM capabilities with browser automation, allowing for natural language-driven interaction with websites. Steward operates by receiving natural language instructions and reactively planning and executing a sequence of actions on websites, looping until completion, making it a practical tool for developers and researchers to use. It achieves high efficiency, completing actions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average of $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a caching mechanism. It runs tasks on real websites with a 40% completion success rate. We discuss various design and implementation challenges, including state representation, action sequence selection, system responsiveness, detecting task completion, and caching implementation.", "sections": [{"title": "1 INTRODUCTION", "content": "Simulating user navigation and interactions on the web is required for a number of use-cases, including web measure- ment studies, UI testing and debugging, analyzing privacy practices, etc. The state-of-the-art (SOTA) approaches re- quire the use of a browser automation framework like Sele- nium, Puppeteer, or Playwright to manually trace, record, and code interactions with HTML elements. This process is infeasible for conducting large-scale tests with many web- page contexts or multiple websites. Consider the task of studying recommendation algorithm behavior. With such content being dynamically generated and location/context- dependent, relying solely on a browser automation tool to record and playback actions would not scale."}, {"title": "1.1 Steward", "content": "We propose Steward, a fast, reliable, and cost-efficient LLM- powered web automation tool. It is an end-to-end system that takes a natural language instruction as input and performs operations on websites until the end state is detected/reached. Steward can simulate user behaviors on websites and even perform entire tasks to completion on real websites, for exam- ple, adding items to e-commerce site shopping carts, search- ing for and sharing YouTube videos, booking tickets, check- ing flight/lodging status or availability, etc. Using OpenAI's language and vision model APIs, Steward intelligently and reactively performs actions on sites in only 8.52-10.14 sec- onds and at a cost of $0.028 per action. Steward also uses a webpage interaction caching mechanism that reduces run-time to 4.8 seconds and $0.013 per action.\nSteward is also generalizable to handle previously unseen web contexts and perform correct action sequences even after sites update or remove their content. Rather than re- lying on fine-tuning or training on datasets, Steward uses purely zero/few-shot prompting. Thus, it is easily deployable, scalable, and relatively low-cost, allowing for plug-and-play integration with any LLM."}, {"title": "1.2 Technical Challenges", "content": "Determining the correct action-element pair to perform on a website requires careful design of a state representation of the webpage's current context. Selecting a correct action sequence on a site using minimal input/output tokens while maintaining accuracy poses another significant challenge. Finally, implementing a system design that runs in the order of seconds constitutes the last major challenge.\nThe SOTA systems have limitations, such as using propri- etary models, lacking in reliability and scalability, cost more, or are not end-to-end systems. They mainly lack integration with browser automation tools and are impractical for larger intelligent web crawls or real-time operation."}, {"title": "1.3 Our Contributions", "content": "Our work consists of two main thrusts: (1) designing an auto- mated framework for modeling the current website contexts and executing UI actions, and (2) the analysis of Steward's performance with respect to various criteria.\nWe address each of the above technical challenges and make the following main contributions:\n(1) A unique LLM-powered web execution procedure that easily fits into browser automation frameworks. Steward was designed specifically for use with the Playwright framework. It is fully autonomous and only requires the user to type out a high-level goal/task to perform on a website in natural language.\n(2) A context-aware, site/app-agnostic UI exercising system capable of automating web interactions on a large scale. Steward can generalize its knowledge to navigate and interact with a variety of websites. For the top 5 elements, Steward is capable of achieving a top-1 action + element selection accuracy of 81.44% without any training or fine- tuning. It achieves a per-step accuracy of 46.70% on the Mind2Web benchmark and a 40% task completion rate, able to execute roughly 56% of the tasks' actions until encountering an error.\n(3) An in-depth evaluation of Steward's runtime and cost in various configurations. Its system design is optimized for runtime and cost efficiency, achieving a median runtime of 8.52 seconds or 10.14 seconds with text entry at a cost of $0.028 per action. It also includes an implementation of a caching mechanism for storing and reusing website interactions, which reduces the runtime and cost of a step by 43.7% and 53.6%, respectively."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 AI UI Exercising Tools", "content": "Earlier web UI automation tools have sought to use nat- ural language inputs to perform complex user interaction sequences on websites. Earlier approaches were not as gen- eralizable to unseen websites or interaction sequences, ei- ther. For example, some of the earlier works exploring ad- vanced natural language UI exercising that leverage NLP and reinforcement learning resulted in lower success rates on websites and interactions outside of the training set dis- tribution [22, 27]. By using LLMs and achieving a balance between a concise yet rich web page representation, Steward achieves accurate and broad coverage comparable to prior approaches.\nGlider [22], an automated and scalable generation of web automation scripts (tasklets) from a natural language task query and a website URL, uses hierarchical reinforcement learning to navigate the website's UI and maximize rewards based on task progress. It generates tasklets which are se- quences of actions to perform on a particular site.\nFLIN [27] proposes a natural language interface for web navigation that maps user commands to concept-level ac- tions. The authors frame their approach as a ranking problem: scoring the most relevant navigation instruction given a user command and a webpage. By using semantic similarity, ac- tion keywords, and the BERT [13] model, FLIN is able to perform basic high-level tasks."}, {"title": "2.2 Natural Language UI Testing", "content": "Other earlier related works use natural language genera- tion to augment UI testing frameworks by generating labels, comments, test inputs, test cases, and more.\nCrawLabel [25] introduces techniques to compute natural- language labels for end-to-end UI test-cases in web appli- cations by using information from the browser's document object model (DOM).\nThe work of Wanwarang et al. [32] introduces an approach called Link for generating realistic test inputs for mobile apps. It leverages knowledge bases and uses label matching, NLP, and clustering, to cover more statements than randomly- generated text inputs for testing mobile apps.\nKirinuki et al. [21] propose script-free testing in web ap- plication development, using NLP and heuristic search al- gorithms to identify web elements and determine test pro- cedures based on test-cases written in a domain-specific language, identifying the web elements to be operated.\nDeng et al. propose creating a general natural language interface for the web to make the Internet more accessible to users with disabilities. They present their ongoing efforts of curating a benchmark dataset of websites and tasks [11].\nHumanoid [23] is a deep learning-based approach to gen- erating test inputs for mobile apps by learning from human interactions in the RICO [10] dataset, prioritizing inputs based on their perceived importance by users."}, {"title": "2.3 LLM Web Automation Systems", "content": "Since the advent of large language models, various open- source tools and datasets have been created to augment its capabilities. For example, AutoGPT [2] was created as an autonomous AI agent that continually loops until its high-level tasks are achieved. While accessing the Internet via BeautifulSoup for web scraping and information retrieval, it is unable to perform actions on sites. Another popular plu- gin, WebPilot [6], a ChatGPT plugin, grants access to a Bing search API and basic website information retrieval capabili- ties. Unlike these two approaches, Natbot [5] was created as an early-stage prototype exploring web interactions using OpenAI's Davinci model and few-shot prompting.\nSteward is not the first to explore the use of LLM's poten- tial for web automation; several researchers have achieved various levels of success in natural language web automation.\nMind2Web [12] is a dataset recently created by researchers. The dataset is a record of Amazon Mechanical Turk users' actions on a web browser using the Playwright [1] automa- tion framework. It serves as the primary data source for evaluating the language models used in our design and our overall system (Section 3). The authors of [12] train their own language model using a derivative model of BERT.\nGur et al. performed an analysis of LLM HTML understand- ing using various transformers like T5 [30] and LaMDA [9]. They subsequently created WebAgent [17], another HTML- T5 type language model combined with a Flan-U-PaLM [8] program synthesis model trained on CommonCrawl [3] HTML and web interaction data. Their specially trained 540B pa- rameter model achieves the best-known accuracy on the Mind2Web benchmark dataset.\nMultimodalweb agents have also been a direction of inter- est as WebGUM [15] uses T5 [30] as an image and embedding encoder and a decoder that selects actions and elements. An- other more general task automation framework leverages GPT-4 [37] using a combination of language and vision mod- els to perform actions on websites and the user's OS.\nSodhi et al. [31] proposed a contextual Markov decision process (MDP) in which the context is the web task objective expressed as an instruction, implicitly in a conversation, or as a set of structured parameters. The state of the MDP is the DOM of the current webapge, and the action and transition functions are based on clicking and typing to interact with an element (represented as an id and a value string). Their system uses hierarchical prompting to break down complex tasks into smaller policies. They note that there are several limitations, for example, when a dropdown menu does not appear in the DOM.\nHe et al. [18] implement their LLM tool by leveraging GPT- 4-ACT [7], a GPT-4-Vision augmentation that uses set of mark prompting [36], to label and reference webpage elements on a screenshot. Their vast majority (as much as 91%) of errors result from navigation getting stuck, the language model hal- lucinating, or an issue with their visual grounding approach. They are specifically investigating GPT-4-Vision's potential for parsing visual and textual information on webpages to use in website navigation.\nHong et al. [19] create models leveraging OCR and visual grounding with captioning datasets for representing GUIs via text. Their results appear to outperform smaller language models both prompted and fine-tuned for web navigation on the Mind2Web dataset for element selection from the top 10 elements, selecting the ground truth element up to 62.3% of the time."}, {"title": "2.4 LLM Web Automation Limitations and More", "content": "Given their limited performance, most of these autonomous agents are yet to be a practical solution for day-to-day us- age [20]. Performing truncation on the HTML to feed into these language model agents was shown to significantly improve the performance over cases without truncation.\nFurota et al. [14] extensively study the transferability of large multimodal agents(LMAs) to more realistic sequential task compositions. They design a new test bed, CompWoB, with 50 compositional tasks."}, {"title": "2.5 Design Comparison with Related Work", "content": "In the domain of natural language web automation, various methodologies have been employed to enhance web navi- gation accuracy. Many of these prior studies share similar design principles. A language model is given access to some state representation of a website and must select the appro- priate HTML element and action given a natural language task specified by the user. These designs roughly fall into the following categories.\nHTML Element Proposal and Selection: This method- ology uses an element proposal followed by the selection of an appropriate action using a language model [12, 31]. It is simplistic, straightforward, and efficient, but yields over-reliance on the model's performance and fine-tuning dataset.\nPlanning and Program Synthesis: The planning-based approach [17] focuses on generating correct subtasks or sub-instructions to automate a given task. However, this approach requires high accuracy in generating subtasks, i.e., the generator must have rich context/data (raw HTML) to make an informed prediction. It must also be consistent in generating browser automation code.\nMultimodal Web Automation: In contrast to the other approaches, multimodal web automation emphasizes using screenshots of the webpage and identifying UI elements to interact with using semantic segmentation and/or element object detection [18, 19, 37].\nOur Approach: Steward adopts an approach most simi- lar to a combination of the 3 approaches. According to our preliminary investigations, this yielded fewer errors when used with off-the-shelf models. Our approach differs from prior work in its end-to-end system design with a reactive planning-based agent. This means our system plans and makes decisions on the fly after each step. It is built to work with off-the-shelf language/vision models with just zero or few-shot prompting while operating as quickly and cost-efficiently as possible, without sacrificing reliability. It does this by filtering and cleaning the set of HTML el- ements to minimize the amount of noise and tokens the language models must process. Our system also integrates an action caching system to further improve runtime and cost-efficiency, reducing the already low 8 seconds per ac- tion step to just 4 seconds per step. Finally, our system also accounts for task completion by detecting the end state and terminating the program. We will discuss the advantages of our system and design philosophy in the following section."}, {"title": "3 DESIGN, IMPLEMENTATION, AND COMPARISON", "content": "We now detail Steward's design and implementation. Steward consists of 3 main components: (1) a large language model (LLM) and prompting framework that can handle webpage state representation and navigation, (2) an HTML cleaner, a runtime/cost-optimized execution pipeline, an action caching system, and (3) an integration with the Playwright browser automation tool.\nThe design of Steward is inspired by how humans per- ceive, process, and interact with websites. The system is built to automate tasks that users (i.e., LLM users, website develop- ers, and researchers) may want to conduct on websites. First, the system analyzes a website, providing a short high-level description of the page. In parallel with this analysis, the sys- tem considers its user-provided goal and a screenshot of the page to determine the next course of action. This contextual information is stored in a state representation that is used in every prompt and LLM query. It looks at the interactable ele- ments from the DOM (in HTML) to select an element for the Playwright browser automation tool to interact with. Finally, the system records the actions taken and memorizes these prior action sequences when considering the next element to interact with. In addition, it caches previously seen contexts and action sequences to avoid repetitive calls to the LLMs."}, {"title": "3.1 Example: Adding Items to Shopping Cart", "content": "We take the following successfully executed example ob- served in our evaluations with Steward. For a website like cabelas.com, if Steward's current user's goal is to \"Add a dome tent to my shopping cart\", and Steward has already per- formed the actions of clicking on \u201cCamping\u201d, clicking \u201cClose\u201d on a promotional popup window, and clicking \u201cDome Tents\u201d, the system will \u201cclick on the first dome tent product displayed\u201d. This is because the list of prior actions, the screenshot of the web page, and the generated page context have all been up- dated with the change in the web page's state. Steward will proceed to filter the set of interactable web page elements down to a list of 15 elements and then select the best match:\n\u201cCLICK\" \nAfter executing this command, the web page will update to this 4-person dome tent's product page. Steward's internal state representation of the website will also update, causing it to select the best matching element after filtering again:\n\u201cCLICK\u201d \nConstructing Website State: After a web page initially loads, or after an action is selected and performed on that website, Steward first \u201cperceives\" the web page by taking a screenshot of the page. A vision transformer is prompted to identify the next best action to perform to achieve the user's task, and the screenshot image is input along with relevant state information (website URL and prior actions performed), e.g., SCREENSHOT RESPONSE: \u201cclick the \u201cCamping\u201d cate- gory on the navigation bar\u201d.\nIn parallel, the page's plaintext is retrieved from the HTML, and a language model is prompted to return a brief high-level summary of the current page context, e.g., PAGE CONTEXT: \"Ecommerce website page for Cabela's featuring a variety of tents for camping and outdoor activities, with filtering options by brand, type, size, and sleeping capacity.\"\""}, {"title": "3.2 HTML Processing", "content": "The first major challenge with LLM web automation is en- abling LLMs to parse large HTML representations of web- sites. These HTML snippets must fit within the context lengths of the language models while avoiding overcrowding the input. To minimize the amount of irrelevant information contained in HTML tags, we utilize a three-step approach to filtering and cleaning the list of HTML elements on a page.\nFiltering for Interactable Elements: First, Steward fil- ters the set of elements to interact with using CSS selectors. We are only concerned with interactable elements, which primarily include buttons, links, tabs, text fields, select op- tions, and other related interactable UI elements. Certain at- tributes also help in identifying these elements, e.g., role=tab, onclick, aria-label, etc. This step will typically reduce the set of HTML elements on a page by an order of magnitude (e.g., from 4564 elements down to 371 elements on cabelas.com).\nAppendix A.2 contains the exhaustive list of CSS selectors.\nElement String Matching: The next step involves lim- iting the HTML element search space by selecting only el- ements with strings relevant to the website's current state. For example, if the screenshot response returns \u201cclick the \"Camping\" category on the navigation bar\u201d, then:\n[\u201cexplore\u201d, \u201ccamping\u201d, \u201coutdoors\u201d, \u201cnavigation\u201d, \u201cmenu\u201d]\nwill be the set of strings to search through the element list. Or. if the response is \u201cclick \u201cTents & Shelters\u201d under the \u201cCamping\u201d category"}, {"title": "3.3 Natural Language Component", "content": "Using a combination of HTML parsing and LLM prompting, we can construct a representation of the current page/s state. This state consists of a high-level user goal, the website's base URL and current page context, a screenshot of the current page, a list of prior actions, a proposed candidate action, and a list of candidate HTML elements to interact with. These states are formatted as variables within prompts, and the prompts used by each component do not necessarily include all state variables. Thus, changes in the state of a web page can be measured with this state representation. For prompts that exceed a language model's context window, the system employs batching of the prompt variables. The following components use LLMs process the webpage state and serve as the execution flow required to perform an action on a site:\n(1) Summarize the current webpage's context.\n(2) Process the page screenshot and suggest a candidate action to perform.\n(3) Propose the top 15 elements to interact with to achieve the user's goal.\n(4) Select the next best action and element combination to perform from the proposed top-15 elements.\n(5) Determine the text to type in or the option(s) to select.\n(6) Determine whether the selected candidate action and element makes sense to perform.\n(7) Determine whether the current state has achieved the user's goal, and thus the program should terminate."}, {"title": "3.4 Caching Action Sequences", "content": "Steward also supports caching previously performed actions to reduce runtime and costs. The cache is first keyed by the website URL. Subsequent key-value pairs map high-level descriptions of actions to the corresponding action verb and HTML element. A language model and prompt are used to determine the semantic similarity between the stored action description (key) and the new action description generated by the screenshot response of the current page. This step is done such that the only LLM query required for each previously executed action is the screenshot response.\nCache Indexing: The cache is implemented as a dictio- nary that uses the stripped base URLs and the screenshot response containing the action descriptions as keys. The se- lected element and action/verb performed on Playwright are stored as the value to retrieve. Finally, the cache write/read timestamps are stored as metadata for the cache replacement policy. When storing new cached actions, another LLM com- ponent is used to ensure the action description matches the selected verb and element.\nCache Replacement Policy: The cache has a maximum number of action description keys of 100 to reduce issues resulting from language models having large input sizes. This is done to keep response times low while maintaining consistency and reducing overlap with the next action key, as LLMs are known to perform worse on reasoning benchmarks with longer inputs. The cache supports either LRU or LFU for its replacement policy."}, {"title": "4 IMPLEMENTATION", "content": "Steward was implemented with 1626 lines of code in Python and 20 prompts. Its main functionalities rely on OpenAI's API, Playwright, BeautifulSoup4, lxml, zxcvbn, NLTK's cor- pus of words, and NopeCHA's API. Depending on the compo- nent, the system utilizes 4 different models: GPT-3.5-Turbo, GPT-3.5-Turbo-16k, GPT-4-Turbo, and GPT-4-Vision. The system makes use primarily of Playwright's Context, Page, and Locator classes and their respective functions. Table 1 contains a list of verbs used in the LLM prompts and their mapping to Playwright functions.\nBrowser Automation. As Steward needs to automat- ically perform tasks on websites, it uses the Playwright browser automation framework to interface with websites. Playwright locators retrieve all visible interactable ele- ments (e.g., , , , , [class*=\u201cui- slider\u201d], etc.), and extracts the outer HTML of each element. After the elements and interactions are chosen by the NLP component, Steward automatically performs the actions us- ing Playwright selectors and locators and the click, type, select, goto, upload file, and enter functions. As these actions are implemented, each interaction and state is recorded. A screenshot is taken, and, optionally, a video recording begins to capture the browsing session. Finally, the browser state is stored. This includes data, such as network traffic, browser cookies, and local browser cache storage. The cached ac- tions and elements performed in prior runs are retrieved and stored in a JSON file for simplicity.\nBypassing Bot Detection. To perform activities on be- half of the user, developer, or researcher, Steward must by- pass bot detection techniques, such as Cloudflare. These"}, {"title": "5 EVALUATION OF STEWARD", "content": "We evaluate Steward's efficacy, and describe the dataset de- tails and model configurations in Section 5.1. The setup for each of evaluation criteria accuracy, runtime, and cost - is discussed in Section 5.2. Accuracy is measured using compo- nent per-step accuracy, the system's end-to-end correctness, and the system's performance running tasks on live websites. Our evaluation results are provided in Section 6."}, {"title": "5.1 Dataset Configuration", "content": "To evaluate Steward's efficacy as a web automation tool, we use the Mind2Web dataset [12], which consists of 2,350 natural language tasks to perform on real websites and over 10,000 recorded actions. This dataset was generated using Playwright, and includes the actions selected, elements se- lected, the raw HTML, and other metadata. Our system is built to run with off-the-shelf models and thus has not been fine-tuned a priori on any datasets. We use the Mind2Web test set solely to evaluate Steward's performance on various tasks. In our experiments, we randomly sampled a subset totalling 122 tasks and 621 actions from the (test_domain, test_site, test_task) test sets.\nLanguage Models Used. We utilize and report results with 3 different base models: GPT-3.5-Turbo, GPT-4-Turbo, and GPT-4-Vision. Table 6 in the Appendix provides more details on the components and their corresponding models."}, {"title": "5.2 Evaluation Methodology", "content": "Overall Evaluation (Correctness, Runtime, Cost): We evaluated each component in an end-to-end fashion on sub- sets of the Mind2Web test dataset which consists of 122 tasks and 621 actions. To ensure the correctness of the web au- tomation sequences, we mapped the actions directly with the ground-truth actions and elements provided in the Mind2Web dataset. One limitation of this dataset is that it is annotated with only one correct ground-truth action sequence per task, an incorrect assumption. In reality, completing a task on a website could be accomplished in any number of alternative sequences. To address this limitation, we manually evaluated another subset of 30 tasks from our randomly sampled test set and reported the behavior correctness at each action step.\nComponent Per-Step Accuracy: Each of the compo- nents was evaluated in isolation on larger 200-sample subsets of from the Mind2Web dataset and the same samples were used for comparing different models. The element proposal samples typically contained 100-200 elements to select from, whereas the element + action selection samples contained 3-5 elements to select from. These components were eval- uated in isolation to select the optimal models to use with each component.\nSystem End-to-End Correctness: For the end-to-end evaluation, the best-performing models for each of the com- ponents were integrated into the designed system as depicted in Fig. 4, with the primary evaluation focused on the perfor- mance of the system: element proposal \u2192 action + element selection \u2192 secondary parameters. Thus, the performance of the element + action selection is dependent on the ele- ment proposal. In this evaluation, each sample consisted of a user-defined task that contained a set of action steps (typi- cally 5-10 actions). The test subset contained 90 tasks across various websites.\nSystem Manual Evaluation: Due to the limitations of the Mind2Web benchmark dataset, we conducted a man- ual evaluation of Steward using tasks from the Mind2Web dataset. The task dates and particular details were updated to reflect the date of testing. Using 30 tasks, we annotated each action the system selected, and the task success rate, as well as any points of failure.\nComponent Runtime: The computation time for each component was recorded in the prior end-to-end evaluation for measuring the system's typical runtime. Steward's LLM prompts are crafted to minimize response time and return only the information essential to continue operation. We do not include page load times nor Playwright interaction times due to (1) variability and dependence on network connection and (2) this runtime is experienced by a normal website user and/or browser automation tool user.\nSystem Cost and Practicality: To evaluate the system's cost, we measured the token count for each component's inputs and outputs. Each component's token usage was mapped to the API costs for their respective best-performing models shown in Section 6.1. Table 6 defines the mapping used for computing the cost of each component in the system for a single pass (element proposal n = 1), where n denotes the number of retries for the element proposal and selection steps."}, {"title": "6 EVALUATION RESULTS", "content": "We have conducted the experiments and evaluations de- scribed in Section 5, evaluating the following key metrics.\n\u2022 System Correctness, or Component Per-Step Accu- racy: We evaluated the performance of 5 of the 8 natural language processing components with various models. The action and element selection performs better than SOTA. For example, the element + action selection step in isolation achieves 81.44% top-1 accuracy using GPT-4. We perform an end-to-end evaluation of Steward's perfor- mance in completing tasks with the system's components working in tandem. The results indicate Mind2Web's ground truth element and action are selected in 46.55-48.50% of the action steps. In our manual evaluation of Steward running live on websites, it achieved a 40% task comple- tion rate and was able to get through an average of 56% step completion before encountering an error. Steward detected an end-state and terminated correctly in 71% of tasks. (Section 6.1)\n\u2022 System Runtime, Cost, Caching, Practicality: To en- sure Steward is practical for web automation, we mea- sured the token counts and API costs at each action step and upon completing a task. Steward is shown to be cost- efficient for online tasks with an average cost of $0.18 USD per task with a median runtime of 8.52 seconds / $0.028 USD per action. Caching reduced this to a median of 4.8 seconds and $0.013 USD per action. In a small evaluation of the caching system, a cache hit occurred on 49% of the actions when repeating tasks. (Section 6.2)"}, {"title": "6.1 System Correctness", "content": "Component Per-Step Accuracy: Section 6.1 demonstrates the performance for each component of Steward. The action and element selection component performs consistently. In isolation, our prompting approach with GPT-4 achieves an 81.44% combined accuracy with element selection at 83.83% and action selection at 88.02% (for n = 5). The element pro- posal component picks the correct element in the top-5 pro- posals 50% of the samples, achieving a 0.5075 recall@5 score and a 0.7437 recall@25 (Fig. 5 portrays the recall up to n = 50). While a higher n yields an improved likelihood of the ground truth being in the proposal set, there is a trade-off between the number of elements proposed and the performance of the element + action selection component. The element proposal component was evaluated without reducing the set of candi- date elements using string search. The fine-tuned GPT-3.5 models perform best for the double-checking (0.9192 preci- sion) and end state termination (0.7980 F1 score) components, though we use the GPT-4 model instead of the fine-tuned models in later evaluations for simplicity. Finally, for select- ing options and typing text, all models perform well in this task, with > 83.00% of samples matching the ground truth.\nSystem End-to-End Step Correctness: The system's end-to-end performance on the Mind2Web benchmark is lower than the evaluation of each component in isolation. This is primarily due to (1) errors propagating from the HTML filtering and top 15 candidate proposal and (2) the limitations of the dataset having only 1 ground truth ele- ment. After filtering for only the list of interactable HTML elements, Steward retains the ground truth element in 58.14- 63.98% of steps. It only proposes the top-1 correctly labeled ground truth element and action in 46.55-48.50% of sam- ples (action steps). Text field input generation matches the ground truth text input 85.71-92.59% of the time. The com- ponents' performance in this end-to-end setting are found in Section 6.1. However, this performance is not representa- tive of the system's actual performance. We observed fairly frequent occurrences of selected elements that were in line with the user's task or elements that were parents/children of the ground truth, but were not considered correct according to the ground truth as shown in Appendix A.1. The search space of valid navigation sequences is practically infinite. For example, one could use either the search bar or the menu navbar to navigate to a product page. Thus, we also evaluate more representative system's capabilities.\nSystem End-to-End Task Correctness (Manual): Our tests found that Steward was able to perform many tasks, including tasks from the Mind2Web dataset, on real web- sites using Playwright. The system performs reliably and accurately, especially on popular websites or sites with fewer interactable HTML elements. For example, Steward was able to complete entire tasks, consistently perform searches, cor- rectly click on links and buttons, and fill out forms. The exhaustive list of example tasks and the result of running them can be found in Section 6.1. Of the 30 tasks, 40% were successfully completed by our tool. 71% of the completed tasks were successfully terminated by Steward after reach- ing the end state. For the remaining tasks that failed, the most common failure reason was that an element required to progress in the task was not in the list of interactables or the list of limited elements after performing a string search. Fol- lowing this, the next most common issues were that the LLM thought search icons were clickable, the end state was de- tected early, elements were hidden, or an issue occurred with the text field generation. A couple of failures were also due to the task not having account credentials set up and from a website error. Of the tasks that failed, the average number of steps completed before encountering issues was 2.33. Over- all, 6 failures resulted from the LLM failing to return a valid sequence, 6 from HTML filtering, 2 from skipping a step, 2 from website errors, and 2 failures resulted from early termi- nation. In general, Steward consistently completed search or e-commerce-related tasks, while it struggled to complete booking-related tasks."}, {"title": "6.2 System Runtime, Cost, Practicality", "content": "Figure 6 presents the distribution of API costs per LLM com- ponent", "variables": "web page length, number of element proposals, number of encountered errors, and whether the action selected is to type or select. With a cache hit, the system will perform the action with a median runtime of 4"}]}