{"title": "Semantic Variational Bayes Based on a Semantic Information Theory for Solving Latent Variables", "authors": ["Chenguang Lu"], "abstract": "The Variational Bayesian method (VB) is used to solve the probability distributions of latent variables with the minimum free energy criterion. This criterion is not easy to understand, and the computation is complex. For these reasons, this paper proposes the Semantic Variational Bayes' method (SVB). The Semantic Information Theory the author previously proposed extends the rate-distortion function R(D) to the rate-fidelity function R(G), where R is the minimum mutual information for given semantic mutual information G. SVB came from the parameter solution of R(G), where the variational and iterative methods originated from Shannon et al.'s research on the rate-distortion function. The constraint functions SVB uses include likelihood, truth, membership, similarity, and distortion functions. SVB uses the maximum information efficiency (G/R) criterion, including the maximum semantic information criterion for optimizing model parameters and the minimum mutual information criterion for optimizing the Shannon channel. For the same tasks, SVB is computationally simpler than VB. The computational experiments in the paper include 1) using a mixture model as an example to show that the mixture model converges as G/R increases; 2) demonstrating the application of SVB in data compression with a group of error ranges as the constraint; 3) illustrating how the semantic information measure and SVB can be used for maximum entropy control and reinforcement learning in control tasks with given range constraints, providing numerical evidence for balancing control's purposiveness and efficiency. Further research is needed to apply SVB to neural networks and deep learning.", "sections": [{"title": "INTRODUCTION", "content": "Machine learning often requires solving the probability distribution P(y) of a latent variable y from observed data x (or probability distribution P(x)), and a group of predictive models or likelihood functions P(x|yj, 0) (j = 1, 2, ...). A popular method is the Variational Bayesian method [1],[2], abbreviated as Variational Bayes (VB). VB has been successfully applied in various scenarios, such as mixture models [3, 4], AutoEncoder [5], active inference with minimum free energy principle [6], and others [7]. Bayesian Inference (BI) is an inference method Bayesians use to infer model parameters, including parameters in the likelihood function P(x|y, 0) and parameterized P(y). Unlike BI, frequentists use Likelihood Inference, which only considers parameters in likelihood functions and does not account for the probability distributions of parameters. I employs frequentist methods and thinks that frequentism should also have a general method for finding the latent variable (actually, the probability distribution P(y)).\nAlthough the Expectation-Maximization (EM) algorithm used by frequentists can also solve for latent variables in mixture models, solving for P(y) remains problematic when likelihood functions, i.e., predictive models, are unchanged. This is particularly true when there are fuzzy range constraints rather than likelihood function constraints. For example, in active inference, given the probability distribution P(x) of an uncontrolled state and several targets y1, y2,..., (representing fuzzy ranges) as constraints, we need to select an action aj to achieve a target yj and optimize P(a1), P(a2), ... This is a problem that needs to be addressed.\nRegularization based on information theory has recently been applied to deep learning [8] and reinforcement learning [9], yielding good results. This method uses the difference between two types of information as the objective function and then minimizes this function using variational methods to solve for latent variables. This paper aligns with this approach but uses the difference between Shannon's mutual information and semantic mutual information as the objective function. The theoretical foundation is traceable to Shannon's pioneering work on the rate-distortion function [10],[11].\nGenerally, the number of possible values of x is much larger than that of y. Given observed data (i.e., a sample distribution P(x)) and the constraint function P(x|y, 0), there is no exact solution for P(y) (due to excessive constraints) that makes $P_\\theta(x)=\\sum_i P(x|y_j,\\theta)P(y_j)$ equal to P(x). However, we can use a specific criterion to obtain an approximate solution, such as using the minimum Kullback-Laibler (KL) divergence KL(P(y)||P(x, y|0)) or KL(P(y|x)||P(x, y|0)) as the optimization criterion in the VB algorithm to obtain an approximate solution for P(y|x) and P(y). This paper proposes the Semantic Variational Bayes method (SVB), which uses the maximum information efficiency criterion compatible with the maximum likelihood criterion and the maximum entropy principle.\nI uses the term \"Semantic\" because SVB is based on a Semantic Information Theory, i.e., the Shannon-Lu theory or the G-theory [12], [13] (G denotes the generalization of the Shannon information Theory). Additionally, SVB uses various learning functions, such as likelihood, truth, membership, similarity, and distortion functions, as constraints. These functions are related to semantics. Although SVB is frequentist, it employs various extended Bayes' formulas and performs tasks similar to those for which VB is used. Hence, the term \"Variational Bayes\" is still employed.\nThe semantic information measure in the G-Theory is called the G measure [13]. We can use a likelihood, truth, membership, similarity, or distortion function with P(x) to express the G measure. The G theory extends the rate-distortion function R(D) to the rate-fidelity function R(G), where R is the minimum Shannon mutual information for given semantic mutual information G. The ratio G/R represents information efficiency. SVB came from the parameter solution of the R(G) function. The variational and iterative methods for SVB originated from Shannon and others' research on the rate-distortion function [11], [14], [15], [16].\nI proposed the G-Theory thirty years ago [17], [18] and has applied it to machine learning in the last decade. Previous papers have discussed methods for solving or optimizing various learning functions from sample distributions and applied these methods to multi-label classification, mixture models [13], Bayesian confirmation [19], and semantic compression [20]. Some papers have involved the issue of solving the probability distributions of latent variables [13], [20].\nThis paper first proposes SVB as an alternative to VB for the following reasons:\n\u2022 The minimum free energy principle used by VB is not easy to understand. There are counterexamples in mixture models (see Section 4.1) that cannot converge when we maximize the negative free energy function $F=Q+H(Y)$ [1], [4]."}, {"title": "THE G-THEORY AND SVB", "content": "A. The P-T Probability Framework and the Semantic Bayes' Formula\nThe probability defined by Kolmogorov [21] is the probability that a random variable belongs to a set, referred to as the probability of the set. However, to obtain this probability, we still need to know the probability of the elements in the set. For example, if we want to see the probability of young people, adults, elderly, etc., we also need to know the probability distribution over different ages. The probability of an age is the probability defined by frequentists [22]. So, the two probabilities are complementary. The P-T probability framework attempts to unify the two kinds of probabilities and follows Zadeh to generalize sets' probability to fuzzy sets' probability [23],[24]. We call a set's probability the logical probability (denoted by T) and an element's probability the statistical probability (denoted by P).\nWe define:\n\u2022 X is a random variable denoting an instance, taking a value $x \\in U={X_1, X_2, \\dots, X_m}$. Y is a random variable denoting a label or hypothesis, taking a value $y \\in V={y_1, y_2, ..., y_n}$. The $\\theta_j(x_i)$ is a proposition, $\\theta_j$ is a fuzzy subset [23] of U, and elements in $\\theta_j$ make $y_j$ true. There is $y_j(x) = $ \"$x \\in \\theta_j$\". The $\\theta_j$ also means a model or a group of model parameters.\n\u2022 A probability defined with \"=\", such as $P(y_j)= P(Y=y_j)$, is statistical; a probability defined with \"$\\in$\", such as $P(X \\in \\theta_j)$, is logical. To distinguish them, we define $T(y_j)=T(\\theta_j)=P(X\\in \\theta_j)$ as the logical probability of $y_j$.\n\u2022 $T(y_j|x)=T(\\theta_j|x)=P(X\\in \\theta_j|X=x) \\in [0, 1]$ is the truth function of $y_j$ and the membership function $m_{\\theta_j}(x)$ of $\\theta_j$, i.e.,\n$T(y_j|x)=T(\\theta_j|x)=m_{\\theta_j}(x)$. (1)\nThe truth function of y indicates its semantics (formal semantics, only related to the extension or denotation of $y_j$) according to Davidson's truth-conditional semantics [25]. A hypothesis's logical probability may differ from its statistical probability [19]. For example, a tautology's logical probability is 1, whereas its statistical probability is almost 0. We have $P(y_1) + P(y_2) + ... + P(y_n) = 1$, but there may be $T(y_1) + T(y_2) + ... + T(y_n) > 1$.\nAccording to the above definition, we have:\n$T(y_z)=T(\\theta_j) = P(X \\in\\theta_j) = \\sum_i P(x_i)T(\\theta_j|x_i)$. (2)\nThis is the fuzzy event's probability defined by Zadeh [25].\nWe can put $T(\\theta_j|x)$ and P(x) into a Bayes' formula to obtain a likelihood function [13]:\n$P(x|\\theta_j)=\\frac{T(\\theta_j|x)P(x)}{T(\\theta_j)}, T(\\theta_j) = \\sum_i T(\\theta_j|x_i)P(x).$ (3)\nWe call (3) the semantic Bayes' formula. Since the maximum of $T(\\theta_j|x)$ is 1, from P(x) and $P(x|\\theta_j)$, we derive:\n$T(\\theta_j | x) = \\frac{P(x | \\theta_j)}{P(x)} = \\frac{P(x | \\theta_j)}{max_x(P(x | \\theta_j))}$ (4)\nA semantic channel $T(y|x)$ consists of a group of truth functions: $T(\\theta_j|x), j = 1, 2, ..., n$, as well as a Shannon channel $P(y|x)$ consists of a group of transition probability functions: $P(y_j|x), j=1, 2, ..., n$. When the semantic channel matches the Shannon channel, i.e., $T(\\theta_j|x)\\propto P(y_j|x)$ or $P(x|\\theta_j)=P(x|y_j), j = 1, 2, ..., n$, the semantic mutual information reaches its maximum and equals Shannon mutual information.\nB. Relationships among Truth, Membership, Similarity, and Distortion Functions\nThe truth function $T(\\theta_j|x)$ of $y_j$ is also the membership function of the fuzzy set $\\theta_j$. Assuming that for each $y_j$, there exists an archetype or Platonic ideal $x_j$ such that $T(\\theta_j|x_j)=1$, then the truth value or membership degree $T(\\theta_j|x_i)$ is the similarity between $x_i$ and $x_j$. If the domains of x and y are the same, i.e., U=V, then $y_j$ becomes an estimate, i.e., $y_j= x_j = $ \"x is about $x_j$\". The similarity function between x and $x_j$, denoted as $S(x, x_j)$, is equivalent to the truth function of $y_j(x)$ and the confusion probability function between them. For instance, the similarity between the indicated GPS location $y_j$ and the actual location $x_i$ is the truth value of $y_j(x_i)$ and their confusion probability.\nThe truth function and the distortion function can be converted into each other. Let $d(y_j|x_i)$ be the (amount of) distortion of $y_j$ representing $x_i$. We define:\n$T(\\theta_j|x_i) = exp[-d(y_j|x_i)]$. (5)\nLet exp and log be a pair of inverse functions, hence:\n$d(y_j|x_i) = -log T(\\theta_j|x_i)$. (6)\nIn some cases, it is difficult to directly define the distortion function, so we can first determine the truth function and then use (6) to obtain the distortion function. For example, it is difficult to define the distortion function for the label \"elderly\" directly, but we can use a logistic function as the truth function of \"elderly\" and then use (6) to obtain the distortion function of \"elderly\".\nSince distortion is generally asymmetric, we use $d(y_j|x_i)$ to represent the distortion of $y_j$ representing $x_i$. For estimation, the distortion function is symmetric. For example, for GPS, the distortion is proportional to the square of distance, and the truth or similarity function is:\n$T(\\theta_j | x) = S(x, x_j) = exp[-d(x, x_j)] = exp[-\\frac{(x-x_i)^2}{2\\sigma^2}]$. (7)\nThe similarity function is also the observer's discriminant function. A GPS device's accuracy RMS (Root Mean Square of error denoted as $\\sigma$ in the above equation) indicates its resolution or discrimination [12].\nNot only does the truth function $T(\\theta_j|x)$ represent the semantics of $y_j$. Membership, similarity, and distortion functions all reflect the semantics of labels because a membership function is equivalent to a truth function; the similarity function is a particular case of the truth function; a distortion function can be converted to a truth function.\nC. The Semantic Information Measure\nShannon's mutual information can be expressed as:\n$I(X; Y) = \\sum_j\\sum_i P(y_j)P(x|y_j)log\\frac{P(x;y;)}{P(x)}$ (8)\nIt represents the average codeword length saved by the probability prediction P(x|y). By replacing P(xi|yj) on the right side of \"log\" with the likelihood function $P(x_i|\\theta_j)$ (leaving the left side unchanged), we obtain the formula for semantic mutual information:\n$I(X; Y_\\theta) = \\sum_j\\sum_i P(y_j)P(x_i | y_j)log\\frac{P(x, \\theta_j)}{P(x)} = \\sum_j P(y_j)P(x | y_j) log \\frac{T(\\theta_j|x_i)}{T(\\theta_j)}$ (9)\nIt represents the average codeword length saved by a subjective probability prediction $P(x|\\theta_j) (j=1,2,...)$ according to the semantics of Y [17].\nWhen Y=yj, semantic mutual information becomes semantic Kullback-Leibler (KL) information:\n$I(X;\\theta_j) = \\sum_i P(x|y_j)log\\frac{P(x_i\\theta_j)}{P(x)} = \\sum_i P(x, y_j) log \\frac{T(\\theta_j|x_i)}{T(\\theta_j)}$ (10)\nNote that in the above formula, P(x|yj) is used for averaging and represents the sample distribution. It can be the relative frequency and may not be smooth or continuous. $P(x|\\theta_j)$ may differ from P(x|yj), meaning that information needs factual verification.\nFurther, if X=xi, semantic KL information becomes semantic information conveyed by yj about xi:\n$I(x_i;\\theta_j)=log\\frac{P(x_i\\theta_j)}{P(x)}=log\\frac{T(\\theta_j | x_i)}{T(\\theta_j)}$ (11)\nBring (5) into (9), we obtain\n$I(X; Y_\\theta) = \\sum_j P(y_j)logT(\\theta_j) \u2013 E_{P(x, y)}d(x, y)= H(Y_\\theta)- \\bar{d}$. (12)\nwhere H(Yo) is semantic entropy and $\\bar{d}$ is average distortion. $I(X; Y_\\theta)$ is like a negative regularized squares measure. Therefore, we can treat the maximum semantic information criterion as a special Regularized Least Squares (RLS) criterion.\nSuppose the truth function in (9) becomes a similarity function. In that case, the semantic mutual information becomes the estimated mutual information [27], which has been used by deep learning researchers in Information Noise Contrastive Estimation (InfoNCE) [28] and Mutual Information Neural Estimation (MINE) [29].\nD. Optimizing Various Learning Functions with Logical Bayes' Inference\nI proposed Logical Bayesian Inference before [13]. Since it is not a Bayesian method, we call it \"Logical Bayes' Inference\" (LBI) hereafter.\nThe most often used learning function is the likelihood function $P(x|\\theta_j)$. We can use the semantic KL formula (10) to optimize $P(x|\\theta_j)$. Since semantic KL information is equal to the difference between two KL divergences:\n$I(X; \\theta_j)=KL(P(x|y_j)||P(x)) - KL(P(x|y_j)||P(x|\\theta_j))$, (13)\nit is easy prove that $I(X; \\theta_j)$ reaches its maximum when $KL(P(x|y_j)||P(x|\\theta_j)) = 0$ or\n$P(x|\\theta_j)= P(x|y_j)$. (14)\nSometimes, we wish to use $P(\\theta_j|x)$, the parameterized $P(y_j|x)$, as the learning function. Fisher calls $P(\\theta_j|x)$ the inverse probability (function) [31]. When n=2, we can use a pair of logistic functions as a pair of inverse probability functions. However, it is not easy to construct $P(\\theta_j|x), j = 1, 2, ..., n$, as n>2 because there is the normalization limit $\\sum_j P(\\theta_j|x) = 1$ for every x. Nevertheless, there is no limit to truth and similarity functions.\nFrom (4) and (14), we derive the optimized truth function:\n$T^*(0, \\vert x) = \\frac{P(x\\vert\\theta_j)}{P(x)} = \\frac{\\frac{P^*(x\\vert\\theta_j)}{max_x (P(x \\vert\\theta_j))}}{P(x)}  = \\frac{\\frac{P(x\\vert y_j)}{max_x (P(x \\vert y_j))}}{P(x)} = \\frac{P(y_j \\vert x)}{max(P(y_j \\vert x))}$ (15)\n$P(x|y_j)$ above is assumed to be a smooth distribution; otherwise, we can only obtain a smooth $T(\\theta_j|x)$ by using the following formula:\n$T^*(0_j | x) = arg max \\sum_{0_j} P(x; y_j)log\\frac{T(0_j | x_i)}{T(0_j)}$ (16)\nIf we only know $P(y_j|x)$ without knowing P(x), we may let P(x)=1/m to obtain $T^*(0_j|x)$ [13]. We call the above method (of solving truth or similarity functions from sample distributions) LBI.\nIn the following part of this section, we assume that all likelihood, truth, and similarity functions are optimized and a sampling distribution is the same as the corresponding probability distribution. We call\n$c(x, y_j) = P(x|\\theta_j)/P(x)=P(x, y_j)/[P(x)P(y_j)]$ (17)\nthe relatedness function. Sklar proposed the copula function [32], [33]. The c(x, yj) is also a two-dimensional copula density function. Since c(x, yj) is proportional to $P(y_j|x)$, we can replace $P(y_j|x)$ with c(x, yj) in a Bayes' formula as follows:\n$P(x|y_j) = \\frac{P(x)c(x,y_j)}{c(y_j)}, c(y_j) = \\sum_i P(x)c(x, y_j),$ (18)\nwhere P(x) may be different from previous P(x). We also call c(x, y) the Bayes' core since $P(x, y) = P(x)c(x, y)P(y)$.\nWhen P(x) is unchanged, $P(x|y_j)=P(x_i)c(x, y_j)$. If P(x) is changed, we have to use (18) to obtain new $P(x|y_j)$ and use SVB to obtain new P(y) and c(x, y).\nFor given P(x) and some constraints, if the expression of $P(y)c(x, y)$ represents a Shannon channel P(y|x), there should be, for every j,\n$\\sum_i P(x,y_j) = \\sum_i P(x)c(x, y_j) =1 and \\sum_i P(x_i)c(x, y_j)P(y_j) = P(y_j)$. (19)\nOtherwise, P(y) and P(y|x) are inappropriate solutions. For example, the results of the E-step in the EM algorithm are often inappropriate.\nIf the domains of x and y are the same, then y=x\u2081 becomes an estimate: \"x is about xj.\" In this case, $T(\\theta_j|x_i)=T(x_j |x_i)=1$ as i=j, and the truth value becomes the symmetric similarity degree:\n$S(x_i | x_j) = \\frac{c(x, y_j)}{max(c(x, y))} = \\frac{c(x_j | x_i)}{max(c(x, y_i))} = S(x_j | x_i)$. (20)\nHowever, truth or membership functions may be asymmetric. Suppose all $y_j$ similar to $y_i$ form a fuzzy subset $O_{xi}$ of V; we have\n$T(O_{xi} | y_t) = \\frac{P(x_i | y_j)}{max_y(P(x_i | y))} = \\frac{P(y_j | x_i)}{max(P(y_j | x))} = T(0_i | x_i)$. (21)\nRecently, truth or similarity functions have also been used as learning functions in deep learning [27]. For example, Oord et al. presented InfoNCE [28] and explicitly pointed out that the learning function is proportional to $m(x, y_j) = P(x|y_j)/P(x)$. The expression in their paper is:\n$f_k (X_{t+k} C_t) \\propto P(X_{t+k} | C_t)/ P(X_{t+k})$, (22)\nwhere ct is the feature vector obtained from previous data, Xt+k is the predictive vector, and fk(x1+k, ct) is a similarity function (between predicted Xt+k and real Xt+k). The estimated mutual information expressed with a similarity function is a particular case of semantic mutual information.\nE. The Origin of SVB: from R(D) to R(G)\nWe call the R(G) function the rate-fidelity function for two reasons. Firstly, verisimilitude [26] or truthlikeness [35] has been discussed for a long time by Popper and other philosophers. It indicates the progress from not too true to truth or the reduction in distortion. $I(x_i; \\theta_j)$ is a proper measure for verisimilitude. Since \"verisimilitude\" and \"truthlikeness\" are rare in technical papers, we use a similar and familiar word, \"fidelity\", as \"verisimilitude\". Secondly, Shannon first proposed the information rate fidelity criterion [10], [11] and later adopted minimum distortion to represent maximum fidelity. It should conform to Popper and Shannon's original ideas using the semantic information measure to denote fidelity.\nSince the rate-distortion is equal to information rate-distortion, Cover and Thomas [36, Section 10.2] also use \"rate-distortion\" as \"information rate-distortion\", not distinguishing the two. For the same reason, we do not differentiate \"rate-fidelity\" and \"information rate-fidelity\" in this paper.\nWe change the distortion limit d\u2264D for R(D) into $I(X; Y_\\theta) \u2265 G$, and then R(D) becomes the information rate-fidelity function R(G) [12], [13]. In this case, we replace $d_{ij} = d(x_i, y_j)$ with $I_{ij} = I(x_i; \\theta_j)$.\nIn addition to the given P (x) and G, there are the following limitations:\n$\\sum_i P(x_i | y_j) = 1, j = 1, 2, ..., n;$ (23)\n$\\sum_jP(y_j) = 1$. (24)\nWe use the Lagrange multiplier method for minimum mutual information (MMI). The Lagrange function is:\n$L(P(y|x), P(y)) = I(X;Y) \u2013 sI(X; Y_\\theta) \u2013 \\mu_1\\sum_j P(x_i;y_j) - \\alpha\\sum_j P(y_j)$, (25)\nwhere $I(X; Y_\\theta)$ and $I(X; Y)$ are expressed with P(y|x) and P(y) as:\n$I(X; Y_\\theta) = \\sum_i P(x)\\sum_j P(y_j | x_i)logm_{ji}, m_{ji} = T(\\theta_j|x_i) / T(\\theta_j) = P(x_i | \\theta_j)/P(x)$, (26)\n$I(X; Y) = \\sum_i P(x)\\sum_j P(y_j |x_i)[log[P(y_j | x_i) \u2013 log P(y_j)]$. (27)\nSince P(yx) and P(y) are interdependent, we alternatively use one as the variation. First, we fix P(y) and let\n$\\partial F/\\partial P(y_j | x_i) = 0, j = 1, 2, ..., n; i = 1, 2, ..., m$. (28)\nHence, we derive\n$P(y_j | x_i) = P(y_j |\\lambda, \\lambda_i = \\sum_j P(y_j)m_{ji}^s, i = 1,2,...; j = 1,2,...$ (29)\nWe can treat $m_{ij}^s$ as an intensive Bayes' core and consider (28) as a generalized Bayes' formula.\nThen, we fix P(y|x) and let\n$\\partial F/\\partial P(y) = 0, j = 1, 2, ..., n$.\nWe derive\n$P (y) = \\sum_i P(x)P(y_j |x_i)$. (30)\nRepeating (28) and (29) as we do for the R(D) function [16, p. 326], we obtain appropriate P(y) and P(y|x). Without this iteration, $m_{ij}^s/\\lambda$, as c(xi, yj), is not an appropriate Bayes' core or copula density function, and $P(x|y_j)=P(x)m_{ijs}/\\lambda$ is not normalized. Formulas (28) and (29) are the MMI iteration, letting the Shannon channel match the semantic channel.\nFinally, we obtain the parameter solution of R(G):\n$G(s) = \\sum_{i, j} P(x)P(y_j | x_i)I_{ij}$,\n$R(s) = sG(s) - \\sum_i P(x) log \\lambda_i$,\nIt is worth noting that for given the semantic channel T(y|x), letting $P(y_j|x) \\propto T(\\theta_j|x)$ or $P(x|y_j) = P(x|\\theta_j)$ does not maximize G, but the information efficiency G/R. We can increase s to further increase both information with (28). As s->\u221e, $P(y_j|x) (j = 1, 2, ..., n)$ only takes value 0 or 1, becoming a classification function.\nWith the Lagrange function in (25), if we fix P(yx) and P(y) and use $T(\\theta_j|x)$ or $P(x\\theta_j), j = 1, 2, ..., n$, as the variation, that is to use LBI to optimize the predictive model. Therefore, we may say that the MMI iteration plus LBI forms SVB. SVB uses the minimum Shannon mutual information criterion plus the maximum semantic mutual information criterion. The two criteria together equal the maximum information efficiency criterion.\nWe can also replace average distortion $\\bar{d}$ with fuzzy entropy H(Yo|X) to obtain the rate-truth function R(\u0398), where is a group of fuzzy sets as the constraint. R(G) is more suitable than R(D) and R(0) when information is more important than truth. P(y) and P(y|x) for R() are different from those for R(G) because the optimization criterion is different. With the maximum truth criterion, P(y|x) becomes\n$P(y_j|x) = P(y_j)[T(\\theta|y_j)]^s/\\sum_jP(y_j)[T(\\theta | y_j)]^s, i = 1,2,...; j = 1,2,... (31)\nIf $T(0_{xi}|y) = exp[\u2212d(x_i|y_j)]$, R(@) becomes R(D) [21]. If $T(\\theta_j)$ is very small, P(yj) for R(G) is larger than that for R(D) and R(O)."}, {"title": "SOLVING LATENT VARIABLES BY USING SVB", "content": "A. Approximate Solutions and Optimization Criterions for Latent Variables\nFrom the information-theoretic perspective", "equations": "n$P(x_i|y_1)P(y_1) + P(x_i|y_2)P(y_2) + ... + P(x_i|y_n)P(y_n) = P(x_i)", "P_\\theta(x)": 33, "obtain": "n$P(y_j|x) = P(y_j)P(x|\\theta_j)/P(x), P_\\theta(x) = \\sum"}]}