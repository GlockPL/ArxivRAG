{"title": "Semantic Variational Bayes Based on a Semantic\nInformation Theory for Solving Latent Variables", "authors": ["Chenguang Lu"], "abstract": "The Variational Bayesian method (VB) is used to solve the probability distributions of latent\nvariables with the minimum free energy criterion. This criterion is not easy to understand, and the computation\nis complex. For these reasons, this paper proposes the Semantic Variational Bayes' method (SVB). The\nSemantic Information Theory the author previously proposed extends the rate-distortion function R(D) to the\nrate-fidelity function R(G), where R is the minimum mutual information for given semantic mutual information G. SVB\ncame from the parameter solution of R(G), where the variational and iterative methods originated from Shannon\net al.'s research on the rate-distortion function. The constraint functions SVB uses include likelihood, truth,\nmembership, similarity, and distortion functions. SVB uses the maximum information efficiency (G/R) criterion,\nincluding the maximum semantic information criterion for optimizing model parameters and the minimum\nmutual information criterion for optimizing the Shannon channel. For the same tasks, SVB is computationally\nsimpler than VB. The computational experiments in the paper include 1) using a mixture model as an example\nto show that the mixture model converges as G/R increases; 2) demonstrating the application of SVB in data\ncompression with a group of error ranges as the constraint; 3) illustrating how the semantic information\nmeasure and SVB can be used for maximum entropy control and reinforcement learning in control tasks with\ngiven range constraints, providing numerical evidence for balancing control's purposiveness and efficiency.\nFurther research is needed to apply SVB to neural networks and deep learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine learning often requires solving the probability distribution P(y) of a latent variable y from observed\ndata x (or probability distribution P(x)), and a group of predictive models or likelihood functions P(x|y, 0) (j = 1,\n2, ...). A popular method is the Variational Bayesian method [1],[2], abbreviated as Variational Bayes (VB). VB has\nbeen successfully applied in various scenarios, such as mixture models [3, 4], AutoEncoder [5], active inference\nwith minimum free energy principle [6], and others [7]. Bayesian Inference (BI) is an inference method Bayesians\nuse to infer model parameters, including parameters in the likelihood function P(x|y, 0) and parameterized P(y).\nUnlike BI, frequentists use Likelihood Inference, which only considers parameters in likelihood functions and does\nnot account for the probability distributions of parameters. I employs frequentist methods and thinks that\nfrequentism should also have a general method for finding the latent variable (actually, the probability distribution\nP(y)).\nAlthough the Expectation-Maximization (EM) algorithm used by frequentists can also solve for latent variables\nin mixture models, solving for P(y) remains problematic when likelihood functions, i.e., predictive models, are\nunchanged. This is particularly true when there are fuzzy range constraints rather than likelihood function\nconstraints. For example, in active inference, given the probability distribution P(x) of an uncontrolled state and\nseveral targets y1, y2,..., (representing fuzzy ranges) as constraints, we need to select an action aj to achieve a target\ny; and optimize P(a1), P(a2), ... This is a problem that needs to be addressed.\nRegularization based on information theory has recently been applied to deep learning [8] and reinforcement\nlearning [9], yielding good results. This method uses the difference between two types of information as the\nobjective function and then minimizes this function using variational methods to solve for latent variables. This\npaper aligns with this approach but uses the difference between Shannon's mutual information and semantic mutual\ninformation as the objective function. The theoretical foundation is traceable to Shannon's pioneering work on the\nrate-distortion function [10],[11].\nGenerally, the number of possible values of x is much larger than that of y. Given observed data (i.e., a sample\ndistribution P(x)) and the constraint function P(x|y, 0), there is no exact solution for P(y) (due to excessive\nconstraints) that makes Po(x)=\u2211i P(x|yj,0)P(yj) equal to P(x). However, we can use a specific criterion to obtain an\napproximate solution, such as using the minimum Kullback-Laibler (KL) divergence KL(P(y)||P(x, y|0)) or\nKL(P(y|x)||P(x, y|0)) as the optimization criterion in the VB algorithm to obtain an approximate solution for P(y|x)\nand P(y). This paper proposes the Semantic Variational Bayes method (SVB), which uses the maximum information\nefficiency criterion compatible with the maximum likelihood criterion and the maximum entropy principle.\nI uses the term \"Semantic\" because SVB is based on a Semantic Information Theory, i.e., the Shannon-Lu\ntheory or the G-theory [12], [13] (G denotes the generalization of the Shannon information Theory). Additionally,\nSVB uses various learning functions, such as likelihood, truth, membership, similarity, and distortion functions, as\nconstraints. These functions are related to semantics. Although SVB is frequentist, it employs various extended\nBayes' formulas and performs tasks similar to those for which VB is used. Hence, the term \"Variational Bayes\" is\nstill employed.\nThe semantic information measure in the G-Theory is called the G measure [13]. We can use a likelihood, truth,\nmembership, similarity, or distortion function with P(x) to express the G measure. The G theory extends the rate-\ndistortion function R(D) to the rate-fidelity function R(G), where R is the minimum Shannon mutual information for\ngiven semantic mutual information G. The ratio G/R represents information efficiency. SVB came from the\nparameter solution of the R(G) function. The variational and iterative methods for SVB originated from Shannon\nand others' research on the rate-distortion function [11], [14], [15], [16].\nI proposed the G-Theory thirty years ago [17], [18] and has applied it to machine learning in the last decade.\nPrevious papers have discussed methods for solving or optimizing various learning functions from sample\ndistributions and applied these methods to multi-label classification, mixture models [13], Bayesian confirmation\n[19], and semantic compression [20]. Some papers have involved the issue of solving the probability distributions of\nlatent variables [13], [20].\nThis paper first proposes SVB as an alternative to VB for the following reasons:\n\u2022 The minimum free energy principle used by VB is not easy to understand. There are counterexamples in\nmixture models (see Section 4.1) that cannot converge when we maximize the negative free energy\nfunction F=Q+H(Y) [1], [4]."}, {"title": null, "content": "\u2022 When we use VB with the mean field approximation (P(y) is replaced with P(y|x)) [3], we can avoid the\ncounterexamples, but calculating P(y|x) is complex, involving both exponential and logarithmic functions.\nSo, we need a simpler method.\n\u2022 Given observed data and various learning or constraint functions, demands for solving latent variables may\nexist. We need a more general solution method.\nThe main purposes of this paper are to:\n1) Provide a theoretically understandable and computationally simpler general method for solving the\nprobability distribution P(y) of the latent variable y from P(x) and various constraint functions;\n2) Carry forward the variational and iterative methods used by Shannon and others in studying the rate-\ndistortion function;\n3) Enhance understanding of the G-Theory.\nThe main contribution of this article is to propose, systematically introduce, and provide examples to validate\nSVB. This work is important because, compared to VB,\n\u2022\nSVB is easier to understand theoretically because it is compatible with the maximum likelihood criterion\nand the maximum entropy principle.\n\u2022 SVB allows the use of various constraint functions and can also strengthen constraints by using the\nexponent s (see (28));\n\u2022 SVB's calculation is simpler because the expressions for the solutions of P(y|x) and P(y) are simple, without\nexponential and logarithmic operations for the same tasks.\nSince this paper adopts information-theoretic methods, it uses the probability distribution or relative frequency\nP(xi) (i = 1, 2, ..., m) of the data instead of the data sequence x(1), x(2), ..., x(N). Please note that the expression of\nthe average log-likelihood in this paper is different from that in statistics."}, {"title": "II. THE G-THEORY AND SVB", "content": ""}, {"title": "A. The P-T Probability Framework and the Semantic Bayes' Formula", "content": "The probability defined by Kolmogorov [21] is the probability that a random variable belongs to a set, referred\nto as the probability of the set. However, to obtain this probability, we still need to know the probability of the\nelements in the set. For example, if we want to see the probability of young people, adults, elderly, etc., we also need\nto know the probability distribution over different ages. The probability of an age is the probability defined by\nfrequentists [22]. So, the two probabilities are complementary. The P-T probability framework attempts to unify the\ntwo kinds of probabilities and follows Zadeh to generalize sets' probability to fuzzy sets' probability [23],[24]. We\ncall a set's probability the logical probability (denoted by T) and an element's probability the statistical probability\n(denoted by P).\nWe define:\n\u2022 X is a random variable denoting an instance, taking a value x\u2208U={X1, X2, ..., m}. Y is a random variable\ndenoting a label or hypothesis, taking a value y\u2208 V={y1, y2, ..., n}. The yj(xi) is a proposition, 0; is a fuzzy\nsubset [23] of U, and elements in 0; make y; true. There is y;(x) = \"x \u0454 0;\". The dj also means a model or a group\nof model parameters.\n\u2022 A probability defined with \"=\", such as P(yj)= P(Y=yj), is statistical; a probability defined with \"\u0454\", such as P(X\n\u20ac \u03b8j), is logical. To distinguish them, we define T(yj)=T(0;)=P(X\u20ac \u03b8;) as the logical probability of yj.\n\u2022 $T(y_j|x)=T(\\theta_j|x)=P(X\\epsilon \\theta_j|X=x) \\epsilon [0, 1]$ is the truth function of y; and the membership function moj(x) of dj, i.e.,\n$T(y_j|x)=T(\\theta_j|x)=m_{\\theta_j} (x)$.\n(1)\nThe truth function of yindicates its semantics (formal semantics, only related to the extension or denotation of yj)\naccording to Davidson's truth-conditional semantics [25]. A hypothesis's logical probability may differ from its\nstatistical probability [19]. For example, a tautology's logical probability is 1, whereas its statistical probability is\nalmost 0. We have P(y1) + P(y2) + ... + P(yn) = 1, but there may be T(v1) + T(y2) + ... + T(yn) > 1."}, {"title": null, "content": "According to the above definition, we have:\n$T(y_j)=T(\\theta_j) = P(X \\epsilon \\theta_j) = \\sum_i P(x_i)T(\\theta_j|x_i)$.\n(2)\nThis is the fuzzy event's probability defined by Zadeh [25].\nWe can put T(0j|x) and P(x) into a Bayes' formula to obtain a likelihood function [13]:\n$P(x|\\theta_j)=\\frac{T(\\theta_j|x)P(x)}{T(\\theta_j)}, T(\\theta_j) = \\sum_i T(\\theta_j |x_i)P(x_i)$.\n(3)\nWe call (3) the semantic Bayes' formula. Since the maximum of T(0j|x) is 1, from P(x) and P(x|0j), we derive:\n$T(\\theta_j |x) = \\frac{P(x|\\theta_j)}{P(x)} / max_x(\\frac{P(x|\\theta_j)}{P(x)})$.\n(4)\nA semantic channel T(y|x) consists of a group of truth functions: T(0j|x), j = 1, 2, ..., n, as well as a Shannon\nchannel P(y|x) consists of a group of transition probability functions: P(yj|x), j=1, 2, ..., n. When the semantic\nchannel matches the Shannon channel, i.e., T(0j|x)\u221dP(yj|x) or P(x|0j)=P(x|yj), j = 1, 2, ..., n, the semantic mutual\ninformation reaches its maximum and equals Shannon mutual information."}, {"title": "B. Relationships among Truth, Membership, Similarity, and Distortion Functions", "content": "The truth function T(0j|x) of y; is also the membership function of the fuzzy set 0j. Assuming that for each yj,\nthere exists an archetype or Platonic ideal x; such that T(0j|xj)=1, then the truth value or membership degree T(0j|xi)\nis the similarity between x\u2081 and xj. If the domains of x and y are the same, i.e., U=V, then y; becomes an estimate, i.e.,\nyj= x; = \"x is about x;\". The similarity function between x and xj, denoted as S(x, xj), is equivalent to the truth\nfunction of y;(x) and the confusion probability function between them. For instance, the similarity between the\nindicated GPS location y; and the actual location x\u2081 is the truth value of y;(xi) and their confusion probability.\nThe truth function and the distortion function can be converted into each other. Let d(yj|xi) be the (amount of)\ndistortion of y; representing xi. We define:\n$T(\\theta_j|x_i) = exp[-d(v_j|x_i)]$.\n(5)\nLet exp and log be a pair of inverse functions, hence:\n$d(y_j|x_i) = -log T(\\theta_j|x_i)$.\n(6)\nIn some cases, it is difficult to directly define the distortion function, so we can first determine the truth\nfunction and then use (6) to obtain the distortion function. For example, it is difficult to define the distortion function\nfor the label \"elderly\" directly, but we can use a logistic function as the truth function of \"elderly\" (see Fig. 5a) and\nthen use (6) to obtain the distortion function of \"elderly\".\nSince distortion is generally asymmetric, we use d(yj|xi) to represent the distortion of yjrepresenting xi. For\nestimation, the distortion function is symmetric. For example, for GPS, the distortion is proportional to the square\nof distance, and the truth or similarity function is:\n$T(\\theta_j | x) = S(x, x_i) = exp[-d(x, x_i)] = exp[-\\frac{(x-x_i)^2}{2\\sigma^2}]$.\n(7)\nThe similarity function is also the observer's discriminant function. A GPS device's accuracy RMS (Root Mean\nSquare of error denoted as o in the above equation) indicates its resolution or discrimination [12]."}, {"title": "C. The Semantic Information Measure", "content": "Shannon's mutual information can be expressed as:\n$I(X; Y) = \\sum_j\\sum_i P(y_j)P(x_i|y_j)log\\frac{P(x_i;y_j)}{P(x)}$.\n(8)\nIt represents the average codeword length saved by the probability prediction P(x|y). By replacing P(xi|yj) on\nthe right side of \"log\" with the likelihood function P(xi|0j) (leaving the left side unchanged), we obtain the formula\nfor semantic mutual information:\n$I(X; Y) = \\sum_j\\sum_i P(y_j)P(x_i | y_j)log\\frac{P(x_i, \\theta_j)}{P(x)} = \\sum_j P(y_j)P(x_i | y_i) log\\frac{T(\\theta_j|x_i)}{T(\\theta_j)}$.\n(9)\nIt represents the average codeword length saved by a subjective probability prediction P(x|0j) (j=1,2,...)\naccording to the semantics of Y [17].\nWhen Y=yj, semantic mutual information becomes semantic Kullback-Leibler (KL) information:\n$I(X;\\theta_j) = \\sum_i P(x|y_i)log\\frac{P(x|\\theta_j)}{P(x)} =\\sum_i P(x_i, y_i) log \\frac{T(\\theta_j|x_i)}{T(\\theta_j)}$.\n(10)\nNote that in the above formula, P(x|yj) is used for averaging and represents the sample distribution. It can be\nthe relative frequency and may not be smooth or continuous. P(x0j) may differ from P(x|yj), meaning that\ninformation needs factual verification.\nFurther, if X=xi, semantic KL information becomes semantic information conveyed by yj about xi:\n$I(x_i;\\theta_j)=log\\frac{P(x|\\theta_j)}{P(x)} =log\\frac{T(\\theta_j | x_i)}{T(\\theta_j)}$.\n(11)"}, {"title": "D. Optimizing Various Learning Functions with Logical Bayes' Inference", "content": "I proposed Logical Bayesian Inference before [13]. Since it is not a Bayesian method, we call it \"Logical Bayes'\nInference\" (LBI) hereafter.\nThe most often used learning function is the likelihood function P(x|0j). We can use the semantic KL formula\n(10) to optimize P(x|0j). Since semantic KL information is equal to the difference between two KL divergences:\n$I(X; \\theta_j)=KL(P(x|y_j)||P(x)) - KL(P(x|y_j)||P(x|\\theta_j))$,\n(13)\nit is easy prove that I(X; 0) reaches its maximum when KL(P(x|yj)||P(x|0j)) = 0 or\n$P(x|\\theta_j)= P(x|y_j)$.\n(14)\nSometimes, we wish to use P(0j|x), the parameterized P(yj|x), as the learning function. Fisher calls P(0j|x) the\ninverse probability (function) [31]. When n=2, we can use a pair of logistic functions as a pair of inverse probability\nfunctions. However, it is not easy to construct P(0j|x), j = 1, 2, ..., n, as n>2 because there is the normalization limit\n\u03a3; \u03a1(0;\\x) = 1 for every x. Nevertheless, there is no limit to truth and similarity functions.\nFrom (4) and (14), we derive the optimized truth function:\n$T^*(\\theta_j | x) = \\frac{P(x|\\theta_j)}{max_x P(x)} = \\frac{P^*(x|\\theta_j)}{P(x)} \\frac{P(x|y_j)}{max_x P(x)} = \\frac{P(x|y_j)}{P(x)} \\frac{P(y_j|x)}{max(P(y_j | x))}$.\n(15)\nP(xyj) above is assumed to be a smooth distribution; otherwise, we can only obtain a smooth T(0j|x) by using the\nfollowing formula:\n$T^*(\\theta_j | x) = arg max_{\\theta_j} \\sum P(x; y_i)log\\frac{T(\\theta_j| x_i)}{T(\\theta_j)}$.\n(16)\nIf we only know P(yj|x) without knowing P(x), we may let P(x)=1/m to obtain T*(0j|x) [13]. We call the above\nmethod (of solving truth or similarity functions from sample distributions) LBI.\nIn the following part of this section, we assume that all likelihood, truth, and similarity functions are optimized\nand a sampling distribution is the same as the corresponding probability distribution. We call\n$c(x, y_j) = P(x|\\theta_j)/P(x)=P(x, y_j)/[P(x)P(y_j)]$\n(17)\nthe relatedness function. Sklar proposed the copula function [32], [33]. The c(x, yj) is also a two-dimensional copula\ndensity function. Since c(x, yj) is proportional to P(yj|x), we can replace P(yj|x) with c(x, yj) in a Bayes' formula as\nfollows:\n$P(x|y_j) = \\frac{P(x)c(x,y_j)}{c(y_j)}, c(y_j) = \\sum_i P(x)c(x, y_j),$.\n(18)\nwhere P(x) may be different from previous P(x). We also call c(x, y) the Bayes' core since P(x, y) = P(x)c(x, y)P(y)."}, {"title": null, "content": "When P(x) is unchanged, P(x|yj)=P(xi)c(x, yj). If P(x) is changed, we have to use (18) to obtain new P(x|yj) and use\nSVB to obtain new P(y) and c(x, y).\nFor given P(x) and some constraints, if the expression of P(y)c(x, y) represents a Shannon channel P(y|x), there\nshould be, for every j,\n$\\sum_i P(x_i,y_j) = \\sum_i P(x)c(x_i, y_j) =1 and \\sum_i P(x_i)c(x_i, y_j)P(y_j) = P(y_j)$.\n(19)\nOtherwise, P(y) and P(y|x) are inappropriate solutions. For example, the results of the E-step in the EM algorithm\nare often inappropriate.\nIf the domains of x and y are the same, then y=x\u2081 becomes an estimate: \"x is about xj.\" In this case,\nT(0;\\x\u2081)=T(x} \\x\u2081)=1 as i=j, and the truth value becomes the symmetric similarity degree:\n$S(x_i | x_j) = \\frac{c(x, y)}{max_x(c(x, y))} = \\frac{c(x_i, y_j)}{c(x, y_j)} = S(x_i | x_j)$.\n(20)\nHowever, truth or membership functions may be asymmetric. Suppose all y similar to y\u012b form a fuzzy subset Oxi\nof V; we have\n$T(\\theta_{xi} | y_i) = \\frac{P(x_i|y)}{max_x(P(x_i, y))} = \\frac{P(y_i|x_i)}{max(P(y_j|x))} = T(\\theta_i | x_i)$.\n(21)\nRecently, truth or similarity functions have also been used as learning functions in deep learning [27]. For\nexample, Oord et al. presented InfoNCE [28] and explicitly pointed out that the learning function is proportional to\nm(x, yj) = P(x|yj)/P(x). The expression in their paper is:\n$f_k (X_{t+k}c_t) \\propto P(X_{t+k} | C_t)/ P(X_{t+k}),$\n(22)\nwhere c\u2081 is the feature vector obtained from previous data, Xt+k is the predictive vector, and fk(x1+k, ct) is a similarity\nfunction (between predicted Xt+k and real Xt+k). The estimated mutual information expressed with a similarity\nfunction is a particular case of semantic mutual information."}, {"title": "E. The Origin of SVB: from R(D) to R(G)", "content": "We call the R(G) function the rate-fidelity function for two reasons. Firstly, verisimilitude [26] or truthlikeness\n[35] has been discussed for a long time by Popper and other philosophers. It indicates the progress from not too true\nto truth or the reduction in distortion. I(x\u2081; 0j) is a proper measure for verisimilitude. Since \"verisimilitude\" and\n\"truthlikeness\" are rare in technical papers, we use a similar and familiar word, \"fidelity\", as \"verisimilitude\".\nSecondly, Shannon first proposed the information rate fidelity criterion [10], [11] and later adopted minimum\ndistortion to represent maximum fidelity. It should conform to Popper and Shannon's original ideas using the\nsemantic information measure to denote fidelity.\nSince the rate-distortion is equal to information rate-distortion, Cover and Thomas [36, Section 10.2] also use\n\"rate-distortion\" as \"information rate-distortion\", not distinguishing the two. For the same reason, we do not\ndifferentiate \"rate-fidelity\" and \"information rate-fidelity\" in this paper.\nWe change the distortion limit d\u2264D for R(D) into I(X; Ye) \u2265 G, and then R(D) becomes the information rate-\nfidelity function R(G) [12], [13]. In this case, we replace dij = d(xi, yj) with Iij = I(xi; 0j).\nIn addition to the given P (x) and G, there are the following limitations:\n$\\sum_i P(x_i | y_j) = 1, j = 1, 2, ..., n$;\n(23)\n$\\sum_j P(y_j) = 1$.\n(24)"}, {"title": null, "content": "We use the Lagrange multiplier method for minimum mutual information (MMI). The Lagrange function is:\n$L(P(y|x), P(y)) = I(X;Y) \u2013 sI(X; Y_o) \u2013 \\mu_1\\sum_j P(x_i;y_j) - \\alpha\\sum_j P(y_j),$.\n(25)\nwhere I(X; Yo) and I(X; Y) are expressed with P(y|x) and P(y) as:\n$I(X; Y_o) = \\sum_i P(x)P(y_j | x_i)logm_{ji}, m_{ji} = T(\\theta_j|x_i) / T(\\theta_j) = P(x_i | \\theta_i)/P(x)$, (26)\n$I(X; Y) = \\sum_j P(x)P(y_j |x_i)[log[P(y_j | x_i) \u2013 log P(y_j)] .$\n(27)\nSince P(yx) and P(y) are interdependent, we alternatively use one as the variation. First, we fix P(y) and let\ndF/dP(yj | xi) = 0, j = 1, 2, ..., n; i = 1, 2, ..., m .\nHence, we derive\n$P(y_j | x_i) = P(y_j |\\lambda, \\lambda_i = \\sum_j P(y_i)m_{ji}^s, i = 1,2,...; j = 1,2,...$\n(28)\nWe can treat mij as an intensive Bayes' core and consider (28) as a generalized Bayes' formula.\nThen, we fix P(y|x) and let\ndF/8P(y) = 0, j = 1, 2, ..., n.\nWe derive\n$P (y) = \\sum_i P(x)P(y_j |x_i)$.\n(29)\nRepeating (28) and (29) as we do for the R(D) function [16, p. 326], we obtain appropriate P(y) and P(y|x).\nWithout this iteration, mijs/di, as c(xi, yj), is not an appropriate Bayes' core or copula density function, and\nP(x|yj)=P(x)mijs/2\u2081 is not normalized. Formulas (28) and (29) are the MMI iteration, letting the Shannon channel\nmatch the semantic channel.\nFinally, we obtain the parameter solution of R(G) (see Fig. 2):\n$G(s) = \\sum_i \\sum_j P(x)P(y_j | x_i)I_{ji}$,\n$R(s) = sG(s) - \\sum_i P(x) log \\lambda_i,$\n(30)"}, {"title": "III. SOLVING LATENT VARIABLES BY USING SVB", "content": ""}, {"title": "A. Approximate Solutions and Optimization Criterions for Latent Variables", "content": "From the information-theoretic perspective, we can view the probability distribution P(x) of observed data as\nthe source and the required solution P(y|x) as the Shannon channel. The usual constraints are P(x|yj) or P(x|0j) (j = 1,\n2, ..., n).\nSolving P(y) for given P(x) and P(xy), we can list m equations:\n$P(x_i|y_1)P(y_1) + P(x_i|y_2)P(y_2) + ... + P(x_i|y_n)P(y_n) = P(x_i), i = 1, 2, ..., m$.\n(32)\nAdding P(y1) + P(v2) + ... + P(yn) = 1, we have a total of m+1 equations. When n=m+1, we may obtain the\nexact solution for P(y). When n>m+1, there are multiple solutions. When n<m+1, there is no solution; however, we\ncan obtain an approximate solution that minimizes the loss according to some criterion.\nIn the following, we consider only cases where n>m+1 and use the maximum information efficiency criterion\nto obtain the approximate solution. If a constraint is T(0j|x) instead of P(x|0j), we can use (3) to obtain P(x|0j) =\nP(x)T(0j|x)/T(0j) as the constraint.\nNote that the constraint functions are distributions over x, such as T(0j|x) and s(x, xj) (j = 1, 2, ..., n), which are\nnot normalized. The functions we need to solve are distributions over y, i.e., P(y|xi) (i = 1, 2, ..., m). P(y|xi) must be\nnormalized. Only P(y|xi) can be placed on the left side of the log for averaging.\nI has considered that since the MMI iteration is to make P(yj|x)\u221dT(0j|x), j = 1, 2, ..., we may let\nP(yj)=T(0;)/\u03a3;T(0) and then obtain P(yj|x)=T(0j|x)P(yj)/T(0j). However, this method cannot guarantee \u2211j P(yj|x) = 1\nfor each x."}, {"title": "B. Explaining and Improving the EM Algorithm for Mixture Models", "content": "We know P(x)=\u2211j P(yj)P(x|yj). Given the sample distribution P(x), we can use Po(x)=\u2211j P(yj)P(x|0j) to\napproximate P(x), ensuring that the relative entropy\n$H(P || P_o) = \\sum P(x)log[P(x_i) / P_o(x)]$\n(33)\napproaches zero. P(y) is the probability distribution of the latent variable y.\nThe EM algorithm initially sets P(x|0j) and P(yj), j = 1, 2, ..., n. Then, in the E-step, we obtain:"}, {"title": null, "content": "$P(y_j|x) = P(y_j)P(x|\\theta_i)/P(x)", "M1-step": "n$P^{+1"}, "y_j) = \\sum_i P(x)P(y_j|x_i),$\n(35)\nand the M2-step:\n$P(x|\\theta_j^{+1}) = P(x)P(y_j |x)/P^{+1}(y_j) = P(x)-\\frac{P(x|\\theta_j) P(y_j)}{P(x) P^{+1}(y_j)}$,\n(36)\nwhich optimizes a group of likelihood functions. For Gaussian mixture models, we use the expectation and standard\ndeviation of the right side as those of the left side.\nThis iterative process continues until the mixture model converges. According to the derivation process of the\ninformation rate-fidelity function, it can be seen that the E-step and M1-step minimize the Shannon mutual\ninformation. According to (14), the M2-step maximizes the semantic mutual information. Therefore, the\noptimization criterion used by the EM algorithm is the maximum information efficiency criterion.\nHowever, there are two problems: 1) P(y) might converge slowly; 2)"]}