{"title": "State-Space Large Audio Language Models", "authors": ["Saurabhchand Bhati", "Yuan Gong", "Leonid Karlinsky", "Hilde Kuehne", "Rogerio Feris", "James Glass"], "abstract": "Large Audio Language Models (LALM) combine the audio perception models and the Large Language Models (LLM) and show a remarkable ability to reason about the input audio, infer the meaning, and understand the intent. However, these systems rely on Transformers which scale quadratically with the input sequence lengths which poses computational challenges in deploying these systems in memory and time-constrained scenarios. Recently, the state-space models (SSMs) have emerged as an alternative to transformer networks.\nWhile there have been successful attempts to replace transformer-based audio perception models with state-space ones, state-space-based LALMs remain unexplored. First, we begin by replacing the transformer-based audio perception module and then replace the transformer-based LLM and propose the first state-space-based LALM. Experimental results demonstrate that space-based LALM despite having a significantly lower number of parameters performs competitively with transformer-based LALMs on close-ended tasks on a variety of datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) [1]\u2013[6] built using the powerful transformer architecture and trained on web-scale text data have made significant progress in Natural Language Processing. LLMs can read, write understand, and reason about the world through text. ChatGPT and similar systems are increasingly becoming commonplace.\nWhile text-based systems have made significant strides, in our day-to-day lives we are surrounded by complex audio signals. Understanding audio is a crucial part of developing the next generation of intelligent systems that can interact with the world. Motivated by this, there have been attempts to supplement the LLMs with the ability to understand audio and speech [7]\u2013[12].\nDespite the success of Transformers [13], they suffer from the quadratic time and memory complexity which is a bottle-neck for long audio, and speech signals and use on devices with low computational resources. While substantial effort has gone into reducing the computational requirements of the transformer architectures [14], [15], there is a need to explore alternative models for use cases with memory and time constraints.\nState-space models (SSMs) have emerged as an alternative to transformer-based models [16]\u2013[19]. SSMs have linear complexity in token length and perform on par with transformers. SSMs demonstrate faster inference time and lower memory requirements than the transformer-based models [18], [20], [21]. SSMs have demonstrated their performance on text [17] and image [18], [21] and are increasingly becoming common for modeling speech and audio signals [19], [22]-\n[24]. Recently, SSMs coupled with knowledge distillation have been shown to outperform transformer-based teachers and achieve state-of-the-art performance on the audio perception task [19].\nWhile there have been works exploring state-space-based audio perception systems, there have been no attempts to explore state-space-based LALMs. In this paper, we system-atically explore the impact of using state-space-based systems in developing LALMs. First, we replace the audio perception module from a transformer-based system, AST, to a state-space-based system, DASS, and keep the transformer-based LLM. Then, we also replace the LLMs with state-space LLMS and propose the (to the best of our knowledge) first state-space LALM. We evaluate the model using a mix of datasets for classification, and caption retrieval tasks and show that the state-space LALM performs competitively with transformer-based LALMs."}, {"title": "II. RELATED WORK", "content": "LLMs trained on wed-scale data with the next token pre-diction task showed impressive reasoning and understanding of world knowledge. These models learn general-purpose representations that can be aligned to desired responses via instruction tuning [25]. Large Audio Language Models extend the capabilities of these models beyond text to include general audio and speech.\nPengi [7] uses hierarchical transformer HTSAT [26] as the audio encoder, CLIP text encoder as the text encoder, and GPT2 [27] as the language model. AudioGPT [11] augments ChatGPT's ability to handle complex audio and speech tasks. AudioGPT analyzes the user prompt and assigns a model based on the prompt for example for speech recognition task, whisper [28] is used whereas for speech enhancement ConvTasNet [29] is used. SALMONN [8] uses dual encoders: a whisper model to extract speech and information about background noises and a BEATs encoder [30] to extract high-level non-speech audio semantics information and LLaMA as the language-model.\nLTU [9] uses AST [31] as the audio encoder and the LLaMA as the language model. LTU outperforms the existing LALMs on the close-ended task and shows free-form open-ended question-answering capabilities. Our approaches build upon LTU and reduce the computational cost of the model by using state-space models while retaining the performance of these models. GAMA [12] is a concurrent approach that builds upon LTU and combines information extracted from various layers from AST via an Audio Q-Former. GAMA contains significantly more trainable parameters (~300M) compared to LTU (~100M) and our proposed models (~40M, ~60M).\nGAMA also proposed and used an improved instruction tuning dataset to improve complex reasoning on the input audio. This work explores alternatives for Transformers backbone and builds computationally efficient audio-language generation systems."}, {"title": "III. STATE-SPACE LARGE AUDIO LANGUAGE MODELS", "content": "Structured state space sequence models (S4) [16] are in-spired by classical state-space models such as Kalman filters and Hidden Markov Models. The state-space models map a 1-D sequence x(t) \u2208 R \u2192 y(t) \u2208 R through a hidden state h(t) \u2208 RN via linear ordinary differential equations as follows:"}, {"title": "A. State-Space Models", "content": "Structured state space sequence models (S4) [16] are in-spired by classical state-space models such as Kalman filters and Hidden Markov Models. The state-space models map a 1-D sequence $x(t) \\in \\mathbb{R} \\rightarrow y(t) \\in \\mathbb{R}$ through a hidden state $h(t) \\in \\mathbb{R}^N$ via linear ordinary differential equations as follows:\n$\\begin{aligned}\nh'(t) &= Ah(t) + Bx(t), \\\\\ny(t) &= Ch(t)\n\\end{aligned}$ \nwhere $A \\in \\mathbb{R}^{N \\times N}$, ($B\\in \\mathbb{R}^{N \\times 1}$,$C \\in \\mathbb{R}^{1 \\times N}$) are called the evolution and projection parameters respectively.\nA discretization step transforms the continuous parameters, A, B to discrete parameters A, B. A commonly used method for discretization, zero-order hold uses a timescale parameter $\\Delta$ to discretize as follows:\n$\\begin{aligned}\n\\overline{A} &= exp(\\Delta A), \\\\\n\\overline{B} &= (\\Delta A)^{-1}(exp(\\Delta A) \u2013 I)\\Delta B\n\\end{aligned}$ \nAfter the discretization step, the state-space equations can written as:\n$\\begin{aligned}\nh_t &= \\overline{A}h_{t-1}+ \\overline{B}x_t \\\\\ny_t &= Ch_t\n\\end{aligned}$ \nThe current view of state-space models is analogous to RNNs where the output of the current time step only depends on the previous hidden state and current input. Since the state-space parameters are not time or input-dependent, SSM can also be viewed as a convolution $Y_t = (x_0, x_1, ..., x_t) * (C\\overline{B}, C\\overline{A}\\overline{B}, ..., C\\overline{A}^{M-1}\\overline{B}) = x * K$ where K is the called the global convolutional kernel.\nSSMs can either be viewed as CNNs or RNNs depending on the task. During training, the convolutional view is used to enable parallel training. During inference, the recurrent view allows faster inference and unbounded context. However, the linear time-invariant nature of these models limits their performance on content-based reasoning tasks.\nGu et al. [17] proposed a parametrization method to make the state-space parameters input-dependent. However, this breaks the convolutional view and poses a challenge for effi-cient computation. To address this, a hardware-aware parallel algorithm is used to efficiently compute the output."}, {"title": "B. DASS: Distilled Audio State-Space Model", "content": "DASS [19] was one of the first attempts to use a pure state-space-based model to classify audio signals. DASS uses AST [31], a transformer-based teacher model, to guide and train a state-space audio classifier. DASS combines the best of both worlds: it outperforms the transformer-based models and retains the computational advantages of state-space models.\nThe DASS model can be divided into four groups and each group consists of a state-space block and the first three groups also contain a downsampling layer. Each group progressively reduces the sequence lengths and increases the feature size. A pooling method generates the final embedding that summarizes the input spectrogram. DASS shows remarkable duration scal-ability: even a model trained with ten-second utterances can infer information from hour-long audio.\nIn this work, we use DASS pretrained on AudioSet-2M [35] dataset with the classification layer removed as the audio features extractor for the input audio. It takes a spectrogram of size 1024*128 as input and generates a feature map of size 32*4*768 as output. To further reduce the spatial dimension, we use a two-dimensional convolution with a kernel size of 3 and stride 2 and then use a linear layer to map the features from 768 dimensions to the input size of the language model i.e. 4096 for the LLaMa and 2560 for the state-space based LLM."}, {"title": "C. LLM", "content": "We use the following LLMs in this work:"}, {"title": "D. Training Objective", "content": "We train our models using the next token prediction task conditioned on the input audio and past tokens. We maximize the following probability, P(xt|x1:t-1, A), by using cross-entropy loss for all the text tokens 1 < t < T in the input text tokens and reference audio. For generation, we use the following settings: Temperature=0.1, Top K=500, and Top P=0.95 with a repetition penalty of 1.1."}, {"title": "E. Experiments", "content": "We train our models on OpenAQA dataset [9]. This dataset contains tuples of Audio, question, answer where the models take Audio and question as input and generate answer as the output.\nWe follow the same training pipeline from LTU for training the models proposed in this paper. We use 4\u00d7 RTX A6000 GPUs for training the models. The hybrid LALM is trained for about 3 days. For the state-space LALM, we can increase the batch size from 4 to 16. We use gradient accumulation to ensure the effective batch size is the same i.e. 256. This speeds up the training and allows us to train the model in less than two days. However, the reason for the training speed up is unclear: it could be simply due to the state-space LLM being smaller or the state-space model being computationally efficient. In the future, when larger state-space LLMs become publicly available, we plan to increase the model size and compare the computational efficiencies of the state-space and Transformer-based LALMs.\nThe small and medium hybrid-LALM contains 42M and 61M trainable parameters out of 6.8B parameters respectively. The small and medium ssLALM contains 43M and 62M train-able parameters out of a total of 2.8B parameters respectively.\nTo reason and understand the audio, the LALM must be able to first recognize the input audio. One important step in bench-marking LALMs is the performance comparison on close-ended tasks where the output labels are predefined. We follow the evaluation pipeline from LTU [9] and compare our models with existing LALMs on 8 audio classification benchmarks and 2 audio captioning benchmarks.\nAudio Classification: LALMs do not directly predict the class index but instead output the audio label names or descriptions. To compute the performance of these models, we first encode the LALM output and the evaluation label using a text encoder and then we compute a cosine similarity between the LALM output and the label. For single-label classification tasks, we use the label with the highest similarity score and compute accuracy or F1-score and for multi-label classification tasks, we use the cosine similarity as the prediction score and compute the mAP. We used the prompt \"write an audio caption describing the sound\" for the classification tasks.\nAudio Captioning: For the caption generation tasks, we use the prompt \"write an audio caption describing the sound\" and take the LALM output as the prediction. We AudioCaps and Clotho datasets and use SPICE as the evaluation metric.\nAs seen in Table 1, our proposed model outperforms the other existing models such as SALMONN, Pengi, and Audio-GPT. For SALMONN [8], Pengi [7], AudioGPT [11], we use the results reported in the GAMA paper. For both the audio classification and audio captioning task our proposed models outperform the existing models in most of the datasets and overall average performance. For the audio captioning task, our models perform similarly to the best-supervised systems.\nOur proposed models match the performance of LTU [9] which our approaches build upon. The audio encoder i.e. AST used in LTU is first pretrained on audio-visual data and then finetuned on the AudioSet-2M dataset. In contrast, the DASS model is trained on the audio data from AudioSet-2M. DASS trained on only AudioSet-2M outperforms and shows more robustness and duration-scalability than AST trained on the same dataset [19].\nThe state-space LLM used in ssLALM is much smaller and is not instruction-trained unlike LLaMA used in hybrid LALMs or transformer LALMs such as LTU. Despite being smaller and having the audio encoder and LLM trained on less data, the ssLALMs perform competitively with the best transformer-based LALMs.\nAlthough all the LALMs, including the ssLALMs, perform poorly on multi-label classification task on AudioSet. We believe it is because LALMs mainly predict the prominent class and underestimate the likelihood of non-prominent sound classes in the input audio which results in low mAP scores. We also show some open-ended question-answering abilities of the models in Table 2. All the models can infer the information from audio and generate reasonable answers."}, {"title": "IV. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we propose the first state-space large au-dio language model. We systematically replace the audio-perception and LLM components in the LALMs and analyze the performance. Our experiments show that ssLALMs per-form competitively with the transformer-based LALMs despite using a significantly lower number of parameters.\nIn the future, we would like to scale up the data used for training the model and more complex reasoning datasets such as CompA-R [12]. We would also like to build the larger ssLALMs with large state-space LLMs. We would also like to explore state-space attention hybrid models such as Jamba as the language models as they combine the best of the state-space and transformers."}]}