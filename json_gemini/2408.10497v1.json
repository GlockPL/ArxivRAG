{"title": "QUITO-X: An Information Bottleneck-based Compression Algorithm with Cross-Attention", "authors": ["Yihang Wang", "Xu Huang", "Bowen Tian", "Yixing Fan", "Jiafeng Guo"], "abstract": "Generative large language models (LLMs) have achieved significant success in various industrial tasks and can effectively adapt to vertical domains and downstream tasks through In-Context Learning (ICL). However, with tasks becoming increasingly complex, the context length required by ICL is also getting longer, and two significant issues arise: (i) The excessively long context leads to high costs and inference delays. (ii) A substantial amount of task-irrelevant information introduced by long contexts exacerbates the \"lost in the middle\" problem.\nRecently, compressing prompts by removing tokens according to some metric obtained from some causal language models, such as llama-7b, has emerged as an effective approach to mitigate these issues. However, the metric used by prior method such as self-information or perplexity (PPL) do not fully align with the objective of distinuishing the most important tokens when conditioning on query. In this work, we introduce information bottleneck theory to carefully examine the properties required by the metric. Inspired by this, we use cross-attention in encoder-decoder architecture as a new metric. Our simple method leads to significantly better performance in smaller models with lower latency.\nWe evaluate our method on four datasets-DROP, COQA, SQUAD, and Quoref. The experimental results show that, while maintaining the same performance, our compression rate can improve by nearly 25% to previous sota. Remarkably, in experiments where 25% of the tokens are removed, our model's Exact Match (EM) score for answers sometimes even exceeds that of the control group using uncompressed text as context.", "sections": [{"title": "Introduction", "content": "In recent years, the rapid development of generative LLMs, such as ChatGPT, has revolutionized many traditional technologies. A critical factor behind their success is their ability to leverage rich contextual information to enhance performance across various tasks. Techniques like ICL (Brown et al. 2020), Retrieval-Augmented Generation (RAG) (Lewis et al. 2020), and the use of agents (Park et al. 2023) have been instrumental in enabling these models to understand and generate contextually relevant content, addressing complex problems through multi-turn dialogues.\nAs tasks become increasingly complex, the importance of context grows. Longer contexts allow LLMs to better capture the nuances and dependencies within the data, leading to more accurate and relevant outputs. However, this benefit comes at a cost.\nWith the increase in context length, however, two major challenges arise: (i) the higher inference cost, especially when using closed-source APIs, and (ii) the introduction of task-irrelevant information, which exacerbates the \"lost in the middle\" problem (Tay et al. 2021), where the model's performance deteriorates as it struggles to maintain focus on the most relevant parts of the context.\nTo mitigate these challenges, one promising approach is context compression. This strategy leverages the inherent redundancy and repetition in natural language (Shannon 1951; Li et al. 2023), aiming to distill the context to its most informative elements while minimizing the loss of crucial information. One such method, the Selective Context (Li et al. 2023) approach, employs self-information as a metric to identify and remove less informative lexical units. This method has demonstrated that context compression can significantly reduce memory usage and improve inference speed with minimal impact on accuracy. Building on this, the LLMLingua series (Jiang et al. 2023b) introduced a more dynamic approach, utilizing perplexity (PPL) as a metric to adaptively compress the context at varying levels of granularity. These advancements highlight the potential of selective compression, though they still face limitations in fully capturing query-specific relevance.\nEven though the LLMLingua series incorporates the query into the PPL calculation, thereby considering the conditional PPL in the query state, PPL still fails to adequately reflect the relevance between the query and various parts of the context. To address this issue, QUITO (Wang et al. 2024) innovatively proposes using attention as a metric to measure the relevance between the query and the context. This method employs a very small model as the compression model, aiming to select intuitively more useful information for the query. This information then holds a higher weight when generating answers.\nAlthough QUITO, which utilizes self-attention, has achieved promising results in information compression, certain limitations have been observed in practical experiments. When using the attention from the final layer, the last few"}, {"title": "Related Work", "content": "Attention Mechanism\nThe Attention Mechanism (Vaswani et al. 2017) is crucial in today's machine learning. It is widely used in many fields including image generation (Rombach et al. 2021), image recognition (Caron et al. 2021), and language processing (Brown et al. 2020; Radford et al. 2019). It starts with an input sequence transformed into query (Q), key (K), and value (V) matrices through linear projections. And the attention matrix is computed as follows:\n$A = \\frac{\\text{softmax}(QK^T)}{\\sqrt{d_k}}$\nAmong these, cross attention is often used as a way of exchanging information between two different types of information, such as between image and text modalities (Rombach et al. 2021; Cho et al. 2021), or for translation between different languages. In practice, Q or query is generally designed based on the type of output that is desired from the model to extract the most important information needed from K and V. In ViT (Dosovitskiy et al. 2020), it is proposed that by observing the attention heatmap, one can determine which NLP token is most important for which part. (Maharjan et al. 2018) uses a constant query \"which representation is most important\" to weight different representations. In our work, we use the T5 model (Chung et al. 2022), with the prompt + query as the KV pair, and the first token of the answer as Q (acting as \"which tokens are most important\" because in normal training, it is necessary to score the token weights to generate answers), to perform cross attention and determine which tokens in the prompt are most important.\nContext Compression\nGenerative LLMs have achieved strong performance across various tasks, but they encounter computational challenges when processing long documents and extended conversations due to increased token counts and context truncation. ICL (Brown et al. 2020) helps mitigate some of these issues by providing task-relevant context directly, reducing the need for specific task Supervised Fine-Tuning (SFT) and lowering costs. However, ICL also increases token numbers and inference costs.\nThe Selective Context method was developed to address these challenges by compressing input context through the removal of redundant information. Li et al. evaluates tokens based on self-information, retaining only the most informative content. This approach reduces token numbers and inference costs while mitigating the \"lost in the middle\" problem (Tay et al. 2021) associated with long contexts. Experiments demonstrate that Selective Context reduces memory usage and generation latency while maintaining comparable performance, especially in tasks involving lengthy documents.\nThe LLMLingua (Jiang et al. 2023b) series, including LongLLMLingua (Jiang et al. 2023a) and LLMLingua2 (Pan et al. 2024), enhance context compression by optimizing context selection strategies and introducing advanced memory network structures. These models aim to improve efficiency and accuracy in processing longer documents and complex dialogues, optimizing context usage without compromising performance.\nQUITO (Wang et al. 2024) introduces self-attention as a metric for context compression, diverging from traditional methods that rely on perplexity or self-information. By concatenating context and query, QUITO uses a small transformer decoder-only model to compute attention, retaining tokens with high attention values.\nHowever, in the self-attention module of QUITO, the query, key, and value are trained together, which primarily aims to obtain a comprehensive global representation, and may lead to information mixing between the input and output. In contrast, we employ cross-attention in an encoder-decoder architecture, where the input and output are separated. We keep the key-value pairs fixed and only train the query representations, which forces the query to learn to distinguish which token representations are most crucial for generating the output."}, {"title": "Method", "content": "Theorem\nThe information bottle-neck(IB) (Tishby, Pereira, and Bialek 1999; Fischer 2020) is a simplistic concept: when facing a task, one should try to accomplish it using the minimal information. In our case, we want to compress the context while retaining the accuray of output answer, which makes it well-suited for modeling using IB theory. So we can use the IB score to formulate our objective as follows:\n$\\min_X IB = I(X; \\hat{X} | Q) - \\beta I(\\hat{X}; Y | Q)$ (1)\nwhere $\\hat{X}$ stands for compressed context, Q stands for query, while Y stands for output. $\\hat{X} = X_1 X_2 X_3...X_n$, in which $x_i$ is the ith token, and $\\beta$ is related to compressed ratio. The first item serves to enhance efficiency while the second term serves to retain as much useful information as possible to enable the LLM generate target outputs.\nNotice that since our compress method is to delete some token from X to get $\\hat{X}$, so for a fixed compressed radio, the first item $I(X; \\hat{X} | Q)$ for efficiency can be ignored. Then our objective is to maximize $I(\\hat{X}; Y | Q)$\nUsing the chain rule of Mutual Information and r-Markov properties, we have\n$I_Q(X; Y) = I_Q(x_1; Y) + ... + I_Q(X_n; Y | x_1, x_2...x_{n-1})$ (2)\n$\\approx I_Q(x_1; Y) + ... + I_Q(x_n; Y | X_{n-1}, ..., X_{n-r+1})$ (3)\n(where $I_Q$ stands for conditioning on Q). So, for a fixed compressed radio, we can calculate the exact number k such that we need to delete k tokens while maximizing the mutual Information. We define $I(x_i; Y | X_1, ..., X_{i-1}, Q)$ as the conditional mutual information of the ith term, noting that the ith token, as well as the r \u2212 1 tokens preceding it, are related to the ith term. Therefore, to delete k tokens, we can consider finding the k terms with the smallest conditional mutual information."}, {"title": "Algorithm", "content": "According to the above analysis, we need a metric to reflect the relative size of $I_Q(x_i; Y | X_{i-1}, ... X_{i-r+1})$. It should satisfy the following properties:\n1.  Conditioning on Q, and it can capture the information from previous $X_i$\n2.  Under the above condition, it can also reflect the importance or similarity of $x_i$ to the output Y\nWe found that cross-attention in the encoder-decoder architecture can effectively satisfy the above properties. Specifically, we put the context (corresponding to X) and the query (corresponding to Q) together in the encoder. Through self-attention, we obtain a representation of the context that can capture the previous information and query information, which we use as the key-value (KV) pair for cross-attention (corresponding to Property 1). Then, we use the first token in the decoder marked as the start as the query (q), performing cross-attention with the KV pair. At this time, the first token in the decoder as q can be considered a constant query: 'which representations are most important to generate output' (corresponding to Property 2). Therefore, the attention scores obtained from the cross-attention between q and the KV pair can be used to represent the relative size of the conditional mutual information in Equation (2).\nFrom Property (1), we need to ensure that after removing some tokens, we can still capture the information preceding $X_i$. According to the r-Markov assumption, we should retain ther tokens preceding those important tokens. In practice, we found that adding a Gaussian filter, which is a softer implementation, works better and is easier to implement than the hard constraints that directly retaining the preceding r tokens. The detailed process is presented in the following Algorithm section.\nIn practice, we use the word as the smallest unit of compression, following the work of previous researchers (Li et al. 2023). We utilize the T5 model's cross-attention mechanism to evaluate the importance of each word in the context relative to a given query. The procedure is as follows:\n1.  Concatenation of Context and Query: The context and query are concatenated to form a single input sequence. This sequence is fed into the T5 model's encoder to generate a feature representation.\n2.  Encoding and Decoding: Leveraging the T5 model's encoder-decoder architecture, the concatenated input is encoded, and the model begins decoding from a start token [start]. During this decoding process, the model generates output tokens while computing cross-attention scores that highlight the importance of each token in the context with respect to the generated output.\n3.  Get the Cross-Attention Scores: Cross-attention weights from the final layer are extracted, specifically focusing on tokens corresponding to the context. The weights from each attention head are averaged to obtain a more robust representation. These averaged weights are then normalized using a softmax function to produce attention-based importance scores for each token.\n4.  Smoothing with Gaussian Filter: To ensure that tokens adjacent to those with high attention scores also receive appropriate attention, we apply a Gaussian filter to the attention scores. This smoothing process helps to distribute the attention more evenly across nearby tokens, enhancing the model's ability to capture relevant context. The Gaussian smoothing process is mathematically defined as:\n$S_t = \\frac{1}{K \\sqrt{2\\pi \\sigma^2}} \\sum_{k=-K}^{K} S_{t+k} \\exp{\\left( -\\frac{k^2}{2 \\sigma^2} \\right)}$ (4)\nwhere $S_t$ is the smoothed attention score for token t, $S_{t+k}$ represents the attention score for neighboring tokens, \u03c3 is the standard deviation that controls the smoothness, and K is the window size for smoothing.\n5.  Reconstruction Based on Word Importance: Based on our practice that words are the smallest semantic units, the reconstructed context is derived by considering the importance of each word. The 'reconstruct' function groups tokens into words and then re-evaluates their importance using the smoothed attention scores. This approach ensures that the semantic integrity of the text is preserved during compression.\n6.  Context Compression: Finally, the context is compressed by selectively retaining words with the highest importance scores. The compression ratio can be adjusted to control the level of detail retained in the compressed context, balancing between context size and information retention.\nFormally, after obtaining the attention scores for each token, we filter words as follows:\n$score_w(word) = \\sum_{t \\in word_i} score_t(token_t)$ (5)\n$filtered\\_words = words [topk(score_w)]$ (6)\nHere, $score_w(word)$ is the cumulative attention score for each word, calculated by summing the attention scores of all tokens within the word. The words are then filtered by selecting the top-ranked ones based on their cumulative scores, ensuring that the selected words collectively satisfy the desired ratio of the total token count. Figure 1 2provides an overview of our algorithm.\nThis approach, by focusing on word-level importance via cross-attention scores, allows for effective compression of long contexts while maintaining essential information. This is particularly useful for tasks that require handling extensive text, where preserving critical details is crucial for maintaining model performance.\nBy focusing on the cross-attention mechanism of the T5 model, our method effectively identifies the most relevant information in a context, enabling efficient compression without significant loss of critical information. This approach is particularly beneficial in scenarios involving long contexts, where the preservation of key information is paramount for maintaining high accuracy in downstream tasks."}, {"title": "Experiments", "content": "Main Experiment: Context Compression on Standard QA Datasets\nDatasets and Models. For our main experiment, we utilized the FLAN-T5-small model (Chung et al. 2022) as the compression model. We conducted this experiment on four widely used QA datasets: CoQA (Reddy, Chen, and Manning 2019), Quoref (Dasigi et al. 2019), DROP (Dua et al. 2019), and SQuAD (Rajpurkar et al. 2016). This experiment aimed to evaluate the effectiveness of our context compression technique on both accuracy and information retention. Specifically:\n\u2022\tFor CoQA and Quoref, we assessed the accuracy of question-answering models (LongChat-13B-16k (Dacheng Li* and Zhang 2023) and LLaMA3-8B-Instruct (AI@Meta 2024)) before and after context compression.\n\u2022\tFor DROP and SQUAD, we focused on whether key in-"}, {"title": "Conclusion", "content": "In this paper, we endeavor to address the challenge of query-based context compression in Retrieval-Augmented Generation (RAG) scenarios. Leveraging the information bottleneck theory, we meticulously analyzed the properties required for metrics that measure token importance. Our approach employs cross-attention and achieves state-of-the-art (SOTA) results across several commonly used datasets. Notably, our method demonstrates superior performance on long texts, sometimes even outperforming the original, which may be attributed to the redundancy inherent in natural language. Our model significantly surpasses strong baselines in both inference latency and performance. The effectiveness of our chunking strategy for longer texts, as well as the reasons behind the exceptional performance of cross-attention, are left for future exploration."}]}