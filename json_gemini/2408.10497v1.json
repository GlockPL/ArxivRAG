{"title": "QUITO-X: An Information Bottleneck-based Compression Algorithm with Cross-Attention", "authors": ["Yihang Wang", "Xu Huang", "Bowen Tian", "Yixing Fan", "Jiafeng Guo"], "abstract": "Generative large language models (LLMs) have achieved significant success in various industrial tasks and can effectively adapt to vertical domains and downstream tasks through In-Context Learning (ICL). However, with tasks becoming increasingly complex, the context length required by ICL is also getting longer, and two significant issues arise: (i) The excessively long context leads to high costs and inference delays. (ii) A substantial amount of task-irrelevant information introduced by long contexts exacerbates the \"lost in the middle\" problem.\nRecently, compressing prompts by removing tokens according to some metric obtained from some causal language models, such as llama-7b, has emerged as an effective approach to mitigate these issues. However, the metric used by prior method such as self-information or perplexity (PPL) do not fully align with the objective of distinuishing the most important tokens when conditioning on query. In this work, we introduce information bottleneck theory to carefully examine the properties required by the metric. Inspired by this, we use cross-attention in encoder-decoder architecture as a new metric. Our simple method leads to significantly better performance in smaller models with lower latency.\nWe evaluate our method on four datasets-DROP, COQA, SQUAD, and Quoref. The experimental results show that, while maintaining the same performance, our compression rate can improve by nearly 25% to previous sota. Remarkably, in experiments where 25% of the tokens are removed, our model's Exact Match (EM) score for answers sometimes even exceeds that of the control group using uncompressed text as context.", "sections": [{"title": "Introduction", "content": "In recent years, the rapid development of generative LLMs, such as ChatGPT, has revolutionized many traditional technologies. A critical factor behind their success is their ability to leverage rich contextual information to enhance performance across various tasks. Techniques like ICL (Brown et al. 2020), Retrieval-Augmented Generation (RAG) (Lewis et al. 2020), and the use of agents (Park et al. 2023) have been instrumental in enabling these models to understand and generate contextually relevant content, addressing complex problems through multi-turn dialogues.\nAs tasks become increasingly complex, the importance of context grows. Longer contexts allow LLMs to better capture the nuances and dependencies within the data, leading to more accurate and relevant outputs. However, this benefit comes at a cost.\nWith the increase in context length, however, two major challenges arise: (i) the higher inference cost, especially when using closed-source APIs, and (ii) the introduction of task-irrelevant information, which exacerbates the \"lost in the middle\" problem (Tay et al. 2021), where the model's performance deteriorates as it struggles to maintain focus on the most relevant parts of the context.\nTo mitigate these challenges, one promising approach is context compression. This strategy leverages the inherent redundancy and repetition in natural language (Shannon 1951; Li et al. 2023), aiming to distill the context to its most informative elements while minimizing the loss of crucial information. One such method, the Selective Context (Li et al. 2023) approach, employs self-information as a metric to identify and remove less informative lexical units. This method has demonstrated that context compression can significantly reduce memory usage and improve inference speed with minimal impact on accuracy. Building on this, the LLMLingua series (Jiang et al. 2023b) introduced a more dynamic approach, utilizing perplexity (PPL) as a metric to adaptively compress the context at varying levels of granularity. These advancements highlight the potential of selective compression, though they still face limitations in fully capturing query-specific relevance.\nEven though the LLMLingua series incorporates the query into the PPL calculation, thereby considering the conditional PPL in the query state, PPL still fails to adequately reflect the relevance between the query and various parts of the context. To address this issue, QUITO (Wang et al. 2024) innovatively proposes using attention as a metric to measure the relevance between the query and the context. This method employs a very small model as the compression model, aiming to select intuitively more useful information for the query. This information then holds a higher weight when generating answers.\nAlthough QUITO, which utilizes self-attention, has achieved promising results in information compression, certain limitations have been observed in practical experiments. When using the attention from the final layer, the last few"}, {"title": "Related Work", "content": "The Attention Mechanism (Vaswani et al. 2017) is crucial\nin today's machine learning. It is widely used in many fields\nincluding image generation (Rombach et al. 2021), image\nrecognition (Caron et al. 2021), and language processing\n(Brown et al. 2020; Radford et al. 2019). It starts with an in-\nput sequence transformed into query (Q), key (K), and value\n(V) matrices through linear projections. And the attention\nmatrix is computed as follows:\n$A = softmax(\\frac{QK^T}{\\sqrt{d_k}})$\nAmong these, cross attention is often used as a way of ex-\nchanging information between two different types of infor-\nmation, such as between image and text modalities (Rom-\nbach et al. 2021; Cho et al. 2021), or for translation be-\ntween different languages. In practice, Q or query is gener-\nally designed based on the type of output that is desired from\nthe model to extract the most important information needed\nfrom K and V. In ViT (Dosovitskiy et al. 2020), it is proposed\nthat by observing the attention heatmap, one can determine\nwhich NLP token is most important for which part. (Ma-\nharjan et al. 2018) uses a constant query \"which representa-\ntion is most important\" to weight different representations.\nIn our work, we use the T5 model (Chung et al. 2022), with\nthe prompt + query as the KV pair, and the first token of the\nanswer as Q (acting as \"which tokens are most important\"\nbecause in normal training, it is necessary to score the token\nweights to generate answers), to perform cross attention and\ndetermine which tokens in the prompt are most important.\nGenerative LLMs have achieved strong performance across\nvarious tasks, but they encounter computational challenges\nwhen processing long documents and extended conversa-\ntions due to increased token counts and context truncation.\nICL (Brown et al. 2020) helps mitigate some of these is-\nsues by providing task-relevant context directly, reducing\nthe need for specific task Supervised Fine-Tuning (SFT) and\nlowering costs. However, ICL also increases token numbers\nand inference costs.\nThe Selective Context method was developed to address\nthese challenges by compressing input context through the\nremoval of redundant information. Li et al. evaluates tokens\nbased on self-information, retaining only the most informa-\ntive content. This approach reduces token numbers and in-\nference costs while mitigating the \"lost in the middle\" prob-\nlem (Tay et al. 2021) associated with long contexts. Exper-\niments demonstrate that Selective Context reduces memory\nusage and generation latency while maintaining compara-\nble performance, especially in tasks involving lengthy doc-\numents.\nThe LLMLingua (Jiang et al. 2023b) series, including\nLongLLMLingua (Jiang et al. 2023a) and LLMLingua2\n(Pan et al. 2024), enhance context compression by optimiz-\ning context selection strategies and introducing advanced\nmemory network structures. These models aim to improve"}, {"title": "Theorem", "content": "The information bottle-neck(IB) (Tishby, Pereira, and\nBialek 1999; Fischer 2020) is a simplistic concept: when\nfacing a task, one should try to accomplish it using the min-\nimal information. In our case, we want to compress the con-\ntext while retaining the accuray of output answer, which\nmakes it well-suited for modeling using IB theory. So we\ncan use the IB score to formulate our objective as follows:\n$min_{X} IB = I(X; X|Q) \u2013 \\beta I(X; Y|Q)$                                                                      (1)\nwhere X stands for compressed context, Q stands for\nquery, while Y stands for output. $X = X_1X_2X_3...X_n$, in\nwhich $x_i$ is the ith token, and $\\beta$ is related to compressed\nratio. The first item serves to enhance efficiency while the\nsecond term serves to retain as much useful information as\npossible to enable the LLM generate target outputs.\nNotice that since our compress method is to delete some\ntoken from X to get X, so for a fixed compressed radio, the\nfirst item $I(X; X|Q)$ for efficiency can be ignored. Then our\nobjective is to maximize $I(X; Y|Q)$\nUsing the chain rule of Mutual Information and r-Markov\nproperties, we have\n$I_Q(X; Y) = I_Q(x_1;Y) + ... + I_Q(x_n; Y|X_1, X_2...X_{n-1})$                                 (2)\n$\\approx I_Q(x_1; Y) + ... + I_Q(x_n; Y|X_{n-1}, ..., X_{n-r+1})$                                               (3)\n(where $I_Q$ stands for conditioning on Q). So, for a fixed\ncompressed radio, we can calculate the exact number k such\nthat we need to delete k tokens while maximizing the mu-\ntual Information. We define $I(x_i; Y|X_1, ..., X_{i\u22121}, Q)$ as the\nconditional mutual information of the ith term, noting that\nthe ith token, as well as the $r - 1$ tokens preceding it, are\nrelated to the ith term. Therefore, to delete k tokens, we can\nconsider finding the k terms with the smallest conditional\nmutual information."}, {"title": "Method", "content": "According to the above analysis, we need a metric to re-\nflect the relative size of $I_Q(x_i; Y|X_{i\u22121}, ...X_{i-r+1})$. It should\nsatisfy the following properties:\n1.  Conditioning on Q, and it can capture the information\n    from previous $X_i$\n2.  Under the above condition, it can also reflect the im-\n    portance or similarity of $x_i$ to the output Y\nWe found that cross-attention in the encoder-decoder archi-\ntecture can effectively satisfy the above properties. Specif-\nically, we put the context (corresponding to X) and the\nquery (corresponding to Q) together in the encoder. Through\nself-attention, we obtain a representation of the context that\ncan capture the previous information and query information,\nwhich we use as the key-value (KV) pair for cross-attention\n(corresponding to Property 1). Then, we use the first token in\nthe decoder marked as the start as the query (q), performing\ncross-attention with the KV pair. At this time, the first to-\nken in the decoder as q can be considered a constant query:\n'which representations are most important to generate out-\nput' (corresponding to Property 2). Therefore, the attention\nscores obtained from the cross-attention between q and the\nKV pair can be used to represent the relative size of the con-\nditional mutual information in Equation (2).\nFrom Property (1), we need to ensure that after removing\nsome tokens, we can still capture the information preceding\n$X_i$. According to the r-Markov assumption, we should retain\nthe $r$ tokens preceding those important tokens. In practice,\nwe found that adding a Gaussian filter, which is a softer im-\nplementation, works better and is easier to implement than\nthe hard constraints that directly retaining the preceding $r$\ntokens. The detailed process is presented in the following\nAlgorithm section.\nIn practice, we use the word as the smallest unit of compres-\nsion, following the work of previous researchers (Li et al.\n2023). We utilize the T5 model's cross-attention mechanism\nto evaluate the importance of each word in the context rela-\ntive to a given query. The procedure is as follows:\n1.  The context and\nquery are concatenated to form a single input sequence.\nThis sequence is fed into the T5 model's encoder to gen-\nerate a feature representation.\n2.  Leveraging the T5 model's\nencoder-decoder architecture, the concatenated input is\nencoded, and the model begins decoding from a start\ntoken [start]. During this decoding process, the model\ngenerates output tokens while computing cross-attention\nscores that highlight the importance of each token in the\ncontext with respect to the generated output.\n3.  Cross-attention\nweights from the final layer are extracted, specifically\nfocusing on tokens corresponding to the context. The\nweights from each attention head are averaged to obtain\na more robust representation. These averaged weights\nare then normalized using a softmax function to produce\nattention-based importance scores for each token."}, {"title": "Algorithm", "content": "4. Smoothing with Gaussian Filter: To ensure that tokens\nadjacent to those with high attention scores also receive\nappropriate attention, we apply a Gaussian filter to the at-\ntention scores. This smoothing process helps to distribute\nthe attention more evenly across nearby tokens, enhanc-\ning the model's ability to capture relevant context. The\nGaussian smoothing process is mathematically defined\nas:\n$S_t = \\frac{1}{Z} \\sum_{k = -K}^{K} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp(-\\frac{k^2}{2\\sigma^2})S_{t+k}$                                                                 (4)\nwhere $S_t$ is the smoothed attention score for token t, $S_{t+k}$\nrepresents the attention score for neighboring tokens, $\\sigma$ is\nthe standard deviation that controls the smoothness, and\nK is the window size for smoothing.\n5. Reconstruction Based on Word Importance: Based\non our practice that words are the smallest semantic\nunits, the reconstructed context is derived by consider-\ning the importance of each word. The \u2018reconstruct' func-\ntion groups tokens into words and then re-evaluates their\nimportance using the smoothed attention scores. This ap-\nproach ensures that the semantic integrity of the text is\npreserved during compression.\n6. Context Compression: Finally, the context is com-\npressed by selectively retaining words with the highest\nimportance scores. The compression ratio can be ad-\njusted to control the level of detail retained in the com-\npressed context, balancing between context size and in-\nformation retention.\nFormally, after obtaining the attention scores for each to-\nken, we filter words as follows:\n$score_w(word) = \\sum_{t \\in word_i} score_t(token_t)$                                                                       (5)\n$filtered_words = words [topk(score_w)]$                                                                        (6)\nHere, $score_w(word)$ is the cumulative attention score for\neach word, calculated by summing the attention scores of all\ntokens within the word. The words are then filtered by se-\nlecting the top-ranked ones based on their cumulative scores,\nensuring that the selected words collectively satisfy the de-\nsired ratio of the total token count. Figure 1 2provides an\noverview of our algorithm.\nThis approach, by focusing on word-level importance via\ncross-attention scores, allows for effective compression of\nlong contexts while maintaining essential information. This\nis particularly useful for tasks that require handling ex-\ntensive text, where preserving critical details is crucial for\nmaintaining model performance.\nBy focusing on the cross-attention mechanism of the T5\nmodel, our method effectively identifies the most relevant in-\nformation in a context, enabling efficient compression with-\nout significant loss of critical information. This approach is\nparticularly beneficial in scenarios involving long contexts,\nwhere the preservation of key information is paramount for\nmaintaining high accuracy in downstream tasks."}, {"title": "Experiments", "content": "Datasets and Models. For our main experiment, we uti-\nlized the FLAN-T5-small model (Chung et al. 2022) as the\ncompression model. We conducted this experiment on four\nwidely used QA datasets: CoQA (Reddy, Chen, and Man-\nning 2019), Quoref (Dasigi et al. 2019), DROP (Dua et al.\n2019), and SQuAD (Rajpurkar et al. 2016). This experiment\naimed to evaluate the effectiveness of our context compres-\nsion technique on both accuracy and information retention.\nSpecifically:\n\u2022  we assessed the accu-\nracy of question-answering models (LongChat-13B-16k\n(Dacheng Li* and Zhang 2023) and LLaMA3-8B-\nInstruct (AI@Meta 2024)) before and after context com-\npression.\n\u2022 For DROP and SQUAD, we focused on whether key in-"}, {"title": "Conclusion", "content": "In this paper, we endeavor to address the challenge of query-based context compression in Retrieval-Augmented Gener-ation (RAG) scenarios. Leveraging the information bottle-neck theory, we meticulously analyzed the properties re-quired for metrics that measure token importance. Our ap-proach employs cross-attention and achieves state-of-the-art (SOTA) results across several commonly used datasets. Notably, our method demonstrates superior performance on long texts, sometimes even outperforming the original, which may be attributed to the redundancy inherent in natu-ral language. Our model significantly surpasses strong base-lines in both inference latency and performance. The effec-tiveness of our chunking strategy for longer texts, as well as the reasons behind the exceptional performance of cross-attention, are left for future exploration."}]}