{"title": "PolarQuant: Quantizing KV Caches with Polar Transformation", "authors": ["Insu Han", "Praneeth Kacham", "Amin Karbasi", "Vahab Mirrokni", "Amir Zandieh"], "abstract": "Large language models (LLMs) require significant memory to store Key-Value (KV) embeddings in their KV cache, especially when handling long-range contexts. Quantization of these KV embeddings is a common technique to reduce memory consumption. This work introduces PolarQuant, a novel quantization method employing random preconditioning and polar transformation. Our method transforms the KV embeddings into polar coordinates using an efficient recursive algorithm and then quantizes resulting angles. Our key insight is that, after random preconditioning, the angles in the polar representation exhibit a tightly bounded and highly concentrated distribution with an analytically computable form. This nice distribution eliminates the need for explicit normalization, a step required by traditional quantization methods which introduces significant memory overhead because quantization parameters (e.g., zero point and scale) must be stored in full precision per each data block. PolarQuant bypasses this normalization step, enabling substantial memory savings. The long-context evaluation demonstrates that PolarQuant compresses the KV cache by over \u00d74.2 while achieving the best quality scores compared to the state-of-the-art methods.", "sections": [{"title": "Introduction", "content": "Transformer-based models form the backbone of modern artificial intelligence systems and have been instrumental in driving the ongoing AI revolution. Their applications span various domains, including frontier language models (LLM) [1, 3, 15] to text-to-image [32, 12, 28], text-to-video synthesis [16, 30], coding assistants [27] and even multimodal models that ingest text, audio, image, and video data [29, 15]. The self-attention mechanism [37] is at the heart of these models as it enables capturing the direct dependencies of all tokens in the input sequence. The ability of these models grows along with their size and context length [21], which leads to computational challenges in terms of huge memory consumption to support fast inference."}, {"title": "Preliminaries", "content": "We use boldface lowercase letters, such as x and y, to denote vectors, and boldface uppercase letters, like M, to denote matrices. To denote a slice of a vector a between the coordinate indices i and j inclusive of the endpoints, we use the notation xij. For a matrix M, we write M\u00bf,: to denote its i-th row vector, which we will simply refer to as Mi."}, {"title": "Efficient Token Generation and KV Caching", "content": "Autoregressive Transformers often utilize cache storage for faster token generation. Given an input prompt, models encode the prompt information into two types of embeddings, called Key and Value. To generate subsequence tokens efficiently, the Key-Value (KV) embeddings are cached to avoid recomputing them.\nThe Key-Value (KV) caching method leverages the architecture of transformer decodcers, where a causal mask in applied in the attention mechanism. Once the keys and values are computed for a given token, they remain unchanged for subsequent token generation. By caching these key-value pairs, the model avoids redundant computations, as it only needs to compute the query for the current token and reuse the cached keys and values for attention.\nThis approach significantly reduces computation time during token generation. Instead of processing the entire sequence repeatedly, the KV cache enables the model to efficiently focus on the incremental computation of new tokens. This makes the method particularly useful in real-time applications, such as conversational AI and text generation, where fast and resource-efficient inference is critical."}, {"title": "Contributions", "content": "We propose quantizing KV vectors in polar coordinates instead of the usual Cartesian coordinates. This shift enables more efficient representation and compression of KV embeddings.\nRandom Preconditioning. We apply a random rotation to the vectors before quantization, which preserves inner products while randomizing the distribution of each vector. This preconditioning causes the angles in polar coordinates to concentrate, allowing us to quantize them with high precision using small bit-widths. We derive the analytical distribution of angles after preconditioning and leverage this insight to construct an optimized quantization codebook, minimizing quantization error.\nRecursive Polar Transformation. We introduce a computationally efficient recursive polar transformation that converts vectors into polar coordinates, enabling practical deployment of our approach. We are able to prove an error bound in Theorem 1 showing our algorithm is asymptotically optimal for worst-case KV embedding vectors.\nPerformance on Long-Context Tasks. We evaluate PolarQuant on long-context tasks and demonstrate that it achieves the best quality scores compared to competing methods while compressing the KV cache memory by over \u00d74.2."}, {"title": "PolarQuant", "content": "We now describe our approach of quantizing angles in polar coordinates and using it to the KV cache problem. In Section 3.1, we introduce how to recursively transform Cartesian vector to polar coordinates. In Section 3.2, we provide an analysis of polar angle distributions with preconditioning. In Section 3.3, we explain details of quantization polar transformed embeddings and practical implementation."}, {"title": "Recursive Polar Transformation", "content": "There are various methods to derive the polar representation of $R^d$. Here we propose a polar transformation that can be recursively computed from the Cartesian coordinates of points in $R^d$. Throughout this work, we assume that d is an integer power of 2.\nAt a high level, our approach begins by grouping pairs of coordinates of a d-dimensional vector x and transforming each pair into 2D polar coordinates. This produces d/2 radius and angle pairs. Next, we gather d/2 of radii and apply the polar transform to them. This procedure is recursively repeated log2 d times and the final output consists of a single final radius and a collection of 1, 2, 4, . . ., d/2-dimensional angle vectors. A formal definition is provided in Definition 1.\nDefinition 1 (Cartesian to Polar Transformation). For any integer power of two d, the polar representation of any vector $x \\in R^d$ includes d \u2013 1 angles and a radius. Angles are organized into a collection of log2d vector of angles $\\psi^{(1)}, \\psi^{(2)},...\\psi^{(log_2d)}$ such that $\\psi^{(1)} \\in [0,2\\pi)^{d/2}$ and $\\psi^{(l)} \\in [0, \\pi/2]^{d/2^l}$ for any l > 2. In other words, the angles are computed in log2 d levels and there are $d/2^l$ angles in level l. These angles are defined by the following relation for $l \\in \\{2, 3, ... log_2 d\\}$:\n$\\psi_j^{(1)} := tan^{-1} (x_{2j}/x_{2j-1}) \\text{  for } j\\in [d/2]$,\n$\\psi_j^{(l)}:= tan^{-1} \\frac{||x_{(j-1/2)2^l+1:j2^l}||_2}{||x_{(j-1)2^l+1:(j-1/2)2^l}||_2} \\text{ for } j \\in [d/2]$"}, {"title": "Distribution of Polar Angles Under Random Preconditioning", "content": "One of our primary objectives is to eliminate the need for explicit normalization (e.g., minimum/maximum values) of the KV cache data prior to quantization, thereby reducing quantization overhead. To achieve this, our algorithm applies random preconditioning to the embedding vectors. This preconditioning involves multiplying each embedding vector by a shared random sketch matrix S with i.i.d. normal entries. By the Johnson-Lindenstrauss (JL) lemma [10], this preconditioning preserves the norms and inner products of the embedding vectors with minimal distortion. A key property of this preconditioning, which we will leverage in our later analysis, is that the embedding vectors after preconditioning follow a multivariate normal distribution. This has been formalized in Fact 3.\nDuring the preconditioning stage, the sketch is applied to all embedding vectors in the KV cache, allowing the analysis of PolarQuant to effectively treat the vectors being quantized as samples from a multivariate normal distribution. So for the analysis and design of PolarQuant we can assume without loss of generality that our goal is to quantize a random vector with multivariate Gaussian distribution. A critical insight is that the distribution of angles after random preconditioning becomes predictable and can be analytically derived, which enables the design of optimal quantization schemes.\nThe polar distribution of a Gaussian vector is derived in the following lemma.\nLemma 2 (Distribution of a Gaussian Vector Under Polar Transformation). For an integer power of two d, suppose that $x \\sim N(0,I_d)$ is a random zero mean isotropic Gaussian random variable in dimension d. Let $ \\alpha(x) := (\\psi^{(1)}, \\psi^{(2)}, ... \\psi^{(log_2d)})$ denote the set of polar angles obtained by applying the polar transformation defined in Definition 1 on x. Denote the radius of x by r = ||x||2. The joint probability density function for $(r, \\psi^{(1)}, \\psi^{(2)}, ... \\psi^{(log_2d)})$ is the following:\n$f_{r,\\psi_\\alpha}(r, \\alpha(x)) = f_R(r) \\cdot \\prod_{l=1}^{log_2 d} f_{\\psi^{(l)}}(\\psi^{(l)}),                                                                                                                                                                                  (1)$\nwhere $f_R(r)$ is the p.d.f. defined in Fact 1, $f_{\\psi^{(1)}}$ is p.d.f. of the uniform distribution over $[0, 2\\pi)^{d/2}$:\n$f_{\\psi^{(1)}}: [0, 2\\pi)^{d/2} \\rightarrow (2\\pi)^{-d/2}$"}, {"title": "Polar Quant Algorithm and Main Theorem", "content": "PolarQuant starts by first applying random preconditioning, then transforming the vectors into polar coordinates, and finally quantizing each angle. Since Lemma 2 shows that the angles in polar coordinates are independent random variables, each angle can be quantized independently to minimize the total mean squared error. Jointly quantizing multiple angle coordinates offers no additional benefit due to their independence, making our approach both computationally efficient and effective. Therefore, we can focus on one angle at level 1 and design optimal quantization scheme for it so as to minimize the mean squared error.\nConsider an angle $\\psi_i^{(l)}$ at some level l. According to Lemma 2, its values lie within the range [0, \u03c0/2] for l > 2 and for l = 1 it takes values in the range [0, 2\u03c0) with a probability density function given by $f_{\\psi^{(l)}}(\\psi_i^{(l)}) := \\Gamma(2^{l-1})/\\prod \\pi 2^{2^{l-1}-2} . \\Gamma (2^{l-2})^2 sin^{2^{l-1}-1} (2\\psi_i^{(l)})$. The goal of quantization to b-bits is to partition the range [0, \u03c0/2] (or [0, 2\u03c0) in case of l = 1) into $2^b$ intervals $I_0^{(l)}, I_1^{(l)}, ... I_{2^b}^{(l)}$ and find corresponding centroids $\\theta_0^{(l)}, \\theta_1^{(l)}, \\theta_{2^b}^{(l)}$ such that the following is mean squared error is minimized:\n$\\mathbb{E} \\[\\sum_{\\psi \\sim f_{\\psi^{(l)}}} \\sum_{j \\in \\[2^b\\]: \\psi \\in I_j^{(l)}} \\[ \\psi - \\theta_j^{(l)} \\]^2\\]                                                                                                                                                                                  (4)$\nThis problem is a continuous analog of the k-means clustering problem in dimension 1. Since we have an explicit formula for the p.d.f. of angle $\\psi_i^{(l)} \\sim f_{\\psi^{(l)}}(\\psi_i^{(l)}) = \\Gamma(2^{l-1})/\\prod \\pi 2^{2^{l-1}-2} . \\Gamma (2^{l-2})^2 sin^{2^{l-1}-1} (2\\psi_i^{(l)})$ the optimal interval partitions and centroids for Eq. (4) can be efficiently computed using numerical"}, {"title": "Experiments", "content": "All experiments are performed with a single NVIDIA RTX A6000 GPU with 48GB VRAM."}]}