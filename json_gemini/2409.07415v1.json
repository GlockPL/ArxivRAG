{"title": "SoK: Security and Privacy Risks of Medical AI", "authors": ["Yuanhaur Chang", "Han Lius", "Evin Jaff", "Chenyang Lu", "Ning Zhang"], "abstract": "The integration of technology and healthcare has ushered in a new era where software systems, powered by artificial intelligence and machine learning, have become essential components of medical products and services. While these advancements hold great promise for enhancing patient care and healthcare delivery efficiency, they also expose sensitive medical data and system integrity to potential cyberattacks. This paper explores the security and privacy threats posed by AI/ML applications in healthcare. Through a thorough examination of existing research across a range of medical domains, we have identified significant gaps in understanding the adversarial attacks targeting medical AI systems. By outlining specific adversarial threat models for medical settings and identifying vulnerable application domains, we lay the groundwork for future research that investigates the security and resilience of AI-driven medical systems. Through our analysis of different threat models and feasibility studies on adversarial attacks in different medical domains, we provide compelling insights into the pressing need for cybersecurity research in the rapidly evolving field of AI healthcare technology.", "sections": [{"title": "1. Introduction", "content": "Growing Market of Medical AI. As technology advances, software systems play an increasingly vital role in com-mercial products and are becoming integral in the medical field [1]. The development of artificial intelligence (AI) and machine learning (ML) has transformed modern healthcare systems, providing valuable new insights obtained from the vast amount of data collected through patient diagnostics, monitoring, and healthcare research [2]. It was predicted that the global healthcare Al market would reach nearly 188 billion U.S. dollars by 2030 [3]. Approximately 22% of healthcare organizations worldwide stated that they were in the early stage of AI model adoption, while 24% reported being in the pilot stage [4]. Meanwhile, a recent survey indicated that 44% of people globally are open to using AI in healthcare for diagnostic and therapeutic purposes [5]. This highlights the significant benefits of improved diagnostic accuracy and treatment precision for patients, as well as the potential to allow medical practitioners to devote more time to patient care instead of routine administrative tasks.\nMotivation. The integration of AI/ML technologies into medical systems inevitably introduces vulnerabilities. Compared to traditional AI domains, the security of medical AI is much more regulated due to its impact on humans, with the International Medical Device Regulators Forum (IMDRF) classifying models that independently diagnose critical disorders (such as cancer and Alzheimer's) as having maximum risk to patients [6]. As a result of these guidelines, recently approved medical AI devices by the FDA have been prohibited from performing autonomous diagnoses and are classified as strictly assistive tools to human profes-sionals [7]. This regulatory action indicates scrutiny of the security and accuracy of current medical AI models to act autonomously, which we aim to explore in this paper. While there have been several works systematizing the state of ML research in domains such as network intrusion detection [8] and security applications [9], as well as ML explainabil-ity [10], accountability [11], and privacy [12], a unified view on the topic in medical settings is not available yet. Since medical AI differs from others in its unique deployment scenario, challenges in data availability, model efficiency, explainability, and safety requirements necessitate a careful evaluation of the risks involved in their deployment in the medical domain.\nScope. The emergence of AI/ML services in healthcare has led to regulatory measures for AI/ML under Software as a Medical Device (SaMD), including premarket clearance 510(k). The U.S. Food and Drug Administration (FDA) describes SaMD as \"software intended to be used for one or more medical purposes without being part of a hardware medical device.\" [13] This definition was utilized to explore existing studies on security and privacy attacks against med-ical AI models. Specifically, our systematization includes cases where the attack targets AI models that independently function as a medical product, particularly when their role is diagnosis, prevention, monitoring, treatment, or alleviation of diseases, as per the definition of \"medical device\" by the FDA. The work in our survey spans from top security (S&P, USENIX Security, AsiaCCS, EuroS&P), machine-learning (AAAI, KDD), computer vision venues (CVPR), as well as biomedical journals and venues (Nature, Nature Medicine, MICCAI) and arXiv in the past 10 years.\nContributions. Recognizing the lack of a holistic view of AI attack research in the medical landscape, we aim to fill this gap by systematically examining the medical application domains and laying the groundwork for future attack research. Our main contributions are as follows:\n\u2022 We provided a taxonomy analyzing how the identities, knowledge, capabilities, and goals of adversaries in the healthcare domain may differ from those considered in"}, {"title": "2. Taxonomy of Threats in Medical AI", "content": "We identify the adversary's potential identity in the healthcare domain and their respective motivations for at-tacking medical AI systems. However, it should be noted that in the real world, multiple adversaries can work together to achieve malicious intent. Their adversarial capability and knowledge may therefore be expanded compared to when the adversaries work on their own."}, {"title": "2.1. Adversary's Identity", "content": "Patient. Patients in the healthcare setting generally would not have direct access to the medical systems that employ ML algorithms. However, except for images and data that come directly from hospital measurements, there are cases where data is provided by the patients themselves, especially with the rise of telemedicine. Malicious patients can be mo-tivated to generate false-negative diagnostic results to avoid social stigma associated with certain medical conditions. They may also be motivated to generate false-positive results to receive higher priority on surgery wait lists.\nMedical Practitioner. Medical practitioners include clin-icians, nurses, or any other person working at or near the point of care. Medical practitioners usually have only black-box access to the medical AI systems. Since their main task is to obtain prediction results from the models, compared to the patients, they have the added capabilities of query access and sometimes access to model explanations. Malicious medical practitioners can be motivated to manipulate model outputs to incur additional expenses for unnecessary treat-ments. They may also conduct healthcare insurance fraud, such as phantom billing, which bills services or supplies that the patients never actually received.\nML Service Provider. Healthcare organizations may opt for cloud-based ML service instead of on-site deployment to minimize investments in hardware and related IT infras-tructures [14]. In such cases, an adversarial ML service provider has capabilities that range from training data con-trol to accessing output explanations. Within the healthcare domain, service providers are typically considered honest-but-curious [15], [16], implying that while model integrity is presumed intact, confidentiality may be at risk.\nThird-party Healthcare Organization. The profit and ser-vice of organizations such as healthcare insurance com-panies may depend directly on the outcome of medical AI predictions. Therefore, these organizations may have the highest monetary incentives to manipulate medical AI. However, they typically have limited capability, knowledge, and access to the models, and are more likely to work with other adversaries to achieve attacks.\nCybercriminal & Business Competitor. Business competi-tors may wish to diminish the performance of ML models to discredit other similar services. Except for direct model control, they have almost all of the adversarial capabilities. Since they work in similar domains, they may leverage do-main expertise to perform gray-box attacks. Cybercriminals, on the other hand, typically attack the availability of medical ML services. By hampering diagnostic systems or com-pletely shutting them down, they can demand ransoms to unlock the service. They may also compromise the privacy of patient data and make profits from it."}, {"title": "2.2. Adversary's Knowledge", "content": "White-box Attacks. In white-box attacks, it is assumed that the adversary has a complete understanding of the ML system. This includes access to the training data, model ar-chitecture, model weights, and the model's hyperparameters. Hospitals using ML services from a third-party organization are at risk of such attacks from their service providers.\nBlack-box Attacks. Black-box attacks assume that the ad-versary has minimal knowledge about the ML system. The adversary may have query access to the model; however, they lack insight into the model's training process and its architecture or parameters. Typically, a malicious patient or clinician has only black-box access to medical ML systems.\nGray-box Attacks. Gray-box attacks encompass a range of techniques that represent a middle ground in adversarial knowledge between black-box and white-box attacks. For example, an adversary may be aware of the model's archi-tecture but not its parameters. Alternatively, the adversary may not know the specific training data but may have access to data identically distributed to the training data, as seen in membership inference attacks."}, {"title": "2.3. Adversary's Capabilities", "content": "Training Data Control. Adversaries may gain partial con-trol over the training data, enabling them to insert or modify training samples. This is a foundational strategy in data poisoning attacks.\nModel Control. Adversaries may take control of the model parameters through various means, such as embedding a Trojan trigger within the model parameters or executing malicious updates in federated learning scenarios.\nTesting Data Control. At the model deployment stage, attackers can introduce perturbations to testing samples. This"}, {"title": "2.4. Analysis of Adversarial Goals", "content": "In this section, we analyze each attack in healthcare sce-narios by contextualizing them with the adversary's identity. Figure 1 shows the adversarial capability and knowledge based on the adversary's identity in the medical setting.\nIntegrity Attacks. In the medical context, patients, medi-cal practitioners, cybercriminals, and business competitors may have motivations to engage in adversarial evasion at-tacks [17], [18]. As they typically lack white-box knowl-edge, adversarial attacks in medical settings are generally executed in black-box or gray-box scenarios. Black-box attacks can be further classified based on the nature of the obtained information: score-based attacks (where the adversary has access to either full or top-K confidence scores) and decision-based attacks (where only the label is accessible). Gray-box attacks represent an intermediary scenario, assuming that the adversary has access to auxiliary information, such as auxiliary datasets and models [19].\nPoisoning attacks include targeted poisoning, backdoor poisoning, clean-label backdoor and poisoning, and model poisoning. Model poisoning, where malicious functionality is embedded directly into the model, is typically executed by ML service providers. These providers may use such attacks to facilitate privacy breaches, such as membership inference. The other three poisoning schemes assume the adversary has control over a portion of the training data, achievable by cybercriminals and business competitors to maliciously impact service operations. In such scenarios, the adversary has gray-box or black-box knowledge.\nConfidentiality Attacks. Confidentiality attacks can occur within white-box, black-box, or gray-box settings. Typically, the adversary's knowledge consists of aspects such as the training algorithm, model architecture, model parameters, training data, training data distribution, and the number of training samples. A prevalent assumption in privacy inference attacks is that the adversary is familiar with the architecture of the target model and possesses an auxiliary dataset derived from a distribution identical to that of the target model's training dataset. Within this threat model,"}, {"title": "Availability Attacks", "content": "The primary category of availabil-ity attacks employs poisoning strategies to indiscriminately compromise the performance of entire machine learning models. This includes data poisoning, clean-label poisoning, and model poisoning techniques. These attacks assume the adversary has control over the training data and are predom-inantly executed by cybercriminals and business competitors aiming to disrupt services. Consequently, these attacks are typically operated with gray-box or black-box knowledge. A notable technique in gray-box settings involves the con-cept of transferability, where an adversary uses an aux-iliary model to generate poisoning samples that are then applied to the target model, aiming to degrade its overall performance [22], [23]. Conversely, clean-label poisoning operates within a more realistic threat model that prevents adversaries from altering training example labels. Unlike other poisoning attacks, model poisoning assumes control over the model, a scenario generally not feasible for cy-bercriminals and business competitors. The second category of availability attacks is energy-latency attacks [24], which require either white-box or black-box access to the target model. Cybercriminals or business competitors can conduct such attacks to induce excessive energy consumption and/or increase inference latency on medical ML models, thereby disrupting their normal operation and availability."}, {"title": "Fairness Attacks", "content": "Fairness attacks generally adopt the framework of data or model poisoning attacks, aiming to undermine the system's fairness. These attacks leverage data poisoning [25] or model poisoning [26] techniques and demand similar capabilities. Cybercriminals or busi-ness competitors may introduce bias or unfairness into the model through poisoned data to make the model produce discriminatory or unfair predictions, damaging the trust and reputation of the healthcare institution or service provider."}, {"title": "Explainability Attacks", "content": "Explainability attacks require ac-cess to the system's explanations. The level of access varies from white-box approaches, where the adversary fully understands the system's inner workings and parameters, to black-box methods, where the adversary's knowledge is limited to the system's explanation outputs. A line of research [27], [28] has demonstrated that existing methods for explainability are vulnerable to adversarial examples. Notably, explainability attacks often coincide with other attacks and require similar knowledge and capabilities to the corresponding attacks. For instance, medical practitioners may use explanations to generate more imperceptible adver-sarial examples, while cybercriminals may leverage them to enhance the effectiveness of backdoor attacks."}, {"title": "Guidelines for Under-explored Attacks", "content": "Under-explored attacks in medical settings can serve as potential future research directions. Our goal is to develop a general guide-line that identifies areas where future research is feasible. We categorized attack feasibility across two dimensions: the technical barrier and the threat model. The technical barrier determines whether an attack can succeed in a specific scenario, while the threat model determines the effectiveness of the attacks. Due to notable differences in data and models, we analyze each medical sub-domain separately.\nTo determine whether an attack can succeed in under-explored scenarios, we drew on existing research in analo-gous domains where data and models exhibit similar charac-teristics (Table 1). For instance, if an attack has been done in medical image classification tasks, it is likely to succeed in medical image detection tasks due to the task/modality sim-ilarities. Furthermore, if we can not find instances of certain attacks in a similar domain but a stronger attack within the same domain is present, it also suggests the feasibility of the attack. For example, suppose related work demonstrates the success of data reconstruction or model inversion attacks. In that case, we can confidently predict that membership inference attacks will achieve a similar level of success, even if we cannot find related literature on membership inference in the domain. Conversely, if we cannot find an attack that works with a similar data modality or model, such an attack may not be feasible. However, this is merely a hypothesis that should be validated through experiments. If an attack is deemed feasible, our next step is to determine its potential effectiveness under a specific threat model, as an attack with limited success may be of little significance. To this end, we selected 5 under-explored domains in \u00a78 and conducted experiments to evaluate attack effectiveness. For our selected adversarial identity, we examined the adversarial knowledge and capabilities required for the attack. If the adversarial role possesses the necessary knowledge and capabilities, the attack is likely to be effective. In the following sections, we delve more deeply into each medical sub-domain to examine existing attacks related to medical AI."}, {"title": "3. Medical Diagnostics Systems", "content": ""}, {"title": "3.1. Medical Image Classification", "content": "Machine learning in medical imaging tasks involves examining image modalities such as X-rays, CT, MRI, and ultrasound to help diagnose, monitor, or treat various medical conditions [29]. In the medical imaging setting, classification involves separating medical images into vari-ous categories based on the image type or the presence of different conditions for a specific disease. Many ML models have been developed for medical image classification [30], [31]. Morabito et al. [32] developed convolutional neural networks (CNN) to classify patients' electroencephalogra-phy (EEG) patterns of Alzheimer's disease and achieved 80% classification accuracy. In the cancer domain, Nazeri et al. [31] proposed a two-stage CNN to classify breast can-cer microscopy images into four categories and the model yielded an accuracy of 95%. When examining blood-related diseases such as leukemia, Kassani et al. [33] suggested a"}, {"title": "3.2. Medical Imaging Detection", "content": "Moreover, while detection may include image-level or region-level classification, it also involves establishing spa-tial localization of regions of interest in medical images [98], [99]. Winkels and Cohen [100] proposed CNN with group convolutions to detect pulmonary nodules in CT scans and their approach outperformed other strong baseline models in several metrics including accuracy, sensitivity, and con-vergence speed. Similarly, Lee et al. [101] used deep convo-lutional networks, attention maps, and iterative processes to detect acute intracranial hemorrhage from CT image inputs. Meanwhile, Maicas et al. [102] used a deep Q-learning re-inforcement learning-based network to detect breast cancer lesions from dynamic contrast-enhanced MRIs."}, {"title": "3.3. Medical Imaging Segmentation", "content": "In addition, medical imaging in segmentation tasks fo-cuses on classification at a pixel or voxel level for a given image type [103], [104]. Hu et al. [105] suggested an approach to generate synthetic tumors in CT scans and demonstrated that machine learning models could accu-rately segment the tumors using the annotation-free syn-thetic tumors. For ultrasound images, Nithya et al. [106] developed an approach to detect and segment kidney stones using artificial neural networks and multi-kernel k-means clustering, achieving an accuracy of 99.61%. Focusing on abdominal anatomy, Gibson et al. [107] used dense CNN to segment abdominal organs from CT scans without the need for registration."}, {"title": "3.4. Electrogram Diagnostics", "content": "An electrogram is a recording and visualization of the electrical activity within the body. ML methods are most commonly involved in diagnosing arrhythmias [108], [109] with electrocardiograms (ECG) or schizophrenia with electroencephalograms (EEG) [110]. Notably, Kiranyaz et al. [111] was the first to use deep neural network over 1D signals, particularly for ECG classification tasks. Other applications include the use of electromyograms (EMG) for neuromuscular disorder diagnostics [112] and electrooculo-grams (EOG) for sleep stage classification tasks [113]."}, {"title": "3.5. Multi-omics Diagnostics", "content": "Multi-omics diagnostics focuses on analyzing data from a wide variety of omics sources, including genetics and metabolomics. ML can be applied to multi-omics data anal-ysis in a diagnostic setting to improve the detection and classification of various diseases [114]. When considering early cancer detection, multi-omics data can include infor-mation about mutations, gene expression, and copy number variation. Schulte-Sasse et al. [115] utilized graph convo-lutional networks to identify new cancer genes from multi-omics pan-cancer data. Meanwhile, multi-omics approaches can also be used to more accurately classify chronic kidney disease to promote optimal treatment plans. Eddy et al. [116] used various ML approaches on molecular data composed of kidney biopsy, blood, and urine samples to classify patients into molecularly defined subgroups that better reflect infor-mation about the underlying mechanisms associated with chronic kidney disease."}, {"title": "3.6. EHR Diagnostics", "content": "Electronic health records (EHRs) store patient medical records in a digital format and contain a variety of data modalities for the efficient search and retrieval of patient information. Traditional computer-aided medical expert sys-tems often assist in diagnosis by employing feature-level fusion or rule-based reasoning. System performance can be significantly affected by decision rules that are subjectively determined by experts in the field and cannot be dynamically updated. Especially for multi-source, unstructured multi-modal healthcare data, traditional methods fall short of offer-ing integration, reasoning, and interactive decision support."}, {"title": "4. Clinical Decision Support", "content": ""}, {"title": "4.1. Clinical Summary & Question Answering", "content": "Language learning model can help manage biomedi-cal text data for named-entity recognition [127], sentence similarity [128], and relation extraction [129] tasks due to its ability to quickly absorb, summarize, and rephrase information. It is especially suitable for routine tasks such as creating discharge summaries, which require interpreting and shortening of information but with minimal need for problem-solving skills [130]. The emergence of multimodal models also expands the applicability to a wider range of data types, such as deciphering doctors' handwriting accurately or classifying pathology reports based on cancer types [81]. Yet, as clinical document texts are usually long, ungrammatical, fragmented, and marred with typos, rigorous validation is essential to guarantee patient safety [131]."}, {"title": "4.2. Automated Clinical Coding", "content": "Clinical coding is the task of transforming medical records, usually represented as free texts written by clini-cians, into structured codes from a classification system like ICD-10. It is a non-trivial task for humans, usually including data abstraction or summarization [132]. More specifically, an expert clinical coder is expected to decipher the largest number of documents about a patient's episode of care, and to select the most accurate codes from a large classification system according to the contexts of the various documents and the regularly updated coding guidelines [133]. While traditional rule-based approaches are available, these are time-consuming and require expert-defined rules and hand-crafted textual features [133]. On the other hand, ML-based approaches employ the encoder-decoder framework, lever-aging entity-mining techniques to extract rich text features for automatic medical code assignment [134]."}, {"title": "5. Therapeutics", "content": ""}, {"title": "5.1. Surgical Treatment", "content": "ML in the therapeutics domain involves aiding clinicians in treating patients and improving the overall experience and success of treatment plans. Zhou et al. [137] divides ML ap-plication in surgery into intraoperative guidance [138]\u2013[142] and surgical robots [143]\u2013[147]. ML tasks in intraoperative guidance provide enhanced visualization and localization for surgery. For instance, sparse principal component analysis and kernel partial least squares regression can be helpful for 3D shape instantiation [138], mitigating the time-consuming process for 3D volume construction from 2D medical im-ages. Moreover, Sganga et al. [139] introduced a deep learning architecture that includes a generative adversarial network to localize a bronchoscope in the lung, reaching successful tracking thresholds even in less conserved re-gions. In terms of system modeling and control for surgical robots, Liu and Jiang [143] modeled the task as a sequential decision-making process using reinforcement learning."}, {"title": "5.2. Therapeutic Effect Prediction", "content": "For therapy and treatment planning, Zhang et al. [148] suggested a metaheuristic-oriented formulation and a sim-ulated annealing algorithm. The strategy generates the op-timal daily routing and scheduling solution that will suit the needs of both patients and caregivers in the healthcare system. For psychotherapy, Zhou and Kosorok [149] pro-posed a causal k-nearest neighbor method to help predict the best treatment regime. They simulated their system on pa-tients with nonpsychotic chronic major depressive disorder to identify the patients that would benefit from undergoing Nefazodone therapy, cognitive behavioral psychotherapy, or a combination of the two plans. Meanwhile, Su et al. [150] developed random forests of iteration trees to help estimate individualized treatment effects and simulated the model with data collected from an acupuncture headache trial."}, {"title": "6. Population Health", "content": ""}, {"title": "6.1. Drug Development Research", "content": "Target Identification & Drug Discovery. Machine learning methods have been utilized to assist drug development in target identification [153]\u2013[156] and drug discovery [157]\u2013[159]. For instance, to establish gene-disease causal as-sociations and assess potential drug targets, Mountjoy et al. [153] proposed an open ML pipeline that performs fine mapping and gene prioritization for 133,441 different loci from genome-wide association studies (GWAS). Moreover, ML is increasingly used in drug discovery and screening. Olivecrona et al. [157] developed a sequence-based genera-tive model that uses deep reinforcement learning to generate drug molecular structures that satisfy desirable properties. Their method yields almost optimal values given parameters such as bioactivity and pharmacokinetic properties.\nDrug-drug Interactions and Complications. Drug-drug interaction (DDI) is described as a change in the effect of one drug due to the presence of another drug [160]. With the rapidly growing number of approved drugs, pre-scriptions with multiple drugs have been a common clinical practice. However, the occurrence of DDI can often lead to unexpected side effects. The availability of large amounts of drug-related information from biomedical texts, EHRs, and public databases provides fertile ground for literature-based extraction methods [161], [162]. These utilize NLP techniques to perform relation extraction tasks regarding DDI from unstructured data, identifying specific relations between the name-entity pair in the documents. Meanwhile, with the construction of publicly available databases, pre-dictive models based on chemical and biological knowledge have great potential for DDI prediction. ML-based predic-tion [163]\u2013[166] methods usually take DDI prediction as a link prediction task, detecting the presence or absence of interactions between drug pairs [167]."}, {"title": "6.2. Epidemiology", "content": "In addition to its use for individual patient care, medical AI has found applications in the management of health on a population-wide scale, especially in the areas of epi-demiology and monitoring of infectious diseases [169]. For instance, AI used for pandemic early-warning can sift, fil-ter, categorize, and compile web texts for indications of infectious disease occurrences with remarkable accuracy and speed [170]. An illustrative example of an early warning ap-plication is HealthMap [171], which employs NLP methods to scan web-posted text for real-time indications of infec-tious disease events, then compare the text with a lexicon of recognized pathogens and geographic regions. Furthermore, Bhatia et al. [172] sought to explore the utilization of data gathered from ProMED [173] and HealthMap for real-time outbreak analysis, using a versatile statistical model to measure spatial variability in the risk of an outbreak spreading and to predict short-term incidence patterns. These applications can also be integrated into medical IoT devices, such as the real-time identification of unusual physiological signals linked to the early onset of infection using smart-watches [174]. Upon recognizing an outbreak, subsequent course of action involves contact tracing and severing the transmission pathways. Sundermann et al. [175] leverage patient's EHR and extract data pertinent to an outbreak, merging whole-genome surveillance sequencing with ML as well as detecting in-hospital transmission routes through the molecular characterization of bacterial isolates."}, {"title": "7. Patient Health Monitoring", "content": ""}, {"title": "7.1. Disease Risk Prediction", "content": "Systems using ML models to analyze personal lifestyles or environmental factors in non-hospital settings can pro-vide a convenient and less invasive way for individuals to understand their health. Parab et al. [178] proposed an intel-ligent system that predicts diseases based on an individual's lifestyle to offer preventative measures. It can evaluate a person's health status and warn about potential lifestyle-related diseases. In essence, ML can predict disease sus-ceptibility [179], survivability [180], occurrence [181], and reoccurrence [182], taking into account a person's genomic information, inheritance, lifestyle, and other relevant traits.\nSusceptibility & Survivability. Substantial research efforts have been dedicated to predicting disease susceptibility, a vital component in improving prognosis and reducing mortality rates. For instance, Ming et al. [179] used ML models to estimate long-term breast cancer risk, surpassing the predictive accuracy of the widely used BOADICEA risk prediction model in clinical practice. Another useful factor in prognosis is disease survivability concerning fatal diseases. Dai et al. [180] suggested a recurrent deep survival machine to predict prostate cancer survivability, estimating survival probability and quantifying prediction uncertainty.\nOccurrence & Recurrence. ML techniques have been im-plemented to predict the onset of diseases [183], [184], including pancreatic cancer, as demonstrated by Placido et al. [181], who trained and evaluated a deep learning algorithm for predicting pancreatic cancer incidence within varying time intervals. Moreover, disease recurrence is a significant consideration in healthcare, particularly in the context of cancer. Kucukkaya et al. [185] proposed the use of CNN to predict liver tumor recurrence in early-stage hepatocellular carcinoma patients, achieving testing area under the curve (AUC) values between 0.71 and 0.85."}, {"title": "7.2. Monitoring & Intervention", "content": "Appropriate interventions need to be tailored to the level of patient engagement and readiness and support them not just in hospitalization. For greater overall engagement, interventions can inform or empower patients, which may contain symptom management, decision-making, and medi-cation administration. For example, Mondol et al. [186] pro-posed a general-purpose medication reminder and tracking system on wrist devices that can be customized according to"}, {"title": "8. Under-Explored Attacks in Medical \u0391\u0399", "content": "To validate the applicability of our guidelines for under-explored attacks, we conducted three case studies, as de-tailed in this section. Five adversarial attacks in diverse under-explored medical domains were devised and executed. Specifically, we explored membership inference attacks in ECG diagnostics (\u00a7 8.1), targeted backdoor attacks in ECG diagnostics and disease risk prediction systems (\u00a7 8.2), and untargeted poisoning attacks in image segmentations and EHR diagnostics models(\u00a7 8.3). These attacks are designed to test the confidentiality, integrity, and availability of the models, respectively, offering insights into the practical im-plications of our proposed guidelines."}, {"title": "8.1. Membership Inference Attacks", "content": "Conjecture. In this section, we investigate the feasibility of conducting Membership Inference Attacks (MIAs) on ECG diagnostics models. MIAs typically assume that the adversary has access to an auxiliary dataset originating from the same distribution as the training dataset of the target model and is aware of the target model's architecture [189], [190], [192]\u2013[194]. In the context of ECG diagnostics, where typical adversaries are cybercriminals, we conjecture that MIAs may not achieve high success rates due to the extensive adversarial knowledge these attacks require.\nThreat Model. Since the adversary does not have access to an auxiliary dataset and knows the target model, we assume an alternative threat model where an adversary has access to a portion of the training data for the target model, which could be achieved through data poisoning within the target model's training dataset [195]. Apart from this capability, the adversary only has access to the output of the target model without knowledge of its parameters or architectures.\nAttack Method. We select four representative membership inference works to apply in our setting: Shokri et al. [191], Salem et al. [190], Yeom et al. [188], and Song et al. [189]. The adversary holds a portion of members from the target model (e.g., injected through data poisoning) and non-members. This combined dataset is then used to train the attack model. Attack efficacy is assessed using the remaining member and non-member data of the target model.\nDatasets and Models. We selected the widely used 2017 PhysioNet/CinC Challenge dataset [109]. The goal of the challenge was to classify single-lead ECG recordings into four types: normal sinus rhythm (Normal), atrial fibrillation (AF), alternative rhythm (Other rhythm), or noise (Noisy). The dataset contains single-lead ECG recordings collected using the AliveCor device, sampled at 300 Hz. In total, there are 8,528 recordings, with durations ranging from 9 seconds to over 60 seconds. We used a 13-layer CNN that won the 2017 PhysioNet/CinC Challenge [196] as our target model. This model has achieved a training accuracy of 98.57% and a validation accuracy of 85.08% on the target dataset.\nExperimental Setup. We split the dataset into two equal subsets: $D_{traint}$ and $D_{test}$. The $D_{traint}$ subset is used to train the target model M, and its samples are considered members of M, whereas samples in $D_{targ}^{test}$are treated as"}, {"title": "8.2. Backdoor Attacks", "content": "Conjecture. We assume the adversary to be either a business competitor or a cybercriminal, who has control over the training and testing data. Regarding attack effectiveness, backdoor attacks have been studied in time series data [198], which possess similar properties to ECG signals. Based on this similarity, we reasonably assume that backdoor attacks will also achieve decent performance on electrogram diag-nostics. Conversely, conducting attacks on EHRs (i.e., the primary data source for disease prediction) may be more challenging due to their heterogeneous nature.\nThreat Model. We follow the common setting in backdoor attacks [56], [64], where the adversary could poison a subset of training data by adding a particular trigger pattern. After the model deployment, the adversary could add the trigger to test an example to induce intended behaviors. We assume the adversary does not know the target training data distribution, the architecture, or the weight of the target model."}, {"title": "8.2.1. ECG Diagnostics", "content": "In this section, we describe our attack methods, experimental settings, and empirical results of backdoor attacks on ECG diagnostics models.\nAttack Method. To pioneer the study of backdoor attacks on ECG data, we propose a straightforward baseline method. We design the trigger as a cosine waveform time series with a fixed amplitude and length. During the experiment, we fixed the amplitude to 25 and investigated the impact of the trigger's length on the performance.\nExperimental Setup. In this experiment, we use the same datasets and models as in \u00a7 8.1. We consider a targeted backdoor attack scenario, where we select the target label as atrial fibrillation (AF). Transforming other labels into AF labels could potentially cause false alarms. To assess the impact of the attack, we vary two parameters: the fraction of poisoned data in the training set (ranging from 0.01 to 0.5) and the trigger length (2% and 5% of the ECG recording length). We repeat the trial 5 times with the random generation of triggers and random subset selection and evaluate the accuracy of the benign and poisoned data."}, {"title": "8.2.2. Disease Risk Prediction", "content": "We present our attack methods and results on disease risk prediction models.\nDatasets and Models. We utilize the widely used Medical Information Mart for Intensive Care III (MIMIC-III) dataset [199"}]}