{"title": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing", "authors": ["Hao Zhang", "Felix Stahlberg", "Shankar Kumar"], "abstract": "Large Language Models (LLMs) excel at rewriting tasks such as text style transfer and grammatical error correction. While there is considerable overlap between the inputs and outputs in these tasks, the decoding cost still increases with output length, regardless of the amount of overlap. By leveraging the overlap between the input and the output, Kaneko and Okazaki [1] proposed model-agnostic edit span representations to compress the rewrites to save computation. They reported an output length reduction rate of nearly 80% with minimal accuracy impact in four rewriting tasks. In this paper, we propose alternative edit phrase representations inspired by phrase-based statistical machine translation. We systematically compare our phrasal representations with their span representations. We apply the LLM rewriting model to the task of Automatic Speech Recognition (ASR) post editing and show that our target-phrase-only edit representation has the best efficiency-accuracy trade-off. On the LibriSpeech test set, our method closes 50-60% of the WER gap between the edit span model and the full rewrite model while losing only 10-20% of the length reduction rate of the edit span model.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models pretrained on vast amounts of texts and then fine-tuned, instruction-tuned, or prompted for generation tasks have obtained remarkable success in the past few years [2]\u2013[6]. These models excel at text rewriting tasks, and text style transfer [7] and grammatical error correction [8], [9] in particular.\nHowever the superior quality of these models comes along with a steep increase in the cost of computation. To enable broad deployment for a large user base, it is crucial to reduce the computational cost while maintaining the accuracy.\nOne of the common characteristics of the above-mentioned rewriting tasks is that their output often repeats spans of text in the input. Exploiting the common sub-strings between the input and the output can result in more compact output representations. LLMs can be fine-tuned on examples that map inputs to their compact rewrite representations instead of plain rewrites. At inference time, such a decoding output needs to be composed with the input to expand into complete rewrites. Kaneko and Okazaki [1] proposed one such representation, which is a numerical span indexing into the input sequence followed by a target phrase that will substitute the source phrase in the given span. We propose two new alternative representations. The first one uses a source-target phrase pair to represent each rewrite pattern, analogous to phrase pairs used by phrase-based statistical machine translation [10]. The second representation only uses a target phrase along with left and right context words that appear in the input. We call the new representations phrase representations to distinguish them from the span representation [1].\nThe clear advantage of compact representations over com- plete rewrites is that the number of decoding steps, and hence the computational cost of inference, is reduced. [1] reported an output length reduction rate of 80%. The disadvantage of such representations is that decoding errors can lead to inconsistency with the input sequence in the expansion stage, causing an error propagation effect. For example, using the numerical span representation, if the left index or the right index is off by one, the source phrase to be substituted will also be off by one. The concatenation of the context and the substitution can therefore become disfluent. With phrase representations, context words are provided before and after substitution phrases, which can alleviate the problem of disfluency upon substitution. However, phrase representations have their own problems too. The predicted context phrase may not match the input, which makes it necessary to dis- card the subsequent rewrite. The main focus of the paper is to evaluate the efficiency-accuracy trade-off of the different representations.\nWe choose Automatic Speech Recognition (ASR) post editing as the task for applying LLM-based rewrite models and report word error rates (WER) and output length reduction rates using the span representation and the full rewrite models as the baselines.\nOur contributions include the following:\n\u2022\tWe propose a compact edit string representation with superior efficiency-accuracy trade-off than [1].\n\u2022\tWe apply edit representation based rewriting LLMs to the task of ASR output correction. To the best of our knowledge, our work is the first to combine compact rewriting with generative LLMs to achieve substantial ASR WER reduction with manageable decoding cost."}, {"title": "II. REWRITE REPRESENTATIONS", "content": "Mathematically speaking, for rewrite examples (x, y), where x is the input string and y is the output string, there is a compression function C and an expansion function E satisfying\n$E(x, C(x, y)) = y$                                                                                                 (1)\nwith the constraint that |C(x, y)| has a much smaller average value than |y|. At training time, examples are converted to (x, \u0177 = C(x,y)), where \u0177 is the reference output. At inference time, the final output is obtained by applying the expansion function E(x, y') where y' is the decoding output for x. The edit representations in this section differ in the choice of the function pair C and E.\n\n\n\n\n\n\n\n\nA. Edit Span Representation\nThe span representation [1] is derived from a word align- ment graph a between x and y. Given a bipartite alignment graph between the input sequence and the output sequence, we can identify pairs of word spans between the two sides. Each span pair is a local rewrite instance indicating that the source span is substituted by the target span. In practice, the alignment is derived from the Levenshtein distance algorithm with the guarantee that the alignment links are monotonically ordered. It is always feasible to represent the entire rewrite as a sequence of local rewrite spans. For LLMs to predict the rewrites, we need a string representation of the span pairs. In their paper, the span representation is specified as (i, j, ya(i...j)), where a(i...j) is the corresponding target span of a source span i...j and ya(i...j) is the target phrase in this span. Under this representation, C is the concatenation of the ordered span representations:\n$C = \\bigoplus_{(i,j)\u2208a} (i, j, ya(i...j)).$                                                                                       (2)\nE is the program of applying the ordered local rewrites to the input sequence.\nB. Phrase Pair Representation\nThe representation in Equation 2 is concise. It uses a pair of integers to represent a source span. However, this implies that LLMs have to count source tokens and generate indexing integer tokens interleaved with content tokens following the predefined format. The structured representation introduces brittleness to the model. A prediction error in the integer token sub-sequence can have a cascading effect when the entire output rewrite string is parsed and applied to the input. Instead, we resort to a natural language representation, which uses the source phrase xi...j for a span (i, j) directly as the prefix for the target phrase ya(i...j). However, one downside of our representation is that when the span (i, j) is small, the subsequence xi...j can be ambiguous, introducing errors into the expansion step. A solution is to add more context to x(i...j) as well as ya(i...j) to make it much less likely to be ambiguous by extending the phrase pair to both the left and the right. Formally, the new function C is\n$C = \\bigoplus_{(i,j)\u2208a} (W: X(i-k...j+k), ya(i-k...j+k)),$                                                                       (3)\nwhere k is called the dilation span, W is a natural lan- guage prompt word like rewrite. The expansion function E involves parsing the pattern, prefix string matching, and replacement on the input.\nC. Target Phrase Representation\nThe representation in Equation 3 using both source phrases and target phrases has the disadvantage of being verbose. Dilation spans on both sides of a phrase, which are intended to make phrases less ambiguous, can exacerbate the prob- lem. However, dilation spans on target phrases can often be sufficient for disambiguation when the span size is three or higher. We can ignore source phrases and just use dilated target phrases as they contain both anchor text in the input and replacement text in the output. The following is the new compression function.\n$C = \\bigoplus_{(i,j)\u2208a} (W: ya(i-k...j+k))$                                                                                           (4)\nString matching and replacement in the implementation of function E deals with discontiguous dilation spans in the form of ya(i-k...i-1)\u2026ya(j+1,j+k)."}, {"title": "III. EXPERIMENTS", "content": "We use a decoder-only LLM for the task of correcting the output of a fast first pass Automatic Speech Recognition (ASR) model. The first pass model is a streaming model that decodes as audio comes in without the full context of the future. Therefore there is sufficient headroom for error correction using a pre-trained LLM with the full ASR transcription as the input. The task can be viewed as a variant of grammatical error correction [11].\nThe ASR model we use is the Google USM model [12]. The LLMs we use are the PaLM 2 Gecko and Otter models [5]. We fine-tune LLMs on the LibriSpeech [13] training set using the dev set for hyper-parameter and checkpoint selection and the test sets for final comparisons. The ASR model is frozen in our experiments. We fine-tune the entire Transformer LLM model to minimize the cross entropy loss on the transcription reference set given the ASR transcription generated by the frozen USM model as the prefix to the LLM decoder. The entire system is depicted in Figure 2. We use two baselines. One is full rewrite model that uses the reference transcription directly. The other is span rewrite [1] that uses the representation in Section II-A. We are interested in two metrics. The quality metric is word error rate (WER) after expanding edit representations. The efficiency metric is decoder output length reduction rate.\nTable II summarizes the main results. We show that the span representation indeed incurs more accuracy loss than the phrase representations. On the clean test set, target only is able to close 57% of the accuracy gap between span and full, while losing 12.5% of the length reduction rate. On the noisier other test set, target only is able to close 54% of the accuracy gap, while losing 22.2% of the length reduction rate.\n\nA. Efficiency and Accuracy Trade-offs\nIn Figure 3, we plot WER versus output length for two model sizes: Gecko and Otter, and varying values of the phrase dilation hyper-parameter k in Equation 3 and Equation 4. Both the phrase pair and the target phrase only strategies yield lower WER with slightly longer outputs than the span strategy. Overall, when k is 3, the target phrase only strategy has the best trade-off. The trend is consistent across the two PaLM 2 model sizes.\n\nB. Recovery Rate of Compact Representations\nIn Section II, we formulated the problem as selecting a pair of compression function C and expansion function E to satisfy Equation 1. The span representation is exact and unambiguous. So when E is applied, the equality is satisfied for all training examples. The phrase representations can be ambiguous and depend on the dilation spans to reduce the likelihood of multi- ple matches when the expansion function is applied. Table III summarizes the recovery rates, which is the percentage of examples in the dev set that satisfy Equation 1. The phrase pair representation has sufficient source context in the source phrase so that its recovery rate is very close to 100%. For the target only representation, word bigrams (k = 2) or trigrams (k = 3) surrounding target phrases are sufficient for uniquely identifying their source side counterparts in most cases."}, {"title": "IV. RELATED WORK", "content": "Orthogonal efforts to speed up decoding include speculative decoding [14], [15]. They leverage the overlap in output distributions between a less accurate faster model and a more accurate slower model as well as hardware accelerators for parallel computing. They do not incur accuracy loss and are not limited to rewriting tasks. Combining compact represen- tations with speculative decoding has the potential for even more speedups."}, {"title": "V. LIMITATIONS", "content": "Concise edit representations presented in the paper are derived from the Levenshtein distance algorithm. The phrases are not linguistically meaningful or optimal from machine learning point of view. They are only minimal according to the edit distance. Going beyond edit distance to use differentiable functions for compression and expansion is an interesting open area for research.\nThe dilation spans we use to anchor phrases in the input are applied uniformly and equally on the left and right of each span of interest. It is likely that longer left context is more useful than right context since the decoder progresses from left to right.\nWith discontiguous phrases, The expansion stage of the target only representation is more algorithmically involved than the other two compact representations.\nFinally, we have not experimented with the latest and largest LLMs. It is to be seen if the gap between full rewrite and edit representations can be reduced further with very large LLMs."}, {"title": "VI. CONCLUSIONS", "content": "We propose two edit phrase representations for rewriting tasks that compactly represent the differences between input and output strings. We use LLMs to predict such edits and expand the edits into complete rewrites with a deterministic string matching and replacement algorithm. Our work is a further development of the span representation [1]. For the task of ASR post editing, we close 50-60% of the WER gap between the most efficient model and the most accurate model, while only slowing down decoding by 10-20% relative to the efficient representation. Future work points to learnable compact representations."}]}