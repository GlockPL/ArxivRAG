{"title": "IMPROVING ROBUSTNESS OF SPECTROGRAM CLASSIFIERS WITH NEURAL\nSTOCHASTIC DIFFERENTIAL EQUATIONS", "authors": ["Joel Brogan", "Olivera Kotevska", "Anibely Torres", "Sumit Jha", "Mark Adams"], "abstract": "Signal analysis and classification is fraught with high lev-\nels of noise and perturbation. Computer-vision-based deep\nlearning models applied to spectrograms have proven use-\nful in the field of signal classification and detection; how-\never, these methods aren't designed to handle the low signal-\nto-noise ratios inherent within non-vision signal processing\ntasks. While they are powerful, they are currently not the\nmethod of choice in the inherently noisy and dynamic critical\ninfrastructure domain, such as smart-grid sensing, anomaly\ndetection, and non-intrusive load monitoring.\nCurrently,\nthese models can be brittle, which makes them susceptible to\nnoisy input. This also means they have sub-optimal stability\nof explanation outputs. Experts and technicians using these\nmodels to make decisions in real world scenarios need assur-\nance that a model is performing as it is supposed to. The clas-\nsification or prediction outputs it generates should be sound\nand grounded, not likely to change in the presence of shifting\nnoise landscapes. In this work, we explore the idea of Neu-\nral Stochastic Differential Equations (NSDE's) to improve the\nrobustness of models trained to classify time series data and\nthe effect of NSDE's on the explainability of outputs. We then\ntest the effectiveness of these approaches by applying them to\na non-intrusive load monitoring (NILM) dataset that consists\nof simulated harmonic signals injected into a real building.", "sections": [{"title": "1. INTRODUCTION", "content": "Applications of 2D deep learning methods towards efforts\nof signal processing and classification have been challenged\nby the need for more availability of sufficiently diverse data\nsets [1,2]. This results in models, such as Convolutional Neu-\nral Networks (CNNs), overfitting to extraneous features of the\ntest environment not relevant to the task at hand, learning to\nmake \"correct\" classifications for the wrong reasons. The de-\nsign of automated AI-based data-driven pipelines for detect-\ning nuanced signal types and characteristics would greatly\nbenefit from the development of algorithms to measure the\nconfidence of neural networks in their responses. Such con-\nfidence metrics will enable human Subject Matter Experts to\nbuild a relationship of trust with robust neural networks that\nhave a history of credible and correctly calibrated responses.\nIn many application domains, such as load characterization,\ncurrent deep learning techniques do not provide this capabil-\nity in any meaningful manner [3,4]. Furthermore, many types\nof signal processing domains,such as jamming detection [5],\nspeech emotion recognition [6], and radar-based classifica-\ntion [7], are plagued with high levels of real-world noise, ei-\nther background or adversarial. We must develop techniques\nto combat scenarios with low Signal-to-noise (SNR) ratios.\nThis work aims to provide model training and inference\nmethods that improve robustness to noisy spectrogram inputs,\nand bolster stability and confidence of subsequent classifica-\ntion explanation maps. The general goal is to train more ro-\nbust and readily explainable neural networks by injecting ap-\npropriately shaped noise during their training. If the deep net-\nwork is explainable, such that a human can get an understand-\ning of the key features of the data that help classify an object,\nthen SMEs can use this information to periodically verify that\nthe network is reasoning soundly and not focusing on features\nspecific to where the training data was collected. We formu-\nlate this noise-aware training as a Neural Stochastic Differen-\ntial Equation (Neural SDE) that provides useful mathemati-\ncal properties when operating in noisy domains. We test our\nmethods on a custom-built dataset of electromagnetic wave-\nforms injected into a building's wiring and re-collected from\nmultiple sensors. In this preliminary work, we contribute the\nfollowing:\n\u2022 A new methodology for training spectrogram-domain\nCNNs to be robust and stable using domain-shaped"}, {"title": "2. RELATED WORK", "content": "We organized the related work into three parts: Explainability\nfor smart grid and spectrogram images (Sec. 2.1), SDEs and\nNSDEs (Sec. 2.2, and noise shaping (Sec. 2.3)."}, {"title": "2.1. Model Explainability", "content": "In [14], the authors used the LIME (local interpretable model-\nagnostic explanations) tool in their classifier and succeeded\nin determining the frequency bands used by the classifier to\nmake decisions about unintended radiated emission during\nelectronic devices. Others [15] used LIME to identify the crit-\nical time-frequency bands influencing the prediction of aver-\nage surface roughness in a smart grinding process. LIME was\nused to understand the model that controlled the HVAC sys-\ntem [16]. However, this work did not use spectrograms as in-\nput data. Some authors [17] used spectrograms to analyze the\nmagnetohydrodynamic behavior of fusion plasmas and CAM\n(class activation mapping) explainability tool.\nSimilarly, in the smart grid domain, there is a work where\nexplainability tools were used to understand better how the\nmodel makes the decision using spectrograms as input data\nand what the key factors that impact those decisions [18].\nOthers [19] used Grad-CAM (gradient-weighted class acti-\nvation mapping) and partial dependence plot to understand\nthe feature's impact on fault zone prediction in smart grids.\nSimilarly, [20] developed a method based on Grad-CAM that"}, {"title": "2.2. SDEs and Neural SDES", "content": "A notable study by et al. [22] introduced Neural SDE net-\nworks, which incorporate random noise injection for regu-\nlarization, enhancing the stability of Neural Ordinary Differ-\nential Equation (ODE) networks. Moreover, [23] explored\nsmoother attributions using Neural SDEs, emphasizing re-\nduced noise, sharper visual outcomes, and enhanced robust-\nness of attributions computed through these models. This\nstudy highlights the benefits of employing Neural SDEs for\nimproved interpretability and reliability in neural network ap-\nplications."}, {"title": "2.3. Noise Shaping", "content": "The shape of injected noise in an NSDE is an important factor\nto consider. In [24], authors investigated the impact of noise\non stationary pulse solutions in spatially extended neural\nfields, emphasizing the importance of noise shaping in neural\nfield models. The authors of [8] show that particular types\nof noise, such as Brownian motion, result in smoother attri-\nbutions and more stable explanations than traditional resnets."}, {"title": "3. METHODS", "content": "This section outlines the methods and implementations with\nwhich we obtain our results. Generally, we implement our"}, {"title": "3.1. Model Types", "content": "Our experiments utilize a single main model - the ConvNeXt\nmodel [9], which is itself a variant of the ResNet50 model.\nBelow we provide a short description of these model architec-\ntures and their relevance. Residual Networks (ResNet) were\ncreated to solve the challenge of exploiting gradients. So, the\nskip connections technique was developed that connects acti-\nvations of a layer to further layers by skipping some layers in\nbetween. This forms a residual block. Resnets are made by\nstacking these residual blocks together. The approach behind\nthis network is that instead of layers learning the underlying\nmapping, we allow the network to fit the residual mapping.\nSo, instead of say H(x), initial mapping, let the network fit,\nF(x) := H(x) x which gives H(x) := F(x) + x. The\nadvantage of adding this type of skip connection is that if any\nlayer hurts the performance of architecture, then it will be\nskipped by regularization.\nA ResNet-50 model, is a 50-layer Convolutional Neural\nNetwork (CNN). The difference between ResNet50 and the\npreviously mentioned ResNet (ResNet34) is that the build-\ning block was modified into a bottleneck design due to con-\ncerns over the time to train the layers. This used a stack of\n3 layers instead of the earlier 2. Therefore, each of the 2-\nlayer blocks in Resnet34 was replaced with a 3-layer bottle-\nneck block, forming the Resnet 50 architecture. This results\nin much higher accuracy than the 34-layer ResNet model.\nConvNeXt is improved version of ResNet50 model. At\nfirst, a visual transformer was integrated into the model, ad-\njusted the number of blocks at each stage, and increased the\nkernel size so that the sliding window did not overlap. Other\nchanges are in the activation function, normalization task, and\nfewer normalization layers. ConvNeXts are good for solving\ngeneral-purpose computer vision tasks, i.e., image segmenta-\ntion and object detection."}, {"title": "3.2. Noise Shaping", "content": "We perform noise shaping, generation, and injection as per\nthe implementation outlined in [8]. Figure 1 shows examples\nof the type of brownian-shaped noise used as injection input\nfor our Neural SDE training approach."}, {"title": "3.3. Explanation Generation", "content": "For the explainability analysis, the Captum library was used\n[26]. It can be applied to any neural network model. Captum\nsupports three types of attributions: primary, layer, and neu-\nron. Primary attribution evaluates each input feature's contri-\nbution to a model's output. Layer attribution evaluates how\na particular layer impacts the output of the model. Neuron\nattribution evaluates each input feature's contribution to acti-\nvating a particular hidden neuron. Neuron attribution is excel-\nlent when combined with layer attribution methods because it\ncan first inspect all the neurons in the layer. Also, a neuron\nattribution technique can be used to understand what a partic-\nular neuron is doing. Each category has a set of functions that\nprovide inside information for the impact of the feature, layer,\nand neurons on the output. We selected Integrated Gradients\n(IG) [27] and NoiseTunnel [28] attributions to understand the\ncontribution of each input feature better.\nIntegrated Gradients (IG) is a technique that aims to ex-\nplain the relationship between model predictions in terms of\ntheir features by highlighting them. This is done by comput-\ning the gradients of the model's prediction output to its input\nfeatures (see Equation 1). There is no need for any modifica-\ntion to the original deep neural network, and it can be applied\nto images, text, or structured data. In Equation 1 m defines\na number of interpolation steps, \u2202 is a variance of the current\nimage, F is a function representing our model, x is an input,\nand x' is the baseline.\n$IGPx (x) = (xi - x) \\sum_{k=1}^{m} \\frac{\\partial F(x' + \\frac{k}{m} \u00d7 (x-x'))}{\\partial xi} \\frac{1}{m}$ (1)\nNoiseTunnel is a technique that improves the accuracy of\nattribution methods. It adds Gaussian noise N(0, 0.012) to\neach input and applies the given attribution algorithm to each\nsample.\n$Mc(x) = \\frac{1}{n} \\sum_{i=1}^{n}Mc(x + N(0, \u03c3\u00b2))$ (2)\nSaliency maps is another technique that highlight the most\nrelevant regions or features within an image in a given model.\nIt computes the gradient of the model's output score with re-\nspect to the input features.. The magnitude of these gradients\nindicates how much the output would change if the input fea-\ntures were modified slightly. Common methods are absolute\ngradient values, gradient normalization, and relevance mask-\ning. In our case absolute gradient technique was used.\nAttribution-based Confidence (ABC) metric [29] serves as\na quantitative measure to assess the reliability of deep neu-\nral network (DNN) outputs on input data. This innovative\nmethod introduces an approach to estimate DNN prediction\nconfidence utilizing attribution techniques, such as IG, to as-\ncertain the confidence level associated with DNN predictions.\nHere's how these components work together in detail Given\nan input sample x to the DNN, IG is applied to compute the\nattribution map A(x). This attribution map indicates the im-\nportance of each input feature in influencing the DNN's pre-"}, {"title": "4. EXPERIMENTS", "content": "In this section, we will outline to dataset collected to train,\ntest, and validate our Nueral-SDE convnext model, along with\na set of experiments performed to compare model robustness\nto noise between the the our Neural-SDE and the original\nNon-Neural SDE variants of the ConvNext model. We per-\nform three types of evaluation to show the efficacy of our\nNueral-SDE method: Accuracy comparison in the presence\nof random noise, a robustness measure in the presence of\nadversarial noise, and an overall Attribute Based Confidence\n(ABC) comparison between the two methodologies."}, {"title": "4.1. Data Source", "content": "The data, the harmonic signals dataset [30] contains known\nsignals with clean collection at injection and varying noise\nat other collection locations. Simulated waveforms were in-\njected into ORNL buildings of a user facility (Figure 3). The\nwaveforms include sine, square, square (75/25 duty), and tri-\nangle waves and are injected at different frequencies (0.5-50\nkHz). The signals are then subsequently measured with sen-\nsors at six locations through building power 3. 16 total classes\nwere collected, including muxed combinations of the pure\nwaveforms, and the Short-Term Fourier transform (STFT)\nwas used to transform the time-series collected data into a"}, {"title": "4.2. Processing and Training", "content": "The goal is to train more robust and readily explainable neural\nnetworks by injecting appropriately shaped noise during their\ntraining. The baseline models used in this work are ResNet,\nResNet-50, and ConvNeXt-Base. Figure 2 presents the pro-\ncess overview, denoting how and where noise is injected dur-\ning training to produce a Nueral-SDE from a generic res-net\nor ConvNext architecture. From our experimental results,\nwe posit that ResNets with stochastic noise injected into the\nresidual layers of our neural SDEs create more robust attribu-\ntions. We show that the logarithm of the sum of the change in\nattributions is smaller for neural SDEs than for neural ODES.\nAs seen in figure 2 and 4 the integrated gradient attribution\nwith noise tunnel for our neural SDE approach (bottom right)\nis visually sharper than the IG attribution as well as IG cou-\npled to a noise tunnel for the neural ODE (bottom left) The\nData from 4.1 randomly partitioned into 70%-15%-15% train-\ning, testing, and validation subsets, respectively."}, {"title": "4.3. Results", "content": "Experiment One evaluates model performance in terms of ac-\ncuracy in the presence of random noise and adversarial noise\nrobustness. We provide accuracy comparisons in Table 1\nfor the baseline ConvNeXt-Base model and our Neural SDE\nvariant. Note that the accuracy of the Neural SDE model is\nslightly less than that of the ConvNeXt-Base model. This is\ndue to the regularization incurred by the noise injection, and\nis an expected tradeoff for robustness."}, {"title": "5. CONCLUSION", "content": "From our experiments, we show that a Neural-SDE variant\nof the ConvNext Res-Net architecture can provide improved\nrobustness and attribution stability for spectrogram classifica-\ntion in the presence of signal noise, with minimal amounts of\ntrade-off in accuracy. This relatively minor modification to\nthe res-net architecture can provide benifits even in low-data-\navailability scenarios, as shown by using our relatively small\ndataset for training and testing. Additionally, with Neural-\nSDES, IG and Noise tunneling can successfully identify the\nimportant signal features in STFT spectrogram images. Al-\nthough these preliminary results are promising, accuracy rates\nin the presence of noise and adversarial input are still too low\nto be reliable. We hope to improve the efficacy of the NSDE\nmethod by investigating improved noise shaping and training\naugmentation schemes that could further bolster performace\nin low-data-availability or few-shot-learning settings."}]}