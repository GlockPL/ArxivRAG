{"title": "Methods and Trends in Detecting Generated Images: A Comprehensive Review", "authors": ["ARPAN MAHARA", "NAPHTALI RISHE"], "abstract": "The proliferation of generative models, such as Generative Adversarial Networks (GANs), Diffusion Models, and Variational Autoencoders (VAEs), has enabled the synthesis of high-quality multimedia data. However, these advancements have also raised significant concerns regarding adversarial attacks, unethical usage, and societal harm. Recognizing these challenges, researchers have increasingly focused on developing methodologies to detect synthesized data effectively, aiming to mitigate potential risks. Prior reviews have primarily focused on deepfake detection and often lack coverage of recent advancements in synthetic image detection, particularly methods leveraging multimodal frameworks for improved forensic analysis. To address this gap, the present survey provides a comprehensive review of state-of-the-art methods for detecting and classifying synthetic images generated by advanced generative AI models. This review systematically examines core detection methodologies, identifies commonalities among approaches, and categorizes them into meaningful taxonomies. Furthermore, given the crucial role of large-scale datasets in this field, we present an overview of publicly available datasets that facilitate further research and benchmarking in synthetic data detection.", "sections": [{"title": "1 Introduction", "content": "The advent of advanced generative models has enabled the creation of highly realistic synthetic images. These images\nare synthesized through various approaches, including conditional methods such as image-to-image translation and\ntext-to-image translation, as well as unconditional generation. The present survey focuses on the detection of fully\nsynthesized images, distinguishing them from subtle manipulations or partial modifications. While prior survey have\naddressed the detection of image manipulations achieved through traditional computer-based techniques [81], image\nediting tools [104], and deepfake methodologies [92], our focus diverges from these areas. Specifically, we examine\ndetection methods designed to identify images synthesized by state-of-the-art generative models [64, 74, 76, 95],\nincluding recent approaches that incorporate multimodal techniques [70] for enhanced detection accuracy. Given the\nincreasing relevance of such methods in forensic image analysis, it is crucial to explore their long-term applicability\nand effectiveness in generative image detection.\nThe detection of images generated by both conditional and unconditional generative methods has emerged as\na rapidly growing research area. However, despite this increasing interest, a comprehensive review consolidating\nand systematically categorizing existing detection methodologies remains lacking. This survey addresses this gap by\nproviding a structured analysis of detection approaches, classifying them based on their underlying techniques and\ncontributions.\nGenerative Adversarial Networks (GANs). GANs were first proposed by Goodfellow et al. [32]. The architecture\nconsists of two neural networks, a generator G and a discriminator D, that compete in an adversarial game, optimizing\nthe adversarial loss:\n$\\min_{G} \\max_{D} E_{x \\sim p_{data} (x)} [\\log D(x)] + E_{z \\sim p_{z}(z)} [\\log(1 \u2013 D(G(z)))].$\n(1)"}, {"title": "2 Reviews", "content": "In the present survey of detection methods for AI-generated images, we categorize them into six distinct groups, with\nthe final category comprising commercial methods. For each group, we discuss the core proposal of each method in\nascending order based on publication date. At the end of each subsection, we present a table summarizing whether the\nexperimental evaluations of the methods satisfy three key criteria, described as follows:\n1. Cross-Family Generators: This criterion evaluates whether a detection method, trained on images from one type\nof generative model (e.g., GAN), was tested on and demonstrated effectiveness in detecting images generated by a\ndifferent type of generative model (e.g., Diffusion models). A method that evaluates images from multiple generative\nmodel types satisfies the cross-family criterion."}, {"title": "2.1 Spatial Domain Analysis / Spatial Feature Analysis Methods", "content": "Spatial domain analysis methods focus on detecting generated images by analyzing intrinsic features within the\npixel-level spatial representation. These methods leverage spatial patterns such as texture irregularities, unnatural edge\nformations, and color inconsistencies, which are common artifacts introduced during the image synthesis process. By"}, {"title": "2.1.1 Co-occurrence Matrices On Detecting GAN Generated Images by Nataraj et al.: 2019", "content": "Nataraj et al. [63] proposed a\nGAN-generated image detection method using co-occurrence matrices extracted from RGB channels. Unlike forensic\ntechniques that used such matrices, their approach bypasses residual computations and directly processes pixel co-\noccurrence matrices using a deep convolutional neural network (CNN). The method computes co-occurrence matrices\non the red, green, and blue channels of an image, capturing statistical anomalies introduced by GAN-generated images.\nThese matrices are structured into tensors, which are subsequently processed through a custom CNN. The network\nconsists of convolutional layers with ReLU activations, max pooling operations, and fully connected layers, concluding\nwith a sigmoid activation for binary classification. The evaluation was conducted on two diverse datasets: CycleGAN\n[106] (unpaired image-to-image translations) and StarGAN [18] (facial attribute manipulations)."}, {"title": "2.1.2 CNN-Generated Images Are Surprisingly Easy to Spot: Wang et al. (2020)", "content": "Wang et al. [91] conducted a pivotal\nstudy demonstrating that a detection method trained on images generated by a single generative model, particularly a\nGAN, can generalize to detect synthetic images from a variety of unseen CNN-based generative models. This finding\nchallenges the previously established view that cross-model generalization is inherently difficult for forensic classifiers.\nTo evaluate this, the authors trained ProGAN [41] on the LSUN dataset [99], producing a dataset of 720K images for\ntraining and 4K for validation, with an equal split between real and generated images. They employed a ResNet-50\n[36], pretrained on ImageNet, as a binary classifier. Robust feature learning was facilitated through various data\naugmentation strategies, including: (a) no augmentation, (b) Gaussian blur, (c) JPEG compression, and (d) combinations\nof both with varying probabilities (50% and 10%). The trained classifier demonstrated strong generalization capabilities,\nsuccessfully detecting images synthesized by other prominent generative models such as StyleGAN [42], BigGAN [8],\nCycleGAN [106], StarGAN [18], and GauGAN [68]. This work highlights the existence of common artifacts across\nCNN-generated images, suggesting that classifiers can leverage these shared patterns for generalizable detection across\ndifferent architectures and tasks."}, {"title": "2.1.3 Detection and Attribution of GAN Images: Goebel et al. (2020)", "content": "Goebel et al. [31] introduced a comprehensive\nframework to detect, attribute, and localize GAN-generated images through co-occurrence matrix analysis and deep\nlearning. This method builds on insights from steganalysis, leveraging pixel-level co-occurrence features to identify\nartifacts introduced during image synthesis. The approach begins by computing co-occurrence matrices from the RGB\nchannels of the input image in four orientations: horizontal, vertical, diagonal, and anti-diagonal. Each co-occurrence\nmatrix captures a 256\u00d7256 histogram of pixel value pairs, normalized and stacked into a tensor of size 256 \u00d7 256 \u00d7 12.\nThe matrices are defined as:\n$C_{i,j} = \\sum_{m,n} \\begin{cases}\n1, & \\text{if } I[m, n] = i \\text{ and } I[m + 1, n] = j\\\\\n0, & \\text{otherwise}.\n\\end{cases}$\nThese features are then processed using a modified XceptionNet [19], designed for three key tasks: 1. Binary Detection:\nClassifying images as real or GAN-generated. 2. Multi-Class Attribution: Identifying the GAN architecture (e.g., ProGAN,\nCycleGAN). 3. Localization: Generating heatmaps to identify manipulated regions through patch-based analysis."}, {"title": "2.1.4 Estimating Artifact Similarity with Representation Learning: Li et al. (2022), GASE-Net", "content": "Li et al. [49] introduced\nGASE-Net, a framework designed to detect GAN-generated images by estimating artifact similarity. This method\naddresses challenges in cross-domain generalization and robustness against post-processing, using a two-stage approach:\nrepresentation learning and representation comparison. In the representation learning stage, a ResNet-50 backbone is\nmodified with instance normalization (IN) applied to the shallow layers to enhance feature extraction by filtering out\ninstance-specific biases. This ensures that the learned representations remain invariant across different domains while\nretaining category-level distinctions. Feature maps from reference images are aggregated element-wise to form domain\nprototypes, which serve as robust representations of GAN or pristine image domains.\nIn the representation comparison stage, the feature map of a consult (suspicious) image is concatenated with the\ndomain prototypes along the channel dimension. A shallow CNN processes the concatenated tensor to output similarity\nscores. The network is optimized using a Category and Domain-Aware (CDA) loss, which maximizes inter-class\nseparation and minimizes intra-class variation by leveraging both domain and category information. The ground truth\nsimilarity scores $v_{\\text{true}}$ for optimization are defined as:\n$v_{n} = \\begin{cases}\n1, & \\text{if } y^* = y_n,\\\\\n0, & \\text{if } y^* \\neq y_n,\n\\end{cases}$\nwhere $y^*$ and $y_n$ denote the category labels of the consult image and the n-th domain prototype, respectively. The\npredicted similarity scores $v_{\\text{pred}}$ are optimized against $v_{\\text{true}}$ using Mean Square Error (MSE) loss.\nDuring inference, similarity scores are averaged across GAN-generated and pristine domains. An image is classified as\nGAN-generated if the average fake score exceeds the pristine score. Extensive experiments demonstrated that GASE-Net\noutperforms state-of-the-art methods in cross-domain scenarios, preserving resilience against various post-processing\ntechniques, including JPEG compression, Gaussian blur, and resizing."}, {"title": "2.1.5 Local Intrinsic Dimensionality Analysis: Lorenz et al. (2023), AdaptedMultiLID", "content": "Lorenz et al. [55] proposed a\nframework utilizing the Multi-Local Intrinsic Dimensionality (multiLID) method for detecting diffusion-generated\nimages. This approach builds upon the earlier work [56] and demonstrates strong performance in distinguishing both\ndiffusion and GAN-generated images. The method starts by extracting low-dimensional feature maps using an untrained\nResNet18 model. MultiLID is then computed to capture local growth rates of feature densities within the latent space.\nThe multiLID feature vector for each sample x is defined as:\n$\\text{multiLID}(x) [i] = - \\log \\frac{d_i(x)}{d_k(x)}$\nwhere $d_i (x)$ and $d_k (x)$ represent the Euclidean distances to the $i^{th}$ and $k^{th}$ nearest neighbors, respectively. A random\nforest classifier is trained on the labeled multiLID scores to perform image classification. Extensive experiments validate\nthe effectiveness of this method, with high detection accuracy achieved for both diffusion and GAN-generated images"}, {"title": "2.1.6 Al-Generated Image Detection: Baraheem et al. (2023)", "content": "Baraheem et al. [4] proposed a framework to detect\nGAN-generated images using transfer learning on pretrained classifiers. The authors compiled a diverse dataset called\nReal and Synthetic Images (RSI), consisting of 48,000 images synthesized by 12 GAN architectures across tasks such\nas image-to-image, sketch-to-image, and text-to-image generation. EfficientNetB4 [85] achieved the best detection\nperformance after fine-tuning on the RSI dataset. The model's architecture was modified by replacing the classifier\nhead with layers for global average pooling, batch normalization, ReLU activation, dropout, and a sigmoid output. The\ntraining utilized the Adam optimizer with a batch size of 64, an initial learning rate of 0.001, and data augmentation\ntechniques such as horizontal flips. To facilitate model explainability, the authors incorporated four Class Activation\nMap (CAM) methods, GradCAM [80], AblationCAM [72], LayerCAM [39], and Faster ScoreCAM [90], to visualize the\ndiscriminative regions influencing classification decisions."}, {"title": "2.1.7 Diffusion Reconstruction Error (DIRE): Wang et al. (2023)", "content": "Wang et al. [93] proposed Diffusion Reconstruction\nError (DIRE), a novel method to detect diffusion-generated images by leveraging reconstruction errors from pre-trained\ndiffusion models. The method addresses the limitations of previous detectors, which struggled to generalize across\ndifferent diffusion models. The key idea is that diffusion-generated images can be reconstructed more accurately by the\npre-trained DDIM model [83] than real images. DIRE is defined as the $L_1$-norm of the difference between the input\nimage $x_0$ and its reconstructed counterpart $x_0'$:\n$\\text{DIRE}(x_0) = ||x_0 - x_0'||_1.$\nThe process involves applying forward noise to the input image and then performing a reverse denoising process to\ngenerate the reconstruction. A ResNet-50 classifier is trained using binary cross-entropy loss on these DIRE representa-\ntions to distinguish between real and generated images. The authors demonstrated that DIRE achieves state-of-the-art\nperformance on their proposed DiffusionForensics dataset, which includes images generated by various diffusion\nmodels across multiple domains (e.g., LSUN-Bedroom [99], ImageNet [78], and CelebA-HQ [41]). Extensive experiments\nshowed that DIRE not only excels in detecting diffusion-generated images but also generalizes well to unseen models\nand maintains robustness under perturbations like Gaussian blur and JPEG compression."}, {"title": "2.1.8 Classification and Explainable Identification: Bird and Lotfi (2024)", "content": "Bird and Lotfi [6] introduced a framework\nfor detecting AI-generated images using a large-scale dataset, CIFAKE, which is detailed in the datasets section. Their\nclassification approach employs a Convolutional Neural Network (CNN) that processes images through stacked convolu-\ntional and pooling layers, followed by fully connected layers with a final sigmoid activation for binary classification. As\nseen in [4], the study emphasizes explainability by implementing Gradient Class Activation Mapping (Grad-CAM) [80],\nwhich generates heatmaps to highlight regions influencing the model's decisions. Grad-CAM computes importance\nweights $a_k$ for each feature map $A_k$, producing a visual explanation as:\n$L_{\\text{Grad-CAM}} = \\text{ReLU} \\Big(\\sum_k a_k A_k\\Big), \\quad a_k = \\frac{1}{Z} \\sum_{i,j} \\frac{\\partial y^c}{\\partial A_{k}^{ij}},$\nwhere $Z$ is the spatial size of the feature map. The heatmaps reveal that the model focuses on subtle imperfections,\noften in the image background, to differentiate between real and synthetic images. This approach improves trust in"}, {"title": "2.1.9 Self-Contrastive Learning on ImagiNet: Boychev et al. (2024)", "content": "Boychev et al. [7] introduce a large-scale dataset,\nImagiNet, consisting of 200,000 real and synthetic images, as detailed in the datasets section. The detection framework\nconsists of two stages: pretraining with Self-Contrastive Learning (SelfCon) and a calibration step. During the pretraining\nphase, the ResNet-50 [36] backbone, initialized with ImageNet weights, is paired with a sub-network that projects\nintermediate feature maps into a shared latent space. This setup produces two output embeddings per input image,\nfacilitating contrastive learning. The SelfCon loss is defined as:\n$L_{\\text{SelfCon}} = \\sum_{i \\in A, \\omega' \\in \\Omega} \\frac{1}{|P(i)| |\\Omega(i)|} \\sum_{p \\in P(i), \\omega' \\in \\Omega} \\log \\frac{\\exp(\\omega(x_i) \\cdot \\omega'(x_p)/\\tau)}{\\sum_{l \\in \\Omega(i)} \\exp(\\omega(x_i) \\cdot \\omega'(x_l)/\\tau)},$\nwhere A represents the set of anchor images in a batch, P(i) denotes positive samples for anchor $x_i$, $\\Omega(i)$ contains\nnegative samples, and $\\omega(x)$ is the normalized embedding. The method balances feature similarities and differences\nusing a temperature parameter $\\tau$.\nIn the calibration step, the sub-network and projection heads are removed, and a multilayer perceptron (MLP)\nclassifier is trained using cross-entropy loss on an equal number of real and synthetic images. This stage enhances\nrobustness by fine-tuning the learned features for both detection and model identification tasks. Experimental results\nindicate that the framework achieves up to 0.99 AUC and 95% balanced accuracy, demonstrating robust zero-shot\nperformance on various synthetic image benchmarks."}, {"title": "2.2 Multimodal Vision-Language Methods", "content": "Multimodal vision-language methods leverage vision-language models (VLMs) trained\non large-scale image-text datasets to detect generated images by aligning visual and textual features. These approaches\ndetect inconsistencies in synthetic images through cross-modal embeddings, enabling effective detection across a wide\nrange of generative methods. Models such as CLIP [70] are commonly employed in transfer learning setups, with novel adaptations to distinguish real"}, {"title": "2.2.1 Universal Fake Image Detector by Ojha (2023)", "content": "Ojha et al. [65] identified that existing fake image detectors\nstruggle with generalization, often misclassifying images from unseen generative models as real. This limitation arises\nfrom classifiers being asymmetrically tuned to detect artifacts specific to training data. To address this, the authors\npropose constructing a meaningful feature space using CLIP:ViT [28, 70], trained on 400 million image-text pairs. They\nemploy two approaches: (a) mapping training images to CLIP's final layer to create feature representations, then during\ninference, classifying images based on cosine distance to the nearest neighbor in real and fake feature spaces, and (b)\naugmenting CLIP with a linear layer for binary classification. Both methods demonstrated generalization, effectively\ndetecting synthetic images from state-of-the-art generative models."}, {"title": "2.2.2 Language-Guided Synthetic Image Detection by Wu (2023)", "content": "Wu et al. [94] propose a language-guided approach for\ndetecting synthetic images by integrating image-text contrastive learning in a VLM. They reformulate the detection task\nas an identification problem, determining whether a query image aligns with an anchor set of text-labeled images. This\nmethod enhances generalization by aligning visual features with carefully designed textual labels such as \"Real Photo,\"\n\"Real Painting,\" \"Synthetic Photo,\" and \"Synthetic Painting.\" The authors found these labels more effective than simpler\n\"real\" or \"fake\" categories, as they account for differences in image types, such as camera-captured versus digitally\ncreated content. The proposed LASTED framework encodes images using a ResNet-50x64 vision encoder and text labels\nusing a transformer-based text encoder. During training, both encoders generate visual and textual representations, $e_v$\nand $e_t$, respectively. A contrastive loss aligns these features, ensuring that matched pairs have higher similarity scores\nthan unmatched ones:\n$L_i = - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(e_{v,i} \\cdot e_{t,i}/\\tau)}{\\sum_{j=1}^{N} \\exp(e_{v,i} \\cdot e_{t,j}/\\tau)}$\nwhere $\\tau$ is a temperature parameter."}, {"title": "2.2.3 GenDet: Good Generalizations by Zhu (2023)", "content": "Zhu et al. [107] address the challenge of detecting synthetic\nimages from unseen generators, which existing methods struggle to classify due to limited generalization. The authors\npropose GenDet, a detection model composed of two key components: Teacher-Student Discrepancy-Aware Learning\nand Generalized Feature Augmentation. These components are trained through an adversarial framework to improve\ngeneralization to both seen and unseen generators. The model employs a feature extractor E, based on CLIP:ViT [70],\nto extract features from input images. The Teacher-Student Discrepancy-Aware Learning is designed to: a) Reduce the\ndifference in output between a teacher network ($N_t$) and a student network ($N_s$) for real images, b) Amplify this\ndifference for fake images to enhance detection. The discrepancy losses are defined as:\n$L_{\\text{min}} = \\frac{1}{b} \\sum_{i=1}^{b} |N_t(f_i) - N_s(f_i)|^2, L_{\\text{max}} = -L_{\\text{min}}$\nwhere $f_i$ represents features from real or fake images, and b is the batch size.\nTo further enhance generalization, the Generalized Feature Augmenter adversarially generates augmented features.\nThis augmenter facilitates the depletion of the difference between the teacher and student networks for augmented fake\nfeatures, encouraging the model to detect unseen synthetic images by maintaining a large discrepancy during inference.\nFinally, a binary classifier $N_c$ is trained on the variation between the teacher and student outputs to classify images as\nreal or fake. Experiments demonstrate that GenDet achieves state-of-the-art performance on the UniversalFakeDetect\nand GenImage datasets, surpassing prior methods in both accuracy and mean average precision (mAP)."}, {"title": "2.2.4 Mixture of Low-Rank Experts: Liu 2024", "content": "Liu et al. [54] propose a transferable AI-generated image detection model\nutilizing CLIP-ViT as the backbone with parameter-efficient fine-tuning. The method modifies the MLP layers of the\nlast three ViT-B/32 blocks through a Mixture of Low-Rank Experts (MoLE), integrating both shared and separate\nLow-Rank Adapters (LoRAs). Shared LoRAs capture common feature representations across datasets, while separate\nLoRAs specialize in diverse generative patterns. A trainable gating mechanism dynamically assigns input tokens to\nappropriate experts, with a load-balancing loss ensuring uniform expert utilization. The model freezes most CLIP\nparameters, adapting only LoRA modules and a new MLP classification head with a sigmoid activation. The forward\noperation in each MLP block is expressed as:\n$\\Delta Wx = B_\\alpha Ax + \\sum_{i=1}^N G_i(x) B_i A_i x,$\nwhere $G_i (x)$ is the gating function, and A, B, $A_i$, and $B_i$ are low-rank matrices.\nThe approach achieves state-of-the-art generalization across unseen diffusion and autoregressive models, with\nsuperior robustness to post-processing perturbations like Gaussian blur and JPEG compression. Experimental results"}, {"title": "2.2.5 MERGING A MIXTURE OF HYPER LORAS: HYPERDET by Cao 2024", "content": "Cao et al. [10] propose a generalizable\nsynthetic image detection framework, HyperDet, leveraging the large pretrained multimodal model, CLIP: ViT-L/14,\nas the backbone, similar to Liu et al. [54], but with few notable innovations. Unlike Liu et al., the authors introduce\na novel grouping of Spatial Rich Model (SRM) filters into five distinct groups to generate multiple filtered views of\ninput images, capturing varying levels of pixel artifacts and features. Along this, HyperDet employs Hyper LoRAs, a\nhypernetwork-based approach that generates Low-Rank Adaptation (LoRA) weights for fine-tuning the CLIP model.\nThese LoRA weights are computed using three types of embeddings: task embeddings, layer embeddings, and position\nembeddings. The outputs of these LoRA experts are merged to form a unified representation for classification, effectively\nintegrating shared and specific knowledge for generalizable feature extraction. During training, HyperDet fine-tunes\nthe last eight fully connected layers of the CLIP: ViT-L/14 model, along with the newly introduced Hyper LoRAs\nmodules. To address imbalanced optimization, the framework employs a composite binary cross-entropy loss function,\nincorporating both original and filtered views of the images. This design obtains robust performance in detecting\nsynthetic images across diverse generative models and datasets."}, {"title": "2.2.6 Forgery-Aware Adaptive Transformer (FatFormer) by Liu: 2024", "content": "Liu et al. [52] introduce FatFormer, a generalizable\nsynthetic image detection framework utilizing the pretrained CLIP model inspired by the work of Ojha et al. [65].\nThe authors address the limitations of freezing CLIP's layers, which hinders the generalization of forgery detection.\nFatFormer integrates two modules: the Forgery-Aware Adapter (FAA) and Language-Guided Alignment (LGA), for\neffective adaptation of CLIP's features. The FAA module extracts forgery artifacts from both image and frequency\ndomains. The Image Forgery Extractor applies lightweight convolution layers to capture low-level artifacts, while\nthe Frequency Forgery Extractor employs Discrete Wavelet Transform (DWT) and grouped attention mechanisms\nto dynamically aggregate multi-band frequency clues. The final adapted feature representation at each ViT stage is\ndefined as:\n$\\mathbb{F}^{(j)} = \\alpha \\mathbb{F}_{img}^{(j)} + (1-\\alpha) \\mathbb{F}_{freq}^{(j)}$\nwhere $\\alpha$ balances image and frequency contributions.\nThe LGA module enhances text prompts using a Patch-Based Enhancer (PBE) and aligns image patch tokens with\ntext embeddings through the Text-Guided Interactor (TGI). Contrastive loss is applied on the cosine similarities between\nimage and text embeddings:\n$\\mathbb{S}^{(i)} = \\cos \\Big(\\hat{f}_{img}^{(i)}, \\hat{f}_{text}^{(i)}\\Big), \\qquad S^{(i)} = \\frac{1}{N} \\sum_{i=1}^N \\cos \\Big(\\hat{f}_{img}^{(i)}, \\hat{f}_{text}^{(i)}\\Big),$\nwhere N is the number of patches."}, {"title": "2.2.7 Raising the Bar with CLIP: By Cozzolino 2024", "content": "Cozzolino et al. [22] implement the CLIP: ViT-L/14 pretrained\nVLM, similar to the approaches in [10, 45, 54], for detecting synthetic images with a straightforward yet impactful\nadjustment. The authors propose generating synthetic images by feeding real-image captions to text-to-image models\nand then extracting feature vectors for both real and synthetic images using CLIP's image encoder. Specifically, feature\nvectors are obtained from the second-to-last layer of the ViT module:"}, {"title": "2.2.8 Representation from Encoder-Decoder for Image Detection by Koutlis: 2025", "content": "Koutlis and Papadopoulos [45] propose\nRepresentations from Intermediate Encoder-Blocks (RINE) to improve synthetic image detection by extracting low-level\nfeatures from multiple layers of CLIP's Vision Transformer (ViT). The method captures both low- and high-level\nvisual semantics by concatenating CLS tokens from each intermediate transformer block into a comprehensive feature\nrepresentation. The extracted CLS tokens from each block $Z_l$ are aggregated:\n$K = \\underset{l=1}{\\overset{n}{||}} Z_l^{[0]} \\in \\mathbb{R}^{b \\times n \\times d}$\nwhere $Z_l^{[0]}$ denotes the CLS token from the l-th transformer block. To improve feature selection, a Trainable Importance\nEstimator (TIE) dynamically assigns weights to these representations:\n$K_{ik} = S(A_{lk}) \\cdot K_{ilk},$\nwhere S(Alk) represents softmax-activated importance scores for each block.\nThe features are processed through a projection network, then passed to a classification head with ReLU-activated\ndense layers and a final sigmoid output for binary classification. The framework optimizes performance using Binary\nCross-Entropy (BCE) and Supervised Contrastive Loss:\n$L = L_{CE} + \\lambda L_{Cont}.$\nwhere $\\lambda$ balances the contributions of both objectives. The authors demonstrate that RINE surpasses state-of-the\nart methods on 20 test datasets, achieving a +10.6% accuracy improvement, with training requiring only one epoch\n(approximately 8 minutes). Additionally, the model is robust to image perturbations, maintaining strong performance\nacross GAN, diffusion, and other synthetic image types."}, {"title": "2.3 Frequency Domain Analysis Methods", "content": "Frequency domain analysis transforms image data into the spectral domain, facilitating the detection of periodic\nartifacts, noise distributions, and variations in frequency components often associated with synthetic image generation.\nTechniques such as the Discrete Fourier Transform (DFT), Discrete Wavelet Transform (DWT), and Discrete Cosine\nTransform (DCT) are commonly used to extract these spectral features. The Fourier Transform, proposed by Fourier [5],\ndecomposes signals into their constituent frequencies and is effective in identifying global periodic patterns, including\ncheckerboard artifacts. The two-dimensional Discrete Fourier Transform (DFT) of an image $f (x, y)$ of size M \u00d7 N is\ngiven by:"}, {"title": "2.3.1 Simulation Artifacts and Detection in Frequency Domain by Zhang: 2019", "content": "Zhang et al. [103] proposed a detection\nframework leveraging frequency-domain analysis to identify GAN-generated images. Their key contributions include\nthe introduction of AutoGAN, a GAN simulator that mimics artifacts found in diverse GAN architectures, and the use\nof frequency spectrum inputs instead of spatial features for classification. AutoGAN generates synthetic images by\nsimulating the up-sampling artifacts present in GAN-generated images, independent of specific GAN architectures. The\ngenerator applies adversarial loss as shown in Table 1 and an $l_1$-norm loss function to minimize differences between"}, {"title": "2.3.2 Fourier Spectrum Discrepancies: Dzanic 2020", "content": "Dzanic et al. [29] presented a method for analyzing high-frequency\nFourier models to highlight the limitations of generative models, such as GANs and VAEs, in reconstructing high-\nfrequency components. Their approach involves applying the Fourier Transform to images to obtain a reduced spectrum,\nwhich is then modeled using two decay parameters: $b_1$, representing the magnitude of high-frequency content, and $b_2$,\nrepresenting the decay rate. These parameters were used to train a KNN classifier capable of distinguishing synthetic\nimages from real ones, achieving 99.2% accuracy on uncompressed high-resolution images. The process involves\nnormalizing the Fourier transform by the DC gain, converting the data to normalized polar coordinates, binning\nand averaging Fourier coefficients to create a reduced spectrum, and fitting the decay parameters above a threshold\nwavenumber. This comprehensive method underscores the effectiveness of frequency domain features in identifying\nthe discrepancies characteristic of synthetic image generation."}, {"title": "2.3.3 Liu's Detection Method Derived from Analysis on Real Images: 2022", "content": "Liu et al. [51] proposed a novel approach\nto detecting synthetic images by focusing on the inherent noise patterns of real images, deviating from existing\nmethods that analyze artifacts in generated images. They introduced the concept of Learned Noise Patterns (LNP), a\nhigh-dimensional spatial mapping derived from neural networks, to characterize the noise properties of real images. By\ncomparing these learned patterns with the noise present in synthetic images, the method identifies discrepancies that\nindicate image generation. Leveraging both spatial and frequency domain representations, this approach demonstrated\nimproved accuracy in detecting synthetic images across multiple domains."}, {"title": "2.3.4 Two-Stream Convolutional Network for Fake Content Detection by Yousaf: 2022", "content": "Yousaf et al. [98] proposed\nTwoStreamNet, a two-stream convolutional neural network designed to enhance the generalizability of fake visual\ncontent detection by jointly analyzing spatial and frequency features. The network comprises two main modules: the\nSpatial Stream and the Frequency Stream, which independently process input images and fuse their outputs at the\nclassification stage to improve detection accuracy.\nThe Frequency Stream captures frequency domain artifacts by first converting images to the YCbCr color space to\ndecorrelate color channels. Discrete Fourier Transform (DFT) and Discrete Wavelet Transform (DWT) are applied\nto each channel. DFT decomposes image signals into real and imaginary components, while DWT captures\nboth low- and high-frequency sub-bands. The resulting frequency features, represented as H \u00d7 W \u00d7 18 feature maps,\nare processed using ResNet-50 [36] to extract discriminative frequency patterns. The Spatial Stream focuses on spatial\ndomain features by processing original RGB images augmented with Gaussian blur and JPEG compression, similar\nto the approach in [91]. These augmented images are passed through a ResNet-50 network to extract spatial features.\nFinally, the outputs of the two streams are fused via probability averaging, ensuring equal contributions from spatial and\nfrequency domains to the final decision. This combined framework highlights the importance of integrating frequency\nfeatures for robust detection of synthesized visual content."}, {"title": "2.3.5 Synthbuster by Bammey: 2023", "content": "Bammey [3] introduced Synthbuster, a forensic method for detecting synthetic\nimages generated by state-of-the-art diffusion models. The method begins by applying a cross-difference filter, originally"}, {"title": "2.3.6 Meng's Artifact Feature Purification: 2024", "content": "Meng et al. [60] addressed two key limitations in existing detection\nmethods: poor generalization across generative models and limited effectiveness on images from diverse large-scale\ndatasets. To overcome these challenges, they proposed the Artifact Purification Network (APN), which extracts gen-\neralizable artifact features through explicit and implicit purification processes. Explicit purification isolates artifact\nfeatures in spatial and frequency domains by employing feature decomposition and frequency-band proposals to detect\nsuspicious patterns. Implicit purification, guided by a classifier, further refines these features using mutual informa-\ntion estimation, enhancing the robustness of detection across various generators and datasets. This dual purification\napproach significantly improves generalization and detection accuracy."}, {"title": "2.3.7 An Image Transformation Perspective by Li: 20"}]}