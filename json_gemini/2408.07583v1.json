{"title": "Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey", "authors": ["Hamza Kheddar"], "abstract": "With significant advancements in Transformers and large language models (LLMs), natural language processing (NLP) has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in intrusion detection systems (IDSs), focusing on different architectures such as Attention-based models, LLMs like bidirectional encoder representations from Transformers (BERT) and generative pre-trained Transformer (GPT), CNN/LSTM-Transformer hybrids, emerging approaches like Vision Transformers (ViTs), among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, internet-of-things (IoT) devices, critical infrastructure protection, cloud computing, software-defined networking (SDN), as well as in autonomous vehicles (AVs). The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.", "sections": [{"title": "1. Introduction", "content": "In today's swiftly evolving network ecosystem, characterized by the emergence of technologies like 5G and the widespread adoption of the Internet-of-things (IoT) [1], the potential for threats and vulnerabilities has expanded significantly. Consequently, concerns regarding network security are escalating. Network security attacks are diverse and continuously evolving, posing significant threats to the integrity, confidentiality, and availability of information systems. These attacks can be broadly categorized into various types, including malware, phishing, denial-of-service (DoS) attacks, man-in-the-middle (MitM) attacks, and advanced persistent threats (APTs) [2]. Malware encompasses viruses, worms, ransomware, and spyware that infiltrate systems to steal data, disrupt operations, or demand ransoms. Phishing attacks deceive users into revealing sensitive information through fraudulent emails or websites. DoS attacks overwhelm systems with excessive traffic, rendering services unavailable. MitM attacks intercept and manipulate communications between parties, while APTs involve prolonged and targeted cyber-espionage campaigns against specific organizations or individuals. To combat these threats, various attack prevention methods have been developed, each with its unique approach and effectiveness. These methods include firewalls, antivirus software, encryption, intrusion detection system (IDS) [3], intrusion prevention systems (IPS) [4], and information and event management (SIEM) [5] systems. Firewalls act as a barrier between trusted and untrusted networks, controlling incoming and outgoing traffic based on predefined security rules. Antivirus software detects and removes malicious software, while encryption ensures data confidentiality by converting information into unreadable code without the correct decryption key. IDS and IPS monitor network traffic for suspicious activities, with IDS alerting administrators of potential threats and IPS actively blocking malicious traffic. SIEM systems aggregate and analyze log data from various sources to detect and respond to security incidents in real-time. Among these, IDS has advanced significantly with machine learning (ML) and deep learning (DL) integration, enhancing host intrusion detection system (HIDS) and network intrusion detection system (NIDS) [3]. HIDS monitors individual devices, focusing on system logs and behaviors to detect threats like unauthorized file changes. However, it is limited to specific hosts. NIDS, conversely, analyzes network traffic to identify threats like distributed denial-of-service (DDoS) attacks and network scanning, offering a broader security perspective. artificial intelligence (AI) enhances both categories by analyzing vast data to recognize complex threat patterns, improving detection accuracy and adaptability to evolving threats. Their versatility stems from the abundance of network data available for training intrusion scenarios and crafting AI-driven IDS models. Furthermore, advancements in technology have reinforced computational capabilities, facilitating faster and more cost-effective model training. The widespread adoption of DL ensures precise model optimization through continuous self-learning.\nHowever, despite these advancements, current ML or DL-based IDS methods still encounter new challenges, as long as network technologies evolving, such as vulnerability to attacks on central entities, decreased system performance with larger user bases, and"}, {"title": "1.1. Existing reviews and our contributions", "content": "Employing Transformers, and specifically large LLMs, although a trending research topic, is still in its early stages. Consequently, there are few recent reviews, and most of them are preprints that are not yet published. These reviews summarize how NLP algorithms could be exploited for more effective detection and mitigation of insider threats in cybersecurity. However, they tend to focus strongly on specific aspects: the use of generative AI and LLMs to strengthen resilience and security, as seen in [17, 18]; the mitigation of insider threats using NLP and DL, as discussed in [19]; the application of LLMs in cybersecurity tasks such as vulnerability detection and malware analysis, highlighted in [20, 21]; or examining the dual impact of LLMs on security and privacy, as explored in [22]. These reviews are generic and cover only a few works in each area of cybersecurity. In contrast, our survey provides a comprehensive review of empowering IDS using Transformers, which are the main component of LLMs, along with some pre-trained LLMs models such as GPT and BERT. Moreover, this survey may assist researchers in building their own LLMs tailored to intrusion detection by providing possible configurations of the Attention layer and existing Transformer and LLMs models. The key contributions of this survey can be summarized as follows:\n\u2022 The survey delves into the background of IDS and covers various types of attacks. It also provides an in-depth taxonomy of the different IDS techniques, including HIDS and NIDS, employed to secure diverse environments.\n\u2022 The survey offers an extensive taxonomy of different Transformer models applicable to strengthen various IDS techniques, including Attention, CNN/LSTM-Transformer, vision Transformer (ViT), GAN-Transformer, GPT, BERT and their variants, among others.\n\u2022 It presents a detailed classification of various Attention mechanisms and their possible configurations with traditional DL models, such as convolution neural network (CNN).\n\u2022 The survey reviews the latest SOTA schemes proposed for different environments and applications, such as computer networks, the IoT and industrial internet of things (IIoT), critical infrastructure, cloud and software-defined network (SDN), and autonomous vehicless (AVs).\n\u2022 Finally, the survey highlights existing research challenges and suggests potential future directions for the field of Transformers and LLM-based IDS."}, {"title": "1.2. Research questions and objectives", "content": "To streamline this survey, the author defined five research questions in Table 2. By following the study's roadmap, readers will grasp the key insights and comprehend the study's objectives. The table offers a structured summary of the research questions (RQ)"}, {"title": "2. Background", "content": "This section covers the key elements of IDS, starting with intrusion detection methods, followed by an overview of attack categories, the datasets used for evaluation, and the metrics employed to measure IDS performance."}, {"title": "2.1. Intrusion detection", "content": "This part elucidates the application of DL in IDS for detecting cyber-attacks, detailing procedures that can be applied universally across various environments. Figure 3 presents the overall architecture of a generic detection system, which consists of two main phases: data pre-processing and IDS framework development. The IDS framework can be either signature-based or anomaly-based, followed by training and testing. The following sections provide detailed information about each category."}, {"title": "2.1.1. Data pre-processing", "content": "Computational efficiency can be improved and memory usage decreased by downsizing the dataset and removing less significant features. This can be achieved by dropping some dataset's columns that are expected to be unaffected by the attacks being studied, such as media access control (MAC) addresses, timestamp, among others. Encoding feature attributes in processed data can be accomplished using various techniques, depending on the data's nature. Common methods include label encoding, suitable for categorical data when categories are ordinal; one-hot encoding, ideal for categorical data lacking ordinal relationships; and binary encoding, which combines benefits from both label and one-hot encoding. These techniques offer versatile ways to represent categorical features effectively in ML models. z-score normalization, is a technique used to standardize numerical features in a dataset. It involves subtracting the mean (u) of the feature from each data point and then dividing by the standard deviation (\u03c3) of the feature. Mathematically, the formula for z-score normalization is represented as:\n$Z = \\frac{X - \\mu}{\\sigma}$ (1)\nWhere, x is the original value of the feature, u is the mean of the feature, o is the standard deviation of the feature, and z is the standardized value, also known as the z-score. This transformation ensures that the standardized feature has a \u03bc of 0 and a of 1, which is particularly useful for algorithms that assume normally distributed data or require features to be on a similar scale for proper convergence. It also facilitates direct comparison and analysis, while also reducing data bias and enhancing stability and reliability. The classes of data are evaluated to determine if they are balanced or not. If necessary, k-means undersampling [23] is employed to expedite computation when data are balanced. k-means is a commonly used clustering algorithm that divides a dataset into k distinct clusters, reducing data size without losing meaningful information by minimizing the distance between data points and their cluster centers. However, k-means is sensitive to initial cluster centers and noisy data. The mini k-means algorithm [24], an improved version, mitigates these issues by selecting random points as initial cluster centers, assigning data points to the nearest centers, and recalculating centroids until convergence is achieved or maximum iterations are reached. The imbalance data could skew the training towards the more abundant samples, under-training the less represented attack types and reducing the model's overall applicability. To address this, researchers used oversampling techniques to balance the dataset, ensuring the model is adequately trained on all attack types. Synthetic minority oversampling technique (SMOTE) is a widely used method to address data imbalance by generating synthetic samples for minority classes [25], thus balancing the dataset. It works by selecting a sample from the minority class, finding its k nearest neighbors, and generating new synthetic samples by interpolating between them. While the mentioned data pre-processing steps are common in DL-based IDS, they are not necessarily used all at once for the same method. Additionally, the data can be converted to a 2D format to be processed as an image, allowing the application of existing 2D DL algorithms such as ViT and Inception pre-trained models."}, {"title": "2.1.2. \u0422\u0430\u0445\u043e\u043f\u043emy of IDS techniques", "content": "After pre-processing, the researchers apply various DL algorithms to build an efficient DL-based IDS model. This phase involves model training and optimization, including cross-validation, enhanced convergence tools, and grid search techniques to assess the model's performance. The DL-based IDS frameworks can be categorized into two types, depending on the availability of attack type information:\n(a) Signature-based model: Are detection techniques in IDS, which involve matching network traffic or system actions against a database of attack signatures to trigger alerts upon detection. It outlines the process of comparing observed data D with a signature database S = {$s_1, s_2,..., s_n$} using a matching function Match(S, D). While effective against known attacks, this method has limitations in detecting new or mutated attacks and may not be suitable for resource-constrained environments like IoT. Research explores pattern-based detection methods as alternatives to address these challenges. Experts evaluate the trained model's performance on test instances. If the results are unsatisfactory, techniques such as principal component analysis (PCA) may be applied as a corrective measure to reduce the complexity of the data and enhance interpretability, thereby improving the performance of DL algorithms. Once the results meet the accuracy criteria, the model becomes capable of effectively identifying and distinguishing various attack types. ViT and pre-trained models such as Inception are widely used for their ability to capture intricate patterns and features from network traffic data. However, several DL algorithms are tailored for packet classification rather than 2D image representation. These include various 1D DL techniques such as LSTM, RNN, 1D CNN, BERT, and others. Tools like AutoKeras [26] and H2O [27] automate model-building, ensuring optimal architecture selection and hyperparameter tuning. This simplifies the complex task of designing an effective IDS. To enhance the performance of DL algorithms, experts employ optimization techniques such as, but not limited to, AdamW and BO-TPE [28]. AdamW improves convergence by optimizing training trajectories, while BO-TPE selects optimal hyperparameters, fine-tuning algorithm configurations.\n(b) Anomaly-based models: Are detection techniques employed when signature-based IDS techniques fail to detect zero-day threats. This technique monitors normal behavior to detect deviations, alerting anomalies beyond predefined thresholds without classifying specific attacks. Techniques such as statistical thresholding utilize the mean (u) and standard deviation (\u03c3), employing a threshold (Threshold = u + k\u00b7 \u03c3) to detect attacks. Distance metrics measure dissimilarities between observed (O) and baseline (B) behaviors, often using Euclidean distance. Probability density estimation, like Gaussian mixture model (GMM), models normal behavior and computes anomaly scores inversely to probability density function (PDF). While ML/DL models learn normal and attack behaviors, establishing standard profiles proves more effective than solely identifying known attacks. Anomaly-based techniques excel in detecting new threats but struggle with establishing accurate baseline profiles. Once an attack is detected using anomaly-based techniques, it becomes a known attack, as these techniques update the signature dataset with the new signature of the detected threat. Anomaly-based techniques often result in higher false alarm rate (FAR). Research addresses this challenge by employing ML/DL to construct robust normal behavior profiles, utilizing advanced methods such as meta-classifiers and hybrid feature selection [29]."}, {"title": "2.2. Attack categories, datasets, and metrics", "content": "Various datasets in IDS research cater to diverse network environments (e.g., network, host, industrial control system (ICS), IoT, cloud, edge, smart grid), encompassing a wide array of attack types categorized as follows:\n\u2022 DoS: Attacks aim to render systems or networks inaccessible. Examples include Transmission Control Protocol (TCP)/User Datagram Protocol (UDP) flooding, Internet Control Message Protocol (ICMP) Smurf/ping of death, IP teardrop, UDPstorm, domain name system (DNS) amplification, TCP SYN/ACK flood, and HyperText Transfer Protocol (HTTP) Slowloris.\n\u2022 Remote-to-local (R2L): Remote attacks exploit vulnerabilities to gain local access, targeting protocols like File Transfer Protocol (FTP), Internet message access protocol (IMAP), HTTP, Simple Mail Transfer Protocol (SMTP), DNS, X11, and Simple Network Management Protocol (SNMP), with attacks like FTP_write, phf, and SNMPgetattack.\n\u2022 User-to-root (U2R): Exploits system vulnerabilities to escalate privileges, targeting protocols such as UDP, TCP, Perl, Structured Query Language (SQL), and HTTP, with attacks like buffer_overflow, rootkit, HTTPtunnel, and SQLattack.\n\u2022 PROBE: Scans networks for vulnerabilities using tools like IPsweep, nmap, Portsweep, Satan, mscan, and saint to gather information for potential future exploitation.\n\u2022 Enduring: Persistent weaknesses in software, hardware, networks, or practices vulnerable to exploits over time. These vulnerabilities persist despite technological advancements and security measures, exploited for attacks like ransomware and phishing, demanding ongoing vigilance, updates, and robust security practices in IDS.\nFor example, the work in [30] studies the vulnerability of the controller area network (CAN) protocol to various attacks, including flood attacks, fuzzy attacks, spoofing attacks, and replay attacks. Flood attacks, which fall under either DoS or DDoS categories. Fuzzy attacks are categorized as PROBE attacks. Spoofing attacks, which can include Internet Protocol (IP) spoofing, DNS spoofing, email spoofing, etc., are classified based on context as either R2L or U2R attacks. Replay attacks, being a type of man-in-the-middle attack, fall within the R2L category.\nGenerally, the intrusion attacks are either collected from real networks or generated from testbeds and stored as datasets. Our previous work in [3] provided comprehensive insights into numerous IDS datasets. However, this survey introduces additional datasets specifically utilized in research involving Transformers and LLM-based IDS, extending the work already reviewed in this survey with further details cited herein. Table 3 offers a detailed overview of each dataset, including the number of features, duration of the attack, number of samples, attack/benign ratio, whether the dataset is labeled, suitable scenarios, related works, attack types, availability links, and the SOTA AI usage (Transformers or LLMs).\nVarious evaluation metrics have been employed to assess the performance of Transformers and LLM-based IDS in detecting various categories and types of attacks detailed in the datasets discussed (Table 3). Common metrics like accuracy (Acc), recall (Rec), precision (Pre), and F1-score (F1) are frequently applied in general DL tasks, with their definitions available in [10, 31, 32]. Below is a summary of additional metrics specifically used for evaluating Transformers and LLM-based IDS.\n\u2022 Matthew's correlation coefficient (MCC): is a measure of the quality of binary classifications. It considers true positives, true negatives, false positives, and false negatives, providing a balanced assessment even when classes are of very different sizes. Mathematically, the MCC is defined as [33]:\n$MCC = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$ (2)\nWhere TP, TN, FP, and FN indicate true positives, true negatives, false positives, and false negatives, respectively. The MCC ranges from -1 to 1, where: -1 indicates perfect disagreement between the predicted and actual classifications, O indicates no better than random classification, and 1 indicates perfect agreement between the predicted and actual classifications.\n\u2022 Fooling rate (FR): is an essential measure for evaluating the effectiveness of adversarial attacks. It quantifies the proportion of data samples that experience a shift in the model's predicted label following adversarial manipulation (as shown in Equation 7). This metric holds significant importance in assessing adversarial attacks, especially in targeted scenarios, where it indicates the percentage of samples successfully misclassified as the desired target label.\n$FR = \\frac{Number \\: of \\: samples \\: with \\: changed \\: predictions}{Total \\: number \\: of \\: adversarial \\: samples}$ (3)\n\u2022 Alert score (AS): the baseline scenario represents the \"normal activity\" utilized to compute the alert scores. This scenario does not involve any attacks and was not included in the training data. The alert score for each log is determined as follows:\n$AS = \\frac{Score \\: - \\: Max \\: baseline \\: score}{Baseline \\: standard \\: deviation}$ (4)\nThe Score is the anomaly score of the log being evaluated, while the Max baseline score and Baseline standard deviation come from the baseline scenario's anomaly scores. An alert score of one indicates the log is one standard deviation above the maximum baseline score. The threshold for detecting attacks can be adjusted according to operational needs: higher to reduce false positives, and lower to reduce false negatives. This threshold can also be fine-tuned over time based on practical experience and the specific network environment [13]."}, {"title": "3. Taxonomy of Transformers in IDS", "content": "This section categorizes the usage of Transformers in IDS based on their detection approaches, including Attention-based methods, CNN/LSTM-Transformer methods, ViT methods, GAN-Transformer methods, and LLM methods such as GPT-based and BERT-based approaches. Table 4 provides a comprehensive comparison between Transformers and LLMs, detailing their suitability for various problem scenarios, as well as highlighting the advantages and disadvantages associated with each algorithm. Table 5 summarizes various research findings on Transformer-based IDS schemes, including the dedicated tasks, comparison methods, datasets used, obtained results, and improvements achieved."}, {"title": "3.1. Attention-based methods", "content": "An Attention Transformer is a DL model that processes data through a mechanism known as \"Attention,\" allowing the model to weigh the importance of different parts of input data differently. It excels in understanding context and relationships within data, making it highly effective for NLP tasks.\nIn computer security, Attention Transformers can enhance threat detection, anomaly detection, and phishing email identification by learning to recognize subtle patterns and anomalies in data that traditional methods might miss. In a simplified form, the Attention mechanism can be represented by the equation [15]:\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$ (5)\nIn this mechanism, Q, K, and V represent queries, keys, and values, respectively. Queries are used to direct the Attention mechanism's focus. Keys are part of the input data used to calculate Attention weights, indicating the relevance of different parts of the input data. Values are the actual content that, after the application of Attention weights, are aggregated to form the output of the Attention mechanism. Additionally, dk denotes the dimension of the keys. This approach enables the model to concentrate on the most relevant parts of the data, thereby enhancing the precision and efficiency of security systems through more accurate threat identification and reduced false positives. Figure 4 summarizes the existing Attention categories, and their benefits.\nIn the architecture of many Transformer-based models, such as those used in IDS tasks, the summation and normalization layer as described in Equation 1, and the feedforward neural network (FFNN) as detailed in Equation 6, often accompany the Attention layer. The FFNN serves as a vital component for feature transformation within the Attention mechanism. It typically comprises two fully connected layers, where the first layer applies the rectified linear unit (ReLU) activation function, promoting non-linearity, and the second layer operates without an activation function. Mathematically, the FFNN can be represented as:\n$FFNN(x) = ReLU(x.W_1 +b_1)W_2 + b_2$ (6)\nWhere, x.W\u2081 + b\u2081 is the output of the first fully connected layer in a FFNN with weights W\u2081 and bias b\u2081, followed by another linear transformation with W\u2082 and bias b2.\nSeveral techniques of Attention-based IDS have been recently proposed. For example, [56] combine CNN and gated recurrent unit (GRU) along with an Attention mechanism, drawing inspiration from contemporary language models, to develop a novel and effective IDS system to tackle SQL injection and cross-site scripting (XSS) attacks. This system is capable of reaching greater accuracy levels, requires a smaller dataset for training, and reduces the duration of the training process.\nSelf-Attention is a mechanism allowing a model to weigh the importance of different positions within the same input sequence for generating a representation. The study [57] introduces the residual 1-D image Transformer (R1DIT) model to address privacy and generalization issues in malware traffic classification. It parses network headers without compromising sensitive data, using DL and MHA mechanisms to differentiate between malware and benign traffic, enhancing privacy and adaptability to new threats like DDoS on transport layer security (TLS). RNN-based cyber-defense approaches are limited by their sequential data processing, relying solely on the hidden state from past data, which can lead to overlooked contextual features. Nguyen et al. in their paper [30] presents a novel multi-class IDS for vehicle CAN bus security using a Transformer-based Attention network. The fields of the CAN bus are described in Figure 5. It surpasses RNN limitations by employing self-Attention for attack classification and replay attack detection, through the aggregation of sequential CAN IDs, without requiring message labeling. The model also utilizes transfer learning (TL) to enhance performance on small datasets from diverse car models. The work in [58], utilizes four advanced algorithms for intrusion detection: LightGBM, XGBoost, CatBoost, and a MHA Transformer. The effectiveness of their proposed method was assessed using a well-known dataset known as CICIDS-2017. The Transformer architecture slightly surpasses LightGBM, XGBoost, and CatBoost in accuracy and efficiency, making it the preferred choice for performance. In [59], Transformer utilizing self-Attention has been used to to capture temporal characteristics in order to improve network security when facing data imbalance issue.\nDNN models often produce a high number of incorrect predictions in unbalanced intrusion datasets, particularly affecting minority classes. The authors in [60] aim to overcome the mentioned DNN limitations through the strategic combination of decision tree (DT) algorithms and feature tokenizer-Transformers based on MHAs. Initially, a DT algorithm distinguishes between normal and malicious traffic. Then, the Transformer categorizes the malicious traffic to pinpoint the specific type of attack. Moving forward, IoT devices face threats like data theft and DDoS attacks, leading to costly security breaches. There's a high demand for robust IDS. Traditional models often fail to detect varied attack types due to limited adaptability. Ahmed et al. [61] introduce an IDS based on a MHA-based Transformer mechanism, showing significant improvements in accuracy, precision, recall, and F-score compared to LSTM and RNN models.\nCross-Attention enables a model to attend to different positions in another sequence, facilitating interaction between two distinct sequences for tasks like translation or summarization. In the context of cyber-security, the Denseformer model, proposed in [62], integrates multiple Transformer-like elements into a multi-layered architecture, featuring encoder and decoder sub-layers with both self and cross-Attention mechanisms. It distinctively utilizes cross-fusion in multi-branch structures, functioning as an Attention network within dense layers to more effectively identify feature correlations. This leads to improved generalization and cyberattack detection, achieving an accuracy of 85.65% on the NSL-KDD dataset. In [63], Attention has been used after the consolidation of alerts, based on a threshold, for the prediction of attacks in a multi-stage offensive."}, {"title": "3.2. CNN/LSTM-Transformer-based methods", "content": "A CNN-Transformer combines the strengths of CNNs and Transformers, two powerful neural network architectures used in DL. CNNs excel in processing structured grid data like images, efficiently capturing local dependencies through convolutional operations. They are renowned for their ability to detect features and patterns at various levels of abstraction, making them pivotal in computer vision (CV) tasks. On the other hand, Transformers leverage self-Attention mechanisms to model long-range dependencies, excelling in handling sequential data such as text and time series. They have revolutionized NLP and are increasingly applied across various domains. By integrating CNNs for effective feature extraction and spatial hierarchy with Transformers' capability to model complex data relationships, CNN-Transformers aim to harness the advantages of both architectures. This synergy enables more robust and efficient learning, particularly in tasks requiring Attention to both local and global data structures, such as advanced image recognition, object detection, and beyond. The possible configurations of CNN-Transformers are illustrated in Figure 6. CNN-Transformers, an innovative architectural fusion, have recently garnered Attention in the realm of IDS. This advanced merger harnesses the strengths of CNNs and Transformers to enhance detection accuracy and efficiency. For example, in [64], feature subsets covering multiple spaces are developed using CNN to enhance the spatial distribution of samples. Subsequently, the Transformer is employed to establish connections between features and to determine essential attributes, including the temporal and detailed aspects of the features, culminating in the successful detection of intrusion activities. Similarly, the work in [65] presents a hybrid neural network model aiming to improve feature extraction and detection effectiveness in traffic analysis. It combines dense CNNs and Transformers for feature fusion and time sequence extraction. Results on the CIC-IDS2018 dataset show 98% accuracy, surpassing existing models in performance metrics. Moving on, the study referenced in [66] employs 1D CNN and time series Transformer architectures to address the challenges posed by RNNs related to computational complexity and detection performance, particularly due to issues of information loss. The performance assessment on the CICDDOS2019 and CSE-CIC-IDS2018 datasets showed outstanding results, with the evaluation metrics mostly falling between 98.07% and 99.99%."}, {"title": "3.3. ViT-based methods", "content": "ViTs and CNNs offer distinct approaches for analyzing visual data. ViTs utilize the Transformer architecture to process images as sequences of patches, applying self-Attention mechanisms to understand global relationships within the image. This method contrasts with CNNs, which analyze images using convolutional filters that focus on local features and incrementally expand their understanding to more complex patterns. In other words, ViTs model relationships across the entire image using Attention, enabling dynamic focus on pertinent areas, whereas CNNs build a hierarchical understanding of local features through successive layers. Consequently, ViTs might perform better in scenarios requiring a comprehend of the global context but usually need more data and computational power for training. In contrast, CNNs excel at efficiently learning spatial hierarchies but may not capture long-distance dependencies as effectively as ViTs. Figure 7 illustrate a basic structure of ViT Transformers. For example, [70] proposed architecture combines a feature fusion network with a ViT, enhancing the overall DL model's ability to handle imbalanced datasets and reducing the amount of sample data required for training. The authors in [71] suggest an IDS approach, wherein they utilize image conversion from network data flow, by mapping flow data to RGB values, to generate an RGB image. This image is then analyzed using the DT algorithm to pinpoint significant features. Additionally, a ViT classifier is employed to categorize the resulting image. Similarly, in their work, Agrafiotis et al. [72] transformed traffic data contained in packet capture (PCAP) files into grayscale images. Following this conversion, they utilized ViT techniques for malware classification, subsequently evaluating its performance in comparison to that of CNN."}, {"title": "3.4. GAN-Transformer-based methods", "content": "The GAN-Transformer is a hybrid model that integrates GANs with Transformer architectures to improve generative tasks across various domains. This model is particularly effective in fields where data can be transformed into text or image, allowing for the application of established NLP techniques, as well as in image generation. By combining these two powerful technologies, the GAN-Transformer leverages the strengths of GANs in generating high-fidelity outputs with the Transformer's ability to handle complex data dependencies, thereby enhancing the model's overall performance in generating sophisticated and contextually relevant outputs. Figure 8 illustrates a general concept of utilizing GAN and ViT to improve threat detection in imbalanced network flow data conditions.\nIntegrating ViTs with GANs involves adapting the traditional GAN framework to leverage the transformer architecture for either the generator, the discriminator, or both. While the core objective function of GANs remains the same, the architecture of the generator and discriminator changes to incorporate transformers. The core objective function of GANs is $\\min_G \\max_D V(D, G)$, where the value function V (D, G) is given by:\n$V(D, G) = E_{x \\sim P_{data}(x)} [log \\: D(x)] + E_{z \\sim p_z(z)}[log(1 \u2013 D(G(z)))]$ (7)\nWhere $P_{data}(x)$ is the probability distribution of the real data. $p_z(z)$ is the probability distribution of the input noise (e.g., a Gaussian distribution) fed into the generator. G(z) is the generator function that maps the noise z to the data space. D(x) is the discriminator function that outputs the probability that the input x is from the real data distribution. When ViTs are used, the functions G and D are replaced with transformer-based models:\n\u2022 ViT Generator G: This transformer-based generator G takes a latent vector z and produces an image or feature map. The output of G(z) is typically processed by a series of transformer blocks that generate high-quality images or features.\n\u2022 ViT Discriminator D: This transformer-based discriminator D takes an image or feature map x and outputs a probability that the input is from the real data distribution.\nLet $G_{Vit}$ denote the ViT-based generator and $D_{Vit}$ denote the ViT-based discriminator. The GAN objective function with ViTs can be expressed as $\\min_{G_{Vit}} \\max_{D_{Vit}} V (D_{ViT}, G_{ViT})$, where\n$V(D_{Vit}, G_{Vit}) = E_{x \\sim P_{data}(x)} [log \\: D_{ViT}(x)] + [E_{z \\sim p_z(z)}[log(1 \u2013 D_{ViT}(G_{ViT}(z)))]$ (8)\nAlthough numerous researchers have introduced various GAN-based methodologies to address the time series anomaly detection issue, challenges such as model collapse, limited generalization, and low accuracy persist. In this paper [42], the authors introduce a dilated convolutional Transformer-based GAN aimed at increasing model accuracy and enhancing its generalization capabilities. The method employs multiple generators and a single discriminator to mitigate the problem of mode collapse. Each generator features a dilated CNN paired with a Transformer block, which consists of MHA, designed to capture both fine-grained and coarse-grained time series data, thereby boosting the model's generalization ability. Additionally, a weight-based mechanism is utilized to maintain equilibrium among the generators.\nThe classification performance for intrusion detection suffers due to imbalanced training data and restricted feature extraction. The researchers in [75], introduces a novel method combining a conditional generative adversarial network (CGAN) with BERT, a pre-trained language model, for multi-class intrusion detection. This technique addresses the imbalance in attack data by leveraging CGAN to augment minority class samples. Additionally, BERT, known for its robust feature extraction capabilities, is integrated into the CGAN discriminator to enhance the relationship between input and output, thereby improving detection accuracy through adversarial training. Similarly, [76] integrates CGAN with ViT to enhance the accuracy of network traffic data detection, particularly when confronted with imbalanced network flow data conditions. Only the encoder segment of the ViT model is utilized."}, {"title": "3.5. Other Transformer-based methods", "content": "Federated learning (FL) is a decentralized DL approach where model training occurs locally on devices or servers holding data. Instead of sending data to a central server, only model updates or gradients are shared. These updates are aggregated to improve the global model, which is then redistributed. This process preserves data privacy, as raw data remains on local devices, mitigating privacy risks associated with centralized data storage. FL also enables collaborative model training across distributed environments, benefiting from diverse data sources without compromising individual privacy. It finds applications in healthcare, finance, and other sectors where data privacy is paramount [77]. Employing Transformers together with FL has been investigated by researchers. For example, in [6], a NIDS method employing FL and an enhanced Transformer model, which includes MHA, addresses issues of prolonged detection time and low accuracy. Data augmentation and local model analysis enhance detection, with final predictions aggregated using a Softmax classifier. Similarly, the work [46] presents FED-IDS, a FL-based IDS that offloads learning to vehicular edge nodes. It uses a context-aware Transformer network with MHA to capture the spatial-temporal patterns of abnormal and normal vehicular data, and blockchain-managed federated training for secure, distributed, and reliable attack detection. Similarly, in [78], the authors claim that existing IDS models have low performance and are typically trained on cloud servers, which jeopardizes user privacy and increases detection delay. To address these issues, they present a Transformer-based model to enhance IDS performance. Additionally, it integrates 5G technology into smart grid systems and proposes HFed-IDS, a hierarchical FL system, to collaboratively train the proposed Transformer-based IDS model and protect user privacy in core networks.\nAn N-gram is a contiguous sequence of n items from a given sample of text. It is used instead of a single word because it provides valuable context information. Han et al. [79], proposed a novel intrusion detection model called GTID, which leverages n-gram frequency and a time-aware Transformer. GTID hierarchically learns traffic features from both packet-level and session-level data, minimizing information loss. It processes packet headers and payloads differently to extract packet-level features effectively, using n-gram frequency to capture payload context. For session-level features, GTID employs a time-aware Transformer with MHA, considering time intervals between packets to learn temporal session features for accurate intrusion detection."}, {"title": "4. LLM-based methods", "content": "The term LLM is used to differentiate language models by the size of their parameters, specifically those considered large-sized pre-trained models. However, the academic community has not reached a formal agreement on the minimum parameter size required for a model to be classified as an LLM, as the model's capacity is closely related to the size of the training data and the overall computational resources available [20"}]}