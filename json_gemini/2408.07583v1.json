{"title": "Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey", "authors": ["Hamza Kheddar"], "abstract": "With significant advancements in Transformers and large language models (LLMs), natural language processing (NLP) has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in intrusion detection systems (IDSs), focusing on different architectures such as Attention-based models, LLMs like bidirectional encoder representations from Transformers (BERT) and generative pre-trained Transformer (GPT), CNN/LSTM-Transformer hybrids, emerging approaches like Vision Transformers (ViTs), among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, internet-of-things (IoT) devices, critical infrastructure protection, cloud computing, software-defined networking (SDN), as well as in autonomous vehicles (AVs). The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.", "sections": [{"title": "1. Introduction", "content": "In today's swiftly evolving network ecosystem, characterized by the emergence of technologies like 5G and the widespread adoption of the Internet-of-things (IoT) [1], the potential for threats and vulnerabilities has expanded significantly. Consequently, concerns regarding network security are escalating. Network security attacks are diverse and continuously evolving, posing significant threats to the integrity, confidentiality, and availability of information systems. These attacks can be broadly categorized into various types, including malware, phishing, denial-of-service (DoS) attacks, man-in-the-middle (MitM) attacks, and advanced persistent threats (APTs) [2]. Malware encompasses viruses, worms, ransomware, and spyware that infiltrate systems to steal data, disrupt operations, or demand ransoms. Phishing attacks deceive users into revealing sensitive information through fraudulent emails or websites. DoS attacks overwhelm systems with excessive traffic, rendering services unavailable. MitM attacks intercept and manipulate communications between parties, while APTs involve prolonged and targeted cyber-espionage campaigns against specific organizations or individuals. To combat these threats, various attack prevention methods have been developed, each with its unique approach and effectiveness. These methods include firewalls, antivirus software, encryption, intrusion detection system (IDS) [3], intrusion prevention systems (IPS) [4], and information and event management (SIEM) [5] systems. Firewalls act as a barrier between trusted and untrusted networks, controlling incoming and outgoing traffic based on predefined security rules. Antivirus software detects and removes malicious software, while encryption ensures data confidentiality by converting information into unreadable code without the correct decryption key. IDS and IPS monitor network traffic for suspicious activities, with IDS alerting administrators of potential threats and IPS actively blocking malicious traffic. SIEM systems aggregate and analyze log data from various sources to detect and respond to security incidents in real-time. Among these, IDS has advanced significantly with machine learning (ML) and deep learning (DL) integration, enhancing host intrusion detection system (HIDS) and network intrusion detection system (NIDS) [3]. HIDS monitors individual devices, focusing on system logs and behaviors to detect threats like unauthorized file changes. However, it is limited to specific hosts. NIDS, conversely, analyzes network traffic to identify threats like distributed denial-of-service (DDoS) attacks and network scanning, offering a broader security perspective. artificial intelligence (AI) enhances both categories by analyzing vast data to recognize complex threat patterns, improving detection accuracy and adaptability to evolving threats. Their versatility stems from the abundance of network data available for training intrusion scenarios and crafting AI-driven IDS models. Furthermore, advancements in technology have reinforced computational capabilities, facilitating faster and more cost-effective model training. The widespread adoption of DL ensures precise model optimization through continuous self-learning.\nHowever, despite these advancements, current ML or DL-based IDS methods still encounter new challenges, as long as network technologies evolving, such as vulnerability to attacks on central entities, decreased system performance with larger user bases, and"}, {"title": "1.1. Existing reviews and our contributions", "content": "Employing Transformers, and specifically large LLMs, although a trending research topic, is still in its early stages. Consequently, there are few recent reviews, and most of them are preprints that are not yet published. These reviews summarize how NLP algorithms could be exploited for more effective detection and mitigation of insider threats in cybersecurity. However, they tend to focus strongly on specific aspects: the use of generative AI and LLMs to strengthen resilience and security, as seen in [17, 18]; the mitigation of insider threats using NLP and DL, as discussed in [19]; the application of LLMs in cybersecurity tasks such as vulnerability detection and malware analysis, highlighted in [20, 21]; or examining the dual impact of LLMs on security and privacy, as explored in [22]. These reviews are generic and cover only a few works in each area of cybersecurity. In contrast, our survey provides a comprehensive review of empowering IDS using Transformers, which are the main component of LLMs, along with some pre-trained LLMs models such as GPT and BERT. Moreover, this survey may assist researchers in building their own LLMs tailored to intrusion detection by providing possible configurations of the Attention layer and existing Transformer and LLMs models. The key contributions of this survey can be summarized as follows:\n\u2022 The survey delves into the background of IDS and covers various types of attacks. It also provides an in-depth taxonomy of the different IDS techniques, including HIDS and NIDS, employed to secure diverse environments.\n\u2022 The survey offers an extensive taxonomy of different Transformer models applicable to strengthen various IDS techniques, including Attention, CNN/LSTM-Transformer, vision Transformer (ViT), GAN-Transformer, GPT, BERT and their variants, among others.\n\u2022 It presents a detailed classification of various Attention mechanisms and their possible configurations with traditional DL models, such as convolution neural network (CNN).\n\u2022 The survey reviews the latest SOTA schemes proposed for different environments and applications, such as computer networks, the IoT and industrial internet of things (IIoT), critical infrastructure, cloud and software-defined network (SDN), and autonomous vehicless (AVs).\n\u2022 Finally, the survey highlights existing research challenges and suggests potential future directions for the field of Transformers and LLM-based IDS."}, {"title": "1.2. Research questions and objectives", "content": "To streamline this survey, the author defined five research questions in Table 2. By following the study's roadmap, readers will grasp the key insights and comprehend the study's objectives. The table offers a structured summary of the research questions (RQ) and their associated motivating factors. Each row in the table addresses a specific research question, offering a concise view of the goals guiding research in automated technologies for Transformers and LLMs-based IDS."}, {"title": "1.3. Survey methodology", "content": "To identify and review existing studies on Transformers and LLM-based ASR, a comprehensive search was conducted across several leading publication databases renowned for their high-quality scientific research. The primary search was carried out in Scopus, which systematically includes databases such as Web of Science, Elsevier, IEEE, ACM Digital Library, Wiley, and IET Digital Library, among others.\nArticles published between 2017 and 2024 were given priority. However, older publications were also considered when necessary to provide a historical context, dataset, metrics, etc. The survey focused on computer science and engineering studies from databases including IEEE Xplore, ScienceDirect, Wiley, Springer, and Taylor & Francis. Additionally, considering the recent and trending nature of the topic, high-quality pre-prints from arXiv, SSRN, and TechRxiv were selected. Only articles written in English were included in the final analysis. Key search terms used in the abstract, article title, and keywords led to the formulation of the following query:\nSelected papers = FROM (\"Abstract\" || \"Title\" || \"keywords\") SELECT ( References WHERE keywords = (Transformer || LLM || NLP) & (Cyberthreat || IDS ))\nThe symbols & and || signify the logical operations AND and OR, respectively. Due to the overlap of the term \"Transformers\" with electrical network equipment, a manual filtering process was applied to exclude irrelevant papers. The final number of selected papers on IDS based purely on Transformers and LLMs is 102. Figure 1 provides more details about the involved research papers in terms of paper type, domain conducted, and distribution of papers based purely on Transformers and LLMs. It is obvious that Transformers and LLM-based IDS has emerged as a recent and rapidly growing field, with a notable surge in publications starting from 2022. Moreover, the inclusion of 27 pre-prints among the 148 cited papers underscores the trending nature and active research pursuit in this domain.\nTo assess the quality and impact of academic work, several metrics are considered in writing the survey: H-index, Q1 ranking, impact factor of the journal, average citations, and database indexing of the paper. These metrics ensure broader visibility, accessibility, and credibility. Additionally, highly important papers are discussed extensively in the text and their findings are reported in the tables, while lower-quality papers are only discussed in the tables."}, {"title": "1.4. Survey organization", "content": "The remainder of this survey is organized as follows: Section 2 summarizes the essential fundamentals, including datasets, metrics, various investigated attacks, pre-processing steps, and a taxonomy of existing IDS techniques. Section 3 details the principles of employing Transformers in IDS and reviews numerous related studies. Section 4 explores the use of LLMs in IDS, accompanied by a review of several studies. Section 5 highlights the most significant applications investigated in the SOTA research. Section 6 provides real-world case studies for both Transformer-based and LLM-based IDSs systems. Section 7 discusses the open challenges facing Transformer and LLM-based IDS schemes and suggests potential future contributions. Section 8 provides a conclusion for this survey. Figure 2 offers a detailed roadmap for this survey and lists the key topics covered in the study."}, {"title": "2. Background", "content": "This section covers the key elements of IDS, starting with intrusion detection methods, followed by an overview of attack categories, the datasets used for evaluation, and the metrics employed to measure IDS performance."}, {"title": "2.1. Intrusion detection", "content": "This part elucidates the application of DL in IDS for detecting cyber-attacks, detailing procedures that can be applied universally across various environments. Figure 3 presents the overall architecture of a generic detection system, which consists of two main phases: data pre-processing and IDS framework development. The IDS framework can be either signature-based or anomaly-based, followed by training and testing. The following sections provide detailed information about each category."}, {"title": "2.1.1. Data pre-processing", "content": "Computational efficiency can be improved and memory usage decreased by downsizing the dataset and removing less significant features. This can be achieved by dropping some dataset's columns that are expected to be unaffected by the attacks being studied, such as media access control (MAC) addresses, timestamp, among others. Encoding feature attributes in processed data can be accomplished using various techniques, depending on the data's nature. Common methods include label encoding, suitable for categorical data when categories are ordinal; one-hot encoding, ideal for categorical data lacking ordinal relationships; and binary encoding, which combines benefits from both label and one-hot encoding. These techniques offer versatile ways to represent categorical features effectively in ML models. z-score normalization, is a technique used to standardize numerical features in a dataset. It involves subtracting the mean (u) of the feature from each data point and then dividing by the standard deviation (\u03c3) of the feature. Mathematically, the formula for z-score normalization is represented as:\n$Z = \\frac{X - \\mu}{\\sigma}$ (1)\nWhere, x is the original value of the feature, u is the mean of the feature, o is the standard deviation of the feature, and z is the standardized value, also known as the z-score. This transformation ensures that the standardized feature has a \u03bc of 0 and a of 1, which is particularly useful for algorithms that assume normally distributed data or require features to be on a similar scale for proper convergence. It also facilitates direct comparison and analysis, while also reducing data bias and enhancing stability and reliability. The classes of data are evaluated to determine if they are balanced or not. If necessary, k-means undersampling [23] is employed to expedite computation when data are balanced. k-means is a commonly used clustering algorithm that divides a dataset into k distinct clusters, reducing data size without losing meaningful information by minimizing the distance between data points and their cluster centers. However, k-means is sensitive to initial cluster centers and noisy data. The mini k-means algorithm [24], an improved version, mitigates these issues by selecting random points as initial cluster centers, assigning data points to the nearest centers, and recalculating centroids until convergence is achieved or maximum iterations are reached. The imbalance data could skew the training towards the more abundant samples, under-training the less represented attack types and reducing the model's overall applicability. To address this, researchers used oversampling techniques to balance the dataset, ensuring the model is adequately trained on all attack types. Synthetic minority oversampling technique (SMOTE) is a widely used method to address data imbalance by generating synthetic samples for minority classes [25], thus balancing the dataset. It works by selecting a sample from the minority class, finding its k nearest neighbors, and generating new synthetic samples by interpolating between them. While the mentioned data pre-processing steps are common in DL-based IDS, they are not necessarily used all at once for the same method. Additionally, the data can be converted to a 2D format to be processed as an image, allowing the application of existing 2D DL algorithms such as ViT and Inception pre-trained models."}, {"title": "2.1.2. \u0422\u0430\u0445\u043e\u043f\u043emy of IDS techniques", "content": "After pre-processing, the researchers apply various DL algorithms to build an efficient DL-based IDS model. This phase involves model training and optimization, including cross-validation, enhanced convergence tools, and grid search techniques to assess the model's performance. The DL-based IDS frameworks can be categorized into two types, depending on the availability of attack type information:\n(a) Signature-based model: Are detection techniques in IDS, which involve matching network traffic or system actions against a database of attack signatures to trigger alerts upon detection. It outlines the process of comparing observed data D with a signature database S = {$1, $2,..., Sn} using a matching function Match(S, D). While effective against known attacks, this method has limitations in detecting new or mutated attacks and may not be suitable for resource-constrained environments like IoT. Research explores pattern-based detection methods as alternatives to address these challenges. Experts evaluate the trained model's performance on test instances. If the results are unsatisfactory, techniques such as principal component analysis (PCA) may be applied as a corrective measure to reduce the complexity of the data and enhance interpretability, thereby improving the performance of DL algorithms. Once the results meet the accuracy criteria, the model becomes capable of effectively identifying and distinguishing various attack types. ViT and pre-trained models such as Inception are widely used for their ability to capture"}, {"title": "2.2. Attack categories, datasets, and metrics", "content": "Various datasets in IDS research cater to diverse network environments (e.g., network, host, industrial control system (ICS), IoT, cloud, edge, smart grid), encompassing a wide array of attack types categorized as follows:\n\u2022 DoS: Attacks aim to render systems or networks inaccessible. Examples include Transmission Control Protocol (TCP)/User Datagram Protocol (UDP) flooding, Internet Control Message Protocol (ICMP) Smurf/ping of death, IP teardrop, UDPstorm, domain name system (DNS) amplification, TCP SYN/ACK flood, and HyperText Transfer Protocol (HTTP) Slowloris.\n\u2022 Remote-to-local (R2L): Remote attacks exploit vulnerabilities to gain local access, targeting protocols like File Transfer Protocol (FTP), Internet message access protocol (IMAP), HTTP, Simple Mail Transfer Protocol (SMTP), DNS, X11, and Simple Network Management Protocol (SNMP), with attacks like FTP_write, phf, and SNMPgetattack."}, {"title": "Matthew's correlation coefficient (MCC)", "content": "is a measure of the quality of binary classifications. It considers true positives, true negatives, false positives, and false negatives, providing a balanced assessment even when classes are of very different sizes. Mathematically, the MCC is defined as [33]:\n$MCC = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$ (2)\nWhere TP, TN, FP, and FN indicate true positives, true negatives, false positives, and false negatives, respectively. The MCC ranges from -1 to 1, where: -1 indicates perfect disagreement between the predicted and actual classifications, O indicates no better than random classification, and 1 indicates perfect agreement between the predicted and actual classifications."}, {"title": "\u2022 Fooling rate (FR)", "content": "is an essential measure for evaluating the effectiveness of adversarial attacks. It quantifies the proportion of data samples that experience a shift in the model's predicted label following adversarial manipulation (as shown in Equation 7). This metric holds significant importance in assessing adversarial attacks, especially in targeted scenarios, where it indicates the percentage of samples successfully misclassified as the desired target label.\n$FR = \\frac{Number\\ of\\ samples\\ with\\ changed\\ predictions}{Total\\ number\\ of\\ adversarial\\ samples}$ (3)"}, {"title": "\u2022 Alert score (AS)", "content": "the baseline scenario represents the \"normal activity\" utilized to compute the alert scores. This scenario does not involve any attacks and was not included in the training data. The alert score for each log is determined as follows:\n$AS = \\frac{Score - Max\\ baseline\\ score}{Baseline\\ standard\\ deviation}$ (4)\nThe Score is the anomaly score of the log being evaluated, while the Max baseline score and Baseline standard deviation come from the baseline scenario's anomaly scores. An alert score of one indicates the log is one standard deviation above the maximum baseline score. The threshold for detecting attacks can be adjusted according to operational needs: higher to reduce false positives, and lower to reduce false negatives. This threshold can also be fine-tuned over time based on practical experience and the specific network environment [13]."}, {"title": "3. Taxonomy of Transformers in IDS", "content": "This section categorizes the usage of Transformers in IDS based on their detection approaches, including Attention-based methods, CNN/LSTM-Transformer methods, ViT methods, GAN-Transformer methods, and LLM methods such as GPT-based and BERT-based approaches. Table 4 provides a comprehensive comparison between Transformers and LLMs, detailing their suitability for various problem scenarios, as well as highlighting the advantages and disadvantages associated with each algorithm. Table 5 summarizes various research findings on Transformer-based IDS schemes, including the dedicated tasks, comparison methods, datasets used, obtained results, and improvements achieved."}, {"title": "3.1. Attention-based methods", "content": "An Attention Transformer is a DL model that processes data through a mechanism known as \"Attention,\" allowing the model to weigh the importance of different parts of input data differently. It excels in understanding context and relationships within data, making it highly effective for NLP tasks.\nIn computer security, Attention Transformers can enhance threat detection, anomaly detection, and phishing email identification by learning to recognize subtle patterns and anomalies in data that traditional methods might miss. In a simplified form, the Attention mechanism can be represented by the equation [15]:\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$ (5)\nIn this mechanism, Q, K, and V represent queries, keys, and values, respectively. Queries are used to direct the Attention mechanism's focus. Keys are part of the input data used to calculate Attention weights, indicating the relevance of different parts of the input data. Values are the actual content that, after the application of Attention weights, are aggregated to form the output of the Attention mechanism. Additionally, dk denotes the dimension of the keys. This approach enables the model to concentrate on the most relevant parts of the data, thereby enhancing the precision and efficiency of security systems through more accurate threat identification and reduced false positives. Figure 4 summarizes the existing Attention categories, and their benefits.\nIn the architecture of many Transformer-based models, such as those used in IDS tasks, the summation and normalization layer as described in Equation 1, and the feedforward neural network (FFNN) as detailed in Equation 6, often accompany the Attention layer. The FFNN serves as a vital component for feature transformation within the Attention mechanism. It typically comprises two fully connected layers, where the first layer applies the rectified linear unit (ReLU) activation function, promoting non-linearity, and the second layer operates without an activation function. Mathematically, the FFNN can be represented as:\n$FFNN(x) = ReLU(x.W_1 +b_1)W_2 + b_2$ (6)\nWhere, x.W\u2081 + b\u2081 is the output of the first fully connected layer in a FFNN with weights W\u2081 and bias b\u2081, followed by another linear transformation with W\u2082 and bias b2.\nSeveral techniques of Attention-based IDS have been recently proposed. For example, [56] combine CNN and gated recurrent unit (GRU) along with an Attention mechanism, drawing inspiration from contemporary language models, to develop a novel and effective IDS system to tackle SQL injection and cross-site scripting (XSS) attacks. This system is capable of reaching greater accuracy levels, requires a smaller dataset for training, and reduces the duration of the training process.\nSelf-Attention is a mechanism allowing a model to weigh the importance of different positions within the same input sequence for generating a representation. The study [57] introduces the residual 1-D image Transformer (R1DIT) model to address privacy and generalization issues in malware traffic classification. It parses network headers without compromising sensitive data, using DL and MHA mechanisms to differentiate between malware and benign traffic, enhancing privacy and adaptability to new threats like DDoS on transport layer security (TLS). RNN-based cyber-defense approaches are limited by their sequential data processing, relying solely on the hidden state from past data, which can lead to overlooked contextual features. Nguyen et al. in their paper [30] presents a novel multi-class IDS for vehicle CAN bus security using a Transformer-based Attention network. The fields of the CAN bus are described in Figure 5. It surpasses RNN limitations by employing self-Attention for attack classification and replay attack"}, {"title": "3.2. CNN/LSTM-Transformer-based methods", "content": "A CNN-Transformer combines the strengths of CNNs and Transformers, two powerful neural network architectures used in DL. CNNs excel in processing structured grid data like images, efficiently capturing local dependencies through convolutional"}, {"title": "3.3. ViT-based methods", "content": "ViTs and CNNs offer distinct approaches for analyzing visual data. ViTs utilize the Transformer architecture to process images as sequences of patches, applying self-Attention mechanisms to understand global relationships within the image. This method contrasts with CNNs, which analyze images using convolutional filters that focus on local features and incrementally expand their understanding to more complex patterns. In other words, ViTs model relationships across the entire image using Attention, enabling dynamic focus on pertinent areas, whereas CNNs build a hierarchical understanding of local features through successive layers. Consequently, ViTs might perform better in scenarios requiring a comprehend of the global context but usually need more data and computational power for training. In contrast, CNNs excel at efficiently learning spatial hierarchies but may not capture long-distance dependencies as effectively as ViTs. Figure 7 illustrate a basic structure of ViT Transformers. For example, [70] proposed architecture combines a feature fusion network with a ViT, enhancing the overall DL model's ability to handle imbalanced datasets and reducing the amount of sample data required for training. The authors in [71] suggest an IDS approach, wherein they utilize image conversion from network data flow, by mapping flow data to RGB values, to generate an RGB image. This image is then analyzed using the DT algorithm to pinpoint significant features. Additionally, a ViT classifier is employed to categorize the resulting image. Similarly, in their work, Agrafiotis et al. [72] transformed traffic data contained in packet capture (PCAP) files into grayscale images. Following this conversion, they utilized ViT techniques for malware classification, subsequently evaluating its performance in comparison to that of CNN."}, {"title": "3.4. GAN-Transformer-based methods", "content": "The GAN-Transformer is a hybrid model that integrates GANs with Transformer architectures to improve generative tasks across various domains. This model is particularly effective in fields where data can be transformed into text or image, allowing for the application of established NLP techniques, as well as in image generation. By combining these two powerful technologies, the GAN-Transformer leverages the strengths of GANs in generating high-fidelity outputs with the Transformer's ability to handle complex data dependencies, thereby enhancing the model's overall performance in generating sophisticated and contextually relevant outputs. Figure 8 illustrates a general concept of utilizing GAN and ViT to improve threat detection in imbalanced network flow data conditions.\nIntegrating ViTs with GANs involves adapting the traditional GAN framework to leverage the transformer architecture for either the generator, the discriminator, or both. While the core objective function of GANs remains the same, the architecture of the generator and discriminator changes to incorporate transformers. The core objective function of GANs is ming max D V (D, G), where the value function V (D, G) is given by:\n$V(D, G) = E_{x~Pdata(x)} [log D(x)] + E_{z~pz(z)}[log(1 \u2013 D(G(z)))]$ (7)\nWhere Pdata (x) is the probability distribution of the real data. p\u2082(z) is the probability distribution of the input noise (e.g., a Gaussian distribution) fed into the generator. G(z) is the generator function that maps the noise z to the data space. D(x) is the discriminator function that outputs the probability that the input x is from the real data distribution. When ViTs are used, the functions G and D are replaced with transformer-based models:\n\u2022 ViT Generator G: This transformer-based generator G takes a latent vector z and produces an image or feature map. The output of G(z) is typically processed by a series of transformer blocks that generate high-quality images or features.\n\u2022 ViT Discriminator D: This transformer-based discriminator D takes an image or feature map x and outputs a probability that the input is from the real data distribution.\nLet Gyit denote the ViT-based generator and Dyit denote the ViT-based discriminator. The GAN objective function with ViTs can be expressed as mingyit maxDVIT V (DViT, GviT), where\n$V(D_{vit}, G_{vit}) = E_{x~Pdata(x)} [log D_{vit}(x)] + [E_{z~pz(z)}[log(1 \u2013 D_{vit}(G_{vit}(z)))]$ (8)\nAlthough numerous researchers have introduced various GAN-based methodologies to address the time series anomaly detection issue, challenges such as model collapse, limited generalization, and low accuracy persist. In this paper [42], the authors introduce a dilated convolutional Transformer-based GAN aimed at increasing model accuracy and enhancing its generalization capabilities. The method employs multiple generators and a single discriminator to mitigate the problem of mode collapse. Each generator features a dilated CNN paired with a Transformer block, which consists of MHA, designed to capture both fine-grained and coarse-grained time series data, thereby boosting the model's generalization ability. Additionally, a weight-based mechanism is utilized to maintain equilibrium among the generators.\nThe classification performance for intrusion detection suffers due to imbalanced training data and restricted feature extraction. The researchers in [75], introduces a novel method combining a conditional generative adversarial network (CGAN) with BERT, a pre-trained language model, for multi-class intrusion detection. This technique addresses the imbalance in attack data by leveraging CGAN to augment minority class samples. Additionally, BERT, known for its robust feature extraction capabilities, is integrated into the CGAN discriminator to enhance the relationship between input and output, thereby improving detection accuracy through adversarial training. Similarly, [76] integrates CGAN with ViT to enhance the accuracy of network traffic data detection, particularly when confronted with imbalanced network flow data conditions. Only the encoder segment of the ViT model is utilized."}, {"title": "3.5. Other Transformer-based methods", "content": "Federated learning (FL) is a decentralized DL approach where model training occurs locally on devices or servers holding data. Instead of sending data to a central server, only model updates or gradients are shared. These updates are aggregated to improve the global model, which is then redistributed. This process preserves data privacy, as raw data remains on local devices, mitigating privacy risks associated with centralized data storage. FL also enables collaborative model training across distributed environments, benefiting from diverse data sources without compromising individual privacy. It finds applications in healthcare, finance, and other sectors where data privacy is paramount [77]. Employing Transformers together with FL has been investigated by researchers. For example, in [6], a NIDS method employing FL and an enhanced Transformer model, which includes MHA, addresses issues of prolonged detection time and low accuracy. Data augmentation and local model analysis enhance detection, with final predictions aggregated using a Softmax classifier. Similarly, the work [46] presents FED-IDS, a FL-based IDS that offloads learning to vehicular edge nodes. It uses a context-aware Transformer network with MHA to capture the spatial-temporal patterns of abnormal and normal vehicular data, and blockchain-managed federated training for secure, distributed, and reliable attack detection. Similarly, in [78], the authors claim that existing IDS models have low performance and are typically trained on cloud servers, which jeopardizes user privacy and increases detection delay. To address these issues, they present a Transformer-based model to enhance IDS performance. Additionally, it integrates 5G technology into smart grid systems and proposes HFed-IDS, a hierarchical FL system, to collaboratively train the proposed Transformer-based IDS model and protect user privacy in core networks.\nAn N-gram is a contiguous sequence of n items from a given sample of text. It is used instead of a single word because it provides valuable context information. Han et al. [79], proposed a novel intrusion detection model called GTID, which leverages n-gram frequency and a time-aware Transformer. GTID hierarchically learns traffic features from both packet-level and session-level data, minimizing information loss. It processes packet headers and payloads differently to extract packet-level features effectively, using n-gram frequency to capture payload context. For session-level features, GTID employs a time-aware Transformer with MHA, considering time intervals between packets to learn temporal session features for accurate intrusion detection."}, {"title": "4. LLM-based methods", "content": "The term LLM is used to differentiate language models by the size of their parameters, specifically those considered large-sized pre-trained models. However, the academic community has not reached a formal agreement on the minimum parameter size required for a model to be classified as an LLM, as the model's capacity is closely related to the size of the training data and the overall computational resources available [20]. The categories of LLMs can be divided into three types:\n\u2022 Encoder-only LLMs: are a type of LLMs that primarily utilize the encoder component of the Transformer architecture and focus on understanding and generating representations of the input data. These models excel at tasks that involve understanding and classifying text, such as sentiment analysis, named entity recognition, and text classification. Masked language modeling (MLM) is a technique used primarily in encoder-only models, like BERT, to train the model on understanding context by predicting missing words in a sentence. In MLM, some tokens in the input sequence are randomly replaced with a special \"[MASK]\" token, and the model is trained to predict the original tokens at these masked positions. The goal is to maximize the likelihood of predicting the original tokens given the masked sequence. Given an input sequence x = (X1, X2, ..., x\u2099), where some tokens are masked (replaced with \u2018[MASK]\u2018), the model's task is to predict the original tokens at the masked positions. Let M be the set of positions that are masked. For each masked position i \u2208 M, the model outputs a probability distribution P(x\u1d62 | X1, X2, ..., x\u2099) over the vocabulary. The objective is to maximize the likelihood of the original tokens x\u1d62 at the masked positions. The MLM loss function is defined as:\n$LMLM = \\sum_{i \\in M} log P(x\u1d62 | X1, X2, ..., x\u2099)$ (9)\nWhere, P(x\u1d62 | X1, X2, ..., x\u2099) is the probability assigned by the model to the actual token x\u1d62 given the entire sequence x, where the model has seen the context around the masked tokens. Cross-Entropy Loss aims to measures how well the model's predicted probability distribution matches the actual token at the masked positions.\n\u2022 Decoder-only LLMs: are a type of language model that primarily utilize the decoder component of the Transformer architecture. Unlike encoder-only or encoder-decoder models, decoder-only models focus on generating text by predicting"}, {"title": "4.1. Encoder-only-based methods", "content": "BERT is a LLM developed by Google. It utilizes the Transformer architecture", "81": "introduces BT-TPF framework", "82": "explore leveraging information from a sequence of network flows to enhance the domain adaptation capability of the NIDS. They propose a framework that utilizes BERT for feature extraction and MLP for classification. Moreover", "83": "utilizing Chinese pre-trained language models to effectively handle English-Chinese word mixing in cyber threat intelligence", "47": "present their innovative predictive model and tool based on SecureBERT LLM model that can generate priority recommendation reports for potential cybersecurity threats and predict their impact.\nThe system log produced by a computer system comprises extensive data gathered simultaneously", "84": "a BERT-based anomaly detection framework", "tasks": "masked log key"}]}