{"title": "ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon Intention Understanding", "authors": ["Zhen Chen", "Zongming Zhang", "Wenwu Guo", "Xingjian Luo", "Long Bai", "Jinlin Wu", "Hongliang Ren", "Hongbin Liu"], "abstract": "Surgical instrument segmentation is crucial in surgical scene understanding, thereby facilitating surgical safety. Existing algorithms directly detected all instruments of pre-defined categories in the input image, lacking the capability to segment specific instruments according to the surgeon's intention. During different stages of surgery, surgeons exhibit varying preferences and focus toward different surgical instruments. Therefore, an instrument segmentation algorithm that adheres to the surgeon's intention can minimize distractions from irrelevant instruments and assist surgeons to a great extent. The recent Segment Anything Model (SAM) reveals the capability to segment objects following prompts, but the manual annotations for prompts are impractical during the surgery. To address these limitations in operating rooms, we propose an audio-driven surgical instrument segmentation framework, named ASI-Seg, to accurately segment the required surgical instruments by parsing the audio commands of surgeons. Specifically, we propose an intention-oriented multimodal fusion to interpret the segmentation intention from audio commands and retrieve relevant instrument details to facilitate segmentation. Moreover, to guide our ASI-Seg segment of the required surgical instruments, we devise a contrastive learning prompt encoder to effectively distinguish the required instruments from the irrelevant ones. Therefore, our ASI-Seg promotes the workflow in the operating rooms, thereby providing targeted support and reducing the cognitive load on surgeons. Extensive experiments are performed to validate the ASI-Seg framework, which reveals remarkable advantages over classical state-of-the-art and medical SAMs in both semantic segmentation and intention-oriented segmentation. The source code is available at https://github.com/Zonmgin-Zhang/ASI-Seg.", "sections": [{"title": "I. INTRODUCTION", "content": "Developing computer-assisted surgery systems can im- prove the quality of interventional healthcare for patients [1], [2], [3], [4], [5]. In particular, surgical instrument segmenta- tion stands as a cornerstone for surgical scene understanding [6], [7], [8], [9], which can benefit visual navigation, pre- cise operation, and instrument tracking, thereby facilitating surgical safety and patient outcomes.\nTo achieve accurate instrument segmentation, existing works [10], [7], [11], [12], [13] have conducted a lot of"}, {"title": "II. RELATED WORK", "content": "Surgical Instrument Segmentation. Existing works [10], [7], [11], [12], [13] conducted surgical instrument segmen- tation from different aspects. In particular, the TernausNet [10] improved the network structure to achieve accurate instrument segmentation. The ISINet [7] achieved the seg- mentation by identifying instrument candidates and assign- ing category labels. The Dual-MF [11] utilized the motion flow of surgical instruments for more accurate segmentation decoding. The S3Net [12] further addressed the difficulty in discriminating instrument categories. In addition, Wang et al. [13] blended the irrelevant tissues with required instruments to facilitate segmentation with augmented samples. Different from these works that directly segmented all instruments of pre-defined categories, our ASI-Seg can segment specific instruments according to the surgeon's intention.\nThe SAM for Medical Imaging. By leveraging both sparse (e.g., point, box, and text) and dense (e.g., mask) prompts, the segment anything model (SAM) [14] has well revealed the advantage in image segmentation across a variety of scenarios. To transfer SAM to downstream scenarios, exist- ing works adopted different fine-tuning strategies, including directly fine-tuning the image encoder [20] or mask decoder [17], and using the parameter efficient fine-tuning (e.g., the low-rank adaptation (LoRA) [21] and adapter [22]), considering the huge amount of SAM parameters. Many medical SAM works [17], [18], [19], [23], [24], [25] have been investigated to customize segmentation capability to medical imaging. Huang et al. [18] explored the impact of different prompts on medical image segmentation, and the MedSAM [17] further fine-tuned the SAM with bounding box prompts on large-scale medical image datasets. For the surgical images, the SurgicalSAM [26] introduced the class prototypes and designated target class to guide the segmenta- tion with the category information. In general, most medical SAMs either demand huge computational resources in fine- tuning [20] or rely on manual annotations for prompt during inference [17], [18], [19], [23], [24], which is impractical for clinical usage."}, {"title": "III. METHODOLOGY", "content": "In this work, we propose the ASI-Seg framework to segment required surgical instruments by following the audio commands of surgeons. As elaborated in Fig. 1, we propose the ASI-Seg framework to segment required surgical instru- ments by following the audio commands of surgeons. Given\na surgical image, the ASI-Seg pareses the audio command for segmentation intention and generates the required instrument masks to meet the demand of surgeons."}, {"title": "B. Intention-Oriented Multimodal Fusion", "content": "To obtain the features of the surgeon's specified instru- ments, we propose an Intention-Oriented Multimodal Fusion module in this section. Firstly, we propose an audio inten- tion recognition module to predict the surgeon's segment intention. Then, we propose a text fusion module and a visual fusion module to inject detailed language description information and richer visual information into a group of learnable queries. Lastly, we utilize the recognized audio intention to select the intention-oriented features.\nAudio Intention Recognition. We sample the discretion audio signals a' from raw audio signals a with 16K Hz. Then, we transfer the discretion audio signals to the Mel spectrogram as follows:\n$A_{mel} = \\pi(\\alpha, \\alpha', C_s, W_s, s),$ (1)\nwhere is the Mel spectrogram transformation [27], Cs is the channel size, Ws is the window size and s is the stride size. For better numerical calculations, we further normalize the scale of Amel to the range of [-1,1], as follows:\n$A_{norm} = \\frac{2 * (A_{mel} - \\mu)}{max(A_{mel}) - min(A_{mel})} - 1,$ (2)\nwhere u is the mean of Mel spectrogram Amel among the training data. To predict the intention of surgeons, we feed Anorm to an Audio Encoder EA and an audio classifier as follows:\n$C = \\phi(E_A(A_{norm})),$ (3)\nwhere C is the audio intention recognition result.\nText Fusion. The surgeon's audio commands may only in- clude the names of instruments. These high-level commands make it challenging for the model to capture the necessary features of the required instruments from visual information. Therefore, we incorporate detailed textual descriptions of each instrument into the learnable query as a complement to the high-level audio commands.\nSpecifically, we first use a Text Encoder Er to extract textual features ft from a pre-prepared Instrument Descrip- tion Bank {Bk}_1, which stores detailed descriptions of K instruments as follows:\n$f_t = concat(E_T(\\left\\{B_i\\right\\}_{i=1}^K)),$ (4)\nwhere Bk refers to the specific instrument description of k- th instrument, and ft \u2208 RK\u00d7d is the concatenated textual feature of all K instruments with feature dimension d.\nThen, we initialize the K learnable queries fe correspond- ing to each surgical instrument. These queries fe are then fused with textual feature ft through a mutual cross-attention"}, {"title": "C. Contrastive Learning Prompt Encoder", "content": "To effectively distinguish between required and irrelevant instrument features, we design the Contrastive Learning Prompt Encoder to provide the mask decoder with the specific prompt of the instrument to be segmented.\nDistinguishing Cross-Attention. We employ a mutual cross- focusing mechanism between the required instrument feature F+ and the irrelevant instrument feature F\u00af, which aims to enhance the focus on the unique properties of the surgical in- struments to be segmented. Firstly, we compute the attention similarity to obtain easily confounded regions as follows:\n$Attention(F^+, F^-) = softmax(\\frac{Q_{F^+}K_{F^-}^T}{\\sqrt{D}})V_{F^-},$ (9)\nwhere QF+, KF-, VF-are attention query, key, and value from the required instrument feature F+ and the irrele- vant instrument feature F\u00af correspondingly. In addition, Attention(F, F+) is the same.\nThen, we adopt an inverse residual mechanism as follows:\n$P^* = P - Attention(F^+, F^-),$ (10)\nwhere P* is the output required instrument feature. P* eliminates information similar to the irrelevant instrument feature and maintains its unique attributes and characteristics, which is essential for accurate segmentation.\nContrastive Learning. To further push relevant instrument features and irrelevant instrument features to be separa- ble, we design a contrast learning between the required"}, {"title": "D. Optimization", "content": "In the training of ASI-Seg, we freeze the image encoder, the audio encoder and the text encoder with massive param- eters, and merely optimize the lightweight instrument clas- sifier and mask decoder, as well as the proposed intention- oriented multimodal fusion and contrastive learning prompt encoder, which makes the end-to-end training efficient. The ASI-Seg is optimized by two loss terms, as follows:\n$L = L_{DICE} + L_{CL},$ (12)\nwhere the dice loss LDICE [32] is for segmentation and the contrastive learning loss LCL is used to dynamically update the learnable query in the ASI-Seg. In this way, ASI-Seg is capable of segmenting the required instruments according to the intention of surgeons."}, {"title": "IV. EXPERIMENT", "content": "We perform the comprehensive evaluation on the EndoVis2018 [33] and EndoVis2017 datasets [34]. To guar- antee fair comparisons, we follow the standard protocol [7], [10]. Specifically, the EndoVis2017 dataset, comprising eight videos, is subjected to a 4-fold cross validation [10]. The video sequences with a high resolution of 1,280 \u00d7 1,024 are acquired from da Vinci Xi surgical system during different porcine procedures. Meanwhile, the EndoVis2018 dataset encompasses 11 training videos alongside four validation videos, thereby presenting a comprehensive platform for benchmarking. Both datasets feature seven unique categories of surgical instruments, enabling an in-depth assessment of our segmentation effectiveness.\nWe implement our ASI-Seg in PyTorch on a single NVIDIA A800 GPU. In our ASI-Seg, we use the pre-trained ViT [36] as the image encoder, and use the text encoder of CLIP [37] as the text encoder, and the pre-trained audio encoder [38] as our audio encoder. Additionally, we randomly initialize audio embeddings as the category query. To enhance architectural stability and concentrate on novel components, we maintain static image encoders while dynamically updating the learnable query and mask decoder weights. We set the temperature factor 7 of contrastive loss as 0.07, Adam as the optimizer with a learn- ing rate of 0.0001 across both datasets to accommodate their distinct complexities. The training leverages pre-computed image embeddings and a batch size of 16 for EndoVis2017 and 64 for EndoVis2018 datasets.\nWe perform the evaluation using three critical segmentation metrics following [26], including the Challenge IoU [34], IoU, and mean class IoU (mc IoU) [7], [12]. These metrics ensure our ASI-Seg is rigorously measured and validated against these benchmarks."}, {"title": "B. Comparisons with State-of-the-arts", "content": "We conduct the comprehensive comparison between our ASI-Seg framework and state-of-the-art surgical instrument segmentation methods and advanced SAM approaches on the EndoVis2018 [33] and EndoVis2017 [34] datasets."}, {"title": "Semantic Segmentation Analysis", "content": "As shown in Table I and Table II, we first perform the comparison of semantic segmentation by segmenting all the instruments in the input image on the EndoVis2018 and EndoVis2017 datasets, re- spectively. In general, our ASI-Seg achieves the best perfor- mance in the semantic segmentation landscape, with IoU of 82.37% and 71.64% on the EndoVis2018 and EndoVis2017 datasets, respectively. Note that our ASI-Seg outperforms the second-best method [26] with an IoU advantage of 2.04% and 1.70% on these two datasets. These improvements indi- cate a profound improvement of our ASI-Seg in the model capacity to distinguish surgical instruments from irrelevant ones and complex backgrounds."}, {"title": "Intention-oriented Segmentation Analysis", "content": "To evaluate the capability of the ASI-Seg, we perform the comparison of intention-oriented segmentation, as shown in Table III and Table IV for EndoVis2018 and EndoVis2017 datasets, respectively. This comparison encompasses a broad spec- trum of surgical instruments, including Bipolar Forceps (BF), Prograsp Forceps (PF), Large Needle Driver (LND), Suction Instrument (SI), Vessel Sealer (VS), Clip Applier (CA), Grasping Retractor (GR), Monopolar Curved Scissors (MCS), and Ultrasound Probe (UP). Specifically, we calcu- late the IoU of each category with segmentation intention and average them for the mean class IoU (mc IoU). Our ASI-Seg achieves the superior mc IoU of 64.18% and 68.17% in the EndoVis2018 and EndoVis2017 datasets, with overwhelming improvement over advanced SAM approaches. In particular, our ASI-Seg reveals the advantage of 5.31% in mc IoU over the second-best SurgicalSAM with category prompt"}, {"title": "C. Robustness Study", "content": "We further investigate the robustness of our ASI-Seg against defective audio commands, e.g., the mispronunci- ation of instrument names. As illustrated in Fig. 3, when there are obvious mispronunciations in the input audio, e.g., surgeons may mistakenly articulate the Bipolar Forceps as Bipolyr Frocips, our ASI-Seg is still capable to recognize the intention into the correct instrument category and complete accurate segmentation. These results confirm the robustness of our ASI-Seg to identify instruments that surgeons intend to use despite verbal errors, which is also an advantage compared to text instructions."}, {"title": "D. Ablation Study", "content": "To validate the effectiveness of the proposed modules, we perform the ablation study on the EndoVis2018 [33] dataset, as shown in Table V. Compared with the vanilla baseline, our framework with the instrument description bank gains a 8.42% increase in mc IoU. This confirms that the integration of textual knowledge is a pivotal component in cultivating distinct learnable queries for different categories, thereby ameliorating the precision of instrument segmentation. On the other hand, our framework obtains a 4.98% increase in mc IoU when adding the contrastive learning in ASI-Seg. In our ASI-Seg, the advantage provided by contrastive learning is mainly attributed to its ability to dynamically emphasize the required instrument features while attenuating irrelevant ones. Therefore, the contrastive learning enables the ASI- Seg to become more proficient in differentiating instruments by focusing on the attributes necessary for differentiation. In this way, the proposed ASI-Seg benefits from these tailored designs, resulting in the performance advantage in surgical instrument segmentation at the operating rooms."}, {"title": "V. CONCLUSIONS", "content": "In this work, we propose the ASI-Seg framework to accu- rately segment the required surgical instruments by parsing the audio commands of surgeons. In our ASI-Seg frame- work, the intention-oriented multimodal fusion can interpret the segmentation intention and retrieve relevant instrument details to facilitate segmentation. Moreover, the contrastive learning prompt encoder can distinguish the required instru- ments from the irrelevant ones to guide our ASI-Seg segment of the required surgical instruments. Therefore, our ASI-Seg can minimize distractions from irrelevant instruments assist surgeons to a great extent, and promote the workflow in the operating rooms. Extensive experiments are performed to validate the ASI-Seg framework, which reveals remark- able advantages over classical state-of-the-art and advanced SAMs in both semantic segmentation and intention-oriented segmentation."}]}