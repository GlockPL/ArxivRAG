{"title": "INTEGRATING IP BROADCASTING WITH AUDIO TAGS: WORKFLOW AND CHALLENGES", "authors": ["Rhys Burchett-Vass", "Arshdeep Singh", "Gabriel Bibb\u00f3", "Mark D. Plumbley"], "abstract": "The broadcasting industry is increasingly adopting IP tech- niques, revolutionising both live and pre-recorded content produc- tion, from news gathering to live music events. IP broadcasting allows for the transport of audio and video signals in an easily con- figurable way, aligning with modern networking techniques. This shift towards an IP workflow allows for much greater flexibility, not only in routing signals but with the integration of tools using stan- dard web development techniques. One possible tool could include the use of live audio tagging, which has a number of uses in the pro- duction of content. These include from automated closed captioning to identifying unwanted sound events within a scene. In this paper, we describe the process of containerising an audio tagging model into a microservice, a small segregated code module that can be in- tegrated into a multitude of different network setups. The goal is to develop a modular, accessible, and flexible tool capable of seamless deployment into broadcasting workflows of all sizes, from small productions to large corporations. Challenges surrounding latency of the selected audio tagging model and its effect on the usefulness of the end product are discussed.", "sections": [{"title": "1. INTRODUCTION", "content": "Internet Protocol (IP) broadcasting describes the process of trans- mitting audio and video signals from one location to another using IP networking. One technique traditionally used for transmitting audio/video is the Serial Digital Interface (SDI), with fixed con- nections between dedicated hardware devices. In comparison, IP broadcasting allows software to replace some of these hardware de- vices, enabling greater scalability and easy re-configuration. Cloud technology and containerisation methods such as Docker [1] can be utilised to take advantage of such scalability.\nThere are a few challenges while creating software for use in an IP broadcasting environment. First, as with most modern web applications, is scalability and containerisation of software which allows the infrastructure to adapt depending on the demand on the system by starting up new containers when required. Containeri- sation also allows for the same task to be conducted independently on different streams or sources by having a container per stream. Another advantage of a containerisation approach means that if a fault occurs on one container, it does not damage the entire system as a whole and can be fixed independently. The second challenge is handling of the inelastic audio and video traffic without introducing delay and jitter to the transmission.\nThe audio track contains a wide array of descriptive informa- tion about the sound events in the scene. Detecting sound events in real-time could have a number of uses from aiding operators in within the broadcasting industry programme creation to enhanc- ing end user accessibility. For example, BBC Research and Devel- opment [2] employs sound events detection framework to identify sounds that may disrupt the ambience of a program. This work was targeted at the BBC Autumnwatch programme which uses wildlife cameras capturing the movement of animals. To avoid undesirable noises interrupting the live stream such as cars passing by or people talking, an icon is overlayed onto the operator's interface, indicat- ing the undesirable sound event so the operator does not to switch to that source. Another use of sound event detection is closed cap- tioning. While majority of the existing work in broadcasting audio has been focused on analysing the speech events [3, 4] and only a few attempts of identifying sound events in real time in a live trans- mission has been conducted [2]. To achieve full captioning (closed captions of both sound events and speech) within IP broadcasting, a general sound event detection models that detects sound events including speech are required.\nTo overcome above challenges and integrate sound event data into IP broadcasting, this paper contributes; (1) by containerising applications to isolate each component from the other elements of the system, i.e. other processing units, transmission and reception code. Code isolation means faults are limited to that specific con- tainer as well as allowing the component to run on any machine. Containerisation makes it easy to create multiple instances of a component if needed allowing for scalability and the use of cloud platforms. (2) We leverage an Artificial Intelligence (AI) model to generate audio tags to transmit meta-information alongside audio. This added information about the contents of the audio track has a number of uses in the area of automation from better production tools to improved accessibility via more descriptive automated cap- tioning."}, {"title": "2. RELATED WORK", "content": "There are a few technologies currently used for IP broadcasting. The first of these are described in standards from the Society of Motion Picture and Television Engineers (SMPTE) as the ST 2110 suite of standards [5]. SMPTE standards are used by the industry with examples including the Serial Digital Interface (SDI) standards for transmission between equipment over a direct connection, i.e. coaxial or fibre optic cable. The Networked Media Open Specifi- cations (NMOS) from the Advanced Media Workflow Association (AMWA) uses ST 2110 along with other standards to define APIs allowing for the connection of multiple receivers and senders on a network in a vendor agnostic way. NMOS is not software, but specifications aiding development in the software. Network Device Interface (NDI) by NewTek [6] on the other hand is an open stan- dard with fully developed software and Software Development Kit (SDK), designed to allows for easy integration of IP broadcasting into existing software by utilising the NDI SDK."}, {"title": "2.1. A Brief Overview on IP Broadcasting Technology", "content": "There has been some work conducted in the broadcasting related to recognition of audio events [2]. However mostly related to speech recognition and transcription that is commonly used for tagging content for archiving purposes. Recognising speech allows for easy searching without having to manually tag content. For example, Raimond et al. [3] describe a system to automate the tagging of con- tent within the BBC's radio archive based on speech audio. Levin et al. [4] describes a system using automatic speech recognition for captioning of news programming. This system runs against a re-speaker which in this context is a person repeating speech in a more readable and understandable way for the system, avoiding the issues surrounding the acoustic environment and overlapping speak- ers. However, this systems only supports the processing of speech and does not consider sound events in general. More modern so- lutions proprietary do exists [7, 8] which remove the requirement for a re-speaker but are still incapable of including sound events. Additionally, BBC Research and Development [2] have designed an application program (a software) to identify sound events for the purpose of audio monitoring.\nIn contrast to previous work, we separate the audio tagging soft- ware from any other application programs. In our work, a modular approach is opted and a container specifically for general audio tag- ging is built to allow multiple applications on the network to take advantage of the technology without repeating work. This is helpful considering the computational overhead associated with AI models. Our system can include the monitoring system as described by BBC Research and Development [2] in addition to other systems e.g. for captioning."}, {"title": "2.2. Audio Recognition in Broadcasting", "content": "For our work, we need an IP technology that is both well used by the industry allowing for wide adoption of the audio tagging tech- nology and is simple to implement. Standards have been created to support new IP based workflows. One example from the Society of Motion Picture and Television Engineers (SMPTE) is the ST 2110 suite of standards, which describes the transport of compressed and uncompressed audio, video and metadata via individual Real-time Transport Protocol (RTP) streams. However, the complexity of un- derstanding these standards means that it is only practical for large corporations to implement. Alternative standards such as the Net- work Device Interface (NDI), which is an open standard created by NewTek [6], has an easy to use Software Development Kit (SDK). Due to easy integration, NDI is available in a wide variety of soft- ware and hardware applications, enabling its wide spread adoption in both large and small operations. This has lead to NDI being the selected technology for our work. NDI transports data in the form of frames that contain the relevant data as well as supporting infor- mation to support its use. There are three types of frames used by NDI: audio, video and metadata. NDI also handles the detection of sources allowing for routing of NDI frames."}, {"title": "3. SYSTEM DESIGN", "content": "To identify audio tags, we leverage AI models particularly convo- lutional neural networks (CNN) that has shown remarkable perfor- mance in many audio classification tasks [9, 10]. For example, pre- trained audio neural networks (PANNs) [10] have been widely used to recognize a variety of audio events. A description of AI models used in this paper for predicting the audio tags is given below,\nPre-trained Audio Neural Networks (PANNs): CNN14 [10] is a pre-trained audio neural network that is trained on Google Au- dioset dataset [11]. CNN14 is trained by extracting the log-mel spectrograms from the audio clips. CNN14 has 81M parameters and it takes 21G multiply-accumulate operations to predict tags of the audio of length 10 seconds. The trained CNN14 can predict wide range of sound events such as car passing by, speech, siren, animal etc. This helps identifying sounds in the wide array of pos- sible scenarios the system could be exposed to, i.e. different types of broadcast programming such as news gathering in various loca- tions or a panel show within a studio.\nEfficient PANNS: E-PANNs [12] is an efficient version of original PANNs with reduced memory requirement (24M parame- ters) and a reduced computational complexity (13G MACs per 10 seconds audio). The efficient AI models are beneficial in an IP net- working environment, especially one involving inelastic traffic (net- work traffic that is sensitive to variations in delay, e.g. audio and video streams). This will be explored in Section 5."}, {"title": "3.1. Selecting IP Broadcasting Technology", "content": "We use the NDI SDK [13] to create a software module including the PANNs algorithm. Due to the reliance on Python based pack- ages within PANNs module such as \"PANNs inference\" [14], we use Python for implementation. Specifically, Python binding made by the community [15] to interface between python and the C++ SDK are used to enable NDI support. An additional Python pack- age is created to simplify the process of integrating NDI into both the PANNs module and the suggested proof of concept applications described in Section 4.\nOur python package contains three classes: A receiver, trans- mitter and finder. This allows an application to receive frames us- ing the receiver class from a given NDI source, which is detected using the finder class. These can then be processed and transmit- ted by creating its own NDI source using the transmitter class. An example of how this is used here can be seen in Figure 1. It is important to note here that the flow of audio, video and metadata frames is uninterrupted between the receiver and transmitter. Each audio frame is intercepted and a copy is taken for analysis while the original copy is sent straight to the transmitter, minimising the delay and jitter. One issue surrounding the community supplied Python bindings were the associated bugs, especially surrounding memory management. This led to having to convert each frame to a Python dataclass so that it could be effectively freed and delt with by the Python garbage collector, an issue that would not have been encountered using the original C++ SDK."}, {"title": "3.2. AI Model used for Audio Tagging", "content": "In order to produce sound event predictions from PANNs model and make it compatible with other NDI applications, we follow the pipeline as shown in Figure 2 that takes the incoming audio frames from NDI and creates metadata frames containing the audio tag to be sent across the network.\nWe use two ring buffers, first ring buffer stores incoming au- dio frames. From each audio frame, we extract the individual audio samples and store them in a second ring buffer. Once a sufficient number of samples has been collected in the second ring buffer the entire contents of the ring buffer is fed into the PANNs model. The size of the second ring buffer is crucial as it determines the dura- tion of the audio window that PANNs analyses. The impact on the size of the window on the models latency is discussed in Section 5. To distribute the predicted sound event across the NDI network, we use metadata frames. These frames transport XML data, which can include third-party metadata as used here. The output string from PANNs is inserted into an XML template for transmission. Other NDI applications can then receive this XML via the metadata frames to access the sound event prediction.\nA summary of various steps is explained below,\n1. Store received audio frames in ring buffer one.\n2. Extract the floating point Pulse Code Modulated (PCM) au- dio samples from each frame and store these in ring buffer two.\n3. Wait until a given number of samples have been collected.\n4. Feed the entire contents of ring buffer two into AI model.\n5. Generate a metadata frame containing the prediction from AI model."}, {"title": "3.3. NDI Integration", "content": "The proposed containerised component allows for the integration of audio tagging capabilities into a multitude of different systems and use cases. Below, we provide two examples of integration of audio tagging system into existing IP broadcasting framework,"}, {"title": "3.4. Integrating Sound Events Metadata", "content": "Figure 3 demonstrates our system inspired by the BBC audiowatch project, where we integrate a separate audio tagging software from other application programs. We use Docker [1] for containerisation, creating multiple instances of the audio tagging software to analyse several NDI sources simultaneously. A sound event detection front end is a dashboard user interface as shown in Figure 2 and it gen- erates metadata corresponding to input audio. Metadata containing sound event information is then sent to the icon selector module for processing. Next, various icon selector containers extract the sound events from the audio track supplied within the metadata frames. After identifying the unwanted sound events, an appropriate icon overlay is transmitted as an NDI video frame. Next, a video mix- ing software such as Open Broadcaster Software (OBS) [16] is used to superimpose the icon onto the original video source for display- ing on the operators multiview, which is used to monitor all video sources."}, {"title": "4. EXAMPLE WORKFLOW", "content": "Another example integration could be the use of audio tagging to enhance closed captioning. As discussed in Section 2.2 while work has been conducted to automated closed captioning in real time us- ing automatic speech recognition, these do not include descriptions of sound events. By combining the two technologies, full closed captioning could be achieved. This would involve first parsing the audio through the audio tagging model using our container. When the result is returned as human speech, the audio would then be passed through a second speech recognition model to generate sub- titles. One major concern would be the accuracy of the audio tag- ging model. If the speech was not always detected, we would miss large portions of speech text. Additionally the difference in latency between a sound event being inserted and speech going through two models would have to be accounted for."}, {"title": "4.1. Audiowatch Example", "content": "There are a number of integration challenges to consider while de- signing AI based software fit for broadcasting. These challenges include the accuracy of the prediction and the latency of the model delaying the signal. Generally, PANNs and E-PANNs give similar prediction results.\nModel latency: The latency of the model here describes the amount of time it takes given a number of audio samples to produce an accurate sound event prediction. Consideration of the model la- tency is significant given that we are dealing with inelastic audio and video network traffic. This means that any delay in process- ing contributes to a delay in the resulting transmission depending on the infrastructure. Delay can be mitigated using a design simi- lar to shown in Figure 1, however there is the issue of predictions being desynced to the audio track. Although we have minimal con- trol over the IP network using the audio tagging module, and thus cannot manage the network's latency, we can still select an optimal model that minimises latency while maintaining accuracy.\nBuffer size versus model latency: To analyse buffer size and latency of model, we perform experimentation using a set of audio recordings with known sound events. The first audio recording is taken directly from the PANNs repository that involves a telephone ringing followed by human speech and is of seven seconds. The second audio recording of a car driving into the distance. The third audio recording is created by mixing a car driving and a running river sound events.\nGiven the audio recordings, we analyse latency of the AI model at different number of audio samples. We generate different length audio segments. The audio samples taken are of multiples of 1024 (assuming frames containing 1024 samples are used) and represents the size of the buffer. Given the audio samples of different length, we use the PANNs or E-PANNs model to produce predictions while measuring the time taken for the model to produce a prediction. Figure 4 shows latency by PANNs and E-PANNs model at differ- ent buffer size. Both PANNs and E-PANNs follow a similar tra- jectory with E-PANNs showing a considerable improvement in la- tency. This suggests that choosing an appropriate model contribute to improve latency and hence making integration of audio events more real-time while using less resources.\nIt is found that a buffer size of 48128 samples (47 * 1024 sample frames) is a sensible choice in having a low latency while produc- ing an accurate result in detecting the sound events correctly. This equates to an audio window with a duration of 1.002s sampled at 48KHz, that gives correct results with the minimal latency. Predic- tion results and model latency computed on a AMD Ryzen 5 2500U and Intel Core i9-13900HX hardware can be found here."}, {"title": "4.2. Online Closed Captioning", "content": "The integration of IP broadcasting with audio tagging offers sig- nificant potential for enhancing broadcast workflows, but it also presents several challenges. The transition to IP broadcasting en- ables a more flexible, scalable, and reconfigurable infrastructure compared to traditional methods based on Serial Digital Interface (SDI). This flexibility is further enhanced by containerisation tech- nologies making the system more resilient and adaptable. However, implementing an audio tagging system introduces challenges pri- marily related to latency and the accuracy of audio tagging models.\nOne of the primary challenges discussed is the latency associ- ated with the audio tagging model. Given the real-time nature of broadcasting, any delays introduced by processing can impact the overall operation. This makes the choice of buffer size crucial. A smaller buffer reduces latency but might compromise the accuracy of sound event detection. Conversely, a larger buffer improves ac- curacy but increases latency. The experiments conducted show that an acceptable balance can be achieved with a buffer size of 48128 samples, which provides an acceptable latency while maintaining accuracy. The use of Efficient PANNS (E-PANNs) further helps in reducing the computational complexity and memory requirements, making it a suitable choice for real-time applications.\nContainerisation offers a robust solution to scalability issues. By isolating the audio tagging functionality into a microservice, it becomes possible to scale the system by simply adding more con- tainers as needed. This isolation also ensures that a fault in one container does not affect the entire system, enhancing overall reli- ability. The use of Docker to containerise these services allows for easy deployment and management across different network setups. Additionally, the integration with NDI technology, which is widely adopted in the industry, ensures broad applicability.\nDespite these advantages, real-world deployment of such a sys- tem is not without hurdles. The reliance on Python bindings to inter- face with the NDI SDK, while practical, introduces potential issues with memory management that need careful handling."}, {"title": "5. AI MODEL INTEGRATION CHALLENGES", "content": "Integrating IP broadcasting with audio tagging presents a promis- ing advancement for the broadcasting industry. The use of con- tainerisation and audio tagging for real-time sound event detec- tion can significantly enhance content production and accessibility. However, addressing the challenges of latency, accuracy and real- world deployment is crucial for the successful implementation of this technology. Future work includes re-writing the codebase to use the NDI C++ SDK directly, avoiding the issues surrounding the Python bindings. Additionally, we would like to analyse more com- plex models such as transformers [17, 18] within our broadcasting framework. Finally, the creation of the discussed proof of concept applications would allow for full demonstration of the usefulness of this technology."}, {"title": "6. DISCUSSION AND CONCLUSION", "content": ""}, {"title": "6.1. Conclusion", "content": ""}]}