{"title": "HDRGS: High Dynamic Range Gaussian Splatting", "authors": ["Jiahao Wu", "Lu Xiao", "Chao Wang", "Rui Peng", "Kaiqiang Xiong", "Ronggang Wang"], "abstract": "Recent years have witnessed substantial advancements in the field of 3D reconstruction from 2D images, particularly following the introduction of the neural radiance field (NeRF) technique. However, reconstructing a 3D high dynamic range (HDR) radiance field, which aligns more closely with real-world conditions, from 2D multi-exposure low dynamic range (LDR) images continues to pose significant challenges. Approaches to this issue fall into two categories: grid-based and implicit-based. Implicit methods, using multi-layer perceptrons (MLP), face inefficiencies, limited solvability, and overfitting risks. Conversely, grid-based methods require significant memory and struggle with image quality and long training times. In this paper, we introduce Gaussian Splatting-a recent, high-quality, real-time 3D reconstruction technique\u2014into this domain. We further develop the High Dynamic Range Gaussian Splatting (HDR-GS) method, designed to address the aforementioned challenges. This method enhances color dimensionality by including luminance and uses an asymmetric grid for tone-mapping, swiftly and precisely converting pixel irradiance to color. Our approach improves HDR scene recovery accuracy and integrates a novel coarse-to-fine strategy to speed up model convergence, enhancing robustness against sparse viewpoints and exposure extremes, and preventing local optima. Extensive testing confirms that our method surpasses current state-of-the-art techniques in both synthetic and real-world scenarios. Code will be released at https://github.com/WuJH2001/HDRGS", "sections": [{"title": "1 Introduction", "content": "In recent years, significant progress has been made in 3D reconstruction technology [50, 31, 18], but these technologies typically assume constant exposure conditions of input images, thus restoring scenes with low dynamic range (LDR). However, high dynamic range (HDR) scenes [37, 46], which are more consistent with the physical world, offer a broader dynamic range and provide a superior visual experience for humans. Traditional HDR image reconstruction techniques [29, 47, 8] still focus on 2D images. How to reconstruct 3D HDR scenes from multi-exposure unstructured LDR images remains a question worthy of investigation.\nCurrent methodologies in this domain can be divided into two distinct categories: explicit-based, represented by HDR-plenoxel [17], and implicit-based, represented by HDR-NeRF [16]. While these methods have achieved some impressive results, HDR-plenoxel relies on grids and spherical harmonics, complicating the construction of high-quality 3D HDR radiance fields and requiring substantial memory. HDR-NeRF models the entire HDR scene and Camera Response Function (CRF)[10] using implicit MLPs, resulting in poor interpretability and slow training and rendering speeds, which poses challenges for real-world applications.With the recent introduction of 3D Gaussian Splatting technology [18], which can efficiently and effectively complete 3D reconstruction, it appears to offer a direction for addressing the aforementioned issues."}, {"title": "2 Related work", "content": "HDR Imaging. The typical approach in traditional HDR imaging [37] is crucial for enhancing people's immersive visual experiences. Currently, tone mapping, generating LDR from HDR, is a mature field [37, 47], but its inverse tone mapping (ITM) [38], recovering HDR from LDR, still faces many challenges. The simplest method is multi-frame LDR image synthesis with different exposures to create an HDR image. As the name suggests, this involves extracting the most visible parts from each LDR image and combining them into a single HDR image. However, such methods"}, {"title": "3 Method", "content": "Given a series of low dynamic range (LDR) images captured under multiple exposure conditions from different viewpoints, our task is to efficiently reconstruct a high-quality high dynamic range (HDR) radiance field solely from these LDR images and obtain HDR images through rendering. The entire framework is illustrated in the Fig 1. In this section, we will provide a detailed introduction to each component of our method. In Sec.3.2, we introduce the basic process of rendering and tone mapping. In Sec.3.3, we discuss the design of our tone mapper function. Then, in Sec.3.4, we introduce our coarse-to-fine strategy, followed by the optimization process in Sec.3.5."}, {"title": "3.1 Preliminary", "content": "3DGS initializes with a sparse point cloud generated from Structure-from-Motion (SfM) [5]. This method models geometric shapes as a set of 3D Gaussian functions defined by covariance matrices and means in world space.\n$G(x) = e^{-(x-\\mu_{3D})^T\\Sigma_{3D}^{-1}(x-\\mu_{3D})}$\nWhere \u00b53D is the mean of the Gaussian point. To ensure the 3D Gaussian covariance matrix retains physical meaning, specifically to maintain positive semi-definiteness, it can be further decomposed into a scale matrix S and a rotation matrix R. Thus, the 3D covariance matrix can be represented as:\n$\\Sigma_{3D} = RSST RT$\nwhere \u03a33D is the covariance matrix of the Gaussian point. To render the image, first approximate the projection of the 3D Gaussian into 2D image space using perspective transformation. Specifically, the projection of the 3D Gaussian is approximated as a 2D Gaussian with center \u00b52D and covariance \u03a32D. \u00b52D and \u03a32D are computed as:\n$\\mu_{2D} = (K((W\\mu_{3D})/(W\\mu_{3D})_z))_{1:2} \\Sigma_{2D} = (JWE_{3D}WT.JT)_{1:2,1:2}$\nW and K are the viewing transformation and projection matrix. Finally, after sorting the Gaussian points by depth, the pixel p value can be computed as:\n$Cp = \\sum_{i \\in N} \\alpha_i \\prod_{j=1}^{i-1}(1 - \\alpha_j), where \\alpha_i = exp(-\\frac{1}{2}(x - \\mu_{2D})T\\Sigma^{-1}_{2D}(x - \\mu_{2D}))$. Ci refers to the RGB color of Gaussian point which is represented by spherical harmonics (SH)."}, {"title": "3.2 Basic process", "content": "Radiance-based a composition. According to the principles of physical imaging, the radiance L emitted by objects in the scene is transformed into irradiance E on the surface of the image sensor as it passes through the camera lens. To simulate this process, we redefine the color of Gaussian points as radiance L, which results in the pixel values of the image formed by Gaussian points splatting onto the HDR plane no longer representing color C, but irradiance E. We can rewrite the rendering formula of 3DGS[18] 4 as follows:\n$E(p) = \\sum_{i=0}^{N} L_i\\alpha_i \\prod_{j=1}^{i-1}(1 - \\alpha_j) where E(p) \\in (0, +\\infty)$\nIn which p means the pixel, E represents pixel irradiance of p. as shown in Fig 1(b), Li represents the radiance of i-th Gaussian point used to render pixel p, as shown in Fig 1(a).\nImaging process. The total irradiance received by the image sensor within the exposure time t results in the accumulated exposure. After undergoing photoelectric conversion by the sensor, analog-to-digital conversion, etc. the LDR pixel value C is obtained. The entire imaging process can be represented by a function called the camera response function (CRF) F(\u00b7). Combining E(\u00b7), we represent the entire imaging process with the following formulation[42]:\n$C(p, t) = F(E(p) * t(p))$\nwhere t represents the exposure time of the camera capturing a light ray (or pixel), which depends on the shutter speed. Following the classic non-parametric CRF calibration method by Debevec and Malik[8], we assume that the CRF F(\u00b7) is monotonic and invertible. Therefore, we can rewrite the Eq. 6 as:\n$ln F^{-1}(C(p, t)) = ln E(p) + lnt(p)$\nAfter further simplification, it can be expressed as the following equation:\n$C(p, t) = g(ln E(p) + ln t(p)), where g = (ln F^{-1})^{-1}$\nThe tone mapper function can thus be transformed into the function g(\u00b7), where we use an asymmetric grid to model. We will illustrate the details of the asymmetric grid in Sec.3.3. Here, for convenience, we disregard the logarithm and denote ln E(p) + lnt(p) as exposure.\nIn practical operations, to streamline parameter learning and minimize the number of iterations required, we directly learn the value of ln E, the logarithmic domain of E. Please note that the radiance of the Gaussian points learned after such transformation also change. We denote it as L'."}, {"title": "3.3 Grid-based tone mapper", "content": "To faithfully simulate the physical imaging process, in this section, we introduce our asymmetrical grid-based tone mapper g to model the process from pixel irradiance E' to color c, as shown in fig.1(b). Recently, there have been several grid-based methods for modeling 3D scenes, such as [51,"}, {"title": "3.4 Coarse to fine optimization strategy", "content": "The above sections describe the main process of our training pipe. However, we empirically found that directly using a grid as the tonemapping function and jointly training it with the attributes of Gaussian points leads to severe coupling and overfitting to the training dataset, as shown in Fig.7. Therefore, we designed a coarse-to-fine strategy: during the coarse phase, we use a fixed function gs as the tonemapper and train only the attributes of the Gaussian points. In the fine phase, we then use the grid as the tonemapper and jointly train it with the attributes of the Gaussian points. Now, the next question is which function gs we should choose as the tonemapper during the coarse phase. According to Debevec [8], we can opt for a monotonically increasing, smooth function bounded between 0 and 1 as our tone mapper for pre-training the Gaussian points. For simplicity, we opt for the sigmoid function. Therefore, in the coarse phase, the function relationship between pixel irradiance E' and LDR color C can be expressed as:\n$C(p,t) = gs(E'(p) + t' (p))$\nDuring the coarse phase, we typically set the number of training iterations to 6000, and the training takes approximately 50 seconds. We also noticed that [49] solely employs a sigmoid function as their tone mapper. Due to the limited expressive power of the sigmoid function, this can lead to their HDR images experiencing floating-point issues and biasing towards white. Lastly, we will highlight the significance of the coarse stage in ablation research Tab.2."}, {"title": "3.5 Loss function", "content": "Reconstruction Loss. We adopt the same loss function as in 3DGS [18] to constrain our Gaussian points:\n$Lrec = (1 - \\lambda_1)L_1 + \\lambda_1LD-SSIM,$\nSmooth Loss: To ensure that our grid g conforms to the CRF properties proposed by Debevec [8] that CRFs increase smoothly, we employ the loss function [17]:\n$Lsmooth = \\sum_{i=1}^{N}\\sum_{e \\in [a,b]}g''(e)^2,$\nWhere the gi' (e) denotes the second order derivative of grid w.r.t. e in the domain (a, b) of the grid.\nUnit Exposure Loss. Learning HDR radiance fields solely from LDR images may result in radiance fields with various scales E. To ensure consistency between the HDR radiance fields we construct and those generated by Blender, facilitating HDR quality evaluation, we employ the loss function introduced by HDRNeRF[16]:\n$Lu = ||g(0) - C_o||^2$.\nWhere Co = 0.73. The ablation studies shown in Table. 2 and Fig. 3.\nTotal loss. Finally, by combining the aforementioned loss functions, we derive the total loss function as follows:\n$Ltotal = Lrec + \\lambda_2Lsmooth + \\lambda_3Lu$"}, {"title": "4 Experiments", "content": "In this section, we will present our experimental setup and the dataset used in Sec.4.1 and 4.2 respectively. We will go into more detail about the experiment results in Sec.4.3. Our experiments on synthetic and real datasets demonstrate that our method achieves state-of-the-art performance."}, {"title": "4.1 Implementation details", "content": "To avoid the explosion of point numbers during training in complex scenes, we employ the pruning strategy proposed by [34], as detailed in the supplementary material. The Gaussian point parameters are set in the same way as in 3DGS[18]. The loss function parameters for synthetic datasets are \u03bb2 = 0.3, \u03bb3 = 0.5, but for real datasets they are \u5165\u2081 = 0.2, \u03bb2 = 1e \u2013 3, 13 = 0. We utilize the Adam optimizer[19] for training. As for the learning rate of the asymmetric grid, we initially set it to 0.02, and then decayed to 5e-6. Our usual setting in sparsely interpolated areas is less than 64 nodes. The entire model is trained on an NVIDIA A100, with 6000-14000 epochs for the coarse phase, completing in under 1 minute, and 17000-30000 epochs for the fine phase, totaling approximately 4-8 minutes of training time. We also test our code on the Tesla V100, and the training time does not exceed 10 minutes. Our model runs consistently with a GPU memory use of less than 5GB during training."}, {"title": "4.2 Dataset", "content": "The dataset we utilize is provided by HDRNeRF[16], consisting of 8 synthetic scenes rendered by Blender and 4 real scenes captured by digital cameras. Each dataset comprises 35 different viewpoints captured by a forward-facing camera, with each viewpoint having 5 exposure levels {t1, t2, 3, 4, 5}, ranging from -4EV to 5EV. We strictly followed HDRNeRF's training and measurement protocols, employing one image per exposure level for each view, resulting in a total of 18 training images. The test images consist of 85 images, covering 5 exposure levels across 17 views. The resolution of each view in synthetic scenes is 400 \u00d7 400 pixels, whereas in real scenes, it is 804 \u00d7 534 pixels."}, {"title": "4.3 Evaluation", "content": "We will compare our method with the following baseline methods: NeRF [31], NeRF-w [28], HDRNeRF [16], HDR-Plenoxel [17], and 3DGS [18]. Additionally, we conducted ablation studies on some of our key modules, as presented in Table. 2. The metrics measured include training time, PSNR, SSIM [48], LPIPS [53], High Dynamic Range-Visual Difference Predictor (HDR-VDP) [24, 25, 26, 33], PUPSNR [3], PUSSIM [3], and FPS. All the HDR results are tone-mapped using Photomatix. These measurement results collectively indicate the efficiency of our proposed method.\nHDR metrics. We employ two Authoritative HDR image measurement methods: High Dynamic Range-Visual Difference Predictor (HDR-VDP) [24, 25, 26, 33] and Perceptually Uniform (PU21) [3]. HDR-VDP simulates the human eye's perception of high dynamic range images under various lighting conditions, offering a means to quantify perceptual differences in images. We have opted for"}, {"title": "5 Conclusion", "content": "To recover 3D HDR radiance fields from 2D multi-exposure LDR images, we propose HDRGS, a real-time rendering-supported method with highly interpretable HDR radiance field reconstruction. We redefine the color of Gaussian points as radiance, implying that each Gaussian point radiates radiance rather than color. Then we construct an uneven asymmetric grid as our tone mapper, mapping irradiance received by each pixel to LDR color values. Additionally, to mitigate the grid's discrete nature and the tendency to fall into local optima, we introduce a novel coarse-to-fine strategy, effectively accelerating model convergence and enhancing reconstruction quality, making it more robust for various complex exposure scenes. Each of our module designs can be mathematically explained. Experimental results on real and synthetic datasets demonstrate that our method achieves state-of-the-art efficiency and quality in reconstructing both LDR images from arbitrary viewpoints and HDR images from arbitrary viewpoints and exposures. To our knowledge, our method marks the pioneering exploration into the potential of Gaussian splatting for 3D HDR reconstruction. Our model and data code will be fully provided for future research endeavors."}, {"title": "A Appendix / supplemental material", "content": "We will provide more detailed information about our experimental procedures in the supplemental material. In Section A.1, we will explore additional implementation details. Section A.2 will outline the derivation process of fH. Section A.3 will discuss some of the inherent issues in the HDRNeRF model. Finally, Section A.4 will present additional experimental results."}, {"title": "A.1 Additional Implementation Details", "content": "Pruning Strategy. We utilize the Ray Contribution-Based Pruning technique described in [34] as our pruning technique. giving each Gaussian point a score:\n$h(p_i) = \\underset{If}{\\text{max}} \\frac{\\alpha_{\\tau}}{\\sqrt{|I_f|}}, r \\epsilon I_f$\nwhere \u03b1\u03c4 represents the contribution of Gaussian point pi to pixel r. A Gaussian point pi can be retained as long as its contribution to any pixel point during a pruning interval exceeds the threshold; otherwise, it must be pruned. This strategy allows for the removal of points that make relatively low contributions to rendering the image. In this case, the threshold is set at 0.02, and pruning begins after 500 rounds, with intervals of 200 rounds each.\nIt is worth noting that this pruning technique must be staggered with transparency reset, as it will cause all Gaussian scores h(pi) to drop below the cutoff value."}, {"title": "A.2 Proof", "content": "The functional relationship f\u3150(\u00b7) between the learnable parameters E' and the HDR image pixel values E will be determined here. If no time scaling is performed, then E satisfies the following equation:\n$C_1 = g_1 (ln E + Int)$\nIf time scaling is performed, then E' satisfies the following equation:\n$C_2 = g_2(r. lnt + s + E')$\nWhere g1 and g2 represent different CRF functions, both of which are monotonic and differentiable. Since the same LDR image is used for supervision regardless of whether time scaling is performed or not, C\u2081 = C2. Therefore, E' and E satisfy:\n$g_1(lnt + ln E) = g_2(r \\cdot lnt + f (ln E)) where f(ln E) = E' + s$\nFixing E, differentiate both sides w.r.t. Int, we obtain:\n$g'_1 = g'_2.r$\nFixing t, differentiate both sides w.r.t. In E, we obtain:\n$g'_1 = g'_2 f'$\nFrom the eq 24 25, we can infer that f' = r, which implies:\n$\\frac{d(E' + s)}{d(ln E)} = r$\nIf the unit exposure loss is adopted, then there exists only a scaling relationship between g1 and g2, without any offset. Therefore, we can easily derive the final equation:\n$E=e^{\\frac{(E'+s)}{r}}$\nFrom Eq.27, we can render HDR images after the model training is completed."}, {"title": "A.3 Discussion", "content": "During our experiments, we observed instances where the tone mapper of HDRNeRF [16] occasionally overfits to LDR, leading to an inaccurate HDR radiance field. As depicted in Fig. 6, it's evident that directly utilizing MLP in HDRNeRF to model the tone mapper is unstable. Employing three separate MLPs in HDRNeRF [16] to handle the RGB channels independently might induce crosstalk problems, where interference between channels could potentially lead to being trapped in local optimal solutions. While HDRNeRF [16] learns the correct LDR, errors arise in HDR radiance field construction. Furthermore, in [17, 49], the authors mention that constructing a tone mapping module based on MLPs may fail to correctly decouple nonlinear components."}, {"title": "A.4 Additional Results", "content": "In this section, we present more experimental results. Table. 5 shows the HDR measurement results compared to HDRNeRF on the synthetic dataset. Tab3 and Tab 4 demonstrate the LDR measurement results of our method compared to other baseline methods across various scenes. In Fig7, we present LDR images of our various ablation methods (with/without coarse stage, with/without unit exposure loss, with/without t scaling, asymmetric or symmetric grid ) and other methods under the same viewpoint but different exposure time. In the ablation experiments of the coarse stage, we can clearly see that without the coarse stage, the asymmetric grid would easily overfit to the exposure time used in the training dataset. In Fig10, we show the distribution of irradiance E' for each scene in the first frame of the test dataset. It can be observed that the radiance distribution of most scenes is highly uneven, especially scenes like \"diningroom\", which easily lead the grid into local optimization problems. In Fig.9, we present additional results, where (e) and (f) respectively show HDR images rendered by HDR-NeRF and our method. The upper right triangles of these images are the rendered HDR images, while the lower left triangles are error maps drawn by HDR-VDP."}]}