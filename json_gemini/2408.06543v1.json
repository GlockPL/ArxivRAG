{"title": "HDRGS: High Dynamic Range Gaussian Splatting", "authors": ["Jiahao Wu", "Lu Xiao", "Chao Wang", "Rui Peng", "Kaiqiang Xiong", "Ronggang Wang"], "abstract": "Recent years have witnessed substantial advancements in the field of 3D\nreconstruction from 2D images, particularly following the introduction of the\nneural radiance field (NeRF) technique. However, reconstructing a 3D high\ndynamic range (HDR) radiance field, which aligns more closely with real-world\nconditions, from 2D multi-exposure low dynamic range (LDR) images continues\nto pose significant challenges. Approaches to this issue fall into two categories:\ngrid-based and implicit-based. Implicit methods, using multi-layer perceptrons\n(MLP), face inefficiencies, limited solvability, and overfitting risks. Conversely,\ngrid-based methods require significant memory and struggle with image quality\nand long training times. In this paper, we introduce Gaussian Splatting\u2014a\nrecent, high-quality, real-time 3D reconstruction technique\u2014into this domain.\nWe further develop the High Dynamic Range Gaussian Splatting (HDR-GS)\nmethod, designed to address the aforementioned challenges. This method\nenhances color dimensionality by including luminance and uses an asymmetric\ngrid for tone-mapping, swiftly and precisely converting pixel irradiance to color.\nOur approach improves HDR scene recovery accuracy and integrates a novel\ncoarse-to-fine strategy to speed up model convergence, enhancing robustness\nagainst sparse viewpoints and exposure extremes, and preventing local optima.\nExtensive testing confirms that our method surpasses current state-of-the-art\ntechniques in both synthetic and real-world scenarios. Code will be released\nat https://github.com/WuJH2001/HDRGS", "sections": [{"title": "1 Introduction", "content": "In recent years, significant progress has been made in 3D reconstruction technology [50, 31, 18],\nbut these technologies typically assume constant exposure conditions of input images, thus restoring\nscenes with low dynamic range (LDR). However, high dynamic range (HDR) scenes [37, 46], which\nare more consistent with the physical world, offer a broader dynamic range and provide a superior\nvisual experience for humans. Traditional HDR image reconstruction techniques [29, 47, 8] still\nfocus on 2D images. How to reconstruct 3D HDR scenes from multi-exposure unstructured LDR\nimages remains a question worthy of investigation.\n\nCurrent methodologies in this domain can be divided into two distinct categories: explicit-based,\nrepresented by HDR-plenoxel [17], and implicit-based, represented by HDR-NeRF [16]. While\nthese methods have achieved some impressive results, HDR-plenoxel relies on grids and spherical\nharmonics, complicating the construction of high-quality 3D HDR radiance fields and requiring\nsubstantial memory. HDR-NeRF models the entire HDR scene and Camera Response Function\n(CRF)[10] using implicit MLPs, resulting in poor interpretability and slow training and rendering\nspeeds, which poses challenges for real-world applications.With the recent introduction of 3D\nGaussian Splatting technology [18], which can efficiently and effectively complete 3D reconstruction,\nit appears to offer a direction for addressing the aforementioned issues."}, {"title": "2 Related work", "content": "HDR Imaging. The typical approach in traditional HDR imaging [37] is crucial for enhancing\npeople's immersive visual experiences. Currently, tone mapping, generating LDR from HDR, is\na mature field [37, 47], but its inverse tone mapping (ITM) [38], recovering HDR from LDR, still\nfaces many challenges. The simplest method is multi-frame LDR image synthesis with different\nexposures to create an HDR image. As the name suggests, this involves extracting the most visible\nparts from each LDR image and combining them into a single HDR image. However, such methods\nbased on multiple exposures [8] are typically constrained by the need for multiple exposure LDR\nimages and are prone to artifacts caused by camera movements during LDR image capture. The\nemergence of deep learning in recent years has brought new solutions, with learning-based methods\nbeing categorized into indirect and direct approaches. Indirect methods [12, 21] typically predict\nmultiple LDR images with different exposures, which are then merged into an HDR image (Indirect\nITM), while direct methods [11, 27] involve neural networks directly predicting HDR images (Direct\nITM). However, these methods can only synthesize HDR images with the original camera poses.\n\nNovel View Synthesis. Previous methods formalized the light field [22] or Lumigraph [5] and\ngenerated novel views by interpolating from existing views. This approach required densely\ncaptured images for realistic rendering. Subsequently, some geometry-based methods were proposed.\nMesh-based methods [9, 39, 43, 45, 1] supported efficient rendering but struggled with surface\noptimization. Volume-based methods used voxel grids [20, 35, 40] or multi-plane images (MPI) [2,\n30, 41, 54], offering detailed rendering but suffering from low memory efficiency or being limited\nto small view variations. In recent years, the method proposed by Neural Radiance Field (NeRF)\n[31] to model 3D scenes using MLP has gained widespread recognition. This has led to a series of\nrelated research efforts, such as speeding up training and inference speeds [7, 14, 32], improving\nrendering quality [4, 44], view synthesis for dynamic scenes [36, 6, 13]. However, these methods have\nconsistently struggled to achieve real-time rendering. Last year, 3DGS [18] emerged and addressed\nthis issue for people. Not only that, but 3DGS also offers higher rendering quality and interpretability.\nMore and more people are getting involved in related research [52, 15, 23]."}, {"title": "3 Method", "content": "Given a series of low dynamic range (LDR) images captured under multiple exposure conditions from\ndifferent viewpoints, our task is to efficiently reconstruct a high-quality high dynamic range (HDR)\nradiance field solely from these LDR images and obtain HDR images through rendering. The entire\nframework is illustrated in the Fig 1. In this section, we will provide a detailed introduction to each\ncomponent of our method. In Sec.3.2, we introduce the basic process of rendering and tone mapping.\nIn Sec.3.3, we discuss the design of our tone mapper function. Then, in Sec.3.4, we introduce our\ncoarse-to-fine strategy, followed by the optimization process in Sec.3.5."}, {"title": "3.1 Preliminary", "content": "3DGS initializes with a sparse point cloud generated from Structure-from-Motion (SfM) [5]. This\nmethod models geometric shapes as a set of 3D Gaussian functions defined by covariance matrices\nand means in world space.\n\n\\(G(x) = e^{-(x-\\mu_{3D})^T\\Sigma_{3D}^{-1}(x-\\mu_{3D})}\\)\n\nWhere \\(\\mu_{3D}\\) is the mean of the Gaussian point. To ensure the 3D Gaussian covariance matrix retains\nphysical meaning, specifically to maintain positive semi-definiteness, it can be further decomposed\ninto a scale matrix S and a rotation matrix R. Thus, the 3D covariance matrix can be represented as:\n\n\\(\\Sigma_{3D} = RSS^{T}R^{T}\\)\n\nwhere \\(\\Sigma_{3D}\\) is the covariance matrix of the Gaussian point. To render the image, first approximate the\nprojection of the 3D Gaussian into 2D image space using perspective transformation. Specifically,\nthe projection of the 3D Gaussian is approximated as a 2D Gaussian with center \\(\\mu_{2D}\\) and covariance\n\\(\\Sigma_{2D}\\). \\(\\mu_{2D}\\) and \\(\\Sigma_{2D}\\) are computed as:\n\n\\(\\mu_{2D} = (K((W\\mu_{3D})/(W\\mu_{3D})_z))_{1:2}\\ \\Sigma_{2D} = (JWE_{3D}W^{T}.J^{T})_{1:2,1:2}\\)\n\nW and K are the viewing transformation and projection matrix. Finally, after sorting the Gaussian\npoints by depth, the pixel p value can be computed as:\n\n\\(C_p = \\sum_{i \\epsilon N} \\alpha_i \\prod_{j=1}^{i-1}(1 - \\alpha_j), \\ \\text{where} \\ \\alpha = exp(-\\frac{1}{2}(x - \\mu_{2D})^T\\Sigma_{2D}^{-1}(x - \\mu_{2D}))\\).\n\nCi refers to the RGB color of Gaussian point which is represented by spherical harmonics (SH)."}, {"title": "3.2 Basic process", "content": "Radiance-based \u03b1 composition. According to the principles of physical imaging, the radiance L\nemitted by objects in the scene is transformed into irradiance E on the surface of the image sensor as\nit passes through the camera lens. To simulate this process, we redefine the color of Gaussian points\nas radiance L, which results in the pixel values of the image formed by Gaussian points splatting\nonto the HDR plane no longer representing color C, but irradiance E. We can rewrite the rendering\nformula of 3DGS[18] 4 as follows:\n\n\\(E(p) = \\sum_{i=0}^{N} L_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j) \\ \\text{where} \\ E(p) \\epsilon (0, +\\infty)\\)\n\nIn which p means the pixel, E represents pixel irradiance of p. as shown in Fig 1(b), Li represents\nthe radiance of i-th Gaussian point used to render pixel p, as shown in Fig 1(a).\n\nImaging process. The total irradiance received by the image sensor within the exposure time\nt results in the accumulated exposure. After undergoing photoelectric conversion by the sensor,\nanalog-to-digital conversion, etc. the LDR pixel value C is obtained. The entire imaging process can\nbe represented by a function called the camera response function (CRF) F(\u00b7). Combining E(\u00b7), we\nrepresent the entire imaging process with the following formulation[42]:\n\n\\(C(p, t) = F(E(p) * t(p))\\)\n\nwhere t represents the exposure time of the camera capturing a light ray (or pixel), which depends\non the shutter speed. Following the classic non-parametric CRF calibration method by Debevec and\nMalik[8], we assume that the CRF F(\u00b7) is monotonic and invertible. Therefore, we can rewrite the\nEq. 6 as:\n\n\\(\\text{ln} F^{-1}(C(p, t)) = \\text{ln} E(p) + \\text{ln} t(p)\\)\n\nAfter further simplification, it can be expressed as the following equation:\n\n\\(C(p, t) = g(\\text{ln} E(p) + \\text{ln} t(p)), \\text{where} \\ g = (\\text{ln} F^{-1})^{-1}\\)\n\nThe tone mapper function can thus be transformed into the function g(\u00b7), where we use an asymmetric\ngrid to model. We will illustrate the details of the asymmetric grid in Sec.3.3. Here, for convenience,\nwe disregard the logarithm and denote ln E(p) + lnt(p) as exposure.\n\nIn practical operations, to streamline parameter learning and minimize the number of iterations\nrequired, we directly learn the value of ln E, the logarithmic domain of E. Please note that the\nradiance of the Gaussian points learned after such transformation also change. We denote it as L'."}, {"title": "3.3 Grid-based tone mapper", "content": "To faithfully simulate the physical imaging process, in this section, we introduce our asymmetrical\ngrid-based tone mapper g to model the process from pixel irradiance E' to color c, as shown in\nfig.1(b). Recently, there have been several grid-based methods for modeling 3D scenes, such as [51,\n6, 13, 14, 32], etc. These grid-based methods accelerate model training while ensuring rendering\nquality. Influenced by their work, we designed an asymmetric grid to model our tone mapper. The\ndifference is that we use an asymmetric grid, while they use a symmetric one. We only need one tone\nmapping module to map irradiance to color for any seen or unseen viewpoint.\n\nLearned asymmetric grid. We have empirically found that in some scenarios, the distribution of\nirradiance values is highly uneven. For instance, the majority of irradiance values might fall within\nthe range (a, b), but a small portion may also lie within the range (b, c). If a uniform symmetric grid\nis used, modeling these two regions presents the following drawbacks: using a sparse grid in the\nrange with dense irradiance distribution can lead to inaccurate mapping, while using a dense grid in\nthe range with sparse irradiance distribution can cause overfitting. Therefore, as shown in Fig.1(c),\nwe introduce an asymmetric grid, which refers to a grid that lacks both central and density symmetry.\nIn the range of values with a dense irradiance distribution (a,b), we use a dense grid (128 nodes per\nunit) to model the tonemapping function. Conversely, we use a sparse grid (64 nodes per unit) for\nregions with a sparse irradiance distribution (b,c). It offers greater flexibility and expressiveness\ncompared to the symmetric grid [51, 6, 13, 14, 17, 32], especially in the scene with uneven irradiance\ndistribution. Considering generality, we usually set b=0, but in some extreme cases, b needs to be\nadjusted according to the scenario.\n\nIn the training phase, gradients would not propagate backward if boundary values were merely\nassigned to the small percentage of exposure values that fall beyond the grid range. Therefore, we\ndesign the following function to deal with these exposure values:\n\n\\(\\text{gleaky}(x) = \\begin{cases} \\beta(x -a), & x 0 \\ \\text{Where} \\ C_0 = 0.73. \\ \\text{The ablation studies shown in Table. 2 and Fig. 3.} \\\\ \\text{Total loss.} \\ \\text{Finally, by combining the aforementioned loss functions, we derive the total loss function} \\\\ \\text{as follows:}\\\\ L_{total} = L_{rec} + \\lambda_2 L_{smooth} + \\lambda_3 L_u\\)"}, {"title": "5 Conclusion", "content": "To recover 3D HDR radiance fields from 2D multi-exposure LDR images, we propose HDRGS, a\nreal-time rendering-supported method with highly interpretable HDR radiance field reconstruction.\nWe redefine the color of Gaussian points as radiance, implying that each Gaussian point radiates\nradiance rather than color. Then we construct an uneven asymmetric grid as our tone mapper,\nmapping irradiance received by each pixel to LDR color values. Additionally, to mitigate the grid's\ndiscrete nature and the tendency to fall into local optima, we introduce a novel coarse-to-fine strategy,\neffectively accelerating model convergence and enhancing reconstruction quality, making it more\nrobust for various complex exposure scenes. Each of our module designs can be mathematically\nexplained. Experimental results on real and synthetic datasets demonstrate that our method achieves\nstate-of-the-art efficiency and quality in reconstructing both LDR images from arbitrary viewpoints\nand HDR images from arbitrary viewpoints and exposures. To our knowledge, our method marks\nthe pioneering exploration into the potential of Gaussian splatting for 3D HDR reconstruction. Our\nmodel and data code will be fully provided for future research endeavors."}, {"title": "A Appendix / supplemental material", "content": "We will provide more detailed information about our experimental procedures in the supplemental\nmaterial. In Section A.1, we will explore additional implementation details. Section A.2 will outline\nthe derivation process of fH. Section A.3 will discuss some of the inherent issues in the HDRNeRF\nmodel. Finally, Section A.4 will present additional experimental results."}, {"title": "A.1 Additional Implementation Details", "content": "Pruning Strategy. We utilize the Ray Contribution-Based Pruning technique described in [34] as our\npruning technique. giving each Gaussian point a score:\n\n\\(h(P_i) = \\frac{\\max\\limits_{l_f \\epsilon I_f} \\alpha_{\\tau}}{\\sqrt{\\tau}}\\)\n\nwhere \\(\\alpha_{\\tau}\\) represents the contribution of Gaussian point pi to pixel r. A Gaussian point pi can be\nretained as long as its contribution to any pixel point during a pruning interval exceeds the threshold;\notherwise, it must be pruned. This strategy allows for the removal of points that make relatively low\ncontributions to rendering the image. In this case, the threshold is set at 0.02, and pruning begins\nafter 500 rounds, with intervals of 200 rounds each.\n\nIt is worth noting that this pruning technique must be staggered with transparency reset, as it will\ncause all Gaussian scores h(pi) to drop below the cutoff value."}, {"title": "A.2 Proof", "content": "The functional relationship f\u3150(\u00b7) between the learnable parameters E' and the HDR image pixel\nvalues E will be determined here. If no time scaling is performed, then E satisfies the following\nequation:\n\n\\(C_1 = g_1 (\\text{ln} E + \\text{ln} t)\\)\n\nIf time scaling is performed, then E' satisfies the following equation:\n\n\\(C_2 = g_2(r\\cdot \\text{ln} t + s + E')\\)\n\nWhere 91 and g2 represent different CRF functions, both of which are monotonic and differentiable.\nSince the same LDR image is used for supervision regardless of whether time scaling is performed or\nnot, C\u2081 = C2. Therefore, E' and E satisfy:\n\n\\(g_1(\\text{ln} t + \\text{ln} E) = g_2(r \\cdot \\text{ln} t + f (\\text{ln} E)) \\ \\text{where} \\ f(\\text{ln} E) = E' + s\\)\n\nFixing E, differentiate both sides w.r.t. Int, we obtain:\n\n\\(g_1' = g_2'.r\\)\n\nFixing t, differentiate both sides w.r.t. In E, we obtain:\n\n\\(g_1' = g_2' f'\\)\n\nFrom the eq 24 25, we can infer that f' = r, which implies:\n\n\\(\\frac{d(E' + s)}{d(\\text{ln} E)} = r\\)\n\nIf the unit exposure loss is adopted, then there exists only a scaling relationship between g1 and g2,\nwithout any offset. Therefore, we can easily derive the final equation:\n\n\\(E=e^{\\frac{(E'+s)}{r}}\\)\n\nFrom Eq.27, we can render HDR images after the model training is completed."}, {"title": "A.3 Discussion", "content": "During our experiments, we observed instances where the tone mapper of HDRNeRF [16]\noccasionally overfits to LDR, leading to an inaccurate HDR radiance field. As depicted in Fig.\n6, it's evident that directly utilizing MLP in HDRNeRF to model the tone mapper is unstable.\nEmploying three separate MLPs in HDRNeRF [16] to handle the RGB channels independently might\ninduce crosstalk problems, where interference between channels could potentially lead to being\ntrapped in local optimal solutions. While HDRNeRF [16] learns the correct LDR, errors arise in\nHDR radiance field construction. Furthermore, in [17, 49], the authors mention that constructing a\ntone mapping module based on MLPs may fail to correctly decouple nonlinear components."}, {"title": "A.4 Additional Results", "content": "In this section, we present more experimental results. Table. 5 shows the HDR measurement results\ncompared to HDRNeRF on the synthetic dataset. Tab3 and Tab 4 demonstrate the LDR measurement\nresults of our method compared to other baseline methods across various scenes. In Fig7, we present\nLDR images of our various ablation methods (with/without coarse stage, with/without unit exposure\nloss, with/without t scaling, asymmetric or symmetric grid ) and other methods under the same\nviewpoint but different exposure time. In the ablation experiments of the coarse stage, we can clearly\nsee that without the coarse stage, the asymmetric grid would easily overfit to the exposure time used\nin the training dataset. In Fig10, we show the distribution of irradiance E' for each scene in the\nfirst frame of the test dataset. It can be observed that the radiance distribution of most scenes is\nhighly uneven, especially scenes like \"diningroom\", which easily lead the grid into local optimization\nproblems. In Fig.9, we present additional results, where (e) and (f) respectively show HDR images\nrendered by HDR-NeRF and our method. The upper right triangles of these images are the rendered\nHDR images, while the lower left triangles are error maps drawn by HDR-VDP."}]}