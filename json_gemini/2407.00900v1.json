{"title": "MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human Curricula", "authors": ["Shubhra Mishra", "Gabriel Poesia", "Belinda Mo", "Noah D. Goodman"], "abstract": "Mathematical problem solving is an important skill for Large Language Models (LLMs), both as an important capability and a proxy for a range of reasoning abilities. Existing benchmarks probe a diverse set of skills, but they yield aggregate accuracy metrics, obscuring specific abilities or weaknesses. Furthermore, they are difficult to extend with new problems, risking data contamination over time. To address these challenges, we propose MathCAMPS: a method to synthesize high-quality mathematical problems at scale, grounded on 44 fine-grained \u201cstandards\" from the Mathematics Common Core (CC) Standard for K-8 grades. We encode each standard in a formal grammar, allowing us to sample diverse symbolic problems and their answers. We then use LLMs to realize the symbolic problems into word problems. We propose a cycle-consistency method for validating problem faithfulness. Finally, we derive follow-up questions from symbolic structures and convert them into follow-up word problems\u2014a novel task of mathematical dialogue that probes for robustness in understanding. Experiments on 23 LLMs show surprising failures even in the strongest models (in particular when asked simple follow-up questions). Moreover, we evaluate training checkpoints of Pythia 12B on MathCAMPS, allowing us to analyze when particular mathematical skills develop during its training. Our framework enables the community to reproduce and extend our pipeline for a fraction of the typical cost of building new high-quality datasets.", "sections": [{"title": "1 Introduction", "content": "As Large Language Models (LLMs) become increasingly capable, mathematical reasoning problems have emerged as a key benchmark for evaluating their abilities. Mathematical reasoning is a critical subproblem of many important tasks, such as scientific question answering and quantitative data analysis, making it a prerequisite for a range of downstream applications. Moreover, mathematical reasoning tests a broad spectrum of reasoning skills, serving as a valuable proxy for assessing reasoning capabilities more generally. Consequently, several benchmarks, notably GSM8K [9] and MATH [14], became popular measures of the progress of LLMs, with each new generation of models demonstrating rapid advancements.\nHowever, the classical approach to benchmarking in Machine Learning, which involves evaluating models on a fixed set of human-created problems, faces new fundamental challenges in the era of LLMs. First, these models are trained on massive public datasets that may unintentionally include the very benchmarks used for evaluation, raising concerns about data contamination [4, 8, 26]. This problem is exacerbated by the lack of access to the training data of most state-of-the-art LLMs, such as GPT-4 [1], Claude [2], and even open-weight models, such as LLaMA [25]. Evaluating LLMs on novel problems could mitigate the data contamination concerns. But creating new mathematical problems is challenging. Crafting new high-quality problems requires expertise and is expensive;"}, {"title": "2 Related Work", "content": "Our work closely relates to (i) current benchmarks of mathematical reasoning in LLMs, (ii) bench-marks constructed using LLMs, and (iii) behavioral testing and applications in NLP.\nBenchmarks of mathematical reasoning MATH [14] and GSM8K [9] have been two leading benchmarks for the evaluation of mathematical reasoning in LLMs. Both datasets consist entirely of human-authored problems a process that is expensive to reproduce, and as a result, neither benchmarks were updated since their initial releases. Given that LLMs are trained on Web data, it is unclear whether they might have been trained on the test problems of these benchmarks [8] \u2013 either directly or from other sources (e.g., all problems in MATH come from past public competitions). In fact, GSM1K [26], a new dataset that independently attempted to reproduce the data distribution of GSM8K, has found reduced performance on several models, suggesting test set contamination.\nLLM-generated synthetic datasets for LLMS As collecting data from human annotaotors at scale is expensive (especially in domains requiring expertise, such as mathematics), prior work has relied on LLMs to aid the generation of large-scale benchmarks [12]. BigToM [10], a benchmark of social reasoning in LLMs, applied the idea of symbolically scaffolding questions for the LLM to realize in natural language, an approach that we transport to mathematics. Dyval [27] proposed a method for generating reasoning problems for LLMs based on a DAG representing the computation. While Dyval contains two mathematical tasks (arithmetic and solving linear equations), MathCAMPS takes this idea further for mathematical reasoning, spanning 44 skills directly grounded on a human curriculum.\nBehavioral testing in NLP Our goal to provide a fine-grained evaluation of mathematical reasoning has parallels with behavioral testing the idea of testing software systems on specific features, as opposed to just their overall adequacy [20]. In particular, CheckList [20] allowed testing machine translation models for fine-grained failure modes. Dynaboard [18] proposed an NLP leaderboard where users can adapt to their own needs by choosing the utility of different metrics; our dataset enables a similar user-customizable comparison between models for mathematical reasoning."}, {"title": "3 MathCAMPS", "content": "We now describe our pipeline for automatically generating mathematical problems and follow-up questions that are grounded in a human curriculum - the Mathematics Common Core (https:"}, {"title": "3.1 The Mathematics Common Core", "content": "//www.thecorestandards.org). Figure 1 overviews our pipeline. We describe the Common Core, how we represent its standards in a grammar, sample symbolic problems, generate follow-ups, realize those in natural language, and finally improve quality by checking for cycle consistency.\nTo ground problems in a human curriculum, we turn to the Common Core State Standards for Mathematics. 41 states in the United States adopt the CC as their curriculum. The CC details the mathematical content that students should master from Kindergarten up to 12th grade. Within each grade, the CC elaborates a series of individual standards, which detail a particular mathematical skill that students should learn at that grade. Each standard has an identifier, such as K.CC.C.7, and a summary description - for K.CC.C. 7, this is \u201cCompare two numbers between 1 and 10 presented as written numerals\u201d. Here, K indicates that this is a standard for the Kindergarten grade level, whereas 8.EE.C.8 \u2014 \u201cAnalyze and solve pairs of simultaneous linear equations\u201d \u2014 is an 8th grade standard.\nWe take 44 standards spanning grades K through 8 to compose MathCAMPS, focusing on standards that are amenable to automatic problem generation with a final answer in text form. The complete CC curriculum has 229 standards across grades K through 8, bring our coverage to 19.2% of the curriculum for these grades. Notably, we currently do not cover standards focusing on conceptual understanding (e.g., 3.0A.D.9 \u2013 \u201cIdentify arithmetic patterns [...], and explain them using properties of operations.", "Represent three-dimensional figures using nets made up of rectangles and triangles, and use the nets to find the surface area of these figures.\"). All 44 standards covered in MathCAMPS are listed in Appendix A.\nRepresenting Common Core standards We represent CC standards as non-terminals in an at-tribute grammar [13": "a rich formalism that can encode semantic, context-sensitive rules. Attribute grammars can encode syntax much like a context-free grammar, but also allow us to embed informa-tion processing (e.g., setting and testing conditions on attributes, such as bounds on constants) in the production rules. We map each standard s to a non-terminal Ps, such that all strings produced by expanding Ps using production rules are valid symbolic representations of a problem pertaining to standard i. Figure 1 shows a (simplified) grammar for the standard 1.0A.A.1 \u2013 \u201cUse addition and subtraction within 20 to solve word problems involving situations of adding to, taking from, putting together", "Python": "each non-terminal becomes a stochastic function that samples and applies a production rule, recursively expanding non-terminals that it produces. In the grammar in Figure 1 (A), sampling a Problem generates a structure such as the one shown in Figure 1 (B).\nEnforcing problem constraints When sampling problems, there is no a priori guarantee that all generated statements are necessary to answer the question. To avoid such statements, we remove them by applying a simple graph reachability algorithm on a dependency graph between statements, removing statements that the answer does not depend on. This enforces the constraint of only having useful statements in problems. Besides this constraint, which we always enforce, each standard can apply specific constraints. The standard 1.0A. A. 1 has an example of such constraint: it requires that students only be asked to use \"addition and subtraction within 20.\" To be faithful to this standard, we must validate that no intermediate values used in the solution exceed 20. To encode this and other constraints across the curriculum, we implement a suite of 6 parameterized filters (detailed in Appendix C) that are selectively applied depending on the standard's specification. Applying rejection sampling from the grammar using the standard's filters gives a procedure for generating valid symbolic problems. For all standards that can be formulated as solving a system of linear equations, we use SymPy [19] to obtain final answers. For other cases, we use two simple custom procedures (to list the factors of numbers and to compare values)."}, {"title": "3.2 From symbolic to word problems", "content": "To realize the symbolic problems into natural language, we use few-shot prompting with GPT-4 (Figure 1 (C)). For each standard, we sampled two valid symbolic problems and manually wrote a"}, {"title": "3.3 Generating follow-up questions", "content": "As human instructors know, follow-up questions are often the best way to probe a student's under-standing. In MathCAMPS, we leverage our symbolic representation of problems to derive follow-up questions. We propose two kinds of questions: counterfactual questions, where we change a constant in the original problem, and incremental questions, where we add a new piece of information. For each CC standard, we mark which (if any) of these two categories of follow-ups are applicable. Sym-bolically, follow-up questions are represented as a difference to be applied to the original question when we apply the difference, we obtain a new problem. We then use the same solver as the original problem to obtain the ground-truth answer to the follow-up question. We employ the same few-shot structure to translate this difference into a natural language question, and parse it back into a symbolic structure to test for cycle consistency."}, {"title": "4 Experiments", "content": "We now evaluate a suite of 23 LLMs from 8 different vendors on MathCAMPS. We evaluate all models by sampling with temperature 0, using a fixed 1-shot prompt with the first example from GSM8K, mostly to demonstrate the format. For all models (most of them instruction-tuned), a single example was enough for to adhere to the task and the format we specify. The rich structure in MathCAMPS allows us to perform a number of unique analyses on LLMs relating to specific mathematical abilities and their corresponding grade levels for human students. Precisely, we investigate: (1)How do LLMs perform overall on MathCAMPS? How does their performance correlate with GSM8k? (2) Do individual models have relative strengths and weaknesses, or does performance improve uniformly across skills? (3) How well do LLMs respond to follow-up questions? How is their accuracy affected when also considering follow-ups?"}, {"title": "4.1 Overall performance", "content": "Table 1 shows both aggregate accuracy on MathCAMPS, as well as accuracy across standards partitioned by grade, whereas Figure 3 compares the aggregate accuracies on MathCAMPS and GSM8K. Closed-weights models are shown above the line, with open-weights models below. GPT-40 ranks at the top in overall accuracy. Since we used GPT-4 to generate the problems, we must rule out familiarity bias [22] in this result. We thus generated a 10%-scale dataset with the same pipeline but using Claude-3 Opus. We found that GPT-40 still outperforms Claude-3 Opus on this dataset (see"}, {"title": "4.2 Follow-up tasks", "content": "We now evaluate the performance of language models when asked follow-up questions. Here, we first give the initial problem, and in case the model answers correctly we ask either an incremental follow-up, a counterfactual follow-up, or both (in separate contexts), depending on the standard (some standards don't have follow-ups, and for some problems we failed to find a cycle-consistent follow-up within the max attempts). Here, we're interested in analyzing the (lack of) robustness that LMs might have when probed with extra questions our follow-ups are generally answerable using the same core mathematical knowledge involved in the initial problem but require longer range attention and dialog understanding."}, {"title": "4.3 Learning dynamics", "content": "Finally, we use Pythia [6] to showcase another analysis that MathCAMPS enables: understanding the learning dynamics of mathematical skills during LM training. We evaluate checkpoints of Pythia 12B on all standards, and track the performance change as the model was trained. Figure 2 shows Pythia's performance evolving during training on all 7 CC standards where the last checkpoint achieves at least 30% accuracy. Early in training, after 28k steps, Pythia performs best in a Kindergarten standard, K.OA.A.5 - \"Fluently add and subtract within 5.\". At 57k steps, its performance is best in both K.OA.A.5 (37% accuracy) and two first-grade standards, 1.OA.A.1 and 1.OA.A.2 - both standards involve simple word problems with addition and subtraction within 20. Pythia starts to become proficient at a sixth-grade standard around midway during training: 6.EE.A.1, which involves evaluating simple expressions using whole-number exponents (e.g, computing squares and cubes). These skills develop in tandem with its linguistic competence at first, Pythia repeats questions verbatim often, but at 57k steps it already often produces responses. Overall, the high-resolution of MathCAMPS as a reasoning benchmark can support future work to deepen our understanding of how language models acquire capabilities during training, and how specific factors (such as data, or scale) contribute to their learning."}, {"title": "5 Conclusion", "content": "We introduce MathCAMPS, a fine-grained synthetic benchmark of mathematical reasoning in LLMs. MathCAMPS is directly grounded on the Common Core Standards, a widely used curriculum in human education. By tying our problems to a human curriculum, we enable a much wider range"}, {"title": "A Common Core Standards in MathCAMPS", "content": "MathCAMPS is available on Github at https://github.com/gpoesia/mathcamps. All of the Common Core standards we implement are described in a configuration file, commoncore.yaml, where standards are instantiated by composing high-level components from the Common Core attribute grammar. Moreover, we provide our prompts used to generate the dataset and model responses, as well as all problems and model responses for all LLMs we evaluated.\nWe list the Common Core standards we represent in MathCAMPS in Tables 4 through 12, segregated by grade. Standards 3.MD.D.8, 4.MD.A.2, 7.NS.A.1, and 7.NS.A.3 are split up into sub-standards. This was done for ease of implementation of the grammar."}, {"title": "B Familiarity bias", "content": "MathCAMPS was generated using GPT-4. GPT-40, a model of the same family, was also the best performer overall (Table 1). To test whether this might be due to a familiarity bias problems being in-distribution for GPT-40, but out-of-distribution for other models \u2014, we generated a 10%-scale dataset using the exact same pipeline, but using Claude 3 Opus for both generating word problems and testing cycle consistency. This dataset has the same distribution of standards as MathCAMPS. We evaluated GPT-40 and Claude 3 Opus on this dataset - accuracies are reported in Table 13. GPT-40 also performs better in this dataset, suggesting that its performance in MathCAMPS was not due to a higher relative familiarity with the problems."}, {"title": "CData generation pipeline details", "content": ""}, {"title": "C.1 Grammar", "content": "We implemented a global attribute grammar in Python, where production rules are implemented as recursive Python functions. Effectively, each CC standard has its own grammar, composed of pieces from components from the global CC grammar, as well as possibly adding unique non-terminals. Each CC standard contains the following parameters:\nDescription: The description of the CC standard.\nShort description: A shortened description of the CC standard.\nFilters: A list of problem filters to ensure that all problems in this standard satisfy some requirement given in the Common Core description of the standard. The ProblemLength filter makes sure that the problem is within the desired length. CheckIntermediateValues filters out any problems with intermediate values greater or lesser than max_value or min_value, respectively. The ChainsOfVariables filter eliminates any problems where variables are assigned to equal exactly another variable, and nothing else. The ContainsTen filter checks if the math word problem contains numbers adding up to 10, or contains a 10 in the problem (for standards K.OA.A.4 and K.NBT.A.1, respectively).\nTransforms: List of problem transformations applied to all symbolic structures from this standard. The NoUselessVariables transform performs dead code elimination it removes any variables that do not contribute to the final answer by applying a simple graph reachability algorithm on a dependency graph between statements, removing statements that the answer does not depend on. The Simplify transform essentially inlines variables that are used only once.\nExpressions: Lists non-terminals available to generate expressions in symbolic structures for this standard. For example, this can make specific binary operations (e.g. addition, division) available on that particular standard.\nMin/max value: Specifies bounds on values for both the final answer and all intermediate values in the solution.\nMin/max number: Specifies bounds on numeric constants sampled in the symbolic structure.\nMax depth: Sets a maximum depth for expressions in the symbolic structure.\nSamples: We include 2+ hand-written, standard-relevant examples of a symbolic problem followed by a relevant natural language problem generation, which we use as few-shot prompts during problem generation. We also use these prompts, but in reverse (natural language followed by symbolic problem), when we prompt GPT-4 during cycle consistency."}, {"title": "C.2 Answer Grading During Evaluation", "content": "Given a solution in natural language, we first use a rule-based answer extractor to extract any model's numerical answer. In cases where a language model doesn't answer in the required format, or answers in an unexpected format, the answer is initially marked as incorrect. For all problems with incorrect answers, we use Llama-3 70B to re-extract the final answer. We few-shot prompt it with hand-generated examples of solutions and extracted final answers, and ask it to extract the final answer from the new solution. If a problem that was previously incorrect is marked as correct (given the newly extracted answer), we rerun the model on any followups the problem might have. Note that this \"regrading\" step can only improve accuracy from the base result, since we only run it on problems that failed under the rule-based evaluation. In practice, we found this process to have negligible false-positive rate only in a handful of cases across all models we observed either answer extraction processes extracting the correct answer out of a wrong response (e.g., if the answer to a problem is 2, and the model responds \"On day 2, Sally bought 9 dolls\", the rule-based parser extracts 2 as being the model's answer, though the sentence implies its answer to be 9). On the other hand, the LLaMA-3 70B extractor greatly reduces our false negative rate in a handful of models (especially DeepSeek) which are more likely to respond in a format different from what our prompt asks for."}, {"title": "C.3 Cost estimate", "content": "All problems in MathCAMPS were generated using OpenAI gpt-4-0613, in May 2024. We estimate an approximate cost of 330 USD to generate 9607 problems (including main problems and follow-ups). This includes the cost to perform cycle consistency, and problems that are discarded by cycle consistency. This gives an average cost of 0.034 USD (3.4 cents) per cycle-consistent problem or follow-up question."}, {"title": "D Correlation between MathCAMPS and GSM8k", "content": "Figure 3 shows accuracies of several models on both GSM8k and MathCAMPS, along with the line of best fit. There is a strong correlation between overall accuracy in both datasets (p = 0.91, p< 10\u22126), though MathCAMPS allows for many fine-grained analysis besides overall performance."}, {"title": "E Largest Model Rank Changes When Focusing on One CC Standard (Complete Table)", "content": "Table 14 shows the full table from which Table 2 was extracted."}, {"title": "F Followup Analysis", "content": "Table 15 lists model accuracies when only looking at the main problems (Main Acc.), their accuracies when only looking at the incremental followups (IFUP Acc.), their accuracies when only looking at the counterfactual followups (CFUP Acc.), and finally, the total number of followups seen by each model. The total number of followups a model sees relies on whether or not they get the main question for that followup correct. If a model does not correctly solve the main question, it is not prompted with follow-ups. Note that each followup serves as a followup to the main question, as opposed to a followup to each other."}]}