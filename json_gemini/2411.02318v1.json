{"title": "Evaluating the Ability of Large Language Models to Generate Verifiable Specifications in VeriFast", "authors": ["Marilyn Rego", "Wen Fan", "Xin Hu", "Sanya Dod", "Zhaorui Ni", "Danning Xie", "Jenna DiVincenzo", "Lin Tan"], "abstract": "Static verification is a powerful method for enhancing software quality, but it demands significant human labor and resources. This is particularly true of static verifiers that reason about heap manipulating programs using an ownership logic. LLMs have shown promise in a number of software engineering activities, including code generation, test generation, proof generation for theorem provers, and specification generation for static verifiers. However, prior work has not explored how well LLMs can perform specification generation for specifications based in an ownership logic, such as separation logic.\nTo address this gap, this paper explores the effectiveness of large language models (LLMs), specifically OpenAI's GPT models, in generating fully correct specifications based on separation logic for static verification of human-written programs in VeriFast. Our first experiment employed traditional prompt engineering and the second used Chain-of-Thought (CoT) Prompting to identify and address common errors generated across the GPT models. The results indicate that GPT models can successfully generate specifications for verifying heap manipulating code with VeriFast. Furthermore, while CoT prompting significantly reduces syntax errors generated by the GPT models, it does not greatly improve verification error rates compared to prompt engineering.", "sections": [{"title": "1 Introduction", "content": "Auto-active (Hoare-logic styled [7], static) verifiers, such as Viper [17], Verus [11], Dafny [12], Gillian [5], and VeriFast [8], are powerful as they can prove the absence of large classes of bugs in code. Ideally, users of such tools need only specify the intended behavior of their code on the code itself (as pre- and postconditions), and the tool will automatically provide feedback on whether or not the code is provably correct with respect to this behavior. In reality, auto-active verifiers require many more auxiliary specifications (such as loop invariants, lemmas, folds, unfolds, etc.) to achieve this goal, burdening their users.\nIn recent years, large language models (LLMs) have been effective in generating code [2, 21], test-cases [3, 13, 18, 22, 23, 25], and proofs in theorem provers (proof assistants) [4, 9, 24, 26, 28]. LLMs have also been shown to be effective for generating specifications supported by auto-active verifiers [6, 10, 14-16]. However, related work has not explored whether or not off-the-shelf LLMs can generate specifications based on a permissions logic, such as separation logic [19], that can be verified by auto-active verifiers such as VeriFast, Gillian, and Viper. Thanks to such specifications, these verifiers do well at verifying programs that manipulate the heap for both memory safety and functional properties. But, permissions logic based specifications (auxiliary and non-auxiliary) are particularly cumbersome to write, because they must specify the shape of the heap alongside functional constraints. This leads to specifications containing a number of predicates that hide heap details; and as a result, numerous lemmas, folds, unfolds, and special loop invariants that are used to connect the content of these predicates. While such specifications are difficult to reason about, they are written in a patterned way that may be amenable to generation via LLMs."}, {"title": "2 Approach", "content": ""}, {"title": "2.1 Categorization of VeriFast Benchmarks", "content": "We did a comprehensive categorization of the publicly available benchmark suite of VeriFast on GitHub, comprising of over 150 examples. Each example was individually analyzed to identify what type of code is being verified. We looked for programs containing recursion, heap manipulation, and concurrency. The categorization revealed that 78 examples contained recursion, 98 examples used concurrency, and 103 examples manipulated the heap in some way. We also found that the recursive examples often contained recursive predicates and any examples manipulating the heap contained separation logic specifications as expected. We are in the process of collecting more fine-grained data on the types of specifications used in each."}, {"title": "2.2 Generating Input-Output Pairs", "content": "To train and test the models, we created input-output pairs with the input specifying only intended behavior of the code on the code itself and the output as the fully statically specified version of the code that is verifiable in VeriFast. We developed three formats for the input-output pairs to represent diverse user inputs: Natural Language (NL) Specification, Mathematical Proof (MP) Format and Weaker Version (WV) Format.\nIn the NL Specification format, each function was preceded by a natural language description of its behavior in a commented paragraph, testing the LLMs' ability to comprehend human language and integrate it with code. The MP Format included only preconditions and postconditions (from the fully specified benchmark), with the LLM tasked to generate partial specifications including recursive predicates, loop invariants, and lemmas. The goal was to evaluate whether providing preconditions and postconditions could enhance the LLMs' performance. The WV Format defines functional behavior via preconditions and postconditions to test if LLMs can convert partial contracts into complete ones with auxiliary specifications. The three formats assess LLMs' ability to generate precise software verification specifications."}, {"title": "3 Experiment 1 - Prompt Engineering", "content": "We evaluated OpenAI's GPT models (3.5-turbo, 4.0, and 4-turbo) for generating software verification specifications due to their strong text comprehension and generation capabilities [1]. We used prompt engineering, where the input is provided with a specific prompt, and the LLM generates a fully specified C file [27]. The prompt used was: \"You are an expert VeriFast programmer. For the C code below, modify it to include a formal code verification in VeriFast that verifies correctly.\" The models were tested across three input formats: NL specification, MP format, and WV format and their outputs were verified by VeriFast. The percentage of errors generated by each model are summarized . Results show GPT-4.0 outperformed GPT-4-turbo and GPT-3.5-turbo, with average error rates of 28.56%, 33.33%, and 32.53%, respectively. NL input had the highest error rates, producing programs with syntax errors and incorrect or missing contracts. The MP input reduced syntax errors but had incorrect specifications, such as incorrect predicate declarations and missing open and close statements for recursive predicates. The WV format performed slightly better than the MP format but still had many incorrect specifications."}, {"title": "4 Experiment 2 - Chain-of-Thought (CoT) Prompting", "content": "The prompt engineering results show that compared to MP and WV format, results in NL format give a much higher error rate. The average error rates for NL inputs were 45.23% for GPT-3.5-turbo, 40.47% for GPT-40, and 47.62% for GPT-4-turbo. These high error rates require improvement since NL input is the most user-friendly format and eliminates the need to know about VeriFast. Thus, to reduce NL input errors, CoT prompting is used, which is a technique for prompting LLMs in a way to facilitate step-by-step reasoning processes and potentially improve the results [20].The approach employed was given an input file, for each function in the C code, extract the descriptions and process them through a series of prompts identifying key aspects, preconditions, postconditions, loop invariants, common predicates, and then generate VeriFast specific specifications. The average prompt chaining error rate was 42.85%, which was not significantly better than that of prompt engineering. This approach often struggled as ChatGPT would usually generate code that was not ideal for the next prompts to chain properly. A standardized procedure proved futile because"}, {"title": "5 Conclusion", "content": "This study established that these models can interpret natural language requirements, though their effectiveness is not uniform and many times unreliable. The principal obstacles to this area are reliance on precise input and problems with fixing incomplete specifications.\nFuture research should focus on training custom LLMs and exploring alternative models to enhance natural language interpretation. Refining prompt engineering processes is also crucial. Investigating open-source LLMs as alternatives to technologies like GPT, and developing diverse prompt strategies, are key steps forward. Additionally, conducting thorough qualitative evaluations and examining the effects of different specification languages supported by frameworks such as Viper, and Gillian will provide deeper insights and improve the reliability and accuracy of software verification processes."}]}