{"title": "TOWARDS LIGHTWEIGHT AND STABLE ZERO-SHOT TTS WITH SELF-DISTILLED\nREPRESENTATION DISENTANGLEMENT", "authors": ["Qianniu Chen", "Xiaoyang Hao", "Bowen Li", "Yue Liu", "Li Lu"], "abstract": "Zero-shot Text-To-Speech (TTS) synthesis shows great promise for\npersonalized voice customization through voice cloning. However,\ncurrent methods for achieving zero-shot TTS heavily rely on large\nmodel scales and extensive training datasets to ensure satisfactory\nperformance and generalizability across various speakers. This\nraises concerns regarding both deployment costs and data security.\nIn this paper, we present a lightweight and stable zero-shot TTS sys-\ntem. We introduce a novel TTS architecture designed to effectively\nmodel linguistic content and various speaker attributes from source\nspeech and prompt speech, respectively. Furthermore, we present\na two-stage self-distillation framework that constructs parallel data\npairs for effectively disentangling linguistic content and speakers\nfrom the perspective of training data. Extensive experiments show\nthat our system exhibits excellent performance and superior stability\non the zero-shot TTS tasks. Moreover, it shows markedly superior\ncomputational efficiency, with RTFs of 0.13 and 0.012 on the CPU\nand GPU, respectively.", "sections": [{"title": "1. INTRODUCTION", "content": "The recent advancements in Text-To-Speech (TTS) synthesis have\ncatalyzed an increasing demand for personalized voice customiza-\ntion, especially in speaker imitation using voice cloning techniques.\nHowever, traditional TTS approaches typically require a large\namount of high-quality speaker speech data as supervision signals\nto adapt to new speakers [1,2]. Such approaches not only demand\nsubstantial resources to fine-tune and maintain a unique TTS model\nfor each user, but also require users to record substantial amounts of\nspeech data. To alleviate this challenge, recent research [3-18] has\nexplored zero-shot voice cloning with multi-speaker TTS systems,\nappearing to provide a promising solution.\nZero-shot TTS provides a highly generalised model that is capa-\nble of synthesising speech that resemble a new speaker with only a\nbrief prompt speech (typically a few seconds). This approach elim-\ninates the need for additional model finetuning. Recent zero-shot\nTTS studies can be broadly categorized into two types: those lever-\naging speaker representation models [3-8] and those employing in-\ncontext learning strategies [14-18]. Speaker representation-based\napproaches enable explicitly modeling a target speaker's represen-\ntation from any given speech input. This is achieved either through\npre-trained speaker encoders [5, 19-21] or through speaker encoders\njointly trained with the TTS model [6, 8-11]. These speaker repre-\nsentations are then utilized as global [4,5] or temporal [8, 12] condi-\ntional encodings within the audio generation. By thoroughly learn-\ning the speaker characteristics of numerous speakers in the training\ndataset, the model can map the speaker latent space to the corre-\nsponding speech properties, thus attaining zero-shot capability. Re-\ncent studies leverage superior in-context learning capabilities of dif-\nfusion models [7, 13] or Large Language Models (LLMs) [14-18] to\nlearn speaker-related characteristics from the prompt speech. These\napproaches demonstrate the efficacy of leveraging contextually rich\ninformation to achieve speech synthesis across unseen speakers.\nDespite significant progress in recent zero-shot TTS, these\nmethods still face considerable challenges in practical applications,\ni.e., high resource dependence and suboptimal synthesis stability.\nFirst, current zero-shot TTS systems rely on large-scale parameters\nto model the inherent diversity of human voices from a massive\namount of data. For example, Cosy Voice [16] employs a 414M\nmodel and requires 171K hours of training data. Such an enormous\nmodel hinders their integration into resource-constrained environ-\nments. It not only increases service costs, but also raises privacy\nand security concerns, as users' speech prompts still need to be\nuploaded to cloud servers. Second, while autoregressive speech\nmodeling methods such as Tacotron [22] and VALL-E [14] improve\nin-contextual modeling and enhance speech expressiveness, they\nalso increase vulnerability to time series prediction errors such as\nomissions, incorrect readings, repetitions, and even severe error\naccumulation, leading to suboptimal stability.\nIn this paper, we present a lightweight and stable zero-shot TTS\nsystem with self-distilled representation disentanglement. On the\none hand, we propose a TTS architecture that effectively models the\nprompt speech into multi-level speaker representations, including\nglobal timbre features and temporal style features (i.e., features that\nchange over time, including speed, pitch, and energy). By explic-\nitly leveraging the multi-level speaker representations, the model can\noperate effectively with less data and simpler architectures, as these\nrepresentations capture the essential characteristics of the speakers.\nOn the other hand, we employ a self-distillation framework to en-\nhance the model's ability to disentangle content from speaker char-\nacteristics. We use a pre-trained teacher model to generate parallel\ndata pairs that differ only in speaker characteristics, thereby guiding\nthe training of a student model. This disentangles the linguistic con-\ntent from the speaker-related attributes at the data level, reinforcing\nthe model's ability to synthesize speech conditioned solely on the\nprovided prompt speech. Extensive objective and subjective evalu-\nations demonstrate that our system outperforms baseline models in\ncontent integrity while showing promise in speaker similarity. More-\nover, our framework exhibits the most exceptional computational ef-\nficiency, rendering it well-suited for resource-constrained environ-\nments and real-time applications."}, {"title": "2. ZEO-SHOT TTS MODEL", "content": "In this section, we introduce the model of our system. Figure 1\nshows the model architecture, which comprises two main parts, i.e.,\ncontent extraction and speaker adaptation."}, {"title": "2.1. Content Extraction", "content": "In the first part, we extract speaker-independent content represen-\ntation from both text (phoneme sequence) and the corresponding\nground truth speech (mel-spectrogram). To enhance the expressive\ncapacity for complex speech content, we integrate a Mel Variational\nAutoencoder (VAE) with a flow-based model to construct a latent\nspace of speech, transcending the limitations of text-only represen-\ntations. We first encode both the text and speech at the phoneme\nlevel using a linguistic encoder and a mel encoder, generating repre-\nsentations $e_{ling}$ and $e_{mel}$, respectively. We use $e_{ling}$ as a condition for\npredicting $e_{mel}$ through Volume-Preserving Flow (VP-Flow). After\nthat, we fuse $e_{ling}$ with $e_{mel}$, forming the content representation $e_{con}$,\nwhich is then fed into the speaker adaptivation part. Specifically, the\ntext encoder consists of multiple feed-forward Transformer blocks\nwith relative positional encoding [23]. The mel encoder consists\nof multiple 2D Residential blocks (ResBlock) to downsample and\nencode the input mel-spectrogram, followed by average pooling to\nmap frame-level features to phoneme-level features. Another multi-\nlevel ResNet estimates the mean and variance of the variational la-\ntent space. Note that the source speech contains not only content\ninformation, but also information related to the target speaker. This\ncontent-speaker coupling leads to the leakage of speaker information\nfrom $e_{con}$. We address this issue using a self-distillation framework\nin Section 3."}, {"title": "2.2. Speaker Adaptation", "content": "In the second part, we explicitly model multi-level speaker-specific\nrepresentations from the prompt speech, i.e., temporal style repre-\nsentations and global timbre representations. These representations\nserve as conditions for steering the synthesis of the target speech, en-\nsuring that it accurately reflects the desired speaker characteristics.\nWe adopt a trainable mel encoder to extract the temporally-\nrelated style representation, $e_{sty}$. This encoder is similar to that\nused in HierSpeech++ [10] and GPT-SOVITS [17], but it includes\npositional encoding and omits the final temporal averaging layer to\nbetter capture temporal information. We use $e_{sty}$ as a condition for a\nvariation adapter, which consists of three predictors for independent\nspeech attribute predictors, i.e., duration, pitch, and energy. We inte-\ngrate and align $e_{sty}$ with the input sequence of the predictors through\na multi-head cross-attention layer. These predictors are individually\ntrained under supervised conditions using annotated data. To ensure\nthe model's stability, we separate the gradients of these predictors\nfrom the main network.\nFor timbre extraction, we utilize a speaker recognition model as\nthe timbre encoder, i.e., ERes2NetV2 [24], pre-trained on a large-\nscale multi-domain dataset, Speaker3D [25]. This encoder gener-\nates a comprehensive speaker latent space that encompasses various\nspeakers and speaking conditions, facilitating the effective applica-\ntion of unseen speaker features to achieve precise modeling of their\nvocal styles. Additionally, a trainable linear layer is integrated into\nthis model to yield the speaker representation $e_{spk}$. To transfer the\ntimbre representation $e_{spk}$ extracted from arbitrary speech into syn-\nthesized speech, we employ a speaker-adaptive mel Decoder based\non a multi-level ResNet architecture with Adaptive Instance Normal-\nization (AdaIN) layer."}, {"title": "3. TWO-STAGE SELF-DISTILLATION", "content": "In this section, we propose a self-distillation framework aimed at\ndisentangling linguistic content and speaker characteristics from the\nperspective of data. The basic idea is to construct parallel data pairs\nfeaturing identical content but distinct speakers, utilizing the model's\ninitial zero-shot capability. Figure 2 shows the training framework,\nwhich involves the sequential training of a teacher model and a stu-\ndent model."}, {"title": "3.1. Stage 1: Training Teacher TTS", "content": "We begin by training a teacher model $M_T$, which is expected to ef-\nfectively reconstruct the linguistic content from the given text and\npreliminarily clone the speaker's voice from the prompt speech.\nGiven a single speech sample $x \\in \\mathbb{R}^{T\\times F}$, where $T$ and $F$ rep-\nresent the temporal and frequency dimensions, respectively, we ex-\ntract $e_{mel}$ from $x$ and $e_{ling}$ from the corresponding text $c$. These\ntwo representations are combined to obtain $e_{con}$. Next, we use the\ntimbre encoder and the style encoder to extract $e_{tim}$ and $e_{sty}$ from\nthe prompt speech $x'$. Using $e_{sty}$ as a condition, we predict time-\ndependent speech attributes and apply these to $e_{con}$. Finally, we input\nthe fused feature into the Mel decoder to reconstruct the speech mel\nspectrogram, ensuring that the generated speech matches the target\nspeaker's timbre by conditioning on $e_{tim}$. We employ the following\nloss functions to guide the training process."}, {"title": "3.2. Stage 2: Training Student TTS", "content": "By leveraging the well-trained teacher model, we proceed to gener-\nate parallel data pairs for distilling a student model.\nGiven a speech sample $x$, and the corresponding text, repre-\nsented as a phoneme sequence $c$, our method begins by employing\nthe teacher model to synthesize a new speech sample $\\hat{x}'$. The syn-\nthetic speech sample preserves the textual content $c$, but is delivered\nby a distinct speaker. To this end, we begin by randomly select-\ning a prompt speech sample $x'$ from our training dataset. The sam-\nple $x'$ corresponds to a different speaker $s'$ than the speaker $s$ in $x$.\nThen, we infer the teacher model with $c$ and $x'$ for the generation of\n$\\hat{x}' = M_T(c, x')$. For the strict temporal alignment of the phonemes\nof the $x$ and $\\hat{x}'$, we explicitly specify the prediction result of the du-\nration predictor as the annotated phoneme duration of $x$ during the\ninference process.\nAfter obtaining the time-aligned parallel data pair ${x, \\hat{x}'}$, we\nproceed to train the student model. The student model mirrors the\nsame model structure as the teacher model but is initialized ran-\ndomly. We input $\\hat{x}'$ as the speech content for the student model,\naccompanied by $c$ as the linguistic content input. Additionally, we\nrandomly select another speech sample $x''$ as the prompt, which is\nuttered by the same speaker as $x$. We optimize the student model fol-\nlowing the same procedure as detailed in Section 3.1, using $x$ along\nwith its detailed annotation serving as the ground truth.\nTo regulate the intensity of distillation, we blend real and synthe-\nsized samples within each training batch. To this end, we introduce\na coefficient $\\sigma \\in [0, 1]$, which modulates the proportion of synthetic\nspeech samples. Specifically, in each batch of speech content input,\nwe replace $\\lfloor \\sigma \\cdot B \\rfloor$ randomly selected speech samples with their cor-\nresponding synthesized speech samples, while the remaining sam-\nples remain as the ground truth speech samples.\nThe self-distillation framework ensures that the student model's\ncontent extraction avoids speaker-specific details, compelling it to\nrely solely on prompt speech for speaker adaptation, thereby enhanc-\ning the separation of the representations."}, {"title": "4. EVALUATION", "content": "We train both the teacher and student models on a diverse dataset,\ncomprising 4,678 speakers, totaling 531 hours of audio. The\npreprocessing pipeline involve extracting 80-dimensional mel-\nspectrograms from the audio clips. For audio reconstruction from\nmel-spectrograms, we employ a NFS-HiFiGAN model [27] as\nvocoder. The TTS models are trained using an AdamW optimizer\nand a batch size of 64 for 200,000 training steps. When training the\nstudent model, we set $\\sigma = 0.8$ by default.\nOur proposed model is compared against four state-of-the-art\nzero-shot Text-to-Speech (TTS) models: Vall-E [14], X-TTSv2 [28],\nCosy Voice [16], and GPT-SOVITS [17]. We reproduce Vall-E based\non the published methodology [14] and trained it on a speech cor-\npus of approximately 60K hours and 19K speakers. The other sys-\ntems utilize the open-source code and pre-trained models provided\nby their respective authors.\nPrompt speeches for the test dataset are collected from 20 volun-\nteers (10 males and 10 females) who are not part of the training set,\nensuring that the baseline system's training data does not contain\nthese test cases. For each volunteer, we record 5 audio clips, en-\nsuring high audio quality, clear pronunciation, and varied durations\nranging from 5 to 10 seconds. For each prompt speech, we gener-\nate 100 synthetic speeches using different sentences, with lengths\nranging from approximately 30 to 50 characters."}, {"title": "5. CONCLUSION", "content": "In this paper, we introduce a lightweight and stable zero-shot TTS\nsystem. We first propose a novel TTS architecture designed to ef-\nfectively model linguistic content and various speaker-related at-\ntributes from source speech and prompt speech, respectively. Next,\nwe present a two-stage self-distillation framework that constructs\nparallel data pairs for effectively disentangling linguistic content and\nspeakers from the aspect of training data. Both objective and subjec-\ntive evaluations demonstrate that our system matches the Baseline\nin zero-shot speech synthesis performance while achieving superior\ncomputational efficiency."}], "equations": ["L_{cyc} = - \\frac{1}{B} \\sum_{i=1}^B \\log \\left( \\frac{\\exp(\\cos(\\hat{e}_i, e_i))}{\\sum_{j=1}^B 1_{j \\neq i} \\exp(\\cos(\\hat{e}_i, e_j))} \\right), (\\alpha)"], "loss_functions": ["L_{rec}", "L_{KL}", "L_{adv}", "L_{cyc}", "L_{pred,i}"]}