{"title": "LLM Inference Acceleration via Efficient Operation Fusion", "authors": ["Mahsa Salmani", "Ilya Soloveychik"], "abstract": "The rapid development of the Transformer-based Large Language Models (LLMs) in recent years has been closely linked to their ever-growing and already enormous sizes. Many state-of-the-art language models contain hundreds of billions to trillions of parameters and require dedicated hardware resources for both training and inference. One of the key challenges inherent to the Transformer architecture is the requirement to support numerous non-linear transformations that involves normalization. For instance, each decoder block typically contains at least one Softmax operation and two Layernorms. The computation of the corresponding normalization scaling factors becomes a major bottleneck because it requires spatial collective operations. In other words, when it comes to the computation of denominators for Softmax and Layernorm, all vector elements must be aggregated into a single location, requiring significant communication. These collective operations slow down inference on Transformers by approximately 20%, defeating the whole purpose of distributed in-memory compute. In this work, we propose an extremely efficient technique that can completely hide the overhead caused by such collective operations. Note that each Softmax and Layernorm operation is typically followed by a large linear layer. Since non-linear and linear operations are performed on different hardware engines, they can be easily parallelized once the algebra allows such commutation. By leveraging the inherent properties of linear operations, we can defer the normalization of the preceding Softmax and Layernorm until after the linear layer is computed. Now we can compute the collective scaling factors concurrently with the matrix multiplication and completely hide the latency of the former behind the latter. Due to the algebraic equivalence, such parallelization preserves the numerical accuracy while significantly improving the hardware utilization and reducing the overall latency.", "sections": [{"title": "I. INTRODUCTION", "content": "Transformer-based architectures [1], particularly large lan- guage models (LLMs), have recently gained significant at- tention due to their impressive capabilities and performance across a wide range of tasks. These LLMs, such as GPT series [2] and LLama series [3], have demonstrated exceptional potential across a wide range of practical applications, includ- ing text generation, conversational processing, and knowledge- based question answering. However, as the size of LLMs grows, their hardware and memory requirements increase significantly, limiting their applicability and hindering their deployment in real-world scenarios.\nVarious approaches targeting different aspects of compute optimization have been studied to address the computational and memory issues arising in such LLMs, including model compression techniques [4, 5], knowledge distillation [6], pruning [7, 8], development of lower precision formats [9, 10] for efficient quantization [11, 12], approximations, and other software optimizations [13]. These methods are commonly employed to reduce both the computational demands and latency of inference in Transformers.\nNotably, most of such methods make the compute more efficient at the expense of model performance, as each com- pression technique introduces errors through approximations. Consequently, any approach that can enhance computational efficiency or reduce latency in high-dimensional computations while preserving the accuracy of the underlying system be- comes of paramount importance. These advancements would be crucial in ensuring the practical deployment and scalability of large-scale LLMs, particularly in real-time applications where both speed and accuracy are critical.\nOne of the main challenges in executing Transformer-based LLMs on existing hardware lies in handling numerous non- linear functions that are not necessarily hardware-friendly. In particular, non-linear operations involving normalization, such as Layernorm and Softmax, require computation of the de- nominator, which necessitates data aggregation across multiple processing units. Such collectives sitting on the critical path often take much time to execute [14, 15] and create numerous bottlenecks.\nTo address these challenges, various parallelization tech- niques have been extensively explored in the literature, aiming to ultimately enhance the computational efficiency and reduce the latency caused by these operations during LLM inference [16, 17]. However, the existing parallelization approaches pri- marily target higher-level computations, such as data, model, or even pipeline parallelization.\nIn this work, we introduce a novel approach to enhance computational efficiency and reduce latency specifically at the Transformer-layer level. In particular, we present a latency reduction strategy for computing Layernorm and Softmax in Transformer-based LLMs. Our technique leverages an innova- tive operation fusion strategy that takes advantage of the in- herent structure of the Transformer block architecture and the sequential ordering of layers within each block to streamline"}, {"title": "II. METHODOLOGY", "content": "One of the key challenges associated with the Transformer architecture is the presence of numerous non-linear trans- formations that involve normalization. Such normalization inherent in operations like Softmax and Layernorm is essential for ensuring the correct scaling of activations, preventing them from exploding or shrinking during training. The computation of such normalization scaling factors necessitates spatial col- lective operations. More specifically, the calculation of denom- inators of Softmax and Layernorm requires the aggregation of vector elements into a single location for proper process- ing. These collective operations are so time-consuming and introduce substantial delays, significantly slowing down the inference process. More importantly, such spatial aggregation defeats the entire purpose of distributed or in-memory compute that all hardware accelerators for LLMs are trying to achieve.\nIn this work, we propose an efficient technique that can completely eliminate the overhead caused by such collective operations in the Transformer architectures. The main idea is based on the observation that all the non-linearities at hand are followed by matrix multiplications. Being a linear operation matrix multiplication commutes with scaling, thus allowing the normalization of Softmax or Layernorm to be deferred and performed immediately after the multiplication. Leveraging this commutativity allows us to completely hide the compu- tation of the denominators behind the corresponding matrix multiplications since these are performed on separate hardware components. The former is executed by the Single Instruction Multiple Data (SIMD) unit [18], while the latter is performed on a Digital In-Memory Compute (DIMC) unit [19, 20]. In other words, we fuse the respective non-linear operations with their subsequent matrix multiplications into single operations, thereby accelerating the overall computation.\nFig. 1 compares the conventional architecture and the proposed fused architecture. The non-linear operation with collection in Fig. la denotes either a Layernorm or a Softmax block, which can be decomposed into two sub-operations as discussed above: an element-wise sub-operation and a collective sub-operation, see Fig. 1b. The element-wise sub- operation can be fused by the linear operation, which is the proceeding linear layer. By executing the aggregation in the collective sub-operation concurrently with the matrix multiplication between the element-wise sub-operation and the linear operation, the latency imposed by the collective operations can be effectively hidden, significantly accelerating the overall computation."}, {"title": "A. Fused Layernorm", "content": "Layernorm is one of the key components in modern LLMs [21]. It adaptively scales the input across the features for each data point and thus helps stabilize the learning process. Given the input vector x \u2208 R1\u00d7n, the Layernorm first computes the mean and the variance of the elements as\n\\begin{equation}\n\\bar{x} = \\frac{1}{n} \\Sigma x_i, \\quad \\sigma^2(x) = \\frac{1}{n} \\Sigma (x_i - \\bar{x})^2,\n\\end{equation}\nand after normalization, Layernorm applies trainable scale fac- tors y and bias \u1e9e to allow the model to adjust the normalized values as follows\n\\begin{equation}\ny = \\frac{x - \\bar{x}}{\\sqrt{\\sigma^2 + \\epsilon}} \\odot \\gamma + \\beta.\n\\end{equation}\nIf \u0393 \u2208 Rn\u00d7n denotes the diagonal matrix whose diagonal ele- ments are the components of the vector y, i.e., \u0393 = Diag(\u03b3), and E\u2208 Rnxn denotes a matrix in which all elements are equal to 1, Eq. 2 can be expressed in matrix form as\n\\begin{equation}\ny = \\frac{x - \\bar{x}}{\\sqrt{\\sigma^2 + \\epsilon}} \\odot \\gamma + \\beta =  \\frac{x}{\\sqrt{\\sigma^2 + \\epsilon}} \\odot (I - \\frac{1}{n} E) \\Gamma + \\beta.\n\\end{equation}"}, {"title": "B. Fused Softmax", "content": "Similarly to Layernorm, the Softmax in Transformers is typically followed by a matrix multiplication. If x, y \u2208 R1xn denote the input and output vectors of a Softmax\u00b9, respectively, the output of the following matrix multiplication reads as\n\\begin{equation}\nyV = \\left[\\frac{e^{x_1}}{\\Sigma i}, \\frac{e^{x_2}}{\\Sigma i} ... \\frac{e^{x_n}}{\\Sigma i}\\right] V,\n\\end{equation}\nwhere V \u2208 Rnxm denotes the Values matrix.\nAnalogously to the treatment of Layernorms described above, the fusion methodology applies here as well. We can first compute the exponential numerators, use them for the matrix operation and then apply normalization as illustrated in Fig. 4.\nNote that unlike the Layernorm case, V is now a matrix of activations and not a static weight matrix. Despite that, the fusion algorithm is still perfectly applicable here because the matrix multiplication retains its linear nature.\n\\begin{equation}\nyV = \\frac{1}{\\Sigma i e^{x_i}} ([e^{x_1} e^{x_2} ... e^{x_n}]V).\n\\end{equation}"}, {"title": "III. EXPERIMENTAL RESULTS", "content": "The proposed technique is based on an algebraically equiv- alent reformulation of the involved operations, ensuring that it maintains complete accuracy without any degradation. There- fore, our methodology can be easily applied to any architecture that involves a Layernorm or Softmax followed by a linear operation, in particular it applies to any Transformer-based LLM. While the key advantage of this technique lies in its abil- ity to significantly reduce the latency, its overall effectiveness largely depends on the underlying hardware architecture and\nthe implementation of computations and collective operations. Key factors such as the throughput and capabilities of different computing units handling linear and non-linear operations play a crucial role in determining the performance gain achieved with the proposed method.\nIn order to provide an experimental analysis, we imple- mented the proposed approach on Corsair [23], our recently launched AI accelerator as described in Section II. The ex- perimental results demonstrate that we get a latency reduction of 20% on most state-of-the-art LLMs such as Llama2 and Llama3. These findings highlight the effectiveness of the fusion strategy in optimizing Transformer-based computations, further validating its potential for accelerating Artificial In- telligence (AI) workloads on advanced hardware platforms. Due to space constraints, we defer the rigorous analysis of computational time gains to our next publication."}, {"title": "IV. CONCLUSION", "content": "In this work, we introduced an operation fusion technique designed to enhance the computational efficiency and reduce the latency of Transformer-based large language models. By decomposing and fusing specific non-linear and linear opera- tions, i.e., those that involve normalization, such as Layernorm and Softmax, with their succeeding linear transformations, we achieved mathematically equivalent results without compro- mising model accuracy. Deployment of the proposed approach on modern AI accelerators demonstrates a 20% reduction of inference latency. These findings underscore the potential of operation fusion strategies to address the computational chal- lenges posed by large scale Transformer-based models, paving the way for more efficient and scalable implementations in real-world applications. Future work will explore extending this methodology to other architectural components and further optimizing hardware-software co-design."}]}