{"title": "Low-Latency Task-Oriented Communications with Multi-Round, Multi-Task Deep Learning", "authors": ["Yalin E. Sagduyu", "Tugba Erpek", "Aylin Yener", "Sennur Ulukus"], "abstract": "In this paper, we address task-oriented (or goal-oriented) communications where an encoder at the transmitter learns compressed latent representations of data, which are then transmitted over a wireless channel. At the receiver, a decoder performs a machine learning task, specifically for classifying the received signals. The deep neural networks corresponding to the encoder-decoder pair are jointly trained, taking both channel and data characteristics into account. Our objective is to achieve high accuracy in completing the underlying task while minimizing the number of channel uses determined by the encoder's output size. To this end, we propose a multi-round, multi-task learning (MRMTL) approach for the dynamic update of channel uses in multi-round transmissions. The transmitter incrementally sends an increasing number of encoded samples over the channel based on the feedback from the receiver, and the receiver utilizes the signals from a previous round to enhance the task performance, rather than only considering the latest transmission. This approach employs multi-task learning to jointly optimize accuracy across varying number of channel uses, treating each configuration as a distinct task. By evaluating the confidence of the receiver in task decisions, MRMTL decides on whether to allocate additional channel uses in multiple rounds. We characterize both the accuracy and the delay (total number of channel uses) of MRMTL, demonstrating that it achieves the accuracy close to that of conventional methods requiring large numbers of channel uses, but with reduced delay by incorporating signals from a prior round. We consider the CIFAR-10 dataset, convolutional neural network architectures, and AWGN and Rayleigh channel models for performance evaluation. We show that MRMTL significantly improves the efficiency of task-oriented communications, balancing accuracy and latency effectively.", "sections": [{"title": "I. INTRODUCTION", "content": "In the rapidly advancing field of NextG communication systems, there is an increasing focus on task-oriented (or goal-oriented) communications. This approach is gaining prominence as it addresses the specific needs of various applications by ensuring that the transmission process is aligned with the ultimate objective of the task at hand [1]\u2013[9]. Unlike traditional communication paradigms that focus on delivering raw data, task-oriented communications (TOC) aims to transmit only the information necessary to accomplish a specific task. Deep learning plays a crucial role in optimizing the encoding and decoding processes for TOC, allowing for efficient and effective transmission of information that directly contributes to the task's success. By leveraging deep learning-driven TOC, NextG communication systems can achieve significant improvements in both performance and resource utilization [10]\u2013[12], making them well-suited for the demands of modern applications such as the Internet of Things (IoT), augmented reality/virtual reality (AR/VR), and vehicle-to-everything (V2X) network systems.\nIn IoT networks, sensors generate vast amounts of data that need to be processed and analyzed to make real-time decisions, such as in smart cities and industrial automation. TOC can significantly reduce the communication overhead by transmitting only the essential information required for decision-making, rather than the raw sensor data. Similarly, in AR/VR applications, low latency and high accuracy are critical to delivering immersive experiences. TOC can help achieve this by optimizing the transmission of visual and sensory data to meet the application's specific needs. In V2X systems, vehicles need to communicate with each other and with infrastructure to ensure safe and efficient transportation. TOC can enhance these interactions by focusing on the transmission of critical information, such as collision warnings and traffic updates, thereby improving response times and reducing network congestion.\nOne of the primary challenges in TOC is balancing task accuracy and latency objectives and requirements. To that end, the age of task information for TOC was studied in [13]. Achieving high accuracy often requires transmitting a large amount of data, which can lead to increased delay (measured by the number of channel uses) and higher bandwidth usage. Conversely, minimizing delay and bandwidth usage can compromise accuracy. This accuracy-delay tradeoff is a significant hurdle that needs to be addressed to realize the full potential of TOC.\nWe propose a novel multi-round, multi-task learning (MRMTL) approach to address this challenge by dynamically updating the number of channel uses in iterative transmissions of TOC. MRMTL involves an encoder at the transmitter that learns compressed latent representations of input data (e.g., images), which are transmitted over a wireless channel. At the receiver, a decoder performs a machine learning task, specifically classifying the received signals. MRMTL is different from the autoencoder-based communications, where the"}, {"title": "II. SINGLE-ROUND, SINGLE-TASK LEARNING", "content": "The system model of SRSTL for TOC is shown in Figure 1. The transmitter has input data samples (e.g., images) x. The DNN-based Encoder E\u2081 at the transmitter performs the combined roles of source coding, channel coding and modulation, and outputs the modulated symbols, E\u2081(x), with size nc,1. The transmitted symbols go through the channel effects (over nc,1 channel uses) and become the input to a Semantic Task Classifier at the receiver, as r = h E\u2081(x)+n, where h is the channel gain and n is the noise. The Decoder, D1, corresponding to the Semantic Task Classifier is also a DNN whose output is the predicted labels, \u01771 = D\u2081(r). The task is accomplished successfully when \u01771 = y*, namely the received signals are correctly classified as the true labels y*. The task accuracy increases with nc,1 as demonstrated in [10]\u2013[12]. By considering SRSTL as the baseline, we also highlight this trend for SRSTL in this section. On the other hand, the increase in nc,1 also increases the latency (in terms of number of channel uses) for task completion. To reduce latency, we introduce MRMTL for TOC in Sec. III.\nFor performance evaluation, we consider the CIFAR-10 dataset that consists of 60,000 color images, each with a resolution of 32x32 pixels, divided into 10 distinct classes as the transmitter input. Each class represents a different category, specifically: Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, and Truck. The dataset is split into 50,000 training images and 10,000 test images, with 6,000 images per class. The DNN architectures for the Encoder and Decoder are shown in Table I."}, {"title": "III. MULTI-ROUND, MULTI-TASK LEARNING", "content": "The system model of MRMTL for TOC is shown in Figure 2. Multi-task learning is used to train Encoders E\u2081 and E2, and Decoders D\u2081 and D2 jointly. The loss function for the jointly trained system is defined as l = w l\u2081 + (1 \u2212 w) 12, where l\u2081 is the loss from Decoder 1 in Round 1, 12 is the loss from Decoder 2 in Round 2, and w is the weight of loss 11 during training (w is set to 0.5 for training in numerical results).\nAfter training, in a latency-constrained system, we first start with transmitting a low number of samples as the transmitter output in Round 1. If the Semantic Task Classifier's accuracy is insufficient, then the receiver requests more encoded samples from the transmitter to improve the classification accuracy in Round 2. The earlier transmitted samples are still re-used to keep the latency low when the receiver needs more samples from the transmitter to improve the accuracy. To achieve this, the system is designed to operate in multiple rounds with a built-in feedback mechanism.\nSimilar to SRSTL, the transmitter has input data samples x. In Round 1, the transmitter encodes x with E\u2081 and transmits the encoded samples over nc,1 channel uses. The received signals are r\u2081 = h\u2081 E1(x) + n\u2081, where h\u2081 is the channel gain and n\u2081 is the noise in Round 1. The receiver classifies r1 with Decoder D\u2081 and predicts labels, \u01771 = D1(r1). In Round 2, the transmitter encodes x with E2 and transmits the encoded samples over nc,2 channel uses. The received signals are r2 = h2 E2(x) + n2, where h2 is the channel gain and n2 is the noise in Round 2. In Round 2, the receiver decodes the concatenation of signals received in both Rounds 1 and 2, [r1,r2], with Decoder, D2, and predicts labels, \u01772 = D2([r1, r2]).\nThe DNN architectures of the encoder-decoder pair for multi-task learning are similar to the architectures shown in Table I. Encoder 1 and Encoder 2 provide nc,1 and nc,2 samples, respectively, as the output. Then the dense layer size is nc,1 for Decoder 1 and nc,1 + nc,2 for Decoder 2 at the receiver. We set nc,1 = nc,2 for performance evaluation.\nTask accuracy of TOC with MRMTL is shown in Ta-ble III under AWGN and Rayleigh channels. Task accuracy of MRMTL for TOC is higher for Round 2 (compared to Round 1) and under the AWGN channel (compared to Rayleigh channel). Comparing results from Tables II and III, each round of MRMTL achieves task accuracy close to that of the corresponding SRSTL with the same number of channel uses. Confusion matrices for the first and second rounds of multi-task learning under the AWGN channel are shown in Figs. 3"}, {"title": "IV. LATENCY REDUCTION WITH DYNAMIC MULTI-ROUND, MULTI-TASK LEARNING", "content": "Adding Round 2 in MRMTL improves the task accuracy compared to the use of Round 1 only as in SRSTL. However, it also increases latency from nc,1 to nc,1 + nc,2. Next, we present a dynamic scheme that adaptively selects when to initiate Round 2 (as shown in Algorithm 1). Let yk,j,m denote the kth entry of decoder Dm's final layer output in Round m = 1,2 of TOC for any input sample xj. To decide on a specific class, we select the maximum of decoder's final layer output and map it to the corresponding class type. Specifically, \u0177j,m = arg maxk Yk,j,m denotes the output label of decoder Dm of Round m = 1,2 in TOC for any input sample xj.\nWe define $y^{max}_{j,m}$ = maxk Yk,j,m. Histograms of ymax values under AWGN and Rayleigh channels are shown in Figs. 7 and 8, respectively. We denote ymax as the random variable corresponding to the maximum of decoder's final layer output entries for a given input sample, \u01771 as the label predicted in Round 1, and y* as the respective true label. Overall, distribution of $y^{max}$ differs significantly under cases when correct and incorrect decisions are made in the first round (namely, when \u0177\u2081 = y* and \u01771 \u2260 y*), showing different levels of confidence in decision-making. As ymax increases, the accuracy tends to increase, as well. Therefore, at the end of Round 1, ymax values closer to 1 indicate a more confident decision in predicting label \u0177j,1 for input sample xj. For any input sample xj, the receiver compares ynax with a threshold \u03b4. If $y^{max}$ < d, the receiver sends a feedback to the transmitter to initiate Round 2 transmissions. Otherwise, Round 2 is not initiated and \u01771 = D1(r1) becomes the predicted label. If Round 2 is initiated, \u01772 = D2([r1, r2]) becomes the predicted label. This way, Round 2 is initiated only when the receiver is not confident on the classification in Round 1.\nThe delay and the task accuracy depend on the threshold \u03b4. For a given threshold 8, the delay for input sample x is given by\nDj = nc,1\u00b71 (ymax \u2265 \u0431)+(nc,1 + nc,2)\u00b71 (ymax < d) . (1)\nUsing ymax for a given input sample. Then, the average delay is expressed as\nD = \u043f\u0441,1. \u0420 (\u0443\u0442\u0430\u0445 \u2265 \u0431) + (\u043f\u0441,1 + nc,2)\u00b7 P (ymax < \u0431). (2)\nFor a given input sample, we denote \u0177m as the random variable corresponding to the predicted label in round m = 1,2. Then the average task accuracy (namely, the probability of successful classification) with MRMTL is given by\nPs = P (\u0177\u2081 = y* | ymax > d) \u00b7 P (ymax > \u03b4)\n+ P (\u01772 = y* | ymax < d) \u00b7 P (ymax < \u03b4). (3)\nTask accuracy and delay (total number of channel uses) are shown in Figs. 9 and 10 as a function of the threshold \u03b4 for round selection under AWGN and Rayleigh channels. Overall, task accuracy is higher and delay is lower when AWGN channel is used compared to Rayleigh channel. As the threshold & increases, the likelihood of initiating Round 2 increases such that both the task accuracy and delay increase. Note that the task accuracy saturates as the threshold 8 increases, indicating that a large threshold is not useful to further increase task accuracy but significantly increases the delay. The underlying relationship between the task accuracy and delay is shown in Fig. 11 under AWGN and Rayleigh channels. We observe operational modes, where the threshold \u03b4 can be gradually increased until a given target level of delay is reached. If further feedback is available on the task accuracy performance achieved over time, the threshold d can be also gradually increased until the target value of task accuracy is reached.\nFrom Figs. 7 and 8, the average values of ymax are shown in Table IVa for AWGN and Rayleigh channels, respectively. An effective separation can be obtained when threshold \u03b4* is selected as\n\u03b4* = 1/2 (E[ymax | \u01771 = y*] + E[ymax | \u01771 \u2260 y*]). (4)\nTable IVa lists the threshold 8* for multi-round selection that is also plotted in Figs. 7 and 8 for AWGN and Rayleigh channels, respectively. Table IV highlights the high accuracy and low delay performance of MRMTL when the threshold \u03b4 is selected as d*. Overall, task accuracy is higher under the AWGN channel compared to Rayleigh channel."}, {"title": "V. CONCLUSION", "content": "In this paper, we considered a TOC framework for NextG systems, where an encoder at the transmitter learns compressed latent representations of data and a decoder at the receiver performs classification tasks. By jointly training the encoder-decoder pair with DNNs, we optimized the transmission process to achieve high accuracy with minimal channel uses. Our novel approach, MRMTL, involves dynamically updating channel uses through iterative transmissions over multiple"}]}