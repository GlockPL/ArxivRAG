{"title": "PERMUTATION-BASED MULTI-OBJECTIVE EVOLUTIONARY\nFEATURE SELECTION FOR HIGH-DIMENSIONAL DATA", "authors": ["Raquel Espinosa", "Gracia S\u00e1nchez", "Jos\u00e9 Palma", "Fernando Jim\u00e9nez"], "abstract": "Feature selection is a critical step in the analysis of high-dimensional data, where the number of\nfeatures often vastly exceeds the number of samples. Effective feature selection not only improves\nmodel performance and interpretability but also reduces computational costs and mitigates the risk\nof overfitting. In this context, we propose a novel feature selection method for high-dimensional\ndata, based on the well-known permutation feature importance approach, but extending it to eval-\nuate subsets of attributes rather than individual features. This extension more effectively captures\nhow interactions among features influence model performance. The proposed method employs a\nmulti-objective evolutionary algorithm to search for candidate feature subsets, with the objectives\nof maximizing the degradation in model performance when the selected features are shuffled, and\nminimizing the cardinality of the feature subset. The effectiveness of our method has been validated\non a set of 24 publicly available high-dimensional datasets for classification and regression tasks,\nand compared against 9 well-established feature selection methods designed for high-dimensional\nproblems, including the conventional permutation feature importance method. The results demon-\nstrate the ability of our approach in balancing accuracy and computational efficiency, providing a\npowerful tool for feature selection in complex, high-dimensional datasets.", "sections": [{"title": "1 Introduction", "content": "In the realm of high-dimensional data analysis, feature selection (FS) [1] is a critical step for improving the perfor-\nmance, interpretability, and generalizability of machine learning models. High-dimensional datasets, characterized\nby a large number of attributes relative to the number of samples, pose significant challenges, including increased\ncomputational cost, the risk of overfitting, and difficulties in model interpretation. FS methods aim to address these\nchallenges by identifying a subset of relevant features that contribute most to the predictive performance of the model.\nFS methods can be broadly categorized into filter, wrapper, and embedded methods [2]. Filter methods operate\nindependently of any learning algorithm and typically evaluate features based on their intrinsic properties, such as\ncorrelation with the target variable. Wrapper methods, by contrast, evaluate features based on the performance of a\nspecific learning algorithm. While wrapper methods are often more accurate than filter methods due to their direct\noptimization of model performance, they are also significantly more computationally expensive, particularly when\ndealing with high-dimensional data. This computational burden often renders wrapper methods impractical for large-\nscale datasets. Embedded methods integrate FS directly into the model training process. They balance the trade-off\nbetween accuracy and computational efficiency better than wrappers but are limited by their dependency on specific\nlearning algorithms, which may lead to biased or suboptimal feature selection.\nFS methods can be further divided into attribute evaluation methods, such as feature ranking (FR) methods, which\nrank individual features, and subset evaluation methods, which assess the quality of subsets of features and require\na strategy to search for candidate subsets of attributes. Subset evaluation methods are inherently multivariate, as\nthey consider the interactions between features, whereas attribute evaluation methods can be either univariate or\nmultivariate, depending on whether they evaluate features in isolation or in the context of other features.\nAn alternative approach that has gained attention is permutation feature importance (PFI), which assesses the im-\nportance of features based on their impact on model performance when their values are randomly shuffled. This\nrandomization disrupts the original relationship between the feature and the target variable, allowing us to measure\nthe feature's contribution to the model's accuracy. PFI seeks to maximize the degradation in model performance when\na feature's values are permuted. The underlying goal is to identify features that, when disrupted, cause the most sig-\nnificant degradation in model performance, thereby indicating their importance in making accurate predictions. PFI\nis considered a multivariate method because it evaluates the importance of each feature by measuring the change in\nmodel performance when the feature's values are permuted, taking into account the context of all other features in the\ndataset. This approach captures the interactions between features and their collective impact on the model's predictive\naccuracy.\nIn this paper, we propose a novel feature selection method that extends PFI by evaluating subsets of attributes rather\nthan individual features. Our method utilizes a multi-objective evolutionary algorithm (MOEA) to search for optimal\nfeature subsets, with objectives that include maximizing the degradation in model performance when subset feature\nvalues are permuted, and minimizing the cardinality of the feature subsets. The proposed permutation-based feature\nselection method, which evaluates subsets of attributes rather than individual ones, goes a step further in reflecting fea-\nture interactions. We call the proposed method PSEFS-MOEA (permutation-based subset evaluation feature selection\nwith MOEA). By considering the collective effect of attribute subsets, our method more effectively captures complex\ninterdependencies among features, leading to a more accurate and holistic selection of relevant attributes. Then this\napproach offers a significant advantage in high-dimensional settings, where the complexity of feature interactions can\ngreatly impact model performance. At the same time, it is computationally more feasible than wrapper methods, mak-\ning it particularly suitable for high-dimensional data. By combining the strengths of multivariate subset evaluation\nwith the efficiency of permutation-based importance measures, the proposed method provides a robust and scalable\nsolution for FS in complex, high-dimensional datasets. The main contributions of this work can be summarized as\nfollows:\n1. We propose PSEFS-MOEA, a novel permutation-based feature selection method that evaluates subsets of at-\ntributes instead of individual features. By exploring complex attribute interactions and dependencies, PSEFS-\nMOEA overcomes the limitations of conventional permutation feature importance methods and other single-\nfeature evaluation approaches.\n2. Two complementary versions of the method are introduced: PSEFS-MOEA-V1, which evaluates attribute\nsubsets on a validation set, and PSEFS-MOEA-V2, which performs evaluations directly on the training set.\n3. The method is tested on 24 datasets, encompassing a variety of tasks (binary, multi-class, imbalanced clas-\nsification, and regression), ensuring robust evaluation across real-world scenarios. The implementation is\nstraightforward and applicable to all task types without modification.\n4. PSEFS-MOEA achieves state-of-the-art performance, consistently outperforming nine well-established high-\ndimensional FS methods in terms of feature reduction and model performance metrics.\n5. The approach demonstrates strong generalization capabilities with low overfitting across tasks. Its multi-\nobjective design balances exploration and exploitation effectively, ensuring scalability and reliability in high-\ndimensional contexts.\nThe remainder of this article is organized as follows: In Section 2, we review related work in the field of feature se-\nlection, highlighting existing methods and their limitations in the context of high-dimensional data. Section 4 presents\na detailed description of the proposed permutation-based subset evaluation feature selection method, outlining the un-\nderlying algorithm and its advantages. Section 5 covers the experiments conducted to evaluate the performance of our\nmethod, including the datasets used, experimental setup, and results. In Section 6, we analyze the experimental results,\ndiscussing the implications and significance of our findings. Finally, Section 7 concludes the article, summarizing the\nkey contributions and suggesting directions for future research."}, {"title": "2 Related works", "content": "To provide a comprehensive understanding of the current landscape in FS for high-dimensional data, this section\nreviews the state-of-the-art methods developed in the previous and current year. The focus will be on the most recent"}, {"title": "3 Permutation feature importance", "content": "PFI is a model-agnostic method widely used to estimate the contribution of individual features to the performance\nof a predictive model. PFI was initially introduced by Breiman [20] for use with random forests. Building on this\nconcept, Fisher et al. [21] extended the idea to create a model-agnostic approach, which they termed model reliance.\nPFI evaluates how much the prediction error of the model increases when the values of a particular feature are ran-\ndomly shuffled while keeping the other features intact. This shuffling disrupts the relationship between the selected\nfeature and the target variable, allowing us to measure the decrease in the model's performance caused by the loss of\ninformation from that feature.\nThe process of computing PFI typically involves the following steps:\n1. Baseline performance measurement: Calculate the initial prediction error, on an evaluation dataset E, of the\ntrained model.\n2. Feature permutation: For each feature, shuffle its values across all samples in the evaluation dataset E to\nbreak its relationship with the target variable.\n3. Performance evaluation: Recalculate the model's prediction error using the evaluation dataset E with the\nshuffled feature."}, {"title": "4 Importance calculation", "content": "Compute the difference between the baseline error and the error with the permuted\nfeature. A larger increase in error indicates a more important feature.\nPFI offers several advantages. It is model-agnostic, meaning it can be applied to any predictive model regardless\nof the underlying algorithm, making it a highly versatile tool for feature evaluation. Its interpretability is another\nkey strength, as the importance scores are directly linked to changes in model performance, providing an intuitive\nunderstanding of feature relevance. Furthermore, because the model's prediction error reflects the combined effects\nof all features, permutation feature importance inherently captures multivariate interactions, allowing it to account for\nrelationships between features that influence the model's performance.\nHowever, PFI also has some limitations. It assumes independence between the permuted feature and the others,\nwhich can lead to misleading importance scores for highly correlated features. Although the method avoids retraining\nthe model, evaluating importance for multiple features can still be computationally expensive, especially for large\ndatasets. The results may also vary due to random permutations and data distribution, requiring repeated evaluations\nto obtain reliable estimates. Additionally, traditional PFI evaluates features individually, which may overlook complex\ninteractions among subsets of features.\nThere is an ongoing debate regarding whether feature importance should be computed using the training or validation\ndata as the evaluation dataset E. Computing on the training data reflects the features' importance as perceived by\nthe model during training, which can be useful for understanding how the model fits the data. However, it may\noverestimate feature importance if the model has overfitted to the training data.\nOn the other hand, computing feature importance on the validation data provides a measure of generalization, of-\nfering insights into how features contribute to predictions on unseen data. This is often considered more desirable\nfor real-world applications. However, as highlighted in [22], the choice between training and validation data is not\nstraightforward, as both perspectives offer valuable insights.\nTo address this uncertainty, we explore both approaches in our experiments. By evaluating feature importance on\nboth the training and validation datasets, we aim to provide a comprehensive understanding of the features' relevance\nduring training and their generalization to unseen data, highlighting the strengths and limitations of this approach\nacross different perspectives. This also sets the stage for the proposed method, which extends permutation feature\nimportance to evaluate subsets of features more effectively."}, {"title": "4 A multi-objective evolutionary algorithm for permutation-based subset evaluation\nfeature selection", "content": "In this section, we introduce the proposed method, PSEFS-MOEA, which employs a MOEA to search for optimal\nfeature subsets. The method aims to maximize the performance degradation of a pre-trained model, where the feature\nvalues of the selected subset are permuted, while simultaneously minimizing the cardinality of the subset. In Section\n4.1, we formulate the proposed subset evaluation FS problem as a multi-objective boolean combinatorial optimization\nproblem, presenting two versions of the problem set up for consideration. Section 4.2 provides a description of the\nMOEA and its characteristics."}, {"title": "4.1 Problem formulation", "content": "We consider two versions of the problem formulation, computing the feature importance on the validation data and\nthe training data, respectively. Let $D = \\{d_1,...,d_s\\}$ be a dataset with s instances. Each instance or sample $d_t =$\n$\\ d_t^1,...,d_t^w,o_t\\}$, t = 1,..., s, has w input attributes of any type, and one output attribute $o_t \\in R$, for regression\nproblems, or $o_t \\in \\{S_1,...,S_q\\}$, q > 1, for classification problems, where $S_k$, k = 1,..., q, are categorical output\nclasses. Dataset D is divided into three partitions R, V and T for training, validation and test with 60%, 20% and 20%\nof the data respectively. We consider the two following multi-objective combinatorial optimization problems:\nMaximize $f_1(x) = F(x, M, V)$,\nMinimize $f_2(x) = C(x)$                                                            (1)\nMaximize $f_1(x) = F(x, M, Q)$,\nMinimize $f_2(x) = C(x)$                                                                 (2)\nwhere $x = (x_1,...,x_w) \\in \\{0,1\\}^w$ is the decision variable set. Each decision variable $x_i$, i = 1,..., w, indicates\nwhether the input attribute i is selected ($x_i = 1$) or not ($x_i = 0$) in the feature selection process. Function F is"}, {"title": "4.2 Multi-objetive evolutionary algorithm", "content": "For the proposed PSEFS-MOEA method, we employed the Non-dominated Sorting Genetic Algorithm II (NSGA-II)\n[23] with binary representation to solve the optimization problems (1) and (2) defined in Section 4.1. NSGA-II is a\nwidely used multi-objective evolutionary algorithm designed to optimize problems with conflicting objectives. The\nalgorithm operates by maintaining a diverse population and employing a fast non-dominated sorting approach to clas-\nsify solutions into Pareto fronts. It also uses a crowding distance mechanism to ensure diversity among solutions. The\npopularity of NSGA-II stems from its efficiency and robustness in handling multi-objective optimization tasks. Al-\nthough NSGA-II was chosen for its well-established performance and accessibility, other multi-objective optimization\nalgorithms could also have been used.\nThe fitness functions correspond to the objective functions $f_1$ and $f_2$ outlined in the optimization problems (1) and\n(2). Since the function $f_1$ is a maximization objective and $f_2$ is a minimization objective, $f_1$ has been multiplied\nby -1 so that both $f_1$ and $f_2$ are treated as minimization objectives in the MOEA implementation. To evolve the\npopulation, we utilized half uniform crossover [24] and bit flip mutation [25] operators. After running the algorithm,\nthe solution with the best $f_1(x)$ value was selected from the Pareto front identified by NSGA-II. To implement our\nmethod, we leveraged the Platypus platform [26], a versatile library for evolutionary computation and multi-objective\noptimization, which facilitated the integration of NSGA-II with our problem-specific requirements."}, {"title": "5 Experiments and results", "content": "In this section, we describe the experiments conducted to evaluate the performance of the proposed PSEFS-MOEA\nmethod. The general goal is to assess its ability to identify effective feature subsets for both classification and re-\ngression tasks, comparing its performance against several established FS methods across a variety of scenarios and\ndatasets.\nWe first provide details of the datasets used in the experiments, which include a variety of high-dimensional datasets\nfrom classification and regression domains (Section 5.1). Next, we describe the comparison methods used as bench-"}, {"title": "5.1 Datasets", "content": "The experiments were conducted on a total of 24 datasets for classification and regression tasks. The number of\nfeatures in these datasets ranges from a minimum of 617 to a maximum of 22,277, ensuring a diverse set of high-\ndimensional scenarios for evaluation.\nThe 14 classification datasets were obtained from the public OpenML platform\u00b3. These datasets vary in the number of\ninstances and include both binary and multi-class classification problems. The class distributions also vary, covering\nboth balanced and imbalanced scenarios. A summary of the key characteristics of the classification datasets is provided\nin Table 1."}, {"title": "5.2 Comparison methods", "content": "The proposed feature selection method for high-dimensional data has been compared against nine established methods,\nencompassing both subset evaluation and attribute evaluation approaches. All comparison methods are filter-based and\ninclude both multivariate and univariate techniques. Wrapper methods were excluded from this study due to their high\ncomputational cost and impracticality when dealing with high-dimensional datasets.\nThe proposed feature selection method, PSEFS-MOEA, is evaluated in two versions, referred to in this paper as\nPSEFS-MOEA-V1 and PSEFS-MOEA-V2, which correspond to the problem formulations (1) and (2), respectively.\nFor classification tasks, the following comparison methods were considered:\n1. CSE-BF: This method uses the ConsistencySubsetEval algorithm in Weka\u2074 [27], which evaluates feature\nsubsets based on their consistency with the target variable [28]. The subset search strategy is performed\nusing a Best First (BF) [29] approach.\n2. CFSSE-BF: The CfsSubsetEval algorithm in Weka evaluates feature subsets based on their predictive ability\nand redundancy [30, 31]. It selects subsets that are highly correlated with the target variable but have low\ninter-correlation. The search is also performed using a BF strategy.\n3. PFI-V1: This is the PFI methods implemented in scikit-learn [32, 33] and described in Section 3, which\nevaluates the importance of individual features by measuring the increase in prediction error when the feature\nvalues are permuted. In this version, the evaluation is performed using a validation dataset.\n4. PFI-V2: Similar to PFI-V1, this version of PFI evaluates feature importance using the training dataset instead\nof the validation dataset.\n5. SAE: The SignificanceAttributeEval method in Weka assesses the significance of individual attributes based\non statistical significance tests [34].\n6. IGAE: The InfoGainAttributeEval method in Weka evaluates individual features by calculating the infor-\nmation gain with respect to the target variable, identifying the attributes that provide the most reduction in\nentropy [35].\n7. RFAE: The ReliefFAttributeEval method in Weka is a multivariate technique that assesses feature importance\nby measuring the ability of each feature to distinguish between instances that are near in the feature space but\nbelong to different classes [36].\n8. CSAE: The ChiSquaredAttributeEval method in Weka uses the Chi-squared test to evaluate the dependency\nbetween individual features and the target variable, ranking features based on their Chi-squared scores [37].\n9. CAE: The CorrelationAttributeEval method in Weka evaluates features based on their Pearson correlation\ncoefficient with the target variable, ranking features with higher absolute correlations as more important [38]."}, {"title": "5.3 Description of the experiments", "content": "Specifically, the experiments are designed to address the following objectives:\n\u2022 Comparison of PSEFS-MOEA versions: To compare the two proposed variants, PSEFS-MOEA-V1 and\nPSEFS-MOEA-V2, in terms of their performance on the optimization problems (1) and (2) described ear-\nlier.\n\u2022 Evaluation against conventional PFI: To determine whether the proposed permutation-based method for sub-\nset evaluation outperforms the conventional PFI method, which evaluates features individually, particularly\nin capturing feature interactions and improving predictive performance.\n\u2022 Effectiveness in reducing feature sets: To assess the ability of the proposed method to reduce the number of\nselected features while maintaining or enhancing the predictive accuracy of the models.\n\u2022 Comparison with established methods: To compare the proposed method with other well-established feature\nselection techniques for high-dimensional data. This includes evaluating predictive performance, overfitting\nanalysis and computational efficiency in terms of execution time.\n\u2022 Robustness across tasks: To validate the robustness of the proposed method in both classification and regres-\nsion tasks, including imbalanced classification problems, by employing appropriate performance metrics for\neach type of task.\nWith these objectives and premises in mind, the experiments were conducted as follows:\n\u2022 The proposed PSEFS-MOEA method is probabilistic, requiring multiple runs with different random seeds to\nensure reliable and statistically sound results. Both versions PSEFS-MOEA-V1 and PSEFS-MOEA-V2 of\nthe proposed method were executed 10 times with different random seeds across the 24 datasets considered\nin this study. RF was used to construct the model for evaluating feature importance. For V1, the model was\ntrained on the dataset R (60% of the data), and features were evaluated on the validation set V (20% of the\ndata), following the problem formulation in Eq. (1). For V2, the model was trained and evaluated on the\ndataset Q = RUV (80% of the data), following the formulation in Eq. (2). The parameter settings used in\nthe executions, as well as those for the comparison methods, are summarized in Table 3.\n\u2022 Since PFI also requires a pre-trained model for evaluating feature importance, it was executed in two versions\nV1 and V2. In V1, the model was trained on the dataset R and features were evaluated on a separate validation\nset V, while in V2 the model was trained on the dataset Q = RUV and features ware evaluated on the same\ndataset Q. The model was built using the RF learning algorithm in both versions.\n\u2022 In contrast, the other FS methods do not use pre-trained models, evaluating features directly on the dataset\nQ = RUV. For these FS methods, it is necessary to first generate a ranking of features and then build\nmodels using the top-ranked features for a predefined number of attributes. In our experiments, models were\nconstructed with the top 10, 100, N1 and N2 features. N1 and N2 are the number of features selected by\nPSEFS-MOEA-V1 and PSEFS-MOEA-V2, respectively, to ensure a fair comparison.\n\u2022 To evaluate the quality of the feature subsets selected by each method, classification or regression models\n(depending on the task) were built using the RF algorithm on the training dataset Q = RUV (80% of the\ndata). The resulting models were then evaluated on both the training dataset Q and an unseen test dataset T\n(20% of the data). The test dataset T was not exposed to any FS algorithm at any point. Evaluating models\non both training and test datasets enables an analysis of overfitting and the generalization capability of the\nmodels produced with each FS method.\n\u2022 The performance of classification models was assessed using ACC and balanced accuracy (BA), while for\nregression models, RMSE and coefficient of determination (R2) were employed. ACC provides a straight-\nforward measure of classification performance but can be misleading in imbalanced datasets. BA adjusts for\nimbalanced datasets by averaging recall across classes. RMSE captures the magnitude of prediction errors\nin regression. R\u00b2 measures the proportion of variance in the target variable explained by the model. These\nmetrics provide a comprehensive evaluation of the predictive performance of the models across different\ntasks.\n\u2022 For subset evaluation methods (PSEFS-MOEA-V1, PSEFS-MOEA-V2, CSE-BF, and CFS-BF), the number\nof selected features was recorded. Additionally, the runtime of all feature selection methods was measured to\nassess their computational efficiency."}, {"title": "5.4 Results", "content": "The results obtained from the experiments are summarized in the following tables:\n\u2022 Table 4 reports the accuracy of the models evaluated on the training set Q = RUV for classification problems.\n\u2022 Table 5 reports the accuracy of the models evaluated on the test set T for classification problems."}, {"title": "6 Analysis of results and discussion", "content": "To rigorously analyze the results obtained in this study, it is essential to employ statistical tests as tools to detect\nwhether the observed differences among the compared FS methods are statistically significant. The use of statisti-"}, {"title": "4 Importance calculation", "content": "Compute the difference between the baseline error and the error with the permuted\nfeature. A larger increase in error indicates a more important feature.\nPFI offers several advantages. It is model-agnostic, meaning it can be applied to any predictive model regardless\nof the underlying algorithm, making it a highly versatile tool for feature evaluation. Its interpretability is another\nkey strength, as the importance scores are directly linked to changes in model performance, providing an intuitive\nunderstanding of feature relevance. Furthermore, because the model's prediction error reflects the combined effects\nof all features, permutation feature importance inherently captures multivariate interactions, allowing it to account for\nrelationships between features that influence the model's performance.\nHowever, PFI also has some limitations. It assumes independence between the permuted feature and the others,\nwhich can lead to misleading importance scores for highly correlated features. Although the method avoids retraining\nthe model, evaluating importance for multiple features can still be computationally expensive, especially for large\ndatasets. The results may also vary due to random permutations and data distribution, requiring repeated evaluations\nto obtain reliable estimates. Additionally, traditional PFI evaluates features individually, which may overlook complex\ninteractions among subsets of features.\nThere is an ongoing debate regarding whether feature importance should be computed using the training or validation\ndata as the evaluation dataset E. Computing on the training data reflects the features' importance as perceived by\nthe model during training, which can be useful for understanding how the model fits the data. However, it may\noverestimate feature importance if the model has overfitted to the training data.\nOn the other hand, computing feature importance on the validation data provides a measure of generalization, of-\nfering insights into how features contribute to predictions on unseen data. This is often considered more desirable\nfor real-world applications. However, as highlighted in [22], the choice between training and validation data is not\nstraightforward, as both perspectives offer valuable insights.\nTo address this uncertainty, we explore both approaches in our experiments. By evaluating feature importance on\nboth the training and validation datasets, we aim to provide a comprehensive understanding of the features' relevance\nduring training and their generalization to unseen data, highlighting the strengths and limitations of this approach\\across different perspectives. This also sets the stage for the proposed method, which extends permutation feature\nimportance to evaluate subsets of features more effectively."}, {"title": "6.1 Comparison of PSEFS-MOEA versions", "content": "The comparison between PSEFS-MOEA-V1 and PSEFS-MOEA-V2 reveals no statistically significant differences for\nACC, BA, or nRMSE (p-value \u2265 0.05 in all cases). However, for R2, the p-value (0.0371) indicates that PSEFS-\nMOEA-V1 achieves statistically significant improvements over V2. To further interpret the results, mean differences\nare examined for metrics without statistical significance:\n\u2022 ACC and BA: Both differences (-0.0071 and -0.0108, respectively) are slightly negative, suggesting V1\nmight underperform V2 in these metrics, though the differences are negligible.\n\u2022 nRMSE: A small positive difference (+0.0031) hints at better performance for V1 in terms of error minimiza-\ntion, aligning with the observed R\u00b2 improvements.\nIn summary, although the differences are subtle in most metrics, V2 shows a slight advantage in classification tasks,\nwhile in regression tasks V1 shows statistical differences over V2 in the R2 metric and a slight advantage in the\nnRMSE metric."}, {"title": "6.2 Evaluation against conventional PFI", "content": "For classification tasks, PSEFS-MOEA-V1 and PSEFS-MOEA-V2 consistently outperform PFI with statistically sig-\nnificant results (p-value < 0.05) for both metrics ACC and BA. This demonstrates the robustness of PSEFS-MOEA\ncompared to PFI, regardless of the number of selected attributes or metric evaluated. For regression tasks, PSEFS-"}, {"title": "6.3 Effectiveness in reducing feature sets", "content": "When comparing PSEFS-MOEA-V1 to models with all features, statistically significant improvements (p-values <\n0.05) are observed for all metrics, including ACC, BA, nRMSE and R\u00b2. This highlights that the feature selection\nperformed by V1 enhances predictive performance while reducing the number of features used. Similar trends are\nobserved when comparing the PSEFS-MOEA-V2 with the models with all features, but for the nRMSE and R2, the\np-values are > 0.05. Mean differences (0.0041 for nRMSE and 0.0237 for R2) indicate that V2 still outperforms\nmodels with all features on average.\nIn conclusion, both versions of PSEFS-MOEA achieve better predictive performance than models with all features,\nwhile simultaneously reducing feature sets, with V1 achieving consistent statistical significance and V2 showing\naverage improvements."}, {"title": "6.4 Comparison with established methods", "content": "The results of the ranking of statistically significant differences between the methods evaluated (including PSEFS-\nMOEA-V1, PSEFS-MOEA-V2, and other established methods) confirm the robustness of the proposed approaches\nacross the four evaluation metrics (ACC, BA, nRMSE, and R\u00b2).\nFor the classification tasks (Tables 14 and 15):\n\u2022 PSEFS-MOEA-V2 achieved the highest position in the ranking for both ACC and BA metrics, with wins-\nlosses differences of +31 and +25, respectively. This confirms its strong performance in classification tasks.\n\u2022 PSEFS-MOEA-V1 ranked second in both metrics (ACC: +29, BA: +23), consistently outperforming all other\nmethods except its successor, PSEFS-MOEA-V2.\n\u2022 The performance gap between PSEFS-MOEA-V2 and V1 highlights that V2 is slightly more effective than\nV1 in classification problems. This aligns with the earlier analysis of classification-specific comparisons\nbetween these two versions.\nFor the regression tasks (Tables 16 and 17):\n\u2022 PSEFS-MOEA-V1 emerged as the top performer in regression tasks for both nRMSE (+15) and R2 (+17).\nThis confirms its capability to minimize error and improve predictive accuracy in regression problems.\n\u2022 PSEFS-MOEA-V2 followed as the second-best method, with a wins-losses difference of +8 for nRMSE and\n+9 for R2.\n\u2022 The rankings show a reversal in performance trends compared to classification tasks, with PSEFS-MOEA-\nV1 outperforming V2 in regression scenarios. This observation corroborates the earlier findings of superior\nnRMSE and R2 performance by PSEFS-MOEA-V1.\nIn general, both versions of PSEFS-MOEA dominate the rankings across all metrics, consistently outperforming\nestablished subset evaluation filter methods such as CSE-BF and CFSSE-BF, attribute evaluation filter methods such\nas IGAE, RFAE, SAE, PFI, CSAE and CAE, and models built with all features. The ranking analysis reinforces the\nadaptability and effectiveness of PSEFS-MOEA versions, with PSEFS-MOEA-V2 excelling in classification tasks and\nPSEFS-MOEA-V1 leading in regression scenarios."}, {"title": "Overfitting analysis", "content": "To further support the performance analysis, Tables 19 and 20 present the degrees of overfitting for the models obtained\nwith all FS methods and the models built using all features.\nFor the classification metrics (ACC and BA), the overfitting ratios were calculated as $\\frac{ACC_{train}}{ACC_{test}}$ and $\\frac{BA_{train}}{BA_{test}}$. For the\nregression metric nRMSE, the overfitting ratio was computed as $\\frac{nRMSE_{test}}{nRMSE_{train}}$. For R2, overfitting was determined as\nthe difference $R^2_{train} \u2212 R^2_{test}$. Ratios greater than 1 for ACC, BA and nRMSE indicate overfitting, with higher values\ndenoting more severe overfitting. Positive differences for R2 indicate overfitting, with larger differences indicating\ngreater overfitting.\nFor the classification tasks (Table 19), PSEFS-MOEA-V1 and PSEFS-MOEA-V2 exhibited the smallest overfitting\nratios for both ACC and BA metrics.These results highlight the effectiveness of the proposed methods in mitigating\noverfitting compared to all other FS methods and models trained with all features. The low overfitting ratios emphasize\nthe generalization ability of PSEFS-MOEA methods, making them highly reliable for classification tasks.\nFor the regression tasks (Table 20), PSEFS-MOEA-V1 achieved the top rank with the smallest overfitting difference\namong all methods for R2 metric. For nRMSE, PSEFS-MOEA-V1 ranked 4th out of 20, further showcasing its\nability to minimize overfitting in regression tasks. PSEFS-MOEA-V2, while performing well overall, ranked 16th\nfor nRMSE and 5th for R\u00b2, indicating a slightly higher tendency for overfitting in regression problems compared to\nPSEFS-MOEA-V1. In general, the overfitting analysis confirms the superiority of PSEFS-MOEA-V1 in regression\ntasks, aligning with its top performance rankings for nRMSE and R2 metrics.\nThis comprehensive evaluation underscores the robustness of the PSEFS-MOEA methods, particularly their ability\nto achieve competitive performance while maintaining low degrees of overfitting, which is critical for real-world\napplications."}, {"title": "Execution times", "content": "The execution times for the PSEFS-MOEA methods, both V1 and V2, were the highest among all the compared meth-\nods for classification and regression tasks, as shown in Tables 21 and 22, respectively. The extended computational\ncost of PSEFS-MOEA methods is an expected consequence of their multi-objective, population-based, meta-heuristic\nnature. Key factors contributing to these higher runtimes include:\n\u2022 Population-based search: Unlike single-solution optimization techniques, MOEAs operate on a population\nof candidate solutions, iteratively evaluating and evolving this population. This increases the computational\neffort due to multiple fitness evaluations per iteration.\n\u2022 Multi-objective optimization: PSEFS-MOEA methods are designed to optimize multiple conflicting objec-\ntives (e.g., feature reduction and predictive performance), which inherently demands more complex calcula-\ntions and trade-off management than single-objective methods.\n\u2022 Diversity preservation mechanisms: MOEAs employ mechanisms such as crowding distance or Pareto domi-\nnance sorting to maintain diversity in the solution set, ensuring robust exploration of the search space. These\nprocesses add computational overhead compared to simpler FS methods.\n\u2022 Evaluation of features: Both versions of PSEFS-MOEA evaluate subsets of features iteratively across a large\nnumber of candidate solutions, requiring repeated evaluation of samples in machine learning models, which\nsignificantly increases runtime."}, {"title": "6.5 Robustness across tasks", "content": "The proposed PSEFS-MOEA method, in both its V1 and V2 versions, has demonstrated remarkable robustness across\na wide variety of tasks, encompassing both classification and regression problems.\nClassification tasks The results indicate consistent and strong performance for both binary and multi-class classifi-\ncation problems. This robustness highlights the method's adaptability to varying levels of class complexity. Among\nthe classification tasks tested, the majority (11 out of 14) were imbalanced datasets, which are particularly challeng-\ning due"}]}