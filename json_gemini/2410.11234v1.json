{"title": "Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning", "authors": ["Jiayu Chen", "Wentse Chen", "Jeff Schneider"], "abstract": "Offline reinforcement learning (RL) is a powerful approach for data-driven decision-making\nand control. Compared to model-free methods, offline model-based reinforcement learning\n(MBRL) explicitly learns world models from a static dataset and uses them as surrogate\nsimulators, improving the data efficiency and enabling the learned policy to potentially\ngeneralize beyond the dataset support. However, there could be various MDPs that behave\nidentically on the offline dataset and so dealing with the uncertainty about the true MDP\ncan be challenging. In this paper, we propose modeling offline MBRL as a Bayes Adaptive\nMarkov Decision Process (BAMDP), which is a principled framework for addressing model\nuncertainty. We further introduce a novel Bayes Adaptive Monte-Carlo planning algorithm\ncapable of solving BAMDPs in continuous state and action spaces with stochastic transitions.\nThis planning process is based on Monte Carlo Tree Search and can be integrated into\noffline MBRL as a policy improvement operator in policy iteration. Our \"RL + Search\"\nframework follows in the footsteps of superhuman Als like AlphaZero, improving on current\noffline MBRL methods by incorporating more computation input. The proposed algorithm\nsignificantly outperforms state-of-the-art model-based and model-free offline RL methods\non twelve D4RL MuJoCo benchmark tasks and three target tracking tasks in a challenging,\nstochastic tokamak control simulator.", "sections": [{"title": "1 Introduction", "content": "The success of reinforcement learning (RL) typically relies on large amounts of interactions with the en-\nvironment. However, in real-world scenarios, such interactions can be unsafe or costly. As an alternative,\noffline RL (Levine et al. (2020)) leverages offline datasets of transitions, collected by a behavior policy, to\ntrain a policy that can transfer to an online task. To avoid overestimation of the value function for some\n(out-of-sample) states in the environment, which can mislead policy learning, model-free offline RL methods\n(Kumar et al. (2020); Wu et al. (2019)) often constrain the learned policy to remain close to the behavior\npolicy or within the support of the offline dataset. However, collecting transitions that comprehensively\ncover possible task scenarios, or acquiring a large volume of demonstrations from a high-quality behavior\npolicy, can be expensive. This challenge has led to the development of offline model-based reinforcement\nlearning (MBRL) approaches, such as (Lu et al. (2022); Guo et al. (2022)). These methods train dynamics\nmodels from offline data and optimize policies using imaginary rollouts generated by the models. Notably,\nthe dynamics modeling is independent of the behavior policy, making it possible to achieve high returns\neven with data collected from a random policy. Furthermore, with careful dynamics modeling and thorough"}, {"title": "2 Background", "content": "A Markov Decision Process (MDP, Puterman (2014)) is described as a tuple M = (S, A,P, R, \u03b3). S and\nA are the state space and action space, respectively. P:S \u00d7 A \u2192 As is the dynamics function and\nR : S \u00d7 A \u2192 \u2206[0,1] is the reward function, where \u2206x denotes the set of possible probability distributions on\nX. \u03b3\u2208 [0, 1) is a discount factor. The goal of solving an MDP is to find a policy \u03c0 : S \u2192 \u25b3\u2081 that maximizes\nthe expected return, defined as the cumulative discounted reward over time.\nA Bayes Adaptive MDP (BAMDP, Duff (2002)) can model scenarios where the precise MDP Mo\n= (S, A, Po, Re, y) is uncertain but is known to follow a prior distribution bo(0). During planning, a Bayes-\noptimal agent would update its belief over the MDP based on experience. Formally, a BAMDP can\nbe described as a tuple M+ = (S+,A,P+,R+, \u03b3). S+ denotes the space of information states (s,b),\nwhich is a composition of the physical state and the current belief over the MDP. After each transition\n(s,a,r,s'), the belief is updated to the corresponding Bayesian posterior: b'(0) \u00d7 b(0)P((s,a,r,s')|0) =\nb(0)Po(s'|s, a)Re(r|s, a). Accordingly, P+ and R+ can be defined as follows:\nP^+((s', b')|(s,b), a) = 1(b\" = b') \\int_{\\Theta} P_\\theta(s'|s,a)b(\\theta)d\\theta, R^+((s,b),a) = \\int_{\\Theta} R_\\theta(s,a)b(\\theta)d\\theta\n(1)"}, {"title": "3 Related Works", "content": "Offline RL (Levine et al. (2020); Chen et al. (2024)) enables an agent to learn control policies from datasets of\nenvironment transitions pre-collected by a behavior policy \u03bc, i.e., D\u00b5 = {[(si, a, r\u00ec)F=1]_1}, circumventing\nthe need for potentially expensive or unsafe online interactions. Model-free offline RL methods directly learn\nvalue/policy functions from Du but restrict the policy to remain close to \u03bc or to behave within the support\nof Du. Offline Model-based RL (MBRL) methods, on the other hand, explicitly learn world models M\u04e9\nfrom Du (through supervised learning) and adopt Me as a surrogate simulator, enabling the learned policy\nto possibly generalize to states beyond D\u00b5. Specifically, both planning methods (Argenson & Dulac-Arnold\n(2021); Zhan et al. (2022); Diehl et al. (2023)), such as MPC (Garcia et al. (1989)), and RL methods (Yu\net al. (2020); Kidambi et al. (2020); Lu et al. (2022); Yu et al. (2021); Guo et al. (2022)) can be applied on\ntop of the learned Me. However, since Du may not span the entire state-action space, Me is unlikely to be\nglobally accurate. Learning/Planning without any safeguards against such model inaccuracy can yield poor\nresults. In this case, the authors of (Yu et al. (2020); Kidambi et al. (2020); Lu et al. (2022)) propose learning\nan ensemble of world models, using ensemble-based uncertainty estimations to construct a pessimistic MDP\n(P-MDP), and learning a near-optimal policy atop it. Ideally, for any policy, the performance in the real\nenvironment is lower-bounded by the performance in the corresponding P-MDP (with high probability), thus\navoiding being overly optimistic about an inaccurate model. For practical implementation of the P-MDP,\nLu et al. (2022) conduct a thorough empirical study to compare a series of heuristics. However, none of\nthese offline MBRL methods have modeled the problem as a BAMDP, even though Bayesian RL/planning\nprovides a principled framework for handling model uncertainty. Additionally, with the learned model Me,\ndeep and broad search in the policy learning process is possible, but the offline RL methods introduced above\nhave not attempted such search-based policy improvement.\nMonte-Carlo Tree Search (MCTS, Browne et al. (2012)) has been successfully integrated with RL, as exem-\nplified by AlphaZero (Silver et al. (2017a)) and MuZero (Schrittwieser et al. (2020)). These methods have\nachieved superhuman performance in domains requiring highly precise and sophisticated decision-making\nprocesses. AlphaZero relies on given world models, whereas MuZero learns the world model and policy si-\nmultaneously by interacting with the environment. Although there have been various extensions of MuZero\n(Hubert et al. (2021); Schrittwieser et al. (2021); Ye et al. (2021); Danihelka et al. (2022); Antonoglou et al.\n(2022); Oren et al. (2022); Chen et al. (2023); Zhao et al. (2024)), most algorithms are designed for online\nMBRL. According to Niu et al. (2023), the applications of MuZero in offline learning, especially for con-\ntinuous control in highly stochastic environments, which is our focus, still require significant improvement.\nAlthough search-based, MuZero learns a dynamics model and a reward model in a latent state space rather"}, {"title": "4 Methodology", "content": "In this work, we propose a novel offline MBRL algorithm based on Bayes Adaptive MCTS. The core challenge\nis to design a Bayes Adaptive planning method that is efficient in large stochastic MDPs. In this case, we\npropose Continuous BAMCP in Section 4.2, which can be applied to continuous control tasks with high\ndynamics stochasticity. Then, in Section 4.3, we present a search-based policy iteration framework, where\nthe search results are distilled into policy and value networks for policy improvement and policy evaluation,\nrespectively, at each iteration. In this way, we integrate offline MBRL with Bayes Adaptive MCTS. Both\ncomponents require the use of an ensemble of world models for practical implementation and uncertainty\nquantification, which is detailed in Section 4.1."}, {"title": "4.1 The Key Role of Deep Ensembles", "content": "Offline MBRL methods estimate world models Me from a static dataset Du, which would inevitably induce\nepistemic uncertainty about the identity of the real MDP M*. Specifically, there could be various potential\nMDPs that behave identically on the limited set of states and actions in Du, but their dynamics and reward\nfunctions may differ, especially on out-of-sample states and actions. Thus, we are actually dealing with a\ndistribution of world models that follow a prior distribution bo(0) \u2261 P(Mo|D\u00b5). As introduced in Section\n2, Bayesian RL based on BAMDP is a principled framework for handling model uncertainty by explicitly\nincluding the belief over the models in its state representation. Essentially, the belief is updated with\nexperience, providing a measure of how the models' uncertainty has changed since the beginning of the\nepisode. As a result, the agent can adjust its behavior upon receiving new information that reduces the\nepistemic uncertainty. Such an adaptive policy is necessary to act optimally in offline RL, as demonstrated\nin (Ghosh et al. (2022)).\nThe idea of deep ensembles (Lakshminarayanan et al. (2017)) is to train multiple deep neural networks\nas approximations of a function, each using a different weight initialization and optimized with a different\nmini-batch sequence. For offline MBRL, we can learn an ensemble of dynamics models {P),\u2026\u2026,PK} and\nreward models {R,\u2026\u2026,R}\u00b2 from the dataset Du by minimizing the following supervised learning loss:\n(i = 1,\u2026, K)\nL(\\mathcal{P}) = -\\mathbb{E}_{(s,a,s') \\sim D_\\mu} [\\log P_\\theta(s'|s, a)], L(\\mathcal{R}) = -\\mathbb{E}_{(s,a,r) \\sim D_\\mu} [\\log R_\\theta(r|s, a)]\n(3)"}, {"title": "4.2 Bayes Adaptive MCTS in Continuous State and Action Spaces", "content": "BAMCP (Guez et al. (2013)) has been successful in solving large-scale BAMDPs, as detailed in Appendix A,\nbut it is limited to scenarios with discrete state and action spaces. In this subsection, we introduce an online\nplanning method to approximate the Bayes-optimal policy at a decision point (s, h). (h denotes the transition\nhistory that ends at s.) In particular, this method can be used to solve BAMDPs with continuous states\nand actions. Specifically, we adopt double progressive widening (DPW, Cou\u00ebtoux et al. (2011); Auger et al.\n(2013)), which maintains a finite list of chance nodes to be searched at each decision point and incrementally\nadds a new chance node to the list based on the visitation counts. For a node (s,h), a new action a is\nsampled (with the current policy) and added to its children set C((s, h)), if [N((s,h))\u00ba] \u2265 |C((s,h))|, where\n\u03b1 \u2208 (0, 1) is a hyperparameter that controls the growth rate and N denotes the visitation counts. Otherwise,\nan action will be sampled from C((s, h)) according to the UCT (Kocsis & Szepesv\u00e1ri (2006)) rule. Similarly,\nto handle the infinitely many possible transitions, a new next state s' is added to the children set C((s, h), a)\nonly if [N((s,h), a)\u1e9e] \u2265 |C((s,h),a)| (\u03b2\u2208 (0,1)). Otherwise, the least visited child in C((s,h), a) will be\nselected as the next state. With DPW, the sets of possible actions or next states for search are all finite,\nallowing deep search as in discrete scenarios, and the more promising states and actions (with higher N)\nhave more subsequent branches, thereby reducing corresponding estimation uncertainty. However, directly\nintegrating DPW and BAMCP (i.e., Algorithm 3) likely cannot solve BAMDPs with continuous state and"}, {"title": "4.3 The Overall Framework: Search-based Policy Iteration", "content": "In this subsection, we present a framework (inspired by MuZero) that integrates continuous BAMCP (i.e.,\nAlgorithm 1) into policy improvement and policy evaluation. By iteratively running these procedures, we\ncan approach a near Bayes-optimal policy, i.e., \u03c0, that can be directly referred to during execution in the\ntrue environment. The pseudo code of the overall framework is shown as Algorithm 2. For efficiency, a\nlearner and a number of actors execute in parallel, reading from and sending data to the replay buffer D\nrespectively. The actors update their copies of policy and value functions every Et learner steps.\nIn particular, each actor would interact with the learned world models to sample trajectories 7.7 At each\ntime step of the trajectory, a SEARCH procedure (defined in Algorithm 1) is executed using the actor's\ncurrent copy of and V for planning at the current decision point. The search result ret is then used\nto indicate the action choice, i.e., a ~ \u3160ret(\u00b7|(s,h)). The subsequent transition process follows a BAMDP,\nwhere r ~ R+(\u00b7|(s,h), a), s' ~ P+(\u00b7|(s, h), a), and the belief b(0) adapts with experience, as described in\nSTATEPW. The collected trajectories are used in the learning process, where is trained to mimic the search\nresult ret by minimizing a cross-entropy loss (i.e., L(\u03c0, {i}-1)), while V is trained to predict the n-step\npessimistic return z. As noted in (Hubert et al. (2021)), \u03c0ret improves at each decision point, so repeatedly\napplying continuous BAMCP to obtain \u00f8ret and projecting the search results to the parameter space of \u03c0 (by\nminimizing L(\u03c0, \u03c4\u03af}-1)) represents the policy improvement process. Meanwhile, value backups to update\nthe Qestimates (i.e., R \u2190 V((s', hars')) in Algorithm 1) and the temporal difference learning for the value\nfunction V constitute the policy evaluation process. In this way, the search algorithm is used as a powerful\nimprovement operator to iteratively enhance the performance of the learned policy \u03c0."}, {"title": "5 Evaluation", "content": "To evaluate the effectiveness of each component in our algorithm design, we introduce three variants: (1)\nBA-MBRL leverages learned world models as surrogate simulators, applying reward penalties to collected\ntransitions and using standard online RL algorithms (e.g., SAC (Haarnoja et al. (2018))) to learn a policy.\nWhile following existing offline MBRL methods, it models the problem as a BAMDP (rather than an MDP),\nwith environment transitions defined by Equations (1) and (4) and the reward penalty by Equation (5), and\nis designed to evaluate the effectiveness of Bayesian RL. (2) BA-MCTS builds on BA-MBRL by introducing\nContinuous BAMCP (Algorithm 1) to plan at decision points, rather than inferring directly from the policy,\nto generate trajectories for downstream SAC, demonstrating the impact of deep search on policy learning.\n(3) BA-MCTS-SL, described in Algorithm 2, replaces the policy learning algorithm in BA-MCTS from\npolicy gradient methods (as in SAC) with supervised learning (SL), allowing us to compare which approach\noffers a more efficient policy update mechanism, particularly for continuous control tasks.\nWe first evaluate our algorithms on a widely-used continuous control benchmark for offline RL methods\nD4RL MuJoCo (Fu et al. (2020)). The evaluation results for three types of robotic agents, each with offline\ndatasets of four different qualities, are presented in Table 1. (1) Compared to SOTA offline MBRL methods,\nour algorithms achieve superior performance on nine out of twelve tasks. In terms of average performance,\nBA-MBRL significantly outperforms the baselines, demonstrating the effectiveness of using BAMDPs to\nhandle model uncertainties in offline MBRL. (2) Both BA-MCTS and BA-MCTS-SL further improve upon\nBA-MBRL, highlighting the enhancement brought by deep search in policy learning. Notably, we apply\nContinuous BAMCP to only 10% of states when collecting training trajectories, while for the remaining\nstates, we sample actions directly from the policy, i.e., a ~ \u03c0(s). Increasing the search ratio could further\nenhance policy performance at the cost of increased computation. (3) BA-MCTS-SL performs similarly to\nBA-MCTS, validating the effectiveness of both policy update mechanisms. However, BA-MCTS-SL struggles\non Walker2d, where a warm-up training phase (using BA-MBRL) is required to establish a better initial\npolicy. On the other hand, the advantage of the SL-based policy update is evident in the training plots of our\nalgorithms in Figure 3, where BA-MCTS-SL exhibits much smoother learning curves compared to the other\ntwo algorithms, indicating greater robustness in model selection. (4) We further compare our algorithms\nwith model-free offline policy learning methods, as shown in Appendix D. The performance improvement\nis even greater than that over model-based methods, highlighting the necessity of model-based learning.\nParticularly, when data quality is low, merely mimicking or staying close to the behavior policy would result\nin an underperforming policy. (5) We provide an ablation study in Appendix F to demonstrate the necessity"}, {"title": "6 Conclusion and Discussions", "content": "In this work, we propose framing offline model-based reinforcement learning (MBRL) as a Bayes Adaptive\nMarkov Decision Process (BAMDP) to better address uncertainties in the world models learned from offline\ndatasets. We also introduce a novel planning method for solving BAMDPs in continuous state and action\nspaces using Monte Carlo Tree Search. This planning process is integrated into a policy iteration frame-\nwork, enabling the derivation of a policy suitable for real-time execution from the planning results. In our\nevaluation, we test several variants of our algorithms to separately highlight the effectiveness of Bayesian\nRL and deep search. Additionally, we compare two different approaches for policy updates (based on the\nsearch results) in continuous control tasks: supervised learning and policy gradient methods. Our findings\ndemonstrate that: (1) adapting beliefs over an ensemble of world models based on experience yields more\naccurate model approximations for MBRL; (2) deep search improves learning performance by incorporating\nplanning and additional computation input; and (3) while supervised-learning-based policy updates result in\nsmoother learning curves, they may struggle in complex continuous control tasks due to their approximation\nof the continuous action space as a finite set of action samples. For future work, our algorithms can be further\nimproved by integrating advancements in offline MBRL and Bayesian RL, such as Bayesian RL methods\nthat do not rely on deep ensembles, techniques to address sparse rewards in MBRL, and more principled\napproaches to construct pessimistic MDPs beyond those based on ensemble discrepancy."}, {"title": "A BAMCP", "content": "Bayes Adaptive Monte Carlo Planning (BAMCP, Guez et al. (2013)) is a sample-based online planning\nmethod, aiming to find the action a* that approximately maximizes the expected return at a decision point\n(s,h) under the BAMDP. Its detailed pseudo code is shown as Algorithm 3. BAMCP has demonstrated\nsuccess in solving BAMDPs with large-scale discrete state and action spaces. Its key algorithmic ideas\ninclude: (1) applying MCTS with an efficient exploration strategy UCT (Kocsis & Szepesv\u00e1ri (2006))\nto the BAMDP in order to simulate the outcomes of different action choices; (2) utilizing root sampling\nto avoid frequent Bayesian posterior updates. Specifically, the UCT rule is used for selecting actions at\nnon-leaf nodes, i.e., a \u2190 arg max Q((s, h), x) + c\u221a,((s)), managing the tradeoff between exploration and\nexploitation. Root sampling refers to sampling the dynamics model only at the root node (i.e., 0 ~ b(\u00b7)) and\nnot adapting the belief b(\u00b7) according to the Bayes rule during the search process, of which the rationality is\njustified in the following lemma.\nLemma A.1. For all suffix histories h' of h, b(0|h') = b(0|h'). Here, b(0|h') is the true posterior probability\nof 0 at the decision point h', while b(0|h') is the probability of experiencing 9 at h' when using root sampling.\nProof. This lemma can be proved by induction.\nBase case: When h' = h, b(0|h') = b(0|h') = b(0).\nStep case:\nb(\\theta|has') = \\frac{P(has'|\\theta)P(\\theta)}{P(has')}\n= \\frac{P(h|\\theta)P_\\theta(s'|s,a)P(\\theta)}{P(has')}\n= \\frac{b(\\theta|h)P(h)P_{\\theta}(s'|s,a)}{P(has')}\n= Z b(\\theta|h)P_{\\theta}(s'|s, a)\n= Z b(\\theta|h)P_{\\theta}(s'|s, a)\n= Z b(\\theta|ha)P_{\\theta}(s's, a) = b(\\theta|has')\n(6)"}, {"title": "B Alternative Design Choices for Continuous BAMCP", "content": "The terms labeled in blue in Algorithm 1 have alternative design choices. Empirical comparisons among\nthese alternatives are reserved for future work.\nQ((s, h), x) in the exploration strategy, i.e., a \u2190 arg maxx\u2208C((s,h)) Q((s, h), x), could take various forms. For\ninstance, in (Cou\u00ebtoux et al. (2011); Guez et al. (2013); Lee et al. (2020)), Q((s,h),x) = Q((s,h), x) +\nCVN((s,h),x);\nLog(); in PUCT (Auger et al. (2013)), Q((s,h), x) = Q((s,h), x)+,((s))e(d), where e(d)\nis a schedule of coefficients related to the search depth d; in Sampled MuZero (a variant of MuZero\nthat can be applied in continuous action spaces (Hubert et al. (2021))), Q((s,h),x) = Q((s,h),x) +\n\u2713N((s,h))\n(x(s,h)) 1+N((s,h),x) C1 (C1 + log (N((sh))+c2+1)). Here, C, C1, C2 are hyperparameters,\nC2\n=\u03b2\u03c01-1/ is a\nsample policy defined upon the real policy \u03c0. In particular, at each decision point (s, h), Sampled MuZero\nwould sample M actions {a} from the distribution \u03c0\u00b9/7 and accordingly define \u1e9e(a|(s,h)) = \u2211i 1a\u2081=a/M,\nwhere T > 0 is a temperature hyperparameter. Thus, Sampled MuZero does not adopt progressive widening\nlike ours. Following BAMCP, we adopt the first definition of Q((s, h), x), though it could potentially be im-\nproved with other choices. In addition, as an implementation trick (Hamrick et al. (2021)), the Q estimates\nare usually normalized into Q \u2208 [0, 1] before being used to calculate Q as above. The normalized estimates\ncan be computed as Q((s, h), x) =Q((s,h),x)-Qmin, where Qmax and Qmin are the maximum and minimum Q\nQmax-Qmin\nvalues observed in the search tree so far.\nAs the planning/search result, Tret can take multiple forms. In (Guez et al. (2013); Sunberg & Kochenderfer\n(2018); Lee et al. (2020)), \u03c0ret((s, h)) = arg maxa\u2208C((s,h)) Q((s, h), a); in (Sampled) MuZero, \u03c0ret(a|(s,h)) =\nN((s,h),a)1/\n((s)) ((s,h),2)1/; in ROSMO (a variant of MuZero with improved performance in offline scenarios\n(Liu et al. (2023))), \u03c0ret(a|(s, h)) \u03b1\u03c0(\u03b1|(s, h)) exp(Q((s, h), a) \u2013 V((s, h))). Here, \u03c4\u2208 (0, 1] is a temperature\nparameter and decays with the training process, ensuring the action selection becomes greedier. We select\nthe second form for ret in Algorithm 1. This is because (1) as described in PUCT, the returned action\nshould be the most visited one, which is not necessarily the one with the highest Q value, and (2) ROSMO\nadopts one-step look-ahead rather than deep tree search at each root node, which does not align with our\napproach.\nAs for the conditions of double progressive widening, PUCT designs a and \u1e9e to be functions of the search\ndepth d, while UCT-DPW (Cou\u00ebtoux et al. (2011)) utilizes a different set of conditions: [KaN((s,h))] \u2265\n|C((s,h))|, [KsN((s,h),a)\u1e9e] \u2265 |C((s,h), a)|, where Ka, Ks, \u03b1, \u03b2 are all constant hyperparameters. When\nthe progressive widening condition for sampling the next state is not satisfied, either the least visited node in\nC((s, h), a) can be selected (following PUCT), or a random node can be sampled from C((s, h), a) following\na distribution proportional to the number of visits (following UCT-DPW). As shown in Algorithm 1, we\nfollow the designs of PUCT, but keep a and \u1e9e as constants for simplicity in hyperparameter fine-tuning.\nFinally, the condition for continuing the simulation procedure, i.e., N((s,h)) > 1, could potentially be\nreplaced with N((s,h),a) > 1 or N((s',hars')) > 0. These conditions indicate that the nodes (s,h),\n(s,h,a), and (s', hars') have been visited before, respectively. At the end of the simulation procedure, we\ncan either apply rollouts, i.e., simulating a single path until the end of an episode, to estimate the expected\nvalue for a leaf node (s, h), or directly use V((s, h)) as the estimation. The former approach is widely used\nin online planning algorithms (Guez et al. (2013); Sunberg & Kochenderfer (2018); Lee et al. (2020)), while\nthe latter is used in iterative frameworks like MuZero."}, {"title": "D Comparisons with Model-free Methods on D4RL MuJoCo", "content": "As a complement to Table 1, we compare our algorithms with a series of model-free offline policy learning\n(Chen et al. (2024)) methods. We include SOTA model-free offline RL methods: CQL (Kumar et al. (2020)),\nBEAR (Kumar et al. (2019)), and BRAC-v (Wu et al. (2019)). Additionally, we show the performance of\ndirectly applying SAC or Behavioral Cloning (BC, Chen et al. (2024)) to the provided offline dataset in the"}, {"title": "E Computation Cost of Sampled EfficientZero on D4RL MuJoCo", "content": "Here, we report the training time of Sampled EfficientZero on the D4RL MuJoCo tasks in Table 6. This\nset of experiments is run on a server with 40 Intel(R) Xeon(R) Gold 5215 CPUs and 4 Tesla V100-SXM2-\n32GB GPUs. Despite the substantial computational input, no corresponding performance improvement is\nobserved. We welcome researchers to use our released code for further hyperparameter fine-tuning and to\nimprove the results we have provided."}, {"title": "F Ablation Study on the Reward Penalty", "content": "To demonstrate the necessity of incorporating the reward penalty in offline MBRL, we conduct an ablation\nstudy by setting A in Eq. (5) to 0, resulting in ablated versions of our proposed three algorithms. The re-\nsults are presented in Table 7. First, the average performance of the algorithms with the reward penalty is\nconsistently better, demonstrating the importance of using reward penalties in offline MBRL to prevent the\noverexploitation of the learned world models (which can be inaccurate). Second, the supervised-learning-based algorithm (i.e., BA-MCTS-SL (X = 0)) is less affected by the absence of the reward penalty, compared\nto the policy-gradient-based methods. Notably, BA-MCTS-SL (X = 0) and BA-MCTS-SL achieve compara-\nble performance in the Hopper and Walker2d tasks. This shows an additional advantage of BA-MCTS-SL\nits reduced sensitivity to model inaccuracies. Lastly, there are instances where superior performance is"}, {"title": "G Details of the Tokamak Control Tasks", "content": "Nuclear fusion is a promising energy source to meet the world's growing demand. It involves fusing the\nnuclei of two light atoms, such as hydrogen, to form a heavier nucleus, typically helium, releasing energy in\nthe process. The primary challenge of fusion is confining a plasma, i.e., an ionized gas of hydrogen isotopes,\nwhile heating it and increasing its pressure to initiate and sustain fusion reactions. The tokamak is one of\nthe most promising confinement devices. It uses magnetic fields acting on hydrogen atoms that have been\nionized (given a charge) so that the magnetic fields can exert a force on the moving particles (Pironti &\nWalker (2005)).\nChar et al. (2024) trained a deep recurrent network as a dynamics model for the DIII-D tokamak, a device\nlocated in San Diego, California, and operated by General Atomics, using a large dataset of operational\ndata from that device. A typical shot (i.e., episode) on DIII-D lasts around 6-8 seconds, consisting of a\none-second ramp-up phase, a multi-second flat-top phase, and a one-second ramp-down phase. The DIII-D\nalso features several real-time and post-shot diagnostics that measure the magnetic equilibrium and plasma\nparameters with high temporal resolution. The authors demonstrate that the learned model predicts these\nmeasurements for entire shots with remarkable accuracy. Thus, we use this model as a \"ground truth\"\nsimulator for tokamak control tasks. Specifically, we generate a dataset of 725270 transitions for offline RL\nand evaluate the learned policy using this data-driven simulator.\nThe state and action spaces for the tokamak control tasks are outlined in Table 8. For detailed physical\nexplanations of their components, please refer to (Abbate et al. (2021); Char et al. (2023); Ariola et al.\n(2008)). The state space consists of five scalar values and six profiles which are discretized measurements\nof physical quantities along the minor radius of the toroid. After applying principal component analysis\n(Ma\u0107kiewicz & Ratajczak (1993)), the pressure profile is reduced to two dimensions, while the other profiles\nare reduced to four dimensions each. In total, the state space comprises 27 dimensions. The action space\nincludes direct control actuators for neutral beam power, torque, gas, ECH power, current, and magnetic\nfield, as well as target values for plasma density and plasma shape, which are managed through a lower-level\ncontrol module. Altogether, the action space consists of 14 dimensions. While for certain tasks, it is possible\nto prune the state and action spaces to reduce the learning complexity, we have chosen not to apply any\ndomain-specific knowledge in these evaluations for general RL algorithms. We reserve the domain-specific\napplications of our algorithms, which would require more domain knowledge and engineering efforts, as an\nimportant future work.\nWe select a reference shot from DIII-D, which spans 251 time steps, and use its trajectories of Ion Rotation,\nElectron Temperature, and \u1e9en as targets for three tracking tasks. Specifically, \u1e9en is the normalized ratio\nbetween plasma pressure and magnetic pressure, a key quantity serving as a rough economic indicator of\nefficiency. Since the tracking targets vary over time, we include the time step as part of the policy input.\nThe reward function for each task is defined as the negative squared tracking error of the corresponding\ncomponent (i.e., temperature, rotation, or \u1e9en) at each time step, and the reward is normalized by the\nepisode horizon (i.e., 251 time steps). Notably, for policy learning, the reward function is provided rather\nthan learned from the offline dataset as in D4RL tasks; and the dataset does not include the reference shot\nor any nearby, similar shots."}]}