{"title": "Semantic Role Labeling of NomBank Partitives", "authors": ["Adam Meyers", "Advait Pravin Savant", "John E. Ortega"], "abstract": "This article is about Semantic Role Labeling for English partitive nouns (5%/REL of the price/ARG1; The price/ARG1 rose 5 percent/REL) in the NomBank annotated corpus. Several systems are described using traditional and transformer-based machine learning, as well as ensembling. Our highest scoring system achieves an F1 of 91.74% using \"gold\" parses from the Penn Treebank and 91.12% when using the Berkeley Neural parser. This research includes both classroom and experimental settings for system development.", "sections": [{"title": "Introduction", "content": "Semantic Role Labeling (SRL), provides a way to represent semantic concepts via labeled predicate/argument pairs.\nFor example, an SRL analysis of the following three sentences include a patient relationship between the predicate: break and the argument glass. The 3rd sentence includes an additional agent relationship between the predicate break and the argument John.\n\u2022 The glass broke\n\u2022 The glass was broken\n\u2022 John broke the glass.\nSRL has become popular in linguistics and NLP following Jeffrey Gruber's dissertation (Gruber, 1965). His study focused on arguments of the main verb in a sentence (They/Agent bought/Predicate a car/Theme). Since then, the scope of semantic role labeling has widened to cover arguments of both verbs and nonverbs. See, for example, work on PropBank, Nombank, the Penn Discourse Treebank, FrameNet among other projects (Palmer et al., 2005; Meyers et al., 2004b; Fillmore and Atkins, 1998; Miltsakaki et al., 2004). Predicates can be not only verbs, but also nouns, subordinate conjunctions, adverbs or adjectives."}, {"title": "Partitive Nouns in an SRL Framework", "content": "SRL can model how different sentences are nearly equivalent semantically, as illustrated in Figure 1. Similarly, at the sentence level, the relations ARGO(eat, clam) and ARG1(eat, tourists) can be realized by the following sentences, all containing forms of eat, clam, and tourists:\n\u2022 The giant clam ate the tourists.\n\u2022 The tourists were eaten by the giant clam.\n\u2022 The tourists, who were eaten by the the giant clam, were very wealthy.\n\u2022 The giant clam, after eating tourists, left town.\n\u2022 The giant clam wanted to eat the tourists.\nThe particular labels used (like ARGO and ARG1) vary between frameworks, although we adopt the nomenclature of PropBank and NomBank.\nWe mark the nominal predicate with the label REL and arguments with argument labels like ARG1, placed at the end of the phrase, e.g., in Figure 2, example 1, there is a PART relation between the predicate components and the argument the index. In the example, the REL and ARG1 labels mark the predicate and the argument (via the head words). Equivalently, we can say that the relation: Part(component,index) holds. We also ignore the question of how the heads or extents of phrases are chosen. We will follow a convention in which the argument is referred to by its label, i.e., if a relation ARGO(eat,clam) holds, we can refer to the clam as the ARGO of eat.\nPartitive noun predicates quantify over their ARG1 so that the relation represents a group of ARG1s or a part of an ARG1. Subclasses include: quant, part, meronym, group, and share, as in Figure 2. Partitive nouns have several features in common. Partitive nouns tend to be transparent: this means that the ARG1 tends to act as a \"semantic\" head. For example, a variety of sandwiches is interpreted as an instance of sandwiches and thus edible, rather than an instance of variety and thus too abstract to eat. Coordinate conjunctions (CCs) like and and or are similar in this respect \u2013 the conjuncts, not the CCs are semantic heads. Thus the phrase the sandwich and the apple is an instance of foods (a generalization of the two conjuncts), not the coordinate conjunction and (the word that links the other words in the phrase together).\nThis paper is about several SRL systems for detecting ARG1s of partitive nouns."}, {"title": "Previous Work", "content": "Some previous work using NomBank SRL (Surdeanu et al., 2008; Haji\u010d et al., 2009) focuses on nominalizations (nouns related to verbs as in Figure 1) as part of a larger task that includes verbs and their arguments). Other work (Jiang and Ng, 2006) uses names of frames for non-nominalizations as features for machine learning. Our approach partitions annotation by classes of nouns that are not related to verb-frames. This approach is similar to FrameNet (Baker et al., 1998). Arguments of nouns sharing the same frame are treated like separate tasks. For example, FrameNet's contingency frame is shared by verbs, nouns and adjectives in the following examples:\n\u2022 Success/OUTCOME may depend/VB on available resources/DETERMINANT\n\u2022 Success/OUTCOME seems dependant/JJ on available resources/DETERMINANT\n\u2022 Success/OUTCOME will be a function/NN of the Available resources/DETERMINANT\nFrameNet Systems attempt to find sets of predicates and arguments belonging to particular frames. Shared Tasks such as ACE (Doddington et al., 2004) also follow this approach. Frame-like classes in NomBank are noun-centric. Figure 2 provides examples of the five partitive classes. In addition, there are 12 other non-nominalization frame-like classes in the NomBank dataset. (See figure 5 in Appendix A. We leave these to future work.) This paper provides a framework for such investigations.\nIn the NomBank (Meyers et al., 2004a) project, approximately 115,000 noun predicates were annotated, along with their arguments. The annotation scheme is compatible with the previous PropBank (Palmer et al., 2005) annotation of verbs and their arguments. Most subsequent automatic SRL systems (Jiang and Ng, 2006) treat NomBank propositions in a similar manner as PropBank. They use frames associated with each predicate, but they do not generalize over shared frames. For example, ARG1s of all 8000 distinct partitive predicate lemmas are assumed to form 8000 distinct classes, whereas our approach generalizes ARG1s of these 8000 predicate lemmas to a single class."}, {"title": "Previous SRL Systems", "content": "Most SRL systems fall into two categories. One approach is to develop a feature space from the input text based on linguistic analysis, and then extrapolate patterns via predictive modeling"}, {"title": "Research in a Classroom Environment", "content": "We used % and partitive tasks as assignments for three graduate NLP classes (Spring 2012, Spring 2022 and Fall 2023). The % task was assigned as one of several shared-task homework assignment during the first half of the semester. During the second half of the semester the partitive task was a final project task, the basis of a final paper and final presentation. The Fall 2023 version of the task is the basis of this article. Students were permitted to use any methods they wanted for the task. For the partitive task, they were free to use our baseline system as a starting point (results are in table 1 and the feature based 1 rows of table 2. Our final results include the incorporation of aspects of student systems, as discussed below."}, {"title": "Data and Evaluation Methodology", "content": "For each annotated NomBank predicate in a subset of the Penn Treebank 2 WSJ corpus (PTB), we created a representation of the sentence containing that word. The representation includes information from both PTB and NomBank. We created datasets for various classes of NomBank predicates. In this paper, we will focus on systems trained and tested on the subset of NomBank devoted to partitive nouns, leaving NomBank subsets for nouns with other frames for future work.\nWe represent each sentence containing a partitive noun predicate as a set of tuples, one tuple for each token in the sentence. Each tuple is on one line in the data file, with blank lines between sentences. This format is an extension of the CONLL 2000 format (Tjong Kim Sang and Buchholz, 2000). We eliminated problematic, but rare types of examples from the data. Since it is rare for a sentence to contain two partitive nouns (less than 1% of all sentences with partitives), we only kept the first instance of the selected predicate type (partitive) in any given sentence. We also eliminate rare cases in which NomBank's tokenization is at odds with the PENN Treebank's tokenization, e.g., the single PTB token warehouse-club is divided into two tokens in NomBank: club is a group noun (a subtype of partitive) and warehouse is its ARG1."}, {"title": "Organization of Datasets", "content": "In the Penn Treebank, the % sign is a noun (just like the word \"percent\"). It is a partitive and it occurs with an ARG1 nearly 3,000 times, making it the most frequent nominal predicate in the corpus. These instances of % are of course part of the approximately 13,000 partitive instances, the most frequent shared frame in NomBank.\nFor our experiments, we created two datasets, one consisting of % instances and a larger one consisting of partitives. We divided each set into three subcorpora: training data consists of instances from PTB directories 02 to 21; development data comes from directory 24 and test data comes from 23 (a standard data split for this corpus). The training corpus contained 1K partitive ARG1s and 2225 % ARG1s; Dev contained 370 and 87; Test contained 550 and 150.\nWe would expect most systems to do better on the % subcorpus than the partitive subcorpus, because the former is based on arguments of the same predicate, while the latter is based on a class of predicates with some elements of meaning in common. Thus the students first made systems for the % task and then for the partitive task.\nFor initial development, we evaluate systems only on gold versions of tasks (manual parses). However, for our final results, we evaluate on both gold and non-gold versions (parser output)."}, {"title": "Evaluation Method", "content": "We report precision, recall and f-scores of ARG1 in tables 1 and 2. There is exactly one ARG1 for each sentence, as per the data format. For example, in Figure 4, Output is labeled ARG1.For most cases, we choose the head noun of the ARG1, e.g., if the ARG1 phrase is The big prize, a correct match requires that the system identify prize as the ARG1. For proper noun phrases, e.g., Exxon Mobil Corp., we assume that any of the name words (NNP) is correct (Exon or Mobil or Corp.)."}, {"title": "Baseline System", "content": "We created a baseline system as both a proof of concept and a starting point for student systems. Through error analysis of the baseline system, the class may better understand the task. Furthermore, it is possible that an improved system could be created both by modifying the baseline system based on features from student systems and/or ensembling it with student systems. First we used Sklearn's Adaboost machine learning algorithm (Freund and Schapire, 1995), with features generated for each token in each sentence. The model was generated from our training corpus and it was used to predict the ARG1 (true or false) status of tokens in the development and test corpora. All experimentation with different features was measured against the development set. Prior to submission of this paper, we ran one system on the test corpus, the one that performed best on the development set."}, {"title": "Baseline Features and their Motivations", "content": "In this section, we enumerate the features used in the baseline system. These features are motivated by our linguistic understanding of SRL with noun predicates in general, and partitives in particular. This includes both: information that characterizes typical ARG1s and predicate nouns (the head words, POS classes, BIO tags, nearby words, frame classes of predicate nouns, etc.) and information about how ARG1s are related to the predicates (token distance, tree distance, etc.). The baseline system provides a testing ground for such features."}, {"title": "Simple ARG1 Features", "content": "Simple ARG1 features include the word (head of ARG1), its POS tag, its BIO tag, and these same features for the two words following and preceding the head. Following Firth (Firth, 1957) and subsequent NLP research, we are assuming that words are, in part, defined by their context. Thus features of neighboring words are assumed to bear on meaning of the current ARG1."}, {"title": "Embedding-based Features", "content": "Word embeddings, generated from large corpora, are also compatible with Firth's principle because each word is defined based on a neural network model of the context of that word. We use pre-trained embeddings from SPACY's pre-trained en_core_web_md tok2vec embeddings which was from a medium sized (43 mb) corpus. For each example ARG1 in the training corpus, we calculate a total of 10 embeddings: two types of embeddings for each of five different n-grams. The five n-grams include: the head ARG1 word itself, and the forward and backward bigrams and trigrams. For example, for the sentence: The consumer price index/ARG1 rose five percent/REL., five n-grams are generated: 1. consumer price index, 2. price index, 3. index, 4. index rose, and index rose five. For each n-gram, we calculate two embeddings: the normal embedding for that n-gram and and the embedding of all words in the sentence with the n-gram removed. We will refer to the second type of embedding as a slash embedding.For each of these 10 types of embeddings, we calculate an average embedding based on the training file. For each ARG1 in the corpus, we find how similar (cosine similarity) the 10 embeddings are to these averages. These similarities correspond to 10 feature values."}, {"title": "Predicate Class Features", "content": "The specific partitive class of the noun, as well as any other NomBank class the predicate is labeled with, e.g., GROUP, MERONYM, PART, QUANT, SHARE, BOOK-CHAPTER, BORDER, CONTAINER, DIVISION, ENVIRONMENT, INSTANCE-OF-SET, NOM, NOMADJ PART-OF-BODY-FURNITURE-ETC, SHARE, WORK-OF-ART. These labels are taken from the lexical entries of each predicate. The labels for well-defined classes (the classes exemplified in Figures 2 and 5) are used consistently according to the aforementioned specifications. The remaining predicate labels are chosen by the annotators during the creation of NomBank. None of these features are used for the % task, since these values would be the same for all instances of %."}, {"title": "Path Features", "content": "Simple path features are based on token distance: number of tokens between predicate and ARG1, tokens between support and ARG1. Positive and negative values reflect the ARG being before or after the support verb or nominal predicate.\nIn the initial baseline systems (Feature based 1 and 2 in Table 2, we approximate paths in the parse tree, based on sequences of BIO tags, collapsing BI* to phrase sequences and including the direction. For example, consider the following sequence with BIO tags from a predicate to its ARG1:\n20\\B-NP %\\I-NP of\\B-PP the\\B-NP pie\\I-NP\nThe path feature generated from this sequence is:\nright_NP_PP_of_NP_NOUN\nThis characterizes the path from a head noun to the object of a complement of the preposition of. Thus different left and right tags characterize paths between predicates and arguments, and between support verbs and arguments. These path features are derived by essentially collapsing sequences of BI* into phrases, e.g., B-NP I-NP I-NP \u2192 NP. The path features encode simulated paths as type 1 path features in order to differentiate them from type 2 path features. We will henceforth refer to this method as Path-heuristic 1.\nFor feature-based system 3, we use a parse tree for generating paths between words. We developed Path heuristic 2, based on observations about sentence structure. This heuristic represents classes of observed grammatical relationships between: the predicate noun and the (head of) ARG1; and also between a support verb and the (head of) ARG1. Path heuristic 2 labels one of four paths for compatible (Manning et al., 2014) parse trees:"}, {"title": "One Hot Encoding", "content": "The results in table 1 correspond to versions of feature based systems in table 2. As we develop our feature based system, we consider different encoding schemes for our categorical features. Feature based system 2, uses one hot encoding over ordinal encoding used in Baseline system (feature based) 1. In table 2, we suspect the jump in F-score from system 1 to system 2 is associated with the use of better path features by virtue of one hot encoding. The embedding based numeric features are not subject to change based on encoding schemes.\nConsider a path feature which has n distinct values corresponding to n paths. Numbering different paths from 0 to (n-1) may not have allowed the model to yield predictive utility (ARG1 vs not-ARG1). The model has to predict the first path as 0, the second path as 1, and so on. By representing the path features in terms of one hot encoding, the model may be able to better use the path features for downstream prediction. We see this empirically, in our F-scores. Imposition of an ordinal relationship between non-ordinal entities like parts of speech tags would imply an ontology which may not exist. Incorporating ordinal information in our model would not be the most appropriate definition of the features since it would entail us putting in additional assumptions not corroborated with linguistic phenomena. Feature based system 3 expands on Feature based system 1. It explicitly uses a parser based system via Path Heuristic 2. It uses one hot encoding rather than ordinal encoding. It also uses the features from previous systems."}, {"title": "Baseline Results", "content": "The baseline system 1 results on the Gold task are discussed here. As illustrated in Table 1, all of the feature types contributed to the results on the development set. Thus removing a type of feature resulted in lower precision and/or recall, optimizing for an F1 score. The development sets for % and partitive included, respectively, 87 and 372 instances of ARG1s. The training corpus had 2225 and 9987 instances respectively. As per section 6, we only ran a limited set of systems on the test set, including the ALL system from table 1 (50 instances of % and 555 instances of partitives)."}, {"title": "Baseline System Modifications", "content": "Building on the baseline features, student group 16 uses a random forest (Breiman, 2001) in order to predict ARG1s based on the same input features as the baseline. They also do a grid search across models in order to obtain an estimator. For the best performing random forest estimator, they evaluate the relative importance of features. Features representing the distance from the predicate and embedding distances are seen to have a higher predictive importance. The f-measure increases slightly to 64.41 (percent) and 62.72 (partitive).\nStudent Group 15's system achieved f-measures of 82.91 (% task) and 77.46 (partitive task)."}, {"title": "Transformer Based Systems", "content": "Student group 2 used a multi-layer-perceptron classifier with BERT (Devlin et al., 2019) embeddings, achieving 91.86 F-score for the % and 79.2 for partitives. Student group 9 uses DistilBert (Sanh et al., 2019) for fine-tuning, with tokens as features, achieving an 88.6 F-score on the partitive task. They report a 2.6 % F-score improvement using DistilBert vs Bert. Student group 10 uses a BILSTM (Hochreiter and Schmidhuber, 1997) with similar features as baseline system 1, plus fasttext word embeddings. They report an F-score of 78.54 for the % task."}, {"title": "Deep Learning Using Linguistic Features", "content": "Student group 4 studied the integration of linguistic features with the representational capacity of attention based deep neural networks (Wu et al., 2018) (Sachan et al., 2021). Works such as (Clark et al., 2019) (Tenney et al., 2019) postulate that pre-trained large language models such as BERT implicitly learn to represent syntactic information without any inductive bias. Student group 4 considered that engineered features based on linguistic considerations could be of utility in predictive modeling for our task. This is corroborated by our baseline systems which are based on linguistics oriented feature engineering. They formulate the learning of linguistic features based on POS tags, BIO tags and directed parse tree distances of a word to the predicate as an auxiliary task performed by a separate head. This auxiliary head has a shared base representation with the BERT model which performs the downstream task of predicting semantic roles. Since the base representation is shared, the model is pushed to learn representations which can perform these tasks simultaneously, potentially enable the learning of better representations for the downstream task. This approach was used for deep learning system 2 in table 2 and achieved higher results than deep learning system 1."}, {"title": "Ensembling", "content": "Based on principles of multi-view learning and ensemble learning (Sun, 2013; Dong et al., 2020), we develop an ensemble model. Governed by linguistics, an underlying data-generating process produces sentences along with their predicates and corresponding semantic roles as input, output pairs. For every model, as we make a choice of representation to define the input and output data, we are modeling a view of the data-generating process. The performance of a model is proportional to the potency of a view to faithfully represent this process. It can be of benefit to construct a model which aggregates/processes information obtained from multiple views.\nOur ensemble system consists of a purely deep learning component and a purely feature-based component. The deep learning component consists of a BERT-based model (Devlin et al., 2019) that uses distributed word vector representations as input. The feature-based component consists of an Adaboost model(Freund and Schapire, 1995) that uses the features motivated by our baseline system as input. We make predictions on semantic roles based on the aggregation of outputs of a BERT-based model and a feature-based model. We develop a voting scheme for deriving the ensemble output based on a weighted aggregation of the outputs of the two models. We learn the weights adaptively as we train the ensemble model on the data. (Jacobs et al., 1991; Eigen et al., 2014; Bishop, 2006)."}, {"title": "Results Summary", "content": "Table 2 lists the results on the gold test set. Table 3 runs these same systems on the non-gold test set, providing insight to how our systems would perform on a realistic pipeline, beginning with raw text. Appendix B provides additional visual comprehension of the results. Feature based system 1 is our baseline system, with feature engineering done based on linguistic considerations. Feature based system 2 modifies the baseline system to use one hot encoding instead of ordinal encoding for feature generation. It also includes additional features denoting paths between a word in the sentence and the predicate. For example, a measure of the path distance to the predicate is considered based on the numbers of BIO chunks in between the predicate and a concerned word. One hot encoding gives concrete improvements, as initially demonstrated in a student system (Section 6.2). Along with type 1 path features, Feature based system 3 incorporates type 2 path features (section 6.1.4).\nInspired by student work, deep learning system 1"}, {"title": "Concluding Remarks", "content": "We observed that a variety of techniques contributed to our best result: traditional ML features, deep learning with/without linguistic features and multi-view ensembling. A cooperative shared task turned out to be an excellent research model. We demonstrated that partitive nouns, the largest frame-like class of Nombank nouns provide a high-performing testbed for SRL, and possibly related tasks, like event and relation extraction. We intend to make our dataset available for research purposes.\nWe may explore avenues of further research, including: meta learning (Finn et al., 2017) across noun categories; parse tree path embeddings (Roth and Lapata, 2016); syntax infused models (Sachan et al., 2021); and in-context learning with large language models (Brown et al., 2020)."}, {"title": "Limitations of Our Approach", "content": "Partitives are possibly the simplest NomBank noun class. They usually are accompanied by one argument and rarely occur with two. Their ARG1s have few semantic limitations \u2013 they need to be measurable by the partitive predicate, e.g., a pound of meat is OK, but a pound of sincerity is semantically ill-formed. We would expect that arguments of other NomBank noun classes (Appendix A) would be more difficult to classify. Systems would achieve lower scores and may require other approaches.\nFurthermore, the WSJ corpus represents the financial news register of 1990s English. Amounts in general and partitives in particular may be idiosyncratic to these data. Running our system and analyzing results for other corpora may provide further insight on semantic roles for noun predicates."}]}