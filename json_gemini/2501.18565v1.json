{"title": "BounTCHA: A CAPTCHA Utilizing Boundary Identification in AI-extended Videos", "authors": ["LEHAO LIN", "KE WANG", "MAHA ABDALLAH", "WEI CAI"], "abstract": "In recent years, the rapid development of artificial intelligence (AI) especially multi-modal Large Language Models (MLLMs), has\nenabled it to understand text, images, videos, and other multimedia data, allowing Al systems to execute various tasks based on\nhuman-provided prompts. However, AI-powered bots have increasingly been able to bypass most existing CAPTCHA systems, posing\nsignificant security threats to web applications. This makes the design of new CAPTCHA mechanisms an urgent priority. We observe\nthat humans are highly sensitive to shifts and abrupt changes in videos, while current Al systems still struggle to comprehend and\nrespond to such situations effectively. Based on this observation, we design and implement BounTCHA, a CAPTCHA mechanism that\nleverages human perception of boundaries in video transitions and disruptions. By utilizing Al's capability to expand original videos\nwith prompts, we introduce unexpected twists and changes to create a pipeline for generating short videos for CAPTCHA purposes.\nWe develop a prototype and conduct experiments to collect data on humans' time biases in boundary identification. This data serves\nas a basis for distinguishing between human users and bots. Additionally, we perform a detailed security analysis of BounTCHA,\ndemonstrating its resilience against various types of attacks. We hope that BounTCHA will act as a robust defense, safeguarding\nmillions of web applications in the AI-driven era.", "sections": [{"title": "1 Introduction", "content": "CAPTCHA [28, 61], an acronym for \"Completely Automated Public Turing test to tell Computers and Humans Apart,\" is\na type of test used to verify whether an online user is a human or a bot. As such, CAPTCHAs are sometimes referred to\nas \"reverse Turing tests\" [73]. Users must complete a given task and submit the result, and only after being verified as\nhuman can they proceed with further actions within an online application. Typically, CAPTCHA pop-up windows that\nappear when users attempt to log in or make network requests too quickly. Login page intercepts are designed to hinder\nbots from the start, while rapid network requests may be interpreted as potential bot activity imitating human actions.\nThe primary purpose of CAPTCHAs is to prevent web crawlers and bots [46] that simulate human behavior from\nsending frequent requests, which can make the operational data of web services more authentic, resistant to Sybil\nattacks [19], and place significant strain on a server's network. A large volume of such requests may be perceived as\na denial-of-service (DoS) attack [39], and attackers with multiple nodes may launch a distributed denial-of-service\n(DDoS) attack [24, 42]. This disrupts the normal use of web applications for legitimate human users. Additionally, bots\ncan attack social media content by promoting keywords to get them trending on X (previously Twitter), thus creating\nfalse trends that can reach a broad audience [21, 29]. The situations above are not only happened in the centralized"}, {"title": "2 Related Work", "content": "We have categorized the current mainstream types of CAPTCHA into four groups: text-based, image-based, 3D &\ngamified, and video CAPTCHAs."}, {"title": "2.1 CAPTCHAS", "content": null}, {"title": "2.1.1 Text-based CAPTCHAs.", "content": "Text-based CAPTCHAs have various forms of representation, shown on the left part\nof Figure 1. These can be broadly categorized into two types: character-based and digit-based. In the character-based\ntype, distorted text is generated for users to recognize and input the corresponding characters. The digit-based type, in\naddition to requiring users to recognize and input numbers, may also involve simple math arithmetic operations [32]. The\ncorrectness of the submitted answer is used to verify whether the user is human. Some websites that specifically serve\ncertain countries or regions use their native language as elements [2, 75, 82, 92]. By overlaying text onto background\nimages after applying filters, some provide a word or short phrase and ask the user to click on the corresponding\ncharacters in the image in the correct sequence to verify users. [89] proposes SS-CAPTCHA, which leverages the human\nability to recognise strangeness in translated sentences to detect malware."}, {"title": "2.1.2 Image-based CAPTCHAs.", "content": "Some of these showcases are presented in the middle and right sections of Figure 1.\nCommon image-based CAPTCHA mechanisms include multiple image selection, jigsaw puzzles, and image position"}, {"title": "2.1.3 3D & Gamified CAPTCHAs.", "content": "Due to considerations of compatibility of 3D object rendering across different\nbrowsers and the complexity of generating 3D content, 3D CAPTCHAs often combine features of both text-based\n[60] and image-based [65, 83] methods. In Figure 2, (a) and (b) illustrate text-based 3D CAPTCHAs, while (c) and (d)\ndemonstrate image-based 3D CAPTCHAs. Additionally, another mechanism, known as gamified CAPTCHAs [58],"}, {"title": "2.1.4 Video-based CAPTCHAs.", "content": "Similarly to our present work, there have been some studies utilizing video as an\nelement in CAPTCHAs. For instance, [45] uses user-provided descriptions of videos to find relevant labels and tags for\nverification purposes. [63] improves upon this method by replacing text descriptions with a selection-based approach.\n[4] combines text-based CAPTCHAs with video backgrounds, requiring users to input the shown text."}, {"title": "2.1.5 Drawbacks & Attacks.", "content": "Although the various CAPTCHA forms mentioned above provide varying degrees of\nprotection for web applications, the advancement of AI capabilities has made these defenses increasingly fragile and\nsusceptible to being bypassed [25]. For text-based CAPTCHAs, besides traditional Optical Character Recognition\n(OCR) [11] attacks, even relatively simple machine learning models or neural network architectures [76], such as Support\nVector Machine (SVM), K-nearest Neighbors (KNN), and Convolutional Neural Network (CNN), can successfully carry\nout attacks [12, 71, 81]. For image-based CAPTCHAs, more complex neural network models, such as ResNet [30] or\nVision Transformer (ViT) [18], can be employed for recognition. Additionally, techniques like edge detection [40, 56],\nobject detection [64], and pixel-level segmentation [13, 52] can be applied to analyze the image, followed by a user\ninteraction simulation programmatically to bypass these defenses [3, 22, 69, 74]. For 3D & gamified CAPTCHAs,\nin addition to traditional attack methods [59], since the challenge is still displayed in the form of images on the\nuser interface, attackers can resort to taking screenshots and treating it as an image-based CAPTCHA. They can\nthen leverage multi-modal LLMs to better understand the task and execute the attack [7, 27, 50] For video-based\nCAPTCHAs, there are already numerous vision language models (VLMs) [5, 55, 95, 98] capable of interpreting video\ncontent and generating textual descriptions. Moreover, this type of CAPTCHA faces challenges related to language\ninternationalization and localization, which limits its usage to specific regions and imposes linguistic barriers, as well\nas difficulties in understanding complex tasks for users in different education levels."}, {"title": "2.2 Video Generative Models", "content": "Video generation models possess the ability to modify the generated content in accordance with user input, which is\noften known as prompts. Based on the kind of prompt, these models can be classified as text-to-video (T2V) models\n[15], image-to-video (I2V) models [68], and video-to-video (V2V) models [85]. Beyond the straightforward generation\nof videos, some I2V and V2V models enable fine-tuning, modification, editing, and even the stitching of generated\nvideos by utilizing additional text prompts [10, 16, 35, 72].\nIn addition to explorations in academic research, video generation technology has been successfully applied in\ncommercial contexts and has reached a relatively mature stage. Companies such as Kling, Runway, Pika, and Stability AI\nsupport text-to-video (T2V) and image-to-video (I2V) generation and provide corresponding API interfaces. According\nto a report [77] and our practical experience, Kling exhibits superior overall performance in generating videos from\nimages, with more stable output quality. Consequently, we have decided to adopt Kling as the model for our subsequent\nvideo generation expansions."}, {"title": "2.3 Video Understanding", "content": "In the realm of video understanding tasks, video foundation models (VFMs) have embarked on promising endeavors in\nexploring model architectures, as evidenced by references such as [47, 79, 87]. Additionally, they have made significant\nprogress in learning paradigms, [49, 84, 86, 90, 94] serving as examples. As for the latest advancements in video\nunderstanding, Tarsier [80] emerges as a prominent family of large-scale video-language models. These models are\nmeticulously designed to craft high-quality video descriptions. Notably, the model, along with its code and data,\nhas been made publicly accessible for inference, evaluation, and deployment purposes. As of September 3rd, 2024,\nextensive evaluation results have demonstrated the remarkable capabilities of Tarsier in general video understanding. It\nhas achieved state-of-the-art (SOTA) performance across six open benchmarks, solidifying its position as a leading\napproach to understand videos. Consequently, in our current research endeavor, we have decided to utilize this model\nto participate in the prompt generation of AI-extended videos."}, {"title": "3 Video Data Preparation (RQ1)", "content": "We collected 25 raw videos which are short (3-10 seconds) without significant changes in content from Pexels. \u03a4\u03bf\navoid the unnatural plot twists commonly found in videos (such as anime) and other films as in the science fiction and\nfantasy genres, which could lead to user misjudgment, we do not select such videos. Instead, we choose those that are\ncaptured with real cameras, without any special effects or filters that could cause strange visual distortions.\nMoreover, to ensure data diversity while avoiding overwhelming participants in the subsequent user study, which\ncould lead to distorted data, we limit the number of selected videos. Within this constraint, we ensure that the videos\nfeature a variety of shots, scenes, backgrounds, subjects, and other elements to make the collected data as representative\nas possible. In this section, we aim to answer RQ1 based on BounTCHA's data preparation. Next, we describe details of\nthe production pipeline for CAPTCHA video and the video properties before and after processing."}, {"title": "3.1 Video Generated Pipeline", "content": "The pipeline used for CAPTCHA video production is shown in Figure 3. We first use Tarsier to understand the video\ncontent and output it as a text description. This description, along with the final frame of the video, is then input into\nGPT-40 to generate prompts for the video generation AI model. The prompts, along with the final frame of the raw\nvideo, are input into the video generation AI to produce the AI-extended part. Next, we merge the raw video with the"}, {"title": "3.2 Prompts", "content": null}, {"title": "3.2.1 Prompts for Content Extraction.", "content": "We provide two prompts to assist with video understanding to extract the\nvideo's content:\n\u2022 Description: Describe the video in detail, covering all events, actions and camera motions. Also, describe the\ncharacters' appearance and the background.\n\u2022 Keywords: Summarize the video with 5 keywords.\nThe video description serves to help the subsequent LLM understand the content of the video, while the five keywords\nenable the LLM to identify the key elements within the video. This information guides the model in determining which\nfactors should be altered or remain unchanged for the transitions."}, {"title": "3.2.2 Prompts for Video Generation Prompts.", "content": "After obtaining the description and five keywords of the raw video,\nwe incorporate this information into the subsequent video generation prompts, along with the last frame of the raw\nvideo. These inputs are then fed into the MLLM to generate a completely new video narrative. This process ensures that\nthe generated prompts significantly differ from the storyline and visuals of the raw video, while still being grounded in\nits foundational elements. Subsequently, the prompts used for video generation are input into the video generative AI\nto contribute to the creation of the AI-extended video."}, {"title": "3.3 Video Quality & Shift Cutting", "content": "To ensure that users can complete the verification process efficiently, our videos range from a minimum length of 5.12\nseconds to a maximum of 14.4 seconds. Prior to compression, the smallest video was 5 MB; however, after compression,\nthe largest video did not exceed 350 KB. To simulate a realistic CAPTCHA usage environment, we utilize the compressed\nvideos for subsequent experiments. The sizes and lengths of the videos before and after compression are illustrated in\nFigure 4. Considering the practical context, to minimize the time users spend on CAPTCHA verification, we selected\ncomposite Al-extended videos with durations ranging from 5 to 15 seconds as our experimental data. Due to the\ninherent compression in the MPEG-4 format, the sizes of the original and compressed videos do not increase linearly\nwith the video length. Some minor discrepancies in size and trends were observed; however, the overall trend in video\nsizes remains consistent. The average size of the compressed videos used is 257.68 KB, which is an acceptable size that\nwill not place significant strain on the network bandwidth.\nIn addition to the operations mentioned above, we can also use video cutting to adjust the boundary position\nthroughout the entire video duration. This allows the creation of multiple copies of the same video with different\nboundary positions. In real-world scenarios, this approach helps expand and enhance datasets, enabling them to handle\nincreased traffic and calls."}, {"title": "3.4 Video Production Time Consumption", "content": "Figure 5 illustrates the time consumption in various stages of video preparation, such as video understanding for 21.7\nseconds, prompt generation for 5.4 seconds, video generation for 521.2 seconds and video compression for 3.1 seconds."}, {"title": "4 BounTCHA Prototype", "content": "The system prototype of BounTCHA is shown in Figure 6. The video playback area is at the top, while the user\ninteraction area is at the bottom. Users can determine the boundary of the composite video by clicking the Play/Pause\nbutton and can also drag the slider to seek and adjust the playback. Additionally, users can see the total video length\nand the current playback time. When users drag the slider to the position they believe marks the boundary, they need\nto click the Submit button to complete the CAPTCHA. The system then transmits the video ID and the user's confirmed\nboundary time to the server backend, which records the user's boundary time and compares it with the actual boundary.\nThe architecture of the prototype is as follows: the frontend is built using Vue.js, the backend utilizes Python with\nFastAPI, and the database is powered by MongoDB.\nIn the experimental part of this work, we will gather statistics on the boundaries confirmed by users to determine\nthe effective range of human judgment. In real CAPTCHA scenarios, the server will return the comparison result with\nthe actual boundary. Trials that fall within the effective range are likely to have been completed by a human rather\nthan a robot."}, {"title": "5 Studies on Human Performance (RQ2)", "content": "In this section, we conduct a user study based on the BounTCHA system to explore the discrepancy between users'\nperceived boundaries of AI-extended videos and the actual boundaries."}, {"title": "5.1 Experiment", "content": null}, {"title": "Setup.", "content": "For ease of both offline and online experiments, we deployed the BounTCHA prototype on a remote server.\nThe server is configured with 2 CPUs and 2GB of memory, running the Ubuntu 22.04.3 LTS operating system.\nEthics. The school's Institutional Review Board (IRB) reviewed and approved this human-subjects research. In order\nto preserve the double-blind review process, the approval number will be provided in the camera-ready version.\nParticipants. We recruited 186 participants for the experiment by posting flyers on campus and advertising on\nsocial media (comprising 83 participants in-person and 103 participants via ZOOM for online experiments). Participants'"}, {"title": "Procedure.", "content": "As shown in Figure 8, at the beginning of the experiment, we first explained the objectives to the\nparticipants and demonstrated how to operate our experimental system. We informed the participants that the first\npart is the original video, while the second part is the extended video. Once the participants became familiar with the\nsystem, they sequentially completed a Boundary selection task for 25 videos (in random order). The total completion\ntime for the 25 video experiments was approximately 10 minutes."}, {"title": "5.2 Results", "content": "Figure 9 shows the results of the experiment. All the data of time bias falls between -1.6s and 2.6s, and it follows a\nnormal distribution. Therefore, we can increase the difficulty of BounTCHA by adjusting the significance level to\nnarrow the time range. In Figure 10, we show the time bias range where the confidence level varies from 0.5 to 0.95,"}, {"title": "6 Security Analysis (RQ3)", "content": "We conduct a security analysis based on three attack methods, namely the random attack, the database attack, and the\nmulti-modal LLM attack to answer RQ3. We denote the full duration length of the video as L, the attack time bias $x \\in X$,\nthe mean value of the time bias as $\\mu$, the standard deviation as $\\sigma$, the significance level as $\\alpha$, the two-tailed confidence"}, {"title": "6.1 Random Attack", "content": null}, {"title": "6.1.1 Uniform Distribution Attack.", "content": "We assume that attackers only know the video length L and use uniform random\n$X \\sim U(0, L)$ to attack BounTCHA. Thus, the attack success probability is\n$\\newline P(X \\in [\\beta_1, \\beta_2]) = \\frac{\\int_{\\beta_1}^{\\beta_2} dy}{L} = \\frac{\\beta_2 - \\beta_1}{L} = \\frac{2\\sigma \\cdot ppf(1 - \\alpha/2)}{L}$", "latex_eq": "\\newline P(X \\in [\\beta_1, \\beta_2]) = \\frac{\\int_{\\beta_1}^{\\beta_2} dy}{L} = \\frac{\\beta_2 - \\beta_1}{L} = \\frac{2\\sigma \\cdot ppf(1 - \\alpha/2)}{L}"}, {"title": "6.1.2 Truncated Normal Distribution Attack.", "content": "We assume that attackers only know the video length L and use\nthe two-tailed truncated normal distribution random to attack BounTCHA, where $\\mu' = \\frac{L}{2}$, and its probability density\nfunction (PDF) is\n$\\newline f(x; \\mu', \\sigma', 0, L) = \\begin{cases}\n\\frac{1}{\\sigma'} \\frac{\\phi(\\frac{x - \\mu'}{\\sigma'})}{\\Phi(\\frac{L - \\mu'}{\\sigma'}) - \\Phi(\\frac{0 - \\mu'}{\\sigma'})} , & 0 \\leq x \\leq L \\\\\n0, & otherwise\n\\end{cases}$\n$\\newline \\phi(x) = \\frac{1}{\\sqrt{2\\pi}} exp(-\\frac{x^2}{2}), \\Phi(x) = \\frac{1}{2} (1 + erf(x/\\sqrt{2}))$\nand $f = 0$ otherwise, where erf() is the Gauss error function\n$\\newline erf(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z exp(-t^2) dt$", "latex_eq": "\\newline f(x; \\mu', \\sigma', 0, L) = \\begin{cases}\n\\frac{1}{\\sigma'} \\frac{\\phi(\\frac{x - \\mu'}{\\sigma'})}{\\Phi(\\frac{L - \\mu'}{\\sigma'}) - \\Phi(\\frac{0 - \\mu'}{\\sigma'})} , & 0 \\leq x \\leq L \\\\\n0, & otherwise\n\\end{cases}"}, {"latex_eq": "\\newline \\phi(x) = \\frac{1}{\\sqrt{2\\pi}} exp(-\\frac{x^2}{2}), \\Phi(x) = \\frac{1}{2} (1 + erf(x/\\sqrt{2}))"}, {"latex_eq": "\\newline erf(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z exp(-t^2) dt"}, {"title": "6.2 Database Attack", "content": "Inspired by the analysis in the work [65], we discuss scenarios where an attacker has partial knowledge of the video\ndatabase. For example, the attacker might have limited access to certain videos or some understanding of the general\nboundary distribution in the video. This consideration allows us to assess the robustness of the proposed BounTCHA\nsystem and identify potential vulnerabilities under such informed attack scenarios.\nSince we utilize a video cutting method that alters video boundary positions, a single video can have multiple\nvariants. The set of all variants from one single video is referred to as \"group\". The entire video database is composed of\nseveral groups of such variants. Therefore, a BounTCHA video may fall into one of the following three scenarios: (1)\nthe attacker knows the video and its boundary; (2) the attacker does not know this particular video, but is familiar with\nother variants in the same group and their boundaries; or (3) the attacker is unfamiliar with both the video and any\nother variants within the same group, as well as their boundaries.\nIn these three scenarios, the attacker's success probability differs. By comparing frames of the video, the attacker\ncan determine whether the video is fully known. If the video is known, the attacker can leverage existing boundary\ninformation to successfully launch an attack. For unknown videos, the attacker can only rely on guesswork. However,\nwhen the attacker has knowledge of other variants within the same group, she/he can rule out certain possibilities,\nthereby increasing the likelihood of a successful guess.\nWe denote the total number of groups as M, with m groups containing at least one variant known by the attacker.\nEvery video group i has Ui variants, where i \u2208 {1, ..., M}. In the i-th group, the number of known videos is ui, and yi\nrefers to the attack success probability. Additionally, we use yo to represent the success probability that is fully based\non guesswork.\nThe probability $\\sum_{i=1}^M \\gamma_i$ of the bot successful attack is\n$\\newline P = \\frac{\\sum_{i=1}^m u_i \\gamma_i}{\\sum_{i=1}^M U_i} +  \\frac{\\sum_{i=m+1}^M (U_i - u_i) \\gamma_i}{\\sum_{i=1}^M U_i} + \\frac{Y_0}{\\sum_{i=1}^M U_i}$", "latex_eq": "\\sum_{i=1}^M \\gamma_i"}, {"latex_eq": "\\newline P = \\frac{\\sum_{i=1}^m u_i \\gamma_i}{\\sum_{i=1}^M U_i} +  \\frac{\\sum_{i=m+1}^M (U_i - u_i) \\gamma_i}{\\sum_{i=1}^M U_i} + \\frac{Y_0}{\\sum_{i=1}^M U_i}"}, {"title": "6.3 Multi-modal LLM (MLLM) Attack", "content": "We tested the recognition capabilities of two open-source MLLMs, Tarsier [80] and MiniVPM-V 2.6 [90], for the boundary\nof AI-extended Videos. Tarsier is at the state-of-the-art (SOTA) in multiple video question answering benchmarks.\nMiniVPM-V 2.6 outperforms commercial closed-source models such as GPT-4V, Claude 3.5 Sonnet, and LLaVA-NeXT-Video-34B in Video-MME performance.\nWe conducted three rounds of tests on the two models (Tarsier and MiniVPM-V 2.6) as well as two commercial\nclosed-source models (GPT-4V and Claude 3.5 Sonnet) using our 25 videos. As shown in Table 2, in a total of 75 tests, the"}, {"title": "7 Limitations and Future Work", "content": "In this section, we point out some limitations of this work, and identify future research directions.\nParticipants' diversity. Since the participants we recruited are from a university setting, they are either currently\npursuing or have already completed higher education, which may result in a lack of diversity among the participants.\nFuture research could expand the range of participants' educational levels to better assess the generalizability of this\ntype of CAPTCHA.\nBidirectional extension. The mainstream direction of video extension is forward-only, but Sora supports bidi-\nrectional extensions, which include both forward and backward. Nevertheless, Sora is currently not available to the\npublic. Research can be conducted on bidirectional extended videos without telling users which part is generated. We\ncan also design the derived CAPTCHA mechanisms from it."}, {"title": "8 Conclusion", "content": "We designed and implemented a novel CAPTCHA, named BounTCHA, to address the growing threat posed by\nincreasingly intelligent Al-powered bots. BounTCHA was designed to differentiate between genuine users and bots\nbased on human perception and identification of time boundaries in video transitions and abrupt changes. We developed\na prototype of BounTCHA to conduct experiments and determine the effective range of human perceptual time bias,\nwhich serves as a basis for distinguishing between real users and bots. A comprehensive security analysis was then\nperformed on BounTCHA, covering random attacks, database attacks, and multi-modal LLM attacks. The results of the\nanalysis demonstrated that BounTCHA effectively defends against various attack vectors. We envision BounTCHA as a\nrobust shield in web security, safeguarding millions of web applications from AI-powered bot threats in an era where\nmachines are becoming increasingly intelligent."}]}