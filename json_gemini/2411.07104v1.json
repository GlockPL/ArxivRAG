{"title": "Learning Multi-Agent Collaborative Manipulation for Long-Horizon Quadrupedal Pushing", "authors": ["Chuye Hong", "Yuming Feng", "Yaru Niu", "Shiqi Liu", "Yuxiang Yang", "Wenhao Yu", "Tingnan Zhang", "Jie Tan", "Ding Zhao"], "abstract": "Recently, quadrupedal locomotion has achieved significant success, but their manipulation capabilities, particularly in handling large objects, remain limited, restricting their usefulness in demanding real-world applications such as search and rescue, construction, industrial automation, and room organization. This paper tackles the task of obstacle-aware, long-horizon pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent reinforcement learning framework with three levels of control. The high-level controller integrates an RRT planner and a centralized adaptive policy to generate subgoals, while the mid-level controller uses a decentralized goal-conditioned policy to guide the robots toward these subgoals. A pre-trained low-level locomotion policy executes the movement commands. We evaluate our method against several baselines in simulation, demonstrating significant improvements over baseline approaches, with 36.0% higher success rates and 24.5% reduction in completion time than the best baseline. Our framework successfully enables long-horizon, obstacle-aware manipulation tasks like Push-Cuboid and Push-T on Go1 robots in the real world.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in quadrupedal robots have significantly improved their ability to traverse challenging terrains [1]\u2013[6]. While many studies have focused on enhancing their mobility and stability of locomotion, the manipulation capabilities of these robots remain relatively limited. Efforts have been made to improve the quadrupedal capabilities in prehensile manipulation through attaching grippers or robotic arms on the robot [7]-[12], and non-prehensile manipulation by using legs [13]-[16] or the head [17], [18] as the end-effectors. Although these advancements enable quadrupeds to handle some routine tasks, their limited ability to manipulate large and heavy objects still restricts their usefulness in demanding fields like search and rescue, construction, industrial automation, and room organization, where both dexterity and strength are essential. To address these challenges, researchers have explored adding support structures to the robots [19], [20], coordinating whole-body movements [21], and using multiple robots [22], [23] to strengthen contact forces and expand operational dimensions. However, achieving long-horizon manipulation of large objects in cluttered environments remains a largely unexplored and challenging task for quadrupeds.\nIn this work, we focus on addressing the challenge of obstacle-aware, long-horizon pushing by coordinating the whole-body motions of multiple quadrupedal robots. We build our work upon recent works of quadrupedal pushing that demonstrate impressive results. As shown in Table I, while many approaches utilize multiple robots to enhance manipulation abilities, few focus on long-horizon pushing and obstacle avoidance, both of which are critical for real-"}, {"title": "II. RELATED WORK", "content": "Researchers have proposed various optimization-based methods for prehensile loco-manipulation [7]\u2013[9], [28]. These approaches often use hierarchical structure to coordinate locomotion and gripper motions [7], decompose tracking objectives [9], or abstract object information for planning [8]. Optimization-based methods have also been applied to single-robot non-prehensile manipulation tasks [17]\u2013[20], [29], many of which rely on modeling and optimizing contacts with either the object or the ground. Murooka et al. demonstrate how humanoid robots can push large, heavy objects through contact posture planning [29], while Polverini et al. introduce a multi-contact controller for a centaur-type humanoid robot to handle similar tasks [19]. Rigo et al. introduce a hierarchical MPC framework for optimizing contact in quadrupedal loco-manipulation, where the robot is constrained to using its head for pushing [17]. Recently, learning-based methods have demonstrated its effectiveness in loco-manipulation for legged robots. Specifically, reinforcement learning (RL) has been used to train short-horizon quadrupedal pushing skills [15], [30], [31], and other non-prehensile loco-manipulation skills such as dribbling a soccer ball [14], manipulating a yoga ball [13] pressing buttons [16], opening doors [32], and carrying boxes [33]. Jeon et al. propose a hierarchical reinforcement learning framework for quadrupedal whole-body manipulation of large objects, capable of inferring manipulation-relevant privileged information through interaction history [21]. Moreover, learning-based whole-body controllers are trained for prehensile manipulation that requires grasping various objects [11], [34] and consuming visual inputs [10], [12], [35]. Our work focuses on quadrupedal pushing, coordinating whole-body motions using RL-trained policies without explicitly modeling contacts."}, {"title": "B. Multi-Agent Collaborative Manipulation", "content": "Optimization-based methods have proven effective in multi-agent collaborative manipulation across various robotic embodiments, such as mobile robots [36]\u2013[39], robotic arms [40], quadrotors [41], and six-legged robots [42]. Some works explore utilizing Model Predictive Control (MPC) to achieve cooperative locomotion for multiple quadrupedal robots holonomically constrained to one another [43]-[45] or collaborative loco-manipulation with objects rigidly attached to each robotic hand [46]. However, these approaches might lack generalizability to more typical scenarios due to their reliance on specific inter-robot connections. The work in [22] introduces a hierarchical adaptive control method enabling multiple quadrupeds to cooperatively push an object with unknown properties along a predetermined path, though the robots are constrained to use their head to push the objects. Moreover, MARL are employed in cooperative biman-ual manipulation for robotic arms [47]-[50] and dexterous hands [51], [52], and collaborative loco-manipulation for quadrupeds [23]-[25], [53], snake robots [54] and bipedal robots [55]. Nachum et al. propose a two-level hierarchical policy in which the high-level policy generates subgoals for each robot to navigate toward [23]. Xiong et al. benchmark MARL with a two-level hierarchical structure in cooperative and competitive tasks, but the methods struggle in a simple"}, {"title": "III. METHODOLOGY", "content": "To enable quadrupedal robots to collaboratively perform long-horizon pushing tasks in environments with obstacles, we propose a hierarchical reinforcement learning framework, as illustrated in Figure 2. This framework consists of three layers of controllers. At the top level, an RRT planner generates a geometrically feasible trajectory without accounting for the robots' pushing capabilities or the dynamics of multiple robots and the object. The high-level adaptive policy then uses this trajectory as a reference to assign a subgoal for the target object, based on the dynamic states of the environment, object, and robots. Using this common subgoal, each robot's mid-level pushing policy provides velocity commands to its corresponding low-level policy. Due to the computational demands of the RRT planner, it is executed only once at the start of each episode. Both the high-level adaptive policy and the mid-level controller operate at a frequency of 50 Hz, with the higher frequency at the high level proving beneficial for more adaptive behavior in our settings. The low-level locomotion policy also runs at 50 Hz, while the PD controller is implemented at 200 Hz in simulation and 2000 Hz on the physical robot. In the following sections, we will introduce each of these three hierarchies in detail."}, {"title": "B. Low-Level Controller", "content": "The low-level controller controls each robot individually to track the mid-level velocity commands. More specifically, each low-level controller $\\pi_{l,i}: O_{l,i} \\rightarrow A_{l,i}$ computes motor commands $a_{l,i}$ to track the mid-level velocity command $a_{m,i}=(\\dot{x}_{t,i}, \\dot{y}_{t,i}, \\dot{\\theta}_{yaw})$. Despite recent progress of learning-based low-level controllers [73], we find these controllers to suffer from a large sim-to-real gap, and cannot accurately track the velocity commands, especially when the robot is pushing a heavy object. Instead, we use Unitree's built-in low-level controller, which tracks the velocity commands significantly more robustly in the real world. For efficient policy training in simulation, we train a learned low-level policy to mimic the behavior of Unitree's built-in controller."}, {"title": "C. Mid-Level Controller", "content": "The mid-level controller $\\pi_{m,i}: O_{m,i} \\rightarrow A_{m,i}$ is a decentralized policy of agent $i$, where $O_{m,i}$ represents the mid-level local observation space of robot $i$, and $A_{m,i}$ is the action space of the mid-level policy of agent $i$. This decentralized policy takes as input the high-level action $a_h$, the local observation of robot $i$, $o_{m,i} \\in O_{m,i}$, which consists of the local observations of the target object state $s_{object} \\in S_{object}$, obstacle state $s_{obstacle} \\in S_{obstacle}$, and the state of other robots, ${s_j}_{j=1,j \\neq i}^N$, all computed in the local torso frame of the robot $i$. For example, $s_{object} = s \\in Sobject$ can be expanded as $(p_{object}, \\dot{p}_{object}, \\theta_{object})$, where $p_{object} = (x_{object}, y_{object})^T$ is the 2D position of the object, and $\\theta_{object}$ is its yaw angle, both in the local frame of robot $i$. The mid-level policy of agent $i$ will output a mid-level action $a_{m,i} \\sim \\pi_{m,i} (a_{m,i} | o_{m,i}, a_h) \\in A_{m,i}$ in a decentralized manner to the low-level controller of robot $i$.\nIn practice, we train a mid-level policy shared by all robots, noted as $\\pi_m$. Following the scheme of centralized training and decentralized execution, it is trained by MAPPO [74] to optimize the objective function $I_m(\\theta) = \\sum_{t=0}^T E_{\\tau \\sim p_\\theta} [\\sum_{t=0}^T \\gamma^t r_m (s_t, a_t, {a_i}_{i=1}^N)]$, where $s_t$ is a joint state at time $t$, $\\tau$ is the trajectory sampled from a distribution $p_\\theta$ induced by policy $\\pi_m$, initial state $p_m$ and the transition probability $p_m$ that are defined by the mid-level task. Here, $r_m (.)$ represents the reward function for the mid-level controller. During training, we randomly sample the subgoals of the object as $a_h$ and freeze the low-level policy. Meanwhile, we specialize the domain randomization for frictions to reduce the Sim2Real gap of pushing."}, {"title": "D. High-Level Controller", "content": "The high-level controller is composed of two elements, a RRT planner $P : M \\times G \\rightarrow T$ and a centralized adaptive policy $\\pi_\\rho : M \\times T \\times S_{object} \\times S_1 \\times S_2 \\times \\dots S_N \\rightarrow A_h$, where $M$ represents the map information space, $G$ represents the goal space of the target object, $T$ represents the trajectory space of the RRT planner, $S_{object}$ denotes the object state space, $S_i$ is the state space of robot $i$, and $A_h$ is the action space of the high-level adaptive policy.\nThe RRT planner takes the desired goal position of the object $g_{object} \\in G$ and the map information $P_{map} \\in M$ encompassing the obstacle position and the initial position of the object as input and outputs a reference trajectory $T_r \\in T$ for the adaptive policy $\\pi_\\rho$. The adaptive policy will use the desired goal $g_{object}$, each robot state $s_i \\in S_i$, the map information $P_{map}$ and the dynamic global object pose $S_{object} \\in S_{object}$, as the input, and output a high-level action $a_h \\sim \\pi_\\rho (a_h | G_{object}, P_{map}, s_1, s_2, \\dots, s_N, S_{object}, T_r) \\in A_h$ as the subgoal position of the target object to the mid-level policy $\\pi_m$. The high-level adaptive policy is a centralized policy and trained via PPO [75] to optimize the objective function $I_h(\\theta) = E_{\\tau \\sim p_\\theta} [\\sum_{t=0}^T \\gamma^t r_h (g_{object,t}, (g_{object, P_{map,t}, S_{object,t}, {s_{i,t}}_{i=1}^N, a_t, T_r)]$ where $\\tau$ is the trajectory sampled from a distribution $p_h$ induced by policy $\\pi_\\rho$, initial state $p_0$ and the transition probability $p_h$ that are defined by the high-level task, Here, $r_h(.)$ represents the reward function for the high-level controller. During training, we freeze the mid-level and low-level controller."}, {"title": "E. Reward Design", "content": "1) Mid-Level Reward: Our mid-level reward function is formulated as $r_m = r_{task}^m + r_{penalty}^m + r_{heuristic}^m$. The mid-level task reward $r_{task}$ incentivizes actions that move the object toward and reach the target point, while the penalty term $r_{penalty}^m$ penalizes agents for close proximities, as well as for exceptions such as robot fall-overs and timeouts.\nThe mid-level heuristic reward $r_{heuristic}^m$ plays a vital role in the pushing process, given the expansive action space and the inherent uncertainty and complexity of interactions during pushing. It is defined as $r_{heuristic}^m = r_{approach}^m + r_{vel}^m + r_{ocb}^m$, where the mid-level approaching reward $r_{approach}^m$ encourages agents to approach the object, and the velocity reward $r_{vel}^m$ rewards agents when the object's velocity exceeds a pre-defined threshold, promoting diverse pushing actions while preventing oscillation near the object.\nImportantly, an occlusion-based (OCB) reward $r_{ocb}^m$, inspired by [38], is introduced to guide agents toward more favorable contact points in the areas where robots' views of the subgoals are blocked. Specifically, the OCB reward of robot $i$ is calculated as $r_{ocb}^m = \\vec{v_i} \\cdot \\vec{u_{target}}$, where $\\vec{v_i}$ is the unit normal vector at the closet point on the object's convex hall to robot $i$ and $\\vec{u_{target}}$ is unit vector directing from the object towards the subgoal, as depicted in Figure"}, {"title": "IV. EXPERIMENTS", "content": "1) Environments and Tasks: We build our simulation environments in IsaacGym [27]. We consider a cluttered environment with randomly placed 1.0m \u00d7 1.0m \u00d7 1.0m obstacles, where multiple quadrupeds need to push a target object to a desired goal. Unitree Gol robots are utilized in simulation to match the physical robots, each with an approximate payload capacity of 5 kg. The robots are tested with three types of objects varying in shape and mass: a 4kg cuboid, a 3 kg T-shaped block, and a 10kg cylinder with a radius of 1.5 m. Each object is larger than the robot in size and close to or exceeds the robot's payload capacity. Different numbers of agents are evaluated across tasks, with two agents for the cuboid and T-shaped block, and up-to four agents for the cylinder. The initial positions and postures of the agents and target objects are randomly set within an area on one side of the room, while the target goals for the object are generated on the other side. The task is considered successful if the center of the object is positioned within 1 m of the target. Failure occurs in the situation described in Sec. III-E.2. The tasks are designed for challenging long-horizon pushing, with initial-to-target distances exceeding 10m.\n2) Baselines: We compare our proposed method with the following baselines.\nSingle-Agent (SA) retains the three hierarchical levels of the policy and the reward function design, but only a single quadrupedal robot is employed for each task.\nHigh-Level+Low-Level (H+L) utilizes both a high-level and a low-level policy, where the high-level policy proposes subgoals for the robots, and the low-level policy aids the robots in navigating to these subgoals. We maintain $r_{task}$ and $r_{penalty}$ mentioned in Sec. III-E.2. This baseline follows a similar approach to [23], with a multi-agent implementation using MAPPO [74]."}, {"title": "Ablation Study", "content": "a) The OCB Reward: To assess the effectiveness of the OCB reward in training the mid-level controller for short-horzion pushing, we conduct an ablation study in a free space environment, where a 6kg cuboid (1.5m \u00d7 1.5 m \u00d7 0.5 m) is placed with random orientations. Two agents are randomly initialized, while the target object position (subgoal for the mid-level controller) is generated within a circular area 1.5 m to 3.0m from the cuboid's initial position randomly. As shown in Table III, our method significantly outperforms the ablation experiment in both success rate (S.R.) and completion time (C.T.). In particular, as the duration of the timeout increases, the success rate of our method improves more rapidly, indicating a better adaptability to adjust the direction of pushing when the object deviates, an issue that often causes failures with shorter timeouts."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we address the challenge of obstacle-aware, long-horizon object pushing by coordinating the whole-body motions of multiple quadrupedal robots. While previous studies make significant progress in improving quadrupedal mobility and some aspects of manipulation, their ability to handle large objects in complex, real-world environments remains limited. To overcome these limitations, we propose a hierarchical MARL framework, consisting of high-level, mid-level, and low-level controllers, to enable effective and coordinated pushing tasks.Through extensive simulation experiments, we demonstrate that our approach significantly outperforms the best baseline methods, achieving a 36.0% higher success rate and a 24.5% reduction in completion time. Through physical robot experiments, our method is validated to effectively handle obstacle-aware, long-horizon tasks such as Push-Cuboid and Push-T, highlighting its potential for real-world applications."}, {"title": "APPENDIX", "content": "of thein simulation approach describ. For detailed im-plementation information, we direct readers to this prior workThe training and setups for low-level locomotion policy follow the scheme introduced in [73], and we refer our readers to the work for details of implementation.\nThe mid-level controller is to enable the robot team to complete shorter-horizon pushing tasks, specifically those where the distance between the initial position and the target position of the object is less than 3.0 m. The environments for the mid-level policies are designed to be free of obstacles, with a dimension of 24 m \u00d7 24 m. The initial object positions are static, while both the target object positions and the initial agent positions are randomized within a circular area measured by a polar coordinate system represented by r and $\\theta$. Additionally, the yaw of both the initial objects and agents is randomized from 0 to 2$\\pi$. This ensures adequate coverage of partial observation settings. In practice, we find that a substantially high threshold for subgoal reaching will improve the success rate of long-horizon pushing. This prevents the policy from becoming trapped into fine-grained manipulation around an intermediate subgoal, which is intended to serve as guidance toward the final target. An episode concludes either when the agent team successfully completes the pushing task or when an exception occurs, including robot fall-overs, collisions, object tilting, or timeouts. The details of the above environment settings are specialized in Table IV."}]}