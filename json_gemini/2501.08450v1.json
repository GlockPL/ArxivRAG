{"title": "Active Sampling for Node Attribute Completion on Graphs", "authors": ["Benyuan Liu", "Xu Chen", "Yanfeng Wang", "Ya Zhang", "Zhi Cao", "Ivor Tsang"], "abstract": "Node attribute, a type of crucial information for graph analysis, may be partially or\ncompletely missing for certain nodes in real world applications. Restoring the missing\nattributes is expected to benefit downstream graph learning. Few attempts have been\nmade on node attribute completion, but a novel framework called Structure-attribute\nTransformer (SAT) was recently proposed by using a decoupled scheme to leverage\nstructures and attributes. SAT ignores the differences in contributing to the learning\nschedule and finding a practical way to model the different importance of nodes with\nobserved attributes is challenging. This paper proposes a novel AcTive Sampling algo-\nrithm (ATS) to restore missing node attributes. The representativeness and uncertainty\nof each node's information are first measured based on graph structure, representation\nsimilarity and learning bias. To select nodes as train samples in the next optimization\nstep, a weighting scheme controlled by Beta distribution is then introduced to linearly\ncombine the two properties. Extensive experiments on four public benchmark datasets\nand two downstream tasks have shown the superiority of ATS in node attribute com-\npletion.", "sections": [{"title": "1. Introduction", "content": "Node attribute, a type of important information for graphs, plays an important role\nin many graph learning tasks, such as node classification [1, 2] and community de-\ntection [3, 4]. The recent Graph Neural Network (GNN) enjoys boosted performance\nleveraging node attributes [5, 6, 7, 8]. Despite of its indispensability, real-world graphs\noften are associated with missing node attributes for various reasons [9]. For example,\nin citation graphs, key terms or detailed content of some papers may be inaccessible\nbecause of copyright protection. In social networks, profiles of some users may be un-\navailable due to privacy protection. As a result, node attribute completion, which learns\nto restore the missing node attributes of a graph, has become an important research di-\nrection in graph learning community, given its expected benefits for downstream graph\nlearning tasks.\nSo far, there are few attempts on the node attribute completion. The popular random\nwalk based method [10] and GNN framework are not designed for this task. [9] propose"}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Deep Graph Learning", "content": "With the development of deep representation learning in the Euclidean vision do-\nmain [17], researchers have studied a lot of deep learning methods on the non-Euclidean\ngraphs [18]. Random walk based methods can learn node embeddings by random walks\n, which only considers the structural information and cannot generalize to new graphs.\nTo tackle this problem, the attributed random walk based methods (e.g.GraphRNA [19])\napply random walks on both structures and attributes. These random walk based meth-\nods are useful, but they demand hardly-acquired high-quality random walks to guar-\nantee good performance. Graph Neural Network (GNN) [20, 5, 6] realizes \"graph-in,\ngraph-out\" that transforms the embeddings of node attributes while maintaining the\nconnectivity [21]. GNN performs a message passing scheme, which is reminiscent of\nstandard convolution as in Graph Convolutional Networks (GCN) [6]. GNN can infer\nthe distribution of nodes based on node attributes and edges and achieve impressive\nresults on graph-related tasks. There are also numerous creative modifications in GNN\nsuch as GAT [8] and GraphSAGE [22].\nIn last few years, more works have emphasized the importance of node attributes\nin graph-related downstream tasks. Both SEAL [23] and WalkPool [24] encode node\nrepresentations with node attributes to achieve superior link prediction performance.\nIn most real-world applications, attributes of some nodes may be inaccessible, so the\nnode attribute completion task appears. Recent SAT [9] assumes a shared-latent space\nassumption on graphs and proposes a novel GNN-based distribution matching model.\nIt decouples structures and attributes and simultaneously matches the distribution of\nrespective latent vectors. WGNN developed by [11] learns node representations in\nWasserstein space without any imputation. [1] propose HGNN-AC to learn topological\nembedding and attribute completion with weighted aggregation. PaGNNs [12] restores\nthe missing attributes based on a partial message propagation scheme. These methods\nimplement several aggregation methods to incorporate the structural information, while\nSAT's shared latent space assumption matches the joint distribution of structure and\nattributes by two distinct encoders. HGCA [25] is proposed for heterogeneous network\nwith missing attributes. Feature Propagation [26] and Amer [13] can resolve high\nmissing rate of the node attributes, but they generally focus on coarse-grained task\nsuch as node classification. Note that they all do not model the different contributions\nof nodes with observed attributes in learning, while the proposed ATS can help them\nachieve this goal."}, {"title": "2.2. Active Sampling on Graphs", "content": "Active learning assists the model to achieve as better performance as possible while\nlabeling as few samples as possible [27]. It's usually combined with deep learning\nprimary model to select the most influential samples from unlabeled dataset and then\nlabel them for training to reduce the annotation cost [28]. There are also some works"}, {"title": "3. Problem Formulation", "content": "For node attribute completion task, we denote G = (V, A, X) as a graph with node\nset V = {v\u2081, v\u2082, ..., vv}, the adjacency matrix A \u2208 \u211d^(N\u00d7N) and the node attribute matrix\nX \u2208 \u211d^(N\u00d7F). V\u00b0 = {v\u2081, v\u2082, ..., v_(N\u00b0)} is the set of attribute-observed nodes. The attribute\ninformation of V\u00b0 is X\u00b0 = {x\u2081, x\u2082, ..., x_(N\u00b0)} and the structural information of V\u00b0 is A\u00b0 =\n{a\u2081, a\u2082, ..., a_(N\u00b0)}. V\u1d58 = {v'\u2081, v'\u2082, ..., v_(N\u1d58)} is the set of attribute-missing nodes. The attribute\ninformation of V\u1d58 is X\u1d58 = {x'\u2081, x'\u2082, ..., x'_(N\u1d58)} and the structural information of V\u1d58 is A\u1d58 =\n{a'\u2081, a'\u2082, ..., a'_(N\u1d58)}. More specifically, we have V = V\u1d58 \u222a V\u00b0, V\u1d58 \u2229 V\u00b0 = \u2205, and N = N\u00b0 +\nNu. In recent proposed works [9, 11, 12], learning the latent representations of attribute-\nmissing nodes V\u1d58 based on the available structures A together with observed node\nattributes X\u00b0, and then translating the latent representations to missing node attributes\nX\u1d58 is a commonly recognized way. The difference among these works is the technique\nof learning latent representations and translating features. Among existing models,\nSAT performs well and has open source codes, so we will refer to SAT as a primary\nbase model to verify the proposed algorithm in later experiments.\nIn our active sampling algorithm, we denote the total training set as T, in which\nthe node attributes are known. The current training set of primary base model is TL\nand the set containing candidate nodes is denoted as TU. We have T = TL \u222a TU. We\ndesign a reasonable sampling strategy named ATS which iteratively transfers the most\nsuitable candidate nodes from TU to TL to boost the training effectiveness of primary\nmodel until TU = \u2205 and the model converges."}, {"title": "4. Method", "content": "We design query strategy of the ATS by measuring the representativeness and\nuncertainty of the candidate nodes. Then we combine the uncertainties and represen-"}, {"title": "4.1. Query Strategy", "content": "Representativeness: The major and typical patterns among the nodes are vital for\nthe model to converge to the right direction. In this section, we introduce the concept\nof representativeness as a sampling metric. This metric is composed of two parts: 1)\ninformation density \u03a6density and 2) structural centrality \u03a6centrality. The former mainly fo-\ncuses on measuring the similarity between the corresponding latent vectors of attributes\nand structures. The latter indicates how closely a node is connected to its neighbours\non graph. In other words, the information density is inspired by the good representa-\ntion learning ability of primary model and the structural centrality is natural to mine\nthe information on the graph structures. These two aspects offer us a comprehensive\nanalysis of the representativeness in both implicit and explicit ways.\nWe first focus on the information density. In particular, we measure the node sim-\nilarities through the latent features learned by the primary model. If there is a dense\ndistribution of representation vectors in a local region of the latent space, the corre-\nsponding nodes will have more similar features and this region will contain further\nmainstream information, so we expect to train these more representative nodes in pri-\nority. In the case of SAT, there are attribute embeddings of attribute-observed nodes\nand structure embeddings of all nodes in SAT. ATS only uses the structure embeddings\nza; to calculate the density as shown in Eq. 1 since we believe the structural represen-\ntations have more information of density on graphs. In order to find the central node\nlocated in high-density region, we employ the K-means algorithm in the latent space\nand calculate the Euclidean distance between each node and its clustering center. Given\nd as the metric of Euclidean distance in l2-norm and C_(za\u1d62), as the clustering center of z_(ai)\nin latent space, the formulation of \u03a6_density is written as:\n\n\u03a6_density(v\u1d62) = 1 / (1 + d(z_(a\u1d62), C_(za\u1d62))), \u2200 z_(a\u1d62) \u2208 T^U  (1)\n\nThe larger the density is, the more representative the node is, and the node contains\nmore representative features that are worthy of the model's attention.\nBesides the feature analysis in latent space, the node representativeness can also\nbe inferred from the explicit graph structures. We can study the connections between\nnodes and develop a metric to calculate the node centrality based on the structural\ninformation. Intuitively, the centrality can have a positive correlation with the number\nof neighbours. At the early stage of training, if we can focus on these nodes, the model\nwill learn the approximate distribution of the data faster and reduce the influence caused\nby the noisy ones. PageRank [36] algorithm is an effective random-walk method to\nacquire the visiting probabilities of nodes. The higher score signifies the higher visiting\nprobabilities, which means that nodes have relatively more neighbors and then contain\nmore structural information. We find that PageRank is the most suitable one because it\nhas well ability of representing centrality [32]."}, {"title": "4.2. Score function and Beta distribution controlled weighting scheme", "content": "We have presented three metrics of our query strategy. Then, a question arises: How\nto combine these metrics to score each node? Combing the metrics with a weighted\nsum is a possible solution but still faces great difficulties. First, the values of different\nmetrics are incomparable because of the distinct dimensional units. Second, the differ-\nent metrics may take different effects at different learning stages. To solve these, we\nintroduce a percentile evaluation and design a Beta-distribution controlled re-weighting\nscheme to exert the power of each metric, since Beta distribution is a suitable model\nfor the random behavior of percentages and proportions [38].\nDenote P_\u03a6(v\u1d62,TU) as the percentage of the candidate nodes in TU which have\nsmaller values than the node v\u1d62 with metric \u03a6. For example, if there are 5 candidate\nnodes and the scores of one metric is [1, 2, 3, 4, 5], the percentile of the corresponding\n5 nodes will be [0, 0.2, 0.4, 0.6, 0.8]. We apply the percentile to three metrics and define\nthe final score function of ATS as:\n\nS(v\u1d62) = \u03b1 \u00b7 P_(entropy)(v\u1d62, TU) + \u03b2 \u00b7 P_(density)(v\u1d62, TU)\n+ \u03b3 \u00b7 P_(centrality)(v\u1d62, TU)  (4)"}, {"title": "4.3. Iterative training and Implementation", "content": "In general, our method consists of two stages: one is primary base model SAT,\nresponsible for the training stage; the other is ATS, responsible for the sampling stage.\nBefore the training, we divide total training set T into TU and TL. We randomly sample\n1% of the nodes in T as the initial nodes of TL and the rest composes TU. SAT will\nbe trained on the changeable TL. Once SAT accomplishes a single training epoch,\nATS starts the sampling process. We sample the most representative and informative"}, {"title": "5. Experiments and Analysis", "content": ""}, {"title": "5.1. Datasets", "content": "We utilize 4 public benchmarks whose node attributes are categorical vectors in-\nstead of numeric ones. For numeric attributes, SAT implements auto-encoding after\nnormalization whose optimization is not stable according to our experiments. The in-\nformation of used datasets is as follows: 1) Cora. Cora [39] is a citation graph with\n2,708 papers as nodes and 10,556 citation links as edges. Each node has a multi-hot\nattribute vector with 1,433 dimensions. The attribute vectors consist of different word\ntokens to determine whether they appear or not. 2) Citeseer. Citeseer [40] is another\ncitation graph which is larger than Cora. It contains 3,327 nodes and 9,228 edges. Like\nCora, each node has a multi-hot attribute vector with 3,703 dimensions. 3) Amazon-\nComputer and 4) Amazon-Photo. These two datasets are generated from Amazon\nco-purchase graph. The node represents the item and the edge represents the two items\nare usually purchased at the same time. The node attribute is a multi-hot vector with\nthe set of words involved in the item description. Amazon-Computer [41] has 13,752\nitems and 245,861 edges. Amazon-Photo [41] has 7,650 nodes and 119,081 edges."}, {"title": "5.2. Experimental setup", "content": "Baselines: We compare ATS with other baselines introduced in [9]: NeighAg-\ngre [42], VAE [43], GCN [6], GraphSage [22], GAT [8], Hers [44], GraphRNA [19],\nARWMF [45], PaGNN [12] and original SAT [9]. Details about how they work on\nnode attribute completion are illustrated in Appendix Appendix A.\nParameters setting: In the experiment, we randomly sample 40% nodes with at-\ntributes as training data, 10% nodes as validation data and the rest as test data. The\nattributes of validation and test nodes are unobserved in training. For the baselines, the\nparameters setting and the experiment results refer to [9]. For our ATS method, the\nSAT's setting remains the same, such as \u03bbc. We mainly have two hyper-parameters:\ne in the weighting scheme and cluster numbers in the estimation of density \u03a6density.\nConsidering the objective of the Beta distribution weighting scheme, e should be larger\nthan the total sampling times. Hence in Cora and Citeseer, we set e = 1500 and when it\ncomes to Amazon_Photo and Amazon-Computer, we set e = 2000. In addition, we set\nthe cluster number as 10, 12, 15, 20 for Cora, Citeseer, Amazon-Photo and Amazon-\nComputer. The threshold for Cora is 300 and the rest is 0.\nEvaluation metrics: In node attribute completion, the restored attributes can pro-\nvide side information for nodes and benefit downstream tasks. By following SAT [9],\nwe study the effect of ATS on two downstream tasks including node classification task\nin the node level and profiling task in the attribute level. In node classification, we train\nan extra classifier based on the recovered attributes of test nodes to evaluate whether"}, {"title": "5.3. Overall Comparison", "content": ""}, {"title": "5.3.1. Node Classification", "content": "Classification is an effective downstream task to test the quality of the recovered\nattributes. In node classification task, the nodes with restored attributes are split into\n80% training data and 20% test data. Then we conduct five-fold cross-validation in\n10 times and take the average results of evaluation metrics as the model performance.\nWe use two supervised classifiers: MLP and GCN. The MLP classifier is composed by\ntwo fully-connected layers, which classifies the nodes based on attributes. The GCN\nclassifier is an end-to-end graph representation learning model, which can learn the\nstructure and attributes simultaneously. Results are shown in Table 1.\nAccording to the results of \"X\" row where only node attributes are used, most of\nthe optimized models with our proposed ATS algorithm achieve obvious improvement\ncompared with original models, especially those models that are not designed specif-\nically for attribute missing graph such as GCN, GAT etc. Our ATS can also adapt to\nSAT with different GNN backbones (e.g. GCN and GAT) and achieve higher classi-\nfication accuracy than the original models. For the results of \"A+X\" row where both\nstructures and node attributes are used by a GCN classifier, the performance of our\nATS combined with other traditional models (e.g. GCN, GAT, GraphSage) achieves an\nincrease of more than 15% compared to that of original models on all datasets, because\nATS contains the density metric and can help the model better learn the inner seman-\ntic structures. Furthermore, we can observe the increment on other newly proposed\nmodels, which reflects the generalizability of our method."}, {"title": "5.3.2. Profiling", "content": "The model outputs the restored attributes in different dimensions with probabili-\nties. Higher corresponding probabilities of ground-truth attributes signify better per-\nformance. In this section, we use two common metrics Recall@k and NDCG@k to\nevaluate the profiling performance between new model and original model. The exper-\niment results are shown in Table 2.\nAccording to the profiling results in Table 2, the majority of models combined with\nATS method perform better than the original models. On the basis of the advantages\nestablished by the SAT model towards other baselines, the combination of the ATS\nalgorithm and SAT model (ATS+SAT) obtains even higher performance in almost all\nthe evaluation metrics and almost all the datasets. For example, ATS+SAT(GAT) ob-\ntains a relative 13.1% gain of Recall@10 and a relative 12.5% gain of NDCG@10\non Citeseer compared with SAT(GAT). Furthermore, ATS method can also augment\nother base model's profiling performance. ATS+GAT achieves a relative 17.4% higher\nperformance of Recall@10 and a relative 24.9% higher performace of NDCG@10 on\nCiteseer compared with original GAT. The main reason of these results is that the ac-\ntive sampling algorithm ATS helps these base models realize different importance of\ndifferent nodes in learning, and achieves more accurate distribution modeling of the\nhigh-dimensional node attributes."}, {"title": "5.4. Ablation Study", "content": ""}, {"title": "5.4.1. Effect of Different Node Degree", "content": "ATS will preferentially select the nodes with larger amount of information to make\nprimary model extract more information from more informative nodes. We thus design\nan experiment to verify whether ATS can facilitate the learning at different information\nlevels and how the information level influences the performance. In particular, Node\ndegree is an important indicator for centrality metric, so we sort the nodes in the test\nset according to their degrees and select a ratio of nodes with a range of degrees for\nexperiment. For example, every 20% of test nodes sorted by degrees corresponds to\none Degree Level. The results are shown in Table 3.\nOn one hand, ATS can achieve improvements at almost all degree levels compared\nto the original primary model. Even less informative nodes benefit from our ATS's\ntraining strategy. The reason is that ATS is capable of capturing the different impor-\ntance of nodes in the learning schedule, helping the primary model converge to a better\nstate. On the other hand, when considering the improvement gap along with degree\nlevels, we find that the gap generally becomes more evident as the node degree level\nincreases. This result verifies the correctness of ATS's idea that employs node infor-\nmation in importance modeling."}, {"title": "5.4.2. Different Weighting Scheme", "content": "Besides the active sampling metrics, the Beta distribution controlled weighting\nscheme is also a highlight of the ATS algorithm. We will verify the effectiveness of"}, {"title": "5.4.3. Different Centrality Metrics", "content": "In Section 4.1, we mention that structural centrality is evaluated using the PageR-\nank score. Meanwhile, there are several centrality metrics other than PageRank in-\ncluding degree centrality, closeness centrality, betweenness centrality and eigenvector\ncentrality. All of these metrics can reflect the structural information contained in each\nnode. We will compare the performance of these different metrics using the SAT(GCN)\nprimary model. All these five metrics are implemented using NetworkX\u00b9.\nThe result is shown in Figure 2. The PageRank metric outperforms other metrics\non almost all the datasets, indicating that PageRank can better leverage structural in-\nformation. Therefore, choosing the PageRank score rather than other metrics as the\nstructural centrality is reasonable."}, {"title": "5.4.4. Different Metric Combinations", "content": "In this section, we conduct the ablation study to investigate the effects of three\ndifferent metrics in ATS. The experimental settings remain the same as the profiling\ntask. We use Recall@20 to evaluate the performance of different metric combinations.\nThe results are shown in Figure 3."}, {"title": "5.5. Empirical Time Complexity Analysis", "content": "Our ATS is an active sampling procedure based on a node attribute completion\nmodel, so it is critical to study the extra processing time cost by ATS. Thus, we con-\nduct an experiment to count the running time of different parts of ATS compared with\nthe original primary base model\u2013SAT. These different parts are forward process, un-\ncertainty and representativeness. The forward process means the forward propagation,\nwhich is essential to calculate the uncertainty score. We implemented the experiment\non a machine with one Nvidia 1080Ti GPU. According to the running time shown in\nFigure 4, forward propagation in ATS takes only a small part of time in SAT since\nback propagation usually costs a lot of time. Although the processing time of uncer-\ntainty metric and representativeness metric is relatively higher than SAT because of the\nclustering and percentile calculations, it's comparable with the time of SAT. With the"}, {"title": "6. Conclusion and Future Work", "content": "In this paper, we propose a novel active sampling algorithm ATS to better solve\nthe problem of node attribute completion. The ATS can also be combined with other\nprimary base models that have latent representations and training loss depending on\ndifferent tasks. In order to distinguish the differences in the amount of information\namong nodes, ATS utilizes the proposed uncertainty and representativeness metrics to\nselect the most informative nodes and renew the training set after each training epoch.\nIn addition, the Beta distribution controlled weighting scheme is proposed to dynami-\ncally adjust the metric weights according to the training status. The sampling process\nincreases the running time of each epoch within an affordable cost but meanwhile helps\nthe primary base model achieve superior performance on profiling and node classifi-\ncation tasks. Therefore, ATS is effective in boosting the quality of restored attributes\nand is applicable to different primary models. In future, we will extend ATS to more\nprimary models and applications."}]}