{"title": "LIMO: Less is More for Reasoning", "authors": ["Yixin Ye", "Zhen Huang", "Yang Xiao", "Ethan Chern", "Shijie Xia", "Pengfei Liu"], "abstract": "We present a fundamental discovery that challenges our understanding of how complex reasoning emerges\nin large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand\nextensive training data (often > 100,000 examples), we demonstrate a striking phenomenon: complex\nmathematical reasoning abilities can be effectively elicited with surprisingly few examples. This find-\ning challenges not only the assumption of massive data requirements but also the common belief that\nsupervised fine-tuning primarily leads to memorization rather than generalization. Through comprehen-\nsive experiments, our proposed model LIMO demonstrates unprecedented performance and efficiency in\nmathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on\nthe highly challenging AIME benchmark and 94.8% on MATH, improving the performance of previous\nstrong SFT-based models from 6.5% to 57.1% on AIME and from 59.2% to 94.8% on MATH, while only\nusing 1% of the training data required by previous approaches. Most remarkably, LIMO demonstrates\nexceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse\nbenchmarks, outperforming models trained on 100x more data, directly challenging the prevailing notion\nthat SFT inherently leads to memorization rather than generalization. Synthesizing these pioneering\nresults, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models\nwhere domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning\ncapabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes.\nThis hypothesis posits that the elicitation threshold for complex reasoning is not inherently bounded by\nthe complexity of the target reasoning task, but fundamentally determined by two key factors: (1) the\ncompleteness of the model's encoded knowledge foundation during pre-training, and (2) the effectiveness\nof post-training examples, which serve as \u201ccognitive templates\" that show the model how to effectively\nutilize its existing knowledge base to solve complex reasoning tasks. To facilitate reproducibility and\nfuture research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at\nhttps://github.com/GAIR-NLP/LIMO.", "sections": [{"title": "1 Introduction", "content": "Complex reasoning has long been considered one of the most challenging capabilities to instill in large language\nmodels (LLMs). While recent work has shown that LLMs can be effectively aligned with user preferences\nthrough relatively small amounts of instruction data (Zhou et al., 2024a), teaching models to reason-particularly\nin mathematics and programming\u2014is widely believed to require vastly more training examples (Paster et al.,\n2023; Yue et al., 2024). This conventional wisdom stems from the inherent complexity of reasoning tasks, which\ndemand multi-step logical deduction, domain knowledge application, and structured solution paths. The resulting\nparadigm typically involves training on tens or hundreds of thousands of examples (Yu et al., 2024; Li et al., 2024b),\nbased on two fundamental assumptions: first, that mastering such complex cognitive processes requires extensive\nsupervised demonstrations, and second, that supervised fine-tuning leads primarily to memorization rather than true\ngeneralization (Zhang et al., 2024; Xu et al., 2024; Chu et al., 2025).\nWhile this approach has shown success, it imposes substantial computational costs and data collection burdens.\nMore importantly, we argue this data-intensive paradigm may no longer be necessary. Recent advances have\nfundamentally transformed how LLMs acquire, organize, and utilize reasoning knowledge, suggesting the possibility\nof a more efficient approach. Two key developments in particular have created the conditions for a fundamental\nreimagining of how we approach reasoning in LLMs:\nKnowledge Foundation Revolution: Modern foundation models now incorporate unprecedented amounts\nof mathematical content during pre-training (Qwen et al., 2025; Yang et al., 2024; Wang et al., 2024). For\nexample: Llama 2's total training data across all domains was 1.8T tokens (Touvron et al., 2023), while Llama\n3 used 3.7T tokens just for mathematical reasoning (Grattafiori et al., 2024). This suggests that contemporary\nLLMs may already possess rich mathematical knowledge in their parameter space, transforming the challenge\nfrom knowledge acquisition to knowledge elicitation.\nInference-time Computation Scaling Revolution: The emergence of techniques scaling longer reasoning\nchains has revealed that effective reasoning requires substantial computational space during inference. Recent\nworks (OpenAI et al., 2024; Qin et al., 2024; Huang et al., 2024) have shown that allowing models to\ngenerate extended reasoning chains significantly improves their reasoning ability. In essence, inference-time\ncomputation provides the crucial cognitive workspace where models can systematically unpack and apply\ntheir pre-trained knowledge.\nWe hypothesize that successful reasoning emerges from the synergy of these two factors: rich pre-trained\nknowledge and sufficient computational resources at inference time. These developments collectively suggest\na striking possibility: if models possess rich reasoning knowledge and are given adequate computational space,\nthen activating their reasoning capabilities may require only a small number of high-quality training samples that\nencourage extended deliberation, rather than massive fine-tuning datasets. Building on this insight, we propose the\nLess-Is-More Reasoning (LIMO) Hypothesis. This hypothesis identifies two critical factors that determine the\nelicitation threshold for complex reasoning: (1) the latent presence of prerequisite knowledge within the model's\nparameter space, and (2) the effectiveness of minimal exemplars in demonstrating systematic problem-solving\nprocesses that encourage extended deliberation. Critically, this suggests that the sample efficiency of eliciting\nadvanced reasoning is not inherently bounded by the complexity of the target reasoning task, but rather by the\ncompleteness of the model's encoded knowledge foundation and its exposure to training samples that effectively\nutilize the inference-time computation space.\nThrough comprehensive experiments, we demonstrate that LIMO achieves 57.1% accuracy on the highly\nchallenging AIME benchmark and 94.8% on MATH with merely 817 training samples, demolishing previous\nstrong SFT-based models while using just 1% of their training data. Most remarkably, these benefits generalize\nacross a diverse spectrum of previously unseen scenarios, with LIMO consistently outperforming models trained on\n100x more data by 40.5% absolute improvement. This discovery has profound implications for artificial intelligence\nresearch: it suggests that even competition-level complex reasoning abilities can be effectively elicited through\nminimal but curated training samples. More fundamentally, it points to a promising technical pathway toward AGI -\nany sophisticated reasoning capability, no matter how complex, could potentially be activated with minimal samples\ngiven two key conditions: (1) sufficient domain knowledge embedded during pre-training, and (2) optimal cognitive\nreasoning chains for activation. This represents not merely an argument for data efficiency, but a fundamental\ninsight into how complex reasoning capabilities emerge in large language models.\nThe main contributions of this work are: (1) We establish the LIMO hypothesis, demonstrating that complex\nreasoning capabilities can be elicited through surprisingly small datasets (hundreds of examples) by leveraging rich\nmathematical knowledge in pre-trained models and detailed reasoning chains. (2) We provide systematic empirical\nevidence challenging current assumptions about scaling laws in reasoning tasks, showing that benefits generalize\nrobustly to out-of-distribution problems and suggesting the acquisition of genuine reasoning capabilities rather"}, {"title": "2 Phenomena Rethinking: Less-is-More and RL Scaling", "content": "The emergence of LIMO represents a paradigm shift in how we conceptualize and activate complex reasoning\ncapabilities in large language models. This section examines two key comparisons that illuminate the fundamental\nnature of this advance: first, contrasting LIMO with LIMA to understand how Less-is-More principles extend from\ngeneral alignment to complex reasoning; and second, comparing LIMO with reinforcement learning (RL) scaling\napproaches to highlight distinct philosophical perspectives on developing reasoning capabilities. Through these\nanalyses, we aim to establish a deeper understanding of how complex cognitive abilities emerge in language models\nand the conditions that enable their efficient activation."}, {"title": "2.1 LIMO vs LIMA", "content": "The emergence of Less-is-More phenomena in LLMs represents a fundamental shift in our understanding of how\ncomplex capabilities can be elicited with minimal data. While LIMA (Zhou et al., 2024a) first demonstrated this\nphenomenon in the context of general alignment, extending this principle to complex mathematical reasoning\npresents unique challenges and requirements. This section examines the critical developments that enable Less-is-\nMore for reasoning, analyzing the essential differences between alignment and reasoning scenarios, and providing\ninsights into the conditions necessary for efficient capability activation in large language models.\nKnowledge Foundation Revolution The past two years have witnessed a transformation in how language models\nacquire and organize mathematical knowledge. While LIMA could rely on general text corpora for alignment,"}, {"title": "2.2 LIMO vs RL Scaling", "content": "The emergence of two distinct approaches to developing reasoning capabilities in large language models - RL\nScaling and LIMO - represents a fundamental divergence in how we understand and enhance model intelligence. RL\nScaling, exemplified by works like o1 (OpenAI, 2024), DeepSeek-R1 (Guo et al., 2025), approaches the challenge\nfrom an engineering optimization perspective. It assumes that reasoning capabilities need to be extensively trained\ninto models through large-scale reinforcement learning. While effective, this approach essentially treats RL as\na broad search mechanism to discover effective reasoning patterns through massive computational resources.\nIn contrast, LIMO introduces a more foundational perspective: reasoning capabilities are already latent within"}, {"title": "3 LIMO Dataset", "content": "3.1 The LIMO Hypothesis\nWe formalize the Less-Is-More Reasoning (LIMO) Hypothesis as follows: In foundation models where domain\nknowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge\nthrough minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis rests on two\nfundamental premises: (I) The latent presence of prerequisite knowledge within the model's parameter space (II)\nThe quality of reasoning chains that precisely decompose complex problems into detailed, logical steps, making the\ncognitive process explicit and traceable. To validate this hypothesis, we propose a systematic approach to construct\na high-quality, minimal dataset that can effectively elicit the model's inherent reasoning capabilities."}, {"title": "3.2 Problem Definition", "content": "In this paper, we focus on the reasoning tasks with verifiable answer. Given a question $q \\in Q$, where Q represents\nthe space of reasoning problems, the goal is to generate an answer $a \\in A$ and a reasoning chain $r \\in R$. We define\na reasoning chain r as a sequence of intermediate steps ${s_1,s_2, ..., s_n}$, where each step $s_i$ represents a logical\ndeduction that bridges the gap between the question and the final answer.\nFormally, we can represent the reasoning process as a function f:\n$f:Q\\rightarrow R\\times A$\nTherefore, the quality of the resulting dataset D is determined by two fundamental yet multifaceted components:\n(1) the quality of questions $q \\in Q$, which encompasses factors such as diversity in problem-solving approaches,\nappropriate difficulty levels to challenge model capabilities, and the breadth of knowledge domains covered, and\n(2) the quality of solutions $(r, a) \\in R \\times A$, which encompasses aspects including pedagogical value, logical\ncoherence, and methodological rigor. Questions should be designed to encourage sophisticated reasoning patterns\nand knowledge integration, while solutions should demonstrate clear logical progression and serve as effective\nlearning examples. These interrelated quality dimensions, among others, guide our systematic data curation process\ndetailed in the following sections."}, {"title": "3.3 High-Quality Data Curation", "content": "Our data curation process focuses on constructing a high-quality dataset $D = \\{(q_i, r_i,a_i)\\}_{i=1}^{N}$ where N is\nintentionally kept small to validate our LIMO hypothesis."}, {"title": "3.3.1 Question Selection", "content": "We hypothesize that high-quality questions $q \\in Q$ should naturally elicit extended reasoning processes. Our\nselection criteria include the following:\nLevel of difficulty We prioritize challenging problems that foster complex reasoning chains, diverse thought\nprocesses, and knowledge integration, enabling LLMs to effectively leverage pre-trained knowledge for high-\nquality inference.\nGenerality Problems that deviate more from the model's training distribution can better challenge its fixed\nthinking patterns, encourage exploration of new reasoning approaches, thus expanding its inference search space.\nKnowledge Diversity The selected problems should cover various mathematical domains and concepts, requiring\nthe model to integrate and connect distant knowledge during problem-solving.\nTo implement these criteria effectively, we first assembled a comprehensive pool of candidate problems from\nvarious established datasets: NuminaMath-CoT, featuring meticulously annotated problems from high school to\nadvanced competition levels; AIME historical examination problems, known for its extremely challenging and\nintegrative problems spanning multiple mathematical domains; MATH (Hendrycks et al., 2021), encompassing\nvarious competitive mathematics problems from prestigious contests; and several other sources of mathematical\nproblems."}, {"title": "3.3.2 Reasoning Chain Construction", "content": "Beyond high-quality questions, the quality of solutions plays a pivotal role in the training phase of large language\nmodels. To curate high-quality solutions, we adopted a comprehensive selection strategy. We began by gathering\nofficial solutions for problems where available, complemented by solutions authored by both human experts and\nAI specialists. Additionally, we leveraged state-of-the-art reasoning models, including DeepSeek R1, DeepSeek-\nR1-Distill-Qwen-32B (Guo et al., 2025), and Qwen2.5-32b-Instruct, to generate diverse solution approaches.\nFurthermore, following the methodology proposed in O1-Journey-Part2 (Huang et al., 2024), we utilized self-\ndistillation techniques based on Qwen2.5-32b-Instruct to create additional model variants, which were then used\nto generate supplementary problem responses. These responses were then filtered according to the correctness\nof the answers to establish a baseline collection of valid solutions. Subsequently, all the authors conducted a\ncomprehensive analysis of these filtered solutions through collaborative examination. Through careful observation\nand systematic review, we identified several key characteristics that distinguish high-quality reasoning chains:\nOptimal Structural Organization: The solution exhibits clear and well-organized structural formatting, with\nadaptive granularity in step decomposition. Particularly, it allocates more tokens and detailed elaboration at\ncrucial reasoning junctures while maintaining concise expressions for straightforward steps. This self-adaptive\napproach to step granularity ensures that complex transitions receive appropriate attention while avoiding\nunnecessary verbosity in simpler deductions.\nEffective Cognitive Scaffolding: High-quality solutions provide strategic educational support by gradually\nbuilding understanding through carefully structured explanations. This includes progressive concept introduction,\nclear articulation of key insights at critical points, and thoughtful bridging of conceptual gaps, making complex\nreasoning processes more accessible and learnable.\nRigorous Verification: High-quality solutions incorporate extremely frequent verification steps throughout the\nreasoning process. This includes validating intermediate results, cross-checking assumptions, and confirming the\nlogical consistency of each deduction, thereby ensuring the reliability of the final answer.\nBased on these identified characteristics, we developed a hybrid approach combining rule-based filtering and\nLLM-assisted curation to select high-quality solutions for each question identified in the previous section. This\nsystematic process ensures that each selected solution adheres to our established quality criteria while maintaining\nconsistency across the dataset. By focusing on a minimal yet meticulously curated set of reasoning chains, we\nembody the core principle of Less-Is-More: high-quality demonstrations, rather than sheer data volume, are key to\nunlocking complex reasoning capabilities. The resulting dataset D consists of carefully curated triples (q, r, a),\nwhere each reasoning chain r satisfies our quality criteria. By maintaining these stringent standards while limiting\nthe dataset size |D|, we aim to demonstrate that high-quality demonstrations, rather than large quantities of training\ndata, are crucial for unlocking complex reasoning capabilities."}, {"title": "4 Methodology", "content": "Based on the Less-Is-More principle, a model with substantial reasoning knowledge from pre-training and the\nability to perform long-chain reasoning at test time can develop robust reasoning abilities. After training on only a\nfew hundred instances of SFT data, the model learns to integrate meta-reasoning tasks into a cohesive reasoning\nchain."}, {"title": "4.1 Training Protocol", "content": "We fine-tune Qwen2.5-32B-Instruct using supervised fine-tuning on our LIMO dataset. The training process em-\nploys full-parameter fine-tuning with DeepSpeed ZeRO-3 optimization (Rajbhandari et al., 2020) and FlashAttention-\n2 (Dao, 2023), with a sequence length limit of 16,384 tokens."}, {"title": "4.2 Evaluation Framework", "content": "4.2 Evaluation Framework\nIn-domain Evaluation To comprehensively assess the models' performance across various reasoning capabilities,\nwe have established a diverse evaluation framework encompassing both traditional and novel benchmarks. Our\nprimary evaluation suite includes several well-established mathematical competitions and benchmarks: the Ameri-\ncan Invitational Mathematics Examination (AIME24), MATH500 (Hendrycks et al., 2021), and the American\nMathematics Competitions (AMC23).\nOut-of-distribution Evaluation To rigorously evaluate the models' performance on out-of-distribution (OOD)\ntasks, we carefully selected benchmarks that differ from our training data in various aspects. These benchmarks can\nbe categorized into three distinct groups:\nDiverse Mathematical Competitions: We furthur selected OlympiadBench (He et al., 2024), which represents\na distinct distribution of mathematical challenges to test models' OOD performance.\nNovel Multilingual Benchmarks: To minimize data contamination, we constructed several benchmarks using\nthe most recent examination problems: CHMath from the 2024 Chinese High School Mathematics League\nCompetition, Gaokao from China's 2024 National College Entrance Examination, Kaoyan from Chinese\nGraduate School Entrance Examinations, and GradeSchool, our newly developed benchmark for elementary\nmathematical reasoning. Notably, all problems in these benchmarks are written in Chinese, while our training\ndata contains no Chinese problems. This introduces an additional OOD dimension, assessing not only the\nmodel's ability to generalize across problem distributions but also its cross-lingual reasoning capabilities when\nconfronted with unseen languages.\nMulti-disciplinary Benchmarks: To assess broader generalization capabilities beyond mathematics (our\ntraining domain), we incorporated Miverva (Lewkowycz et al., 2022) (which includes undergraduate-level\nSTEM problems) and GPQA (Rein et al., 2023). These benchmarks evaluate reasoning abilities across multiple\ndisciplines and cognitive levels, providing insights into the model's capacity to transfer mathematical reasoning\nskills to broader contexts.\nPerformance metrics We evaluate performance using the pass@1 metric across our suite of benchmarks. All\nevaluations are conducted in a Zero-shot Chain-of-Thought (CoT) setting to better assess the model's reasoning\ncapabilities. For benchmarks including MATH500, OlympiadBench, Gaokao, Kaoyan, GradeSchool, MinervaMath,\nand GPQA, we employ a straightforward approach using greedy decoding with a single sample to assess correctness.\nHowever, for the smaller benchmarks containing fewer than 50 problems each (specifically AIME24, AMC23,\nand CHMATH), we implement a more thorough evaluation protocol, generating 16 samples with a temperature\nsetting of 0.7 and calculating the unbiased pass@1 metric as introduced in Chen et al. (2021). For problems where\nanswers are well-structured numerical values, we directly apply rule-based evaluations to check for mathematical\nequivalence. For more complex answer formats such as expressions, equations, or structured solutions-we\nleverage an LLM-based evaluator, which we have validated for high reliability. Throughout all evaluations, we\nmaintain a maximum output length of 32,768 tokens to minimize the potential for output truncation, ensuring our\nassessment captures complete problem-solving attempts. Additionally, when evaluating LIMO, we observed that\ninference-time scaling occasionally results in repetitive patterns at the end of lengthy outputs. In such cases, we\nextract the most likely final answer from the model's response for evaluation to ensure accurate assessment of its\nproblem-solving capabilities."}, {"title": "5 Experiment", "content": "5.1 Baselines\nWe compare LIMO against a comprehensive set of baselines with the following prominent models:\nOpenAI-01-preview (OpenAI, 2024), a large language model that has demonstrated advanced mathematical\nreasoning abilities across various complex tasks.\nQwQ-32B-Preview (Team, 2024b), a model specifically designed for mathematical problem-solving with strong\nreasoning capabilities.\nQwen2.5-32B-Instruct, which serves as our base model for comparative analysis.\nFor evaluation, we use the OpenAI API to access OpenAI-01-preview, while using VLLM (Kwon et al., 2023) to\ndeploy other open-weight models (e.g. QwQ-32B-Preview). To ensure fair comparison, all models follow the same\nevaluation protocol with identical inference hyper-parameters.\nTo investigate the impact of training data efficiency, we conduct comparative experiments using mainstream\nopen-source reasoning datasets for supervised fine-tuning on our base model. For a fair comparison, all experiments\nuse the same LLM backbone as LIMO, ensuring that performance differences are solely attributable to the training\ndata characteristics."}, {"title": "5.2 Main Results", "content": "OpenThoughts-114k:\u00b9 A synthetic reasoning dataset containing 114k examples covering mathematics, science,\ncoding, and puzzles. The solutions follow a structured reasoning format generated by DeepSeek-R1.\nNuminaMath-100k: A randomly selected 100k subset of NuminaMath-CoT, featuring mathematical problems\nranging from Chinese high school exercises to international mathematics olympiad competitions. Each solution\nfollows a Chain of Thought (CoT) format (Wei et al., 2022).\nThese datasets contain substantially more samples than LIMO's training set (817 examples), allowing us to examine\nthe relationship between data quantity and model performance.\n5.2 Main Results\nOur experimental results demonstrate LIMO's superior performance across both in-domain and out-of-domain\ntasks, as shown in Table 3.\nIn-domain Performance On in-domain tasks, LIMO achieves the best results across all benchmarks. For\nAIME24, LIMO achieves 57.1% accuracy, outperforming QwQ-32B-Preview (50.0%) and OpenAI-01-preview\n(44.6%) by significant margins. Most notably, on MATH500, LIMO achieves 94.8% accuracy, surpassing QwQ-\n32B-Preview (89.8%) and OpenAI-01-preview (85.5%). The performance gap is even more pronounced on AMC23,\nwhere LIMO reaches 92.0% accuracy compared to QwQ-32B-Preview's 83.6%.\nOut-of-domain Generalization LIMO demonstrates strong generalization capabilities across diverse out-of-\ndomain tasks. On OlympiadBench, LIMO achieves 66.8% accuracy, significantly outperforming QwQ-32B-Preview\n(58.5%) and the base model (45.3%). Similar improvements are observed on other challenging benchmarks such as\nCHMath (75.4% vs 68.5%) and GradeSchool (76.2% vs 63.8%). Notably, LIMO maintains competitive performance\neven on GPQA, where it achieves 66.7% accuracy, close to OpenAI-01-preview's leading score of 73.3%.\nComparison with Larger Datasets Our experiments reveal that despite larger scale, both baseline datasets\nunderperform compared to LIMO. NuminaMath-100k shows significant degradation (32.3% vs. base model's\n49.9%) due to uncurated reasoning chains, while OpenThoughts-114k achieves suboptimal results (58.3%) probably\ndue to unfocused problem selection. In contrast, LIMO's carefully curated 817 problems yield superior performance\n(72.8%), demonstrating that targeted selection and high-quality annotations are more crucial than data quantity for\ndeveloping robust reasoning capabilities.\nOverall Performance LIMO achieves the highest average performance of 72.1% across all benchmarks, sub-\nstantially outperforming OpenAI-01-preview (67.8%), QwQ-32B-Preview (66.4%), and other baselines. This\ncomprehensive evaluation demonstrates that LIMO's carefully curated training approach with just 817 examples\ncan outperform models trained on datasets that are orders of magnitude larger."}, {"title": "5.3 Analysis", "content": "5.3.1 RQ1: Impact of Reasoning Chain Quality\nTo gain a deeper understanding of why Less-Is-More achieves such remarkable results, we investigate the quality\nof reasoning chains (CoT). A fundamental question naturally arises: what characteristics define a high-quality\nreasoning chain that leads to superior model performance? To address this, we conducted a controlled comparative\nstudy examining how solutions of varying quality for the same problem statements affect the performance of models\ntrained on them.\nSetup To conduct this analysis, we selected 500 problems from the LIMO dataset. The selection was based on\nthe intersection of problems for which the models used in rejection sampling exhibited performance differences\nand those with corresponding human-annotated solutions, ensuring consistency across comparisons. For these\n500 problems, we collected and categorized solutions into five distinct quality levels based on our comprehensive\nevaluation framework. These solutions were sourced from various origins, including human experts, AI specialists,\nand model-generated responses, then classified strictly based on their reasoning quality rather than their source.\nQuality Measure Following the principles outlined in Section 3.3.2, we took a holistic approach to categorize\nthe reasoning chains into five quality levels (L1-L5, with L5 being the highest). Our assessment focused on several\nkey aspects: how well the steps were organized and connected, whether important logical transitions were properly\nexplained, and if the solution included self-verification steps to check the work. Using these general guidelines,\nwe classified L5 solutions as those showing excellent organization with clear, well-explained steps and thorough\nself-verification. L4 solutions were also well-structured but perhaps with slightly less rigorous checking. L3\nsolutions showed decent organization but sometimes skipped over explaining crucial logical leaps. L2 solutions"}, {"title": "5.3.2 RQ2: Impact of Question Quality", "content": "We hypothesize that more challenging problems foster complex reasoning chains, diverse thought processes,\nand enhanced knowledge integration, enabling LLMs to better leverage pre-trained knowledge for high-quality\ninference. To validate this hypothesis, we investigate how question quality affects the reasoning capabilities of\nmodels fine-tuned on these questions and their corresponding solutions.\nSetup We selected three sets of problems of similar size but increasing difficulty, constructing solutions in a\nconsistent manner to form three training datasets. Our findings indicate that models trained on more challenging\ndatasets exhibit superior reasoning performance. Specifically, we sampled three sets of problems, each containing\n500 samples, from MATH and AIME:\nSimple-500: 500 simple problems randomly selected problems from MATH levels 1 and 2.\nComplex-500: 500 complex problems randomly selected problems from MATH levels 3, 4, and 5.\nAdvanced-500: 500 advanced problems randomly selected problems from past AIME tests."}, {"title": "5.3.3 RQ3: LLM Backbone", "content": "Building on our LIMO hypothesis, which emphasizes the importance of latent prerequisite knowledge within the\nmodel's parameter space, we examine how different pre-training data affect a model's capacity to leverage minimal\nexemplars for mathematical reasoning. This investigation allows us to assess the first key factor of our hypothesis:\nthe role of pre-trained knowledge in enabling complex reasoning capabilities.\nSetup To isolate the impact of pre-training while controlling for model architecture and fine-tuning procedures,\nwe conduct experiments using two 32B-parameter variants of the Qwen model family: Qwen1.5-32B-Chat (Team,\n2024a) and Qwen2.5-32B-Instruct (the base model of LIMO). Both models share the same architecture and\nparameter count, while Qwen2.5 demonstrates significant improvements in pre-training data quality, particularly\nin mathematical and code-related data, compared to its predecessor. We SFT both models using identical LIMO\ndatasets and evaluation protocols, assessing their performance on the AIME2024 and MATH500 benchmarks.\nResults Our experiments reveal that the choice of pre-trained model dramatically impacts reasoning performance,\nas demonstrated in 4. LIMO, built on Qwen2.5-32B-Instruct, significantly outperforms its predecessor across both\nbenchmarks. On the challenging AIME2024 test, LIMO achieves 57.1% accuracy, a remarkable 47.1 percentage\npoint improvement over Qwen1.5-32B-Instruct's 10.0%. Similarly, on MATH500, LIMO demonstrates exceptional\nperformance with 94.8% accuracy, surpassing Qwen1.5-32B-Instruct by 34.4 percentage points. These substantial\nimprovements suggest that the enhanced pre-training in Qwen2.5 creates a stronger foundation for mathematical\nreasoning. The results align with our LIMO hypothesis, indicating that richer pre-trained knowledge within the\nmodel's parameter space enables more effective utilization of minimal exemplars during fine-tuning."}, {"title": "5.3.4 Case study", "content": "Qualitative Analysis Fig. 5 compares responses generated by Qwen2.5-32B-Instruct, DeepSeek-R1, and LIMO.\nLIMO achieves capabilities and behaviors comparable to DeepSeek-R1, despite using minimal data and\ncompute resources (only 817 training samples). Notably, LIMO demonstrates strong self-reflection and long\nchain-of-thought generation capabilities. LIMO verifies its own statements (\u201cWait, 24 minutes is 0.4 hours? Wait,\nno. Wait, 60 minutes is 1 hour, so 24 minutes is 24/60, which is 0.4 hours\") and validates its calculations (\"But let\nme check again. Maybe I made a mistake in calculations.\"). Furthermore, it learns to allocate additional tokens\n(compute) for detailed complex equation-solving (\u201cNow let's compute the left side, \u2026\u2026\u2026, multiply both sides by 2)\nto prevent errors. In contrast, the base model Qwen2.5-32B-Instruct exhibits limitations in its reasoning process,\nbeing unable to correct inaccurate statements and failing to cross verify equation 2 in its solution. These results"}, {"title": "Quantitative Analysis", "content": "Table 4 demonstrates the differences between models trained with varying sample quality.\nWe observe a general trend where increasing post-training example quality leads to models that generate longer\nresponses with more lines. Additionally, these higher-quality models employ more self-reflecting transitions (e.g.,\nwait, perhaps, maybe, therefore) to allocate additional inference tokens (compute) for deeper thinking."}, {"title": "6 Background and Related Work", "content": "6.1 Evolution of Mathematical Reasoning in LLMs\nLarge-scale training data has been the driving force behind the development of reasoning abilities in LLMs. In\nthe pretraining phase, the reasoning ability of LLMs can be enhanced by relevant corpora (Wang et al., 2024;\nAzerbayev et al., 2024; Paster et al., 2023; Shao et al., 2024). These curated corpora can be composed of multiple\nsources, such as textbooks, scientific papers, and mathematical code, which capture diverse human cognitive\npatterns used to solve problems. In the post-training phase, a line of research focuses on curating large-scale\ninstruction data to teach LLMs to reason (Yue et al., 2023, 2024; Li et al., 2024a). This includes scaling the number\nof questions and their corresponding solutions. The scaling approach is promising and has achieved significant\nperformance gains. However, the reasoning ability gained through this method has been criticized for relying on the\nmemorization of fixed patterns rather than achieving true generalization (Mirzadeh et al., 2024; Zhang et al., 2024).\nFor example, Mirzadeh et al. (2024) finds that LLMs exhibit noticeable variance when responding to different\ninstantiations of the same question, and their performance declines when only the numerical values in the question\nare altered. This raises doubts about the generalization capability of SFT methods (Chu et al., 2025) and whether\nLLMs can be true reasoners rather than mere knowledge retrievers (Kambhampati, 2024)."}, {"title": "6.2 Test-time Scaling and Long Chain Reasoning", "content": "Instead of focusing on scaling model parameters and training data (Kaplan et al., 2020), recent work has shifted to\nexploring test-time scaling (OpenAI, 2024; Snell et al., 2024), i.e., increasing the number of tokens to improve"}, {"title": "6.3 Data Efficiency in Language Models", "content": "Zhou et al. (2024a) demonstrates that with just 1,000 carefully curated prompts and responses, models can learn\nto follow specific formats and generalize well to unseen tasks. The findings emphasize the importance of quality\nover quantity in the alignment process. However, whether this lesson can be applied to reasoning tasks remains\nuncertain, given the potential high computational complexity of such tasks (Merrill and Sabharwal, 2024; Xiang\net al., 2025). While some work on reasoning highlights the importance of quality during the curation of training\ndata (Zhou et al., 2024b), the quantity of such data is still much larger compared to that in LIMA. Our work extends\nthe ideology of LIMA to reasoning tasks by investigating what constitutes high-quality questions and solutions, and\ndemonstrates that the reasoning ability of LLMs can be enhanced in a highly data-efficient manner."}, {"title": "7 Future Work", "content": "While LIMO demonstrates remarkable success in mathematical reasoning with minimal data", "Generalization": "First, extending the LIMO hypothesis to broader reasoning domains represents a critical\nnext step. While our"}]}