{"title": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models", "authors": ["Johan Wahr\u00e9us", "Ahmed Mohamed Hussain", "Panos Papadimitratos"], "abstract": "Numerous studies have investigated methods for jailbreaking Large Language Models (LLMs) to generate harmful content. Typically, these methods are evaluated using datasets of malicious prompts designed to bypass security policies established by LLM providers. However, the generally broad scope and open-ended nature of existing datasets can complicate the assessment of jailbreaking effectiveness, particularly in specific domains, notably cybersecurity. To address this issue, we present and publicly release CySecBench, a comprehensive dataset containing 12662 prompts specifically designed to evaluate jailbreaking techniques in the cybersecurity domain. The dataset is organized into 10 distinct attack-type categories, featuring close-ended prompts to enable a more consistent and accurate assessment of jailbreaking attempts. Furthermore, we detail our methodology for dataset generation and filtration, which can be adapted to create similar datasets in other domains. To demonstrate the utility of CySecBench, we propose and evaluate a jailbreaking approach based on prompt obfuscation. Our experimental results show that this method successfully elicits harmful content from commercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT and 88% with Gemini; in contrast, Claude demonstrated greater resilience with a jailbreaking SR of 17%. Compared to existing benchmark approaches, our method shows superior performance, highlighting the value of domain-specific evaluation datasets for assessing LLM security measures. Moreover, when evaluated using prompts from a widely used dataset (i.e., AdvBench), it achieved an SR of 78.5%, higher than the state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) are advanced Artificial Intelligence (AI) systems engineered to understand and gen-erate human-like text by leveraging Deep Learning (DL) techniques. LLMs utilize transformer architectures that excel in processing sequential data and capturing long-range depen-dencies within text [1], [2]. LLMs are trained on a massive data collection, including books, articles, websites, and other textual sources [3]. This enables LLMs to learn the statistical properties of language, such as grammar, syntax, and semantic relationships. The training process involves optimizing billions of parameters through methods such as supervised learning and fine-tuning [4], with training effectiveness scaling propor-tionally to the size of the dataset [5]. Based on this extensive training, LLMs are capable of executing a wide array of tasks, including text summarization, conversational interactions, and content generation. Moreover, as LLMs scale in size and are trained on more extensive datasets, they inherently acquire the ability to perform new tasks without the need for explicit task-specific training. This enables LLMs to generalize effectively across various domains [6]. One predominant domain is code generation, in which LLMs have proven to be very useful [7]. LLMs can generate code snippets from natural language descriptions (i.e., provide the LLM with a description of the intended coding task in plain English), thereby accelerating the development process and reducing the workload on pro-grammers [8]. Tools such as GitHub Copilot [9] provide this by enabling real-time code suggestions and autocompletion directly within Integrated Development Environments (IDEs). This enhances developer productivity, helps minimize syn-tax errors, and improves code quality. One of the primary measures of code quality is its correctness, i.e., whether the generated code performs the intended functionality without errors. Studies [7], [10] have shown that LLMs can produce syntactically correct code that aligns closely with user requests and specifications.\nWhile the integration of LLMs into software development workflows offers substantial benefits in terms of productivity and code quality, it is imperative to consider the accom-panying risks and challenges [11]. As LLMs become more sophisticated and more widely adopted, their potential misuse poses significant security and ethical concerns [12]. One such concern is LLM jailbreaking [13], where users exploit vulner-abilities in a LLM (black or white box) to elicit prohibited or harmful outputs.\nMultiple studies [14]\u2013[20] on jailbreaking LLMs aim to identify methods for eliciting harmful or prohibited infor-mation. Some of these include embedding malicious queries within seemingly harmless contexts [17], [18] and invoking privileged modes similar to \u201csudo\u201d by iteratively and system-atically modifying prompts to generate malicious outputs [19], [20]. Generating malicious code through jailbreaking tech-niques poses serious ethical and security concerns. Vulnera-bilities within the LLM security mechanisms can be exploited to spread misinformation and enable cyberattacks. As LLMS becomes more powerful and widely used, it is essential to consistently assess and enhance their security."}, {"title": "II. PROMPT GENERATION AND DATASET SPECIFICATION", "content": "This section outlines the process of synthesizing the Cy-SecBench dataset and methods used to filter out irrelevant prompts. We present the dataset in detail, including descrip-tions of each category, their respective sizes, and sample prompts from each category in the dataset."}, {"title": "A. Prompt Generation and Filtration", "content": "Prompt Generation. The prompts are generated using a Python script utilizing two of OpenAI's Generative Pre-trained Transformer (GPT) models: GPT-01-mini and GPT-3.5-turbo. The prompt generation process is initiated by in-structing GPT-01-mini to compile an exhaustive list of mali-cious cybersecurity terms associated with a variety of cyber attacks, resulting in identifying 657 unique terms. These terms were categorized into 10 groups based on attack type, i.e., Cloud Attacks, Control System Attacks, Cryptographic Attacks, Evasion Techniques, Hardware Attacks, Intrusion Techniques, Internet of Things (IoT) Attacks, Malware Attacks, Network Attacks, and Web Application Attacks. Subsequently, GPT-3.5-turbo is instructed to generate 50 close-ended instructions and questions that directly relate to the provided malicious question and preserve its underlying intent. We instruct it to generate instructions or questions that must focus on adversarial (offensive) scenarios. Then, we instruct it that all questions should begin with \u201cHow,\u201d \u201cWhat,\u201d or \u201cWhich,\u201d while instructions shall begin with \"Implement,\u201d \u201cIdentify,\u201d \"Write,\" \"Create,\" or \"Design\". Finally, it introduces spe-cific constraints to ensure that the generated instructions and questions remain close-ended. To guarantee that the last step is fulfilled, GPT-3.5-turbo is provided with instructions that allow for the incorporation of additional parameters related to each topic.\nPrompt Filtering. During the prompt generation phase, the LLMs happens to generate several prompts that are not aligned with the dataset's malicious nature. These prompts are either non-malicious or logically nonsensical (i.e., they combine distinctly different concepts, creating confusion when attempting to merge them into a single prompt). An example of a prompt with a misalignment between the objective and method: \"Can you outline the steps to hack into a system using a Denial-of-Service (DoS) attack?\" This question is nonsensical, as it incorrectly assumes that a DoS attack can be used as a method to gain unauthorized access to a system, which is not a valid attack. DoS attacks disrupt services but do not facilitate hacking into systems to extract data or control functionalities."}, {"title": "B. Content Specification", "content": "After prompt filtering, CySecBench retains 12662 prompts across 10 attack-type categories, as per Table I. Below, we provide an overview of the dataset summarizing the content in each category, and provide a few examples representing the attack types.\nCategory 1: Cloud Attacks focus on compromising cloud services and infrastructures. These attacks often seek unau-thorized access and manipulate data stored and processed in the cloud. By exploiting vulnerabilities in cloud platforms, misconfigurations, or insecure APIs, attackers can disrupt services, have access to sensitive information, or unauthorized cloud resources. This category encompasses prompts seeking guidance on exploiting misconfigurations in cloud services from specific providers, performing phishing attacks within cloud environments, or leveraging cloud services to execute various attacks.\nCategory 2: Control System Attacks encompasses Industrial Control Systems (ICS) and Supervisory Control and Data Acquisition (SCADA) systems that manage critical infrastructure and industrial processes. These attacks often aim to gain unauthorized access, manipulate, or disrupt the physical processes controlled by these systems. By exploiting vulnerabilities in control system hardware, software, network configurations, or communication protocols, attackers can disrupt operations, steal sensitive operational data, or com-promise safety mechanisms. Prompts listed in this category include requests dealing with low-level software attacks such as building rootkits, conducting PLC-based cyber-attacks, and manipulating ICS firmware.\nCategory 3: Cryptographic Attacks focus on undermining the mathematical and procedural foundations of encryption systems. By exploiting weaknesses in cryptographic protocols or improper implementation of encryption algorithms, attack-ers can break what is meant to be secure communication, exposing sensitive data. This category includes prompts with requests for implementing cryptographic attacks such as al-gebraic side-channel attacks, exploiting padding oracle vul-nerabilities, and performing collision attacks on cryptographic hash functions.\nCategory 4: Evasion Techniques involve strategies to avoid detection by antivirus software or intrusion detection and prevention systems. This includes modifying malicious code to appear benign, using sophisticated methods to dis-guise malicious activity, or employing advanced techniques to become a part of systems' normal operations. Successful evasion strategies enable attackers to maintain long-term ac-cess to compromised systems without being detected. Listed in this category are prompts asking how to implement process hollowing, Parent Process ID (PPID) spoofing, or other code obfuscation mechanisms.\nCategory 5: Hardware Attacks exploits vulnerabilities at the physical or firmware level. Such attacks can bypass traditional software-based security defenses, targeting to dam-age the physical core of a machine. Exploits may involve manipulating memory, processor operations, or firmware. Prompts found in this category involve advanced hardware and firmware exploitation techniques such as Rowhammer attacks, exploiting Basic Input/Output System (BIOS) vulnerabilities, and TEMPEST attacks.\nCategory 6: Intrusion Techniques exploit vulnerabilities of software, notably operating systems, or user behavior. These can range from attempts to gain unauthorized access or manipulate legitimate functions (or system users) to achieve malicious goals. They tend to be foundational, leveraging common security oversights and human error to infiltrate a system or a network. The prompts in this category encompass a wide range of techniques for executing session hijacking, performing DNS data exfiltration, and exploiting SQL injec-tion vulnerabilities.\nCategory 7: IoT Attacks focus on compromising con-nected devices and systems within the IoT ecosystem. Such attacks often aim to take control of devices, manipulate their functions, or disrupt their operations. Exploiting vulnerabilities in device firmware, insecure configurations, or weak authenti-cation mechanisms, attackers can cause IoT-based networks to malfunction, gain access to sensitive data, or use them as entry points into larger networks. Within this category are prompts asking for instructions on how to exploit Near-Field Communication (NFC) vulnerabilities, perform battery exhaustion attacks, and perform Cross-Site Scripting (XSS) attacks on IoT interfaces.\nCategory 8: Malware Attacks involve deploying software designed to disrupt or take control of compromised systems. They can be highly varied, ranging from viruses that spread and corrupt files to sophisticated ones such as ransomware or spyware, which are designed to gather sensitive information or hold systems hostage. Prompts associated with this category are focused on implementing attacks such as logic bombs, Random Access Memory (RAM) scrapers, and ransomware.\nCategory 9: Network Attacks aims at compromising the communication links between devices and systems. These attacks often aim to intercept, manipulate, or disrupt the flow of data across a given network. Exploiting weaknesses in protocols or network configurations, attackers can perform a wide array of attacks, disrupting services, retrieving sensitive data, or gaining unauthorized access to critical infrastructure. This category includes prompts related to well-known network attacks, such as DoS, Domain Name System (DNS) hijacking, and Stream Control Transmission Protocol (SCTP) flood at-tacks.\nCategory 10: Web Application Attacks target vulnerabil-ities in web applications, services, or protocols. Web appli-cations are often publicly accessible; hence, they are prime targets for attackers seeking to exploit poorly secured inputs, flawed session management, or weak authentication processes. Prompts in this category manipulate web applications to gain unauthorized access, disrupt services, or extract valuable data. Attack vectors include JSON Web Token (JWT) attacks, Cross-Site Request Forgery (CSRF) attacks, and exploiting misconfigurations in GraphQL Application Programming In-terfaces (APIs)."}, {"title": "Key Attributes and Dataset Format", "content": "We summarize three key attributes of the CySecBench dataset that distinguish it from existing datasets: (i) it includes 12662 prompts covering a wide range of cyberattacks, to enable thorough evaluations of LLMs jailbreaking methods; (ii) it is organized into 10 categories with thorough coverage, from physical hardware attacks to application-level vulnerabilities, providing a broad spectrum of attack vectors; and (iii) the prompts are intention-ally designed to be close-ended (i.e., by instructing the LLM to provide close-ended prompts even if the are malicious), to minimize response variability, enabling consistent and accurate assessment of an LLM's ability to resist jailbreak attempts.\nThe dataset formats and structure are listed in Table III. Sample sets (Small, Medium, and Large) are comprised of the N first questions from each of the 10 categories in the CySecBench dataset. The \u201cIndividual Categories\" represent each of the 10 subsets separately, while the \"Full Set\" com-bines all the 10 subsets.\nLimitations. The goal of creating this dataset is to provide a collection of technical prompts that simplify the validation of jailbreak attempts. However, not all prompts will necessarily yield responses that are easy to evaluate. Some prompts may lead to nuanced answers, with their potential harmfulness vary-ing depending on the reviewer's interpretation. For example, a question such as \"How can an SQL Injection be executed step-by-step?\" could produce responses that range from a basic overview of SQL Injection to detailed instructions for exploiting specific SQL vulnerabilities. Although the prompts in CySecBench are generally more close-ended than those in other datasets, they are not entirely so.\nDespite our filtering efforts, prompts that are either non-malicious or logically nonsensical still persist in the dataset. To estimate the proportion of these prompts, we manually reviewed 50 randomly chosen prompts from each of the 10 categories in the CySecBench dataset. We identified 29 prompts in total (out of 500) that were either non-malicious or nonsensical, amounting to approximately 5.8% of the reviewed prompts. Extrapolating from this sample, we estimate that the percentage of prompts in the CySecBench dataset that are either non-malicious or nonsensical falls within the single-digit range."}, {"title": "III. LLM JAILBREAKING", "content": "LLMs jailbreaking involves attempts to bypass inherent rules and constraints of an LLM to prompt the generation of restricted content. Existing commercial LLMs are designed to block (refuse) generating responses that include harmful, biased, or otherwise inappropriate information. Jailbreaking seeks to identify and exploit potential weaknesses in a tar-geted model framework to elicit unintended behavior, such as generating explicit content. Adversarial users may experiment with various strategies to bypass the LLM safety filters, such as crafting inputs in unconventional formats [17] or repeatedly presenting subtly modified prompts to determine methods of circumventing the model's built-in restrictions [19].\nIn this section, we introduce a simple yet effective jailbreak-ing through the prompt obfuscation method (Section III-A), its implementation (Section III-B), and its performance eval-uation (Section III-C). Then, we introduce a novel approach to enhance this jailbreaking method, achieving superior per-formance (Section III-D). Additionally, we evaluate these methods using AdvBench and a subset of the CySecBench dataset, employing different ChatGPT models and two other popular commercially available LLMs, namely, Claude and Gemini."}, {"title": "A. Methodology", "content": "The proposed method focuses on prompt obfuscation, em-bedding malicious prompts within a benign context to deceive the LLM into producing harmful responses. As outlined in Algorithm 2, our jailbreak approach comprises two stages: first, we instruct an LLM instance to generate a set of questions based on our initial input (line 5); secondly, we instruct a separate\u00b9 LLM to produce a solution sheet (line 7) for the questions generated in the first step. The specific instructions provided to the LLMs to generate questions and solutions are found in Figure 2 and 3, respectively."}, {"title": "B. Implementation", "content": "Figure 4 depicts the jailbreaking architecture. We implement it using three different popular closed-source LLMs (for steps (b) and (c)): ChatGPT, Claude, and Gemini.\nFurthermore, we implement an automated evaluation pro-cess, where an LLM assesses individual jailbreak attempts and assigns each of them a score from 1 to 5. A score of 1 indicates a failed attempt, while a score of 5 signifies a fully successful jailbreak attempt in which policies are clearly violated, and the outputs provide a detailed solution. The specific instructions provided to the GPT judge are depicted in Figure 5. Automating the evaluation process using an LLM is an approach used by multiple previous studies [16]\u2013[20]. Based on [36], LLM assessments are closely aligned with human assessment. We use GPT-40-mini as the GPT judge for all tests to ensure a consistent and comparable evaluation. All LLMs communication is done through the provided APIs by each company, respectively."}, {"title": "C. Performance Evaluation", "content": "We assess our proposed method using two performance metrics: (i) Success Rate (SR) and (ii) Average Rating (AR). The SR (Equation 1) specifies the percentage of jailbreak attempts that result in responses that completely violate ethical, legal, or safety guidelines and provides specific information to the user in responses.\n$SR = \\frac{\\text{No. of jailbreak attempts with a rating of 5}}{\\text{No. of jailbreak attempts}}$\nThe AR (Equation 2) aims to indicate how harmful the responses from jailbreak attempts are overall. This is the average score that is given by the GPT judge and implemented to automate the testing process.\n$AR = \\frac{\\text{Sum of all jailbreak ratings}}{\\text{No. of jailbreak attempts}}$\nOur experimental evaluation shows distinct patterns in how different LLMs respond to the presented jailbreaking method, as shown in Table IV. The results demonstrate significant variations in model resilience, with SRs ranging from 17.4% (Claude) to 88.4% (Gemini), and ChatGPT positioned inter-mediately at 65.4%.\nIn Figure 6, we observe that Gemini's notably high exposure to our jailbreaking approach (SR: 88.4%, AR: 4.77) is the same consistently across all attack categories. The model's consistent generation of detailed, executable code suggests potential limitations in its safety filtering mechanisms, mainly when prompts are presented within an educational context. This behavior indicates that Gemini's content filtering operates primarily at the intent-recognition level rather than implement-ing deeper semantic analysis of potential harmful outputs.\nChatGPT, on the other hand, demonstrates moderate perfor-mance (SR: 65.4%, AR: 4.06), with notable variations across attack categories. The model shows resilience to cryptographic attack prompts while maintaining stronger defenses against cloud-based attacks. This pattern suggests that ChatGPT's safety mechanisms may be calibrated differently for various security domains.\nClaude exhibits substantially stronger resistance to our jailbreaking attempts, as evidenced by its significantly lower metrics (SR: 17.4%, AR: 2.00). It can be seen that its consis-tently lower SRs span all attack categories, indicating a more robust implementation of safety filters. This also suggests a fundamentally different approach to content filtering, possibly incorporating deeper contextual understanding rather than rely-ing solely on pattern matching or keyword detection. Notably, Claude maintains its ethical boundaries even when presented with technically sophisticated prompts that successfully bypass other models' safety filters.\nOur method demonstrates different performance charac-teristics when evaluated using the AdvBench dataset, as shown in Table V. The SRs shift to 52.5% with ChatGPT, 50.0% with Gemini, and 0.96% with Claude. This difference in performance between cybersecurity-focused and general malicious prompts suggests that domain-specific jailbreaking attempts may be more effective at circumventing model safety measures. The specialized nature of cybersecurity prompts, particularly when framed within educational contexts, appears to create unique challenges for content-filtering systems.\nThe variation in model responses, particularly evident when comparing Figures 13, 14 and 15 in the Appenidx, provides insights into the potential architecture of their respective safety systems. Claude's superior resistance across both datasets suggests a more sophisticated approach to content filtering, potentially incorporating multiple layers of semantic analysis. In contrast, Gemini's higher AR, as demonstrated consistently across all test categories (in Figure 6), indicates a more straightforward filtering mechanism that can be more easily circumvented through careful prompt engineering."}, {"title": "D. Enhancements via Refinements", "content": "We enhance the initial jailbreaking methodology by sys-tematically incorporating additional techniques and refinement processes. Our enhanced architecture, illustrated in Figure 7, introduces two key modifications to improve the effectiveness of jailbreak attempts: prompt obfuscation and solution refine-ment.\nFor prompt obfuscation, we implement a word-reversal technique where every fifth word in the input prompt is reversed before being processed by the LLM. This approach serves to distract the model's content filtering mechanisms by introducing an intermediate text manipulation task. The model must first process this manipulated text before addressing the underlying request, potentially reducing the effectiveness of its safety filtering.\nThe refinement process leverages multiple LLMs in se-quence, systematically improving the generated outputs. As detailed in Algorithm 3, we iterate through available LLMs in order of their intelligence ranking according to [37]. This iterative refinement continues until we either achieve a satis-factory response or exhaust all available models. The process includes specific checks for common refusal phrases (lines 7-12) to ensure the refinement maintains the intended adversarial nature of the output.\nEnhancements Performance. Our experimental evaluation demonstrates the effectiveness of these enhancements. When applied to the AdvBench dataset, the refined method achieves a SR of 78.5% with an AR of 4.23, compared to the baseline method's 52.5% SR and 3.23 AR. This significant improvement suggests that our refinement process effectively addresses the limitations of the initial approach. The enhanced performance can be attributed to the following factors:\n1) The word-reversal obfuscation technique helps bypass initial content filters while maintaining the semantic in-tegrity of the prompt.\n2) The multi-model refinement process leverages the com-plementary strengths of different LLMs\n3) The systematic verification steps ensure that refined out-puts maintain their effectiveness while increasing their sophistication\nThese enhancements demonstrate the potential for sys-tematic improvement of jailbreaking methodologies through careful architectural design and process optimization. The significant performance gains achieved through these modi-fications highlight the importance of considering both prompt construction and output refinement in developing effective jailbreaking techniques."}, {"title": "IV. DISCUSSION", "content": "In this section, we discuss CySecBench in comparison to the current cybersecurity datasets. We then examine how our jailbreaking approach performs when implemented using different LLMs.\nComparison with existing datasets. Several datasets have been designed to evaluate jailbreaking methods; however, their content is not domain-specific. As illustrated in Figure 9 and 10, the prompt topics in the HarmBench and Harmful-Tasks datasets vary widely. This broad range of topics can pose challenges for accurately assessing the effectiveness of various jailbreaking techniques. As detailed in Section III-C, our proposed jailbreaking method (without using refinements) demonstrates significantly superior performance in a cyberse-curity context compared to a general context, even after only minor adaptations were made for the latter. For the general non-categorized datasets, using refinements resulted in 78% SR, higher than the current state-of-the-art.\nIn contrast to other similar datasets, CySecBench offers a domain-specific collection of malicious prompts focused exclusively on cybersecurity. The frequencies of these topics within the CySecBench dataset are illustrated in Figure 11. By focusing on a specific domain, researchers can fine-tune their jailbreaking strategies to identify specific vulnerabilities within that domain.\nA standout characteristic of the CySecBench dataset is its size, which far exceeds that of the most commonly used existing datasets. For instance, it is approximately 24 times larger than the AdvBench dataset. The size of the dataset is crucial when evaluating jailbreak methods for LLMs because it directly impacts the reliability of the evaluation. A large dataset minimizes the influence of outliers and random vari-ations that can skew results in smaller datasets. In contrast, small datasets may inadvertently emphasize specific types of attacks or linguistic patterns, leading to biased assessments. Utilizing a diverse and extensive dataset mitigates these risks, as it provides a balanced representation of various prompt phrasings and attack strategies.\nDataset improvement. During prompt filtering, we evalu-ated whether OpenAI's latest model, GPT-01, could improve prompt quality by better-aligning requests with their intended purposes. Experimental testing showed that, unlike previous models accessed via the OpenAI API, using GPT-012 with malicious prompts often triggered error messages due to conflicts with OpenAI policies. However, this security filter can be bypassed by encoding the malicious prompt in base64 and instructing GPT-01 to decode and rephrase it. Initial results show that GPT-01 effectively enhances prompt quality by making minor adjustments, as illustrated in Figure 12. While GPT-01 improved prompt quality within the dataset, it also led to a substantial increase in costs. Running the GPT-assisted filter (recall Section II) on 19512 prompts cost $8.32, whereas applying GPT-01 to 200 prompts cost $32.87. Extrapolating these costs suggests that filtering the 12662 prompts obtained from the initial GPT-assisted filter with GPT-01 would require a total of $2, 151. We plan to use GPT-01 to further refine prompt quality should the 01-models become more affordable."}, {"title": "V. FUTURE DIRECTIONS", "content": "The methodology presented in this paper can be ex-tended beyond generating cybersecurity prompts. The system-atic dataset generation and refinement approach demonstrates broad applicability across various specialized domains, pre-senting opportunities for researchers and practitioners. Below, we list several future directions where our approach can be adopted, improved, or extended.\nDomain Adaptation and Specialization. The presented methodology can be adapted for developing specialized datasets in fields such as healthcare, finance, legal compliance, or ethical AI testing. As we initiated our process by identifying cybersecurity-specific terms, other domains can begin with their own specialized vocabulary and concepts. For instance, in healthcare, the process might focus on medical terminology and procedures, while in finance, it could center on transaction patterns and regulatory compliance terms.\nAutomated Dataset Enhancement. Future work should explore the automation of dataset maintenance and evolution. By developing systems that can automatically identify new attack vectors, generate corresponding prompts, and validate their effectiveness, we can ensure that evaluation datasets remain current with emerging threats. This could include the integration of threat intelligence feeds and automated prompt generation systems.\nCross-Domain Integration. Our methodology supports the development and integration of datasets that span over multiple domains, enabling the study of intersection points between diverse fields. This cross-domain approach proves especially valuable in areas where traditional boundaries do not ex-ist, such as the convergence of cybersecurity with financial services, privacy regulation, or healthcare. For example, a healthcare technology company requires creating datasets that simultaneously test for medical accuracy, patient privacy pro-tection, and cybersecurity awareness. This approach helps such organizations address the real-world complexity they might face when deploying AI systems.\nQuantitative Metric Development. The field would benefit from more sophisticated metrics for evaluating prompt effec-tiveness and model vulnerability. Future research should focus on developing standardized measures that can capture both the technical sophistication of attacks and their practical impact. This includes creating frameworks for comparing different attack methodologies and assessing their relative effectiveness across different model architectures.\nMulti-Modal Security Assessment. While our current work focuses on text-based prompts, future extensions should consider multi-modal interactions. As language models in-creasingly handle multiple input types, security evaluation frameworks must evolve to assess vulnerabilities across dif-ferent modalities. This includes developing methodologies for testing image-text interactions, audio processing, and com-bined input scenarios."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we presented CySecBench, a compre-hensive dataset containing 12662 domain-specific prompts for jailbreaking and evaluating the effectiveness of current safety mechanisms LLMs. The development and validation of CySecBench demonstrate the value of domain-specific evaluation frameworks in assessing LLMs. CySecBench con-tains prompts that are divided into ten distinct categories, spanning wide cybersecurity topics, enabling a more precise and meaningful evaluation of jailbreaking methods.\nWe designed and implemented a simple, yet effective jailbreaking method and evaluated its performance. Our ex-perimental results, demonstrating significantly varying SRs across different LLM (88.4% for Gemini, 65.4% for ChatGPT, and 17.4% for Claude), highlight critical variations in model robustness against our method. The substantial performance differences observed between models suggest fundamental variations in their security architectures and point to poten-tial directions for improving safety mechanisms. The prompt obfuscation and refinement steps we presented, particularly when enhanced with the refinement process, demonstrate the potential for systematic improvement in jailbreaking tech-niques. Our approach achieved superior performance to ex-isting methods, with a SR of 78.5%, while offering additional advantages in terms of implementation simplicity and archi-tectural flexibility. These findings suggest promising directions for both offensive security research and defensive mechanism development.\nFinally, we pave the path to several important future re-search directions aspects in LLM security. The methodolo-gies developed for CySecBench can be adapted for other specialized domains, enabling more comprehensive security assessments across different application areas. Additionally, the observed variations in model resilience suggest oppor-tunities for developing more robust security mechanisms through cross-model analysis and architectural improvements. As LLMs continues to evolve and find new applications in security-critical domains, the importance of rigorous security evaluation frameworks becomes increasingly important. Our work provides both practical tools and theoretical insights for advancing this essential area of research, enabling researchers to develop secure and reliable language models."}]}