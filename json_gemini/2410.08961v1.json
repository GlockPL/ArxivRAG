{"title": "Evaluating Federated Kolmogorov-Arnold Networks on Non-IID Data", "authors": ["Arthur M. Sasse", "Claudio M. de Farias"], "abstract": "Federated Kolmogorov-Arnold Networks (F-KANs) have already been proposed, but their assessment is at an initial stage. We present a comparison between KANS (using B-splines and Radial Basis Functions as activation functions) and Multi-Layer Perceptrons (MLPs) with a similar number of parameters for 100 rounds of federated learning in the MNIST classification task using non-IID partitions with 100 clients. After 15 trials for each model, we show that the best accuracies achieved by MLPs can be achieved by Spline-KANs in half of the time (in rounds), with just a moderate increase in computing time.", "sections": [{"title": "Introduction", "content": "Since the paper by [21], many works have investigated Federated Learning potential as a privacy-focused alternative to centralized Machine Learning (ML) models, offering the possibility of collaborative ML without sharing users' data, just models. Seven years later, there are still challenges to the real-life application of federated learning, like guaranteeing high accuracies across clients with non-iid data and reducing communication costs between clients and servers as described by [20].\nIn 2024, [19] proposed a new kind of neural network that could help tackle these problems. The Kolmogorov-Arnold Networks (KANs) promise higher accuracy with fewer parameters than Multi-Layer Perceptrons, the traditional neural networks. Having a completely different structure, inverting the roles of edges and nodes in a neural network, and built with learnable activation functions, KANS also offer the possibility of creating more interpretable and lean models [19], which in turn could help scientists discover even symbolic formulas for their tasks. The paper by [19] has not yet been officially published or peer-reviewed, but since it's been made available on Arxiv\u00b9, about 199 studies have been produced exploring the possibilities opened by this new paradigm in ML, according to Google Scholar.\nKANs have already been combined with other relevant concepts in ML, like convolutional models by [5], or graph neural networks by [14]. Similarly, [28] and [7] initiated studies exploring KANS in federated learning. The potential for achieving higher accuracies with fewer parameters than MLPs and avoiding catastrophic forgetting, as described by [19], could be useful for handling communication restraints and heterogeneous data across clients, respectively. However, there is no evidence to guarantee the convergence rate of F-KANs, and the few experimental evaluations publicly available do not explicitly address the non-iid problem, nor do they compare the F-KANs against federated MLPs with a similar number of parameters. Also, initial empirical evaluation of KANS against MLPs suggests that the execution times of the former could be ten times larger than those of the latter, threatening its use in practice.\nThis work aims to fill the gaps described above by conducting simulations with federated learning for KANs and MLPs under experimental conditions that are more similar to typical studies in FL."}, {"title": "Related Works", "content": "The original KAN paper by [19] presented the theory behind these networks and focused on empirical evaluations with physical and mathematical problems having small associated datasets. Our study focuses on a federated setup with a much larger dataset (MNIST), which is classically used for model evaluation in the Machine Learning area.\n[17] presented a new type of KAN, using radial basis functions as activations functions, and made the implementation available in GitHub as a package for Python called FastKAN. The author claims that FastKAN is faster than the traditional B-Spline implementation based on an evaluation with a centralized MNIST classification task. We use FastKAN to classify our pathologically partitioned MNIST dataset.\n[28] coined the term \"F-KAN\", presenting the method for federating KANs, based on the FedAvg algorithm for MLPs. The authors evaluated the proposed model through a classification task using the Iris dataset [12]. The Iris dataset consists of data for 50 samples of flowers from three different species, described by four features. The dataset is partitioned between two clients in an iid way. The simulation comprised one execution for 20 rounds of federated learning, comparing an MLP and a Spline-KAN model, each with three hidden layers of 20 neurons. The KAN model showed superior performance for four metrics: accuracy, recall, precision, and F1-score. This work also compares KANs and MLPs in a federated setting. However, we change some parameters in trying to simulate a more realistic FL configuration: instead of the Iris Dataset, we use MNIST, which comprises 70,000 examples (60,000 for training and 10,000 for testing) from 10 classes of handwritten digits, represented by images with 28*28 pixels (784 features); we divide the data over 100 clients in a non-iid manner, and only 10% of clients participate in each round, simulating clients that are not always available; we run 15 executions for each model to achieve more statistically significant results; we compare MLPs not only against Spline-KANs but also against RBF-KANs; we run 100 communication rounds to capture more of each model behavior through time; and we reduce the number of KAN neurons so both models have a similar number of parameters. Table 1 shows the detailed comparison between each study."}, {"title": "Methodology", "content": "This work is guided by two research questions and their respective metrics and hypotheses.\n\u2022 Research Question 1\nQuestion: Do KANs achieve higher test accuracies than MLPs across 100 rounds of federated learning in the MNIST classification task?\nMetric: Test accuracy of the global models for rounds [10, 20, 30, 40, 50, 60, 70, 80, 90, 100].\nNull Hypothesis: KANs achieve a test accuracy lower than or equal to that of MLPs in a given round of federated learning.\nAlternative Hypothesis: KANs achieve a higher test accuracy than MLPs in a given round of federated learning.\nHypothesis Test: Welch's t-test [26], since we can't assume the models' accuracies will have the same variances, and it's a robust test against deviations from normality [22].\n\u2022 Research Question 2\nQuestion: What is the relationship between the execution times of KANs and MLPs for 100 rounds of federated learning in the MNIST task?\nMetric: Execution time, in seconds.\nMethod: Calculate the confidence interval for the ratio between the execution times of KANs and MLPs, using bootstrapping [11] (10,000 bootstrapped samples of size 15).\nExperimental Setup\nTo make a fair comparison between KANs and MLPs, we defined a maximum number of parameters for each model. First, we fix the architecture of the MLP classifier, inspired by one of the models studied on the FedAvg paper by [21], a [28*28, 200, 200, 10] neural network. Based on that, we built a Spline-KAN model with the same number of layers, calculating the number of neurons needed in each layer to achieve, at most, the same number of parameters used by the MLP, resulting in a [28*28, 24, 24, 10] network. We repeated this architecture for the RBF-KAN. Table 1 details the resulting architectures and hyperparameters.\nWe implemented our simulation using PyTorch ([2]) and the Flower ([3]) framework for federated learning. We used the efficient-kan implementation ([4]) of Spline-KANs, whose contribution to enhancing KAN's performance has already been acknowledged by the authors of the original KAN paper in a subsequent article ([18]). For RBF-KANs, we used [17] implementation. Our code repository is available at https://github.com/artsasse/fedkan.\nAll simulations were run in the Google Colaboratory \u00b2 environment, with an Intel Xeon CPU @ 2.20 GHz with two virtual CPUs and 13 GB RAM, alongside a NVIDIA Tesla T4 GPU with 15GB VRAM.\nThe local models were aggregated by the server using the FedAvg algorithm ([21]) with momentum, since [10] proved how using momentum can make FedAvg converge without needing to use decaying learning rates, thus simplifying our setup.\nThe MNIST ([15]) is a handwritten digits dataset commonly used for evaluating classification models. We used its original split, with 60,000 samples for training and 10,000 for testing. We kept the test samples together for centralized evaluation of the global model at the end of each round of FL. The training set was \"pathologically\" partitioned between 100 clients so that each client holds examples for only two digits, guaranteeing the non-iid property, a common challenge for real-life FL applications. Figure 1 shows graphically how the data was partitioned. Overall, the partitions are slightly balanced: the number of examples for each client varies between 488 and 792, with mean = 600. This skewed division was inspired by [21] and [16]. However, the former used balanced partitions, and the latter created unbalanced partitions following a power law.\nSince we only select a fraction of the clients for training in each round of FL, the training accuracy is calculated using different data for each round of communication. Thus, its values could change dramatically from one round to the next even if the global model stayed the same."}, {"title": "Results", "content": "To answer our Research Question 1, we gathered the test accuracy metric, represented in the graph from Figure 2, for each of the 100 rounds of FL. Intuitively, the Spline-KAN appears to have the overall best performance, followed by the MLP, while the RBF-KAN shows very low accuracy, even in the final rounds of communication. For completeness, in Figure 3, we also represent the training accuracy (3a), the test loss (3b), and the training loss (3c), which show tendencies aligned with what we observe for the test accuracy.\nSince the intersecting error bands can confound our visual analysis, we also present the test accuracies for ten different rounds of our simulation in Table 2, showing each model's mean and standard error. The results of the actual hypotheses tests are represented further in Table 3, where we present the p-values for the Welch's t-test [26]. Comparing Spline-KANs and MLPs, for a 90% significance level, we can refute the null hypothesis for all the selected rounds. For a 95% significance level, we refute the null hypothesis from round 10 to 70. The differences between mean test accuracies are greater than 20% from round 10 to round 60. Besides the statistically significant superiority of Spline-KANs during all the selected rounds, we note that the Spline-KAN's test accuracy increases faster during the first half of the simulation, slowing down to almost a linear increase for the second half. The MLPs presented a practically linear increase during the 100 rounds. On average, the best accuracies are achieved by the MLPs during the last two rounds, while similar values had already been achieved by Spline-KANs in round 40. The RBF-KANs also showed a linear-like increase in its metric, but much slower than the MLPs, reaching a maximum average accuracy a little higher than 0.2. We conjecture that RBF-KANs may need other aggregation algorithms, more adapted to its parameters than FedAvg, to reach good accuracies, which should be investigated in future works."}, {"title": "Limitations", "content": "A more realistic way to assess federated learning concerning data partitioning would have been to utilize the FEMNIST (Federated Extended MNIST) dataset [9]. Instead of creating an artificial posthoc partition, FEMNIST gives each client access to data from just one handwritten digit author, generating naturally non-i.i.d partitions. In the future, we plan to evaluate F-KANs on FEMNIST and other tasks besides digit classification.\nConvolutional models are generally considered a better option for classifying images than fully connected networks [13]. We are unsure if using convolutional networks would cause the disparity between MLPs and KANs to grow or shrink. Even though convolutional KANs have already been proposed [5], because these works are relatively recent, we believe potential flaws in these early implementations could skew the results and jeopardize the study's validity.\nThere is still no standard method for comparing KANs and MLPs for the same tasks. In an effort to provide a fair comparison in terms of each model's generalization capacity, we decided to utilize a similar number of parameters. On the other hand, arbitrary choices made about the construction of both models could have affected the outcomes in unknown ways. In future works, we intend to perform a hyperparameter optimization to make a fairer comparison between each type of neural network. Also, there are still other activation functions available for KANs that were not investigated in this work, like wavelets [8], ReLU [23], Chebyshev [25], Jacobi [24] [1] and Fourier [27] functions.\nSince MLPs have been used much longer than KANs, many specific software and hardware optimiza-tions not used in this work might drastically reduce the MLP execution times. However, if KANs become a popular model for the machine learning community, new optimizations and performance tricks are expected to be created for them, too."}, {"title": "Conclusion", "content": "In this work, we compared two types of KANs against an MLP model over 100 rounds of federated learning for image classification. We focused on the test accuracy for many simulation phases and the total execution time. Spline-KANs performed better than MLPs during all the simulation phases, especially during the first 60 rounds, showcasing how these types of KANs could be an alternative to achieve higher accuracies in fewer rounds of FL. RBF-KANs reached accuracies well below those of MLPs, demonstrating how the KAN activation function may strongly impact its performance. Both KAN implementations observed execution times at most 1.36\u00d7 the MLP's execution time, which is way faster than initial implementations of KAN achieved in centralized and federated simulations. Before obtaining a theoretical explanation for the differences between each model, it is impossible to estimate the generality of these results for different FL configurations. Regardless, this highly significant result indicates that KANs could be an interesting path to solve some of the most relevant problems in federated learning."}]}