{"title": "A Measure of the System Dependence of Automated Metrics", "authors": ["Pius von D\u00e4niken", "Jan Milan Deriu", "Mark Cieliebak"], "abstract": "Automated metrics for Machine Translation have made significant progress, with the goal of replacing expensive and time-consuming human evaluations. These metrics are typically assessed by their correlation with human judgments, which captures the monotonic relationship between human and metric scores. However, we argue that it is equally important to ensure that metrics treat all systems fairly and consistently. In this paper, we introduce a method to evaluate this aspect.", "sections": [{"title": "Introduction", "content": "Recent years have seen significant advances in machine translation (MT), marked notably by the introduction of the transformer architecture (Vaswani et al., 2017). Current large-scale commercial systems such as GPT (Brown et al., 2020) continue this trend and show promising results (Kocmi et al., 2023; Hendy et al., 2023; Wu and Hu, 2023). A critical supplement to these advancements is thorough and reliable evaluation procedures, which are essential not only for measuring overall progress but also for effectively comparing different systems. While evaluation based on human raters is still considered the gold standard, it is expensive and time-intensive. Therefore, considerable efforts have been made to develop automated metrics for assessing translation quality. Notably, the WMT Metrics series of shared tasks are dedicated to this purpose (Freitag et al., 2023, 2022, 2021, i.a.). Automated metrics usually assign a scalar\u00b9 quality rating to a candidate translation based on the source segment and a reference translation. A system-level rating is derived by averaging the segment ratings over a test set.\nTo measure a metric's usefulness, we usually measure two aspects: its correlation to human judgments on the segment-level (which checks if there"}, {"title": "Averaging Metric Scores", "content": "Assume we are given a set of K machine translation systems \u03c0\u03ba to evaluate. A translation system maps an input sentence i \u2208 I in a fixed source language to an output sentence o \u2208 O in a fixed target language: \u03c0\u03ba : I \u2192 O. The usual human evaluation scenario involves curating a test set of N inputs T = {ii)|1 \u2264 j \u2264 N} c I for which we collect the output of each system \u03c0\u03ba for each input i\u2208 T, and then ask human annotators to produce ratings. This results in a set of ratings where h\u2208 R is a scalar rating provided by human annotators measuring the quality of the translation provided by \u03c0\u03ba for input i(i). We will assume that higher human ratings indicate higher translation quality. In this setting, it is natural to measure the overall quality of system \u03c0\u03ba by the average human rating it achieves \\mu_H^{\\kappa} = \\frac{1}{N} \\sum_{j=1}^{N} h_{(j)}^{\\kappa}. This is an estimator of the expected human rating \u03bc\u03b5 = E[hk] achieved by \u03c0\u03ba for any input in I, assuming that T is appropriately chosen.\nIn many cases, we want to replace human raters with an automated scalar metric M : IXO \u2192 R, which maps an input and translation to a scalar value. For our test set T, we can collect all metric ratings where \\mu_M^{\\kappa} = \\frac{1}{N} \\sum_{j=1}^{N} m_{(j)}^{\\kappa} to measure the quality of system \u03c0\u03ba, which is an estimator of the expected metric rating \u03bc\u039c = E[mk] achieved by \u03c0\u03ba.\nThe goal of automated evaluation is to use M as a proxy measure for ut, in particular, to rank the systems \u03c01,...,\u03c0\u03ba according to their performance. In the following, we will study the relationship between \u03bc and M, which is expressed by an unknown function fe that maps from the metric scale to the human scale. There are two requirements to this function: first, that it is monotonic (i.e., that it respects the order of the metric scale), and second, that it does not depend on the system under evaluation \u03c0\u03ba (i.e., that it is the same for all systems) The goal is to find the relation between \u03bc\u03b5 and M. The idea is to express E[hk] in terms of an expectation over metric ratings as follows (for full derivation, see Appendix A): The crucial element of Equation ?? is the conditional expectation Epk(h) [hm]. Here we consider the expectation according to pk(h), the distribution of human ratings for system \u03c0\u03ba. Equation ?? describes the relationship between \u03bc\u03b5 and \u00b5 by expressing the expected human rating in terms of an expectation over metric ratings. We interpret this element as a function fk, which takes a metric rating as input and returns the expected human rating. Equation ?? yields a function fk for each system separately, which is not necessarily the same across systems. At this point, we can restate the introductory discussion using our formalism.\nWhen averaging metrics \u00fb to rank systems, we implicitly assume that there is a global function f that is equal to all the system-specific functions fk, i.e., fg = f1 = ... = fk, and thus, only measure if fg is monotonic (through correlation to human judgments). However, as shown in Figure 1, this assumption does not hold in practice (where blue is fg, and we have an fk for the two other systems respectively). To show that this is insufficient, we consider the effects of violating the assumption. Let us assume f1 \u2260 f2, but both are monotonic. Consider the extreme example that \u03bc = 1, i.e., systems \u03c0\u2081 and 2 are of the same quality under the metric. However, consider the case f1(m) = f2(m) + C, C > 0. Then; f1(m\u00b3)) = C + \u03a3; f2(m\u00b2) > \u03a3; f2(m2), thus, yielding that 71 is better than 72 in human space. This shows the necessity of measuring the monotonicity of a global function fG and the dependence of the metric on the systems under evaluation.\nWe first introduce the Expected Deviation (ED), which measures the difference between fg and fk for all k \u2208 {1. . . K}, which tells us how much a system is over-or-underestimated according to the metric. That is the difference\nED(k) = {N}\\\\{N\\\\ H\\\\ G}- {1}\\\\{N\\\\ Nj=1} fk(m_j^{\\kappa}),\\qquad (1)\nThis is equivalent to \u03bc\u03b5 \u2013 \u03bc\u03b5, where \u03bc\u03b5 = {N\\\\ thus, we measure the difference between the average rating according to the global function and the average rating of the system-specific function, which corresponds to the human rating-average. Note that a mis-ranking occurs if one system is severely overrated while another is severely underrated. Thus we define the system dependence score SysDep(M) as the worst case of this effect:\nSysDep(M) = max\u3160 ED(k) \u2013 min\u03c0\u03ba ED(k) (2)"}, {"title": "Experiments", "content": "Estimating the Conditional Expectation. Even though the functions fk and fG are unknown in general, we can estimate them from data. We will use Isotonic Regression (IR) (Barlow and Brunk, 1972) for this purpose, which estimates a monotonic function fk minimizing \u2211;(fk(mp)) \u2013 h())2. To estimate fg, we utilize the same approach, pooling all paired data from all systems. To compute the SysDep of a metric, we compute the ED for each MT system under that metric. For this, we compute the average human rating = , the average metric rating , as well as average remapped rating= NM \u03a3 fG(m)) for each MT system. We provide our code in Appendix E.\nData. We rely on data from the WMT 23 Metrics shared task (Freitag et al., 2023). The data includes translations for 3 language pairs: English to German (en-de), Hebrew to English (he-en), and Chinese to English (zh-en). The translations were produces by 12-15 systems (depending on the language pair) which participated in the general MT task (Kocmi et al., 2023). Human ratings are available in the form of MQM annotations (Lommel et al., 2014), which are based on error-span annotations by experts that are subsequently transformed into a numeric value by assigning scores to errors based on their severity. Here, we will present results for the XCOMET (Guerreiro et al., 2023) metric (best metric according to correlation to human judgments) and the zh-en language pair, where we have access to NM = 1976 segments per system rated by the metric and NH = 1177 of these segments rated with human MQM ratings. Results for the other language pairs and an additional metric are shown in Appendix B. To estimate the conditional expectation functions fk, we use the 1177 paired ratings for each system \u03c0\u03ba. We employ B = 200 bootstrap samples of the paired data to fit B IR models. Our estimate, fk, represents the average of these B IR models. In Figure 1, we also present the range between the 2.5% and 97.5% percentiles.\nResults. We show the results in Table 1. We can see that the ED ranges from -0.82 to 1.996, thus yielding a SysDep score of 2.816. We see that both Lan-BridgeMT and GPT4-5shot are underrated by the metric (negative ED), but Lan-BridgeMT more so, enough to invert their order. At the bottom of the ranking, we see a relatively large absolute ED. Ranking errors reflect an interplay between the systems' rating gap and the EDs. For example, Online-A loses 2 ranks according to the metric even though it has the lowest absolute ED. We also note that even though fe is monotonic, the ranking between the metric and the remapped scores does not match completely. This can be attributed to the uncertainty introduced by bootstrapping and extrapolating to the unpaired metric ratings. It can be seen for ONLINE-W and ZengHuiMT, which have similar metric ratings."}, {"title": "Related Work", "content": "The derivation in Section 2 closely follows Wu and Resnick (2024), who provide the same argument in the context of binary prevalence estimation. In our case, the conditional expectation E[h|m] plays the same role as the calibration curve in their framework. Under that lens, the Expected Deviation is analogous to the Expected Calibration Error (Posocco and Bonnefoy, 2021). Following the same analogy, evaluating a new MT system is similar to applying a classifier to a new domain.\nPrevious studies by Deriu et al. (2023) and von D\u00e4niken et al. (2022) have highlighted that metric performance depends on the system under test. They employed a Bayesian framework to determine the proportions of binary or preference human ratings from metric scores; critically relying on confusion matrices estimated for each MT system. In this discrete rating context, these confusion matrices represent the same concept as E[h|m]. In follow-up work, von D\u00e4niken et al. (2024) find that some metrics disproportionately favor certain MT systems over others compared to human preference ratings. Our finding provides a plausible explanation.\nChaganty et al. (2018) shows how to combine human ratings and metric ratings to derive an unbiased estimate of the true expected human rating \u03bc\u0397 while reducing the number of annotations needed. The proposed control variates estimator is based only on human and metric scores for a given MT system, even when estimating their correlation, thus avoiding the problem we describe.\nWei and Jia (2021) consider disagreements in the ordering of systems when using \u00b5 instead of \u03bc. In particular they study the sign error, cases where sign(\u03bc \u2013 \u03bc21) \u2260 sign(\u03bc \u2013 \u03bc). They apply a bias variance decomposition to this error and find that while the human estimator is unbiased, it exhibits high variance while the opposite is the case for metrics. Our SysDep score presents a way to quantify this bias."}, {"title": "Conclusion", "content": "In this paper, we emphasize the importance of ensuring that automated metrics treat all MT systems consistently, a factor overlooked in current evaluations. By mapping metric scores to the human rating scale, we estimate how much a metric misjudges individual system performance. We compute the range of these deviations to assess how consistently a metric treats different systems. In Appendix C, we re-evaluate WMT23 metrics from this perspective. Additionally, in Appendix D, we confirm that these results stem from systematic differences in how metrics treat systems by measuring deviations within splits of a single system's ratings."}, {"title": "Limitations", "content": "This paper is intended to explore an overlooked aspect of the evaluation of automated metrics. The SysDep measure we developed will hopefully provide a starting point for the development of more refined evaluation of the way metrics treat different systems differently.\nOur experiments are based solely on data from the WMT23 Metrics shared task. To further solidify our findings a larger scale study with more domains and larger sample sizes are needed.\nWhile we provide a way to measure the system dependence of a metric, we do not provide any suggestions on how to develop metrics that minimize the SysDep."}, {"title": "Full Derivation", "content": "In Equation 3, we give the full derivation of Equation ?? in Section 2. In the following pk(h) is the density of human ratings for system \u03c0\u03ba, pk(m) is its density of metric ratings, and pk(h, m) the joint density."}]}