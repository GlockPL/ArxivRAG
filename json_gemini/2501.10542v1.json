{"title": "Improved IR-based Bug Localization with Intelligent Relevance Feedback", "authors": ["Asif Mohammed Samir", "Mohammad Masudur Rahman"], "abstract": "Software bugs pose a significant challenge during development and maintenance, and practitioners spend nearly 50% of their time dealing with bugs. Many existing techniques adopt Information Retrieval (IR) to localize a reported bug using textual and semantic relevance between bug reports and source code. However, they often struggle to bridge a critical gap between bug reports and code that requires in-depth contextual understanding, which goes beyond textual or semantic relevance. In this paper, we present a novel technique for bug localization -BRaIn- that addresses the contextual gaps by assessing the relevance between bug reports and code with Large Language Models (LLM). It then leverages the LLM's feedback (a.k.a., Intelligent Relevance Feedback) to reformulate queries and re-rank source documents, improving bug localization. We evaluate BRaIn using a benchmark dataset -Bench4BL- and three performance metrics and compare it against six baseline techniques from the literature. Our experimental results show that BRaIn outperforms baselines by 87.6%, 89.5%, and 48.8% margins in MAP, MRR, and HIT@K, respectively. Additionally, it can localize ~52% of bugs that cannot be localized by the baseline techniques due to the poor quality of corresponding bug reports. By addressing the contextual gaps and introducing Intelligent Relevance Feedback, BRaIn advances not only theory but also improves the IR-based bug localization.", "sections": [{"title": "I. INTRODUCTION", "content": "Software bugs can cause major financial losses and lead to data breaches, security vulnerabilities, and operational disruptions [1], [2]. A recent software bug from Microsoft-owned CrowdStrike caused several hours of disruption in the U.S. airline industry, nearly halting operations and resulting in over $10 billion in damages [3], [4]. Developers at the major IT companies, such as Microsoft and Google, have reported bug resolution as a top concern [5]. According to existing studies up to 50% of the programming time is spent by developers on finding, understanding, and fixing software issues [6]\u2013[8]. Thus, any automated support to tackle these challenges can greatly benefit the developers.\nSoftware bugs are submitted to bug-tracking systems (e.g., Bugzilla, JIRA) as bug reports, which might capture crucial hints for resolving software-related issues. Developers often rely on these reports to trace the origin of bugs in the code. However, the content and quality of bug reports can vary significantly based on their submitters' level of expertise and articulation skills. In particular, there might be variations in word choice and presence of technical terms [9], [10]. Such variations pose challenges for developers when pinpointing the root cause of defects, even for seasoned practitioners [11]. \u03a4\u03bf address these challenges, there has been significant research targeting the detection or localization of software bugs over the last few decades.\nResearchers have presented two major categories of meth- ods to automatically localize software bugs: program spec- trum analysis and Information Retrieval. First, spectrum-based methods rely on program execution traces for fault localiza- tion. However, the execution traces are not always readily accessible, which makes these methods less scalable [12], [13]. On the other hand, Information Retrieval (IR)-based methods use overlapping terms or keywords between bug reports and source code to localize bugs [14]-[18]. They are lightweight and scalable. However, they also struggle with the vocabulary mismatch problems [9] and may not always deliver satisfactory results due to sporadic term matching. Researchers have also incorporated historical data from past bug reports, code change history, past bug fixes, and bug recurrences [19], [20]. Although these enhancements have been reported to improve the performance of the IR-based methods in localizing bugs, a recent study [21] suggests that they do not significantly outperform the previous methods.\nRecent IR-based techniques focus on search queries and attempt to improve their queries by capturing syntactic, co-occurrence, and hierarchical dependencies among the words in bug reports [10], [11], [22], [23]. However, these methods only use terms found in bug reports, which could be poorly written or insufficient [11]. As a result, they frequently fail to bridge the gap between natural language from bug reports and programming code from a project when searching for software bugs. To address this issue, several techniques at- tempt to enhance queries with relevant terms extracted from source documents through relevance feedback mechanisms [10], [24]-[29]. However, the majority of these techniques naively consider the top few documents (based on textual similarity) as relevant, overlooking the need for a compre- hensive understanding of the code. As a result, they may not always capture the most meaningful terms from source code for their search queries. [22], [27]. Thus, the existing IR- based techniques for bug localization suffer from two major challenges as follows.\n(a) Relevance feedback against search queries might not be always relevant: Gay et al. [29] proposed a manual, iterative approach that leverages relevance feedback from developers and constructs queries to search for buggy source documents. In contrast, Sisman et al. [30] and Kim et al. [27] select the top few documents as relevant (a.k.a., pseudo relevance feedback) and leverage the feedback to improve their search queries. However, these techniques rely heavily on textually similar documents, which may not be always relevant, especially when dealing with source code and bug reports. Thus, a deeper understanding of both bug reports and source code is warranted to improve the relevance feedback mechanism and the subsequent steps of Information Retrieval (e.g., query reformulation, retrieval).\n(b) Textual and semantic relevance might not be suf- ficient: Bug reports contain not only natural language texts but also technical jargons, commit diffs, stack traces, and pro- gram elements [10]. These artifacts describe the context and symptoms of encountered bugs [22]. Since natural language is loosely structured, it can introduce ambiguity by expressing the same idea in various ways [9]. Similarly, programming languages are more structured yet allow syntactically diverse expressions (e.g., iterative vs. functional approaches) and arbitrary naming conventions [30]-[32]. This flexibility can result in textual mismatches, where keywords or key phrases in the bug report (e.g., \"download failed\") do not directly match the identifiers in the code (e.g., fetchResource).\nAt the same time, semantic mismatches can arise when a problem encountered in the bug report does not correspond to the programming task implemented in the code. For exam- ple, the encountered problem \"download failed\" - might not align well with the task \u201cHTTP/FTP operation task and get packets\" if the word-level semantics are considered only. It requires an understanding of the relationship between network operations and file downloading to establish their connection. In other words, to localize such bugs, automated tools or methods need to go beyond surface-level matching and comprehensively understand the context of an encountered problem as well as the functionality of the corresponding source code.\nIn this paper, we present a novel technique BRaIn - to support bug localization using Information Retrieval (IR) and Intelligent Relevance Feedback (IRF). Our approach over- comes the challenge of contextual understanding of software bugs using Transformer models [33] and localizes the bugs leveraging such understanding. First, BRaIn collects poten- tially buggy documents from a codebase by analyzing their contextual relevance to a bug leveraging transformer models (e.g., Mistral [34]). That is, unlike the existing methods, our method captures more human-like feedback to a query (a.k.a., Intelligent Relevance Feedback). Second, it extracts appropriate terms from these documents and expands the original query by further leveraging the captured feedback."}, {"title": "II. MOTIVATIONAL EXAMPLE", "content": "In this section, we present a motivating example to demon- strate the benefits of our proposed technique for bug local- ization. Let us consider the example bug report in Table I that discusses access problems to an LDAP server. The bug manifests as a failure in the authentication process, where the system returns an HTTP code of 403 (a.k.a., forbidden) instead of prompting for necessary credentials. This behavior results in a denial of access to the LDAP services and hinders any migration to a newer version of the services.\nFig. 1 presents the source code triggering the bug. The root cause of this bug is a subtle omission in the code handling authentication process. We see that the switch statement in the buggy version of code fails to account for the BASIC authentication type. Instead, it handles the PLAIN authenti- cation type, which is semantically closer but not equivalent. On the other hand, the bug report mentions \u201cBASIC\u201d HTTP authentication, which is not present in the target code. This terminology mismatch creates a disconnect between the high- level system behavior described in the bug report and the code level implementation, making the detection of bugs challenging. As a result, traditional text-based search methods perform poorly and retrieve the buggy code at 72nd, 13th, and 24th positions when title, description, or their combination are used as queries, respectively (Table I). Even after employing embedding-based semantic relevance, NextBug [37] struggles to link the code to the bug, placing it at 25th position.\nThe above evidence suggests that an in-depth analysis involving contextual relevance is essential. A seasoned devel- oper would recognize the missing clause of BASIC HTTP authentication in the code, although it is not explicitly stated in the bug report. By probing deeper, they would infer that the missing BASIC authentication is likely the root cause of the reported issue.\nLarge language models (e.g., Mistral) are exposed to a vast amount of data, including text and code. As a result, they can identify patterns (as humans do) and infer missing details, making them adept at handling tasks that require deep contex- tual understanding. As shown in Table I, BRaIn leverages such capabilities to obtain intelligent feedback against its query, reformulates the query, and returns the buggy code as the topmost result by executing the query against an IR-based method (e.g., BM25 [39])."}, {"title": "III. METHODOLOGY", "content": "Fig. 2 shows the schematic diagram of our proposed tech- nique \u2013 BRaIn \u2013 for software bug localization. We discuss its different steps in the following section.\n\nTo detect software bugs using Information Retrieval (IR), the first step is to index the source code docu- ments from a code repository. We chose Elasticsearch [40] for indexing due to its reliability, support for diverse data types, and easy integration with computing systems (e.g., cloud). We collected 45 subject systems from an existing benchmark dataset Bench4BL [21] and indexed the source code (Step 1, Fig. 2) from 684 buggy versions of these systems. Our idea was to detect a bug in the exact version of the software system stated in the corresponding bug report. During the indexing, we employed Elasticsearch's default analyzer to perform common pre-processing operations (e.g., tokenization, lowercase conversion, and removal of stop words).\nTo retrieve potentially buggy documents, we use bug reports (i.e., bug title and description) as queries (Step 2, Fig. 2). When we pass these queries to Elasticsearch, it preprocesses them using the standard analyzer and returns the top-K (e.g., 50) results through query execution. To narrow down the search results, we also apply additional filters, such as system and version information from each bug report. With- out these filters, the retrieved documents could be irrelevant or noisy. This step provides a set of source documents ranked by their textual relevance against a bug report by employing Elasticsearch's default retrieval algorithm, Okapi BM25 [39].\n\nOnce we have the results from Elasticsearch, we employ advanced prompt methods and Large Language Models (LLM) to determine the relevance between a bug report and each result (i.e., source code). LLMs have shown remarkable capa- bilities understanding natural language texts and source code [41]-[43]. We leverage their capabilities to capture intelligent relevance feedback against a query (a.k.a., bug report) as follows. To achieve this, we use prompt engineering, document segmentation, and finally relevance estimation as follows.\nPrompting is a novel method that instructs the LLMs (e.g., Mistral) to generate meaningful responses without any expensive training [44]\u2013[46]. It involves crafting appropriate instructions to guide LLM outputs and make them applicable to different problem-solving tasks [47]\u2013 [51]. LLMs have been found to be effective with well-designed prompts that are clear, specific, and actionable [45], [52]. We first developed a candidate prompt to determine whether a given code segment triggers a reported bug. It instructs the LLM to find the relevance, deliver the output in a JSON format, and act as a rational software engineer, incorporating the contextual information from the bug report and code segment."}, {"title": "C. Query Expansion", "content": "Using the Intelligent Relevance Feedback (IRF), we expand an original query (Step 6, Fig. 2). Unlike the earlier work that relies on pseudo-relevance feedback [27], [29], we choose the source code documents marked as relevant by the LLM for query expansion. In pseudo-relevance feedback, the top few documents retrieved by an IR method are naively considered as relevant and are used for query expansion. On the other hand, our underlying idea is that documents contextually relevant to the bug reports (i.e., IRF) provide terms that can complement the original query. We adapt an existing work of Rahman et al. [61] to capture appropriate terms from the relevant source documents as follows.\nFirst, we parse each of the source documents retrieved by Elasticsearch and extract class, method, and field signatures from them. These signatures capture the intent of the code, whereas the detailed implementation code could be noisy [61]. We extract the signatures using a lightweight Python library Javalang [62]. Next, we split camel case tokens from these signatures and turn them into textual phrases by combining their split tokens. We preprocess each of the phrases by filtering out stop words and programming keywords. Then, we construct a term graph G(V, E) by encoding terms as vertices (V) and co-occurring terms as connecting edges (E). Subsequently, we apply the PageRank algorithm [63] in Eq. 1 to the term graph to select the influential terms.\n$\\displaystyle PR(V_{i})=\\frac{1-d}{N}+d\\sum_{V_{j}\\in M(V_{i})}\\frac{PR(V_{j})}{L(V_{j})}$ (1)\nHere, PR(Vi) represents the PageRank of vertex Vi. The term d denotes the damping factor with a default set to 0.85. N refers to the total number of vertices in the graph, while M(Vi) indicates the set of vertices linked to Vi. Finally, PR(Vj) and L(Vj) represent the PageRank and outbound links of vertex Vj.\nThe algorithm assigns an initial score to each vertex (Vi) and iteratively updates it, prioritizing the vertices with higher connections. This process repeats until the scores stabilize or the algorithm reaches its maximum iteration limit (e.g., 100). Once the computation is done, we select the top-N (e.g., 10) weighted terms returned by the algorithm [61]. Finally, we expand the original query (i.e., bug report) with these terms, complementing bug reports with contextually relevant terms from source code, leveraging intelligent relevance feedback."}, {"title": "D. Bug Localization", "content": "We leverage our expanded query above and Intelligent Relevance Feedback (IRF) from the LLM to localize the buggy source documents as follows.\n1) Reranking: We determine the relevance between each result from Elasticsearch and our expanded query employing BM25 algorithm, and rerank them according to their relevance (Step 7, Fig. 2). This step provides us with ranked results and their BM25 scores. The updated ranks could be useful since the expanded query contains more meaningful terms from the source documents.\n2) Rescoring: We also enhance the ranking of source documents by incorporating IRF from LLM into their scores (Step 8, Fig. 2). First, we normalize the BM25 scores of the retrieved documents using a softmax function [64], producing a set of scores that add up to 1. The softmax function amplifies the differences among its input values exponentially, making their difference clearer. Given that the BM25 scores of the documents could have high variance, this allows the softmax function to highlight the textually relevant results. However, since textual relevance might not be sufficient, we also leverage the LLM's feedback against each result docu- ment. To incorporate this, we promote the relevant documents and penalize the irrelevant ones, marked by the LLM. This combined approach (Eq. 2) incorporates both textual and contextual relevance in the document ranking as follows.\n$\\text{score}_{i}=\\frac{e^{z_{i}r_{i}}}{\\sum_{j=1}^{n}e^{z_{j}}}$ (2)\n$\\displaystyle r_{i}=\\begin{cases}1, & \\text{if ith doc is relevant}\\\\0, & \\text{otherwise}\\end{cases}$ (2)\nHere, $score_i$ represents the score of the ith document. The term ri denotes the binary relevance feedback, indicating whether the document is relevant or not (details in Section III-B). The terms zi and zj in the softmax function correspond to the BM25 scores of documents i and j.\nFinally, we rank the source documents based on their scores for their potential to be buggy (Step 9, Fig. 2) and return the top-K (e.g., K=10) documents. Our scoring process aims to bridge the gap between bug reports and source code by incorporating a deeper contextual understanding of the LLM and going beyond their textual and semantic relevance (Table I)."}, {"title": "IV. EXPERIMENTS", "content": "We curate a dataset of \u22484.7K bug reports from the bench- mark dataset Bench4BL and evaluate using three appropriate metrics from the relevant literature Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and HIT@K (K=1, 5, 10) [10], [35]. We experiment with three different LLMs and compare our solution \u2013BRaIn\u2013 against eight relevant baselines to place our work in the literature. Through our experiments, we answer three research questions as follows:\n\u2022 RQ1: (a) How does BRaIn perform in localizing software bugs? (b) Does BRaIn enhance the localization of bugs that require changing multiple documents? (c) Can it improve the localization of bugs that are reported poorly?\n\u2022 RQ2: How do IRF-based query expansion and document ranking contribute to the performance of BRaIn?\n\u2022 RQ3: Can BRaIn outperform the relevant baseline tech- niques in bug localization?\nIn our experiment, we used the Bench4BL [21], a compre- hensive benchmark dataset that contains 10,017 bug reports from 51 open-source systems, covering a total of 695 software versions. Our initial assessment revealed that bug reports from the older systems lacked crucial versioning information, making them unsuitable for our study. Additionally, we could not accurately link some bug reports to their corresponding buggy code within the code repositories. Hence, we excluded these bug reports from our dataset. We also found bug reports containing only stack traces without accompanying any textual descriptions of their bugs. We identified those bug reports using regular expressions [10] and excluded them from the dataset. Following these refinement steps, our final dataset comprised 4,683 bug reports from 42 different systems span- ning across 684 versions. Table III-a summarizes our curated dataset.\nTo conduct our experiments and compare with deep learning-based baseline techniques, we used an optimal 80:20 dataset split [65]. This split was done chronologically within each system to ensure that the training set consists of the older 80% of the data, while the test set contains the newest 20% to imitate a real-world scenario. Table III-b provides a summary of our training and test datasets."}, {"title": "B. Evaluation Metrics", "content": "Mean Average Precision (MAP): Precision@K indicates the precision for each instance of a buggy source document in the ranked list. Average Precision computes the average Precision@K for all buggy documents in relation to a specific search query. Consequently, Mean Average Precision (MAP) is obtained by averaging the Average Precision values across all queries (Q) within a dataset.\n$\\displaystyle AP@K = \\frac{1}{K} \\sum_{k=1}^{K} P_{k} \\times B_{k}$ MAP = $\\displaystyle\\frac{1}{|Q|}\\sum_{q=1}^{Q}AP_K$ Here, AP@K computes average precision for top-K results, where Pk is precision at position k and Bk indicates if item k is buggy (1) or not (0). MAP averages this across all queries q in dataset Q, with D being the ground truth documents.\nMean Reciprocal Rank (MRR): Reciprocal Rank (RR) refers to the rank of the first relevant result retrieved by a technique. It is defined as the reciprocal of the rank of the first relevant source document within the ranked list for each query.\n$\\displaystyle RR_{q} = \\frac{1}{Rank \\space of \\space First \\space Relevant \\space Item}$ MRR = $\\displaystyle\\frac{1}{|Q|}\\sum_{q=1}^{Q}RR_q$ Here, MRR averages the Reciprocal Ranks (RRq) across all queries q in set Q, where RRq is the Reciprocal Rank for query q.\nHIT@K: HIT@K [35] measures the proportion of queries for which a technique retrieves at least one relevant document among the top-K results. Higher HIT@K values indicate better performance in bug localization techniques.\n$\\displaystyle HIT@K = \\frac{1}{|Q|} \\sum_{\\tau \\in Q} \\begin{cases} 1, & r_q \\in G \\\\ 0, & otherwise\\end{cases}$ Here, rq returns 1 if query q has a ground truth item in the top-K results (0 otherwise), where Q is the set of all queries."}, {"title": "D. Evaluating BRaIn", "content": "Answering RQ1 - Performance of BRaIn: We evaluate the performance of BRaIn using Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and HIT@K against top 1, 5, and 10 results. Table IV summarizes our performance details.\nFrom Table IV, we see that our proposed technique performs well in detecting the software bugs. BRaIn, powered by Mistral exhibits strong performance, with a Mean Average Precision (MAP) of 0.537. This indicates BRaIn's ability to rank the relevant documents (a.k.a., buggy source documents) higher than the irrelevant ones. Our technique achieves a Mean Reciprocal Rank (MRR) of 0.570 suggesting that the first relevant document is found within the top two positions. BRaIn (Mistral)'s HIT@1 score of 0.469 shows that, for nearly 47% of bug reports, the most relevant document appears at the top position. BRaIn (Mistral) also performs well in HIT@5 and HIT@10, with approximately 71% and 78% of bug reports having at least one relevant buggy document found within the top 5 and top 10 positions, respectively. BRaIn (LLaMA) delivers nearly comparable results to BRaIn (Mistral), trailing by 1.9% in HIT@10. Although BRaIn (Qwen) demonstrates decent performance, it lags behind both BRaIn (Mistral) and BRaIn (LLaMA) in all metrics. It achieves MAP and MRR scores of 0.492 and 0.523, which are about 9.1% lower than BRaIn (Mistral)'s best performance in each metric.\nAccording to our investigation, some bugs trigger changes to a single document during bug resolution, whereas others trig- ger changes to multiple documents. We thus evaluate BRaIn's performance in localizing bugs that warrant changes across multiple source documents. Table V shows the performance of BRaIn, powered by three different LLMs, in terms of MAP, MRR, and HIT@10. We grouped bugs from our dataset into four categories based on the number of their changed documents: 1, 2, 3, and 4 or more. Our findings show that BRaIn performs strongest when paired with Mistral. For the 1,949 bugs requiring changes to a single document, BRaIn (Mistral) achieves a MAP of 0.542, representing a significant 14.3% improvement over the baseline counterpart. The im- provements extend to other metrics, with a 14.3% increase in MRR and 7.1% in HIT@10. BRaIn (Mistral) also excels in resolving bugs that require multiple document changes, achieving improvements of 7.0-10.6% in MAP, 7.9-10.6% in MRR, and 6.0-6.9% in HIT@10. The other variants of BRaIn also outperform the baseline in localizing bugs that require multiple document changes, achieving improvements of up to 9.2% in MAP, 10.0% in MRR, and 5.1% in HIT@10.\nWe also investigate how BRaIn performs in localizing bugs where the bug reports could be of low quality (Fig. 3-a). According to existing literature [11], low-quality bug reports lack sufficient information and provide queries that cannot re- trieve at least one relevant result within their top 10 positions. In our dataset, we identified 1,101 bug reports that fall into this category. Of these, 581 bug reports (i.e., queries) do not contain any ground truth within their top 50 results returned by Elasticsearch. These bug reports were not considered, which leaves us with 520 low-quality bug reports for our analysis. Our findings demonstrate BRaIn's promising results even with the low-quality reports. BRaIn (Mistral) emerges as the top performer, successfully localizing 268 bug reports (51.5% of low-quality bug reports) within the top 10 results. BRaIn (LLaMA) and BRaIn (Qwen) follow, identifying 225 and 198 reports, respectively. Notably, all three models identified 130 bugs, with BRaIn (Mistral) uniquely localizing an additional 70 bugs, followed by BRaIn (LLaMA) and BRaIn (Mistral) with 30 and 23 bugs. We also assess BRaIn's capability to detect the first relevant document (a.k.a., buggy source document), where 130 bug reports from above were considered (Fig. 3-b). Interestingly, BRaIn (Qwen) outperformed the other variants in this metric, localizing 26.9% of the bugs at the top positions, followed by BRaIn (LLAMA) at 18.5% and BRaIn (Mistral) at 10%. All the findings above suggest BRaIn's ability to analyze, enhance, and localize bug reports, even with low-quality reports."}, {"title": "Answering RQ2 - Contribution of Query Expansion and Document reranking", "content": "BRaIn leverages Intelligent Relevant Feedback (IRF) to expand its original query and rerank the documents. Query expansion can improve bug localization by adding relevant keywords to an original query. Similarly, incorporating contextual understanding into document scoring can help go beyond just textual and semantic matching during document ranking. We examine the contribution of these two components (Table VI) to BRaIn's performance as follows.\nTo determine the impact of query expansion in isolation, we evaluated BRaIn's performance without the reranking com- ponent (Table VI). Interestingly, all BRaIn variants achieved similar MAP scores of around 0.49, falling short of opti- mal performance. For example, BRaIn (LLaMA) and BRaIn (Mistral) saw MAP scores decrease by 7.2% and 7.8%, re- spectively, while their MRR scores remained relatively stable. BRaIn (Qwen), on the other hand, showed a 7.8% increase in MRR alongside a slight improvement in MAP. However, all BRaIn variants outperformed the baseline by 10.5-10.9% in MRR and 2.4-2.9% in MAP.\nIn contrast, when reranking was applied isolately, MAP scores for BRaIn (LLaMA), BRaIn (Mistral), and BRaIn (Qwen) dropped by 3.0%, 3.2%, and 6.1%, respectively, while their MRR scores remained consistent. Despite these decreases, all variants showed improvements of 3.1-11% in MRR and 1.0-7.1% in MAP over the baseline.\nThese findings show that Intelligent Relevance Feedback improves the individual components of query expansion and reranking. However, they also underscore the importance of the synergy between these components in achieving optimal performance."}, {"title": "Answering RQ3 - Comparison with Basline Techniques", "content": "To place our work in the literature, we compare BRaIn with relevant baseline techniques in terms of their MAP, MRR, and HIT@10. Given our methodology, we choose two types of baseline techniques IR methods [10], [29], [35] and deep learning based methods [36]\u2013[38]. For comparison with baselines, we use BRaIn (Mistral) in our experiments since it is the best-performing variant of BRaIn.\nTo replicate the traditional IR-based Baseline VSM, we index all source documents of a repository and use bug reports (title + description) as queries. These queries are executed with the Elasticsearch [40], which retrieves relevant documents using the BM25 algorithm [39] and Boolean search, with default parameters for k and b. Other traditional approaches from literature- BLUiR [35] and Blizzard [10]- use struc- tured information from bug reports and source code for bug localization. BLUiR calculates suspiciousness scores using class names, method names, variable names, comments, and bug report elements (title, description), combining multiple searches into an overall score. Blizzard categorizes bug reports into three types and constructs text graphs from these reports to generate queries and retrieve relevant buggy source documents. For both approaches, we employ Apache Lucene [68] for retrieval. We replicated these methods by adapting them from Bench4BL repository [21] and Blizzard's replication package [69] from the authors. We compare BRaIn with these IR-based techniques to assess the performance of our technique against established models in the field.\nRelevance feedback-based techniques like Rocchio [70] and the Spatial Code Proximity (SCP) model [29] aim to enhance bug localization by refining queries based on the results of initial searches. Rocchio is a widely-used relevance feedback technique for information retrieval [70]. We leverage relevance feedback to reformulate queries and Apache Lucene to execute the queries and retrieve the documents. Our reformulated queries were optimized using \u03b1, \u03b2, and y parameters [70]. Similarly, we implemented Sisman et al.'s SCP model [29] to reformulate queries based on term proximity within source code. It prioritizes terms that frequently co-occur within the same method or class, using the best parameters w, x, and y suggested by the authors. We compare these techniques to our approach to highlight the importance of contextual understanding during relevance feedback of search queries.\nSince Machine Learning (ML) techniques can capture com- plex patterns in data using non-linear relationships, we com- pare BRaIn against three ML-based techniques- DNNLOC [36], NextBug [37], and RLocator [38]. DNNLOC combines multiple features- rVSM score [71] for bug report-source code similarity, class name similarity, collaborative filtering, and bug report recency and frequency\u2014and uses a neural network to predict suspiciousness scores to rank documents. NextBug employs Word2Vec [72] embeddings to capture semantic relations between bug reports and source code and thus to localize the buggy documents. In our experiments, we substituted Word2Vec with CodeT5 embeddings [73] to cap- ture more nuanced text-level semantic associations, as opposed to token-level. RLocator is a recent deep-learning technique that employs a reinforcement learning model, framed as a Markov Decision Process, to optimize ranking of buggy docu- ments. We replicated DNNLOC and NextBug by following the respective authors' approaches and replicate RLocator using the authors' provided replication package on Zenodo [74].\nTable VII-a summarizes our comparison details with the baseline techniques. Among traditional IR-based approaches, Blizzard achieves a MAP score of 0.506, while Baseline VSM and BLUiR scores are 0.484 and 0.450. BRaIn out- performs them with a MAP of 0.537, achieving a maximum improvement of 19.3% over these techniques. Similarly, BRaIn achieves notable gains in MRR_and HIT@10, with increases of up to 17.5% and 12.2%. Among the IR based approaches that leverage relevance feedback, Rocchio's algorithm achieves a MAP of 0.489, slightly above the baseline, while Sysman- SCP falls short by 2.5%. BRaIn again leads here with the improvements in MAP, MRR, and HIT@10 of 13.8%, 5.5%, and 3.7%, respectively. These results underscore the advan- tages of Intelligent Relevance Feedback, which uses contextual understanding over traditional techniques based on textual relevance for bug localization.\nAs shown in VII-b, BRaIn also outperforms the machine learning techniques that require training. We evaluated both BRaIn and the baseline techniques on the test set only to ensure a fair comparison. It should be noted that old bug reports and their corresponding code were used for training and the recent bugs and their corresponding code were used for testing. DNNLOC performs significantly lower with a MAP of 0.283, 87.6% lower than BRaIn's optimal score of 0.531. In comparison, RLocator and NextBug achieve MAP scores of 0.488 and 0.469, with BRaIn outperforming them by 8.8% and 13.2%, respectively. Similar improvements are observed for other metrics, with BRaIn showing 4.4-89.5% improvements in MRR and 3.7-48.8% in HIT@10. Such performance under- scores the superiority of BRaIn's performance with intelligent relevance feedback (IRF) compared to baseline techniques."}, {"title": "V. RELATED WORK", "content": "Bug localization techniques can broadly be classified into two main groups: spectra-based and information retrieval (IR)- based approaches [14", "12": [13], "76": "have been enhanced by integrating additional contexts, such as bug report history, code modifications, and version history [23", "77": [78], "35": "leverage bug report and source code structures, capture eight components from the code and bug reports, and perform eight pairwise searches using a sophisticated retrieval technique, Indri [79", "71": "combines a modified VSM (rVSM [71", "19": "integrates BLUiR, BugLocator, and version history to better detect buggy documents. AmaLgam+ [80"}, {"71": [81], "21": "."}]}