[{"title": "Drift to Remember", "authors": ["Jin Du", "Xinhe Zhang", "Hao Shen", "Xun Xian", "Ganghua Wang", "Jiawei Zhang", "Yuhong Yang", "Na Li", "Jia Liu", "Jie Ding"], "abstract": "Lifelong learning in artificial intelligence (AI) aims to mimic the biological brain's ability to continuously learn and\nretain knowledge, yet it faces challenges such as catastrophic forgetting. Recent neuroscience research suggests\nthat neural activity in biological systems undergoes representational drift, where neural responses evolve over\ntime, even with consistent inputs and tasks. We hypothesize that representational drift can alleviate catastrophic\nforgetting in AI during new task acquisition. To test this, we introduce DriftNet, a network designed to constantly\nexplore various local minima in the loss landscape while dynamically retrieving relevant tasks. This approach\nensures efficient integration of new information and preserves existing knowledge. Experimental studies in image\nclassification and natural language processing demonstrate that DriftNet outperforms existing models in lifelong\nlearning. Importantly, DriftNet is scalable in handling a sequence of tasks such as sentiment analysis and question\nanswering using large language models (LLMs) with billions of parameters on a single Nvidia A100 GPU. DriftNet\nefficiently updates LLMs using only new data, avoiding the need for full dataset retraining. Tested on GPT-2 and\nROBERTa, DriftNet is a robust, cost-effective solution for lifelong learning in LLMs. This study not only advances\nAI systems to emulate biological learning, but also provides insights into the adaptive mechanisms of biological\nneural systems, deepening our understanding of lifelong learning in nature.", "sections": [{"title": "Introduction", "content": "Biological brains exhibit remarkable lifelong learning skills, acquiring new abilities while retaining\npreviously learned information throughout lifetime. In contrast, this lifelong learning capability, known in\nartificial intelligence (AI) as continual learning, where a system with limited memory can sequentially\nlearn new tasks without forgetting previous ones, remains a significant challenge. The primary issue is\ncatastrophic forgetting, a phenomenon where the performance in previously learned tasks deteriorates\nsignificantly as new tasks are learned1-3 (Figure 1a). This catastrophic forgetting issue limits the lifelong\nlearning capability of current large models, preventing them from evolving over time, especially in\napplications such as autonomous vehicles, robotics, and natural language processing (NLP).\nTo address catastrophic forgetting, current lifelong learning methods fall primarily into three categories:\nregularization, replay, and architectural methods. Regularization-based methods 4-7 adjust neural network\nparameters for new tasks while constraining changes in crucial parameters of previous tasks by imposing\nconstraints on training objectives, such as elastic weight consolidation (EWC)4 and synaptic intelligence\n(SI)5. Replay approaches typically involve training generators for all tasks or maintaining a sample buffer\nthat stores data from previous tasks9,10. When learning new tasks, data from previous tasks (either as\npseudo-samples generated by the generators or as direct samples from the buffer) regularize the training\nobjective. This replay helps to ensure that the performance on previous tasks experiences only minimal\ndegradation. Replayed samples can also prevent gradient updates in crucial directions11\u201313. Architectural\nstrategies allocate new parameters for every task, which can be further divided into two subcategories:\n(1) fixed architecture, which uses a shared fixed network and trains a distinct set of parameters for every\ntask14-16, and (2) dynamic architecture, which sequentially expands the model structure for new tasks17\u201319.\nHowever, most existing architectural strategies require task identities during both training and testing\nphases to be known. Empirical deep learning studies20,21 have demonstrated that while these methods,\nsuch as the experience replay (ER)10 and the generative classifier22, perform well on simpler tasks\ninvolving classify datasets such as MNIST23 and CIFAR-10024 (with a feature extractor pre-trained in\nCIFAR-10), they struggle with more challenging tasks such as those involving Mini-ImageNet25 and\nCIFAR-100 without pre-trained information. This difficulty is due to the increasing complexity of the\ndata distribution and the higher dimensionality. Empirical experiments20,21 showed that regularization-\nand replay-based methods that train a single large network face difficulties in encoding new information\nwithout compromising existing knowledge.\nThis raises a fundamental question: What features of biological brains enable them to efficiently encode\nnew information, retain previous knowledge, and effectively recall relevant information upon recurrence\nof a learned task? Although exact mechanisms remain unclear, recent biological research suggests that\neven as animals receive the same sensory input and maintain consistent performance on a task, their\nneural responses can undergo significant drift over time a phenomenon termed neural representational\ndrift26,27 (Figure 1b). This phenomenon, once considered mainly as measurement artifacts, has been\nrepeatedly confirmed by numerous long-term stable measurements in multiple regions of the brain enabled\nby advanced measurement techniques28-33.\nHere, we propose that introducing drift in artificial neural networks (ANNs) could be a crucial\nmechanism to enable lifelong learning by reducing catastrophic forgetting of learned tasks (Figure\n1c). Although recent biologically inspired network experiments have suggested multiple mechanisms\nfor implementing representational drift in ANNs34\u201337, these have not been designed to improve lifelong\nlearning capacity. To address this, we hypothesize that the implementation of a drift mechanism encourages\nan ANN's weights and associated hidden representations to continuously change, exploring regions of low\nloss in the loss landscape, resulting in drifting across multiple local minima. This continuous exploration"}, {"title": "Results", "content": "prevents the network from easily overwriting previously learned weights when acquiring new tasks,\nthereby enhancing lifelong learning capability. In contrast, a stable deep learning network, where the\nweights stop changing after converging, ceases to explore different local minima in its loss landscape,\nleading to a single minimum. When learning a new task, the newly learned weights can overwrite previous\nones, causing catastrophic forgetting of earlier tasks (Fig. 2a).\nTo test our hypothesis, we developed DriftNet, a drift-inspired lifelong learning framework consisting\nof two key components: an evolving network that continuously explores various local minima driven by\nexternal noise, and a knowledge base that organizes these minima into task-specific groups. DriftNet\noperates through three steps: exploration, encoding, and retrieval. During exploration, external noise\n(e.g., batch sampling, dropout, gradient noise, input noise) induces drift in the network's weights, leading\nto the discovery of diverse local minima in the loss landscape. In the encoding step, these minima are\norganized within the knowledge base as task-specific groups even when their identities are not explicitly\nknown. New minima related to previously learned tasks are added to existing groups, while unrelated\nminima form new groups without overwriting prior knowledge. During retrieval, when presented with\ntest inputs, DriftNet assesses the uncertainty of predictions from each group to identify the most relevant\ntask-specific group, enabling the recall of learned knowledge. Local minima from relevant tasks produce\noutputs with minimal uncertainty, while those from irrelevant tasks yield high uncertainty. Therefore, this\ndrift mechanism is essential as it facilitates continuous exploration and acquisition of diverse local minima,\ncapturing the rich characteristics of each task. These characteristics allow DriftNet to preserve existing\nknowledge and ensure accurate and robust retrieval, thereby enhancing lifelong learning capability in\ndynamic environments.\nWe demonstrate the superior lifelong learning performance of DriftNet by conducting benchmark\nexperiments on simulated datasets and two representative data domains in the field of deep learning: image\nclassification and NLP. Our comparisons include a Stable baseline, where a network is continuously fine-\ntuned, and a fixed number of recent network copies are retained as knowledge to generate predictions for\ntest inputs. However, in DriftNet, selective copies are retained and adaptively chosen to enhance prediction\naccuracy. On simulated data, DriftNet achieves an average test loss of $(1.01 \\pm 0.07) \\times 10^{-2}$, statistically\nsignificantly lower than the Stable baseline $4.22 \\pm 0.15$. For image classification tasks, DriftNet achieves\nan average test accuracy of $80.19 \\pm 0.67\\%$ on CIFAR-10 and $41.83 \\pm 0.75\\%$ on CIFAR-100, compared\nto the Stable baseline $19.18 \\pm 0.02\\%$ and $12.84 \\pm 0.07\\%$. In NLP, involving a pre-trained model of\n125 millions of parameters, DriftNet achieves an average test accuracy of $70.37 \\pm 1.22\\%$, substantially\noutperforming the Stable baseline $18.29 \\pm 0.06\\%$. This results demonstrate the robustness, scalability, and\nwide applicability of drift-inspired models in lifelong learning."}, {"title": "DriftNet: a drift-inspired lifelong learning framework", "content": "Figure 2b presents an overview of DriftNet, which comprises two main components: an evolving model for\nexploration and a knowledge base for encoding and retrieving grouped task-specific information. DriftNet\noperates through three main steps: local minima exploration, task encoding, and knowledge retrieval.\nFirst, to allow the network to actively explore local minima in the loss landscape, we introduced\nvarious types of noise into the network (see Methods). These include batch sampling noise from stochastic\ngradient descent (SGD), dropout38 (which randomly zeros out nodes), gradient noise39 (which imposes\nGaussian white noise on the gradient), and input noise40 (which adds Gaussian white noise to the inputs).\nAs the network optimizes the objective function and approaches a local minimum, the introduced noise\nprompts the network to move away from the current minimum, thus ensuring continuous exploration"}, {"title": "Benchmarking DriftNet's lifelong learning performance using simulated datasets", "content": "Second, during the encoding step, DriftNet employs drift-induced exploration to organize the knowl-\nedge base without the need for explicit task identities during training. The knowledge base groups diverse\nlocal minima into task-specific groups based on their performance characteristics: local minima from the\nsame task excel at that task but perform differently on unrelated tasks (see Methods). To further reduce\nmemory costs in large language models (LLMs), we employ Parameter-Efficient Fine-Tuning (PEFT)41,42\nstrategies, where each local minimum has fewer trainable parameters, such as Low-Rank Adaptation\n(LORA)43 (see Methods).\nThird, DriftNet addresses the challenge of efficiently retrieving related knowledge for learned tasks\nby utilizing encoded knowledge, specifically the task-specific groups of diverse local minima. DriftNet\nfocuses on identifying and retrieving relevant task-specific groups to enhance performance on learned\ntasks. Each task-specific group in DriftNet, composed of local minima, generates a set of outputs for any\ngiven input. The variance of these outputs within each group is quantified as the output uncertainty, which\nis then used to identify the task identity of any test input. The group with the lowest output uncertainty is\nselected for retrieval (Fig. 2e). Various uncertainty measurement methods, including the variance of hard\noutputs, the variance of soft outputs, and entropy, were explored and detailed in Methods.\nIn summary, DriftNet continuously explores task-specific loss landscapes, encodes and groups task-\nspecific local minimal in the knowledge space, and uses output uncertainty within each group for retrieval.\nThis drift-induced exploration of loss landscapes provides rich characteristics of task-specific loss land-\nscapes, which is crucial for (1) avoiding overwriting existing knowledge from other tasks during the\nlearning of new tasks and (2) retrieving relevant knowledge when queried by test inputs from learned tasks.\nConsequently, DriftNet is expected to learn an increasing number of tasks throughout its lifetime, while\nreducing the forgetting of previous tasks, thus achieving a robust lifelong learning capability.\nTo evaluate and understand the lifelong learning performance of DriftNet, we applied it to simulated\ndatasets. We used linear regression tasks for these simulations because (1) their geometry of local\nminima can be easily described in a closed form and (2) they provide a clear understanding of how\nDriftNet functions. In these simulations, the input variables $(x_1, x_2, x_3)$ were used with the output defined\nas $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\varepsilon$, with $\\varepsilon \\sim N(0, 0.01)$ representing Gaussian noise. The input\ncovariance matrix was set to be singular, leading to multiple minima (see Methods).\nWe trained DriftNet using SGD with Gaussian white noise (mean 0 and variance $\\sigma^2 I_4$) injected into\nthe gradient during training, where $I_4$ is a 4 \u00d7 4 identity matrix. Model weights were saved at the end of\neach epoch, defined as a single pass through the entire training dataset. Every 10 epochs, the DBSCAN\nclustering algorithm44 was used to group stored local minima into task-specific groups. During retrieval,\nfor any test input, DriftNet selected the task-specific group with minimal output variance to provide the\naverage output. In contrast, the stable baseline as a control was trained with SGD without noise injection.\nThe weights were stored at each epoch, and the average output of all stored weights was used for any test\ninput. Experiments were conducted with 50 repetitions for the simulated datasets. After learning both\ntasks, the test loss of DriftNet averaged over two tasks is $(1.01 \\pm 0.07) \\times 10^{-2}$ when noise level $\\sigma = 3$,\nwhich is significantly lower than the Stable baseline's average test loss of $4.22 \\pm 0.15$. These results\ndemonstrate that DriftNet continuously learns both tasks without forgetting.\nTo understand how drift contributes to avoiding forgetting, we first validated that noise drives the\nexploration of local minima, which leads to drift. We found that with Gaussian noise injection, the\ntraining loss remained steady for both tasks when noise levels ranged from 0 to 10 (Fig. 3d).Meanwhile,\nthe model weights of DriftNet continued to change during learning, actively exploring the task-specific"}, {"title": "DriftNet enhances lifelong learning performance in deep learning", "content": "manifold of local minima (Fig. 3e). Across experiments with various noise levels, the drift rate increased\nproportionally with the noise level (Extended Data Fig. 1a). Secondly, we demonstrated that drift-induced\ntask-specific groups of local minima could effectively identify whether any test input was from the relevant\ntask (in-distribution) or irrelevant tasks (out-distribution) based on uncertainty quantification (Fig. 3h).\nThe results indicated a statistically significant lower uncertainty for in-distribution test inputs, with values\nof $(7.47 \\pm 0.02) \\times 10^{-2}$ and $(7.29 \\pm 0.02) \\times 10^{-2}$ for the local minima of tasks 1 and 2, respectively. In\ncontrast, the out-of-distribution test inputs showed higher uncertainty, with values of $((1.88 \\pm 0.01)\\times10^{-1}$\nand $(3.50 \\pm 0.02) \\times 10^{-1}$ of local minima from Tasks 1 and 2, respectively. The p-value was below 0.001\nfor all tasks in both the Student's t-test (comparing the means of two groups) and the Mann-Whitney\nU-test (comparing the distributions of two groups) (see Methods), suggesting that the uncertainties are\nstatistically different for in-distribution and out-distribution data. This finding further corroborates that\ndrift enables the accurate retrieval of local minimal in the drifting network.\nWe then quantitatively assessed DriftNet's accuracy in retrieving the relevant task-specific group of\nlocal minima. The retrieval accuracy increased from $49.98 \\pm 0.09\\%$ (near the random guess 50%) to\n$94.36 \\pm 1.79\\%$ as the noise level increased from 0.001 to 0.3, remaining high for noise levels ranging\nfrom 0.3 to 6, but then decreased to $49.09 \\pm 0.81\\%$ at a noise level of 10.\nTo understand how DriftNet can continuously group local minima without knowing the task identities,\nwe visualized the performance vectors of local minima using their first two principal components (PCs)\n(see Methods). The results showed that the performance vectors were well-separated (Fig. 3f). We also\nused the adjusted rank index (ARI) score45 (see Methods) to quantitatively evaluate the grouping quality.\nThe results indicated a high grouping accuracy (above $0.94 \\pm 0.01$ when $\\sigma = 6$) as long as the injected\nnoise was not excessively large ($\\sigma \\leq 6$) (Extended Data Fig. 1b), where the local minima had low training\nlosses (Fig. 3d).\nOverall, the results indicated that higher noise levels led to a more extensive exploration of the local\nminima manifold (Fig. 3d-e). These post-exploration task-specific diverse local minima produced similar\noutputs (low uncertainty) for relevant task inputs and dissimilar outputs (high uncertainty) for irrelevant\ntask inputs. Consequently, the drift mechanism enabled continuous exploration, producing diverse local\nminima that helped determine the relevance of task-specific local minima to given inputs. This mechanism\nwas particularly effective in retrieving relevant knowledge during lifelong learning when the injected noise\nwas moderately large ($1 < \\sigma \\leq 6$, Fig. 3a-b), thereby preventing forgetting (Fig. 3f) and ensuring good\nperformance (Fig. 3a-c).\nTo demonstrate DriftNet's ability to enhance lifelong learning performance in deep learning, we applied\nit to two image classification datasets, CIFAR-10 and CIFAR-10024, which contain 10 and 100 classes,\nrespectively. We used a challenging class-incremental learning scenario21, which requires the algorithm to\nincrementally learn to classify objects from an increasing number of classes. Specifically, CIFAR-10 and\nCIFAR-100 were divided into 5 and 10 subsets with distinct sets of classes, respectively.\nWe used the trace of the variance of soft predictions as the uncertainty measurement in classification\ntasks. For simulated regression datasets, we used the variance of one-dimensional outputs. However,\nclassification tasks with multidimensional soft probabilities present additional challenges. For CIFAR-10,\nwe compared the variance of hard outputs, the variance of soft outputs, and the entropy of average soft\noutputs (see Methods). The trace of the variance of the soft outputs consistently performed the best in\nterms of average test accuracy across all noise types and datasets (Extended Data Fig. 3b).\nWe applied DriftNet implemented with different sources of noise, including batch sampling noise from\nSGD, dropout, and additive Gaussian noise applied to gradients and inputs. Baseline methods including"}, {"title": "DriftNet builds effective lifelong learning large language models", "content": "Fine-tune (sequentially fine-tuning a network and using it for evaluation), Joint (training on data from\nall tasks simultaneously in an offline manner to establish the performance upper bound), Theoretical\nLimits (training and testing with known task identities to achieve the upper bound with optimal retrieval),\nand Stable (sequentially fine-tuning a network and retaining a fixed number of copies of recent network\nparameters for evaluation) and state-of-the-art lifelong learning algorithms including Experience Replay\n(ER)10 and Generative Classifier22 were used for comparison. All experiments in this section were\nconducted for 10 repetitions.\nDriftNet achieved an average test accuracy of $80.19 \\pm 0.67\\%$ (mean $\\pm$ SE, n = 10 throughout the\nsection unless otherwise specified) for CIFAR-10 and $41.83 \\pm 0.75\\%$ for CIFAR-100, which was close\nto the Joint baseline of $84.34 \\pm 0.10\\%$ and $49.29 \\pm 0.25\\%$ for CIFAR-10 and CIFAR-100, respectively\n(Fig. 4a). In contrast, the stable baseline yielded an average test accuracy of $19.18 \\pm 0.02\\%$ for CIFAR-10\nand $12.84 \\pm 0.07\\%$ for CIFAR-100, with no significant improvement as more tasks were learned, indicating\nits inability to learn new tasks without forgetting previous ones. Among the other lifelong learning\nbaselines, the Generative Classifier22 achieved the highest average test accuracy, with $58.67 \\pm 1.77\\%$\nfor CIFAR-10 and $25.22 \\pm 0.43\\%$ and CIFAR-100 for CIFAR-100. In summary, DriftNet outperformed\nall state-of-the-art lifelong learning algorithms and the stable baseline by mitigating forgetting as more\ntasks are learned in both CIFAR-10 and CIFAR-100 (Extended Data Fig. 2a). Additionally, DriftNet's\nperformance is close to the Joint baseline (Fig. 4a).\nTo investigate whether DriftNet's ability to alleviate forgetting on CIFAR-10 and CIFAR-100 datasets\nis due to the drift dynamics of the network, we first examined training loss during the CIFAR-10 task\nlearning with noise levels $\\sigma$ ranging from 0 to 0.01, with Gaussian white noise (mean 0, variance $\\sigma^2$)\ninjected into the gradient (Fig. 4b). Representations of the first category, projected onto the first two PCs\nof drifts (see Methods), changed from the 50th to the 100th epoch during the first task when the noise\nlevel $\\sigma = 0.001$ (Fig. 4c). These results demonstrate that DriftNet maintained steady performance when\nits parameters kept changing over time, indicating the exploration of various local minima.\nNext, we tested whether DriftNet can retrieve the relevant task-specific group of local minima. First,\nwe evaluated the uncertainty of each task-specific group across all tasks. On CIFAR-10, the uncertainty of\noutputs from the local minima of relevant tasks (in-distribution) was relatively low, with values for five\ntasks ranging from $(8.22 \\pm 0.45) \\times10^{-3}$ to $(2.61 \\pm 0.07) \\times 10^{-2}$. In contrast, the uncertainty of outputs\nfrom the local minima of irrelevant tasks (out-of-distribution) was relatively high, with mean values for\nfive tasks concentrating at 0.04 at a noise level of $\\sigma = 0.001$ (Fig. 4f). The results show that the p-value\nis below 0.001 for all tasks in both the Student's t-test and Mann-Whitney U-test (see Methods). This\nindicates that the uncertainty difference is statistically significant for the network in retrieving the specific\ntask from the corresponding group. Second, we assessed the retrieval accuracy. The results show that\nDriftNet achieved retrieval accuracy with the lowest values of $97.60 \\pm 0.34\\%$ at $\\sigma = 0.003$ (Fig. 4e), with\ngradient noise ranging from 0 to 0.03. These results suggest that this drift-induced exploration enabled the\nsuccessful retrieval of groups of local minima from relevant tasks.\nWe further assessed DriftNet's ability to group local minima without prior knowledge of their task\nidentities. First, the visualization showed that the performance vectors, projected on the first two principal\ncomponents, were well-separated for CIFAR-10 at a noise level of 0.001 (Fig. 4d). Next, we quantitatively\nevaluated the grouping quality using the ARI (see Methods). For both datasets and various gradient noise\nscales, the ARI score was consistently 1 for all repetitions when the batch size was 16, indicating that\nDriftNet can perfectly group stored local minima based on their task identities. These results suggest\nthat stored local minima can be effectively separated by clustering algorithms because outputs from local\nminima of relevant tasks produced similar results, while those from irrelevant tasks diverged.\nNext, we tested DriftNet's robustness to various noise sources with a batch size of 16. First, we"}, {"title": "Discussion", "content": "introduced gradient noise ranging from $\\sigma = 0$ to $\\sigma = 0.03$. DriftNet's performance remained stable,\nwith values between $80.90 \\pm 0.47\\%$ and $84.96 \\pm 0.42\\%$ (Extended Data Fig. 2b). Second, we compared\ndifferent noise sources, including node dropout, gradient noise, input noise, and batch sampling noise (see\nMethods), using CIFAR-10 and CIFAR-100. The average test accuracy ranged from $80.19 \\pm 0.67\\%$ to\n$81.04 \\pm 0.67\\%$ for CIFAR-10 and from $41.83 \\pm 0.75\\%$ to $43.95 \\pm 0.44\\%$ for CIFAR-100 (Extended Data\nFig. 3a). These results suggest that external noises, including dropout, gradient noise, and input noise,\ndid not significantly affect performance when the batch size was small (16). The pairwise Mann-Whitney\nU-test p-values between noise types are all above 0.05, suggesting they are not statistically different. This\nmotivated us to further assess DriftNet with reduced batch sampling noise.\nWe then examined the impact of external noise on DriftNet's lifelong learning capability with varying\nbatch sizes. First, we evaluated DriftNet with batch sampling noise only. The results showed that average\ntest accuracy varied slightly from $83.77 \\pm 0.56\\%$ to $83.57 \\pm 0.39\\%$ for small batch sizes (16 to 100), but\ndropped significantly to $51.87 \\pm 2.47\\%$ when batch sizes increased from 100 to 3000 (Extended Data\nFig. 4a). This indicates that lifelong learning ability is maintained for small batch sizes, but degrades\nsignificantly for larger batch sizes. Second, we investigated whether dropout noise could mitigate the\ndecline in lifelong learning ability as batch sampling noise decreases. With dropout, the average test\naccuracy decreased from $84.39 \\pm 0.67\\%$ to $60.40 \\pm 1.95\\%$ for batch sizes from 100 to 3000 (Extended\nData Fig. 4a). The ratio of the average test accuracy with dropout to that with only sampling noise\nwas $1.19 \\pm 0.07$. These findings suggest that DriftNet maintains its lifelong learning capability under\nsignificant noise, whether the batch sampling noise is large (small batch sizes) or small (large batch\nsizes) with additional dropout noise. Third, we evaluated DriftNet's lifelong learning ability in relation\nto the severity of noise-induced drift (drift rate) (Extended Data Fig. 4b). The drift rate decreased from\n$1765.20 \\pm 76.97$ to $1.69 \\pm 0.64$ as batch sizes increased with only batch sampling noise. With dropout, the\ndrift rate was higher, decreasing from $2955.41 \\pm 190.18$ to $24.28 \\pm 3.03$ as the batch sizes increased from\n16 to 3000. Furthermore, the results show that the slope of the fitted linear regression line of average test\naccuracy relative to the logarithm of drift rate is 0.08, indicating that the average test accuracy increases\nas the drift rate decreases. These findings suggest the crucial role of drift induced by externally injected\nnoise, especially when batch sampling noise is reduced, in enhancing lifelong learning capability.\nNLP is an important field that aims to enable machines to understand and generate human language 46, 47.\nRecently, advances in LLMs have shown significant advancements in model architectures, such as\nTransformer with self-attention48, and the development of powerful pre-trained language models49,50.\nHowever, training a new language model from scratch for every new task is computationally expensive.\nFor example, training GPT-3, which has 175 billion parameters, requires $3.14 \\times 10^{23}$ floating-point\noperations per second (FLOPS)51. Under hypothetical conditions without memory limitations, completing\nthis training on a single Nvidia V100 GPU would take around 288 years52. On the other hand, naively\nfine-tuning the pre-trained LLM on a sequence of different language tasks with changing distributions can\nlead to potential catastrophic forgetting of previously learned knowledge53\u201355.\nGiven these challenges, we explored whether DriftNet, designed to be compatible with general model\narchitectures and noise sources, could build effective lifelong learning LLMs with limited computational\nresources. We integrated DriftNet with a pre-trained LLM, GPT-256, and sequentially trained it on four\nlanguage datasets related to topic classification and sentiment analysis: AG's News, Amazon Review\nFull, DBpedia, and Yahoo! Answers57 (see Methods). DriftNet achieved an average test accuracy\nof $70.37 \\pm 1.22\\%$ (mean $\\pm$ SE from 5 repetitions for this section) over 4 tasks, while the Theoretical\nLimits baseline has $80.61 \\pm 0.09\\%$, the Joint baseline has $80.42 \\pm 0.09\\%$, and the Stable baseline has"}, {"title": "Methods", "content": "$18.29 \\pm 0.06\\%$ (Fig. 5b-d). This result demonstrates substantially that DriftNet effectively learns new\ntasks while mitigating the forgetting of previously learned tasks (Fig. 5c and Extended Data Fig. 5b).\nTo assess whether drift-induced exploration of various local minima can retrieve related knowledge,\nwe evaluated the uncertainty of outputs from local minima corresponding to relevant and irrelevant tasks,\nrespectively. For all tasks, the uncertainty of outputs from local minima of relevant tasks (in-distribution)\nis relatively low (ranging from $0.32 \\pm 0.04 \\times 10^{-4}$ to $1.46 \\pm 0.03 \\times 10^{-4}$), whereas for irrelevant tasks\n(out-of-distribution), it is relatively high (ranging from $3.10 \\pm 0.04 \\times 10^{-4}$ to $7.67 \\pm 0.19 \\times 10^{-4}$). The\nuncertainty values are statistically significantly different between the in-distribution and out-of-distribution\ndata, with p-values below $10^{-3}$ for all tasks, under both Student's t-test and Mann-Whitney U-test (see\nMethods). These results suggest that task-specific groups of local minima exhibit different uncertainty\nlevels in response to related or irrelevant task inputs, allowing the retrieval of relevant knowledge.\nIn this paper, we demonstrate that the implementation of drift within artificial neural networks is a\ncritical mechanism to enhance lifelong learning by mitigating catastrophic forgetting. This is achieved\nthrough the exploration of diverse local minima in the loss landscape, which helps prevent overwriting\nand facilitates the future retrieval of previously learned knowledge. Based on this discovery, we introduce\na unified general framework, DriftNet, which continuously explores diverse local minima across the\nloss landscape and encodes them as task-specific groups without overwriting existing tasks. Importantly,\nDriftNet utilizes uncertainty measures to identify the task-specific group that provides similar outputs with\ncomparatively lower uncertainty. This dynamic process, involving exploration, encoding, and retrieval\nsteps, offers a robust and general lifelong learning solution. We verified the performance of the DriftNet\non simulated datasets, image datasets (CIFAR-10 and CIFAR-100), and NLP tasks using pre-trained\nLLMs. In these experiments, DriftNet demonstrates superior performance in alleviating the forgetting of\nlearned tasks. Specifically, DriftNet achieved an average test accuracy of $70.37 \\pm 1.22\\%$ across NLP tasks,\nsignificantly higher than the Stable baseline's $18.29 \\pm 0.06\\%$ and approaching the Theoretical Limits\nbaseline's $80.61 \\pm 0.09\\%$ and Joint baseline's $80.42 \\pm 0.09\\%$. Additionally, DriftNet is designed to be\nhighly scalable, reducing computational costs and making it feasible to run on a single Nvidia A100\nGPU. While AI has shown impressive capabilities in static environments, its ability to continually adapt\nto new environments without forgetting acquired skills a hallmark of human intelligence \u2013 remains\nunderdeveloped. This adaptability is crucial for autonomous systems like self-driving vehicles and robots,\nwhere environments are constantly changing. By incorporating neural representational drift observed\nin biological brains, DriftNet significantly enhances the lifelong learning capability of artificial neural\nnetworks, offering a scalable and adaptable solution applicable across various model structures and AI\nsystems operating in dynamic environments.\nWe envision several directions for future research. First, further studies on the selection of local\nminima could improve the effectiveness of uncertainty measures in retrieving relevant knowledge. For\nexample, selecting local minima that are well-separated under certain distances could ensure greater\ndiverse local minima, thereby improving generalization. Second, DriftNet could be broadly applied in\nscenarios involving multi-modal data (e.g., vision, language, and audio) or real-time learning systems (e.g.,\nautonomous vehicles and robotics), where continuous learning and decision-making are critical. Third,\nhybridizing DriftNet with other lifelong learning approaches could create hybrid models that leverage the\nstrengths of multiple methodologies and make the learning system more robust. For example, combining\nDriftNet with reply-based methods 10 to train a shared feature extractor, or with architecture-based methods\nto train generators, could address challenges related to high dimensionality22."}, {"title": "Definitions", "content": "Let $\\mathbb{N"}, "denote the set of all positive integers and $\\mathbb{R}$ denote the set of all real numbers. Define $[n"], "as": "n$\\arg\\max(A) \\triangleq \\{i \\in [q] : a_i = \\max(A)\\"}, {"as": "n$\\text{entropy}(\\mathbf{a}) \\triangleq - \\sum_{i=1}^{q} a_i \\log a_i$.\nThe indicator function $\\mathbb{1}(E)$ for any"}]