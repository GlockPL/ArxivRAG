{"title": "Finding Words Associated with DIF: Predicting Differential Item Functioning using LLMs and Explainable AI", "authors": ["Hotaka Maeda", "Yikai Lu"], "abstract": "We fine-tuned and compared several encoder-based Transformer large language models (LLM) to predict differential item functioning (DIF) from the item text. We then applied explainable artificial intelligence (XAI) methods to these models to identify specific words associated with DIF. The data included 42,180 items designed for English language arts and mathematics summative state assessments among students in grades 3 to 11. Prediction $R^2$ ranged from .04 to .32 among eight focal and reference group pairs. Our findings suggest that many words associated with DIF reflect minor sub-domains included in the test blueprint by design, rather than construct-irrelevant item content that should be removed from assessments. This may explain why qualitative reviews of DIF items often yield confusing or inconclusive results. Our approach can be used to screen words associated with DIF during the item-writing process for immediate revision, or help review traditional DIF analysis results by highlighting key words in the text. Extensions of this research can enhance the fairness of assessment programs, especially those that lack resources to build high-quality items, and among smaller subpopulations where we do not have sufficient sample sizes for traditional DIF analyses.\nKeywords: BERT, assessment fairness, artificial intelligence, natural language processing, Transformer, large language model", "sections": [{"title": "Introduction", "content": "Fair assessments are paramount in education. Students should not be given any advantage or disadvantage based on their demographic backgrounds. Items that exhibit such bias can be identified using differential item functioning (DIF) analysis. DIF is when correct item response probability depends on the examinee's demographic background, even after accounting for ability (Holland & Wainer, 2012). Although DIF analysis has become a standard procedure for evaluating item fairness in educational and psychological assessment (AERA et al., 2014), it is difficult to perform for several reasons. First, it requires data from at least hundreds, if not thousands of examinees for each group for each item. Obtaining the sample size is especially challenging for minority groups that make up a smaller proportion of the population. Second, identified DIF items are not always considered unfair, and should not be automatically removed from assessments. For example, DIF can simply indicate legitimate differences in ability between groups due to sub-domains present in the test blueprint by design (Osterlind & Everson, 2009). Evaluating DIF items require thoughtful qualitative human review (AERA et al., 2014), but the source of DIF is often difficult to identify (Angoff, 1993).\nRecently, large language models (LLMs) based on Transformers (Vaswani et al., 2017) have quickly grown in their capability to understand text data (Devlin et al., 2019). LLMs are commonly recognized as black-box machine learning (ML) models due to the complexity of their model architecture. However, explainable artificial intelligence (XAI) methods have been developed to provide interpretable explanations to better understand predictions or decisions made by these black-box models (Kokhlikyan et al., 2020). More specifically, XAI methods can identify and attribute the influence of specific features of an ML model (e.g., variables or tokens) on its predictions. If LLMs can be used to predict DIF, and XAI methods can describe how the model makes such prediction, then we can identify specific words in the items that are associated with DIF (see Figure 1). As a consequence, the combination of LLMs and XAI methods may be able to: (1) screen words associated with DIF during the item-writing process for immediate revision, (2) help review traditional DIF results by highlighting key words in the text, or (3) even replace the traditional DIF analysis process altogether if the prediction accuracy is sufficient.\nThis paper contains two investigations. The purpose of"}, {"title": "Background on DIF", "content": "Dichotomous and polytomous items in this paper were calibrated based on the two-parameter logistic model (Birnbaum, 1968) and generalized partial credit model (Muraki, 1992), respectively. The scores based on these models were used to calculate the following DIF statistics for the respective item type, and used as labels for prediction."}, {"title": "Dichotomous Items", "content": "For evaluating DIF among dichotomously scored items, the Mantel-Haenszel delta-difference (MH D-DIF) statistic (Dorans & Holland, 1992; Mantel & Haenszel, 1959) is commonly used. MH D-DIF for a particular item is expressed as\n$MH D-DIF = -2.35 ln(a),$    (1)\nwhich is a normalized transformation of the common odds ratio a. To calculate a, let $n_{Rsk}$ and $n_{Fsk}$ represent the number of reference and focal group examinees in the sth item score value in ability level group k. The reference group is often defined as the majority group which is suspected to have an advantage, while the focal group refers to the group suspected to have a disadvantage. Ability level groups are created based on deciles (i.e., ten equal size bins) of reference group 9. The focal group responses are categorized into those ability groups. For dichotomous items, $s \\in \\{0, 1\\}$,\n$\\alpha = \\frac{\\sum_k n_{R1k}n_{F0k}/n_{++k}}{\\sum_k n_{F1k}n_{R0k}/n_{++k}}$      (2)\nwhere the + indicates a summation over the particular index. For example, $n_{++k}$ is the number of all reference and focal examinees with any item score in ability group k. Standard error (SE) of MH D-DIF (Dorans & Holland, 1992; Holland & Thayer, 2013) is calculated as\n$SE(MH D-DIF) = 2.35 \\sqrt{Var[ln(\\alpha_{MH})]},$     (3)\nand\n$Var[ln(\\alpha_{MH})] = \\sum_k \\frac{n_{R1k}n_{F0k}}{(n_{R1k}n_{F0k} + \\alpha n_{R0k}n_{F1k})^2}  \\times [n_{R1k} + n_{F0k} + \\alpha(n_{R0k} + n_{F1k})].$     (4)\nA negative, zero, and positive MH D-DIF shows that the item favors the reference group, no group, and focal group, respectively. The strength of DIF is interpreted as follows (Dorans & Holland, 1992; Zieky et al., 1993):\n*   Negligible DIF (A): |MH D-DIF| < 1 or not significantly different from zero\n*   Intermediate DIF (B): |MH D-DIF| \u2265 1, significantly different from zero, and |MH D-DIF| < 1.5 or not significantly greater than 1\n*   Large DIF (C): |MH D-DIF| \u2265 1.5 and significantly greater than 1 in absolute value"}, {"title": "Polytomous items", "content": "In this paper, for polytomous items scored in 3 or more categories, the effect size (ES) statistic based on the standardized mean difference (SMD) are used to evaluate DIF (Zwick & Thayer, 1996; Zwick et al., 1997):\n$ES = \\frac{SMD}{\\sigma_{pooled}}$     (5)\nand\n$SMD = \\sum_k P_{Fk}m_{Fk} - \\sum_k P_{Fk}m_{Rk},$     (6)\nwhere $P_{Fk} = \\frac{F_{+k}}{N_{++k}}$ is the proportion of focal group members in group k, and $m_{Fk}$ and $m_{Rk}$ are the mean item scores for the respective groups. Ability level groups are formed based on following the same procedure as the dichotomous items. The $\\sigma_{pooled}$ is the pooled standard deviation (SD) of item scores, calculated as\n$\\sigma_{pooled} = \\sqrt{\\frac{(N_F - 1)\\sigma_F^2 + (N_R - 1)\\sigma_R^2}{N_F + N_R - 2}},$     (7)\nwhere N is the number of examinees, and $\\sigma_F^2$ and $\\sigma_R^2$ are the group item score variance. The SE of SMD can be calculated based on the hypergeometric variance (Mantel, 1963):\n$SE(SMD) = \\sqrt{\\sum_k \\frac{P_{Fk}^2}{N_{R+k}N_{F+k}} + \\frac{1}{N_{R+k}}Var(F_k)},$      (8)\nand\n$Var(F_k) = \\frac{N_{++k}}{n^2_{++k}(n_{++k}-1)}[(\\sum_s z_s n_{+sk})^2 - \\sum_s n_{+sk}z_s^2],$     (9)\nwhere $z_s$ is the score value of score index s. For example, $\\sum_s Z_s n_{+sk}$ represents the sum of all scores in group k. The SE of SMD can be rescaled to the SE of ES using $\\sigma_{pooled}$:\n$SE(ES) = \\frac{SE(SMD)}{\\sigma_{pooled}}$      (10)\nA negative, zero, and positive ES shows that the item favors the reference group, no group, and focal group, respectively. The strength of DIF is interpreted as follows (Michaelides, 2008):\n*   Negligible DIF (A): |ES| < 0.17 or not significantly different from zero\n*   Intermediate DIF (B): 0.25 > |ES| \u2265 0.17 and significantly different from zero\n*   Large DIF (C): |ES| \u2265 0.25 and significantly different from zero"}, {"title": "Background on Transformer LLM", "content": "The introduction of Transformers (Vaswani et al., 2017) revolutionized natural language processing (NLP) by leveraging attention mechanisms to process entire text sequences in parallel, enabling faster training and scalability. BERT (Devlin et al., 2019) is the widely adopted Transformer-based LLM architecture, which advanced the field by offering pre-trained models for downstream tasks such as text classification and question answering. Transformers excel at capturing contextual and semantic nuances, achieving state-of-the-art performance across benchmarks.\nDespite the current popularity of decoder-only LLMs like GPT (T. B. Brown et al., 2020), encoder-only models like BERT have a significant efficiency advantage when the task is specialized and non-generative (Warner et al., 2024). Building upon BERT, DeBERTa (Decoding-enhanced BERT with disentangled attention) was introduced with innovations in attention mechanisms (He et al., 2021). Each word in DeBERTa is represented by two independent vectors: one for content and another for position. Using these vectors, the model computes the attention weights that take into account the interaction between word content and position. (He et al., 2021). For example, the words \"artificial\" and \"intelligence\" takes on a unique meaning when they occur adjacently, which is something DeBERTa can distinguish. In this paper, we model DIF by fine-tuning the pre-trained DeBERTa-v3-large model."}, {"title": "Background on XAI", "content": "The goal of Explainable AI (XAI) is to enhance the interpretability of machine learning models, making them more transparent and trustworthy for human users (Hassija et al., 2024; Tjoa & Guan, 2021). As outlined by Tjoa and Guan (2021), XAI approaches typically aim to achieve one or more of the following objectives: (1) explaining the decisions or predictions made by a machine learning model, (2) finding the patterns within the internal mechanisms of the model, or (3) presenting the system with coherent models or mathematics.\nIn our study, XAI plays a crucial role in enhancing the utility and usability of a DIF prediction model by explaining the predictions made by Transformer LLMs and generating explanations that are mathematically coherent. There are two advantages in achieving these goals. First, it provides item writers with interpretable insights that can guide the creation of unbiased test items. By making the model's output understandable, item writers can interpret its predictions and adjust their items accordingly to ensure fairness. Second, it helps identify potential model misclassifications. By providing a clear understanding of the model's decision-making logic, it helps reduce the likelihood of item writers being misled by incorrect predictions, ensuring greater accountability of the model.\nHowever, explaining the predictions of Transformer-based LLMs presents several unique challenges, particularly when dealing with textual data. For example, one broad area of XAI approaches explains the decision of a model by assigning values or creating an interpretable model that reflect the importance of features in their contribution to that decision, which includes the saliency methods (Tjoa & Guan, 2021), explanation by simplification (Angelov et al., 2021), or explanation by feature relevance/importance (Angelov et al., 2021). However, understanding the importance of specific words requires analyzing their role within the context of a"}, {"title": "Study 1: Comparison of Categorical and Continuous Models", "content": "The goal of Study 1 is to identify the methodology that offers the best combination of accuracy and interpretability of finding words associated with DIF. Using gender DIF data, we fine-tune models that predict DIF from the item text, and apply SHAP on the resulting model. Then we compare the utility of SHAP in identifying keywords associated with the DIF prediction models.\nAlthough data were available for seven other DIF group pairings (see Study 2 for details), we focused exclusively on gender in Study 1 as it had the largest sample size in terms of both items and the number of focal and reference students per item. Therefore, we expected the gender data to provide the most reliable and generalizable results."}, {"title": "Item Data", "content": "We used 42,176 items designed for English language arts and mathematics state summative assessments among students in grades 3 to 11. These items were written in accordance with the fairness Standards for Educational and Psychological Testing (AERA et al., 2014) by removing unnecessary barriers to the success of diverse groups of test takers. These basic guidelines were followed: (1) avoid measuring irrelevant constructs that advantages or disadvantages particular subgroups, (2) avoid angering, offending, upsetting, or distracting test takers, and (3) show respectful representation and treatment of all representatives of various cultures and backgrounds. Therefore, these items were designed to avoid bias and DIF. We expect any remaining bias to be rare and difficult to identify both objectively and subjectively.\nAll items have been previously field-tested and calibrated among grade-appropriate public school students in 20 states and territories in the United States from 2013 to 2023. Up to 10 unscored field-test items were embedded within a test with 20 to 35 scored items. Exams were administered in a computer adaptive testing format, but the field-test items were administered randomly (i.e., non-adaptive). Most students took both the English language arts and mathematics exams. About 30% of these items are no longer in operational use for various reasons, including but not limited to (1) failing to pass field-testing due to DIF or poor item statistics, or (2) they were publicly released.\nTo prepare the data for modeling, all item text was concatenated with a [SEP] separator between prompts and any available answer options. Only multiple-choice (33%), multiple-select (14%), and hot-text (8%) items had item options. Any reading or listening passages were excluded from the item text data due to their extensive lengths and the token limit of our models, which affected 37% of items. The maximum token size was set at 512, which caused 1% of items to have text removed from their tail ends (M tokens=124,"}, {"title": "DIF Prediction", "content": "We partitioned the data randomly into approximately 80% training, 10% validation, and 10% test data. Items designed as a testlet with common passages were always assigned to the same data group. DeBERTa V3-large Transformer LLM (He et al., 2021) was fine-tuned to predict the DIF metric from the item text in Python using the pytorch library (Paszke et al., 2019) and a single NVIDIA A10G Tensor Core 24GB graphics processor (batch size=8, learning rate=4e-6, weight decay=.001). Training items were used to fine-tune the model, while the validation data was used to find the model with the lowest loss across epochs, which resulted in keeping the 2nd epoch for all four models described below. Test data were used to evaluate and report the prediction results."}, {"title": "Continuous Model", "content": "In the continuous model, we treat DIF as a continuous variable, and predict it from the item text. The mean-squared-error (MSE) loss is minimized in the fine-tuning process, which quantifies the distance between the target and predicted DIF values:\n$MSE = \\frac{\\sum_i (\\hat{Y_i} - Y_i)^2}{N}$      (12)\nwhere Y; is the target DIF statistic from the input data for item i, \u0176; is its predicted value, and N is the number of items."}, {"title": "Categorical Model", "content": "We also model DIF as a categorical variable in order to utilize its intermediate DIF cutoff values of \u2265 1 and \u2264 -1. This ensures the model to be more accurate around these boundaries. Instead of predicting the categories directly, which would have resulted in severely unbalanced groups, we predict the probability of being classified into one of three DIF group categories (favoring reference, focal, or no group) upon repeated experiments. The probability of classifying item i into DIF category g, denoted as $P_{ig}$, is derived based on the sampling distribution of the target DIF statistic, which is assumed to be normal with \u03bc = $Y_i$ and \u03c3\u00b2 = $SE_i^2$. The probability $P_{ig}$ is computed by using the cumulative distribution function, \u03a6i, for N($Y_i, SE_i^2$):\n$P_{ig} = \\begin{cases} \\Phi_i(-1), & \\text{if } g = \\text{Reference group} \\\\ \\Phi_i(1) - \\Phi_i(-1), & \\text{if } g = \\text{No DIF group} \\\\ 1 - \\Phi_i(1), & \\text{if } g = \\text{Focal group} \\end{cases}$     (13)\nwhere -1 and 1 represent the DIF group cutoff thresholds. Model output logits $L_{ig}$ were passed through a softmax function to calculate $P_{ig}$, the predicted value of $P_{ig}$\n$P_{ig} = \\frac{e^{L_{ig}}}{\\sum_g e^{L_{ig}}}$     (14)\nso that $\\sum_g P_{ig} = 1$. The cross entropy loss (CEL) is minimized during the fine-tuning process, which quantifies the distance between target and predicted probabilities:\n$CEL = -\\frac{1}{N} \\sum_i \\sum_g P_{ig} log P_{ig}$    (15)\nFrom here on, item subscript i is suppressed from all expressions for succinctness (e.g., $P_{ig} = P_{g}$)."}, {"title": "Alternative Random Seed Models", "content": "Fine-tuning Transformer LLMs involves stochastic optimization. We suspected that this could heavily influence the outcomes of SHAP, as XAI methods are often sensitive to minor variations and may produce inconsistent results (Pirie et al., 2023). To investigate this, we replicated the continuous and categorical models with an additional different random seed prior to fine-tuning. Therefore, a total of four models were fine-tuned."}, {"title": "XAI for the Continuous Model", "content": "The Explainer class from the shap python package (Lundberg & Lee, 2017) was used to calculate token attributions for all models (fixed_context=1, algorithm=\"partition\"). For each item i, we calculated attribution values d\u2081 for every token t to the predicted DIF \u0176. Each item can have up to 512 tokens. Token attributions are similar to regression slopes, and is on the same scale as \u0176. For example, an attribution value of 0.1 means that the token contributes to the prediction Y by 0.1."}, {"title": "XAI for the Categorical Model", "content": "Unlike the continuous model, the categorical model outputs three probabilities per item, representing the three DIF"}, {"title": "Evaluation of Prediction and XAI Methods", "content": "Using the test data, accuracy, simplicity, and interpretability of the prediction and attribution values were compared between models. Output predicted values were entered into a multiple regression model to re-predict Y to calculate $R^2$. For the continuous models, \u0176 was the sole explanatory variable. For the categorical models, the logit of the three probability values Pg were entered as explanatory variables, including all interaction terms. This is a simple approach to compare all models with $R^2$, especially because CEL is difficult to interpret. Higher $R^2$ values indicate a better prediction, which also suggests that the attribution values are more accurate.\nThere are no agreed upon methods to evaluate post-hoc XAI metrics, but they should quantify the simplicity and accuracy of the interpretations (Zhou et al., 2021). Attributions were evaluated in the following manner. First, token attributions should be unbiased, such that when Y = 0, average attribution should also be 0 when the baseline predicted probabilities (i.e., Po) is 0. Therefore, we calculated bias by taking the mean of ot for 100 items with Y values closest to 0. Further, given the properties of the attribution values ot, it should be positively correlated with Y. Additionally, a peaky marginal distribution of ot is desirable, where most tokens have near-zero influence on the predicted DIF, while\n$\\phi_t = \\begin{cases} -1 & \\text{if } \\phi_{t1} > 0,  \\ 1 & \\text{if } \\phi_{t3} > 0, \\ 0 & \\text{otherwise,}  \\text{otherwise,} \\end{cases}$    (16)\nwhere $\\phi_{t1}$ is token attribution for the reference group, and $\\phi_{t3}$ is the focal group. As the equation shows, we ignored $\\phi_{t2}$ (i.e., no DIF group) because it tended to capture the noise in the DIF data, and focusing on the other two groups isolated and retained the important information. We also ignored negative values for $\\phi_{t1}$ and $\\phi_{t3}$ as they indicated an association with the other two groups, which was redundant. As a result, in both the continuous and categorical models, a negative ot indicated favoring the reference group (male students), while a positive ot indicated favoring the focal group (female students). Correlations of attributions with the original three attributions were $r(\\phi_t, \\phi_{t1}) = -.46, r(\\phi_t, \\phi_{t2}) = -.34,$ and $r(\\phi_t, \\phi_{t3}) = .75$."}, {"title": "Study 1: Results", "content": "The $R^2$ for the test data was .33, .33, .31, and .31, for the two continuous models and the two categorical models, respectively, showing similar predictive power across all four models. The $R^2$ for the average of the two seed alternatives for the continuous and categorical models showed no meaningful change (.33 and .32, respectively). Attribution value bias was minimal for all models (between -0.0001 and 0.0002; see Table 1). The correlation of attribution values between two random seed alternative models was r = .66 (RMSE=0.016) for continuous and r = .60 for categorical models (RMSE=0.0031). Based on these correlations and the Spearman-Brown Prophecy formula (W. Brown, 1910; Spearman, 1910), the reliability of the average of two models was p\u2081 = .80 and .75 for continuous and categorical models, respectively. The improvement in reliability resulted in about a 10% increase in r(ot,Y) for both models. This shows that fine-tuning and averaging multiple models can improve the accuracy of the attribution values. Correlations between the attributions and DIF showed that interpreting individual attributions from the categorical model (r = .200) is more accurate and less noisy compared to the continuous model (r = .088). Kurtosis was also higher for the categorical model (515) than continuous (90), showing easier interpre-"}, {"title": "Study 2: Application of the Averaged Categorical Model to Eight DIF Groups", "content": "In this section, we apply the averaged categorical model to eight DIF group pairs separately. Other than the inclusion of additional groups, the item data and general procedures were identical to the averaged categorical model approach in Study 1. The focal/reference group pairs included (1) female/male, (2) Asian and Pacific Islanders (Asian)/White, (3) Black or African American (Black)/White, (4) Hispanic or Latino (Hispanic)/White, (5) Native American and Alaskan Native (Native)/White, (6) lower social economic status (LSES)/Non-LSES, (7) students with disabilities (SWD)/Non-SWD, and (8) English learners (EL)/Non-EL. Students associating with multiple races were excluded from race-based DIF analyses because they could not be classified into one group. Items having less than 100 examinees per focal and reference group were excluded from the analyses for the respective group pair. A total of 42,180 items had at least one DIF group data available. However, the number of students per item for the focal group was sometimes very low, which limited the number of included items and increased the DIF SE (see Table 3). In particular, Native American and Alaskan Native group had the lowest number of items (4,682) and students per item (166).\nIdentical to Study 1, data were randomly partitioned into approximately 80% training, 10% validation, and 10% test data. After fine-tuning, SHAP procedure was applied and used to find prominent words that were associated with DIF. To find the top 10 tokens favoring the focal group, tokens with ot < 0.01 and non-alphabet characters were removed from the test data. Tokens were then converted to all lower case. Tokens occurring less than 3 times were also removed. Finally, the top tokens were found based on the highest within-token mean of ot across all items. Similarly, the top tokens favoring the reference group were found by removing the data with ot > \u22120.01, and finding the tokens with the lowest mean of ot."}, {"title": "Study 2: Results", "content": "Results from the test data are reported in this section. The $R^2$ varied considerably depending on the group (see Table 4). For example, the female/male group simultaneously had the highest (1) number of items, (2) sample size per item, and (3) $R^2$ of .32. On the contrary, all three of these statistics were the lowest among the Native/White group. One reason for these disparities is the small sample size, which leads to poor generalizability, affecting model performance on test data. An alternative reason is that DIF items were more common among some groups than others, as evidenced by the varying DIF SD ranging from 0.39 to 0.89. DIF with a higher SD is presumably easier to predict because there are more items with extreme DIF effects. The final possible reason for the group differences in $R^2$ is that DIF for some groups are simply more difficult to predict based on the text. For example, Hispanic/White group had similar sample sizes and DIF SD as female/male but had much lower $R^2$ of .16. Additionally, differences in the prediction RMSE and correlations showed that DIF favoring the reference group is sometimes easier to predict than the focal, or vice versa. For example, items favoring female students were easier to predict (RMSE=0.11, r=.62) than male students (RMSE = 0.14, r=.33).\nMean attribution values were near 0 for all groups (range from -.001 to .000; see Table 5). The poor overall model fit of the Native/White group was reflected in the low $r(\\phi_1, Y)$ = .02 and $p_{\\phi_1}$ = .21. Kurtosis was highest for SWD/Non-SWD, and $r(\\phi_1, Y)$ was highest for the female/male group. Other than the Native/White group, $p_{\\phi_1}$ was near .70 for all (range from .61 to .75)."}, {"title": "Qualitative Results", "content": "Example item text along with their token attribution values are plotted in Figure 2. These items all had observed DIF of < -1 or > 1, which may warrant item review in traditional DIF analyses. Highlighting text based on token attributions may assist in understanding what may have caused the DIF flagging to occur. For example, the fourth item on the figure shows \"dog walks\" as a phrase that favors non-SWD students, potentially because they are able to walk dogs, while some SWD students may have walking limitations. However, many of the token attributions are still difficult to interpret qualitatively. Nevertheless, visualizations in this format may be helpful for item developers for pinpointing, understanding, or revising the potential sources of DIF.\nTop 10 tokens favoring with each focal and reference groups are listed in Table 6, although some groups had less than 10 tokens that qualified. Some words simply seemed spurious and not meaningful (e.g., a, two, for, is). The literature recognizes that Transformer models attend to irrelevant information (Ye et al., 2024), which could explain these attribution values that seem like noise. Interestingly, names such as \"John\", \"Karen\", and \"Aaron\" tended to favor White students. However, many of these influential words were directly related to the English language arts subject (e.g., text, summarize) or mathematics (e.g., multiplication, equation), suggesting that the DIF associated with these words may be minor sub-domains included in the test blueprint by design, rather than construct-irrelevant item content that should be removed from assessments."}, {"title": "Discussion", "content": "To the authors' knowledge, this is the first paper to predict DIF using LLMs or NLP, and also the first to identify words associated with DIF using XAI. DIF prediction accuracy was low to moderately high depending on the group. Our method may help improve item development by providing a type of contextual feedback that was previously unavailable. Our"}, {"title": "Limitations", "content": "One of the challenges of training our own LLM is computational capacity. Fine-tuning a single LLM and calculating its SHAP attribution values can take multiple days. This was the main reason why models from only two different seeds were trained. Fine-tuning and averaging more than two models will further improve attribution value accuracy as a result of increased fine-tuning reliability, and is recommended if resources allow.\nHowever, fine-tuning instability may go beyond the random seed we considered in this paper. For example, fine-tuning using slightly different hyperparameters or training data may have considerable influence on the attribution val-\nues. Also, the original DIF target variable is already an estimate associated with error variance. In particular, fine-tuning a well-fitting model is extremely difficult for low-sample size groups with high DIF standard error, such as the Native American and Alaskan Native group.\nFurther, formatting item text for LLM input is often challenging. In our case, the item text was stored in HTML format within JSON data, which we had to convert to plain text before LLM consumption. Items included features like bold, underlined, or italicized text, as well as images, graphs, tables, audio, and various accessibility options. These elements that extend beyond plain text, long reading passages, and the correct answer keys had to be excluded from the input data. Additionally, when students take the exam on a computer, they interact with information positioned in specific screen locations (e.g., spacing of paragraphs and response options), which can influence their experience of the item. These factors make it difficult for AI to replicate the way human test-takers perceive and engage with items."}, {"title": "Future Directions", "content": "SHAP (Lundberg & Lee, 2017) is just one of the many XAI methods that exist in the literature. Testing others may be worthwhile as they tend to provide varied results (Pirie et al., 2023). Calculating and displaying standard error-like metrics to characterize the precision of token attributions may further help their interpretability among the lay audience (e.g., item writers). Our proposed approach may be\nfurther enhanced by including covariates in the prediction, which are variables linked to item difficulty but not captured by LLMs. These may include readability indices, text length (AlKhuzaey et al., 2023), or response time (Duan & Cheng, 2024).\nA recently introduced encoder Transformer LLM called ModernBERT could address multiple limitations we faced in this paper (Warner et al., 2024). For example, its alternating attention, unpadding, and flash attention mechanisms provide considerable computational efficiency improvements. Having a 8,192 token limit, it can process items with very long reading passages. Lastly, ModernBERT was trained on web documents and code, so it may be able to understand raw HTML item text without the loss of nuanced item formatting information.\nSimilarly, a new LLM architecture called the Diff Transformer (Ye et al., 2024) may also be useful for furthering our research. Their paper recognizes that traditional Transformers tend to over-allocate attention to irrelevant information. This aligns with our observations in the current paper, especially with the continuous model. Diff Transformers are able to amplify model attention to the relevant context, while canceling irrelevant noise. This may translate to not only increased predictive power, but also improved precision of XAI methods in identifying words linked to DIF.\nAI researchers continue to propose new methods that could advance our contributions in this paper. We remain eager and vigilant for these developments."}]}