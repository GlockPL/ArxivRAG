{"title": "MolCap-Arena: A Comprehensive Captioning Benchmark on Language-Enhanced Molecular Property Prediction", "authors": ["Carl Edwards", "Ziqing Lu", "Ehsan Hajiramezanali", "Tommaso Biancalani", "Heng Ji", "Gabriele Scalia"], "abstract": "Bridging biomolecular modeling with natural language information, particularly through large language models (LLMs), has recently emerged as a promising interdisciplinary research area. LLMs, having been trained on large corpora of scientific documents, demonstrate significant potential in understanding and reasoning about biomolecules by providing enriched contextual and domain knowledge. However, the extent to which LLM-driven insights can improve performance on complex predictive tasks (e.g., toxicity) remains unclear. Further, the extent to which relevant knowledge can be extracted from LLMs also remains unknown. In this study, we present Molecule Caption Arena: the first comprehensive benchmark of LLM-augmented molecular property prediction. We evaluate over twenty LLMs, including both general-purpose and domain-specific molecule captioners, across diverse prediction tasks. To this goal, we introduce a novel, battle-based rating system. Our findings confirm the ability of LLM-extracted knowledge to enhance state-of-the-art molecular representations, with notable model-, prompt-, and dataset-specific variations.", "sections": [{"title": "1 Introduction", "content": "Biological research critically relies on molecular understanding to advance drug discovery. Machine learning methods have gained significant attention in recent years, accelerating and improving diverse tasks such as molecular design (Pang et al., 2023), reaction mining (Zhong et al., 2023) experiment procedure design (Zhong et al., 2024), reaction prediction (Wang et al., 2023), and quantitative structure-activity modeling (QSAR) (Heid et al., 2023). In particular, molecular property prediction remains a cornerstone in computational drug discovery (Tropsha et al., 2024), enabling diverse critical tasks such as bioactivity prediction, toxicity characterization, de-novo design, and virtual screening, among others."}, {"title": "2 Why do we need a benchmark?", "content": "With the introduction of molecule-text multimodality (Edwards et al., 2021; Zeng et al., 2022), considerable interest has arisen in the problem of molecule captioning (Su et al., 2022; Edwards et al., 2022; Liu et al., 2023). These captions are typically evaluated using traditional machine translation metrics such as BLEU and ROUGE scores. Additionally, metrics have been introduced which use similarity scores between representations (Edwards et al., 2022) or which extract specific text properties (Edwards et al., 2024b). However, these metrics still fundamentally rely on ground truth captions and, therefore, are intrinsically limited by the fact that molecules can be described with far more linguistic variety than traditional image captions (Edwards et al., 2024a). For example, a caption on paclitaxel might describe the different synthetic routes for its synthesis, but it could alternatively describe its mechanism of action for treating cancer, or even its pharmacokinetics and tissue distribution within the body.\nTo address these limitations, we introduce MOLCAP-ARENA. Our objective is to target an impactful, real-world problem: enchancing molecular property prediction models. In particular, we argue that the informativeness of a caption (and, in turn, of an LLM captioner) is closely tied to its ability to capture properties of interest, therefore connecting LLM-driven insights to tangible predictive tasks.\nThis supports three key goals: 1) improving existing molecular property prediction models by leveraging LLM-extracted knowledge, 2) grounding the evaluation of molecule captioners into their usefulness for real-world applications, and 3) enhancing explainability in molecule property prediction tasks by providing captions that aid human scientists in interpreting model predictions.\nWe note that very recent work has started exploring using LLMs to augment property prediction models (Guo et al., 2024; Nguyen et al., 2024; Xu et al., 2024). However, these studies generally rely on a single caption source with different integration strategies, making it hard to isolate the impact of the captions. In contrast, MOLCAP-ARENA offers a comprehensive benchmark across multiple captioners and tasks, with a standardized evaluation protocol for evaluating their contribution. This offers insights into the ability of LLMs to enhance molecular representations, providing a critical resource for further research into this promising area."}, {"title": "3 Ranking Captioners", "content": "Given that we are interested in evaluating the additional knowledge provided by different captions, we propose to augment a molecule-only model with caption information. We consider a simple yet flexible modeling paradigm based on late fusion of the molecule and text information. Each dataset is split into training, preference, validation and test splits, following a scaffold splitting strategy to simulate chemical distribution shifts (Wu et al., 2018)."}, {"title": "3.1 Augmenting molecular representations with caption information", "content": "We first train a GNN on the molecular structures. This model is based on graph isomorphism networks with edge features (Xu et al., 2019), such that \\(y_{\\text{task}} = GNN(g(\\text{mol}))\\), where g denotes the graph representation of the molecule \\(\\text{mol}\\), and task refers to the specific molecular property (e.g., toxicity, bioactivity, binding, etc.). This model provides a parameter-efficient yet robust baseline of the information that can be extracted solely from the molecular structure. Next, we train a text encoder on the captions using a specialized BERT model, bioLinkBERT-base (Yasunaga et al., 2022), such that \\(y_{\\text{task}} = \\text{BERT}(c(\\text{mol}))\\), where c is a captioner describing the molecule \\(\\text{mol}\\). Both the GNN and the BERT model are trained on the same training set, using the validation set for early stopping. Finally, caption embeddings \\(x_c\\), and molecule embeddings \\(x_m\\) are extracted from BERT and the GNN, respectively, and concatenated to obtain \\(x_{\\text{concat}} = x_c \\oplus x_m\\). A support vector machine model (Pedregosa et al., 2011) is trained on such joint embeddings, such that \\(y_{\\text{task}} = SVM(x_{\\text{concat}})\\), using the preference set as training data.\nWe observe how this strategy mitigates overfitting a source of information, as each modality is independently used to train a specialized architecture first, and the joint embedding is used to train a shallow model. This allows us to directly test the impact of a given caption on predictive performance. Similar latent fusion strategies have been successfully applied to integrate molecular descriptors (Heid et al., 2023) or to combine phenotypic readouts, such as imaging, with molecular representations (Moshkov et al., 2023). The predictions \\(y_{\\text{task}}\\) are used to calculate standard metrics for each captioner, such as ROC-AUC or MAE."}, {"title": "3.2 Rating captioners", "content": "In addition to computing standard metrics for each captioner, we introduce a novel rating system inspired by Chatbot Arena (Chiang et al., 2024). This strategy allows for a more direct and comprehensive comparison of the impact of different caption sources. In particular, it provides two key benefits: 1) it allows for the seamless integration of multiple datasets, spanning both regression and classification tasks, into a single metric, and 2) it offers a fine-grained, molecule-specific evaluation of captioners relative to each other (whereas standard metrics are calculated for each model before comparing different caption sources).\nSimilar to Chatbot Arena, we consider a ranking based on \u201cbattles\u201d between different caption sources. However, instead of human preference feedback, we use the prediction error \\(E(\\hat{y}_{\\text{task}}, y_{\\text{task}})\\), where \\(\\hat{y}_{\\text{task}}\\) is the ground truth label, as feedback. Specifically, given two captions \\(c_i(\\text{mol}), c_j(\\text{mol})\\) as input to the model M, the caption that yields the lowest error is deemed the \"winner\". In our setup, the prediction error serves as a proxy for the additional information contributed by the caption to the task-specific molecular property.\nThis approach requires a model M trained to understand captions and molecule structure; here, a naive approach would be to train on all captions combined. However, this can bias the model towards caption distributions that are overrepresented in the benchmark (e.g., when multiple variations of the same captioner are present). To avoid this bias and improve robustness, we instead train a model \\(M_{ij}\\) for each pair of captioners, constructing a dataset consisting of both \\(x_c^i\\) and \\(x_c^j\\). This \u201chead-to-head\" approach ensures no caption source has an unfair advantage due to related captions not being used in the battle. As detailed in Subsection 3.1, we leverage an SVM on top of pre-trained embeddings for \\(M_{ij}\\). Importantly, for a given molecule, the same GNN embedding \\(x_m\\) is used for both caption sources. This ensures that, given two captions for a molecule, the error difference exclusively comes from the capability of the caption to augment the molecular representation as optimized by the GNN. We further found this strategy to facilitate training.\nAfter constructing these battles, we calculate ratings for each model (as well as model-vs-model win rates). We employ the Bradley-Terry model (Bradley and Terry, 1952), a well-known approach to rank items, to produce ratings (see Chiang et al. (2024)). This also allows us to bootstrap confidence intervals for our ratings (10 bootstrap experiments are performed: each is 250,000 battles, uniformly sampled across all datasets)."}, {"title": "4 Experiments", "content": "Molecular datasets. To fairly rank captions, it is important to evaluate across multiple tasks. We select six datasets from the MoleculeNet benchmark (Wu et al., 2018) (Appendix A). These datasets represent diverse real-world applications, privileging small/medium datasets, as they can benefit more from multimodal augmentations.\nCaption sources. We consider a wide variety of domain-specific captioners (Edwards et al., 2022; Christofidellis et al., 2023) and general-purpose LLMs (Achiam et al., 2023; Dubey et al., 2024; Team et al., 2024). For general-purpose LLMs, we consider different \u201cpersonas\u201d for each model: a biologist, chemist, drug researcher, quantum chemist, and generic chemist. We supply the target molecule to each LLM either as the commonly-employed SMILES string (Weininger, 1988; Weininger et al., 1989), or as a set of fragments (using the the BRICS algorithm (Degen et al., 2008)). We also consider two theoretical caption sources as controls: the \"BlankCaption\", which contains no content, and a \u201cBBBP Oracle\u201d, which states the BBBP ground truth, reporting consistent results (see Appendix H). All results are repeated across 5 folds.\nCaptions consistently lead to improved performance, with dataset- and task-specific variations. Table 1 presents results for models. Overall, captions enhance the baseline performance of the GNN across all tasks. For classification tasks, the average ROC-AUC increases from 85.34 to 87.37, while for regression tasks, the average R2 improves from 0.397 to 0.413. We observe how multiple captioners improve GNN performance on some datasets, such as ClinTox, ESOL, and FreeSolv, while their impact is less pronounced on others, thus underscoring the task-specific nature of LLM-driven enhancements.\nDomain-specific captioners lead to best improvements, followed by large-scale, general-purpose LLMs. Generally, domain-specific captioners such as BioT5_plus and LLasMol lead to the best results. This is consistent with their specialized fine-tuning datasets. However, we also observe that strong general-purpose captioners, such as GPT-4o and Llama3.1-405B, rank among the top models, demonstrating their ability to perform competitively even without domain-specific training.\nLarger models correlate with score improvements, but model-specific safeguards can lead to reduced performance. Grouping ratings by model family (Table 5), we observe that Llama3.1-405B slightly outperforms the 70B version, and both significantly outperform the 8B version. Interestingly, however, Llama3-8B outperforms Llama3.1-8B. Upon further investigation, we discovered that this is due to an overzealous safeguarding mechanism in Llama3.1-8B, which frequently refuses to describe molecular structures (see Appendix L for examples). We note this may be a concerning development for the use of general LLMs in molecule-related tasks.\nDifferent personas and molecular representations can benefit different tasks and models. Grouping general-purpose LLMs by persona and molecular representation used, we observe both general trends and task-specific variations. On average, the chemist and drug researcher personas obtained the highest rankings (Table 6), which is consistent with their general, while still domain-specific prompt. Additionally, we find that captions of SMILES strings generally outperform captions of BRICS fragments (Table 4). However, we observe how the impact of different prompts is task-, representation- and model-specific, thus highlighting the need for future research into the connection between prompts and downstream performance.\nTask-specific prompts enhance performance. In addition to different personas, we evaluate the impact of task-specific prompts in general-purpose LLMs, where the model is provided a custom prompt for each different task. For this, we implement the Llama3-8B-Task captioner. Notably, when compared to other Llama-8B variations (Appendix E), the task-specific model leads to the best performance. This highlights the importance of application-specific prompts for molecular modeling tasks and underscores a trade-off between efficiency (i.e., general captions which can support multiple tasks) and accuracy.\nBattle-based rating provides a more robust and complementary evaluation metric. In addition to the theoretical advantages provided by the proposed rating metric (Subsection 3.2), we investigated its empirical robustness. Notably, we observe how our proposed rating system, even if correlated with standard metrics (Table 2), has better agreement across different tasks. For example, between"}, {"title": "5 Conclusion", "content": "We introduce MOLCAP-ARENA, a robust, large-scale benchmark for evaluating the capability of molecule captions to augment real-world property prediction tasks. Further, we propose a rating system for ranking models, which allows performance to be aggregated across different task types. This enables a new way of evaluating molecule captioners, which have primarily been evaluated using string-based metrics. Future work may also consider multi-modality evaluation. We make this benchmark and generated resources available to the community."}, {"title": "Limitations", "content": "Although we make considerable effort to produce a fair and accurate ranking between caption sources, there is likely still effects from unforeseen biases in the source models and their training datasets. Future captioners may also try to game the leaderboard. To some extent, this can be alleviated by extending future versions of the leaderboard to new datasets. We note multi-task datasets as an interesting direction for future research. Further, we use a fairly simple architecture which may limit the capabilities of some captioners. We leave the study of more advanced molecule-language fusion architectures to future work. Another possible limitation is that we choose battles based purely on which caption source produces a lower error. It may be beneficial to take into account margin of victory in future work. In our preliminary testing, we found this to produce similar results, so we opted not to introduce additional complexity."}, {"title": "Ethical Considerations", "content": "The use of predictive models for biological and chemical modeling unfortunately suffers from a dual use problem (Urbina et al., 2022), where the technology can be greatly beneficial but also detrimental. We note that our benchmark is created from open datasets where predictive models already exist, so our work introduces no new harm. Further, we believe our benchmark to be useful here: the usage of natural language captions for improving predictive performance increases explainability by allowing domain scientists to read which relevant information improves molecule property prediction. This can potentially mitigate harms in employing this technology by allowing a clearer view into the molecular discovery process."}]}