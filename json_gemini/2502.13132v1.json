{"title": "LEARNING TO DEFER FOR CAUSAL DISCOVERY WITH IMPERFECT EXPERTS", "authors": ["Oscar Clivio", "Divyat Mahajan", "Perouz Taslakian", "Sara Magliacane", "Ioannis Mitliagkas", "Valentina Zantedeschi", "Alexandre Drouin"], "abstract": "Integrating expert knowledge, e.g. from large language models, into causal\ndiscovery algorithms can be challenging when the knowledge is not guaranteed\nto be correct. Expert recommendations may contradict data-driven results, and\ntheir reliability can vary significantly depending on the domain or specific query.\nExisting methods based on soft constraints or inconsistencies in predicted causal\nrelationships fail to account for these variations in expertise. To remedy this, we\npropose L2D-CD, a method for gauging the correctness of expert recommenda-\ntions and optimally combining them with data-driven causal discovery results.\nBy adapting learning-to-defer (L2D) algorithms for pairwise causal discovery\n(CD), we learn a deferral function that selects whether to rely on classical causal\ndiscovery methods using numerical data or expert recommendations based on\ntextual meta-data. We evaluate L2D-CD on the canonical T\u00fcbingen pairs dataset\nand demonstrate its superior performance compared to both the causal discovery\nmethod and the expert used in isolation. Moreover, our approach identifies domains\nwhere the expert's performance is strong or weak. Finally, we outline a strategy\nfor generalizing this approach to causal discovery on graphs with more than two\nvariables, paving the way for further research in this area.", "sections": [{"title": "1 INTRODUCTION", "content": "Causal discovery is a fundamental task in artificial intelligence and data science, where the goal\nis to identify the unknown causal relationships between a set of random variables from their\nobservations (Spirtes et al., 2001; Pearl, 2009; Peters et al., 2017). This typically involves inferring\nstatistical dependencies in the data (Spirtes et al., 2001), leading to identification of the causal\ngraph up to a Markov Equivalence Class (MEC), whose members all imply the same statistical\ndependencies (Verma & Pearl, 1990; Spirtes et al., 2001). Hence, we are not guaranteed a unique\nsolution unless we make further distributional or functional assumptions to reduce the set of potential\nsolutions (Shimizu et al., 2006; Peters et al., 2014).\nIn addition to exploiting statistical dependencies, the use of large language models (LLMs) has\nrecently gained significant attention in the causal discovery literature. This growing interest is largely\ndriven by the promising performance of LLMs in various causal discovery studies (Willig et al.,\n2022; Jin et al., 2024b). Typically, approaches query an LLM about the causal relationships between\ntwo or more variables, by prompting them with the variables' textual names, and optionally with"}, {"title": "2 METHODOLOGY", "content": "We consider the task of determining causal relationships between two causal variables, identified\nby their names (u, v). We denote the corresponding observational data with N samples for these\nvariables as (xu, xv) where Xu, Xv \u2208 RN. Further, we assume access to metadata C some textual\ncontext/description that provides extra information about the relationship between these variables.\nWe represent the combined numerical data and textual description associated with the causal variables\nas x = (C, u, v, Xu, xv).\nLet y = luv, where II is the indicator function, denote a binary label for the causal relationship, i.e.\ny = 1 if u causes v and y = 0 otherwise. We assume access to the following predictors for causal\nrelationships:\n\u2022 Expert Predictor. Query an expert, typically an LLM, to obtain the causal relationship, where\nthe expert uses only the textual description for prediction, i.e., h\u2081(x) = Expert(C, u, v).\n\u2022 Causal Discovery Methods. Train a causal discovery method using a numerical/observa-\ntional dataset to predict causal relationships, i.e., ho(x) = CD(xu, xv).\nOur goal is to construct a predictor h* (x) of y that optimally combines the predictions from the expert\nh\u2081(x) and the causal discovery method ho(x)."}, {"title": "2.1 BACKGROUND ON LEARNING TO DEFER", "content": "To combine multiple predictors, we use techniques from the literature on learning to defer\n(L2D) (Madras et al., 2018; Mozannar & Sontag, 2021; Mao et al., 2024). Consider the task of binary\nprediction for input x and labels y, with values in X and Y respectively, where Y is finite. Given\nthe base predictor h(x) = ho(x) and ne expert predictors {hj(x)}=1, the goal is to learn a deferral\nfunction r(x) = arg maxj\u2208[0,ne] rj (x) that chooses between the different predictors for the sample\nx; e.g. r(x) = 0 means that r chooses the base predictor h and r(x) = j means that r chooses\nthe j-th expert predictor hj. Critically, only one of the base predictors and any expert predictor\nis chosen for a given instance x. As a result, the combined predictor h*(x) can be constructed as\nh*(x) = \u2211jne Ir(x)=jhj(x).\nIn order to learn the deferral function, we use the loss objective from Mao et al. (2024):\nLdef(h, r, x, y) = h(x)\u2260yIr(x)=0+ \u2211nej=1 Cj (x, y)Ir(x)=j,\nwhere cj (x, y) \u2208 [0, 1] denotes the cost of predicting y from instance x associated with expert pre-\ndictor h(x), while the cost of the base predictor h is the standard 0-1 loss. Essentially, Ldef is the loss\nincurred by the only base or expert predictor that r chooses on instance x; indeed, only one of Ir(x)=j\nfor j = 0, 1, ..., ne is one and all others are zero. This leads to the following optimization problem,"}, {"title": "2.2 CONSISTENCY", "content": "L2D typically uses a surrogate loss; however does minimizing it actually minimize the original loss?\nTo assess this, we employ the concept of H-consistency bounds (Awasthi et al., 2022; Mao et al.,\n2024). Let H be a hypothesis class, l be a non-negative function over (h, x, y) where h \u2208 His the\npredictor, x the features and y the label. Let Ee(h) := Ex,y [l(h, x, y)] and Et (H) := minhen Ee(h).\nFurther, denote Me(H) := E\u207a(H) \u2013 Ex [infhen Ey|x[l(h, x, y)]] the minimizability gap of H and\nl, which is non-negative and vanishes when Et (H) coincides with the Bayes error of l. Then, an H-\nconsistency bound of a surrogate loss ls with respect to an original function lo is a bound of the form\nTh\u2208 H, Ee (h) \u2013 Et (H) + Me\uff61(H) \u2264 \u0393 (\u0415\u0435 (h) \u2013 Et (H) + Me (H)),\nwhere I is a non-decreasing concave function such that F(0) = 0. Such a bound is desirable as\nit implies Bayes-consistency of ls w.r.t. lo, while also quantifying how improvements in Ee(h)\ntranslate to improvements in Ee(h) for h \u2208 H. From now on, we use \u201cconsistency bound\u201d to refer\nto an H-consistency bound without specifying the hypothesis class H.\nTheorem 6 of Mao et al. (2024) shows that when the surrogate loss used to learn the predictor h and\nthe surrogate loss for the deferral function r have consistency bounds with respect to the multi-class\n0-1 loss, then the L2D surrogate loss Lsurr has a generalized form of consistency bound with respect\nto the original L2D loss Ldef in Equation 1. However, this result relies on the assumption that\nCj \u2264 Cj (x, y) \u2264 \u0109j, \u2200j,x,y\nfor some c > 0 and \u0113j < 1, which does not apply when cj(x,y) = Ih;(x)\u2260y for which\nVj, cj(x, y) = 0 for some x, y. However, as we show next, the L2D loss does have such a consistency\nbound in this case, if one further assumes a binary y. We provide the proof in Appendix A."}, {"title": "2.3 POST-HOC L2D WITH A 0-1 COST FUNCTION AND A SINGLE EXPERT CAN BE REDUCED\nTO STANDARD CLASSIFICATION", "content": "We now show that post-hoc L2D applied to our setting can be reduced to binary classification. Indeed,\nassume cj (x, y) = [h\u2081(x)\u2260y and let\nDo = {(x, y) | Th(x)\u2260y = Ihz(x)\u2260y \u2200j = 1,...,ne}\nwhich is the set of pairs of features and labels on which all predictors are equally correct or wrong.\nThen it turns out that for any (x, y) \u2208 D\uff61, Ldef(h,r,x,y) = h(x)\u2260y does not depend on r. In\nparticular, noting D\u00ba the complement of Do this leads to\nEx,y [Ldef(h, r, x, y)] = p(D)Ex,y [Ldef(h, r,x,y)|(x,y) \u00a2 Do] + p(Do)Ex,y [Ih(x)\u2260y|(x,y) \u2208 Do]\nThus, we target the alternative loss Ex,y [Ldef(h, r, x, y) (x,y) \u2209 D.] instead of the original loss\nEx,y [Ldef (h, r, x, y)]. Further, when ne = 1, for any (x, y) \u2209 Do, then Ih(x)\u2260y = Ih\u2081(x)=y. Thus,\nLdef(h, r, x, y) = Ih\u2081(x)=yIr(x)=0 + Ih\u2081(x)\u2260yIr(x)=1 = Ir(x)\u2260Ih\u2081(x)=y\nis the 0-1 classification loss for features x and label [h1(x)=y, which is the indicator that h\u2081 (x) returns\nthe correct prediction y. This amounts to learning to predict when the expert is correct or, equiva-\nlently, when the base predictor is wrong. As a result, in this setting, post-hoc L2D reduces to binary\nclassification over samples not in D.. This allows us to use any off-the-shelf classification method\nfor training, while directly optimizing the expected surrogate loss without removing samples in Do\nwould generally only be feasible through automatic differentiation, as done in the implementation of\n(Mao et al., 2024)."}, {"title": "2.4 PROPOSED APPROACH: L2D-CD", "content": "Based on the above discussion, our training procedure to obtain a fitted deferral function \u00ee from a set\nof m examples (xi, Yi)1 when ne = 1 is thus to:\n1. Compute the set S = {i|[h(xi)\u2260yi \u2260 [h1(xi)\u2260y;}. Note that when y is binary, we generally\nhave D. = {(x, y) | x \u2208 X\uff61} where X := {x | h(x) = hj(x) Vj = 1, . . ., ne}, so here\nS can also be computed as S = {i | h(xi) \u2260 h\u2081(xi)}.\n2. Obtain by fitting any binary classification method to the set (xi, Yi)ies where\nYi = Th\u2081(xi)=Yi\nFor pairwise causal discovery, we apply this procedure to x = (C, u, v, xu, xv), y = Iu\u2192v, h\u2081(x) =\nExpert(C, u, v), h(x) = ho(x) = CD(Xu, xv). We call the resulting method L2D-CD. Thus, intu-\nitively L2D-CD learns which causal discovery predictor, statistical-based method or metadata-based\nexpert, returns the correct causal direction when the two predictors differ in their predictions."}, {"title": "3 RESULTS ON T\u00dcBINGEN DATASETS", "content": "We now apply the approach from Section 2.4 to the T\u00fcbingen pairs (Mooij et al., 2016), a canonical\nbenchmark for pairwise causal discovery."}, {"title": "4 EXTENSION TO GRAPHS WITH 3 VARIABLES OR MORE", "content": "To extend L2D-CD to graphs of 3 variables or more, we propose building on methods for ranking\nfrom pairwise comparisons (Braverman & Mossel, 2007; Ailon, 2011; Rajkumar & Agarwal, 2014;\nMao et al., 2017; Falahatgar et al., 2018; Ren et al., 2021), where a ranking over V is a function \u03c0 that\nis bijective from V to {1, . . ., |V|}. We let \u03c0(u) < \u03c0(u') indicate that u is ranked before u' for any\nu, u' \u2208 V. While these methods differ in their exact problem formulation, they all amount to learning\na ranking (u) over items u in a set V from samples (ui, u\u015b, yi), where yi \u2208 {\u22121,1} indicates a\ncomparison between ui and u, with Yi = 1 indicating that ui is deemed as ranked before u and\nYi = -1 as the converse. Thus, the learning algorithm A attempts to find a ranking \u3160 that best fits\nthe individual comparisons; the latters are allowed to be contradictory. Thus we propose generalizing\nour method to causal discovery on more than two variables as follows.\nGraph notations: Let G be a graph having nodes VG, where every node is represented by its name,\nand edges EG. Define \u03a3(VG, EG) as a matrix of pairwise ancestries derived from EG, where (u, u')\nis equal to 1 if u precedes u' in EG, \u22121 if u' prcedes u, and 0 if no ancestry relationship between u and\nu' exists. Additionally, let CG be the graph's textual context, and XG = (Xu)u\u2208VG its numerical data.\nProblem definition: Let u, v be the textual names of nodes, X the full numerical data of a graph,\nand C a textual context. Assume access to an expert Expert(C, u, v) and a causal discovery oracle\nCD(u, v; X), both of which determine the ancestry relationship between u and v. The causal\ndiscovery oracle applies the causal discovery method to X and, like the expert, may return the\nabsence of ancestry.\nTraining: Assume that we have a training set of graphs Gtrain = (VG\u2081, EG\u2081, CG\u2081, XGi)i=1,...,mtrain \u2022\nThen, train a deferral function r(C, u, v, X) between the expert and the causal discovery oracle"}, {"title": "5 CONCLUSION", "content": "We have designed a procedure leveraging learning-to-defer to combine two pairwise causal discovery\nmethods, one conventional method and one expert. Experiments on T\u00fcbingen pairs showed that\nthe combined method generally improves over each separate method, and so for both synthetic and\nreal-world LLM-based experts. The learnt deferral function can also identify the expert's strong and\nweak domains. Note that our methodology can also be applied to human knowledge. An inherent\nlimitation of the L2D-CD approach is the need for a training set, while an improvable one is that\nour training procedure does not generalize straightforwardly to more than two methods. We also did\nnot yet implement our strategy to generalize L2D-CD for bivariate causal discovery to more general\ncausal discovery, which is future work."}]}