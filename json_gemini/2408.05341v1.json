{"title": "CAR: Contrast-Agnostic Deformable Medical Image Registration with Contrast-Invariant Latent Regularization", "authors": ["Yinsong Wang", "Siyi Du", "Shaoming Zheng", "Xinzhe Luo", "Chen Qin"], "abstract": "Multi-contrast image registration is a challenging task due to the complex intensity relationships between different imaging contrasts. Conventional image registration methods are typically based on iterative optimizations for each input image pair, which is time-consuming and sensitive to contrast variations. While learning-based approaches are much faster during the inference stage, due to generalizability issues, they typically can only be applied to the fixed contrasts observed during the training stage. In this work, we propose a novel contrast-agnostic deformable image registration framework that can be generalized to arbitrary contrast images, without observing them during training. Particularly, we propose a random convolution-based contrast augmentation scheme, which simulates arbitrary contrasts of images over a single image contrast while preserving their inherent structural information. To ensure that the network can learn contrast-invariant representations for facilitating contrast-agnostic registration, we further introduce contrast-invariant latent regularization (CLR) that regularizes representation in latent space through a contrast invariance loss. Experiments show that CAR outperforms the baseline approaches regarding registration accuracy and also possesses better generalization ability to unseen imaging contrasts. Code is available at https://github.com/Yinsong0510/CAR.", "sections": [{"title": "1 Introduction", "content": "Multi-contrast images in magnetic resonance imaging (MRI) provide comple- mentary information for characterizing tissues of human bodies, which are widely used in both qualitative and quantitative imaging in clinical diagnosis. However, due to variations of acquisition protocols as well as possible patient movement during the acquisition process, the images are often spatially misaligned, which entails image registration for accurate downstream multi-contrast analysis and interpretation. Registering images between different contrasts can be challenging due to the complex relationship between their intensity profiles. Conventional approaches typically tackle this problem by optimizing information-theoretic similarity measures such as mutual information over the misaligned image pairs [13,14]. Nevertheless, the iterative-based optimization framework is usually time-consuming and therefore can be limited in real-world applications.\nRecently, deep learning-based registration approaches have shown their great potential for fast and accurate medical image registration. Several learning-based methods have also attempted to solve the multi-contrast image registration task. For instance, Qiu et al. [21] have proposed to leverage the conventional mutual information as the similarity metric and embed it into an end-to-end learning- based registration framework. An alternative way proposed by Qin et al. [19] reduced the multi-contrast registration task to a mono-contrast one by disen- tangling images into shape and appearance latent spaces and aligning the image contrasts with style transfer approaches. Dey et al. [7] also proposed to use contrastive representation learning to learn a similarity loss between the warped and fixed multi-contrast images based on multi-scale PatchNCE using an autoen- coder. Nevertheless, due to the network generalizability issues, these approaches can only deal with fixed imaging contrasts, which require the contrast of the image pairs to be the same during both training and inference stages.\nTo address the contrast variations between training and inference, there have also been some recent efforts in achieving contrast agnostic registration. Hoff- mann et al. proposed to synthesize arbitrary contrast of images based on seg- mentation or randomly generated labels [11]. They used these generated images as the input to the registration network and optimized that based on original segmentation labels using soft dice loss [8]. However, this approach relies on the availability of segmentation maps and may not register images well in terms of fine-grained details, as the employed soft dice loss mainly focuses on the struc- ture overlaps rather than pixel-level or volume-level differences. Several recent works proposed training a neural network to learn a certain distance metric for multi-modal registration. Sideri et al. [26] relied on modality and affine geometric augmentation for learning a contrast-agnostic distance measure based on image patch centers to tackle multi-contrast rigid registration. Ronchetti et al. [22] proposed to use a small CNN to learn a feature-level distance that approximates the conventional LC2 similarity. However, both approaches focused on similarity approximation, without explicitly enforcing contrast-agnostic learning.\nIn this work, we propose a contrast-agnostic registration framework (CAR) for deformable image registration that only relies on single contrast images for training but can be generalized to other unseen contrasts. Specifically, we propose a novel random convolution-based contrast-augmentation scheme to simulate ar- bitrary contrasts based on single contrast images. To encourage contrast-agnostic learning, we further enforce the network to learn contrast-invariant features by regularizing the feature representation in latent spaces using a proposed con- trast invariance loss. Unlike existing learning-based multi-contrast registration"}, {"title": "2 Methodology", "content": "Multi-contrast deformable image registration establishes dense and non-linear deformation field $ \\phi $ between a multi-contrast image pair, i.e. a moving image M and a fixed image F. Learning-based approaches estimate the deformation field through a neural network $ f_{\\theta} $, $ \\phi = f_{\\theta}(M, F) $, with loss functions typically consisting of a dissimilarity term $ L_{sim} $ and a regularization term $ L_{reg} $, i.e.,\n$ L_{total} = L_{sim}(M \\circ \\phi, F) + \\lambda L_{reg}(\\phi), $\n(1)\nwhere $ \\lambda $ is a regularization hyperparameter. Conventional learning-based meth- ods typically forecast the desired spatial transformation directly from image appearances, which not only renders the learned model contrast-dependent but disregards the fact that registration aims to find the intrinsic structural cor- respondence usually invariant to image contrasts. To achieve contrast-agnostic registration that can deal with even unobserved contrasts as discussed in Section 1, in our method, we propose a random convolution-based contrast-augmentation scheme for contrast simulation (Section 2.1) and a contrast invariance loss for contrast-invariant representation learning (Section 2.2). Our overall framework is shown in Fig. 1."}, {"title": "2.1 Contrast Augmentation via Random Convolution", "content": "To enable a learning-based registration framework that can be contrast-agnostic, we first propose to simulate the variations of image contrasts by leveraging ran- dom convolution (RC) [33] for contrast augmentation. RC has been shown ef- fective for learning robust representations for domain generalization in both the general computer vision field and medical imaging domain for tackling clas- sification and segmentation tasks [18,26]. Specifically, RC leverages randomly initialized convolution kernels to convolve with input images to adjust their ap- pearances while preserving inherent shape information. In earlier works [18,33], it is shown that RC with large kernel size would introduce blurring effect, which can adversely impact the registration problem as it heavily relies on fine-grained details of images for learning pixel-level correspondences. To avoid this, we pro- pose to set the kernel size to 1\u00d71 to minimize the potential effects of the induced artifacts on structural details. This allows the contrast of similar tissues to vary homogeneously in the process, and this synchronizes with the rendering of multi-contrast images determined by imaging physics. Additionally, to ensure that the augmented images can maximally capture the variations of image contrasts, we further propose to stack multiple RC layers with LeakyReLu activation as pro- posed in [33] to model more complex and diverse contrast mappings, as well as simulating the non-linear intensity relationships as among different MRI con- trasts. In each RC layer, the parameters of the convolution kernel are sampled independently from certain uniform distributions to enable the mapping to be diverse. The generated augmented images are then used as inputs to train our contrast-agnostic registration network. Illustrations of this contrast augmenta- tion scheme and augmented images can be found in supplementary materials."}, {"title": "2.2 Contrast-Invariant Latent Regularization", "content": "To equip the network with the ability to learn contrast-invariant feature repre- sentations and stabilize the training process, we propose to regularize the fea- ture representation in latent space using our proposed contrast invariance loss. Specifically, we assume that the latent representation of images that share the same shape information but with different contrasts should be close in the la- tent space. Therefore, we aim to pull the feature representations of the images with the same shape information to be close in latent space. Inspired by the contrastive learning-based methods [6,31], we also leverage a projection head to map the representation to a low dimensional space before computing the loss. Instead of using contrastive-based approaches, we propose to use a mean squared difference loss to calculate the difference of projected representations, which can provide a more direct constraint than the InfoNCE loss [6]. In detail, for each optimization step, we perform the network forward passes with two contrast- augmented moving-fixed image pairs {Mc\u2081, FC1} and {MC2, FC2}. Through the encoder and the projection head, the latent representations of the two image pairs can then be obtained, i.e., $ f_{M_1}^{proj}, f_{F_1}^{proj}, f_{M_2}^{proj}, f_{F_2}^{proj} $ with dimension (C, Nx, Ny), where C is the channel dimension and (Nx, Ny) is the spatial dimension of the representation. Our contrast invariance loss function can be defined by\n$ L_{contrast} = \\frac{1}{N_x N_y} \\sum_{i=1}^{N_x} \\sum_{j=1}^{N_y} [(\\text{Proj}(f_{M_1}(i, j)) - \\text{Proj}(f_{M_2}(i, j)))^2 + (\\text{Proj}(f_{F_1}(i, j)) - \\text{Proj}(f_{F_2}(i, j)))^2], $\n(2)\nwhere Proj represents the projection head. Therefore, the contrast invariance loss essentially enforces the encoder to capture the intrinsic structural features that are invariant to various image contrasts or appearances [1]."}, {"title": "2.3 Overall Network Architecture and Loss Function", "content": "The overall registration framework is illustrated in Fig. 1. The registration net- work is a U-shape network [3] except we duplicate the encoders for moving images and fixed images respectively. Skip connections [9] are adopted between the two Siamese encoders and the decoder. The moving image and the fixed im- age are taken as the input of their respective encoder and transformed to their corresponding latent representations f\u2122 and fF, with 1/16 of the original image size. The two latent representations are concatenated and fed into the decoder to output the final deformation field.\nTo train the registration network, our loss function is comprised of three terms: a dissimilarity loss Lsim, a regularization loss Lreg, and our proposed con- trast invariance loss Lcontrast. Since the input contrast-augmented image pair {MC, FC} originated from the mono-contrast image pair {M, F}, we propose to use the pre-augmented mono-contrast image pairs for loss supervision with a mono-contrast dissimilarity measure. This therefore alleviates the need to fit any multi-contrast similarity loss through a neural network. In our work, we adopt the negative local normalized cross-correlation (LNCC) [2] as the dissim- ilarity, and a regularization loss formulated in the form of the L2-norm of the deformation field gradient. The overall loss function can be formulated as\n$ L_{total} = - LNCC(M \\circ \\phi, F) + \\lambda_1 ||\\nabla \\phi||_2 + \\lambda_2 L_{contrast}, $\n(3)\nwhere $ \\lambda_1 $ and $ \\lambda_2 $ are the respective hyperparameters that control the strength of the regularization loss and contrast invariance loss. In this way, we will be able to achieve contrast-agnostic registration by jointly optimizing the deformation field"}, {"title": "3 Experiments and Results", "content": "Datasets. We evaluated CAR on two applications: (1) inter-subject 2D brain MRI registration of T1 weighted (T1w) - T2 weighted (T2w) images, and (2) intra-subject registration of multiple contrasts of T1 mapping acquisition in cardiac MRI. The 2D experiments on brain MRI registration mainly served as a proof-of-concept study to validate the method's feasibility, and the cardiac datasets only contain 2D stacks of data for 2D experiments. For the brain regis- tration task, we use Tlw and T2w images of 652 subjects from the Cambridge Centre for Ageing and Neuroscience (CamCAN) project [24,27]. We sampled the middle slice along the z-axis and cropped the sampled image into 160 \u00d7 192. The dataset was randomly split into 600, 10, and 42 volumes for training, validation, and testing, where subjects were randomly selected to form T1w-T2w image pairs. For the cardiac registration task, we used the public CMRxRecon cardiac datasets [29,30]. The CMRxRecon dataset consists of two parts: (1) Cine MRI, and (2) T1 mapping with images of nine different T1 weightings. Both Cine MRI and T1 Mapping data in the dataset contain 167 subjects. Each subject includes 7 to 12 or 4 to 5 short-axis view slices for Cine MRI data and T1 Mapping data respectively. We cropped images in Cine and T1 Mapping data into 128 \u00d7 128. The dataset was randomly split into 137, 10, and 20 for training, validation, and testing. For the cardiac data, we simulated different random deformations to dis- tort each image in the sequence using artificially generated FFDs with various mesh spacings following the approach from [12]. For sequences within the same slice, we randomly selected an original image and a spatially distorted image as an image pair.\nEvaluation metrics. We evaluated the registration accuracy and the deforma- tion regularity of the registration. The registration accuracy was measured with Dice score between the segmentation of the warped moving image and the fixed image. To quantify the registration accuracy of the boundary of the structure of interest, we also measure the 95% of the Hausdorff distance (HD95). We also evaluated the extent of extreme deformations, i.e., folding, by computing the ra- tio of points with negative Jacobian determinant (J<0%), and the smoothness of the deformation by computing the magnitude of the spatial gradient of Jacobian determinant $ ||\\nabla J|| $ [21].\nImplementation details. The backbone network has two Siamese encoders and a decoder. The channel dimension of each convolutional layer in the two encoders is 128, and 256 for the decoder. The projection head for contrast-invariant latent regularization consists of a 1 \u00d7 1 convolution layer with the output channel set to 32. The random convolution-based contrast augmentation consists of 4 RC layers, where the kernel weights of each layer in section 2.1 are sampled from a uniform distribution U(0,10). The kernel weights are reshifted to be zero- centered after sampling. A LeakyReLu activation with a negative slope of 0.2 is added after each RC layer. Experiments were conducted using Adam Optimizer on an NVIDIA A5000 GPU with a batch size of 16. The initial learning rate was set to be 1 \u00d7 10-4 and started to decay to 1 \u00d7 10-5 from the 5th epoch for convergence.\nComparison study. CAR was first compared with a traditional iterative regis- tration method SyN [2] with mutual information and a default Gaussian smooth- ing of 3 and three scales with 180, 80, 40 iterations, respectively following the setting of TransMorph [5]. We then compared CAR with SOTA learning-based methods, including VoxelMorph [3] using LNCC (VXM-LNCC) and MIND [10] (VXM-MIND) as the dissimilarity metrics respectively, a multi-contrast mutual information-based registration approach MIDIR [21], and a contrast-agnostic registration framework SynthMorph [11]. Note that the channel dimensions of all convolutional layers in VXM-LNCC, VXM-MIND, and SM-shape are set to 256 for a fair comparison with CAR. VoxelMorph and MIDIR were trained using all the available contrasts, i.e., both T1w and T2w images in brain MRI and all nine T1 weighting images in cardiac MRI. Synthetic images used for training SynthMorph were generated based on random label maps (SM-Shape). In con- trast, CAR used only single contrast images for training, i.e., T1w images in brain MRI and TI\u2081 images in cardiac MRI.\nA quantitative comparison result on both brain MRI and cardiac MRI is summarized in Table 1. For the cardiac registration task, the result is reported for registering misaligned images of TIi-2:9 to TI1. It can be observed that CAR achieves the best registration accuracy while possessing good deformation reg- ularity for both tasks. For the brain MRI registration task, CAR demonstrates superior generalization ability as CAR only used T1w images for training but performed the best on the T1w-T2w registration task for all learning-based ap- proaches in terms of registration accuracy and deformation regularity. This is similar to the cardiac registration task when our model was trained using only TI\u2081 image but can be generalized to all other contrasts of images in T1 Mapping data. It can be noted that CAR has the lowest folding ratio. This is because we are dealing with nine different weightings of T1 images and the learned contrast- invariant representations may help the network get a more stable solution from images with different contrasts. Fig 2 and Fig 3 also show the qualitative compar- ison results of brain and cardiac registration tasks respectively, demonstrating that CAR can achieve low registration error and smooth deformation field. Both figures also show that CAR can register more fine-grained details than baseline approaches, which are shown in the highlighted regions.\nTo further evaluate the generalizability of CAR, we also trained our model and baseline methods on CMRxRecon Cine MRI data and evaluated model performance on the multi-contrast T1 Mapping data (also registering misaligned images of TIi=2:9 to TI1). The results are shown in Table 2. It can be shown all other methods except CAR and SM-shape failed to register the image with contrast they had not seen during training. In contrast, only CAR and SM- shape can still achieve good registration results. However, CAR has a significant increase in dice score and better deformation regularity than SM-shape, which demonstrates CAR's superior generalizability.\nAblation studies. To verify the effectiveness of CAR, we conducted ablation studies on the proposed random convolution-based contrast augmentation and CLR on the CamCAM datasets. We tested the effect of the random convolution kernel sizes on the model performance, where RC-3 represents that the kernel size is 3. We also replaced our proposed contrast invariance loss with InfoNCE loss [6] in contrastive learning to study its effectiveness. The results are summa- rized in Table 3. We can observe that any missing components of our model can lead to an inferior model performance from the results. Without CLR, the reg- istration accuracy gets lower with a higher folding rate. Similar results can also be observed when replacing the proposed contrast invariance loss with InfoNCE loss in [6]. This means that CLR not only facilitates contrast-invariant repre- sentation learning but also improves the deformation regularity by producing a more plausible deformation field. Besides, a larger kernel size in the random convolution layer also leads to a performance drop. This is because a larger ker- nel size would lead to a blurring effect for the images, which can damage the voxel-to-voxel correspondences and therefore harm the registration task."}, {"title": "4 Conclusion", "content": "We present a novel contrast-agnostic deformable image registration framework (CAR) that can register arbitrary contrasts of images without observing them during training. To achieve this, we propose a random convolution-based ap- proach to simulate variations of contrasts and develop a contrast invariance loss to regularize the feature representation in latent space for contrast-invariant feature representation learning. Experiments showed that on both tasks, CAR outperformed the state-of-the-art methods and demonstrated superior generaliz- ability although CAR is contrast-agnostic. For future work, we will extend CAR to 3D applications as well as to more general multi-modal registration tasks."}]}