{"title": "ACEPARSE: A COMPREHENSIVE DATASET WITH DIVERSE STRUCTURED TEXTS FOR ACADEMIC LITERATURE PARSING", "authors": ["Huawei Ji", "Cheng Deng", "Bo Xue", "Zhouyang Jin", "Jiaxin Ding", "Xiaoying Gan", "Luoyi Fu", "Xinbing Wang", "Chenghu Zhou"], "abstract": "With the development of data-centric AI, the focus has shifted from model-driven approaches to improving data quality. Academic literature, as one of the crucial types, is predominantly stored in PDF formats and needs to be parsed into texts before further processing. However, parsing diverse structured texts in academic literature remains challenging due to the lack of datasets that cover various text structures. In this paper, we introduce AceParse, the first comprehensive dataset designed to support the parsing of a wide range of structured texts, including formulas, tables, lists, algorithms, and sentences with embedded mathematical expressions. Based on AceParse, we fine-tuned a multimodal model, named AceParser, which accurately parses various structured texts within academic literature. This model outperforms the previous state-of-the-art by 4.1% in terms of F1 score and by 5% in Jaccard Similarity, demonstrating the potential of multimodal models in academic literature parsing. Our dataset is available at https://github.com/JHW5981/AceParse.", "sections": [{"title": "1. INTRODUCTION", "content": "The increasing focus on data-centric AI has shifted the emphasis from model development to the critical role of data quality in advancing AI technologies [1]. High-quality data are essential for training advanced models, particularly in domains requiring a deep understanding of complex information. Academic literature is a valuable data source due to its rich scientific content [2], but it is often stored in non-machine-readable formats like PDFs, necessitating effective parsing techniques to extract their information.\nAcademic literature typically features a blend of structured content, including tables, formulas, lists, and algorithms, all of which work together to convey scientific insight. Parsing literature presents several key challenges. First, OCR-based methods [3, 4, 5] often focus on character recognition, leading to loss of structural information. Second, while modular approaches [6, 7, 8] can handle predefined content types like tables and formulas, they struggle with complex structures like algorithms and lists. Existing end-to-end parsing models, such as Nougat [9], are often trained on narrow proprietary datasets with a limited diversity of structured content, restricting their ability to generalize effectively across diverse structures. Finally, existing open-source datasets remain limited to character-level parsing [10, 11] or focus on specific content types like tables or formulas [12, 13], which does not cover the full diversity of structured elements present in academic documents.\nTo advance the unified parsing of academic literature containing various types of structured texts, we propose AceParse. Unlike previous datasets [10, 11] that either provide only character-level parsing results or focus on a single type of structured text [12, 13], AceParse encompasses a broad range of structured texts, including formulas, tables, lists, algorithms and sentences with embedded mathematical expressions. These texts are annotated using the LaTeX markup language to accurately describe their structure. As far as we know, AceParse is the first open-source dataset specifically designed for handling diverse structured content in academic literature parsing. \nThe main contributions of this paper are as follows:\n\u2022 We introduced AceParse, the first comprehensive, open-source dataset designed for parsing diverse structured texts in academic literature.\n\u2022 We fine-tuned a multimodal model to develop AceParser, an end-to-end structured text parsing method capable of generating structured text in markup languages.\n\u2022 We systematically compare the performance of current parsing methods and provide an extensive overview of existing parsing datasets, aiming to serve as a reference for the document parsing community."}, {"title": "2. METHODOLOGY", "content": ""}, {"title": "2.1. Dataset Construction", "content": "Existing academic parsing datasets [9] rely on a PDF page matching data construction mechanism. Still, the inherent limitations in the page matching model often lead to the loss of structured text at the end of pages. Moreover, this approach requires the collection of a large volume of academic literature, creating substantial challenges for scalability. To address these issues, we use a data synthesis approach. By randomly combining structured texts extracted from source code to generate new LaTeX code and then compiling it, we obtain high-quality image-annotation pairs. Additionally, the randomness in sampling allows us to develop a large volume of data from a small amount of source code. The dataset is built upon three key dimensions, as illustrated in Fig. 1(a):\nDocument Collection: Using ArXiv IDs listed in Papers with Code following [15], we collected 10,000 open-access LaTeX source files from 102 subfields within computer science on ArXiv. During this process, we developed custom parsing scripts to address the structural and formatting inconsistencies of LaTeX files across various subfields. These scripts were specifically designed to normalize differing LaTeX conventions and ensure consistent content extraction.\nData Synthesis: We applied a combination of rule-based techniques, leveraging our domain-specific knowledge of academic writing conventions and LaTeX syntax to clean the source code. To avoid issues such as inconsistent citation formats, overly complex or redundant user-defined commands, and non-standard sectioning in the LaTeX files, we developed custom parsing rules to filter out irrelevant content, normalize references and citations, and standardize or replace non-standard commands. Following this cleaning step, we extracted diverse structured texts, specifically focusing on sentences with embedded structures, formulas, tables, lists, and algorithms while excluding plain text sentences. This approach allowed us to collect over 700,000 structured items, as shown in Figure 2(a).\nWhen randomly sampling and combining these items to synthesize new LaTeX files, one of the main challenges was ensuring that the files would compile successfully despite the randomness of the content. Since the combination of different structures, such as formulas, tables, and custom com-"}, {"title": "2.2. AceParser Network Architecture", "content": "The proposed AceParser Network is fine-tuned based on the architecture of Florence-2 [14], as illustrated in Fig.1(b). Florence-2 is a robust multi-task multimodal model pre-trained on 5 billion data instances and equipped with OCR capabilities, but it lacks the ability to parse structured texts. The model comprises a vision encoder, DaViT[16], and a multimodal encoder-decoder based on BART [17]. A document image, denoted as $I \\in R^{H\\times W\\times3}$, is divided into patches and embedded into visual token embeddings $V \\in R^{N_v\\times d}$ by the vision encoder, where $N_v$ represents the number of visual tokens, and $d$ denotes the dimension of the hidden layers. The task prompt is also embedded into text token embeddings $T \\in R^{N_t\\times d}$ by the text embedding layer, where $N_t$ represents the number of text tokens. By concatenating these visual and text token embeddings and applying positional encoding, we obtain the multimodal token embeddings $X \\in R^{(N_v+N_t)\\times d}$, which serve as input to the multimodal encoder. Using teacher forcing and an autoregressive loss during training, we can fine-tune AceParser by providing the annotations as input to the decoder:\n$L = - \\sum_{t=1}^{T} log P(y_t | y_{1:t-1}, X)$\nwhere $y_t$ is the actual token at time step t, $y_{1:t-1}$ are the previous tokens generated by the decoder, and x represents the input sequence (e.g., the annotations)."}, {"title": "3. EXPERIMENTS AND DISCUSSIONS", "content": ""}, {"title": "3.1. Experimental Setup", "content": "The AceParse dataset is divided into training, validation, and test sets with a ratio of 8:1:1. All comparison results are reported based on the test set. The AceParser model is initialized with pre-trained weights from Florence-2 [14] and trained with the AdamW optimizer, using a learning rate of 1\u00d710-5 and a linear learning rate schedule, which includes a 10% warm-up phase. Training is performed on four NVIDIA GeForce RTX 3090 GPUs with a batch size of 8."}, {"title": "3.2. Comparison with Different Parsing Methods", "content": "As shown in Table 3, we compared various academic literature parsing methods on the AceParse dataset using evaluation metrics including Levenshtein Distance [18], BLEU [19], F1-score, and Jaccard Similarity [20]. Methods that are not structure-aware, such as Tesseract [3] and PPOCR [4], generally showed lower performance, due to their limited ability to handle markup languages. Among structure-aware methods, modular approaches like Pix2Text [6] and Mineru [7] were also less competitive, attributed to error accumulation across modules. Our end-to-end method achieved the best parsing results, with a 4.1-point improvement in the F1 score and a 5-point improvement in Jaccard similarity. However, a current limitation of our approach is its relatively slow parsing speed of 5.92 seconds per sample, which will be a focus for future optimization."}, {"title": "3.3. Case Study", "content": "We provide a case study where AceParser parses an academic document image containing complex formulas, as shown in Figure 3. The two images on the left show the original image and the feature map extracted from the image encoder. It can be observed that the image encoder captures not only the areas with plain text but also focuses on the special symbols and structures within the formulas. The images on the right illustrate the cross-modality attention matrices before and after training. These matrices capture the relationships between the input tokens (both visual and textual) and the corresponding parsed output tokens, highlighting how the model aligns visual and textual information to achieve accurate parsing results. We observe a substantial rise in attention scores within the formula region, suggesting that the model, which initially had difficulty parsing structured text, significantly improved its ability to do so after being trained on AceParse."}, {"title": "4. CONCLUSIONS", "content": "In this paper, we introduced AceParse, the first comprehensive, open-source academic literature parsing dataset containing multiple types of structured texts, addressing the issue of the lack of diverse structured content in previous datasets. The dataset includes 500k parsed document pairs, annotated in LaTeX, designed to teach models how to represent diverse structured texts using markup language. Based on this dataset, we trained an end-to-end model, AceParser, which achieved state-of-the-art parsing performance. Our work establishes a foundation for academic literature parsing and the development of end-to-end parsing models. In future work, we plan to enhance dataset quality, increase document length, and explore smaller models to improve parsing speed."}]}