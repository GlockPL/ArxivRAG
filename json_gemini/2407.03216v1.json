{"title": "Learning Disentangled Representation in\nObject-Centric Models for Visual Dynamics Prediction\nvia Transformers", "authors": ["Sanket Gandhi", "Atul", "Samanyu Mahajan", "Vishal Sharma", "Rushil Gupta", "Arnab Kumar Mondal", "Parag Singla"], "abstract": "Recent work has shown that object-centric representations can greatly help im-\nprove the accuracy of learning dynamics while also bringing interpretability. In\nthis work, we take this idea one step further, ask the following question: \"can learn-\ning disentangled representation further improve the accuracy of visual dynamics\nprediction in object-centric models?\" While there has been some attempt to learn\nsuch disentangled representations for the case of static images [26], to the best of\nour knowledge, ours is the first work which tries to do this in a general setting for\nvideo, without making any specific assumptions about the kind of attributes that\nan object might have. The key building block of our architecture is the notion of a\nblock, where several blocks together constitute an object. Each block is represented\nas a linear combination of a given number of learnable concept vectors, which\nis iteratively refined during the learning process. The blocks in our model are\ndiscovered in an unsupervised manner, by attending over object masks, in a style\nsimilar to discovery of slots [21], for learning a dense object-centric representation.\nWe employ self-attention via transformers over the discovered blocks to predict\nthe next state resulting in discovery of visual dynamics. We perform a series of\nexperiments on several benchmark 2-D, and 3-D datasets demonstrating that our\narchitecture (1) can discover semantically meaningful blocks (2) help improve\naccuracy of dynamics prediction compared to SOTA object-centric models (3) per-\nform significantly better in OOD setting where the specific attribute combinations\nare not seen earlier during training. Our experiments highlight the importance\ndiscovery of disentangled representation for visual dynamics prediction.", "sections": [{"title": "1 Introduction", "content": "Predicting visual dynamics is an important task for a wide variety of applications including those in\nComputer Vision and Model-based RL. Recent work has seen a surge of approaches which exploit\nthe power of transformers for predicting future dynamics. It has also been observed that making the\nmodel object-aware can help improve the quality of predictions, as well as bring as interpretability.\nAccordingly, a number of models such as [19, 37] have been proposed which explicitly discover\nobject representations, and make use of a GNN [19] or a transformer [37] to predict future dynamics.\nComing from the other side, there has been some recent work on unsupervised discovery of object\nattributes in static images [26]. To the best of our knowledge, there is no existing work which does\nthis for video, in a general setting, without making an explicit assumption about the kind of attributes\nthat an object might have.\nMotivated by this research gap, we ask the following question: Is it possible to learn disentangled\nrepresentation in an unsupervised manner in object-centric models, and does such a representation\nhelp improve the results, for the task of video dynamics prediction. Presumably, such a representation,\nif learned well, could also help better generalization in OOD settings, i.e., when certain attribute\ncombinations, either appearing in a single object or multiple objects, have not been seen during the\ntraining. In response, we propose a novel architecture, which factorizes the object representation\nusing the notion of a block, where each block can be thought of representing a latent attribute. Blocks\nin our model are represented via a fixed \u00b9 number of learnable concept vectors. An easy way to\nthink about concepts is to refer to the idea of a set of basis vectors, such as RGB for the latent\ncolor attribute. We note that these are discovered automatically in our model, without any explicit\nsupervision. Once the block representation has been discovered, the objects, factorized in terms of\nblocks, are passed via a transformer based model, to predict the future state, again represented in\nterms of blocks at the next time step.\nIn terms of overall architecture pipeline, our algorithm proceeds as follows. We first use make use of\nSAM [15], initialized with with points obtained from another object-centric model, SAVi [13], which\nrepresents objects in the form of slots, to discover object masks. A simple CNN encoder is then used\nto get the object representation from object masks. Once the object latents have been discovered,\neach of them is attended by a set of blocks, which are iteratively refined, to learn a distentangled\nrepresentation, as a linear combination of learnable concept vectors. These blocks are then coupled via\na self-attention module, to learn an object aware representation, which is then passed to transformer\nto predict future dynamics. Our decoder is an extension of spatial-broardcast-decoder [34], extending\nto the factored block representation, where each block is used to decode a specific dimension of the\nresultant feature map. Our loss consists of the following components: (a) image reconstruction loss\n(b) image prediction loss (c) latent block prediction loss (d) object mask prediction loss. While the\nfirst loss corresponds to reconstruction at a given time step, the remaining correspond to dynamics\nprediction at the next time step. Our model is trained jointly, for the discovery of disentangled object\nrepresentation, as well as the parameters of the dynamics prediction module. We refer to our approach\nas DisFormer.\nWe experiment on a number of 2-D and 3-D benchmark datasets to analyze whether our approach\n(a) can learn semantically meaningful object representations in terms constituent blocks (b) improve\nthe prediction accuracy of visual dynamics (c) better generalize to OOD settings, where certain\nattribute combinations have never been before. Our results demonstrate the strength of our model,\nto disentangle object attributes such as color, size, and position, and significantly help improve the\naccuracy of visual dynamics prediction, especially in the OOD setting. We also present a series of\nablations to give further insights into the working of various components of our model.\nOur contributions can be summarized as follows (a) We present the first approach for learning\ndisentangled representation in object-centric models for the task of visual dynamics prediction,\nwithout making any explicit assumption about the kind of attributes that an object might have (b) our\narchitecture makes use of a novel object mask detector via SAM initialized with another slot-attention\nbased visual dynamics prediction module (c) we jointly learn the disentangled object representation\nwith dymamics prediction via transformers (d) extensive experiments demonstrate the efficacy of our\napproach in discovering semantically meaningful object attributes, as well as significantly improve"}, {"title": "2 Related Work", "content": "We provide brief over view of related work. Broadly the related work can be split as object-centric\nmodels for images, videos, dynamics models and attribute level disentanglement models.\nObject Centric Image and Video models Unsupervised object centric learning from images and\nvideos have been well studied area. Starting from spatial attention modes1 AIR [7], MONet [2],\nSPACE [20], these models include recent spatial mixture models IODINE [9], GENESIS [5], [21]\nGENESIS-V2 [6], SLATE [23], and are trained for image reconstruction objectives.\nDynamics Prediction Models: Recently, [1, 3, 8, 35] focus on generating predictions of future\nvideo sequences, specifically emphasizing objects within the scene. In this context, \u2018object-centric'\nmeans that the models and algorithms used for video prediction are designed to understand and\ntrack individual objects or entities within the video frames. Object-centric video prediction aims to\nanticipate how these objects will move, change, or interact in future video frames. However, during\ntraining, these approaches require supervision in terms of ground truth properties of the objects.\nMore recent works [17, 22, 41, 43] overcome this limitation of direct supervision by employing a\npre-trained detector as object feature extractor and model the dynamics using graph neural network\n[10, 14, 32, 40]. On the other hand, works such as [16, 18, 28, 38] develop a completely unsupervised\ntechnique. [12, 13, 25] decompose videos into temporally aligned slots.\nObject Disentangelment Models: [11] proposes a framework to learn scalable object-oriented\nrepresentation of videos by decomposing foreground and background. [18] disentangles various\nobject features such as positions, depth, and semantic attributes separately. [4, 28, 31, 36, 38, 45]\nleverage the prowess of transformer [30] and GNN [14, 32, 40] for object aware video representation\nlearning. However, most of the methods don't consider or partially consider the object attributes\nwhile modeling the object dynamics. We show that modeling object dynamics with object-tied\nattributes improves model predictions and enhances generalization capabilities in the context of\nout-of-distribution (OOD) scenarios."}, {"title": "3 DISFORMER", "content": "We describe the architecture of DisFormer in this section. There are four important parts to the\narchitecture: (1) Mask Extractor: This module extracts the object masks in an unsupervised manner\nusing pre-trained architectures. (2) Block Extractor: This a novel module that disentangles each\nobject in terms of an underlying block representation via iterative refinement (3) Dynamics Predictor:\nThis module predicts the next state of object latents, in terms of their block representation via a\ntransformer-based architecture (4) Decoder: This module inputs the disentangled object representation\nand decodes it to get the final image. Figure describes the overall architecture. We next describe each\nmodule in detail."}, {"title": "3.1 Mask Extractor", "content": "We first pre-train the object-centric model for video which is modified SAVi[37]. We train this object-\ncentric model with N slots using reconstruction loss on clips from the target dataset. The spatial\nmixture decoder of the modified SAVi produces temporally aligned object masks while decoding the\nslots. These object masks are good indicator of where the object is in the frame. To obtain better\nmasks we use SAM [15] with the prompts generated from the trained SAVi masks. Formally given\na sequence of frames $\\{x_t\\}_{t=1}^T$ for T time-steps where $x_t \\in \\mathbb{R}^{H \\times W \\times 3}$, we first extract the decoder\nmasks from SAVi denoted as $m_i \\in [0,1]^{H \\times W}$ where $i \\in 1,..., N$ for each frame $x_t$. We then\nthreshold the object mask $m_i$ to get binary mask of object $M_i \\in \\{0,1\\}^{H \\times W}$. We convolve $M_i$ with\nall one filter repeatedly for three times. Then from this convolved mask, a subset of pixel positions\nbelonging to maximum pixel value are sampled randomly. Denote this point set as $p_i$. These pixel\npositions are considered as positive point prompts to SAM. Positive point prompts from the rest of\nthe objects are chosen as the negative points denoted as $p^\\prime_i = \\cup_{j,j\\neq i}p_j$. These point prompts $p_i \\cup p^\\prime_i$"}, {"title": "3.2 Block Extractor", "content": "In our disentangled representation, each object is represented by a set of blocks. We obtain blocks by\niterative refinement with attention over a latent object representation. Further, to learn a disentangled\nrepresentation, we enforce that each block representation is represented as a linear combination of\na fixed set of underlying learnable concepts. This combination is discovered at every step of the\nblock-refinement algorithm. Additionally we initialize the blocks of the object with some learnable\nfunction of the object blocks from previous time-step.\nFormally the extracted object masks $m^i_t$ are multiplied element-wise with the corresponding input\nframe and passed through a Convolutional Neural Network (CNN) to obtain latent object repre-\nsentations denoted by $z^i_t \\in \\mathbb{R}^{f \\times d_f}$ Given a latent object representation $z^i_t$ for the ith object, its\nrepresentation in terms of blocks is given as $\\{b^i_{t,j}\\}_{j=1}^r$, where r is the number of blocks, and is a"}, {"title": "3.3 Dynamics", "content": "Transformers[29] have been very effective at sequence-to-sequence prediction tasks. Some recent\nwork on unsupervised video dynamics prediction [37],[39], has given SOTA results on this task\non 3-D datasets, and have been shown to outperform more traditional GNN based models for this\ntask [19]. While some of these models are object-centric, they do not exploit disentanglement over\nobject representation. Our goal in this section is to integrate our disentanglement pipeline described\nin Section 3.2 with downstream dynamics prediction via transformers. Here are some key ideas that\ngo into developing our transformer for this task:\n1. We linearly project each $b^i_{t,j}$ to a higher \u00e2 dimensional space using a shared $W_{up}$. $\\hat{b}^i_{t,j} = W_{up}b^i_{t,j}$,\nwhere $W_{up} \\in \\mathbb{R}^{d_b \\times d}$\n2. The input to transformer encoder is T step history of all object blocks that is, at time step t, the\ninput is $\\{\\hat{b}^i_{t,j} | t \\in \\{0, .., T - 1\\}, i \\in \\{1, .., N\\}, j \\in \\{1, .., r\\}\\}$.\n3. We need positional encoding to distinguish between (a) different time steps in the history (b)\nblocks belonging to different objects (c) different blocks in an object. We would also like to\nmaintain the permutation equivariance among objects. Having explicit positional encoding for\neach object will break the permutation equivariance among objects. Inspired by [26] we first add\nthe learnable position encoding $P_j$ to each block and then couple the blocks belonging to same\nobjects using block coupler which is one layer transformer encoder. These removes the need of\npositional encoding for the objects. Similar to [37], we add sinusoidal time step embedding $P_t$ for\neach block depending on the frame (time-step) it belongs to. Formally these steps are summarized\nas:\n$\\hat{b}^i_{t,j} = \\hat{b}^i_{t,j} + P_j$\n$O^i_t = BlockCoupler(\\hat{b}^i_{t,1}, .., \\hat{b}^i_{t,r})$\n$\\tilde{b}^i_{t,j} = O^i_t + P_t$\nHere, $\\hat{b}$, $O$ and $\\tilde{b}$ denote the block representations after incorporating block positional embedding,\ncoupling, and frame positional embeddings, respectively.\n4. The transformer encoder output be $\\hat{b}^i_{t,j}$. We down project it to do dimensional space using shared\n$W_{down}$. Thus the final output of dynamics predictor is $b^i_{t,j} = W_{down}\\hat{b}^i_{t,j}$"}, {"title": "3.4 Decoder", "content": "Similar to much of the previous work, we use spatial mixture models to generate the final image. As\neach object is represented by r vectors of blocks, we use a slightly different approach than previous\nmethods which have a single vector representing the object. We first use shared Spatial Broadcast"}, {"title": "3.5 Training and Loss", "content": "We use a history of length T and do a T' step future prediction. We use three-phase training. In\nfirst phase we only train block extractor and decoder with image reconstruction objective. In second\nPhase we train dynamics predictor with block extractor and decoder being freeze. In third phase, all\nmodules are trained together.\nPhase 1: In first phase we only reconstruct the T history frames. Reconstructing image only involves\nblock extractor and decoder in forward pass. The reconstruction loss is $L_1 = L_{rec} = \\sum_{t=1}^T(x_t - \\hat{x_t})^2$\nPhase 2: In second phase the losses used to train dynamics predictor are image prediction loss\n$L_{img} = \\sum_{t=T+1}^{T+T'}(x_t - \\hat{x_t})^2$, the latent block prediction loss $L_{block} = \\sum_{t=T+1}^{T+T'} \\sum_i \\sum_j (b^i_{t,j} - \\hat{b}^i_{t,j})^2$\nand mask prediction loss $L_{mask} = \\sum_{t=T+1}^{T+T'} \\sum_i(m^i_t - \\hat{m}^i_t)^2$. The total loss is $L_2 = L_{img} +$\n$\\lambda_{block}L_{block} + \\lambda_{mask}L_{mask}$.\nPhase 3: In third phase all modules are trained with objective $L_3 = L_1 + L_2$. Decoder parameters\nare trained only using $L_{rec}$."}, {"title": "4 Experiments", "content": "We perform a series of experiments to answer the following questions: (1) Does DisFormer result in\nbetter visual predictions than the existing SOTA baselines on the standard datasets for this problem?\n(2) Does learning disentangled representations with DisFormer lead to better performance in OOD\nattribute combinations? and (3) Can DisFormer indeed discover the disentangled representation\ncorresponding to natural object features such as color, size, shape, position, etc.? We first describe our\nexperimental set-up, including the details of our datasets and experimental methodology, followed by\nour experimental results, answering each of the above questions in turn."}, {"title": "4.1 Experimental Setup", "content": "We experiment on a total of four datasets for video dynamics prediction; two are 2-dimensional\nand two are 3-dimensional. While the 2D dataset are toy in nature, they allow us with controlled\nexperiments. 3D domains contain more realistic video dynamics.\n2D Bouncing Circles (2D-BC): This 2D dataset is adapted from the bouncing balls Interaction\nenvironment in [19] with modifications in the number and size of the balls. Our environment\ncomprises three circles of the same size but different colors that move freely in the 2D space with a\nblack background, colliding elastically with the frame walls and each other. The training split contain\n1000 videos whereas val and test split contain 300 videos each. All videos have 100 frames.\n2D Bouncing Shapes (2D-BS): We create another 2D dataset, which is an extension of the 2D-BC\nenvironment with increased visual and dynamics complexity. Two circles and two squares move freely\nin the 2D space with a checkered pattern as background. Collisions happen elastically among various\nobjects and frame walls while also respecting respective object geometries. We use the MuJoCo\nphysics engine [27] to simulate the domain with camera at the top, and objects with minimum height\nto have a 2-D environment. The split details are same as that of 2D-BS.\nOBJ3D: A 3D environment used in GSWM [19] and SlotFormer [37], where a typical video has\na rubber sphere that enters the frame from front and collides with other still objects. The dataset\ncontain 2912 train videos and 200 test videos. Each video has 100 frames."}, {"title": "4.1.2 Baselines, Metrics and Experimental Methodology", "content": "Baselines: We compare DisFormer with two existing baselines, GSWM [19] and SlotFormer [37].\nGSWM is the object-centric generative model. SlotFormer is state-of-the-art slot based object-centric\ndynamics model. In addition, we also create our own baseline, called DenseFormer, which is variation\nof our model where objects have dense representation rather than representation factorized in terms\nof blocks. This is achieved by replacing the block extractor and block coupler modules by a single\nlayer MLP. This baseline was created to understand the impact of having an object representation\nfactorized in terms of blocks in DisFormer.\nEvaluation Metrics: Following existing literature, for 2D-BC and 2D-BS domains, we compare\nthe quality of predicted future frames vis-a-vis the ground truth using two metrics: (1) PErr (Pixel\nmean-squared error) (2) PSNR. For OBJ3D and CLEVRER, we compare various algorithms on 3\ndifferent metrics: (1) PSNR (2) LPIPS [44], and (3) SSIM [33].\nExperimental Methodology: We use the official implementation for each of our baselines: 2. All the\nmodels (except GSWM) are trained to predict 10 frames of video given 6 frames of history. For fair\ncomparison, GSWM, which is a generative model, is provided with 16 frame clips during its training.\nWhile testing all the models are conditioned on 6 frames of history and are required to predict future\nframes. For BC and BS datasets, we unroll for 15 future steps during testing. For OBJ3D we only use\nonly first 50 frames for training and testing as most of interactions between objects ends in first 50\nframes [37][19]. For CLEVRER we subsample the frames by a factor of 2 and remove the clips where\nnew objects enter the frame during rollout, as done in existing literature. During testing, OBJ3D and\nCLEVRER models are unrolled upto 44 and 42 frames respectively following [19, 37], for a direct\ncomparison. For each of the datasets, the reported numbers are averaged over the unrolled future step\nprediction.\nOOD Setting: For OOD experiments we considered the unseen attribute combinations in the three of\nthe datasets. For 2D-BC dataset, all three balls in the video have unique color among red, blue green.\nThe OOD 2D-BC dataset have all possible color combinations of balls. 2D-BS dataset have 2 squares\nand 2 circles in video clips. For OOD dataset, we created 2D-BS variant where all possible shape\ncombinations are allowed (and were randomly chosen). For OBJ3D, we created OOD dataset where\nentering sphere is metallic instead of rubber (all training set instances had entering rubber sphere). In\neach case, we used the model trained on the original dataset, and tested on the OOD dataset. The\nnumber of test instances, the history length, as well as the number of rollouts for prediction were kept\nthe same as in the case of in distribution setting for each of the datasets."}, {"title": "4.2 Visual Dynamics Prediction", "content": "2D datasets: Table 1 presents the results on the 2-D datasets. DisFormer beats all other approaches\non both pixel-error and PSNR metrics. Our closest competitor is GSWM, and we have up to 10%\n(relative) lower value of PErr on both the datasets. On PSNR, we do moderately better on 2D-BC,\nand marginally better on 2D-BS. Interestingly, Slotformer does quite badly on both the 2D datasets.\nDenseformer does somewhere in between, comparable to GSWM in PixErr and doing worse on\nPSNR. We see a significant gain in the OOD setting, where we beat the closest competitor GSWM,\nwith up to relative margin of 15% on PixErr, and a moderate gain in PSNR. There is a significant\ndrop in the performance of Slotformer when going to OOD setting. This highlights the strength of\nour approach in generalizing to unseen attribute combinations.\n3D datasets: Table 2 presents the results on the 3-D datasets. As in the case of 2-D datasets,\nDisFormer beats all other approaches on both pixel-error and PSNR metrics. Our closest competitor\nin this case is Slotformer, which is comparable to our model in SSIM on OBJ3D, and LPIPS on\nCLEVEREd, but does worse on other metrics on the two datasets. DenseFormer (on OBJ3D) performs\nsomewhere between Slotformer and GSWM which performs worst. The strength of our approach is\nfurther highlighted in the OOD setting, where the margin of our performance gain with respect to the\ncompetitors increases significantly. We perform close to 8% (relative) better on PSNR, 5% (relative)"}, {"title": "4.3 Disentanglement of Object Attributes", "content": "We conduct a post-hoc analysis to gain insights into the learned disentangled representations of\nobjects in terms of their correlation to their attributes. We trained k different probes corresponding\nto k different attributes of object to predict the attribute given the block representation of object.\nThese probes are implemented as gradient boosted trees. The ground truth attributes are generated\nfrom respective simulators. Mapping between ground truth object attributes and object blocks is\ndone via decoder mask and simulator mask greedy matching. After training these probes we find\nthe importance of each dimension of blocks and sum the importance of dimensions belonging to\none block. This set up is very similar to the methodology adopted by [24] for evaluating the\ndisentanglement of attributes in static images. Figure 4 shows the relative importance of each of the"}, {"title": "4.4 Ablation", "content": "In this section, we analyze the robustness of our approach to variation in number of concepts and\nnumber of blocks used during training of our model. Table 3 presents the performance of DisFormer\nas we vary the number of concepts from 4 to 7 keeping number of blocks fixed at 6. Similarly,\nTable 4 presents the performance as we vary the number of blocks from 6 to 8 keeping the number\nof concept fixed at 4. Both experiments are done in 2D-BS dataset. As can be seen, while the\nperformance fluctuates with varying number of concepts, it goes down as the number of available\nblocks is increased during learning. In other words, using larger number of blocks than required\nresult in degrading of the performance of our model. Analysing the impact of these hyperparameters\nmore deeply is a direction for future work."}, {"title": "5 Conclusion and Limitations", "content": "We have presented an approach for learning disentangled object representation for the task of\npredicting visual dynamics via transformers. Our approach makes use of unsupervised object\nextractors, followed by learning disentangled representation by expressing dense object representation\nas a linear combination of learnable concept vectors. These disentangled representations are passed\nthrough a transformer to obtain future predictions. Experiments on three different datasets show that"}, {"title": "B1. SAVi Training", "content": "Following [37] we pre-train SAVi to discover objects. SAVi first applies slot attention [21] on each\nframe to discover objects in each slots. These slots are decoded using Spatial Mixture Decoder to\nreconstruct the frame. The decoder is implemented as Spatial Broadcast Decoder [34]. And for\ntemporal consistency initializes the slots of next frame with some learnable function on current frame\nslots.\nFormally given input frames $\\{x_t\\}_{t=1}^T$, first each frame is passed through CNN to obtain image feature\n$h_t \\in \\mathbb{R}^{UXU \\times D_{in}}$. Then spatial positional encoding is added to $h_t$. After this per frame ordered set of\nN slots $S_t$ are obtained from $h_t$ by spatial competitive attention between $h_t$ and slots. For temporal\nconsistency the slots from previous time step are used to initialize slots at current time step. This steps\ncan be summarized as $S_t = f_{trans}(S_{t-1})$ and $S_t = f_{sa}(h_t, S_t)$ where $f_{sa}$. Following [37] $f_{trans}$\nis implemented as one layer transformer and $f_{SA}$ is the iterative refinement loop from [21]. Each slot\nfrom ordered set $S_t$ is passed through shared Spatial Broadcast Decoder to generate the mixing masks\n$m_i$ and content of Spatial Mixture Decoder. Here $i \\in \\{1, .., N\\}$. We found that for 3D datasets\n(OBJ3D and CLEVRER) even though the slots are made compete to attend on spatial location, slot\nare not guaranteed to generate mutually exclusive masks $m_i$. That is two generated masks $m^j$ and $m^k$\nmay have high IOU score for $i \\neq j$. The problem with overlapping masks is that they will generate\nthe contradicting prompts for SAM. We further explain this in 5. To address this issue, we found\nempirically that it was sufficient to add mask intersection loss as extra auxiliary loss to make masks\nexclusive. The mask intersection loss formally is $L_{int} = \\frac{1}{H,W,N(N-1)} \\sum_{b}^{H,W}\\sum_{i}^{N} \\sum_{j \\neq i} m_i[a, b]*m_j[a, b]$"}, {"title": "B2. SAM prompting", "content": "We observe that masks generated by slots, even though gives rough masks of object, can be improved.\nWe use Segment Anything (SAM)[15] to further improve the mask quality. SAM is promptable\nimage segmentation model, where prompt can be foreground / background points, a approximate\nmask or bounding box. We generate the points and mask prompt from masks generated by slots and\npass it to SAM to get better mask.\nWe generate set of $n_p$ foreground / positive point prompts per slot $S_i$ denoted as $p^i_t$. A good choice\nfor $p^i_t$ is the points which are on the object and not on the boundary of the object. Our approach is\nto choose pixel positions with maximum pixel values. To make sure all points are well inside the\nobject we first threshold the slot masks $m^i_t$ with $thresh$ to make it binary mask $M^i_t$. Then convolve\n$M^i_t$ with all one filter of size (3, 3) for $l$ times. Denote the resultant mask as $\\tilde{M}^i_t$. We choose top $n_p$\npoints which have maximum pixel value from $\\tilde{M}^i_t$. For negative points we use the positive points of\nremaining slots which gives $(N - 1)n_p$ negative / background points per slot. Along with this $Nn_p$\npoint prompt, we pass $m^i_t$ as mask prompt to SAM. Algorithm 2 gives pseudo code for sampling\npoint prompts from slot masks.\nTo handle the cases where slot does not represent object we ignore such slot masks and return all\nzero mask instead. If sum of pixel values of $\\tilde{M}^i_t$ is less that $mthresh$, we consider that slot does not\nrepresent object. We use the official release of SAM with ViT-H backbone. 4."}]}