{"title": "Efficiently Dispatching Flash Attention For Partially Filled Attention Masks", "authors": ["Agniv Sharma", "Jonas Geiping"], "abstract": "Transformers are widely used across various applications, many of which yield sparse or partially filled attention matrices. Examples include attention masks designed to reduce the quadratic complexity of attention, sequence packing techniques, and recent innovations like tree masking for fast validation in MEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art algorithm Flash Attention still processes them with quadratic complexity as though they were dense. In this paper, we introduce Binary Block Masking, a highly efficient modification that enhances Flash Attention by making it mask-aware. We further propose two optimizations: one tailored for masks with contiguous non-zero patterns and another for extremely sparse masks. Our experiments on attention masks derived from real-world scenarios demonstrate up to a 9x runtime improvement. The implementation will be publicly released to foster further research and application.", "sections": [{"title": "1 Introduction", "content": "Transformers [25] revolutionized sequence modeling by using self-attention mechanisms, enabling efficient parallelization and handling of long-range dependencies. A key component of transformers is the attention mechanism. However, in many practical scenarios\u2014such as when processing very long sequences, reducing computational costs, or working with structured data like graphs \u2014 it becomes advantageous to restrict the range of elements each item can attend to. This leads to the development of masked attention mechanisms, which limit interactions to a subset of elements.\nOne application that leads to such partially filled masks is the creation of efficient transformers. Approaches like LongFormer and BigBird [2, 28] employ fixed sparsity patterns to alleviate the quadratic complexity inherent in self-attention, while other approaches exploit low-rank approximations to achieve similar objectives [26, 9]. Despite the theoretically lower computational complexity of these methods, many results fail to translate to reduced wall-clock runtimes. The currently prevalent practical solution is instead to simply implement dense attention more efficiently. Flash Attention [10, 8, 20] is a such a hardware-aware algorithm sufficient to boost the speed of dense attention and obviate the need for a more efficient attention in many applications.\nHowever, because the initial flash attention implementation was written with only causal masks in mind, research using alternative attention patterns has slowed down. A previous approach to remedy this, [17], enables a few types of mask in flash attention, but only considers a small subset of cases and requires the user to carefully change the GPU kernel parameters for every new mask. To overcome these limitations, we introduce Binary Block Masking (BinBlkMsk) - a novel algorithm that extends Flash Attention to support any attention masks while also being user-friendly. Our approach builds on two key insights: (1) Flash Attention processes attention matrices in blocks, so we only need to handle blocks containing at least one non-zero mask value, and (2) these blocks are independent"}, {"title": "2 Related Work", "content": "Since their inception, transformers have employed masked attention for various applications. The original transformer[25] uses causal masking to maintain the sequential nature of text, and standard techniques, such as in [19], utilize attention masks to 'pack' multiple sequences into one block during pretraining. Masking has also been utilized to create efficient transformers [14, 23]. Models like Star Transformer[11], Longformer[2], Big Bird[28], and ETC[1] use attention masks with global and local token windows to speed up attention calculations while preserving information flow. Sparse transformers [6] use factorized attention mask to attend to different parts of input across different heads and Reformer[12] uses s locality-sensitive-hashing (LSH) to select keys which limit attention to keys and queries that collide to the same hash.\nHowever, masked attention is a natural building block in a number of applications across domains, such as graph learning with transformers [3] or selective perception in vision transformers [27]. On the other hand, inference techniques to speed up generation in text generation, such as [4, 15, 21] all also use some form of a tree attention mask to increase the number of candidates validated at each step of speculative decoding[13, 5]. All these approaches utilize masked attention, but ours is the first method to combine the advantages of state-of-the-art Flash Attention with masking, achieving the best of both techniques.\nOur method closely relates to two previous approaches: Block Sparse Attention[10] and Faster Causal Attention[17]. In Block Sparse Attention[10], authors skip processing some blocks randomly, but unlike our approach, they do not use a mask to guide this, resulting in a mask-agnostic method that provides only approximate attention, whereas ours offers exact calculations. Faster Causal Attention[17] handles only two specific masks-QK masks (dropping random query keys) and hash sparse masks\u2014but it lacks flexibility for general masks and requires significant adaptation for new methods. In contrast, our method is generalizable and intuitive across different masking strategies."}, {"title": "3 Background", "content": "Standard Attention In standard self-attention[25], the Query (Q) and Key (K) matrices, both of size $N \\times D$ (where N is the number of tokens and D is the dimensionality), are multiplied to produce an $N \\times N$ attention matrix. This matrix is normalized using softmax to form a probability distribution, which is then multiplied by the Value (V) matrix ($N \\times D$) to generate the final output. In multi-head attention, this process is repeated across multiple heads, and typically processed over batches to enhance parallelism and capture diverse patterns.\nFlash Attention Flash Attention[10, 8] improves self-attention efficiency through two key optimizations. First, it computes the attention matrix in blocks using a running softmax, reducing data transfers from High Bandwidth Memory (HBM). Second, it recomputes the attention matrix during the backward pass, saving memory and optimizing IO operations. These techniques make Flash Attention particularly effective for large models and long sequence lengths.\nFlash Attention with naive masking. Flash Attention currently only supports standard causal masks. Extending Flash Attention to handle custom masks using a naive approach would involve reading the corresponding mask block for each Flash Attention block. While this method would correctly apply the mask, it introduces additional memory accesses for reading the mask and extra computation for applying it, resulting in a slower runtime than the base Flash Attention. We refer to this method as Naive Attention Masking, and benchmark its performance to highlight the trade-offs."}, {"title": "4 Method", "content": "Binary Block Masking Our method focuses on optimizing Flash Attention by processing only the blocks of the attention matrix that have non-zero entries in their corresponding mask blocks. We first preprocess the attention mask to create a binary matrix, termed the \"Binary Block Matrix\" (BinBlkMat). This matrix has dimensions $N//BLOCKSIZE_I \\times N//BLOCKSIZE_J$ where N is the size of the attention mask and $BLOCKSIZE_I$ and $BLOCKSIZE_J$ represent the block sizes in the row and column dimensions respectively. Each entry in BinBlkMat is set to 1 if any value in the corresponding mask block is non-zero."}, {"title": "5 Experiments", "content": "In the following section, we evaluate the performance of our \"Binary Block Masking\" algorithm using attention masks from three real-world use cases: (1) tree attention masks for Speculative Decoding in MEDUSA[4], (2) masks from packed finetuning[19] of an LLM on the ALPACA dataset[22], and (3) fixed masks from the LongFormer paper[2]."}, {"title": "5.1 Experimental Setup", "content": "Evaluation Metrics All experiments report total runtime (forward and backward pass) averaged over 100 runs, with a batch size of 4 and 32 attention heads. The BLOCKSIZE for Flash Attention[10, 8]is fixed to (128, 32), which is fine-tuned to achieve optimal performance on our hardware configuration.\nBaselines We compare against two baselines: the Triton implementation[24] of Flash Attention[10, 8] and a naive masking algorithm that applies masks block by block, as previously described. Although Flash Attention is mask-unaware (and hence does not actually produce a correct result), it serves as a useful baseline to display performance gains with our method. We use Flash Attention's Triton implementation (instead of the CUDA implementation [16]) for fairer comparisons, as Triton introduces its own overhead.\nHardware and Precision Experiments were conducted on an RTX 3060 (6GB memory) using bfloat16 precision for query, key and value vectors. For a more detailed analysis on different hardware, please consult Appendix B."}, {"title": "5.2 Preprocessing", "content": "We implement a Triton kernel to preprocess the attention mask into the binary block mask format. The preprocessing runtime is comparable to the forward pass of a single attention head. However, this overhead is minimal as the Flash Attention runtime is dominated by the backward pass. Additionally, preprocessing is done once and shared across multiple heads and transformer layers. The following table provides runtime comparisons for various mask dimensions."}, {"title": "5.3 Evaluation on MEDUSA Tree Masks", "content": "Figure 3 shows the results for tree masks derived from the MEDUSA architecture[4], a speculative decoding approach that speeds up inference by using multiple heads to predict future tokens in parallel. MEDUSA generates several candidate tokens per head, verified through a tree-based attention mechanism. We evaluate performance gains from using these tree attention masks.\n The size of a MEDUSA tree mask is given by $\\sum_{k=1}^K \\prod_{i=1}^k s_i$, where K is the number of decoding heads and sk is the number of candidates per head. Our current configuration uses 4 heads with 4 candidates each, resulting in a 340x340 attention matrix. At this scale, Flash Attention shows no significant speedup, so we use the native PyTorch implementation as baseline.\nOur experiments demonstrate scalability with minimal computational overhead, allowing more decoding heads and candidates per head. Figures 3b, and 3c show two scenarios: one with fixed decoding heads(K = 4) and varying number of candidates(sk), and another with fixed number of"}, {"title": "5.4 Evaluation on packed ALPACA finetuning", "content": "ALPACA [22] is an instruction fine-tuning dataset comprising input, instruction, and output components. We combine the input and instruction into a single input and pack multiple sequences into longer ones, using masks to prevent cross-sequence attention. Figure 4 demonstrates our algorithm's performance on two ALPACA packing approaches: (1) Sequential Masks: Input and output are concatenated, with tokens attending only to previous tokens. And (2) Input-Bidirectional Mask: Input tokens attend to each other, while output tokens attend to input and preceding output tokens. Both masks consist of contiguous non-zero blocks, enabling the use of our Dense BinBlkMsk algorithm variant.\nPacking improves upon padding, where shorter sequences in a batch are extended with a special token to match the longest sequence's length. While padding wastes GPU resources, it's more compatible with Flash Attention. Our experiments show that our algorithms efficiently combine the benefits of Flash Attention with packing, resulting in approximately a 9x performance increase in both cases."}, {"title": "5.5 Evaluation on LongFormer Attention Masks", "content": "We evaluate our algorithm across three LongFormer[2] masking patterns: Windowed, Dilated, and Global. For the Windowed mask, we use the Dense BinBlkMsk, while the base algorithm is used for the other two masks. The performance results, along with the corresponding masks, are presented in Figure 5.\nOur findings demonstrate that our method consistently outperforms the baseline Flash Attention algorithm across all three masks. Moreover, these results highlight that the performance improvements"}, {"title": "6 Discussion, Limitations, and Future Work", "content": "Our experimental results demonstrate that the proposed algorithm effectively leverages the mask structure to achieve notable speedups, though the extent of these improvements is highly dependent on the mask's fill pattern. Specifically, as the mask becomes increasingly filled, the processing time grows correspondingly. However, even in extreme cases\u2014such as a fully dense mask (composed entirely of ones) or a causal mask\u2014our algorithm maintains performance that is competitive with Flash Attention[10, 8]. This is primarily due to the optimization introduced by the Dense BinBlkMsk representation, which ensures that the only additional overhead compared to the base implementation is the reading of binary values for the Binary Block Mask (Appendix C).\nOne scenario where our method may exhibit slower performance than Flash Attention arises when the mask is nearly full but consists of non-contiguous non-zero blocks. In such cases, Flash Attention may outperform our algorithm in raw speed. However, it is important to note that Flash Attention is mask-agnostic, meaning the output it generates does not inherently respect the mask constraints. To apply masking with Flash Attention, a post-processing step is required, which can negate its initial speed advantage and make the overall process slower.\nFor future work, a key direction would be migrating our custom Triton kernels[24] to CUDA[16] and integrating them with the state-of-the-art Flash Attention framework to further enhance performance. Additionally, it is crucial to evaluate the algorithm on real-world, end-to-end tasks rather than focusing solely on isolated mask cases. Another avenue is exploring alternatives to Reverse Cuthill-McKee[7]"}, {"title": "7 Conclusion", "content": "In this paper, we present our implementation of Binary Block Masking that optimizes Flash Attention for partially filled attention masks. By preprocessing the mask blocks in parallel, our method identifies non-zero blocks and restricts attention computation to these blocks, enhancing efficiency. This approach is both general-purpose and easy to integrate, making it broadly applicable. Our experiments demonstrate substantial performance improvements over base Flash Attention and highlight the versatility of our method across diverse applications. We will release our Triton kernels and mask-processing code to encourage further research in this area."}]}