{"title": "Mamba in Vision: A Comprehensive Survey of Techniques and Applications", "authors": ["MD MAKLACHUR RAHMAN", "ABDULLAH AMAN TUTUL", "ANKUR NATH", "LAMYANBA LAISHRAM", "SOON KI JUNG", "TRACY HAMMOND"], "abstract": "Mamba is emerging as a novel approach to overcome the challenges faced by Convolutional Neural Networks (CNNs) and\nVision Transformers (ViTs) in computer vision. While CNNs excel at extracting local features, they often struggle to capture\nlong-range dependencies without complex architectural modifications. In contrast, ViTs effectively model global relationships\nbut suffer from high computational costs due to the quadratic complexity of their self-attention mechanisms. Mamba addresses\nthese limitations by leveraging Selective Structured State Space Models to effectively capture long-range dependencies with\nlinear computational complexity. This survey analyzes the unique contributions, computational benefits, and applications of\nMamba models while also identifying challenges and potential future research directions. We provide a foundational resource\nfor advancing the understanding and growth of Mamba models in computer vision. An overview of this work is available at\nhttps://github.com/maklachur/Mamba-in-Computer-Vision.", "sections": [{"title": "INTRODUCTION", "content": "The evolution of deep learning has significantly advanced computer vision, with Convolutional Neural\nNetworks (CNNs) [69] playing a key role. CNNs revolutionized the field by enabling machines to learn complex\npatterns directly from pixel data through convolutional layers that capture features at multiple scales and\nbuild spatial hierarchies. Despite their success, CNNs encounter inherent challenges in capturing long-range\ndependencies due to their localized receptive fields. Addressing these challenges often requires deeper and more\ncomplex architectures, which increase computational cost and reduce efficiency [49, 121, 137].\nTo improve sequence modeling and global context understanding, Recurrent Neural Networks (RNNs)\n[126] were initially developed, followed by the introduction of Transformers [142], which brought significant\nbreakthroughs in deep learning. RNNs, especially those with Long Short-Term Memory (LSTM) units [53],\nimproved the ability to capture temporal dependencies in sequential data. However, their sequential nature\nlimits parallel processing, which slows down speed and reduces scalability [22, 53]. With their self-attention\nmechanisms, Transformers overcame this limitation by allowing models to dynamically prioritize different parts of\nthe input data [142]. The Vision Transformer (ViTs) were developed for images and treated them as sequences\nof patches, capturing global dependencies more effectively than CNNs [25, 68, 120]. However, despite their strong\nperformance in various computer vision tasks, ViTs face computational efficiency challenges due to the quadratic\ncomplexity of their self-attention mechanism, especially in high-resolution and real-time applications [120].\nHybrid models have emerged to address the limitations of traditional architectures by integrating the strengths\nof CNNs, RNNs, and Transformers in computer vision tasks. For instance, Convolutional LSTMs (ConvLSTMs)\n[133] enhance the model's ability to capture spatial-temporal relationships by integrating convolutional operations\nwithin LSTM units [133]. Similarly, MobileViT merges the local feature extraction of CNNs with the global context\nmodeling of Transformers [106]. Hybrid architectures aim to balance high performance and computational\nefficiency but add complexity due to component optimization requirements.\nRecently, State Space Models (SSMs) have gained attention as a promising alternative, especially for handling\nsequential data where efficiently managing long-range dependencies is crucial [38\u201340]. The Structured State\nSpace Sequence (S4) model is a notable development in this domain, which leverages state space representations\nto achieve linear computational complexity. As a result, long sequences can be processed efficiently without\ncompromising accuracy [39]. The S4 model achieves this by integrating both recurrent and convolutional\noperations, which helps reduce the computational demands typically associated with sequence modeling.\nBuilding on the foundational principles of SSMs, the Mamba model [37] represents a significant leap in\nsequence modeling. Mamba integrates state space theory with advanced deep learning techniques, employing\nselective state representations that dynamically adjust based on the input data. This selective state mechanism\ndynamically filters out less important information to focus on the most relevant parts of the input sequence,\nthereby reducing computational overhead and enhancing efficiency [37]. The Mamba architecture utilizes a\nhardware-aware, scan-based algorithm optimized for GPUs, avoiding the inefficiencies of traditional convolution-based SSMs. This leads to faster training and inference, enabling more efficient handling of visual data and a\ntransformative approach to computer vision [203].\nMamba models are particularly advantageous for tasks such as video processing [8], for long temporal sequences\nprocessing; remote sensing [12], for large spatial datasets; and medical imaging [159], for efficient and precise\nhigh-resolution data processing. CNNs and Transformers face scalability issues due to high computational\ndemands, while Mamba models overcome this by offering linear scalability with sequence length, making them\nideal for real-time and large-scale applications. Their integration of state space principles with selective attention\nmechanisms offers a robust approach to handling complex visual tasks, enabling more efficient and scalable\ncomputer vision solutions.\nWhile recent survey papers have explored various aspects of Mamba models-such as SSMs [154], applications\nin computer vision [93, 170, 188], and medical image analysis [52]-our paper provides a unique perspective by\ndistinguishing itself in areas like model taxonomy, scanning methods, application domains, comparative analysis\nwith CNNs and Transformers, and future directions, as outlined in Table 1."}, {"title": "TAXONOMY OF MAMBA MODELS", "content": "The adaptation of Mamba for visual tasks began in early 2024, with models like VMamba [94] and Vision Mamba\n(Vim) [203]. These initial models have pushed the boundaries of visual processing, providing efficient solutions\nto complex challenges. For the convenience of future researchers, we have developed a comprehensive taxonomy,\nas shown in Figure 2. This categorization highlights the diverse applications of Mamba-based models across nine\ncategories, with significant contributions to medical image analysis."}, {"title": "OVERVIEW OF MAMBA MODELS", "content": "In this section, we provide the basic pipeline for Mamba-based vision models in Figure 3. This pipeline starts with\npatching the input image, followed by a series of scanning operations designed to extract multi-scale features.\nThe patches are then passed through a Mamba block, which typically consists of linear projections, convolutional\nlayers, SiLU activation, and SSM operations to extract features optimally. Depending on the specific task, many\nmodels then integrate CNN and transformer blocks to enhance performance. Now, we explore the inner workings\nof the Mamba block in the following section."}, {"title": "Fundamentals of Mamba Models", "content": "This section provides an overview of the structure and technical details of Mamba models."}, {"title": "State Space Model.", "content": "SSMs are a fundamental class of models in deep learning for sequence data. These\nmodels are designed to map a one-dimensional input sequence, denoted as $x(t)$ and existing in a real vector\nspace $R^1$, to an output sequence $y(t)$ in the same space, via an intermediate latent state $h(t)$ residing in $R^N$. The\ndynamics of these models are controlled by a set of linear transformations [37], as described by the equations:\n$h'(t) = Ah(t) + Bx(t),  y(t) = Ch(t).$\n(1)\nHere, $A$, $B$, and $C$ are system matrices of appropriate dimensions that dictate the state transitions, input, and\noutput mapping, respectively. These matrices are defined as $A \u2208 R^{N\u00d7N}$, $B \u2208 R^{N\u00d71}$, and $C\u2208R^{N\u00d71}$.\nFor practical applications, the continuous-time model needs to be converted into a discrete-time model for\nimplementation in digital systems. This discretization is typically achieved using a zero-order hold assumption,\nwhere the continuous-time system parameters $A$ and $B$ are transformed into their discrete counterparts over a\nsampling timescale \u2206, which is a positive real number. The discretized system is represented as:\n$\\overline{A} = e^{A}, \\overline{B} = (\\Delta e^{\\Delta A} - I) \\cdot \\Delta B.$\nThe resulting discrete model equations are:\n$h_t = \\overline{A}h_{t\u22121} + \\overline{B}x_t,  Y_t = Ch_t.$\n(3)\nTo improve computational efficiency, the output for the entire sequence can be computed simultaneously\nthrough a global convolution operation, enhancing both scalability and processing speed. This is formulated as:\n$y = x \\otimes K,  K = (C\\overline{B}, C\\overline{A}\\overline{B}, ..., C\\overline{A}^{-L-1} \\overline{B}),$\n(4)\nwhere $\u2297$ represents the convolution operation, $L$ represents the length of the sequence, and $K$ is the kernel\nderived from the SSM, specifically designed to handle sequences efficiently."}, {"title": "Selective State Space Model.", "content": "Building upon the traditional SSM framework, the Selective SSM, called\n\"Mamba\", [37] introduces a dynamic and adaptive mechanism for managing the interactions between sequential\nstates. Unlike traditional SSM, which utilize fixed transition parameters A and B, Mamba models feature input-\ndependent parameters, enabling a more flexible and context-aware parameterization.\nIn Mamba models, the parameters B and C are not static but are computed as functions of the input sequence\nx. This dynamic computation allows the model to adapt its behavior based on the specifics of the input sequence,\nthus providing a more nuanced understanding and processing of sequential data. The dimensions of these\nparameters are B \u2208 RB\u00d7L\u00d7N and C\u2208 RB\u00d7L\u00d7N, where B, L, and N denote the batch size, sequence length, and\nnumber of states, respectively. The Mamba model ensures linear scalability with respect to sequence length and\ndemonstrates robust performance across various domains, particularly in computer vision tasks."}, {"title": "Scanning Methods in Mamba", "content": "Scanning is a crucial process in Mamba, converting 2D visual data into 1D sequences for more efficient model\nprocessing, with various methods emerging to balance spatial integrity and computational efficiency without\ncompromising accuracy.\nDifferent scanning techniques serve distinct purposes in Mamba models [94, 194, 203]. For example, Local\nscanning [59] divides an image into smaller windows and processes each window independently. This method\npreserves local details but may fail to capture the broader context of the image. In contrast, Global scanning\n[173, 203] processes the entire image in one pass, which captures broader patterns but may miss finer details.\nMulti-head scanning [55, 135] divides image patches into multiple subspaces, allowing the model to capture\ncomplex patterns while managing computational resources. On the other hand, Bidirectional scanning [118, 203]\nprocesses images in both horizontal and vertical directions. This approach captures spatial information effectively\nbut requires more computational power.\nTo illustrate various traversal paths, Figure 4 provides a holistic overview of scanning techniques such as\nsequential, zigzag, spiral, radial, and hilbert curve scanning. Sequential scans, whether horizontal or vertical,\nare straightforward but may struggle with capturing long-range dependencies. Zigzag scans balance local and\nglobal information by alternating traversal direction after each row or column. Spiral and radial scans emphasize\ncomprehensive coverage, moving from the center outward or the edges inward. They are particularly useful in\napplications like medical imaging and remote sensing, where detailed spatial analysis is critical. These diverse\ntraversal paths enable Mamba models to adapt to the characteristics of different datasets and the requirements of\nvarious tasks. These basic methods can be combined into more complex scanning approaches, as shown in Figure\n5. For example, we can derive the Omnidirectional selective scanning (Figure 5(f)) [135, 196] by combining the\nfirst, second, fifth, and seventh column from sequential scanning (Figure 4(row A)) and third, second, eighth and\nfifth column from zigzag scanning (Figure 4(row C)).\nScanning techniques can be enhanced by combining different traversal directions-local, global, or atrous-with\ncontinuous or discontinuous patterns. This adaptability optimizes the capture of both local and global features.\nFor instance, atrous scanning (also known as efficient or skipping scanning) uses a skipping mechanism to capture"}, {"title": "MAMBA IN COMPUTER VISION APPLICATIONS", "content": "This section demonstrates the contributions and versatility of Mamba models in different computer vision tasks,\nincluding General Purpose Framework 4.1, Image Classification, Object Detection, and Segmentation 4.2, Image\nEnhancement 4.3, Generation and Restoration 4.4, 3D Point Cloud 4.5, Video Processing 4.6, Remote Sensing 4.7,\nMedical Image Analysis 4.8, and Multimodal Models 4.9. We present the distribution of Mamba models in Figure\n6, which highlights their usage in various computer vision tasks."}, {"title": "General Purpose", "content": "General Purpose Mamba frameworks are designed to be flexible and adaptable for classification, detection, and\nsegmentation tasks. VMamba [94] enhances performance by integrating 1D scanning with 2D vision data through\nVSS blocks and the SS2D module, while Vision Mamba [203] overcomes unidirectional scanning limitations\nusing bidirectional Mamba blocks with position embeddings. Despite advancements, capturing the global context\nremains challenging. Vim-F [189] enhances the global receptive field using frequency domain information via\nFast Fourier Transform (FFT), while Mamba-R [145] reduces artifacts in feature maps with register tokens for\ncleaner outputs.\nAs models evolve, it becomes crucial to balance long-range dependency learning with computational efficiency.\nIn order to optimize these aspects, MSVMamba [134] offers a balanced solution with a multi-scale 2D scanning\nmethod combined with a Convolutional Feed-Forward Network (ConvFFN). FractalVMamba [139] enhances\nspatial relationship modeling with fractal scanning curves that adapt to varying image resolutions. LocalMamba\n[59] further optimizes these tasks by introducing windowed selective scan methods that dynamically adjust\nscanning strategies at different network layers, outperforming ViTs and CNNs. The challenge of balancing\naccuracy with computational demands is also addressed by EfficientVMamba [116], which integrates atrous-based\nselective scanning with efficient skip sampling, successfully reducing FLOPs while maintaining high performance.\nExpanding Mamba's capabilities to high-dimensional data introduces new challenges. Mamba-ND [74] tackles\nthese by alternating sequence orderings across dimensions, preserving the linear complexity of SSMs while\nachieving high accuracy in image classification and weather forecasting tasks. To enhance flexibility in image\nmodeling, SUM [54] pairs the Mamba framework with a U-Net structure. Meanwhile, Heracles [115] addresses\nthe complexities of high-resolution images and time-series analysis by integrating local and global SSMs with\nattention mechanisms.\nMambaMixer [5] proposes a dual token and channel selection mechanism to improve inter- and intra-dimension\ncommunication in both vision and time series tasks. SiMBA [114] is designed to provide a simpler yet effective\narchitecture. It integrates Einstein FFT (EinFFT) for channel modeling, setting a new standard for SSMs in both\nimage and time-series tasks. PlainMamba [173], with its focus on spatial continuity and directional awareness,"}, {"title": "Image Classification, Object Detection, and Segmentation", "content": "Mamba improves image classification, object detection, and segmentation tasks by capturing local and global\nfeatures. In image classification, challenges like distinguishing pests with high camouflage and species diversity\nin agricultural settings have been addressed by InsectMamba [150]. This model enhances accuracy by integrating\nSSMs with CNNs and Multi-Head Self-Attention. Similarly, Res-VMamba [9] advances food image classification by\ncombining the Mamba mechanism with deep residual learning, setting a new standard in fine-grained recognition.\nAdditionally, Mamba models such as RSMamba [14] and SpectralMamba [180] have proven effective in classifying\ncomplex remote sensing data, which will be discussed in detail in Section 4.7. Mamba's versatility extends to\nmedical image classification. Models like MedMamba [185] and MamMIL [28] optimize feature extraction across\nvarious imaging modalities to improve classification performance and diagnostic accuracy. These advancements\nwill be discussed in Section 4.8.1.\nIn object detection, Fusion-Mamba [24] improves cross-modal detection accuracy by mapping features into a\nhidden state space to reduce disparities between different modalities. However, small object detection, particularly\nin aerial imagery, remains difficult due to minimal data and background noise. SOAR [143] addresses this by\nintegrating SSMs with the lightweight YOLO v9 architecture. Mamba-YOLO [156] also builds upon the YOLO\narchitecture by integrating SSMs with LSBlock and RGBlock modules to improve the modeling of local image\ndependencies for more precise detections. Moreover, MIM-ISTD [16] employs a nested Mamba architecture to\nimprove infrared small target detection (ISTD). In 3D object detection, Voxel Mamba [186] uses a group-free\nSSM to enhance feature extraction in point cloud data. This approach overcomes the challenge of maintaining\nvoxel spatial proximity during serialization. Additionally, HTD-Mamba [130] focuses on hyperspectral data,\ncombining a pyramid SSM with spectrally contrastive learning and spatial-encoded spectral augmentation to\ncapture long-range dependencies and fuse multiresolution spectral features effectively."}, {"title": "Image Enhancement", "content": "Mamba has significantly advanced image enhancement across various domains. In endoscopic imaging, exposure\nabnormalities often result in poor image quality. FD-Vision Mamba (FDVM-Net) [199] addresses this by combining\nconvolutional layers and SSMs within a C-SSM block. This approach processes phase and amplitude information\nseparately to achieve high-quality image reconstruction. Underwater imaging faces unique issues of color distor-tion and blurring. PixMamba [86] efficiently captures global contextual information using a dual-level architecture\nthat enhances underwater images while managing computational costs. Similarly, WaterMamba [41] tackles\nunderwater image challenges using spatial-channel omnidirectional selective scan (SCOSS) blocks to manage\ndependencies and model pixel and channel information flow effectively. To reduce the FLOPs, MambaUIE&SR\n[19] integrates VSS blocks with dynamic interaction blocks.\nThe challenge in low-light image enhancement lies in balancing brightness with noise reduction. RetinexMamba\n[4] combines traditional Retinex methods with SSMs, using innovative illumination estimators and damage\nrestorers to maintain image quality and processing speed. LLEMamba [192] advances low-light enhancement by\nintroducing a relighting-guided Mamba architecture within a deep unfolding network. This approach balances\ninterpretability and distortion through Retinex optimization and Mamba deep priors. For single-image dehazing,\nUVM-Net [198] solves the challenge of handling long-range dependencies by combining local feature extraction\nwith Bi-SSM blocks, which efficiently manage computational resources.\nMamba models also provide significant improvements in super-resolution tasks. DVMSR [71] leverages\nVision Mamba and Residual State Space Blocks (RSSBs) to enhance efficiency without sacrificing performance.\nFourierMamba [72] improves image deraining by integrating Mamba into the Fourier space with zigzag coding to\ncorrelate low and high frequencies. In optical Doppler tomography, SRODT [80] enhances B-scan reconstruction\naccuracy by employing SSM-based learning to capture sequential and interaction information within A-scans.\nMLFSR [33] and LFMamba [99] apply SSM blocks to light field image super-resolution, capturing spatial and\nangular correlations to significantly boost performance.\nRemote sensing presents unique challenges, such as haze and low-resolution imagery. HDMba [31] addresses\nthese by using window selective scan modules to capture local and global spectral-spatial information flow,\nthereby improving scene reconstruction in hyperspectral image dehazing. To facilitate the evaluation of both\ntraditional and Mamba models, BVI-RLV [84] introduces a comprehensive dataset along with their framework\nfor low-light video enhancement."}, {"title": "Generation and Restoration", "content": "In image generation, the DiS model [29] substitutes traditional U-Net-like architectures with SSM, thus minimizing\ncomputational overhead and producing high-resolution images. This capability is crucial for satellite imagery and\nhigh-definition content creation applications. Moving into medical applications, MD-Dose [32] leverages Mamba's\ndiffusion model to simulate radiation dose distributions for cancer treatment, offering precise, patient-specific\ndose mappings that enhance therapy outcomes. This transition from DiS to MD-Dose showcases Mamba's\nversatility across fields that require detailed outputs. Similarly, Gamba [131] combines Gaussian splatting with\nMamba's state-space blocks for efficient 3D reconstruction from single-view inputs, which is particularly useful"}, {"title": "Point Cloud Analysis", "content": "Mamba has progressed point cloud analysis by tackling challenges such as large data volumes, unstructured\ndata, and high computation. PointMamba [82] approaches this by aligning points in a sequence, which enables\nefficient processing of 3D point clouds. This method reduces parameters by 44.3% and FLOPs by 25%, outperform-ing transformer-based models in accuracy. Similarly, Point Cloud Mamba (PCM) [191] enhances modeling by\nconverting 3D point clouds into 1D sequences through Consistent Traverse Serialization (CTS) while preserving\nspatial adjacency. Combined with advanced positional encoding, this approach achieves SOTA performance on\nbenchmarks such as ScanObjectNN and ModelNet40.\nAddressing noise in large-scale point clouds is another crucial challenge. 3DMambaIPF [202] introduces a\ndifferentiable rendering loss to preserve geometric detail and enhance realism in denoised structures. This model\nhandles high-noise environments across synthetic and real-world datasets. 3DMambaComplete [76] excels in\npoint cloud completion by transforming sparse inputs into dense outputs using a HyperPoint Generation module.It sets new benchmarks by retaining local details during reconstruction. Mamba3D [44] focuses on precise\ngeometric feature extraction with its Local Norm Pooling (LNP) blocks and uses bidirectional SSMs to integrate\nglobal features. Additionally, Point Mamba [91] employs an octree-based ordering system, organizing data points\ninto a z-order curve that preserves spatial locality for effective SSM processing.\nFor more advanced applications, MAMBA4D [89] targets 4D point cloud video understanding by disentangling\nspatial and temporal features with Intra-frame Spatial Mamba and Inter-frame Temporal Mamba blocks, effectively\ncapturing long-range motion dependencies while reducing GPU memory usage. OverlapMamba [163] is designed\nfor place recognition and compresses visual representations into sequences, which improves loop closure detection.\nPointABM [13] combines bidirectional SSMs with multi-head self-attention to capture comprehensive features,\nenhancing the analysis of point clouds. Mamba24/8D [77] introduces a multi-path serialization strategy along with\nConvMamba blocks to effectively handle long-range dependencies. Finally, PoinTramba [155] optimizes point\ncloud recognition and segmentation by integrating Transformer and Mamba architectures, using a bi-directional\nimportance-aware ordering (BIO) strategy."}, {"title": "Video Processing", "content": "Mamba has advanced video processing through its ability to address challenges like managing long sequences\nand efficiently handling high-resolution data. ViS4mer [61] directly confronts the inefficiencies of self-attention\nmechanisms in long-range video understanding. By combining a Transformer encoder for short-range feature\nextraction with a multi-scale temporal S4 decoder, ViS4mer processes data over 2.5 times faster, uses 8 times less\nmemory than pure self-attention models, and achieves SOTA accuracy in long-form video classification.\nAdditionally, the VideoMamba Suite [10] showcases the versatility of Mamba-based models across a variety of\nvideo understanding tasks. It categorizes Mamba's applications into roles like temporal modeling, multimodal\ninteraction, and spatial-temporal processing. These categorizations reveal Mamba's adaptability to different video\nprocessing needs. RhythmMamba [207] addresses the challenge of capturing quasi-periodic rPPG patterns for"}, {"title": "Remote Sensing", "content": "Mamba's capability of processing high-dimensional data from satellite and aerial imagery makes it suitable\nfor Remote Sensing applications. To classify hyperspectral images, SpectralMamba [180] efficiently addresses\nthe challenges of high dimensionality and inter-band correlation by directly integrating spectral data into the\nclassification process. HSIMamba [175] further improves upon traditional models by incorporating bidirectional\nprocessing. It helps to distinguish subtle differences in spectral signatures crucial for applications like vegetation\nanalysis and land cover change detection. Additionally, the introduction of the 3DSS-Mamba framework [51]\nprovides a 3D-Spectral-Spatial approach, using a spectral-spatial token generation module and a novel selective\nscanning mechanism. RSMamba [14], SS-Mamba [58], and S\u00b2Mamba [146] enhance terrain analysis accuracy\nthrough various innovative techniques. Among them, RSMamba introduces position-sensitive dynamic multi-path\nactivation to handle 2D non-causal data effectively and enhance classification accuracy across diverse terrains. SS-\nMamba and S\u00b2Mamba further refine this approach by employing spectral-spatial Mamba blocks and bi-directional\nscanning mechanisms, respectively, ensuring precise spatial-spectral fusion for accurate classification. However,\nin semantic segmentation, Mamba-based models RS3Mamba [103] and Samba [205] employ dual-branch networks\nand encoder-decoder architectures. These approaches enhance global and local data comprehension to optimize\nthe extraction of multilevel semantic information from high-resolution images.\nIn pan-sharpening, merging low-resolution multi-spectral images with high-resolution panchromatic images\nto enhance visual details poses a significant challenge. Pan-Mamba [50] addresses this issue by utilizing channel-swapping and cross-modal Mamba blocks, which facilitate efficient data interchange between different image\nmodalities. This method significantly enhances the spatial and spectral details to produce high-quality pan-sharpened images that preserve essential information from both input sources. Similarly, hyperspectral image\ndenoising, which requires preserving essential details while eliminating noise, is adeptly handled by HSIDMamba\n[95]. It employs continuous scan blocks and bidirectional scanning to maintain the integrity of hyperspectral\ndata.\nFurthermore, change detection, an essential aspect of remote sensing, requires precise identification of differ-ences between multi-temporal images. ChangeMamba [12] and RSCaMa [88] excel in this domain by employing\ndifferent scanning mechanisms and spatial difference-guided SSM. These models effectively manage bi-temporal\nfeature interactions and provide accurate spatial change detection. This capability is vital for monitoring envi-ronmental changes, urban development, and disaster impact assessment. Another important task named Image\nfusion, aiming to integrate spatial and spectral data without losing fine details, is addressed by LE-Mamba [7]. It\nuses Mamba blocks within U-shaped networks and locally enhanced vision Mamba blocks, respectively. To tackle\nthe limitations of large-scale images and object variations, CM-UNet [92] combines CNN-based encoders with\nMamba-based decoders to enhance global-local information fusion. Furthermore, RSDehamba [201] integrates\nthe SSM framework into a U-Net architecture for remote sensing image dehazing, while FMSR [165] employs\na multi-level fusion architecture for super-resolution. Models like Seg-LSTM [204], PyramidMamba [149], and\nCDMamba [187] refine multi-scale feature representation to enhance segmentation accuracy and change detection\nprecision. Lastly, VMRNN [140] integrates Vision Mamba blocks with LSTM to balance efficiency and accuracy\nin spatiotemporal prediction tasks."}, {"title": "Medical Image Analysis", "content": "Mamba is tremendously popular for medical image analysis. This section explores Mamba models for different\nmedical image analysis tasks such as classification, segmentation, and reconstruction."}, {"title": "Medical Image Classification.", "content": "MedMamba [185] addresses the challenges of CNNs and ViTs by integrating\nconvolutional layers with SSMs. Building upon the strengths of SSMs, BU-Mamba [109] applies Vision Mamba to\nbreast ultrasound image classification. It achieves better performance, especially when working with limited data.\nHowever, classifying Whole Slide Images (WSIs) is challenging due to their gigapixel scale, which complicates\nefficient feature extraction. To address this, MamMIL [28] integrates bidirectional SSMs with a 2D context-aware\nblock to improve WSIs feature detection. It preserves spatial relationships while also reducing memory usage.\nMambaMIL [176] further optimizes tissue sample arrangement with the Sequence Reordering Mamba (SR-Mamba)\nby improving feature extraction while reducing overfitting. Vim4Path [110] takes WSI analysis a step further,\nusing self-supervised learning with the DINO framework to significantly improve feature encoding."}, {"title": "Medical Image Segmentation.", "content": "U-Mamba [101] is one of the early approaches to use SSMs in medical image\nsegmentation. It employs a hybrid CNN-SSM block structure within a U-Net framework to capture long-range\ndependencies, which traditional methods often struggle to handle. Leveraging the strengths of SSMs, Mamba-UNet\n[159] and VM-UNet [123] offer refinements through symmetrical and asymmetrical encoder-decoder structures.\nThese models perform better across various medical image datasets, including abdominal and skin lesion imaging.\nTo further improve performance, Swin-UMamba [90] combined Mamba with the hierarchical attention mecha-nisms of Swin Transformers to obtain the benefits from ImageNet pre-training. VM-UNet V2 [190] introduced\na Semantics and Detail Infusion (SDI) mechanism to optimize feature fusion, while H-vmunet [161] addressed\nredundancy in long-range feature extraction through selective scanning. LightM-UNet [83] prioritized computa-tional efficiency by employing residual Vision Mamba layers. UltraLight VM-UNet [162] significantly reduced\nparameters without compromising accuracy, and LMa-UNet [147] utilized large-window SSM blocks for efficient\nlong-range feature capture. To improve spatial-channel integration, TM-UNet [138] incorporated Triplet-SSM\nmodules, and Mamba-HUNet [125] focused on hierarchical upsampling. P-Mamba [181] addressed noise reduction\nand efficient feature extraction using a dual-branch framework with DWT-based Perona-Malik Diffusion blocks.\nIn weakly supervised learning, Weak-Mamba-UNet [157] explores the combination of CNNs, ViTs, and Mamba\narchitectures to address the challenges of scribble-based annotations. Semi-Mamba-UNet [100] enhances feature\nlearning from unlabeled data through self-supervised contrastive learning. ProMamba [166] specializes in polyp\nsegmentation by integrating Vision Mamba with prompt technologies. MUCM-Net [183] and AC-MambaSeg\n[111] target skin lesion segmentation, combining Mamba with advanced feature extraction techniques. While\nMUCM-Net optimizes the Mamba layer to emphasize mobile deployment, AC-MambaSeg focuses on improving\nfeature extraction and background noise suppression using a CBAM-based attention mechanism [121]. For micro-scopic instance segmentation, ViM-UNet [2] provides a global field of view with greater efficiency. Addressing\nthe challenges of reduced resolution and information loss, HC-Mamba [169] employs dilated and depthwise\nseparable convolutions.To improve local feature modeling, SliceMamba [27] introduces a Bidirectional Slice Scan\nmodule, while xLSTM-UNet [15] incorporates Vision-LSTM to capture long-range dependencies and outperforms\ntraditional segmentation models."}, {"title": "Medical Image Reconstruction.", "content": "In medical image reconstruction, converting raw data from MRI, CT, and PET\ninto high-quality images is crucial for accurate diagnosis and treatment. However, the process faces challenges,\nincluding noise, artifacts, and the need for computational efficiency. To address these issues, MambaMIR [57]\nwas developed for fast MRI and SVCT tasks. It enhances Monte Carlo-based uncertainty estimation and reduces\nnoise and artifacts by using an arbitrary mask mechanism. While this approach improved image clarity, the need\nfor further refinement led to the creation of MambaMIR-GAN [57]. This variant incorporates adversarial training\nto sharpen images and enhance visual quality.\nThe challenge of integrating Multimodal MRI data for better reconstruction led to the development of MMR-Mamba [208]. This model combines spatial and frequency domain information using the Target modality-guided\nCross Mamba (TCM) module for spatial data integration and the Selective Frequency Fusion (SFF) module for\nrecovering high-frequency details. The Adaptive Spatial-Frequency Fusion (ASFF) module also integrates data\nacross both domains, providing a robust solution for Multimodal MRI reconstruction."}, {"title": "Other Tasks in Medical Imaging.", "content": "Mamba has driven significant advancements in various medical imaging\ntasks beyond traditional applications such as classification and segmentation. For example, VMambaMorph [158]\ntackles complex Multimodality image alignment using a hybrid VMamba-CNN network, but handling intricate\nstructures remains challenging. Similarly, the Motion-Guided Dual-Camera Tracker (MGDC Tracker) [193]\nimproves endoscopy tracking through a cross-camera template strategy and Mamba-based motion prediction.\nHowever, maintaining consistent accuracy across varying environments remains a challenge.\nIn radiotherapy, MD-Dose [32] enhances radiotherapy dose prediction with a Mamba-based diffusion model.\nthis approach improves precision but requires better anatomical integration. Meanwhile, BI-Mamba [179] reduces\nradiation exposure in cardiovascular risk prediction by capturing long-range dependencies in chest X-rays.\nDespite these benefits, achieving high accuracy with lower-resolution imaging still remains a challenge."}, {"title": "Multimodal", "content": "Multimodal models handle diverse data types like images, text, audio, and video, with the key challenge being\nthe fusion of heterogeneous data to leverage complementary information from each modality. SurvMamba [18]\naddresses this by integrating pathological images and genomic data to improve survival predictions. It employs a\nHierarchical Interaction Mamba (HIM) module to capture detailed intra-modal interactions and an Interaction\nFusion Mamba (IFM) module to merge these modalities, resulting in comprehensive representations. Similarly,\nTransMA [160] accelerates mRNA drug delivery screening by predicting ionizable lipid nanoparticle (LNPs)\nproperties through a 3D Transformer and molecule Mamba for feature alignment."}, {}]}