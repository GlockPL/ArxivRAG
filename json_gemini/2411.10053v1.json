{"title": "That Chip Has Sailed: A Critique of Unfounded Skepticism Around AI for Chip Design", "authors": ["Anna Goldie", "Azalia Mirhoseini", "Jeff Dean"], "abstract": "In 2020, we introduced a deep reinforcement learning method capable of generating superhuman chip layouts, which we then published in Nature and open-sourced on GitHub. AlphaChip has inspired an explosion of work on AI for chip design, and has been deployed in state-of-the-art chips across Alphabet and extended by external chipmakers. Even so, a non-peer-reviewed invited paper at ISPD 2023 questioned its performance claims, despite failing to run our method as described in Nature. For example, it did not pre-train the RL method (removing its ability to learn from prior experience), used substantially fewer compute resources (20x fewer RL experience collectors and half as many GPUs), did not train to convergence (standard practice in machine learning), and evaluated on test cases that are not representative of modern chips. Recently, Igor Markov published a \"meta-analysis\" of three papers: our peer-reviewed Nature paper, the non-peer-reviewed ISPD paper, and Markov's own unpublished paper (though he does not disclose that he co-authored it). Although AlphaChip has already achieved widespread adoption and impact, we publish this response to ensure that no one is wrongly discouraged from innovating in this impactful area.", "sections": [{"title": "1 Introduction", "content": "Following its publication in Nature, AlphaChip [30] has inspired an explosion of work on AI for chip design [41, 39, 43, 40, 10, 18, 5, 34, 8, 12, 17, 37, 7, 29]. It has also generated superhuman chip layouts used in three generations of TPU (see Figure 1), datacenter CPUs (Axion), and other chips across Alphabet, and been extended to new areas of chip design by external academics and chipmakers [25, 11].\nEven so, Igor Markov published a criticism of our work in the November 2024 issue of Communications of the ACM [27], which is presented as a \u201cmeta-analysis\" of our Nature paper and two non-peer-reviewed papers:\n\u2022 Cheng et al.: The first is an invited ISPD paper\u00b9 by Cheng et al. [9]. This paper did not follow standard machine learning practices, and its reinforcement learning methodology and experimental setup diverged significantly from those described in our Nature paper."}, {"title": "2 Errors In Cheng et al.'s Reproduction of Our Method", "content": "Cheng et al. claim to evaluate our method against alternative approaches on new test cases. Unfortunately, Cheng et al. did not run our method as described in Nature, so it is unsurprising that they report different results. In this section, we describe major errors in their purported reproduction:\n\u2022 Did not pre-train the RL method. The ability to learn from prior experience is the key advantage of our learning-based method, and to remove it is to evaluate a different and inferior approach. Incidentally, pre-training also gives rise to the impressive capabilities of large language models like Gemini [36] and ChatGPT [32] (the \u201cP\u201d in \u201cGPT\" stands for \"pre-trained\"). See Section 2.1.\n\u2022 Used an order of magnitude fewer compute resources: 20x fewer RL experience collectors (26 vs 512 in Nature) and 2x fewer GPUs (8 vs 16 in Nature). See Section 2.2.\n\u2022 Did not train to convergence. Training to convergence is standard practice in machine learning, as not doing so is well known to harm performance [1]. See Section 2.3.\n\u2022 Evaluated on non-representative, irreproducible benchmarks. Cheng et al.'s benchmarks have much older and larger technology node sizes (45nm and 12nm vs sub-7nm in Nature), and differ substantially from a physical design perspective. Additionally, the authors were unable or unwilling to share the synthesized netlists necessary to replicate the results in their main data table. See Sections 2.4 and 4.2.\n\u2022 Performed \"massive reimplementation\" of our method, which may have introduced errors. We recommend instead using our open-source code. See Section 4.\nThese major methodological differences unfortunately invalidate Cheng et al.'s comparisons with and conclusions about our method. If Cheng et al. had reached out to the corresponding authors of the Nature paper, we would have gladly helped them to correct these issues prior to publication\"."}, {"title": "2.1 No Pre-Training Performed for RL Method", "content": "Unlike prior approaches, AlphaChip is a learning-based method, meaning that it becomes better and faster as it solves more instances of the chip placement problem. This is achieved by pre-training, which consists of training on \u201cpractice\" blocks (training data) prior to running on the held-out test cases (test data).\nAs we showed in Figure 5 of our Nature paper (reproduced below as Figure 2), the larger the training dataset is, the better the method becomes at placing new blocks. As described in our Nature article, we pre-trained on 20 blocks in our main data table (Nature Table 1).\nCheng et al. did not pre-train at all (i.e., no training data), meaning that the RL agent had never seen a chip before and had to learn how to perform placement from scratch for each of the test cases. This removed the key advantage of our method, namely its ability to learn from prior experience.\nBy analogy to other well-known work on reinforcement learning, this would be like evaluating a version of AlphaGo [14] that had never seen a game of Go before (instead of being pre-trained on millions of games), and then concluding that AlphaGo is not very good at Go.\nWe discussed the importance of pre-training at length in our Nature paper (e.g. the word \u201cpre-train\" appeared 37 times), and empirically demonstrated its impact. For example, Nature Figure 4 (reproduced here as Figure 3) showed that pre-training improves placement quality and convergence speed. On the open-source Ariane RISC-V CPU [16], it took 48 hours for the non-pretrained RL"}, {"title": "2.2 RL Method Provided with Far Fewer Compute Resources", "content": "In Cheng et al., the RL method is provided with 20x fewer RL experience collectors (26 vs 512 in Nature) and half as many GPUs (8 vs 16 in Nature). Using less compute is likely to harm performance, or require running for considerably longer to achieve the same (or worse) performance.\nAs shown in Figure 4 (reproduced from a follow-up paper [42]), training on a larger number of GPUs speeds convergence and yields better final quality. If Cheng et al. had matched the experimental settings described in Nature, this would likely have improved their results."}, {"title": "2.3 RL Method Not Trained to Convergence", "content": "As a machine learning model trains, loss typically decreases and then plateaus, representing \u201cconvergence\" \u2013 the model has learned what it can about the task it is performing. Training to convergence is standard practice in machine learning, and not doing so is well known to harm performance [1]."}, {"title": "2.4 Cheng et al.'s Test Cases Not Representative of Modern Chips", "content": "In our Nature paper, we report results on Tensor Processing Unit (TPU) blocks with sub-7nm technology node size, which is typical of modern chips. In contrast, Cheng et al. reports results on older technology node sizes (45nm and 12nm), which differ substantially from a physical design"}, {"title": "3 Other Issues With Cheng et al.", "content": "In this section, we describe other concerns with Cheng et al., including its comparison with closed-source commercial autoplacers, its contrived \u201cablation\u201d of initial placement in standard cell cluster rebalancing, its flawed correlation study, and its erroneous claim of validation by Google engineers."}, {"title": "3.1 Inappropriate Comparison With Commercial Autoplacers", "content": "Cheng et al. compares a severely weakened RL method against unpublished, closed-source, proprietary software released years after our method was published. This is not a reasonable way to evaluate our method \u2013 for all we know, the closed-source tool could have built directly on our work."}, {"title": "3.2 Contrived \u201cAblation\u201d of Initial Placement in Standard Cell Cluster Rebalancing", "content": "Prior to running the methods evaluated in our Nature paper, an approximate initial placement from physical synthesis, the previous step of the chip design process [35], was used to resolve imbalances in the sizes of standard cell clusters from hMETIS [23].\nCheng et al. ran an \u201cablation\u201d study on a single block (Ariane-NG45). Instead simply skipping the cluster rebalancing step, they tried placing all chip components on top of each other in the lower left corner13, causing the rebalancing step to produce degenerate standard cell clusters. When this harmed performance, Cheng et al. concluded that our RL agent was somehow making use of initial placement information, even though it does not have access to the initial placement and does not place standard cells.\nWe ran an ablation study which eliminated any use of initial placement whatsoever, and observed no degradation in performance (see Table 2). We simply skipped the cluster rebalancing step and instead reduced hMETIS\u2019s cluster \u201cunbalancedness\u201d parameter to its lowest setting (UBFactor=1)14, which causes hMETIS to generate more balanced clusters [23]. This ancillary preprocessing step has been documented and open-sourced since June 10, 2022 [21], but is unnecessary and has already been removed from our production workflow."}, {"title": "3.3 Flawed Study of Correlation Between Proxy Cost and Final Metrics", "content": "Cheng et al. claimed that our proxy costs are not well-correlated with final metrics, but their correlation study actually showed a weak but positive correlation between overall proxy cost and all final metrics except standard cell area (see Cheng et al.\u2019s Table 2, reproduced here as Figure 6). Note that we treat area as a hard constraint, and therefore do not optimize for it.\nProxy costs used in ML-based optimization are often only weakly correlated with the target objective. For example, large language models like Gemini [36] and ChatGPT [32] are trained to guess the next word in a sequence, which is an intrinsically noisy signal.\nAdditionally, Cheng et al.\u2019s correlation study made some surprising choices:"}, {"title": "3.4 Cheng et al.'s Incorrect Claim of Validation by Google Engineers", "content": "Cheng et al. claimed that Google engineers confirmed its technical correctness, but this is untrue. Google engineers (who were not corresponding authors of the Nature paper) merely confirmed that they were able to train from scratch (i.e. no pre-training) on a single test case from the quick start guide in our open-source repository. The quick start guide is of course not a description of how to fully replicate the methodology described in our Nature paper, and is only intended as a first step to confirm that the needed software is installed, that the code has compiled, and that it can successfully run on a single simple test case (Ariane).\nIn fact, these Google engineers share our concerns and provided constructive feedback, which was not addressed. For example, prior to publication of Cheng et al., through written communication and in several meetings, they raised concerns about the study, including the use of drastically less compute, and failing to tune proxy cost weights to account for a drastically different technology node size.\nThe Acknowledgements section of Cheng et al. also lists the Nature corresponding authors and implies that they were consulted or even involved, but this is not the case. In fact, the corresponding authors only became aware of this paper after its publication."}, {"title": "4 Transparency & Reproducibility", "content": "We have open-sourced a software repository [21] to fully reproduce the methods described in our Nature paper. Every line of our RL method is freely available for inspection, execution, or modification, and we provide source code or binaries to perform all preprocessing and postprocessing steps. Open-sourcing the code took over a year of effort by the TF-Agents team16, and included independent replication of our methodology and the results in our Nature paper. From our open-source repository:"}, {"title": "4.1 AlphaChip is Fully Open-Source", "content": "We have open-sourced a software repository [21] to fully reproduce the methods described in our Nature paper. Every line of our RL method is freely available for inspection, execution, or modification, and we provide source code or binaries to perform all preprocessing and postprocessing steps. Open-sourcing the code took over a year of effort by the TF-Agents team, and included independent replication of our methodology and the results in our Nature paper. From our open-source repository:"}, {"title": "4.2 Cheng et al. Claim They Cannot Share Their \"Open\" Test Cases", "content": "One of the criticisms put forth in Cheng et al. was that the Nature evaluation was done on proprietary TPU blocks (in addition to the open-source Ariane block that was also evaluated, and the public ISPD 2015 benchmark in a follow-up publication [22]). Cheng et al. claimed to evaluate on a set of open test cases to improve reproducibility, but when we corresponded with the authors, they were unable or unwilling to provide the synthesized netlists necessary to replicate their results on the \"open\" test cases in their main data table (Table 1).\nUnfortunately, this means that we cannot replicate any of the results in Cheng et al.'s Table 1:\n\u2022 GF12 (12nm): These test cases are proprietary and unavailable to the public, and Cheng et al.'s results are obfuscated, meaning that even if an external researcher were to obtain access, a direct comparison would still be impossible.\n\u2022 NG45 (45nm): Cheng et al. have not shared the synthesized netlists necessary to reproduce their NG45 results, despite 10+ requests since February 2024. Note that other papers evaluate on the NG45 blocks, but their results are inconsistent with those in Cheng et al.'s Table 1 (e.g. see Table 2 of AutoDMP [2]), underscoring reproducibility challenges.\nIt is unfortunate that modern chip IP is sensitive and proprietary, and to our knowledge, there are no open benchmarks available for cutting edge processes. We encourage the chip design community to create more open designs for modern sub-7nm processes, as this will help push the field forward. At the moment, fully open designs are typically 28nm, 45nm, or even 130nm, and many physical design issues are quite different than in sub-7nm processes."}, {"title": "5 Conclusion", "content": "In Cheng et al.'s attempt to reassess our work, the authors did not run our method as described in Nature (e.g. they performed no pre-training, used substantially less compute, and did not train to convergence), reported results on benchmarks that are neither representative nor reproducible, and ran questionable ablation/correlation studies.\nIn his paper [27], Markov published baseless allegations of fraud based on a \u201cmeta-analysis\" of Cheng et al. (which did not reproduce our methodology) and an anonymous PDF (that Markov actually coauthored), whose results could not be reproduced and for which \"the claims and conclusions in the draft are not scientifically backed by the experiments\" [33].\nMeanwhile, AlphaChip has inspired an explosion of work on AI for chip design, and its superhuman layouts have been taped out in multiple generations of TPU deployed in Google datacenters all over"}]}