{"title": "Exploring Pose-Based Anomaly Detection for Retail Security: A Real-World Shoplifting Dataset and Benchmark", "authors": ["Narges Rashvand", "Ghazal Alinezhad Noghre", "Armin Danesh Pazho", "Shanle Yao", "Hamed Tabkhi"], "abstract": "Shoplifting poses a significant challenge for retailers, resulting in billions of dollars in annual losses. Traditional security measures often fall short, highlighting the need for intelligent solutions capable of detecting shoplifting behaviors in real time. This paper frames shoplifting detection as an anomaly detection problem, focusing on the identification of deviations from typical shopping patterns. We introduce PoseLift, a privacy-preserving dataset specifically designed for shoplifting detection, addressing challenges such as data scarcity, privacy concerns, and model biases. PoseLift is built in collaboration with a retail store and contains anonymized human pose data from real-world scenarios. By preserving essential behavioral information while anonymizing identities, PoseLift balances privacy and utility. We benchmark state-of-the-art pose-based anomaly detection models on this dataset, evaluating performance using a comprehensive set of metrics. Our results demonstrate that pose-based approaches achieve high detection accuracy while effectively addressing privacy and bias concerns inherent in traditional methods. As one of the first datasets capturing real-world shoplifting behaviors, PoseLift offers researchers a valuable tool to advance computer vision ethically and will be publicly available to foster innovation and collaboration. The dataset is available at https://github.com/TeCSAR-UNCC/PoseLift.", "sections": [{"title": "1. Introduction", "content": "Shoplifting is a persistent issue that significantly impacts businesses, communities, and the economy. Retailers face substantial financial losses, operational inefficiencies, and security challenges due to undetected thefts [3]. In the United States alone, retail theft resulted in $112.1 billion in lost revenue in 2022 and an estimated $121.1 billion in 2023. Projections indicate these losses could exceed $143 billion by 2025 [32], as shown in Fig. 1. Despite these escalating figures, current security measures remain insufficient, with only about 2% of shoplifters apprehended [32].\nThe limitations of traditional security measures have driven significant interest in artificial intelligence-powered systems for shoplifting detection [4, 18]. Video surveillance systems, while ubiquitous, generate vast amounts of data that security personnel cannot analyze in real time, creating opportunities for automated detection systems [5, 23, 29]. AI systems integrated with existing security infrastructures have the potential to enable real-time theft detection, generate alerts for personnel, and provide actionable insights, such as identifying high-risk areas and peak shoplifting times [17].\nDespite advances in AI-based computer vision, research in vision-based shoplifting detection faces three critical challenges. First, the lack of real-world datasets hampers progress in shoplifting detection research. Existing datasets [3, 5, 23] often rely on staged scenarios with actors or data aggregated from online sources [33], failing to capture the complexities of authentic shoplifting incidents. Such datasets lack the contextual nuances specific to individual retail environments, limiting their applicability in real-world settings.\nSecond, the complexity and scarcity of labeled data fur-"}, {"title": "2. Related Works", "content": "In the field of shoplifting detection, some studies focus on creating datasets, while others concentrate on developing algorithms for detection.\nMuneer et al. [23] presents a large dataset with 900 videos. While the developed dataset is extensive, it lacks critical features for real-world applications. Notably, despite being labeled as a real-world dataset, it involves pre-arranged scenarios where individuals are asked to steal items in four predefined ways-placing items in their pockets, shirts, jackets, or college bags. Furthermore, these actions are performed exclusively by boys, which limits the diversity of the dataset. Additionally, all videos in this dataset were captured from a single camera angle, reducing the variety of perspectives needed for robust detection.\nIn another dataset presented in [3], a shoplifting dataset was created using a 32 Megapixel camera. The proposed dataset consist of two classes: the normal class and shoplifting class. The normal class includes video clips depicting activities such as walking or inspecting shop items, while the shoplifting class contains clips of theft-related actions, such as concealing items under clothing or in bags. However, all videos were captured from a single camera angle, and the environment used is not a real retail store.\nIn contrast, many studies on shoplifting detection, such as those by Kirichenko et al. [17], Nazir et al. [25], and Ansari et al [4], have introduced algorithms that detect shoplifting at the pixel level. These studies rely on the UCF-Crime dataset [33], the only publicly available dataset containing real-time shoplifting video clips. For instance, Kirichenko et al. [17] addressed the problem of shoplifting by proposing a hybrid neural network to classify video fragments as either shoplifting or non-shoplifting, with their experiments conducted using the UCF-Crime dataset [33]. Nazir et al. [25] proposed a method for detecting suspicious behavior based on temporal feature extraction and time-series classification, using UCF-Crime dataset [33] for their experiments. Similarly, Ansari et al. [4] introduced a shoplifting detection system that combines YOLOv5 object detection [12] and Deep Sort tracking [35], also tested on"}, {"title": "3. PoseLift Dataset", "content": "This section provides an overview of the process involved in developing the dataset, covering data collection, preparation, and annotation, along with several segmented samples."}, {"title": "3.1. Data Collection", "content": "The dataset was created using real-world data directly sourced from actual retail environments. We collaborated with stakeholders to obtain CCTV footage from a retail store, capturing real-world instances of both normal and shoplifting behaviors. The footage was recorded at a local retail store in the USA. All the videos were captured from high-angle views with a resolution of 1920 \u00d7 1080 and a frame rate of 15 frames per second. The original videos were processed to separate instances of shoplifting, resulting in a final collection of 155 videos. These videos vary in length and camera views, with the longest video being 331 seconds. The store is equipped with a total of 6 indoor cameras (C1 to C6), placed across various aisles and shelves in different locations throughout the store. The camera locations and their coverage are shown in the bird's-eye view in Fig. 2. Additionally, Fig. 3 illustrates the segmented camera views used in the dataset to capture normal and shoplifting video footage.\nThe distribution of videos by camera is as follows: 42 videos from C1, 33 videos from C2, 34 videos from C3,"}, {"title": "3.2. Data Preparation, and Annotation", "content": "As previously mentioned, the PoseLift dataset provides pose sequence data, specifically designed for pose-based shoplifting detection. To protect individual privacy, privacy-preserving techniques were applied by removing personal identifiers and anonymizing sensitive data. Instead of providing raw pixel-level video data, PoseLift offers pose sequence data, which captures abstract representations of human-body movements to ensure privacy and bias concerns mentioned in Sec. 1.\nIn line with the principles of unsupervised anomaly detection and established methodologies in the field [9,21,27], we have provided frame-level anomaly annotations for the PoseLift dataset. Trained annotators meticulously reviewed the footage, labeling each frame based on the presence or absence of shoplifting activities. This rigorous process involved multiple reviews of each video to ensure labeling accuracy. Shoplifting behaviors identified include actions such as placing items into pockets or bags, or concealing them under clothing. Conversely, standard shopping activities were labeled as normal. Consequently, each frame was classified as either shoplifting (anomalous, labeled as 1) or non-shoplifting (normal, labeled as 0). This labeling protocol was consistently applied across videos from cameras C1 through C6.\nThe anonymized data provided by the PoseLift dataset includes bounding box annotations, person ID annotations, and human pose annotations. We have used a similar approach to [29] for extracting the annotations. YOLOv8 object detection model [16] was employed to detect and localize individuals within each frame of the video. YOLOv8 generates bounding boxes around detected persons, indicating their positions within the scene. These bounding box annotations serve as a foundational step for subsequent stages, including human pose estimation and tracking.\nFor person ID annotations, we have utilized the ByteTrack [37] algorithm, enabling robust tracking of multiple individuals across frames. ByteTrack ensures the tracking of individuals, even in crowded scenes, allowing the system to assign IDs to each person. This temporal tracking capability is essential for detecting progressing anomalies, as it enables the detection of shoplifting behaviors that unfold over time.\nFor human pose annotations we leveraged HRNet [34], a state-of-the-art model for human pose estimation. HRNet extracts 2D skeletons by identifying keypoints on the human body, providing a detailed representation of body movements. These keypoints are presented according to the COCO17 [20] keypoint format, which includes 17 distinct points that define human body pose.\nTo further improve the quality of the data, several modifications were applied. First, due to the presence of many shelves in the store and the resulting occlusions, we defined specific areas of interest for each camera and excluded individuals outside these designated regions. This allowed us to focus on relevant actions for each camera and minimize distractions caused by areas with limited visibility. To improve the accuracy and continuity of pose annotations, we applied linear interpolation to fill in any missing poses between frames. Additionally, an 8-frame window was used for data smoothing, ensuring that body movements appear continuous while reducing potential noise or errors in pose tracking. These steps collectively ensure the dataset's pose annotations are both accurate and reliable, making it well-suited for training robust shoplifting detection models."}, {"title": "3.3. Samples from PoseLift Dataset", "content": "Since PoseLift exclusively provides pose sequence data rather than raw pixel data, we can only present selected segmented images from the developed dataset, highlighting various behaviors associated with both shoplifting and normal shopping activities.\nFig. 4 presents four distinct examples of typical shopping behavior that are considered normal. As shown in the images, each image features 17 keypoints for each detected individual, along with a bounding box and an associated person ID. Fig. 4.1 depicts a person looking at items on the shelves. Fig. 4.2 shows an individual walking through the store, followed by Fig. 4.3, where an individual is picking an item from the shelf, and finally, Fig. 4.4 shows a person holding a bottle in their hands as they move through the store."}, {"title": "4. PoseLift Analysis and Comparison with Existing Datasets", "content": "PoseLift contains 155 videos captured from 6 different camera views. Of these, 112 videos include normal behaviors, while 43 capture shoplifting behaviors. Since the dataset provides pose sequence data from real-world behaviors, we define a scenario based on the detection of a person until their person ID disappears. This allows us to differentiate between normal shopping scenarios and shoplifting scenarios. In total, the dataset contains 1,500 anomalous frames, captured across 43 shoplifting scenarios. In contrast, there are 779 normal scenarios from six different camera perspectives, which correspond to 55,574 frames of normal frames.\nTab. 1 presents a comparison between the PoseLift dataset and existing datasets in the field of shoplifting detection. As shown in the table, PoseLift is one of the first datasets sourced from real-world data captured within a single retail store. In contrast, datasets such as those by Arroyo et al. [5], Ansari et al. [3], and Muneer et al. [23] rely on staged behaviors, typically involving actors. While the UCF-Crime dataset [33] includes real-world videos, it gathers videos from online sources like YouTube, sourced from multiple retail locations. What distinguishes PoseLift is its approach of collaborating with stakeholders to collect data directly from CCTV cameras in a retail store, ensuring the capture of normal and shoplifting behavior in a realistic retail setting.\nConsidering the length of the datasets and the distribution of shoplifting and normal videos, UCF-Crime [33] is the longest, with a total duration of 460,800 seconds. However, it is important to note that UCF-Crime [33] covers 13 different categories of anomalies and is not specifically designed for shoplifting detection. It includes 50 instances of shoplifting in different retail stores. In contrast, while PoseLift contains fewer shoplifting videos than datasets with prearranged shoplifting scenarios featuring actors, such as those by Arroyo et al. [5], Ansari et al. [3], Iqra Muneer [23], it offers realistic instances of shoplifting captured in actual retail environments. Additionally, compared to the shoplifting subset of the UCF-Crime [33], which contains collected real video footage, PoseLift contains a nearly identical number of shoplifting videos. Furthermore, in terms of total dataset length, PoseLift surpasses the length of those by Arroyo et al. [5], Ansari et al. [3], Iqra Muneer [23].\nOn the other hand, PoseLift is the only one that addresses privacy concerns by using pose sequence data, whereas all other datasets rely on raw pixel data. This privacy-preserving approach is the key distinction, and due to this difference, we are unable to directly compare the number of shoplifting and normal scenarios, as certain features are not accessible for comparison.\nThe datasets also vary in terms of Frames Per Second (FPS), with values of 10, 15, and 30 FPS. Furthermore, while other datasets are captured from a single camera view, with the exception of UCF-Crime [33], PoseLift stands out as the only one that incorporates six distinct camera views. This diversity in camera perspectives enhances the robustness and applicability of PoseLift for real-world retail settings."}, {"title": "5. Metrics", "content": "In the context of shoplifting detection, it is essential to choose the appropriate evaluation metrics to assess model performance effectively. Given the class imbalance typically encountered in such tasks, where instances of shoplifting are much less than normal behaviors, metrics like AUC-ROC, AUC-PR, and EER provide complementary insights. These metrics will be discussed in the following subsections."}, {"title": "5.1. AUC-ROC", "content": "AUC-ROC (Area Under the Receiver Operating Characteristic Curve) is a widely used performance metric in binary classification tasks, such as anomaly detection. The ROC curve visually represents a model's ability to discriminate between the normal and anomalous classes. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR), showing how well the model distinguishes anomalies from normal instances at various classification thresholds. However, since AUC-ROC does not take the false Negative Rate (FNR) into account, it is important to consider additional metrics for a more comprehensive evaluation of the model's performance."}, {"title": "5.2. AUC-PR", "content": "AUC-PR (Area Under the Precision-Recall Curve) is a performance metric used for binary classifiers, particularly effective for imbalanced datasets. It evaluates a model's ability to identify the positive class by plotting precision against recall at various thresholds. A higher AUC-PR value indicates better performance in detecting anomalies."}, {"title": "5.3. EER", "content": "EER (Equal Error Rate) is a performance metric used in binary classification that identifies the point where the FPR equals the false negative rate (FNR), meaning the rate of false positives (normal instances incorrectly classified as shoplifting) matches the rate of false negatives (failing to detect actual shopliftings). A lower EER indicates better model performance, as it reflects a balanced trade-off between these two types of errors. In applications like shoplifting detection, where it is crucial to reduce both false positives and false negatives, EER proves to be an essential metric for evaluating model effectiveness."}, {"title": "6. Model Benchmarking", "content": "We employ available state-of-the-art pose-based anomaly detection models to demonstrate the effectiveness of the developed dataset. Specifically, we utilize STG-NF [13], GEPC [22], and TSGAD [26], all of which are trained within an unsupervised learning framework. In this approach, the models are trained to minimize a specific objective that allows them to learn the normal behavior in retail environments. Once trained, the models are then evaluated on a separate test set to assess their ability to detect shoplifting incidents as anomalies.\nThe separation of training and test sets is crucial for providing an unbiased evaluation of model performance. Detailed information on this separation can be found in Tab. 2. As shown, the training set includes only normal shopping scenarios, allowing the models to learn typical customer behavior. The test set is carefully designed to maintain a balance between normal and anomalous instances, ensuring that the models are not biased toward one category. As shown in Tab. 2, the number of normal and anomalous frames are nearly identical, with 1,500 anomalous frames and 2,221 normal frames. Although the number of normal and abnormal scenarios differs, 19 normal scenarios compared to 43 shoplifting scenarios, this imbalance reflects the nature of the shoplifting scenarios, which tend to occur more rapidly than typical shopping behaviors. This training and test set division is used consistently for the training and evaluation of all models.\nIn the training phase, we used the default settings from"}, {"title": "7. Conclusion", "content": "In this study, we introduced PoseLift, a benchmark dataset that offers pose sequences of real-world behaviors for shoplifting detection, addressing the limitations of previous approaches by focusing on privacy-preserving, real-world data. By framing shoplifting detection as an anomaly detection problem, we demonstrated the feasibility of using pose data to identify anomalous behaviors associated with shoplifting. The comprehensive evaluation of state-of-the-art anomaly detection models on PoseLift highlights the potential of this approach to mitigate privacy and bias concerns. The PoseLift dataset is publicly available to support future research and development, and we are continuously collecting more data to expand further and enhance the dataset."}]}