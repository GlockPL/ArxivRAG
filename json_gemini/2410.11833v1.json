{"title": "MITIGATING SUBOPTIMALITY OF DETERMINISTIC POLICY GRADIENTS IN COMPLEX Q-FUNCTIONS", "authors": ["Ayush Jain", "Norio Kosaka", "Xinhu Li", "Kyung-Min Kim", "Erdem B\u0131y\u0131k", "Joseph J Lim"], "abstract": "In reinforcement learning, off-policy actor-critic approaches like DDPG and TD3 are based on the deterministic policy gradient. Herein, the Q-function is trained from off-policy environment data and the actor (policy) is trained to maximize the Q-function via gradient ascent. We observe that in complex tasks like dexterous manipulation and restricted locomotion, the Q-value is a complex function of action, having several local optima or discontinuities. This poses a challenge for gradient ascent to traverse and makes the actor prone to get stuck at local optima. To address this, we introduce a new actor architecture that combines two simple insights: (i) use multiple actors and evaluate the Q-value maximizing action, and (ii) learn surrogates to the Q-function that are simpler to optimize with gradient-based methods. We evaluate tasks such as restricted locomotion, dexterous manipulation, and large discrete-action space recommender systems and show that our actor finds optimal actions more frequently and outperforms alternate actor architectures.", "sections": [{"title": "1 INTRODUCTION", "content": "In sequential decision-making, the goal is to build an optimal agent that maximizes the expected cumulative returns (Sondik, 1971; Littman, 1996). Value-based reinforcement learning (RL) approaches learn each action's expected returns with a Q-function and maximize it (Sutton & Barto, 1998). However, in continuous action spaces, evaluating the Q-value of every possible action is impractical. This necessitates an actor to globally maximize the Q-function and efficiently navigate the vast action space (Grondman et al., 2012). Yet, this is particularly challenging in tasks such as restricted locomotion, with a non-convex Q-function landscape with many local optima (Figure 2).\nCan we build an actor architecture to find closer to optimal actions in such complex Q-landscapes? Prior methods perform a search over the action space with evolutionary algorithms like CEM (De Boer et al., 2005; Kalashnikov et al., 2018; Shao et al., 2022), but this requires numerous costly re-evaluations of the Q-function. To avoid this, deterministic policy gradient (DPG) algorithms (Silver et al., 2014), such as DDPG (Lillicrap et al., 2015) and TD3 (Fujimoto et al., 2018) train a parameterized actor to output actions with the objective of maximizing the Q-function locally.\nA significant challenge arises in environments where the Q-function has many local optima, as shown in Figure 2. An actor trained via gradient ascent may converge to a local optimum with a much lower Q-value than the global maximum. This leads to suboptimal decisions during deployment and sample-inefficient training, as the agent fails to explore high-reward trajectories (Kakade, 2003).\nTo improve actors' ability to identify optimal actions in complex, non-convex Q-function landscapes, we propose the Successive Actors for Value Optimization (SAVO) algorithm. SAVO leverages two"}, {"title": "2 RELATED WORK", "content": "Q-learning (Watkins & Dayan, 1992; Tesauro et al., 1995) is a fundamental value-based RL algorithm that iteratively updates Q-values to make optimal decisions. Deep Q-learning (Mnih et al., 2015) has been applied to tasks with manageable discrete action spaces, such as Atari (Mnih et al., 2013; Espeholt et al., 2018; Hessel et al., 2018), traffic control (Abdoos et al., 2011), and small-scale recommender systems (Chen et al., 2019). However, scaling Q-learning to continuous or large discrete action spaces requires specialized techniques to efficiently maximize the Q-function.\nAnalytical Q-optimization. Analytical optimization of certain Q-functions, such as wire fitting algorithm (Baird & Klopf, 1993) and normalized advantage functions (Gu et al., 2016; Wang et al., 2019), allows closed-form action maximization without an actor. Likewise, Amos et al. (2017) assume that the Q-function is convex in actions and use a convex solver for action selection. In contrast, the Q-functions considered in this paper are inherently non-convex in action space, making such an assumption invalid. Generally, analytical Q-functions lack the expressiveness of deep Q-networks (Hornik et al., 1989), making them unsuitable to model complex tasks like in Figure 2.\nEvolutionary Algorithms for Q-optimization. Evolutionary algorithms like simulated annealing (Kirkpatrick et al., 1983), genetic algorithms (Srinivas & Patnaik, 1994), tabu search (Glover, 1990), and the cross-entropy method (CEM) (De Boer et al., 2005) are employed in RL for global optimization (Hu et al., 2007). Approaches such as QT-Opt (Kalashnikov et al., 2018; Lee et al., 2023; Kalashnikov et al., 2021) utilize CEM for action search, while hybrid actor-critic methods like CEM-RL (Pourchot & Sigaud, 2018), GRAC (Shao et al., 2022), and Cross-Entropy Guided Policies (Simmons-Edler et al., 2019) combine evolutionary techniques with gradient descent. Despite their effectiveness, CEM-based methods require numerous Q-function evaluations and struggle with"}, {"title": "3 PROBLEM FORMULATION", "content": "Our work tackles the effective optimization of the Q-value landscape in off-policy actor-critic methods for continuous and large-discrete action RL. We model a task as a Markov Decision Process (MDP), defined by a tuple {S, A, T, R, y} of states, actions, transition probabilities, reward function, and a discount factor. The action space A\u2286RD is a D-dimensional continuous vector space. At every step t in the episode, the agent receives a state observation st \u2208 S from the environment and acts with at \u2208 A. Then, it receives the new state after transition st+1 and a reward rt. The objective of the agent is to learn a policy \u03c0(\u03b1 | s) that maximizes the expected discounted reward, $max_\\pi E_\\pi [\\sum_t \\gamma^t r_t]$.\n3.1 DETERMINISTIC POLICY GRADIENTS (DPG)\nDPG (Silver et al., 2014) is an off-policy actor-critic algorithm that trains a deterministic actor \u03bc\u03c6 to maximize the Q-function. This happens via two steps of generalized policy iteration, GPI (Sutton &\nBarto, 1998): policy evaluation estimates the Q-function (Bellman, 1966) and policy improvement greedily maximizes the Q-function. To approximate the arg max over continuous actions in Eq. (2), DPG proposes the policy gradient to update the actor locally in the direction of increasing Q-value,\n$Q(s, a) = r(s, a) + \\gamma E_{s'} [Q^\\mu(s', \\mu(s'))],$ (1)\n$\\mu(s) = arg \\max_a Q(s, a),$ (2)\n$\\nabla_{\\phi}J(\\phi) = E_{s \\sim \\rho^\\mu}[\\nabla_aQ(s,a)|_{a=\\mu(s)} \\nabla_{\\phi} \\mu_{\\phi}(s)].$ (3)\nDDPG (Lillicrap et al., 2015) and TD3 (Fujimoto et al., 2018) made DPG compatible with deep networks via techniques like experience replay and target networks to address non-stationarity of online RL, twin critics to mitigate overestimation bias, target policy smoothing to prevent exploitation of errors in the Q-function, and delayed policy updates so critic is reliable to provide actor gradients.\n3.2 THE CHALLENGE OF AN ACTOR MAXIMIZING A COMPLEX Q-LANDSCAPE\nDPG-based algorithms train the actor following the chain rule in Eq. (3). Specifically, its first term, $\\nabla_aQ(s, a)$ involves gradient ascent in Q-versus-a landscape. This Q-landscape is often highly non-convex (Figures 2, 3) and non-stationary because of its own training. This makes the actor's"}, {"title": "APPROACH: SUCCESSIVE ACTORS FOR VALUE OPTIMIZATION (SAVO)", "content": "output \u03bc(s) get stuck at suboptimal Q-values, thus leading to insufficient policy improvement in Eq. (2). We can define the suboptimality of the \u00b5 w.r.t. Qr at state s as\n$\\Delta(Q, \\mu, s) = arg \\max_a Q(s, a) \u2013 Q^\\pi(s, \\mu(s)) \\geq 0.$ (4)\nSuboptimality in actors is a crucial problem because it leads to (i) poor sample efficiency by slowing down GPI, and (ii) poor inference performance even with an optimal Q-function, Q* as seen in Figure 3 where a TD3 actor gets stuck at a locally optimum action ao in the final Q-function.\nThis challenge fundamentally differs from the well-studied field of non-convex optimization, where non-convexity arises in the loss function w.r.t. the model parameters (Goodfellow, 2016). In those cases, stochastic gradient-based optimization methods like SGD and Adam (Kingma & Ba, 2014) are effective at finding acceptable local minima due to the smoothness and high dimensionality of the parameter space, which often allows for escape from poor local optima (Choromanska et al., 2015). Moreover, overparameterization in deep networks can lead to loss landscapes with numerous good minima (Neyshabur et al., 2017).\nIn contrast, our challenge involves non-convexity in the Q-function w.r.t. the action space. The actor's task is to find, for every state s, the action a that maximizes Q\u2122(s, a). Since the Q-function can be highly non-convex and multimodal in a, the gradient ascent step $\\nabla_aQ(s, a)$ used in Eq. (3) may lead the actor to converge to suboptimal local maxima in action space. Unlike parameter space optimization, the actor cannot rely on high dimensionality or overparameterization to smooth out the optimization landscape in action space because the Q-landscape is determined by the task's reward. Furthermore, the non-stationarity of the Q-function during training compounds this challenge. These properties make our non-convex challenge unique, requiring a specialized actor to navigate the complex Q-landscape.\nTasks with several local optima in the Q-function include restricted inverted pendulum shown in Figure 3, where certain regions of the action space are invalid or unsafe, leading to a rugged Q-landscape (Florence et al., 2022). Dexterous manipulation tasks exhibit discontinuous behaviors like inserting a precise peg in place with a small region of high-valued actions (Rajeswaran et al., 2017) and surgical robotics have a high variance in Q-values of nearby motions (Barnoy et al., 2021).\n3.2.1 LARGE DISCRETE ACTION RL REFRAMED AS CONTINUOUS ACTION RL\nWe discuss another practical domain where non-convex Q-functions are present. In large discrete action tasks like recommender systems (Zhao et al., 2018; Zou et al., 2019; Wu et al., 2017), a common approach (Van Hasselt & Wiering, 2009; Dulac-Arnold et al., 2015) is to use continuous rep-resentations of actions as a medium of decision-making. Given a set of actions, I = {I1, ..., IN}, a predefined module R : I \u2192 A assigns each I \u2208 I to its representation R(I), e.g., text embedding of a given movie (Zhou et al., 2010). A continuous action policy \u03c0(\u03b1 | s) is learned in the action representation space, with each a \u2208 A converted to a discrete action I \u2208 I via nearest neighbor,\n$f_{NN}(a) = arg \\min_{I_i \\in I} ||R(I_i) \u2013 a||_2$.\nImportantly, the nearest neighbor operation creates a challenging piece-wise continuous Q-function with suboptima at various discrete points as shown in Figure 2 (Jain et al., 2021; 2020).\n4 APPROACH: SUCCESSIVE ACTORS FOR VALUE OPTIMIZATION (SAVO)\nOur objective is to design an actor architecture that efficiently discovers better actions in complex, non-convex Q-function landscapes. We focus on gradient-based actors and introduce two key ideas:\n1. Maximizing Over Multiple Policies: By combining policies using an arg max over their Q-values, we can construct a policy that performs at least as well as any individual policy (\u00a74.1).\n2. Simplifying the Q-Landscape: Drawing inspiration from tabu search (Glover, 1990), we propose using actions with good Q-values to eliminate or \"tabu\" the Q-function regions with lower Q-values, thereby reducing local optima and facilitating gradient-based optimization (\u00a74.2)."}, {"title": "4.1 MAXIMIZER ACTOR OVER ACTION PROPOSALS", "content": "We first show how additional actors can improve DPG's policy improvement step. Given a policy \u03bc being trained with DPG over Q, consider k additional arbitrary policies v\u2081, . . ., vk, where vi : S\u2192A and let \u03bd\u03bf = \u03bc. We define a maximizer actor \u03bc\u03bc for a\u2081 = vi(s) for i = 0, 1, . . ., k,\n$\\mu_\\mu(s) := arg \\max_{a\\in{a_0,a_1,...,a_k}} Q(s, a),$ (5)\nHere, \u03bc\u03bc is shown to be a better maximizer of Q(s, a) in Eq. (2) than \u00b5 \u2200s:\n$Q(s, \\mu_\\mu(s)) = max Q(s, a_i) > Q(s, a_0) = Q(s, \\mu(s)).$\nTherefore, by policy improvement theorem (Sutton & Barto, 1998), V\u00b5m (s) > V(s), proving that \u00b5\u2122 is better than a single \u03bc for a given Q. Appendix A proves the following theorem by showing that policy evaluation and improvement with \u03bc\u03bc converge.\nTheorem 4.1 (Convergence of Policy Iteration with Maximizer Actor). A modified policy iteration algorithm where vo = \u00b5 is the current policy learned with DPG and maximizer actor \u00b5m defined in Eq. (5), converges in the tabular setting to the optimal policy.\nThis algorithm is valid for arbitrary V\u2081, . . . Vk. We experiment with v's obtained by sampling from a Gaussian centered at \u00b5 or ensembling on \u00b5 to get diverse actions. However, in high-dimensionality, randomness around \u03bc is not sufficient to get action proposals to significantly improve \u03bc."}, {"title": "4.2 SUCCESSIVE SURROGATES TO REDUCE LOCAL OPTIMA", "content": "To train additional policies vi that can improve upon \u00b5m, we introduce surrogate Q-functions with fewer local optima, inspired by the principles of tabu search (Glover, 1990), which is an optimization technique that uses memory structures to avoid revisiting previously explored inferior solutions, thereby enhancing the search for optimal solutions. Similarly, our surrogate functions act as memory mechanisms that \"tabu\" certain regions of the Q-function landscape deemed suboptimal based on previously identified good actions. Given a known action a\u2020, we define a surrogate function that elevates the Q-values of all inferior actions to Q(s, a\u2020), which serves as a constant threshold:\n$\\Psi(s, a; a^\\dagger) = max{Q(s, a), Q(s, a^\\dagger)}.$ (6)\nExtending this idea, we define a sequence of surrogate functions using the actions from previous policies. Let a<i = {a0, a1,..., ai\u22121}. The i-th surrogate function is:\n$\\Psi_i(s, a; a_{<i}) = max \\{ Q(s, a), max_{j0)$.\nThis means the policy gradient only updates vi when Q(s, a) \u2265 \u03c4, which may slow down learning. To address this issue, we ease the gradient flow by learning a smooth lossy approximation of \u03a8\u2081.\nWe approximate each surrogate \u03a8\u2081 with a neural network . This approach leverages the universal approximation theorem (Hornik et al., 1989; Cybenko, 1989) and benefits from empirical evidence that deep networks can effectively learn non-smooth func-tions (Imaizumi & Fukumizu, 2019).\nThe smooth surrogate enables continuous gradient propagation, which is essential for optimizing the actors vi. We train ; to approach \u03a8 by minimizing the mean squared error at two critical points:\n1. \u00b5m(s) is the action selected by the current maximizer actor \u03bc\u03bc, having a high Q-value,\n2. vi(s; a<i) is the action proposed by the i-th actor conditioned on previous actions a<i>,\n$L_{approx} = E_{s \\sim \\rho} \\sum_{a \\in {\\mu_\\mu(s), v_i(s; a_{<i>})}  || \\Psi_i(s, a; a_{<i>}) \u2013 \\tilde{\\Psi}_i (s, a; a_{<i>}) || ^2$ (9)\nThis design ensures is updated on high Q-value actions and thus the landscape is biased towards those values. This makes the gradient flow trend in the direction of high Q-values. So, even when ai from v\u2081 falls in a region of zero gradients for \u03a8\u2081, in would provide policy gradient in a higher Q-value direction, if it exists. Figure 5 shows \u03a8\u2081 and  in restricted inverted pendulum task."}, {"title": "4.5 SAVO-TD3 ALGORITHM AND DESIGN CHOICES", "content": "While the SAVO architecture (Figure 4) can be integrated with any off-policy actor-critic algorithm, we choose to implement it with TD3 (Fujimoto et al., 2018) due to its compatibility with continuous and large-discrete action RL (Dulac-Arnold et al., 2015). Using the SAVO actor in TD3 enhances its ability to find better actions in complex Q-function landscapes. Algorithm 1 depicts SAVO (highlighted) applied to TD3. We discuss design choices in SAVO and validate them in \u00a76.\n1. Removing policy smoothing: We eliminate TD3's policy smoothing, which adds noise to the target action a during critic updates. In non-convex landscapes, nearby actions may have significantly different Q-values and noise addition might obscure important variations."}, {"title": "5 ENVIRONMENTS", "content": "We evaluate SAVO on discrete and continuous action space environments with challenging Q-value landscapes. More environment details are presented in Appendix C and Figure 13.\nLocomotion in Mujoco. We evaluate on MuJoCo (Todorov et al., 2012) environments of Hopper-v4, Walker2D-v4, Inverted Pendulum-v4, and Inverted Double Pendulum-v4.\nLocomotion in Restricted Mujoco. We create a restricted loco-motion suite of the same environments as in MuJoCo. A hard Q-landscape is realized via high-dimensional discontinuities that re-strict the action space. Concretely, a set of predefined hyper-spheres (as shown in Figure 6) in the action space are sampled and set to be valid actions, while the other invalid actions have a null effect if selected. The complete details can be found in Appendix C.3.1.\nAdroit Dexterous Manipulation. Rajeswaran et al. (2017) propose manipulation tasks with a dexterous multi-fingered hand. Door: In this task, a robotic hand is required to open a door with a latch. The challenge lies in the precise manipulation needed to unlatch and swing open the door using the fingers. Hammer: the robotic hand must use a hammer to drive a nail into a board. This task tests the hand's ability to grasp the hammer correctly and apply force accurately to achieve the goal. Pen: This task involves the robotic hand manipulating a pen to reach a specific goal position and rotation. The objective is to control the pen's orientation and position using fingers, which demands fine motor skills and coordination.\nMining Expedition in Grid World. We develop a 2D Mining grid world environment (Chevalier-Boisvert et al., 2018) where the agent (Figure 13) navigates a 2D maze to reach the goal, removing mines with correct pick-axe tools to reach the goal in the shortest path. The action space includes navigation and tool-choice actions, with a procedurally-defined action representation space. The Q-landscape is non-convex because of the diverse effects of nearby action representations.\nSimulated and Real-Data Recommender Systems. RecSim (Ie et al., 2019) simulates sequential user interactions in a recommender system with a large discrete action space. The agent must recommend the most relevant item from a set of 10,000 items based on user preference information. The action representations are simulated item characteristic vectors in simulated and movie review embeddings in the real-data task based on MovieLens (Harper & Konstan, 2015) for items."}, {"title": "6 EXPERIMENTS", "content": "6.1 EFFECTIVENESS OF SAVO IN CHALLENGING Q-LANDSCAPES\nWe compare SAVO against the following baseline actor architectures, where k = 3:\n\u2022 1-Actor (TD3): Conventional single actor architecture which is susceptible to local optima.\n\u2022 1-Actor, k samples (Wolpertinger): Gaussian sampling centered on actor's output. For discrete actions, we select k-NN discrete actions around the continuous action (Dulac-Arnold et al., 2015).\n\u2022 k-Actors (Ensemble): Best Q-value among diverse actions from ensemble (Osband et al., 2016).\n\u2022 Evolutionary actor (CEM): Iterative CEM search over the action space (Kalashnikov et al., 2018).\n\u2022 Greedy-AC: Greedy Actor Critic (Neumann et al., 2018) trains a high-entropy proposal policy and primary actor trained from best proposals with gradient updates.\n\u2022 Greedy TD3: Our version of Greedy-AC with TD3 exploration and update improvements.\n\u2022 SAVO: Our method with k successive actors and surrogate Q-landscapes.\nWe ablate the crucial components and design decisions in SAVO:\n\u2022 SAVO - Approximation: removes the approximate surrogates (\u00a74.4), using \u03a8\u017c instead of \u02dc.\n\u2022 SAVO - Previous Actions: removes conditioning on a0, a1,..., ai\u22121 in SAVO's actors and surrogates.\n\u2022 SAVO + Action Smoothing: TD3's policy smoothing (Fujimoto et al., 2018) compute Q-targets.\n\u2022 SAVO + Joint Action: trains an actor with a joint action space of 3 \u00d7 D. The k action samples are obtained by splitting the joint action into D dimensions. Validates successive nature of SAVO.\nAggregate performance. We utilize performance profiles (Agarwal et al., 2021) to aggregate results across different environments in Figure 7a (evaluation mechanism detailed in Appendix G.1). SAVO"}, {"title": "6.2 Q-LANDSCAPE ANALYSIS: DO SUCCESSIVE SURROGATES REDUCE LOCAL OPTIMA?", "content": "consistently outperforms baseline actor architectures like single-actor (TD3) and sampling-augmented actor (Wolpertinger), showing wide robustness across challenging Q-landscapes. In Figure 7b, SAVO outperforms its ablations, validating each proposed component and design decision.\nPer-environment results. In discrete action tasks, the Q-value landscape is only well-defined at exact action representations and nearby discrete actions might have very different values (\u00a73.2.1). This makes the Q-value landscape uneven, with multiple peaks and valleys (Figure 2). For example, actions in Mining Expedition involve both navigation and tool-selection which are quite different, while RecSim and RecSim-Data have many diverse items to choose from. Methods like Wolpertinger that sample many actions a local neighborhood perform better than TD3 which considers a single action (Figure 8). However, SAVO achieves the best performance by directly simplifying the non-convex Q-landscape. In restricted locomotion, there are several good actions that are far apart. SAVO actors can search and explore widely to optimize the Q-landscape better than only nearby sampled actions. Figure 16 ablates SAVO in all 7 environments and shows that the most critical features are its successive nature, removing policy smoothing, and approximate surrogates."}, {"title": "6.3 CHALLENGING DEXTEROUS MANIPULATION (ADROIT)", "content": "Figure 9 visualizes surrogate landscapes in Inverted-Pendulum-Restricted for a given state s. Successive pruning and approximation smooth the Q-landscapes, reducing local optima. A single actor gets stuck at a local optimum ao (left), but surrogate 1 uses ao as an anchor to find a better optimum a1. The maximizer policy finally selects the highest Q-valued action among ao, a1, a2. Figure 24 extends this visualization to Inverted-Double-Pendulum-Restricted. Figure 23 shows how one actor is sufficient in the convex Q-landscape of unrestricted Inverted-Pendulum-v4. Figures 25, 26 show how Hopper-v4 Q-landscape provides a path to global optimum, while Hopper-Restricted is non-convex.\nIn Adroit dexterous manipulation tasks (Door, Pen, Hammer) (Rajeswaran et al., 2017), SAVO improves the sample efficiency of TD3 (Figure 10). The non-convexity in Q-landscape likely arises from nearby actions having high variance outcomes like grasping, missing, dropping, or no impact."}, {"title": "6.4 QUANTITATIVE ANALYSIS: THE EFFECT OF SUCCESSIVE ACTORS AND SURROGATES", "content": "We investigate the effect of increasing the number of successive actor-surrogates in SAVO in Pendulum (Figure 11a) and MineWorld (Figure 11b). Additional actor surrogates significantly help to reduce severe local optima initially. However, the improvement saturates as the suboptimality gap reduces."}, {"title": "6.5 DOES RL WITH RESETS ADDRESS THE ISSUE OF Q-FUNCTION OPTIMIZATION?", "content": "Primacy bias (Nikishin et al., 2022; Kim et al., 2024) occurs when an agent is trapped in suboptimal behaviors from early training. To mitigate this, methods like resetting parameters and re-learning from the replay buffer aim to reduce reliance on initial samples. We run TD3 in MineEnv with either a full-reset or last-layer reset every 200k, 500k, or 1 million iterations. None of these versions outperformed the original TD3 algo-rithm without resets. This is because resetting does not help an actor to navigate the Q-landscape better and can even cause an otherwise optimal actor to get stuck in a suboptimal solu-tion during retraining. In contrast, the SAVO actor architecture specifically addresses the non-convex Q-landscapes, being a more robust method to finding closer to optimal actions."}, {"title": "6.6 FURTHER EXPERIMENTS TO VALIDATE SAVO", "content": "\u2022 Unrestricted locomotion. Figure 15 shows that both SAVO and baselines achieve optimal per-formance in simple Q-landscapes"}, {}, {"title": "7 LIMITATIONS AND CONCLUSION", "content": "Introducing more actors in SAVO has negligible influence on GPU memory, but leads to longer inference time (Table 1). However, even for 3 actor-surrogates, SAVO achieves significant improvements in all our experiments. Further, for tasks with a simple convex Q-landscape, a single actor does not get stuck in local optima, making the gain from SAVO negligible. In conclusion, we improve Q-landscape optimization in deterministic policy gradient RL with Successive Actors for Value Optimization (SAVO) in both continuous and large discrete action spaces. We demonstrate with quantitative and qualitative analyses how the improved optimization of Q-landscape with SAVO leads to better sample efficiency and performance."}]}