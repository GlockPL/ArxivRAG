{"title": "Analysis of LLMs vs Human Experts in Requirements Engineering", "authors": ["Cory Hymel", "Hiroe Johnson"], "abstract": "The majority of research around Large Language Models (LLM) application to software development has been on the subject of code generation. There is little literature on LLMs' impact on requirements engineering (RE), which deals with the process of developing and verifying the system requirements. Within RE, there is a subdiscipline of requirements elicitation, which is the practice of discovering and documenting requirements for a system from users, customers, and other stakeholders. In this analysis, we compare LLM's ability to elicit requirements of a software system, as compared to that of a human expert in a time-boxed and prompt-boxed study. We found LLM-generated requirements were evaluated as more aligned (+1.12) than human-generated requirements with a trend of being more complete (+10.2%). Conversely, we found users tended to believe that solutions they perceived as more aligned had been generated by human experts. Furthermore, while LLM-generated documents scored higher and performed at 720\u00d7 the speed, their cost was, on average, only 0.06% that of a human expert. Overall, these findings indicate that LLMs will play an increasingly important role in requirements engineering by improving requirements definitions, enabling more efficient resource allocation, and reducing overall project timelines.", "sections": [{"title": "I.INTRODUCTION", "content": "Recently, software development has been significantly impacted by AI and, more specifically, by large language models (LLMs). These advancements have reshaped how developers approach tasks across the software development lifecycle (SDLC)\u2014from requirements gathering to design, coding, testing, and maintenance. For example, LLMs have been instrumental in automating code generation, debugging, and documentation tasks, demonstrating their potential to streamline traditionally time-intensive processes.\nDespite their growing adoption, much of the research and practical applications of LLMs in software engineering have focused on later SDLC phases, such as code development (56.65% of studies), software maintenance (22.71% of studies), and quality assurance (15.14% of studies) with relatively limited exploration of their role in requirements engineering (RE), which accounts for only 3.90% of studies [1]. This disparity is striking, considering the foundational role of RE in setting the roadmap and ensuring alignment between stakeholder needs and system functionality [2, 3, 4]."}, {"title": "II.BACKGROUND ON REQUIREMENTS", "content": "Requirements engineering (RE) is fundamental to successful software development, encompassing the systematic approach to defining, documenting, and maintaining requirements throughout a project's lifecycle. As Nuseibeh and Easterbrook [5] note, RE ensures that the final product aligns with both user needs and business objectives.\nWithin RE, elicitation is a crucial first step. Zowghi and Coulin [6] describe it as the process of seeking, uncovering, acquiring, and elaborating requirements for computer-based systems. RE can be broken down into two broad categories: early requirements and late requirements [7]. Early requirements are those that are created before the software is created, while late requirements are those created once a system is deployed. In this study, we have focused on early requirements and more specifically, the first generation of requirements created."}, {"title": "III.RESEARCH OBJECTIVES", "content": "This study compares the performance of a large language model (LLM) against the performance of human experts in the early stage RE process. Our primary goal is to assess the viability of using LLMs as a tool in the early stage of RE, specifically focusing on their ability to generate meaningful epics and user stories based on limited input. We structured our research around three key questions:\nQ1: How does the perceived alignment of LLM-generated requirements compare to those produced by human experts?\nQ2: Can Participants accurately distinguish between LLM-generated and human-generated requirements?\nQ3: How does familiarity with AI tools influence perceptions of requirement quality and the ability to identify AI-generated content?\nTo address these questions, we designed a comparative study with the following parameters:\n\u2022\tParticipant Condition: Participants provided a natural language description in text format of a piece of software they wanted to build.\n\u2022\tLLM Condition: A single-shot prompt was given to an advanced LLM, asking it to generate requirements for the specified software system.\n\u2022\tHuman Expert Condition: An experienced project manager was given 1hr to generate requirements without the use of an LLM based on the same software system.\nThe outputs from these three conditions were structured as epics and user stories to maintain consistency and reflect common industry practices. To evaluate the results, we employed a mixed-methods approach: quantitative analysis of alignment scores and completeness ratings, quantitative assessment of Participants' ability to identify the source of each document, and exploration of potential correlations between Participants' AI tool experience and their perceptions of the documents.\nThis research is intended to further the understanding of how LLMs can play a role in the RE process and fill the need for empirical evidence to evaluate the performance of LLMs in a traditionally human-centered task like requirements elicitation. By directly comparing the outputs of an LLM and a human expert under controlled conditions, we provide data on the strengths and limitations of each approach."}, {"title": "IV.STUDY DESIGN", "content": "We crowd-sourced a group of business stakeholders (Participants) who were interested in having a piece of software developed. These Participants submitted their ideas via web form in text format only. Once an idea was submitted, we used the below process (Fig. 1) to create two anonymized requirements documents based on their submission.\n1.\tDocument \"01\": Created by an LLM, ChatGPT-4.0 using a single-shot prompt.\n2.\tDocument \"02\": Created by a human expert during a one-hour, closed session.\nOnce an idea was submitted, Participants were no longer able to provide input or feedback to their submission. Both documents were structured using epics and user stories to maintain consistency and reflect industry standards. This choice reflects current industry practices and provides a standardized format for comparison. Documents were titled either \"Document 01\" or \"Document 02\" and Participants were not told from which source each was generated."}, {"title": "A. SINGLE-SHOT AND ONE-HOUR", "content": "One of the harder aspects of designing this study was creating environmental parity for the LLM and human experts. LLMs can generate huge amounts of information quickly, making it hard to measure performance in a reasonable timeframe against a human expert. We decided on comparing a single-shot prompt to one hour of human-expert working time as a reasonable study baseline.\nHowever, this decision was also a limiting factor for this study. The RE process usually takes place over multiple weeks or months and a large volume of back-and-forth communication between Participants and human experts is common. In the future, this study should be expanded to review the comparative analysis over a complete RE engagement, rather than a simple prompt and time box condition. These limitations are further expanded upon below, preceding the Results section."}, {"title": "B. SELECTION OF THE LLM AND HUMAN EXPERT", "content": "LLM Selection: For this experiment, we used GPT-4 as the LLM Participant. The LLM was provided with a single- shot prompt designed to elicit comprehensive software requirements based on a given project description. The prompt given to GPT-4 is available in Appendix Fig. a-1.\nIt's important to note that in the prompt, we explicitly state the number of features to create which differs from what was provided to the human experts. In our early tests using GPT4.0 to generate categories and features, we found that without explicit instruction on a minimum number of features to create, the AI would generate very few items. With further testing, we also found that even when explicitly told to generate a minimum number of features, GPT4.0 would generate between a minimum 15 and maximum 80 features with an average of 38.16.\nHuman Expert Selection: The human experts were crowdsourced from the popular workplace site Upwork. We sought to use a different expert per Provider submission. The expert was tasked with producing requirements for the same project within a one-hour timeframe and provided a sample spreadsheet consisting of two columns, one for epics and one for features. Human experts were sourced with the following criteria:\n\u2022\tLocation: Global\n\u2022\tTalent Type: Freelancers\n\u2022\tTalent Type: Independent\n\u2022\tEnglish level: Conversational\n\u2022\tSkills and Expertise:\n\u039f\tRequirements Specification\n\u039f\tProject Management\n\u039f\tAgile Software Development\n\u039f\tProduct Requirements Document\n\u039f\tProject Requirements\nThe job posting can be found in Appendix Fig. a-2. Once the applicable agreements were in place with Upwork, the human expert was given unedited submission from the Participant. If the human expert had any further questions about the project, they were told no additional information was available and that they should use their best guess and expertise. After initial response tests with human experts, we selected a minimum of 20 features. The range count of submitted features varied with an average of 33.07."}, {"title": "C. PARTICIPANT SELECTION", "content": "In addition to crowdsourcing from the Upwork platform, Participants were also found through the social network site LinkedIn, where a general post was submitted asking anyone with a software idea to submit their concept. Participants sourced through social media were provided with the two requirements documents and a short survey. No monetary compensation was given. Due to a lack of participation, we shifted to source Participants from Upwork, where they were also given the two requirements documents and survey, but also monetary compensation for their time. Providers were also asked how frequently they used AI tools and their self- reported level of AI-tool expertise."}, {"title": "D. DATA COLLECTION PROCESS", "content": "Participants were provided with both documents (\"01\" and \"02\") without knowing which was created by the LLM and which by the human expert. They were then asked to complete a survey consisting of the following six questions:\n1.\t\"How aligned is the document with \"01\" compared to your initial idea?\" (Scale of 1-10, with 10 being most aligned and 1 being least aligned)\n2.\t\"How aligned is the document with \"02\" compared to your initial idea?\" (Scale of 1-10, with 10 being most aligned and 1 being least aligned)\n3.\t\"Who do you think created each document \"01\" and \"02\"?\" (Human or AI)\n4.\t\"How complete do you think each document is?\" (Not Complete, Fairly Complete, Fully Complete)\n5.\t\"How often do you use tools like ChatGPT, Claude, Gemini, or other large language models?\" (Daily, Weekly, Monthly, Never)\n6.\t\"What would you consider your level of \"expertise\" with tools like ChatGPT, Claude, Gemini, or other?\" (Novice, Intermediate, Advanced, Expert)\nThis survey design allowed us to collect both quantitative and qualitative data on the perceived quality and effectiveness of the LLM-generated requirements, as compared to those created by a human expert."}, {"title": "E. DATA ANALYSIS", "content": "The survey responses were collected and analyzed to evaluate the following:\nAlignment Scores: The average alignment scores for each document (01 and 02) were calculated and compared to determine which method (LLM or human) produced requirements more in line with stakeholder expectations.\nOrigin Identification: The accuracy with which Participants identified the origin of each document was analyzed to assess whether stakeholders could reliably distinguish between AI-generated and human-generated requirements. Note: In some instances, due to an unknown bug in the survey, Participants were able to answer both AI and Human. Data for those subjects were removed during direct analysis or cross- analysis.\nCompleteness Ratings: The perceived completeness of each document was analyzed to determine which method produced more comprehensive requirements.\nImpact of AI Familiarity and Expertise: The correlation between Participants' familiarity and expertise with AI tools and their responses to the survey questions was examined to understand how prior experience with AI might influence their perceptions."}, {"title": "F. LIMITATIONS", "content": "Our study had several limitations:\n1.\tSingle-instance comparison: We compared only one LLM-generated document against one human-generated document, which may not be representative of all possible outputs. Expanding to allow for multiple iterations of each document would allow for stronger generalizability.\n2.\tSubjective nature of evaluations: Perceptions of alignment and completeness are inherently subjective and may vary based on individual expectations and experiences. Future studies could incorporate more objective metrics or automated tools to evaluate the quality of requirements.\n3.\tPotential bias in self-reported expertise: Participants' self-assessment of their AI tool expertise may not accurately reflect their actual proficiency. Including independent validation of Participant expertise could address this limitation.\n4.\tSmall sample size: The small sample size (n = 50) could influence findings at a larger scale. Broader demographics and a larger Participant pool would provide more robust insights and help verify patterns observed in this study.\n5.\tSingle shot = 1hr: In order to fit into the experiment budget and resourcing, we settled on the assumption that a single shot prompt would equate to roughly 1 hour of human work. This assumption, while pragmatic, introduces variability in effort and output quality. Alternative configurations or time constraints could enhance comparability in future studies.\nThese limitations will be considered when interpreting our results and drawing conclusions."}, {"title": "V.RESULTS", "content": "A. ALIGNMENT SCORES:\nThe study found a statistically significant (t(48) = 3.179, p = 0.002) understanding of alignment score with the AI- generated document scored higher on average, with a mean difference of 1.122 (Appendix Fig. 3).\nNotably, the Al document received more consistently high scores, while the human document had a wider range of scores. Diversity in the scoring of the human documents could mean that humans produce more diverse output due to the different interpretations of the project needs and their work experiences as opposed to shared conglomerate knowledge of an LLM. This finding is consistent with other literature available on the homogenization effects of large language models [13, 14].\nUsers tended to believe solutions they perceived as more aligned were generated by human experts. Solutions perceived to be Al-generated received lower scores, with seven ratings of 3 or lower, while solutions perceived to be human-generated had only one score of 3 and none lower. Scores for what providers thought were human-generated solutions were higher (mean: 8.52) than for what they thought were AI-generated solutions (mean: 7.38)\u2014with A\u0399 scores showing greater variability (variance: 4.85 vs. 1.96 for human scores). This finding is consistent with other literature showing bias in favor of human output over AI due to multiple reasons, such as the work having less intention and less creativity [17, 18, 19, 20, 21].\nB. DOCUMENT SOURCE IDENTIFICATION\nDocument source identification showed a pattern (t(41) = 2.44, p = 0.019) in Participants' ability to distinguish between AI-generated and human-generated requirements documents.\nNotably, 60.5% of Participants correctly identified Document 01 as AI-generated, while only 41.9% accurately recognized Document 02 as human-generated. Participants chose AI as the primary document creator both times, suggesting a tendency to over-attribute content to AI sources.\nC. COMPLETENESS RATINGS\nCompleteness ratings were rather interesting with roughly the same scores across both the AI-generated and human-generated documents. We did not find any significant statistical difference between (t(94) = 0.155, p = 0.877) with Al scoring marginally higher (+10.2%) in the \"fairly complete\" category compared to the human-generated documents. Interestingly, the human-generated document received a higher proportion of \"Not Complete\" ratings (+14.3%) compared to the AI-generated document.\nThis disparity could be attributed to several factors, but is primarily due to the LLMs ability to generate a broad range of requirements quickly, covering more ground than a human expert constrained by time limitations. However, it is crucial to note that completeness in this context is subjective and based on Participants' perceptions rather than an objective measure of requirements coverage.\nThere was a significant relationship between Participants' completeness ratings and source attribution for both AI- generated documents (t(84) = 5.24, p = 1.18E-06) and human-generated documents (t(84) = 6.30, p = 1.30E-08). Of the Participants who rated the AI-generated document as \"Fully Complete\", 57.1% correctly identified it as AI- generated (Appendix Table A-3)\nConversely, of the Participants who rated the human- generated document as \"Fully Complete\", only 43.9% correctly identified it as human-generated (Appendix Table A-4). This suggests a potential cognitive bias where high levels of perceived completeness are more readily associated with Al authorship. Such a bias could have significant implications for how professionals evaluate and trust different sources of requirements documentation.\nD. IMPACT OF AI FAMILIARITY AND EXPERTISE\nOne important insight we wanted to investigate was how familiarity with AI tools might influence perceptions of requirement quality and the ability to identify AI-generated content.\nWe found, when cross-analyzing usage frequency and alignment scoring, daily users tended to rate both documents higher (t(37) = 3.407, p = 0.002, Appendix Table A-5), with a preference for the Al-generated document indicating frequent exposure to AI tools may correlate with a higher perceived alignment of AI-generated content. This may possibly be due to increased familiarity with Al output patterns or evolving expectations of document quality. We were not able to get reliable signals from the other usage groups (weekly, monthly, never).\nAnalyzing expertise and document identification, we also saw intermediate users (t(33) = 2.076, p = 0.046) correctly identify the Al-generated document 63.20% of the time while only identifying the human-generated document correctly 31.60%. Contrary to initial expectations, higher self-reported expertise in Al tools did not consistently correlate with better identification accuracy. Due to the limited study size, we are only able to reliably show that Participants with self-reported 'intermediate' Al skills 'had a significantly higher ability to recognize the Al-generated documents than human- generated documents (Appendix Table A-6)."}, {"title": "VI.DISCUSSION", "content": "We found that looking at the perceived alignment of LLM- vs. human-generated output, Participants objectively found LLM-generated content to be more aligned. A surprising find, however, was that Participants more often believed highly aligned documents to be human generated.\nThese findings are consistent with the literature, demonstrating that content origination has no impact on trustworthiness or informativeness [22]. The higher distribution of alignment scores for human-generated documents also suggests higher levels of biases due to a single source creator drawing on singular experiences as compared to the more homogenous knowledge of LLMs. While LLMs may be able to produce well informed baseline content, here they show a certain lack of creativity and intuition that is important during the RE process.\nOur second question was about Participants' abilities to identify LLM- vs human-generated documents. It was also interesting, in that both documents were over-attributed to AI. AI -generated documents were, on average, five user stories longer - which initially we expected would draw attention to LLM-created documents. Our findings were to the contrary. Other literature has shown that often vague content is attributed to LLM's [23]. Given that human Participants were only given the project description, it is likely that their output was just as vague, leading Participants to over-index on their belief that content was LLM-generated. In longer sessions, where there is more 'back and forth' between Participants and generators (LLMs and humans), we would expect identification to improve-with creativity and domain expertise being top indicators for determining document origin.\nWe also found that Participants' level of expertise with AI did not correlate to significantly higher identification results. This was also surprising as one would assume that frequent use of AI and exposure to typical LLM output patterns would make identification easier. It did not. There were also not any significant signals in the frequency of use of AI tools that led to stronger identification abilities. These two findings together highlight how LLM-generated content in the requirements engineering process could fit in without significant repercussions from stakeholders or others."}, {"title": "A. IMPACT TO REQUIREMENTS ENGINEERING", "content": "Our findings in this study highlight several key insights into the potential role of LLMs in requirements engineering (RE). At the initial stages of RE, particularly in requirements elicitation, LLMs demonstrated a significant ability to create aligned and comprehensive requirements quickly.\nToday we see LLMs serving as accelerators for requirements generation versus fully replacing human experts. By analyzing artifacts such as meeting notes, audio transcripts, and existing documentation, LLMs will be able to act as \"first draft agents.\" Human experts, leveraging their contextual understanding and domain-specific knowledge, can then validate and refine those drafts into more holistic outputs. We see this hybrid approach not only enhancing efficiency but also introducing new methods for identifying and documenting software.\nIn the future, we expect LLMs to play a much larger role in the RE process. In this study, we saw LLM-generated requirements were regarded as more aligned (+1.12) and with a trend to being more complete (+10.2%), which when coupled with the cost to generate each document (AI = $0.06, Human = $100, Appendix Table A-7). It's hard to ignore the financial incentive for companies to adopt LLMs and ultimately replace a large portion of the work human experts do today. However, this study is a small sample of the work that goes into the full process of RE.\nWe expect that humans will still play a critical role in enabling a successful RE process as LLMs and human experts possess complementary strengths in requirements generation: LLMs excel at producing comprehensive and syntactically correct requirements with human experts bringing domain knowledge, contextual understanding, and the ability to identify nuanced stakeholder needs. As LLMs (and AI) continue to evolve, they will likely take on a more integrated role in the software development lifecycle, with human experts transitioning to roles that focus on orchestration and oversight."}]}