{"title": "MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning", "authors": ["Jiazhen Pan", "Che Liu", "Junde Wu", "Fenglin Liu", "Jiayun Zhu", "Hongwei Bran Li", "Chen Chen", "Cheng Ouyang", "Daniel Rueckert"], "abstract": "Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice.", "sections": [{"title": "1 Introduction", "content": "Radiological images are fundamental to modern healthcare, with over 8 billion scans performed annually [2]. As diagnostic demand grows, the demand for efficient AI-driven interpretation becomes increasingly acute. Medical Vision-Language Models (VLMs), developed for radiological visual question answering (VQA) in MRI, CT and X-ray images, offer substantial promise in assisting clinicians/patients. Recent advances in general-purpose LLMs/VLMs (e.g., GPT-40 [16], Claude-3.7 Sonnet [3]) highlight sophisticated reasoning capabilities. However, the medical domain places an especially high premium on explainable decision-making: both clinicians/patients need to understand not just what conclusion was reached, but also why. Existing medical VLMs often provide only final answers or \"quasi-explanations\" derived from pre-training pattern matching, which do not necessarily reflect genuine, step-by-step reasoning. Consequently, ensuring interpretability and trustworthiness remains an urgent challenge in real-world clinical settings.\nWe argue that the limited reasoning capability for existing medical VLM is primarily due to the inherent drawbacks of Supervised Fine-Tuning (SFT) [1, 25, 9] which is the most common strategy for adapting large foundation models for specialized medical tasks [13, 21, 6]. Despite its simplicity, SFT faces two critical challenges: 1) An over-reliance on final-answer supervision often leads to overfitting, shortcut learning, and weaker performance on out-of-distribution (OOD) data - an issue particularly consequential in high-stake medical scenarios [11]. 2) Direct supervision with only final answers provides minimal incentive for cultivating reasoning abilities within VLMs. A possible mitigation is distilling a more capable teacher model's chain-of-thought (CoT) reasoning for SFT [31, 20]. However, constructing high-quality CoT data is prohibitively expensive to scale in specialized domains like healthcare. As a result, current medical VLMs that rely on SFT often fall short of delivering transparent explanations and robust generalizations when confronted with unfamiliar data.\nIn contrast, Reinforcement Learning (RL) [26] offers a compelling alternative for cultivating emergent reasoning by rewarding models for discovering their own logical steps rather than memorizing final answers or copying teacher CoT rationales. Indeed, a recent work SFT Memorizes, RL Generalizes [11] confirms that RL-trained models often display superior generalization compared to their SFT counterparts. However, conventional RL pipelines typically depend on auxiliary neural reward models, requiring substantial resources to continuously update both policy and reward models [35, 24]. A promising alternative, group relative policy optimization (GRPO) [27], eliminates the need for neural reward models by employing a rule-based group-relative advantage strategy (see sec. 3 for more details). This approach has demonstrated advanced reasoning, fostering capabilities while reducing computational demands in DeepSeek-R1 [12]. Despite its potential benefits for resource- and data-constrained domains like healthcare, GRPO remains largely unexplored in medical contexts.\nIn this work, we introduce MedVLM-R1, the first medical VLM capable of generating answers with explicit reasoning by training with GRPO for radiology VQA tasks. Our contributions are as follows:"}, {"title": "2 Medical VLM with Explicit Reasoning", "content": "We introduce MedVLM-R1, the first lightweight medical VLM capable of generating explicit reasoning alongside the final answer, rather than providing only the final answer."}, {"title": "3 Emerging Reasoning Without Explicit Supervision", "content": "Unlike traditional SFT methods that require data with complex reasoning steps, MedVLM-R1 is trained using GRPO with datasets containing only final answers, demonstrating emergent reasoning capabilities without explicit supervision."}, {"title": "3 Superior Generalization and Efficiency", "content": "MedVLM-R1 achieves robust generalization to out-of-distribution data (e.g. MRI \u2192 CT/X-ray) and outperforms larger models like Qwen2VL-72B and Huatuo-GPT-Vision-7B, despite being a compact 2B-parameter model trained on just 600 samples."}, {"title": "2 Related Work", "content": "Medical VLMs and Their Limitations. The rise of large-scale VLMs has spurred numerous domain-specific adaptations for healthcare, with systems such as LLaVA-Med [19] and HuatuoGPT-Vision [7] achieving impressive results in radiology VQA and related diagnostic tasks. Despite these advancements, using SFT on final-answer labels remains the dominant strategy for tailoring large models to medical domains [34, 6, 33, 5]. This approach generally requires substantial amounts of high-quality image-text data (ranging from 660k [19] to 32M samples [32]) which is costly to curate and often hampered by noise/privacy concerns. Moreover, the reliance on final-answer supervision provides limited scope for exposing a model's intermediate reasoning an important factor in building clinicians' trust. In addition, SFT-based models often overfit to narrow training distributions, leading to weaker generalization on OOD clinical scenarios.\nReinforcement Learning for Enhanced Reasoning. To mitigate SFT's limitations, RL [29, 10, 35, 24, 17] has emerged as a compelling alternative for improving model interpretability and robustness. Classic RL methods, such as proximal policy optimization (PPO) [26], have been widely adopted in text-based learning (e.g., policy shaping for LLMs) and can reward not only correctness but also the quality of intermediate reasoning steps. Recent studies suggest that while SFT \"memorizes,\" RL can help models \"generalize\" [11], offering a more stable trajectory toward domain-transferable representations. Notably, GRPO [27] extends PPO by eliminating its (neural) value function estimator and focusing on a rule-based group-relative advantage for selecting actions, showing promise in resource-constrained settings like DeepSeek-R1 [12]. Such RL-driven frameworks could be particularly beneficial for medical tasks, where limited data availability, high-stakes decision-making, and the need for explicit reasoning converge. In the following section, we will detail how these insights motivate our approach."}, {"title": "3 Methods", "content": "Overview. We leverage RL to incentivize explicit reasoning capabilities in medical VLMs, specifically employing GRPO due to its efficiency and effectiveness."}, {"title": "Group Relative Policy Optimization (GRPO)", "content": "To encourage robust, interpretable responses, we employ GRPO [27], an RL algorithm that extends PPO by focusing on a group-relative advantage instead of a learned value function. Concretely, at each training step:\n1. We sample G candidate outputs {o_i}^G_{i=1} from \\pi_{\\theta_{old}}, the model parameters before the current update.\n2. We compute a reward r_i for each output using a reward function (see next paragraph). Based on r_i we calculate a group relative advantage A_i which is normalized by the group statistics: A_i = \\frac{r_i - mean({r_1, r_2, ..., r_G})}{std({r_1, r_2, ..., r_G})}. A reward above the group average is advantaged and can further incentivize the model.\n3. Our VLM model \\pi_{\\theta} is then updated by maximizing J_{GRPO} which incorporates a clipped regularization on the relative advantage estimation for model preference alignment and training stability:\nJ_{GRPO}(\\theta) = E_{v \\sim P(V)} E_{{o_i}_{i=1}^G \\sim \\pi_{\\theta_{old}}(v)}\n\\frac{1}{G} \\sum_{i=1}^G [min(\\frac{\\pi_{\\theta}(o_i|v)}{\\pi_{\\theta_{old}}(o_i|v)} A_i, clip(\\frac{\\pi_{\\theta}(o_i|v)}{\\pi_{\\theta_{old}}(o_i|v)}, 1 \\pm \\epsilon) A_i) - \\beta D_{KL}(\\pi_{\\theta}||\\pi_{ref})]\n(3.1)\nwith ratio = \\frac{\\pi_{\\theta}(o_i|v)}{\\pi_{\\theta_{old}}(o_i|v)}. An additional Kullback-Leibler term D_{KL}(\\pi_{\\theta}||\\pi_{ref})\nis applied to penalize divergence from a reference model \\pi_{ref} (the initial checkpoint), helping prevent catastrophic forgetting. \\epsilon, \\beta \\in R^{\\ge 0} control the regularization strengths.\nReward function. Specifically, for multiple-choice medical VQA tasks, we propose a two-part rule-based reward function, inspired by [12]:\n1) Format Reward. We incentivize outputs that provide a reasoning trace within the tags <think> </think> and a succinct final answer within the tag <answer> </answer>. If all four tags are present exactly once and no content is present outside these tags, we assign a format reward of 1. Any missing/duplicated tags or content outside yield 0.\n2) Accuracy Reward. After verifying the correct format, we evaluate the correctness of the final answer. Specifically, If the letter choice A, B, C, D,... inside the <answer> </answer> tag and it responds with the ground-truth choice exactly, that is an exact match with a reward of 1 point. Further, if the letter is correct but contains additional explanations or uses the choice content instead of the corresponding letter (e.g., \"A: Pulmonary nodule\" or \"Pulmonary nodule\"), that is a partial match with a 0.5 points reward. However, if the letter does not match, is missing, or the answer is not enclosed in the answer tag, that is an incorrect or missing answer and no reward would be granted (0 points).\nThe total reward r_i \\in [0, 2] is the sum of both format and accuracy reward. By structuring the reward function in this hierarchy (format before correctness), we guide the model to first adopt the desired response structure, then refine its answer selection for accurate, interpretable medical reasoning. It is worth noting that both terms are necessary since without Format Reward, the final answer cannot be extracted while without Accuracy Reward, the model cannot converge."}, {"title": "4 Experiments", "content": "Dataset. We conduct our experiments using the HuatuoGPT-Vision [7] evaluation dataset, which is a processed and combined dataset from several publicly available medical VQA benchmarks, including VQA-RAD [18], SLAKE [22], PathVQA [14], OmniMedVQA [15], and PMC-VQA [34]. In total, the dataset comprises 17,300 multiple-choice questions linked to images covering various medical imaging modalities, with 2-6 possible choices per question. For this study, we focus on radiology modalities: CT, MRI, and X-ray. Specifically, we use 600 MRI image-question pairs for training and set aside 300 MRI, 300 CT, and 300 X-ray pairs for testing. The MRI test set is used as in-domain test, whereas the CT and X-ray test set serve as OOD test.\nImplementation details. We adopt Qwen2-VL-2B as our base VLM. This model is originally trained on data from curated web pages, open-source datasets, and synthetic sources. To adapt it to the medical domain, we employ the GRPO reinforcement learning framework outlined in Section 3. Our implementation builds on the public VLM reasoning repositories [23, 8, 28]. We perform fine-tuning on two NVIDIA A100 SXM4 80GB for 300 steps, using a batch size of 2, which takes approximately 4 hours. Generation candidate number G is set to 6. The other training optimization hyper-parameters are set as suggested by [8].\nBaseline methods and evaluation metric. We compare MedVLM-R1 with the following baselines: 1. Qwen2-VL family [4] including Qwen2-VL-2B (the unmodified base model), Qwen2-VL-7B and -72B which are the large/huge model variants. 2. HuatuoGPT-vision [7]: A medical VLM built upon Qwen2-VL-7B. 3. SFT: The same Qwen2-VL-2B base model fine-tuned with standard SFT, using the same training setting with 600 MRI question-answer pairs. We apply negative log-likelihood as the loss function to carry out the SFT training. All baselines use a simple prompting format, e.g., {Question} Your task: provide the correct single-letter choice (A, B, C, D, ...). In contrast, MedVLM-R1 uses the RL-based prompt as described in Section 3, designed to elicit explicit reasoning. For evaluation, each model receives one point for the correct single-letter answer and zero otherwise. In the test of MedVLM-R1, only the correct choice enclosed in the <answer> </answer> tag is scored as correct; any deviation from this format, even if semantically correct, results in a zero score."}, {"title": "5 Results and Discussion", "content": "Overall Performance. Table 1 summarizes both in-domain (ID) and out-of-domain (OOD) performance for various VLMs. Note that ID/OOD comparisons specifically refer to models fine-tuned on MRI data. Unsurprisingly, VLMs fine-tuned with both GRPO and SFT significantly outperform zero-shot general-purpose VLMs on in-domain tasks. Our GRPO-trained model shows very strong OOD performance, achieving a 16% improvement on CT and a 35% improvement on X-ray compared to SFT counterparts, underscoring GRPO's superior generalizability. Furthermore, despite being a compact 2B-parameter model trained on just 600 samples, MedVLM-R1 outperforms larger models like Qwen2-VL-72B and HuatuoGPT-Vision-7B, with the latter being specifically trained on large-scale medical data. This highlights the immense potential of RL-based training methods for efficient and scalable medical VLM development.\nReasoning Competence and Interpretability. Beyond strong generalization, a central strength of MedVLM-R1 is its ability to produce explicit reasoning a capability absent in all baselines. As illustrated in Figure 2, MedVLM-R1 presents a logical thought process within the <think> tag, with the final decision enclosed in the <answer> tag. Notably, for relatively simpler questions (problem 1 and 2), the reasoning appears cogent and aligned with medical knowledge. However, more complex queries sometimes reveal heuristic or just partial reasoning. For example, in the third sample, the model arrives at the correct answer via the process of elimination rather than detailed medical analysis, suggesting it leverages cue-based reasoning instead of domain expertise. Likewise, in some instances (e.g., question 4), the causal chain between reasoning and conclusion remains unclear, raising the question of whether the model merely retrofits an explanation after predicting the correct answer. Despite these imperfections, MedVLM-R1 represents a notable step toward interpretability in radiological decision-making.\nLimitations. Although MedVLM-R1 demonstrates promising results in MRI, CT, and X-ray datasets, several limitations remain: 1. Modality Gaps: When tested on other medical modalities (e.g., pathology or OCT images), the model fails to converge. We hypothesize this arises from the base model's insufficient exposure to such modalities during pre-training. 2. Closed-Set Dependence: The current approach is tailored to multiple-choice (closed-set) VQA. In open-ended question settings where no predefined options are provided, the model's performance degrades substantially. This is also a common challenge for many VLMs. 3. Superficial/hallucinated Reasoning: In some reasoning cases, MedVLM-R1 provides a correct answer without offering a meaningful reasoning process (e.g., <think>To determine the correct observation from this spine MRI, let's analyze the image.</think><answer>B</answer>). Moreover, sometimes the model concludes a correct choice while providing an inference that can lead to another answer. This phenomenon underscores that even models designed for explainability can occasionally revert to superficial/hallucinated justifications, highlighting an ongoing challenge in generating consistently transparent and logically sound rationales. Regarding all these issues, we believe the current 2B-parameter scale of our base model constitutes a potential bottleneck, and we plan to evaluate MedVLM-R1 on larger VLM backbones to address these concerns."}, {"title": "6 Conclusion", "content": "We present MedVLM-R1, a medical VLM that integrates GRPO-based reinforcement learning to bridge the gap between accuracy, interpretability, and robust performance in radiology VQA. By focusing on explicit reasoning, the model fosters transparency and trustworthiness-qualities essential in high-stakes clinical environments. Our results demonstrate that RL-based approaches generalize better than purely SFT methods, particularly under OOD settings. Although VLM-based medical reasoning is still at a nascent stage and faces considerable challenges, we believe that its potential for delivering safer, more transparent AI-driven healthcare solutions will be appreciated and should be encouraged."}]}