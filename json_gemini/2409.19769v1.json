{"title": "Adaptive Event-triggered Reinforcement Learning Control for Complex Nonlinear Systems", "authors": ["Umer Siddique", "Abhinav Sinha", "Yongcan Cao"], "abstract": "In this paper, we propose an adaptive event-triggered reinforcement learning control for continuous-time nonlinear systems, subject to bounded uncertainties, characterized by complex interactions. Specifically, the proposed method is capable of jointly learning both the control policy and the communication policy, thereby reducing the number of parameters and computational overhead when learning them separately or only one of them. By augmenting the state space with accrued rewards that represent the performance over the entire trajectory, we show that accurate and efficient determination of triggering conditions is possible without the need for explicit learning triggering conditions, thereby leading to an adaptive non-stationary policy. Finally, we provide several numerical examples to demonstrate the effectiveness of the proposed approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Event-triggered control constitutes a paradigmatic ap-proach to control system implementation, wherein data ex-change between the plant and its controller is precipitated by the satisfaction of a state- or output-dependent criterion. The underlying rationale is to facilitate communication between the plant and its controller solely when necessitated by the pursuit of desired control objectives, diverging from traditional time-triggered (periodic) strategies that schedule com-munication instants based on elapsed time rather than actual system requirements. The motivation behind event-triggered control lies in its applicability to resource-constrained scenarios, where the costs associated with communication, computation, and control input updates are non-negligible, and the optimal sampling rate is unknown or varies over time, such as in networked control systems and embedded systems.\nWhile there exists a multitude of works addressing the design of event-triggered controllers (see, for example, [1] and references therein), a vast majority of them require knowledge of the system model to be controlled. In practice, it is hard to model general nonlinear systems with complex interactions (e.g., when modeling via first principles may not be tractable and or a desired level of accuracy in system identification cannot be reached due to noisy data), and model-free control strategies appear alluring in such cases. A few previous works have presented data-driven techniques to improve and augment event-triggered control and state estimation, e.g., [2]\u2013[5]. In these studies, learning is leveraged to approximate intractable conditional probability densities that emerge in decentralized optimization problems or to derive tractable solutions to Hamilton-Jacobi-Bellman equations that yield optimal control policies. This is achieved through various methodologies, including model-free approaches such as Q-learning, which eschew explicit system modeling in favor of iterative learning, as well as neural network-based methods that harness the representational power of artificial neural networks to approximate complex value functions or control policies.\nCurrently, only a few works are available for data-driven event-triggered control, e.g., [4], [6]-[9] where discrete-time formulation has been presented. On the contrary, the formulation in [10] is continuous-time. It is worth noting that the learning process, particularly for linear time-invariant systems, simplifies in [6], [7], [9] by ignoring the effect of disturbances in the offline data collection process. Conversely, the approach proposed in [8] adopts a more realistic paradigm, wherein the controller and triggering policy are designed solely based on a single batch of noisy data collected from the system, thereby introducing an additional layer of complexity due to the presence of disturbances and measurement errors. On the other hand, disturbances are taken into account during both the learning phase and closed-loop operation in the work of [10], where a dynamic triggering strategy is introduced to guarantee the preservation of L2 stability.\nAs a sequential decision-making strategy, deep reinforcement learning (RL) has been shown to be successfully applied to many control problems in, e.g., robotics [11]. However, the typical focus in such problems is on the design of control policies only while the consideration of communication cost is often overlooked. There are a few studies where model-free RL-based design has been presented for event-triggered control. For example, in [12], an actor-critic method has been presented to learn an event-triggered controller with a predefined communication trigger. This essentially implies that the decision about when to communicate is not learned from scratch. The work in [13] considered a fixed error threshold for communication triggering and leverages approximate dynamic programming to learn an event-triggered controller, whereas both the model of the system and an optimal event-triggered controller with fixed communication trigger have been simultaneously learned in [14]. The work"}, {"title": "II. BACKGROUND AND PRELIMINARIES", "content": "We consider a general nonlinear system subject to bounded uncertainty,\n$\\dot{x} = f(x) + b(x)u + d,$\n(1)\nwhere $x \\in \\mathbb{R}^n$ is the state, $u \\in \\mathbb{R}^m$ is the control input at time $t \\in \\mathbb{R}_{\\geq 0}$ and $d \\in \\mathbb{R}^n$ denotes the exogenous disturbance such that $||d|| \\leq d_{\\text{max}}$ for some $d_{\\text{max}} \\in \\mathbb{R}_{+}$. Without loss of generality, we assume that the nonlinear unknown function, $f(x)$, is locally Lipschitz, that is, $||f(x_1) - f(x_2)|| \\leq L||x_1 - x_2||$ for some $x_1, x_2 \\in \\mathbb{R}^n$, $L \\in \\mathbb{R}_{+}$, and for all time $t \\in \\mathbb{R}_{\\geq 0}$. The goal is to learn a suitable policy $u = K(x)$ to stabilize (1) while considering resource-aware scheduling of control and communication. The resource-aware controller is implemented using zero-order hold devices, leading to the control input $u(t) = u(t_k)$, for all $t \\in [t_k, t_{k+1})$ such that the inter-event time $\\Delta = t_{k+1} - t_k$ is not necessarily fixed for all $k \\in \\mathbb{N} \\cup \\{0\\}$. Later, we show that $\\Delta$ is lower bound by a positive constant, thus eliminating the possibility of the Zeno phenomenon.\nWithout loss of generality, we assume that the scheduling is initiated at $t = 0$, thus $t_0 = 0$ and the sequence of event times can be expressed iteratively as $t_{k+1} = \\inf \\{ t : t > t_k, \\Phi (t, \\hat{x}(t), x(t)) \\geq 0 \\}$ for some triggering condition $\\Phi$, which is a function of the true state at the triggering instant $x(t)$ and the last broadcast state $\\hat{x}(t)$. Then, one may note that in event-triggered scheduling, the control effort at the next triggering instant takes the form,\n$u(t) =\\begin{cases}u(t_k); &\\text{if } \\Phi (t, \\hat{x}(t), x(t)) \\geq 0 \\quad \\forall t \\in [t_k, t_{k+1}),\\\\u(t_{k-1}); & \\text{otherwise,}\\end{cases}$\n(2)\nwhere $u(t_k) = K(x(t_k))$ is the control computed at the triggering instant $t_k$, $\\Phi \\geq 0$ is the triggering condition that decides when to schedule a control input, and $\\hat{x}(t_k)$ is the last broadcast state. Specifically, one has $\\hat{x}(t) = x(t_k), \\quad \\forall t \\in [t_k, t_{k+1})$. In the proposed approach, we aim to jointly learn both the control policy $u(t)$ as well as the triggering condition $\\Phi$ (which essentially translates to the communication policy or when to communicate). To this end, we present a brief overview of reinforcement learning below."}, {"title": "A. Reinforcement Learning", "content": "Reinforcement Learning (RL) is a framework where an autonomous agent learns to make decisions through interactions with an environment that is typically unknown. These interactions occur sequentially at each time step $t_k$, where the agent observes the current state $x(t_k)$, selects an action $u(t_k)$, receives a reward $r(t_k)$, and transitions to the next state $x(t_{k+1})$. This process is usually modeled as a Markov Decision Process (MDP) [17], which is defined by the tuple $\\langle \\mathcal{X}, \\mathcal{U}, P, r, \\gamma \\rangle$. Here, $\\mathcal{X}$ and $\\mathcal{U}$ are the state and action spaces, $P$ is the state transition probability function, $r$ is the reward function, and $\\gamma \\in [0, 1)$ is the discount factor that determines the importance of future rewards.\nThe agent's goal is to learn an optimal policy $\\pi$ that maximizes the expected cumulative discounted reward, $E_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_t]$. Here the expectation is taken over the stochastic outcomes of states and actions under the policy $\\pi$, which maps states to a probability distribution over actions. A policy can be either deterministic, $\\pi(x) = u$, or stochastic,"}, {"title": "III. PROPOSED APPROACH", "content": "In this section, we introduce the adaptive event-triggered proximal policy optimization (ATPPO) method, which extends the standard proximal policy optimization (PPO) [19] algorithm to jointly learn both the control policy and the triggering condition. This unified approach addresses the limitations of traditional event-triggered control methods that rely on triggering conditions designed manually or based on state differences by integrating the learning of these aspects into a single framework. While our approach is general and can be applied to any RL method, we specifically choose PPO due to its robustness, stability, and strong empirical performance across various tasks [20].\nIn traditional event-triggered control, the triggering condition is often manually designed or based on state differences and is treated separately from the control policy. ATPPO jointly learns both the control action $u(t)$ and the triggering condition $\\Phi$ as part of the policy $\\pi(x)$ where $x$ is the augmented state that includes accumulated rewards and past actions. By incorporating the triggering condition $\\Phi$ within the policy, ATPPO optimizes the control strategy while dynamically determining when communication should occur, effectively reducing the number of parameters and computational overhead when learning them separately.\nSimilar to the PPO, ATPPO is a policy gradient method designed to directly optimize policies in deep RL. In other words, it directly explores the policy space instead of learning a Q-function, which is typically defined by parameterized functions, such as deep neural networks. The parametric policy is denoted as $\\pi_{\\theta}$, where $\\theta$ represents the policy parameters. Thus, the goal of ATPPO is to maximize the expected sum of rewards akin to the standard RL goal, however with an additional penalty mechanism to discourage excessive triggering. If the triggering condition $\\Phi$ is always true, meaning the agent tends to communicate at every time step, a penalty is imposed to encourage the agent to make more judicious decisions about when to trigger communication. This penalty promotes more efficient use of communication resources by optimizing the balance between control performance and communication frequency.\nFormally, in ATPPO, the policy is trained to learn both the control action and the triggering condition, that is,\n$(\\Phi, u) = \\pi(x),$\nwhere $x = (x, r_{\\text{acc}})$ represents the augmented state. The augmentation of the state with accrued rewards allows the agent to consider historical information when making decisions, enabling a more comprehensive and accurate determination of both control actions and triggering conditions. Thus, the optimization objective in ATPPO can be expressed as\n$J(\\theta) = E_{u \\sim \\pi_{\\theta}(\\cdot|x)} \\left[ \\min \\left( \\rho_{\\theta} A(x, u), \\text{clip}(\\rho_{\\theta}, 1-\\epsilon, 1+\\epsilon) A(x, u) \\right) - \\Psi \\cdot \\mathbb{I}(\\Phi > 0) \\right],$\n(3)\nwhere $\\rho_{\\theta} = \\frac{\\pi_{\\theta}(u|x)}{\\pi_{\\theta_{\\text{old}}}(u|x)}$, $\\pi_{\\theta_{\\text{old}}}$ represents the policy generating the transitions, $\\epsilon$ is a hyperparameter that controls the constraint, and $\\Psi$ is a hyperparameter controlling the penalty for frequent triggering. The indicator function $(\\Phi > 0)$ applies the penalty when the triggering condition is met. Here, $A$ denotes an advantage function that quantifies the relative value of taking a specific action $u$ in the current state $x$ under the policy $\\pi_{\\theta}$.\nSince we learn with a gradient-based approach, learning or estimating the Q-value function can have a higher variance as it estimates the expected total reward from taking a specific action in a given state and then following the current policy. To reduce the variance, the advantage function is used, which subtracts the state value $V(x_t)$ and helps cancel out some of the variability that is common to all actions in a given state. In ATPPO, the advantage $A$ is estimated using $\\lambda$-returns [21], and can be defined as\n$A(x, u) = \\sum_{t=1}^{\\infty} (\\gamma \\lambda)^{t-1} r_t + \\gamma V_\\phi(x') - V_\\phi(x),$\n(4)\nwhere $x' = x + \\delta t$ is the next state. The advantage function and value function are learned with the augmented state that enhances the ability of the advantage function and value function to capture the agent's history, allowing for more informed decisions that consider long-term consequences rather than just immediate rewards. This approach also leads to"}, {"title": "IV. ENVIRONMENTS FOR EXPERIMENTS", "content": "To demonstrate the effectiveness of ATPPO, we conduct experiments in various environments that include perturbed single integrator dynamics, MuJoCo environments [22] (such as Half-Cheetah, Hopper, Reacher), and in a target capture scenario where a pursuer captures a moving target [23]. Below, we first provide the details of each environment, followed by the results and discussion in the next section."}, {"title": "A. Single Integrator", "content": "In our initial experimental phase, we conducted experiments on a single integrator dynamics perturbed by a bounded uncertainty, which serves as a preliminary benchmark due to its simple dynamics that models a wide class of phenomena. This environment can be represented as\n$\\dot{x} = u + d,$\n(5)\nwhere $||d|| \\leq d_{\\text{max}}$. In this environment, the agent is tasked with stabilizing the system in the presence of uncertainties, that is, to drive the system's state to the desired equilibrium point. Specifically, the state variable is required to converge to the origin from an initial value, $x(0) = 5$. In this situation, the agent receives the current state $s_t$ and takes an action $a_t$ to adjust the state incrementally over time, with the goal of minimizing the distance between the current state and the origin (the equilibrium state). The reward function encourages the agent to reduce the absolute value of the state (i.e., getting closer to the equilibrium point) while penalizing large or unnecessary control actions. This environment favors intuition, simplicity of presentation, and a framework for validating the agent's learning capacity. Specifically, it allows us to test the proposed ATPPO algorithm in a minimalistic setting, focusing on how efficiently the agent learns to stabilize the system using optimal event-triggered control actions. The transition dynamics are governed by (5), where the agent interacts with the system through control actions that influence the state evolution."}, {"title": "B. Half-Cheetah", "content": "The Half-Cheetah environment represents a planar biped robot with a 17-dimensional state space and a 6-dimensional action space. The state vector comprises joint angles and velocities, while the action space consists of joint torques. The reward function maximizes the forward velocity and minimizes control effort, which in other words, encourages the cheetah to move faster while penalizing control effort. The transition function governs the dynamics of a bipedal robot to move to the next state and provides a realistic scenario for evaluating locomotion efficiency."}, {"title": "C. Hopper", "content": "The Hopper environment is highly nonlinear and models a monopedal robot with an 11-dimensional state space and a 3-dimensional action space. The state vector includes body positions and velocities, and the action space consists of joint torques. The reward function is based on forward velocity and control costs. The transition function includes the dynamics of the monopedal about how it moves, allowing for realistic simulations of hopping and balance."}, {"title": "D. Reacher", "content": "The Reacher environment simulates a two-degree-of-freedom robotic arm with nonlinear dynamics. Its state space includes arm joint angles, velocities, and target position, while the action space is a 2-dimensional vector of joint torques. The reward function is based on the distance between the end effector and the target. In this environment, the state transitions are governed by physics-based simulations, which model the complex nonlinear dynamics of the robotic arm which ensures realistic control and target reaching tasks."}, {"title": "E. Target Capture", "content": "In the target capture case, we consider one or more pursuers (denoted as $P_i$) and one or more targets (denoted as $T_j$). Each vehicle $k$ $(\\forall k \\in \\{P_i, T_j\\} | i,j \\in \\mathbb{N})$ is nonholonomic, with its motion described by the following equations:\n$\\dot{X}_k = v_k \\cos \\psi_k,$\n(6)\n$\\dot{Y}_k = v_k \\sin \\psi_k,$\n(7)\n$\\dot{\\psi}_k = a_k,$\n(8)\nwhere $[X_k, Y_k] \\in \\mathbb{R}^2$ represents the instantaneous po-sition, $\\psi_k \\in (-\\pi,\\pi]$ is the heading angle, and $v_k$ is the speed of the $k^{th}$ vehicle. The lateral acceleration $a_k$ steers the vehicle and accounts for its turning constraints. The system includes a first-order autopilot lag model to simulate real-world conditions:\n$\\frac{a_{p, \\text{achieved}}}{a_{p, \\text{commanded}}} = \\frac{1}{1 + sT_i},$\n(9)\nwhere $\\tau_i$ is the corresponding time constant.\nFor pursuer-target pairs, it is often beneficial to express these dynamics in a relative frame of reference, particularly in scenarios where absolute measurements are challenging or costly to obtain. The kinematics of relative motion between a pursuer $P_i$ and a target $T_j$ can be described as\n$\\dot{r}_{ij} = v_{T_j} \\cos (\\psi_{T_j} - \\eta_{ij}) - v_{P_i} \\cos (\\eta_{P_{ij}} - \\eta_{ij}),$\n(10a)\n$\\dot{\\eta}_{ij} = \\frac{v_{T_j}}{r_{ij}} \\sin (\\psi_{T_j} - \\eta_{ij}) - \\frac{v_{P_i}}{r_{ij}} \\sin (\\eta_{P_{ij}} - \\eta_{ij}),$\n(10b)\n$\\dot{\\sigma}_{p_{ij}} = \\sigma_{p_{ij}} - \\eta_{ij},$\n(10c)\nwhere $v_{r_{ij}}$ and $v_{\\eta_{ij}}$ are the components of the relative velocities along and across the line-of-sight, respectively. The"}, {"title": "V. RESULTS", "content": "In this section, we present the results of our experiments across various environments to demonstrate the effectiveness of the proposed ATPPO method. Through these experiments, we try to answer the following questions: (A) Why is event-triggered learning necessary, and how does it help to save resources while stabilizing complex nonlinear systems? (B) How does the proposed ATPPO method perform in complex, nonlinear, and highly uncertain robotic environments (such as those that require rendezvous or target capture)? (C) Can the proposed ATPPO approach generalize to high-dimensional, multi-degree-of-freedom MuJoCo tasks?\nQuestion (A) To answer this question, we first perform experiments on a perturbed single integrator. The goal of this experiment is to investigate how ATPPO's resource-aware scheduling mechanism minimizes the frequency of control actions, thereby saving communication and computation resources while still effectively stabilizing the system to the equilibrium state. Fig. 1 presents the performance comparison between PPO and the proposed ATPPO method for this case. The initial condition x(0) = 5, and the agent is tasked to drive the initial state to the equilibrium x(0) = 0 to stabilize the system. The results show that, compared to standard PPO, ATPPO is able to stabilize the system while saving at least 80% of resources and requires significantly less communication. Fig. 1a shows the decay of the Lyapunov function 0.5x2 with over time. Both PPO and ATPPO allow for the gradual reaching of the equilibrium point, demonstrating their ability to stabilize the system. Fig. 1b presents the communication frequency with the policy, showing that ATPPO significantly reduces the number of communication events compared to PPO. Despite fewer triggering instances, the system remains stable, indicating that ATPPO effectively balances resource savings and performance. Finally, Fig. 1c shows the inter-event time between consecutive decisions and confirms that it remains non-zero and varies over time, ensuring that the triggering frequency is dynamically adjusted without leading to infinite triggerings.\nQuestion (B) To answer this question, we conduct experiments where a pursuing vehicle needs to rendezvous with a target vehicle, which is moving. Fig. 2 shows the performance comparison of PPO and ATPPO in capturing a single target moving with a constant speed of 20 m/s and a heading angle of 40\u00b0. The pursuer has an initial speed of 40 m/s and"}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we introduced the adaptive event-triggered proximal policy optimization (ATPPO) approach, a novel method for jointly learning control and communication policies in continuous-time non-linear systems subject to bounded uncertainties. By integrating the learning of both policies and augmenting the state space with accrued rewards, ATPPO enables the learning of a non-stationary policy that optimizes behavior over entire trajectories. This approach facilitates more accurate and efficient determination of triggering conditions without explicit training while reducing computational complexity and parameter count. Through several illustrative simulations, we demonstrated ATPPO's effectiveness in complex nonlinear systems where resource efficiency is crucial, showcasing its potential for real-world applications. Our work represents a unique and efficient approach to learning event-triggered controllers, offering a more cohesive method for simultaneously optimizing communication and control policies in complex systems. As a future work, we plan to extend ATPPO to multi-agent RL in both centralized and distributed settings."}]}