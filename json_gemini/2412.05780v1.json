{"title": "BudgetFusion: Perceptually-Guided Adaptive Diffusion Models", "authors": ["Qinchan (Wing) Li", "Kenneth Chen", "Changyue (Tina) Su", "Qi Sun"], "abstract": "Diffusion models have shown unprecedented success in the task of text-to-image generation. While these models are capable of generating high-quality and realistic images, the complexity of sequential denoising has raised societal concerns regarding high computational demands and energy consumption. In response, various efforts have been made to improve inference efficiency. However, most of the existing efforts have taken a fixed approach with neural network simplification or text prompt optimization.\nAre the quality improvements from all denoising computations equally perceivable to humans? We observed that images from different text prompts may require different computational efforts given the desired content. The observation motivates us to present BudgetFusion, a novel model that suggests the most perceptually efficient number of diffusion steps before a diffusion model starts to generate an image. This is achieved by predicting multi-level perceptual metrics relative to diffusion steps. With the popular Stable Diffusion as an example, we conduct both numerical analyses and user studies. Our experiments show that BudgetFusion saves up to five seconds per prompt without compromising perceptual similarity. We hope this work can initiate efforts toward answering a core question: how much do humans perceptually gain from images created by a generative model, per watt of energy?", "sections": [{"title": "1. Introduction", "content": "Diffusion models have been shown to produce images and videos of increasingly high fidelity [14]. However, this capability comes at the cost of significant computational demands due to the large number of inference steps required to generate images of high quality, which each require evaluation of large neural networks commonly having over 1 billion parameters [47]. The substantial computational burden of diffusion models has prohibited on-device deployment and led to societal concerns about their energy consumption and impact on the environment [18, 20, 49].\nA significant body of recent literature has attempted to improve the efficiency of denoising diffusion models, such as reducing the number of denoising steps via neural network simplifications [23, 25] and model distillation [28, 37, 48, 50]. However, such one-size-fits-all approaches"}, {"title": "2. Related Work", "content": "inevitably lead to images generated with inconsistent quality depending on the desired content reflected in the text prompts. For example, consider the two prompts: 1) \u201ca white and empty wall\" vs. 2) \"a colorful park with a crowd\" as shown in Figure 2. Intuitively, the two prompts may require different number of inference steps to generate images of high quality. With this inspiration, we present BudgetFusion, a perceptually-aware guidance model for text-to-image diffusion models. Specifically, for a given text prompt, BudgetFusion estimates the correlation between various visual quality metrics and number of diffusion inference steps as a time series function. The estimation then suggests the optimal number of denoising steps with a gradient-based analysis. That is, we determine the plateau point where additional denoising steps will benefit only marginally in terms of visual perception. To achieve this goal, we first created a large-scale generative dataset using more than 18,000 text prompts \u00d7 12 timesteps. By leveraging this data, we can measure perceptual similarity at various scales, ranging from per-pixel errors to spatial layout and semantics. From this perceptual assessment of generative images and the Markov Chain nature of diffusion models, we developed a long short-term memory (LSTM) neural network [11] that predicts the relationship between quality enhancement and diffusion steps.\nWe conduct a series of objective analyses and subjective user studies. Our findings demonstrate BudgetFusion's effectiveness in significantly enhancing the computational efficiency of diffusion models, measured as perceptual gain per diffusion step. Moreover, this enhancement is achieved without compromising subjectively perceived quality. We hope this work will spark a series of future initiatives on human-perception-centered approaches for computation- and energy-friendly generative models\u00b9."}, {"title": "2.1. Optimizing Generative Diffusion Models", "content": "Diffusion models have recently exploded in popularity due to their high performance on tasks such as image and video generation, audio generation, and 3D shape generation [14, 46]. Latent diffusion models [47] have significantly improved training and inference efficiency, but still require a high number of forward denoising neural network evaluations to produce high-quality results. To tackle this problem, an extensive body of literature has been proposed to optimize and accelerate diffusion models from different perspectives. For example, optimizing the sampling strategy may enable more efficient denoising computation [2, 23, 25], such as timestep integration [40] or conditioning on the denoising [44]. Additionally, by approximating the direct mapping from the initial noise to generated im-"}, {"title": "2.2. Perceptually-Guided Computer Graphics", "content": "A wealth of literature in the graphics community has studied applications of human perceptual data to optimize visually-based algorithms. For example, several image quality metrics have been proposed which generally agree with experimental data, such as SSIM [56] or PSNR, or neural network-based metrics like LPIPS [60]. Metrics which are based on psychophysical models are more highly correlated with human responses, and can model temporal, high luminance, and foveated artifacts [7, 33, 35]. Recently, metrics such as DreamSim [9] have been proposed to determine higher-order image differences, such as layout or semantics."}, {"title": "3. Method", "content": "We develop BudgetFusion to suggest the optimal number of inference, or denoising, steps based on predicted perceptual quality. Toward this aim, and as visualized in Figure 3, we first created a dataset of images generated with different numbers of denoising steps (Section 3.1). To comprehensively measure the perceptual quality of this large-scale synthetic dataset, we leveraged perceptual metrics at various levels of detail, including their pixel-wise error, layout, and overall semantic similarity with respect to \"reference\" images generated with a large number of inference steps (Section 3.2). With the jointly labelled data of text prompt, perceptual scores, and denoising steps, we train an LSTM-based BudgetFusion model to predict metric scores against denoising steps as a time series function, given input prompts (Section 3.3). Lastly, we employ BudgetFusion to estimate the least required number of denoising steps which produces high-quality images using plateau point estimation (Section 3.4)."}, {"title": "3.1. Synthetic Dataset Generation", "content": "We first generated an image dataset by running the forward pass of a diffusion model at different number of denoising steps in order to understand the change in perceptual quality throughout timesteps. To this end, we sample a large set of text prompts and denoising steps to be used to generate images with a diffusion model.\nWe used the popular pre-trained Stable Diffusion 2 model (SD)\u00b2, a variant of Stable Diffusion [47], as a drop-in to demonstrate BudgetFusion. Stable Diffusion 2 generates high-quality images despite faster inference time compared to larger models, like Diffusion XL. Note that BudgetFusion may be applied to other diffusion model architectures without loss of generality.\nCreating and sampling prompts. To obtain prompts that cover a wide range of semantics from real-world scenarios, we use an existing image-caption pair dataset, COCO [26]. However, the dataset contains more than 330,000 images, making it computationally infeasible to generate a corresponding image for each of the captions as a prompt using diffusion models. Therefore, we systematically sample the most representative captions using the Cosine dissimilarity score $C_{clip}$ in the CLIP-encoded space [45], where higher\n$S(p,p') < \\hat{S}, \\forall p, p' \\in P$.\nWe experimentally set the value of $\\hat{S}$ to .75, and acquired 18,384 prompts to compose P. The sampled text prompts are then used to generate images at different denoising steps using SD."}, {"title": "Sampling denoising timesteps.", "content": "In most existing diffusion models, the computation could be reduced by skipping denoising timesteps with the cost of reduced quality. Therefore, the main aim of BudgetFusion is to suggest the most efficient trade-off between perceptual quality and computation cost based on a specific text prompt. Toward this aim, for all text prompts in P we generate images at various denoising steps to make a systematic measurement of the change in image quality at each step, as shown in Figure 3b. Because it is computationally infeasible to perform image generation for all possible timesteps, we consider a power law as guidance to sample the diffusion timestep set $T := \\{t_1,...,t_n\\}$. Specifically, we select $t_1 = 1 + 2^{i-1}$, and additionally include a boundary timestep $\\{t_1 = 1\\}$ and empirically sample intermediate conditions, $\\{2^2, 2^7, 4^2\\}$, to further increase the sampling density. We sample this range up to 129 (i = 8), which is above the commonly accepted sufficient range of 100 inference steps for Stable Diffusion [39]. This sampling strategy results in N = 12 time steps in total."}, {"title": "Scheduler and generation.", "content": "All the experiments in our work use the widely used Euler Scheduler [19]. We make this choice due to the scheduler's more consistent and cohesive evaluation results because of its fixed start (1) and end (1000) timesteps. We have 4 fixed random seeds as the generator of the initial noise and PyTorch manual random seeds that control the stochastic denoising process. Using the sampled conditions, we generate an image $I_{p,t}$ from each of the selected prompts in P at each representative time step in T and 4 random seeds using SD. Overall, the dataset contains 18,384 (p) \u00d7 12 (N) \u00d7 4(seeds) = 882, 432 images of resolution 768 \u00d7 768. We reserved the images from 90% of randomly sampled prompts as the training set (Section 3.3) and the remaining 10% for evaluation (Section 4)."}, {"title": "3.2. Multi-Scale Perceptual Metrics", "content": "We measure the perceptual quality throughout the synthetic dataset of an image I corresponding to prompt p and timestep t, denoted as $\\{I_{p,t}\\}$. The denoising process over t iteratively removes noise to generate the final image. Each step jointly enhances local details and/or global image-text alignment [58]. Therefore, image quality should be measured at various perceptual scales in terms of level of detail and semantics. To this end, we propose to leverage three perceptual metrics to compare images generated at each step. First, we measure the most detailed pixel-level quality using Laplacian signal-to-noise ratio (L-SNR) [21]. Then, we leverage the recent learning-based layout-aware Dream-Sim metric (D-SIM) [9] to measure mid-level content similarity. Lastly, we measure the high-level semantic alignment using the Cosine similarity in CLIP-encoded latent space (I-CLIP) [45]. Both D-SIM and I-CLIP metrics compare each image $I_{p,t}$ against the target image ($I_{p,t_N, t_N = 129}$).Pixel-level: Laplacian signal-to-noise ratio (L-SNR \u2208 [0,1] \u2193) An insufficient number of forward denoising steps may generate noisy and distorted images [58]. Therefore, we first measure each image's pixel-level quality. The signal-to-noise ratio (SNR) is a widely adopted metric for image quality [21] and deep metric learning [59]. To our aim, we employ the SNR as a no-reference L-SNR metric, which assesses the SNR by comparing the original image to its Laplacian-filtered version, thereby evaluating its sharpness [42].\nL-SNR (p, t) := SNR ($I_{p,t}$, G \u2297 $I_{p,t}$), where G and \u2297 are the Gaussian kernel (\u03c3 = 1) and convolution operator, respectively.We developed this measurement instead of using reference-based image quality metrics (e.g., PSNR) against the target image $I_{p,t_N}$ due to the stochastic and random nature of diffusion model-based generation, where pixel-level values can significantly vary based on the added random noise.\nMid-level: DreamSim (D-SIM\u2208 [0,1] \u2193) The DreamSim metric was recently proposed by Fu et al. [9]. It measures mid-level perceptual similarity between two images, considering non-pixel factors such as spatial layouts. We compute the D-SIM metric as\nD-SIM(p, t) := Ds($I_{p,t}$, $I_{p,t_N}$),\nwhere $D_S$ indicates the DreamSim distance."}, {"title": "Semantic-level: image caption encoded by CLIP (I-CLIP \u2208 [0,1] \u2191)", "content": "The CLIP model [45] maps text and image embeddings into the same latent space, and has been used for image captioning [1, 26] and other text-conditional generation tasks, such as image generation[47, 57], and 3D mesh generation [38]. On the highest level, a poorly generated image may semantically mis-align with an input prompt [58]. Therefore, determining the captions between a pair of images may reflect their contextual and semantic distances. With this observation in mind, we generate the CLIP-encoded caption for each image, and compare their distance in the encoding space,\nI-CLIP (p, t) := cos (C ($I_{p,t}$), C($I_{p,N}$)),\nwhere C is CLIP captioning, and $C_l$ is the latent embedding."}, {"title": "3.3. Learning to Predict Perceptual Metrics from Text", "content": "The multi-scale perceptual analysis of each image $I_{p,t}$ creates a large-scale dataset of prompt - timestep pairs which map to the three metrics scores computed on images generated at the corresponding timestep by averaging all scores over the four random seeds,\n(p, t) \u2192 \\{L-SNR (p, t), D-SIM (p, t), I-CLIP (p, t)\\}.\nFor each prompt p, the per-time step data informs the perceptual quality at a given denoising timestep t. We then trained neural networks $\\theta_m$ to predict each of the above time series for novel text prompts. Here, $m\\in \\{L-SNR, D-SIM, I-CLIP\\}$ are the individual perceptual metrics."}, {"title": "Model architecture", "content": "To simulate the change in image quality with respect to the number of denoising steps, the model architecture should reflect the locally dependent chain nature of the denoising process of diffusion models. With this insight, we choose to use a bidirectional LSTM (BiLSTM) [10] given the robustness and accuracy observed for various similar time series prediction tasks [51]. Our model architecture is visualized in Figure 4. To transfer the stochastic chain process to a recurrent form for the LSTM, the CLIP-embedded text prompt p and position-embedded (as in Kenton and Toutanova [22]) timestep t vectors are combined as input to predict metric scores at timestep t,\nm_t(p) = \\theta_m (p + t).\nThe implementation details of our model are discussed below.\nModel implementation details As shown in Figure 4 and similar to prior literature [3, 32, 41, 54], we implement $\\theta_m$ as a simple BiLSTM, followed by a feed-forward fully connected network (MLP) and Sigmoid activation. Specifically, for the LSTM, we use a two-layer BiLSTM with hidden features of size 512. The three-layer MLP transforms the LSTM-returned hidden scores of size 1024 (due to the bidirectional design) to 128 and then 1. We train the model with our held-out 90% (Section 3.1) training images from 16,545 prompts with L2 loss, a batch size of 32, and a learning rate of 1e-4. During training, we used the widely"}, {"title": "3.4. Predicting Denoising Steps from Perceptual Metrics", "content": "For a new prompt p, we leverage the trained LSTM models $\\theta_m$ to predict the time series $m_t$ for each $m \\in \\{L-SNR, D-SIM, I-CLIP\\}$. Our goal is to suggest the total denoising timestep of the diffusion models before generation. As shown in Figure 5, we intuitively find the \"turning\" timestep t* where all metrics plateau. That is, additional inference steps (t > t*) will only improve users' perceived image quality marginally at all scales, indicating reduced computational efficiency. Note that the perceptual metrics typically exhibit a monotonic relationship with regard to number of denoising timesteps, i.e., larger timesteps do not reduce the perceived quality. Inspired by prior literature on turning point detection for psychological time series data [8], we employ a statistical approach to determine optimal time step,\nt^*(p) := max t_m(p), m\u2208 \\{L-SNR, D-SIM, I-CLIP\\}\nt_m(p) := max t s.t. mt(p) \u2265 \u03bc (\\{mt(p)\\}_{t_1}^t) + w_m\u03c3 (\\{mt(p)\\}_{t_1}^t).\nHere, \u03bc(\u00b7)/\u03c3(\u00b7) indicates the median and standard deviation of the time series data. Similar to Engbert and Mergenthaler [8], we determine the weights $W_m$ ($W_{L-SNR}$ = 0.3, $W_{D-SIM}$ = 0.2, $W_{I-CLIP}$ = 0.5) of the three metrics using the efficiency measurement as detailed in Section 4.2. The effectiveness of the suggested step numbers on each perceptual metrics is visualized in Figure 6."}, {"title": "4. Evaluation", "content": "Beyond the qualitative results demonstrated in Figure 7 and the supplementary material, we first evaluate the model prediction accuracy as an ablation study in Section 4.1. Then, we measure BudgetFusion's effectiveness in enhancing computational efficiency as \"perceptual gain per diffusion step\" Section 4.2, as well as the generated content diversity Section 4.3. Lastly, we conduct an user study for subjective measurement of overall generated image quality Section 4.4. To this aim, we study and compare three conditions:\n\u2022 OURS: Our adaptive BudgetFusion method;\n\u2022 UNIFORM: Instead of adaptive guidance, we construct a baseline which suggests a single timestep for all prompts. By calculating the average step numbers from BudgetFusion throughout the evaluation set, this condition achieves, on average, the same computation cost to OURS;\n\u2022 REFERENCE: Within our sampled timesteps T, we select t = 65, which is close to the commonly and empirically suggested number of denoising steps. Higher step sizes will only marginally enhance quality.\nIn the following experiment, we use the holdout test set of 1,839 prompts, which is 10% of the entire image dataset."}, {"title": "4.1. Ablation Study: Model Prediction Accuracy", "content": "We evaluate our design of the proposed text-to-metric time series LSTM model (Section 3.3) as an ablation study. We computed mean absolute error (MAE, lower is better) between the predicted and measured ground truth to measure prediction accuracy. Specifically, we compare the MAE of OURS, and OURS without text embedding (to validate the choice of embedding), as well as replacing our LSTM model with MLP + Tanh activation (to validate the choice of using LSTM). Table 1 shows the results of our ablation. OURS exhibits significantly lower error among all alternative design choices. The results evidence the effectiveness of the text prompt encoding and LSTM neural network."}, {"title": "4.2. Quality and Computational Efficiency", "content": "The ultimate goal of BudgetFusion is to optimize the \"quality gain per watt of computation\" of diffusion models. Therefore, we evaluate its quality versus its computation efficiency against the aforementioned conditions.\nQuality-Computation Efficiency We measure quality-computation efficiency as the perceptual quality gain per TFLOPS computation in the GPU. That is, for each evaluation prompt p and metric m, we compute\n$\\eta = \\frac{m_t(p)}{\\eta}$"}, {"title": "Overall quality gain", "content": "The previous analysis suggests that OURS and UNIFORM seem to exhibit comparable efficiency per step, and it may not be immediately clear that our technique is better than selecting a constant number of steps, as with the UNIFORM condition. Note that OURS and UNIFORM share the same total number of denoising steps; we further measure their relative quality as the ratio between the two,\n$\\frac{m_t^{OURS}(p)}{M_t^{UNIFORM}(p)}-1.$\nHere, $t_{OURS}$ and $t_{UNIFORM}$ indicate the timestep numbers suggested by OURS and UNIFORM, respectively. Intuitively, the positive or negative ratio values indicate how much OURS outperforms or underperforms than UNIFORM. Figure 8a visualizes the results after computing average relative quality over all images in the test set. This results in relative quality of 6.6% \u00b1 0.8% for m=L-SNR, 8.4% \u00b1 0.9% for m=D-SIM, and 8.7% \u00b1 0.9% for m=I-CLIP. These results show that, despite having the same total number of denoising timesteps (i.e., same total computation), OURS exhibits significant quality gain over UNIFORM. This is also evidenced by our subjective study in the following Section 4.4."}, {"title": "Time consumption", "content": "To measure the tangible benefits between REFERENCE and OURS, we measure their time consumption as seconds per image (SPI) on an NVIDIA RTX 3090 GPU. As shown in Figure 8b, with the entire evaluation dataset, OURS achieved 2.89 \u00b1 0.87 SPI versus REFERENCE at 8.00 \u00b1 0.039 SPI, which is 63.9% of the time of REFERENCE, indicating significantly reduced computational resource usage."}, {"title": "4.3. Image Quality and Diversity", "content": "We also compute the inception score (IS) to measure the generated image quality and diversity of the three conditions. The IS scores of OURS, UNIFORM, and REFERENCE were 27.9, 27.9, and 28.7, respectively, which demonstrates that BudgetFusion achieves computation savings without substantially compromising image quality and diversity. Furthermore, we include visual results of our method in the Supplementary Materials."}, {"title": "4.4. Subjective Crowdsourcing Study", "content": "As visualized in the insets of Section 4.4, we conducted a crowd-sourced user study with 72 participants through the platform Prolific to determine whether images generated with our technique are of high quality. Mantiuk et al. [34] found that forced-choice studies require fewer trials, reduce variance in results, and provide an unambiguous task for participants. As such, we ran a two-alternative forced-choice (2AFC) study to measure participants' preference for images generated by OURS and UNIFORM. During each trial, participants were presented with the reference and two test images, one generated by either OURS or UNIFORM. Users were instructed to select the image with higher image quality, with respect to REFERENCE. In total, participants completed 100 trials, with prompts randomly sampled from the dataset described in Sec. 3.1. An additional 2AFC study was conducted on 78 participants to compare the quality between REFERENCE and OURS. Our study was approved by an Institutional Review Board (IRB)."}, {"title": "Results and discussion", "content": "The results are visualized in Section 4.4 as a distribution of percentage selection of OURS. Here, we define percentage selection as the proportion of trials in which users selected OURS over UNIFORM or REFERENCE (depending on the study task). The mean percentage of selection of OURS was 60.3 \u00b1 3.9%. In the study in which users compare OURS and REFERENCE, participants selected OURS 35.2 \u00b1 8.4% of the time, lower than 25% selection which is commonly used to define the 1 Just-Noticeable-Difference (1 JND) threshold in psychophysical studies. The 1 JND range is commonly used in prior literature attempting to determine perceptually unnoticeable threshold [9]. These results evidence that our BudgetFusion-guided diffusion model achieves significant computation savings without sacrificing subjectively perceived quality. It may be interesting, in future work, to determine a mapping from plateau computation (e.g., defined in Sec. 3.4) to percentage preference. We expand on this discussion in the next section."}, {"title": "5. Limitations and Future Work", "content": "Sparsely sampled diffusion timesteps While preparing the dataset in Section 3.1, we sample the timesteps T in a non-uniform fashion considering the perceptual power law and then linearly interpolate the perceptual metrics among them during model training (Section 3.3). This is due to the storage and computation efficiency. As a future follow-up, evaluating perceptual quality with dense and uniform sampling may provide more fine-grained guidance.\nVariable savings. In this work, we develop an algorithm to compute the plateau point of three metrics (see Sec. 3.4), which is used to determine a dataset for driving our technique. Ultimately, optimal timesteps determined by OURS depends on this dataset. It may be useful to determine how this dataset could be modified, for example in order to select optimal timesteps which require a pre-defined amount of compute."}, {"title": "6. Conclusion", "content": "In this paper, we present BudgetFusion, a predictive perceptual guidance for diffusion models. Given a text prompt, it suggests the most efficient timesteps before the computationally intensive neural-network-based denoising diffusion starts. That is, it guides the diffusion toward optimized \u201cperceptual gain per step\". Our objective analysis and user studies show that this is achieved without compromising perceptual quality and content diversity. As \u201cmaking an image with generative AI uses as much energy as charging your phone\u201d [36], we hope this research will pave the way for future human-centered approaches to address the surging questions of how to optimize the computational and emission costs of generative models."}]}