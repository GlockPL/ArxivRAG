{"title": "Learning Valid Dual Bounds in Constraint Programming: Boosted Lagrangian Decomposition with Self-Supervised Learning", "authors": ["Swann Bessa", "Darius Dabert", "Max Bourgeat", "Louis-Martin Rousseau", "Quentin Cappart"], "abstract": "Lagrangian decomposition (LD) is a relaxation method that provides a dual bound for constrained optimization problems by decomposing them into more manageable sub-problems. This bound can be used in branch-and-bound algorithms to prune the search space effectively. In brief, a vector of Lagrangian multipliers is associated with each sub-problem, and an iterative procedure (e.g., a sub-gradient optimization) adjusts these multipliers to find the tightest bound. Initially applied to integer programming, Lagrangian decomposition also had success in constraint programming due to its versatility and the fact that global constraints provide natural sub-problems. However, the non-linear and combinatorial nature of sub-problems in constraint programming makes it computationally intensive to optimize the Lagrangian multipliers with sub-gradient methods at each node of the tree search. This currently limits the practicality of LD as a general bounding mechanism for constraint programming. To address this challenge, we propose a self-supervised learning approach that leverages neural networks to generate multipliers directly, yielding tight bounds. This approach significantly reduces the number of sub-gradient optimization steps required, enhancing the pruning efficiency and reducing the execution time of constraint programming solvers. This contribution is one of the few that leverage learning to enhance bonding mechanisms on the dual side, a critical element in the design of combinatorial solvers. To our knowledge, this work presents the first generic method for learning valid dual bounds in constraint programming. We validate our approach on two challenging combinatorial problems: The multi-dimensional knapsack problem and the shift scheduling problem. The results show that our approach can solve more instances than the standard application of LD to constraint programming, reduce execution time by more than half, and has promising generalization ability through fine-tuning.", "sections": [{"title": "Introduction", "content": "Combinatorial optimization has applications across diverse fields, such as aerospace, transportation planning, scheduling, and economics. The objective is to identify an optimal solution from a finite set of possibilities. A significant challenge in combinatorial optimization is the state-space explosion problem: the number of potential solutions grows exponentially with the size of the problem, rendering large-scale problem-solving intractable. Constraint programming (CP) is a versatile tool for addressing combinatorial optimization problems (Rossi, Van Beek, and Walsh 2006) as it is not confined to linear models, unlike mixed-integer programming (MIP). It can theoretically address any combinatorial problem, including those with non-linear constraints or objectives. Standard CP solvers rely on a search procedure that exhaustively yet efficiently enumerates all possible assignments of values to variables until the optimal solution is found.\nThe search is commonly conducted depth-first, combined with branch-and-bound techniques. The search backtracks to the previous decision point when an infeasible solution is encountered due to an empty domain. Through this procedure, and given that the entire search space is explored, the final solution found is proven to be optimal. Unfortunately, this tree search grows exponentially with the number of variables, making a complete enumeration of solutions still intractable for large problems. To address this challenge, CP search is first enhanced through a mechanism called propagation, which reduces the number of possible combinations and, consequently, the size of the tree search. Given the current domains of the variables and a constraint, propagation eliminates values from the domains that cannot be part of the final solution due to constraint violations. This process is repeated with each domain change and for each constraint until no more values can be removed from the domains.\nGiven its reliance on branch-and-bound, a first design choice in CP is the branching strategy, which dictates how the search space is explored. Well-designed heuristics are more likely to discover promising solutions, whereas poor heuristics may lead the search into unproductive regions of the solution space. The selection of an appropriate branching strategy is generally non-trivial, and its automatic design with learning methods has been extensively studied by the research community (Michel and Van Hentenryck 2012; Song et al. 2022; Marty et al. 2023). A second critical choice, though much less explored through the lens of learning, is the bounding strategy. Most existing CP solvers rely on a straightforward mechanism. When a feasible solution is found, a new constraint is added to ensure the following solution is better than the current one. This new constraint can enable more propagation. Unfortunately, the filtering provided by this approach is often relatively poor. Furthermore, this mechanism only allows the computation of a primal bound and does not offer dual bounds acting as a certificate of optimality. A significant shortcoming of current CP technologies is the lack of a general and efficient dual-bounding mechanism for constraint programs, unlike MIP solvers, which rely on the linear relaxation. This limitation is one of the main reasons why CP solvers are often less competitive than MIP solvers for many combinatorial problems. This paper proposes to tackle this limitation."}, {"title": "Related Work", "content": "Cost-Based Domain Filtering. To our knowledge, Focacci, Lodi, and Milano (1999) were the first to leverage bounding methods from mathematical programming to improve CP solvers generically. They propose to embed in constraints an optimization component consisting of a linear program representing a linear relaxation of the constraint itself. This is commonly referred to as cost-based domain filtering and directly enables the computation of a dual bound when solving the relaxed problem. However, a drawback of cost-based domain filtering is that the bound is local to each constraint for which a custom relaxation module needs to be developed. At the time of writing, the global constraints catalog reports more than 400 different existing global constraints (Beldiceanu et al. 2007), but only two of them have been considered in this work: ALLDIFFERENT and PATH. This naturally led researchers to implement this idea for other global constraints (Fahle and Sellmann 2002; Sellmann 2003; Houndji et al. 2017), or to leverage different types of relaxations. For instance, Benchimol et al. (2012) proposed using the Held and Karp relaxation to improve the filtering of the WEIGHTEDCIRCUIT constraint.\nCP-based Lagrangian Relaxation. Cost-based domain filtering has a second drawback: examining only one constraint at a time risks significantly underestimating the actual objective cost. This occurs because the mechanism often assumes it is possible to improve the objective, which can be impossible due to other constraints. This approach acts locally and lacks a global view of the problem. One solution is to soften all constraints and incorporate them into the objective function with a penalty term, referred to as Lagrangian multipliers. This approach simplifies the problem. In CP, this procedure was introduced by Sellmann and Fahle (2001) and is commonly known as CP-based Lagrangian relaxation. Sellmann (2004) later proved that suboptimal Lagrangian multipliers can have stronger filtering abilities than optimal ones, which is advantageous as it allows for better filtering with less computational power. This idea has subsequently been used to solve various problems (Sellmann and Fahle 2003; Cambazard, O'Mahony, and O'Sullivan 2010; Isoart and R\u00e9gin 2020; Boudreault and Quimper 2021), involving more and more constraints (Menana and Demassey 2009; Cambazard and Fages 2015; Berthiaume and Quimper 2024), and has shown promise in improving the performance of CP solvers (Fontaine, Michel, and Van Hentenryck 2014). A drawback of CP-based Lagrangian relaxation is that the initial problem loses part of its original structure as the combinatorial constraints are removed from the constraint store and moved into the objective function. Additionally, the penalized constraints must be either linear or easily linearizable, which is not often true for most problems.\nLagrangian Decomposition in CP. This limitation has progressively led researchers to consider Lagrangian decomposition (Guignard and Kim 1987) as an alternative bounding scheme, with the advantage of applying it to various constraints. Lagrangian decomposition breaks down the initial problem into a master problem and several independent sub-problems, each of which is more manageable to solve. A vector of Lagrangian multipliers is associated with each sub-problem, and the master problem iteratively adjusts these multipliers to identify the optimal penalties, thereby guiding the solution process toward optimality. This is typically achieved through a sub-gradient procedure (Shor 2012), wherein each sub-problem must be solved exactly once per sub-gradient iteration. Cronholm and Ajili (2004) were the first to apply this concept within the context of constraint programming, proposing a specific algorithm for the multicast network design problem. A decade later, H\u00e0, Quimper, and Rousseau (2015) advanced this idea to develop an automatic bounding mechanism for constraint programming. Concurrently, Bergman, Cire, and van Hoeve (2015) suggested utilizing Lagrangian decomposition for constraints that can be represented by decision diagrams (Andersen et al. 2007). Both studies underscored the potential of Lagrangian decomposition as an automatic dual-bounding mechanism for constraint programming.\nHowever, two significant challenges arise with Lagrangian decomposition. First, a specialized and efficient algorithm is required to solve the sub-problems. Second, it incurs substantial computational overhead as each sub-problem must be solved at every sub-gradient iteration. While the former challenge has been addressed by Chu, Gange, and Stuckey (2016), who proposed solving the sub-problems via a search procedure rather than a specialized propagator, the latter remains unresolved. To our knowledge, the efficient utilization of Lagrangian decomposition for CP has not been further investigated and remains an open research question for the practical application of this approach.\nLearning Valid Dual Bounds. In another context, machine learning has been considered to enhance the solving process of branch-and-bound solvers, both for integer programming (Khalil et al. 2016) and constraint programming (Cappart et al. 2021). We refer to the surveys of Bengio, Lodi, and Prouvost (2021) and of Cappart et al. (2023) for an extended literature review. From these surveys, one can observe that most attempts to use learning inside branch-and-bound operate on the branching decisions (i.e., directing the search) or on the primal side (i.e., acting as a heuristic). However, learning to improve the quality of relaxations using better dual bounds has been much less considered. This has only been addressed for the restricted use case of solvers based on decision diagrams (Cappart et al. 2022), for combinatorial optimization over factor graphs (Deng et al. 2022), integer linear programs (Abbas and Swoboda 2024), and the specific WEIGHTEDCIRCUIT constraint (Parjadis et al. 2023). To our knowledge, learning has never been used for improving CP solvers through Lagrangian decomposition."}, {"title": "Statement of Contributions", "content": "Building on this context, this paper contributes a novel learning approach dedicated to generating tight and valid dual bounds in CP. We propose to achieve this by learning appropriate multipliers for a Lagrangian decomposition, significantly reducing the number of sub-gradient iterations required. The learning process is self-supervised, utilizing a graph neural network to represent the problem structure. Unlike most related work, which focuses on learning primal heuristics, our approach develops a learning-based strategy to derive valid and tight dual bounds. To our knowledge, this is the first approach to offer a generic method for learning valid dual bounds for any type of combinatorial problem. Experiments are conducted on two challenging case studies: the multi-dimensional knapsack problem and the shift-scheduling problem."}, {"title": "Lagrangian Decomposition in CP", "content": "Formally, a constrained optimization problem (COP) is defined as a tuple (X, D(X), C, f), where X is a set of discrete variables, D(X) is the set of domains for these variables, C is the set of constraints that restrict assignments of values to the variables, and f(\u00b7) : X \u2192 R is an objective function. A feasible solution is an assignment of values from D(X) to X that satisfies all the constraints in C. An optimal solution is feasible and also maximizes the objective function. The canonical expression of a COP involving m constraints is formalized below.\n$\\max_{X \\in D(X)} \\{f(X) | \\bigwedge_{i=1}^{m} C_i(X)\\}$\n(1)\nThe notation $C_i(X)$ indicates that constraint $C_i$ has the variables $X$ in its scope. Lagrangian decomposition (LD) involves relaxing this problem to get a valid, ideally tight, upper bound on the solution. To do so, variables involved in each constraint are duplicated, except for the first constraint.\n$\\max_{X_1} \\{f(X_1) | (\\bigwedge_{i=1}^{m} C_i(X_i)) \\wedge (\\bigwedge_{i=2}^{m} X_i = X_1)\\}$\n(2)\nHere, $X_1$ refers to the initial variables, and $X_2$ to $X_m$ refers to newly introduced variables. Next, a penalty term is introduced for each duplicated variable and is added to the objective function with a cost $\\mu_i \\in R^{\\vert X \\vert} \\ge 0$.\n$\\max_{X_1} \\{f(X_1) + \\sum_{i=2}^{m} \\mu_i \\cdot (X_1 - X_i) | (\\bigwedge_{i=1}^{m} C_i(X_i)) \\wedge (\\bigwedge_{i=2}^{m} X_i = X_1)\\}$\n(3)\nThese costs are the Lagrangian multipliers. We note that the resulting problem retains the same optimal solutions as the initial problem in Equation (1). We now relax the problem by removing all constraints linking the variables.\n$B(\\mu) = \\max_{X} \\{f(X_1) + \\sum_{i=2}^{m} \\mu_i \\cdot (X_1 - X_i) | (\\bigwedge_{i=1}^{m} C_i(X_i))\\}$\n(4)\nThe resulting COP, formalized below, is a relaxation of Equation (1) and solving it will provide a dual bound B(\u03bc). This corresponds to an upper bound as we are maximizing. Interestingly, each constraint Ci has a different set of variables and could then be solved independently. We can consequently reorganize the expression as follows.\n$B(\\mu) = \\Phi(X_1) + \\sum_{i=2}^{m} \\Psi_i(X_i)$\n(5)\n$\\Phi(X_1) = \\max \\{f(X_1) + \\sum_{i=2}^{m} \\mu_i \\cdot X_1 | C_1(X_1)\\}$\n$\\Psi_i(X_i) = \\max \\{ - \\mu_i \\cdot X_i | C_i(X_i)\\} \\forall i \\in \\{2,...,m\\}$\nA valid dual bound can be obtained from a given set of multipliers by solving m independent sub-problems, which are generally easier to solve than the original problem as they involve only a single constraint. However, the quality of the bound (i.e., how close it is to the optimal solution) heavily depends on the values of the multipliers. Finding the optimal multipliers is the main challenge in LD as Equation (5) is not differentiable. The most common way is to proceed with a subgradient optimization (Shor 2012). Starting from an arbitrary value, multipliers are iteratively updated.\n$\\mu_i^{t+1} = \\mu_i^t + \\alpha(X_1^t - X_i^t) \\forall i \\in \\{2, ...,m\\}$\nHere, $\\mu_i^t$ refers to the set of multipliers i at iteration t, $X_i^t$ refers to the optimal values of variables $X_i$ at iteration t, and \u03b1 is a learning rate."}, {"title": "Learning Lagrangian Multipliers", "content": "The bound computed in Equation (5) has two notable properties: (1) it can be parameterized using the Lagrangian multipliers \u03bc, and (2) it is always valid, meaning it will never underestimate the actual profit. Both properties present an opportunity to employ a learning-based approach to compute the bound. Inspired by Parjadis et al. (2023), we propose building a model \u03a9\u0473 : G(V, E) \u2192 R|V| capable of directly predicting all the \u00b5 multipliers for a COP instance given as input and represented by a graph G. The graph comprises a set of V nodes (one per multiplier) with their relationships expressed by edges in E. The model N\u0259 is parameterized by a set of p parameters 0 = {01,..., 0p} and must be differentiable. The goal is to eliminate sub-gradient iterations by directly learning the multipliers that produce a tight bound, thereby saving execution time and enhancing filtering. The requirement for the model to be differentiable is essential for our methodology, which involves computing the bound using gradient-based optimization.\nThe goal is to find model parameters @ that yield the lowest possible bound. Thanks to the second property, the obtained bound is provably valid, regardless of the model's accuracy. To our knowledge, this paper presents the first generic method for learning valid dual bounds for any type of discrete COP. This is a notable strength, considering the difficulties in getting guarantees with learning in combinatorial optimization (Kotary et al. 2021). Given the multipliers \u03bc = \u3008\u03bc2,..., \u00b5m), the bound minimization problem for an instance G and its related gradient are as follows.\n$\\min B(\\mu) \\to \\nabla B(\\mu) \\text{ with } \\mu = \\Omega_\\theta(G)$\n(7)\nHowever, computing the gradient of this expression is not trivial, as the bound is obtained through the solutions of combinatorial sub-problems. The chain rule can be applied since the multipliers are parameterized by 0.\n$\\nabla B(\\mu) = \\frac{\\partial B(\\mu)}{\\partial \\mu} \\cdot \\frac{\\partial \\mu}{\\partial \\theta}$\n(8)\nThe right term, corresponding to the differentiation of the model \u03a9\u03b8 (i.e., a graph neural network), is easily obtained via backpropagation. However, the left term requires differentiating the expression in Equation (5) for all \u03bc.\n$\\frac{\\partial B(\\mu)}{\\partial \\mu} = \\frac{\\partial \\Phi(X_1)}{\\partial \\mu} + \\sum_{i=2}^{m} \\frac{\\partial \\Psi_i(X_i)}{\\partial \\mu}$\n(9)\nThe analytic partial derivative cannot be obtained as it requires solving combinatorial sub-problems. Instead, we propose using a locally valid derivative by considering the optimal solution of each sub-problem, allowing us to set all variables $X_i$ for all $i \\in \\{1, ..., m\\}$. Considering a specific multiplier vector $\u03bc_i$, we expand the functions \u03a6(.) and \u03a8(.), which involve only additions of $\u03bc_i$ and constant multiplications. The partial derivatives of \u03bc\u03b5 are then as follows.\n$\\frac{\\partial B(\\mu)}{\\partial \\mu_i} = (X_1 - X_i) \\forall i \\in \\{2,...,m\\}$\n(10)\nThis yields the gradient in Equation (5), used to minimize a dual bound obtained through multipliers.\n$\\nabla B(\\mu) = ((X_1 - X_2),..., (X_1 - X_i), ... \\forall i \\in \\{2,...,m\\}$\n(11)\nWe propose leveraging a self-supervised learning approach to parameterize the model \u03a9\u03c1. The training procedure is formalized in Algorithm 1. It takes as input a dataset D consisting of a set of graphs G(V, E) serving as instances of a specific COP. These instances can be obtained from historical data or synthetic generation. The features of the graph are problem-dependent. At each iteration, LD is carried out, and all the sub-problems are solved through a dedicated procedure to obtain the optimal values of Xi for current multipliers \u03bc. The bound and its gradient are then computed, and a gradient descent step is executed. This will change the values of the multipliers for the next iteration and, consequently, the optimal values of variables X\u2081. Finally the parameters of the trained neural network \u03a9\u03b8 are returned."}, {"title": "Graph Neural Network Architecture", "content": "Graph neural networks (GNNs) (Scarselli et al. 2008) are a specific neural architecture designed to compute a vector representation, also referred to as an embedding, for each node of an input graph. This architecture has been widely used in related works on learning for combinatorial optimization (Cappart et al. 2023). Learning is carried out by aggregating information from neighboring nodes multiple times. Each aggregation operation is referred to as a layer of the GNN and involves weights that must be learned.\nLet G(V, E) be a simple graph used as input of Og in Algorithm 1, let hi \u2208 RP be a p-dimensional vector representation of the input features of a node v \u2208 V, and let ku,v \u2208 Rq be a similar q-dimensional representation of the input features of an edge (u,v) \u2208 E. Finally, let L be the number of layers of the GNN. Our implementation is based on RES-GATEDCONV architecture (Bresson and Laurent 2017). The inference process involves computing the next representation (h+1) from the previous one (h) for each node v. This process is formalized in Equation (12), where (01,...,04) are the weights in layer l, N(v) are the neighboring nodes of v, is the element-wise product, \u03b7\u03c5,u is an edge gate between nodes v and u, and o is a sigmoid activation.\n$\\textbf{h}_v^{l+1} = \\text{RELU} \\left( \\theta_1^l \\textbf{h}_v^l + \\sum_{u \\in N(v)} (\\eta_{v,u} \\odot \\theta_2^l) \\textbf{k}_{u,v} \\right)$\n(12)\n$\\eta_{v,u} = \\sigma(\\theta_3^l \\textbf{h}_v^l + \\theta_4^l \\textbf{h}_u^l)$\n(13)\nThe edge features ku,v are concatenated to each message. Applying the inference on the L layers results in an embedding he for each node v \u2208 V. Similarly, we also compute a graph embedding \u0393\u2208 R by averaging information from each node, as shown in Equation (14).\n$\\Gamma = \\frac{1}{\\vert V \\vert} \\sum_{v \\in V} (\\textbf{h}_v^L)$\n(14)\nConsidering both a graph and node embedding allows each node to combine a global and a local representation of the problem. Finally, both embeddings are concatenated (||) and given as input of a fully-connected neural network (FCNN) to obtain the Lagrangian multipliers, as shown in Equation (15). We recall that there is one node per multiplier in our representation. The architecture has two messages passing steps with a size of 64 each and the fully-connected neural network has two layers with 256 and 128 neurons.\n$\\mu_v = \\text{FCNN} (\\textbf{h}_v^L || \\Gamma) \\forall v \\in \\{1, ..., V\\}$\n(15)"}, {"title": "Case studies", "content": "This paper addresses two challenging COPs: the multi-dimensional knapsack and the shift scheduling problem, chosen for direct comparison with H\u00e0, Quimper, and Rousseau (2015). Further details are in the appendix.\nMulti-dimensional Knapsack Problem (MKP). It is an extension of the classic knapsack problem. In the MKP, we are given a set of items, each with a specific profit and multiple weights (one per dimension). Additionally, there are multiple knapsack constraints, each representing a capacity limit for a different dimension. The objective is to select a subset of items that maximizes the total profit while ensuring that the total weight in each dimension does not exceed the respective capacity constraints. In our CP formalization, for an instance of dimension d, we have d specific knapsack constraints, which result in d sub-problems. We solve each sub-problem with a standard dynamic programming algorithm for one-dimensional knapsack, which runs in O(nW), with n the number of items and W the capacity. For training and evaluation, a synthetic dataset is generated following the protocol of Pisinger (Kellerer, Pferschy, and Pisinger 2004). Instances of 30, 50, and 100 items and up to 5 dimensions are considered. Profits follow a uniform distribution between 0 and 500, weights between 0 and 100, and the capacity is a percentage of the total weight per dimension.\nShift Scheduling Problem (SSP). The problem involves assigning employees to shifts in a way that meets work regulations while maximizing profit. We consider a challenging version where the work regulations are so complex that modeling them for a single employee requires numerous REGULAR constraints (Pesant 2004). The goal is to assign one activity per shift such that the transitions between activities over the periods satisfy the constraints and maximize the total profit. We consider instances with two REGULAR constraints (yielding two sub-problems) and 50 periods, as they are already challenging to solve and cannot all be solved to optimality within a 2-hour timeout. The sub-problems are solved using the procedure introduced by Pesant (2004), which runs in O(nmQ), where n is the number of variables, m is the number of symbols, and Q is the number of states in the REGULAR automaton. A synthetic dataset is generated for training and evaluation, following the procedure of Pesant (2004). Instances with two constraints, 50 items, and three configurations of symbols and states (10/20, 10/80, 20/20) are considered, where x/y denotes a configuration with x symbols and y states. Profits follow a uniform distribution between 0 and 100, with 0.3 as the proportion of undefined transitions and 0.5 as the proportion of final states."}, {"title": "Experimental Protocol", "content": "This section outlines the protocol used to evaluate the efficacy and reliability of our approach, detailing the datasets and configurations used across various testing scenarios.\nCompetitors. The methods tested in the experiments are:\n\u2022 CP. A pure constraint programming approach without Lagrangian decomposition or learning.\n\u2022 CP+SG. The same CP model is enhanced with LD and sub-gradient optimization as proposed by H\u00e0, Quimper, and Rousseau (2015). We re-implemented this approach, and our results for this baseline are much better than those reported in the initial paper, likely due to the improved branching heuristics and the choice of CP solver.\n\u2022 CP+Learning(all). The CP model with LD, using the bounds learned in Alg. 1 instead of sub-gradient, with learning applied at every node of the search tree.\n\u2022 CP+Learning(all)+SG. Similar to the previous approach, the learned bounds are further improved by sub-gradient optimization. The trained model \u03a9\u03b8 is called at every search tree node to obtain the bounds.\n\u2022 CP+Learning(root)+SG. Model N\u0259 is used only at the root node, with the resulting bound serving as the initial value for bootstrapping sub-gradient in the other nodes.\nImplementation. The graph neural network has been implemented in Python using Pytorch Geometric (Fey and Lenssen 2019). The CP models are solved with Gecode (Schulte, Lagerkvist, and Tack 2006), a CP solver implemented in C++. The interface between both languages has been implemented using libtorch. The implementation and the datasets used are released on GitHub\u00b9.\nTraining and Validation. The training phase follows Algorithm 1. For each configuration, a specific model is trained using synthetic instances. A training set of 900 instances is used for the MKP and 200 for the SSP, with data augmentation from partially solved instances up to a depth of 5. The self-supervised learning process eliminates the need for labeled multipliers. Training was conducted on a Tesla V100-SXM2-32GB GPU for 24 hours, with no extensive hyperparameter tuning (details in the attached files). Models were trained and tested in a single run, with convergence observed on a validation set of 100 MKP and 30 SSP instances. Adam optimizer (Kingma and Ba 2014) with a learning rate of 0.001 has been used."}, {"title": "Empirical Results", "content": "This section presents the results of our experimental analysis, offering a detailed quantitative evaluation of our approach through a series of experiments.\nMain Result: Quality of the Learned Bounds. Table 1 shows the performance of all approaches on a test set of 50 instances for both problems. We first observe that CP without LD, although being the fastest in the easiest configuration (MKP with 30 items), has a very large number of explored nodes, which quickly explodes for larger instances. Interestingly, we observe that learning alone (CP+Learning(all)) consistently results in a larger search tree (i.e., more nodes) compared to sub-gradient optimization (CP+SG), indicating weaker bounds. However, this approach allows the solver to skip all sub-gradient iterations, often leading to significant execution time savings (e.g., from 158 to 36 seconds for MKP with 50 items, or from 3,300 to 1,300 seconds for SSP with 80 states). Unfortunately, these time savings are insufficient for the most challenging configurations, where the number of nodes is much higher. In such cases, using learning to initialize the sub-gradient procedure (CP+Learning(root)+SG) offers the best performance (e.g., reducing nodes explored from 4,100 to 2,100 for MKP with 100 items, decreasing average execution time from 2,400 to 1,100 seconds, and solving 8 additional instances). In conclusion, learning has significantly improved the application of LD in CP, either by replacing sub-gradient steps or by bootstrapping them. Performance on individual instances (Figure 2) supports this conclusion. The significance of each improvement in time was validated using a one-sided Wilcoxon signed-rank test. For each configuration, at least one of our approaches offers a significant improvement (p-value < 0.001), excepting SSP-20-20 (p-value < 0.1) and SSP-10-20 (no significant improvement).\nAnalysis: Fine-Tuning to OOD Instances. This experiment evaluates our approach's ability to generalize to instances from an unseen distribution when fine-tuning is applied. We used the standard benchmark of Shih (1979), consisting of 30 MKP instances with 30 to 90 items. Fine-tuning was limited to 2 hours (10 epochs) with 100 instances from a similar distribution. Results in Table 2 show that we outperform the baseline for most instances, thanks to fine-tuning. Although CP+Learning(all) offers the best overall performance, it falls short in a few cases due to the intrinsic lack of guarantees in learning-based methods. Combining learning with sub-gradient (CP+Learning(root)+SG) mitigates this risk and provides more stable results. We also report that performance without fine-tuning is worse than the baseline.\nAnalysis: Evolution of the Bounds. This experiment analyzes the tightness of bounds obtained with different approaches and how they improve with each sub-gradient step. Figure 3 shows the average bounds on 50 instances for the two hardest configurations, measured by the gap with the optimal solution. In both cases, learning alone provides much better multipliers than random ones (LD before any sub-gradient step). After a number of iterations (600 for MKP and 40 for SSP), sub-gradient alone manages to improve the bounds but is still outperformed by our approach, which uses learning to initialize the sub-gradient procedure."}, {"title": "Conclusion", "content": "Previous works have identified Lagrangian decomposition as a promising approach for developing automatic bounding mechanisms in constraint programming. However, the practicality of LD is limited by the prohibitive execution time required for sub-gradient optimization at each node of the tree search. This paper addresses this challenge with an innovative approach that learns Lagrangian multipliers to produce tight dual bounds. The self-supervised learning process, which eliminates the need for labeled bounds, leverages a graph neural network to represent the problem structure. Experiments demonstrate the potential of this approach, either by replacing sub-gradient iterations entirely or by bootstrapping them. Fine-tuning has shown promise for generalizing to out-of-distribution instances. To our knowledge, this is the first contribution proposing a generic method for learning valid dual bounds in constraint programming."}, {"title": "Multi-Dimensional Knapsack Problem", "content": "The Multi-Dimensional Knapsack Problem (MKP) is an extension of the classic knapsack problem (Kellerer et al. 2004). In the MDKP, there are multiple constraints (or dimensions) rather than just one. Each item i has a value vi and multiple weights {w, ..., w}, each corresponding to a different dimension d. The goal is to select a subset of items that maximizes the total value while ensuring that the sum of the weights in each dimension does not exceed the capacity Wd of that dimension. This problem has numerous practical applications, including resource allocation, budgeting, and portfolio optimization, where multiple constraints must be simultaneously satisfied."}, {"title": "Constraint Programming Model", "content": "Let xi be a binary variable indicating if the item i is inserted into the knapsack. This problem can be easily formalized as a constrained optimization problem.\n$\\max_{X} \\sum_{i=1}^{n} V_iX_i$\n(16)\ns. t. $\\sum_{i=1}^{n} w_{id}x_i \\leq W_d$ $\\forall d \\in \\{1, ..., D\\}$\n$x_i \\in \\{0,1\\}$ $\\forall i \\in \\{1,..., n\\}$"}, {"title": "Graph Encoding", "content": "We propose the following encoding to represent a MKP instance into a graph G(V, E). We recall that in our representation, each node v \u2208 V corresponds to a specific Lagrangian multiplier. A node v is created for each pair of variable and constraint present in the problem instance. Then, an instance of n items and d dimensions yields a graph with n \u00d7 d nodes. Nodes are linked by edges if they share the same variable or constraint. Each node v is also decorated with six features:\n\u2022h: the index of the related variable.\n\u2022 h2: the index of the related constraint.\n\u2022 h\u00b3: the profit of the item.\n\u2022 ht: the weight of the item on the related dimension.\n\u2022 h: the ratio of profit to weight.\n\u2022h: the ratio of weight to capacity."}, {"title": "Shift Scheduling Problem", "content": "The Shift-Scheduling Problem (SSP) involves scheduling employees' activities (e.g., work, break, lunch, rest) while adhering to labor regulations. Constraint programming is well-suited for solving this problem, as complex work regulations can be naturally formalized using REGULAR constraints (Pesant 2004). These constraints are typically specified using a non-deterministic finite automaton A, defined by the 5-tuple A = (Q, A, \u03b4, qo, F) where:\n\u2022 Q is a finite set of states;\n\u2022 A is the alphabet;\n\u2022 \u03b4: Q \u00d7 A \u2192 Q is the transition function;\n\u2022 qo\u2208Q is the initial state;\n\u2022 FCQ is the set of final states.\nThe constraint REGULAR((x1,..., xn), A) is satisfied if the sequence of variable values (x1,...,xn) belongs to the regular language recognized by a deterministic finite automaton A. Intuitively, each variable xi represents an employee's activity at time i. The constraint is satisfied if if the sequence of states complies with the transition function and leads to a final state in the automaton. We consider in this paper a simple yet challenging version of the SSP with one employee, complex labor regulations, and multiple regular constraints."}, {"title": "Constraint Programming Model", "content": "Let W be the set of work activities, T the set of periods, A the set of deterministic finite automata representing labor regulations, and pij (from a table P) the profit from assigning activity i \u2208 W to period j\u2208 T. Each period must have an assigned activity that respects the automata transitions and maximizes total profit. Let xi"}]}