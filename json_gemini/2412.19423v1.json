{"title": "REVISITING PCA FOR TIME SERIES REDUCTION IN TEMPORAL DIMENSION", "authors": ["Jiaxin Gao", "Wenbo Hu", "Yuntian Chen"], "abstract": "Deep learning has significantly advanced time series analysis (TSA), enabling the extraction of complex patterns for tasks like classification, forecasting, and regression. Although dimensionality reduction has traditionally focused on the variable space-achieving notable success in minimizing data redundancy and computational complexity-less attention has been paid to reducing the temporal dimension. In this study, we revisit Principal Component Analysis (PCA), a classical dimensionality reduction technique, to explore its utility in temporal dimension reduction for time series data. It is generally thought that applying PCA to the temporal dimension would disrupt temporal dependencies, leading to limited exploration in this area. However, our theoretical analysis and extensive experiments demonstrate that applying PCA to sliding series windows not only maintains model performance, but also enhances computational efficiency. In auto-regressive forecasting, the temporal structure is partially preserved through windowing, and PCA is applied within these windows to denoise the time series while retaining their statistical information. By preprocessing time-series data with PCA, we reduce the temporal dimensionality before feeding it into TSA models such as Linear, Transformer, CNN, and RNN architectures. This approach accelerates training and inference and reduces resource consumption. Notably, PCA improves Informer training and inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%, without sacrificing model accuracy. Comparative analysis against other reduction methods further highlights the effectiveness of PCA in improving the efficiency of TSA models.", "sections": [{"title": "Introduction", "content": "Time series analysis (TSA) plays a pivotal role across various fields , owing to its ability to extract valuable information from sequential data, facilitating accurate predictions and classifications. Recent advancements in the field have witnessed the emergence of sophisticated deep-learning models  designed to effectively analyze time series data.\nDimensionality reduction techniques have been successfully applied to reduce complexity in time series data, but their focus has primarily been on the variable dimension . These methods, which aim to minimize redundancy in variable space, have been effective in reducing computational complexity and improving model performance. However, far less attention has been given to reducing the temporal dimension, despite the potential benefits of alleviating the burdens associated with processing long time series.\nThe time series lengths are generally larger than the number of variable sizes, suggesting that temporal dimensionality reduction should provide better compression. The long time series is segmented to time series windows in the auto-regressive forecasting task, and recent studies show that larger window length includes more temporal information"}, {"title": "Related Work", "content": "Time Series Analysis Models. Traditional TSA models like ARMA and ARIMA  rely on statistical foundations, assuming linear relationships between past and present observations to discern patterns in the time series data. However, the rise of deep-learning models has gained significant attention in TSA due to their enhanced expressive capability and ability to effectively utilize available data, leading to improved performance over traditional statistical models. RNN-based models  are initially employed in TSA tasks due to their capability to process sequences and capture temporal dependencies. However, due to their inherent difficulties in propagating gradients through many time steps, RNN-based models often encounter issues of gradient vanishing or gradient explosion . CNN-based models  constitute a major branch within deep-learning, aiming to capture temporal dependencies through convolutional layers. Notably, TimesNet transforms 1D time series into a set of 2D tensors based on multiple periods and employs a CNN structure to extract features from these tensors. Transformer-based models have also found widespread application in TSA, leveraging self-attention  to capture long-term dependencies across different time steps. Informer  achieves a complexity reduction to O(L log L) by replacing the conventional self-attention mechanism with KL-divergence-based ProbSparse attention. FEDformer  achieves a complexity reduction to O(L) by employing frequency-domain self-attention through the use of Fourier or wavelet transforms and the random selection of frequency bases. Fredformer  is designed to address frequency bias in TSF by ensuring equal learning across various frequency bands. PatchTST  partitions the time series into multiple segments, treating each as a token, and employs an attention module to learn the relationships between these tokens. Additionally, DLinear  leverages a linear model to attain noteworthy results in TSF tasks, demonstrating the efficacy of linear models in the domain of TSA. SparseTSF  is a lightweight Linear-based model that uses Cross-Period Sparse Forecasting to decouple periodicity and trend, achieving superior performance with fewer parameters.\nPCA applications in various domains. PCA  has diverse applications in various domains . In computer vision,  proposes a texture-defect detection method using PCA, requiring only a few unlabelled samples and outperforming traditional and deep-learning methods for small and low-contrast defects.  introduces a tensor robust kernel PCA model to effectively capture the intrinsic low-rank structure of image data. For natural language processing, R\u00e9mi and Ronan simplify word embeddings via PCA, outperforming existing methods on named entity recognition and movie review tasks . In bioinformatics,"}, {"title": "Methodology", "content": "Given a historical series window H = {X1, ..., XL}, where L represents the length of the series window, we consider three TSA core tasks. 1) Time Series Classification (TSC): The objective is to predict a discrete class label C for the series H; 2) Time Series Forecasting (TSF): The goal is to forecast future values of the same series, denoted as F = {XL+1, ..., XL+T}, where T is the number of future time steps to predict; 3) Time Series Extrinsic Regression (TSER): In some applications like predicting heart rate from photoplethysmogram and accelerometer data , neither forecasting nor classification is applicable. TSER involves predicting a single continuous target value V external to the series based on the input historical series H.\nBy encapsulating all inputs (historical series window H) and outputs (class label C, future series F or external target value V), a complete series dataset D can be formed, where D = [X; Y]. Here, X is composed of all the historical series, X = [H1; . . . ; Hm], with m representing the number of samples, and Y consists of the corresponding targets to be predicted (C, F, or V). The entire dataset D can be split into the training set Dtrain, validation set Dval, and test set Dtest. PCA-related parameters (covariance matrix, eigenvalues, and eigenvectors) are obtained from the training set Dtrain and subsequently applied to both the validation set Dval and the test set Dtest without re-estimation."}, {"title": "Enhancing Time Series Analysis with PCA", "content": "Principal Component Analysis (PCA)  is a classical technique for dimensionality reduction and feature extraction. For the training set of the series dataset Dtrain, which consists of n training samples, each with L features (where L is also the length of the series window, and each time step corresponding to a feature), PCA aims to transform the series data into a new coordinate system where the data variance is maximized along the principal components. The process can be summarized as follows:\n1. Mean-Centering: Before applying PCA, the mean of each feature (time step) is subtracted from the corresponding column to center the data. The mean-centered matrix is denoted as Dcentered, with each element given by:\nDcentered(i, j) = Dtrain(i, j) \u2013 \\overline{Dtrain} (j),\nwhere \\overline{Dtrain} (j) is the mean of the j-th column.\n2. Covariance Matrix: The covariance matrix C is computed based on the mean-centered data:\nC = \\frac{1}{n-1}Dcentered^T \\cdot Dcentered.\n3. Eigenvalue Decomposition: PCA involves finding the eigenvalues \\lambda_1, \\lambda_2, ..., \\lambda_m and corresponding eigenvectors u_1, u_2, ..., u_m of the covariance matrix C. The eigenvalues represent the variance along each principal component.\n4. Selecting Principal Components: The principal components are selected based on the proportion of variance they explain. The k-th principal component is given by PCk = Dcentered \\cdot u_k, where vk is the k-th eigenvector.\n5. Reducing Dimensionality: To reduce the dimensionality of the time series data, the original data matrix is projected onto the first k principal components, forming a new matrix Dpca \\in R^{n \\times k}:\nDpca = Dcentered \\cdot V_k,"}, {"title": "Intuitional Justifications on PCA's Effectiveness in Time Series Reduction", "content": "PCA is effective in time series data reduction due to several key advantages. It serves as an efficient noise reduction tool by filtering out low-variance noise and retaining high-variance features. Additionally, PCA preserves the critical statistical characteristics of the original series.\nPCA acts as an efficient tool for noise reduction within historical series. By projecting the original historical series onto a new set of orthogonal components, PCA effectively filters out the noise contained in the lower variance components, thus retaining the core information of the historical series. Consequently, the PCA process helps to mitigate overfitting in subsequent TSA models.\nPCA retains the critical statistical characteristics of the original time series data. The key statistical characteristics preserved by PCA include the mean, sum, peak values, and higher-order moments. Specifically, for the mean/sum values of historical series, PCA simply maps the original time points into a different coordinate system, preserving the relative mean/sum values of different historical series. Furthermore, PCA preserves higher-order moments, including skewness and kurtosis , because its linear transformation ensures that these higher-order statistical characteristics remain intact. The preservation of those distinctive statistical characteristics in the PCA-reduced series enables effective learning by subsequent TSA models.\nSpecific trends and periodic patterns in historical series may not be crucial for the learning of TSA models. While some time series research focuses on extracting trends or periodicity , we argue that these specific trends and periodic patterns are not necessarily essential for effective TSA model learning. For example, if all positive trends in the historical series are reversed, the relative distribution of the data remains unchanged, and the TSA model's ability to learn and predict is not impaired. Similarly, if the periodic components in all historical series are minified or magnified, the model's predictive capabilities should not be affected. These observations suggest that the presence of specific trend or periodicity in historical series is not necessarily essential for the learning process of TSA models. Instead, the presence of consistent and coherent patterns is sufficient for models to provide accurate predictions. Therefore, although PCA may alter the trend or periodicity, it introduces new coherent patterns\u2014such as the main directions of variation, denoised low-dimensional representations, and latent features\u2014that effectively benefit TSA model learning."}, {"title": "Experiments", "content": "To validate the effectiveness of PCA in time series compression and temporal dimensionality reduction, experiments are conducted on three mainstream TSA tasks: time series classification (TSC), time series forecasting (TSF), and time series extrinsic regression (TSER). Four types of advanced time series models are evaluated for TSA. 1) MLP(linear)-based model: Linear ; 2) Transformer-based models: Informer , FEDformer , and PatchTST ; 3) CNN-based model: TimesNet ; 4) RNN-based models: Gated Recurrent Unit (GRU)  and Long Short-Term Memory (LSTM) . Sections 4.1-4.3 compare the performance of TSA models in TSC, TSF, and TSER tasks, both with and without PCA preprocessing. The results show that PCA maintains model performance by retaining the principal information of the original time series. Section 4.4 highlights PCA's optimization of training/inference in TSA models, notably accelerating Informer by up to 40% and reducing TimesNet's GPU memory usage by 30%."}, {"title": "Time Series Classification", "content": "We perform sequence-level TSC experiments using five datasets from the UEA archive . Our experiments adhere to the settings outlined in the \u201cTime Series Library\u201d , with the exception that we focus solely on the last dimension of each dataset, resulting in a univariate TSC problem. Model performance is assessed using the accuracy metric. Historical series lengths vary across datasets, with principal components set to 16, 48, and 96."}, {"title": "Time Series Extrinsic Regression", "content": "We conduct TSER experiments using four univariate datasets from the study . These datasets are from the domains of environmental monitoring and disease diagnosis. The metrics RMSE and MAE are used to evaluate the performance of the models. The length of historical series varies across datasets, with a length of 266 for FloodModeling datasets and 84 for the Covid3Month dataset. The number of principal components is set to 48 for FloodModeling and 16 for Covid3Month."}, {"title": "Training/inference Optimization with PCA", "content": "The aforementioned results indicate that PCA successfully preserves essential information in time series while maintaining TSA models' performance across tasks of TSC, TSF, and TSER, and we also investigate the impact of PCA on reducing computational burden and accelerating training/inference."}, {"title": "Conclusion", "content": "Our study challenges the perception that PCA, by disrupting sequential relationships in time series, is unsuitable for TSA. Instead, we find it efficient for handling TSA tasks. PCA is innovatively applied to achieve temporal dimensionality reduction while safeguarding essential information within time series. Its effect is evaluated on four types of advanced time series models, namely Linear, Transformer, CNN and RNN models, across three typical TSA tasks: classification, forecasting, and regression. The results show that PCA reduces computational burden without compromising performance. Specifically, in TSC, the performance with PCA is better in 50.0% of cases; in TSF, 49.6% of cases; and in TSRE, 66.7%. Notably, PCA accelerates Informer's training and inference by up to 40%, with a minimum of 10% speedup for other models. Additionally, PCA reduces GPU memory usage by 15% for Transformer-based models and 30% for CNN-based models. The study discusses PCA's theoretical effectiveness in denoising and preserving statistical information, further substantiating its superiority over alternative dimensionality reduction methods such as series shortening, downsampling, and the integration of additional reduction layers."}, {"title": "Additional Experiments of PCA Preprocessing in TSA", "content": "To better illustrate the of PCA in TSA, we have supplemented this section with extensive experiments. These include tests on additional models and datasets, as well as comparisons between PCA and representation learning-based models."}, {"title": "Time Series Classification on UCR Datasets", "content": "The UCR dataset [Dau et al., 2019] contains many time series classification datasets. To comprehensively evaluate the performance of PCA in TSC tasks, 16 datasets from the UCR dataset are selected for testing, as shown in Table 8. The results demonstrate that PCA preprocessing retains the principal information of the series on the UCR dataset, matches the TSC performance of the original series, and enables faster training and inference."}, {"title": "PCA's Applications in Additional TSC Models", "content": "Some effective specialized TSC models, such as InceptionTime [Ismail Fawaz et al., 2020] and ResNet [Cheng et al., 2021], have been developed and widely applied in various TSC tasks. We also applied PCA to these models. The results in Table 9 show that PCA is model-agnostic and remains effective even when applied to these specialized TSC models."}, {"title": "Comparison of PCA with Representation Learning-based Methods", "content": "Some representation learning-based methods, such as TS2Vec [Yue et al., 2021], T-Loss [Franceschi et al., 2019], and TimeVQVAE [Lee et al., 2023], can also compress time series data by learning their representations and then use"}, {"title": "TSF Results of PatchTST with PCA Preprocessing", "content": "PCA preprocessing is separately applied to each patch series in the patch-based time series model PatchTST. Additionally, to enhance prediction stability, PatchTST employs instance normalization technology [Kim et al., 2022]. However, integrating this technology with PCA series poses challenges: the fluctuation of PCA series is considerable, and adding instance normalization further destabilizes the predictions. Consequently, after applying PCA processing, we exclude the instance normalization module from PatchTST. For comparative analysis, we also assess the performance of PatchTST without the instance normalization module on the original series."}, {"title": "TSF Results of RNN-based Models with PCA Preprocessing", "content": "Due to issues with gradient vanishing or exploding [Hanin, 2018], RNN-based models exhibit unstable performance in TSA with long historical series windows and have consequently been increasingly supplanted by Transformer, linear, and CNN-based models. Nonetheless, to more comprehensively evaluate the impact of PCA preprocessing, we assess its effect on RNN-based models for TSF tasks. Specifically, two typical RNN-based models, GRU [Chung et al., 2014] and LSTM [Hochreiter, 1997], are tested. Original historical series or PCA series are fed into the GRU or LSTM cells to extract features, and their hidden state h, containing the feature information, are projected and transformed to obtain the final predictions. Additionally, since RNN models process time series sequentially, their computational cost is more sensitive to the length of the model input. Although RNN-based models are not as commonly used as other models, PCA remains an effective tool for time series reduction in scenarios where they are appropriate."}, {"title": "PCA's Tests on Electricity and Traffic Datasets", "content": "We also applied PCA to the commonly used TSF datasets, Electricity and Traffic. The results in Table 15 show that PCA preprocessing retains series principal information on Electricity and Traffic datasets, matching TSF performance with original series, and enabling training/inference acceleration."}, {"title": "Detailed Training/inference Time", "content": "Table 16 presents the average training and inference time (including PCA processing time) for various time series models, evaluated across different TSA tasks. With the assistance of PCA preprocessing, the training and inference of the models are accelerated to varying degrees."}, {"title": "Impact of the Number of Principal Components", "content": "The number of principal components is a crucial hyperparameter in PCA. If too many principal components are selected, the reduction in dimensionality may be insufficient, failing to achieve the desired acceleration in training/inference. Conversely, too few principal components can result in the loss of important features, leading to a decline in model performance."}, {"title": "PCA Visualizations and Prediction Showcases", "content": "Fig. 7 depicts the shapes of series after PCA preprocessing and the series obtained by inverse transforming PCA series. It is evident that PCA series include the primary information of the original series with a small subset of initial values (principal components), while the remaining values exhibit minimal fluctuations. The similarity of the original series can also be reflected in the PCA series. Furthermore, series inverse transformed from PCA series appear significantly smoother compared to the original series, effectively achieving denoising of the series."}]}