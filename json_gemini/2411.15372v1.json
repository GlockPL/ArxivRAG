{"title": "Transforming NLU with Babylon: A Case Study in Development of\nReal-time, Edge-Efficient, Multi-Intent Translation System for Automated\nDrive-Thru Ordering", "authors": ["Mostafa Varzaneh", "Pooja Voladoddi", "Tanmay Bakshi", "Uma Gunturi"], "abstract": "Real-time conversational AI agents face chal-\nlenges in performing Natural Language Un-\nderstanding (NLU) in dynamic, outdoor envi-\nronments like automated drive-thru systems.\nThese settings require NLU models to handle\nbackground noise, diverse accents, and multi-\nintent queries while operating under strict la-\ntency and memory constraints on edge devices.\nAdditionally, robustness to errors from up-\nstream Automatic Speech Recognition (ASR)\nis crucial, as ASR outputs in these environ-\nments are often noisy. We introduce Babylon,\na transformer-based architecture that tackles\nNLU as an intent translation task, converting\nnatural language inputs into sequences of reg-\nular language units ('transcodes') that encode\nboth intents and slot information. This formula-\ntion allows Babylon to manage multi-intent sce-\nnarios in a single dialogue turn. Furthermore,\nBabylon incorporates an LSTM-based token\npooling mechanism to preprocess phoneme se-\nquences, reducing input length and optimiz-\ning for low-latency, low-memory edge deploy-\nment. This also helps mitigate inaccuracies\nin ASR outputs, enhancing system robustness.\nWhile this work focuses on drive-thru order-\ning, Babylon's design extends to similar noise-\\prone scenarios, for e.g. ticketing kiosks. Our\nexperiments show that Babylon achieves signif-\nicantly better accuracy-latency-memory foot-\nprint trade-offs over typically employed NMT\nmodels like Flan-T5 and BART, demonstrat-\ning its effectiveness for real-time NLU in edge\ndeployment settings.", "sections": [{"title": "1 Introduction", "content": "In the evolving landscape of conversational AI,\nefficient natural language understanding compo-\nnents are crucial, particularly for real-time, edge-\ndeployed applications like automated drive-thru or-\ndering. A recent trend has framed intent classifica-\ntion and slot filling as a translation task (De bruyn\net al., 2022; Sowa\u0144ski and Janicki, 2023; Hoscilowicz\net al., 2024), transforming user utterances into\nstructured outputs with intent and slot labels. How-\never, not many works have captured effort entailed\nin making this approach highly performant and ef-\nficient for low-latency, edge-deployed systems. To\naddress this, we propose a method that represents\nintents and slots as regular language units. This\nrepresentation enhances processing efficiency, par-\nticularly in edge environments with limited com-\nputational resources and memory. Furthermore,\nwe introduce Babylon, a custom model optimized\nfor edge deployment, which uses LSTM-based to-\nken pooling alongside a Transformer architecture\nto manage multi-intent scenarios efficiently. This\nenhancement plays a critical role in reducing the"}, {"title": "2 System Description", "content": "As depicted in Figure 2, our drive-thru conversa-\ntional AI integrates four core components: Au-\ntomatic Speech Recognition (ASR), Natural Lan-\nguage Understanding (NLU), Dialog Management,\nand Text-To-Speech (TTS) synthesis. This setup\nenables real-time, accurate customer service in-\nteractions with limited compute resources. The\nworkflow is as follows (Refer to Figure 2):\n1. Neural ASR transcribes real-time audio into\nphonemes, input for the NLU component.\n2. NLU converts phonemes into regular lan-\nguage units, transcodes, representing cus-\ntomer intents.\n3. Dialog Management processes transcodes, up-\ndates order state, and triggers TTS to respond\nto the customer.\nThis work focuses specifically on the NLU com-\nponent that employs a NMT model, whose input is\na sequence of phonemes, and output is a sequence\nof \"transcodes\". For e.g, given a natural language\ndrive-thru order (speech) p that can be transcribed\ninto a phoneme sequence p = [P1,P2, \u2026, Pi, \u2026, Pm],\ngoal is to predict the customer intent, a transcode\nsequence t = [t1, t2, 13, ..., ti, ...tn]. Here, pi is a\nphoneme; unit of textual representation of sound\nemitted from our ASR component, and transcode\nti is a token (word) of an internally created reg-\nular language that represents complex customer\nintents. Note that \"complex\" in this context means\ncompositionally representing multiple intents and\nslots referenced within one inference call. For e.g.\n\"Hi, can i get two chocolate chip cookies please\nah ... actually no ... can you just do two peanut\nbutter cookies instead?\" - this single turn within\na customer order contains multiple intents such as\ntwo addItem intents and one deleteItem intent in\nreference to three different entities (quantity, item\nnames), all processed within a single customer turn\n(a single inference call.)"}, {"title": "3 Related Work", "content": "3.1 Conventional NLU approaches\nTask-oriented dialogue systems have traditionally\nemployed (1) separate models for intent detec-\ntion (ID) and slot filling (SF), such as support\nvector machines, conditional random fields (Xu\nand Sarikaya, 2014) and recurrent neural networks\n(RNNs) of various types (Kurata et al., 2016),\nwhich often result in error propagation and sub-\noptimal performance due to underutilized cross-"}, {"title": "3.2 Neural Machine Translation for\nMulti-Intent Detection", "content": "NMT has proven highly effective in various do-\nmains, transforming input sequences into output\nsequences for structured tasks like regex modeling\n(Locascio et al., 2016) and formal languages such\nas SQL (Ferreira et al., 2020; Bandyopadhyay and\nZhao, 2020). While works such as (De bruyn et al.,\n2022; Sowa\u0144ski and Janicki, 2023; Hoscilowicz\net al., 2024) have explored NMT-based approaches\nfor ID and SF, to the best of our knowledge, their\napplication to low latency and edge-efficient multi-\nintent detection remains largely unexplored.\nTo this end, this paper proposes Babylon, a cus-\ntom NMT-based architecture designed for trans-\nlating user queries represented in the form of\n\"phonemes,\" into regular language units referred to\nas \"transcodes,\" which encapsulate complex cus-\ntomer intents. Babylon is tailored for multi-intent\ndetection in a real time drive-thru order-taking sce-\nnario, operating on resource-constrained edge de-\nvices without cloud connectivity."}, {"title": "3.3 Limitations of vanilla Transformers", "content": "Computational Complexity: The standard Trans-\nformer's time and memory complexity, scales\nwith input size and layers, and is impractical for\nresource-constrained settings, especially with long\ninput sequences. Sub-word tokens like phoneme-\nbased textual representation further increases input\nlength, worsening computational demands. For\ne.g.,\"can I get a coffee please?\" becomes k \u0259n a\u0131\ng\u0131t\u0259k\u0259 fipliz which has 16 tokens\nrather than 6.\nLoss of Sequential Information: While Trans-\nformers can handle long sequences, they funda-\nmentally rely on positional encodings (Chen et al.,\n2021) and lack an inherent mechanism to capture\nsequential order (Haviv et al., 2022). This can be\nsuboptimal for tasks where the order of input ele-\nments is critical, such as in our drive-thru use case.\nFor example, people tend to repeat themselves\nwhen ordering food (\"cookie uh cookie ... choco-\nlate\"), produce false starts (\"and for the cookie...\nuh yea the chocolate cookie\"), or repeat entire\nphrases (\"I'll get the cookie... uh yeah I'll just\nget a cookie\"). Here, the challenge for the model\nis to detect and correct duplicated intents based on\nthe position, distance and context.\nInadequate Local Context Representation:\nTransformers primarily focus on global dependen-"}, {"title": "4 Babylon Architecture", "content": "The components that are unique to Babylon are\nhighlighted in Figure 3. We process inputs through\nbidirectional LSTM network, followed by a token-\npooling layer, finally passing the now context-rich,\ndownsampled inputs to the Transformer network.\nWe believe that this approach is easy to implement\nand addresses challenges discussed earlier in sec-\ntion 3.3:"}, {"title": "4.1 LSTM, the hero!", "content": "To this end, we incorporate an LSTM layer fol-\nlowed by a token pooling layer prior to generating\nthe input embedding and positional encoding. Ad-\nditionally:\nLSTM as neural rolling memory: The LSTM\nacts as a learnable memory for the transformer\nlayer, providing context access.\nLSTM for keeping temporal information: Al-\nthough positional embeddings capture relative to-\nken positions, transformers struggle with temporal\nor spatial information (Dosovitskiy et al., 2021;\nDai et al., 2019; Zeng et al., 2022) while an LSTM\nlayer by nature preserves sequential information.\nLSTM as regularizer: Our findings indicate that\nvanilla Transformers exhibit sensitivity to noisy to-\nkens from upstream ASR errors. By employing\nan LSTM with a low-dimensional hidden space,\nwe can effectively regularize and compress infor-"}, {"title": "4.2 Downsampling by token pooling", "content": "Passing LSTM outputs to the transformer increases\ncomputational and memory demands. We apply to-\nken pooling, passing every kth LSTM output unit to\nthe transformer, reducing computations and further\naiding in regularization of erroneous ASR tokens.\nTo ensure the model sees the <EOS> token for\ndecoding, pooling starts from the end. In this work,\nwe experimented with a range of k and selected\nk=4 for optimal performance."}, {"title": "5 Experiments", "content": "5.1 Hardware Setup\nFor training our models, we utilized AWS Cloud,\nspecifically p3.16xlarge instances equipped with\nNVIDIA Tesla V100 GPUs. Each p3.16xlarge\ninstance features Intel Xeon 2.3GHz CPUs, 488\nGB of memory, and 8 NVIDIA Tesla V100 GPUs.\nWe implement the Transformer-variants using Py-\nTorch* v2.0.0. The LLM based models are from\nthe Transformers library v4.0.1 (Wolf et al., 2020).\nThe training times, number of parameters and the\nsizes for each candidate model architecture are\nelaborated in Table 1. Additional information re-\nlated to training hyperparameters used are reported\nin Table 3."}, {"title": "5.3 Model architectures and Training\nHyperparameters", "content": "Atlantis (CNN + Bi-directional LSTM) A1-D\nconvolutional layer to encode input to a 2-layer\nbidirectional LSTM network, inspired by (Graves,\n2012), trained with CTC loss (Graves et al.).\nDelphi (LSTM encoder-decoder with Atten-\ntion) 3-layered LSTM encoder decoder architec-\nture with 256-dimensional embeddings, following\n(Bahdanau et al., 2014)\nCamelot (Vanilla Transformer) A base vanilla\nTransformer proposed by (Vaswani et al., 2017)\nwithout any architectural upgrades.\nBabylon (LSTM-Pooled Transformer) A Bi-\nLSTM stack to encode the input sequence, fol-\nlowed by a pooling layer producing a 256 dimen-\nsional vector as input to the standard Transformer\nencoder-decoder. For more details, refer to Section\n4. Training configurations for optimal results are\nin Appendix A.1."}, {"title": "5.3.1 LLM based translation models", "content": "We compare the above baselines with two open\nsource off-the-shelf LLM-NMT models: (i) flan-\nt5-small (Chung et al., 2024) and (ii) bart-\nbases (Lewis et al., 2019)"}, {"title": "5.4 Evaluation Metrics", "content": "Accuracy This metric checks for a 1:1 match\nbetween the NLU model's output - sequence of\ntranscodes - with the ground truth, and accounts for\nexact match of customer intents as well as menu en-\ntity names and number of the corresponding menu\nitems."}, {"title": "6 Results", "content": "We summarize the results across the following met-\nrics: (a) accuracy, (b) latency, and (c) memory foot-\nprint trade-offs and compare these results across\nthe model architectures chosen earlier in Section\n5.3.\nBabylon is the best-performing model in\nterms of accuracy. From Table 2, Babylon\nachieves the highest accuracy at 90.07%. While\nCamelot has a slightly smaller memory footprint,\nwith 88.87% accuracy, we choose Babylon of the\ntwo since the 1.2% lift in accuracy significantly\naffects the application's business metrics.\nCamelot has the fastest latency. Camelot\ndemonstrates the best average inference latency\nwith 1.56 ms per phoneme and 77 ms per order\nturn, making it ideal for real-time applications.\nHowever, its lower accuracy compared to Babylon,\nparticularly in handling sequential dependencies,\nmakes it less suitable for our production needs (ex-\namples provided in A.3). Moreover, increasing the\nnumber of parameters for Camelot did not improve\nthe accuracy of the model significantly.\nBabylon manages memory footprint effec-\ntively. While attention-based models (Babylon,\nCamelot and Delphi) have a larger footprint rela-\ntive to Atlantis, Babylon remains manageable for\nedge hardware considerations with a memory foot-\nprint of 118 MB. LLMs such as BART are too\ncumbersome to deploy efficiently in our settings as\nopposed to other candidates.\nFlan-T5 and BART are not suitable for our"}, {"title": "7 Limitations and Future Work", "content": "This work did not explore alternative pooling strate-\ngies such as CNN-based pooling that is typically\nused in vision-based transformers. Future work\ncould evaluate whether this alternative would yield\nfurther improvements w.r.t model memory foot-\nprint. Additionally, this work currently uses greedy\ndecoding strategy, and it remains to be seen if em-\nploying beam search decoding strategy would pro-\nvide further accuracy gains."}, {"title": "8 Conclusion", "content": "This work introduces Babylon, an edge-efficient\narchitecture designed for NMT-based intent trans-\nlation in drive-thru ordering systems. Babylon\nutilizes an LSTM-based token pooling mecha-\nnism alongside a standard Transformer to trans-\nlate customer speech (phonemes) into actionable\nintents (which we call transcodes), enhancing the\nmodel's ability to manage multi-intent scenarios.\nThe LSTM component provides a rolling memory"}, {"title": "A Appendix", "content": "A.1 Training Hyperparameters\nIn Table 3 below, we provide the hyperparame-\nters we used to finetune the models used in this\nwork. Note that, for all the LSTM based models, a\ndropout with a probability of 0.1 is applied to the\nLSTM layers to prevent overfitting, as suggested\nby (Zaremba et al., 2014)."}, {"title": "A.2 Limitations of BLEU and ROUGE\nMetrics for Transcodes Evaluation", "content": "To gain better insight into why evaluation metrics\nsuch as BLEU and ROUGE are not applicable to\nour use case, we selected a random 200 orders\nand analyzed the output transcodes generated by\nour candidate model Babylon. We classified each\noutput into one or more of the following eight cat-\negories of prominent errors:"}, {"title": "A.2.1 Lack of Semantic Understanding", "content": "The primary limitation of using BLEU for our\ntranscodes lies in their inability to accurately as-\nsess the semantic content of the outputs. These\nmetrics focus on the exact matching of words and\ntheir order, which is not always indicative of the\nuser intent captured within our transcodes. The\nfollowing examples illustrate common scenarios\nwhere these metrics fall short in our use case:\nWord Order Errors One significant limitation\nof the BLEU metric is its position independence.\nBLEU penalizes candidate transcodes that do not\ncapture the exact order of the items, even if the\nsemantics remain unchanged. For example:\nReference: [1, fudge, item, add, 1, cookie, item,\nadd]\nHypothesis: [1, cookie, item, add, 1, fudge, item,\nadd]\nBLEU Score: 0.759\nDespite both sequences representing the same set\nof items, the BLEU score is penalized due to the\ndifference in order. Conversely, consider below\nexample:\nReference: [1, fudge, item, add, 1, cookie, item,\nadd]\nHypothesis: [1, fudge, item, add, 1, cookie, item,\nadd]\nBLEU Score: 1\nThe BLEU score is a perfect 1 when the order\nmatches exactly, showing its insensitivity to se-\nmantically equivalent variations in order.\nSynonyms BLEU metric fail to account for syn-\nonyms that are common in our transcodes lan-\nguage."}, {"title": "A.2.2 Severity of errors not considered", "content": "Within a product use case, such as ours, agent's\nmistaking one item for another or the inability to\ndelete / add / update items when the models should,\ncan translate to significant operational and financial"}, {"title": "A.2.3 Sentence length affects the scores", "content": "n-gram based BLEU scores are unfairly affected\nby the length of the sentences. In drive-thru orders,\nwhich often contain long orders / sequences, the\nn-gram based BLEU score is particularly sensitive\nto mismatches. For instance:\nReference: [yes, polar_answer]\nHypothesis: [yes, polar_answer, thanks]\nBLEU Score: 0.0\nHere, the addition of thanks in the hypothesis\nresults in a BLEU score of 0, despite capturing the\nmain intent (i.e., a yes) correctly.\nNormalization errors BLEU metrics penalize\nbased on exact matches, which can be problematic\nwhen the orders are not normalized (e.g., plurals vs.\nsingular, inconsistent capitalization). For example:\nReference: [2, cone, item, add, 1]\nHypothesis: [2, Cone, item, 'add, 1]\nBLEU Score: 0.286\nIdeally, the BLEU score should be 1, but the\nscores were significantly lower despite the orders\nbeing identical."}, {"title": "A.3 Examples to showcase performance of\nCamelot vs. Babylon", "content": "Camelot exhibits loss of sequential informa-\ntion.\nInput: two scoop with sprinkles with medium\nwaffle cone\nCamelot output: [\"new_item\", \"1\", \"scoop\",\n\"extras\", \"1\", \"sprinkles\", \"new_item\", \"1\",\n\"Medium\", \"size\", \"1\", \"waffle cone\", \"quantity\",\n\"2\", \"add_item\"]\nBabylon output:\n[\"new_item\", \"2 scoop\", \"extras\", \"1\", \"sprin-\nkles\", \"new_item\", \"1\", \"Medium\", \"size\", \"1\",\n\"waffle cone\", \"quantity\", \"1\", \"add_item\"]\nThe phrase \"two scoop with sprinkles\" above\ninvolves a clear sequential relationship between"}, {"title": "Camelot shows inadequate local context repre-sentation.", "content": "Input: small caramel ice cream with whipped\ncream and ice cream cone\nCamelot output: [\"new_item\", \"1\", \"small\",\n\"size\", \"1\", \"caramel ice cream\", \"extras\", \"1\",\n\"whipped cream\",\"new_item\", \"1\", \"ice cream\",\n\"quantity\", \"1\", \"cone\", \"add_item\"]\nBabylon output: [\"new_item\", \"1\", \"small\",\n\"size\", \"1\", \"caramel ice cream\", \"extras\", \"1\",\n\"whipped cream\",\"add_item\", \"new_item\", \"1\",\n\"ice cream cone\", \"add_item\"]\nIn the above case, the phrase \"ice cream cone\"\nis a compound noun that refers to a single item.\nCamelot incorrectly interprets it as two separate\nitems (\"ice cream\" and \"cone\"), failing to capture\nthe local relationship between the words. This hap-\npens because Transformers, with their global at-\ntention mechanism, often overlook the tight, local\ndependencies between nearby tokens, especially in\nphrases that represent singular entities (like \"ice\ncream cone\")."}, {"title": "Babylon acts as an effective regularizer for erro-neous phoneme sequences. Input (erroneousfrom upstream ASR): small caremel ice creamwith mcadam nuts", "content": "Correct Input: small caramel ice cream with\nmacadamia nuts\nCamelot output: [\"new_item\", \"1\", \"small\",\n\"size\", \"1\", \"caramel ice cream\", \"add_item\"]\nBabylon output: [\"new_item\", \"1\", \"small\",\n\"size\", \"1\", \"caramel ice cream\", \"extras\", \"1\",\n\"macademia nuts\", \"add_item\"]\nThe error in the above case involves the substi-\ntution of the word \"macadamia\" with \"McAdam,\"\na phonetically similar but incorrect entity for the\ndomain. Camelot (vanilla Transformer) ultimately\nignores this entity entirely. Babylon's LSTM and\ntoken pooling architecture handles this error much\nbetter; it sequentially processes the phoneme input\nand retains a local context that helps identify famil-\niar patterns like \"macadamia nuts\" with erroneous"}]}