{"title": "How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning", "authors": ["Giuseppe Serra", "Ben Werner", "Florian Buettner"], "abstract": "Many real-world applications require machine-learning models to be able to deal with non-stationary data distributions and thus learn autonomously over an extended period of time, often in an online setting. One of the main challenges in this scenario is the so-called catastrophic forgetting (CF) for which the learning model tends to focus on the most recent tasks while experiencing predictive degradation on older ones. In the online setting, the most effective solutions employ a fixed-size memory buffer to store old samples used for replay when training on new tasks. Many approaches have been presented to tackle this problem. However, it is not clear how predictive uncertainty information for memory management can be leveraged in the most effective manner and conflicting strategies are proposed to populate the memory. Are the easiest-to-forget or the easiest-to-remember samples more effective in combating CF? Starting from the intuition that predictive uncertainty provides an idea of the samples' location in the decision space, this work presents an in-depth analysis of different uncertainty estimates and strategies for populating the memory. The investigation provides a better understanding of the characteristics data points should have for alleviating CF. Then, we propose an alternative method for estimating predictive uncertainty via the generalised variance induced by the negative log-likelihood. Finally, we demonstrate that the use of predictive uncertainty measures helps in reducing CF in different settings.", "sections": [{"title": "1 INTRODUCTION", "content": "Typical machine learning models assume to work in a single-task static scenario where multiple epochs are performed over the same data until convergence. In many real-world situations, however, this setting is too limiting. As an example, let us consider the problem of classifying new COVID-19 variants. Given the evolving nature of the virus, new variants (i.e., new classes) arise over time. In this context, a typical learning model would fail since the standard setting does not consider the dynamic increment of new classes. For this reason, online Continual Learning (online-CL) has been constantly more explored. In online-CL, a single model is required to learn continuously from a sequence of tasks that comes as a stream of tiny batches which can be processed only once [2, 3, 22]. Given the overlap between old and new information, the model tends to forget about the past knowledge to focus more on the newest tasks, leading to a performance degradation on previous tasks. This challenge is usually referred to as catastrophic forgetting (CF) [23, 26]. Many approaches have been developed to prevent cat- astrophic forgetting and the most successful ones employ a memory buffer for rehearsal [10, 28]. Memory-based strategies tackle CF by training the model on both current samples and some old samples stored in a limited size memory. In this context, what differentiates each memory-based approach are the retrieval strategy and the update strategy [22] - i.e., how to populate and update the memory with meaningful and representative samples, and how to efficiently sample from the memory respectively. Although many approaches have been developed in this direction - ranging from using ran- dom approaches [10] to exploiting the gradient [3, 9, 21] or the loss [5] information \u2013 the effect of predictive uncertainty estimates for memory management is largely unexplored. One of the few methods in this direction is presented in Bang et al. [4], where the authors propose a combination of predictive uncertainty and data augmentation at the memory-level to generate a memory of diverse samples - from the most uncertain to the most representative ones for each class. Despite remarkable results, the contribution of the uncertainty score on the overall performance is not easily quantifi- able. Furthermore, considering the quickly expanding landscape of recently rehearsal-based methods proposed, it is difficult to identify a clear strategy to exploit the memory at its best. In fact, contrast- ing strategies can be found in the literature. Some of them suggest to use the most representative samples [17, 33], while others con- sider the samples near the decision boundary as the most useful to reduce CF [19]. Hence, it is unclear which type of data points would reduce CF in a consistent manner: Are the easiest-to-forget or the easiest-to-remember samples more effective in combating CF? To answer these and other questions, this paper investigates the contribution of predictive uncertainty scores on reducing CF. Intuitively, using predictive uncertainty to populate the memory represents a solution for identifying the location of the instances in the decision space and considering either the most representative \u2013 i.e., the k samples with the lowest uncertainty (bottom-k) or the marginal ones (i.e., the top-k with the highest uncertainty). In the first part of this work, we evaluate and compare different combina- tions of uncertainty scores and sorting on CIFAR-10 [18], a dataset commonly used in CL. The objective is to understand how different uncertainty estimates behave when used in different ways under same conditions. In order to achieve the intended goal, following a recent trend on research transparency and comparability [24], the evaluation framework will be freed from any other 'trick' that could create ambiguity in the assessment. The investigation will focus on describing the characteristics of the considered uncertainty scores, while providing an in-depth analysis of the effect of the metrics under a memory-based regime. In addition to the comparison of"}, {"title": "2 RELATED WORK", "content": "We can identify three different scenarios of incremental learning (IL) [30]; task-, domain-, and class-IL. In task-IL, task IDs are explicitly provided along with the data during training and testing. In domain- IL, the problem setting remains unchanged over time but the input distribution (or domain) shifts. Finally, in the class-IL scenario, the model learns to discriminate across a growing number of classes over time. An additional categorization of CL problems considers whether the data can be stored and seen multiple times (offline) or the data are considered as a stream of data which can only be used once (online) [22]. In this work, we focus on the most challenging and realistic scenario, namely, online class-IL.\nIn online-CL, where data arrive in single mini-batches and the model is updated with high frequency, memory-based methods are preferred to more sophisticated solutions, e.g., generative methods, for their flexibility and reduced training time [22]. In this context, the main question is how to optimally manage the memory. In order to reduce catastrophic forgetting, samples in the memory should be representative of their own class, discriminative towards the other classes, and informative enough for the model to recall the information about the old classes. In the literature, many conflicting strategies can be found. Some of them suggest to use the most representative samples [17, 33], while others consider the samples near the decision boundary as the most useful to reduce CF [19].\nAs already anticipated, Rainbow Memory (RM) [4] is the only approach that makes explicit use of uncertainty measures to evalu- ate the relative position of perturbed samples in the decision space. Given a list P of perturbations, the authors propose to use an agree- ment score $u_{RM}(x)$ such that, if all the perturbed versions of x are predicted with the same label, then $u_{RM}(x)$ will be 0. Otherwise, the higher is $u_{RM}(x)$ the most uncertain is the model about the considered sample.\nNoticeable, the definition of online setting in their paper differs from the popular one. In their case, online means that the training stream is only processed once and the memory is updated at the end of each task. In the standard scenario, instead, online implies that only a mini-batch of data points is available to be processed [22], and the model and the memory are updated with high frequency [28]. In addition to this, to further enhance diversity, they employ data augmentation on the memory. Thus, the contribution of $u_{RM}$ on reducing CF is unclear.\nIn the remainder of this work, we systematically investigate the effect of established uncertainty scores - including $u_{RM}$ as well as the newly proposed Bregman Information on combating catastrophic forgetting in a realistic online-CL setting."}, {"title": "3 PRELIMINARIES AND NOTATION", "content": "Following the notation proposed in Bang et al. [4], we assume to have a set $C = {c_1,..., c_n}$ of n different classes. Each class can be randomly assigned to a task t. $T_t$ represents a subset of classes determined by an assign function $\\varphi(c)$ such that $T_t = {c | \\varphi(c) = t}$.\nFor each task t, we have an associated dataset $D_t = {(x_i, y_i)}_{i=1}^{n_t}$ with $x_i$ an input sample, $y_i$ the corresponding class label, and $n_t$ the number of training samples. In the proposed online setting, we assume that samples for each task t come gradually in a stream of mini-batches $b_t = {(x_i, y_i) \\in D_t}_{i=1}^{b_t}$ which can be only processed once. It is important to notice that, for each task t, $n_t$ can vary depending on whether we are working on a class-balanced setting or not. Finally, for replay, we introduce a fixed-size memory buffer M to store a portion of samples from past tasks. The memory is updated with high-frequency whenever a mini-batch from the stream of data is processed. Following the standard procedure in online-CL, for replay, we assume to extract from the memory a number of samples equal in size to the batch size."}, {"title": "4 METHODOLOGY", "content": "To facilitate a fair evaluation of the framework, we focus on the assessment of the uncertainty metrics under same conditions. In this way, we eliminate any ambiguous effect which could be inherited from other methodological choices. As anticipated in Section 1, there are two main objectives when dealing with memory-based approaches: 1) memory populating, and 2) memory sampling."}, {"title": "4.1 Memory Management", "content": "Similar to Chrysakis and Moens [11], the memory population strat- egy is based on a class-balanced update, which is crucial to consider in case the stream of data is highly imbalanced. In fact, if classes are not equally represented in the memory, sampling from the memory may further deteriorate the predictive performance of the framework for under-represented classes. Differently from the pop- ulating strategy proposed in Chrysakis and Moens [11], the criteria to decide which samples to keep in the memory is not random, but based on predictive uncertainty estimates in order to intentionally store samples with desired characteristics for each class. There are different ways to select the samples. We can decide to store a) the class-representative data points by selecting the least uncer- tain ones for each class (bottom-k); b) a diverse set of data points, spanning from the most representative to the most uncertain ones (step-size); and c) the easiest-to-forget samples by sampling the ones with high uncertainty (top-k).\nFrom the second task on, we need a strategy for sampling from the memory a subset of data points used for replay (replay set). Since the memory represents a prototypical set of data points for each class, we assume that a random sampling is sufficient to extract informative data points from the memory. Following the standard practice, the number of samples in the replay set is equal to the batch size. It is important to note that the memory may already contain samples from the current task. In such cases, samples from the current task are excluded from the sampling process to ensure that the focus remains solely on those belonging to past tasks."}, {"title": "4.2 Uncertainty in the Logit Space", "content": "Different measure of predictive uncertainty can capture distinct aspects of a model's irreducible aleatoric uncertainty (inherent in the data) and its epistemic uncertainty (that stems from the finite training data and can be reduced by gathering more data). In online CL, the most commonly used measures are derived directly from the confidence scores of the model and mostly capture the irreducible aleatoric uncertainty [31]. Here, we hypothesize it may be more beneficial for the model to replay instances which are representative in the sense that there is low uncertainty about the data generat- ing process (we refer to this as low epistemic uncertainty, while acknowledging that there exist varying definitions). To not rely on specialized Bayesian models, we leverage a recently proposed bias-variance decomposition of the cross-entropy loss and compute a generalized variance term directly from the loss [15]. Such bias- variance decomposition decomposes the expected prediction error (loss) into the model's bias, variance, and an irreducible error (noise term). The latter is related to aleatoric uncertainty, whereas the variance term can directly be related to epistemic uncertainty [14]. Gruber and Buettner [15] have recently shown that a bias-variance decomposition of cross-entropy loss gives rise to the Bregman In- formation as the variance term and measures the variability of the prediction in the logit space. We illustrate the different aspects of uncertainty captured by confidence scores and BI respectively in Figure 2. For example, data points close to the decision boundary have a low confidence score due to the inherently high aleatoric uncertainty; in contrast, due to the high density of observed data, there is actually a low uncertainty about the data generating process (DGP), resulting in a low BI (low epistemic uncertainty). Outliers far away from the decision boundary can have a high confidence score, but will have a high BI due to the high uncertainty regarding the DGP. We hypothesize that it is samples with a low BI that are most useful for replay in online-FCL.\nTo populate the memory with representative samples, we therefore propose using an uncertainty estimator based on Bregman Infor- mation (BI) [15]. The authors demonstrate that BI at the sample level can be estimated through deep ensembles or test-time aug- mentation (TTA). However, to reduce the computational overhead, we employ TTA for computing the estimations. Let us consider the problem of multi-class classification (as in our case), where the standard loss is represented by the cross-entropy. Considering a set P of perturbations for a given data point x, we can compute the variance term of the classification loss $u(x)_{BI}$ as follows:\n$u(x)_{BI} = \\sum_{i=1}^{P} LSE(z_i) - LSE(\\frac{1}{P} \\sum_{i=1}^{P} z_i)$ (1)\nwhere $z_i \\in \\mathbb{R}^C$ represent the logit predictions and $LSE(x_1,...,x_n) = ln \\sum_{i=1}^{n} e^{x_i}$ the LogSumExp (LSE) function respectively. Intuitively, a large value of $u(x)_{BI}$ means that the logits predicted across the perturbations vary significantly, suggesting a high uncertainty of the prediction and the DGP at this point of the input space. The use of this estimator is also motivated by the fact that, in comparison with other uncertainty scores such as entropy, smallest margin, or least confidence, there is no information loss in the estimation step. In fact, if we inspect these alternative metrics, we can notice that they either need a normalization step to move the logits in probability space or rest on the largest activation value only. Furthermore, as reported in Gruber and Buettner [15], Ovadia et al. [25], and Tomani and Buettner [29], common confidence scores are reliable only in case of well-calibrated models.\nIn contrast, the BI-based estimation of the epistemic uncertainty is meaningful also under distribution shift and able to identify robust and representative samples [15]."}, {"title": "5 EXPERIMENTS", "content": "Datasets. To understand the change in the performance when us- ing different uncertainty strategies, we employ CIFAR-10 [18]. To configure the online-CIL setup, we randomly assign with different random seeds a set of classes to 5 tasks. Thus, for each random seed, the class assignment per task is different. In this way, we can identify the strategy providing greater flexibility and effective- ness irrespective of the composition of the tasks. Once the best strategy is identified, we evaluate our findings on class-imbalanced scenarios to assess the performance under more realistic conditions. First, we employ two artificially controlled imbalanced datasets, CIFAR10-LT and CIFAR100-LT [7]. As explained in Section 1, with this experiment we want to replicate a realistic scenario in which recent tasks contain less data than the older ones. For this, the most appropriate type of imbalance is the long-tailed (LT) one [12] which follows an exponential decay to choose the sample size of each class. Specifically, for both datasets, we decided to use an imbalance fac- tor $p$ equal to 0.1. This means that, if the largest class contains, e.g., 500 data points, then the smallest one consists of 50 instances. Finally, we decide to focus on biomedical image analysis using the microscopic peripheral blood cell images dataset (BloodCell) which consists of 8 classes annotated by expert clinical pathologists [1, 32]. To reflect the intended realistic conditions, we assign 2 classes for each tasks by increasing size. In this way, older tasks have more data points than the most recent ones.\nExperimental Settings. The baseline for assessing the goodness of the proposed strategies will be Experience Replay (ER) [10]. Our decision to use ER is based on the fact that, despite its simplicity, it is surprisingly competitive when compared with more sophisticated and newer approaches as shown in recently conducted empirical surveys [28].\nIn all the experiments, we employ a slim version of Resnet18 [16] - as done in previous works [17, 19, 21, 28] \u2013, and use the SGD optimizer with a learning rate of 0.1. Following standard practice, we set the batch size equal to 10. We set the memory size to different values for each dataset to evaluate a variety of memory configura- tions considering both large or small buffers. For evaluation, we employ last accuracy (A) and last forgetting (F) defined in Chaudhry et al. [8]. The compared uncertainty scores are least confidence (LC), smallest margin (SM), ratio of confidence (RC), entropy (EN), and the rainbow memory agreement score (RM) [4, 6, 13, 20, 27]. All the experiments are run on three different random seeds."}, {"title": "5.2 Empirical Results", "content": "Table 1 shows the results for all the uncertainty metrics and sorting strategies employed for populating the memory. From the values reported in the tables, we can observe that selecting the most rep- resentative samples for each class ('Bottom' column) consistently improves the results in terms of both accuracy (A) and forgetting (F) across all the considered uncertainty scores and memory sizes. Thus, our results indicate that the easiest-to-remember samples are beneficial in improving the predictive performance of the learning model and in reducing considerably CF.\nTo provide a graphical intuition of the differences between the sort- ing strategies and their plausible effect on the learning capabilities"}, {"title": "6 CONCLUSION", "content": "In this paper, we propose to leverage estimates of predictive un- certainty for memory management. The investigation focuses on understanding the characteristics samples should have to alleviate CF. The results demonstrate that the most representative samples"}]}