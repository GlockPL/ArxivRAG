{"title": "Multi-View Attention Syntactic Enhanced Graph\nConvolutional Network for Aspect-based Sentiment\nAnalysis", "authors": ["Xiang Huang", "Hao Peng", "Shuo Sun", "Zhifeng Hao", "Hui Lin", "Shuhai Wang"], "abstract": "Aspect-based Sentiment Analysis (ABSA) is the task aimed at pre-\ndicting the sentiment polarity of aspect words within sentences. Recently, incor-\nporating graph neural networks (GNNs) to capture additional syntactic structure\ninformation in the dependency tree derived from syntactic dependency parsing\nhas been proven to be an effective paradigm for boosting ABSA. Despite GNNS\nenhancing model capability by fusing more types of information, most works only\nutilize a single topology view of the dependency tree or simply conflate different\nperspectives of information without distinction, which limits the model perfor-\nmance. To address these challenges, in this paper, we propose a new multi-view\nattention syntactic enhanced graph convolutional network (MASGCN) that weighs\ndifferent syntactic information of views using attention mechanisms. Specifically,\nwe first construct distance mask matrices from the dependency tree to obtain\nmultiple subgraph views for GNNs. To aggregate features from different views,\nwe propose a multi-view attention mechanism to calculate the attention weights\nof views. Furthermore, to incorporate more syntactic information, we fuse the\ndependency type information matrix into the adjacency matrices and present a\nstructural entropy loss to learn the dependency type adjacency matrix. Compre-\nhensive experiments on four benchmark datasets demonstrate that our model\noutperforms state-of-the-art methods. The codes and datasets are available at\nhttps://github.com/SELGroup/MASGCN.", "sections": [{"title": "1 Introduction", "content": "Aspect-based sentiment analysis (ABSA) is a fine-grained task aimed at classifying the\nsentiment polarity of given aspect words in a sentence [48]. It has drawn significant"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Aspect-based Sentiment Analysis", "content": "In contrast to traditional sentiment tasks that are sentence-level or document-level,\nABSA is entity-level oriented and more fine-grained for sentiment polarity analysis.\nEarly works [12,13,4] extract sentiment features based on handcrafted rules and perform\npoorly in capturing rich sentiment information.\nRecently, various context-based methods [37,25,2,7,10,8,18,33] propose to utilize\nattention mechanisms to model the contextual semantic information between the aspect\nterm and its context words. ATAE-LSTM [37] proposes an attention-based LSTM model\nto concentrate on different parts of sentences to generate attention vectors for aspect\nsentiment classification. IAN [25] learns attention between the contexts and targets in\nan interactive manner and generates representations for targets and contexts separately.\nRAM [2] leverages the multiple-attention mechanism to capture sentiment features\nseparated by a long distance. MGAN [7] proposes a fine-grained attention mechanism to\ncapture the word-level interaction between aspects and context and then compose it with\ncoarse-grained attention mechanisms. [18] design a hierarchical attention mechanism to\nfuse the information of the aspect terms and the contextual words. [33] propose a dual\nattention mechanism to address the problem of recognizing conflicting opinions. Despite\nthese context-based methods achieving good results, they are unable to distinguish\nthe relation between the aspect term and multiple opinion words. This hampers their\nperformance in sentences having multiple aspects with different polarities.\nOwing to the rapid development of syntactic parsing methods, another trend ex-\nplicitly utilizes the parsed dependency tree to reveal the connection between aspect\nterms and opinion words and learn the syntactic features of aspect terms. These methods"}, {"title": "2.2 Structural Entropy", "content": "Information entropy was proposed to meet the demand for measuring uncertainty in in-\nformation transmitted through communication systems. Correspondingly, to measure the\ninformation uncertainty in graph-structured data, structural entropy was also proposed\nand used to evaluate the complexity of the hierarchical structure of a graph by defining\nthe encoding tree and structural entropy [15]. The process of constructing and optimizing\nthe encoding tree is also a natural vertices clustering method for graphs. Due to the\ntheoretical completeness and interpretability of structural entropy theory, it has great\npotential for application in graph analyses such as graph hierarchical pooling [41] and\ngraph structure learning [52,6]. Moreover, the two-dimensional and three-dimensional\nstructural entropy, which measure the complexity of hierarchical structures at two and\nthree dimensions, respectively, have found applications in fields such as medicine [16],\nbioinformatics [17], social bot detection [28,45,42], network security [14], natural lan-\nguage understanding [11] and reinforcement learning [44]."}, {"title": "3 The proposed framework", "content": "This section outlines the overall architecture of MASGCN and details each component\nof our proposed model."}, {"title": "3.1 Overall Architecture", "content": "We illustrate the overall structure of MASGCN in Figure 1. First, following the approach\nof [49], we utilize the aspect-aware attention and self-attention mechanisms to obtain\nenhanced semantic matrices. Second, we parse the sentence syntactically to obtain a"}, {"title": "3.2 Semantic Feature", "content": "Given a sentence-aspect pair $W \u2013 A$, where $A = \\{a_1, ..., a_M\\}$ represents the aspect\nwords set and is also a sub-sequence of the sentence $W = \\{w_1,..., w_N\\}$, with $w_i$ and\n$a_j$ denoting the i-th and j-th words in $W$ and $A$, and N and M representing the lengths\nof the sentence and the aspect, respectively. We derive the contextual feature of sentences\nbased on the sentence encoder like BERT [3] and obtain the low-dimensional embedding\nmatrix $H \\in R^{N\u00d7D}$, where the i-th word $w_i$ corresponds to the i-th row feature $h_i$ with\ndimension D. Aspect features $H_a \\in R^{M\u00d7D}$ are derived from H.\nFor enhanced semantic features, we follow [49] and apply the aspect-aware attention\nand self-attention mechanism to obtain aspect attention matrices $\\{A_{asp}^1,..., A_{asp}^P\\}$ and\nself-attention matrices $\\{A_{self}^1, ..., A_{self}^P\\}$, where P is the number of attention heads. The\ni-th matrices are calculated as follows:\n$A_{asp}^i = tanh (\\hat{H_a}W_a^i (HW_a^i)^T + b_a)$,                                         (1)\n$A_{self}^i = \\frac{Q_iW_Q^i \u00d7 (K_iW_K^i)^T}{\\sqrt{D}}$.                                                            (2)\nHere, $\\hat{H_a} \\in R^{N\u00d7D}$ represents the N times repeated mean of $H_a$, $W_a^i \\in R^{D\u00d7D}$ and\n$W_q^i \\in R^{D\u00d7D}$ are learnable weights for aspect-aware attention of the i-th attention\nhead, and $b_a$ is the learnable bias. The query Q and the key K are equal to the feature"}, {"title": "3.3 Syntactic Feature", "content": "In this section, we first utilize the Stanford Parser CoreNLP [26] to parse the sentence into\na syntactic dependency tree, and then extract the distance information and dependency\ninformation from the dependency tree.\n\u2022 Distance information. We treat the syntactic dependency tree as an undirected graph,\nwith each word as a node. Then, we define the distance $d(v_i, v_j)$ between nodes $v_i$ and\n$v_j$ as the number of hops between the two nodes in the graph. The shortest path distance\nbetween nodes $v_i$ and $v_j$ is calculated as follows:\n$D_{ij} = min d(v_i, v_j)$.                                                          (4)\nWe consider different scales of distance information by defining various distance mask\nmatrices $M^k$ as follows:\n$M_{ij}^k = \\begin{cases}\n0, D_{ij} \\leqslant k, \\\\\n-\\infty, otherwrise,\n\\end{cases}$                                            (5)\nwhere $k \u2208 [1, P]$ and P is the number of attention heads mentioned in Section 3.2. As\n$k\u2208 [1, P]$ increases, the scope of distance information that the mask matrix $M^k$ covers\nalso increases.\n\u2022 Dependency information. We define the initial dependency type adjacency matrix\n$A_{type} \\in IR^{N\u00d7N}$ as follows:\n$A_{type} [i, j] = \\begin{cases}\nid_{type}, D_{ij} = 1, \\\\\n0, otherwise,\n\\end{cases}$                                                 (6)\nwhere $id_{type} \u2208 [1, U]$ and U is the size of the dependency type vocabulary. To utilize\nthe dependency information of the dependency tree, we initialize the dependency type\nfeature matrix $H_{type} \\in R^{U\u00d7D}$. The global attention mechanism [50] calculates the\ndependency type attention matrix as follows:\n$\\alpha$ = $softmax(H_{type} W_t)$,\n$A_{type} [i, j] = \\begin{cases}\n[\\alpha[A_{type} [i, j]], A_{type} [i, j] \\neq 0, \\\\\n0, otherwise,\n\\end{cases}$                                                 (8)\nwhere $W_t \u2208 R^{D\u00d71}$ represents the learnable weights for global attention, $\\alpha \u2208 R^{U\u00d71}$\ndenotes the attention score. $A_{type} \u2208 R^{N\u00d7N}$ is the dependency type information matrix\noutput."}, {"title": "3.4 GNN and Multi-View Attention Mechanism", "content": "Now we have obtained the sentence embedding H and P distinct adjacency matrices\n$\\{A^1,..., A^P\\}$. The output embedding of the l-th GNN layer is calculated as follows:\n$H_l = \\sigma (AGG_{i=1}^{P}(A^iH_{l-1})W_l), H_0 = H$,                                                  (11)\nwhere $H_l \u2208 R^{N\u00d7D}$ is the l-th layer output embedding, $W_l \u2208 R^{D\u00d7D}$ represents the l-th\nlayer's learnable weights, and $\\sigma$ denotes an activation function. The operator $AGG_{i=1}^{P}$\nis the multi-view attention mechanism we propose, which is as follows:\n$H_{avg}^i = \\frac{1}{N X D} \\sum_{r,s=1}^{N,D} (A^iH_{l-1})_{r,s}$,                                           (12)\n$A_{view} = W_2 (\\sigma (W_1 H_{avg}))$,                                                  (13)\n$AGG_{i=1}^{P}(A^iH_{l-1}) = \\sum_{i=1}^{P}\\alpha_{view}^i @_{view} A^i H_{l-1}$.                                       (14)\nHere, $H_{avg} \\in R$ is i-th row of $H_{avg} \u2208 R^{P\u00d71}$, $@_{view} \u2208 R^{P\u00d71}$ is the view weights of P\nviews, $W_1$ and $W_2$ are the learnable weight, $\\alpha_{view}^i$ is the i-th row of view. The multi-view\nattention mechanism adaptively extracts useful information from various views, and\nleverages the diversity and complementarity of information across different perspectives,\nthereby enhancing the representation capabilities for downstream tasks. Additionally,\nthe multi-view attention mechanism reduces noise introduced by too many views [49]."}, {"title": "3.5 Loss Function with Structural Entropy", "content": "Structural entropy theory [15] has demonstrated its capability in multiple graph-related\nworks [41,52,6]. For a graph $G = (X, E, W)$, X is the set of graph datapoints, and E\nand W are the edges and corresponding edge weights. We define a two-level encoding\ntree T, where the intermediate tree nodes $\\alpha$ represent a partitioned subset of the graph"}, {"title": "4 Experiment Setup", "content": "In this section, we detail the baselines, datasets, and implementation details of MASGCN."}, {"title": "4.1 Datasets", "content": "Following previous ABSA works, we evaluate MASGCN on four benchmark datasets:\nRestaurant14, Restaurant16, Laptop14, and Twitter. The Restaurant14 and Laptop14\ndatasets include reviews in the restaurant and laptop domains from SemEval-2014 [32].\nThe Restaurant16 dataset is from SemEval-2016 [31]. The Twitter dataset is collected\nfrom tweets by [5]. Each aspect is annotated with one of three polarities: positive, neutral,\nand negative. The statistics of these datasets are listed in Table 1."}, {"title": "4.2 Baselines", "content": "To comprehensively evaluate the performance of MASGCN, we compare it with SOTA\nbaselines, which are briefly described as follows:\n1) Context-based methods.\n\u2022 ATAE-LSTM [37] is an LSTM model with the attention mechanism on aspects.\n\u2022 IAN [25] interactively learns attention scores for aspects with their context.\n\u2022 RAM [2] proposes a recurrent attention memory network to capture the aspect-specific\nsentence representation.\n\u2022 MGAN [7] applies a fine-grained attention mechanism to capture token-level interac-\ntions between aspects and contexts.\n\u2022 TNet [20] utilizes a CNN model to extract salient features for sentiment analysis.\n2) Syntax-based GNN methods.\n\u2022 ASGCN [46] applies GCN on the raw topology of the dependency tree to extract\nsyntactic information.\n\u2022 kumaGCN [1] uses gating mechanisms to acquire syntactic features with latent\nsemantic information.\n\u2022 DGEDT [34] combines transformer and graph-based representations from the cor-\nresponding dependency graph to diminish the error induced by incorrect dependency\ntrees.\n\u2022 BIGCN [47] uses convolutions on hierarchical lexical and syntactic graphs to integrate\ntoken co-occurrence and dependency type information.\n\u2022 R-GAT [36] uses a star-induced graph with minimum distances and dependency types\nas edges and applies a relational GAT for attention-based aggregation.\n\u2022 T-GCN [35] uses attention to distinguish relation types and applies an attentive layer\nensemble for feature learning from GCN layers.\n\u2022 DualGCN [19] introduces syntactic and semantic information through SynGCN and\nSemGCN modules simultaneously.\n\u2022 SSEGCN [49] combines aspect-aware attention, self-attention, and minimum tree\ndistances to enhance sentiment information with syntactic information.\n\u2022 SenticGCN [21] integrates external knowledge from SenticNet to enhance the depen-\ndency graphs of sentences."}, {"title": "4.3 Experimental Settings", "content": "In experiments, we initialize token embeddings with pre-trained 300-dimensional Glove\nvectors [30], combine them with 30-dimensional part-of-speech and position embeddings,\nand feed them into a BiLSTM model with a 0.7 dropout rate. The batch size is 16 and\nthe number of GCN layers is 2. The learning rate of the Adam optimizer is 0.002.\nThe view number and attention head number P are set to 10. The hyperparameter $\\gamma$\nof the structural entropy loss is 0.01. Model+BERT utilizes the bert-base-uncased [3]\nEnglish version. We utilize accuracy (Acc.) and macro-F1 (F1) to evaluate classification\nperformance. The baseline results are from [50] and [51]."}, {"title": "5 Experiments", "content": "In this section, we conduct comparative experiments with baselines for effectiveness\ncomparison. Moreover, we carry out the ablation study, parameter sensitivity experiments,\nand investigations into adjacency matrices to explore each component of our model."}, {"title": "5.1 Performance Comparision", "content": "We conduct the experiments on four benchmark datasets and report the experiment results\nin Table 2. MASGCN achieves state-of-the-art results on all four datasets, except for\naccuracy on the Restaurant16 dataset when using Glove embeddings. Specifically, with\nGlove embeddings, MASGCN outperforms all baselines with improvements of at least\n0.17-0.79 in accuracy and 0.48\u20130.99 in macro-F1 across the Restaurant14, Laptop14,\nand Twitter datasets. On the Restaurant16 dataset, MASGCN still excels in macro-F1,\nshowing a 1.22 improvement over the SOTA SenticGCN model. Besides, MASGCN\n+ BERT surpasses other Model+BERT combinations across all datasets, except for\nmacro-F1 on the Twitter dataset and the Laptop14 dataset. These results demonstrate the\neffectiveness of our model. Compared to R-GAT, SPGCN, CSADGCN, and RDGCN,\nwhich also utilize three types of information, MASGCN still outperforms due to its\nenhanced utilization of this information through multi-view information aggregation."}, {"title": "5.2 Ablation Study", "content": "We evaluate each component of MASGCN on four datasets and report the results in\nTable 3. The absence of structural entropy loss $L_{SE}$ and the multi-view attention mech-\nanism results in a decline in model performance on most datasets, with an average\nreduction of 0.18% and 1.20% in accuracy, and 0.47% and 3.63% in macro-F1, re-\nspectively. This demonstrates the effectiveness of structural entropy loss in learning\ndependency type information and our proposed multi-view attention mechanism in\nfusing syntactic information."}, {"title": "5.3 Hyperparameter Analysis", "content": "Two hyperparameters are introduced in our model: the view number P and the coefficient\n$\\gamma$ for the structural entropy loss $L_{SE}$. We vary their values on three benchmark datasets\nand illustrate the results in Figure 2. When the view number P is not greater than 10,\nmore views improve the model performance. However, the performance decreases when\nP is 15. This is because the number of views exceeds the maximum distance in the syntax\ntree, thereby introducing only additional noise. Despite this, compared to SSEGCN [49],\nwhich fuses different views of distance information indiscriminately and only integrates\nfour different views when balancing the introduction of additional noise, MASGCN still\nimproves the ability to fuse more information and reduce noise. Furthermore, we find\nthat a smaller $\\gamma$ for structural entropy loss is preferred for the model. MASGCN achieves\nthe best performance with $\\gamma$ being 0.01, except on Twitter, where $\\gamma$ is 0. \u0391 $\\gamma$ larger than\n0.01 leads to a decrease in model performance."}, {"title": "5.4 Insight of Matrices", "content": "To elucidate the effectiveness of each type of information, we visualize the semantic\nmatrix $A_{sem}$, the masked matrix $A_{mask}$ (incorporating semantic and distance information),\nand the final fused adjacency matrix A (incorporating semantic, distance, and dependency\ninformation) mentioned in Section 3. As depicted in Figure 3 (a), with the integration\nof distance information and dependency information, the correlation of the aspect term\ncoffee with the polarity word OUTSTANDING is enhanced, and the attention to other\nuncorrelated words is gradually reduced. Figure 3 (b) shows that the integration of\ndistance information and dependency information decreases the irrelevant attentions,\nenhances the attention of the aspect term roll to its polarity words BEST spicy, and\nreduces the attention to the polarity word great, which is connected with the other aspect\nterm salad. The improvement can also be observed in the row of the aspect term salad.\nThese two cases directly demonstrate the effectiveness of MASGCN."}, {"title": "6 Conclusion", "content": "In this paper, we propose a new multi-view attention syntactic enhanced graph convolu-\ntional network MASGCN. We propose a multi-view attention mechanism to aggregate"}, {"title": "7 Limitations and Future Work", "content": "Our work incorporates various syntactic information from the dependency tree to improve\nthe effectiveness of ABSA tasks. However, it is limited to the information from the parsed\ndependency tree and does not consider external knowledge. In future work, it would be\nvaluable to exploit information from other models, such as external knowledge graphs or\nlarge language models like GPT-4 [27]."}]}