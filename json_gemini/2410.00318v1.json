{"title": "Probing Mechanical Reasoning in Large Vision Language Models", "authors": ["Haoran Sun", "Qingying Gao", "Haiyun Lyu", "Dezhi Luo", "Hokin Deng", "Yijiang Li"], "abstract": "Mechanical reasoning is a fundamental ability that sets human intelligence apart from other animal intelligence. Mechanical reasoning allows us to design tools, build bridges and canals, and construct houses which set the foundation of human civilization. Embedding machines with such ability is an important step towards building human-level artificial intelligence. Recently, Li et al. built CogDevelop2K, a data-intensive cognitive experiment benchmark for assaying the developmental trajectory of machine intelligence [Li et al., 2024]. Here, to investigate mechanical reasoning in Vision Language Models, we leverage the MechBench of CogDevelop2K, which contains approximately 150 cognitive experiments, to test understanding of mechanical system stability, gears and pulley systems, seesaw-like systems and leverage principle, inertia and motion, and other fluid-related systems in Large Vision Language Models. We observe diverse yet consistent behaviors over these aspects in VLMs.", "sections": [{"title": "1 Introduction", "content": "Humans are the only species on this planet which know how to build complex mechanical systems. Since the beginning of civilization, we have created tools to improve our ability to shape the world around us. Early humans made simple tools from stone, bone, and wood, using them to hunt, gather, and build. Over time, our tools became more complex, enabling greater feats in agriculture and architecture. Nowadays, we have cars, airplanes, and computers that enable us to travel and communicate so seamlessly that the entire planet feels like a connected village [Harari, 2014]. Behind these remarkable achievements lies the power of reasoning about the relations and interactions of objects in the physical world, or in other words, mechanical reasoning [Heidegger, 1927, Clark, 2010, Harman, 2011, Vaesen, 2012]. Few evidences suggest animals also have the ability to reason mechanically about objects [Shumaker et al., 2011], but humans remain distinctive in terms of our flexibility, complexity, and creativity in mechanical reasoning abilities [Allen et al., 2020, Allen, 2021]. Recently, Li et al. built CogDevelop2K, a data-intensive cognitive experiment benchmark for assaying the developmental trajectory of machine intelligence [Li et al., 2024]. We here leverage"}, {"title": "2 Methods", "content": "the MechBench of CogDevelop2K, which contains around 150 cognitive experiments to investigate mechanical reasoning abilities in current Vision Language Models. We have aligned 60 models for our analysis, including both closed models such as GPT series, Gemini series, Claude series, and open models such as LLaVA series and Qwen-vl series [Gemini, Bai et al., 2023, Wang et al., 2024, Hong et al., 2024, Li et al., 2023, OpenAI]. Tests in MechBench generally investigate 6 aspects of mechanical reasoning abilities: 1. mechanical system ability 2. pulley systems, 3. gear systems, 4. seesaw-like systems and leverage principle, 5. inertia and motion, and 6. fluid mechanics. In general, Vision Language Models achieve above-chance performances on MechBench (Figure 1). Yet, models demonstrate diverse but consistent behaviors over the 6 aspects of MechBench."}, {"title": "2.1 Dataset", "content": "We leverage MechBench from the CogDevelop2K to construct our set of cognitive experiments to investigate mechanical reasoning abilities in Vision Language Models. MechBench is composed of 141 image-based cognitive experiments."}, {"title": "2.2 Model Selection and Experiment", "content": "We evaluate the intent inference capabilities of three categories of Visual Language Models (VLMs). To ensure a fair comparison, all VLMs are evaluated on their ability to reason over images and texts under a zero-shot, open-ended generation task. Detailed documentation of the inference methodology can be found in the paper reporting the CogDevelop2K benchmark [Li et al., 2024].\nThe models are categorized as follows:\n1. Open-source VLMs with Multi-Image Reasoning: Includes models with different sizes and other variants such as CogVLM Series[Hong et al., 2024], Qwen series(Qwen-VL [Bai et al., 2023], Qwen-2 [Wang et al., 2024]), and Blip2 [Li et al., 2023], LLaVA-Next [Liu et al., 2024], which are capable of reasoning over interleaved multiple images and texts.\n2. Closed-source VLMs with Multi-Image Reasoning: Includes proprietary models such as GPT series [OpenAI] (GPT-4v, GPT-4-turbo, GPT-40-mini), Gemini Series [Gemini], and Claude Series [claude]. These models also support reasoning across interleaved images and texts,"}, {"title": "3 Result", "content": "VLMs performance in intuitively evaluating system stability is not ideal, however here we observe some interesting phenomena. Models excel at identifying objects in images, and they not only understand what the objects are, the models also successfully recognize their mechanical states as well. Also, the models could effectively connect mechanical descriptions with the corresponding mechanical scenarios (Figure 2). In Experiments 2B and 2F, the models could recognize the two chairs and the two bottles, and their mechanical situations; however, the models still failed to provide the correct answers in the experiments. For instance, in Experiment 2B, the model explains, \"the stool on the right is more likely to tip over when an active child sits on it because its legs are splayed at an angle, making the base wider and potentially more stable under normal circumstances but also adding a tipping hazard due to the non-vertical configuration of the legs\". The model correctly notices that it's the angle of the leg that matters for the stability of the system. However, it reasons completely the opposite way to correct answers. When the leg's angle wider, it's actually more stable. It's a very intuitive physical problem for humans but the models fail, even though they still demonstrate step-by-step reasoning abilities in this case. In Experiment 2F, the model is correct that the bottle on the bottom \"has a larger area of contact with the surface, creating more friction, which helps prevent rolling\". However, the model fails to realize one bottle is standing, and one bottle is rolling, and sliding friction and rolling friction are completely different. In contrast, humans can easily solve these problems intuitively."}, {"title": "3.2 VLMs Don't Understand Pulley Systems", "content": "We find that current VLMs struggle to handle pulley systems (Figure 3). We observe that there are generally three failures in VLMs reasoning about pulley systems: first, VLMs are not able to identify which are the movable pulleys in the system, and second, VLMs exhibit relatively low accuracy in determining whether an object is rising or falling through pulley systems.\nVLMs perform poorly in recognizing movable pulley systems. In Experiment 3A, the image includes a standard single movable pulley system and a standard single fixed pulley system. The question \"Which system requires less effort?\" is essentially asking whether the model can correctly select the movable pulley. Clearly, the model failed in its selection, as it straightforwardly provided an incorrect answer in its explanation. VLMs also struggle in predicting whether a suspended weight is being lifted or lowered through a pulley system. Experiments 3B-F either directly or indirectly reflect this issue, with Experiment 3D being the most direct and concise. In Experiment D, the weight is directly attached to the movable pulley, and by pulling the other end of the rope, the pulley and the weight are lifted. However, the model's response was the exact opposite of the correct answer. In its explanation, the model seemed to imply that the pulley was not fixed (though it did not explicitly state that it was a movable pulley), and the physics it provided was entirely incorrect. Therefore, we can hypothesize that the model's poor performance in predicting the weight's movement may be due to its limited ability to recognize movable pulleys. However, the specific reasons require further experiments to be analyzed in detail.\nThe above two issues confirm that VLMs still have limitations in recognizing pulley systems, whereas for individuals with some mechanical experience, identifying simple pulley systems through basic diagrams is not difficult (especially in the case of Experiments 3A and 3D)."}, {"title": "3.3 VLMs Understand Gear Systems", "content": "On gear and conveyor belt problems, VLMs are able to give highly accurate judgments (Figure 4). In particular, VLMs can reliably determine the rotation of one pulley based on the direction of another.\nCompared to pulley systems, we speculate several reasons why VLMs perform better in gear systems. First, gear problems typically involve simple mechanical setups with fixed scenarios, meaning the layout and interactions between gears are often straightforward. This reduces the complexity for the model when analyzing the system, as it does not need to account for too many"}, {"title": "3.4 Seesaw-like systems and Leverage Principle", "content": "We observe diverse behaviors in VLMs on solving seesaw-like mechanical systems and applying leverage principle (Figure 5). Experiments 5A, 5C, 5D, and 5F involve relatively simple scene-based question answering and basic extrapolation. Experiments 5A and 5D are straightforward question-answer tasks. As long as the model identifies the key information in the images such as the length of the wrench in Experiment 5A or the unequal torques in Experiment 5D-it can easily solve the problems. Experiments 5C and 5F involve simple extrapolations based on scene information, like understanding how changes in the length of the resistance arm affect force in Experiment 5C or predicting the outcome when the weights on a balance scale are unequal in Experiment 5F. It is evident that the model performed exceptionally well in these experiments, providing both correct answers and detailed explanations. However, in Experiments 5B and 5E, VLMs failed to give the correct answers. We suspect that VLMs struggle when the mechanical reasoning processes, particularly applying the leverage principle and solving seesaw-like systems, require multiple steps. For example, in Experiment 5B, while VLMs recognize it is a seesaw-like system, they fail to understand how to operate within this system: sliding backward would give more leverage. This reasoning process may require backtracking through several steps: first, identifying the seesaw-like system; second, understanding how to apply leverage; third, realizing that sliding backward increases leverage; fourth, recognizing that more leverage is needed to level the system; and finally, concluding that sliding backward is necessary. The same issue arises in Experiment 5E."}, {"title": "3.5 Inertia and Motion", "content": "We also observe very diverse behaviors in VLMs' understanding of inertia and motion (Figure 6). Similar to seesaw-like systems, VLMs are able to identify the mechanical situations in the problem settings. However, they lack the ability to effectively predict the next step based on the current scene.\nExperiment 6E involves scene-based question. The model not only identified that the cart was not stationary, but it also further analyzed that if the object suspended on the cart were stationary, it would fall vertically due to gravity. However, the depicted scene likely indicates that the cart is accelerating forward. Experiments 6A, 6B, and 6D, on the other hand, are cases of simple extrapolation. They are categorized as \"simple extrapolation\" because the scenarios are straightforward and involve a single, clear change. For example, in Experiment 6A, the change involves the cart suddenly stopping-a relatively simple scene (involving only two objects, the drink and the cart, with straight-line motion and vertical force equilibrium). Similar patterns apply to Experiments 6B and 6D.\nExperiments 6C and 6F, however, involve more complex reasoning related to inertia and motion prediction. Experiment 6F is a derivative question from Experiment 6E, asking what would happen if the cart suddenly stopped. The suspended object would continue moving forward due to inertia. Although the scene in Experiment 6F is similar to Experiment 6E, and the question is similar to that in Experiment 6A, it involves more objects (the pulling rope, the cart, and the pulled object), more physical principles, and a sudden change in force (as the rope loses tension). The model clearly struggled with this problem, providing an incorrect answer and an explanation that did not meet expectations. Notably, Experiment 6C asks a common-sense question based on pendulum motion. The image in Experiment 6C marks three points in the half-arc of the pendulum's swing: the highest point, the quarter-arc point, and the lowest point. This information is also specified in the prompt. However, the model gave an incorrect answer, and its explanation was confused. The model completely misunderstood the designated points in the diagram and in the prompt, mistaking the first and third points as the left and right highest points of the pendulum's motion and the second point as the lowest point. One possible reason for this error is that the model may have mistaken the half-arc pendulum motion in Experiment 6C for a full pendulum motion. A deeper explanation might suggest that the model has issues with perceptual constancy. The image in Experiment 6C might have been misinterpreted as a 3D scene, and this visual misperception could be the root of the error. This hypothesis requires further experimentation to be confirmed."}, {"title": "3.6 Fluid Systems", "content": "The fluid-related experiments involved properties such as fluid flow, buoyancy, and volume. In fluid-related systems, VLMs still face the aforementioned issues, particularly the challenge of complex inference. However, a notable highlight is that VLMs have demonstrated impressive scene understanding and detail-capturing abilities.\nExperiments 7B and 7C highlighted the model's weaker inference abilities. In Experiment 7B, the hole in the bucket was positioned at the middle of the bucket's wall. Clearly, once the water level drops to the level of the hole, water will stop flowing out, meaning some water will remain in the bucket. However, the model failed to capture this crucial detail about the hole's location, leading to an incorrect answer. Similarly, in Experiment 7C, although the model provided the correct answer, its explanation was incorrect.\nIt is worth noting that the model excelled in image comprehension and detail recognition, especially in Experiments 7D and 7F. In Experiment 7D, the measuring cup had a narrow base and a wider top, meaning the scale markings could not be evenly spaced. The model successfully captured these details, including the design of the measuring cup, the distribution of the scale, and the corresponding numerical values and units, offering a detailed explanation. The same can be said for Experiment 7F. The model accurately identified that the balloon was fully inflated with gas and incorporated the concepts of gas and liquid density to explain the current state and predict the next step of the experiment."}, {"title": "4 Discussion", "content": "Here, we leverage MechBench, an assay of 141 cognitive experiments that include many aspects of mechanical reasoning to investigate Vision Language Models. We investigate VLM abilities in reasoning about mechanical stability, gear systems, pulley systems, inertia and motion, and fluid systems. In general, we observe very diverse phenomena in VLM mechanical reasoning abilities. It's definitely the case that VLMs already have some sorts of mechanical reasoning abilities. Nevertheless, we also observe that VLMs have numerous failure modes in reasoning about physical objects and their mechanical interactions in the world.\nOne parallelism might be drawn here between current machine intelligence and animal intelligence. Whether animals have the ability to mechanically reason about objects in the world still remain a hotly"}, {"title": "5", "content": "variables or unpredictable changes in the structure. In other words, a gear system could be reduced into a logical diagram, and solved correctly using that diagram.\nSecondly, gear problems rely on a few key physical principles. Specifically, there are two important rules: adjacent gears always rotate in opposite directions, and smaller gears rotate faster than larger ones. Unlike problems that involve complex mathematical calculations, these rules are straightforward for the model to understand and apply consistently. As a result, the model can accurately analyze and predict gear behavior. Similarly, these physical rules can be reduced to logical rules, allowing the model to solve the problem using fixed logical principles."}, {"title": "6", "content": "debated scientific topic [Hall, 1963, Van Lawick-Goodall, 1971, Seed and Byrne, 2010, Andrews, 2020]. Some examples of mechanical reasoning in animals have been observed. Chimpanzees, for example, could transform a leafy vine into a thin, flexible tool for fishing termites, while crafting a sturdier rod for dipping into ant nests [Goodall, 1986]. A New Caledonian crow uses its beak to shape the serrated leaves of a Pandanus tree into strips, which the crow then employs to extract insects or larvae from hard-to-reach places [St Clair and Rutz, 2013]. Nevertheless, animals are not able to achieve the level of perplexity and creativity in terms of manipulating and building mechanical systems comparing to humans [Birch et al., 2020]. One interesting direction would be juxtaposing animal intelligence, machine intelligence, and human intelligence with our mechanical reasoning assay, and learn from the similarities and differences in how animals, machines, and humans reason about physical objects and their interactions in the world.\nThe computational basis of mechanical reasoning in the human mind has been a focus of research. One of the core theories in cognitive science regarding human mechanical reasoning is mental simulation. As proposed by Hegarty [1992], when people reason about mechanical systems, such as gears or pulley systems, they engage in a step-by-step mental process where each component is animated sequentially. This is different from imagining a static scene; rather, people simulate the dynamic changes within the system incrementally. This mechanism reflects a fundamental limitation in working memory, in that humans cannot animate the entire system holistically, but instead rely on breaking it down into smaller causal units to reason through the interaction of its components.\nSimilarly, in our experiments, Vision-Language Models (VLMs) exhibited signs of piecemeal reasoning but showed limitations in comprehending the full causal chain, much like the sequential cognitive processes seen in human mental simulation. The VLMs often struggled to reason about components deeper within the causal chain, such as inferring the behavior of a secondary gear or pulley based on the motion of a primary one. This suggests that while VLMs can engage with the initial stages of a problem, they may not yet possess the cognitive flexibility required to simulate complex mechanical systems across multiple stages.\nFurthermore, studies by [Schwartz and Black, 1996] demonstrated that humans use analog pro-cesses in mechanical reasoning, meaning their cognitive processes mirror the physical dynamics they aim to understand. For example, when reasoning about the interaction between gears, humans often rely on mental rotation\u2014where the time to mentally rotate an object is proportional to the angle of rotation. This ability to accurately simulate physical processes is crucial for reasoning about real-world mechanical systems, such as predicting how two interlocking gears will turn.\nHowever, our analysis of VLMs in the gear systems tasks from MechBench reveals that while VLMs sometimes mimic human-like performance on surface-level tasks, their ability to engage in these analog processes remains underdeveloped. VLMs often fail to understand proportionality in systems, such as recognizing how changes in one gear affect subsequent ones. This suggests that their internal representation of mechanical systems lacks the depth of human analog reasoning.\nFinally, the role of spatial representation in mechanical reasoning is crucial. Dual-task studies by [Hegarty and Sims, 1994] revealed that spatial working memory plays a significant role in solving mechanical reasoning tasks, as visuospatial information needs to be continuously processed and manipulated. VLMs, while equipped with powerful vision-language integration, appear to lack this level of spatial manipulation, often failing in tasks that require maintaining a coherent internal spatial representation of the objects involved. This limitation leads to errors when reasoning about inertia, fluid dynamics, or the spatial consequences of motion within a system.\nThus, by applying insights from human mechanical reasoning research to our findings with VLMs, it is evident that current VLMs do not yet fully replicate the sophisticated cognitive processes humans use. Mechanical reasoning requires the integration of mental simulation, analog processes, and spatial manipulation, all of which present ongoing challenges for machine intelligence. The path forward involves enhancing VLMs' ability to engage in these types of reasoning, which could be guided by further exploration of how humans\u2014and even animals-approach the same problems."}]}