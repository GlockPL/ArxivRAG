{"title": "LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven Curriculum Learning", "authors": ["Tao Fang", "Derek F. Wong", "Lusheng Zhang", "Keyan Jin", "Qiang Zhang", "Tianjiao Li", "Jinlong Hou", "Lidia S. Chao"], "abstract": "While large-scale language models (LLMs) have demonstrated remarkable capabilities in specific natural language processing (NLP) tasks, they may still lack proficiency compared to specialized models in certain domains, such as grammatical error correction (GEC). Drawing inspiration from the concept of curriculum learning, we have delved into refining LLMs into proficient GEC experts by devising effective curriculum learning (CL) strategies. In this paper, we introduce a novel approach, termed LLM-based curriculum learning, which capitalizes on the robust semantic comprehension and discriminative prowess inherent in LLMs to gauge the complexity of GEC training data. Unlike traditional curriculum learning techniques, our method closely mirrors human expert-designed curriculums. Leveraging the proposed LLM-based CL method, we sequentially select varying levels of curriculums ranging from easy to hard, and iteratively train and refine using the pretrianed T5 and LLaMA series models. Through rigorous testing and analysis across diverse benchmark assessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19 development sets, our approach showcases a significant performance boost over baseline models and conventional curriculum learning methodologies.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have garnered considerable attention due to their exceptional performance across a wide array of downstream Natural Language Processing (NLP) tasks (Brown et al., 2020; Touvron et al., 2023a,b; OpenAI, 2024). Notably, the deployment of LLMs has been particularly influential in the fields of text continuation, general dialogue, and an assortment of other text generation tasks (Jiao et al., 2023; Hendy et al.,"}, {"title": "2 Method", "content": "Our primary focus revolves around constructing an adaptable curriculum learning framework and leveraging the curated curriculum data to train LMs into adept specialists for downstream tasks. Our methodology comprises two key phases: Firstly, employing an LLM to assess the difficulty level of the training data, followed by categorizing the scored data into three tiers: easy, medium, and hard courses. Secondly, sequentially exposing the LM to these curriculum tiers for training-commencing with easier samples, progressing to medium difficulty, and culminating with challenging material ultimately achieving proficiency across the entire learning process."}, {"title": "2.1 Curriculum Design", "content": "Large-scale Language Model The pre-training of large-scale language models relies on a vast amount of unsupervised data (Touvron et al., 2023a,b; OpenAI, 2024). Assuming a sentence X consists of N tokens $x_1, ..., x_n$, and a context window of size k, the training objective of the LLM is:\narg max p(X; \u03b8) = \\sum log p_\u03b8(x_i | x_{i-k:i-1}), (1)\n\u03b8\nwhere \u03b8 denotes the trainable parameter."}, {"title": "2.2 LLM-based CL", "content": "When we construct curriculums of varying difficulty levels using an LLM, we adhere to the principles of curriculum learning, gradually progressing from easy to hard through iterative training.\nEasy-stage During the initial stage, we only utilize easy samples for supervised fine-tuning until the model converges.\narg max log P (t|s; \u03b8) (3)\n\u03b8e\nDe\nwhere \u03b8 is based on the LLM initialization.\nMedium-stage During the second stage of learning intermediate difficulty curriculums, we steadily introduce new levels of difficulty data into the model trained during the first stage. Concurrently, to safeguard against the model forgetting the samples learned in the previous stage (Hui et al., 2022), we integrate the data from the preceding stage into the training process.\narg max log P (t|s; \u03b8m ) (4)\n\u03b8m\nDe\u222aDm"}, {"title": "3 Experimental Settings and Results", "content": "where $\u03b8_e$ is based on the model initialization from the easy stage.\nHard-stage Following a methodology similar to our previous training process, we persistently iterate by introducing increasingly challenging samples while reinforcing previously learned ones to safeguard against the model forgetting earlier knowledge (Hui et al., 2022). The training process concludes once the model ultimately converges, marking the end of the training.\narg max log P (t|s; \u03b8_m) (5)\n\u03b8_h\nDe\u222aDm\u222aDh\nwhere \u03b8n is based on the model initialization from the medium stage, and \u03b8m is the parameter that encapsulates the entirety of the LLM based CL training process."}, {"title": "3.1 Data and Evaluation", "content": "For fine-tuning baseline PLMs, we adopt the approach of Rothe et al. (2021) by exclusively utilizing the English cLang8 data for fine-tuning. This decision was informed by their observation that additional fine-tuning on high-quality English datasets, such as FCE v2.1 (Yannakoudakis et al., 2011) and W&I (Yannakoudakis et al., 2018), resulted in decreased performance. It should be highlighted that deduplication has been executed on the cLang8 dataset. In the selection process of easy, medium, and hard data samples by LLM, we exclude those samples that are correct-correct pairs. We mandate the LLM to rank exclusively the samples that are error-correct pairs. We utilize the CoNLL13 (Ng et al., 2013) for our validation set, while the test set is based on the widely used official-2014.combined.m2 version of CoNLL14 (Ng et al., 2014). We also use the BEA19 (Bryant et al., 2019) development data when testing the BEA19 English test sets."}, {"title": "3.2 Implementation Details and Training", "content": "To optimally utilize the capabilities of large models to rank the difficulty levels of the GEC training set data, we choose the large-scale language model, LLaMA2-70b (Touvron et al., 2023b), as a dedicated expert in determining grammatical difficulty. To enhance its ability to recognize and correct the difficulty level of grammatical errors, We carefully designed a prompt for it."}, {"title": "3.3 Baselines", "content": "To investigate the impact of our proposed LLM-based CL approach for GEC, our models are compared with the following baselines. The GEC-ToR (Omelianchuk et al., 2020) and TMTC (Lai et al., 2022) models employ a sequence tagging approach to enhance GEC performance through multi-stage training. The TagGEC (Stahlberg and Kumar, 2021) model enhances the performance of GEC through data augmentation, specifically by generating synthetic data guided by error type tags. The GEC models, SADGEC (Sun et al., 2021), T5-large/xl/xxl (Rothe et al., 2021) are based on pre-trained language models. SADGEC utilizes the BART pre-trained model, while T5-large/xl/xxl"}, {"title": "3.4 Experimental Results", "content": "To effectively illustrate the efficiency of our proposed LLM-based CL GEC models, we executed comparative experiments in conjunction with existing studies. The results of these experiments are clearly outlined . As expected, the methods of learning through a designed curriculum and progressively training on the data, namely the Len-based CL and proposed LLM-based CL methods, have significantly better results than the different types of baseline models fine-tuned with the full mixed data (i.e. T5-xx/ LLaMA2-xx GEC VS T5-xx/ LLaMA2-xx Len/ LLM-based CL). This has been demonstrated across three evaluation data sets: the CoNLL14 test, BEA19 test, and BEA19"}, {"title": "4 Analysis", "content": "development sets. This marks our inaugural, comprehensive demonstration that curriculum learning proves effective in handling GEC tasks. It is noteworthy that, while the Len-based CL methods exhibit an enhanced performance compared to the implemented robust baseline models, the degree of their improvement is not as significant as that of our proposed LLM-based CL methods, when evaluated with the $F_{0.5}$ score under the same model size ( i.e. T5-xx/LLaMA2-xx Len-based CL VS T5-xx/LLaMA2-xx LLM-based CL). This suggests that our introduced LLM-based CL methods demonstrate greater efficacy in utilizing the curriculum learning strategy for GEC task.\nInterestingly, we observed that LLMs with a decode-only architecture, such as LLaMA2-7/13b, did not perform as well as models with encoder-decoder architecture, like T5-large/xl. Their parameters are significantly less than LLaMA2, with T5-large (770M) being much smaller than LLaMA2-7b, and T5-xl (3b) being substantially less than LLaMA2-13b. This was particularly evident in the performance on BEA19-test/dev, and the discrepancy was almost similar on CoNLL14 test. We speculate that this discrepancy may be due to the architectural differences having a significant impact on the GEC task. The GEC task requires the generation of correct source words for copying, making the encoder-decoder structure more suitable. Additionally, as suggested by Zhang et al. (2023), LLaMA series of LLMs also have hallucination issues that limit their performance on the GEC task. While this issue warrants further investigation in future research, it is important to note that our proposed LLM-based CL method remains remarkably effective on models of the same series and size."}, {"title": "4.1 New Baselines Comparison", "content": "During the training phase of curriculum learning, the common approach is to start with simple data and gradually introduce more complex samples for training. This prevents knowledge forgetting that may occur when encountering slightly more challenging data right after completing the introductory phase of the course. However, this approach might be contentious. For instance, we usually compare this method to a baseline that uses the entire default training data. However, during the process of curriculum learning, data from earlier stages might be repeatedly studied, which could lead to questions about the fairness of the baseline comparison. To address this issue, we designed an ablation study using the data of the same type and quantity for training."}, {"title": "4.2 Learning Guidelines: From Hard to Easy", "content": "To validate whether the model can only be effective through progressive training from easy to hard samples, we adjust the learning strategy to start with hard data and gradually incorporate simpler data."}, {"title": "4.3 Types of Error Analysis", "content": "We employ the ERRANT toolkit (Bryant et al., 2017) to examine the performance of GEC systems in correcting diverse types of errors. Figure 3 illustrates the performance of T5-xl and LLaMA2-13b's Baseline, Len-based CL, and LLM-based CL methods on the CoNLL14 test set in terms of POS-based fine-grained error types. It is evident from the figure that, regardless of whether it is the baseline or CL method, LLaMA2-13b outperforms the T5-xl model in the majority of fine-grained error types (i.e., DET, NOUN:NUM, VERB, VERB:TENSE, NOUN, VERB:FORM, ADJ, CONJ). However, there are a few error types where the trend is reversed, such as in the case of PREP and PRON performance. This could potentially be attributed to the differences in the structures of the two models. Moreover, the LLM-based approach evidently shows superior performance over the Len-based CL in many error types, including but not limited to DET, NOUN:NUM, VERB, PUNCT, NOUN, MORPH, SPELL, PRON, and CONJ. Overall, the CL methods exhibit good performance across various errors in GEC, particularly our proposed LLM-based CL method, which excels in error correction performance."}, {"title": "5 Related Work", "content": "GEC in Language Models The rise of PLMs has led to a surge in research efforts that utilize these models to boost the efficiency of GEC. Recent studies show the positive impact of PLMs on the enhancement of GEC tasks."}, {"title": "Curriculum Learning", "content": "The concept of Curriculum Learning (CL) has been introduced in the field of machine learning, and is used for tasks such as image classification and language modeling (Bengio et al., 2009). This method entails a categorization of examples into easy and difficult and the organization of a curriculum from simple to complex. Studies have shown that during training, a model can take advantage of the CL strategy (Ren et al., 2019; Saxena et al., 2019). The crucial aspect here is to devise the right challenging examples during the curriculum design process. To elaborate further, CL has played a pivotal role in NLP applications including but not limited to dependency parsing (Spitkovsky et al., 2010), sentiment analysis (Cirik et al., 2016), neural machine translation (NMT) (Kocmi and Bojar, 2017; Kumar et al., 2019; Zhang et al., 2018; Liu et al., 2020; Zhou et al., 2020b) and reading comprehension (Tay et al., 2019). The key to effective CL lies in setting appropriate difficulty levels for different tasks. To achieve this goal, numerous strategies have been proposed, which are deemed to guide the step-by-step learning of the model. For instance, Zhou et al. (2020b) introduced an uncertainty-aware CL concept. They use the perplexity of training examples as a criterion for curriculum difficulty and employ a self-adaptive learning strategy for the NMT task. Zhao et al. (2020) suggest a method for generating responses by incorporating a knowledge selection module into GPT-2. Wang et al. (2022) have incorporated CL to structure the transformed data from easy to complex. This is done to fine-tune pre-existing models that are pre-trained for the purpose of understanding source code. At present, our paper is the first to consider using CL methods in the GEC task. It is also the first time that a LLM is used to automatically analyze the difficulty level of data, and this method has achieved significant results."}, {"title": "6 Conclusion", "content": "This paper proposes a novel method for CL based on LLMs, called the LLM-based CL method. Compared to conventional CL approaches, our proposed LLM-based CL method aligns closely with curriculums designed by human experts, indicating that leveraging the powerful semantic understanding and discriminative capabilities of LLMs enables the differentiation and reasonable arrangement of GEC training data from easy to hard samples into different curriculums. Upon obtaining GEC data of varying difficulty curriculum levels, we iteratively train and refine models from easy to hard samples using the T5 and LLaMA series. Through testing and analysis on various English GEC benchmarks, we find that the proposed LLM-based CL method significantly outperforms baseline models and previous CL methods. Furthermore, we discuss the sequence of training in CL forms and find that models only benefit from learning processes that progress from easy to hard; conversely, learning from hard to easy is almost ineffective, validating the effectiveness of CL in GEC. In addition to comparing against the traditional baseline of training data directly at once, we also compare CL step-by-step iterative training with training by adding the same amount of data at once as new baselines, finding that progressively adding difficult samples enhances GEC model performance."}, {"title": "Limitations", "content": "The paper aims to propose an LLM-based curriculum learning paradigm to enhancing the English GEC performance. While experiments and analyses validate the effectiveness of this method, there are still aome limitations: Firstly, the process of annotating and scoring millions of training data instances using a large-scale language model to differentiate between different courses is time-consuming; however, this may represent a trade-off between performance and time. Secondly, we have only validated the effectiveness of constructing a new curriculum learning paradigm using a large-scale language model in the context of the GEC task. Whether this approach is effective for other NLP tasks remains to be further explored. In the future, we plan to extend our research to include a broader range of NLP tasks."}]}