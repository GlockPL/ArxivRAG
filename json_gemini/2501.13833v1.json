{"title": "On the Reasoning Capacity of AI Models and How to Quantify It", "authors": ["Santosh Kumar Radha", "Oktay Goktas"], "abstract": "Recent advances in Large Language Models (LLMs) have intensified the debate surrounding the fundamental nature of their reasoning capabilities. While achieving high performance on benchmarks such as GPQA and MMLU, these models exhibit limitations in more complex reasoning tasks, highlighting the need for more rigorous evaluation methodologies. We propose a novel phenomenological approach that goes beyond traditional accuracy metrics to probe the underlying mechanisms of model behavior, establishing a framework that could broadly impact how we analyze and understand AI systems. Using positional bias in multiple-choice reasoning tasks as a case study, we demonstrate how systematic perturbations can reveal fundamental aspects of model decision-making. To analyze these behaviors, we develop two complementary phenomenological models: a Probabilistic Mixture Model (PMM) that decomposes model responses into reasoning, memorization, and guessing components and an Information-Theoretic Consistency (ITC) analysis that quantifies the relationship between model confidence and strategy selection. Through controlled experiments on reasoning benchmarks, we show that true reasoning remains challenging for current models, with apparent success often relying on sophisticated combinations of memorization and pattern matching rather than genuine logical deduction. More fundamentally, we demonstrate that accuracy alone often overstates a model's reasoning abilities, as model behavior can be characterized through underlying mechanisms in the phase space of cognitive strategies, revealing how models dynamically balance different approaches when responding to queries. This framework enables quantitative criteria for real-world deployments, allowing applications to specify reliability thresholds based on strategy distributions rather than aggregate performance metrics. By establishing principled methods for probing and quantifying reasoning behavior, our work provides both theoretical insights into model capabilities and practical tools for developing more reliable evaluation methodologies.", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of increasingly capable Large Language Models (LLMs) has raised fundamental questions about the nature of artificial intelligence and its capacity for genuine reasoning. While models like GPT-4, Claude, LLama and Gemini demonstrate remarkable performance across wide spectrum of complex tasks [1-3], distinguishing between true logical deduction and non-deductive cognitive processes remains a central challenge in AI research [4, 5]. Traditional evaluation methodologies, primarily centered on benchmark performance and accuracy metrics, have provided valuable but incomplete insights into model capabilities, with standard reasoning benchmarks such as GSM8K [6], GPQA [7], and Big-Bench [8] demonstrating impressive yet potentially misleading performance figures. Recent investigations systematically challenge these results - experiments with GSM-Symbolic reveal significant performance degradation under minor question reformulations despite preserved logical structure [9], while analyses of other benchmarks demonstrate how success often stems from dataset-specific regularities rather than genuine logical inference [10, 11]. These findings, coupled with the increasing deployment of LLMs in domains ranging from scientific research to educational assessment, highlight the limitations of current evaluation frameworks and underscore the need for comprehensible benchmarks and explainable AI (XAI) [12] approaches to elucidate the mechanisms underlying model behavior. Increasingly widespread usage of LLMs, and AI models in critical domains such as scientific research and educational assessment necessitates methodologies that can systematically characterize and interpret the fundamental processes driving model predictions [13]. The ability to distinguish between genuine reasoning and other cognitive processes becomes particularly crucial in applications where verifiable logical deduction directly impacts system reliability and user outcomes.\nExisting approaches to understanding model reasoning capabilities have primarily focused on enhancing evaluation frameworks through techniques such as Chain-of-Thought prompting [5], Iteration-of-Thought [14], and self-consistency checks [15]. While these methods demonstrate improved performance on reasoning benchmarks, they primarily operate by structuring the model's input and output format rather than providing insights into the underlying decision-making processes. Quantitative analyses of these enhanced frameworks reveal that improvements often arise from better exploitation of model priors rather than enhanced logical deduction capabilities [16, 17]. For instance, contemporary studies demonstrate that models can achieve high accuracy on reasoning tasks while producing logically inconsistent intermediate steps [18, 19], suggesting that apparent reasoning success may emerge from sophisticated pattern matching rather than systematic logical inference.\nThe fundamental limitation of current evaluation methodologies lies in their treatment of model behavior as a black-box optimization problem, where success is primarily measured through aggregate performance metrics such as accuracy and F1 scores. While computationally tractable, this approach fails to capture the dynamic interplay between different cognitive strategies employed by these models. More critically, it provides limited insight into how models transition between different decision-making modes when confronted with varying task complexities or input perturbations. These limitations become particularly evident when examining how model performance varies systematically under controlled modifications to input structure, suggesting the need for evaluation frameworks that can probe and quantify the fundamental mechanisms underlying model behavior.\nTo address these methodological limitations, we propose adopting a phenomenological modeling approach to AI systems analysis. In physical scientific domains, phenomenological models M provide valuable insights by constructing theoretical frameworks of complex underlying systems that capture observable behaviors under specific experimental conditions. While phenomenological approaches have been successfully applied to understand neural network dynamics and training trajectories [20, 21], their application to language models remains largely unexplored. While such models may not fully characterize the underlying system, they enable quantitative predictions within well-defined regimes and offer systematic approaches to understanding complex phenomena. We develop two such models: a Probabilistic Mixture Model (PMM) that decomposes cognitive strategies (fig. 1) and an Information-Theoretic Consistency (ITC) analysis that quantifies decision-making uncertainty. Applied to language models, this framework enables the decomposition of model behavior into measurable components through controlled perturbations of input conditions. Consider a language model B operating on input query space X. Traditional evaluation approaches treat this as a mapping B : X \u2192 Y, where Y represents the output response space. In contrast, our phenomenological approach introduces an intermediate representation B : X \u00a6 S\u21e8 Y, where S represents a space of cognitive strategies, and mappings \u03ba, \u03c9 characterize how the model selects and applies these strategies."}, {"title": "II. RELATED WORK: BENCHMARKS, BIAS, AND REASONING IN LANGUAGE MODELS", "content": "Reasoning benchmarks such as GPQA, GSM8K, and Big-Bench have become vital tools for evaluating the logical and deductive reasoning capabilities of LLMs across diverse tasks, including arithmetic, abstract problem-solving, and contextual understanding [6-8]. Techniques such as CoT prompting and its iterative extension, Iteration-of-Thought (IoT), Tree-of-Thought (ToT) have demonstrated notable improvements in reasoning performance by guiding models to produce intermediate reasoning steps, enabling more structured logical deduction and enhanced problem-solving [5, 14, 24]. However, recent studies have questioned whether these methods and benchmarks genuinely assess reasoning abilities or primarily capture the models' reliance on dataset-specific heuristics and statistical patterns [25, 26]. Approaches like GSM-Symbolic have tackled these concerns by employing symbolic templates for controlled question generation, revealing substantial performance declines when faced with slight changes in phrasing or irrelevant numerical adjustments, even when the underlying reasoning process remains constant [9]. These findings highlight"}, {"title": "III. MODELS AND FRAMEWORK", "content": "The GPQA benchmark has emerged as a powerful tool for evaluating reasoning capabilities in language models, offering a diverse set of multiple-choice questions that probe various aspects of logical and analytical thinking. While positional bias is a well-documented phenomenon, its manifestation and impact can vary significantly depending on the specific model architecture, model parameters, and dataset characteristics. In this work, we first develop a systematic framework to analyze these position-dependent effects in the context of GPT-4o-mini's performance on GPQA. This focused examination serves two purposes: first, it provides detailed insights into how positional information influences reasoning behavior in a specific model-benchmark pair, and second, it establishes a methodological foundation for broader investigations of our reasoning in the next section."}, {"title": "A. Positional Bias Analysis", "content": "The core of our analysis centers on the GPQA benchmark, a multiple-choice reasoning evaluation framework where each question presents four options, exactly one of which is correct. Let Q = {q_n | n \u2208 [1,M]} denote our question set, where M is the total number of questions. Each question q\u2208Q is associated with one correct answer c(q) and three incorrect options {w_1(q), w_2(q), w_3(q)}. These options are presented in fixed positions drawn from the set O = {A, B, C, D}.\nWe treat the language model B as a black-box system, which for any given question q and its associated options, generates a probability distribution over the possible answer positions:\nB(q) \u2192 \u03a0(q) = [\u03c0_\u0391, \u03c0_\u0392, \u03c0_C, \u03c0_D]\nwhere \u03c0_o \u2208 [0, 1] represents the probability assigned to position o \u2208 O, and \u03a3_{o\u2208O}\u03c0_o = 1.\nTo systematically evaluate positional dependencies, we introduce an order parameter \u03b8\u2208 [0,1] that determines"}, {"title": "B. Probabilistic Mixture Model", "content": "To dissect model behavior, we develop a framework decomposing responses into three fundamental cognitive strategies: memorization, reasoning, and guessing. This decomposition, while not unique, provides a natural basis for quantifying the balance between pattern matching and logical inference in positional reasoning tasks. As we will see in section IV B, the model's empirical validation and observed deviations suggest potential extensions for different experimental conditions while confirming its utility for the studied tasks.\nTo begin with, building on our earlier notation, we consider the black box model B operating on questions q\u2208Q with options in positions O. For each question, we hypothesize that the model employs a mixture of strategies with probabilities P_M(q), P_R(q), and P_G(q), representing the probability of using memorization, reasoning, and guessing strategies respectively for question q. These probabilities sum to one (complete basis assumption):\nP_M(q) + P_R(q) + P_G(q) = 1\nThe probability of a correct response under this mixture model for a question q and position o \u2208 O is:\nP_{correct}(q, o) = P_M(q) \\cdot P_M(o) + P_R(q) \\cdot P_R + P_G(q) \\cdot P_G\nwhere P_M(o) represents the success probability under memorization strategy for position o, while P_R and P_G are position-independent success probabilities for reasoning and guessing strategies, respectively. Intuitively, this equation describes how the model combines three different approaches to answer a question: when it uses memorization (P_M(q)), its success depends on the specific position; when it uses reasoning (P_R(q)), it has a fixed success"}, {"title": "C. Information-Theoretic Consistency Analysis", "content": "While our probabilistic mixture model reveals the strategies employed by the language model, we require a complementary framework to assess the quality and calibration of its predictions. Our analysis involves several interrelated probability distributions: the model's raw output probabilities \u03c0_o(q), the accuracy-dependent reference probabilities we'll define below, and"}, {"title": "IV. RESULTS AND DISCUSSION", "content": "To systematically investigate positional bias in language model reasoning, we conducted experiments using the GPT-4o-mini model on the GPQA-Diamond dataset, comprising 198 curated questions spanning physics, mathematics, and computer science. Our experimental framework centers on manipulating the position of correct answers while maintaining question content, allowing us to isolate and quantify position-dependent effects on model behavior."}, {"title": "A. Position-Dependent Accuracy Under Randomization as order-parameter", "content": "Before delving into our more sophisticated modeling frameworks, we first establish foundational insights through a systematic analysis of positional effects under controlled randomization conditions. Building on our positional bias framework from eq. (2), we investigate two distinct randomization scenarios to probe the model's reliance on positional cues. For each position o \u2208 O and randomization parameter \u03b8\u2208 [0,1], our experimental protocol is as follows: We take a fraction @ of the questions and randomize their correct answer placements according to two distinct schemes, while for the remaining (1-\u03b8) fraction, the correct answer remains at position o. In the first scheme (inclusive randomization), the correct answer can be placed at any position including o, while in the second (exclusive randomization), the correct answer is explicitly prevented from appearing at position o. This controlled approach enables us to disentangle different aspects of positional dependence.\nWe investigate position-dependent model behavior through two randomization protocols, systematically varying the placement of correct answers across positions. For inclusive randomization, we maintain the original position-specific accuracy metric a\uff61(q,\u03b8) but now track its mean and variance across multiple trials:\n\u03bc^{inc}_{ao} (\u03b8) = \\frac{1}{N_t} \u03a3_{n=1,q\u2208Q}^{N_t} E[a_o (q,\u03b8)] = \\frac{1}{N_t} \u03a3_{n=1,q\u2208Q}^{N_t} a_o^{inc}(q, \u03b8)\n\u03c3^{inc}_{ao} (\u03b8)^2 = \u03a3_{q\u2208Q} Var[a\uff61(q, \u03b8)] = \\frac{1}{N_t} \u03a3_{n=1 q\u2208Q}\u03a3 (a\uff61(q,\u03b8) \u2013 \u03bc^{inc}_{ao} (\u03b8))^2\nFor exclusive randomization, we modify our framework to exclude the original position from the randomization explicitly set 0% = 0 \\ {o}, giving us similarly exc(0) and exc (0)2.\nFor a given position o and randomization parameter \u03b8, we select a portion of questions for randomization. In inclusive randomization, these questions can have their correct answer placed at any position, while in exclusive randomization, the correct answer is prevented from appearing at position o. The correct answer maintains its original position for the remaining (1 - \u03b8) portion of questions.\nThe difference between these metrics provides insight into how strongly the model associates specific positions with correct answers:\n\u0394\u03bc_o(\u03b8) = \u03bc^{inc}_{ao}(\u03b8) \u2013 \u03bc^{exc}_{ao} (\u03b8)"}, {"title": "B. Decomposing Language Model Decisions: Reasoning, Memorization, or Guessing?", "content": "Having established the prevalence of positional dependencies in model behavior, we now turn to a more fundamental question: what cognitive strategies drive these responses? Our probabilistic mixture model, introduced in eqs. (5) and (6), provides a principled framework for decomposing model behavior into three fundamental strategies-pure logical reasoning (applying systematic deduction), pattern-based memorization (leveraging learned position-specific heuristics), and random guessing (selecting answers without clear rationale). This decomposition builds directly on our positional bias observations, where varying performance across positions suggested the interplay of multiple decision-making processes. By quantifying the relative contributions of each strategy, we move beyond aggregate performance metrics to understand the underlying mechanisms that shape model predictions.\nThe key insight of our analysis lies in leveraging positional information to distinguish between these strategies. As formalized in eq. (7), memorization manifests as strong position-specific preferences, while true reasoning should exhibit position-invariant performance. By measuring the accuracy differential between memorized and non-memorized positions (eq. (16)), we can isolate the contribution of memorization (PM(q)) and subsequently derive the probabilities of reasoning (PR(q)) and guessing (PG(q)) through eqs. (19) and (20). This approach capitalizes on the position-dependent effects observed in section IV A, using systematic variations in performance across positions as a probe for understanding strategy selection. This analysis reveals striking patterns in strategy selection across different question types.\nBefore examining the full distribution of cognitive strategies, we first establish the validity of our phenomenological model through rigorous comparison with empirical data. For each question q, we derive the position-averaged observed accuracy as:\nC_observed(q) = \\frac{a_{om} (q) + 3x_{other} (q)}{4}"}, {"title": "V. CONCLUSION", "content": "This work introduces a fundamentally new approach to understanding and evaluating reasoning capabilities in language models through the lens of phenomenological analysis. By developing dual frameworks - the Probabilistic Mixture Model (PMM) and Information-Theoretic Consistency (ITC) Analysis - we demonstrate how controlled experimentation (like positional bias) can reveal deeper insights into model behavior than traditional accuracy metrics alone. Our analysis reveals several critical findings about current language models' reasoning capabilities. First, the persistent coupling between positional structures and strategy selec-"}]}