{"title": "Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces", "authors": ["Jiyuan Wang", "Weishan Ye", "Jialin He", "Li Zhang", "Gan Huang", "Zhuliang Yu", "Zhen Liang"], "abstract": "With the rapid advancement of deep learning, attention mechanisms have become indispensable in electroencephalography (EEG) signal analysis, significantly enhancing Brain-Computer Interface (BCI) applications. This paper presents a comprehensive review of traditional and Transformer-based attention mechanisms, their embedding strategies, and their applications in EEG-based BCI, with a particular emphasis on multimodal data fusion. By capturing EEG variations across time, frequency, and spatial channels, attention mechanisms improve feature extraction, representation learning, and model robustness. These methods can be broadly categorized into traditional attention mechanisms, which typically integrate with convolutional and recurrent networks, and Transformer-based multi-head self-attention, which excels in capturing long-range dependencies. Beyond single-modality analysis, attention mechanisms also enhance multimodal EEG applications, facilitating effective fusion between EEG and other physiological or sensory data. Finally, we discuss existing challenges and emerging trends in attention-based EEG modeling, highlighting future directions for advancing BCI technology. This review aims to provide valuable insights for researchers seeking to leverage attention mechanisms for improved EEG interpretation and application.", "sections": [{"title": "1. Introduction", "content": "Research in brain-computer interfaces (BCIs) has long been challenged by the need to process large, complex datasets of brain signals [1]. The primary difficulty arises from the diverse and complicated nature of electroencephalography (EEG) signals, which requires efficient and effective strategies for signal analysis and modeling [2]. Attention mechanism-based models have shown exceptional promise in tackling the complexities of EEG signal processing [3]. By selectively focusing on critical information within extensive brain signal datasets, Attention models help to minimize irrelevant noise, thereby significantly improving data processing efficiency [4]. Attention mechanisms not only enhances the effectiveness of BCI research but also introduces greater flexibility and intelligence in the development of models tailored for BCI applications [5].\nAttention mechanisms draw inspiration from biological visual and auditory processes, as well as cognitive processes in psychology [6]. Research has demonstrated that during visual and auditory recognition tasks, humans naturally focus on key elements while suppressing irrelevant information, thereby enhancing the accuracy and efficiency of recognition and decision-making [7]. Leveraging this principle, attention mechanisms in models, commonly referred to as attention models, are designed to assign flexible weights to different features [8]. This enables the model to concentrate on critical information pertinent to the target task while filtering out extraneous data [9]. Attention models also enhance the understanding of the relationships between input and output data, which in turn improves the model's interpretability [10]. This enhancement not only maximizes the effective utilization of data but also reduces the impact of data variability caused by individual differences, as the model can learn to prioritize more representative features. In BCI research, these capabilities are particularly valuable, as they help enhance the accuracy of neural decoding, improve the robustness of brain modeling, and enable more adaptive and personalized brain-computer interaction [11]. Furthermore, attention models are well-suited for multimodal BCI applications, where they facilitate the efficient fusion of features extracted from different modalities [12]. As a result, the use of attention models in BCI-related research tasks holds significant potential and offers promising avenues for further exploration.\nAttention models have been initially applied extensively in computer vision and natural language processing (NLP) domains, typically integrated as modules within the backbone frameworks of convolutional neural networks (CNNs) and recurrent neural networks (RNNs) [13]. In 2014, Mnih et al. [14] and Bahdanau et al. [15] introduced attention mechanisms in RNNS for image classification and machine translation tasks in NLP, respectively. The introduction of a novel self-attention mechanism by Vaswani et al. [16] in 2017, through the \"Transformer\u201d model architecture for machine translation tasks, further propelled the use of attention models. Since then, Transformer models and their variants have been applied across various tasks [17]. For example, Dosovitskiy et al. proposed the Vision Transformer [18], demonstrating that a pure Transformer architecture could be effective for computer vision tasks without relying on CNN modules. Additionally, Liu et al. introduced the Swin Transformer [19], which utilizes a windowed self-attention mechanism to reduce the computational complexity of Transformer models. Nowadays, attention mechanisms are foundational to modern deep learning, with their flexibility and efficacy continuing to revolutionize artificial intelligence research and applications.\nBuilding on significant breakthroughs in computer vision and NLP, attention models have also attracted substantial interest in the BCI field, catalyzing rapid progress in integrating EEG signal processing with attention mechanisms. To assess the growing interest in this area, we conduct a literature search using Google Scholar to track the number of publications since 2019. The search is performed using two keyword combinations: (1) \"attention\" +\"EEG\" +\"deep learning\", and (2) \"attention\" +\"EEG\" +\"Transformer\" +\"deep learning\".The searching results are presented in Fig. 2, showing the number of papers retrieved from Google Scholar for each of the two keyword combinations. In the BCI domain, attention mechanism modeling generally falls into two categories. (1) Traditional Attention Mechanism-Based Modeling. It calculates attention weights for various types of information in EEG signals, such as spatial, temporal, and spectral features, prioritizing those most relevant to the task. (2) Transformer-Based Multi-Head Self-Attention Modeling. It employs multiple attention heads to simultaneously focus on different parts of the EEG data, enabling the model to capture both global and local relationships across various dimensions [20]. Furthermore, extending these two modeling strategies to multimodal applications significantly enhances the model's ability to process and integrate information from different modalities [21]. Such integration is particularly critical for developing efficient and accurate BCI systems, as it allows for a more comprehensive understanding of the user's intentions and mental state.\nThe following sections provide a comprehensive exploration of the applications of attention models in BCIs, focusing on their role in enhancing the understanding of EEG signals and advancing BCI technology. Section 2 introduces the concept of traditional attention mechanisms and categorizes their specific applications in EEG signal modeling. Section 3 details EEG signal modeling methods based on Transformer multi-head self-attention mechanisms. Given the growing popularity of multimodal models, Section 4 discusses the application of attention models in multimodal contexts. Finally, Section 5 summarizes the key points of this work and provides future perspectives on the use of attention mechanisms in EEG signal modeling."}, {"title": "2. Traditional Attention Mechanisms in EEG", "content": "Traditional attention mechanism-based modeling enhances performance and generalization by efficiently selecting features through adaptive weighting and combining different types of information. Given input data, attention modeling dynamically computes feature weights based on prior knowledge or task-specific requirements. Depending on how these weights are applied, attention mechanisms can be broadly categorized into soft and hard attention. In the soft attention mechanism, each feature is assigned a weight that is continuously distributed between 0 and 1. These weights are differentiable, which allows them to be optimized through continuous learning within the network model [22] Compared to hard attention, where features are either entirely selected or ignored, soft attention provides a more refined weighting approach, enabling the model to learn the relative importance of features more effectively. This leads to smoother gradient flow during backpropagation, contributing to more stable and efficient training [23]. In contrast, hard attention assigns non-differentiable weights, which cannot be optimized through conventional deep learning techniques [24] Due to these limitations, hard attention is challenging to integrate directly with traditional deep learning models. Therefore, this paper will not cover or summarize research work related to hard attention mechanisms.\nBuilding on this foundational understanding of attention mechanisms, it is crucial to explore their implementation in practical scenarios, particularly with EEG data. Attention modules can vary significantly depending on their scope and integration into the model, and analyzing these variations provides a more comprehensive understanding of their impact. To do so, we will focus on two critical aspects of attention module implementation: the specific types of attention modules used and their methods of embedding within broader model architectures."}, {"title": "2.1. Types of Attention Modules", "content": "In brain modeling tasks, attention mechanisms enhance feature extraction from EEG signals across channel, temporal, and frequency dimensions by assigning weights to highlight the most relevant information, as shown in Fig. 3.\n(1) Channel Attention Module. The channel attention module is designed to assess and adaptively weight the importance of each EEG channel. Given that different brain regions contribute unequally to various tasks, the channel attention module enables the model to prioritize channels with the most relevant information while minimizing the impact of less informative channels, thereby improving task-specific analysis and reducing noise.\nFor a given multi-channel EEG signal vector $X_c \\in R^{1\\times C}$, where $C$ denotes the number of channels, an attention weight vector $W_c \\in R^{1\\times C}$ is initialized. The model calculates a weighted combination of $X_c$ and $W_c$, followed by the application of the softmax function to produce the output $\\widetilde{X}_c$ from the channel attention module, as shown below:\n$\\widetilde{X}_c = \\text{softmax}(X_cW_c)$ (1)\nIn BCI applications, the channel attention module effectively identifies the most relevant brain regions for specific tasks, such as motor imagery or emotional state classification [25, 26, 27, 28]. By concentrating on channels linked to critical brain functions, this module improves feature extraction, leading to enhanced performance in BCI systems and other brain modeling applications.\n(2) Temporal Attention Module. The temporal attention module is designed to effectively capture the dynamic fluctuations in brain activity that occur over time, acknowledging that EEG signals reflect varying brain states depending on the nature of a given task. During task execution, brain activity often exhibits multiple phases of relevance, with some fluctuations directly related to the task while others contribute less meaningful information. By assigning higher attention weights to time periods that are directly correlated with the task at hand, the module enhances the model's ability to extract task-relevant features and reduce the influence of irrelevant temporal variations.\nFor a given EEG signal vector $X_t \\in R^{1\\times T}$, where $T$ represents the temporal dimension, an attention weight vector $W_t \\in R^{1\\times T}$ is initialized. The model then calculates a weighted combination of $X_t$ and $W_t$, followed by the application of the softmax function to yield the temporal attention output $\\widetilde{X}_t$, as shown below:\n$\\widetilde{X}_t = \\text{softmax}(X_tW_t)$ (2)\nIn BCI applications, the temporal attention module effectively captures relevant patterns of brain activity during cognitive or emotional tasks [29, 30]. For example, in motor imagery tasks, it highlights critical moments by identifying peaks in attention at significant time intervals [31, 32]. By concentrating on time segments that are closely linked to the task, the temporal attention module ensures that the model accurately captures the dynamic characteristics of EEG signals, leading to improved performance in analyzing time-series brain activity data and enhancing the robustness of BCI systems.\n(3) Frequency Attention Module. The frequency attention module is designed to leverage the significance of frequency components in EEG signals, as frequency-domain analysis reveals crucial insights into neural activities across different scales. Features such as Power Spectral Density (PSD) and Differential Entropy (DE) are valuable indicators of brain activity, and the frequency attention module adaptively highlights these key features by assigning weights to various frequency components within the signal.\nFor a frequency vector $X_f \\in R^{1\\times F}$, where $F$ represents the frequency dimension, an attention weight vector $W_f \\in R^{1\\times F}$ is initialized. The model computes a weighted combination of $X_f$ and $W_f$, and applies the softmax function to derive the output $\\widetilde{X}_f$ from the frequency attention module, as shown below:\n$\\widetilde{X}_f = \\text{softmax}(X_fW_f)$ (3)\nIn BCI applications, the frequency attention module effectively identifies and enhances the most informative frequency components, such as Alpha, Beta, and Gamma bands, which are known to be critical in tasks like attention monitoring, motor imagery, or emotional state classification [33, 34]. By adaptively assigning higher weights to these significant frequency bands, the module ensures that the model focuses on the most relevant aspects of the EEG data, ultimately enhancing the feature extraction process. This selective attention to key spectral features contributes to improved classification accuracy and robustness in BCI systems, thereby advancing the reliability of EEG-based analyses.\nThe aforementioned mentioned attention modules, including channel, temporal, and fre-quency, can be used individually or in combination based on the specific demands of the task. The selection and integration of these attention modules are determined by the unique characteristics of the problem being addressed. By strategically choosing the appropriate combination of attention mechanisms, researchers can effectively customize EEG signal processing and model design, providing a more adaptable and efficient solution tailored to the task's requirements."}, {"title": "2.2. Embedding Methods of Attention Modules", "content": "In embedding attention modules, two primary approaches are employed: single attention module embedding and multi-attention module integration. The single attention module approach thoroughly explores the internal dynamics, efficacy, and performance of a particular attention module across diverse application scenarios, providing insights into how it influences model learning and performance [8]. This allows researchers to tailor optimization strategies for specific tasks and datasets. In contrast, the multi-attention module approach integrates multiple attention modules, leveraging their complementary strengths to handle complex data more effectively [6]. This integration enhances the model's generalization capabilities and facilitates a deeper understanding of how different attention mechanisms interact and contribute to information extraction, as illustrated in Fig. 4.\n(1) Single Attention Module Embedding. In BCI modeling, embedding a single attention mechanism is widely used approach to enhance a model's ability to focus on critical features. The key idea is to emphasize specific information dimensions (channel or temporal or frequency) by leveraging a particular type of attention mechanism, thereby improving feature extraction and recognition efficiency.\nFor example, embedding a channel attention module specifically enhances the model's ability to capture important features at the channel level. Du et al. proposed the ATDD-LSTM model, which combined a channel attention module with a long short-term memory (LSTM) network. In this approach, the channel attention module was applied to feature vectors extracted by the LSTM layers, allowing the model to concentrate on channels most relevant to specific emotions while downplaying less relevant ones, which improved the accuracy of emotion recognition [35]. Inspired from this, Xu et al. integrated channel attention into a graph convolutional network (GCN), taking into account the spatial relationships between EEG recording electrodes [36]. Beyond channel attention, some studies focus on embedding temporal attention module to emphasize the temporal dynamics of EEG signals. Zhang et al. proposed a convolutional recurrent attention model that used CNNs to encode high-level representations and combined them with recurrent attention mechanisms (including LSTM networks and temporal attention modules). This method calculated attention weights on dynamic temporal features, allowed the model to focus on the most informative time periods, and extracted more valuable temporal information [37]. Inspired by the psychological peak-end rule, Kim et al. developed a model that integrated a bidirectional LSTM network with a temporal attention module. It assigned greater weight to emotional states occurring at key moments, capturing the dynamic variability of emotions over time and enhancing the model's interpretability [38]. For frequency-domain features, frequency attention is rarely modeled in isolation. Instead, it is typically integrated with spatial and temporal features to enhance EEG signal representation.\nWhile embedding a single attention module can effectively improve performance in specific tasks, several challenges remain. Choosing the right attention module is crucial. Determining how to integrate it optimally into the model framework for different scenarios is also important. Additionally, understanding how the placement of the attention layer affects model performance requires further exploration. Researchers need to carefully assess both the model architecture and the task requirements to design an optimal embedding strategy.\n(2) Multi-Attention Module Integration. Embedding multiple attention modules helps overcome the limitations of single attention module embedding by enabling the model to simultaneously capture various aspects of different feature dimensions. This approach enhances the model's capacity to learn diverse and informative features, thereby improving its robustness and generalization capabilities. Consequently, transitioning from single to multiple attention embedding is a natural step to better manage the complexity of real-world EEG data."}, {"title": "3. Transformer-based Multi-Head Self-Attention Mechanisms in EEG", "content": "The Transformer model, first proposed by Vaswani et al. in 2017 [16], introduced a self-attention mechanism that revolutionized machine translation tasks. The Transformer archi-tecture is composed of encoder and decoder modules, which are built by stacking multiple sub-layers, including self-attention, feed-forward neural networks, residual connections, and normalization layers [16, 49]. This composition enables the model to effectively encode input sequences and generate outputs, with the self-attention mechanism at its core enhancing the ability to capture contextual and temporal relationships [50].\nSince then, the Transformer and its variants have seen widespread application across fields such as natural language processing, and computer vision [51]. The core strength of the Transformer lies in its capacity to capture long-range dependencies and interactions among input features, making it particularly effective for time series modeling and also achieving significant advancements in temporal signal processing task. Table 2 summarizes a survey of studies that employ the transformer architecture. For example, AST [52] utilized a generative adversarial encoder-decoder framework to train a sparse Transformer model for time series prediction. It demonstrates that adversarial training can enhance time series prediction by directly shaping the network's output distribution to mitigate error accumulation during one-step-ahead inference. FEDformer [53] applied attention mechanisms in the frequency domain using Fourier and wavelet transforms, achieving linear complexity by randomly selecting a fixed-size subset of frequencies. It is noted that FEDformer's success has spurred increased interest in exploring the self-attention mechanism in the frequency domain for time series modeling [54]."}, {"title": "3.1. Multi-Head Self-Attention Mechanisms", "content": "The self-attention mechanism, also known as \"Scaled Dot-Product Attention\" [55], offers a significant advantage over traditional attention models. It captures contextual relationships effectively, especially in long sequences. This helps overcome challenges like information loss and long-term dependencies. Self-attention analyzes correlations between positions in the input sequence, making it more efficient than CNNs and RNNs for sequence modeling.\nIn the Transformer model, self-attention is implemented using three matrices: Query (Q), Key (K), and Value (V). These matrices are derived from the input feature matrix $I \\in R^{L\\times D}$, where L is the sequence length and D is the feature dimension. The matrices Q, K, and V are obtained by applying linear transformations to I:\n$Q = I \\Omega W,$ (4)\n$K = I \\Omega W^K,$ (5)\n$V = I \\Omega W^V,$ (6)\nwhere $W^Q \\in R^{D\\times D_k}, W^K \\in R^{D\\times D_k}$, and $W^V \\in R^{D\\times D_v}$ are trainable weight matrices. The self-attention layer then calculates attention weights and the output:\n$A = \\text{softmax}(\\frac{QK^T}{\\sqrt{D_k}}).$ (7)\n$\\text{Attention}(Q, K, V) = AV.$ (8)\nTo allow multiple self-attention processes to run in parallel, the multi-head attention mechanisms is suggested. For H self-attention heads, the output of the multi-head attention is:\n$\\text{MultiHeadAttn}(Q, K, V) = \\text{Concat}(\\text{head}_1,..., \\text{head}_H)W^O,$ (9)\nwhere $W^O$ is a trainable weight matrix. This ensures the output size matches the input. The multi-head approach allows the model to focus on different parts of the sequence simultaneously. This enriches the representation and improves model performance."}, {"title": "3.2. Strategies for Applying Transformers", "content": "In practical applications, the complete Transformer architecture is not always necessary for every task. Many models modify specific Transformer components or integrate elements into existing frameworks. Broadly, Transformer applications can be categorized into three main strategies [51].\n(1) Encoder-Decoder Combination. This strategy suits sequence-to-sequence tasks, where the input sequence is processed by an encoder and then decoded into a target sequence. This approach addresses long-range dependencies by fully leveraging contextual information. While this method increases model complexity and requires more parameters, longer training times, and larger datasets, it generally yields improved performance.\n(2) Encoder Only. Used for non-sequence-to-sequence tasks, this strategy employs only the encoder to convert the input sequence into a specialized representation for subsequent processing. By simplifying the model structure and reducing parameters and training time, it provides a more efficient approach. However, the encoded representation may lack the depth of contextual information needed for complex sequence generation tasks.\n(3) Decoder Only. Typically paired with a pre-trained encoder module, this strategy is ideal for generative tasks. The decoder, utilizing self-attention, generates the target sequence based on the representations from upstream tasks. This setup captures comprehensive contextual information, though it may increase time complexity.\nIn BCI applications, the core benefit of Transformers lies in the self-attention mechanism, essential for capturing temporal correlations and performing effective feature encoding. Conse-quently, BCI applications frequently implement either the encoder or encoder-decoder strategy, with self-attention as a pivotal component for handling EEG-based tasks."}, {"title": "3.3. Practical Applications of Transformer Models in EEG Analysis", "content": "The use of Transformer-based self-attention mechanisms shows great potential for enhancing EEG modeling. These mechanisms help capture relevant information from complex, non-stationary EEG signals. We introduce EEG modeling with Transformers in three areas: temporal, spatial, and combined temporal-spatial.\n(1) Application in Temporal Dimension. In the temporal dimension, researchers have successfully leveraged CNN-Transformer models to extract valuable temporal information from EEG signals [56]. Enhancing the self-attention layer has enabled more effective capture of EEG patterns and detection of neural activity changes over time. For example, Li et al. [42] integrated CNN and Transformer architectures to adapt the self-attention mechanism to EEG-specific features. Unlike traditional approaches, their model segmented EEG signals into 30-second sequence samples and then generated time-frequency spectrograms by Short-Time Fourier Transform. These time-frequency spectrograms undergo preliminary feature extraction through multiple convolutional layers before entering an optimized Transformer module. Transformer module reduces the dimensions of the Query and Key matrices in the self-attention layer, lowering model complexity. Additionally, bias terms [19] were introduced to enhance positional information, further improving the model's ability to handle complex temporal details in EEG signals. Similarly, Qu et al. adopted 30-second signal sequences as the model's core units for sleep stage classification [43]. They implemented an encoder-decoder structure, beginning with a feature extraction layer combining convolution and pooling layers for initial preprocessing, followed by an encoder with two residual blocks to encode each signal segment. In the decoder, a Transformer-based self-attention mechanism was combined with fully connected layers to capture correlations across temporal sequence samples. The decoder processes these 30-sample subsequences by applying positional encoding to each sample before feeding it through the self-attention layer. The output then moves through two fully connected layers with normalization and dropout, culminating in sleep stage predictions via a softmax activation function. These studies demonstrate that Transformer-based self-attention mechanism can effectively extract dynamic changes in time-series EEG data, improving model efficiency and training speed through parallel computation.\n(2) Application in Spatial Dimension. Inspired by the Vision Transformer (ViT) model for image processing, researchers have developed innovative neural network models tailored for spatial feature analysis in EEG signals. For example, Guo et al. proposed a model called the Deep Convolutional and Transformer Network (DCoT) [44], focusing on the importance of each EEG channel in emotion recognition and visualizing these features. Based on the extracted differential entropy features, EEG signals were formed in a three-dimensional representation (Time \u00d7 Channel \u00d7 Frequency), which is similar to RGB representation in image. Inspired from ViT structure, DCoT was designed to analyze correlations between EEG electrodes, with each Transformer token representing a specific EEG electrode channel. The model applied positional encoding to the input tokens (i.e., EEG channels) and introduced an additional token dedicated to classification. After processing the encoded signals through the Transformer encoder module, a fully connected layer generated the final classification results. Furthermore, Zheng et al. proposed a Copula-Based Transformer model (CBT) [45], designed to learn spatial dependencies between EEG channels while optimizing classification performance. By reducing the size of the attention matrix, CBT lowered dependency on large datasets, improving computational efficiency. The CBT model excelled in an EEG-based visual discomfort assessment task, pioneering the revelation of temporal characteristics in EEG signals linked to visual discomfort. These studies underscore that Transformer-based self-attention mechanisms effectively capture the spatial dynamics of EEG signal sequences, enhancing spatial information processing and optimizing model training efficiency and accuracy.\n(3) Application in Combined Temporal and Spatial Dimensions.\nResearchers have leveraged the powerful capabilities of the Transformer self-attention mech-anism to simultaneously capture both the temporal and spatial dimensions of EEG signals, enabling the extraction of highly discriminative features. For example, Song et al. [46] developed an EEG decoding model that primarily relies on the Transformer's self-attention layers to enhance feature representations in both dimensions. In the spatial transformation component, the self-attention mechanism weighted each channel, emphasizing signals with higher relevance. Meanwhile, the temporal transformation component employed convolutional layers to encode temporal features, with channel compression and sample segmentation steps to reduce com-putational load. Finally, global average pooling and fully connected layers were combined to classify EEG signals effectively. Similarly, Du et al. proposed an EEG spatio-temporal Transformer network [47], which introduced Temporal Transformer Encoder (TTE) and Spatial Transformer Encoder (STE) modules to independently capture temporal and spatial features. In the TTE, temporal attention mechanisms calculated correlations among sampling points within each sample, extracting temporal features. Since channel correlations are often unique to individuals, the STE calculated spatial attention among channels and applies positional encoding to retain spatial location information. This allows the model to capture inter-channel relationships more accurately, improving its ability to identify individuals based on unique EEG patterns. Additionally, Si et al. [48] proposed integrating the Selective Kernel (SK) attention mechanism with the Transformer self-attention mechanism to identify and select the channels most relevant to the current task. This approach enables the model to filter out noise and irrelevant signals, simplifying the processing of complex temporal information. Once the SK attention mechanism isolated key channels, the Transformer encoder module deeply extracted temporal features from these selected channels. This approach's strength lies in its ability to focus on the most informative parts of the data while preserving sensitivity to temporal dependencies, a critical factor for managing complex tasks in BCIs. These studies demonstrate that an incorporation of both temporal and spatial information can significantly enhance the accuracy and efficiency of EEG signal analysis."}, {"title": "4. Attention Models for Multimodal Applications", "content": "To improve the accuracy of EEG signal recognition, recent studies have gradually introduced multimodal data (such as speech, images, text, etc.) to be jointly trained with EEG signals (as shown in Fig. 6). By leveraging traditional attention mechanism modeling strategies or Transformer-based multi-head self-attention mechanism modeling strategies, these approaches achieve effective fusion of signals from different modalities, enhancing the accuracy and stability of recognition. As multimodal applications in brain modeling continue to expand, the ability to efficiently utilize information from each modality becomes increasingly critical.\nAt the current stage, multimodal tasks face two core challenges: (1) effectively fusing multimodal data and (2) facilitating better interaction between different modalities. The key to the first challenge lies in assessing the importance of each modality for a given task and assigning appropriate weights to generate a unified feature representation for downstream processing. The selection of these weights significantly impacts model performance. Traditionally, this process relies on manual tuning, which is both time-consuming and suboptimal. In contrast, automated weight optimization not only improves efficiency but also enhances model performance. This is typically achieved by incorporating attention models that introduce an attention parameter matrix to dynamically adjust weight distribution. Attention modeling can be broadly categorized into traditional attention mechanisms and Transformer-based self-attention strategies. The latter emphasizes capturing complementary and shared information between modalities, ensuring that the information from one modality is reflected in another. For example, in cross-modal generation tasks using diffusion models, cross-attention mechanisms are often employed to facilitate information exchange and allocate importance between modalities."}, {"title": "4.1. Application of Attention Mechanisms in the Fusion of EEG Signals with Multimodal Data", "content": "In multimodal emotion recognition tasks, Liu et al. employed two modality fusion strategies: a basic weighted fusion method and an attention-based fusion method [62]. In the basic weighted fusion approach, the weight coefficients for each modality were manually adjusted, and the weighted sum of data from different modalities was computed to generate a fused output. The fusion process was represented as:\n$O = a_1o_1 + a_2o_2.$ (10)\nHere, $a_1$ and $a_2$ represented the weight coefficients assigned to different modalities, satisfying $a_1 + a_2 = 1$. The model's performance was thoroughly evaluated by manually testing various combinations of $a_1$ and $a_2$. In the attention-based fusion method, although a weighted summation formula was still used to obtain the fused feature $O$, in solving for $a_1$ and $a_2$, the authors first initialized an attention layer, which was then optimized through the network. The final attention weights, $a_1$ and $a_2$, were derived from the softmax function applied to the attention layer's output. This approach allows the model to adaptively adjust the attention weights, dynamically learning an optimal set of weight parameters.\nOn the other hand, Qiu et al. developed a correlation attention network that effectively integrated eye movement and EEG signals for fusion analysis. The unique aspect of the correlation attention network is that it not only computed attention weights for the two modalities but also adopted the canonical correlation analysis (CCA) method from statistics. This analysis method generated a cross-correlation coefficient matrix $C$ by calculating the cross-correlation between outputs of each recurrent unit in the bidirectional gated recurrent unit (GRU) layer. Utilizing this matrix along with the average features extracted by the bidirectional GRU, the fusion feature matrix $F$ was constructed to determine the attention weights. The computation followed a structured process. The attention score $u_{it}$ at each time step was derived as\n$u_{it} = \\tanh(W_{w1}f_{it} + W_{w2}c_{it} + b_w)$ (11)\nThe attention weights for each time point were obtained by normalizing the attention scores using the softmax function $a_{it}$, as\n$a_{it} = \\frac{\\exp(u_{it})}{\\Sigma_t \\exp(u_{it})}.$ (12)\nThe final attention-weighted output $s_i$ was computed as follows:\n$s_i = \\Sigma_t a_{it}h_{it}$ (13)\nHere, $f_{it}$ and $c_{it}$ represented elements from the fusion feature matrix $F$ and the cross-correlation coefficient matrix $C$, respectively. $W_{w1}, W_{w2}$, and $b_w$ were trainable weight parameters. This approach effectively combined the complementarity of multimodal data with statistical correlation analysis, introducing an innovative attention mechanism for multimodal tasks. This not only enhances the model's efficiency in understanding and utilizing multimodal data features but also demonstrates how traditional attention mechanisms can be extended and improved through statistical analysis methods.\nAdditionally, Zhang et al. proposed a multimodal neural network model for integrating demographic data and EEG signals [63]. This model incorporated an attention mechanism to effectively combine the two data types, aiming to uncover complex relationships between EEG signals and demographic factors. Such integration is particularly valuable for applications like depression detection, where demographic information can provide crucial contextual insights. The model first used a one-dimensional CNN to process EEG signals, resulting in a feature matrix $Z \\in R^{Cout\\times m}$, while demographic data were encoded as a feature vector $S \\in R^d$. The two data types were fused through an attention mechanism, with the calculation as follows:\n$A = \\tanh((W_{fe}Z + b_{fe}) \\oplus (W_{de}S + b_{de})).$ (14)\nHere, $W_{fe}$ and $b_{fe}$ were trainable weights and biases related to EEG signal features. $W_{de}$ and $b_{de}$ were weights and biases related to demographic features. The symbol $\\oplus$ represented the fusion operation between the two modalities. This approach enhances the integration of demographic data and EEG signals by leveraging attention mechanisms, helping to mitigate the impact of individual differences on model performance. By dynamically adjusting the influence of demographic information, the model achieves a more effective and personalized feature representation, improving the accuracy of multimodal analysis.\nFurthermore, Choi et al. proposed a multimodal attention network to explore the fusion of facial video and EEG signals [64]. Unlike traditional methods that rely on a single fusion layer, this network utilized multimodal fusion layers incorporating bilinear and trilinear pooling to extract and integrate deep features. This approach not only enhances the integration of features from both modalities but also improves model performance by dynamically allocating attention weights, enabling more effective cross-modal information exchange.\nRecently, Transformer-based multi-head self-attention mechanisms have become a prevalent approach in multimodal data fusion, enabling efficient processing and integration of features from diverse modalities. In the model framework, each token received by the Transformer encoder corresponds to a specific modality, allowing the model to effectively capture and integrate information at the feature level. By leveraging complex inter-modal interactions, the self-attention mechanism uncovers intrinsic relationships and complementary information between modalities, dynamically refining feature representations. A well-known example is the MMASleepNet model [65], which not only incorporated the concept of the Squeeze-and-Excitation (SE) module from SENet [66] but also integrated the advantages of the Transformer encoder module. This combination enhances the model's ability to effectively handle multimodal data by dynamically adjusting feature representations across different modalities, thereby improving overall performance."}, {"title": "4.2. Application of Attention Mechanisms in Information Interaction Between EEG Signals and Multimodal Data", "content": "Compared to self-attention, cross-attention extends its functionality by integrating informa-tion from multiple modalities, enabling more precise modeling of inter-modal associations [67]. When the query (Q) matrix and the key (K) and value (V) matrices originate from different modalities, the computation shifts from capturing intra-modal correlations to establishing inter-modal relationships, effectively transforming self-attention into cross-attention [68]. Also referred to as cross-modal attention, this mechanism introduces additional input sequences from distinct modalities, enhancing information fusion and improving the overall representation of multimodal data [69][70].\nIn EEG-related research, visual reconstruction from EEG signals is a particularly challenging yet rapidly evolving field. The objective is to decode the information embedded in EEG signals and use it to reconstruct corresponding visual stimuli. This process requires capturing the intricate relationships between EEG and visual modalities, often achieved through cross-attention mechanisms. By computing the relevance between EEG segments and visual data, EEG signals serve as conditioning inputs to guide the generation of visual stimuli. A well-known example is the DreamDiffusion model [71], which employed an EEG encoder trained on large-scale EEG data using a masking strategy to enhance feature extraction. The extracted EEG features then conditioned the Stable Diffusion model to generate images. Within Stable Diffusion, the cross-attention mechanism computes correlations between EEG-derived and image-derived features, enabling coherent information exchange between the two modalities. The mathematical formulation of cross-attention can be expressed as follows:\n$Q = W^{(i)}_Q \\cdot Y^{(i)}_t(z_t), K = W^{(i)}_K \\cdot \\tau^{(i)}_\\theta(y), V = W^{(i)}_V \\cdot \\tau^{(i)}_\\theta(y).$ (15)\nHere, $Y^{(i)}_t(z_t) \\in R^{N\\times d_e}$ is the output of the noise prediction model Unet, and $\\tau^{(i)}_\\theta(y) \\in R^{M\\times d_r}$ is the encoded representation obtained by projecting the EEG features output from the EEG encoder through an additional layer. $W^{(i)}_Q \\in R^{d\\times d_e}, W^{(i)}_K \\in R^{d\\times d_r}$, and, $W^{(i)}_V \\in R^{d\\times d_r}$ are learnable parameters. In this setup, the Q matrix is derived from image data, while the K and V matrices are derived from EEG signals. By substituting these Q, K, and V matrices into the self-attention computation formula, the attention score matrix is obtained, leading to the output of the cross-attention mechanism. This process facilitates effective information interaction between EEG and image modalities, enabling more precise multimodal feature integration."}, {"title": "5. Conclusion and Future Works", "content": "This paper explores the integration of brain-computer interface (BCI) technology with attention models and their applications, highlighting recent advancements in the field. Traditional attention mechanisms, as a classical approach, are widely applied in EEG signal analysis. Transformer-based methods open up new avenues for processing and"}]}