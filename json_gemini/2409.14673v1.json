{"title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science", "authors": ["Taihang Wang", "Xiaoman Xu", "Yimin Wang", "Ye Jiang"], "abstract": "Real-world applications of large language models (LLMs) in computational social science (CSS) tasks primarily depend on the effectiveness of instruction tuning (IT) or in-context learning (ICL). While IT has shown highly effective at fine-tuning LLMs for various tasks, ICL offers a rapid alternative for task adaptation by learning from examples without explicit gradient updates. In this paper, we evaluate the classification performance of LLMs using IT versus ICL in few-shot CSS tasks. The experimental results indicate that ICL consistently outperforms IT in most CSS tasks. Additionally, we investigate the relationship between the increasing number of training samples and LLM performance. Our findings show that simply increasing the number of samples without considering their quality does not consistently enhance the performance of LLMs with either ICL or IT and can sometimes even result in a performance decline. Finally, we compare three prompting strategies, demonstrating that ICL is more effective than zero-shot and Chain-of-Thought (CoT). Our research highlights the significant advantages of ICL in handling CSS tasks in few-shot settings and emphasizes the importance of optimizing sample quality and prompting strategies to improve LLM classification performance. The code will be made available.", "sections": [{"title": "1 Introduction", "content": "Instruction tuning (IT) of large language models (LLMs) has shown exceptional capability in understanding language across various tasks (Ouyang et al., 2022). However, the large number parameters of LLMs makes it challenging to transfer the pre-trained knowledge to downstream tasks (Naveed et al., 2023; Xu et al., 2024). Alternatively, in-context learning (ICL) enables LLMs to perform downstream tasks by conditioning on task-specific prompts, thus eliminating the need for explicit gradient updates (Dong et al., 2022; Wang et al., 2024b; Jiang, 2023). Recent successful deployment of LLMs in practical applications largely hinges on the effectiveness of the ICL and the IT.\nPrevious studies have extensively assessed the zero-shot capabilities of LLMs in computational social science (CSS) tasks, including hate speech detection (Roy et al., 2023) and rumour stance detection (Yang et al., 2024b). However, CSS is a dynamic research area that involves detailed linguistic analysis and deep semantic comprehension. Direct zero-shot prompting LLMs to CSS tasks may even underperform compared to fully fine-tuned, task-specific smaller models like BERT (Juan Jos\u00e9 Bucher and Martini, 2024). Meanwhile, studies on ICL and IT typically occur independently, with direct comparisons between these approaches often overlooked.\nTo address the above issues, this paper raises the following research questions (RQ):\n\u2022 RQ 1: What are the performance differences between LLMs with ICL and IT in few-shot CSS tasks?\n\u2022 RQ 2: How do varying numbers of sample influence the performance of LLMs with ICL and IT?\n\u2022 RQ 3: How different prompting strategies affect the proficiency of LLMs in CSS tasks?\nTo answer the above questions, we extensively investigate six open-source LLMs in a total of five publicly accessible social media datasets within n-shot settings, where n \u2208 {1, 8, 16, 32}.\nInitially, we compare the few-shot classification performance of LLMs with ICL and IT separately. We then assess how performance varies with an increase in the number of samples. Lastly, we"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Instruction tuning for large language models", "content": "IT (Jiang et al., 2023; Wang et al., 2024a; Parthasarathy et al., 2024) is an effective technique that updates LLM parameters in a supervised fashion by modifying the context of inputs to follow specific instructions. Previous studies have extensively discussed the advancements of IT in LLMs. For example, Zhang et al. (2023); Qin et al. (2024) provide a comprehensive overview of the IT in LLMs, explaining the process of fine-tuning LLMs with instruction pairs and analyzing key factors that impact IT results. Ouyang et al. (2022) thoroughly examines data selection strategies for IT in LLMs, emphasizing the critical role of data quality over quantity and offering methods for selecting the most effective datasets to improve LLMs\u2019 instruction-following abilities. Hu et al. (2024) proposes a Sequential Instruction Tuning (SIT) method that systematically incorporates continuous tasks into the training process to enhance the model's capability to follow long, multi-step instructions. However, the aforementioned studies primarily assess IT in data-rich or zero-shot settings, leaving the few-shot performance of IT relatively underexplored."}, {"title": "2.2 Comparison between instruction tuning and in-context learning", "content": "ICL enables LLMs to quickly adapt to tasks by learning from samples without updating the model's weights (Yang et al., 2023b; Brown et al., 2020). Dong et al. (2022) comprehensively summarizes the progress and challenges of ICL, discussing related techniques including prompt design and training strategies, and explores effective application scenarios of ICL in enhancing the inferential capabilities of LLMs. Coda-Forno et al. (2023) further explores how LLMs enhance their capabilities through the ICL paradigm by adjusting learning strategies and prior knowledge, through regression and multi-armed bandit tasks.\nRecent studies have also focused on exploring the connections between IT and ICL. For example, Mosbach et al. (2023) evaluates the generalization capabilities of Pattern-based fine-tuning (PBFT) and ICT for out-of-domain (OOD) tasks under the same experimental settings in a few-shot context. They find that PBFT achieves better generalization. Duan et al. (2023) investigates how ICL and IT modify the hidden layer states of LLMs to achieve task adaptability in LLMs, finding that ICL is implicit IT. Our work differs from previous research in that we directly compare the classification performance between the ICL and IT in various CSS tasks."}, {"title": "2.3 Large language models in computational social science", "content": "LLMs have demonstrated exceptional capabilities in CSS (M\u00f8ller and Aiello, 2024; Jiang, 2023; Xu et al., 2024; Jiang et al., 2023). For example, Ziems et al. (2024) has outlined a roadmap for using LLMs as tools for CSS, recommending best practices for prompting and conducting an extensive evaluation of the zero-shot performance of thirteen language models across twenty-four representative CSS benchmark tasks. Additionally, Mu et al. (2024) has assessed the zero-shot performance of two LLMs under six CSS tasks, while also researching the effects of various prompting strategies. However, the emerging CSS topics demand that LLMs quickly adapt to limited annotated data (Jiang et al., 2024), therefore it is crucial to evaluate their few-shot performance in CSS tasks. Our work aims to explore the performance differences between ICL and IT in CSS tasks within few-shot settings, as well as how to enhance the"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Instruction tuning for CSS", "content": "Following the IT strategy outlined by (Duan et al., 2023), we first create a task-specific Instruction (e.g., \"Analyze the content and determine if it includes label\", where label represents task-specific labels) to define the objective of each task. We then incorporate a set of Constraints (e.g., \"Respond only with label or not label, without offering additional context or explanation\") to guide the LLMs\u2019 responses. The detailed workflow of the IT process is presented in Figure 1 (a) and Appendix A.2.\nConsidering the computational efficiency and challenges of fine-tuning LLMs, we employ LoRA (Hu et al., 2021) for instruction-tuning across all models. Specifically, we set the dropout probability at 0.1 and the learning rate at 1e-4. As recommended by Duan et al. (2023), the scaling factor is set to 32, with a rank of 8. The models are fine-tuned over three epochs using Brain Floating Point 16 (bf16) precision."}, {"title": "3.2 In-context learning for CSS", "content": "In accordance with the in-context learning prompts described by Jiang and Wang (2024), we create input prompts consisting of Instruction, Constraints, Samples (e.g., \"Tweet: How to not miss"}, {"title": "3.3 Comparing in different prompting strategies", "content": "In the zero-shot setting, we compose the prompt by combining Instruction, Constraints, and Text. For ICL, the detailed workflow is presented in Section 3.2.\nInspired by Dogan et al. (2024), we utilize the ChatGPT-4 model\u00b9 to automatically generate CoT descriptions for each sample. For example, we input the tweet along with prompts in Bragging (e.g., \u201cAnalyze the content and determine if it includes a bragging statement by using the CoT method. Tweet: For a minute I was tired of being the bigger man, until I realized that's just who I am\u201d). These"}, {"title": "4 Experimental setups", "content": ""}, {"title": "4.1 Data", "content": "To assess the classification performance of LLMs, five publicly available datasets are selected, encompassing a broad spectrum of computational social science topics. The statistics of these datasets are presented in Table 1.\nBragging (Jin et al., 2022) : This dataset is designed to facilitate a comprehensive semantic analysis of tweets to ascertain whether they contain narratives of bragging, specifically identifying the subject of the author's boast.\nComplaint (Preo\u0163iuc-Pietro et al., 2019): This task aims to identify whether tweets from social media contain complaints, where the complaint content expresses a mismatch between reality and expectations in a specific context.\nSarcasm (Farha et al., 2022): This task aims to conduct semantic analysis on texts to determine whether they contain sarcasm.\nRumour Stance (Derczynski et al., 2017): This task aims to perform semantic analysis on tweets (rumours) in social media to assess the stance classification of the rumours.\nGossip Cop (Shu et al., 2020): This task aims to perform semantic analysis on news articles in entertainment media to determine the authenticity of the news articles.\nFor each benchmark task, we utilize stratified random sampling to divide the dataset into 70% for training, 10% for validation, and 20% for testing. The 10% validation set is used for hyperparameter tuning during the instruction tuning, and the"}, {"title": "4.2 Baselines", "content": "To ensure a fair comparison of LLMs in CSS tasks, we utilize Huggingface\u00b2 to select six different open-source LLMs, with model sizes ranging from 7B to 9B, namely Qwen2-7B-Instruct (Qwen2) (Yang et al., 2024a), Baichuan2-7B-Chat (Baichuan2) (Yang et al., 2023a), GLM4-9B-chat (GLM4) (GLM et al., 2024), Meta-llama3-8B-instruct (LLama3) (Meta, 2024), Gemma-2-9B-it (Gemma2) (Team et al., 2024), and Phi-3-Small-128K-Instruct (Phi-3) (Abdin et al., 2024)."}, {"title": "5 Results", "content": "The overall experimental results are presented in Table 2 and Table 3. For each n-shot setting, we evaluate the LLMs by computing the average accuracy (Acc) and macro-F1 (F1) scores across five random seeds\u00b3.\nComparing between IT and ICL: We first calculate the average accuracy and F1 scores across"}, {"title": "6 Analysis", "content": "The experimental results underscore the proficiency of LLMs in CSS tasks that require comprehension of complex real-world contexts. Next, we will contextualize these findings within the framework of our three research questions:\n(RQ1) What are the performance differences between LLMs with ICL and IT in few-shot CSS tasks?\nThe experimental results reveal that LLMs with ICL generally outperform those with IT in few-shot CSS tasks. ICL exhibits strong adaptability, likely due to the extensive knowledge acquired during the pre-training phase. This allows the model to comprehend and swiftly adapt to complex tasks by leveraging pre-trained knowledge. While IT also enhances LLM capabilities through instructions, its performance is comparatively more sensitive than that of ICL, as illustrated in Figure 2.\nAdditionally, ICL enables the model to directly leverage the input-label pairs provided in the samples to guide inference without requiring gradient updates. For LLMs with IT, insufficient training samples can lead to overfitting, instability, and may even impair the inferential capacity of the models. For example, the average accuracies of GLM4 and Llama3 are 56.0% and 53.9% in the 1-shot setting. However, both models achieve higher average accuracies of 58.1% and 58.8% in the zero-shot settings, respectively.\n(RQ2) How do varying numbers of sample influence the performance of LLMs with ICL and IT?\nOur experimental results suggest that merely increasing the number of training samples does not consistently improve the performance of LLMs with either ICL or IT, and in some cases, it even leads to a decline.\nGiven the characteristics of few-shot settings, we speculate that the contextual diversity of samples is more crucial than their quantity, regardless of whether LLMs use IT or ICL. If the additional sam-"}, {"title": "7 Conclusion", "content": "In this paper, we first evaluate the performance of LLMs with IT and ICL in few-shot CSS tasks. We also investigate whether increasing the number of training samples affects LLM performance. Lastly, we compare different prompting strategies and analyze their efficiency in few-shot settings.\nIn our experiments, we evaluate six open-source LLMs on five publicly available CSS datasets. Our results indicate that: 1) LLMs with ICL generally outperform those with IT in tackling complex CSS tasks in few-shot settings; 2) merely increasing the number of samples without considering their quality does not consistently improve the perfor-"}, {"title": "8 Limitations", "content": "This study acknowledges several limitations, including: 1) Due to computational resource constraints and the context length limitations of LLMs, larger n-shot settings remain underexplored. 2) Our experiments primarily compare LLMs with parameters ranging between 7B and 9B, due to hardware restrictions. 3) The generation of CoT descriptions relies mainly on GPT-4, without manual assessment, which may result in inconsistencies in CoT quality."}, {"title": "9 Ethic statement", "content": "This work has received ethical approval from the Ethics Committee of our university and adheres to the research policies of Twitter. All datasets were obtained via links provided in the respective research papers or directly from the authors upon request. Additionally, we confirm that the data was fully anonymized prior to being used for model inference with the LLMs. Due to the time-intensive and challenging nature of generating the CoT strategy, we solely rely on ChatGPT-4 to automatically generate the CoT descriptions, without manually crafting any CoT strategies."}]}