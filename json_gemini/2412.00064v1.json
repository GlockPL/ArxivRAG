{"title": "DiffGuard: Text-Based Safety Checker for Diffusion Models", "authors": ["Massine El Khader", "Elias Al Bouzidi", "Abdellah Oumida", "Mohammed Sbaihi", "Eliott Binard", "Jean-Philippe Poli", "Wassila Ouerdane", "Boussad Addad", "Katarzyna Kapusta"], "abstract": "Recent advances in Diffusion Models have enabled the generation of images from text, with powerful closed-source models like DALL-E and Midjourney leading the way. However, open-source alternatives, such as StabilityAI's Stable Diffusion, offer comparable capabilities. These open-source models, hosted on Hugging Face, come equipped with ethical filter protections designed to prevent the generation of explicit images. This paper reveals first their limitations and then presents a novel text-based safety filter that outperforms existing solutions. Our research is driven by the critical need to address the misuse of AI-generated content, especially in the context of information warfare. Diff-Guard enhances filtering efficacy, achieving a performance that surpasses the best existing filters by over 14%.\nIndex Terms-Safety filter, Large Language Models, Text-to-Image, Explicit content filtering, Responsible AI, Diffusion models.", "sections": [{"title": "INTRODUCTION", "content": "With the rise of Generative AI (Gen AI), sophisticated models creating realistic images from text descriptions have become increasingly popular. They have a wide range of applications, from entertainment to professional content creation. However, the ability to generate any false images also brings the risk of creating harmful content. For instance, in a military conflict, adversaries can leverage manipulated media to spread misinformation, influence public perception, and instill fear. Filters are crucial to ensuring that the generated content adheres to safety and ethical standards, preventing the misuse of these powerful tools.\nIn this paper, we address the novel and critical issue of security of diffusion models, a class of models first introduced by Ho et al. in 2020 [1]. Diffusion models have gained significant traction for text-to-image generation and image editing applications. Similar to large language models (LLMs), diffusion models remain a focal point of contemporary research, with ongoing efforts to enhance generation quality, mitigate social biases, and extend capabilities to multi-frame generation, exemplified by recent developments such as OpenAI's SORA [2]. These models are typically trained on extensive datasets, including CelebA, ImageNet, and LAION. However, these datasets often contain extreme content, such as nudity (both adult and child), violence, and gore, raising substantial security concerns.\nThe proliferation of open-source models, commonly hosted on platforms like Hugging Face's hub and accessible via the diffusers library, underscores the necessity for robust safety measures. Currently, most diffusion models use a safety filter that is integrated within the diffusers library. However, recent studies, which we will examine later in this paper, have demonstrated the limitations of this filter, particularly in filtering out violent and disturbing content, see Fig. 1 for examples.\nIn response to these challenges, we propose DIFFGUARD, an efficient text-based NSFW filter designed for seamless integration with any text-to-image and text-to-video diffusion model. Our model, DIFFGUARD, outperforms existing solutions by providing more accurate and reliable filtering of NSFW content. Our work highlights the importance of developing advanced safeguards in AI technologies to ensure the responsible use of Gen AI. This paper details the development of DiffGuard, aiming to enhance the security framework of diffusion models. DiffGuard outperformed four other advanced solutions as it delivers 8% higher precision and 14% greater recall. Section II outlines related works, while Section III details our contributions. Later, in Section IV we assess our model, against various datasets and benchmark it against competing solutions. We conclude with an ablation study to demonstrate the impact of preprocessing on our model's performances in Section V."}, {"title": "RELATED WORKS", "content": "Diffusion models, especially Denoising Diffusion Probabilistic Models (DDPMs), represent a significant advancement in the field of generative modeling since their introduction by Ho et al. in 2020 [1]. DDPMs generate samples by reversing a process that progressively adds Gaussian noise to training data. This involves training a neural network to denoise images step-by-step, often conditioned on text using Contrastive Language-Image Pretraining (CLIP [3]), resulting in high-fidelity and semantically coherent text-to-image generation. Since then, various enhancements have emerged. Vision Transformers (ViT), introduced by Dosovitskiy et al. (2020) [4], replace the traditional U-Net architecture in some models, offering a global context understanding and improving image quality [5]. Attention-based Diffusion Models (ADM) integrate attention mechanisms directly into the diffusion process, enhancing the generation of detailed and coherent images [6].\nThe versatility of diffusion models extends to multi-frame generation. OpenAI's SORA model, for example, generates sequences of frames with temporal coherence, crucial for video and animation tasks [2].", "subsections": [{"title": "Open and Closed Source Models", "content": "The landscape of diffusion models includes both closed-source and open-source implementations. Closed-source models like OpenAI's DALL-E offer high performance and robust support but limit accessibility and customization. In contrast, open-source models, such as Stable Diffusion available on Hugging Face, democratize access to advanced generative technologies, fostering community-driven development and transparency. However, they may face challenges in quality control and consistency due to their reliance on community contributions. Refer to Fig. 2 for a comparison of closed and open-source models."}, {"title": "Existing filters", "content": "To the best of our knowledge, there is no publicly available documentation detailing the filtering mechanisms of popular closed-source models such as Midjourney and DALL-E. It is suggested that Midjourney employs AI moderators to block specific keywords (e.g., \u201cblood\u201d, \u201cbreast\"), but the precise architecture and modalities of these filters remain unclear. Despite these safety measures, Midjourney emphasizes its community guidelines, which explicitly prohibit the generation of content involving nudity, gore, and other disturbing material.\nIn contrast, several open-source safety checkers and NSFW detection techniques are available, and some of them could be integrated into diffusion models to ensure content safety.\n1) Stable Diffusion Safety Checker: To prevent the generation of explicit (NSFW) content, diffusion models incorporate safety checkers. For closed source models, the specific architecture and mechanisms employed remain undisclosed. However, open source models, such as those accessible via Hugging Face's diffusers library, utilize an openly available and accessible safety checker. The main components of this filter are illustrated in Fig. 3.\nHere is how the safety filter operates:\nUser prompt: The user provides a prompt, such as \"a photograph of an astronaut riding a horse\". The Stable Diffusion model generates an image based on this prompt.\nImage encoding: Before the image is shown to the user, it is processed through CLIP's image encoder to obtain an embedding a high-dimensional vector representation of the input."}]}, {"title": "Cosine similarity calculation", "content": "The cosine similarity between this embedding and 17 different fixed embedding vectors, each representing a pre-defined sensitive concept, is computed."}, {"title": "Threshold comparison", "content": "Each concept has a pre-specified similarity threshold. If the cosine similarity between the image and any of the concepts exceeds the respective threshold, the image is discarded."}, {"title": "Adversarial Attacks on Safety Checkers", "content": "In addition to simple techniques like prompt dilution, researchers have uncovered more sophisticated methods to bypass even the most robust filters, such as DALL-E's closed-box safety checker :\n1) SneakyPrompt: Yang et al. [10] introduce SneakyPrompt, the first automated attack framework designed to jailbreak text-to-image generative models, enabling them to produce NSFW images despite the presence of safety filters. When a prompt is blocked by a safety filter, SneakyPrompt repeatedly queries the text-to-image model and strategically perturbs tokens in the prompt based on the query results to circumvent the filter. Specifically, SneakyPrompt employs reinforcement learning to guide the perturbation of tokens, effectively evading the safety measures. We contacted the authors and they have provided us with a dataset in order to evaluate our model.\n2) MMA-Diffusion: MultiModal Attack on Diffusion Models: Yang et al. (2024) introduce MMA-Diffusion, a framework that poses a significant and realistic threat to the security of text-to-image (T2I) models by effectively circumventing current defensive measures in both open-source models and commercial online services [11]. Unlike previous approaches, MMA-Diffusion leverages both textual and visual modalities to bypass safeguards such as prompt filters and post-hoc safety checkers, thereby exposing vulnerabilities in existing defense mechanisms. We will evaluate our model against the text modality of MMA-Diffusion using a dataset obtained from the authors. Consequently, we have three datasets for evaluation: our own dataset, the dataset provided by the SneakyPrompt team, and this new dataset."}, {"title": "CONTRIBUTIONS", "content": "In this section, we develop a robust NSFW filter that can be seamlessly integrated into any diffusion model, demonstrating the novelty of our work through several key contributions :\nOur filter significantly outperforms existing solutions on established benchmarks.\nWe present DIFFGUARD in three different sizes, with the smallest model comprising only 67 million parameters.\nWe introduce the NSFW-Safe-Dataset, a meticulously curated prompt dataset designed for fine-tuning and benchmarking our models."}, {"title": "Types of NSFW filters", "content": "Yang et al. [10] identify three potential types of safety checkers, as illustrated in Fig. 4."}, {"title": "Justification for Choosing a Text-Based Architecture", "content": "In this paper, we have chosen to adopt a text-based architecture for the following reasons:\nLeveraging Advances in Language Models: We aim to capitalize on recent advancements in language models, particularly their zero-shot capabilities. As demonstrated by Wang et al., large language models excel as zero-shot text classifiers [12].\nInference Efficiency: Based on preliminary comparisons, we assume that processing an image is more costly than processing a prompt (typically a short text of limited sequence length) in terms of inference time, memory requirements, and computational complexity.\nScalability to Multi-Frame Diffusion Models: By focusing solely on the prompt, our filter can be easily extended to multi-frame diffusion models (video generators). We will elaborate on this point in the dataset section.\nCompatibility with Prompt-Based Models: Our filter is designed to work with any prompt-based model, as it only depends on prompts."}, {"title": "Data Collection", "content": "Regardless of the specific text-based model architecture selected, it is crucial to have a substantial dataset of prompts aimed at generating explicit content, including sexual material, nudity, and gore, as well as safe content. However, acquiring such datasets is challenging due to their scarcity on public platforms. To address this, we created our own dataset. In this section, we detail the sources used for data collection."}, {"title": "Zero-shot classifier", "content": "The zero-shot classification process involves transforming the classification problem into a Natural Language Inference (NLI) problem [13]. The model evaluates whether each hypothesis (topic) is entailed by the paragraph (prompt). The topics for which the hypotheses are entailed are the predicted categories for the paragraph. This approach allows the model to perform classification without needing specific training data for each topic, leveraging its understanding of natural language inference instead. When the model is given a premise and a hypothesis, it determines the relationship (entailment, contradiction, or neutral) through a sequence of computational steps.\nInitially, both the premise and hypothesis are tokenized and converted into dense vector embeddings. These tokens are concatenated into a single sequence, with special tokens added to demarcate the premise and hypothesis. This combined sequence is then processed by multiple layers of transformer encoders, utilizing self-attention mechanisms to build contextualized representations. The output corresponding to the '[CLS]' token, which aggregates information from the entire sequence, is passed through dense layers followed by a softmax layer to produce probabilities for each class. The class with the highest probability is selected as the final prediction, indicating whether the hypothesis is entailed by, contradicts, or is neutral with respect to the premise. This process leverages the model's deep learning architecture and extensive pre-training on natural language inference tasks to perform zero-shot classification effectively."}, {"title": "Fine-tuning", "content": "Fine-tuning large language models is a crucial step in adapting pre-trained models to specific tasks like NSFW content detection. In our study, we employed Hugging Face's Trainer API10 for fine-tuning our selected models: DistilBERT, DistilRoBERTa, and RoBERTa. The Trainer API provides a streamlined interface for training and fine-tuning transformer models. All training processes were conducted on the Data Centre d'Enseignement DCE [15].\nFor our experiments, we used standard fine-tuning where all model weights are updated during training. This approach allows the model to fully adapt to the specific characteristics of our dataset, optimizing its performance for the classification of prompts into safe and NSFW categories.\n1) Fine-Tuning Process: The fine-tuning process involved training each model on our dataset for a specified number of epochs. We used the following configurations:\ndistilBERT: Fine-tuned for 3 epochs, taking approximately 40 minutes.\ndistilroBERTa: Fine-tuned for 3 epochs, taking approximately 45 minutes.\nroBERTa: Fine-tuned for 5 epochs, taking approximately 1 hour and 27 minutes.\nWe consider the weighted cross-entropy loss\n$Loss(y, \\hat{y}) = \\sum_{i=1}^{N}(W_{safe} (1 - Y_i) \\log(1 - \\hat{y}_i)+W_{nsfw} Y_i \\log(\\hat{y}_i))$,\nwhere\n$W_{safe} = 1 - \\frac{N_{safe}}{N}$ and $W_{nsfw} = 1 - \\frac{N_{nsfw}}{N}$."}, {"title": "EVALUATION", "content": "To demonstrate the robustness of our model against known adversarial attacks, we will evaluate its performance not only on the collected test set but also against the adversarial attacks identified in the State-of-the-Art section. Whenever feasible, we will compare our results with existing filters. Our models will be identified as follows:"}, {"title": "Abbreviations", "content": "For each model, we will add the suffix pp to indicate whether the training and evaluation datasets were conducted with preprocessing which includes normalizing case, removing numbers, punctuation, brackets, URLs, HTML tags, and Twitter mentions. (npp means that both were non preprocessed). The pp suffix can take two arguments: t (for training) and e (for evaluation). For example, the model DiffGuard-small-pp-t-e signifies that it was trained on a preprocessed dataset and evaluated on a preprocessed dataset. Conversely, a model labeled DiffGuard-small-pp-t indicates that the model was trained on a preprocessed dataset, but the evaluation dataset did not undergo preprocessing. This distinction will also facilitate an ablation study of the preprocessing phase."}, {"title": "Evaluation Metrics", "content": "We evaluate our models using the F1 score metric, which effectively captures both precision and recall, thus addressing potential imbalances in label distribution. In addition to the F1 score, we consider false positive rates (FPR) and false negative rates (FNR) as critical metrics. Therefore, our comparison tables will include F1 score, accuracy, FPR, and FNR.\nNote: Although we use the F\u2081 score for evaluation, our training objective is to optimize the F\u03b2 score with \u03b2 = 1.6. This setting assigns weights of 0.3 and 0.7 to false positives and false negatives, respectively, to better align with our specific requirements for model performance."}, {"title": "Test dataset", "content": "From the dataset we collected, a subset of 40,241 prompts was reserved for testing. This test set comprises 60% safe prompts and 40% NSFW prompts. The performance metrics of the three models on this test set are illustrated in Figure 5."}, {"title": "Against Sneaky Prompt", "content": "In this section, we evaluate our models on a dataset based on the prompts used in the paper by Yang et al. (2023) [10]. We have augmented the dataset with safe examples generated by LLama 3. After manually correcting the labels and removing duplicates, the dataset comprises 600 prompts, with 200 labeled as NSFW and the remaining 400 as safe. We assessed the models' performance on this dataset, both with and without preprocessing during training and evaluation. The results are summarized in Table III."}, {"title": "Against MMA-Diffusion Text-Modality", "content": "Yang et al. (2024) introduce multi-modality attacks in their work on MMA-Diffusion [11]. Given that our filter is text-based, we limit our evaluation to the text-modality adversarial attacks described in their study."}, {"title": "Evaluation of zeroshot classifiers", "content": "In this section, we will present a comparative analysis of the zero-shot classifier's performance with two distinct thresholds of similarity. The first threshold, 0.8, will be applied to critical applications, while the second threshold, 0.95, will be used for normal applications."}, {"title": "ABLATION STUDY", "content": "Recall that our pipeline employs pre-processing at both the training and evaluation stages. We examine three scenarios in our ablation study:"}, {"title": "CONCLUSION", "content": "In this paper, we developed DIFFGUARD, a robust NSFW filter designed to seamlessly integrate with prompt-based models, particularly text-to-image diffusion models. Our evaluation demonstrated that DIFFGUARD consistently outperforms existing filters, including the Multi-headed SC filter proposed by Qu et al. This exceptional performance underscores the efficacy of our approach in addressing the generation of unsafe and harmful content by AI models.\nThe significance of DIFFGUARD extends beyond its immediate application in diffusion models. In the context of defense and information warfare, the ability to filter and manage the generation of inappropriate content is crucial. Malicious actors can exploit Al-generated media to spread misinformation, and instill fear. Our model not only mitigates these risks but also enhances the overall security framework of AI applications. By ensuring the safe deployment of generative models, we contribute to the defense against digital threats and the protection of information integrity.\nOne of the most promising aspects of DIFFGUARD is its adaptability. While our primary demonstration involved a diffusion model from StabilityAI, the filter has shown potential to work effectively across various prompt-based models. This flexibility indicates a broad applicability, making DIFFGUARD a versatile tool in the arsenal against unsafe AI-generated content.\nLooking ahead, we are eager to develop advanced features that can dynamically adapt to new types of unsafe content as they emerge. This would involve enhancing DIFFGUARD to recognize and respond to evolving threats in real-time, ensuring that our filter remains effective as AI-generated media continues to evolve.\nIn conclusion, DIFFGUARD represents a significant advancement in Al safety and security. Its robust performance against existing filters highlights its importance in the ongoing effort to safeguard digital environments."}]}