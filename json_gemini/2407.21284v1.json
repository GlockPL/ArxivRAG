{"title": "Robust Box Prompt based SAM for Medical Image Segmentation", "authors": ["Yuhao Huang", "Xin Yang", "Han Zhou", "Yan Cao", "Haoran Dou", "Fajin Dong", "Dong Ni"], "abstract": "The Segment Anything Model (SAM) can achieve satisfactory segmentation performance under high-quality box prompts. However, SAM's robustness is compromised by the decline in box quality, limiting its practicality in clinical reality. In this study, we propose a novel Robust Box prompt based SAM (RoBox-SAM) to ensure SAM's segmentation performance under prompts with different qualities. Our contribution is three-fold. First, we propose a prompt refinement module to implicitly perceive the potential targets, and output the offsets to directly transform the low-quality box prompt into a high-quality one. We then provide an online iterative strategy for further prompt refinement. Second, we introduce a prompt enhancement module to automatically generate point prompts to assist the box-promptable segmentation effectively. Last, we build a self-information extractor to encode the prior information from the input image. These features can optimize the image embeddings and attention calculation, thus, the robustness of SAM can be further enhanced. Extensive experiments on the large medical segmentation dataset including 99,299 images, 5 modalities, and 25 organs/targets validated the efficacy of our proposed RoBox-SAM.", "sections": [{"title": "Introduction", "content": "The Segment Anything Model (SAM) has marked a significant breakthrough in image segmentation, due to its powerful zero-shot performance [13]. SAM was trained on a large annotated dataset with 11M images and 1B masks based on the Vision in Transformer (ViT). It supports different types of prompts, including point, box, text, and mask. Recent studies have widely explored the potential of applying SAM to the medical imaging domain [9,21], which demonstrates that SAM can outperform the task-specific state-of-the-art conventional deep models after fine-tunning with additional medical images [19]."}, {"title": "Methodology", "content": "Fig. 2 shows the schematic view of our proposed Robox-SAM. In stage 1, to eliminate the effects of low-quality prompts, the proposed PRM first predicts the offsets to transfer the input box prompt into a high-quality one. In stage 2, we adopt the prompt correction knowledge to output potential points via the proposed PEM. In the last stage, we leverage the prior knowledge encoded by SIE to optimize the image embeddings, and update the attention calculation in the mask decoder. This can further optimize the feature activations under suboptimal box prompts, thus, improving the robustness of SAM.\nOriginal SAM. SAM mainly consists of three parts: image encoder, prompt encoder, and mask decoder [13]. The image encoder is used to extract the image embedding, while the prompts are encoded using different methods (e.g., point and box: positional encoding [25], text: CLIP [24]). Before being fed to the mask decoder, the image embedding should add the positional encoding, while the prompt tokens should concatenate with two output tokens, i.e., IoU token and mask token, to form the embedding (e) and tokens (t), respectively. The mask decoder mainly contains two Transformer layers, and each layer contains four modules: 1) self-attention of tokens, 2) token-to-image cross-attention, 3) Multi-Layer Perception (MLP) on tokens, and 4) image-to-token cross-attention. For example, the token-to-image cross-attention can be formulated as $\\text{C'Att} (t, e)=\\text{softmax}(Q(t), K(e)^T/\\sqrt{d_k})\\cdot V(e)$, where d(k) represents the dimension of the keys. Finally, the mask token is separated from tokens and operated with the final up-scaled image embedding to predict multiple outputs. Besides, the IoU token predicts the IoU score for each mask.\nPRM for Target Perception and Box Prompt Correction. Inaccurate box prompts may cause the SAM's mask decoder to activate imprecise regions, and the cross-attention between image features and tokens may further accu-"}, {"title": "Experimental Results", "content": "Materials and Implementations. We validated the RoBox-SAM on a large medical segmentation dataset, including 99,299 images/slices, 5 modalities, and 25 types of targets. This dataset contains 14 public and 5 in-house datasets (marked with *). Specifically, for CT datasets [11, 20, 22], the targets include chest (heart, lung, trachea) and abdomen (aorta, bladder, gall bladder, kidney, liver, pancreas, spleen, stomach). MRI datasets contain the brain [2], brain tumor [10], knee (femur and tibia) [15], and prostate [16]. Ultrasound datasets have nine objects, including breast nodule*, eyes*, femur*, heart [14] (left atrium, left ventricle endocardium, left ventricle epicardium), kidney pelvis*, nuchal translucency*, and thyroid nodule [30]. We also considered two other targets from two modalities, i.e., Colonoscopy: polyp [6] and Fundus: OpticDisc [5,18]. Approved by the local IRB, the in-house ultrasound datasets were collected and annotated by an experienced sonographer under strict quality control. CT/MR volumes were sliced into 2D images similar to [9]. The whole dataset was split into 6:1:3 for fine-tuning, validation, and testing, respectively.\nWe implemented RoBox-SAM in Pytorch, using an NVIDIA A40 GPU. The batch size, learning rate, and total epoch are set to 16, 1e-4, and 20, respectively. SAM's backbone was ViT-H. GT box was generated by the adjacency box of the annotated mask. Input box prompts were randomly shifted by 0-30% based on the GT box to evenly mimic the human operations with varying experiences/annotation preferences, with the condition that their IoU>0.5. We followed the SAM to use cross-entropy and dice losses to optimize the model via AdamW optimizer. We further added L. and Lp to assist the model learning. The proposed PRM, PEM, and SIE are two-layer CNNs with linear projection. They encode the high-dimensional features and output offsets, points, or low-dimensional features to satisfy specific requirements. As shown in Fig. 2, during fine-tuning, the original components in SAM were frozen, whereas other parts were learnable. During testing, all the parameters were frozen, and the mask"}, {"title": "Conclusion", "content": "In this paper, we propose RoBox-SAM for improving SAM's robustness to box prompts of different qualities. We first directly refine the input box with poor quality via the proposed PRM to obtain a high-quality box prompt. Then, we automatically generate potential point prompts through REM to assist the box-promptable segmentation. Last, we mine the self-information in the image to further ensure robustness via attention optimization. Extensive experiments on the large medical dataset validate the efficacy of RoBox-SAM. In the future, we will extend the framework to more modalities and 3D segmentation tasks."}]}