{"title": "Human-Alignment Influences\nthe Utility of AI-assisted Decision Making", "authors": ["Nina L. Corvelo Benz", "Manuel Gomez Rodriguez"], "abstract": "Whenever an AI model is used to predict a relevant (binary) outcome in AI-assisted decision making,\nit is widely agreed that, together with each prediction, the model should provide an AI confidence value.\nHowever, it has been unclear why decision makers have often difficulties to develop a good sense on when\nto trust a prediction using AI confidence values. Very recently, Corvelo Benz and Gomez Rodriguez have\nargued that, for rational decision makers, the utility of AI-assisted decision making is inherently bounded\nby the degree of alignment between the AI confidence values and the decision maker's confidence on\ntheir own predictions. In this work, we empirically investigate to what extent the degree of alignment\nactually influences the utility of AI-assisted decision making. To this end, we design and run a large-scale\nhuman subject study (n = 703) where participants solve a simple decision making task an online\ncard game-assisted by an AI model with a steerable degree of alignment. Our results show a positive\nassociation between the degree of alignment and the utility of AI-assisted decision making. In addition,\nour results also show that post-processing the AI confidence values to achieve multicalibration with respect\nto the participants' confidence on their own predictions increases both the degree of alignment and the\nutility of AI-assisted decision making.", "sections": [{"title": "Introduction", "content": "State-of-the-art AI models have matched, or even surpassed, human experts at predicting relevant outcomes\nfor decision making in a variety of high-stakes domains such as medicine, education and science [1-3].\nConsequently, the conventional wisdom is that human experts using these AI models should make better\ndecisions than those not using them. However, multiple lines of empirical evidence suggest that this is not\ngenerally true [4-7].\nIn the canonical case of binary outcomes and binary decisions, Corvelo Benz and Gomez Rodriguez [8] have\nrecently argued that the way in which AI models quantify and communicate confidence in their predictions\nmay be one of the reasons AI-assisted decision making falls short. Their key argument is that, if an AI\nmodel uses calibrated estimates of the probability that the predicted outcome matches the true outcome as\nAI confidence values, as commonly done [9-14], a rational human expert who places more (less) trust on\npredictions with higher (lower) AI confidence may never make optimal decisions. Further, they show that the\noptimality gap is proportional to the maximum alignment error (MAE) between the AI confidence and the\nhuman expert's confidence, i.e.,\nMAE = max\nh\u2264h',a\u2264a'\nP(Y = 1 | A = a, H = h) \u2013 P(Y = 1 | A = a', H = h'),\nwhere A and H are the AI and the expert's confidence values in the outcome Y = 1, respectively.\nWhile the above theoretical results are illuminating, they do not elucidate to what extent there is an\nassociation between the degree of alignment between the AI confidence and the human expert's confidence"}, {"title": "AI-Assisted Game Design", "content": "The game consists of different rounds i \u2208 {1,..., 24}. In each round, the game shows the participant 21 cards\nout of a game pile of 65 cards and picks one card at random from the pile. Based on the cards shown, it asks\nthe participant to guess the color of the card picked and give their (initial) confidence that the card picked is\nred. Then, it shows the participant the AI confidence that the card picked is red and allows them to revise"}, {"title": "A perfectly calibrated AI, by design", "content": "Participants in the first three groups are shown 24 different types of game piles S, a different type per round\nin randomized order. Each type of game pile (r, a) \u2208 S is defined by the fraction r of red cards in the game\npile and the AI confidence a. The AI confidence a takes values from the set A = {1/13, 2/13...,12/13} and\nthe fraction r of red cards is given by\nr \u2208 {a \u2013 vara, a + vara},\nwhere vara is chosen such that, in Eq. 2, r65 is an integral number of cards for all r. To ease interpretability,\nparticipants are shown re-scaled AI confidence values rounded to the nearest integer between 0 and 100. Asr\ndetermines the probability that the game picks a red card from the game pile and each participant is shown\neach type of game pile once, the AI confidence is perfectly calibrated, i.e., for all a \u2208 A, it holds that\nP(C = red | A = a) = a.\nWe also validated empirically that the rounded AI confidence values indeed obtain a low calibration error as\nwell (see Appendix Table 2).\nFurther, since one can argue that the AI model has essentially information about only 13 cards whereas\nparticipants have information about 21 cards, the latter have more information about each pile than the\nformer."}, {"title": "Steering alignment by biasing the proportion of red and black cards shown to\nthe participants", "content": "In each round, the 21 cards of the game pile shown to the participants in the first three groups are sampled\nfrom a Wallenius' noncentral hypergeometric distribution with an odds ratio wg(r, a) defined by the group g\nparticipants are assigned to.\nThe odds ratio wg(r, a) biases the likelihood of the color ratio in the cards shown to the participants if\nwg(r, a) is greater (smaller) than 1, it is more likely to sample a red (black) card than a black (red). More\nformally, the fraction z of red cards shown to the participant is given by\nz ~ wnchypg (21, 65 \u00b7 r, 65 \u00b7 (1 - r), wg (r, a)) .\nand, for each group g, the odds ratio wg(r, a) biases the generation as follows:\n\u2022 Group \u2192\u2190: Bias the fraction of reds shown towards the AI confidence the odds of sampling reds\n(blacks) is greater than the odds of sampling blacks (reds) when the true probability r is lower (greater)\nthan the AI confidence a. We set\nw_g(r, a) = \\begin{cases}\n1/4 & \\text{if } r > a, \\\\\n4 & \\text{if } r < a.\n\\end{cases}\n\u2022 Group \u2190\u2192 Bias the fraction of reds shown away from the AI confidence the odds of sampling\nreds (blacks) is greater than the odds of sampling blacks (reds) when the true probability r is greater\n(lower) than the AI confidence a. We set\nw_g(r, a) = \\begin{cases}\n4 & \\text{if } r > a, \\\\\n1/4 & \\text{if } r < a.\n\\end{cases}"}, {"title": "Increasing alignment via multicalibration", "content": "Participants in the Group \u2192 Rare shown the same type of game piles with the same fraction z of red\ncards shown as participants in the Group. However, the AI confidence is post-processed by the\nuniform mass histogram binning algorithm introduced by Corvelo Benz and Gomez Rodriguez [8] for the\npurpose of multicalibration. We run the algorithm with additional (held-out) calibration data from the Group\n\u2192\u2190 for a different set of games from the same distribution. Refer to the Materials and Methods section\nand the Appendix Figure 8 for more information regarding the post-processing algorithm and the calibration\ndata."}, {"title": "Results", "content": "We first quantify the degree of alignment between the AI confidence and the human expert's confidence for\neach group by measuring the empirical maximum alignment error (MAE, Eq. 1) and expected alignment\nerror (EAE, Eq. 3). Table 1 summarizes the results. We observe the lowest alignment error in Group \u2022\nclosely followed by Group \u2190\u2192, and the highest alignment error in Group \u2192\u2190, where we observe\nmore than twice higher MAE and a 20 times higher EAE than in Group \u2022 \u2022. Further, in Group \u2192 R,\nwe observe that the multicalibration algorithm helps to reduce the EAE by a third and the MAE by almost\nhalf in comparison to Group \u2192 \u2190. In addition, in Figure 9 in the Appendix, we also observe that, within\neach group, the initial confidence of participants is normally distributed around the percentage z of reds"}, {"title": "Materials and Methods", "content": "Our data and analysis code are available at https://github.com/Networks-Learning/human-alignment-\nstudy. The study setup is available to play at https://hac-experiment.mpi-sws.org/?PROLIFIC_PID=\ntest&STUDY_ID=test&SESSION_ID=test&LEVEL=B&GAME_BATCH=0. The study reported in this article was\napproved by the institutional review board of ETH Zurich under Institutional Review Board Protocol EK\n2024-N-49 (\"User Study on Human-Aligned Calibration for AI-Assisted Decision Making\"). All participants\ngave informed consent in advance.\nRecruitment of Participants. We recruited in total 703 participants on Prolific for our study (average\nage, 38.44 years; age range, 18 to 79 years; gender, 345 females, 8 non-binary, 4 not disclosed). Out of these\nparticipants, 302 participants were recruited to obtain calibration data for the multicalibration algorithm [8].\nFor the different groups, balanced condition assignment and repeat-participant exclusion were performed\ndirectly on Prolific. Upon joining the study, each participant was assigned to Group \u2190\u2192 (100 participants),\nGroup \u2022 \u2022 (99 participants), Group \u2192\u2190 (102 participants), or Group \u2192R (100 participants). The\nstudy included an instruction block, a practice game, three comprehesion/attention checks, 24 game instances,\nand a set of end of game and end of study surveys, as well as a demographic survey. More information about\nthe surveys, as well as an overview of the study timeline, is provided in Figures 10 to 17 in the Appendix. The\nthree attention checks were designed as simple game instances with a fraction of reds of 0%, 75%, and 100%\nwith no bias in the cards shown to the participants. The same three attention checks were used in each group,\nwith the same configuration of cards shown to participants, and in the same order and place in the study\ntimeline. We used the attention checks to filter out participants who performed out of the norm compared to\nmost other participants: In more than one attention check, the confidence of the filtered participant was more\nthan one standard deviation away from the mean confidence computed for each attention test over all groups.\nThis resulted in on average 15.25 participants being excluded per group (15, 20, 17, and 9, respectively). We\nperform the same filtering procedure with participants recruited for obtaining calibration data (the mean and\nstandard deviation being computed over only these participants), resulting in the exclusion of 42 participants.\nGame Batches. For each group, we construct 20 game batches with one game pile (r, a, z) per type of game\n(r, a) \u2208 S by sampling z from the Wallenius' non-central geometric distribution defined in Eq. 2. Similarly,\nwe construct 60 game batches using the same distribution as in Group \u2192 to gather calibration data.\nEach participant completes one of the game batches consisting of 24 games such that, for each game batch,\nwe obtain data from 4 to 6 participants.\nMetrics and Evaluation. In each game, participants are asked to state their initial confidence that the card\npicked is red in a range from 0 and 100. We transform this recorded confidence into a discretized confidence\nh\u2208 H = {very low, low, high, very high} by dividing the confidence range into four regions of equal size\n([0,25], [26, 50], [50, 75], [76, 100]). When the initial confidence is 50, we assign the confidence value high (low)\nif the initial guess is red (black).\nTo quantify the level of alignment error, we use the Maximum Alignment Error (MAE) and the Expected\nAlignment Error (EAE). The MAE is defined by Eq. 1 and the EAE is defined as\n\\text{EAE} = \\frac{1}{N} \\sum_{\nh<h',a<a'} [P(Y = 1 | A = a, H = h) \\newline \u2212P(Y = 1 | A = a', H = h')]_+\nwhere N = |{h \u2264 h', a \u2264 a'}|.\nTo run the Bayesian A/B tests, we use the \"brms\" package in R to fit the following model type\nQ' | trials(Q') ~ 0 + g * Q + (1 | participant)\nusing the binomial family. By default, this fits a logistic model for the proportions. We set an\nuninformative standard normal prior N(0, 1) for all parameters of the model. We fit two different models:\none for groups \u2190\u2192,\u2022 \u26ab, and\u2192\u2190\u2022 and one for groups \u2192\u2192\u2190\u2190 and \u2192 R. The Bayesian"}, {"title": "A Additional Experimental Results", "content": "We make two observations matching the theoretical assumptions and results in Corvelo Benz and Gomez\nRodriguez [8] in the study.\nFigure 3 shows the probabilities of guessing red given the initial and final guess of participants. We\nobserve that participants' initial guess is monotonic in their initial confidence, and that the final guess is\nmonotonic in their initial confidence and the AI confidence.\nGiven the record of initial confidence hi,j, the initial di,j and final d'i,j color guess made by the participant\nj in the game instance i, we compute the best monotone \u03c0mono and best joint joint guess for all games played.\nThe best joint guess maximizes the expected accuracy of the guess given only confidence hi and ai the\nguess is red (black) if the mean probability of a red card for game instances with confidence hi and ai is\nhigher (lower) than 50%. The best monotone guess maximizes the expected accuracy among guesses made by\nrational decision makers-guesses that are monotone in confidence hi and ai.\nWe measure the expected accuracy of each guess averaged across participants and games,\n\\frac{1}{\\#\\text{participants}} \\sum_j \\frac{1}{24} \\sum_i \\mathbb{E}_{C_i \\sim \\text{Be}(r_i)} [1[\\pi(i, j) = C_i]]\nwhere \u03c0(i, j) \u2208 {di,j, d'i,j, \u03c0joint (i, j), \u03c0mono(i, j)} denotes the guess evaluated. Figure 7 shows the utility in\nterms of expected accuracy (Eq. 4) of the best joint guess and the best monotone guess that could be made\nby the participants based on their own and the AI confidence values. Although the expected accuracy of the\nbest joint guess is mostly equivalent across groups and the expected accuracy of the AI alone is the same by\ndesign, the expected accuracy of the best monotone guess is lower in groups with lower degree of alignment.\nHowever, while the difference in the best monotone guess is barely noteable, the difference in the expected\naccuracy of the final guess is more prominent."}, {"title": "A.1 Expected Calibration Error", "content": "Table 2: Expected Calibration Error (ECE) measured empirically for each group. As expected, all values are\nrelatively low. The AI in group, while empirically the least aligned, also obtains lower ECE than\nin groups \u2190\u2190 and. \u2022. The post-processing algorithm does not significantly reduce ECE of the AI in\ngroup\u2192\u2190R."}]}