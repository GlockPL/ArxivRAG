{"title": "HourVideo: 1-Hour Video-Language Understanding", "authors": ["Keshigeyan Chandrasegaran", "Agrim Gupta", "Lea M. Hadzic", "Taran Kota", "Jimming He", "Cristobal Eyzaguirre", "Zane Durante", "Manling Li", "Jiajun Wu", "Li Fei-Fei"], "abstract": "We present HourVideo, a benchmark dataset for hour-long video-language understanding. Our dataset consists of a novel task suite comprising summarization, perception (recall, tracking), visual reasoning (spatial, temporal, predictive, causal, counterfactual), and navigation (room-to-room, object retrieval) tasks. HourVideo includes 500 manually curated egocentric videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and features 12,976 high-quality, five-way multiple-choice questions. Benchmarking results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve marginal improvements over random chance. In stark contrast, human experts significantly outperform the state-of-the-art long-context multimodal model, Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are available at hourvideo.stanford.edu.", "sections": [{"title": "1 Introduction", "content": "Humans demonstrate a remarkable ability to process visual stimuli over long time horizons, enabling them to perceive, plan and act in the real world. Consider the routine task of cooking a meal. This activity involves a continuous and adaptive visual process: identifying and using ingredients and tools, monitoring state changes of various dishes, and adjusting cooking duration/techniques based on visual cues such as color and texture. Such sustained visual processing is crucial to achieving the desired culinary outcomes. Naturally, endowing autonomous agents with this capability has been a long-standing goal in the field of Artificial Intelligence.\nIn recent years, large multimodal models [1-3] have emerged as a promising approach toward achieving this goal. Typically, these models are evaluated using multiple datasets that test capabilities such as object recognition [4, 5], image comprehension [6-8], and action recognition [9]. However, these benchmarks are often restricted to single images or short video clips, usually lasting from a few seconds to no more than three minutes [9-12]. While these benchmarks have spurred significant advancements, a deeper exploration into long-form video-language understanding is essential to develop multimodal systems that can form the basis for future autonomous agents and assistants.\nA significant challenge in evaluating long-form video-language understanding capabilities is designing tasks that genuinely necessitate long-term comprehension, i.e., tasks that require long-range dependencies. Merely posing questions that can be answered by watching a brief segment of a lengthy video effectively reduces the task to a combination of temporal localization and short-clip understanding. Furthermore, while intriguing narrative inquiries can certainly be formulated for long-form videos such as television shows and films, it is imperative to ensure that the questions are not trivially answerable due to the vast prior knowledge encoded in modern large language models.\nIn this work, we introduce HourVideo\u2014a benchmark dataset designed for long-form video-language understanding. To design tasks that require long-term comprehension, we first propose a novel task"}, {"title": "2 Benchmark Design and Construction", "content": "While open-ended question answering closely emulates human interaction, automating the evaluation of free-form natural language responses remains challenging. Given that our primary goal is to assess long-form video-language understanding capabilities, we opt for a five-way multiple-choice question-answering (MCQ) task. This approach simplifies the evaluation process by allowing to calculate an aggregate question-answering accuracy metric. In the following section, we describe our task suite and question-answer generation pipeline in detail, both of which are designed to curate diverse high-quality five-way multiple-choice questions (MCQs)."}, {"title": "2.1 Task Suite", "content": "Creating a comprehensive benchmark for long-form video-language understanding is challenging, primarily because formulating meaningful questions that require processing and synthesizing information across various temporal segments is highly nontrivial, even for expert human annotators. Moreover, we note that even benchmarks for image or short video clip understanding are difficult to construct. As a result, we typically observe two common strategies for benchmark creation: (1) pre-defined label spaces testing for a specific skill or within narrow domains (e.g., Kinetics [9] and Something-Something [15]); or (2) gluing together different datasets, each designed to test a specific model capability [16-19]. In contrast, a single benchmark that can comprehensively test a suite of model capabilities can significantly benefit the research community.\nWe draw inspiration from both lines of research methodologies and introduce a novel suite of tasks designed to benchmark long-form video-language understanding capabilities for one-hour-long videos. Our task suite encompasses a comprehensive set of perceptual and cognitive tasks, including summarization, perception (recall, tracking), visual reasoning (spatial, temporal, predictive, causal, counterfactual), and navigation (room-to-room, object retrieval) tasks. Our strategy draws inspiration from the two common approaches previously discussed: (1) designing narrowly focused question prototypes to significantly streamline the question-answer creation process, and (2) creating a diverse suite of tasks that holistically evaluate a broad spectrum of multimodal capabilities. Our task suite with manually designed question prototypes are shown in Table 1. In particular, there are 18 sub-tasks in our proposed task suite and example MCQs from HourVideo are shown in Fig. 1."}, {"title": "2.2 Dataset Generation Pipeline", "content": "In this section, we provide an overview of the question-answer creation pipeline that we developed to create Hour Video. The pipeline is summarized in Fig. 2.\nVideo curation, Stage 1. A crucial design consideration for this benchmark is the selection of video sources and types. We chose the Ego4D [13] dataset for our videos for multiple reasons: (1) its egocentric perspective aligns well with the typical visual input for autonomous agents and assistants; (2) it features extensive visual narrations, which aid in creating diverse multiple-choice questions; and (3) it is readily accessible under the Ego4D license. We manually reviewed 1,470 videos, ranging from 20 to 120 minutes, from the Ego4D dataset, assessing their potential to generate relevant questions for various tasks in our task suite. We engaged five human experts for video curation. Following this process, we curated 500 egocentric videos.\nCandidate MCQ Generation, Stage 2. The objective of this stage is to produce high-quality MCQs for each task, requiring analysis and synthesis of information across multiple temporal segments in a long-form video. Initially, we manually develop question template(s) for each task in the suite. As shown in Table 1, transforming a question template into an actual question involves incorporating video-specific information tailored to the task and template. To facilitate this, we utilize the detailed narrations from the Ego4D dataset, transforming them into a structured format that can be processed by an LLM. Specifically, we segment the video at 20-minute intervals, with each segment's representation including a summary and a list of tools, food items, technology, humans, pets, and physical locations encountered by the camera wearer in the video. We note that synthesizing a structured representation and a question template into a valid question with correct and incorrect answers presents a significant challenge, even for advanced LLMs. Consequently, for each task, we formulate detailed prompts that offer question prototypes, comprehensive instructions, in-context examples, and step-by-step guidance on how to transform a question template into a valid candidate MCQ2. In total, we developed 25 task-specific prompts.\nMCQ Refinement with LLMs using Human Feedback, Stage 3. The purpose of this phase is to refine MCQ2, created in the previous stage. MCQ2 may contain invalid questions, incorrect answers, trivial incorrect options, and various other issues. We identified that a significant source of these issues stemmed from relying on the noisy narrations in Ego4D. For example, different narrators within the same video could refer to a dishwasher as a \"plate rack\" or use other terms, and an individual might be described as an \"adult,\" \"person with a red and white shirt,\" \"man Y,\" or \"teenager\" at various times in the narration. These inconsistencies, combined with our automatic question generation in the first stage, could lead to generation of invalid MCQs. To address noisy MCQs, we implement a human feedback system where trained annotators are tasked with: 1) assessing the validity of each question to ensure it aligns with the video content, 2) verifying the accuracy of the given answer-if found incorrect, they provide the correct answer in free-form text, 3) ensuring that all incorrect options are factually wrong and clearly distinguishable from the correct answer. We gather human feedback for all MCQ2, involving over 400 hours of human effort. We then design prompts, to automatically refine MCQ2 using this human feedback to produce MCQ3. We engaged seven trained annotators in this stage.\nBlind filtering, Stage 4. Modern LLMs possess extensive prior knowledge and can thus easily answer certain questions without needing to analyze the videos. The objective of this phase is to eliminate questions that can be answered through prior knowledge or can be trivially answered without requiring any information from the video. To address this, we do blind filtering of MCQ3, utilizing two separate blind LLMs (GPT-4-turbo and GPT-4). Specifically, we exclude any MCQ that is correctly answered by at least one LLM without video input. Although this method may aggressively remove MCQs, it ensures that the remaining MCQ4 are of high quality and specifically tailored to test long-form video-language understanding."}, {"title": "2.3 Hour Video Statistics", "content": "Hour Video consists of 500 videos from the Ego4D dataset, covering 77 daily life scenarios such as cooking, cleaning, eating, watching TV, baking, etc. (Fig. 3). The dataset includes 381 hours of video footage, with video durations ranging from 20 to 120 minutes (Figure 3). On average, each video is approximately 45.7 minutes long, which 15\u00d7 larger than prior work in long-form video-language understanding [12]. Additionally, 113 videos in our dataset exceed one hour in length. Each video is accompanied by an average of 26 high-quality, five-way multiple-choice questions, totaling 12,976 questions in the dataset. Finally, we strive to ensure an even distribution of MCQs across all tasks in our suite, with the exception of causal, counterfactual, and navigation tasks, where questions were manually generated for a selected group of videos."}, {"title": "3 Experiments", "content": "3.1 Evaluation Protocol\nHourVideo includes five-way multiple-choice questions, for which we report accuracies per task and in aggregate across the entire dataset. A significant challenge in evaluating MCQs over long videos is preventing information leakage across questions. Ideally, each MCQ should be evaluated independently to avoid this issue, but unfortunately, this approach is computationally expensive and time-consuming. Therefore, for our evaluation, we assess the questions in batches, with each batch containing all questions related to a specific task or sub-task. For predictive tasks (reasoning), we provide precise timestamps to trim the videos for targeted evaluation. Details on tasks and sub-tasks requiring independent evaluation are provided in Supplementary B.\n3.2 Baselines\nIn this section, we compare the performance of different multimodal models on understanding long videos in a zero-shot setting. Specifically, we evaluate three classes of models: (1) Blind LLMs, (2) Socratic Models [21], and (3) Native multimodal models. All these models operate under a common function $A = M(V, \\tau, Q)$ where $V, \\tau, Q, M, A$ refer to the long-form video input, prompt (instruction), multiple-choice question, multimodal model, and text output respectively.\nBlind LLMs. Modern LLMs possess extensive prior knowledge, enabling them to easily answer certain questions without the need to analyze videos. Furthermore, it is likely that some questions can be trivially answered by exploiting biases in the question-answer pairs. The \u2018blind' LLM baseline is designed to evaluate this by asking the LLM to answer the multiple-choice question without considering any visual information from the video, i.e., $A = M(\\tau, Q)$, where $\\tau$ is a generic task-agnostic prompt prepended to the question Q. We use GPT-4 [22] as our LLM for this baseline.\nSocratic Models. Most current state-of-the-art multimodal models are unable to process very long videos. Therefore, to benchmark these models, we use the Socratic models approach [21]. In this approach, the video V, with a total duration of t minutes, is segmented into one-minute intervals, each denoted as V[i] for minute i. Each segment V[i] is independently captioned, yielding a sequence of captions $z_1, z_2, z_3, ..., z_t$, where $z_i = Video-Captioner(V[i])$. These captions are aggregated to form a comprehensive language-based representation of the video, referred to as the world state history, which includes timestamps. This textual representation, along with a generic task-agnostic prompt $\\tau$, serves as the input for long-form video-question answering: $A = M([\\tau, z_1, z_2, ..., z_t, Q])$. We sample one-minute video clips at a rate of 0.5 fps and a resolution of 512\u00d7384. We test using both GPT-4 [22] and LLaVA-NeXT-34B-DPO [14] as the Video-Captioner. Finally, we use GPT-4 for actual question answering, as LLaVA-NeXT-34B-DPO does not support the extended context length required to process our world state history.\nNative Multimodal Models. Multimodal video models, such as Gemini 1.5 Pro [3], are trained jointly on multimodal data, including audio, video, images, and text. These models are particularly adept at handling very long context lengths (2M+), making them ideal for end-to-end evaluation using our benchmark. Evaluating these models is straightforward, as they can directly process hour-long videos as $A = M(V, \\tau, Q)$. For all experiments, we use a sampling rate of 0.5 frames per second, a resolution of 512 \u00d7 384, and a temperature setting of 0.1.\nHuman performance. Due to the high costs associated with human evaluations, we sampled 14 videos from our benchmark, which included more than 18 scenarios in total including craft-ing/painting, cooking, construction/renovation, gardening, cleaning/laundry and yard work. We ask three human experts to conduct evaluations on 11.2 hours of video content, encompassing a total of 213 MCQs. To prevent any contamination, we ensured that human experts who evaluated videos were not involved in the annotation of the same videos at any earlier stage (Stages 3 and 5 discussed in Sec. 2). The human experts achieve an accuracy of 85.0%. The results are shown in Fig. 4.\n3.3 Results\nWe report all task and sub-task level quantitative results in Tab. 2. Qualitative evaluations, including human evaluation numbers, are presented in Fig. 4. We remark that random guessing corresponds to 20% accuracy. Below, we discuss our key observations."}, {"title": "4 Related Work", "content": "Dataset Comparison. Existing video benchmarks [23, 24, 11, 25-31], primarily focus on specific domains or short videos, which limit their ability to assess long-form video understanding comprehensively. Efforts like WebVid10M [32], InternVid [33], and Panda-70M [34] include detailed captions to provide video pretraining data but consist primarily of short video clips less than one minute in length and do not provide QA pairs. Recent works have introduced several benchmarks specifically designed for long video understanding, such as Next-QA [35], Next-GQA [36], VideoChatGPT [37], EgoSchema [12], MovieChat-1K [38] and MovieNet-QA [39]. [40] introduced benchmarks for evaluating relational space-time query tasks. Perception Test [41] proposed a diagnostic benchmark for multimodal models, probing for memory, abstraction, physics, and semantic capabilities using short video clips (23s average duration). However, the average video length in these datasets is still relatively short, with Ego-Schema having an average duration of 3 minutes. In contrast, we focus on hour-long video-language understanding, with videos averaging 45.7 minutes in duration and tasks requiring long-term comprehension.\nVideo Understanding Tasks. Significant efforts have been made to design tasks appropriate for evaluating multimodal large language models (MLLMs) . The evaluation of vision-language models (VLMs) focuses mainly on visual perception tasks such as image-text matching, retrieval, captioning, object detection, and visual grounding tasks) [44-46]. Methods revolving around contrastive learning on image-text pairs have proven to be effective methods for learning transferable representations for these visual tasks [51-53], and have been shown to be effective in more specific domains such as multi-disciplinary scientific understanding [49, 54] and multi-modal mathematical reasoning [47, 48]. Later work has improved upon the visual reasoning capabilities of VLMs [1, 55-62] and their ability to reason across complex spatio-temporal video data [63-70]. To better evaluate spatio-temporal abilities, specific benchmarks [12, 28, 30, 71, 72] have been developed. However, the questions in"}, {"title": "5 Conclusion", "content": "We introduce HourVideo, a novel benchmark dataset designed to rigorously evaluate the capabilities of multimodal models to comprehend one-hour-long videos. Our dataset consists of a novel task suite comprising summarization, perception (recall, tracking), visual reasoning (spatial, temporal, predictive, causal, counterfactual), and navigation (room-to-room, object retrieval) tasks. This benchmark includes 500 egocentric videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and features 12,976 high-quality five-way multiple-choice questions. Our zero-shot evaluation on HourVideo reveal that multimodal models such as GPT-4V and LLaVA-NeXT exhibit performance levels only slightly better than random guessing. In stark contrast, human expert performance substantially surpasses state-of-the-art long-context multimodal model Gemini 1.5 Pro (85.0% accuracy versus 37.3%), highlighting significant research gap. We aim to establish HourVideo as a benchmark challenge to spur the development of advanced multimodal models capable of truly understanding endless streams of visual data.\nLimitations and future work. Despite our substantial efforts to create a high-quality benchmark dataset, there may still be some inconsistencies within the multiple-choice questions. Additionally, while this is currently the largest long-form video-language understanding benchmark of its kind to the best of our knowledge, we acknowledge the need for more holistic benchmarks that include diverse video sources such as sports and YouTube videos. Lastly, we note that incorporating support for the audio modality is essential for more comprehensive evaluation of multimodal models. We also remark that our world extends beyond visual and auditory stimuli to include other sensory modalities (e.g., tactile), suggesting opportunities to explore these additional modalities in future work. We discuss broader impact of HourVideo in Supplementary E."}, {"title": "A Hour Video Release v1.0", "content": "We are releasing HourVideo v1.0, our proposed benchmark dataset for one-hour video-language understanding. The benchmark dataset is provided as a single JSON file for ease of use and for straightforward integration with existing benchmarking pipelines. For each video, the dataset includes metadata and contains multiple-choice questions covering multiple tasks from our proposed task suite. Each task is accompanied by a set of multiple-choice questions, each with five possible answers. For predictive visual reasoning tasks, relevant timestamps are provided to allow precise video trimming. Additionally, a PyTorch dataloader is provided to efficiently load the video and the benchmark dataset. We provide all the 500 video_uids used in our benchmark, and users can simply download the corresponding videos from the Ego4D website after reviewing and accepting the Ego4D license agreement. We provide 2 sample videos with annotations from HourVideo. All materials are available at hourvideo.stanford.edu."}, {"title": "B Data Generation Pipeline: Additional details", "content": "B.1 Prompt Design\nWe meticulously designed 25 prompts in total for tasks/ sub-tasks in our proposed task suite. For 9 out of 15 tasks, we generate questions first, followed by jointly generating answers and wrong answers. For the predictive visual reasoning and temporal pre-requisites tasks, we jointly generate questions and answers first, followed by generating wrong answers. For causal, counterfactual, spatial layout and navigation tasks, we generate questions, answers and wrong answers manually. We also designed prompts for narration compilation and paraphrasing answers for the summarization, temporal pre-requisites, and predictive visual reasoning tasks.\nB.2 Narration Compilation Details\nWe segment all our videos at 20 minute intervals and extract a semi-structured representation which includes title, description, start_identifier, end_identifier, list of tools, list of food items, list of technology objects, list of humans interacted, list of pets interacted and list of unique locations in the video segment.\nFinally, these segments are compiled by a LLM to form a single structured representation for each video. The prompt is shown in Considering that Ego4D offers two independently collected sets of narrations for each video, we select the narration set with the higher token count. This design choice is based on our empirical observation that a larger number of tokens typically ensures more comprehensive coverage of visual elements.\nB.3 Human Feedback and Expert\nRefinement Details\nFor MCQ Refinement with Large Language Models using Human Feedback we engaged seven annotators who had been trained to provide human feedback based on examples created by our team. Continuous quality assessments were conducted throughout this stage to ensure the integrity and high quality of the feedback obtained for MCQ Refinement. More than 400 hours of human effort were spent in this stage. For Expert Refinement we engaged four human experts dedicating over 250 hours of human effort for QAW Refinement.\nC Evaluation details\nEvaluation Protocol. As discussed in Sect. 3, we have developed an evaluation protocol that assesses multimodal models at the level of individual tasks and sub-tasks. The specific tasks and sub-tasks requiring independent evaluation are listed in Tab. C.1.\nThis structured approach minimizes information leakage across questions and mitigates the substantial costs associated with individual MCQ evaluation. It is important to note that the costs of individual MCQ evaluation are proportional to the number of questions, emphasizing the need for our proposed assessment strategy."}, {"title": "D Additional Experiments", "content": "D.1 Additional Baselines\nWe conduct an additional experiment using the recently released Tarsier model [101] which reports state-of-the-art results in multiple short-form video understanding benchmarks. Following the exact setup in Tarsier for long-video understanding, we use the publicly available Tarsier-7B model with 16 frames uniformly sampled from the entire video. We remark that Ego4D [13] is used in Tarsier pre-training (Video captioning task). Prompts. For all our baseline experiments, we use a generic task-agnostic prompt together with the video and MCQ tests for evaluation. All our prompts for baseline evaluations are included in the evaluation toolkit.\nD.2 Model Refusal Rates\nProprietary models, such as GPT-4 and Gemini 1.5 Pro can abstain from responding to MCQs for various reasons, including video content filtering, privacy concerns, and other undisclosed factors."}, {"title": "E Broader Impact", "content": "The Long-form Video-Language Understanding Benchmark introduced in this work has the potential to significantly advance the field of AI video understanding and enable a wide range of useful applications. By focusing on long-form video, HourVideo challenges models to demonstrate high-level reasoning and comprehension skills that more closely mirror human intelligence. Success on this benchmark could lead to AI systems that can effectively perceive and interact with the real world over extended periods of time, unlocking transformative capabilities in areas like embodied AI and robotics, autonomous vehicles, smart environments, and augmented/virtual reality.\nEmbodied AI and robotics, which aim to develop artificial agents that can perceive, navigate, and physically interact with their environment, could benefit greatly from advances in long-form video understanding. A robot or embodied agent that can maintain a coherent, long-term understanding of its surroundings and goals would be far more capable and adaptable than one operating with only short-term perception. It could handle more complex, multi-stage tasks, learn from extended observations, and build rich mental models to support planning and decision making.\nProgress on HourVideo could also contribute to the development of large world models \u2013 AI systems that learn comprehensive, multi-modal representations of the world from vast amounts of data.\nLong-form video understanding is also crucial for creating compelling augmented reality (AR) and virtual reality (VR) experiences.\nWhile these exciting applications underscore the importance of advancing long-form video under-standing, it is equally critical to consider the potential risks and ethical implications involved. Video data, particularly long-running egocentric video as used in HourVideo, can be highly sensitive and revealing of personal details. As AI video understanding capabilities grow, robust safeguards must be put in place to protect individual privacy, ensure secure data handling, maintain transparency around data collection and use, and prevent unauthorized surveillance or abuse."}]}