{"title": "IMPROVING LLM REASONING THROUGH SCALING IN-FERENCE COMPUTATION WITH COLLABORATIVE VERIFICATION", "authors": ["Zhenwen Liang", "Ye Liu", "Tong Niu", "Xiangliang Zhang", "Yingbo Zhou", "Semih Yavuz"], "abstract": "Despite significant advancements in the general capability of large language models (LLMs), they continue to struggle with consistent and accurate reasoning, especially in complex tasks such as mathematical and code reasoning. One key limitation is that LLMs are trained primarily on correct solutions, reducing their ability to detect and learn from errors, which hampers their ability to reliably verify and rank outputs. To address this, we scale up the inference-time computation by generating multiple reasoning paths and employing verifiers to assess and rank the generated outputs by correctness. To facilitate this, we introduce a comprehensive dataset consisting of correct and incorrect solutions for math and code tasks, generated by multiple LLMs. This diverse set of solutions enables verifiers to more effectively distinguish and rank correct answers from erroneous outputs. The training methods for building verifiers were selected based on an extensive comparison of existing approaches. Moreover, to leverage the unique strengths of different reasoning strategies, we propose a novel collaborative method integrating Chain-of-Thought (CoT) and Program-of-Thought (PoT) solutions for verification. CoT provides a clear, step-by-step reasoning process that enhances interpretability, while PoT, being executable, offers a precise and error-sensitive validation mechanism. By taking both of their strengths, our approach significantly improves the accuracy and reliability of reasoning verification. Our verifiers, Math-Rev and Code-Rev, demonstrate substantial performance gains to existing LLMs, achieving state-of-the-art results on benchmarks such as GSM8k and MATH and even outperforming GPT-40 with Qwen-72B-Instruct as the reasoner.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023a;b; Jiang et al., 2023; Team et al., 2024) have demonstrated exceptional performance across various natural language tasks. Notably, the reasoning tasks such as math problem solving (Cobbe et al., 2021; Hendrycks et al., 2021), code completion (Austin et al., 2021; Chen et al., 2021), multi-modal reasoning (Yue et al., 2024a; Liang et al., 2024a) have attracted significant attention from Al researchers. Since reasoning is a critical component of many important high-level tasks, such as scientific discovery (Liang et al., 2024a; Miret & Krishnan, 2024), world model (Hao et al., 2023), embodied agents (Song et al., 2023), etc. However, even the most advanced LLMs still face challenges in complex multi-step reasoning problems (Zhang et al., 2024a; Shi et al., 2024; Trinh et al., 2024). To improve the performance of LLMs on reasoning, recent studies (Yu et al., 2024b; Yue et al., 2024b; Gou et al., 2024; Luo et al., 2023; Wei et al., 2024; Tang et al., 2024; Yue et al., 2024c) have mainly focused on generating synthetic question-answering pairs from stronger LLMs like GPT-4 (Achiam et al., 2023) or utilizing human-annotated rationales (Toshniwal et al., 2024) for supervised fine-tuning. These approaches have achieved outstanding performance on reasoning benchmarks like GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021; Lightman et al., 2023), MBPP (Austin et al., 2021), etc."}, {"title": "2 OUR METHOD", "content": "The workflow of our method is presented in Fig. 3. After collecting a diverse set of solutions, including both correct and incorrect ones, we train our verifiers, which can be implemented using any open-weight LLM (e.g., Mistral-7B-instruct-v0.3). During the inference stage, the reasoner LLM generates responses to an input question, and the verifier is applied to score multiple sampled solutions from the reasoner."}, {"title": "2.1 DATA COLLECTION FOR TRAINING VERIFIERS", "content": "Math Reasoning We use the training sets of GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) as seed datasets and sample model solutions from multiple backbone models: (1) general-purpose LLMs, including Mistral (Jiang et al., 2023) and Phi3 (Abdin et al., 2024); and (2) math-specialized models, including InternLM2-Math (Ying et al., 2024) and MAmmoTH2-plus (Yue et al., 2024c). For each question in GSM8k and MATH, we sample 10 Chain-of-Thought (CoT) solutions and remove duplicates. Using functions provided by (Ying et al., 2024), we extract answers from model predictions and compare them with ground truth, resulting in 159,778 correct and 100,794 incorrect solutions for the training of Math-Rev, with an average of 10.67 correct and 6.73 incorrect solutions per problem. For the evaluation on the MATH dataset, we follow Lightman et al. (2023) and use the subset - MATH500, the same as previous work Wang et al. (2023); Gao et al. (2024). For the results in Figure 2, we use the full test set of MATH to ensure fairness.\nCode Reasoning Similarly, we utilize general-purpose LLMs, including LLaMA-3-8B (Touvron et al., 2023b) and Phi3 (Abdin et al., 2024), and code-specialized models, including CodeGemma-7B-it (Team, 2024a) and CodeQwen1.5 (Team, 2024b). We select the training sets of MBPP (Austin et al., 2021) and the Python subset of MagiCoder-75k (Wei et al., 2024) as seed datasets. In code generation tasks, test cases are usually required to determine the correctness of solutions. The original MBPP training set includes test cases, but the MagiCoder does not. To address this, we use GPT-40 to generate test cases for each problem in the Python subset of MagiCoder-75k, retaining only test cases that the reference solution passed. If no generated test case matches the reference solution, we repeat the process with a temperature of 0.8 up to three times. This process results in 11,527 problems with test cases in the MagiCoder-75k dataset. We then generate 50 solutions for each seed problem in both that subset and MBPP, resulting in 132,089 correct and 145,345 incorrect solutions with an average of 11.10 correct and 12.21 incorrect solutions per problem, which are used for training our Code-Rev."}, {"title": "2.2 TRAINING MATH-REV AND CODE-REV", "content": "The verifiers, implemented using LLMs (e.g., Mistral), need to be trained with appropriate training methods to ensure their effectiveness during inference. We extensively investigate various usable methods that are introduced next.\nReward-based: ORMs and PRMs. Following the widely accepted definition in (Uesato et al., 2022), there are two categories of reward-based methods for building verifiers: outcome-reward models (ORMs) (Cobbe et al., 2021) and process-reward models (PRMs) (Lightman et al., 2023). ORM, commonly used in RLHF (Ouyang et al., 2022), can produce scalar scores on model re-sponses, whereas PRM evaluates the reasoning path step-by-step. Despite better performance, PRMS need to collect process supervision data, relying on either human annotation (Lightman et al., 2023) or per-step Monte Carlo estimation (Wang et al., 2023), both of which are prohibitively expensive to scale. Moreover, the PRM method requires the solution to be formatted as step-by-step reasoning chains (Lightman et al., 2023; Wang et al., 2023; Luo et al., 2024), where steps need to be clearly separated by special tokens or periods to be scored, thereby limiting the application scenario of PRM. Consequently, in this paper, we do not assign per-step scores on reasoning paths, but instead calculate a final score for the whole solution.\nPreference-tuning: DPO and Beyond. Direct Preference Optimization (DPO) (Rafailov et al., 2024) is one of the most popular offline preference optimization methods. Unlike ORM or PRM which rely on learning an explicit reward model, DPO proposes a novel loss function based on pref-erence pairs, which reparameterizes the reward function and applies it into the the Bradley-Terry (BT) ranking objective. This innovation has inspired various follow-up studies, such as IPO (Azar et al., 2024), KTO (Ethayarajh et al., 2024), CPO (Xu et al., 2024), and R-DPO (Gallego, 2024). Besides them, the reference-free variants including ORPO (Hong et al., 2024) and SimPO (Meng et al., 2024) argue that reference models in the above reward functions would incur additional mem-"}, {"title": "Our Verifiers Training.", "content": "Although those reference-tuning methods are primarily designated to align LLMs with human preferences, they can also be adapted for training verifiers (Hosseini et al., 2024). By feeding the backbone LLM of the verifiers with pairs of correct and incorrect solutions, designated as chosen and rejected outputs, and applying the mentioned training methods, the verifier can be trained to assign higher generation probabilities to correct solutions over incorrect ones. Then the probability can be served as a score for ranking solutions. In our paper, Math-Rev and Code-Rev are trained separately by their respective training data with one of the preference-tuning methods SimPO. We believe that such verifiers have a significant advantage over ORMs: it does not introduce additional training parameters and not change the goal of generation for LLMs, aligning better with the original usage of LLM."}, {"title": "2.3 INFERENCE ENHANCED BY VERIFICATION WITH COTNPOT", "content": "During the inference stage, after deploying our Math-Rev and Code-Rev verifiers, we identify dis-tinct challenges in verifying math and code reasoning. For math reasoning, while model-based ver-ifiers can effectively detect surface-level logical errors such as incorrect use of operators, numbers, and methods, they struggle to catch subtle mistakes such as calculation errors and deeper logical inconsistencies. In code reasoning, the structured and abstract nature of code makes it difficult to read and understand, leading verifiers to assign similar scores to different solutions, indicating their difficulty in accurately identifying errors within the code.\nTo address these challenges, we propose a method called CoTnPoT, which enhances verification by leveraging the connection and complementary strengths of the Chain of Thought (CoT) and Program of Thought (POT) solution formats.\nFor math reasoning, we use an external LLM, DeepseekV2-chat-Lite (Zhu et al., 2024), to transform CoT solutions SCOT into PoT counterparts SPOT based on problem descriptions Q,\n$S_{POT} = CoderLLM(Q, S_{CoT}).$ (1)\nWe choose DeepseekV2-chat-Lite because it obtains both strong math reasoning and coding capabil-ities and we need to apply them to translate CoT solutions into PoT programs for math problems. We then verify whether the transformed final answer from the execution of SPOT matches the final an-swer from SCOT. Our motivation is that logical errors in SCOT would cause run-time errors in SPOT, while calculation errors in SCOT would result in mismatched answers between SCOT and SPOT, as PoT solutions ensure calculation correctness by using the Python interpreter. This approach takes advantage of the executable nature of program-based solutions.\nFor code reasoning tasks, we find that directly training verifiers on Python code alone leads to inferior performance. This may be due to the increased difficulty in reading and understanding code compared to human language, which can make it harder to detect reasoning errors. Therefore, we use the same LLM to generate both the code solution SPOT and the corresponding step-by-step description Spes that explains why the solution is correct. Because using the same LLMs for both code and description generation reduces over-reliance on external LLMs. During both training and inference and code verification, we concatenate the description and the code as an integrated input for the verifier, as shown in Equation 2. This method provides richer information in the code solutions, making the LLM-based verification process more effective.\n$S_{Des} = CoderLLM(Q, S_{POT})$ (2)"}, {"title": "3 EXPERIMENTS", "content": "For all experiments in Figure 4, we use the latest Mistral-7B-instruct-v0.3 as the backbone LLM for building the verifiers and apply LoRA with a dropout rate of 0.1 to reduce the computational load during verifier training. The training batch size is set to 64, and the learning rate to 0.00002 for all verifiers. For ORM, we add an additional computational head on the per-token"}, {"title": "LLM Reasoners in Evaluation.", "content": "To evaluate the reasoning performance on the GSM8k dataset, we use LLaMA2-7B-base and Mistral-7B-v0.1, both fine-tuned on GSM8k, along with Gemma-7B-it, Phi-14B, InternLM2-Math-7B, and LLaMA3-70B as our reasoners. For LLaMA2 and Mistral, we sample 100 solutions per problem for voting and verification, while 64 solutions are generated for the rest. On the MATH dataset, which contains much harder problems than GSM8k, we replace LLaMA2-7B-base and Mistral-7B-v0.1 with LLaMA3-8B-instruct and Mistral-7B-v0.3 for their superior reasoning ability, along with other four reasoners. For all problems in MATH500, we generate 64 solutions individually. All LLM output sampling in our paper is based on a temperature of 0.8 and top-p of 0.95."}, {"title": "Experimental Results.", "content": "The results are shown in Figure 4. We observe that the verifiers consistently improve the greedy decoding baseline, especially for weaker reason-ers such as LLaMA2-7B. We also evaluate in-distribution (ID) LLMs, which are the source LLMs used to generate the training data for verifiers, such as Mistral, InternLM2-Math, and Phi, and out-of-distribution (OOD) LLMs, such as LLaMA2-7B and Gemma-7B. The results show no significant difference between ID and OOD perfor-mance improvement by verifiers, suggesting that our ap-proach can extend to any LLM reasoners and is not lim-ited to the LLMs that generate the training data. Furthermore, preference-tuning-based verifiers, including DPO and SimPO, outperform ORM, similar to the findings in Hosseini et al. (2024). The potential reason is that DPO and SimPO train LLMs without changing their structure, thus aligning better with their previous training goals of auto-regressive text generation. Additionally, ORPO and SimPO consistently out-perform DPO, potentially because the regularization term on the reference model in the DPO loss might negatively impact verifier training. In other words, we do not need to control the divergence of the SFT model and the final verifier because it will not be used for text generation anymore. Therefore, we can conclude that the reference-free method is more suitable for verifier training.\nAdditionally, preference-tuning methods such as DPO and SimPO theoretically enable auto-regressive LLMs to generating solutions. However, we observe that the generation ability of verifiers trained with preference pairs degrades rapidly, rendering them incapable of generating coherent sen-tences. This observation is also consistent with the findings in Hosseini et al. (2024). We attribute this degradation to that the verifier training process involves more steps and larger learning rates than typical alignment practices, which likely causes the verifier's weights to diverge significantly from the fine-tuned checkpoint. Consequently, these verifiers lose their generation capability and are instead better suited for calculating the likelihood of pre-generated solutions."}, {"title": "3.2 EVALUATION OF VERIFIERS WITH COTNPOT", "content": "This section focuses on evaluating the inference performance using the trained verifiers with the designed CoTnPoT filtering. We upgraded the backbone model of our verifier in math reasoning from Mistral-7B to MAmmoTH-7B-plus to enhance performance.\nMath Reasoning. We further enhance the inference process by combining majority voting with verifier scores, using the scores from verifiers as weights in the voting process. Specifically, we apply Gumbel Softmax (Jang et al., 2022) with the hyperparameter $\\tau$ to regulate the influence of"}, {"title": "Table 3: Our verifier Math-Rev outperforms two baselines with fewer solutions sampled per problem", "content": "on both GSM8k and Math500 datasets, demonstrating the effectiveness of our verifier training and\nCoTnPoT verification."}, {"title": "We also pair our Math-Rev with one of the strongest open models, Qwen-72B-Instruct. As shown in", "content": "Figure 2, the final performance of Qwen-72B + Math-Rev on MATH surpasses all SOTA baselines including GPT-40. This experiment demonstrates that Math-Rev can enhance even the most power-ful LLM reasoners, despite being trained on data from smaller and weaker models, highlighting the promising effectiveness of verification - learning from errors."}, {"title": "3.4 ABLATION STUDY ON COTNPOT", "content": "In this section, we compare our pro-posed CoTnPoT with two ablated ap-proaches:\nA1. Prompting the same coder LLMto generate the final answer directlythrough code, and filtering out CoTsolutions that do not match the codesolution. This ablation isolates thescenario where the coder LLM relies solely on its inherent strong math problem-solving ability,instead of analyzing and transforming the CoT solution.\nA2. Prompting the same coder LLM to generate comments that analyze the CoT solutions and assess their correctness. This approach intuitively leverages LLMs as filters for verification.\nWe implement and compare CoTnPoT, A1, and A2 across all settings and both datasets in Figure 5. The accuracy is averaged at the dataset level for better visibility. We observe that CoTnPoT consistently outperforms both Al and A2. The potential reason is that the task of translating CoT solutions to PoT solutions is easier and requires less reasoning than the processes in Al and A2. Therefore, although A1 and A2 are more direct methods to verify a solution, their performance is limited by the capability of the coder LLM. On the other hand, CoTnPoT relies less on complex reasoning, making it more effective overall."}, {"title": "3.5 ANALYSIS ON COTNPOT", "content": "Our method, CoTnPoT, for math reasoning is designed to filter out low quality solutions by exam-ining the match between CoT and PoT solutions. This approach essentially functions as a binary classification task. By defining the ground truth label of a correct CoT solution as 1 and an incorrect CoT solution as 0, the correspondence between CoT and PoT solutions is used as the prediction label, where a match is labeled as 1 and a mismatch as 0. The effectiveness of the CoTnPoT filter is directly correlated to the performance of this binary classifier, aiming to retain all solutions labeled as 1 and discard those labeled as 0.\nTo validate this method, we randomly selected 50,000 correct and 50,000 incorrect CoT solutions from our verifier training set and applied the CoTnPoT filter. The performance of the classifier is summarized in the confusion matrix presented in Table 4. The results demonstrate that the CoTnPoT classifier effectively identifies correct solutions, as evidenced by high True Positive Rate (TPR) and False Negative Rate (FNR). While the False Positive Rate (FPR) and True Negative Rate (TNR) are moderate, indicating some incorrect solutions are not filtered out, the majority of correct solutions are preserved for further verification. This experiment provides strong evidence of the significant performance improvement that the CoTnPoT-based filter brings to math reasoning. Figure ?? in the appendix shows the examples of true positive, false negative, false negative, and true negatives of the above CoTnPoT classifier."}, {"title": "Table 4: Confusion Matrix for the CoTnPoT-based filter.", "content": "Correct CoT Solution\nActually Negative:\nWrong CoT Solution\nPredicted Positive:\nCoTnPoT Match\nCoTnPoT Mismatch\nFalse Negatives (FNR): 9.91%\nTrue Negatives (TNR): 79.70"}, {"title": "Actually Positive:", "content": "True Positives (TPR): 90.09%\nFalse Positives (FPR): 20.30%\nPredicted Negative:\nCoTnPoT Mismatch"}, {"title": "4 RELATED WORK", "content": "Cobbe et al. (2021) is the pioneering work that applies verifiers in mathematical reasoning, where they train token-level reward models to give scores on problem solutions. Then Uesato et al. (2022); Lightman et al. (2023) dive into the application of PRM - process reward models, where scores are assigned to each intermediate step of solutions, providing more fine-grained feedback. Math-Shepherd (Wang et al., 2023) and MiPS (Wang et al., 2024b) propose using Monte-Carlo Tree-Search (MCTS) to automate the data collection process instead of human labeling. OVM Yu et al. (2024a) employs outcome supervision for training a value model, which prioritizes steps that lead to accurate conclusions during inference. V-Star (Hosseini et al., 2024) presents an iterative frame-work in LLM training, which collects both correct data for supervised fine-tuning and wrong data for verifier training. They also showed that DPO is stronger than ORMs in verification. Built on reranking strategies such as verifiers, multiple studies Brown et al. (2024); Snell et al. (2024) found that scaling up inference-time computing is much more cost-effective than training. To achieve more effective and efficient inference-time verification, our approach samples solutions from various LLM reasoners and comprehensively compares different verifier training methods. Our best verifier Math-Rev achieves strong performance on math solution verification using only outcome-based labels in training and even outperforms PRM baselines."}, {"title": "4.1 SCALING UP INFERENCE-TIME COMPUTING", "content": "Cobbe et al. (2021) is the pioneering work that applies verifiers in mathematical reasoning, where they train token-level reward models to give scores on problem solutions. Then Uesato et al. (2022); Lightman et al. (2023) dive into the application of PRM - process reward models, where scores are assigned to each intermediate step of solutions, providing more fine-grained feedback. Math-Shepherd (Wang et al., 2023) and MiPS (Wang et al., 2024b) propose using Monte-Carlo Tree-Search (MCTS) to automate the data collection process instead of human labeling. OVM Yu et al. (2024a) employs outcome supervision for training a value model, which prioritizes steps that lead to accurate conclusions during inference. V-Star (Hosseini et al., 2024) presents an iterative frame-work in LLM training, which collects both correct data for supervised fine-tuning and wrong data for verifier training. They also showed that DPO is stronger than ORMs in verification. Built on reranking strategies such as verifiers, multiple studies Brown et al. (2024); Snell et al. (2024) found that scaling up inference-time computing is much more cost-effective than training. To achieve more effective and efficient inference-time verification, our approach samples solutions from various LLM reasoners and comprehensively compares different verifier training methods. Our best verifier Math-Rev achieves strong performance on math solution verification using only outcome-based labels in training and even outperforms PRM baselines."}, {"title": "4.2 CONNECT BETWEEN CHAIN-OF-THOUGHT AND PROGRAM-OF-THOUGHT", "content": "PAL (Gao et al., 2023) and PoT (Chen et al., 2023) are two early studies that incorporate Python programs into LLM reasoning. MathCoder (Wang et al., 2024a) proposes a method of generating novel and high-quality datasets with math problems and their code-based solutions. As for the code-based verification and feedback, Zhou et al. (2024a) employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. Zhou et al. (2024b) autoformalizes informal mathematical statements into formal Isabelle code to verify the internal consistency. ART (Miao et al., 2024) introduces relation tuples into the reasoning steps and verifies them with code interpreter to provide feedback, finally improving reasoning accuracy. Different from these papers, we are the first to investigate the effectiveness of combining CoT and PoT solutions in verification and show promising results on both mathematical and code reasoning tasks."}, {"title": "5 CONCLUSION", "content": "In this paper, we address the challenge of improving reasoning verification in LLM by integrating CoT and Program-of-Thought PoT. Firstly, we collect a comprehensive binary dataset, derived from multiple LLM reasoners for both math and code reasoning tasks, providing a robust foundation for training verifiers. Next, through an extensive comparison of outcome reward models (ORMs) and preference-tuning methods, we identify that reference-free preference tuning, particularly SimPO, offers superior performance. Moreover, we introduce techniques to generate CoT/PoT based on their PoT/CoT counterparts for further verification. Our resulting verifiers, Math-Rev and Code-Rev, outperform existing baselines and achieve state-of-the-art results on benchmarks such as GSM8k and MATH. We believe this paper could serve as a strong baseline in reasoning verification and facilitate future studies on reasoning, verifying, reinforcement learning and related areas."}, {"title": "Limitation", "content": "While our approach demonstrates significant improvements in reasoning verification, it also comes with certain limitations. First, the sampling and re-ranking strategy introduces ad-ditional computational overhead compared to greedy decoding, which can be resource-intensive, especially when applied to large-scale datasets or deployed in real-time applications. Secondly, our verifier is based on an outcome reward model (ORM) that provides feedback at the solution level rather than at the step level. This solution-level granularity, while effective in overall verification, lacks the finer granularity of process reward models (PRMs) that evaluate each step of the reasoning path. PRMs can potentially offer more detailed feedback and facilitate more precise corrections, particularly in complex multi-step reasoning tasks. However, implementing step-level verification would require extensive process supervision data, which is expensive and challenging to scale."}, {"title": "A APPENDIX", "content": "In this experiment, we evaluated the performance of our Math-Rev verifier in identifying and high-lighting errors in mathematical solutions. Each column in the provided figure represents a math problem, including both a correct solution and a deliberately modified incorrect solution. We in-put both solutions into our Math-Rev verifier, and highlight tokens in the wrong solution with log probabilities less than -10 in red to indicate detected errors, as shown in Figure 6."}, {"title": "Figure 6: The figure illustrates two mathematical problems, each paired with a correct solution and", "content": "an intentionally incorrect solution. The Math-Rev verifier highlights errors in the incorrect solutions in red, effectively identifying and marking the parts with low log probabilities.\nFor the first problem, Math-Rev successfully identified the incorrect use of the multiplication opera-tor and also recognized the incorrect final answer, highlighting these segments in red. This indicates the verifier's sensitivity to mathematical operations and the final conclusion drawn from these oper-ations. In the second problem, the verifier detected the discrepancy in the calculations and identi-fied the deviation from the problem's requirements, marking the erroneous parts accordingly. This demonstrates Math-Rev's effectiveness in pinpointing computational errors and inconsistencies with problem statements."}, {"title": "Table 1: Performance improvement brought by the proposed CoTnPoT. The best performance each", "content": "row is highlighted. Green arrow denotes the percentage improvement over greedy decoding, bluearrow indicates the improvement over the baseline without CoTnPoT."}, {"title": "Our Verifiers Training.", "content": "Although those reference-tuning methods are primarily designated toalign LLMs with human preferences, they can also be adapted for training verifiers (Hosseini et al.,2024). By feeding the backbone LLM of the verifiers with pairs of correct and incorrect solutions,designated as chosen and rejected outputs, and applying the mentioned training methods, the verifiercan be trained to assign higher generation probabilities to correct solutions over incorrect ones. Thenthe probability can be served as a score for ranking solutions. In our paper, Math-Rev and Code-Revare trained separately by their respective training data with one of the preference-tuning methodsSimPO. We believe that such verifiers have a significant advantage over ORMs: it does not introduceadditional training parameters and not change the goal of generation for LLMs, aligning better withthe original usage of LLM."}, {"title": "During the inference stage, after deploying our Math-Rev and Code-Rev verifiers, we identify dis-", "content": "tinct challenges in verifying math and code reasoning. For math reasoning, while model-based ver-ifiers can effectively detect surface-level logical errors such as incorrect use of operators, numbers,and methods, they struggle to catch subtle mistakes such as calculation errors and deeper logicalinconsistencies. In code reasoning, the structured and abstract nature of code makes it difficult toread and understand, leading verifiers to assign similar scores to different solutions, indicating theirdifficulty in accurately identifying errors within the code.\nTo address these challenges, we propose a method called CoTnPoT, which enhances verification byleveraging the connection and complementary strengths of the Chain of Thought (CoT) and Programof Thought (POT) solution formats.\nFor math reasoning, we use an external LLM, DeepseekV2-chat-Lite (Zhu et al., 2024), to transformCoT solutions SCOT into PoT counterparts SPOT based on problem descriptions Q,"}, {"title": "4.1 SCALING UP INFERENCE-TIME COMPUTING", "content": "Cobbe et al. (2021) is the pioneering work that applies verifiers in mathematical reasoning, where they train token-level reward models to give scores on problem solutions. Then Uesato et al. (2022); Lightman et al. (2023) dive into the application of PRM - process reward models, where scores are assigned to each intermediate step of solutions, providing more fine-grained feedback. Math-Shepherd (Wang et al., 2023) and MiPS (Wang et al., 2024b) propose using Monte-Carlo Tree-Search (MCTS) to automate the data collection process instead of human labeling. OVM Yu et al. (2024a) employs outcome supervision for training a value model, which prioritizes steps that lead to accurate conclusions during inference. V-Star (Hosseini et al., 2024) presents an iterative frame-work in LLM training, which collects both correct data for supervised fine-tuning and wrong data for verifier training. They also showed that DPO is stronger than ORMs in verification. Built on reranking strategies such as verifiers, multiple studies Brown et al. (2024); Snell et al. (2024) found that scaling up inference-time computing is much more cost-effective than training. To achieve more effective and efficient inference-time verification, our approach samples solutions from various LLM reasoners and comprehensively compares different verifier training methods. Our best verifier Math-Rev achieves strong performance on math solution verification using only outcome-based labels in training and even outperforms PRM baselines."}, {"title": "5 CONCLUSION", "content": "In this paper, we address the challenge of improving reasoning verification in LLM by integrating CoT and Program-of-Thought PoT. Firstly, we collect a comprehensive binary dataset, derived from multiple LLM reasoners for both math and code reasoning tasks, providing a robust foundation for training verifiers. Next, through an extensive comparison of outcome reward models (ORMs) and preference-tuning methods, we identify that reference-free preference tuning, particularly SimPO, offers superior performance. Moreover, we introduce techniques to generate CoT/PoT based on their PoT/CoT counterparts for further verification. Our resulting verifiers, Math-Rev and Code-Rev, outperform existing baselines and achieve state-of-the-art results on benchmarks such as GSM8k and MATH. We believe this paper could serve as a strong baseline in reasoning verification and facilitate future studies on reasoning, verifying, reinforcement learning and related areas."}, {"title": "4.1 SCALING UP INFERENCE-TIME COMPUTING", "content": "Cobbe et al. (2021) is the pioneering work that applies verifiers in mathematical reasoning, where they train token-level reward models to give scores on problem solutions. Then Uesato et al. (2022); Lightman et al. (2023) dive into the application of PRM - process reward models, where scores are assigned to each intermediate step of solutions, providing more fine-grained feedback. Math-Shepherd (Wang et al., 2023) and MiPS (Wang et al., 2024b) propose using Monte-Carlo Tree-Search (MCTS) to automate the data collection process instead of human labeling. OVM Yu et al. (2024a) employs outcome supervision for training a value model, which prioritizes steps that lead to accurate conclusions during inference. V-Star (Hosseini et al., 2024) presents an iterative frame-work in LLM training, which collects both correct data for supervised fine-tuning and wrong data for verifier training. They also showed that DPO is stronger than ORMs in verification. Built on reranking strategies such as verifiers, multiple studies Brown et al. (2024); Snell et al. (2024) found that scaling up inference-time computing is much more cost-effective than training"}]}