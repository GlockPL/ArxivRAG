{"title": "How To Think About End-To-End Encryption and AI: Training, Processing, Disclosure, and Consent", "authors": ["Mallory Knodel", "Andr\u00e9s F\u00e1brega", "Daniella Ferrari", "Jacob Leiken", "Betty Li Hou", "Derek Yen", "Sam de Alfaro", "Kyunghyun Cho", "Sunoo Park"], "abstract": "End-to-end encryption (E2EE) has become the gold standard for securing communications, bringing strong confidentiality and privacy guarantees to billions of users worldwide. However, the current push towards widespread integration of artificial intelligence (AI) models, including in E2EE systems, raises some serious security concerns.\nThis work performs a critical examination of the (in)compatibility of AI models and E2EE applications. We explore this on two fronts: (1) the integration of AI \"assistants\" within E2EE applications, and (2) the use of E2EE data for training AI models. We analyze the potential security implications of each, and identify conflicts with the security guarantees of E2EE. Then, we analyze legal implications of integrating AI models in E2EE applications, given how AI integration can undermine the confidentiality that E2EE promises. Finally, we offer a list of detailed recommendations based on our technical and legal analyses, including: technical design choices that must be prioritized to uphold E2EE security; how service providers must accurately represent E2EE security; and best practices for the default behavior of AI features and for requesting user consent. We hope this paper catalyzes an informed conversation on the tensions that arise between the brisk deployment of AI and the security offered by E2EE, and guides the responsible development of new AI features.", "sections": [{"title": "1 Introduction", "content": "Widespread adoption of end-to-end encryption (E2EE) has improved the confidentiality and integrity of data in various contexts, including messaging, video, and voice communication. Leading applications such as iMessage [8], WhatsApp [142], and Signal [122] have integrated E2EE into their systems by default, bringing strong confidentiality and privacy to billions of users' communications. This trend appears poised to expand further, with an increasing number of services committing to the adoption of E2EE in order to ensure \"safer, more secure and private service\" to users [61, 53].\nIn an era where so many forms of communication have moved online, widespread E2EE has become a critical underpinning for the security of communications, providing critical protections for journalists, dissidents, and activists, as well as the everyday family and social communications of billions. As underscored by independent experts from the UN Office of the High Commissioner on Human Rights, strong encryption is essential to fundamental human liberties including free expression, the right to hold opinions, the right to privacy, and the right to assembly [86]. Beyond individual privacy, the ubiquitous nature of E2EE offers systemic protections for particular at-risk users, preventing their activities from appearing exceptional: widespread deployment of E2EE has made privacy the norm. This normalization of secure communications, in turn, makes it harder to distinguish whether individuals are engaged in frivolous or sensitive activities, and deters the targeting of individuals merely for using privacy technologies, resulting in a critical \"network effect\" [73] that creates the conditions necessary for journalists, human rights defenders, activists, and others to communicate securely with less fear of surveillance and repercussions.\nThe movement towards widespread E2EE communication stayed strong alongside the \"big data\" trends of recent decades: a recognition of the value of keeping private messaging data out of increasingly vast repositories of personal information, even as data was called the new oil by some [135]. Privacy technologies gained momentum as prominent examples of government and corporate intrusions on individual privacy came to light [72, 26, 70], and people became increasingly aware of their digital activity being tracked and profiled at large scale [18]. Those providing E2EE services were popularly seen as protectors of consumer privacy and freedom of speech [27]. Both corporate and government surveillance were key issues, but corporations often stepped into the \"protector\" role and deployed E2EE technologies that would permit nobody other than the users participating in a communication to access private communication content: a decision that made business sense at the time (e.g., [26]).\nFollowing remarkable recent advances and an explosion of interest in large language models and generative artificial intelligence (AI) more broadly, however, we observe three trends that raise alarms for E2EE security.\nFirst, the way people interact with AI models is changing. While they originally served as standalone applications, AI models are now increasingly incorporated into other everyday applications, including messaging applications, in the form of AI \"assistants.\" Interacting with these assistants is often baked into the user experience by default, made readily available as part of the application client (e.g., within a messaging app). Such integration creates systemic data flows at scale between previously separate systems, and accordingly raises security and privacy considerations not limited to the realm of E2EE.\nSecond, high-quality training data is becoming scarce. This has created a race between model developers, with tech companies under increasing pressure to tap any potential source of human-written content [139]. That which is publicly accessible online has already been harnessed, so privately held data is naturally the next resort, and indeed, companies have been quietly changing their terms of service to enable training AI models on more and more of the user data they hold [134]\u2014a practice about which the U.S. Federal Trade Commission has already raised concerns [5]. At the time of writing, E2EE messaging data on major existing platforms remains off limits for AI training, but companies may be incentivized to reconsider these policies in the near future. Apple has very recently launched AI assistant features that can process E2EE messaging content (see Section 2.5), and other companies may be considering similar options.\nThird, recent trends in AI appear to have shifted business incentives. Over the last decade, E2EE became a central part of the business model of online intermediaries. Business incentives to protect proprietary and user data were aligned with human rights protections. For example, in 2016, Apple and Facebook were lauded for their public resistance to government requests for access to E2EE data, including their refusal to comply with government requests to build systemic weaknesses into their encryption systems which would allow law enforcement access to messaging data [26]. However, in the last couple of years we have seen a shift towards"}, {"title": "1.1 Summary of contributions", "content": "We systematically analyze the above question from both a technological and a legal perspective, combining the expertise of an multidisciplinary research team across cryptography and security, AI and LLMs, and technology law.\nWe identify the key confidentiality and integrity properties provided by E2EE, for both individual and systemic privacy considerations, which might be adversely impacted by feeding E2EE content into AI models. This requires a novel conceptualization of key properties of E2EE, ranging from the strictly definitional, to realities of E2EE deployments in practice and their divergences from the strict definitions, to essential properties that only hold when E2EE is used at scale, to many important properties that are widely associated with E2EE but not technically within the scope of formal definitions.\nWe then examine a wide range of detailed technical configurations that could fall under the broad umbrella of \"feeding E2EE content to AI models,\" taking into consideration the state of the art in cryptography and privacy technologies, as well as the latest technical developments in AI. We distinguish the use of E2EE content for inference, for training, or for both. We assess the capacity of each technical configuration-that is, of each approach to integrating AI features into an initially E2EE system to uphold the key guarantees of E2EE. We conclude that some configurations cannot uphold these guarantees, while some others can.\nNext, we overview relevant areas of law, and provide a detailed analysis of the circumstances under which E2EE service providers are likely to be able to offer AI features which use E2EE content, consistent with their legal obligations under current US and EU law alongside a brief note on other jurisdictions. We highlight areas of legal uncertainty, and provide a detailed exposition of pending legal (or quasi-legal) processes where relevant. In addition, looking beyond the limits of existing law, we discuss relevant critiques of current law and diverse academic theories of consent and privacy (legal and otherwise).\nPutting together all of the above, our analysis yields the following key recommendations summarized here and elaborated in Section 7. Key terms are in italics; we provide detailed definitions in Section 3.\n1.  Training. Using end-to-end encrypted content to train shared AI models is not compatible with E2EE.\n2.  Processing. Processing E2EE content for AI features (such as inference or training) may be compatible with end-to-end encryption only if the following recommendations are upheld:\n(a) Prioritize endpoint-local processing whenever possible.\n(b) If processing E2EE content for non-endpoint-local models,\n(i) No third party can see or use any E2EE content without breaking encryption, and\n(ii) A user's E2EE content is exclusively used to fulfill that user's requests.\n3.  Disclosure. Messaging providers should not make unqualified representations that they provide E2EE if the default for any conversation is that E2EE content is used (e.g., for AI inference or training) by any third party.\n4.  Opt-in consent. AI assistant features, if offered in E2EE systems, should generally be off by default and only activated via opt-in consent. Obtaining meaningful consent is complex, and requires careful consideration including but not limited to: scope and granularity of opt-in/out, ease and clarity of opt-in/out, group consent, and management of consent over time."}, {"title": "2 Background I: Encryption, AI, and Trusted Hardware", "content": "This section covers technical background, focusing on end-to-end encryption (Section 2.1), AI assistants (Section 2.2), how it is possible for AI assistants to process E2EE content (Section 2.3), and trusted hardware (Section 2.4). Lastly, Section 2.5 gives a brief overview of Apple's recently deployed system in which E2EE content can be processed by AI assistants."}, {"title": "2.1 Background on End-to-End Encryption (E2EE)", "content": "Online messaging systems that allow communication between users (such as iMessage, WhatsApp, and Signal) are generally intermediaries to every communication. Although the user experience may make it seem like messages go directly from a sender S to a recipient R, in reality every message goes from S to the platform P and then from P to R. This means that, absent any additional guarantees, the platform has access to all communications of users on the platform.\nEnd-to-end encryption (E2EE) is a standard of security for communication in which only the sender and an intended recipient can read communications between them. In particular, even the platform P cannot read these communications, despite P still acting as the intermediary for communication: that is, communications still go from S to P then P to R.\nThe key tool used in E2EE is encryption. Encryption is a method for a sender S to \"scramble\" their communications in a manner which is easy for their intended recipient R (and S) to unscramble, but practically impossible for anyone else. Using encryption, S will scramble (or encrypt) each message for R before sending it to platform P, which only handles messages in scrambled form (and does not have the ability to unscramble messages). When R receives a scrambled message from S via P, they are able to unscramble the message (or decrypt it) and read the original message that S wrote."}, {"title": "2.1.1 The E2EE Confidentiality Guarantee", "content": "The core privacy guarantee of an E2EE system is called confidentiality. Informally, E2EE confidentiality states that plaintext messages handled by an E2EE system can only be seen by the sender and intended recipients: no one else, including law enforcement or the service provider, can learn anything about the plaintext messages [89]. Importantly, this guarantee is stronger than saying that nobody else can learn the plaintext messages: E2EE confidentiality requires that nothing can be learned about the content of an E2EE message, including the message itself, but also any derivative of the message content. Thus, E2EE confidentiality means that anyone besides the sender and intended recipient must not be able to learn, for example, what letter of the alphabet a message begins with, or whether the message contains a given word.\nE2EE confidentiality refers to a specific technical definition of confidentiality for messages, based on encryption-E2EE confidentiality (or E2EE security) is a term of art with a precise meaning, and does not ensure perfect \u201cconfidentiality,\u201d \u201cprivacy,\u201d or \u201csecurity\u201d for every colloquial meaning of the terms. In general use, these terms are very broad and mean different things in different contexts.\nConsider the context of messaging. The E2EE confidentiality guarantee is both stricter than one might expect, in some senses, and less strict in other respects. If strictly enforced, it is incompatible with many features common in messaging apps.\nStrictness of the E2EE confidentiality guarantee. A strict reading of the E2EE confidentiality guarantee would be incompatible common features such as link previewing, where an app displays a \"preview\" of a link that is sent by E2EE messaging. In this case, the link itself is E2EE message content, and in order to fetch and display the preview, a third party must process that link. Some E2EE applications only display link previews upon explicit user opt-in, but some have it on by default.\nPermissiveness of the E2EE confidentiality guarantee. The E2EE confidentiality guarantee covers only encryption of message contents, as noted above. For any other auxiliary information related to messaging, such as who is messaging whom and at what times, E2EE does not impose any requirements or provide any guarantees. To give a more complex example, in 2021, the Government of India introduced legislation mandating that messaging platforms provide message \"traceability,\" or methods for determining which user first shared particular content [129]. Which user first shared content is not the content itself, and this feature does not violate E2EE confidentiality.\nMany academic definitions of E2EE messaging consider a simple model where the only activity that occurs is literal messaging-sending messages between senders and receivers and within group chats. In practice, E2EE messaging apps-like any modern messaging app-are more richly featured (e.g., including options to back up your data and see link previews). How such additional features process message contents, and whether such processing respects E2EE confidentiality, has been a subject of long debate. Some common features of E2EE applications enhance confidentiality despite not being part of the basic definition of E2EE; other common features of E2EE applications may limit confidentiality (such as link previewing).\nNext, in Sections 2.1.2-2.1.3, we present a range of technical and non-technical properties often associated with E2EE applications in practice, but which are distinct from the basic E2EE confidentiality guarantee."}, {"title": "2.1.2 Systemic Properties of E2EE Systems", "content": "Even if an E2EE application provides strong technical privacy guarantees (including and beyond E2EE confidentiality), the strength of the privacy guarantee it offers will also depend on its usage in practice. Messaging applications that are widely used for diverse purposes offer additional protection because their use is unremarkable, and hence, the use of the application in itself does not draw attention. In economic terms, this is a network effect where the (privacy-related) utility of the application for any given user depends on other users. For example, suppose that a particular E2EE messaging app A happens to have been developed by a developer who is an outspoken member of a particular political community and has primarily advertised A in the social circles and publications of that community. Accordingly, A grows such that the userbase disproportionately represents members of this political community. Even if the application has no flaws whatsoever in providing E2EE confidentiality (and other properties), knowing that a particular person uses application A can divulge their political views, if only by association.\nThis type of example motivates the intuition that to realize the full privacy potential that E2EE can provide, the use of the application should be commonly used and unremarkable in the relevant context. This \"unremarkableness\" is not a technical property: it is instead an emergent social and contextual property of the system's use as a whole considered over the entire userbase (relevant to a context of use). Existing security and privacy literature has discussed similar network effects in privacy technologies not limited to encrypted messaging, and noted their critical importance to engineering effective privacy technologies [60].\nIn summary, there is a key privacy property of E2EE systems which cannot be achieved by technical means alone, but is attained by popular E2EE applications such as iMessage and WhatsApp today-namely, the network effect gained from wide usage, so that the use of the system in itself is unremarkable."}, {"title": "2.1.3 Auxiliary Security Properties of E2EE Systems", "content": "Besides the core guarantees mentioned so far in this section, there are additional (cryptographic) security properties that E2EE applications often provide in practice. While distinct from basic E2EE confidentiality, these properties have become very commonly associated with, or even expected of, modern E2EE applications. Notable examples include:\nForward secrecy: An adversary who has compromised the communication channel (e.g., by stealing the victims' secret session key) cannot recover messages that were transmitted before the session was compromised.\nPost-compromise security: An adversary that has compromised the communicate channel cannot recover messages that are transmitted after the protocol \u201cheals\u201d itself (e.g., after session keys are rotated).\nDeniability: No party can prove that a message was authored by any particular user of the system, even if the prover was one of the intended recipients of the message.\nUnlinkability: Knowledge of the fact that a user authored a particular message reveals no information about whether they authored other messages.\nState-of-the-art E2EE messaging protocols, such as the Signal protocol [122], meet these (and many additional) security properties."}, {"title": "2.1.4 Necessary Conditions for E2EE Confidentiality", "content": "End-to-end encryption provides its promised confidentiality guarantee only under certain conditions: namely, when (1) the underlying encryption method is secure and (2) that encryption method is securely implemented in the application being used.\nCryptographic assumptions. Every practical encryption scheme relies upon a hardness assumption: an assumption that a particular mathematical problem is difficult for computers to solve. The hardness of the mathematical problems underlying today's widely used encryption schemes has held strong in the face of heavy scrutiny over decades by mathematicians, computer scientists, and others, leading to a strong community belief that it is acceptable to use cryptography that relies on these assumptions in security-critical contexts. When new encryption schemes that rely on new, less-tested assumptions are proposed, they are not considered suitable for deployment until they have been vetted in a multi-year process of scrutiny by mathematicians and computer scientists worldwide.\nPractical assumptions. Even if the math underlying an encryption scheme is perfect in theory, the process of deploying the encryption scheme in software or hardware introduces avenues for security failures, and the use of the encryption scheme by fallible humans in practice introduces further potential for security failures. In order for E2EE confidentiality to hold: (1) the code implementing the E2EE application must be correct, (2) the hardware device that the application runs on must be secure, and (3) the users using the application must use it correctly and take adequate security precautions. Undermining any one of these three could undermine E2EE confidentiality: for example, (1) the software might fail to perform the encryption correctly; (2) the hardware device might fail to generate a secret key correctly, or it might be infected by malware; and (3) a user might fail to keep their secret key secret. Errors can be introduced in myriad ways: through simple mischance, human mistake, or intentional sabotage.\nTypically, in current practice, a messaging application is implemented by an intermediary platform that provides the associated messaging service. Thus, the party with the most control over and visibility into the implementation process is the platform. Users therefore rely on platforms for (1), i.e., that the code implements E2EE correctly. Users rely on device manufacturers (and their suppliers) for (2), i.e., that the device running the app is secure. Finally, the users themselves determine (3), i.e., their usage of the application; that said, users often lack the expertise to confidently take adequate security precautions."}, {"title": "2.2 Background on AI Assistants", "content": "AI assistants are programs designed to interpret everyday language and perform computational tasks. Today's AI assistants, such as ChatGPT, Gemini, and Llama, are able to handle a wide range of tasks, including text analysis, content creation, code generation, language translation, and more. At the core of these technologies are statistical models-programs trained on data to identify patterns. Specifically, AI assistants typically rely on large language models (LLMs), which are trained on vast datasets of text to generate outputs based on predictions of the next word. Due to the size of their training datasets and sophisticated architectures, LLMs can handle complex and previously unseen user inputs (queries) and provide contextually relevant responses."}, {"title": "2.2.1 Training and Inference", "content": "AI assistants undergo two key phases: training, when the model is built to identify patterns, relationships, and rules; and inference, when real-time responses are generated based on user inputs. Training itself often involves multiple stages, typically referred to as pretraining and fine-tuning. In pretraining, the model takes in vast, general datasets to learn structures and patterns in the data. Fine-tuning then refines this pretrained model by training it on additional, more specialized datasets, improving its accuracy and relevance in particular contexts. We describe these processes in further detail below.\nTraining. Models have configurable parameters which are numerical values that determine how outputs are generated. These parameters are used in calculations to predict outputs when the model is queried with an input. During training, the model processes large datasets such as books, websites, code repositories, and conversational datasets in the case of LLMs-to adjust (or \"tune\") its parameters. This tuning happens over many iterations of training, during which the model's parameters capture more and more refined patterns in the data. The finalized set of parameters constitutes the trained model, which can then be prompted with inputs (e.g., questions) to generate outputs (e.g., answers). Models at the scale of commercial AI assistants contain billions or even trillions of parameters trained over the course of months on high-performance computers.\nInference. Once trained, models can perform inference, during which they generate real-time responses to user inputs. When a user inputs a query to the model, it is processed through many steps using the model parameters to calculate an output. While inference is less resource-intensive than training, it still requires significant computational power and is typically handled by centralized servers."}, {"title": "2.2.2 Data Collection", "content": "Once the AI assistant is deployed, various types of data may be collected. This can include chat logs, which record user inputs (text, voice, or image data) and the system's responses. These historical records, even if kept by the platform, may or may not be visible to users. Additionally, metadata, such as timestamps, device type, language, and geographic region, as well as user preferences and settings, may also be collected and stored by the platform. The collection of this data serves multiple purposes, which can be broadly categorized as follows:\n1.  Ease of use: The data is used to enable certain features, such as maintaining conversation histories, or maintaining continuity within a session or across sessions, for an individual user.\n2.  Personalization: The data is used to identify usage patterns, such as query frequency and commonly accessed features, to tailor the system's functionality for an individual user.\n3.  Analytics: The data and usage patterns is aggregated across users to steer further improvements and developments of the model.\n4.  Model refinement: The data is used as training data to fine-tune the model and continuously improve its performance.\nThese categories are not mutually exclusive, as data may be collected to serve any or all of these purposes. The collection of user data is commonly enabled by default with collected user data transmitted and stored on centralized cloud servers. In some cases, some or all user data may remain local on the user's device, eliminating external transmission."}, {"title": "2.2.3 Architecture of AI Assistants in Messaging Platforms", "content": "Next, we overview the architecture of AI assistants integrated in messaging platforms. The architecture is how the tool is structured and has implications on how and where data is processed. First, it is important to distinguish between the messaging application, the AI assistant, and the model. The messaging application is the application installed on the user's device which facilitates communication between users and provides the interface for interacting with the AI assistant, e.g., WhatsApp, Messenger, and Signal. The AI assistant is the system that performs tasks such as answering queries, generating responses, or providing recommendations. At its core is the model-in this case, an LLM-the underlying computational engine that processes inputs and generates outputs based on patterns learned during training.\nWhen the messaging application gets an input from the user, this input is sent to the model in the form of a request. Once the request is processed and an output has been generated by the model, the output is sent back to the platform as a response. As such, while the messaging application itself will be installed on the user's device, the model need not also be directly on the device; network connections can facilitate this communication between the application and model.\nHere we differentiate between two types of architectures: cloud-based and local, illustrated in Figure 3.\nCloud-Based. Cloud servers are designed to handle large computation and storage demands. Given the demands of an LLM-recall that there may be trillions of parameters to be processed simultaneously, requiring very high levels of computation power as well as memory to store the parameters and data-typically, models are hosted on cloud servers. Cloud servers can carry thousands of high-performance computers (Graphics Processing Units or Tensor Processing Units), exceeding the capabilities of a typical laptop by many magnitudes.\nLocal. In limited scenarios, computations can occur locally-that is, directly on the user's device using lightweight versions of the model or specific components. Local processing ensures that data does not leave the device, but is only possible with smaller, less powerful models due to reduced processing power and storage capacity of local hardware."}, {"title": "2.2.4 Availability in E2EE Applications", "content": "AI assistants are already available in major E2EE applications as a standalone tools outside of conversations between users. For example, WhatsApp features a search bar that can be used both to search through conversations and to \"Ask Meta AI\". When the user inputs a search query, the results show matches from their conversation history (via a local search) as well as an option to invoke Meta AI with the search query. In this case, we do not describe Meta AI as being integrated with E2EE content, as only explicit Meta AI queries initiated by the user are processed by Meta AI, without any E2EE content. The availability of such non-integrated features in E2EE applications is outside the scope of our concern in this paper, which is focused on AI models processing E2EE content.\nIn contrast, integrating AI assistants as a feature within E2EE conversations would typically require the AI assistant to process E2EE content. For example, this could be in the form of allowing users to invoke the AI assistant within an ongoing conversation to provide contextually relevant support or responses."}, {"title": "2.3 How Is It Technically Possible for E2EE Data to Be Fed Into AI Assistants?", "content": "The definition of E2EE may make it sound like it should be impossible to feed E2EE content into a (cloud-based) AI assistant. How can an AI model have access to plaintext data, when E2EE guarantees that only the sender and recipient can access E2EE content in plaintext?\nCertainly, the cloud-based service provider has access to the encrypted message the ciphertext\u2014and could input that ciphertext to an AI assistant. In this case, however, the AI assistant would not be able to read any meaningful information from the ciphertext, and thus would not be able to produce a useful answer, either.\nThe current discourse around using E2EE data in AI assistants, therefore, is focused primarily on the idea of feeding readable plaintext versions of E2EE messaging data into AI assistants. Since only the sender and recipient devices in an E2EE system can read the plaintext messages, the sender or recipient devices would be the only ones who could provide the plaintext data to the AI assistants. This possibility appears to be primary focus of the ongoing discourse: that users' devices would be set up to send E2EE content to cloud-based AI assistants for processing.\nNothing about E2EE technically prevents this from happening-indeed, no technical measure would both be able to allow senders and recipients to read their own messages, and prevent them from showing them to anyone else. This is much like no matter how secretively you encode and deliver your hand-written letter personally to your friend, nothing you can do can stop your friend from showing the decoded letter to other people once they have it.\nBut then, what is the point of end-to-end encryption?. We have just explained that E2EE is a technology that allows transmission of messages in encrypted form so that nobody but the sender and intended recipient can read them, not even intermediary platforms. We have also explained that E2EE does not stop the sender and recipient devices from sending plaintext copies of the messages to anybody and that devices can do this in a way that is very hard to detect for average users.\nThen, is there any point to using E2EE? After all, our devices could be routinely sending plaintext copies of all our messages to the messaging platforms themselves, and E2EE would not stop this yet such a practice would undermine precisely the benefit that E2EE purports to provide.\nE2EE does still have a point, for a number of reasons. First, it provides important protections when service providers choose not to be able to read your messages, and implement E2EE in order to achieve this goal. Such a choice might be motivated by various reasons: it might provide a competitive edge, provide protection in the event of data breaches, or reduce compliance costs for companies. Once such a choice is made, the platform moreover has a reputational interest in making sure E2EE is achieving the stated goal that the platform not be able to read messages. Second, E2EE provides stronger protections in the case of popular messaging platforms that are widely used and therefore widely scrutinized by technical experts, including security and privacy experts. While subversions like sending plaintext copies of messages to the platform would be hard to detect for an average user, they would be much more easily detected under such scrutiny by experts and from the experts' scrutiny, average users benefit too. Third, E2EE provides even stronger protections in the case of popular messaging platforms that choose to openly disclose the details of how their system and security features work, for similar reasons. Balsa et al. [22] provide more detailed discussion of these and related issues around platforms implementing E2EE and other cryptography."}, {"title": "2.4 Background on Trusted Execution Environments (TEEs)", "content": "Trusted hardware refers to physical devices that are designed to ensure the confidentiality and integrity of computations, even if the owner of the hardware may be compromised. Trusted hardware is designed to create a secure area of computation, in which no information about the computation is released outside of the trusted hardware during execution of code, and that code will run exactly as specified. There are many types of technologies that fall under the broad umbrella of trusted hardware such as hardware security modules (HSMs), trusted platform modules (TPMs), and trusted execution environments (TEEs) - which offer different capabilities. Of these technologies, TEEs are the most relevant for the implementation of AI functionalities, as they are designed to run arbitrary programs; other technologies currently have more limited, bespoke use cases, such as key generation and storage. We thus focus on TEEs in this subsection, and in the rest of the paper.\nWe can conceputalize a TEE as a physical \"lockbox\", which is designed to execute arbitrary code such that no outside entities (including, for example, the operating system, other device processes, and third parties) can observe nor manipulate the state of the code as it is running. During initial setup of the TEE, a computer program-call it \u041f-is loaded inside the TEE. The TEE then has a single input/output interface, through it which receives inputs x from outside the TEE, computes the program II on the inputs, and outputs the result II(x) from the TEE. During execution of II, the TEE maintains some internal ephemeral state in memory, which stores temporary values during computation. TEEs are also often equipped with persistent storage, on which they can store data longer-term. See [121] for a formal treatment of TEE abstractions.\nTEE security goals. TEEs are intended to provide two core security guarantees. The first is confidentiality, which means that the internal state of the TEE should not be visible to any parties during execution of II; only x and I(x) are revealed to third parties. Similarly, the persistent storage of the TEE should always be kept secret. The second goal of TEEs is integrity, which means that II should be the exact program run by the TEE, which cannot be undetectably tampered with nor replaced by another program. Besides these two core guarantees, some TEE implementations have a third security goal, remote attestation, which means that the TEE should be able provide convincing evidence that a given program II is the code that is loaded inside the TEE. Critically, these guarantees should hold even in the presence of a compromised TEE owner (e.g., a cloud provider): using a number of hardware-level protections, TEEs are designed to be tamper-resistant (for example, TEE chips often have physical seals that are designed to detect attempts to open them, upon which the TEE's storage is immediately erased).\nThe goal of TEE design is to enable outsourcing of computations over user data to potentially untrusted (but more computationally powerful) environments, simply by loading the program inside a server-side TEE, and processing user data inside of it. As described so far, the inputs to and outputs from the TEE can be seen by the owner of the TEE. However, inputs and outputs can be hidden from the TEE owner with an additional step: encrypt inputs and outputs so that they can only be decrypted using secret key that is stored inside the TEE. Then, computations inside the TEE can still happen in plaintext, within the \"shield\" of the hardware protections provided by the TEE.\nTEE performance. Modern TEEs (such as Intel SGX [52] and AMD SEV [85]) are capable of supporting complex computations with practical overheads. For example, TEE-based security has been deployed in Signal's contact discovery mechanism [98], and even entire blockchains [111]. However, efficiency is not the only engineering consideration for TEEs: their design of strict isolation imposes important limitations on the programs that TEEs can run, since they cannot have non-trivial interfaces to the outside world. So, for example, TEEs generally cannot make Internet requests, and cannot rely on external sources of state."}, {"title": "2.4.1 Necessary conditions for TEE security", "content": "The security guarantees of TEEs naturally rely on the correct implementation of the TEE, and that the hardware has not been tampered with at any point in the TEE's life cycle (e.g., tamper-proof seals have not been removed, backdoors have not been inserted, etc). This requires trust in various actors across the hardware supply chain, from manufacturing, to assembly, to deployment. Further, users need assurance that the application provider is actually running a legitimate TEE on their servers. While this can be implemented via the TEE's attestation capabilities, the attestation service needs to leverage some trusted authority, such as the hardware provider.\nEven if the hardware is correctly implemented according to specification, the security of TEEs additionally relies on the specification itself being free of vulnerabilities. Unfortunately, there is a history of practical attacks on various TEE implementations [138, 67]. These attacks range across a variety of threat models-including adversaries that have physical access to the TEE, can launch co-located TEEs with arbitrary programs, control the system's boot process, etc-and the adversary's goal is to violate the confidentiality and integrity of the TEE, i.e., recover sensitive runtime state or tamper with the data in the TEE. Notable examples of attack vectors include physical and micro-architectural side-channels, which exploit shared caches [35], branch predictors [90], page faults [144], transient execution [136], and more. This history of attacks has prevented TEEs from reaching mainstream adoption for security applications."}, {"title": "2.4.2 TEES vs E2EE", "content": "While TEEs and encryption have similar security goals"}]}