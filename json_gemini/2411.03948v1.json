{"title": "LONG-FORM TEXT-TO-MUSIC GENERATION WITH ADAPTIVE\nPROMPTS: A CASE OF STUDY IN TABLETOP ROLE-PLAYING GAMES\nSOUNDTRACKS", "authors": ["Felipe Marra", "Lucas N. Ferreira"], "abstract": "This paper investigates the capabilities of text-to-audio\nmusic generation models in producing long-form mu-\nsic with prompts that change over time, focusing on\nsoundtrack generation for Tabletop Role-Playing Games\n(TRPGs). We introduce Babel Bardo, a system that uses\nLarge Language Models (LLMs) to transform speech tran-\nscriptions into music descriptions for controlling a text-to-\nmusic model. Four versions of Babel Bardo were com-\npared in two TRPG campaigns: a baseline using direct\nspeech transcriptions, and three LLM-based versions with\nvarying approaches to music description generation. Eval-\nuations considered audio quality, story alignment, and\ntransition smoothness. Results indicate that detailed mu-\nsic descriptions improve audio quality while maintaining\nconsistency across consecutive descriptions enhances story\nalignment and transition smoothness.", "sections": [{"title": "1. INTRODUCTION", "content": "Recent text-to-audio music generation models such as Mu-\nsicLM [1] and MusicGen [2] are capable of producing\nhigh-quality music in the audio domain that aligns with a\ngiven textual description. These models typically generate\nmusic autoregressively by predicting the next token from\na context window, which limits the size of the signal they\ncan model. While the context size is limited, these mod-\nels can generate longer signals by sliding a context win-\ndow through time. Regardless of this capability, they have\nmainly been evaluated with a fixed prompt and for rela-\ntively short music durations. For instance, MusicGen [2]\nwas evaluated considering 30-second music pieces, each\ngenerated from a single music description. In this paper,\nwe are interested in evaluating whether text-to-music mod-\nels can maintain music quality while generating long music\npieces, where music descriptions change over time.\nIt is important to evaluate text-to-music models consid-\nering long music pieces (greater than 30 seconds, for ex-\nample) because many music production scenarios involve\nmusic durations longer than one can generate with a single\nshort audio context window (e.g., pop music composition,\njazz improvisation, soundtrack generation). One key prob-\nlem of generating long sequences from a small context is\nthat a model has to split the generation into multiple parts,\nensuring that the independent parts are smoothly connected\nin the final composition. Moreover, one might change the\ninitial prompt at any time step, steering the composition in\na different direction, and the model must consider both the\nprevious audio context and the new prompt.\nIn this paper, we investigate long generation with text-\nto-audio models in the context of Tabletop Role-Playing\nGames (TRPGs). In this scenario, a music generator takes\nspeech as input and must generate music that matches the\nstory being told by the players. We chose this problem be-\ncause it inherently poses the challenge of long music gen-\neration, where prompts have to change over time to adjust\nfor different story scenes. We also use TRPGs as a research\nobject because TRPG players often enhance their gaming\nexperience by manually selecting songs to play as back-\nground music [3], which allows us to compare the results\nof a generator against a human baseline."}, {"title": "2. RELATED WORKS", "content": "This section reviews audio-based text-to-music models and\nprevious soundtrack generation approaches, focusing on\nbackground music generation for TRPGs."}, {"title": "2.1 Text-to-Music Models", "content": "Text-to-music is the task of generating music pieces from\nmusic descriptions in textual format, e.g., \"70s punk rock\nsong with fast tempo\". In recent years, various neural\nmodels have been proposed to solve this problem in the\naudio domain [1, 2, 6]. For example, MusicGen [2] is an\nautoregressive transformer that operates on quantized au-\ndio units produced by the EnCodec [7] audio tokenizer. It\ncan be conditioned on textual descriptions using various\ntext encoding methods (e.g., T5 [8], FLAN-T5 [9], and\nCLAP [10]), or on melodic structures through an unsuper-\nvised approach utilizing chromagram information.\nMusicLM [1] is another text-to-music model that ex-\ntends AudioLM's [11] multi-stage autoregressive model-\ning approach by incorporating text conditioning, which is\nachieved by leveraging MuLan [12] to project music and\ntextual descriptions into a shared embedding space. Mo\u00fb-\nsai [6] employs a two-stage cascading diffusion approach,\nwhere the first stage utilizes a novel diffusion magnitude-\nautoencoding (DMAE) technique to train a music encoder\nthat compresses audio into a reduced representation. In\nthe second stage, Mo\u00fbsai implements text-conditioned la-\ntent diffusion (TCLD) to generate this reduced represen-\ntation while conditioning on textual descriptions, enabling\nthe model to produce music that corresponds to given text\ninputs. Other examples include commercial models such\nas Suno [13], Mubert [14], and Riffusion [15]."}, {"title": "2.2 Soundtrack Generation", "content": "Soundtrack generation has been investigated in different\nmediums such as films [5], video games [16], stories [17],\nand others. This problem has been mainly studied in the\nsymbolic domain. For example, Bardo Composer [4] gen-\nerates background symbolic music for TRPGs by tran-\nscribing the players' speeches into text at every time step t\nand feeding these transcriptions to a music classifier. The\nemotion given by this classifier is used to condition an au-\ntoregressive model that generates music by decoding a se-\nquence using a variation of Stochastic Beam Search. Ba-\nbel Bardo is similar to Bardo Composer because it also\nuses transcriptions of players' speeches to condition the\nmusic generation. However, it generates music in audio\nformat by conditioning a text-to-music model with a mu-\nsic description at every time step t, instead of using a music\nemotion classifier to condition an autoregressive symbolic\nmusic model.\nAnother important related work is Herrmann-1 [5],\nwhich combines an LLM and a text-to-music model to\ngenerate background music for movie scenes. Herrmann-\n1 uses BLIP2 [18] and CLIP [19] to extract a textual de-\nscription and the affective characteristics of the video, re-\nspectively. These characteristics are then provided as in-\nput to GPT-4 [20], which generates a description of an\nappropriate music with the given characteristics. Finally,\nthe description generated by GPT-4 is passed to Music-\nGen [2], which produces the background music in audio\nformat. Babel Bardo is similar to Herrmann-1, because it\nalso employs an LLM to generate music descriptions for\na text-to-music, however, it takes text as input instead of\nvideos. Moreover, in Babel Bardo, the music descriptions\nchange over time, whereas Herrmann-1 uses a single de-\nscription for each video."}, {"title": "3. BABEL BARDO", "content": "In TRPGs, players collaboratively construct a narrative\nthrough iterative cycles of scene descriptions, decision-\nmaking, and action resolutions. The game master presents"}, {"title": "4. EXPERIMENTS AND RESULTS", "content": "We evaluate Babel Bardo\u00b9 in the task of soundtrack gen-\neration for two different TRPG campaigns: Call of the\nWild (COTW) and O Segredo da Ilha (OSNI). The for-\nmer is a Dungeons & Dragons campaign played in Ameri-\ncan English and the latter is in Brazilian Portuguese. Both\nof them were played on YouTube. We used COTW be-\ncause it was also used to evaluate Bardo Composer [4].\nWe've also included OSNI to evaluate Babel Bardo's per-\nformance with a Latin American language. COTW is com-\nposed of 11 episodes, with a total of 6 hours and 37 min-\nutes of gameplay-each episode is approximately 33 min-\nutes long. OSNI is composed of 6 episodes, with a total\nof 26 hours and 22 minutes of gameplay each episode is\napproximately 4 hours and 20 minutes long.\nWe measured the performance of Babel Bardo with re-\nspect to three objective metrics: audio quality, alignment\nwith the story, and transition smoothness between tran-\nscriptions. Audio quality was measured using Fr\u00e9chet Au-\ndio Distance (FAD) [21], which compares statistics com-\nputed on a set of reconstructed music clips to background\nstatistics computed on a large set of studio-recorded music.\nFollowing the approach of Hermann1 [5], we collected 32\nhours of high-quality cinematic soundtracks as reference\nstudio-recorded music. Alignment with the story was cal-"}]}