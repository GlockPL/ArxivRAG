{"title": "Mechanistic Interpretation through Contextual Decomposition in Transformers", "authors": ["Aliyah Hsu", "Yeshwanth Cherapanamjeri", "Anobel Y. Odisho", "Peter R. Carroll", "Bin Yu"], "abstract": "Transformers exhibit impressive capabilities but are often regarded as black boxes due to challenges in understanding the complex nonlinear relationships between features. Interpreting machine learning models is of paramount importance to mitigate risks, and mechanistic interpretability is in particular of current interest as it opens up a window for guiding manual modifications and reverse-engineering solutions. In this work, we introduce contextual decomposition for transformers (CD-T), extending a prior work on CD for RNNs and CNNs, to address mechanistic interpretation computationally efficiently. CD-T is a flexible interpretation method for transformers. It can capture contributions of combinations of input features or source internal components (e.g. attention heads, feed-forward networks) to (1) final predictions or (2) the output of any target internal component. Using CD-T, we propose a novel algorithm for circuit discovery. On a real-world pathology report classification task: we show CD-T distills a more faithful circuit of attention heads with improved computational efficiency (speed up 2x) than a prior benchmark, path patching. As a versatile interpretation method, CD-T also exhibits exceptional capabilities for local interpretations. CD-T is shown to reliably find words and phrases of contrasting sentiment/topic on SST-2 and AGNews datasets. Through human experiments, we demonstrate CD-T enables users to identify the more accurate of two models and to better trust a model's outputs compared to alternative interpretation methods such as SHAP and LIME.", "sections": [{"title": "1. Introduction", "content": "Transformers (Vaswani et al., 2017) have recently demonstrated impressive predictive capabilities (Brown et al., 2020) by learning intricate nonlinear relationships between features. However, the challenge of comprehending these relationships has resulted in transformers largely considered as black boxes. Despite this, transformers are increasingly utilized in high-stakes domains such as medicine (e.g. medical image analysis (He et al., 2023)) and science (e.g. protein structure prediction aiding drug discovery (Jumper et al., 2021)). This underscores the necessity of understanding and anticipating potential model behaviors. To strengthen trust in the deployment of advanced black-box models like transformers, researchers emphasize the urgent need for reliably interpreting them (Hendrycks et al., 2023; R\u00e4uker et al., 2023) to mitigate risks and address issues like fairness (Nemani et al., 2024). Mechanistic interpretability, a study to explain behaviors of machine learning (ML) models in terms of their internal components, is at the frontier of interpretability research (Geiger et al., 2021; Geva et al., 2020; R\u00e4uker et al., 2023), as it uniquely provides an avenue for guiding manual modifications (Elhage et al., 2021; Vig et al., 2020) and reverse-engineering solutions (Elhage et al., 2022; Meng et al., 2023).\nIn this work, we introduce contextual decomposition for transformers (CD-T) \u00b9, a novel interpretation method that enables mechanistic interpretation. It explains contributions of combinations of input features or source internal components (e.g. attention heads, feed-forward networks) to (1) final predictions or (2) the output of arbitrary target internal component, without any modifications to the underlying model. Our proposed method, CD-T, is a general technique that can be applied to a wide range of standard transformer-based models and data types.\nThis work consists of three novel contributions. First, the development of CD-T, which is a generalization of CD, a previous method for obtaining importance scores for CNNs and RNNs (Murdoch et al., 2018; Singh et al., 2018), to transformers. As transformers became a dominant deep learning (DL) architecture in state-of-the-art applications, this generalization ensures CD-T benefits a broader group"}, {"title": "2. Related Work and Connections to Our Work", "content": "Interpreting deep neural networks (DNNs) is a growing field (Adadi & Berrada, 2018; Murdoch et al., 2019; R\u00e4uker et al., 2023) which encompasses a broad set of techniques including adversarial techniques (Carmichael & Scheirer, 2023; Chen et al., 2019), input attribution methods (Sundararajan et al., 2017; Ribeiro et al., 2016), mechanistic interpretability methods (Zhao et al., 2020; Wang et al., 2023), and others (Mao et al., 2019; Hsu et al., 2023). Our work focuses on both local interpretations (i.e. interpret individual predictions made by a DNN) and mechanistic interpretability (i.e. explain behaviors of DNNs in terms of their internal components).\nLocal interpretation The majority of previous research has concentrated on attributing local importance to individual features, such as pixels in an image or words in a document. Various methods exist to assign feature-level importance for different architectures, including gradient-based (Sundararajan et al., 2017; Springenberg et al., 2014; Selvaraju et al., 2016; Baehrens et al., 2009), decomposition-based (Murdoch & Szlam, 2016; Shrikumar et al., 2016; Bach et al., 2015) and others (Dabkowski & Gal, 2017; Fong & Vedaldi, 2017; Ribeiro et al., 2016; Zintgraf et al., 2017). Ancona et al. (2017) and Lundberg & Lee (2017) discussed rigorously the similarities among the methods.\nSpecifically for LSTMs, Murdoch et al. (2018) highlighted the shortcomings of previous interpretation methods relying on word-level scores. They introduced contextual decomposition (CD), an algorithm capable of extracting feature interactions learned by LSTMs by generating phrase-level importance scores. Singh et al. (2018) extended CD to RNNs and CNNs, and proposed an hierarchical interpretation method using feature clustering. To capture feature interactions in transformers, Tsang et al. (2020) proposed an axiomatic feature attribution framework and Hao et al. (2020) proposed a self-attention attribution method.\nHowever, no existing work is versatile enough to be able to provide individual prediction analysis while enabling mechanistic interpretability to interpret interactions of internal components in DNNs. Such interpretation tool would benefit practitioners greatly because of its diverse use cases. To address this problem, this work introduces CD-T as a principled way to, for the first time, provide both local interpretation and mechanistic interpretability for transformers, a critical DL architecture in state-of-the-art applications."}, {"title": "3. Our Method: CD-T and Mechanistic Interpretability", "content": "In this section, we describe the extension of contextual decomposition to transformers, CD-T, and its use in mechanistically interpreting the behavior of these models. When given a set of source and target activations in the network,\nCD-T produces a decomposition of the target activations into two components one reflecting the contributions of the source activations and the other, the contributions of the rest of the network. While we describe CD-T specifically for BERT-based models, it generalizes straightforwardly to more general attention-based models including decoder-only models such as GPT-4. Since contextual decompositions are computed by propagating a decomposition of the input through the nodes of the network, our primary contribution here will concern with the propagation of an input decomposition through the (self-)attention module. This is because all other modules in a typical transformer (such as linear transformations and the application of element-wise non-linear activation functions) have been addressed by prior work (Murdoch et al., 2018).\nFor the rest of the section, we first recall the basic operations of BERT-based models (Section 3.1). We then present the extension of the contextual decomposition framework to transformers (Section 3.2) before concluding with a description of its use in a circuit building algorithm for mechanistic interpretability (Section 3.3).\n3.1. Transformers\nIn BERT-based models, the input is typically represented as a sequence of tokens where the first token is always the classification token, usually denoted by [CLS]. For each token in the sequence of length l, {ti}=1, a d-dimensional embedding is constructed {x}_1 with xi \u2208 Rd. This embedding encodes information from the value of the token ti as well as its position in the sequence i. These embeddings are then propagated through a series of encoder modules where each encoder module takes as input a sequence of embeddings {x}=1 and outputs another sequence {x}}=1 with x \u2208 Rd. Since, the number and dimensionality of the token embeddings remain constants through any encoder module, a series of these may be applied to obtain progressively more sophisticated representations of the input sequence. The key component that enables sharing of information across the various tokens in the sequence is the self-attention module which we describe in detail. The self-attention module comprises NA independent attention heads with each one taking as input a sequence {x}}=1 with xi \u2208 Rd and producing an output {y}=1 with y\u00bd \u2208 Rda where da = d/NA. For each attention head, a, there exists key fk : Rd \u2192 Rk, query fq : Rd \u2192 Rk, and value fv: Rd \u2192 Rda. In most transformer models, these are either simple linear transformations or a linear transformation followed by a position-wise non-linearity. The output of the"}, {"title": "3.2. Contextual decomposition for transformers (CD-T)", "content": "We will now describe our implementation of Contextual Decomposition. Contextual Decomposition (CD) propagates a decomposition of the input (or of the activation of the transformer at any layer of the transformer) through the model. Formally, given a decomposition of an input vector x = \u03b2 + y \u2208 Rd where \u03b2 represents the relevant portion and y the irrelevant portion, Contextual Decomposition defines a set of rules which determine the decomposition of the output of a module f : Rd \u2192 Rk which operates on x. For instance, for the case of element-wise ReLU activation function, the output decomposition of Contextual Decomposition (f(x) = \u03b2\u00ba + \u03b3\u00ba) is defined as follows:\n\u03b2\u00b0 = 1/2([ReLU(\u03b2)] + [ReLU(\u03b2 + \u03b3) \u2013 ReLU(7)])\n\u03b3\u00b0 = 1/2([ReLU(\u03b3)] + [ReLU(\u03b2 + \u03b3) \u2013 ReLU(\u03b2)])\nWe refer the interested reader to Murdoch et al. (2018) for decompositions of other modules such as linear transformations. As previously explained, the only module not accounted for in the context of transformers is the self-attention module described in Section 3.1.\nIn the context of transformers, we assume a decomposition of the input to the attention head, {x\u2081 = Bi + Vi}i=1 where Bi and Yi denote the relevant and irrelevant portions of the input. We index the decomposition with the position in the sequence for ease of presentation. We compute the decomposition of the output of the attention head {yi = Ba + vi}=1 as follows:\nVi \u2208 [1] : fv(xi) = \u03b2 + v\u00ec\nVi \u2208 [1] : \u03b2\u00ba = \u03a3\u03b1\u03ca, \u03b2), Vi\nl\n\u03b3\u03b5 = \u03a3\u03b1\u03ca\u03b3\nj=1\nNote that we do not decompose the attention weights, ai,j, into relevant and irrelevant components. While it is possible to do so within the framework, we found that a simple"}, {"title": "3.3. Mechanistic interpretation with CD-T", "content": "Having formally defined CD-T in Section 3.2, we now introduce a novel, computationally efficient algorithm for mechanistic interpretation in transformers via circuit discovery using CD-T. Before delving into our algorithm, we first formally define a circuit. If we view a model as a computational graph M, where nodes are activations of the model components in its forward pass (e.g. input embeddings, attention heads) and edges are the interactions between those components (e.g. an attention module, position-wise feed-forward networks), a circuit C is a subgraph of M responsible for the behavior of some component of the network, such as the output logits of a prediction task. Given an input x, similarly as to how the entire model defines a function M(x) from inputs to logits, we also associate each circuit C with a function C(x), defined by ablating away the effect of all components in M\\C (i.e. the components not included in C) leading up to the target component of the circuit C. This method improves the interpretability of the entire model by distilling it into a small-sized circuit which nevertheless, faithfully explains most of its behavior.\nNext, we present our algorithm for constructing circuits with CD-T where we focus specifically on computing circuits with the output logits as the target component. Our algorithim, starting from the output logits of the network, constructs a circuit by iteratively identifying vital internal components through the various layers of the network. In each iteration, we define a source component s and a target set of receivers R, a set of internal components (for instance, these are the output logits in the first iteration), and our goal is to measure the direct effect of s on R. Here, we impose the restriction that all components in R have to be downstream of s i.e. we only search for influential s in the same layer or layers before (with layer index smaller than or equal to) the components in R. In path patching (Wang et al., 2023), a prior method proposed for circuit discovery in GPT-2 small, this is achieved by ablating s with its mean response, computing the resulting activations of R after this change (with one inference pass), and measuring the difference in the output logits with another inference pass after substituting the activations of R with the new values just computed. In other words, path patching requires two passes of inference runs to measure the direct effect on R for just one s, which could lead to large computational costs"}, {"title": "4. Experimental Results", "content": "We now present empirical validation of CD-T on BERT-based models. In Sec. 4.2, we distill an attention head circuit using CD-T in BERT trained on real-world pathology reports. Our algorithm is then evaluated against a prior benchmark called path patching (Wang et al., 2023), qualitatively through circuit visualizations, and quantitatively through comparisons of computational efficiency and faithfulness (how much full model performance can a circuit achieve for the task). In Sec. 4.3 and Sec. 4.4, we focus on CD-T's ability to provide local interpretations for BERTs trained on SST-2 and AGNews. The two datasets were chosen to demonstrate CD-T's capabilities on tasks with different levels of difficulty, with SST-2 being a simpler binary classification task with shorter samples, and AGNews a harder classification task with longer texts and diverse topics.\n4.1. Experimental setups\nTo understand the utility of CD-T for real-world use cases in critical domains such as medicine, we collected a corpus of 2907 structured pathology reports under an institutional review board (IRB) approval. The corpus includes pathology reports for patients that had undergone radical prostatectomy for prostate cancer at the University of California, San Francisco (UCSF) from 2001 to 2018. The reports contain an average of 471 tokens, much longer than samples in SST-2 or AGNews. Our pathology reports dataset is not publicly available due to protected patient information; however, we provide a few anonymized samples in Appendix A.1 as illustrations. We fine-tune an uncased base BERT model on primary Gleason score classification using standard best practices (See Appendix A.2 for fine-tuning details), and the model attains an accuracy of 85.8%.\nIn the circuit discovery experiment, we obtained mean activations of all components in the model for mean ablations by averaging over 500 pathology report samples. To ensure stability, we extracted 20 candidate circuits 2 by running Alg. 1 on 20 randomly selected report samples, and a final circuit was determined by groups of attention heads that appear with the highest frequency among the candidate circuits for each level. In this paper, we show results from extracting 6 attention heads for each level of the circuit by setting N = 6 in Alg. 1. We experimented with N = 1, 3, 6, and empirically found setting N = 6 yielded a more stable circuit composition.\nFor SST-2 and AGNews, we use the fine-tuned models initialized with uncased base BERT that are available on TextAttack (Morris et al., 2020). They attain accuracies of 92.4% and 95.1% separately. The weakened models for the human evaluations are obtained from the original models by randomly permuting a small percentage of their weights, following a similar setup as in Singh et al. (2018). For SST-2 and AGNews, 5% and 10% of weights are randomized, reducing test accuracy from 92.4% and 95.1% to 60.9% and 66.7%."}, {"title": "4.2. Discovering circuits of attention heads in transformers", "content": "In this experiment, we focus on building a circuit of attention heads for a real-world primary Gleason classification task, and evaluate our algorithm against a prior method, path patching (Wang et al., 2023), qualitatively on circuit visualizations, and quantitatively on computational efficiency as well as circuit faithfulness (how much of a full model's performance can a circuit account for).\n4.2.1. CIRCUIT VISUALIZATIONS\nAfter distilling the final circuit in the fine-tuned BERT for primary Gleason classification, we investigate the functionality of attention head groups at each level of the circuit qualitatively by inspecting word clusters each of the attention head group pays most attention to. In Wang et al. (2023), this is done in a more nuanced fashion with positions information also taken into account, on their custom indirect object identification dataset, which comes with word-level labels. However, our pathology reports dataset is a more general case without the rigid structural restrictions of Wang et al. (2023) and such word-level labels are not available. As a remedy, we introduce a novel procedure to aid the interpretation of attention head groups in such general cases.\nGiven a group of attention heads from a level in the final circuit, we first compute their average attention map, standardize the map, and select words with attention scores that are 23 standard deviations higher than the mean. Next, we convert the selected words to their word2vec (Mikolov et al., 2013) embeddings and run k-means clustering after performing PCA on the embeddings to obtain influential word clusters for the given attention head group. We visualize the final circuit obtained using CD-T and its influential word clusters in Fig. 1."}, {"title": "4.3. Identifying top-scoring phrases", "content": "Mechanistic interpretation while providing more information, is often challenging due to the range of user choices in their definition which frequently require sophisticated un-"}, {"title": "4.4. Human experiments", "content": "In this section, we demonstrate through human experiments that CD-T allows users to better trust and reason about the accuracy of transformers. Human subjects consist of eleven graduate students at the author's institution, and all of them have a research background in ML. Each of the human subjects was asked to fill in a survey with two types of questions: (1) whether, using a given interpretation method, they could identify the more accurate of two models, and (2) whether the method led them to trust a model's output, following a similar protocol as prior work (Singh et al., 2018). These two types of questions were asked on two datasets: SST-2 and AGNews, and CD-T was compared against three baselines: LIME (Ribeiro et al., 2016), SHAP (Lundberg & Lee, 2017), and Integrated Gradients (Sundararajan et al., 2017). The exact survey prompts can be found in Appendix A.4.\n4.4.1. IDENTIFYING AN ACCURATE MODEL\nIn this experiment, to avoid variance due to samples, we collected the same two sets of samples that were used across all interpretation methods from the two datasets. For each question, we presented two visualizations of a given interpretation method (one generated from the model with higher predictive accuracy, and the other from the weakened version of that same model), and a subject was asked to identify which of the two visualizations were from the more accurate model. Each subject was asked to make this comparison for each combination of dataset and interpretation method, for 24 total comparisons. The samples shown were chosen to maximize disagreement between models: for each question, only either the first model predicts correctly or the second model predicts correctly.\nFig 2A shows the results of the survey. For both SST-2 and"}, {"title": "Limitations", "content": "Despite having shown both qualitative and quantitative evidence of the benefits of CD-T, our results is limited in scale and to the datasets and prior interpretation methods evaluated. More work is needed to generalize these findings to a broader set of models, datasets, and interpretation methods. Although our proposed algorithm for building circuits using CD-T works for constructing circuits of any internal components, we only discussed and interpreted circuits built with purely attention heads to be comparable with prior methods. Circuits built with different/heterogeneous internal components (e.g. feed-forward networks, layer norms) can be a promising direction for further investigation. Finally, our proposed algorithm for circuit discovery is limited in that it requires manual effort to define the number of attention heads to extract for each level, and that it extracts a fixed number of attention heads for every level in the circuit. A fully automated and more flexible circuit discovery algorithm is an important direction for future work."}, {"title": "5. Conclusions", "content": "In this work, we adapted contextual decomposition to transformers (CD-T), and proposed a novel algorithm for circuit discovery using CD-T to computationally efficiently enable mechanistic interpretability. Our proposed algorithm is agnostic to transformer types and is able to construct circuits of arbitrary internal components in a model. On a real-world pathology reports dataset, we demonstrate that the attention head circuit built using CD-T is not only computationally more efficient (speed up 2x) but more faithful (achieves 46% of full model performance with only 0.04% of attention heads in the full model, compared with 41.9% using 0.03% of the attention heads) than a prior mechanistic interpretation method, path patching.\nAdditionally, we propose a pipeline to interpret the extracted circuits by capturing influential word clusters for each group of attention heads in the circuit in an unsupervised fashion. The result reveals, first, attention heads at different levels of the circuit typically focus on the same three groups of concepts: punctuation/numbers, biomedical terms, and helper words (i.e. words that often exist in the vicinity of actual Gleason scores, such as 'grade' and 'pattern'), and second, attention heads closer to the output logits, at layers 11 or 10, focus more on helper words and punctuation, and that it was only until backtracking to attention heads at layers 7-9 that we start to see Gleason scores (e.g. '3', '4', '5') become influential. Circuit visualization obtained from path patching overall exhibits the same trend as what we see in the circuit obtained using CD-T, except that the middle level attends much less medical terms relevant to prostate cancer. From the circuit analysis, we demonstrate how CD-T helps disentangle different aspects of knowledge about the reports encoded in attention heads, and the hierarchy of the knowledge learned by the model.\nFinally, with CD-T being a versatile interpretation method, we showcase its capability for local interpretations both qualitatively and quantitatively on two datasets, SST-2 and AGNews. We first show CD-T is able to reliably find words and phrases of contrasting sentiment/topic on SST-2 and AGNews. Through human experiments, we demonstrate CD-T enables users to identify the more accurate of two models and to better trust a model's outputs compared to alternative interpretation methods such as SHAP and LIME."}, {"title": "6. Impact Statement", "content": "The proposed algorithm, Contextual Decomposition for Transformers (CD-T), provides a general mechanistic interpretation method for deep neural networks called transformers, which are the fundamental architectures in chatGPT and GPT4. CD-T has good computational efficiency, and can be used to understand the inner workings of transformers for human understanding and inspection, in order to help ensure safety of deep learning models in AI, especially in high-stakes areas such as medicine and cyber-security"}]}