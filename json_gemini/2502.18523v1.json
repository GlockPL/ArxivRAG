{"title": "End-to-End Deep Learning for Structural Brain Imaging: A Unified Framework", "authors": ["Yao Su", "Keqi Han", "Mingjie Zeng", "Lichao Sun", "Liang Zhan", "Carl Yang", "Lifang He", "Xiangnan Kong"], "abstract": "Brain imaging analysis is fundamental in neuroscience, providing valuable insights into brain structure and function. Traditional workflows follow a sequential pipeline-brain extraction, registration, segmentation, parcellation, network generation, and classification-treating each step as an independent task. These methods rely heavily on task-specific training data and expert intervention to correct intermediate errors, making them particularly burdensome for high-dimensional neuroimaging data, where annotations and quality control are costly and time-consuming. We introduce UniBrain, a unified end-to-end framework that integrates all processing steps into a single optimization process, allowing tasks to interact and refine each other. Unlike traditional approaches that require extensive task-specific annotations, UniBrain operates with minimal supervision, leveraging only low-cost labels (i.e., classification and extraction) and a single labeled atlas. By jointly optimizing extraction, registration, segmentation, parcellation, network generation, and classification, UniBrain enhances both accuracy and computational efficiency while significantly reducing annotation effort. Experimental results demonstrate its superiority over existing methods across multiple tasks, offering a more scalable and reliable solution for neuroimaging analysis.", "sections": [{"title": "Introduction", "content": "The human brain, with its billions of interconnected neurons that form the connectome, is the foundation of our cognitive functions and behaviors. Understanding this intricate connectivity is crucial for decoding the brain's mechanisms in development and degeneration. However, accurately mapping the connectome remains a significant challenge due to limitations in current methods. Traditional workflows rely on structural or functional neuroimaging data processed through fragmented steps-brain extraction, registration, segmentation/parcellation, and network generation-often requiring manual quality control, which is costly and represents a critical barrier for quantitative brain biomarkers to enter clinical practice. Furthermore, piecemeal approaches prevent simultaneous optimization of interdependent stages, leading to inefficiencies and limiting the discovery of nuanced connections. Errors introduced in earlier steps propagate through subsequent analyses, resulting in potentially misleading interpretations of brain dynamics. Moreover, the time-intensive nature of these workflows hinders scalability and efficiency.\nInstead of tedious, step-by-step processing for brain imaging data, recent studies support transforming these pipelines into deep neural networks for joint learning and end-to-end optimization (Ren et al. 2024; Agarwal et al. 2022). While several approaches have been proposed such as joint extraction and registration (Su et al. 2022b), joint registration and parcellation (Zhao et al. 2021; Lord et al. 2007), and joint network generation and disease prediction (Campbell et al. 2022; Mahmood et al. 2021; Kan et al. 2022a)-there is currently no framework that unifies and simultaneously optimizes all these processing stages to directly create brain networks from raw imaging data. Mapping the connectome of human brain as a brain network (i.e., graph), has become one of the most pervasive paradigms in neuroscience (Sporns, Tononi, and Kotter 2005; Bargmann and Marder 2013). Representing the brain as a graph of nodes (regions) and edges (structural or functional connections) en-"}, {"title": "Related Works", "content": "In the literature, related tasks in brain imaging analysis have been extensively studied. Conventional methods primarily focus on designing methods for brain extraction (Kleesiek et al. 2016; Lucena et al. 2019), registration (Sokooti et al. 2017; Su et al. 2022a), segmentation (Akkus et al. 2017; Kamnitsas et al. 2017; Chen et al. 2018), parcellation (Thyreau and Taki 2020; Lim et al. 2022), network generation (\u0160koch et al. 2022; Yin et al. 2023) and classification (Li et al. 2021; Kawahara et al. 2017; Kan et al. 2022b) separately under supervised settings. However, in brain imaging studies, the collection of voxel-level annotations, transformations between images, and task-specific brain networks often prove to be expensive, as it demands extensive expertise, effort, and time to produce accurate labels, especially for high-dimensional neuroimaging data, e.g., 3D MRI. To reduce this high demand for annotations, recent works have utilized automatic extraction tools (Smith 2002; Cox 1996; Shattuck and Leahy 2002; S\u00e9gonne et al. 2004), unsupervised registration models (Balakrishnan et al. 2018; Su et al. 2022a), inverse warping (Jaderberg et al. 2015), and correlation-based metrics (Liang et al. 2012) for performing extraction, registration, segmentation, parcellation and network generation. Nevertheless, these pipeline-based approaches frequently rely on manual quality control to correct intermediate results before performing subsequent tasks. Conducting such visual inspections is not only time-consuming and labor-intensive but also suffers from intra- and inter-rater variability, thereby impeding the overall efficiency and performance. More recently, joint extraction and registration (Su et al. 2022b), joint registration and segmentation (Xu and Niethammer 2019), joint extraction, registration and segmentation (Su et al. 2023), and joint network generation and classification (Kan et al. 2022a) have been developed for collective learning. How-ever, partial joint learning overlooks the potential interrelationships among these tasks, which can adversely affect overall performance and limit generalizability. There is a pressing need for more integrated, automated and robust methodologies that can seamlessly integrate and optimize all stages of raw brain imaging-to-graph analysis within a unified framework."}, {"title": "Our Approach", "content": "UniBrain integrates multiple modules for brain extraction, registration, segmentation, parcellation, network generation, and classification, seamlessly connecting them within an end-to-end framework to enable collective learning. Below, we provide a detailed description of each module."}, {"title": "Extraction Module", "content": "The extraction module aims to extract brain from the raw image with assistance from two components:\nExtraction Network: $f_e$. The extraction network $f_e(\u00b7)$ acts as an annotator, intended to identify brain and non-brain tissues in the source image S and delineate their locations, thus providing the guidance for subsequent non-brain tissue elimination. Specifically, we employ the 3D U-Net as the base network to learn $f_e(\u00b7)$. The process can be formally expressed as:\n$M = f_e (S),$ (1)\nwhere M is predicted extraction mask. During inference, M is binarized by a Heaviside step function.\nOverlay Layer: OL. The overlay layer serves to eliminate non-brain tissues by applying the predicted brain mask M to the source image S. The final extracted image is $E = S \\circ M$, where $\\circ$ denotes the element-wise multiplication."}, {"title": "Registration Module", "content": "The registration module aims to align the extracted image with the target image, providing transformations for subsequent segmentation and parcellation tasks. This module comprises two main components:\nRegistration Network: $f_r$. The registration network $f_r(\u00b7,\u00b7)$ processes the extracted image E and target image T to learn the affine transformation A, which establishes the coordinate correspondence between source and target image space. A 3D CNN-based encoder is used to learn $f_r(\u00b7,\u00b7)$ as:\n$A = f_r (E, T).$ (2)\nWe leverage the multi-stage registration technique (Su et al. 2022a; Zhao et al. 2019) to boost registration performance, where E is recursively aligned with T though M stages.\nSpatial Transformation Layer: STL. A key step in image registration is reconstructing the warped image W from the extracted image E using the affine transformation A. This warping process is facilitated by a spatial transformation layer (STL), which resamples voxels from the extracted image E to produce the warped image W = $T(E, A)$. Given the affine transformation operator, we hold\n$W_{xyz} = E_{x'y'z'},$ (3)"}, {"title": "Segmentation & Parcellation Module", "content": "The segmentation and parcellation module creates segmentation and parcellation masks on the source image. Leveraging recent developments in one-shot learning (Wang et al. 2020; Ding, Yu, and Yang 2021; Su et al. 2023), the module can generate these masks using a single labeled template image. The module contains two main components:\nInverse Warping Utilizing a single labeled example (i.e., target image T with its corresponding segmentation mask B and parcellation mask P) and the learned affine transformation A, we apply the inverse transformation $A^{-1}$ to generate warped segmentation mask V = $T(B, A^{-1})$ and parcellation mask U = $T(P, A^{-1})$ in the source image space as:\n$V_{cxyz} = B_{cx'y'z'}, \\forall c \\in \\{1, ..., C'\\},$ (4)\n$U_{kxyz} = P_{kx'y'z'}, \\forall k \\in \\{1, ..., K\\},$ (5)\nwhere coordinate correspondence $[x', y', z', 1]^T = A^{-1}[x, y, z, 1]$, c is the index for tissue class and k is the index for ROIs. Same as the STL layer in Registration Module, we then apply a differentiable transformation based on trilinear interpolation.\nSegmentation Network: $f_s$. The segmentation network $f_s(\u00b7)$ aims to generate a segmentation mask for the source image S that matches the synthesized warped segmentation mask V. We employ the widely-used 3D U-Net as the base network to learn $f_s(\u00b7)$. Formally, we have:\n$R = f_s(S).$ (6)"}, {"title": "Brain Network Module", "content": "The brain network module generates the brain network using ROI information from parcellation mask U and the source image S. The modules include three components:\nOverlay Layer: OL. Similar to OL in the Extraction Module, this component is responsible for isolating each ROI from the source image S using parcellation mask U. The parcellated image F = $S \\circ U$ is generated by applying an element-wise product between S and U.\nBrain Network Function: $f_o$. The brain network function aims to learn the representation for each ROI within the parcellated image F. A weight-sharing Multilayer Perceptron (MLP) is employed to learn $f_o(\u00b7)$, ensuring consistent feature extraction and generalization, which is expressed as:\n$H_k = f_o(F_k), \\forall k \\in \\{1, ..., K\\},$ (7)\nwhere k is the index for the ROIs.\nBrain Network Generation. The step generates a brain network based on the similarity between ROI representation pairs. Without loss of generality, here we use inner-product to measure the edge weights of the brain network. However, other differentiable similarity functions (e.g., Mahalanobis distance and cosine similarity) can be used. To compute the connectivity matrix C, each ROI representation $H_k$ is first normalized with the $l^2$-norm, followed by the inner-product:\n$C = HH^T.$ (8)\nThis normalization scales the values of C to the range of [-1, 1], ensuring the stabilization of the learning process and maintaining consistent weight magnitudes in the network."}, {"title": "Classification Module", "content": "The classification module makes a final predictive diagnosis.\nClassification Network: $f_g$. The classification network $f_g(\u00b7,\u00b7)$ aims to make a prediction based on the generated brain network while feeding task-specific insights to the preceding module, facilitating the brain network generation. We leverage the GCN (Kipf and Welling 2017) as the base network. The prediction \u0177 is obtained as:\n$\\hat{y} = f_g(C, H),$ (9)\nwhere H is the initial node features and C is the learnable connectivity matrix provided by the brain network module."}, {"title": "End-to-End Training", "content": "We train UniBrain by minimizing the objective function as:\n$L = L_{cls} (y,\\hat{y}) + \\alpha L_{ext} (M, \\hat{M})+\n\\beta L_{sim} (W, T) + \\gamma L_{seg} (R, V),$ (10)\nwhere $L_{cls}(\u00b7,\u00b7)$ is classification loss term, $L_{ext}(\u00b7,\u00b7)$ is extraction loss term, $L_{sim}(\u00b7,\u00b7)$ is image dissimilarity loss term, and $L_{seg}(\u00b7,\u00b7)$ is segmentation loss term. This equation incorporates bidirectional supervision ($L_{cls}(\u00b7,\u00b7)$ and $L_{ext}(\u00b7,\u00b7)$), which envelops the entire network to ensure positive forward propagation and controllable feedback across tasks. Additionally, unsupervised and one-shot guidance ($L_{sim}(\u00b7,\u00b7)$ and $L_{seg}(\u00b7,\u00b7)$) within the model reduces reliance on high-cost annotations. The loss terms are scaled by \u03b1, \u03b2, and \u03b3 to balance their impacts.\nBy leveraging the differentiability in each component of this design, our model achieves joint optimization in an end-to-end manner. All tasks are unified within a single model for collective learning, mutually boosting their performance with limited labels."}, {"title": "Experiments", "content": "Experimental Settings\nDatasets. We evaluate the effectiveness of our proposed method on the public real-world ADHD dataset with 3D brain sMRI (consortium 2012). The dataset contains records for 776 subjects, labeled as real patients (positive) and normal controls (negative). The original dataset is unbalanced, following (Kong et al. 2013), we randomly sampled 100 ADHD patients and 100 normal controls from the dataset for performance evaluation. Out of the 200 scans, 160 are used for training, 20 for validation, and 20 for testing. All scans are cropped and resized to 96 \u00d7 96 \u00d7 96 dimensions. We use MNI 152 with the AAL atlas (Tzourio-Mazoyer et al. 2002) as the template image for registration and parcellation."}, {"title": "Compared Methods", "content": "We compare our UniBrain with several representative baselines. 1) Extraction: BET (Smith 2002) and SynthStrip (Hoopes et al. 2022); 2) Registration: FLIRT (Jenkinson and Smith 2001), VM (Balakrishnan et al. 2018) and ABN (Su et al. 2022a); 3) Segmentation and Parcellation: DW (Jaderberg et al. 2015); 4) Network Generation (Zhou et al. 2022); 5) Classification: GCN (Kipf and Welling 2017), BGN (Li et al. 2021) and BNT (Kan et al. 2022b); 6) Partial Joint: DeepAtlas (Registration-Segmentation) (Xu and Niethammer 2019), ERNet (Extraction-Registration) (Su et al. 2022b) and JERS (Extraction-Registration-Segmentation) (Su et al. 2023). Notably, there are no existing solutions that can simultaneously perform all tasks in an end-to-end framework. Thus, for comparison, we designed a pipeline-based solution by combining different state-of-the-art methods for each task. The summary of baselines is shown in Table 2.\nImplementation. Our experiments are conducted on Ubuntu 20.04 LTS, utilizing an AMD EPYC 7543 CPU and an NVIDIA Tesla A100-80G GPU. We split the datasets into training, validation, and test sets as introduced in the Datasets section. The training set is for learning model parameters, the validation set evaluates hyperparameter settings (e.g., loss term weights), and the test set is used only once to report the final evaluation results. The code is implemented in Python 3.7.6, and the neural networks are built using PyTorch 1.7.1. The source code is available at https://github.com/Anonymous7852/UniBrain."}, {"title": "Experimental Results", "content": "We compare UniBrain with baseline methods in terms of extraction, registration, segmentation, parcellation, and classification accuracy and efficiency. Additionally, we evaluate UniBrain against voxel-based end-to-end brain imaging analysis solutions, which bypass brain network generation and rely solely on voxel-level information from images for predictions. Experimental results show that: 1) UniBrain consistently outperforms other methods in extraction, registration, segmentation, parcellation, and classification, while also being time-efficient; 2) UniBrain also surpasses voxel-based end-to-end brain imaging analysis solutions. Similar results were also observed on the ABIDE (Tyszka et al. 2014) datasets. Due to space constraints, we will present these findings in detail in a future journal publication.\nOverall Results. Table 1 show the results of the compared methods and the proposed UniBrain in extraction, registration, segmentation, parcellation, and classification tasks. Based on the comprehensive evaluation on the public dataset, UniBrian outperforms existing methods in all tasks. 1) In extraction, we observed that joint-based extraction methods (ERNet, JERS and UniBrain) outperform single-stage extraction methods (BET and Synth). Specifically, UniBrain achieves up to a 5.4% improvement in extraction dice scores over the best single-stage method Synth. 2) For the registration task, methods with strong extraction results typically yield better registration accuracy, highlighting the dependency of accurate registration on prior extraction quality. 3) Good registration enhances segmentation and parcellation performance, as these tasks rely on accurate registration. 4) Classification task results also reflect this trend, with higher parcellation accuracy (like Synth-based, JERS-based, UniBrain) yielding better outcomes due to the classification network leveraging parcellation masks for brain network construction. Overall, there's a clear interdependence among brain imaging analysis tasks, with strengths and errors propagating across them. Partially joint methods like ERNet, JERS, and DeepAtlas show improved performance in their joint tasks but are limited when combined with other separate models. In contrast, UniBrain, benefiting from full end-to-end joint learning, uniquely excels across all tasks."}, {"title": "Running Efficiency", "content": "We measure the efficiency of UniBrain by comparing its inference time with other baselines. The measurement is made on the same device with an AMD EPYC 7543 CPU and an NVIDIA Tesla A100 GPU. The running time is reported as the average processing time for each image in its corresponding task. As indicated in Table 3, fully separate methods are the slowest due to the need for individual optimization of each task. Partially joint learning methods demonstrate increased speed in their joined tasks but still require combination with other methods, limiting overall time efficiency. UniBrain is the fastest method, which efficiently performs all tasks in an end-to-end manner on the same device, enhancing overall speed."}, {"title": "Voxel-based End-to-End Learning", "content": "We compare UniBrain with voxel-based end-to-end brain imaging analysis solution. In this experiments, we disregard graph-based models, relying only on voxel information from images for final classification predictions. We devised three groups: 1) Direct use of raw MRI images as input (including non-brain tissues, images in different coordinate spaces) for label classification. 2) Use of extracted brain images as input (still in different coordinate spaces) for label classification. 3) Use of the brain been extracted and registered to a standard space as input for classification. As shown in Table 4, we observed that the performance is worse when using raw images as input due to the inclusion of non-brain tissues and spatial transformation noise. Images processed through extraction and registration yielded higher accuracy. UniBrain, integrating preprocessing and classification in a joint learning approach, outperformed all other models."}, {"title": "Conclusion", "content": "This paper presents a novel unified framework, UniBrain, the first end-to-end model to jointly perform a diverse set of brain imaging analysis tasks, including extraction, registration, segmentation, parcellation, network generation and classification. UniBrain integrates heterogeneous information into a single system, enabling efficient knowledge transfer across different modules, and avoiding the need for extensive task-specific labels. Experimental results show that UniBrain outperforms state-of-the-art methods in all tasks while also demonstrating robustness and time efficiency."}]}