{"title": "Computational Safety for Generative AI: A Signal Processing Perspective", "authors": ["Pin-Yu Chen"], "abstract": "AI safety is a rapidly growing area of research that seeks to prevent the harm and misuse of frontier AI technology, particularly with respect to generative AI (GenAI) tools that are capable of creating realistic and high-quality content through text prompts. Examples of such tools include large language models (LLMs) and text-to-image (T2I) diffusion models. As the performance of various leading GenAI models approaches saturation due to similar training data sources and neural network architecture designs, the development of reliable safety guardrails has become a key differentiator for responsibility and sustainability. This paper presents a formalization of the concept of computational safety, which is a mathematical framework that enables the quantitative assessment, formulation, and study of safety challenges in GenAI through the lens of signal processing theory and methods. In particular, we explore two exemplary categories of computational safety challenges in GenAI that can be formulated as hypothesis testing problems. For the safety of model input, we show how sensitivity analysis and loss landscape analysis can be used to detect malicious prompts with jailbreak attempts. For the safety of model output, we elucidate how statistical signal processing and adversarial learning can be used to detect AI-generated content. Finally, we discuss key open research challenges, opportunities, and the essential role of signal processing in computational AI safety.", "sections": [{"title": "I. INTRODUCTION", "content": "The year 2023 marks the 75th anniversary of the IEEE Signal Processing Society (SPS). Signal processing has played a pivotal role in ensuring the stability, security, and efficiency of numerous engineering systems and information technologies, including, but not limited to, telecommunications, information forensics and security, machine learning, data science, and control systems. With the recent advances, wide accessibility, and deep integration of generative AI (GenAI) tools into our society and technology, such as ChatGPT and the emerging agentic AI applications, understanding and mitigating the associated risks of the so-called \u201cfrontier AI technology\" is essential to ensure a responsible and sustainable use of GenAI. In addition, as the performance of state-of-the-art GanAI models surpasses that of an average human in certain tasks, but reaches a plateau in standardized capability evaluation benchmarks due to similar training data sources and neural network architecture design (e.g., the use of decoder-only transformers), improving and ensuring safety is becoming the new arms race among GenAI stakeholders. In addition to ongoing activities in risk governance, policy-making, and legal regulation (e.g. EU AI Act, AI safety institutes, etc.), there are growing concerns about the broader socio-technical impacts [1]. Notable examples include breaking the embedded safety alignment of GenAI models to induce misbehavior (e.g., asking large language models how to perform illegal or dangerous activities) [2] or generate inappropriate content, and misusing GenAI tools to create and spread misinformation (e.g., using text prompts to create deepfakes and influence public opinion) [3].\nThe main goal of this paper is to present the central message in a technically sound and rigorous manner as follows: While AI safety can be seen as a significant challenge in the context of modern GenAI technology, it is possible to reformulate many of the associated problems as a classical detection tasks. Furthermore, the insights and methods developed in the field of signal processing can be applied to improve AI safety. Specifically, we define computational AI safety as the set of safety problems that can be formulated as a hypothesis testing task in signal processing. We also provide examples of how signal processing techniques such as sensitivity analysis, subspace projection [4], loss landscape [5], and adversarial learning can be used to study and improve AI safety.\nWithout loss of generality, we divide a GenAI system into three parts: the model input that processes a user's query, the GenAI model that performs inference and action (for agentic workflows), and the model output produced by the model given the input. The two representative safety challenges highlighted for model input and output safety in GenAI are the detection of unsafe queries and AI-generated content. It is worth noting that what is unique about GenAI is that the validity of the safety hypothesis (e.g. whether the input or output is safe) requires an additional judge function for certification, which can be either a rule-based approach (e.g. keyword matching) or an AI-based evaluation (e.g. LLM-as-a-judge or external contextual classifiers). For example, when assessing the risk of jailbreak attacks, one can use refusal-related keywords to quantify the rejection rate in GenAI, or use another LLM as a judge to provide a numerical rating of the safety level of the response.\nTo bridge AI safety and signal processing, this paper presents an overview of computational safety for GenAI, consolidates numerous computational AI safety problems through the lens of a unified detection task in signal processing, and provides comprehensive technical information to elucidate how signal processing methodologies can facilitate and enhance the development of AI safety tools. The rest of the paper is organized as follows. First, we provide background on mainstream GenAI models and preliminaries on hypothesis testing and common evaluation metrics. Next, we formally introduce the signal processing framework for computational AI safety. We then examine two representative use cases in AI safety \u2013 jailbreak detection and mitigation, and AI-generated content. Finally, we provide our perspective on the position of AI safety and discuss several ongoing research challenges and opportunities at the intersection of signal processing and AI safety."}, {"title": "II. BACKGROUND AND PRELIMINARIES", "content": "This paper focuses on the safety challenges of two mainstream GenAI models: Large Language Models (LLMs) and Diffusion Models (DMs). LLMs are autoregressive neural networks built on self-attention-based transformers with billions of trainable parameters, which dominate text-to-text generation applications (e.g. ChatGPT) and underpin many multi-modal GenAI tools, such as vision-language models, audio-language models, and code-language models. DMs are state-of-the-art neural networks capable of high-quality text-to-image generation through a mathematical diffusion process, with extensions to other modalities such as video, audio, and molecular dynamics. The deployment of LLMs and DMs has two modes: direct use of the off-the-shelf GenAI model, or customized fine-tuning with some task-specific downstream datasets. Throughout this paper, we will use \\(\\theta\\) to denote the set of trainable parameters (model weights) for GenAI models.\nLarge language models (LLMs): The development of LLMs has two primary phases: pre-training and align-ment. LLM pre-training refers to unsupervised machine learning on a large dataset (e.g. text extracted from the Internet) denoted by \\(D_p\\). Let \\(x = \\{x_1,x_2,...,x_T\\}\\) denote a sequence of tokens (i.e., \u201cwords\u201d defined in the LLM vocabulary) randomly drawn from \\(D_p\\). For pre-traing, the Next-token prediction (NTP) loss is commonly used, which is defined as the expected loss of the negative log-likelihood\n\\[\\textrm{LOSSNTP} = -E_{x \\sim D} \\frac{1}{|x|}\\sum_{t=1}^{|x|} log P_\\theta(x_t|x_{<t}),\\]\nwhere \\(|x|\\) denotes the token length of \\(x\\), \\(x_{<t} := \\{x_1,x_2,...,x_{t-1}\\}\\) denotes the tokens preceding the t-th token and \\(x_{<1} := \\emptyset\\) (an empty set), \\(p_\\theta(x_t|x_{<t})\\) is parameterized by a softmax function over all tokens. This loss is also known as causal language modeling, by using previous tokens to predict the next token. The family of LLMs trained with NTP loss is also called autoregressive LLMs. After pre-training on \\(\\theta\\), an LLM undergoes the alignment process to teach the model to follow instructions and value alignment. Safety alignment is also implemented in this stage to guide the model in refusing to respond to unsafe queries. Specifically, for supervised fine-tuning (SFT) (or instruction-tuning), let \\((x, y)\\) denote a pair of model input and the desired model output randomly drawn from a dataset \\(D_a\\), the SFT loss is used to update the pre-trained model weights by generalizing the NTP loss as\n\\[\\textrm{LOSSSFT} = -E_{(x,y) \\sim D_a} \\frac{1}{|y|} \\sum_{t=1}^{|y|} log P_\\theta(y_t|x, y_{<t}).\\]\nHere, \\(x\\) can also be conceived as the context, which is used to model the conditional probability of generating \\(y\\) given \\(x\\) as \\(p_\\theta(y|x) = \\prod_t p_\\theta(y_t|x, y_{<t})\\). In addition to SFT, other forms of alignment include strategies such as reinforcement learning with human feedback (RLHF) [6] and direct preference optimization (DPO) [7].\nDiffusion Models (DMs): During training, DMs follow a mathematical diffusion process to gradually encode data samples into random Gaussian noises, and train a neural network to learn to decode (reconstruct) the data samples based on multiple denoising steps. Furthermore, DMs can be integrated with text-based prompts to guide the generation process. We briefly explain the basics of DM as follows. Let \\(t\\) denote the diffusion step and let \\(x_t\\) be the data sample (or its latent representation) obtained via a forward diffusion process subject to noise injection. Given a text prompt (context) \\(c\\), we denote \\(\\epsilon_\\theta(x_t|c)\\) as the noise generator parameterized by a neural network \\(\\theta\\). A reverse diffusion process is also used to estimate the underlying noise. In its simplest form, the diffusion process is given by\n\\[\\epsilon_\\theta(x_t|c) = (1 - \\lambda) \\cdot \\epsilon_\\theta(x_t|0) + \\lambda \\cdot \\epsilon_\\theta(x_t|C),\\]\nwhere \\(\\epsilon_\\theta(x_t|c)\\) signifies the ultimate noise estimation attained by utilizing the DM conditioned on \\(c\\), \\(\\lambda \\in [0, 1]\\) is a guidance weight, and \\(\\epsilon_\\theta(x_t|0)\\) represents the contribution of the unconditional DM. During inference, the DM initializes with a standard Gaussian noise \\(z_T \\sim N(0,I)\\) with zero mean and identity covariance matrix \\(I\\). The noise \\(z_T\\) is then denoised using \\(\\epsilon_\\theta(x_t|c)\\) to obtain \\(z_{T-1}\\). This procedure is repeated to generate the authentic data at \\(t = 0\\). A commonly used training objective for DM is the mean-squared-error (MSE), defined as\n\\[\\textrm{LOSSMSE} = E_{t,\\epsilon \\sim N (0,1)} [||\\epsilon - \\epsilon_\\theta(x_t|C) ||^2].\\]\nFor simplicity, we omit the expectation over the training data \\(\\{(x, c)\\} \\) for DM training."}, {"title": "B. Hypothesis Testing and Common Evaluation Metrics", "content": "Generative Hypothesis Testing: Hypothesis testing refers to statistical methods for data-driven validation among a set of mutually exclusive hypotheses, of which only one is true. It is a classic methodology that is widely used in signal processing, such as detection and estimation problems. In particular, binary hypothesis testing involves an alternative hypothesis \\(H_1\\) and a null hypothesis \\(H_0\\). In principle, a test statistic \\(s\\) is derived from the observed data samples and is compared to a threshold \\(\\eta\\) for testing \\(H_1\\) versus \\(H_0\\). Unlike classic signal processing problems, what is unique about computational safety for GenAI is that a hypothesis of interest is context-dependent and usually cannot be explicitly defined in clear mathematical forms. For example, does the user query (model input) violate the usage policy, or does the model output contain any harmful content? To address this issue, it is common practice to employ a judge function \\(J(x, y) \\in \\{0,1\\}\\) for hypothesis validation, which takes a pair of model input and output \\((x, y)\\) and returns a binary label that serves as a proxy for the ground-truth hypothesis. The judge function \\(J\\) can be as simple as a rule-based system, such as flagging problematic context or content using keyword filtering. It can also be a complex and opaque decision-making process performed by another AI model, such as LLM-as-a-judge. However, these judge functions have their limitations. Rule-based judge functions often fail to generalize in sophisticated situations that require deep contextual understanding or reliability against intentional manipulation, while AI-based solutions may inherit data biases from the training data and exhibit unfairness or hallucination in their judgments [8]. We refer to the scenario of using an AI-based judge function as a generative hypothesis testing problem setting. Verifying and improving the accuracy of the judge functions is an important topic in computational AI safety, but it is also beyond the scope of this paper.\nCommon Evaluation Metrics: Let \\(\\hat{H}\\) denote the predicted hypothesis using a test statistic \\(s\\) and let \\(H\\) be the \"actual\" hypothesis determined by a judge function. Let \u201c1\u201d denote True and \u201c0\u201d denote False in binary hypothesis testing. Given an instance, the test result is called a true positive (TP) if \\(\\hat{H} = H = 1\\), a true negative (TN) if \\(\\hat{H} = H = 0\\), a false positive (FP) if \\(\\hat{H} = 1\\) but \\(H = 0\\), and a false negative (FN) if \\(\\hat{H} = 0\\) but \\(H = 1\\). Extending the evaluation to a set of instances using the same test statistic, and denoting the number of instances of a test result by \\(#\\). The true positive rate (TPR) is defined as \\(\\textrm{TPR} = \\frac{\\textrm{#TP}}{\\textrm{#TP}+\\textrm{#FN}}\\) , the false positive rate is \\(\\textrm{FPR} = \\frac{\\textrm{#FP}}{\\textrm{#FP}+\\textrm{#TN}}\\) , the false negative rate is \\(\\textrm{FNR} = \\frac{\\textrm{#FN}}{\\textrm{#FN}+\\textrm{#TP}}\\) , and the true negative rate is \\(\\textrm{TNR} = \\frac{\\textrm{#TN}}{\\textrm{#FP}+\\textrm{#TN}}\\) . Accuracy measures the overall correctness of the predictions, which is defined as the ratio of the correctly tested instances to the total number of instances, \\(\\textrm{accuracy} = \\frac{\\textrm{#TP}+\\textrm{#TN}}{\\textrm{#TP}+\\textrm{#TN}+\\textrm{#FP}+\\textrm{#FN}}\\) . The equal error rate (EER) is defined as the point on the receiver operating characteristic (ROC) curve where FPR = FNR, which is equivalent to \\(\\textrm{EER} = \\frac{\\textrm{FPR}+\\textrm{FNR}}{2}\\). Furthermore, by varying the threshold \\(\\eta\\) for the test statistic \\(s\\) in hypothesis testing, one can obtain the area under the receiver operating characteristic curve (AUROC), which provides a threshold-independent evaluation of the capability in hypothesis testing and reflects the trade-off between TPR and FPR. Another popular evaluation metric is to report TPR@(FPR= a%) or FPR@(TPR= \\(\\beta\\)% ) for some \\(a\\) and \\(\\beta\\) by setting the threshold to meet the criterion."}, {"title": "III. SIGNAL PROCESSING FOR COMPUTATIONAL AI SAFETY", "content": "With the necessary background and preliminaries introduced in the previous section, we formally present a set of signal processing techniques that can be applied to evaluate and improve computational AI safety. Specifically, Table I shows a non-exhaustive list of problems in AI safety that can be formulated as a binary hypothesis testing task in a unified way. Jaibreak is concerned with detecting model inputs that attempt to bypass safety guardrails. AI-generated Content tests whether a data sample is AI-generated. Model Fine-tuning checks if model updates would compromise safety alignment. Watermark verifies if a data sample is watermarked. Membership Inference examines if a data sample has been used for model training, which is highly relevant for machine unlearning. Data Contamination looks into whether a data set has been used for model training, which is critical for ensuring the authenticity of model evaluations on public benchmarks. This paper focuses on the first three problem areas in Table I. We then present a set of signal processing techniques that can be used to study computational AI safety.\nInspired by the well-known principle of studying the robustness of signal processing methods in the presence of noise, sensitivity analysis quantifies the amount of change in data representations subject to data manipulation. Let \\(x\\) be a data sample and let \\(g(x)\\) denote the vectorized continuous latent representation (i.e., embedding) of \\(x\\) obtained from a deep learning model \\(g\\). There are two common approaches to measuring sensitivity. The first one is to use a metric \\(M(g(x), g(T(x))\\) to compare the similarity (or difference) between a pair of data embedding \\(g(x)\\) and its manipulated version \\(g(T(x))\\), where the function \\(T\\) denotes a transformation operation on \\(x\\). In particular, additive Gaussian noise perturbation is a common approach to implement \\(T\\). The second approach is to compute the gradient of a loss function \\(l\\) evaluated at \\(g(x)\\) (wherever applicable), denoted by \\(\\nabla l(g(x))\\), and use the gradient norm \\(||\\nabla l(g(x))||\\) as a measure of sensitivity. For AI-generated image detection, Ricker et al. implement \\(T\\) as an image reconstruction function based on an autoencoder and assign \\(M\\) as the reconstruction error for the test statistic [9]. He et al. use an image foundation model for feature extraction and implement additive Gaussian noise for sensitivity analysis to detect AI-generated images [10], where the metric \\(M\\) is the cosine similarity. For jailbreak, Hu et al. compute the gradient norm of an affirmation loss on each token of the model input to identify and mitigate jailbreak prompts [11].\nSubspace modeling involves learning and processing complex (often high-dimensional) data in a compressed low-dimensional subspace. Classic examples include principal component analysis (PCA), sparse signal recovery, subspace tracking, information geometry, and manifold learning, to name a few. In [12], Hsu et al. leverage the technique of subspace projection to mitigate safety degradation during model fine-tuning. Given a pair of base and aligned LLM, denoted as \\(\\{\\theta_b, \\theta_a\\}\\), where they share the same neural network architecture, and \\(\\theta_b\\) is the model obtained after the pre-training phase, whereas \\(\\theta_a\\) is obtained after fine-tuning \\(\\theta\\) with alignment data. Such a pair of model weights is often publicly available on open-weight model releases. They model the subspace for alignment as the span of the the difference between \\(\\theta_b\\) and \\(\\theta_a\\), formally defined as \\(V = \\theta_a \u2013 \\theta_b\\). The weight difference \\(V\\) encapsulates the effect of effort that leads a pre-trained model to become an aligned model. Next, when the aligned model \\(\\theta_a\\) is to be fine-tuned for a downstream task, incurring a weight change denoted by \\(\\Delta\\theta\\). Before \\(\\Delta\\theta\\) is applied to \\(\\theta_a\\), the similarity between \\(V\\) and \\(\\Delta\\theta\\) is checked for each layer in the neural network. If the similarity is above a threshold, indicating that the model update \\(\\Delta\\theta\\) is aligned with the subspace spanned by \\(V\\), the model update is performed as is. Otherwise, the model update is performed by first projecting \\(\\Delta\\theta\\) onto the subspace spanned by \\(V\\) and then adding the projected update to the aligned model \\(\\theta_a\\). By doing so, when compared to naive model fine-tuning without subspace projection, possible safety degradation can be mitigated while maintaining high capability on the downstream task.\nGiven a loss function \\(f : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\), loss landscape analysis refers to the exploration of the loss function by perturbing the input of \\(f\\) with a set of random and orthogonal directions. For the purpose of loss landscape visualization, the number of perturbations is often limited to 1 or 2 dimensions. The exploration of loss landscape for deep neural networks has facilitated the understanding and advancement of generalization errors, optimization trajectories, and model merging and ensembling [5], [13]. Consider the case of a two-dimensional loss landscape visualization centered on a point \\(u \\in \\mathbb{R}^d\\) as an input to \\(f\\). The loss landscape \\(F\\) is obtained by perturbing \\(u\\) along two random orthogonal unit vectors \\(v_1\\) and \\(v_2\\), where \\(\\alpha\\) and \\(\\beta\\) represent the respective coefficients and evaluating them with \\(f\\), which is defined as\n\\[F(\\alpha, \\beta|u) = f(u + \\alpha \\cdot v_1 + \\beta\\cdot v_2).\\]"}, {"title": "IV. MODEL INPUT SAFETY: JAILBREAK PROMPT DETECTION AND MITIGATION", "content": "In general, a jailbreak prompt is a user query (model input) designed to bypass the embedded safety guardrail of a GenAI system, where the system can be as simple as an open-weight LLM, or an integrated closed-source LLM with additional moderation mechanisms in place. Examples of jailbreak attack strategies include role-playing, changing system prompts, adding distractive context, and instructing another LLM to design and generate prompts. A judge function inspects the output of the target GenAI system to evaluate the success of a jailbreak prompt. A rule-based judge function uses keyword matching and declares the attack in veil if the output contains some keywords like \u201cSorry\u201d, \u201cI can't fulfill\", etc. However, such a judgment lacks a comprehensive understanding of the context and might overlook jailbreaks that force the target model to respond in an apologetic tone. On the other hand, one can use another LLM or a customized LLM (i.e., an LLM guard model) as a judge to evaluate whether the output is safe or unsafe. While LLM-based judge functions possess contextual awareness, they still lack transparency about the decision and may have their own biases. Nevertheless, despite their limitations, these judge functions are poised as hypothesis-testing verifiers in our computational AI safety framework.\nSafety-Capability Trade-off: When defending against jailbreak attempts, an implicit requirement of a practical mitigation approach is the ability to effectively identify and thwart jailbreak prompts while minimizing the impact on the capability of the protected model. A model that is over-aligned to cause over-rejection of benign requests (i.e., high safety but low capability), or performant but under-aligned to be easily jailbroken (i.e., high capability but low safety), is undesirable. Therefore, exploring and optimizing the safety-capability trade-off of a deployed jailbreak mitigation is essential to computational AI safety. Next, we present two methods for detecting and mitigating jailbreak prompts to achieve a good safety-capability tradeoff.\nIn [14], Hu et al. apply loss landscape analysis to the embedding of jailbreak prompts and the responses of an LLM. They show that jailbreak prompts exhibit different characteristics than benign (safe) prompts in the landscape studied, and this finding can be used to detect jailbreak prompts. Specifically, given a prompt \\(x\\) of token length \\(|x|\\), an LLM first embeds the tokens of \\(x\\) into a token embedding \\(E(x) \\in \\mathbb{R}^{|x| \\times d}\\), where \\(d\\) is the embedding dimension. The function \\(g\\) denotes an LLM, and \\(g(E(x))\\) means the response of \\(g\\) given the context \\(x\\). A judge function \\(J(g(E(x)), x) \\in \\{0, 1\\}\\) is used for safety verification, where \u201c1\u201d means \\(x\\) is a successful jailbreak prompt that causes harmful behavior of \\(g\\), and \u201c0\u201d otherwise. Finally, Hu et al. explore the loss landscape around \\(J(g(E(x)),x)\\) by defining a non-refusal loss on \\(x\\) as \\(f(x|g, J) = 1-\\frac{1}{N} \\sum_{i=1}^N J(g^{(i)}(E(x)), x)\\), which signifies the average non-refusal rate over \\(N\\) independent inferences of \\(g\\) on \\(x\\) (each inference is indexed by \\(i\\)), taking into account the randomness in the decoding process (e.g., probabilistic sampling of the next token). Finally, the two-dimensional loss landscape function centered on the input embedding \\(E(x)\\) is computed by \\(F(\\alpha, \\beta|E(x)) = f(E(x) \\oplus \\alpha \\cdot v_1 \\oplus \\beta\\cdot v_2|g, J)\\), where \\(v_1, v_2 \\in \\mathbb{R}^d\\) are two random directions sampled from a standard multivariate Gaussian distribution, and the notation \\(\\oplus\\) denotes the row-wise broadcast function that adds a vector to each row of \\(E(x)\\).\nFigure 2 shows the average loss landscapes of benign and jailbreak prompts evaluated with Vicuna-7B-V1.5 [20], a fine-tuned LLM based on Meta's Llama-2-7B model [21]. The benign prompts are 100 queries sampled from Chatbot Arena\u00b9, while the 100 jailbreak prompts are generated by the Greedy Coordinate Gradient (GCG) attack on AdvBench for suffix optimization [2]. Keyword matching is used as the judge function [14]. The results show that benign and malicious prompts differ significantly in their respective loss landscapes. The loss landscape of benign prompts is flat and close to 1, suggesting that the perturbed input embeddings are unlikely to cause rejections. On the other hand, malicious prompts are sensitive to embedding perturbations. The average non-refusal rate peaks at the origin (because most jailbreak attempts are successful), but quickly declines as the input embeddings are perturbed away from the origin, suggesting that the perturbed embeddings of malicious prompts are more likely to be refused by the LLM. Based on the loss landscape analysis, Hu et al. propose a jailbreak prompt detection method called Gradient Cuff [14]. The detection of Gradient Cuff consists of two steps. In the first step, the detector flags a jailbreak if the non-refusal loss \\(f\\) is less than 0.5, which checks the tendency of the LLM to reject a query. In the second step, the detector computes the gradient norm of \\(f\\) at the input embedding \\(E(x)\\) and uses it to quantify the changes in the loss landscape under random perturbations. If the gradient norm is greater than a\""}, {"title": "V. MODEL OUTPUT SAFETY: AI-GENERATED CONTENT DETECTION", "content": "With the rapidly increasing ability of GenAI tools to generate realistic, high-quality, and creative content across modalities, including but not limited to text, image, audio, and video, procedures to ensure that AI-generated content (AIGC) can be reliably validated are critical to the responsible use and governance of GenAI. Notable challenges associated with AIGC detection include misuse to create and disseminate disinformation at scale (e.g., AI-generated deepfakes), contamination of future training data, intellectual property protection, hallucinated responses, and inappropriate or unethical content generation. It is worth noting that watermarking is only a partial and somewhat limited solution to the challenge of AIGC detection, because watermarking only regulates responsible GenAI service providers, but not bad actors. A bad actor can train his own GenAI model or use any unregulated GenAI tool to create AIGC without watermarking. Therefore, watermarking only provides passive protection for AIGC, and reliable AIGC detectors without watermark assumption are in high demand as proactive AI safety tools. In this section, we discuss two AIGC detectors for image and text that are enhanced by signal processing techniques.\nFor the detection of AI-generated images, we focus on training-free approaches that are built upon off-the-shelf AI models, as recent work has shown comparable or even better detection performance of training-free methods than training-based approaches [10], [33]. Recall that sensitivity analysis computes a metric \\(M\\) and uses it for hypothesis testing. Ricker et al. propose AEROBLADE [9], a detector that uses the reconstruction error obtained from the pre-trained autoencoder of a latent diffusion model (LDM) for detection, where AI-generated images are found to have lower reconstruction errors than real images. He el al. propose RIGID [10], which computes the cosine similarity between the representations of a pair of the original and perturbed images subject to Gaussian noise, using the DINOV2 model [34] for feature extraction. They find that real images tend to have higher cosine similarity to their perturbed versions than AI-generated images.\nUsing the two-dimensional loss landscape analysis, we explore the landscape of cosine similarity \\(F(\\alpha, \\beta)\\) between an original image and its perturbed version in the embedding space of DINOV2, where two independent noises drawn from the standard Gaussian distribution with their respective scaling factor \\(\\alpha\\) and \\(\\beta\\) are added to the pixel values of the original image. Figure 4 shows the average landscape of cosine similarity over 100 real and AI-generated images, respectively, where the real ones are sampled from ImageNet, and the AI-generated ones are generated by the ablated diffusion mode (ADM) [35]. It can be observed that real images attain a higher similarity after perturbation than AI-generated images. The reason is that DINOV2 is trained only on real images and their data augmentations, which makes its embedding space less sensitive to noise perturbations on real images. Following [10], we compare AEROBLADE and RIGID on three datasets consisting of real and AI-generated images: ImageNet, LSUN-Bedroom, and DF40 (deepfakes) [36]. The AI-generated images are collected from a variety of generative models, including different variants of diffusion models, generative adversarial networks (GANs), and variational autoencoders (VAEs). Table II reports their AUROC scores, where higher values indicate better detection performance. RIGID is shown to outperform AEROBLADE. The difference can be explained by the fact that AEROBLADE relies on the pre-trained autoencoder of an LDM for detection, which may have limited generalization ability for images generated by different models. On the other hand, RIGID is agnostic to image generation models.\nDue to the nature of autoregressive and probabilistic sampling of next-token prediction in LLMs, statistical approaches that derive statistics from LLMs for AI-generated text detection are popular baselines. Examples include log-likelihood (log p), token ranks (rank), and predictive entropy (entropy). Mitchell et al. propose DetectGPT [37], a detection method that adopts a log-likelihood ratio test between the original text and its perturbed-and-reworded versions. However, recent studies have shown that many AI-generated text detectors are vulnerable to AI paraphrasing [38] \u2013 using an LLM to rewrite the original AI-generated text can circumvent the reliability of many detectors. To address this challenge, Hu et al. [19] propose a detection method called RADAR, which uses adversarial learning to iteratively train a detector and a paraphraser to improve the robustness of AI-generated text detection. During RADAR's training, the paraphraser uses proximal policy optimization to update its parameters, using the detector's predictions about its paraphrased text as a reward. The detector takes human-written text, original AI-generated text, and paraphrased AI-generated text as input, to maximize detection performance on human and AI-generated text. This iterative training process continues until detection performance is saturated on a validation dataset. With this adversarial learning setup, the detector learns to be robust in detecting both original and paraphrased AI-generated text.\nFollowing the same experimental setup as in [19], we collect human-written and AI-generated text from four different text datasets, including text summarization, question answering, Reddit's writing prompts, and TOFEL exam essays, where 8 different LLMs are used for text generation. In the evaluation, we also use OpenAI's GPT-"}, {"title": "VI. DISCUSSION: WHEN AGI MEANS ARTIFICIAL GOOD INTELLIGENCE", "content": "This section discusses several insights to accelerate research and innovation in safety from a signal processing perspective. While the trends of GenAI lean toward the mindset of creating frontier AI technology by moving fast and scaling everything, especially for frequent model/product releases and intensified training and inference resources, embracing boldness and imperfection is becoming the new norm for AI safety. However, we argue that the development and use of scientific methods and tools, such as signal processing, is the most important milestone that must be achieved to claim an undisputed victory in GenAI. In particular, the computational AI safety framework introduced in this paper can be further developed into a new discipline that encompasses a comprehensive list of topics such as safety risk exploration (e.g., benchmarks, red-teaming, adversarial testing, and attacks), safety risk mitigation (e.g., detection, model updates, and safety-enhanced training), and evaluation (e.g., risk-capability analysis, safety certification, human-centered interaction, and governance). By studying and practicing computational AI safety, we believe we are just at the beginning of a new era and a bright future towards the ultimate goal of building a Safety Generalist to \u201cuse AI to govern AI,", "GenAI": "While GenAI may seem like a new technology, many lessons and past experiences learned from safeguarding pre-GenAI machine learning (ML) systems can be used as a foundation to quickly build safety guardrials for GenAI. This kind of", "16": "for pre-GenAI machine learning systems and the seemingly new safety challenges in GenAI. The problems therein can be formulated using the same principles", "39": "explores cross-domain ML by attaching input and output transformation functions to a weight-frozen ML model"}, {"GenAI": "GenAI technology is on its way to accepting inputs from multiple data types and potentially generating outputs with mixed modalities. While multi-modal GenAI may introduce increased safety risks by providing additional attack surfaces"}, {"GenAI": "The next generation of GenAI is to be empowered with the ability to act on behalf of the user by autonomously creating agentic"}]}