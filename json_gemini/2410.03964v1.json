{"title": "Variational Language Concepts for Interpreting\nFoundation Language Models", "authors": ["Hengyi Wang", "Shiwei Tan", "Zhqing Hong", "Desheng Zhang", "Hao Wang"], "abstract": "Foundation Language Models (FLMs) such as\nBERT and its variants have achieved remark-\nable success in natural language processing.\nTo date, the interpretability of FLMs has pri-\nmarily relied on the attention weights in their\nself-attention layers. However, these atten-\ntion weights only provide word-level interpreta-\ntions, failing to capture higher-level structures,\nand are therefore lacking in readability and\nintuitiveness. To address this challenge, we\nfirst provide a formal definition of conceptual\ninterpretation and then propose a variational\nBayesian framework, dubbed Variational Lan-\nguage Concept (VALC), to go beyond word-\nlevel interpretations and provide concept-level\ninterpretations. Our theoretical analysis shows\nthat our VALC finds the optimal language con-\ncepts to interpret FLM predictions. Empirical\nresults on several real-world datasets show that\nour method can successfully provide concep-\ntual interpretation for FLMs\u00b9.", "sections": [{"title": "1 Introduction", "content": "Foundation language models (FLMs) such as\nBERT (Devlin et al., 2018) and its variants (Lan\net al., 2019; Liu et al., 2019; He et al., 2021; Portes\net al., 2023) have achieved remarkable success in\nnatural language processing. These FLMs are usu-\nally large attention-based neural networks that fol-\nlow a pretrain-finetune paradigm, where models are\nfirst pretrained on large datasets and then finetuned\nfor a specific task. As with any machine learn-\ning models, interpretability in FLMs has always\nbeen a desideratum, especially in decision-critical\napplications (e.g., healthcare).\nTo date, FLMs' interpretability has primarily\nrelied on the attention weights in self-attention lay-\ners. However, these attention weights only provide\nraw word-level importance scores as interpreta-\ntions. Such low-level interpretations fail to capture\nhigher-level semantic structures, and hence lack\nreadability and intuitiveness. For example, low-\nlevel interpretations often fail to capture influence\nof similar words to predictions, leading to unstable\nor even unreasonable explanations (see Sec. 5.4 for\ndetails).\nIn this paper, we aim to go beyond word-level\nattention and interpret FLM predictions at the con-\ncept level. Such higher-level semantic interpreta-\ntions are complementary to word-level importance\nscores and often more readable and intuitive. For\nexample, as shown in Fig. 1, VALC interprets the\nFLM with the following multi-level concepts (de-\ntails in Appendix F):\n\u2022 Dataset-level concepts are highlighted by the\ntop words and the distribution of their embed-\ndings in the PLM (Fig. 1(left)). For example,\nConcept 20 (Government) corresponds to the\nred ellipse, encompassing words relevant to\ngovernment entities and activities, as shown\nin Fig. 1(left).\n\u2022 Document-level concepts are demonstrated\nby each document's topics; for instance, in\nthe 3 bars representing probability distribu-\ntion over 3 concepts for the document in Fig.\n1(middle), VALC identifies Concept 24, i.e.,\n'politics', and Concept 20, i.e., 'government',\nas considerably more relevant concepts com-\npared to Concept 27, i.e. \u2018names'.\n\u2022 Word-level concepts are identified by words\nin documents. For example, in the box dis-\nplaying the document in Fig. 1(right), VALC\nhighlights the words 'chided' and 'tax' be-\ncause they are highly related to Concept 20,\ni.e., 'government'. Terms like 'Senate' and\n'bitty' are associated with Concept 24, i.e.\n'politics', aligning with the document-level\nconcepts.\nWe start by developing a comprehensive and for-\nmal definition of conceptual interpretation with\nfour desirable properties: (1) multi-level structure,\n(2) normalization, (3) additivity, and (4) mutual\ninformation maximization. With this definition,\nwe then propose a variational Bayesian framework,\ndubbed Variational Language Concept (VALC), to\nprovide dataset-level, document-level, and word-\nlevel (the first property) conceptual interpretation\nfor FLM predictions. Our theoretical analysis\nshows that maximizing our VALC's evidence lower\nbound is equivalent to inferring the optimal con-\nceptual interpretation with Properties (1-3) while\nmaximizing the mutual information between the in-\nferred concepts and the observed embeddings from\nFLMs, i.e., Property (4).\nDrawing inspiration from hierarchical Bayesian\ndeep learning (Wang and Yeung, 2016, 2020; Wang\net al., 2016), the core of our idea is to treat a FLM's\ncontextual word embeddings (and their correspond-\ning attention weights) as observed variables and\nbuild a probabilistic generative model to automati-\ncally infer the higher-level semantic structures (e.g.,\nconcepts or topics) from these embeddings and at-\ntention weights, thereby interpreting the FLM's\npredictions at the concept level. Our VALC is com-\npatible with any attention-based FLMs and can\nwork as an conceptual interpreter, which explains\nthe FLM predictions at multiple levels with theoret-\nical guarantees. Our contributions are as follows:\n\u2022 We identify the problem of multi-level inter-\npretations for FLM predictions, develop a for-\nmal definition of conceptual interpretation,\nand propose VALC as the first general method\nto infer such conceptual interpretation."}, {"title": "2 Related Work", "content": "Foundation Language Models. Foundation lan-\nguage models are large attention-based neural net-\nworks that follow a pretrain-finetune paradigm.\nUsually they are first pretrained on large datasets\nin a self-supervised manner and then finetuned for\na specific downstream task. BERT (Devlin et al.,\n2018) is a pioneering FLM that has shown impres-\nsive performance across multiple downstream tasks.\nFollowing BERT, there have been variants (He\net al., 2021; Clark et al., 2020; Yang et al., 2019;\nLiu et al., 2019; Lewis et al., 2019) that design dif-\nferent self-supervised learning objectives or train-\ning schemes to achieve better performance. While\nFLMs offer attention weights for interpreting pre-\ndictions at the word level, these interpretations lack\nreadability and intuitiveness because they fail to\ncapture higher-level semantic structures.\nInterpretation Methods for FLMs. Existing\nconceptual interpretation methods for FLMs typi-\ncally rely on topic models (Blei et al., 2003; Blei\nand Lafferty, 2006; Blei, 2012; Wang et al., 2012;\nChang and Blei, 2009; Mcauliffe and Blei, 2007;\nHoffman et al., 2010) and prototypical part net-\nworks (Chen et al., 2019). There has been re-\ncent work that employs deep neural networks to\nlearn topic models more efficiently (Card et al.,\n2017; Xing et al., 2017; Peinelt et al., 2020), using\ntechniques such as amortized variational inference.\nThere is also work that improves upon traditional\ntopic models by either leveraging word similarity as\na regularizer for topic-word distributions (Das et al.,\n2015; Batmanghelich et al., 2016) or including\nword embeddings into the generative process (Hu\net al., 2012; Dieng et al., 2020; Bunk and Kres-\ntel, 2018; Duan et al., 2021). There is also work\nthat builds topic models upon embeddings from\nFLMS (Grootendorst, 2020; Zhang et al., 2022;\nWang et al., 2022; Zhao et al., 2020; Meng et al.,\n2022). However, these methods often rely on a\npipeline involving dimensionality reduction and ba-\nsic clustering, which is not end-to-end, leading to"}, {"title": "3 Methods", "content": "potential information loss between FLM embed-\ndings and clustering outcomes. This can result in\nunfaithful interpretations for the underlying FLM.\nAdditionally, they typically generate interpretations\nat a single level (e.g., document level), lacking a\nmulti-level conceptual structure.\nBeyond topic models, attribution-based ap-\nproaches such as LIME (Ribeiro et al., 2016) and\nSHAP (Lundberg and Lee, 2017) assign impor-\ntance to input features to explain predictions. Con-\ncept bottleneck models (CBMs) (Koh et al., 2020;\nYuksekgonul et al., 2023; Yang et al., 2023; Kim\net al., 2018; Schulz et al., 2020; Paranjape et al.,\n2020; Schrouff et al., 2021) offer interpretations by\nlearning conceptual activation and then performing\nclassifications on these concepts, while inherent\nmodels (Xie et al., 2023; Ren et al., 2023; Shi et al.,\n2021) focus on model redesign/re-training for in-\nterpretability. However, these approaches often re-\nquire extra supervision or re-training, making them\nunsuitable for our setting. In contrast, our method\nis inherently multi-level and end-to-end, models\nconcepts across dataset, document, and word lev-\nels, and produces faithful post-hoc interpretations\nfor any models based on FLMs with theoretical\nguarantees.\nIn this section, we formalize the definition of con-\nceptual interpretation, and describe our proposed\nVALC for conceptual interpretation of FLMs.", "subsections": [{"title": "3.1 Problem Setting and Notation", "content": "We consider a corpus of M documents, where the\nm'th document contains Jm words, and a FLM\nf(Dm), which takes as input the document m (de-\nnoted as Dm) with Jm words and outputs (1) a\nCLS embedding cm \u2208 Rd, (2) Jm contextual word\nembeddings em \u2252 [emj]3m1, and (3) the attention\nweights am) [a\u2081 between each word and\nmjj=1\nJm\nthe last-layer CLS token, where h denotes the h'th\nattention head. We denote the average attention\nweight over H heads as amj = 1H (h)\nmjj=1\ncorrespondingly am = [amj]1 (see the FLM\nat the bottom of Fig. 2). In FLMs, these last-\nlayer CLS embeddings are used as document-level\nrepresentations for downstream tasks (e.g., docu-\nment classification). Furthermore, our VALC as-\nsumes K concepts (topics) for the corpus. For\ndocument m, our VALC interpreter tries to in-\nfer a concept distribution vector Om\u2208 RK (also\n\u2211Hh=1 am and"}, {"title": "3.2 Formal Definition of Language Concepts", "content": "Below we formally define 'conceptual inter-\npretation' for FLM predictions (see notations\nin Sec. 3.1):\nDefinition 3.1 (Conceptual Interpretation). As-\nsume K concepts and a dataset D containing M\ndocuments, each with Jm words (1 < m \u2264 \u041c).\nConceptual interpretation for a document m con-\nsists of K dataset-level variables {\u03a9k}K1, a\ndocument-level variable Om, and Jm word-level\nvariables {$mj}m1 with the following properties:\n(1) Multi-Level Structure. Conceptual interpreta-\ntion has a three-level structure:\n(a) Each dataset-level variable \u03a9k = (\u03bc\u03ba, \u03a3k)\ndescribes the k'th concept; \u03bc\u03b5 \u2208 Rd and\n\u03a3\u03ba\u2208 Rdxd denote the mean and covariance\nof the k'th concept in the embedding space\n(i.e., emj \u2208 Rd), respectively.\n(b) Each document-level variable Om \u2208 RK\ndescribes document m's relation to the K\nconcepts.\n(c) Each word-level variable $mj \u2208 Ro describes word j's relation to the K concepts.\n(2) Normalization. The document- and word-"}]}, {"title": "3.3 Variational Language Concepts (VALC)", "content": "Method Overview. Drawing inspiration from hier-\narchical Bayesian deep learning (Wang and Yeung,\n2016, 2020; Wang et al., 2016; Mao et al., 2022;\nYan and Wang, 2023; Xu et al., 2023; Wang et al.,\n2024), we propose our model, VAriational Lan-\nguage Concepts (VALC), to infer the optimal con-\nceptual interpretation described in Definition 3.1.\nDifferent from static word embeddings (Mikolov\net al., 2013) and topic models, FLMs produce con-\ntextual word embeddings with continuous-value en-\ntries [emj]m1 and more importantly, associate each\nword embedding with a continuous-value attention\nweight [amj]; therefore this brings unique chal-\nlenges.\nTo effectively discover latent concept structures\nlearned by FLMs at the dataset level and interpret\nFLM predictions at the data-instance level, our", "subsections": [{"title": "3.4 Objective Function", "content": "VALC treats both the contextual word embeddings\nand their associated attention weights as observa-\ntions to learn a probabilistic generative model of\nthese observations, as shown in Fig. 2. The key idea\nis to use the attention weights from FLMs to com-\npute a virtual continuous count for each word, and\nmodel the contextual word embedding distributions\nwith Gaussian mixtures. The generative process of\nVALC is as follows (we mark key connection to\nFLMs in blue and show the corresponding graphi-\ncal model in Fig. 3):\nFor each document m, 1 < m < M,\n1. Draw the document-level concept distribution\nvector 0m ~ Dirichlet(a).\n2. For each word j (1 \u2264 j \u2264 Jm),\n(a) Draw the word-level concept index zmj\n~ Categorical(0m).\n(b) With a continuous word count Wmj \u2208 R\nfrom the FLM's attention weights, draw\nthe contextual word embedding of the FLM\nfrom the corresponding Gaussian component\nemj ~ N(\u03bczmj, \u03a3zmj).\nGiven the generative process above, discov-\nery of latent concept structures in FLMs at the\ndataset level boils down to learning the parameters\n{\u03bc\u03ba, \u03a3\u03ba}=1 for the K concepts. Intuitively the\nglobal parameters {\u03bc\u03ba, \u03a3\u03ba}K-1 are shared across\ndifferent documents, and they define a mixture of\nK Gaussian distributions. Each Gaussian distribu-\ntion describes a 'cluster' of words and their contex-\ntual word embeddings.\nSimilarly, interpretations of FLM predictions at\nthe data-instance level is equivalent to inferring the\nlatent variables, i.e., document-level concept distri-\nbution vectors Om and word-level concept indices\nzmj. Below we highlight several important aspects\nof our VALC designs.\nAttention Weights as Continuous Word\nCounts. Different from typical topic mod-\nels (Blei et al., 2003; Blei, 2012) and word em-\nbeddings (Mikolov et al., 2013) that can only han-\ndle discrete word counts, our VALC can handle\ncontinuous (virtual) word counts; this better aligns\nwith continuous attention weights in FLMs. Specif-\nically, we denote as wmj \u2208 R>0 the (non-negative\nreal-valued) continuous word count for the j'th\nword in document m. We explore three schemes of\ncomputing Wmj:\n\u2022 Identical Weights: Use identical weights for\ndifferent words, i.e., wmj = 1,\u2200m, j. This is\nequivalent to typical discrete word counts.\n\u2022 Attention-Based Weights with Fixed Length:"}, {"title": "3.4.1 Inference", "content": "Use Wmj = J'amj, where J' is a fixed se-\nquence length shared across all documents.\n\u2022 Attention-Based Weights with Variable\nLength: Use Wmj = Imamj/mami, where\nJm is true sequence length without padding.\nNote that in practice, mami \u2260 1 due to\npadding tokens in FLMs.\nContextual Continuous Word Representa-\ntions. Note that different from topic models (Blei\net al., 2003) and typical word embeddings (Mikolov\net al., 2013; Dieng et al., 2020) where word repre-\nsentations are static, word representations in FLMs\nare contextual; specifically, the same word can have\ndifferent embeddings in different documents (con-\ntexts). For example, the word 'soft' can appear as\nthe j\u2081'th word in document m\u2081 and as the j2'th\nword in document m2, and therefore have two dif-\nferent embeddings (i.e., em1j1 \u2260 em2.j2).\nCorrespondingly, in our VALC, we do not con-\nstrain the same word to have a static embedding;\ninstead we assume that a word embedding is drawn\nfrom a Gaussian distribution corresponding to its\nlatent topic. Note that word representations in our\nVALC is continuous, which is different from typi-\ncal topic models (Blei et al., 2003) based on (dis-\ncrete) bag-of-words representations.\nInferring Document-Level and Word-Level Con-\ncepts. We formulate the problem of interpret-\ning FLM predictions at the concept level as in-\nferring document-level and word-level concepts.\nSpecifically, given global concept parameters\n{(\u03bc\u03ba, \u03a3\u03ba)}=1, the contextual word embeddings\nem [emj]1, and the associated attention\nweights am [amj]1, a FLM produces for each\ndocument m, our VALC infers the posterior distri-\nbution of the document-level concept vector 0m,\ni.e., p(0m|em, am, {(\u03bc\u03ba, \u03a3k)}=1), and the poste-\nrior distribution of the word-level concept index\nzmj, i.e., p(zmj|em, am, {(\u03bck, \u2211k)}=1).\nVariational Distributions. These posterior dis-\ntributions are intractable; we therefore resort to\nvariational inference (Jordan et al., 1998; Blei et al.,"}]}, {"title": "4 Theoretical Analysis", "content": "2003) and use variational distributions q(0m|Ym)\nand q(zmj|mj) to approximate them. Here Ym \u2208\nRK and mj [mjk]K=1 \u2208 RK are variational\nparameters to be estimated during inference. This\nleads to the following joint variational distribution:\nq(0m, {Zmj}jm1|7m, {mj}jm1)\nq(0mm)\u00b7 \u041f j=1q(zmjmj).\n(1)\nEvidence Lower Bound. For each document m,\nfinding the optimal variational distributions is then\nequivalent to maximizing the following evidence\nlower bound (ELBO):\nL(Ym> {$mj}j=1; \u03b1, {(\u03bc\u03ba, \u03a3\u03ba)}k=1)\nEq[log p(@ma)] + Eq[logp(zmj|0m)]\n+ Eq[log p(emj/zmj, zmj, zmj)]\nEg [log q(0m)] \u2013 \u03a3; Eq[log q(zmj)], (2)\nwhere the expectation is taken over the joint varia-\ntional distribution in Eq. 1.\nLikelihood with Continuous Word Counts.\nOne key difference between VALC and typical\ntopic models (Blei et al., 2003; Blei, 2012) is the\nvirtual continuous (real-valued) word counts (dis-\ncussed in Sec. 3.3). Specifically, we define the\nlikelihood in the third term of Eq. 2 as:\np(emj|zmj, zmj, zmj) = [N(emj; \u00b5mj, Emj)]Wmj. (3)\nNote that Eq. 3 is the likelihood of Wmj (virtual)\nwords, where Wmj is a real value derived from\nthe FLM's attention weights (details in Sec. 3.3).\nTherefore, in the third item of Eq. 2, we have:\nEq [log p(emj|zmj, zmj, zmj)]\n= \u03a3. \u03a6mjkWmj log N(emj|\u00b5k, \u03a3k)\nk\n= \u2211 @mjkWmj{--(emj \u2013 \u03bck) (emj \u2013 k)\nk\nlog[(2\u03c0)d/2|2k|1/2]}. (4)\nUpdate Rules. Taking the derivative of the ELBO\nin Eq. 2 w.r.t. Omjk (see Appendix A for details)\nand setting it to 0 yields the update rule for $mjk:\nmjk X Wmj\n\u03a3\u03ba 1/2\nexp[\u03a8(mk) \u2013 \u03a8(\u03a3/mk')\n-1 (emj \u2013 \u03bck) (emj \u2013 \u03bck)], (5)\nK\nk=1\nwith the normalization constraint \u22111 mjk = 1.\nYmk\n\u03b1\u03ba +\nJm\nj=1\nOmjkWmj,\n(6)"}, {"content": "In this section, we provide theoretical guarantees\nof VALC on the four properties in Definition 3.1.\nMulti-Level Structure. As shown in Alg. 1,\nVALC (1) learns the dataset-level interpretation\n{k}1 describing the K concepts, (2) infers the\ndistribution of document-level interpretation \u04e9\u0442\nfor document m, i.e., q(0m|Ym) (parameterized\nby ym), and (3) infers the posterior distribution of\nword-level concept index, i.e., q(zmj|\u03a6mj), param-\neterized by mj. Such three-level interpretations\ncorrespond to Property (1) in Definition 3.1.\nNormalization. The learned variational distribu-\ntion q(0m|ym) (described in Eq. 1) is a Dirichlet\ndistribution; therefore we have \u03a3\u03ba=10mk = 1.\nThe update of $mj (Eq. 5) is naturally constrained\nby k=1mjk = 1 since $mj parameterizes a\nCategorical distribution (over zmj).\nAdditivity. VALC is able to perform Concept\nEditing, i.e, add/subtract the learned concept acti-\nvation \u03bc\u03b5 from FLMs via the following Quadratic\nProgramming (QP) problem (x = [xk]K_1):\nminx\u2208RK\nX uk ||2,\nsubject to x > 0 and \u03a3"}, {"title": "5 Experiments", "content": "K\nk=1\nem ||2, (8)\n\u03a31. Given\nlearned concepts {(\u03bc\u03ba, \u03a3k)}=1, VALC ob-\ntains this QP's optimal solution x* \u2208 RK and\nadd/subtract any concept k from arbitrary FLM\nembedding em by: em \u2190 em \u00b1 xuk. Alg. 2\nsummarizes this concept editing process; one can\nalso replace emj with the CLS embedding cm for\ndocument-level editing (details in Appendix D).\nMutual Information Maximization. Theo-\nrem 4.1 below shows that our inferred document-\nlevel and word-level interpretation, Om and\n{m} 1, satisfy Property (4), Mutual Informa-\ntion Maximization, in Definition 3.1.", "subsections": [{"title": "5.1 Experiment Setup", "content": "Datasets. We use three datasets in our experiments,\nnamely 20 Newsgroups, M10 (Lim and Buntine,\n2015), and BBC News (Greene and Cunningham,\n2006). For preprocessing details, see Appendix C.\nBaselines. We compare our method with the\nfollowing state-of-the-art baselines:\n\u2022 SHAP and LIME (Lundberg and Lee, 2017;\nRibeiro et al., 2016) are interpretation meth-\nods that attribute importance scores to input\nfeatures. In this paper, we use embeddings of\n'CLS' token as input to SHAP/LIME.\n\u2022 BERTopic (Grootendorst, 2020) is a\nclustering-based model that uses HDB-\nSCAN (McInnes and Healy, 2017) to cluster\nsentence embeddings from BERT, performs\nUniform Manifold Approximation Projection\n(UMAP) (McInnes et al., 2018), and then\nuses class-based TF-IDF (c-TF-IDF) to obtain\nwords for each cluster."}, {"title": "5.2 Comparison on Four Properties\nin Definition 3.1", "content": "In Sec. 4 we show that VALC satisfies the four\nproperties of conceptual interpretation in Defini-\ntion 3.1. In contrast, baseline models do not neces-\nsarily learn concepts that meet these requirements.\nTable 1 summarizes the comparison between VALC\nand the baselines. We can see that VALC is superior"}, {"title": "5.3 Results", "content": "Accuracy Gain. We perform greedy concept edit-\ning (Koh et al., 2020) for BERTopic, CETopic, and\nour VALC to evaluate the quality of their learned\nconcepts. Higher accuracy gain after pruning indi-\ncates better performance.\nTable 2 show the results for different methods\nin three real-world datasets, where 'Finetune (Or-\nacle)' refers to finetuning both the backbone and\nthe classifier of BERT. VALC's concept editing can\nimprove the accuracy upon the unedited model by\nmore than 11% in 20 Newsgroups and M10, almost"}, {"title": "5.4 Conceptual Interpretation (More for\nDifferent Tasks in Appendix F)", "content": "on par with 'Finetune (Oracle)'. Compared with\nthe baselines, VALC achieves the most accuracy\ngain in 20 Newsgroups and M10 and the second\nmost accuracy gain in BBC News, demonstrating\nthe effectiveness of VALC's four properties in Def-\ninition 3.1. Note that SHAP and LIME both inter-\npret the CLS token's embedding and therefore has\nidentical accuracy gain (details in Appendix D).\nAblation Study. Thanks to its full additivity\n(Definition 3.1), VALC is capable of different con-\ncept editing schemes, including 'Random', \u2018Un-\nweighted', and \u2018Weighted'. Specifically, weighted\npruning uses the concept editing algorithm in Alg. 2\nwith the optimal hyperparameter w; unweighted\npruning runs Alg. 2 with w = 1; random pruning\nfirst randomly picks a concept k (k \u2208 {1, ..., K}),\nsets w\u2022xk = 1/K, and then runs Alg. 2. Table 3\nshows accuracy for VALC's different schemes. As\nexpected, random pruning barely improves upon\nthe unedited model. Unweighted pruning improves\nupon the unedited model by 1.5 ~ 3.5%. Weighted\npruning improves the accuracy by around 11%\nupon the unedited model on 20 Newsgroups and\nM10.\nFaithfulness. Table 4 shows the faithfulness of\nVALC and baselines on the 20 Newsgroups, M10,\nand BBC News datasets. These results show that\nour VALC significantly outperforms the baseline\nmodels, achieving the highest faithfulness accu-\nracy scores in the 20 Newsgroups (89.8%), M10\n(99.5%), and BBC News (100.0%) datasets.\nNote that the dataset size of 20 Newsgroups,\nM10, and BBC News is 16,309, 8,355, and 2,225,\nrespectively. BBC News contains significantly less\ndata, making it easier to achieve a high faithfulness\nscore. This explains why both CETopic and our\nVALC obtain a faithfulness score of 100.0%.\nBaseline methods such as BERTopic and CE-\nTopic represent language concepts as discrete bags\nof words, which lack flexibility and accuracy. In\ncontrast, VALC infers continuous concepts for\ndatasets, documents, and words with theoretical\nguarantees. Consequently, it provides optimal and\nfaithful conceptual explanations of high quality.\nSee Appendix G for more quantitative results."}]}, {"title": "6 Conclusion", "content": "We address the challenge of multi-level interpre-\ntations for FLM predictions by defining concep-\ntual interpretation and introducing VALC, the first\nmethod to infer such interpretations effectively.\nEmpirical results are promising, and theoretical\nanalysis confirms that VALC reliably produces op-\ntimal conceptual interpretations by our definition."}, {"title": "7 Limitations", "content": "Our proposed method assumes access to the hidden\nlayers of Transformer-based models, and therefore\ncan be naturally extended to Transformer-based\nmodels including RoBERTa (Liu et al., 2019), De-\nBERTa (He et al., 2021), ALBERT (Lan et al.,\n2019), Electra (Clark et al., 2020), and decoder-\nonly models, such as GPTs (Radford et al., 2019;\nBrown, 2020). Although our VALC is initially\ndesigned for Transformer-based models, it is also\ngeneralizable to other architectures, such as Convo-\nlutional Neural Networks (CNNs) (LeCun et al.,\n2015) and Long Short-Term Memory networks\n(LSTMs) (Hochreiter and Schmidhuber, 1997), by\nsimply setting identical attention weights. Future\nwork may include extending VALC beyond Trans-\nformer variants and natural language applications.\nHowever, many other foundation language models\nprovided by proprietary sources may not expose\ntheir internal states, limiting the applicability of\nour method in such cases."}, {"title": "8 Ethical Considerations", "content": "VALC, as the first to comprehensively interpret\nFLMs at the concept level, holds significant\npromise for advancing societal and technologi-\ncal progress. By elucidating the inner workings\nof these complex FLMs, we enable greater trans-\nparency and trust in AI systems, which is crucial for\ntheir widespread adoption. This transparency en-\nsures that AI-driven decisions in critical areas such\nas healthcare, law, and finance are more explain-\nable and accountable, thus safeguarding against\nbiases and errors. Additionally, our VALC fosters\nenhanced collaboration between AI and human ex-\nperts, as interpretable models can provide insights\nthat are more easily understood and acted upon\nby domain specialists. This symbiotic relationship\nhas the potential to accelerate innovation, improve\ndecision-making processes, and ultimately lead to\nmore ethical and equitable AI applications, thereby\nbenefiting society at large."}, {"title": "A Details on Learning VALC", "content": "Update Rules. Similar to Sec. 3.4.1 of the main paper", "0": "n$\\frac{\\partial L"}, {"\u03bc\u2081": "n$\\mu_k = \\frac{\\sum_{m", "Ei": "n$\\Sigma_k \\sim IW(\\Lambda_0", "as": "n$\\mu_k \\leftarrow E_{NIW} [\\mu_k"}]}