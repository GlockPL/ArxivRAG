{"title": "Allo-AVA: A Large-Scale Multimodal Conversational AI Dataset\nfor Allocentric Avatar Gesture Animation", "authors": ["Saif Punjwani", "Larry Heck"], "abstract": "The scarcity of high-quality, multimodal training data severely hinders the creation of lifelike avatar animations for conversational AI in virtual environments. Existing datasets often lack the intricate synchronization between speech, facial expressions, and body movements that characterize natural human communication. To address this critical gap, we introduce Allo-AVA, a large-scale dataset specifically designed for text and audio-driven avatar gesture animation in an allocentric (third person point-of-view) context. Allo-AVA consists of ~1,250 hours of diverse video content, complete with audio, transcripts, and extracted keypoints. Allo-AVA uniquely maps these keypoints to precise timestamps, enabling accurate replication of human movements (body and facial gestures) in synchronization with speech. This comprehensive resource enables the development and evaluation of more natural, context-aware avatar animation models, potentially transforming applications ranging from virtual reality to digital assistants.", "sections": [{"title": "1 Introduction", "content": "Recent advances in generative, Transformer-based methods (Vaswani et al., 2017) for Natural Language Processing (NLP) combined with state-of-the-art virtual reality technology have opened new avenues for human-computer interaction. However, creating lifelike avatar animations remains challenging, primarily due to the scarcity of high-quality, multimodal training data that captures the synchronization between speech, facial expressions, and body movements.\nExisting datasets often lack precise speech-gesture synchronization, focus on isolated communication aspects, or fail to capture the allocentric perspective crucial for natural gesture generation in virtual environments (Kucherenko et al., 2020;\nYoon et al., 2020). These limitations result in unnatural or misaligned avatar animations, particularly in complex, context-dependent scenarios (Yoon et al., 2019; Ginosar et al., 2019).\nTo address these gaps, we introduce Allo-AVA\u00b9, a large-scale multimodal dataset designed for text and audio-driven avatar animation from an allocentric viewpoint. Allo-AVA provides comprehensive data to train models that capture the relationships between linguistic content, acoustic features, visual cues, and conversational context, enabling more natural and contextually appropriate avatar animations."}, {"title": "2 Dataset Creation", "content": "The Allo-AVA dataset was curated using a comprehensive pipeline that leverages diverse online video sources to capture a wide range of human communicative behaviors. This section details the data collection process, processing pipeline, and keypoint extraction methods."}, {"title": "2.1 Data Collection Process", "content": "We constructed a list of 140 unique search queries focusing on allocentric gestures and nonverbal communication in various contexts. Using the YouTube Data API v3, we retrieved the top 50 video results for each query, resulting in approximately 7,500 unique video URLs. To ensure quality and relevance, we applied the following filters:\n\u2022 Video duration: \u2265 5 minutes\n\u2022 Minimum view count: 10,000 views\n\u2022 Language: English\n\u2022 Categories: Education, Entertainment, Science & Technology, News & Politics\nThis filtering process yielded a diverse set of 7,500 unique video URLs, spanning a wide range of topics and communication styles."}, {"title": "2.2 Data Processing Pipeline", "content": "Our data processing pipeline, illustrated in Figure 1, consists of several key steps designed to extract and align multimodal data from the collected videos (Ginosar et al., 2019; Ferstl and McDonnell, 2020)."}, {"title": "2.2.1 Video and Audio Extraction", "content": "We utilized the yt-dlp library to download videos in the highest available quality, typically 1080p at 30 fps. Audio tracks were extracted using the moviepy library and saved as 16-bit PCM WAV files with a 48 kHz sampling rate (Wang et al.,\n2023)."}, {"title": "2.2.2 Transcription", "content": "For generating accurate transcriptions with word-level timestamps, we employed OpenAI's Whisper ASR model (Radford et al., 2022). We used the \"base\" model for efficiency, achieving a Word Error Rate (WER) of approximately 6% on our validation set. An example of the transcript output is shown below:"}, {"title": "2.2.3 Keypoint Extraction", "content": "A key innovation in our pipeline is the keypoint extraction process, which combines two state-of-the-art pose estimation models: OpenPose (Cao et al., 2018) and MediaPipe (Lugaresi et al., 2019). This dual approach allows us to capture a comprehensive set of body keypoints with high accuracy and detail. Figure 2 illustrates our keypoint extraction process.\nOpenPose OpenPose provides robust full-body pose estimation, extracting 18 keypoints corresponding to major body joints. It uses a bottom-up approach, first detecting parts and then associating them with individuals, making it effective for capturing large-scale body movements and handling multiple people in a scene (Cao et al., 2018).\nMediaPipe MediaPipe offers 33 additional keypoints with enhanced detail on hands and facial landmarks. It uses a top-down approach, first detecting the person and then estimating the pose, which allows for more precise capture of subtle gestures (body and facial) (Lugaresi et al., 2019).\nCombining OpenPose and MediaPipe To leverage the strengths of both models, we implemented a novel fusion algorithm. For each frame, we extract keypoints using both OpenPose and MediaPipe. We then align the keypoints from both models using a spatial matching algorithm. For keypoints detected by both models, we calculate a weighted average based on the confidence scores provided by each model. This approach allows us to benefit from OpenPose's robustness in full-body pose estimation and MediaPipe's precision in capturing fine-grained details, resulting in a more comprehensive and accurate set of keypoints.\nKeypoint Coordinate System Each keypoint is represented by x, y, and z coordinates, along with a visibility score. The coordinates are normalized as follows:\n\u2022 x: Horizontal position, normalized to [0, 1] from left to right of the frame\n\u2022 y: Vertical position, normalized to [0, 1] from top to bottom of the frame\n\u2022 z: Depth, normalized to [-1, 1], with 0 at the camera plane, negative values closer to the camera, and positive values farther away\n\u2022 visibility: A value in [0.0, 1.0] indicating the confidence of the keypoint's presence and location\nThe visibility score, derived from MediaPipe, represents the model's confidence in the keypoint's presence and accuracy. A score of 1.0 indicates high confidence, while lower scores suggest potential occlusion or uncertainty. This score is crucial for applications to determine which keypoints to consider in downstream tasks.\nThis normalization ensures that the keypoint data is consistent across different video resolutions and camera setups, facilitating easier processing and comparison across the dataset.\nOn average, we extracted 112,500 keypoints per minute of video, resulting in a total of approximately 135 billion keypoints across the entire dataset. The keypoints are stored in a JSON format, with each entry containing a timestamp, an array of keypoint dictionaries, and the corresponding transcribed text."}, {"title": "2.3 Dataset Statistics and Structure", "content": "A summary of the Allo-AVA dataset statistics are shown in Table 1. The Allo-AVA dataset is organized into several components, each stored in a separate directory:\n\u2022 video/: Contains the original MP4 video files\n\u2022 audio/: Stores the extracted WAV audio files\n\u2022 transcript/: Contains TXT files with word-level transcriptions and timestamps\n\u2022 keypoints/: Stores JSON files with frame-level keypoint data\n\u2022 keypoints_video/: Contains MP4 files visualizing the extracted keypoints overlaid on the original video\nThe entire pipeline was implemented in Python, leveraging GPU acceleration where possible to process the large volume of data efficiently. On average, processing a 10-minute video took approximately 15 minutes on eight NVIDIA A40 GPUs. The total processing time for the entire dataset amounted to approximately 3,000 GPU hours.\nTo provide deeper insights into the dataset, we conducted several correlational studies on the extracted keypoints. Table 3 presents some interesting findings that highlight the intricate relationships between various aspects of nonverbal communication captured in the Allo-AVA dataset."}, {"title": "3 Dataset Analysis", "content": "To gain deeper insights into the Allo-AVA dataset, we conducted a comprehensive analysis of the extracted keypoints and their relationship to speech patterns. This analysis provides valuable information about the distribution of body movements and their correlation with verbal communication, which is crucial for developing accurate avatar animation models."}, {"title": "3.1 3D Keypoint Distribution", "content": "Figure 3 presents a 3D scatter plot of the keypoint distribution across all videos in the dataset. The X, Y, and Z axes in this visualization represent the normalized spatial coordinates of the keypoints:\n\u2022 X-axis: Represents the horizontal position, normalized to [0, 1] from left to right of the video frame.\n\u2022 Y-axis: Represents the vertical position, normalized to [0, 1] from top to bottom of the video frame.\n\u2022 Z-axis: Represents the depth, normalized to [-1, 1], where 0 is at the camera plane, negative values are closer to the camera, and positive values are farther away.\nThis normalization ensures that the keypoint data is consistent across different video resolutions and camera setups, facilitating easier processing and comparison across the dataset. The color gradient on the Z-axis provides an additional visual cue for depth, with warmer colors (yellows and greens) representing points closer to the camera and cooler colors (blues and purples) representing points farther away.\nThis visualization offers several key insights into the spatial characteristics of our data. The keypoints span a wide range of 3D space, indicating that our dataset captures a diverse set of body poses and gestures (body movements and facial expressions). This diversity is crucial for training models that can generate natural and varied animations (Kucherenko et al., 2022; Ferstl and McDonnell, 2020).\nVisible clusters of keypoints, particularly in the central regions of the plot, likely correspond to common pose configurations, such as neutral standing positions or frequently used gestures. The depth information captured by the Z-axis is essential for creating realistic 3D avatar animations, as it allows for accurate representation of the spatial relationships between different body parts.\nSome keypoints appear as outliers, positioned far from the main clusters. These may represent more extreme or less common gestures, which are important to include for comprehensive coverage of human movement. The richness of this 3D keypoint data enables the development of models that can understand and generate complex, spatially-aware gestures, a significant advancement over 2D-only datasets."}, {"title": "3.2 Speech Rate vs. Movement Intensity", "content": "Figure 4 illustrates the relationship between speech rate (words per second) and movement intensity across the dataset. This analysis reveals several important aspects of the relationship between speech and gesture.\nThe plot shows a very weak positive correlation (0.0285) between speech rate and movement intensity. This suggests that, unlike common assumptions, there isn't a strong correlation between speech rate and gesture intensity in our dataset.. There is considerable variability in movement intensity across all speech rates, indicating that speakers in our dataset exhibit a wide range of gestural behaviors, regardless of how quickly they are speaking.\nSeveral data points show high movement intensity at various speech rates. These outliers may represent particularly emphatic gestures or moments of high emotion, which are valuable for training expressive avatar models. The high p-value (0.6249) indicates that the observed correlation is not statistically significant. This finding challenges simplistic assumptions about the relationship between speech rate and gesture intensity, highlighting the complex nature of human communication.\nThe lack of a strong correlation between speech rate and movement intensity underscores the importance of a large, diverse dataset like Allo-AVA. It suggests that effective avatar animation models need to consider factors beyond just speech rate to generate appropriate gestures (Kucherenko et al., 2019; Ahuja and Morency, 2019)."}, {"title": "3.3 Temporal Movement Intensity", "content": "To further understand the dynamics of movement in our dataset, we analyzed the temporal patterns of movement intensity across different body parts.\nFigure 5 presents a heatmap visualization of this analysis, offering several crucial insights into the nature of human motion captured in Allo-AVA.\nThe heatmap reveals differential movement patterns across body parts, with hands and arms generally showing higher movement intensities. This aligns with the expectation that gestures primarily involve upper limb movements. The vertical axis, representing time, allows observation of how movement intensity changes throughout interactions, noting periods of high intensity interspersed with relative stillness. This pattern reflects the natural rhythm of human communication and provides valuable data for generating realistic temporal dynamics in avatar animations (Kucherenko et al., 2018; Bhattacharya et al., 2021).\nImportantly, the heatmap includes detailed information on smaller body parts such as individual fingers and facial features, demonstrating the high level of detail in our keypoint extraction process. This granularity is crucial for creating nuanced and expressive avatar animations. Furthermore, patterns of coordinated movement across multiple body parts are visible, particularly in symmetrical limbs, which is essential for generating natural-looking avatar animations that maintain appropriate relationships between different body segments."}, {"title": "3.4 Distribution of Gesture Types", "content": "To assess the diversity of gestures in our dataset, we categorized them into four main types: beat, iconic, deictic, and metaphoric. Figure 6 illustrates the distribution of these gesture types, demonstrating a relatively balanced representation crucial for training comprehensive gesture generation models (Ahuja and Morency, 2019; Alexanderson et al., 2020).\nBeat gestures, which are rhythmic movements often used to emphasize speech, are the most common in our dataset (Kucherenko et al., 2021b; Ferstl and McDonnell, 2018). This aligns with linguistic research suggesting that beat gestures are fundamental to natural speech patterns (McNeill, 1992). The significant presence of iconic (representing concrete concepts) and metaphoric (representing abstract ideas) gestures indicates that our dataset captures a wide range of communicative intents. While slightly less common, the presence of deictic (pointing) gestures ensures that models trained on this dataset can generate appropriate referential movements.\nThis diverse representation of gesture types enables the development of avatar animation systems capable of producing a wide range of naturalistic gestures, enhancing the expressiveness and communicative power of virtual avatars across various contexts and communication styles."}, {"title": "3.5 Cross-correlation of Speech and Gesture", "content": "To investigate the temporal relationship between speech and gesture, we performed a cross-correlation analysis between speech features (pitch and volume) and gesture intensity. Figure 7 presents the results of this analysis, revealing complex temporal dynamics in speech-gesture coordination.\nThe non-uniform nature of the correlation curves indicates a complex, non-linear relationship between speech features and gesture intensity. This complexity underscores the need for sophisticated models capable of capturing intricate temporal dynamics in avatar animation systems. Notably, pitch and volume show different correlation patterns with gesture intensity, with volume appearing to have a more immediate correlation while pitch shows more varied, longer-term correlations (Jonell et al., 2020; Ahuja and Morency, 2019).\nThe presence of both positive and negative lags in the correlation suggests that gestures can both anticipate speech events (negative lag) and react to them (positive lag). This bidirectional relationship is crucial for generating natural-looking gesture sequences that maintain appropriate temporal relationships with speech. Additionally, the quasi-periodic nature of the correlation curves, particularly for pitch, may reflect the rhythmic aspects of speech and gesture coordination, providing valuable insights for modeling the prosodic elements of gesture generation.\nOur comprehensive analysis demonstrates that the Allo-AVA dataset provides a nuanced and multifaceted representation of human communicative behavior. The spatial distribution of keypoints, temporal dynamics of movement, diverse gesture types, and complex speech-gesture relationships captured in the dataset offer a solid foundation for advancing the state-of-the-art in avatar animation. These insights enable the development of more natural, expressive, and context-aware animation models that go beyond simple correlations and capture the true complexity of human communication. Future work leveraging the Allo-AVA dataset can focus on integrating these various aspects to create highly realistic and adaptive avatar animation systems capable of nuanced, context-appropriate gestural communication (Rebol and Knoche, 2021)."}, {"title": "4 Baseline Experiments and Results", "content": "To demonstrate the utility of the Allo-AVA dataset, we implemented a basic Large Body Language Model (LBLM) architecture and conducted experiments to evaluate its performance."}, {"title": "4.1 LBLM and Experimental Setup", "content": "The LBLM generates human-like gestures (body and facial movements) and responses from multimodal inputs. We define the LBLM inference problem as generating an optimal multimodal gesture sequence G* given a multimodal input sequence X and conversational context C:\n$G^* = \\arg \\max_G p_\\theta(G | X, C)$    (1)\nwhere $X = {(T_t, A_t, V_t)}_{t=1}^{T}$, with $T_t$ representing text, $A_t$ audio, and $V_t$ video at time t, and $p_\\theta$ denotes the model parameterized by \u03b8.\nFigure 8 illustrates the high-level architecture of the LBLM. The LBLM consists of three main components:\n1) Input Layer: Accepts multimodal input sequence X.\n2) Multimodal Embedding: Transforms inputs into a joint embedding space using an encoder function E(.).\n3) Output Generation: Produces an optimal gesture sequence G* with a mesh-mapped output based on the multimodal embedding and context.\nWe split the Allo-AVA dataset into training (80%), validation (10%), and test (10%) sets. The baseline LBLM was trained on 8 NVIDIA A40 GPUs for 2 days, using a batch size of 32 and the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-4.\nFor evaluation, we used the following metrics: Fr\u00e9chet Gesture Distance (FGD), Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017; Liu et al., 2022; Neff et al., 2007), and Average Pairwise Distance (APD). The APD is calculated as:\n$APD = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=i+1}^{N} ||p_i - p_j||$    (2)\nwhere N is the batch size and pi, pj are pose vectors of the i-th and j-th generated gestures."}, {"title": "4.2 Results and Discussion", "content": "Table 4 presents a comparison of our baseline LBLM with existing approaches. The baseline LBLM, trained on Allo-AVA, outperforms existing approaches across all metrics. We observed a 35% reduction in FGD and a 25.6% improvement in FID compared to the best previous model (Speech2Gesture). The APD of 0.84 indicates an 18.3% improvement in gesture diversity.\nTo assess perceptual quality, we conducted a human evaluation study with 50 participants rating the naturalness and appropriateness of gestures on a scale of 1 to 5. Results are shown in Table 5.\nTo analyze gesture coherence, we introduced a Temporal Coherence Score (TCS):\n$TCS = \\frac{1}{T-1} \\sum_{t=1}^{T-1} cos(g_t, g_{t+1})$    (3)\nwhere T is the total frames and gt is the gesture vector at time t. Our model achieved a TCS of 0.89, compared to 0.81 for Speech2Gesture (Wang et al., 2023) and 0.76 for Gesticulator (Kucherenko et al., 2020), indicating smoother and more natural-looking animations.\nThese results underscore the importance of large-scale, diverse datasets like Allo-AVA in training models for complex tasks such as gesture generation. The comprehensive coverage of different speakers, contexts, and gesture types enables models to learn more nuanced relationships between speech content and corresponding gestures, paving the way for more natural and engaging virtual human interactions (Zhou et al., 2020; Tang et al., 2023)."}, {"title": "5 Conclusion and Future Work", "content": "We presented Allo-AVA, a large-scale multimodal dataset for avatar animation, which significantly advances the field by providing researchers with comprehensive, temporally-aligned data for developing and evaluating models. Our baseline experiments demonstrate the dataset's utility in improving gesture generation across multiple metrics.\nFuture work on Allo-AVA will focus on:\n\u2022 Expanding linguistic and cultural diversity to enable cross-cultural studies and globally adaptive systems.\n\u2022 Enhancing annotations with fine-grained labels for gestures, emotions, and semantic meanings (Kucherenko et al., 2019).\n\u2022 Incorporating multiview recordings and interaction scenarios to support 3D reconstruction and study of interactive behaviors.\n\u2022 Improving multimodal synchronization for capturing subtle expressions and movements (Ferstl and McDonnell, 2020).\n\u2022 Developing domain-specific subsets to facilitate targeted research in various contexts.\nThese enhancements will further our understanding of human communication and nonverbal behavior, supporting the development of more natural and engaging embodied conversational AI. We invite researchers from diverse fields to collaborate and leverage Allo-AVA in pushing the boundaries of avatar animation and human-computer interaction."}, {"title": "6 Limitations", "content": "While Allo-AVA and our baseline LBLM model demonstrate significant improvements in gesture generation, several limitations must be acknowledged. The dataset's primary focus on English-language content from Western cultures may restrict the model's ability to generate culturally appropriate gestures for non-Western contexts, potentially impacting its global applicability. The model may also struggle with highly complex or abstract gestures requiring deep contextual understanding. The significant computational resources required for training models on Allo-AVA could limit accessibility for researchers with constrained computing capabilities. Additionally, the current implementation may not be optimized for real-time performance, potentially limiting its immediate applicability in interactive systems requiring low-latency gesture generation. Lastly, while our model shows improved short-term gesture coherence, maintaining consistency over extended periods remains challenging and requires further investigation."}, {"title": "7 Ethical Considerations", "content": "The development and use of large-scale datasets and AI models for human behavior synthesis raise important ethical considerations. Privacy and data protection are paramount; although we have implemented strict anonymization and data handling protocols, the risk of potential re-identification in large-scale datasets cannot be entirely eliminated. There is also a potential for the model to learn and reproduce gestural stereotypes, necessitating ongoing analysis and implementation of debiasing techniques. The use of publicly available data for AI model training raises questions about informed consent, requiring clear communication about the potential uses of such content in AI research. As with any technology capable of generating human-like behavior, there is a potential for misuse in creating deceptive content. We strongly advocate for responsible use and the development of safeguards against such misuse, including watermarking techniques and detection tools for AI-generated content. Transparency and accountability are crucial; we are committed to maintaining openness about our models' capabilities and limitations, and we support efforts to establish industry-wide standards for ethical use of avatar animation technologies."}, {"title": "A Dataset Collection and Processing\nDetails", "content": ""}, {"title": "A.1 Keyword Selection", "content": "The Allo-AVA dataset was curated using a comprehensive list of 140 keywords and phrases related to allocentric gestures and nonverbal communication. These keywords were carefully selected to capture a wide range of communication scenarios and contexts. The full list of keywords used in our data collection process is as follows:\n\u2022 TED talk gestures\n\u2022 TED talk body language\n\u2022 TED talk hand movements\n\u2022 TED talk nonverbal communication\n\u2022 podcast gestures\n\u2022 podcast body language\n\u2022 podcast hand movements\n\u2022 podcast nonverbal communication\n\u2022 comedy show gestures\n\u2022 comedy show body language\n\u2022 comedy show hand movements\n\u2022 comedy show nonverbal communication\n\u2022 interview gestures\n\u2022 interview body language\n\u2022 interview hand movements\n\u2022 interview nonverbal communication\n\u2022 news anchor gestures\n\u2022 news anchor body language\n\u2022 news anchor hand movements\n\u2022 news anchor nonverbal communication\n\u2022 public speaking gestures\n\u2022 public speaking body language\n\u2022 public speaking hand movements\n\u2022 public speaking nonverbal communication\n\u2022 presentation gestures\n\u2022 presentation body language\n\u2022 presentation hand movements\n\u2022 presentation nonverbal communication\n\u2022 lecture gestures\n\u2022 lecture body language\n\u2022 lecture hand movements\n\u2022 lecture nonverbal communication\n\u2022 vlog gestures\n\u2022 vlog body language\n\u2022 vlog hand movements\n\u2022 vlog nonverbal communication\n\u2022 interview show gestures\n\u2022 interview show body language\n\u2022 interview show hand movements\n\u2022 interview show nonverbal communication\n\u2022 talk show gestures\n\u2022 talk show body language\n\u2022 talk show hand movements\n\u2022 talk show nonverbal communication\n\u2022 debate gestures\n\u2022 debate body language\n\u2022 debate hand movements\n\u2022 debate nonverbal communication\n\u2022 panel discussion gestures\n\u2022 panel discussion body language\n\u2022 panel discussion hand movements\n\u2022 panel discussion nonverbal communication\n\u2022 conference gestures\n\u2022 conference body language\n\u2022 conference hand movements\n\u2022 conference nonverbal communication\n\u2022 seminar gestures\n\u2022 seminar body language\n\u2022 seminar hand movements\n\u2022 seminar nonverbal communication\n\u2022 webinar gestures\n\u2022 webinar body language\n\u2022 webinar hand movements\n\u2022 webinar nonverbal communication"}, {"title": "A.2 Video Filtering Criteria", "content": "To ensure the quality and relevance of the collected videos, we applied the following filtering criteria:\n\u2022 Minimum duration: 5 minutes\n\u2022 Minimum view count: 10,000 views\n\u2022 Language: English\n\u2022 Categories: Education, Entertainment, Science & Technology, News & Politics\n\u2022 Video quality: At least 720p resolution\n\u2022 Content focus: Clear visibility of speaker's full body"}, {"title": "A.3 Extra Data Processing Pipeline\nInformation", "content": "Our data processing pipeline consists of several key steps designed to extract and align multimodal data from the collected videos. Figure 9 provides a detailed overview of this pipeline.\nThe pipeline includes the following major components:"}, {"title": "A.3.1 Video and Audio Extraction", "content": "We utilized the yt-dlp library to download videos in the highest available quality, typically 1080p at 30 fps. Audio tracks were extracted using the moviepy library and saved as 16-bit PCM WAV files with a 48 kHz sampling rate."}, {"title": "A.3.2 Transcription", "content": "For generating accurate transcriptions with word-level timestamps, we employed OpenAI's Whisper ASR model. We used the \"base\" model for efficiency, achieving a Word Error Rate (WER) of approximately 6% on our validation set."}, {"title": "A.3.3 Keypoint Extraction", "content": "Our keypoint extraction process combines two state-of-the-art pose estimation models: OpenPose and MediaPipe. This dual approach allows us to capture a comprehensive set of body keypoints with high accuracy and detail."}, {"title": "A.4 Keypoint Extraction and Fusion", "content": "The keypoint extraction and fusion process is a critical component of our pipeline. We leverage the strengths of both OpenPose (Cao et al., 2018) and MediaPipe (Lugaresi et al., 2019) to obtain a rich set of keypoints. Algorithm 3 details the fusion process.\nThis algorithm ensures that we leverage the strengths of both models, resulting in a more comprehensive and accurate set of keypoints."}, {"title": "A.5 Temporal Alignment", "content": "To ensure precise synchronization between keypoints, transcriptions, and audio, we developed a custom temporal alignment algorithm based on Dynamic Time Warping (DTW) (Sakoe and Chiba, 1978). Algorithm 2 outlines this process."}, {"title": "B Gesture Classification", "content": ""}, {"title": "B.1 Gesture Taxonomy", "content": "To categorize the diverse range of gestures in the Allo-AVA dataset, we adopted the gesture taxonomy (McNeill, 1992) and extended it to include additional categories relevant to our dataset. Table 6 presents an overview of this classification system."}, {"title": "C Speech-Gesture Correlation Analysis", "content": ""}, {"title": "C.1 Cross-modal Correlation Metrics", "content": "To analyze the relationship between speech and gesture, we employed several established cross-modal correlation metrics. Table 7 provides an overview of these metrics and their interpretations. These metrics provide a comprehensive view of the complex relationships between speech and gesture in our dataset (Cover and Thomas, 2006)."}, {"title": "D Advanced Keypoint Analysis", "content": ""}, {"title": "D.1 Principal Component Analysis of\nKeypoints", "content": "To better understand the underlying structure of our keypoint data, we performed Principal Component Analysis (PCA) (Jolliffe and Cadima, 2016) on the extracted keypoints. Figure 10 shows the distribution of keypoints projected onto the first two principal components.\nThis visualization reveals several key insights:\n\u2022 The wide spread of points indicates a high degree of variability in the captured poses, suggesting that our dataset encompasses a diverse range of gestures and body positions.\n\u2022 The presence of distinct clusters (e.g., the dense regions in the center and the sparser regions at the edges) may correspond to common pose types or gesture categories.\n\u2022 The continuous nature of the distribution suggests that our dataset captures smooth transitions between different poses, which is crucial for generating natural-looking animations."}, {"title": "D.2 Truncated Singular Value Decomposition\nof Keypoints", "content": "To further analyze the dimensionality of our keypoint data, we performed a truncated Singular Value Decomposition (SVD) (Trefethen and Bau III, 1997). Figure 11 displays the results of this analysis.\nThe truncated SVD visualization provides additional insights:\n\u2022 The overall shape of the distribution is similar to the PCA plot, confirming the consistency of our dimensional reduction techniques.\n\u2022 The more pronounced \"tails\" in the SVD plot may indicate the presence of rarer, more extreme poses or gestures in our dataset.\n\u2022 The density variations across the plot suggest that certain pose configurations are more common than others, which aligns with expected patterns in natural human movement."}, {"title": "D.3 MiniBatch K-means Clustering of\nKeypoints", "content": "To further investigate underlying structure and patterns within our keypoint data, we employed MiniBatch K-means clustering (Sculley, 2010), a variant of the K-means algorithm optimized for large datasets. Figure 12 presents the results of this clustering analysis."}, {"title": "D.4 Keypoint Extraction and Fusion", "content": "Our keypoint extraction process leverages two state-of-the-art pose estimation models: OpenPose and MediaPipe. Figure 13 shows the keypoint structure used by OpenPose, while Figure 14 illustrates the MediaPipe keypoint model."}, {"title": "D.4.1 OpenPose", "content": "OpenPose provides a robust framework for multi-person keypoint detection, employing a bottom-up approach that first detects body parts and then associates them with individuals. This method allows for efficient handling of multiple subjects within a single frame, a crucial feature for analyzing complex scenes. OpenPose detects 18 key body points, including major joints and select facial landmarks.\nThe 18 keypoints detected by OpenPose are as follows:\n0  Nose\n1  Neck\n2  Right Shoulder\n3  Right Elbow\n4  Right Wrist\n5  Left Shoulder\n6  Left Elbow\n7  Left Wrist\n8  Right Hip\n9  Right Knee\n10 Right Ankle\n11 Left Hip\n12 Left Knee\n13 Left Ankle"}, {"title": "D.4.2 MediaPipe", "content": "MediaPipe complements OpenPose by offering a more detailed keypoint model, particularly excelling in hand and facial feature detection. We specifically utilized MediaPipe's Pose Landmarker model, which detects 33 body keypoints. This expanded set of points allows for finer tracking of subtle movements and expressions, crucial for capturing the nuances of human communication.\nThe MediaPipe Pose Landmarker model identifies the following 33 keypoints:\n0  Nose\n1  Left Eye (Inner)\n2  Left Eye\n3  Left Eye (Outer)\n4  Right Eye (Inner)\n5  Right Eye\n6  Right Eye (Outer)\n7  Left Ear\n8  Right Ear\n9 Mouth (Left)\n10 Mouth (Right)\n11 Left Shoulder\n12 Right Shoulder\n13 Left Elbow\n14 Right Elbow\n15 Left Wrist\n16 Right Wrist\n17 Left Pinky\n18 Right Pinky\n19 Left Index\n20 Right Index\n21 Left Thumb\n22 Right Thumb\n23 Left Hip\n24 Right Hip\n25 Left Knee\n26 Right Knee\n27 Left Ankle\n28 Right Ankle\n29 Left Heel\n30 Right Heel\n31 Left Foot Index\n32 Right Foot Index\nThis comprehensive set of keypoints enables precise tracking of facial expressions, hand gestures, and full-body movements."}, {"title": "D.4.3 Fusion Algorithm", "content": "To leverage the strengths of both models, we developed a novel fusion algorithm that combines the outputs of OpenPose and MediaPipe. Algorithm 3 outlines this process."}, {"title": "E Ethical Considerations and Data\nPrivacy", "content": ""}, {"title": "E.1\nConsent and Usage Rights", "content": "All videos in the Allo-AVA dataset were collected from publicly available sources on YouTube. To ensure ethical use of the data, we implemented the following measures:\n\u2022 Adherence to YouTube's Terms of Service and API usage guidelines.\n\u2022 Exclusion of content marked as \"unlisted\" or \"private\" by content creators.\n\u2022 Implementation of a takedown procedure for content creators who wish to have their data removed from the dataset.\n\u2022 Creation of usage guidelines that prohibit the use of the dataset for individual identification or tracking purposes."}, {"title": "E.2 Anonymization Techniques", "content": "To protect the privacy of individuals in the dataset, we applied several anonymization techniques:\n\u2022 Face blurring in keypoint visualization videos using OpenCV's (Bradski, 2000) face detection and blurring algorithms.\n\u2022 Voice pitch modification in audio files using the librosa library (McFee et al., 2015), with random pitch shifts within a range that preserves intelligibility but alters voice characteristics.\n\u2022 Replacement of proper nouns in transcripts with generic placeholders using Named Entity Recognition (NER) techniques from the spaCy library (Honnibal and Montani, 2017)."}, {"title": "E.3 Bias Mitigation", "content": "To address potential biases in the dataset", "strategies": ""}]}