{"title": "Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation", "authors": ["Shuai Zhao", "Xiaobao Wu", "Cong-Duy Nguyen", "Meihuizi Jia", "Yichao Feng", "Luu Anh Tuan"], "abstract": "Parameter-efficient fine-tuning (PEFT) can bridge the gap between large language models (LLMs) and downstream tasks. However, PEFT has been proven vulnerable to malicious attacks. Research indicates that poisoned LLMs, even after PEFT, retain the capability to activate internalized backdoors when input samples contain predefined triggers. In this paper, we introduce a novel weak-to-strong unlearning algorithm to defend against backdoor attacks based on feature alignment knowledge distillation, named W2SDefense. Specifically, we first train a small-scale language model through full-parameter fine-tuning to serve as the clean teacher model. Then, this teacher model guides the large-scale poisoned student model in unlearning the backdoor, leveraging PEFT. Theoretical analysis suggests that W2SDefense has the potential to enhance the student model's ability to unlearn backdoor features, preventing the activation of the backdoor. We conduct experiments on text classification tasks involving three state-of-the-art language models and three different backdoor attack algorithms. Our empirical results demonstrate the outstanding performance of W2SDefense in defending against backdoor attacks without compromising model performance\u00b9.", "sections": [{"title": "1 Introduction", "content": "Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains. As the number of parameters in LLMs increases, full-parameter fine-tuning becomes challenging, which requires substantial computational resources. To address this issue, a series of parameter-efficient fine-tuning (PEFT) algorithms, such as LoRA , p-tuning , and FourierFT , have been proposed. These PEFT methods update only a small number of model parameters, offering an effective alternative to fine-tune LLMs for downstream tasks.\nMuch like a coin has two sides, despite PEFT achieving impressive performance, they are criticized for their susceptibility to backdoor attacks. Recent research indicates that if third-party LLMs are implanted with backdoors, these backdoors can still be activated even after PEFT . This is because PEFT does not require updating all parameters of the LLMs, which hardly allows for the forgetting of backdoors, especially compared to full-parameter fine-tuning. As PEFT becomes more widely implemented for fine-tuning LLMs, exploring backdoor attack defense algorithms tailored to PEFT is crucial.\nFor the backdoor attack, the fundamental concept involves adversaries strategically corrupting the training dataset to internalize malicious functionalities within the language model through training . In the model testing phase, when encountering the predefined trigger, the model will consistently output content as specified by the adversaries . Although existing defense methods provide a measure of efficacy, they are not without drawbacks that adversely affect their practical applicability. On one hand, the majority of defense algorithms tend to sacrifice the normal performance of the model to achieve enhanced defensive capabilities. On the other hand, as the number of model parameters increases, defense algorithms based on backdoor unlearning that rely on full-parameter fine-tuning, which requires substantial computational resources, become more challenging to implement. Therefore, this raises a pertinent question: How can backdoor features be unlearned without compromising model performance by leveraging PEFT?\nTo address the above issues, in this study, we propose a novel unlearning algorithm to defend against backdoor attacks, Weak-to-Strong Defense (W2SDefense), which enables a poisoned student model to unlearn backdoors through knowledge distillation from a clean teacher model. Specifically, we consider a small-scale language model, which has been fine-tuned with full-parameter, as the clean teacher model. Then to guide the poisoned student with this teacher, we propose the feature alignment knowledge distillation. It aligns the features of the student model to the teacher model by through PEFT, which only update a small number of parameters. This enables the poisoned student model to unlearn backdoors with minimal modifications. Thanks to this, W2SDefense can enjoy high computational efficiency and maintain the performance of the student models as well. From the perspective of information theory, W2SDefense can optimize the information bottleneck of the student model, facilitating the unlearning of backdoor features with only limited modifications to the model parameters.\nWe construct extensive experiments to investigate the efficacy of our W2SDefense method, which include three datasets with various attack algorithms. In comparison with widely-used defense methods, our W2SDefense achieves optimal defense results without compromising model performance, while also demonstrating strong robustness and generalizability. To summarise, our contributions are as follows:\n\u2022 We propose W2SDefense, a novel unlearning algorithm for defense against backdoor attacks. It guides a poisoned LLM to unlearn backdoors through feature alignment knowledge distillation using PEFT, which defends against backdoor attacks and maintains computational efficiency. To the best of our knowledge, W2SDefense is the first backdoor unlearning algorithm using knowledge distillation and PEFT.\n\u2022 We theoretically and empirically demonstrate the effectiveness of feature alignment knowledge distillation in defense against backdoor attacks. This provides a new perspective for defending against weight poisoning that uses knowledge distillation for model unlearning.\n\u2022 This study enriches the understanding of leveraging knowledge distillation for defense against backdoor attacks, highlights the significance of establishing comprehensive backdoor unlearning mechanisms within the NLP community, and provides insightful perspectives for ensuring LLM security."}, {"title": "2 Preliminary", "content": "In this section, we present the threat model concerning backdoor attacks and defenses, and highlight the potential security vulnerabilities of PEFT."}, {"title": "2.1 Threat Model", "content": "We introduce the problem formulation of threat models on addressing backdoor attacks in text classification, specifically focusing on defending against poisoned weights. Without loss of generality, this formulation can be broadly applicable to additional NLP tasks. Consider a third-party LLM $f$ that has been compromised by a malicious attacker through backdoor attacks, which allows the model's responses to be manipulated by specific triggers:\n$\\forall x \\in D_{\\text{clean}}^{\\text{test}}, f(x) = y;$\n$\\forall x' \\in D_{\\text{poison}}^{\\text{test}}, f(x') = y_b;$\nwhere $(x, y) \\in D_{\\text{clean}}^{\\text{test}}$ denotes clean test dataset; $(x', y_b) \\in D_{\\text{poison}}^{\\text{test}}$ stands for poisoned test dataset; $x'$ represents poisoned test samples that contain specific triggers; $y_b$ stands for target label. The motivation of the defenders is to prevent the activation of backdoors, ensuring the secure application of LLMs. Consequently, we assume that the defenders have access to the poisoned LLMs $f$ and possess clean training dataset $D_{\\text{clean}}^{\\text{train}}$. In our study, we wish to reduce the likelihood of backdoor activation through unlearning. Therefore, the key concept of unlearning backdoor attacks can be distilled into two objectives:\nObj. 1: $\\forall x \\in D_{\\text{clean}}^{\\text{test}}, CA(f'(x)) \\thickapprox CA(f(x))$,\nObj. 2: $\\forall x' \\in D_{\\text{poison}}^{\\text{test}}, ASR(f'(x)) < ASR(f(x))$,\nwhere $f'$ denotes the defended LLMs; ASR stands for attack success rate; CA represents the clean accuracy. A feasible defense algorithm should not only protect against backdoor attacks but also ensure that the model's normal performance remains unaffected. Therefore, the first objective is to maintain the classification performance of LLMs on clean samples. When leveraging PEFT, such as"}, {"title": "2.2 Potential for Vulnerabilities in PEFT", "content": "Previous research has shown that models compromised by backdoor attacks retain their trigger patterns even after fine-tuning with PEFT algorithms. This persistence is attributed to the fact that PEFT only updates a small subset of model parameters, which may hardly facilitate the \u201cforgetting\" of the backdoor, in alignment with the principles of the information bottleneck theory :\nTheorem (Information Bottleneck): In the supervised learning setting, the optimization objective of the model is to minimize the training loss:\n$l[p(x|x)] = I(X; \\widehat{X}) \u2013 \\beta I(\\widehat{X}; Y)$,\nwhere I denotes the mutual information; $\\beta$ represents the Lagrange multiplier; $\\widehat{X} \\in \\widehat{X}$ stands for intermediate feature; $x \\in X$ denotes the input, and Y represents the output of the model.\nThe core of information bottleneck theory lies in retaining the most useful information $\\widehat{X}$ about the output Y while minimizing the information about the input X. However, in PEFT, only a few parameters are updated, which means that the information bottleneck formed during the poisoning phase may remain unchanged during the fine-tuning process, making it difficult for the model to forget the backdoor."}, {"title": "3 Backdoor Unlearning", "content": "In light of the limitations presented by PEFT in fully eradicating the effects of backdoors, exploring novel defense algorithms is necessary. Knowledge distillation , whereby a student model assimilates behavior from a teacher model, emerges as a potential solution. This method provides an unlearning mechanisms by reconstructing the knowledge base, effectively mitigating internalized backdoors. Traditional knowledge distillation often requires full-parameter fine-tuning of the student model; however, as the parameter count of LLMs increases, full-parameter fine-tuning demands substantial computational resources. Consequently, a natural question arises: How can knowledge distillation be utilized to defend against backdoor attacks targeting PEFT?\nTo address the aforementioned issue, this study introduces a weak-to-strong backdoor unlearning algorithm via feature alignment knowledge distillation (W2SDefense). The fundamental concept of W2SDefense is that a small-scale teacher model is trained through full-parameter fine-tuning on the clean training dataset $D_{\\text{clean}}^{\\text{train}}$. Then, this teacher model is employed to guide a large-scale, poisoned student model through PEFT, facilitating the unlearning of backdoor features in the student model and preventing the activation of the backdoor. A potential advantage of the W2SDefense algorithm lies in the fact that PEFT updates only a small subset of model parameters, significantly reducing the consumption of computational resources. Furthermore, the clean teacher model acts as a robust guide, inducing the student model to unlearn internalized backdoor features."}, {"title": "3.1 Clean Teacher Model", "content": "In traditional knowledge distillation, the choice of the teacher model prioritizes its complexity and expressiveness , which frequently results in a teacher model that exhibits greater complexity than the student model. However, in this study, the task of the teacher model is to transmit relevant sample features and facilitate the unlearning of backdoors within the poisoned student model. Therefore, we employ a smaller-scale BERT as the teacher model. Specifically, the teacher model $f_t$ is trained by performing full-parameter fine-tuning on the target dataset $D_{\\text{clean}}$. It should be noted that in order to facilitate feature alignment and knowledge transfer between the teacher and student models, we add an additional linear layer $g$ to the teacher model. This modification ensures that the feature dimensions outputted by the teacher model are consistent with those outputted by the student model:\n$z_t^{(L+1)} = g(z_t^{(L)}) = W_{\\text{dim}(d_s \\times d_t)} \\cdot z_t^{(L)}+b_{\\text{dim}(d_s)}$,\nwhere $W$ denotes the weight of the linear layer, and $b$ is the bias vector; $d_t$ and $d_s$ represent the feature dimensions of the teacher and student models, respectively; $L$ represents the last layer of the teacher model; $z_t$ denotes the logits output by the teacher model. Finally, the objective for optimizing the teacher model is:\n$L_t = E_{(x,y)\\sim D_{\\text{clean}}^{\\text{train}}} [l(g(f_t(x; \\theta_t)), y)_{f\\text{pft}}]$,\nwhere $f\\text{pft}$ denotes the full-parameter fine-tuning; $l$ is the cross-entropy loss;"}, {"title": "3.2 Poisoned Student Model", "content": "In our study, we assume that third-party LLMs such as LLaMA and Qwen , which serve as the student models $f_s$, have been poisoned. To reduce the consumption of computational resources, PEFT algorithms such as LoRA are used for optimizing large-scale student models to adapt to downstream tasks:\n$L_s = E_{(x,y)\\sim D_{\\text{clean}}^{\\text{train}}} [l(f_s(x; \\theta_s), y)_{\\text{peft}}]$,\nwhere $\\text{peft}$ denotes the parameter-efficient fine-tuning. Previous research indicates that PEFT, which updates only a limited subset of language model parameters, is insufficient for mitigating backdoors compared to full-parameter fine-tuning . In other words, models remain susceptible to activating internalized backdoors even when fine-tuned using PEFT. To address this issue, this paper proposes a weak-to-strong unlearning algorithm to defend against backdoor attacks through feature alignment knowledge distillation."}, {"title": "3.3 Weak-to-Strong Backdoor Unlearning via Knowledge Distillation", "content": "In this study, to facilitate the unlearning of backdoor features in poisoned student models, we propose the W2SDefense algorithm. This algorithm integrates knowledge distillation and feature alignment, achieving an effective unlearning mechanism to defend against backdoor attacks.\nKnowledge Distillation Unlearning Defending against backdoor attacks necessitates not only reducing the attack success rates but also maintaining the model's performance on clean samples. Therefore, in this study, we first employ cross-entropy loss to encourage the student model $f_s$ to learn the correct sample features, achieving Objective 1:\n$l_{ce}(\\theta_s) = CE(f_s(x; \\theta_s)_{\\text{peft}}, y)$,\nwhere $CE$ denotes the cross-entropy loss; $\\theta_s$ stands for the poisoned student model's parameters; training sample $(x, y) \\in D_{\\text{clean}}^{\\text{train}}$. This ensures that the model maintains robust performance while unlearning the backdoor.\nFurthermore, to facilitate the unlearning of backdoor features, knowledge distillation loss is employed, guiding the student model $f_s$ to learn from a smaller-scale, clean teacher model $f_t$, which aims to enable the poisoned student model to emulate the behavior of the teacher model. Specifically, we minimize the Kullback-Leibler (KL) divergence between the output logits of the teacher and student models:\n$P_t(x;\\theta_t)_{f\\text{pft}} = \\text{softmax}(\\frac{z_t}{\\tau})$,\n$P_s(x;\\theta_s)_{\\text{peft}} = \\text{log\\_softmax}(\\frac{z_s}{\\tau})$,\n$l_{kdu}(\\theta_s,\\theta_t)=\\frac{1}{T^2}\\sum_{i=1}^C P_t(x;\\theta_s)_{f\\text{pft}}\\cdot \\text{log}(\\frac{P_t(x;\\theta_s)_{f\\text{pft}}}{P_s(x;\\theta_t)_{\\text{peft}}} )$\nwhere $z_t$ and $z_s$ respectively represent the logits output by the clean teacher model and the poisoned student model; $\\tau$ stands for the temperature scaling factor."}, {"title": "Feature Alignment Unlearning", "content": "To facilitate the transfer of correct features from the clean teacher model to the poisoned student model and promote the unlearning of backdoor features, we introduce the feature alignment loss. This involves minimizing the Euclidean distance between the feature vectors of the teacher and student models:\n$E\\_\\text{distance} = ||h_t(x; \\theta_t)_{f\\text{pft}}-h_s(x; \\theta_s)_{\\text{peft}}||_2$,\n$l_{fau}(\\theta_t, \\theta_s) = \\text{mean}(E\\_\\text{distance}^2)$,\nwhere $h_s$ and $h_t$ correspond to the final hidden states of the clean student and poisoned teacher models, respectively. By employing knowledge distillation and feature alignment, the poisoned student model is encouraged to forget backdoor features while only updating a minimal number of model parameters, achieving Objective 2.\nOverall Training The optimization objective for the student model is formally defined as the minimization of a composite loss function, which encompasses cross-entropy, knowledge distillation, and feature alignment losses:\n$\\theta_s = \\underset{\\theta_s}{\\text{arg min}} l(\\theta_s)_{\\text{peft}}$,\nwhere the loss function $l$ is defined as:\n$l(\\theta_s)=\\alpha l_{ce}(\\theta_s)+\\beta \\cdot l_{kdu}(\\theta_t, \\theta_s)+\\gamma \\cdot l_{fau}(\\theta_t, \\theta_s)$.\nThis method effectively defends against backdoors by utilizing feature alignment knowledge distillation while mitigating the consumption of computational resources. The complete algorithm of W2SDefense is shown in Algorithm 1.\nCorollary: Variation in mutual information between the output Y and intermediate feature X:\n$I(\\widehat{X}; Y)_{\\text{peft}} \\leq I(\\widehat{X}^{\\text{W2SDefense}}; Y)_{\\text{peft}}$,\nwhere $\\widehat{X}$ represents intermediate feature of student model. In the W2SDefense algorithm, through feature alignment knowledge distillation, the student model increases mutual information $I(\\widehat{X}; Y)$, aligning the outputs of the student model with those of the teacher model, reducing sensitivity to the features of the backdoor."}, {"title": "4 Experiments", "content": "In this section, we first introduce the experimental details, including the dataset, evaluation metrics, attack algorithms, defense models, and experimental settings. Then, we analyze the performance of W2SDefense."}, {"title": "4.1 Experimental details", "content": "Dataset To validate the efficacy of W2SDefense, we select three text classification datasets: SST-2 , CR , and AG's News . IMDB serves as the proxy dataset for SST-2, and MR serves as the proxy dataset for CR to simulate backdoor attacks by poisoning the model weights. Due to the large size of the AG's News dataset, we choose 8,000 samples each for the proxy dataset and the training dataset.\nEvaluation metrics In our study, clean accuracy (CA) and attack success rate (ASR) serve as evaluation metrics representing the model's accuracy on clean samples and the proportion of poisoned samples outputting the target label, respectively.\nAttack algorithms To poison model weights, we select three backdoor attack algorithms: BadNet, InSent, and SynAttack. BadNet , which uses the rare characters \"mn\" as its specific trigger; InSent , employing the phrase \"I watched this 3D movie\" as its trigger; and SynAttack , leveraging the syntactic structure \"(S(SBAR)(,)(NP)(VP))\" as its specific trigger. To enhance the stealthiness of the attacks, all algorithms are implemented with clean-label, following.\nDefense models To demonstrate the effectiveness of W2SDefense, we compared it with several widely-used defense algorithms. These include ONION , which identifies triggers by calculating perplexity; SCPD , avoiding backdoor activation by rewriting syntactic structures; Back-Tr. , rewriting sentences with translation models; and Prune , which prunes and fine-tunes model weights to defend against backdoor attacks.\nExperimental settings We select three of the state-of-the-art LLMs as victim models: LLaMA3-8B , Vicuna-7B , and Qwen2.5-7B . For the weight poisoning stage, the number of poisoned samples is 1000, and the ASR of all pre-defined weight-poisoning attacks consistently exceeds 90% through full-parameter fine-tuning. The target labels for the three datasets are \u201cnegative\u201d, \u201cnegative\", and \"world\". For the defense phase, we use full-parameter fine-tuning for the teacher model and leverage LoRA as the fine-tuning method for the student models. Additionally, for the student model, we use the AdamW optimizer, set epochs to 5, the batch size to 32, the learning rate to 2e-4, the temperature scaling factor to 2, and r to 512. We set $\\alpha$ to {1.0, 5.0}, $\\beta$ to {0.001, 0.2}, and $\\gamma$ to {0.001, 0.2}, for different datasets and vicitim models. We also verify the effectiveness of various PEFT methods, which include p-tuning and prompt-tuning . All experiments are deployed on NVIDIA RTX A6000 GPUs."}, {"title": "4.2 Effectiveness of the W2SDefense", "content": "To verify the effectiveness of the W2SDefense algorithm, we conduct detailed experiments with different settings. The results of the experiments are shown in Tables 1 to 3, from which the following conclusions can be drawn:\nThe CA of W2SDefense fulfills Objective 1: Ideally, a feasible defense algorithm should maintain the model's normal performance without degradation. For instance, in the Vicuna model of Table 1, when faced with the BadNet backdoor attack, although the SCPD method can effectively reduce the ASR, it also leads to a 10.44% decrease in model accuracy. In contrast, our W2SDefense algorithm, while effectively countering backdoor attacks, simultaneously increases the CA by 0.65%. This demonstrates that W2SDefense, which utilizes feature alignment knowledge distillation, not only facilitates the unlearning of backdoor features but also assists the student model in learning the target task, thereby improving performance.\nW2SDefense achieves Objective 2 with significantly reduced ASR: Compared to previous defense algorithms, W2SDefense achieves optimal results in all settings under the premise of maintaining the model's clean accuracy. For example, as shown in Table 2, when facing the InSent backdoor attack, the poisoned model fine-tuned with the LORA algorithm has an average ASR of 94.04%. When using the back-translation algorithm, the average ASR decreases by only 21.56%; with the ONION algorithm, the average ASR increases by 1.24%. Although the Prune algorithm reduces the average ASR by 54.75%, it significantly decreases the model's CA in the Qwen model. In the W2SDefense algorithm, the average ASR is reduced by 82.89%, this phenomenon also observed in other datasets. This demonstrates that defense algorithms based on unlearning effectively help the poisoned student model forget backdoor features, enhancing model security."}, {"title": "The generalizability of W2SDefense", "content": "When confronted with more complex multi-class tasks, the W2SDefense algorithm consistently exhibits robust performance. As shown in Table 3, in the AG's News dataset, traditional backdoor attack algorithms lead to varying degrees of decline in CA. For example, when facing different attack methods in the Qwen model, the SCPD algorithm results in an average decline in CA of 10.94%. Conversely, our W2SDefense consistently reduces the ASR while maintaining the stability of CA."}, {"title": "4.3 Generalization and Ablation Studies", "content": "Poisoning Model uses Target Dataset In the aforementioned studies, we poisoned model weights using proxy datasets. Another potential backdoor attack scenario involves attackers having access to the datasets used for downstream tasks . Therefore, we evaluate the performance of W2SDefense when model weights are poisoned using the same dataset. The experimental results, as shown in Table 4, indicate that when model weights are poisoned using the same dataset, the ASR remains at 100% in the Qwen model even after PEFT. However, when faced with W2SDefense, the ASR drops to 5.83%, while the CA only decreases by 0.93%. This demonstrates the strong generalization performance of W2SDefense.\nDifferent Teacher Model We also validate the impact of using GPT-2 as the smaller-scale teacher model on defense performance."}, {"title": "Ablation Experiments", "content": "To verify the impact of different components on the performance of W2SDefense, we conduct ablation experiments on three LLMs, as shown in Table 7. First, by isolating different components, we find that compared to knowledge distillation loss, feature alignment loss is more conducive to unlearning backdoor. For example, in the LLaMA model, using only cross-entropy and feature alignment loss, the ASR is 5.39%. However, knowledge distillation loss also possesses the capability to unlearn backdoor; for instance, in the Qwen model, when using cross-entropy and knowledge distillation loss, the ASR reduces to 68.54%. Secondly, we demonstrate the impact of different ranks in LoRA on defense performance, as shown in Figure 2. It is evident that as r increases, LoRA is insufficient to unlearn backdoor. However, in W2SDefense, when r exceeds 16, the ASR rapidly decreases."}, {"title": "5 Conclusion", "content": "In this work, we focus on defending against backdoor attacks targeting poisoned model weights. To facilitate the forgetting of backdoors in PEFT, we propose a novel unlearning algorithm named W2SDefense, which leverages weak teacher models to guide large-scale student models in unlearning backdoors through feature alignment knowledge distillation. Empirical results indicate that our W2SDefense method can effectively reduce the attack success rate while maintaining the normal accuracy of the model. We hope our work can promote awareness of model security within the NLP community, especially regarding backdoor attacks."}, {"title": "Limitations", "content": "Although W2SDefense demonstrates viable defense capabilities, we recognize two limitations of the algorithm: (i) It relies on knowledge distillation, which requires access to model weights, limiting its utility in black-box scenarios. (ii) Despite utilizing smaller-scale teacher models, the approach still demands additional computational resources for training the teacher models."}]}