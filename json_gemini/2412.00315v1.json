{"title": "One Model for One Graph: A New Perspective for Pretraining with Cross-domain Graphs", "authors": ["Jingzhe Liu", "Haitao Mao", "Zhikai Chen", "Wenqi Fan", "Mingxuan Ju", "Tong Zhao", "Neil Shah", "Jiliang Tang"], "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful tool to capture intricate network patterns, achieving successes across different domains. However, existing GNNs require careful domain-specific architecture designs and training from scratch on each dataset, leading to an expertise-intensive process with difficulty in generalizing across graphs from different domains. Therefore, it can be hard for practitioners to infer which GNN model can generalize well to graphs from their domains. To address this challenge, we propose a novel cross-domain pretraining framework, \"one model for one graph,\" which overcomes the limitations of previous approaches that failed to use a single GNN to capture diverse graph patterns across domains with significant gaps. Specifically, we pretrain a bank of expert models, with each one corresponding to a specific dataset. When inferring to a new graph, gating functions choose a subset of experts to effectively integrate prior model knowledge while avoiding negative transfer. Extensive experiments consistently demonstrate the superiority of our proposed method on both link prediction and node classification tasks.", "sections": [{"title": "1 Introduction", "content": "As a ubiquitous data structure, graphs can represent a wide range of structural data across different domains, such as academia (Yang et al., 2016), e-commerce (Ying et al., 2018; Borisyuk et al., 2024; Fan et al., 2019; Tang et al., 2020), and molecule (Ying et al., 2021). Graph neural networks (GNNs) have exhibited great performance when learning and inferring on a single graph dataset. However, most GNNs fail to generalize across domains due to the feature heterogeneity problem, in which graphs from different sources often contain node features with varying semantic meanings and dimensions.\nRecently, feature dimension heterogeneity can be solved via two steps: (i) transform node features into textual descriptions (ii) employ Large Language Models (LLMs) to encode them into the aligned textual representation space. Multiple graph models (Liu et al., 2023a; Huang et al., 2023; Chen et al., 2024b,a) are then developed with inductive inference capability across graphs. Nonetheless, a recent benchmark (Chen et al., 2024b) reveals that, even within the aligned textual representation space, the positive transfer can only be found within the single domain, while the semantic disparity happens across different domains. Moreover, graphs from various domains exhibit significantly different structural properties. For example, the homophily property, a crucial factor affecting the node classification performance of GNNs, varies significantly across graphs. As noted by Mao et al. (2023), a single GNN struggles to capture varying levels of homophily simultaneously.\nThe aforementioned observations suggest that pretraining a single model for graphs from multiple domains is suboptimal. Therefore, in this work, we propose to individually pretrain one expert"}, {"title": "2 Related work", "content": "Cross-graph machine learning. The graph machine learning community has recently witnessed a growing trend to extend models designed for a single graph across different graphs (or datasets) (Mao et al., 2024). The key obstacle to cross-graph learning stems from feature and structural heterogeneity. Early endeavors typically address feature heterogeneity by neglecting the original features (Qiu et al., 2020) and adopting GNN-based self-supervised learning to extract transferrable structural patterns. However, such a strategy performs poorly on text-rich networks and suffers from negative transfer (Xu et al., 2023) due to the structure shift across different datasets. Zhao et al. (2024a) adopt dimensionality reduction to unify the feature dimension while features remain poorly aligned. To generate high-quality unified features across graphs, LLM and prompt engineering(Liu et al., 2023a)"}, {"title": "3 Method", "content": "In this section, we introduce our one model for one graph pretraining framework with cross-domain graphs, OMOG. It is an implementation of the new pipeline shown in Figure 1b. It consists of two stages - the pretraining stage and the inference stage. In the pretraining stage, OMOG will pre-train one model for one graph with one associated gate separately. In the inference stage, it will adaptively choose suitable experts for a test graph according to the associated gates and the test task. Before we detail these two stages, we start with introducing the problem formulation.\nProblem formulation: In this work, we focus on text-attributed graphs (TAGs), or more generally, text-space datasets (Chen et al., 2024b) whose features can be converted into text-attributes. An input graph can be defined as G = (V, A,S), where V = {V1, V2, ..., Vn} is the set of n nodes, and A \\in Rnxn represents the adjacency matrix of the graph, and S = {s1, s2, ...} is the set of text descriptions for all nodes. We focus on cross-graph pretraining with a transferring setting. Specifically, assuming that we are given N pretraining graphs {G1,\u2026\u2026,GN}, we would like to pretrain a model bank M with one model for each pretraining graph and then transfer knowledge in M to unseen test graphs. We focus on two downstream tasks: i.e., node classification and link prediction. For node classification, we aim to predict the category yr of the target node vi. For the link prediction, we predict whether there is a link between two target nodes vi and vj."}, {"title": "3.1 The Pretraining Stage", "content": "The whole pre-training process is illustrated in Figure 2, which contains the following steps:\n1. Incorporating attribute and structure information: To achieve cross-graph pre-training across diverse domains, we first adopt LLMs to generate node features for each graph in a unified text space. Based on the unified feature space, we adopt non-parametric message passing (Wu et al., 2019) to generate node-level embeddings incorporating structural information.\n2. Pretraining graph-specific experts: This step involves pre-training models that can effectively transfer to downstream datasets. As shown in (Xu et al., 2023), pre-training a single model across graphs with diverse structural properties results in negative transfer and catastrophic forgetting."}, {"title": "4 Experiment", "content": "In this section, we conduct comprehensive experiments to evaluate the effectiveness of our proposed method OMOG from the following perspectives:\n1. RQ1: Can our method effectively transfer pre-trained models to unseen test data in zero-shot and few-shot settings?\n2. RQ2: How does each component of our method influence the transfer effectiveness?\n3. RQ3: Why does expert gate selection notably enhance transfer effectiveness?"}, {"title": "4.1 Experimental Setup", "content": "Datasets. We utilize 10 diverse texture-attributed graphs sourced from Chen et al. (2024b). These datasets span a wide range of domains, including citation networks, social networks, and e-commerce networks. The graph sizes range from thousands to millions of nodes, with the number of classes across datasets spanning from 3 to 39. These datasets exhibit both domain shift and structural property shift (Chen et al., 2024b), effectively reflecting the challenges encountered when transferring pre-trained graph models to novel domains in real-world scenarios.\nEvaluation settings. To evaluate the effectiveness of our methods under the transferring set-ting (RQ1), we adopt a widely adopted setting that pre-trained models that are adapted to unseen test datasets with little (few-shot) or no downstream task supervision (zero-shot) (Chen et al., 2024b; Liu et al., 2023a). We consider both node classification and link prediction as target tasks. To test the transferring capability of models, we adopt a leave-one-out strategy. Specifically, given 10 adopted datasets, each time, one of them will be selected as the target downstream test data, and the other nine datasets will be used as the pre-training data. Regarding evaluation metrics, we adopt accuracy for node classification and Hits@100 for link prediction.\nBaselines. To demonstrate the effectiveness of our framework, we consider state-of-the-art cross-graph learning baselines, which can be categorized as the \"single model\u201d and \u201cmixture of models\u201d frameworks."}, {"title": "4.2 RQ1: Evaluating the transferability", "content": "In this subection, we evaluate the transferability of different cross-graph pretraining methods by comparing their performance on downstream tasks. Specifically, we focus on zero-shot and few-shot settings."}, {"title": "4.2.1 Transferring in a zero-shot setting", "content": "We first evaluate different cross-graph pretraining methods under zero-shot learning scenarios. We choose all baseline models applicable to the zero-shot learning settings, including OneForAll, LLaGA, AnyGraph, ZeroG, and our method. Since LLaGA adopts an LLM as the backbone model, it takes a considerably longer time to evaluate using the leave-one-out strategy. As a result, we pre-train it on Arxiv and Products and test it on every downstream task. To prevent data leakage when the target dataset is Product, we pre-train it using Arxiv and Sports."}, {"title": "4.2.2 Transferring in a few-shot setting", "content": "We then evaluate different cross-graph pretraining methods under the few-shot setting. We consider all applicable baselines, including GCC, GraphMAE, OneForAll, LLaGA, GraphAlign, GCOPE, and Prodigy. For OneForAll and Prodigy, we follow (Liu et al., 2023a) to augment the target prediction subgraph with graph prompts sampled from each class. GraphAlign and our method use the inference strategy introduced in Section 3.2 to generate label embeddings for each class. Other baseline methods directly adopt the few-shot labels as supervision to fine-tune the prediction head. For Prodigy, we"}, {"title": "4.3 RQ2: Ablation Study", "content": "We then study how each key component of OMOG affects the transferring effectiveness to answer RQ2. We identify three key components of OMOG:\n\u2022 Expert module acts as the backbone model to solve the prediction task for each graph.\n\u2022 SGC module generates node-level graph embeddings using message passing.\n\u2022 Gate module takes node-level embeddings as input and generates a relevance score to select the related experts.\nAs shown in Figure 4, we find that\n1. Every component contributes to effective transferring. This ablation study reveals that each component significantly contributes to the model's overall performance.\n2. Gating mechanism is crucial to cross-graph node classification. For node classification, we find that removing the gating mechanism results in a significant performance drop, which suggests that gating plays an important role in addressing data heterogeneity by adaptively selecting experts from the proper domain. As a comparison, SGC components play the most important role in link prediction, which means structural information is vital for link prediction.\n3. LLM embedding plays an important role. When solely using the aggregated LLM embedding for prediction, the model can still have good performance, indicating the importance of aligned feature space in the zero-shot learning scenario."}, {"title": "4.4 RQ3: Investigating the Gate Design", "content": "Considering the importance of expert selection and distinguishing our work from existing ones (Hou et al., 2024; Xia & Huang, 2024), we further study the influence of different expert selection strategies to answer RQ3. We compare the following strategy variants to our original design stated in Section 3: (1) \"No weights\u201d still adopts the TopK selection strategy while removing the weights for each expert; (2) \"Random K\u201d randomly selects K experts instead of experts with highest scores; and (3) \u201cLeast K\u201d selects K experts with lowest scores.\nAs shown in Figure 5, we observe that the original gate design in OMOG outperforms all variants, which suggests that"}, {"title": "4.5 Case study", "content": "To visually demonstrate how expert selection ad-dresses the data heterogeneity problem in cross-graph pretraining, we present a case study that investigates how our proposed gating functions adaptively select proper experts based on downstream datasets. We consider the 9 downstream datasets for zero-shot node classification. Specifically, we use the ego-subgraphs of 10 randomly sampled nodes from each dataset and visualize the average relevance score given by different gate functions. As shown in Figure 7, we observe that gates pre-trained on datasets similar to the target dataset exhibit higher scores. For instance, when Cora is the target, gates pre-trained on Citeseer and Dblp assign higher scores, likely because all three datasets are citation graphs within computer science."}, {"title": "5 Conclusion", "content": "In this paper, we present a new perspective together with an easy yet effective framework, \"one model for one graph\" (OMOG), to achieve effective cross-graph learning. Through exten-sive experiments, we develop the following practices for cross-graph learning: training one expert model for each graph and then utilizing pre-trained gate functions to select the experts most proper for downstream tasks adaptively. Our perspective can benefit future development in related areas, such as graph foundation models."}, {"title": "1 Incorporating attribute and structure information", "content": "First, we calculate the normalized adjacency matrix of the pretraining graph,\n$$J = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$$\nwhere A = A + I is the adjacency matrix with self-loop, and \u010e is the degree matrix of \u00c3. Then we could use J to update the node features with neighborhood information,\n$$H^{(a)} = JH^{(a-1)}$$\nwhere we set H(0) = X, and a is the number of neighborhood hops that are used to update the node features. By repeating the Equation 2, we can get node features H(\u221d) integrated with different hops of structural information."}, {"title": "Pretraining gate modules", "content": "Meanwhile, the mask matrix itself could be regarded as a negative sample since it is supposed to not have domain information. We train the MLP gate also in a mini-batch manner. When it takes in a feature h\u2081, it will generate an mask matrix by a\u017c = MLP(hi). Then the filtered feature is calculated as hi = hi + ai, which is viewed as a positive sample of the domain. Meanwhile, the mask matrix a\u017c is viewed as a negative sample. Then we will use the expert to encode h\u2081 and a\u017c repectively, it will result in a positive embedding f\u2081 = Expert(hi) and a negative embedding o\u2081 = Expert(ai). We want the positive embedding to be close to the centroid of the domain embedding cluster while the negative embedding to be distant from the centroid f center, that the training loss of the gate is designed as below,\n$$L_{gate} = dis(f_i, f_{center}) + \\frac{1}{dis(o_i, f_{center})}$$\nwhere dis(,) is the Euclidian distance between two vectors, and f center can be calculated as f center = MEAN(Expert(H))."}, {"title": "The Inference Stage", "content": "For the pth expert and gate which is trained on graph Gp with node embeddings as Hp, the relevance score is calculated as follows:\n$$U_p = sim(MEAN(Expert(Gate(h_{test}))), f_{center,p})$$\nwhere the sim(,) is the operation to calculate the cosine similarity between two vectors, and \u0192 center,p can be calculated as \u0192 center,p = MEAN(Expert(Hp)).\nAfter getting the relevance values for all experts, we will select top-k values with E = top-k(v1, v2, ...). Then we scale the weights with softmax(E). Next we would use them to weight their corresponding expert models to produce a pretrained model.\nOnce the pretrained model is ready, we can use it to infer the target node feature htest and generate the output embeddings ftest. For zero-shot node classification, the label whose embedding has the highest cosine similarity with the test node output embedding is regarded as the prediction. For zero-shot link prediction, the logit of link existence is the cosine similarity between the two test nodes' output embeddings."}, {"title": "Extension to few-shot learning setting", "content": "For the few-shot learning, we follow the same process as zero-shot learning to produce a pretrained model. The key difference from zero-shot node classification is that we use both the label embedding and the centroid embedding of each class to compute the final predictions. Specifically, suppose that there are s classes in the support sets, we input all the samples in the support set and calculate the average of output embeddings for each class. Thus, for each class, there is a centroid embedding \u0192 avg,i, where 0 \u2264 i \u2264 s. Suppose that the label embedding of each class is li, then the predicted label Ytest for a test node with output embedding f test can be calculated as following,\n$$Y_{test} = argmax_{0<i<s}[sim(f_{test}, f_{avg,i}) + sim(f_{test}, l_i)]$$\nwhere the sim(, ) is the operation to calculate the cosine similarity between two vectors."}]}