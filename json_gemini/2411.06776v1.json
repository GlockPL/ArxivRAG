{"title": "Machine vision-aware quality metrics for\ncompressed image and video assessment", "authors": ["Mikhail Dremin", "Konstantin Kozhemyakov", "Ivan Molodetskikh", "Malakhov Kirill", "Sagitov Artur", "Dmitriy Vatolin"], "abstract": "A main goal in developing video-compression algorithms is to\nenhance human-perceived visual quality while maintaining file size. But\nmodern video-analysis efforts such as detection and recognition, which\nare integral to video surveillance and autonomous vehicles, involve so\nmuch data that they necessitate machine-vision processing with minimal\nhuman intervention. In such cases, the video codec must be optimized for\nmachine vision. This paper explores the effects of compression on detec-\ntion and recognition algorithms (objects, faces, and license plates) and\nintroduces novel full-reference image/video-quality metrics for each task,\ntailored to machine vision. Experimental results indicate our proposed\nmetrics correlate better with the machine-vision results for the respective\ntasks than do existing image/video-quality metrics.", "sections": [{"title": "1 Introduction", "content": "As the field of computer-vision continues to evolve, an increasing number of al-\ngorithms are being deployed in real-world applications. A popular application of\nthis technology is video analytics, which has become integral to video surveil-\nlance, autonomous vehicles and other systems. Video analytics, for example, has\ntwo crucial tasks: detection and recognition. Depending on the system, the sub-\njects of these tasks can be traffic signs, vehicles (object detection), human faces\n(detection/recognition), license plates (detection/recognition), and so on. As the\nnumber of video-surveillance cameras increases, automating these tasks becomes\nmore critical given that human operators are incapable of processing such vast\nquantities of data.\nTo ensure efficient storage and transmission of such extensive data, the cap-\ntured images and videos require compression. Lossy compression standards such"}, {"title": "2 Related work", "content": "Methods for image-quality assessment have garnered considerable attention from\nresearchers in the field of visual-perception as high-quality video content is im-\nportant for retaining viewer interest [16]. Initially, these efforts evaluated image\nquality on the basis of how the human eye perceives visual information in generic\nvideos and in specific video types such as streaming, user-generated content\n(UGC), 3D, and virtual and augmented reality. Recently, although computer\nvision increasingly permeates everyday life, efforts to develop machine-vision-\naware image- and video-quality metrics have been less extensive."}, {"title": "2.1 Objective image- and video-quality assessment", "content": "Many methods and algorithms evaluate the visual quality of images. Among the\nmost widely used are PSNR and SSIM; they assess image quality on the basis of\nsignal (pixel) similarity between the original and evaluated images. When applied\nto videos, these algorithms work frame by frame and then average the results.\nMore-modern approaches to assessing video quality have emerged: for example,\nVMAF demonstrates a higher correlation with subjective human evaluations\nrelative to PSNR and SSIM.\nThere are also no reference (NR) methods that take only a single image as\ninput. Among them is an early NR quality metric: NIQE [17], which evaluates an\nimage's \"naturalness\" and serves in cases where the original image is unavailable.\nRecently, NR metrics have approached the quality of FR metrics. For instance,\nDOVER [25] and MDTVSFA [13] correlate highly with subjective quality [5].\nThese methods are widely used to develop and optimize video-compression\nand video-processing algorithms. Some exhibit high correlation with human-\nperceived visual quality, but they were not designed to predict detection and\nrecognition performance on compressed images and video."}, {"title": "2.2 Image- and video-quality assessment for detection", "content": "Kong et al. [12] introduced a no-reference IQA algorithm for object detection.\nThis algorithm integrates classical computer vision and selects 13 image features,\nincluding gradient-vector metrics and HOG descriptors [9]. Random forest is\ntrained to predict detection quality using the revised frame-detection-accuracy\nmetric, which is akin to Intersection over Union (IoU).\nRahman et al. [20] presented an algorithm that employs statistical features\nbased on the internal representations of images input to a neural-network-based\nobject detector. It then uses these statistics to train a LightGBM algorithm,\nwhich performs classification to predict whether the object-detection accuracy\nfor a given input image will surpass a certain threshold or fail.\nSchubert et al. [22] suggested predicting an object-detection algorithm's ac-\ncuracy on the basis of its confidence in the results. The authors analyzed non-\nmaximum suppression step using features and statistics from the detection re-\nsults to predict accuracy without ground-truth annotations. Their underlying\nhypothesis is that the more objects this stage filters out, the higher the confi-\ndence in the remaining object's actual presence.\nBeniwal et al. [7] proposed a metric based on the quantization error from\nH.264 compression. The mean DCT ratio of all filters of first Faster R-CNN con-\nvolutional layer is used as a quality label. Higher values indicate higher quanti-\nzation loss and lower quality. They train CNN network to predict these quality\nlabels using cropped patches as an input."}, {"title": "2.3 Image- and video-quality assessment for recognition", "content": "Best-Rowden and Jain [8] introduced an automatic method for predicting the\nquality of face images, integrating two assessment strategies: subjective evalua-\ntions by humans and objective measurement based on similarity scores for face\nrecognition utility. Both approaches utilize features from deep neural networks as\ninputs for SVMs, allowing for a quantification of face image quality that reflects\nhuman perception and the operational performance of face-recognition systems.\nHernandez-Ortega et al. [11] introduced FaceQnet, a tool that estimates the\nquality of face images with respect to their utility in face recognition. Face-\nQnet operates by fine-tuning a preexisting face-recognition neural network for\nregression; the goal is to predict face-image quality as a continuous value.\nTerhorst et al. [23] proposed an unsupervised approach to face-image-quality\nestimation called SER-FIQ. They compute the quality score as the mean Eu-\nclidean distance of the multiple embedding features from a recognition model\nwith different dropout patterns.\nOu et al. [18] introduced SDD-FIQA, a method that uses inter- and intraclass\ncomparison scores to determine the quality of face images by creating distribu-\ntions of genuine and impostor scores for each image. The quality of an image\nis assessed by calculating the mean Wasserstein distance between these distri-\nbutions across multiple iterations. This approach then refines a face-recognition\nmodel with these quality scores, similarly to the FaceQNet method.\nMost of these studies consider quality loss due to shooting conditions such\nas poor lighting, motion blur, and noise-but neglect artifacts that arise during\ncompression with different quality factors. They also use knowledge of architec-\nture and intermediate results of detection/recognition algorithms, although in\nsome cases the algorithm is inaccessible."}, {"title": "3 Datasets", "content": "Our research hinges on carefully collected datasets, each of which is pivotal to\ndeveloping and testing image-quality metrics. We used the validation and/or test\nparts of popular datasets pertinent to the respective tasks: COCO 2017 [14] for\nobject detection, WIDER FACE [27] for face detection, CCPD [26] for license-\nplate detection and recognition, and CelebA [15] for face recognition. Our test\nset contained proprietary unlabeled videos from CCTV cameras for all tasks,\nexcept Glint360k images for face recognition.\nTo ensure our method's proper function despite various distortions, we se-\nlected several practical video and image codecs (ravle, x264, x265, VVenC, and\nJPEG) to encode the dataset images. We balanced the dataset by calculating\nthe PSNR of the compressed images and adjusting the codec quality parameters\nto achieve a similar distribution over all codecs, as Fig. 1 illustrates.\nIn total, every image in our datasets has 20 quality degradations. For JPEG,\nwe used 20 compression degrees but retained only two quality factors because\nthey represent two boundary values: the minimum, below which severe image\ndegradation occurs, and the maximum, above which quality improvement is in-\ndiscernible. Note that we were unable to obtain completely uncompressed data;\nit would have been helpful, however, as open datasets usually employ JPEG\ncompression and our proprietary videos employ H.264/H.265, so our compres-\nsion distortions are on top of existing compression. This limitation should not\nsignificantly affect the results because open datasets have undergone quality con-\ntrol and avoid extreme compression, and because the camera codecs have a high\nbitrate."}, {"title": "Test-dataset labeling", "content": "The videos in our test set were unlabeled, but ground-\ntruth labels are crucial for correct target-metric calculations, correlation-score\nmeasurement, and object extraction from images. We therefore used an auto-\nmatic labeling pipeline in four of five tasks: object detection, face detection,\nlicense-plate detection, and license-plate recognition.\nWe extracted all the frames from videos and ran detection algorithms to\nlabel frames (each case used the algorithm's most complex version for precise\nlabeling: i.e., YOLOv5X for object detection, RetinaFace for face recognition,\nand LPRNet for license-plate recognition). We selected only objects with a cor-\nresponding high confidence and ignored detector errors. Instead, we looked at\nperformance deterioration on compressed videos and picked about 1,000 frames\ncontaining several objects per task. Our choices were approximately 100 frames\napart since nearby frames are usually similar \u2014 to ensure diversity and reduce\nthe number of noisy labels. We employed distinct frames (images), not videos,\nto train and validate results because video detection and recognition usually\nconsider each frame and average the results for the entire video. Fig. 2. show\nlabeling examples. For license-plate detection we applied an extra filter using\na recognition algorithm: it picked frames with fully recognized characters and\nfiltered frames with similar license plates using the Levenshtein distance. The\ndatasets were subsequently reviewed by humans for gross detection errors (false\npositive and false negative detections). The pixel-level accuracy of the bounding\nboxes was not meticulously verified: even though automatic annotation is often\ninaccurate, it is reasonable to expect that if a sophisticated detection algorithm\nfor GT labeling fails to identify an object in an uncompressed image, the tar-\nget detection algorithm will also struggle to detect the object once the image\nundergoes compression."}, {"title": "4 Proposed method", "content": ""}, {"title": "4.1 Detection methodology", "content": "Object detection, crucial for applications like surveillance, identifies and locates\nobjects in images by combining classification and localization. Key performance\nmetrics include confidence score and Intersection over Union (IoU) with ground\ntruth (GT). These metrics are essential for distinguishing between false neg-\natives (missed detections) and false positives (incorrect detections), especially\nwhen objects are scarce. Correctly identifying missed detections is particularly\nimportant in surveillance, as human oversight can address false positives.\nObject detectors tend to underperform in images that contain small or oc-\ncluded objects [21], a limitation attributable to their implementation details\nrather than compression effects. Given our focus on information loss due to\ncompression, it is essential to select quality metrics that are not biased by object-\ndetector limitations.\nThe Mean Average Precision (mAP) metric assesses the detection algorithm's\nconfidence and incorporates an IoU threshold to determine the object-matching\naccuracy. However, its utility diminishes in images that contain few objects: here,\nmAP values may provide no meaningful insight because they may be binary (e.g.,\n0 or 1 for images with a single object).\nSimilarly, Average Precision (AP), which accounts for precision and recall,\nis poorly suited to single images or frames. The metric fails to differentiate be-"}, {"title": "4.2 Detection metric", "content": "All target detectors in this study are YOLOv5 variations tailored to specific de-\ntection tasks: YOLOv5s for object detection, YOLO5Face for face detection, and\nLPD YOLOv5 for license-plate detection. First we analyzed almost 40 IQA and\nVQA metrics to determine how they correlate with the detectors' performance.\nWe applied each metric to every compressed image in our datasets.\nOur analysis included the following:\nImage/video-quality metrics\n\u2022 Full-reference: PSNR (peak signal-to-noise ratio), SSIM (structural sim-\nilarity index), MS-SSIM (multiscale structural similarity index), VMAF\n(video multi-method assessment fusion)\n\u2022 No-reference: NIQE (natural image quality evaluator), BRISQUE (Blind/Referenceless\nimage spatial quality evaluator)\nOther Metrics: SAM (spectral angle mapper), SRE (spatial resolution en-\nhancement), DSS (decision support system), NLPD (normalized Laplacian\npyramid distance), GMSD (gradient magnitude similarity deviation), MDSI\n(mean deviation similarity index), VSI (visual saliency-induced Index), ERQA\n(edge-based region quality assessment)\nNSS (natural scene statistics): blockiness, total variation, colorfulness, bright-"}, {"title": "4.3 Face-recognition methodology", "content": "Standard metrics for measuring face-recognition efficiency include false-acceptance\nrate (FAR) and false-rejection rate (FRR). However, these can only be calculated\nfor an entire dataset, whereas sometimes it is necessary to assess the quality of a\nsingle photo. A common methodology for neural-network-based face recognition\ninvolves computing embeddings for a corpus of reference images, each associ-\nated with known individuals, as well as for a query image. The identification\nprocess entails choosing the reference image whose embedding is most similar to\nthat of the query image. To quantify the impact of image compression on face\nrecognition performance, we calculated ArcFace [10] embeddings for all images\n(database and queries). Consider cosine similarity between image embeddings:\n$R(I_a, I_b) = cosSim(emb(I_a), emb(I_b))$\n(1)\nThe proposed target metric is calculated as follows:\n$F(I_{ref}, I_{compr}) = R(I_{ref}, I_{database}) \u2013 R(I_{compr}, I_{database})$\n(2)\nCompression algorithms will influence correct recognition of the query face,\nso the cosine similarity between database and compressed images will differ. The\nface-recognition performance metric measures this difference in cosine similarity;\nif the difference is high, the face-recognition system is more likely to fail."}, {"title": "4.4 Face-recognition metric", "content": "Recognition often follows detection, so we focused on image crops. As with de-\ntection, our first task was to analyze the performance of standard IQA/VQA\nmetrics and existing quality-assessment methods for face recognition. The cor-\nrelation scores for all metrics were low, suggesting that none is practical for\nevaluating face-recognition performance on compressed videos (Fig. 7).\nFor face recognition we used ResNet-18 and a one-layer regression head to\npredict cosine-similarity deltas between source and compressed images. First, we\ntrained our metric to predict a target score for a pair of reference and compressed\nface crops; the result was a 0.59 SRCC. We thus concluded that the metric\nprovides unstable results on individual face images. To address this instability,\nwe propose predicting image/video quality on small subsets of images as single\ninputs and averaging CNN feature before regression head to predict scores (Fig.\n8). The SRCC of the metric trained with the proposed strategy is 0.85 SRCC."}, {"title": "4.5 License-plate-recognition methodology", "content": "To crop license plates from the full-size images, we used bounding boxes from GT.\nIn license-plate recognition, the outcome is a sequence of characters, so the qual-"}, {"title": "4.6 License-plate-recognition metric", "content": "The standard IQA/VQA methods exhibit bad correlation scores for our proposed\ntarget metric (Fig. 9).\nWe developed a neural-network metric to predict the performance of a license-\nplate-recognition method that takes as input a grayscale license-plate image. The\nmodel is a classic CNN, and its output is a single real number representing the"}, {"title": "5 Conclusion", "content": "This paper comprehensively explores the development and validation of novel\nimage-quality metrics for machine vision, with a focus on assessing detection\nand recognition performance on compressed images and videos. These metrics\naddress a major gap in standard IQA/VQA methods, which cater mainly to\nhuman visual perception and inadequately predict machine-vision performance\non compressed images and videos.\nOur metrics proved to be 3-5 times more computationally efficient than the\ntarget examined algorithms. For detection, we produced a general metric that ap-\nplies to various detection subtasks, making the computations even more efficient\nsince several detectors of the same family (e.g., YOLOv5) are used simultane-\nously in some cases (e. g., object and license-plate detection).\nAlthough our proposed metrics yielded promising results, they are designed\nfor specific tasks and, unmodified, may be inapplicable to other machine-vision\ntasks. Future work could explore development of more-general metrics that can\nhandle a wider range of detection and recognition tasks. Additionally, further\nresearch could examine integration of these metrics into video-compression al-\ngorithms to automate optimization for machine vision.\nOur metrics can also be used for other tasks that consume image/video data\nas an input. For example, visual question answering uses multimodal fusion\nof features extracted from both image and text. Taking a look at the existing\nVideoQA models, we can observe that the majority of them are utilizing features\n(rather than predicted labels) extracted from an image or a video by familiar ob-\nject detection models, e.g. Faster-RCNN features in ViteVQA [29] and BUTD [4],\nor ResNet features in MFH [28]. Therefore, we expect that our results will ex-\ntrapolate to those models. Applicability to newer models (e.g. Fuyu-8B) that\nuse direct linear projection of image patches requires a separate study. As our\nmachine-vision metrics are feature-focused for image/video data, they can serve\nas a reference point for developing a metric for the VisualQA task."}]}