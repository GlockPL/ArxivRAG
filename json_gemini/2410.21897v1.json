{"title": "Semi-Supervised Self-Learning Enhanced Music Emotion Recognition", "authors": ["Yifu Sun", "Xulong Zhang", "Monan Zhou", "Wei Li"], "abstract": "Music emotion recognition (MER) aims to identify the emotions conveyed in a given musical piece. But currently in the field of MER, the available public datasets have limited sample sizes. Recently, segment-based methods for emotion-related tasks have been proposed, which train backbone networks on shorter segments instead of entire audio clips, thereby naturally augmenting training samples without requiring additional resources. Then, the predicted segment-level results are aggregated to obtain the entire song prediction. The most commonly used method is that segment inherits the label of the clip containing it, but music emotion is not constant during the whole clip. Doing so will introduce label noise and make the training overfit easily. To handle the noisy label issue, we propose a semi-supervised self-learning (SSSL) method, which can differentiate between samples with correct and incorrect labels in a self-learning manner, thus effectively utilizing the augmented segment-level data. Experiments on three public emotional datasets demonstrate that the proposed method can achieve better or comparable performance.", "sections": [{"title": "1 Introduction", "content": "The music emotion recognition (MER) task aims to recognize the emotion expressed in a given music clip automatically. Music emotion recognition can be widely used in many fields, such as dynamically generating music to adapt to the emotion of scenes in movies or games [1], music-assisted psychological or physical therapy, personalized recommendation in stream media, human-machine interaction, music retrieval, and so on, which has broad application prospects. In recent years, As the amount of data grows, data-driven deep learning methods have become the mainstream method in the Music Information Retrieval (MIR) field [2, 3].\nAt present, the duration of audio clips in public music emotion datasets is 30 ~ 45 seconds. Although the longer the duration is, the more helpful it is to distinguish emotions, according to the study of music psychology, it is found that the duration of about one second of music is sufficient to evoke an emotional reaction [4]. To address the issue of limited annotated data in emotion recognition tasks, some segment-based methods [5-8] have been proposed recently, which naturally increase the amount of training data and can make full use of every audio sample in the dataset.\nAfter the audio clip is divided into segments, Sarkar et al. [7] make each segment inherit the label of the clip containing it, which is also the simplest method, then majority vote and maximum run length are used to obtain clip-level results. However, the emotion of the music is not constant. Therefore, each segment may actually carry different emotions, which also introduces the problem called noisy label. He et al. [8] used an unsupervised method, i.e. using AutoEncoder to reconstruct the masked Mel-spectrogram of the segment to obtain audio segment embedding. Then a supervised learning structure using BiLSTM is employed to capture temporal music information and perform emotion classification. However, it is unknown how many emotion-related features are included in the embedding. In the field of speech emotion recognition, Mao et al. [9] leveraged a self-learning framework to update model parameters and segment labels iteratively in the training process and used soft labels instead of hard labels, which to some extent solved the problem of noisy labels. However, only using the output probability distribution of the model as the soft label of the next epoch will excessively rely on the prediction ability of the model. Once the model makes a prediction error, this error will deepen with the training, which is called confirmation bias.\nInspired by [10], when the mixture of correct and incorrect labels are fed to the deep neural networks, networks have a tendency to learn the later after the former. Therefore, we propose a Semi-supervised Self-learning Framework (SSSL), to model each training sample's loss value, and to distinguish the training samples most likely to be clean from those most likely to be noisy. Then we use the Mixup [11] data augmentation algorithm and consistency regularization to prevent the confirmation bias of the model's prediction. After obtaining a label noise-robust segment-level emotion predictor, we can use it to generate the predicted probabilities of each segment in the song-level data as a structured feature representation. Finally, a second machine learning algorithm is employed to predict the overall emotion for each song.\nOur main contributions are: 1) Instead of directly inheriting clip-level labels for each segment or unsupervised methods that do not use labels at all, we use semi-supervised learning to deal with noisy labels. 2) Combining noisy label processing with semi-supervised learning, to mitigate the confirmation bias issue associated with self-training, which can lead to the accumulation of model errors. 3) Compared with baseline models, the effect is improved."}, {"title": "2 Method", "content": "Our method is divided into two steps. The first step is to train a segment-level classifier robust to label noise on the expanded segment-level dataset. Then the second step uses the original song-level dataset. Predict each segment in each song, obtain the statistical value of the probability distribution of each segment, and aggregate them as the feature representation of the song. Then, use a machine learning method to complete the emotion prediction of the song. Figure 1 illustrates the overall framework of our proposed algorithm."}, {"title": "2.1 Semi-supervised Self-learning Framework", "content": "We propose a semi-supervised self-learning framework (SSSL) on the extended segment dataset, aiming to obtain a label-noise robust segment-level classifier. Unlike conventional semi-supervised learning methods, our labeled data and unlabeled data generation is dynamic. Moreover, we do not assign a fixed pseudo-label to the unlabeled data.\nAt the beginning of each epoch, the training set is partitioned into a clean set and a label noisy set using a 2-component Gaussian mixture model (GMM) by the cross entropy loss value for each training sample. Then the Expectation-Maximization (EM) algorithms is leveraged to fit the GMM to the observation. Then, the semi-supervised learning method is used to treat the clean set as the labeled set X while the noisy set as the unlabeled set U. We erase the label of the U set, use the predicted value of the model as the soft label, and then use the Sharpening algorithm to get the pseudo soft label. Then we obtain the total loss using Mixup and consistency regularization."}, {"title": "2.1.1 Task Formulation", "content": "In a k-class classification problem, the training data with n training samples denoted as X = [x1,...,xn], along with their corresponding ground-truth labels Y = [y1, \u2026\u2026\u2026, Yn].\nEach y\u017c represents the true class label and is represented as a k-dimensional one-hot vector. Classification problems on clean label datasets are often defined as:\nmin\u03b8C(\u03b8|X, Y), (1)"}, {"title": "2.1.2 Training Samples Partition", "content": "Deep neural networks have been observed to prioritize learning from simple and logically consistent samples in the presence of noisy labels, resulting in reduced loss for these samples, particularly in the early stages of training [12]. This phenomenon suggests that the loss distributions of clean and noisy samples during training can be approximated by two Gaussian distributions, with the clean samples having a smaller mean loss. Leveraging this training characteristic, we employ a Gaussian mixture model to differentiate between noisy and clean samples by utilizing the per-sample loss as input. The probability density function of the loss K-component mixture model is defined as:\np(l) = \\sum_{k=1}^{K} \\lambda_{k} p(l_{k}), (3)\nwhere Ak indicates the mixing coefficients for the combination of each individual probability density function p(lk). Regarding this case, we can utilize a 2-component GMM to model the distribution of clean and noisy samples. We utilize a two-component Gaussian Mixture Model (GMM) and then feed the cross-entropy loss L of every training data to the GMM.\nThen, to estimate the parameters of the GMM we apply the EM algorithm. We define the posterior probability wi as the probability that the ith data sample belongs to the Gaussian component with a smaller mean, given its loss li. By setting a global threshold T for the probability w of all training samples' label to be clean, we can split the expanded segment-level dataset D into two parts: set X and set U. The labels in set X are with more possibilities to be correct, and thus, it will be used as the labeled set. On the other hand, the labels in set U are more likely to be incorrect, and will be erased in the subsequent steps. Therefore, set U will be used as the unlabeled set."}, {"title": "2.1.3 Modified Semi-supervised Learning", "content": "We first preprocess the set U. For the set U, the initial labels are likely to be incorrect and have been erased. As a result, we generate pseudo soft labels \u0177 by sharpening the predicted distribution of the model.\n\u0177 = Sharpening(fj(0,xi))\n= \\frac{f(0,x_{i})^{k}}{\\sum_{k=1}^{K} f_{j}(0,x_{i})^{-k}}, (4)"}, {"title": "2.1.4 Loss Function", "content": "Eq. (7) can be regarded in the loss as l = dlp + (1-8)lq, and the standrad cross entropy loss is leveraged for semi-supervised learning part:\nL_{MIX} = \\frac{1}{n}\\sum_{i=1}^{N} \\sum_{j=1}^{k} \\delta[y_{i,p}log(f(x_{i}))] + (1 \u2013 \\delta)[y_{i,q}log(f(x_{i}))] (8)\nConfirmation bias, resulting from the accumulation of errors, is a common issue in self-training. Model ensembling is a widely adopted approach to mitigate this bias. Dropout, which can be viewed as an implicit form of model ensembling, is commonly used during network training. However, when it comes to sample selection or inference, dropout is typically disabled to ensure consistency. In the presence of label noise, the decision boundaries between classes become blurred, leading to significant inconsistencies among sub-models. To address this, we incorporate the R-Drop loss [14], a straightforward yet effective dropout regularization method, to promote consistency among the sub-models:\nL_{KL} = \\sum_{x \\in U}(\nDKL(P1(x,0)||P2(x, 0))\n+DKL(P2(x,0)||P1(x, 0))) (9)\nTherefore, the total loss is:\nL = L_{MIX} + \\lambda L_{KL} (10)\nwhere X is the hyper-parameters to control the weight."}, {"title": "2.2 Song-level Decisions", "content": "The final goal is to get song-level prediction results. Given a sequence of probability distributions of emotional states generated by a segment-level classifier for a song, we can make decisions based on the information."}, {"title": "3 Experiments", "content": "We tested our proposed algorithm on 3 public datasets: PMEmo dataset [15], Emotion in Music [16] and 4Q dataset [17]. Deep learning is data hungry, these data volumes are difficult to support models with strong training generalization ability, and are easy to overfit."}, {"title": "3.1 Experiment Setup", "content": "We convert each segment unit into Mel-spectrogram, with the Hanning window length of 1024 and window hop size of 512 using the Librosa [18]. We utilize 128 mel bins. Once the audio clip is segmented into smaller parts, the amount of training samples is dozens of times larger than the original. Our segment classifier uses VGG16 [19], we only modify the input channel and output number of classification. SVM is used for song-level decision."}, {"title": "3.2 Datasets", "content": "PMEmo: This dataset contains 794 pieces of music, all of which are pop music in mp3 format. The audio duration is mainly around 45 seconds. We utilize 767 of these clips with static V/A annotations in our work.\n4Q: There are 900 music clips in this dataset. All music clips are divided into four parts according to Russell's V/A quadrant, with 225 clips in each part. Most audio clips are approximate 30 seconds."}, {"title": "3.3 Audio Pre-process", "content": "In the previous works, in the methods of taking the entire clip as input [20, 21], 30 seconds of audio are generally reserved, and the part less than 30 seconds is padded with zeros. In [8], for clips less than 30 seconds, they paded them to 30 seconds by repeating themselves continuously from the beginning to the end.\nSince the variance of sample duration in different data sets is large, when the method of filling zeros is used, many blank segments will be generated, which is invalid training data. Using the circular padding method will generate too many repeated training samples. Therefore, we did not do any padding."}, {"title": "4 Results", "content": "In this section, we present the performance results for different segment durations and compare them with the results obtained from other MER methods."}, {"title": "4.1 Performance at Different Segment Duration", "content": "According to the previous music psychology research [4, 22], people can react to and make judgments about the emotions in music within one second. The longer the segment, the more helpful it is for emotion recognition, but too long segments will reduce the data volume of the segment dataset, so a compromise is required. Thus, we experimented with integer segment duration from 1 second to 5 seconds, the overlap of adjacent segments is 1 second less than the segment duration. After segmentation, see the table 1 for the sample number comparison between the expanded segment-level dataset and the original song-level dataset. The experimental results of different segment duration on binary classification are shown in the table 2.\nThe findings also indicate that shorter segment durations exhibit better performance in capturing the Arousal dimension, whereas longer segment durations are advantageous for recognizing Valence. For example, in the PMEmo dataset, the 1s segment showed the best Arousal result with an accuracy of 85.42% and a Fl-score of 86.61%, while the 3s segment showed a better Valence accuracy of 83.19% and Fl-score: 82.31%. The results on the 4Q dataset show a similar trend.\nFor such results, our analysis may be that Arousal and intensity are more correlated, and intensity is relatively easier to be preferentially recognized by the auditory system. The identification of Valence requires a longer period, this dimension involves more psychological knowledge, and the perception process of psychology is much more complicated, so it needs a longer time period [4]. Moreover, we found it is not that the longer the segment duration, the better the experimental results. It may be related to the overlap value used by different segment durations. To ensure the amount of data, we leverage a larger overlap value in long segments, which will lead to data redundancy and make the model overfit the data. Leading to poor generalization ability to new data."}, {"title": "4.2 Validation Experiment", "content": "Figure 2 shows an example of emotion predictions for each segment of the test sample MT0010465830 in the 4Q dataset, which is labeled as happy (Q1). The segment classifier has four outputs corresponding to four different emotional states: Q1, Q2, Q3, and Q4.\nAs shown in the figure, the probabilities for each emotion change throughout the entire song. The true emotion of the whole song is Q1, and as seen in the figure, the Q1 probability is highest in most segments, but there are also other emotions dominating some parts of the song. While not all songs exhibit distinctive segment-level outputs, we can employ the song-level classifier to discern them. The high-pitched and cheerful singing of the male singer at the beginning of the song kept the Q1 prediction probability the highest for the first few seconds. Then the vocals quickly changed to a low tone, and Q3 (sadness) took over. At 7 seconds, the trumpet and drums come in and shift the emotion back to happiness. At 20 seconds, although the vocals turned to a low tone again, the background drumbeats and trumpet sounds continued, so the Q1 probability only slightly decreased, still dominant overall. The audio content confirms that the emotional expression in music changes dynamically throughout the song, rather than being consistent throughout."}, {"title": "4.3 Experimental Results Compare with Other Models", "content": "In order to make a fair comparison, our experimental results are all the average values obtained under the cross-validation of ten fold. Among them, the proposed* is the ablation experiment, which does not contain LKL. Bold numbers indicate the best result.\nIn comparison to the models listed in Table 3, our model, which utilizes segment-level data, demonstrates superior performance when compared to other models that directly use entire music clips. Emotional states in long music fragments may have changed or be in transition between different emotional states [22], which may confuse the learning model and make it difficult to extract unified musical features specific to one emotion. In addition, the emotional value of music may be influenced by the processing of Western harmony, particularly the extensive use of minor keys. This processing requires more time to fully comprehend compared to the arousal dimension, which is primarily associated with"}, {"title": "5 Conclusion", "content": "A semi-supervised self-learning (SSSL) framework is proposed to deal with the label noise problem caused by segment-based methods. The framework can use unlabeled data through self-learning, and use labeled data to guide the model to learn the correct feature representation, so as to effectively deal with the problem of label noise. In the self-learning process, the model may be too confident in its own prediction results, resulting in high confidence in the prediction results, which will accumulate errors. This method addresses this issue by introducing an additional consistency regularization, which improves the generalization and robustness of the model. Further research on song-level decision-making will be conducted in the future."}]}