{"title": "Localization, balance and affinity: a stronger multifaceted collaborative salient object detector in remote sensing images", "authors": ["Yakun Xie", "Suning Liu", "Hongyu Chen", "Shaohan Cao", "Huixin Zhang", "Dejun Feng", "Qian Wan", "Jun Zhu", "Qing Zhu"], "abstract": "Abstract-Despite significant advancements in salient object detection (SOD) in optical remote sensing images (ORSI), challenges persist due to the intricate edge structures of ORSIS and the complexity of their contextual relationships. Current deep learning approaches encounter difficulties in accurately identifying boundary features and lack efficiency in collaboratively modeling the foreground and background by leveraging contextual features. To address these challenges, we propose a stronger multifaceted collaborative salient object detector in ORSIs, termed LBA-MCNet, which incorporates aspects of localization, balance, and affinity. The network focus on accurately locating targets, balancing detailed features, and modeling image-level global context information. Specifically, we design the edge feature adaptive balancing and adjusting (EFABA) module for precise edge localization, using edge features to guide attention to boundaries and preserve spatial details. Moreover, we design the global distributed affinity learning (GDAL) module to model global context. It captures global context by generating an affinity map from the encoder's final layer, ensuring effective modeling of global patterns. Additionally, deep supervision during deconvolution further enhances feature representation. Finally, we compared with 28 state-of-the-art approaches on three publicly available datasets. The results clearly demonstrate the superiority of our method. The codes of our method are available at https://github.com/little1bold/LBA-MCNet.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual saliency detection aims to simulate the human visual system to recognize salient areas [1], [2]. Building this concept, salient object detection (SOD) focuses on identifying and isolating the most visually salient objects within images. SOD finds applications in various fields, such as photo cropping [3], target tracking [4], dynamic detection [5], [6], and semantic segmentation [7], [8]. Originally applied to natural scene images (NSIs), SOD has now expanded to encompass optical remote sensing images (ORSIS). SOD plays a crucial role in scene classification [9-11], object localization [12], and the dynamic monitoring and analysis of remote sensing images [13], [14]. However, salient object detection in optical remote sensing images (ORSI-SOD) poses greater challenges than salient object detection in natural scene images (NSI-SOD), due to the diversity of scenes, complex backgrounds, and the presence of multiple interfering factors [15], [16].\nCurrent ORSI-SOD methods can be broadly classified into two categories: traditional methods and deep learning (DL) methods. Initially, traditional methods primarily depended on specific handcrafted features to detect salient objects, such as feature fusion [17] and color content [18]. With the expansion of data volume, several machine learning approaches emerged to address the challenges, including support vector machine [19], [20] and regularized random walk ranking [21], [22]. However, the complexity of ORSIs hinders traditional methods from automatically extracting features. Their limited non-linear expressiveness constrains the capture of deep semantic image features, resulting in low detection accuracy for salient objects. As shown in Fig. 1, two traditional methods like DSG [23] and RCRR [21] demonstrate constrained proficiency in extracting features for ORSI-SOD, frequently mistaking background elements for salient objects and introducing considerable noise.\nThe advancements in convolutional neural networks (CNN) have resulted in significant improvements in the accuracy of ORSI-SOD, which faces challenges from complex backgrounds and significant interference [24-27]. Initial research on ORSI-SOD focused on capturing information at various levels within the network structure to reduce interference from cluttered backgrounds [28]. As research progresses, an increasing number of researchers are leveraging the edges of salient objects as auxiliary information [29], [30]. However, unlike NSIS, ORSIs typically feature lower resolution, more complex texture structure and lower quality edge display. Most existing methods for acquiring edge information rely on the shallow features output by encoders [31], [32]. During transmission, these shallow features are vulnerable to background noise, resulting in blurred boundary outputs. As shown in Fig. 2, the boundaries of the aircraft detected by these boundary-based methods float within the yellow buffer zone, making it difficult to pinpoint the aircraft boundary. However, ORSIs contain a vast array of target types, and the backgrounds are significantly more complex. The existing methods possess limited capability in responding to the differences and relationships between the foreground and background [33-35]. As illustrated in Fig. 3, the global-local embedding module fails to effectively capture and integrate global and local information, leading to misidentifications such as mistaking roads adjacent to buildings as salient objects. It also exhibits limited discriminative ability between salient objects and backgrounds. Overall, the primary challenges revolve around how to balance detailed features with local semantics across various levels, ensuring each feature layer contains rich semantic information and precise location details, and how to model the image-level global context to encourage an affinity for common characteristics."}, {"title": "II. RELATED WORK", "content": "In this section, we introduce SOD methods that focus on boundary and context information, respectively.\nDue to the multiple targets and complex edge textures in ORSIs, salient images frequently display low-quality boundary details. This occurs if fine edges or boundary details of objects are not adequately considered during salient detection. In the quest for high-precision ORSI-SOD, the importance of edge details has grown increasingly significant. Some methods utilize shallow features, which are rich in boundary information, to directly enhance the accuracy of salient object detection by leveraging edges derived from these features [36]. For example, generating boundary information through edges involves monitoring them with boundary labels and emphasizing salient areas [29]. Furthermore, refining the initial saliency map's coarse boundaries using rich edge features leads to the achievement of a more detailed saliency map [31]. However, the coarse boundaries obtained directly from shallow features are often challenged by noise and complex backgrounds. With the advent of various attention mechanisms, some methods have started to process edge information by establishing these mechanisms [37-39]. For instance, spatial attention and channel attention contribute to boundary exploration [39], and the resulting boundary-aware saliency map acts as a cue for implementing edge perception attention mechanisms [32]. However, applying attention mechanisms requires calculating attention weights for the entire feature map, leading to significant computational costs during training. Some research explores lightweight ORSI-SOD possibilities based on semantic matching and edge alignment [40]. Additionally, shallow features typically capture only local information and lack an understanding of the global context and structure of the image. In contrast, deep features often contain rich semantic information. Many methods achieve SOD by fusing different layers' features. For example, combining high-level semantics, low-level details, and edge information for salient object detection [34]. Under the guidance of attention and reverse attention mechanisms, merging high and low-level semantics to progressively refine predictions from coarse to fine [41].\nIn summary, the aforementioned methods utilize boundary information obtained from shallow features for SOD. Shallow features are limited to local areas and lack deep semantic information, making them less effective in handling noise and complex backgrounds. Introducing attention mechanisms and deep semantic information can significantly improve SOD accuracy, but it also requires considering computational efficiency and the complexity of integrating different information sources. Inspired by these methods, our approach incorporates both spatial and channel attention, effectively integrating them with deep-level information. This allows us to balance detailed features and local semantic information across different levels while accurately locating salient targets."}, {"title": "III. METHODOLOGY", "content": "Fig. 4 depicts the comprehensive architecture of LBAMCNet, characterized by a symmetrical U-shaped structure. This architecture integrates three core components: the encoder, the EFABA module, and the GDAL module. The Transformer-based encoder facilitates the extraction of features over extended spatial ranges. The EFABA module ensures a balance of local semantics with spatial details, whereas the GDAL module is dedicated to modeling the global context and enhancing feature affinity across the network. Deconvolution processes in the final step refine the feature maps to distinctly highlight salient objects, underscoring the network's refined edge detection. The methodological synergy of location, boundary, and affinity elements forms a complex, collaborative network, markedly enhancing the precision of edge recognition and the network's overall discernment capabilities.\nFor a single image $i \\in R^{3\\times H\\times W}$, we initially utilize the Pyramid Vision Transformer (PVT-V2-B2) to extract multilayer features, denoted as $f_i \\in R^{C_i\\times H_i\\times W_i}$. Here, $C_i$ represents the number of channels mapped by the features, for $i$={1, 2,3, 4}, with the corresponding number of channels for each layer being {64, 128, 320, 512}. $H_i = (H/2^{i+1})$ denotes the height and $W_i = (W/2^{i+1})$ represents the width. Subsequently, the EFABA and GDAL are applied to the first three layers of features, ${f_i}_{i=1}^3$, of the encoder. The output feature mappings, denoted as ${F,G_i}_{i=1}^3$, from EFABA and GDAL, are then effectively fused into the decoder, facilitating the synthesis of these advanced features for precise object detection. Ultimately, a sequence of addition fusion and deconvolution operations is performed to reconstruct the resolution of the feature mapping, thereby refining the details and clarity of the final image output. Furthermore, each layer of the decoder outputs a side prediction detection map of varying resolution, ${P_i}_{i=1}^4$, using 1x1 convolutions, and undergoes deep supervision. This approach enables the network to jointly detect representations of multi-source information, with $P_1$ serving as the final output. This strategy ensures that the network leverages the comprehensive information available across various scales and depths for accurate detection."}, {"title": "B. Edge Feature Adaptive Balance Adjustment Module", "content": "Fig. 5 showcases the intricate architecture of EFABA, which includes the edge clue detector (ECD) and the feature adaptive balance adjuster (FABA). Edge clues, characterized as shallow features, are predominantly located in the initial layers of the encoder. The edge information of salient objects significantly improves their localization, thereby enhancing the overall performance of detection mechanisms. Therefore, the integration of edge information into the network's architecture is crucial for optimizing its performance in detecting and localizing objects. Consequently, we implement ECD to encourage the deep network to identify edge clues within fine-grained features. ECD also lays the groundwork for feature balancing across various levels in the subsequent stages, ensuring a harmonized and effective feature representation throughout the network. Leveraging this foundation, the network perceives a richer local context without losing fundamental semantics at any level. Ultimately, fusion features are derived through the feature fusion process, which can be outlined in two main steps:\n1) Edge clue detector: Firstly, to capture basic edge features, a first-order differential operator, namely the Sobel operator, is applied to the feature mapping of the first two layers of the encoder, denoted as ${f_i}_{i=1}^2$. Specifically, we construct two 3\u00d73 matrices, $\\begin{bmatrix}1 & 0 & -1\\\\ 2 & 0 & -2\\\\ 1 & 0 & -1\\end{bmatrix}$ and $\\begin{bmatrix}1 & 2 & 1\\\\ 0 & 0 & 0\\\\ -1 & -2 & -1\\end{bmatrix}$ and use these to perform convolution operations on the original feature mappings of ${f_i}_{i=1}^2$, to approximate the grayscale deviations in the horizontal and vertical directions. Subsequently, the gradient magnitude for each point in the feature mapping is calculated, denoted as g. Finally, we acquire the basic edge features, denoted as ${e_i}_{i=1}^2$. The entire process can be described as\n$g=\\sqrt{x^2+y^2}$ \n${e_i}_{i=1}^2 = \\sigma(g) \\otimes {f_i}_{i=1}^2$\nwhere x, y represents the grayscale deviations in the horizontal and vertical directions respectively, $\\sigma(\\cdot)$ represents the Sigmoid activation function, $\\otimes$ represents element-by-element multiplication.\nSecondly, ${e_i}_{i=1}^2$are input into a 3\u00d73 convolution layer to refine the edge information for the first time. Subsequently, the processed edge information is fused back with ${f_i}_{i=1}^2$ through an addition operation. Lastly, the number of feature channels is reduced to 1 using 1\u00d71 convolution block to obtain preliminary fused edge clues ${e'_i}_{i=1}^2$. The process can be expressed as\n${e'_i}_{i=1}^2 = CBR_{1\\times1}[C_{3\\times3}({e_i}_{i=1}^2) \\oplus {f_i}_{i=1}^2]$\nwhere CBR1\u00d71(\u2022) denotes a convolutional layer with a 1x1kernel, followed by Batch Normalization (BN) and ReLU activation function, C3\u00d73(\u2022) denotes a convolutional layer with a 3\u00d73 kernel, \u2295 denotes element-by-element addition.\nFinally, to acquire more comprehensive edge clues, the edge features from two layers are further fused. Specifically, the procedure begins by upsampling $e'_i$ to match the size of $e'_1$. Next, the edge features from these two layers are input into a 1x1 convolution block, where fusion occurs at the channel level. Ultimately, rich edge attention is achieved by applying the Sigmoid function. The process can be represented as\n$e^{att} = \\tau(C_{1\\times1} \\{ \\varphi [CBR_{1\\times1}(E),CBR_{1\\times1}(U(e'))]\\})$\nwhere $e^{att}$ denotes the output edge attention, $\\varphi(\\cdot)$ denotes the concatenate operation, $U(\\cdot)$ denotes the upsampling operation, C1\u00d71(\u2022) denotes a convolutional layer with a 1\u00d71 kernel.\nAdditionally, to improve the network's representation of multi-source features, the interpolated $e^{att}$ is considered a side output of the network. Then we supervise it deeply through the real boundary.\n2) Feature adaptive balance adjuster: To adjust the balance between detailed features and local semantic context across various levels, thereby directing the network to emphasize boundary regions for the retention of more complex spatial and geometric details, we firstly use $e^{att}$ as the edge attention to enhance the network's sensitivity to object edges. Meanwhile, we conduct element-by-element multiplication between $e^{att}$ and the feature maps from the first three layers of the encoder, denoted as ${f_i}_{i=1}^3$. This step is aimed to minimize the loss of boundary clues during forward propagation. Through the adaptive process of"}, {"title": "C. Global Distribution Affinity Learning", "content": "Fig.6 illustrates the detailed architecture of GDAL. Global context enhances detection accuracy by ensuring semantic consistency across the image and establishing associations between target objects. Consequently, GDAL is meticulously crafted to model the image-level global context, thereby sharpening the network's ability to distinguish salient objects. This enhancement not only elevates distinctive feature representation but also boosts the network's capacity to differentially respond to foreground and background elements. To achieve this, GDAL models global semantic cues using a dual approach of explicit and implicit global affinity learning, operationalized through two distinct branches:\n$R = \\sigma \\{ \\sigma^{\\tau} \\{ F_{\\level4} \\}} \\otimes {\\delta^{\\tau}}_{i=1}^{3}$\n1) Explicit affinity learning: The last layer of encoder rich in semantic knowledge, is crucial for enabling the network to capture image-level global context when utilized effectively. First, the last layer of the encoder, denoted as ${f_4}$, undergoes global average pooling followed by a passage through a fully connected layer. This process is designed to extract imagelevel information, resulting in a feature representation identified as ${F_4}_{\\level{image}}$. Subsequently, the spatial attention mechanism detailed in Section III-B2 is applied to ${F_4}_{\\level{image}}$, allowing the model to focus on particular objects or regions. This attention-enriched feature is then propagated to other encoder layers through direct and effective element-wise multiplication. This step facilitates the self-adaptation of global affinity features and leads to the formation of an imagelevel global context association graph, denoted as R. Finally, the feature association diagram is redistributed across each layer, instructing the model to accentuate global features. The utilization of the Squeeze and Excitation (SE) attention mechanism further sharpens the focus on the most distinctive feature channels. This enhancement bolsters the model's capacity to differentiate between object categories and identify discriminative features more effectively. This process is mathematically expressed as follows\n${F_4}_{\\level{image}}= F[{\\theta}(f4)]$\n$R= \\sigma[{{f_i}_{i=1}^{3} \\otimes l [SF({{F_4}_{\\level{image}}})]}$\n${f^{Exp}_{i}}_{i=1}^{3} = SE(R \\otimes {{f_i}_{i=1}^{3}})$\nwhere \u00c5(\u2022) represents global pooling, F(\u2022) represents fully connected layer, l(\u2022) represents sampling operation, ST(\u2022) represents spatial attention, SE(\u2022) represents SE attention, ${f^{Exp}_{i}}_{i=1}^{3}$ is the final output of the explicit affinity learning.\n2) Implicit affinity learning: To capture hierarchical features from images and effectively discern global patterns, our study adopts implicit affinity learning for encoding global information. Similarly utilizing the image-level features ${F_4}_{\\level{image}}$, we first align its channel dimensions with ${f_i}_{i=1}^{3}$. For simplicity, we assume that after dimension adjustment, ${f_4}_{\\level{image}} \\in \\mathbb{R^{N,C,H,W}}$. Then, the feature map is transformed into ${f_4}_{\\level{image}} \\in \\mathbb{R^{N,C,HW}}$, whereupon applying the softmax function across the spatial dimension HW generates an attention map, denoted as M$\\in \\mathbb{R^{N,C,HW}}$. This process can be described as follows\n$M=[{\\tau({f_4}_{\\level{image}})}]^{\\level{S}}$\nwhere \\tau(\u2022) represents softmax function and T(\u2022) represents transpose operation.\nNext, M is transposed, and through matrix multiplication with ${f_4}_{\\level{image}} \\in \\mathbb{R^{N,C,HW}}$, we derive the semantic assignment features, denoted as ${f^d_4}_{\\level{image}} \\in \\mathbb{R^{N,C,C}}$. It's important to note that ${f^d_4}$, being the result of operations on ${f_4}$, embodies diverse global representations across its dimensions. Therefore, this method implicitly incorporates a semantic region from the feature map into each layer.\n${f^d_4}_{\\level{image}}= {\\tau^{\\tau}(M)} {f_4}_{\\level{image}}$\nwhere () represents matrix multiplication.\nFurthermore, ${f^d_4}_{\\level{image}}$ is designed to adaptively select affinities with each encoding layer and is reshaped to match the dimensions of each encoding layer. To preserve global semantic integrity, it is then fused with the layer possessing the strongest semantic information, yielding the final implicit affinity feature, denoted as ${f^{Imp,3}_i}_{i=1}^{3} \\in \\mathbb{R^{N,C,kH_i,kW_i}}$. Ultimately, the GDAL's final output, denoted as ${F^{Exp-Imp}_{i}}_{i=1}^{3}$, is obtained by integrating features from both explicit and implicit learning pathways.\n${F^{Exp-Imp}_{i}}_{i=1}^{3} = \\alpha( {{f^Imp_i}_{i=1}^{3}, {f^{Exp}_{i}}_{i=1}^{3}} ) \\oplus CBR_{1\\times1}[\\delta( {f^d_4} )]$\n$CBR_{3\\times3} \\delta({f^{Imp,3}_{i}}_{i=1}^{3}, {f^{Exp}_{i}}_{i=1}^{3}))$"}, {"title": "IV. EXPERIMENTS", "content": "Our method is trained, tested, and evaluated across three publicly accessible ORSI-SOD datasets.\nThe ORSSD dataset is the inaugural publicly available dataset for ORSI-SOD. It consists of 800 optical remote sensing images sourced from Google Earth and various other RSI datasets, each accompanied by manually annotated pixel-level GT images. ORSSD is divided into 600 images for training and 200 images for testing, with each image matched with its corresponding GT image.\nThe EORSSD dataset extends the variety of salient object types and challenging scenarios provided by the ORSSD dataset. It contains 2000 images, each accompanied by their respective pixel-level GT images, with 1400 allocated for training and 600 for testing.\nThe ORSI-4199 dataset includes a collection of remote sensing images that span a variety of sizes, scales, and complex backgrounds, offering a more rigorous challenge for SOD. It comprises 4199 images across diverse scenarios, each accompanied by its corresponding pixel-level ground truth (GT) images. The dataset is divided into 2000 images for training and 2199 for testing.\nAll experiments were conducted on a server equipped with an NVIDIA GeForce RTX 3090 24GB GPU. We resized the images to 352\u00d7352 pixels and implemented various data augmentation strategies, such as random rotation, cropping, and contrast adjustments, to mitigate overfitting. The Adam optimizer was employed, with a learning rate of le-4 and a batch size of 8. The training duration was set to 50 epochs for the ORSSD dataset, 45 epochs for EORSSD, and 35 epochs for ORSI-4199, adjusting the learning rate to 0.1 after every 30 epochs. Fig. 7 illustrates the loss during training of the ORSI4199 dataset. To maintain fairness in our experiments, we obtained the results for other advanced methods using either the provided source codes or saliency maps by their authors. Specifically, for the NSI-SOD method, we retrained using the ORSI-SOD dataset according to the recommended configurations."}, {"title": "B. Comparison with State-of-the-Art Methods", "content": "We compare and analyze the method proposed in this study with 28 advanced methods, including two traditional methods, seven NSI-SOD methods, and 19 ORSI-SOD methods.\nTo ensure a comprehensive evaluation of all methodologies, we utilized an array of curves and metrics to facilitate a detailed quantitative analysis.\nWe evaluated the performance of the saliency model using F-measure curves and Precision-Recall (P-R) curves, presented across two rows in Fig. 8. For the Fmeasure curves, a broader area under the curve indicates superior model performance, while in the P-R curves, proximity to the upper right corner (coordinate (1,1)) denotes better performance. Fig. 8 clearly shows our method outperforming competing approaches across both metrics.\nTo assess the performance across the three datasets, we utilized three widely recognized SOD metrics: mean absolute error value (M) [58], S-measure(Sa) [59], maximum F-measure (Fmax) [60], mean F-measure (Fmean), adaptive F-measure (Fadp), maximum E-measure (Emax) [61], mean E-measure (Emax), adaptive E-measure (Emax). A lower M score is indicative of superior performance, whereas higher values of others reflect better performance. The formulas for each evaluation indicator are described below. The comparative results are detailed in Table I, Table II and Table III.\n$M = \\frac{1}{H \\times W}\\sum_{x=1}^{W} \\sum_{y=1}^{H}||S(x,y) - G(x,y)||$ \nwhere M represents the mean absolute error value between the predicted saliency map and GT image. S represents the predicted saliency map, G represents GT image, H and W represent the height and the width of S and G respectively.\n$S_a = \\alpha \\times S_o + (1 - \\alpha) \\times S_r$\nwhere Sa represents region-aware and object-aware structural similarity evaluation value, So represents the object-aware structural similarity measure, S, represents the region-aware structural similarity measure, \u03b1 is set to 0.5.\n$F_{\\beta} = \\frac{(1+\\beta^2)Precision Recall}{\\beta^2Precision+Recall}$\nwhere F\u03b2 represents the weighted harmonic of precision and recall, \u03b22 is set to 0.3.\n$E_{\\xi} = \\frac{1}{W\\times H}\\sum_{x=1}^{W} \\sum_{y=1}^{H} \\phi_{FM}(x,y)$\nwhere E\u03be represents binary foreground map evaluation enhanced-alignment measure, \u00d8FM represents the enhanced alignment matrix, H and W are the height and the width of the map, respectively.\nOverall, our method demonstrates strong competitiveness and superiority across the three test datasets. Specifically, as shown in Table I, on the ORSSD dataset, our method slightly underperforms ESGNet inM, Emax and mean. However, it outperforms ESGNet on other metrics (S&:0.9532 (Ours) vs. 0.9480 (ESGNet); Fax:0.9274 (Ours) vs. 0.9220 (ESGNet); Fmean: 0.9154 (Ours) vs. 0.9113 (ESGNet); Fade: 0.9078 (Ours) vs. 0.9076 (ESGNet); Eado: 0.9820 (Ours) vs. 0.9661 (ESGNet)). As shown in Table II, on the EORSSD dataset, our method's performance is slightly below BANet in the Fmax, slightly below IP2GRNet in Fmean, and slightly below ESGNet in Fade. However, overall, our method still maintains superiority. As shown in Table III, on the challenging ORSI4199 dataset, our method outperforms GLGCNet in nearly all metrics except for a marginal 0.04% lower score in Emean (M:0.0268 (Ours) vs. 0.0274 (GLGCNet); S\u03b1: 0.8884 (Ours) vs. 0.8839 (GLGCNet); Fax: 0.8845 (Ours) vs. 0.8808 (GLGCNet); Fmean: 0.8787 (Ours) vs. 0.8712 (GLGCNet); Fadp: 0.8764 (Ours) vs. 0.8672 (GLGCNet); Emax: 0.9518 (Ours) vs. 0.9508 (GLGCNet); E Eado: 0.9491 (Ours) vs. 0.9245 (GLGCNet)). Our method exhibits significant improvements compared to all other evaluated methods.\nIn summary, our method has demonstrated its superiority and effectiveness over these advanced methods through comprehensive qualitative analysis."}, {"title": "D. Computational Efficiency Analysis", "content": "To evaluate the computational efficiency of our method, we benchmarked it against others based on network parameters (Param), floating-point operations (FLOPs), and frames per second (FPS), with the findings illustrated in Fig. 11. In Fig. 11, red bars represent Param, blue bars represent FLOPs, and the black line represents FPS. The results clearly indicate that ACCoNet has the highest Param, yet its FPS performance is not optimal. MJBRM exhibits high FLOPs, indicating substantial computational demand during execution, but it has a lower FPS. Notably, FSMINet and SeaNet, which are tailored for lightweight applications, register the minimal network parameters and FLOPs, as well as the highest FPS. Compared to most other methods, our model maintains a better balance in network parameters and FLOPs while excelling in various performance metrics. Additionally, our accuracy on three datasets significantly surpasses theirs. Combined with the results presented in Table I, Table II and Table III, our method maintains favorable network parameters, FLOPs, and FPS, while ensuring high-precision outcomes. In summary, our method surpasses the other methods in these aspects. In conclusion, our method exceeds the performance of competing methods across these key metrics."}, {"title": "E. Ablation Analysis", "content": "In this section, we validate the effectiveness of key components within the network through extensive ablation studies conducted on the ORSI4199 dataset.\nTo verify the effectiveness of the EFABA and GDAL modules, we designed three variants by sequentially adding modules to the baseline: 1) Baseline, where we use PVT-V2-B2 as the baseline; 2) Baseline + EFABA; 3) Baseline + GDAL. The quantitative analysis results are shown in Table V. In table, adding our proposed EFABA module and GDAL module to the baseline separately results in a significant improvement in accuracy. This confirms the effectiveness of the EFABA and GDAL modules within the network. However, the accuracy of these variants is significantly lower than that of the complete LBA-MCNet, which demonstrates that both modules are essential for effective salient object detection. This further proves the superiority of our LBA-MCNet. Additionally, we compare the saliency maps of different variants in Fig. 12. From the figure, it is evident that the Baseline is susceptible to errors and omissions due to interference from complex backgrounds. The incorporation of the EFABA module, which captures edge details and perceives local context, enables effective detection of salient objects while being more sensitive to boundary information. The GDAL module is designed to model the global context information at the image level, effectively suppressing interference from complex backgrounds and accurately identifying salient objects. The figure shows that adding each module can effectively detect salient objects, but there are still certain limitations. Our method combines the two, judiciously utilizing the strengths of both modules to correctly and completely detect salient objects while overcoming complex background interference."}, {"title": "F. Failure Cases and Limitations", "content": "While the experiments conducted have comprehensively affirmed the efficacy of our approach, LBA-MCNet, there remain scenarios where it faces challenges. Fig. 17 showcases some illustrative failure cases, which essentially fall into two primary categories: 1) multiple salient objects, as depicted in the first two columns. It's significant to acknowledge that while the training set of the original dataset includes images of these objects, which might be prominent in various scenes, the dataset seldom features images with two prominent objects possessing distinct attributes. This scarcity can contribute to a discrepancy between the results produced by our method and the actual ground truth values. 2) similar object and background colors, as illustrated in the last two columns. The close resemblance in colors between background and salient objects presents a significant challenge in completely mitigating interference. Moreover, as Table III illustrates, our method demonstrates substantial benefits when applied to larger and more complex datasets, such as ORSI-4199. This success can be chiefly ascribed to our approach's proficiency in harmonizing boundary details with local semantics, alongside its effectiveness in modeling the image-level global context. However, the constraints posed by the limited size and diversity of datasets might restrict our ability to fully address a broad spectrum of scenarios and variations, as well as to adequately capture essential semantic information and spatial details."}, {"title": "V. CONCLUSION", "content": "In this paper, we delve into the significance of boundary and contextual features for SOD and present LBA-MCNet, a cutting-edge approach tailored for ORSI-SOD. Leveraging a transformer as our foundational backbone for feature extraction, LBA-MCNet integrates two pivotal modules: EFABA and GDAL. The EFABA module is adept at identifying edge details within shallow features, significantly improving the model's capacity to locate salient objects. It synergizes these details with an attention mechanism that adeptly captures the local context, all the while maintaining essential semantics. This strategic combination ensures a harmonious equilibrium between spatial precision and local semantic richness across the network's various layers. Moreover, the GDAL module concentrates on leveraging deep features to construct image-level global context association graphs through explicit affinity learning. It further enriches the model by encoding global information via implicit affinity learning, efficiently recognizing global patterns within images. This module also fosters a more cohesive expression of multilayer features by applying deep supervision within the decoding layer. An extensive comparative analysis, involving 28 different methods across three publicly accessible datasets, underscores LBA-MCNet's exceptional performance and its distinguished place within the domain of ORSI-SOD, confirming the method's effectiveness and superior capability. Additionally, we plan to improve EFABA and GDAL to make them plug-and-play modules, allowing for more effective application to other salient object detection tasks in the future."}]}