{"title": "Personalized 2D Binary Patient Codes of Tissue Images and Immunogenomic Data Through Multimodal Self-Supervised Fusion", "authors": ["Areej Alsaafin", "Abubakr Shafique", "Saghir Alfasly", "H.R.Tizhoosh"], "abstract": "The field of medical diagnostics has witnessed a transformative convergence of artificial intelligence (AI) and healthcare data, offering promising avenues for enhancing patient care and disease comprehension. However, this integration of multimodal data, specifically histopathology whole slide images (WSIs) and genetic sequencing data, presents unique challenges due to modality disparities and the need for scalable computational solutions. This paper addresses the scarcity of multimodal solutions, primarily centered around unimodal data solutions, thus limiting the realization of the rich insights that can be derived from integrating images and genomic data. Here, we introduce MarbliX \"Multimodal Association and Retrieval with Binary Latent Indexed matriX,\" an innovative multimodal framework that integrates histopathology images with immunogenomic sequencing data, encapsulating them into a concise binary patient code, referred to as \"monogram.\" This binary representation facilitates the establishment of a comprehensive archive, enabling clinicians to match similar cases. The experimental results demonstrate the potential of MarbliX to empower healthcare professionals with in-depth insights, leading to more precise diagnoses, reduced variability, and expanded personalized treatment options, particularly in the context of cancer.", "sections": [{"title": "Introduction", "content": "The traditional diagnosis of cancer primarily relies on the microscopic examination of tissue slides. However, over recent decades, the diagnostic process has undergone a transformation with the integration of molecular tests. This evolution has led to more precise grading and prognosis for cancer patients. While molecular assays have made remarkable strides in various aspects, the intricate task of assessing tissue morphology still falls within the realm of skilled pathologists. The landscape of cancer diagnosis is currently experiencing a rapid shift, largely driven by the advent of digital pathology. Machine learning techniques have the potential to revolutionize the clinical workflow. While traditional approaches may focus solely on morphological features in histopathology whole slide images (WSIs) when studying tumors in a given sample case, recent advancements in machine learning techniques have expanded our ability to extract valuable insights from complex molecular data. Generally, relying solely on morphological features to understand cancer may provide an incomplete picture. However, with the advent of deep learning, researchers have gained the ability to delve deeper into the molecular and genetic aspects of cancer. For instance, deep learning models can analyze extensive biological sequence databases, uncovering intricate patterns and associations that might have been overlooked using conventional methods.\nIn the domain of cancer immunogenomics, the adaptive immune system has demonstrated its remarkable ability to detect tumor antigens at an early stage, initiating a defense against cancer cells [1, 2]. This immune response involves the proliferation of tumor antigen-specific T lymphocytes (T cells) and B lymphocytes (B cells), which play pivotal roles in the fight against cancer [3]. Similarities found in both T cell receptor (TCR) and B cell receptor (BCR) sequences, which are protein complexes located on the surface of T cells and B cells, signify shared antigen specificity among receptors, offering a promising avenue for the discovery of novel therapeutic targets [3\u20135]. The quantifiable measures of the adaptive immune system's diversity, as provided by immunogenomic data, contribute to the improved categorization of patients who would derive optimal benefits from specific treatments [3, 6]. This addresses the prevalent observation that patients sharing the same clinical diagnosis or symptoms often respond differently to identical treatments [7, 8]. The analysis of immune cell sequencing data holds promise in predicting the effectiveness of treatments on specific cancerous cells, thereby curbing costs and enhancing the sustainability and efficiency of healthcare systems[9]. Recent research underscores the capacity of deep neural networks to glean meaningful insights from complex immunogenomic data patterns. This encompasses tasks such as outcome prediction or clustering of TCR and BCR sequences to uncover their underlying similarities [4, 10\u201312].\nThe integration of diverse data streams derived from various sources provides pathologists and clinicians with a holistic perspective on cancer's complex manifestations. Each modality illuminates unique facets of tumor biology, collectively offering crucial insights into a patient's prognosis and treatment options. However, the inherently high-dimensional nature of some of most of the clinical data modalities poses a formidable challenge for manual interpretation, making it arduous for clinicians to extract meaningful information from these multimodal biomedical datasets in order"}, {"title": "Results", "content": "MarbliX framework is designed with the purpose of fusing the subtle morphological characteristics found in histopathology images with the intricate immune response patterns within immune cell sequencing profiles. The underlying rationale for this choice is deeply rooted in the profound clinical implications of these modalities. Histopathology data, with its potentials to unveil the subtleties of tissue structures, and immune cell sequencing data, capable of unraveling the molecular nuances of immune responses.\nMarbliX was implemented and evaluated using datasets obtained from The Cancer Genome Atlas (TCGA), which included histopathology images and genomic data. Specifically, cases from two primary sites, lung and kidney, were sourced from TCGA. Only cases featuring both WSI and genomic profiles were considered. This resulted in a dataset comprising 535 lung adenocarcinoma (LUAD) cases and 510 lung squamous cell carcinoma (LUSC) cases for the lung primary site. For the kidney primary site, the dataset consisted of three classes: 508 kidney renal clear cell carcinoma (KIRC) cases, 248 kidney renal papillary cell carcinoma (KIRP) cases, and 38 kidney chromophobe (KICH) cases. MarbliX was evaluated using 5-fold cross validation for lung dataset while 2-fold cross validation was used to evaluate kidney dataset due to the limited"}, {"title": "MarbliX Training Evaluation", "content": "Different experiments were conducted to evaluate the quality of the patient representation. An experiment was performed to analyze the quality of the multimodal latent association applied through the hybrid autoencoders to map histopathology features to immunogenomic features, and vice versa. As shown in Figure 2, the cosine similarity was calculated between every histopathology and immunogenomic embedding extracted by the pretrained model with the reconstructed histopathology and immunogenomic, respectively, embeddings by the trained hybrid autoencoders. The violin plots in Figure 2 show the distribution of the cosine similarities between the original embedding and the reconstructed embedding of every case in the test sets of (a) lung dataset and (b) kidney dataset. The median cosine similarities of 0.95 for lung histopathology and 0.91 for kidney histopathology indicate a strong overall resemblance between the original and reconstructed embeddings, suggesting effective feature retention. Similarly, in the immunogenomic distribution, with median cosine similarities of 0.93 for lung and 0.94 for kidney, the model maintains a high degree of similarity on average during the reconstruction process. The tightly packed quartiles in both lung and kidney histopathology plots imply consistent performance across the majority of cases. However, the wider spread in the upper quartile of the immunogenomic kidney plot suggests challenges in accurately reconstructing complex patterns, potentially attributed to the limited representation of the KICH subtype in the training set. This limitation may lead to deviations in reconstructions when confronted with new, less familiar patterns in the test set.\nIn Figure 2, the accompanying bar plot provides a quantitative assessment of the reconstruction quality by illustrating the average mean squared error (MSE) between the original embeddings and their reconstructed counterparts for both lung and kidney datasets. Remarkably, the MSE values exhibit consistency across both histopathology and immunogenomics reconstructions, ranging between 0.020 and 0.030 for both datasets. The MSE values indicate that the model performs well in minimizing the squared differences between the original and reconstructed embeddings, reaffirming the reliability of the reconstruction process for both histopathological and immunogenomic features in the lung and kidney datasets.\nThe training curves shown in Figure 2 (c) provide an overview of the learning curves of the final MarbliX models trained on triplet loss, aiming to learn monogram representations for lung and kidney cases. The curves represent the evolution of the loss function over training epochs for each model. Initially, both models exhibit a sharp decrease in the loss, indicative of effective learning and convergence tendency. However, as the epochs progress, subtle differences emerge. The model trained on the kidney dataset demonstrates a slightly more gradual decline in the loss compared to its lung counterpart. This might suggest that the kidney dataset poses greater complexity or variability, requiring the model to adapt more cautiously.\nThe binary monograms generated by MarbliX were analyzed to assess both inter and intra-dissimilarity between the representations. To address the limited number of"}, {"title": "MarbliX Generates Discriminative Patient Representations", "content": "In assessing the efficacy of MarbliX in generating high quality multimodal patient representations, an experiment was conducted, comparing them to unimodal representations derived from histopathology and immunogenomics data. PCA transformation was applied to extract the top 64 components from each embedding in the lung and kidney test sets, using folds not employed during training. These components were then projected into a high-dimensional space, forming the t-SNE maps presented in Figure 3. The maps provide comprehensive insights into feature properties, highlighting distinctions among various subtype classes. In Figure 3 (a), lung histopathology image embeddings of LUAD and LUSC are intermixed, lacking a clear discriminative pattern. Figure 3 (b) demonstrates a more discriminative pattern in immunogenomic embeddings, although some LUSC features intertwine with LUAD features. Merging histopathology and immunogenomics features into a monogram using MarbliX enhances distribution in Figure 3 (c) and (d), showcasing real and binary features. Notably, the t-SNE maps reveal improved separation of LUAD and LUSC, particularly using binary monogram representations. Transitioning to kidney maps, Figure 3 (e) shows two well-separated clusters of mixed cased from the three kidney subtypes in the histopathology t-SNE map. Analysis reveals that this separation is influenced by different hospitals, indicating the dominance of WSI technical properties in the embeddings. Immunogenomic t-SNE map (Figure 3 (f)) exhibits a more meaningful distribution than histopathology, with KIRC being the most discriminated class due to the available number of cases during training compared to KIRP and KICH. Despite this, MarbliX application refines the distribution of patient representations (Figure 3"}, {"title": "Efficient Multimodal Similarity Search", "content": "MarbliX demonstrated its primary utility through the application of the monogram representation for conducting efficient multimodal search and retrieval (Figure 4. The monogram, a condensed representation, integrates diverse features extracted from multimodal data, thereby streamlining the process of multimodal search and retrieval. MarbliX was employed to fuse each patient's histopathology image and immunogenomic data into a monogram. To assess MarbliX's performance, \"leave-one-out\" validation was performed, utilizing monograms generated by MarbliX for cases in the test folds of lung and kidney datasets. Cases in the test folds were not part of the training set. The evaluation involved searching for similar cases within the monograms archive, treating each case as a query case by excluding it from the search process. The majority vote criterion was applied for top-3 (MV@3), top-5 (MV@5), and top-10 (MV@10) retrievals, requiring at least n/2 + 1 of the top-n retrieved cases to belong to the same diagnosis class as the query case."}, {"title": "MarbliX Monogram Binary Representation Analysis", "content": "The primary goal of MarbliX is to generate multimodal monogram representations that are similar for patients with similar characteristics and distinct for those with different attributes. In this study, similarity is defined based on a shared diagnosis, specifically the commonality of cancer subtypes. Figure 6 illustrates MarbliX's efficacy in achieving this objective. The figure presents four LUAD monograms and four LUSC monograms generated for randomly selected samples from each class (refer to Figure2 (d) for a quantified monogram difference). It is evident that matrices from patients with the same subtype exhibit a consistent pattern (intra-similarity).\nTo highlight this intra-similarity and inter-dissimilarity, the figure displays the result of applying bitwise XOR on each LUAD matrix with every other matrix within the LUAD set. The same process was applied to the LUSC set, resulting in {LUSCset- LUSCset}. Additionally, the set of LUAD was XORed with the set of LUSC to identify the differences between each LUAD matrix and every LUSC matrix in the sets. Yellow pixels represent values turning from 0 to 1, while purple indicates the shift from"}, {"title": "Discussion", "content": "The motivation for this research work stems from the significant challenges and unexplored opportunities that arise due to the conspicuous scarcity of multimodal solutions adept at merging histopathology images with genetic sequencing data in general, and with immunogenomic data in particular. Furthermore, as far as knowledge extends, the fusion of histopathology images with immunogenomic data remains an untapped area. By fusing these data modalities within the MarbliX framework, the objective is to effectively bridge critical research gaps and contribute to a more comprehensive understanding of cancer cases, while also unlocking new avenues for potential research in this domain. The work at hand brings forth four substantial contributions to the burgeoning field of multimodal research.\nFirst and foremost, MarbliX introduces a novel approach that condenses diverse data modalities into a concise representation, referred to as monogram. This innovative concept enables the generation of monogram for each individual patient, serving as a tangible representation of their multimodal data. This representation encapsulates the intricate patterns unearthed from various data sources, thereby affording each patient a highly informative imprint or signature. This approach empowers researchers and medical practitioners to extract rich insights from a patient's multimodal clinical data, facilitating personalized and data-driven healthcare.\nThe second contribution of MarbliX lies in its ability to create a binary barcode of multimodal patient data, which often comprises voluminous files like histopathology whole slide images (WSIs) and genetic sequencing data. This compactness holds profound implications for time efficiency, computational complexity, and storage requirements. Employing monograms in diverse downstream tasks such as search and retrieval not only streamlines these operations but also unlocks a powerful tool that can handle data in varying formats, sizes, complexities, and sources.\nFurthermore, the third significant contribution of MarbliX is its backbone independence, a feature that distinguishes it as a versatile multimodal AI framework. The capability to map different modalities within a latent space not only contributes to the model's training efficiency but also enables the extraction of associated patterns from data modalities originating from diverse sources within the medical field. This adaptability is paramount for addressing the heterogeneous nature of medical data.\nLastly, but no less important, MarbliX serves as a valuable asset in facilitating multimodal search, a crucial task in medical research and diagnosis. With the ability to construct a monogram-indexed archives, MarbliX empowers practitioners with an interactive tool to multimodal search for cases similar to a given query. This process involves generating a monogram for the query case and then scouring the archive to retrieve the most analogous monograms that share similar patterns with the query. This feature enhances the discoverability of relevant cases and expedites the decision-making process in healthcare.\nThe evaluation of MarbliX has demonstrated its remarkable success in capturing multimodal patient representations by integrating histopathology images and immunogenomic data to generate monogram representations. These monograms serve as condensed binary representations of patients' multimodal features, achieved through unimodal processing and transformation, multimodal latent association by training"}, {"title": "Methods", "content": "This section describes the details of MarbliX's design and the intricate process involved in generating a unique monogram representation. An overview of MarbliX is illustrated in Figure 1. The design of MarbliX involves three main phases: unimodal transformation, multimodal latent association, and monogram representation. The details of every phase is described below.\nUnimodal Transformation\nThe integration of histopathology images and immune cell sequencing data into a shared computational framework requires an alignment step to transform them into a common format. This enables joint manipulation and integration within a unified model. Hence, as a first stage, each modality, is processed and transformed into a single feature vector or embedding. For simplicity, in the context of the notation (I, S), I represents the histopathology image, while S represents the immunogenomic data (a set of immune cell sequences) of a given case."}, {"title": "Image processing:", "content": "to represent the WSIs, SPLICE [17] was employed to select representative patches, forming a collage for image I after segmenting the tissue region from the background using Otsu thresholding. This collage is a condensed representation, composed of a select set of representative patches extracted from I, capturing the crucial tissue characteristics that define the image. The collage was generated by setting the similarity threshold to the 30th percentile, striking a balance between performance and computational/storage requirements. Once the collage is generated for I, the next step involves extracting deep features from the individual patches that compose the collage. This process is achieved by leveraging a pre-trained deep neural network F, here we used DINO ViT [18], which possesses the ability to extract patterns and meaningful information within these patches. As a result of this indexing, a feature vector $f_i$ of patch $P_i$ within the collage is generated by applying $f_i = F(P_i)$, where $f_i \\in \\mathbb{R}^{l \\times 1}$.\nTo craft an all-encompassing feature vector that encapsulates the entirety of image I and effectively represents its rich content, a widely adopted practice involves computing the average of the patch-level feature vectors [19], resulting in a single feature vector $f \\in \\mathbb{R}^{l \\times 1}$ that serves as a holistic representation of the entire image I. As we used DINO ViT, this resulted in a 768-dimensional embedding for the entire WSI."}, {"title": "Sequencing data processing:", "content": "for the immunogenomic data, raw RNA-seq files from TCGA were utilized to reconstruct the immune repertoire of every patient. TRUST4 [20] was employed to obtain the TCR and BCR sequences of each patient from their RNA-seq profiles. Rare sequences and artifacts were filtered out by excluding those that were not common to a sufficient proportion of patients within each subtype class. Through experimentation, for the lung dataset, sequences that were not common to at least 30% of the patients within the subtype class were excluded, while a lower threshold of 15% was applied to kidney cases due to the limited number of samples. Before we encode the sequencing data into a dense vector, we applied Seqwash [21] method to preprocess the sequencing profiles and prepare them for feature extraction. Seqwash is a \"harmonization\" approach tailored to genetic sequencing data and serves as a crucial preprocessing step, aimed at preparing these sequences for analysis using deep models designed for textual data by overcoming the impact of the variability among patients in terms of sequence lengths and unregulated sequence orders. Therefore, Seqwash was used to unify the patient profiles by aligning them into a standardized representation before proceeding with deep feature extraction. The ultimate goal is to create a single, coherent embedding that encapsulates vital information while negating the effects of varying sequence orders within each patient's profile. Applying Seqwash on sequencing profile S results in a harmonized set $S_h$. A pre-trained deep learning model G is then employed to distill features from the sequences by applying $g = G(S_h)$, where $g \\in \\mathbb{R}^{l \\times 1}$ represents a feature vector extracted from the harmonized sequences set $S_h$. Here, we employed BERT which resulted in a 768-dimensional embedding."}, {"title": "Multimodal Latent Association", "content": "Following the transformation of each modality, histopathology image I into embedding f and immune cell sequence profile S into embedding g, the association between these"}, {"title": "Algorithm 1 Multimodal Latent Association", "content": "embeddings is learned. However, as these embeddings come from different models, they have different ranges. Therefore, min-max rescaling was performed to bring the embeddings to a common scale before learning the association between them. After normalization, association learning was performed by projecting the two embeddings into a shared latent space. In this shared space, the embeddings from both modalities coalesce to form a concise patient representation. This step addresses the issue of non-relevant features that may exist in the uni-modal data obtained from a pretrained network. By merging these embeddings into an encoded representation, we want to extract and consolidate the pertinent features from each modality, enhancing their combined synergy and informative value in subsequent analyses.\nTo accomplish this, as shown in Algorithm 1, two deep neural networks with an encoder-decoder (autoencoder) architecture are employed, each tailored to emphasize the salient features from its corresponding modality while suppressing extraneous information. This step addresses the issue of non-relevant features that may exist in the uni-modal data obtained from a pretrained network. By merging these embeddings into an encoded representation, we want to extract and consolidate the pertinent features from each modality, enhancing their combined synergy and informative value in subsequent analyses. This is achieved by training the two hybrid models to generate an encoded latent representation, highlighting the relevant features. Specifically, each autoencoder model is designed to take one modality and reconstruct the other, resulting in a latent representation that embodies the dominant features of its respective modality."}, {"title": "MarbliX Monogram", "content": "After generating the two latent representations u and v, the next crucial step within the MarbliX framework is the projection and indexing of these representations into a 2D binary matrix, referred to as \"monogram.\" This matrix serves as an effective representation for capturing the intricate relationships and correlations that exist between the two modalities. The process of learning the monogram representation is the core of the MarbliX framework, enabling a comprehensive exploration of the joint information encoded within u and v.\nTo accomplish this task, a deep neural network, denoted as Q, is designed to uncover the correlations between histopathology and immunogenomic features based on diagnosis. This process aims to unveil the underlying structure that interlinks histopathological characteristics with immune cell behavior among cases within the same diagnostic class. In other words, the model is engineered to capture the commonality in multimodal relationships among cases that share similar diagnoses, embedding these features within their respective monograms. Simultaneously, it strives to discern the distinctions between cases of different diagnostic classes and accentuate these"}, {"title": "Algorithm 2 Learning Multimodal Monogram Representation", "content": "Within each branch (Algorithm 2, Lines 11-15), the pair of u and v is projected into a matrix through the computation of the outer product between their respective layers. This tensor is then flattened and passed through three consecutive dense layers of size 1024, 256, and 64 to learn the deep multimodal relationship. The last layer of the model has a binary branch that generates a binary representation of the last layer. This is crucial as it enables the generation of compact binary representations, highly efficient for subsequent indexing and storage. As the tanh function results in"}, {"title": "Algorithm 2 Learning Multimodal Monogram Representation", "content": "The triplet loss function (Equation 1) calculates the distances between the anchor's predicted matrix and both the positive and negative matrices.\n$L_{triplet}(a, p, n) = max\\{d(a, p) \u2013 d(a, n) + \u03b1, 0\\}$\nIn Equation 1, a represents the anchor case, p signifies a positive case (sharing the same diagnosis as the anchor), and n denotes a negative case (with a different diagnosis from the anchor). d(a, p) calculates the distance between the anchor and positive case matrices, while d(a, n) computes the distance between the anchor and negative case matrices. The margin parameter \u03b1 ensures a minimum separation between the positive and negative cases.\nModel Q was trained for 150 epochs using the tanh activation function and Adam optimizer with a learning rate of $1 \\times 10^{\u22125}$. After training the Q model, it was utilized to generate binary monogram representations for new cases. This was done by applying $monogram = Q(\\{u, v\\})$, where monogram represents an 8 \u00d7 8 binary matrix (with encoding capability to cover $2^{64} = 1.8 \\times 10^{19}$ combinations) derived from the latent representations u and v of the histopathology image and immune cell sequences, respectively."}, {"title": "Competing Interests", "content": "The Authors declare no Competing Financial."}, {"title": "Data Availability", "content": "The data used in this work is publicly available in The Cancer Genome Atlas (TCGA) (URL: https://portal.gdc.cancer.gov/)."}, {"title": "Author Contributions", "content": "AA implemented the approach, run all experiments and wrote the first draft of the paper. AS and SA contributed to implementation and experiments. HRT conceptually designed the approach, supervised the implementation and experiments, and wrote and rewrote large parts of the paper."}]}