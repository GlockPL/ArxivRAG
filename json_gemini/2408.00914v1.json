{"title": "Granting GPT-4 License and Opportunity: Enhancing Accuracy and Confidence Estimation for Few-Shot Event Detection", "authors": ["Steven Fincke", "Adrien Bibal", "Elizabeth Boschee"], "abstract": "Large Language Models (LLMs) such as GPT- 4 have shown enough promise in the few-shot learning context to suggest use in the generation of \"silver\" data and refinement of new ontologies through iterative application and review. Such workflows become more effective with reliable confidence estimation. Unfortunately, confidence estimation is a documented weakness of models such as GPT-4, and established methods to compensate require significant additional complexity and computation. The present effort explores methods for effective confidence estimation with GPT-4 with few-shot learning for event detection in the BETTER ontology as a vehicle. The key innovation is expanding the prompt and task presented to GPT-4 to provide License to speculate when unsure and Opportunity to quantify and explain its uncertainty (L&O). This approach improves accuracy and provides usable confidence measures (0.759 AUC) with no additional machinery.", "sections": [{"title": "1 Introduction", "content": "Large language models such as GPT-4 have shown particular utility in the few-shot learning context, offering the promise of facilitating the creation of large annotation sets for new tasks. This study explores prompting strategies to enhance confidence estimation, which we refer to as License & Opportunity (L&O). Our task is the detection of events in English-language news stories in the BETTER ontology (Mckinnon and Rubino, 2022). L&O simply uses one query to the LLM to obtain the output along with the confidence estimation, and it does not require access to the LLMs internal statistics, nor any LLM fine-tuning. Reliable confidence estimation is particularly useful when such a tool is used for tasks with categories that are easily confused, leading to low inter-rater agreement. It also guides the evaluation of LLM output, both for purposes of refining an ontology under development and prioritizing the review of \"silver\" annotations."}, {"title": "2 Related Work", "content": "Few-shot learning has been successfully applied with neural LMs, including LLMs, to the task of event detection (Barth, 2022; Gao et al., 2024); i.e., the detection of event phrases (triggers or anchors) and labeling the according to a specified ontology of event types. Such methods have even been applied to highly inclusive event ontologies such as Open Information Extraction (Ling et al., 2023; McCusker, 2023). Deng et al. (2023) provide a survey of recent work in information extraction and directly compare various approaches built upon BERT-like LMs and those using LLMs; they report 0.539 micro F1 for the full ACE05 event detection task using a 5-shot training strategy with GPT-4, where the SOTA fine-tuning a BERT-like LM with a full training set is at 0.837 micro F1.\nConfidence estimation, however, has not been a focus of prior scholarship on few-shot event detection with LLMs. If we consider a broader array of NLP tasks, we see that various methods have been developed to extract reliable confidence estimations from LLMs. The fact that GPT-4 usually produces high confidence values when asked naively significantly complicates the task (Singh et al., 2023). Many studies frame the problem as confidence calibration; that is calibrating the output confidence probability to the actual observed probability of correctness in a labeled dataset (Guo et al., 2017; Tian et al., 2023). One family of approaches exploits the internal statistics of the LLM, such as the log-probs of tokens conveying a particular answer.\nFor example, Wu et al. (2024) compare variants of \"Einstein was born in the year X\", where X is 1878, 1879, or 1880. They observe the log probabilities for the various year strings, hopefully providing the highest probability for 1879. Such white box approaches contrast with black box techniques which do not use such internal statistics. Some find an advantage in using natural language expressions of confidence instead of generating numbers (Lin et al., 2022; Tian et al., 2023). These techniques, e.g. Singh et al. (2023), often involve complex, multi-stage strategies such as \"Chain of Thought\" (Wei et al., 2023) and \"Tree of Thoughts\" (Yao et al., 2023).\nOne important feature of L&O (our approach) is requesting explanations in addition to answers and confidence ratings. Some pre-existing \"black-box\" methods also include the generation of explanations as part of their confidence pipelines. Li et al. (2024) generate justifications for each of a few possible answers and then estimates confidence from these explanations. Xiong et al. (2024) report that their self probing prompting method is particularly effective for GPT-4; this approach considers each possible answer to a question separately and requests an explanation and confidence; these are reviewed together to generate normalized confidence levels for all the options. Unlike L&O, these do not provide explanations along with the final confidence output."}, {"title": "3 Data and Task", "content": "We utilize here a portion of IARPA's BETTER task (Mckinnon and Rubino, 2022), which focuses on information extraction from news stories in the cross-lingual context. The Basic portion has an event ontology which eventually expanded, in the third and final phase of the program, to 114 categories; these are grouped into 12 topics ranging from crime to finance. All the annotations for development are for English texts. The full three phases provided annotations for 732 English-language news stories. However, the program evaluations only considered performance on texts in Arabic, Farsi, Russian, Chinese, and Korean.\nAn effective solution to the BETTER Basic event extraction task uses BIO token labeling to mark event phrases for the full ontology in one pass (Jenkins et al., 2023). XLM-ROBERTa-large (Conneau et al., 2020) was fine-tuned to the full training set with the base model providing effective zero-shot cross-lingual transfer from the English training data to the BETTER program languages.\nBy contrast, this effort utilizes few-shot learning with a single submission to an LLM. We eliminate the cross-lingual aspect, evaluating on English data, instead. We ask the LLM to mark only the beginning of each event phrase with a vertical pipe (1); we do not ask for the entire span because of the difficulty of crafting effective guidelines for selecting the exact scope of phrases in the few-shot context. For resource reasons, we constrain our efforts to only three of the twelve basic topics, as detailed in Table 1. We also exclude all event types with fewer than 10 instances in all our available annotations."}, {"title": "4 System", "content": "We provide example prompts and raw output in Figure 1. We will first highlight some important features of our approach and then explain details. L&O adopts two related strategies: 1) urging the LLM to provide guesses when in doubt, and 2) providing the LLM ample opportunity to characterize uncertainty in generating the event type label (or lack thereof). The latter was provided, in part, to allow consumers of the output to differentiate between outright guesses and confident answers.\nThe LLM is prompted to provide a confidence rating ranging 1-5, where 5 indicates the highest confidence in the presence of an event of the specified type in the sentence, and 1 marks the greatest confidence in the absence of the event type. 1 is appropriate when no event instances are predicted, as well as for very unlikely guesses. Unlike more conventional approaches, this does not convey the LLM's confidence in its answer, regardless of its content; the consequences of such a strategy are discussed in Section 5.\nA complete response also includes an explanation. Additionally, the LLM is encouraged to ask a (fictional) expert yes/no questions about the specific event type, with an eye on refining annotation conventions. These requests are inspired by techniques such as CoT (Wei et al., 2023), but our prompt provides no example of expected explanations or yes/no questions and does not encourage decomposing the task into simpler logical steps. Though not explicitly encouraged by our prompt, the LLM often uses the explanation to account for its confidence rating, not just the presence or absence of the label."}, {"title": "4.1 Task and prompting details", "content": "Each topic is treated as a separate task, and each query to the LLM asks for labels for a single event type at a time, but the LLM is told which other event types fall within the same topic. This design simplifies the task by allowing the LLM to focus on one event type at a time; it increases the possibility of accuracy suffering due to incompatible outputs for different event types for the same sentence, but the scope of this effort does not include a mechanism for reconciling output for related event types.\nOur prompt provides up to 5 sentences with at least one instance of the event type in question marked with a vertical pipe before the first word of each event phrase. To select these, we group each instance of the given event type by the lemma of the first event word, ignoring those with only one instance, and then randomly sample according to lexical type. If fewer than five lexical types are attested, more examples are taken from more frequent types. All few-shot examples are excluded from the testing pool, as well as all sentences with fewer than 25 characters. The test repeats a cycle of one sentence with at least one event in the chosen topic in the reference annotations and then three sentences with no in-topic events. The prompt lists the names of other event types in the same topic and notes that no word can bear more than one event type label, i.e., they are mutually exclusive.\nThe LLM provides its answer by repeating the sentence with vertical pipes marking the beginning of event phrases; our scripts include text alignment code to be robust to the imperfections in the LLM's copy. If there are no instances of the event in the sentence, the LLM is simply instructed to repeat the whole sentence unaltered. As discussed earlier, the LLM is also asked to provide a confidence rating, explanation and a list of yes/no questions. When more than one event phrase is indicated, the LLM generates a separate triplet for each event phrase."}, {"title": "4.2 Scoring", "content": "We evaluate the LLM's predictions according to the output confidence level. The explanation and yes/no questions are not used by any subsequent step (unlike Li et al., 2024; Xiong et al., 2024) and not in adjudication, but Section 5 will demonstrate their contribution to system performance. A response is judged correct if the marked word coincides with the first word of a reference event phrase of the specified type. No credit is given for marking another word within a phrase or indicating a related but different event type. If the reference has multiple phrases for the same event, the LLM is expected to mark the beginning of each. As we sweep our confidence levels, we include all the outputs for higher levels of confidence (if any). We characterize the performance for each topic with precision, recall, and macro F1 score of the confidence level with the highest F1. We also compute a ROC AUC\nfor each positive output from our system, we provide the generated confidence score and a label for correctness. This statistic indicates the probability of a randomly selected true positive having a"}, {"title": "5 Ablation studies", "content": "We will explore here the impact of various design features of L&O. To contain this effort, we will focus on the initial 210 sentences of the run for the Law topic. In one set of variants, we exclude components of L&O that are means for the LLM to characterize its stance to its output. First, we request a confidence level for each output but do not ask for an explanation or any yes/no questions. We add a variant of this where the confidence score is more conventional; i.e., the prompt asks the LLM to generate a 5 if it is highly confident in its answer and 1 if quite uncertain regardless of the content of the answer; a 5 score can be provided when highly confident in the absence, as well as presence, of an event. We also consider the inverse, where the prompt requests both an explanation and list of yes/no questions but no confidence score. We exclude our L&O pleas for guessing with explanation in the last variant. Table 3 indicates performance on our 210-sentence Law subset for our full system and these ablations.\nRequesting confidence only, i.e., dropping the explanation and yes/no questions, degrades F1 somewhat, especially for recall, but the AUC is actually somewhat better. Since explanations often provide rationales for the confidence rating, not just the labeling decision, it appears that GPT-4 generates better confidence rating when given the opportunity to account for its ouptut. However, the conventional variant, where 5 is requested when the LLM is highly confident in both positive labels and the lack thereof, is markedly worse in terms of F1 and AUC. Excluding the confidence ratings necessarily reduces the AUC to chance, but the F1 is comparable to Confidence only. Quite strikingly, dropping the appeals to guess leaves F1 largely unchanged in relation to the full system, except for a modest drop in recall, but the confidence scores lose most of their value, and the AUC is only 0.080 above chance."}, {"title": "6 Discussion", "content": "This study shows that GPT-4 provides increasingly better output as it is given more opportunity to characterize and explain its response. This is more effective when the structure avoids complicating the logical structure of the task. For example, we interpret the absolute 0.111 F1 gain from using our default confidence scoring scheme instead of the conventional one as indicating that GPT-4 performs better when fewer logical operations are required. The task already probes the presence or absence of events of the given type; we maintain consistency by allowing the LLM to provide confidence in the presence of the event. The conventional approach requires an additional level of indirection, by asking for a characterization of the LLMs response, not the input data.\nIf we also urge GPT-4 to guess and explain itself when uncertain, we get useful confidence estimations and a modest improvement in recall. Without these appeals, GPT-4's event extraction output is rather similar: it continues to speculate, but the confidence scores no longer indicate less certain outputs. We observe that GPT-4 is capable of directly indicating its level of confidence, but it needs to be given explicit license to speculate along with ample means to provide its stance to its response. GPT-4 has clearly been subject to significant examination and public scrutiny; it is not an open-source effort, but we speculate that L&O bypasses features imposed onto GPT-4 to guard against embarrassing output. Gaining confidence scores with a useful AUC significantly increases the utility of models such as GPT-4 in developing and extending modest annotation resources for tasks such as event detection."}, {"title": "7 Future Work", "content": "Though modest in scope, this study presents a promising technique for eliciting useful confidence judgments from GPT-4 while improving F1 in the few-shot setting. Various additional lines of research would expand our understanding of value of this approach. First, additional baselines would be helpful. Evaluating a BERT-like model on the present version of the BETTER Basic event detection task, both few-shot and fine-tuned, would facilitate the assessment of the accuracy of L&O. A baseline for confidence estimation using a white-box technique, such as the use of the log-probabilities from GPT-4 for vertical pipes in output would help contextualize our reported AUC values.\nSecond, we did not explore the issue of calibration error: many systems express confidence as probability of correctness, and analyses examine the statistical gap between these figures and observed accuracy rates. Instead, we only requested confidence scores 1-5, and we did not attempt to associate each of these scores with specific precision rates. We would also like to explore the use of verbalized confidence (Tian et al., 2023), which could easily be applied to the present task.\nOther potential lines of study include the application of L&O to LLMs other than GPT-4 and tasks beyond our variant of BETTER. We observed that prediction in the presence of a label was associated with better F1 and AUC than confidence in the correctness of the response, regardless of its content. Extending this logic to some NLP tasks could be challenging, but we would like to better understand the scope of applicability and relevance of this technique."}, {"title": "8 Conclusion", "content": "This study presents L&O, which combines a set of prompting techniques for effectively performing the event detection task with GPT-4 in the few-shot setting. More importantly, we provide a strategy for exposing GPT-4's capacity to provide useful confidence scores. This crucially depends upon urging the model to speculate and explain: simply providing the opportunity to explain is not enough. We suspect that GPT-4 has been engineered to default to indicating high certainty, and our appeal lifts this \"hold\" on the model's functionality. However, the design and breadth of opportunities to explain also impact performance. Eliminating explanations and yes/no questions lowers F1 and AUC. We interpret this, in part, as GPT-4 being designed to avoid \"traps\": it indicates more of its weaknesses when encouraged to do so. The venues for characterizing uncertainty need to be ample but also well suited to the task presented to the model, as shown with the degradation employing the \"conventional\" confidence approach."}, {"title": "9 Limitations", "content": "Section 7 described various limits of this study which could be addressed in later efforts. We also note that we only used GPT-4 and only examined annotation of English texts."}, {"title": "Ethics Statement", "content": "This system enhances F1 and confidence estimation, but many errors remain: users cannot assume that system output is accurate, when marked with high confidence."}]}