{"title": "Attack-Aware Noise Calibration for Differential Privacy", "authors": ["Bogdan Kulynych", "Juan Felipe Gomez", "Georgios Kaissis", "Flavio du Pin Calmon", "Carmela Troncoso"], "abstract": "Differential privacy (DP) is a widely used approach for mitigating privacy risks when training machine\nlearning models on sensitive data. DP mechanisms add noise during training to limit the risk of information\nleakage. The scale of the added noise is critical, as it determines the trade-off between privacy and utility. The\nstandard practice is to select the noise scale in terms of a privacy budget parameter \u025b. This parameter is in turn\ninterpreted in terms of operational attack risk, such as accuracy, or sensitivity and specificity of inference attacks\nagainst the privacy of the data. We demonstrate that this two-step procedure of first calibrating the noise scale\nto a privacy budget &, and then translating & to attack risk leads to overly conservative risk assessments and\nunnecessarily low utility. We propose methods to directly calibrate the noise scale to a desired attack risk level,\nbypassing the intermediate step of choosing \u025b. For a target attack risk, our approach significantly decreases\nnoise scale, leading to increased utility at the same level of privacy. We empirically demonstrate that calibrating\nnoise to attack sensitivity/specificity, rather than 8, when training privacy-preserving ML models substantially\nimproves model accuracy for the same risk level. Our work provides a principled and practical way to improve\nthe utility of privacy-preserving ML without compromising on privacy.", "sections": [{"title": "1 Introduction", "content": "Machine learning and statistical models can leak information about individuals in their training data, which can\nbe recovered by membership inference, attribute inference, and reconstruction attacks (Fredrikson et al., 2015;\nShokri et al., 2017; Yeom et al., 2018; Balle et al., 2022b). The most common defenses against these attacks are\nbased on differential privacy (DP) (Dwork et al., 2014). Differential privacy introduces noise to either the data, the\ntraining algorithm, or the model parameters (Chaudhuri et al., 2011). This noise provably limits the adversary's\nability to run successful privacy attacks at the cost of reducing the utility of the model.\nIn DP, two parameters \u025b and \u03b4 control the privacy-utility trade-off by determining the scale (e.g., variance) of\nthe noise added during training. Smaller values of these parameters correspond to larger noise. Larger noise\nprovides stronger privacy guarantees but reduces the utility of the trained model. Typically, d is set to a small fixed\nvalue (usually between 10-8 and 10\u22125), leaving \u025b as the primary tunable parameter. Without additional analysis,\nthe parameters (\u03b5, \u03b4) alone do not provide a tangible and intuitive operational notion of privacy risk (Nanayakkara\net al., 2023). This begs the question: how should practitioners, regulators, and data subjects decide on acceptable\nvalues of \u025b and 8 and calibrate the noise scale to achieve a desired level of protection?\nA standard way of assigning operational meaning to DP parameters is mapping them to attack risks. One\ncommon approach is computing the maximum accuracy (equivalently, advantage) of membership inference\nattacks that (\u03b5, \u03b4) allows (Wood et al., 2018). An alternative is to compute the trade-off curve between sensitivity\nand specificity of feasible membership inference attacks (Dong et al., 2022), which was recently shown to be\ndirectly related to success rates of record reconstruction attacks (Hayes et al., 2024; Kaissis et al., 2023a). All these\nanalyses map (\u03b5, \u03b4) to a quantifiable level of risk for individuals whose data is present in the dataset. Studies"}, {"title": "2 Problem Statement", "content": "2.1 Preliminaries\nSetup and notation. Let Dn denote the set of all datasets of size n over a space ID, and let S ~ S' denote a\nneighboring relation, e.g. that S, S' differ by one datapoint. We study randomized algorithms (mechanisms) M(S)\nthat take as input a dataset S \u2208 Dn, and output the result of a computation, e.g., statistical queries or an ML model.\nWe denote the output domain of the mechanism by \u0398. For ease of presentation, we mainly consider randomized\nmechanisms that are parameterized by a single noise parameter \u03c9 \u2208 \u03a9, but our results extend to mechanisms\nwith multiple parameters. For example, in the Gaussian mechanism (Dwork et al., 2014), M(S) = q(S) + Z,\nwhere Z ~ \u039d(0, \u03c3\u00b2) and q(S) is a non-private statistical algorithm, the parameter is w = \u03c3 with \u03a9 = R+. We\ndenote a parameterized mechanism by Mw (S).\nDifferential Privacy. For any y \u2265 0, we define the hockey-stick divergence from distribution P to Q over a\ndomain O by\n\\(D_\u03b3(P \\| Q) \\triangleq \\sup_E P(E) \u2212 \u03b3Q(E)\n\\)\nwhere the supremum is taken over all measurable sets EC O. We define differential privacy (DP) (Dwork et al.,\n2006) as follows:\nDefinition 2.1. A mechanism M(\u00b7) satisfies (\u03b5, \u03b4)-DP iff for all S ~ S', \\(D_\u03b5 \u2208 (M(S), M(S')) \u2264 \u03b4\\).\nLower values of \u025b and 8 mean more privacy, and vice versa. In the rest of the paper we assume w.l.o.g. that\nlarger value of the parameter w means that the mechanism M(\u00b7) is more noisy, which translates into higher\nlevel of privacy (smaller \u03b5, \u03b4), but lower utility.\nMost DP algorithms satisfy a collection of (\u03b5, \u03b4)-DP guarantees. We define the privacy profile (Balle and Wang,\n2018), or privacy curve (Gopi et al., 2021; Alghamdi et al., 2022) of a mechanism as:\nDefinition 2.2. A parameterized mechanism (.) has a privacy profile \u025bw : R \u2192 [0, 1] if for every \u03b4 \u2208 [0, 1],\n\u039c\u03c9(\u00b7) is (\u03b5(\u03b4), \u03b4)-DP.\nIn a slight abuse of notation, we refer to the function \u03b4\u03c9(\u03b5), defined analogously, also as the privacy profile.\nDP-SGD. A common algorithm for training neural networks with DP guarantees is DP-SGD (Abadi et al.,\n2016). The basic building block of DP-SGD is the so-called subsampled Gaussian mechanism, defined as M(S) =\nq(PoissonSample,\u25e6S) + Z, where Z ~ N(0, \u22062\u00b702\u00b7 Ia), and PoissonSample, is a procedure which subsamples a\ndataset S such that every record has the same probability p \u2208 (0, 1) to be in the subsample. DP-SGD, parameterized\nby p, \u03c3, and T \u2265 1, is a repeated application of the subsampled Gaussian mechanism: M(1) \u25cb M(2) \u25cb\u06f0\u06f0\u06f0\u25cb M(T) (S),\nwhere q(i) (\u00b7) is a single step of gradient descent with per-record gradient clipping to \u22062 Euclidean norm. In line\nwith a standard practice (Ponomareva et al., 2023), we regard all parameters but o as fixed, thus w \u03c3.\nPrivacy profiles for mechanisms such as DP-SGD are computed via numerical algorithms called accoun-\ntants (see, e.g., Abadi et al., 2016; Gopi et al., 2021; Doroshenko et al., 2022; Alghamdi et al., 2022). These algorithms\ncompute the achievable privacy profile to accuracy nearly matching the lower bound of a privacy audit where\nthe adversary is free to choose the entire (pathological or realistic) training dataset (Nasr et al., 2021, 2023). Given\nthese results, we regard the analysis of these accountants as tight, and use them for calibration to a particular\n(\u03b5, \u03b4)-DP constraint."}, {"title": "2.2 Operational Privacy Risks", "content": "It is possible to interpret differential privacy entirely through the lens of membership inference attacks (MIAs)\nin the so-called strong adversary model (see, e.g., Nasr et al., 2021). In this framework, the adversary aims to\ndetermine whether a given output \u03b8\u2208 O came from M(S) or M(S'), where S' = SU {z} for some target\nexample z \u2208 D. The adversary has access to the mechanism M(\u00b7), the dataset S, and the target example z \u2208 D.\nSuch an attack is equivalent to a binary hypothesis test (Wasserman and Zhou, 2010; Kairouz et al., 2015; Dong\net al., 2022):\n\\(H_0: \u03b8 \\sim M(S), \\quad H_1: \u03b8 \\sim M(S'),\n\\)\nwhere the MIA is modelled as a test 6 : \u2192 [0, 1] that maps a given mechanism output @ to the probability\nof the null hypothesis Ho being rejected. Dong et al. (2022) analyzes this hypothesis test through the trade-off\nbetween the achievable false positive rate (FPR) \u03b1\u03c6\u2261 EM(S)[$] and false negative rate (FNR) \u03b2$ \u22661-EM(s')[d],\nwhere the expectations are taken over the coin flips in the mechanism. Dong et al. (2022) formalizes the trade-off\nfunction and defines f-DP as follows:\nDefinition 2.3. A trade-off curve T(M(S), M(S')) : [0, 1] \u2192 [0, 1] represents the FNR of the most powerful\nattack at any given FPR level a:\n\\(T(M(S), M(S'))(a) = \\inf\\{\\beta_\u03c6 \\| \u03b1_\u03c6 \u2264 \u03b1, \u03c6 : \u0398 \u2192 [0,1]\\}\n\\)"}, {"title": "Standard Calibration", "content": "The procedure of choosing the parameter \u03c9 \u2208 \u03a9 to satisfy a given level of privacy is\ncalled calibration. In standard calibration, one chooses w given a target DP guarantee &* and an accountant that\nsupplies a privacy profile \u03b5\u03c9(\u03b4) for any noise parameter \u03c9 \u2208 N, to ensure that M(S) satisfies (\u03b5*, *)-DP:\n\\( \\underset{\\Theta \\ni \\omega}{ \\operatorname{min}} \\omega \\text { s.t. } \\varepsilon_{\\omega}\\left(\\delta^{\\*}\\right) \\geq \\varepsilon^{\\*},\\)\nwith d* set by convention to d* = 1/cn, where n is the dataset size, and c > 1 (see, e.g., Ponomareva et al.,\n2023). The parameter \u025b* is also commonly chosen by convention between 3 and 10 for privacy-persevering ML\nalgorithms with practical utility (Ponomareva et al., 2023; Sander et al., 2023; De et al., 2022; Wutschitz et al.,\n2022). In Eq. (2) and the rest the paper we denote by * the target value of privacy risk.\nAfter calibration, the (\u025b*, \u03b4*) parameters are often mapped to some operational notation of privacy attack\nrisk for interpretability. In the next section, we introduce the hypothesis testing framework of DP, f-DP, and the\nnotions of risk that (\u03b5, \u03b4) parameters are often mapped to. In contrast to standard calibration, in Section 2.3, we\ncalibrate w to minimize these privacy risks."}, {"title": "2.3 Attack-Aware Noise Calibration", "content": "The standard practice in DP is to calibrate the noise scale w of a mechanism M\u0ed6(\u00b7) to some target (\u03b5*, *)-DP\nguarantee, with \u025b* from a recommended range, e.g., \u03b5* \u2208 [3, 10], and d* fixed to d* < 1/n, as in Eq. (2). Then,\nthe privacy guarantees provided by the chosen (\u025b*, \u03b4*) are interpreted by mapping these values to bounds\non sensitivity and specificity (by Proposition 2.5) or advantage (by Proposition 2.6) of membership inference\nattacks. In this work, we show that if the goal is to provide an operational and interpretable privacy guarantee,\nthis approach leads to unnecessarily pessimistic noise requirements and a deterioration in utility due to the\nintermediate step of setting (\u03b5*, \u03b4*). Then, we show that it is possible to skip this intermediate step by using"}, {"title": "3 Numeric Calibration to Attack Risks", "content": "In this section, we provide methods for calibrating DP mechanisms to the notions of privacy risk in Section 2.2.\nAs a first step, we introduce the core technical building blocks: methods for evaluating advantage nw and the\ntrade-off curve fw (a) of a mechanism for a given value of w.\nDominating Pairs and PLRVs. We make use of two concepts, originally developed in the context of computing\ntight privacy profiles under composition: dominating pairs (Zhu et al., 2022) and privacy loss random variables\n(PLRV) (Dwork and Rothblum, 2016).\nDefinition 3.1. We say that a pair of distributions (P, Q) is a dominating pair for a mechanism M(\u00b7) if for every\n\u2208 \u2208 R and S ~ S', we have Dee (M(S) || M(S')) \u2264 Dee (P || Q).\nImportantly, a dominating pair also provides a lower bound on the trade-off curve of a mechanism:\nProposition 3.2. If (P,Q) is a dominating pair for a mechanism M(\u00b7), then for all S ~ S':\n\\(T(M(S), M(S')) \\geq T(P,Q).\\)\nThe proofs of this and all the following statements are in Appendix D. Proposition 3.2 implies that a mechanism\nM(\u00b7) is f-DP with f = T(P,Q). Next, we introduce privacy loss random variables, which provide a natural\nparameterization of the curve T(P,Q).\nDefinition 3.3. Suppose that a mechanism M(\u00b7) has a discrete-valued dominating pair (P, Q). Then, we define\nprivacy loss random variables (PLRVs) (X, Y) as Y \u2252 log Q(0)/P(o), with 0 ~ Q, and X log Q(o')/P(o') with\n0~ P.\nWe can now state the result which serves as a main building block for our calibration algorithms, and forms\nthe main theoretical contribution of our work.\nTheorem 3.4 (Accounting for advantage and f-DP with PLRVs). Suppose that a mechanism M(\u00b7) has a discrete-\nvalued dominating pair (P, Q) with associated PLRVs (X, Y). Attack advantage n for this mechanism is bounded:\n\\(\u03b7 < Pr[Y > 0] \u2013 Pr[X > 0].\\)\nMoreover, for any\u0442\u2208 R, \u04af\u2208 [0, 1], we have T(P,Q)(\u03b1(\u03c4,\u03b3)) = \u03b2(\u03c4, \u03b3), where:\n\\(\u03b1(\u03c4, \u03b3) = Pr[X > \u03c4] + y Pr[X = \u03c4], \u03b2(\u03c4, \u03b3) = Pr[Y < \u03c4] \u2212 y Pr[Y = \u03c4].\\)\nWe remark that similar results for the trade-off curve appear in (Zhu et al., 2022), though the y terms are\nnot included as the PLRVs (X, Y) are assumed to be continuous. In this work, we rely on the technique due\nto Doroshenko et al. (2022) (summarized in Appendix E), which discretizes continuous mechanisms such as the\nsubsampled Gaussian in DP-SGD, and provides a dominating pair that is discrete and finitely supported over\nan evenly spaced grid. As the dominating pairs are discrete, the y terms are non-zero and necessary to fully\nreconstruct the trade-off curve."}, {"title": "3.1 Advantage Calibration", "content": "First, we show how to instantiate Eq. (9) to calibrate noise to a target advantage \u03b7* \u2208 [0, 1]. Let no denote the\nadvantage of the mechanism M\u0ed6 (\u00b7) as defined in Eq. (7):\n\\( \\underset{\\Theta \\ni \\omega}{ \\operatorname{min}} \\omega \\text { s.t. } \u03b7_{\\omega} \\leq \u03b7^{\\*}.\\)\nGiven the PLRVs (Xw, Yw), we can obtain a substantially tighter bound than converting (\u03b5, \u03b4) guarantees using\nProposition 2.6 under standard calibration. Specifically, Theorem 3.4 provides the following way to solve the\nproblem:\n\\( \\underset{\\Theta \\ni \\omega}{ \\operatorname{min}} \\omega \\text { s.t. } \\operatorname{Pr}\\left[Y_{\\omega}>0\\right]-\\operatorname{Pr}\\left[X_{\\omega}>0\\right]<\\eta^{\\*}\\)\nWe call this approach advantage calibration, and show how to practically ensure it in Algorithm 3 in the Appendix.\nGiven a method for obtaining valid PLRVS X\u03c9, \u03a5\u03c9 for any w, advantage calibration is guaranteed to ensure\nbounded advantage, which follows by combining Proposition 3.2 and Theorem 3.4:\nProposition 3.5. Given PLRVs (Xw, Yw) of a discrete-valued dominating pair of a mechanism M\u025b(\u00b7), choosing w*\nusing Eq. (14) ensures \u03b7\u03c9* < \u03b7*.\nUtility Benefits. We demonstrate how calibration for a given level of attack advantage can increase utility.\nAs a mechanism to calibrate, we consider DP-SGD with p = 0.001 subsampling rate, T = 10,000 iterations,\nand assume that d* = 10-5. Our goal is to compare the noise scale o obtained via advantage calibration to the\nstandard approach.\nAs a baseline, we choose o using standard calibration in Eq. (2), and convert the resulting (\u03b5, \u03b4) guarantees\nto advantage using Proposition 2.6. We consider target values of advantage \u03b7* \u2208 [0.01, 0.25]. As we show in\nFigure 3a, for any target value of advantage, the direct calibration procedure enables to reduce the noise scale by\nup to 2x.\nPitfalls of Calibrating for Advantage. Calibration to a given level of membership advantage is a compelling\nidea due to the decrease in noise required to achieve the same level of attack risk. Despite this increase in utility,\nwe caution that this approach comes with a deterioration of privacy guarantees other than maximum advantage.\nCalibrating for a given level of advantage can be particularly dangerous without understanding the risks, as it\nallows for increased attack success in the security-critical regime of low attack FPR (see Section 2.2). The next\nresult quantifies this pitfall:\nProposition 3.6 (Cost of advantage calibration). Fix a dataset size n > 1, and a target level of attack advantage\n\u03b7* \u2208 (\u03b4*, 1), where d* = 1/cn for some c > 1. For any 0 < a < \\(1-\\frac{n^{\\*}}{2}\\), there exists a DP mechanism for which the\ngap in FNR fstandard(a) obtained with standard calibration for \u025b* that ensures \u03b7 \u2264 n*, and FNR fadv (a) obtained\nwith advantage calibration is lower bounded:\n\\( \\Delta \\beta(\\alpha) \\triangleq \\operatorname{fstandard}(\\alpha)-\\operatorname{fadv}(\\alpha) \\geq \\eta^{\\*}-\\delta^{\\*}+2 \\alpha \\frac{\\eta^{\\*}}{\\eta^{\\*}-1}\\)\nFor example, if we aim to calibrate a mechanism to at most \u03b7* = 0.5 (or, 75% attack accuracy), we could\npotentially increase attack sensitivity by \u2206\u03b2(\u03b1) \u2248 30 p.p. at FPR a = 0.1 compared to standard calibration with\n8* = 10-5 (see the illustration in Figure 3b).\nNote that the difference A\u1e9e in Proposition 3.6 is an overestimate of the actual harm due to advantage\ncalibration for realistic mechanisms. Indeed, the increase in attack sensitivity can be significantly lower for\nmechanisms such as the Gaussian mechanism (see Figure 6 in the Appendix)."}, {"title": "3.2 Safer Choice: Calibration to FNR within a Given FPR Region", "content": "In this section, we show how to calibrate the noise in any practical DP mechanism to a given minimum level\nof attack FNR \u1e9e* within an FPR region a \u2208 [0, a*]. We base this notion of risk off the previous work (Carlini\net al., 2022; Rezaei and Liu, 2021) which argued that MIAs are a relevant threat only when the achievable TPR\n1 - \u1e9e is high at low FPR a. We instantiate the calibration problem in Eq. (9) as follows, assuming M\u0ed6 (\u00b7) satisfies\nfw(a)-DP:\n\\( \\underset{\\Theta \\ni \\omega}{ \\operatorname{min}} \\omega \\text { s.t. } \\underset{0<a>\\)*}\n\\)"}, {"title": "3.3 Other Approaches to Trade-Off Curve Accounting", "content": "In this section, we first contextualize the proposed method within existing work. Then, we discuss settings in\nwhich alternatives to PLRV-based procedures could be more suitable.\nBenefits of PLRV-based Trade-Off Curve Accounting. Computational efficiency is important when estimat-\ning fw(a), as the calibration problem requires evaluating this function multiple times for different values of w as\npart of binary search. Algorithm 1 computes fw (a) for a single w in \u2248 500ms, enabling fast calibration, e.g., in\n\u2248 1 minute for DP-SGD with T = 10,000 steps on commodity hardware (see Appendix F). Existing methods for\nestimating fw (a), on the contrary, either provide weaker guarantees than Proposition 3.8 or are substantially less\nefficient. In particular, Dong et al. (2022) introduced \u00b5-GDP, an asymptotic expression for f(a) as T\u2192 \u221e, that\noverestimates privacy (Gopi et al., 2021), and thus leads to mechanisms that do not satisfy the desired level of"}, {"title": "4 Experiments", "content": "In this section, we aim to empirically evaluate the utility improvements of our calibration methods in simulations\nas well as in realistic applications.\nSimulations. First, we demonstrate the noise reduction from calibrating for given error rates in the case of\nthe DP-SGD algorithm using the setup in Section 3.1. We fix three low FPR regimes: a* \u2208 {0.01, 0.05, 0.1}, and\nvary maximum attack sensitivity 1 \u2013 \u03b2* from 0.1 to 0.5 in each FPR regime. We show the results in Figure 4. We\nobserve a significant decrease in the noise scale for all values. Although the decrease is less drastic than with\ncalibration for advantage (see Figure 3a), in this case, we avoid the pitfalls of advantage calibration by directly\ncalibrating for risk in the low FPR regime.\nLanguage Modeling and Image Classification. We have showed that FNR/FPR calibration enables to\nsignificantly reduce the noise scale, but it is unclear how much of this reduction in noise translates into actual\nutility improvement in downstream applications. To answer this question, we evaluate our method for calibrating\nnoise in private deep learning on two tasks: text sentiment classification using the SST-2 dataset (Socher et al.,\n2013), and image classification using the CIFAR-10 dataset (Krizhevsky et al., 2009).\nFor sentiment classification, we fine-tune GPT-2 (small) (Radford et al., 2019) using a DP version of LoRA (Yu\net al., 2021). For image classification, we follow the approach of Tramer and Boneh (2021) of training a convolu-\ntional neural network on top of ScatterNet features (Oyallon and Mallat, 2015) with DP-SGD (Abadi et al., 2016).\nSee additional details in Appendix F. For each setting, by varying the noise scale, we obtain several models at\ndifferent levels of privacy. For each of the models we compute the guarantees in terms of TPR 1 \u2013 3 at three"}, {"title": "5 Concluding Remarks", "content": "In this work, we proposed novel methods for calibrating noise in differentially private learning targeting a given\nlevel of operational privacy risk: advantage and FNR/FPR of membership inference attacks. Using simulations\nand end-to-end experiments, we showed that our calibration methods significantly decrease the required level\nof noise compared to the standard approach at the same level of operational risk. In the case of calibration for\nadvantage, we also showed that the noise decrease could be harmful as it could allow for increased attack success\nin the low FPR regime, whereas calibration for a given level of FNR/FPR mitigates this issue. Next, we discuss\nlimitations and possible directions for future work.\nChoice of Target FPR. We leave open the question on how to choose the target FPR a*, e.g., whether standard\nsignificance levels in sciences such as a* = 0.05 are compatible with data protection regulation. Future work is\nneeded to develop concrete guidance on the choice of target FPR informed by legal and practical constraints.\nTight Bounds for Privacy Auditing. Multiple prior works on auditing the privacy properties of ML algo-\nrithms (Nasr et al., 2021; Liu et al., 2021; Jayaraman and Evans, 2019; Erlingsson et al., 2019) used conversions\nbetween (\u03b5, \u03b4) and operational risks like in Proposition 2.5, which we have shown to be loose for practical mech-\nanisms. Beyond calibrating noise, our methods provide a tight and computationally efficient way of obtaining\nbounds on attack success rates for audits.\nAccounting in the Relaxed Threat Model. Although we have focused on DP, our methods apply to any notion\nof privacy that is also formalized as a hypothesis test. In particular, our method can be used as is to compute\nprivacy guarantees of DP-SGD in a relaxed threat model (RTM) proposed by Kaissis et al. (2023b). Previously,\nthere was no efficient method for accounting in the RTM.\nApplications Beyond Privacy. Our method can be applied to ensure provable generalization guarantees in deep\nlearning. Indeed, prior work has shown that advantage \u03b7 bounds generalization gaps of ML models (Kulynych\net al., 2022). Thus, even though advantage calibration can exacerbate certain privacy risks, it can be a useful tool\nfor ensuring a desired level of generalization in models that usually do not come with non-vacuous generalization\nguarantees, e.g., deep neural networks."}, {"title": "A Attack-Aware Noise Calibration with Black-box DP Accountants", "content": "Advantage Calibration. Proposition 2.6 implies that (0, \u03b4)-DP mechanisms ensure bounded advantage \u03b7 < \u03b4.\nTherefore, given access to a black-box accountant \u025b\u025b(\u03b4) or \u03b4\u03b7(\u03b5) we can calibrate to a given level of advantage\n\u03b7* by ensuring (0, \u03b7*)-DP:\n\\( \\underset{\\Psi \\ni \\omega}{ \\operatorname{min}} \\omega \\text { s.t. } \\varepsilon_{\\omega}\\left(\\eta^{\\*}\\right)=0 \\text { or } \\delta_{\\omega}(0)=\\eta^{\\*}\\)\nThis is a more generic way to perform advantage calibration using an arbitrary black-box accountant. It is\nequivalent to our procedure in Section 3.1 when using Doroshenko et al. (2022) accountant.\nFPR/FNR Calibration with Grid Search. Given a black-box DP accountant, i.e., a method which computes the\nprivacy profile \u025bw(\u03b4) of a mechanism M\u0ed6(\u00b7), we can approximate fw (a) by discretizing the range of \u03b4 \u2208 [0, 1]\nand solving Eq. (6) as:\n\\( \\operatorname{f}_{\\omega}(\\alpha) \\geq \\sup _{\\delta \\in\\left\\{\\delta_{1}, \\delta_{2}, \\ldots, \\delta_{u}\\right\\}} \\max \\left\\{0,1-\\delta-\\varepsilon_{\\omega}(\\delta) \\alpha, e^{-\\varepsilon_{\\omega}(\\delta)}(1-\\delta-\\alpha)\\right\\},\\)\nwhere 0 < \u03b4\u2081 < \u03b42 < ... < \u03b4u \u2264 1. It is possible to perform an analogous discretization using \u03b4\u03c9(\u03b5) and\nProposition D.1, in which case we have to additionally choose a bounded subspace & \u2208 [Emin, Emax] CR.\nEquivalent procedures to Eq. (20) have previously appeared in Nasr et al. (2023); Zheng et al. (2020).\nPlugging in Eq. (20) into the problem in Eq. (18), we can calibrate mechanisms to a given a*, \u03b2* using binary\nsearch (see Section 2.3) in a space [wmin, max] \u2286 \u03a9 to additive error werr > 0. Denoting by v:\n\\(v=\\frac{\\omega_{\\max }-\\omega_{\\min }}{\\omega_{\\mathrm{err}}},\\)\nthe calibration requires u \u00b7 [log2 v] evaluations of \u025b\u025b(\u03b4). For instance, a single evaluation of the bound in Eq. (20)\ntakes approximately one minute with u = 100, and six minutes with u = 1,000 for DP-SGD with T = 10,000\nusing Gopi et al. (2021) accountant as an instantiation of \u025b\u03c9 (\u03b4) on commodity hardware (see Appendix F). In\ncontrast, evaluating fw(.) using Algorithm 1 in the same settings takes approximately 500ms at the default\ndiscretization level \u2206 = 10-4 (see Appendix E).\nAlthough this approach is substantially less computationally efficient than our direct procedure in Section 3.2,\nits strength is that it can be used to calibrate noise in any DP algorithm which provides a way to compute its\n(\u03b5, \u03b4) guarantees."}, {"title": "B Calibrating Gaussian Mechanism", "content": "In the case where the trade-off curve of the mechanism has a closed form, we can solve the calibration problems\nin Eqs. (13) and (18) exactly without resorting to the numerical procedures in Sections 3.1 and 3.2.\nDefinition B.1. For a given non-private algorithm q : 2D \u2192 Rd, a Gaussian mechanism (GM) is defined as\nM(S) = q(S) + \u03be, where \u00a7 ~ N(0, \u22062 \u00b7 \u03c3\u00b2 \u00b7 Id) and \u25b32\u2261 sups~s' ||q(S) \u2013 q(S')||2 is the sensitivity of q(S).\nFor the Gaussian mechanism, we can exactly compute the relevant adversary's error rates:\nProposition B.2 (Balle and Wang (2018); Dong et al. (2022)). Suppose that Mo(S) is GM with sensitivity A2 and\nnoise variance \u03c3\u00b2. Denote by \u00b5 = \u25b32/6 and by \u03a6(t) the CDF of the standard Gaussian distribution N (0, 1). Then,\n\u2022\nThe mechanism satisfies (\u03b5, \u03b4)-DP if the following holds:\n\\(\\delta=\\Phi\\left(\\Phi^{-1}(1-\\alpha)-\\sqrt{\\frac{\\mu^{2}}{\\sigma^{2}}}\\right)-\\mathrm{e}^{\\varepsilon} \\alpha\\left(-\\Phi^{-1}\\left(-\\alpha\\right)-\\sqrt{\\frac{\\mu^{2}}{\\sigma^{2}}}\\right)\n\u2022 It satisfies f-DP with:\n\\(f(\\alpha)=\\Phi\\left(\\Phi^{-1}(1-\\alpha)-\\sqrt{\\mu}\\right)\\)\nWith these closed-form expressions, we can solve the calibration problems exactly:"}, {"title": "C Calibration to Other Risk Notions", "content": "Noise calibration for a given FPR/FNR level can be seen as a basic building block to calibrate for other operational\nmeasures of risk that are functions of FPR a and FNR \u03b2.\nFor instance, Rezaei and Liu (2021) propose to measure the risks of membership inference attacks in terms\nof accuracy acc and FPR a, where: acc(\u03b1, \u03b2) \u2266 1/2 \u00b7 ((1 \u2212 a) + (1 \u2013 \u03b2)). We can calibrate for a given level of\naccuracy acc* and FPR a* using the method in Section 3.2 by solving the expression for accuracy for a given \u1e9e*.\nJayaraman et al. (2021) propose to measure positive predictive value, or precision, of attacks:\n\\(p p v(\\alpha, \\beta) \\triangleq \\frac{1-\\beta}{1-\\beta+\\alpha}\nAlthough precision alone is not sufficient to determine the level of privacy, like with accuracy, we can calibrate\nfor a given level of precision ppv* and FPR a* by deriving the corresponding \u03b2*.\nWe provide the exact conversions in Table 1. These enable practitioners to use the calibration method\nin Section 3.2 while reporting technically equivalent but potentially more interpretable measures, e.g., attack\naccuracy at a given FPR.\nBeyond FNR and FPR. Another avenue of calibrating to other risk measures is by leveraging different notions\nof divergence between probability distributions M(S) and M(S') instead of the the hockeystick divergence in\nSection 2.1. For instance, using Hellinger distance, one could model the success of membership inference attacks\nin which the attacker is given not a single observation as in Section 2.2, but countably many (see Polyanskiy\nand Wu, 2022, Chapter 7). This, therefore, could capture risk with respect to practical repeated-observation\nattacks (e.g., Jayaraman et al., 2021)."}, {"title": "D Deferred Proofs", "content": "First, let us provide an analogous counterpart to Proposition 2.5:\nProposition D.1. A mechanism M satisfies (\u03b5, \u03b4(\u03b5))-DP for all \u03b5\u025b \u2208 R iff it is f -DP with\n\\(f(\\alpha)=\\sup _{\\varepsilon \\in \\mathbb{R}} \\max \\left\\{0,1-\\delta(\\varepsilon)-e^{\\varepsilon} \\alpha, e^{-\\varepsilon}(1-\\delta(\\varepsilon)-\\alpha)\\right\\} .\\)"}, {"title": "E Dominating Pairs", "content": "E.1 Constructing Discrete Dominating Pairs and their PLRVs\nWe summarize the technique from Doroshenko et al. (2022) to construct a dominating pair from a composed\nmechanism M(S) = M(1) \u3002M(2) \u3002\u06f0\u06f0\u06f0\u25cb M(T) (S'). This models the common use case in privacy-preserving ML\nwhere a simple mechanism, such as the subsampled Gaussian in DP-SGD, is applied T times. We assume that\neach sub-mechanism M(i), i \u2208 [T"}]}