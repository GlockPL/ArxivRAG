{"title": "Attack-Aware Noise Calibration for Differential Privacy", "authors": ["Bogdan Kulynych", "Juan Felipe Gomez", "Georgios Kaissis", "Flavio du Pin Calmon", "Carmela Troncoso"], "abstract": "Differential privacy (DP) is a widely used approach for mitigating privacy risks when training machine learning models on sensitive data. DP mechanisms add noise during training to limit the risk of information leakage. The scale of the added noise is critical, as it determines the trade-off between privacy and utility. The standard practice is to select the noise scale in terms of a privacy budget parameter \\(\\varepsilon\\). This parameter is in turn interpreted in terms of operational attack risk, such as accuracy, or sensitivity and specificity of inference attacks against the privacy of the data. We demonstrate that this two-step procedure of first calibrating the noise scale to a privacy budget \\(\\varepsilon\\), and then translating \\(\\varepsilon\\) to attack risk leads to overly conservative risk assessments and unnecessarily low utility. We propose methods to directly calibrate the noise scale to a desired attack risk level, bypassing the intermediate step of choosing \\(\\varepsilon\\). For a target attack risk, our approach significantly decreases noise scale, leading to increased utility at the same level of privacy. We empirically demonstrate that calibrating noise to attack sensitivity/specificity, rather than \\(\\varepsilon\\), when training privacy-preserving ML models substantially improves model accuracy for the same risk level. Our work provides a principled and practical way to improve the utility of privacy-preserving ML without compromising on privacy.", "sections": [{"title": "1 Introduction", "content": "Machine learning and statistical models can leak information about individuals in their training data, which can be recovered by membership inference, attribute inference, and reconstruction attacks (Fredrikson et al., 2015; Shokri et al., 2017; Yeom et al., 2018; Balle et al., 2022b). The most common defenses against these attacks are based on differential privacy (DP) (Dwork et al., 2014). Differential privacy introduces noise to either the data, the training algorithm, or the model parameters (Chaudhuri et al., 2011). This noise provably limits the adversary's ability to run successful privacy attacks at the cost of reducing the utility of the model.\nIn DP, two parameters \\(\\varepsilon\\) and \\(\\delta\\) control the privacy-utility trade-off by determining the scale (e.g., variance) of the noise added during training. Smaller values of these parameters correspond to larger noise. Larger noise provides stronger privacy guarantees but reduces the utility of the trained model. Typically, \\(\\delta\\) is set to a small fixed value (usually between 10^{-8} and 10^{-5}), leaving \\(\\varepsilon\\) as the primary tunable parameter. Without additional analysis, the parameters (\\(\\varepsilon\\), \\(\\delta\\)) alone do not provide a tangible and intuitive operational notion of privacy risk (Nanayakkara et al., 2023). This begs the question: how should practitioners, regulators, and data subjects decide on acceptable values of \\(\\varepsilon\\) and \\(\\delta\\) and calibrate the noise scale to achieve a desired level of protection?\nA standard way of assigning operational meaning to DP parameters is mapping them to attack risks. One common approach is computing the maximum accuracy (equivalently, advantage) of membership inference attacks that (\\(\\varepsilon\\), \\(\\delta\\)) allows (Wood et al., 2018). An alternative is to compute the trade-off curve between sensitivity and specificity of feasible membership inference attacks (Dong et al., 2022), which was recently shown to be directly related to success rates of record reconstruction attacks (Hayes et al., 2024; Kaissis et al., 2023a). All these analyses map (\\(\\varepsilon\\), \\(\\delta\\) to a quantifiable level of risk for individuals whose data is present in the dataset. Studies"}, {"title": "2 Problem Statement", "content": ""}, {"title": "2.1 Preliminaries", "content": "Setup and notation. Let \\(\\mathcal{D}_n\\) denote the set of all datasets of size n over a space \\(\\mathcal{I}\\), and let \\(S \\sim S'\\) denote a neighboring relation, e.g., that \\(S, S'\\) differ by one datapoint. We study randomized algorithms (mechanisms) \\(M(S)\\) that take as input a dataset \\(S \\in \\mathcal{D}_n\\), and output the result of a computation, e.g., statistical queries or an ML model. We denote the output domain of the mechanism by \\(\\Theta\\). For ease of presentation, we mainly consider randomized mechanisms that are parameterized by a single noise parameter \\(\\omega \\in \\Omega\\), but our results extend to mechanisms with multiple parameters. For example, in the Gaussian mechanism (Dwork et al., 2014), \\(M(S) = q(S) + Z\\), where \\(Z \\sim \\mathcal{N}(0, \\sigma^2)\\) and \\(q(S)\\) is a non-private statistical algorithm, the parameter is \\(\\omega = \\sigma\\) with \\(\\Omega = \\mathbb{R}^+\\). We denote a parameterized mechanism by \\(M_\\omega(S)\\).\nDifferential Privacy. For any \\(\\gamma \\ge 0\\), we define the hockey-stick divergence from distribution \\(P\\) to \\(Q\\) over a domain \\(\\mathcal{O}\\) by\n\\[D_\\gamma(P \\| Q) \\triangleq \\sup_{E \\subseteq \\mathcal{O}} P(E) - \\gamma Q(E)\\]\nwhere the supremum is taken over all measurable sets \\(E \\subseteq \\mathcal{O}\\). We define differential privacy (DP) (Dwork et al., 2006) as follows:\nDefinition 2.1. A mechanism \\(M(\\cdot)\\) satisfies \\((\\varepsilon, \\delta)\\)-DP iff for all \\(S \\sim S'\\), \\(D_\\varepsilon (M(S), M(S')) \\le \\delta\\).\nLower values of \\(\\varepsilon\\) and \\(\\delta\\) mean more privacy, and vice versa. In the rest of the paper we assume w.l.o.g. that larger value of the parameter \\(\\omega\\) means that the mechanism \\(M(\\cdot)\\) is more noisy, which translates into higher level of privacy (smaller \\(\\varepsilon\\), \\(\\delta\\)), but lower utility.\nMost DP algorithms satisfy a collection of \\((\\varepsilon, \\delta)\\)-DP guarantees. We define the privacy profile (Balle and Wang, 2018), or privacy curve (Gopi et al., 2021; Alghamdi et al., 2022) of a mechanism as:\nDefinition 2.2. A parameterized mechanism \\(M_\\omega(\\cdot)\\) has a privacy profile \\(\\varepsilon_\\omega : \\mathbb{R} \\to [0, 1]\\) if for every \\(\\delta \\in [0, 1]\\), \\(M_\\omega(\\cdot)\\) is \\((\\varepsilon_\\omega(\\delta), \\delta)\\)-DP.\nIn a slight abuse of notation, we refer to the function \\(\\delta_\\omega(\\varepsilon)\\), defined analogously, also as the privacy profile.\nDP-SGD. A common algorithm for training neural networks with DP guarantees is DP-SGD (Abadi et al., 2016). The basic building block of DP-SGD is the so-called subsampled Gaussian mechanism, defined as \\(M(S) = q(\\text{PoissonSample}_p \\circ S) + Z\\), where \\(Z \\sim \\mathcal{N}(0, \\frac{\\Delta_2^2 \\cdot \\sigma^2}{p \\cdot n} \\cdot \\mathbb{I}_d)\\), and \\(\\text{PoissonSample}_p\\) is a procedure which subsamples a dataset \\(S\\) such that every record has the same probability \\(p \\in (0, 1)\\) to be in the subsample. DP-SGD, parameterized by \\(p, \\sigma\\), and \\(T \\ge 1\\), is a repeated application of the subsampled Gaussian mechanism: \\(M^{(1)} \\circ M^{(2)} \\circ \\dots \\circ M^{(T)}(S)\\), where \\(q^{(i)}(\\cdot)\\) is a single step of gradient descent with per-record gradient clipping to \\(\\Delta_2\\) Euclidean norm. In line with a standard practice (Ponomareva et al., 2023), we regard all parameters but \\(\\sigma\\) as fixed, thus \\(\\omega = \\sigma\\).\nPrivacy profiles for mechanisms such as DP-SGD are computed via numerical algorithms called accoun- tants (see, e.g., Abadi et al., 2016; Gopi et al., 2021; Doroshenko et al., 2022; Alghamdi et al., 2022). These algorithms compute the achievable privacy profile to accuracy nearly matching the lower bound of a privacy audit where the adversary is free to choose the entire (pathological or realistic) training dataset (Nasr et al., 2021, 2023). Given these results, we regard the analysis of these accountants as tight, and use them for calibration to a particular \\((\\varepsilon, \\delta)\\)-DP constraint."}, {"title": "2.2 Operational Privacy Risks", "content": "It is possible to interpret differential privacy entirely through the lens of membership inference attacks (MIAs) in the so-called strong adversary model (see, e.g., Nasr et al., 2021). In this framework, the adversary aims to determine whether a given output \\(\\theta \\in \\mathcal{O}\\) came from \\(M(S)\\) or \\(M(S')\\), where \\(S' = S \\cup \\{z\\}\\) for some target example \\(z \\in \\mathcal{D}\\). The adversary has access to the mechanism \\(M(\\cdot)\\), the dataset \\(S\\), and the target example \\(z \\in \\mathcal{D}\\). Such an attack is equivalent to a binary hypothesis test (Wasserman and Zhou, 2010; Kairouz et al., 2015; Dong et al., 2022):\n\\[\\mathcal{H}_0: \\theta \\sim M(S), \\quad \\mathcal{H}_1: \\theta \\sim M(S'),\\]\nwhere the MIA is modelled as a test \\(\\phi : \\Theta \\to [0, 1]\\) that maps a given mechanism output \\(\\theta\\) to the probability of the null hypothesis \\(\\mathcal{H}_0\\) being rejected. Dong et al. (2022) analyzes this hypothesis test through the trade-off between the achievable false positive rate (FPR) \\(\\alpha_{\\phi} \\triangleq \\mathbb{E}_{M(S)}[\\phi]\\) and false negative rate (FNR) \\(\\beta_{\\phi} \\triangleq 1 - \\mathbb{E}_{M(S')}[\\phi]\\), where the expectations are taken over the coin flips in the mechanism. Dong et al. (2022) formalizes the trade-off function and defines f-DP as follows:\nDefinition 2.3. A trade-off curve \\(T(M(S), M(S')) : [0, 1] \\to [0, 1]\\) represents the FNR of the most powerful attack at any given FPR level \\(\\alpha\\):\n\\[T(M(S), M(S'))(\\alpha) = \\inf \\{ \\beta_{\\phi} \\mid \\alpha_{\\phi} \\le \\alpha, \\, \\phi : \\Theta \\to [0, 1] \\}\\]"}, {"title": "Standard Calibration", "content": "The procedure of choosing the parameter \\(\\omega \\in \\Omega\\) to satisfy a given level of privacy is called calibration. In standard calibration, one chooses \\(\\omega\\) given a target DP guarantee \\(\\varepsilon^*\\) and an accountant that supplies a privacy profile \\(\\varepsilon_\\omega(\\delta)\\) for any noise parameter \\(\\omega \\in \\Omega\\), to ensure that \\(M(S)\\) satisfies \\((\\varepsilon^*, \\delta^*)\\)-DP:\n\\[\\min_{\\omega \\in \\Omega} \\omega \\quad \\text{s.t.} \\quad \\varepsilon_\\omega(\\delta^*) \\ge \\varepsilon^*,\\]\nwith \\(\\delta^*\\) set by convention to \\(\\delta^* = 1/cn\\), where \\(n\\) is the dataset size, and \\(c > 1\\) (see, e.g., Ponomareva et al., 2023). The parameter \\(\\varepsilon^*\\) is also commonly chosen by convention between 3 and 10 for privacy-persevering ML algorithms with practical utility (Ponomareva et al., 2023; Sander et al., 2023; De et al., 2022; Wutschitz et al., 2022). In Eq. (2) and the rest the paper we denote by \\(\\cdot^*\\) the target value of privacy risk.\nAfter calibration, the \\((\\varepsilon^*, \\delta^*)\\) parameters are often mapped to some operational notation of privacy attack risk for interpretability. In the next section, we introduce the hypothesis testing framework of DP, f-DP, and the notions of risk that \\((\\varepsilon, \\delta)\\) parameters are often mapped to. In contrast to standard calibration, in Section 2.3, we calibrate \\(\\omega\\) to minimize these privacy risks."}, {"title": "2.3 Attack-Aware Noise Calibration", "content": "The standard practice in DP is to calibrate the noise scale \\(\\omega\\) of a mechanism \\(M_\\omega(\\cdot)\\) to some target \\((\\varepsilon^*, \\delta^*)\\)-DP guarantee, with \\(\\varepsilon^*\\) from a recommended range, e.g., \\(\\varepsilon^* \\in [3, 10]\\), and \\(\\delta^*\\) fixed to \\(\\delta^* \\ll 1/n\\), as in Eq. (2). Then, the privacy guarantees provided by the chosen \\((\\varepsilon^*, \\delta^*)\\) are interpreted by mapping these values to bounds on sensitivity and specificity (by Proposition 2.5) or advantage (by Proposition 2.6) of membership inference attacks. In this work, we show that if the goal is to provide an operational and interpretable privacy guarantee, this approach leads to unnecessarily pessimistic noise requirements and a deterioration in utility due to the intermediate step of setting \\((\\varepsilon^*, \\delta^*)\\). Then, we show that it is possible to skip this intermediate step by using"}, {"title": "3 Numeric Calibration to Attack Risks", "content": "In this section, we provide methods for calibrating DP mechanisms to the notions of privacy risk in Section 2.2. As a first step, we introduce the core technical building blocks: methods for evaluating advantage \\(\\eta_\\omega\\) and the trade-off curve \\(f_\\omega(\\alpha)\\) of a mechanism for a given value of \\(\\omega\\).\nDominating Pairs and PLRVs. We make use of two concepts, originally developed in the context of computing tight privacy profiles under composition: dominating pairs (Zhu et al., 2022) and privacy loss random variables (PLRV) (Dwork and Rothblum, 2016).\nDefinition 3.1. We say that a pair of distributions \\((P, Q)\\) is a dominating pair for a mechanism \\(M(\\cdot)\\) if for every \\(\\epsilon \\in \\mathbb{R}\\) and \\(S \\sim S'\\), we have \\(D_\\epsilon (M(S) \\| M(S')) \\le D_\\epsilon (P \\| Q)\\).\nImportantly, a dominating pair also provides a lower bound on the trade-off curve of a mechanism:\nProposition 3.2. If \\((P,Q)\\) is a dominating pair for a mechanism \\(M(\\cdot)\\), then for all \\(S \\sim S'\\):\n\\[T(M(S), M(S')) \\ge T(P,Q).\\]\nThe proofs of this and all the following statements are in Appendix D. Proposition 3.2 implies that a mechanism \\(M(\\cdot)\\) is f-DP with \\(f = T(P,Q)\\). Next, we introduce privacy loss random variables, which provide a natural parameterization of the curve \\(T(P,Q)\\).\nDefinition 3.3. Suppose that a mechanism \\(M(\\cdot)\\) has a discrete-valued dominating pair \\((P, Q)\\). Then, we define privacy loss random variables (PLRVs) \\((X, Y)\\) as \\(Y \\triangleq \\log \\frac{Q(o)}{P(o)}\\), with \\(o \\sim Q\\), and \\(X \\triangleq \\log \\frac{Q(o')}{P(o')}\\) with \\(o' \\sim P\\).\nWe can now state the result which serves as a main building block for our calibration algorithms, and forms the main theoretical contribution of our work.\nTheorem 3.4 (Accounting for advantage and f-DP with PLRVs). Suppose that a mechanism \\(M(\\cdot)\\) has a discrete-valued dominating pair \\((P, Q)\\) with associated PLRVs \\((X, Y)\\). Attack advantage \\(\\eta\\) for this mechanism is bounded:\n\\[\\eta < \\Pr[Y > 0] - \\Pr[X > 0].\\]\nMoreover, for any \\(\\tau \\in \\mathbb{R}\\), \\(\\gamma \\in [0, 1]\\), we have \\(T(P,Q)(\\alpha(\\tau, \\gamma)) = \\beta(\\tau, \\gamma)\\), where:\n\\[\\begin{aligned}\n\\alpha(\\tau, \\gamma) &= \\Pr[X > \\tau] + \\gamma \\Pr[X = \\tau],\n\\\\\\beta(\\tau, \\gamma) &= \\Pr[Y < \\tau] - \\gamma \\Pr[Y = \\tau].\n\\end{aligned}\\]\nWe remark that similar results for the trade-off curve appear in (Zhu et al., 2022), though the \\(\\gamma\\) terms are not included as the PLRVs \\((X, Y)\\) are assumed to be continuous. In this work, we rely on the technique due to Doroshenko et al. (2022) (summarized in Appendix E), which discretizes continuous mechanisms such as the subsampled Gaussian in DP-SGD, and provides a dominating pair that is discrete and finitely supported over an evenly spaced grid. As the dominating pairs are discrete, the \\(\\gamma\\) terms are non-zero and necessary to fully reconstruct the trade-off curve."}, {"title": "3.1 Advantage Calibration", "content": "First, we show how to instantiate Eq. (9) to calibrate noise to a target advantage \\(\\eta^* \\in [0, 1]\\). Let \\(\\eta_\\omega\\) denote the advantage of the mechanism \\(M_\\omega(\\cdot)\\) as defined in Eq. (7):\n\\[\\min_{\\omega \\in \\Omega} \\omega \\quad \\text{s.t.} \\quad \\eta_\\omega \\le \\eta^*.\\]\nGiven the PLRVs \\((X_\\omega, Y_\\omega)\\), we can obtain a substantially tighter bound than converting \\((\\varepsilon, \\delta)\\) guarantees using Proposition 2.6 under standard calibration. Specifically, Theorem 3.4 provides the following way to solve the problem:\n\\[\\min_{\\omega \\in \\Omega} \\omega \\quad \\text{s.t.} \\quad \\Pr[Y_\\omega > 0] - \\Pr[X_\\omega > 0] < \\eta^*\\]\nWe call this approach advantage calibration, and show how to practically ensure it in Algorithm 3 in the Appendix. Given a method for obtaining valid PLRVS \\(X_\\omega, \\Upsilon_\\omega\\) for any \\(\\omega\\), advantage calibration is guaranteed to ensure bounded advantage, which follows by combining Proposition 3.2 and Theorem 3.4:\nProposition 3.5. Given PLRVs \\((X_\\omega, Y_\\omega)\\) of a discrete-valued dominating pair of a mechanism \\(M_\\omega(\\cdot)\\), choosing \\(\\omega^*\\) using Eq. (14) ensures \\(\\eta_{\\omega^*} < \\eta^*\\).\nUtility Benefits. We demonstrate how calibration for a given level of attack advantage can increase utility. As a mechanism to calibrate, we consider DP-SGD with \\(p = 0.001\\) subsampling rate, \\(T = 10,000\\) iterations, and assume that \\(\\delta^* = 10^{-5}\\). Our goal is to compare the noise scale \\(\\sigma\\) obtained via advantage calibration to the standard approach.\nAs a baseline, we choose \\(\\sigma\\) using standard calibration in Eq. (2), and convert the resulting \\((\\varepsilon, \\delta)\\) guarantees to advantage using Proposition 2.6. We consider target values of advantage \\(\\eta^* \\in [0.01, 0.25]\\). As we show in Figure 3a, for any target value of advantage, the direct calibration procedure enables to reduce the noise scale by up to 2\\(\\times\\).\nPitfalls of Calibrating for Advantage. Calibration to a given level of membership advantage is a compelling idea due to the decrease in noise required to achieve the same level of attack risk. Despite this increase in utility, we caution that this approach comes with a deterioration of privacy guarantees other than maximum advantage. Calibrating for a given level of advantage can be particularly dangerous without understanding the risks, as it allows for increased attack success in the security-critical regime of low attack FPR (see Section 2.2). The next result quantifies this pitfall:\nProposition 3.6 (Cost of advantage calibration). Fix a dataset size \\(n > 1\\), and a target level of attack advantage \\(\\eta^* \\in (\\delta^*, 1)\\), where \\(\\delta^* = 1/cn\\) for some \\(c > 1\\). For any \\(0 < \\alpha < \\frac{1 - \\eta^*}{2}\\), there exists a DP mechanism for which the gap in FNR \\(f_{\\text{standard}}(\\alpha)\\) obtained with standard calibration for \\(\\varepsilon^*\\) that ensures \\(\\eta \\le \\eta^*\\), and FNR \\(f_{\\text{adv}}(\\alpha)\\) obtained with advantage calibration is lower bounded:\n\\[\\Delta \\beta(\\alpha) \\triangleq f_{\\text{standard}}(\\alpha) - f_{\\text{adv}}(\\alpha) \\ge \\eta^* - \\delta^* + 2 \\alpha \\frac{\\eta^*}{\\eta^* - 1}.\\]\nFor example, if we aim to calibrate a mechanism to at most \\(\\eta^* = 0.5\\) (or, 75% attack accuracy), we could potentially increase attack sensitivity by \\(\\Delta \\beta(\\alpha) \\approx 30\\) p.p. at FPR \\(\\alpha = 0.1\\) compared to standard calibration with \\(\\delta^* = 10^{-5}\\) (see the illustration in Figure 3b).\nNote that the difference \\(\\Delta \\beta\\) in Proposition 3.6 is an overestimate of the actual harm due to advantage calibration for realistic mechanisms. Indeed, the increase in attack sensitivity can be significantly lower for mechanisms such as the Gaussian mechanism (see Figure 6 in the Appendix)."}, {"title": "3.2 Safer Choice: Calibration to FNR within a Given FPR Region", "content": "In this section, we show how to calibrate the noise in any practical DP mechanism to a given minimum level of attack FNR \\(\\beta^*\\) within an FPR region \\(\\alpha \\in [0, \\alpha^*]\\). We base this notion of risk off the previous work (Carlini et al., 2022; Rezaei and Liu, 2021) which argued that MIAs are a relevant threat only when the achievable TPR \\(1 - \\beta\\) is high at low FPR \\(\\alpha\\). We instantiate the calibration problem in Eq. (9) as follows, assuming \\(M_\\omega(\\cdot)\\) satisfies \\(f_\\omega(\\alpha)\\)-DP:\n\\[\\min_{\\omega \\in \\Omega} \\omega \\quad \\text{s.t.} \\quad \\inf_{\\alpha \\in [0, \\alpha^*]} f_\\omega(\\alpha) \\ge \\beta^*.\\]"}, {"title": "3.3 Other Approaches to Trade-Off Curve Accounting", "content": "In this section, we first contextualize the proposed method within existing work. Then, we discuss settings in which alternatives to PLRV-based procedures could be more suitable.\nBenefits of PLRV-based Trade-Off Curve Accounting. Computational efficiency is important when estimat- ing \\(f_\\omega(\\alpha)\\), as the calibration problem requires evaluating this function multiple times for different values of \\(\\omega\\) as part of binary search. Algorithm 1 computes \\(f_\\omega(\\alpha)\\) for a single \\(\\omega\\) in \\(\\approx 500\\text{ms}\\), enabling fast calibration, e.g., in \\(\\approx 1\\) minute for DP-SGD with \\(T = 10,000\\) steps on commodity hardware (see Appendix F). Existing methods for estimating \\(f_\\omega(\\alpha)\\), on the contrary, either provide weaker guarantees than Proposition 3.8 or are substantially less efficient. In particular, Dong et al. (2022) introduced \\(\\mu\\)-GDP, an asymptotic expression for \\(f(\\alpha)\\) as \\(T \\to \\infty\\), that overestimates privacy (Gopi et al., 2021), and thus leads to mechanisms that do not satisfy the desired level of"}, {"title": "4 Experiments", "content": "In this section, we aim to empirically evaluate the utility improvements of our calibration methods in simulations as well as in realistic applications.\nSimulations. First, we demonstrate the noise reduction from calibrating for given error rates in the case of the DP-SGD algorithm using the setup in Section 3.1. We fix three low FPR regimes: \\(\\alpha^* \\in \\{0.01, 0.05, 0.1\\}\\), and vary maximum attack sensitivity \\(1 - \\beta^*\\) from 0.1 to 0.5 in each FPR regime. We show the results in Figure 4. We observe a significant decrease in the noise scale for all values. Although the decrease is less drastic than with calibration for advantage (see Figure 3a), in this case, we avoid the pitfalls of advantage calibration by directly calibrating for risk in the low FPR regime.\nLanguage Modeling and Image Classification. We have showed that FNR/FPR calibration enables to significantly reduce the noise scale, but it is unclear how much of this reduction in noise translates into actual utility improvement in downstream applications. To answer this question, we evaluate our method for calibrating noise in private deep learning on two tasks: text sentiment classification using the SST-2 dataset (Socher et al., 2013), and image classification using the CIFAR-10 dataset (Krizhevsky et al., 2009).\nFor sentiment classification, we fine-tune GPT-2 (small) (Radford et al., 2019) using a DP version of LoRA (Yu et al., 2021). For image classification, we follow the approach of Tramer and Boneh (2021) of training a convolu- tional neural network on top of ScatterNet features (Oyallon and Mallat, 2015) with DP-SGD (Abadi et al., 2016). See additional details in Appendix F. For each setting, by varying the noise scale, we obtain several models at different levels of privacy. For each of the models we compute the guarantees in terms of TPR \\(1 - \\beta\\) at three"}, {"title": "5 Concluding Remarks", "content": "In this work, we proposed novel methods for calibrating noise in differentially private learning targeting a given level of operational privacy risk: advantage and FNR/FPR of membership inference attacks. Using simulations and end-to-end experiments, we showed that our calibration methods significantly decrease the required level of noise compared to the standard approach at the same level of operational risk. In the case of calibration for advantage, we also showed that the noise decrease could be harmful as it could allow for increased attack success in the low FPR regime, whereas calibration for a given level of FNR/FPR mitigates this issue. Next, we discuss limitations and possible directions for future work.\nChoice of Target FPR. We leave open the question on how to choose the target FPR \\(\\alpha^*\\), e.g., whether standard significance levels in sciences such as \\(\\alpha^* = 0.05\\) are compatible with data protection regulation. Future work is needed to develop concrete guidance on the choice of target FPR informed by legal and practical constraints.\nTight Bounds for Privacy Auditing. Multiple prior works on auditing the privacy properties of ML algo- rithms (Nasr et al., 2021; Liu et al., 2021; Jayaraman and Evans, 2019; Erlingsson et al., 2019) used conversions between (\\(\\varepsilon\\), \\(\\delta\\)) and operational risks like in Proposition 2.5, which we have shown to be loose for practical mech- anisms. Beyond calibrating noise, our methods provide a tight and computationally efficient way of obtaining bounds on attack success rates for audits.\nAccounting in the Relaxed Threat Model. Although we have focused on DP, our methods apply to any notion of privacy that is also formalized as a hypothesis test. In particular, our method can be used as is to compute privacy guarantees of DP-SGD in a relaxed threat model (RTM) proposed by Kaissis et al. (2023b). Previously, there was no efficient method for accounting in the RTM.\nApplications Beyond Privacy. Our method can be applied to ensure provable generalization guarantees in deep learning. Indeed, prior work has shown that advantage \\(\\eta\\) bounds generalization gaps of ML models (Kulynych et al., 2022). Thus, even though advantage calibration can exacerbate certain privacy risks, it can be a useful tool for ensuring a desired level of generalization in models that usually do not come with non-vacuous generalization guarantees, e.g., deep neural networks."}]}