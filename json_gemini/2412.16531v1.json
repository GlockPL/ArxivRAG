{"title": "FROM CREATION TO CURRICULUM: EXAMINING THE ROLE OF GENERATIVE AI IN ARTS UNIVERSITIES", "authors": ["Atticus Sims"], "abstract": "The age of Artificial Intelligence (AI) is marked by its transformative \"generative\u201d capabilities, distinguishing it from prior iterations. This burgeoning characteristic of AI has enabled it to produce new and original content, inherently showcasing its creative prowess. This shift challenges and requires a recalibration in the realm of arts education, urging a departure from established pedagogies centered on human-driven image creation. The paper meticulously addresses the integration of AI tools, with a spotlight on Stable Diffusion (SD), into university arts curricula. Drawing from practical insights gathered from workshops conducted in July 2023, which culminated in an exhibition of AI-driven artworks, the paper aims to provide a roadmap for seamlessly infusing these tools into academic settings. Given their recent emergence, the paper delves into a comprehensive overview of such tools, emphasizing the intricate dance between artists, developers, and researchers in the open-source Al art world. This discourse extends to the challenges and imperatives faced by educational institutions. It presents a compelling case for the swift adoption of these avant-garde tools, underscoring the paramount importance of equipping students with the competencies required to thrive in an AI-augmented artistic landscape.", "sections": [{"title": "Introduction", "content": "It can be contended that we currently find ourselves in the age of Artificial Intelligence (AI). While AI has seamlessly integrated into various facets of our daily lives over the years, the distinct characteristic that sets the present wave apart is its \"generative\u201d nature. Instead of merely cataloging and organizing information as previous iterations might have, contemporary AI has the capability to synthesize and produce novel information, making it inherently 'creative'. This burgeoning shift not only introduces an innovative technical methodology for image production but also necessitates a paradigmatic rethinking in the domain of arts education. As AI delves into realms previously reserved for human imagination, it challenges traditional pedagogies and conceptual frameworks surrounding the process of image creation.\nThis paper seeks to address the integration of AI tools in university arts education. In order to explore this topic at a practical level, a series of workshops were held in July 2023 which led to a group exhibition of AI generated works created by participants in the workshops. These activities focused on understanding the techniques and processes of creating images with Stable Diffusion, an open-source generative AI tool that provides much greater control in determining the final form of an image. These workshops and subsequent one-on-one guidance of students using these technologies will be examined in the third section in order to explore possible ways in which these tools can be incorporated into university classrooms and curriculua.\nAs these tools are a recent development and represent a drastically different approach to image creation, an overview of the technology behind these tools as well as the most important technical aspects of their operation will be described in detail. First, an overview of the key terms and concepts behind AI image generation will be presented with the"}, {"title": "Overview of generative AI art tools", "content": "In this section, we will provide a basic overview of the generative AI art tools for image creation, briefly examining the of the nature of generative AI, followed by an overview of the technology behind AI image generation tools, an analysis of the open-source ecosystem that is driving generative AI art, and a practical survey of the key components of Stable Diffuion."}, {"title": "What is generative AI?", "content": "Artificial Intelligence (AI) is the capability of a machine to imitate intelligent human behavior. It encompasses systems or machines that can perform tasks that usually require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\nAI, as opposed to traditional computer programming, is based on the concept of learning rather than providing step-by-step instructions to the computer. Al programs are often referred to as 'models' which learn to identify patterns in the data in a process called 'training'. Once a model is trained, it can then make predictions or decisions based on data it has not seen before. This process is called 'inference'. This is analogous to a radiologist being trained to recognize patterns in medical images from a text book, then applying this pattern recognition ability in clinical settings with medical images they have never seen before.\nAl models can be broken down into two main categories: discriminative models and generative models. Discriminative models, as the name suggests, discriminate certain patterns in data. Examples of this are facial recognition software or optical character recognition. Generative models on the other hand, are trained to produce novel data that is similar to the training data. It can be said that the goal of generative models is to \"build a model that can generate new sets of features that look as if they have been created using the same rules as the original data.\" An example of this is instructing and image generation model to create an image of an astronaut riding a horse in the style of cubism. The result will be an image that contains the key aesthetic features of cubism while being a novel composition.\nGenerative models are probabilistic rather than deterministic, and they require a stochastic element that influences the outcome. This is often referred to as the \"seed\" or \"random seed\", which will be discussed in greater detail below [1].\nThe recent boom in AI is due primarily to advancements in generative modelling, specifically breakthroughs in large language models (LLM) such as OpenAI's GPT models. LLMs are generative models because they are trained on a large dataset (text on the internet, etc.) and produce new data that is similar to the training data, i.e., natural language. Since these models are trained on an enormous body of text produced by humans, we are able to interact with them using natural language.\nGenerative models for image creation such as Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE) have been around for a number of years, but the recent advancements in AI image creation were enabled through the integration of generative language models such as GPT. Image generation models such as Stable Diffusion differ from previous models such as GAN due to the ability to take a text description of an image and create an image that matches the verbal description. This process is known as text-to-image (TTI or t2i) and is the foundation of all commonly used AI image software in use today."}, {"title": "Overview of the technology behind AI image generation tools", "content": null}, {"title": "Early attempts at TTI", "content": "Text-to-image (TTI) models are a relatively new area of research in the field of computer vision and artificial intelligence. The first notable attempt at text-to-image synthesis was in 2014 by a team of researchers from the University of Montreal, who proposed a model called Generative Adversarial Networks (GANs), which could generate images from textual descriptions [2]. Although GANs are not technically considered TTI models, there were notable applications of GANs in early attempts at text to image generation such as the Deep Convolutional GANs (DCGANs) [3], and Attention-based GANs (AttnGANs) [4].\nAnother notable advancement in image generation is Variational Autoencoder (VAE), which plays a significant role in latent diffusion models (see Fig. 3). The core concept behind VAE is that data, for example, pixel values from an image, are encoded into a mathematical representation of the image, often referred to as \"latent space\", and then decoded to reconstruct the original image (Fig. 1). For a more detailed technical description see [5] [6] [7]."}, {"title": "Diffusion Models", "content": "In 2021, OpenAI introduced a new approach to text-to-image synthesis using the diffusion model which was first introduced in 2015 [9]. A diffusion model is trained to generate high-quality images by iteratively \"diffusing\" the image with Gaussian noise through a sequence of discrete time steps. During each time step, the model applies a series of invertible transformations to the image, which results in a sequence of progressively \"noisier\u201d representations. When used to generate novel images, a random noise image is used as the \"seed\" and the model then generates a sequence of images that become increasingly sharper and more detailed a certain number of \"steps\u201d, resulting in an image that matches the input specifications, which in the case of TTI is the 'prompt'. [10] [11]. This process can be intuitively understood by observing Fig X below. During training, noise is iteratively introduced to an image until a 'noise' image is achieved. This is represented as the \"Fixed Forward Diffusion Process\u201d in the figure. When the trained model is used to generate images, the model is provided with a noise image, the 'seed', and noise is reduced in a series of steps. This is represented in the figure as \"Generative Reverse Denoising Process\u201d. The key idea is that through the denoising process the model will seek to match the noisy image with the prompt. A loose analogy is that the visual phenomenon of pareidolia, which is the tendency to see meaningful images in random patterns such as clouds [12].\nThe diffusion model has several advantages over previous types of models, including better stability and the ability to generate higher quality images with more detail and fewer artifacts [13]. However, it also has some limitations, such as being computationally intensive and requiring a larger amount of training data [14]."}, {"title": "CLIP", "content": "A key component in the TTI revolution is the introduction of Contrastive Language-Image Pre-Training (CLIP), a neural network model developed by OpenAI in 2021 that can understand natural language text and images in a joint embedding space [16]. Simply put, a joint embedding space is like a meeting point where different types of data such as pictures and words are turned into similar formats so they can be easily compared or matched. The CLIP model is pre-trained on a large dataset of images and their associated captions, allowing it to learn to associate textual descriptions with visual concepts and features [17].\nIts purpose is to enable machines to understand natural language in the context of visual information, allowing them to perform tasks such as image classification, image retrieval, and text-to-image synthesis. [18] [19]. In text-to-image synthesis, CLIP plays a critical role in enabling models to generate images that match a given textual description. One of the unique features of CLIP is its ability to perform cross-modal retrieval, which means it can retrieve images that are semantically similar to a given textual description and vice versa. This makes CLIP highly versatile and applicable to a wide range of tasks beyond text-to-image synthesis, including image classification, image retrieval, and visual question answering [20]\nThe use of transformers, the architecture used in CLIP, is a significant contributor to the recent advancements in computer vision with significant advantages over convolutional neural networks utilized in GANs. Distinguishing features of transformers include bidirectional feature encoding and a capacity for large-scale pre-training, allowing them to process multiple modalities such as text, audio, images and video [21], In addition to the advances in transformer architecture, the release of CLIP source code [22] by OpenAI under the MIT License has been fundamental in the current explosion of generative media software."}, {"title": "Latent Diffusion Models", "content": "The CompVis group at LMU Munich introduced the latent diffusion model (LDM) in a paper published in December 2021 [23]. In contrast to previous diffusion models, the LDM works in latent space [6] [3] instead of pixel space. This means that instead of generating images directly as pixels, the LDM is trained to encode raw data such as images and text into compressed mathematical representations (latent space). The diffusion process is then carried out on these compressed representations, the result of which which is then decoded as pixels. The key advantages of the LDM over previous diffusion models are that it allows for better control over the generated images, it can generate images at higher resolutions with fewer artifacts than previous diffusion models [14], and due to the compression of images,\nthe training of these models can be carried out with significantly less computation and results in much smaller model sizes that can be run on consumer hardware [24]..\nThe original latent diffusion model was further developed and trained on the LAION-5B image dataset [25] with the support of Stability.AI, a London-based AI startup [26], resulting in significant improvements in image quality and model compression [26]. The model and source code were publicly released in August 2022 as Stable Diffusion under a Creative ML OpenRAIL-M license, allowing commercial and non-commercial use [27]. This resulted in the spawning of hundreds of model variants and community driven innovations that have been incorporated into open-source implementations of the core model [28] and has seen unprecedented adoption among developers [29]."}, {"title": "The Generative AI Art Ecosystem", "content": "In the previous section, a technical overview of the development of the primary tools and frameworks utilized in generative AI image creation was given. These developments represent significant advancements in the fields of computer vision and artificial intelligence that have proven to be transformative in how media is created, and their applications have proven to have a significant impact on society. In the case of artistic and creative media such as image, video and sound, diffusion based models have garnered much attention in the public sphere, focusing both on their capabilities as well as the disruptive effects they are having on artistic production and practices. In this section I will examine the role of community led development of generative art tools, focusing primarily on the open-source development that utilizes Stable Diffusion as its basis."}, {"title": "TTI Frameworks", "content": "The three primary TTI frameworks that have been made publicly available are DALL-E, Midjourney and Stable Diffusion. OpenAI's DALL-E 2 was opened for research preview in July 2022 and made publicly accessible in September [30]. This model garnered significant attention due to it being the first TTI capable of producing high quality images from text prompts. OpenAI has not released the source code for DALLE-2, and at present it is only accessible through OpenAI's website as a paid service. Midjourney is an \"independent research lab [31]\u201d whose TTI service entered open beta in July 2022\u00b9 and operates on a similar credit subscription business model as DALL-E 2. Training data sets and the underlying TTI model used by Midjourney are unknown, but it is assumed that it shares similar training data and architecture with OpenAI and Stable Diffusion. Image generation in Midjourney is conducted entirely through a discord server though formatted commands.\nAlthough sites such as these allow for the fast generation of high quality images, these services, like DALL-E 2 and Midjourney, are limited to image generation and lack the breadth of affordances that are available in open-source implementations of Stable Diffusion. For the remainder of this article, I will focus solely on open-source tools that are actively used in the generative AI art communities."}, {"title": "Low-code/No-code User Interfaces for still images", "content": "Prior to the release of Stable Diffusion, there were a number of AI art frameworks that utilized low-code/no-code UIs, primarily Google Colab [32]. Google Colab is a browser based Python notebook similar to Jupyter, which allows users to write and run Python based machine learning applications that run on Google cloud servers [33]. Due to their notebook format, it is possible to use a shared machine learning Colab notebook with little or no knowledge of coding, and the use of cloud compute enabled artists to use computationally intensive processes (such as GANs) without a"}, {"title": "Key Components of Stable Diffusion", "content": "To grasp the basic operation of Stable Diffusion, it becomes crucial to understand the integral components that steer image generation. For the context of this section, the Automatic1111 user interface serves as our focal point of analysis. However, it's essential to note that the predominant aspects guiding and controlling image generation can be extended to other popular user interfaces, such as ComfyUI. These aspects are intrinsic to the Diffusers Pipeline, the predominant open-source code framework tailored for developing applications based on diffusion-centric generative AI technology.\nDelving into the intricacies of Stable Diffusion, it's pivotal to recognize that its foundational architecture is an assemblage of subcomponents, each playing a distinctive role, as depicted in Fig. 3. Each facet of the Stable Diffusion architecture emanated from diverse research endeavors. Furthermore, multiple variants of each component exist, each capable of modifying the final image output in different magnitudes. Although every element of the base architecture offers the potential for manipulation, this article's scope will be confined to those predominantly utilized in artistic workflows."}, {"title": "Primary Parameters and Controls", "content": "Checkpoints Frequently dubbed as the 'model', the checkpoint stands out as the pivotal component in AI image generation. Stability AI, the entity behind Stable Diffusion, has unveiled multiple iterations of the Stable Diffusion base model (Table 1). Trained on expansive image datasets like the LAION 5B, these models harness the computational might of extensive GPU arrays. As open-source entities, these base models provide a springboard for developers to craft bespoke models-either by supplementing images or amalgamating existing checkpoints. This has catalyzed the emergence of a plethora of custom models, accessible on platforms like CivitAI. The eclectic range of these models accentuates the allure of open-source software in AI image generation.\nSeed At the core of Stable Diffusion lies the concept of the seed value. Each seed value engenders a distinct noise image, foundational to the diffusion process. It's paramount to discern that identical seed values will replicate an image, while variations will yield different outcomes.\nSteps The 'Steps' setting delineates the iteration frequency of noise image diffusion until the final rendition. Its relevance is intertwined with the Sampling Method discussed subsequently. Generally, an incremental step count elevates image quality, albeit with diminishing returns beyond a specific threshold.\nSampling Method Referred to interchangeably as 'sampler' or 'scheduler', the sampling method reconstructs the image post each diffusion iteration. From the artist's perspective, the crux lies in recognizing the variance in image outcomes based on the sampling method chosen.\nPrompt Prompting remains pivotal to TTI generation. In the context of Stable Diffusion, the emphasis is on iterative experimentation. The Automatic1111 interface also introduces a 'Negative prompt' feature, allowing artists to delineate unwanted elements in the final image.\nWithin Stable Diffusion, prompts can be weighted, highlighting their relative significance. This weighting can be achieved using nested parentheses or through a formatted directive like (text:weight), offering granular control over the diffusion process. For example, (happy dog: 0.8) will assign a weight of 0.8 to the phrase 'happy dog'"}, {"title": "Extensions", "content": "Understanding the intricacies of AI image generation necessitates diving into extensions that elevate the capability of base models. Herein, we discuss prominent extensions, their functionalities, and how they empower artists in their creative endeavors.\nExtra Networks Extra Networks is a collection of three specific fine-tuning methodologies: textual inversion, LORA, and Hypernetworks. However, due to the predominant use and efficacy of the more recent LoRA models, Hypernetworks will not be covered in this discussion.\nThough each of these methodologies has distinct underlying technologies, they all share a common principle. Think of them as supplementary layers appended to the principal checkpoint, influencing the style or subject of a resultant image.\nTextual Inversions Often termed 'embeddings', textual inversions are unique trigger words predominantly used as negative prompts. Their primary function is to suppress undesired attributes within the generated image.\nLORA An acronym for Low Rank Adaption, LoRA's origins can be traced back as a technique developed to fine-tune substantial language models [37]. Its adaptation to the realm of image diffusion has been transformative, positioning it as a worthy successor to the less efficient Hypernetworks. Fundamentally, the training of LoRA instills new weights into a base model without tampering with its original structure. A pivotal aspect of LoRA is its compatibility. A LORA model honed on a specific Stable Diffusion base model seamlessly integrates with any custom model under the same category. However, cross-category compatibility is absent.\nTraining a LoRA model is quite flexible. While as few as 10 images can suffice, a range of 30 to 300 images is typically recommended. Other merits of LoRA include modest processing power requirements and swift training times. Such attributes have cemented LoRA's position as a quintessential tool for AI artists, as evidenced by the plethora of LORA models on platforms like CivitAI.\nLoRA's versatility is evident in its broad categorizations: style, aesthetic, and subject. When initiating a LoRA,\nspecific prompt formats, often inclusive of weight assignments, are employed. Additionally, certain trigger words may be mandated. An intriguing facet is the potential to amalgamate multiple LoRAs within a singular image (Table 3) to mix multiple styles, aesthetics or subjects in unique ways.\nControlNet Introduced in February 2023, ControlNet is a groundbreaking neural network architecture tailored for spatial conditioning control within diffusion models. It seamlessly amalgamates various computer vision techniques, from depth and edge detection to pose estimation and object segmentation, offering artists unprecedented control over image composition [38].\nBefore ControlNet's advent, artists relied on diverse tactics for directing image composition. ControlNet's emergence reshaped this landscape, offering artists a streamlined, potent mechanism for image composition guidance.\nIn essence, both ControlNet and LoRA represent monumental advancements in the AI image generation sphere. They have equipped AI artists with an unparalleled degree of compositional and stylistic control, particularly in a domain characterized by its inherent randomness and unpredictability."}, {"title": "AI image generation tools and education", "content": null}, {"title": "A Constructionist Perspective on Learning Generative AI Tools: A Comparative Analysis with Photography", "content": "This section elucidates the learning process of generative AI tools through the lens of Seymour Papert's constructionist theory of learning and education [39]. By juxtaposing AI image creation with the well-established medium"}, {"title": "Pedagogical Considerations within the Constructionist Framework", "content": "Building upon the discussion in the previous section, this segment aims to address key pedagogical considerations for teaching AI art tools.\nFor artists to effectively utilize these tools to bring their imaginative visions to life, a foundational understanding of the basic concepts is essential. Drawing parallels with photography, it's noteworthy that a wide spectrum of technical proficiency can elevate an image's aesthetic quality. However, technical mastery isn't strictly necessary to produce aesthetically pleasing images. Esteemed photographers and artists such as Ansel Adams, Helmut Newton, Andy Warhol, and Robert Mapplethorpe often leveraged the simplicity of Polaroid cameras to profound artistic effect.\nSimilarly, artists can produce original artworks without delving into the granular parameters and extensions described earlier, offering more refined control over images. This is evidenced by artists who predominantly employ tools like MidJourney through straightforward prompting. Notably, many of the foundational concepts of AI image generation software can be intuitively grasped through analogy or heuristics, rendering them accessible to those without a technical background.\nIn line with the constructionist pedagogical approach, instruction should strike a balance: it should endow all students with a sufficient understanding of image generation, without inundating those less technically inclined with jargon or overly detailed descriptions. Given that these tools are open-source, those with a technical appetite can probe as"}, {"title": "Workshops, case studies and exhibition", "content": "In order to explore the feasibility of teaching open source software for AI image generation to university students with varying technical abilities it was decided to hold two workshops to teach the fundamentals of image creation with open-source AI tools.\nThese workshops were held on July 19 and July 26 2023 and were open to undergraduates and graduates from any department at the university. Calls for participants were conducted through the posting of fliers around the university,\nword of mouth, and an announcement on the university's digital bulletin system (Seika Portal). In total, 47 students from various departments enrolled in the workshops.\nThe workshops were designed such that students with no experience using these tools could participate in either the\nfirst workshop, the second workshop or both workshops. Additionally, online text and video tutorials were created and\nshared with participants as reference, and participants were invited to a private Discord server so that students could\nshare their work, provide additional learning resources and receive technical support.\nAt the end of each workshop a call was made for students to continue to explore AI tools in order to create works\nfor an exhibition which was held from August 26th-September 3rd at the university. In total, three students from the\nworkshops and one student who did not participate in the workshops created works for exhibition using the AI tools\ndiscussed here. The creation of exhibition ready images required a much greater degree of knowledge and technique,\nso the information presented at the workshops was expanded upon greatly over the following month by providing\nindividual instruction and technical assistance to the exhibiting artists. A more detailed examination of this process is\noutlined below."}, {"title": "Workshops", "content": "When planning the workshops, the following assumptions were made:\n\u2022 Students will primarily use Apple computers, eliminating the possibility of installing the software locally"}, {"title": "Results of the workshops", "content": "Following the workshop, a survey was sent to all workshop participants requesting feedback about their perceptions of AI art, the tools introduced and their interest in attending future workshops or officially organized courses. Unfortunately, the number of respondents to the request were too few to present quantitative data in this paper.\nHowever, I would like to present some qualitative data based on my observations and from informal discussions with students which will influence the planning and conducting of future classes and workshops.\n1. The setup and launching of the Google Colab notebook was complicated, and a number of students experi-enced trouble following the steps\n2. The time required to initiate the Automatic1111 UI was quite long due to limited cloud processing speed on free accounts and the requirement for downloading large files from the internet to the cloud drive.\n3. A certain percentage of participants received errors during the initiation process, causing frustration and a need to restart the process from the beginning.\n4. Once the technical difficulties were overcome, students generally had no problem using the software. Some students followed the instructions of the teacher precisely, while others immediately began experimenting and creating unique images.\n5. Many students expressed an interest in participating in future workshops."}, {"title": "Analysis of Workshop Results", "content": "The feedback and observations from the workshop on AI image creation using open-source tools provide valuable insights into both the challenges faced by the students and the potential areas of improvement for the facilitators."}, {"title": "Initial Technical Hurdles", "content": "\u2022 A prominent challenge was the technicality involved in setting up and launching the Google Colab notebook. Students struggled with this initial step, highlighting a potential need for clearer instructions or a more user-friendly interface.\n\u2022 The long initiation time of the Automatic1111 UI, compounded by the cloud processing speed constraints of free accounts and the need to download sizable files, further exacerbated the learning curve. This might have deterred some students or negatively impacted their enthusiasm in the early stages."}, {"title": "Errors and User Experience", "content": "Technical errors during the initiation process were encountered by a segment of the participants. The necessity to start over after encountering such issues can be a significant source of frustration. This indicates a potential need for refining the process, improving user guidance during this phase, or both."}, {"title": "Positive Engagement Post-Technical Setup", "content": "\u2022 Notably, after crossing the initial technical barriers, students seemed to navigate the software effectively. This suggests that the software, when running, is intuitive or that the instructions provided during the workshop were sufficient.\n\u2022 It's worth noting the dichotomy in approach: some students adhered strictly to the workshop guidelines, while others opted for a more exploratory route. This highlights the diverse learning and creative styles of the participants, and future workshops might consider offering dual paths or flexible guidance to cater to both types of learners."}, {"title": "Interest in Continued Learning", "content": "Despite some of the initial technical challenges, there was a tangible interest among students to participate in subsequent workshops. This speaks to the inherent allure of AI image creation and suggests that if some of the initial barriers are addressed, engagement and satisfaction levels among participants could be even higher."}, {"title": "Recommendations and Future Steps", "content": "Considering the feedback and observations, a few recommendations can be made:\n\u2022 Refinement of the Initial Setup: Simplifying the setup process or providing additional guidance during the initial steps can reduce early-stage friction for students. This could be achieved by using a paid cloud computing service which would result in more, fewer steps for initiation, more efficient loading, and no need for downloading large files."}, {"title": "Learning by Making: A Collaborative Exploration", "content": "Following the described workshops, three participants, coming from distinct academic backgrounds and having varied prior experiences with AI, volunteered to produce artworks using AI tools. These works were showcased in a week-long group exhibition at Kyoto Seika University, commencing on 26 August 2023."}, {"title": "Student Profiles", "content": "1. Student 1:\n\u2022 Affiliation: A first-year master's student from the printmaking department.\n\u2022 Background: Lacked prior exposure to AI tools and had limited experience with digital art tools.\n2. Student 2:\n\u2022 Affiliation: A fourth-year undergraduate from the printmaking department.\n\u2022 Background: Engaged in AI-related artworks for his graduation, familiar with MidJourney and ChatGPT, and had a basic understanding of Stable Diffusion and principles of diffusion models.\n3. Student 3:\n\u2022 Affiliation: A third-year undergraduate majoring in architecture from the faculty of design.\n\u2022 Background: Proficient in AI art tools, recognized as a top architectural model creator on CivitAI. He showcased proficiency in creating and publishing various LoRA and custom models. Familiarity with Stable Diffusion was complemented by extensive work with MidJourney."}, {"title": "Artistic Endeavors and Pedagogical Approaches", "content": "\u2022 Student 1: Inspired by her emulsion wash silkscreen prints, the objective was to emulate this style in her AI creations. This necessitated the creation of a style-specific LoRA derived from her prints.\n\u2022 Student 2: His exploration revolved around the confluence of human spirituality \u2013 focusing on Buddhism and Shintoism \u2013 with AI and the philosophical debate of machine spirituality. Drawing from conversations with ChatGPT on the topic, his vision was to produce images combining ancient Buddhist iconography with futuristic styles, encapsulating machine intelligence. To actualize this, he proposed training a LoRA grounded in imagery from ancient Japanese religious art.\n\u2022 Student 3: With a vision to demonstrate the capabilities of AI in image creation, he aspired to amalgamate 200 diverse AI-generated images into one expansive artwork. Additionally, he displayed some of his architectural visualizations."}, {"title": "Analysis from a Constructionist Learning Perspective", "content": null}, {"title": "Learning by Doing:", "content": "The hands-on experience of the students clearly aligns with the constructionist principle of \"learning by doing.\" [40] Rather than merely consuming knowledge, the students were actively involved in producing tangible artworks. This experiential learning allowed the students to deeply understand the AI tools in context, connecting their academic knowledge to real-world applications."}, {"title": "Creation as Reflection of Understanding:", "content": "As the students produced artworks, their creations became a reflection of their understanding. The nuances in their artistic choices, their interaction with the AI tools, and the final artworks all manifest their evolving comprehension of AI and its artistic possibilities. This aligns with the constructionist belief that when learners create something meaningful, it provides a tangible artifact of their understanding [41]."}, {"title": "Personalization of Learning:", "content": "Each student brought their unique background and interests to the table, from emulsion wash silkscreen prints to explorations of spirituality and architectural visualizations. In constructionist learning, this personal context is pivotal. It aids in making the learning experience more engaging and ensures the knowledge gained is relevant to the individual learner's context [42]."}, {"title": "Iterative Process and Feedback:", "content": "The iterative testing, use of the shared Miro Board for feedback, and the continuous refinement of their artworks mimic the iterative processes emphasized in constructionist pedagogy. Continuous feedback and iteration not only refine the end product but also deepen understanding and promote resilience in the face of challenges."}, {"title": "Collaboration and Shared Learning:", "content": "The collaborative nature of the learning process, evident in the shared board and the group exhibition, aligns with the constructionist perspective that learning is a social process. By collaborating, students could share insights, provide feedback, and learn from one another's experiences."}, {"title": "Bridging the Knowledge Gap through Mentorship:", "content": "The instructor's involvement in providing technical assistance, especially in the more complex areas like training the LORA or enlarging images, exemplifies a scaffolding approach. This guided mentorship, where learners are supported in bridging their knowledge gaps, is a vital component of constructionist learning, allowing learners to undertake and succeed in tasks they might not manage alone [43]."}, {"title": "Emphasis on Understanding Over Memorization:", "content": "By engaging with the foundational concepts of LoRA training and the nature of digital images, students were encouraged to prioritize deep understanding over rote memorization. This approach, where students understand the \"why\" behind processes, is central to constructionist pedagogy [43]."}, {"title": "Learning in a Real-world Context:", "content": "The goal of creating artworks for an actual exhibition provides a real-world context to the learning process. Such authentic tasks make the learning meaningful and are a hallmark of constructionist pedagogy."}, {"title": "Discussion", "content": "It is widely believed that the current Al revolution is one of the most significant changes in human history. In a recent interview, AI pioneer Geoffrey Hinton stated that it is as important as the invention of the printing press or the invention of the wheel [44"}]}