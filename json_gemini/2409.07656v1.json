{"title": "Passed the Turing Test: Living in Turing Futures", "authors": ["Bernardo Gon\u00e7alves"], "abstract": "The world has seen the emergence of machines based on pretrained models, transformers, also known as generative artificial intelligences for their ability to produce various types of content, including text, images, audio, and synthetic data. Without resorting to preprogramming or special tricks, their intelligence grows as they learn from experience, and to ordinary people, they can appear human-like in conversation. This means that they can pass the Turing test, and that we are now living in one of many possible Turing futures where machines can pass for what they are not. However, the learning machines that Turing imagined would pass his imitation tests were machines inspired by the natural development of the low-energy human cortex. They would be raised like human children and naturally learn the ability to deceive an observer. These \u201cchild machines,\u201d Turing hoped, would be powerful enough to have an impact on society and nature.", "sections": [{"title": "What Is the Turing Test?", "content": "In 1950 (1), Alan Turing proposed to replace the question \u2018Can machines think?' with a new question based on what he called \u201cthe imitation game\u201d (p. 433) and his \u201ctest\u201d (pp. 446-447, 454). In its most familiar form, the new question was whether a machine playing A, the deceiver, could imitate B, the human assistant, in a remotely played conversation game to pass as B in the eyes of an average human interrogator playing C, the judge.\nNote that the challenge of conscious impersonation and deception on the machine side slips into the problem of mere indistinguishability in the judgment of the human playing C, \u201cwho should not be expert about machines\u201d (2). It is often asked why Turing made player C a non-specialist, thus making the test somewhat easier for the machine. Looking at the historical context (3, 4), we see that he was challenging his critics' confidence in the unassailable superiority of humans over other species in nature, to which he half-seriously included machines. For example, Turing referred to intelligent machines in a letter as \u201canother species of the thinking genus\" (3). Having an ordinary human identify the machine in a blind test seems like a lighthearted way of posing that challenge. To illustrate that, Turing had his imaginary machine mimic human cultural stereotypes. For example, it would pose as an enlightened English individual who could compose a Shakespearean sonnet and discuss it metaphorically in relation to Mr. Pickwick (1, p. 446), a character in the literary work of the English writer Charles Dickens."}, {"title": "The Turing Test and Early AI", "content": "Turing's test inspired early artificial intelligence (AI) scientists and has long been the most widely recognized criterion for machine intelligence. John McCarthy and Claude Shannon referred to it in their collection Automata Studies (Princeton, 1956), as \"the Turing definition of thinking\" and Turing's \u201cstrong criterion.\" Together with Marvin Minsky and Nathaniel Rochester, McCarthy and Shannon in 1955 defined \u201cthe AI problem\u201d as \u201cthat of making a machine behave in ways that would be called intelligent if a human were so behaving\u201d (5, p. 7). In 2013, when asked about Turing's test in a taped interview, Minsky said: \u201cThe Turing test is a joke, sort of, about saying \u2018A machine would be intelligent if it does things that an observer would say must be being done by a human.\u201d This materially connects the early definition of \"the AI problem\" to Turing's test.\nIn the late 1960s, Minsky advised Stanley Kubrick and Arthur Clarke on their screenplay 2001: A Space Odyssey, which featured the Turing test-passing HAL:\n\"The sixth member of the crew cared for none of these things, for it was not human. It was the highly advanced HAL 9000 computer, the brain and nervous system of the ship Whether HAL could actually think was a question which had been settled by the British mathematician Alan Turing back in the 1940s Turing had pointed out that, if one could carry out a prolonged conversation with a machine without being able to distinguish between its replies and those that a man might give, then the machine was thinking, by any sensible definition of the word . . . HAL could pass the Turing test with ease.\"\nAfter HAL, Turing's test would become legendary.\nEvery time AI succeeds in automating a new task that would require intelligence if performed by humans, \u201cthe Turing definition\u201d conquers new territory, and the importance of Turing's early message becomes clearer. The elegance of the Turing test definition, and the reason it has stood the test of time, lies in Turing's observation that human intelligence itself was largely unknown, and would likely remain so for some time. He was responding to one of his contemporaries who quoted Ren\u00e9 Descartes to argue for the special place of the human brain and language in nature (4, Ch. 4). Especially in the absence of a widely accepted definition of human intelligence, it is not surprising that machine intelligence will ultimately be judged by the tasks it can perform."}, {"title": "A Thought Experiment", "content": "Turing predicted that \u201cat the end of the century\u201d a learning machine would be able to play the imitation game well and pass the test, and that talk of \u201cmachines thinking\" would be commonplace in \"the general educated opinion.\u201d At the time he made these two predictions (1, p. 442), a computer storage capacity of 10\u00ba units was still fanciful speculation (see Fig. 1).\nHowever, instead of specifying a controlled experiment, Turing continuously varied the conditions of his test (having player B as a woman, a man, another machine, etc.), and in effect used it as a thought experiment to argue for machine intelligence (4, Ch. 5). He also wrote that the \"only really satisfactory support\u201d that can be given for the two predictions would be \u201cdoing the experiment described\u201d (1, p. 455). Did he mean recruiting women, men, machines, etc. for a practical \u201cimitation game\u201d? No, absolutely not. The gender and the machine-versus-human elements were a half-serious way of responding to his critics (3). Turing immediately shifted the focus to research on learning machines: \u201cWhat steps should be taken now if the experiment is to be successful?\u201d (1, p. 455). The rhetoric of his end-of-the-century experiment can be best understood as part of his propaganda for what he thought, writing in 1948 (6), \u201cwould probably have some effect\u201d in convincing critics and opponents: \u201cthe actual production of machines.\u201d This would be the realization of his thought experiment.\nIf anyone still wanted to seriously pursue a literal reading of Turing's imitation game and perform practical \u201cTuring\u201d tests using human participants, the research of Joseph Weizenbaum in the mid-1960s suggested that there was no point in doing so (7). Using preprogramming and psychological methods, Weizenbaum presented evidence that people's attitudes towards talking machines may depend not only on the machines' behavior, but also on their own drives and prior assumptions. Turing had actually noted this in 1948 (6): \u201cThe extent to which we regard something as behaving in an intelligent manner is determined as much by our own state of mind and training as by the properties of the object under consideration.\""}, {"title": "\"The Nature of an Adequate Proof\"", "content": "Weizenbaum wrote that his experiment was a \u201cstriking form of Turing's test\u201d (7, p. 42). However, Turing was not interested in cheap deception and psychological tricks. He implied on several occasions that the ability of a machine to learn by itself was the key to an adequate proof of concept for machine intelligence, and thus the proper approach to preparing it for his test.\nThe machines he envisioned in 1946 would be able to change their structure autonomously by learning from experience, like brains, \"... by changing its neuron circuits\" through \u201cthe growth of axons and dendrites\u201d (8). Writing in 1950, he suggested that the ability to represent human-like fallibility should be acquired as a by-product of the learning process: \u201cAnother important result of preparing our machine for its part in the imitation game by a process of teaching and learning is that \u2018human fallibility' is likely to be omitted [from the teaching] in a rather natural way, i.e., [learned] without special \u2018coaching \u201d . . . Processes that are learnt do not produce a hundred per cent, certainty of result; if they did they could not be unlearnt\u201d (1, p. 459). Overall, Turing did not conceive of machine intelligence without a learning foundation.\nLater, in 1951, he further developed the link between learning and proving the concept of his test (9):\nMy contention is that machines can be constructed which will simulate the behaviour of the human mind very closely It would be the actual reaction of the machine to circumstances that would prove my contention, if indeed it can be proved at all . . . Let us go rather more carefully into the nature of this 'proof.'\nHe continued:\nIt is clearly possible to produce a machine which would give a very good account of itself for any range of tests, if the machine were made sufficiently elaborate.\nHowever, this again would hardly be considered an adequate proof. Such a machine would give itself away by making the same sort of mistake over and over again, and being quite unable to correct itself, or to be corrected by argument from outside. If the machine were able in some way to \u2018learn by experience' it would be much more impressive. If this were the case there seems to be no real reason why one should not start from a comparatively simple machine, and, by subjecting it to a suitable range of 'experience' transform it into one which was more elaborate, and was able to deal with a far greater range of contingencies.\nTuring then distinguished the above method based on learning from preprogramming:\nThis process could probably be hastened by a suitable selection of the experiences to which it was subjected. This might be called 'education.' But here we have to be careful. It would be quite easy to arrange the experiences in such a way that they automatically caused the structure of the machine to build up into a previously intended form, and this would obviously be a gross form of cheating, almost on a par with having a man inside the machine.\nThis explains why Turing saw research on \u201clearning machines\u201d as the steps that \u201cshould be taken if the experiment [his test] is to be successful\u201d (1, p. 455). Preprogrammed machines are irrelevant to the test, as they are the same as having \u201ca man inside.\u201d They are prone to \u201cLady Lovelace's objection\u201d (10), \u201cwhich stated that the machine can only do what we tell it to do.\" For this reason, preprogrammed machines could never be a solid technological basis for making talk of \u201cmachine thinking\u201d commonplace in the \u201cgeneral educated opinion\u201d (1, p. 442)."}, {"title": "Turing Test Passed", "content": "The \"attention\u201d mechanism (11, 12) of the transformer architecture, and its AI systems, has yielded important empirical results in the imitation of human behavior. Hype aside, it has laid the groundwork for proving the concept associated with Turing's test argument: the existence of machines built with a relatively simple logical structure, whose intelligence grows by learning from experience to perform well at tasks once thought to be the province of humans in nature. Moreover, they can do so without resorting to preprogramming and Weizenbaum-style tricks.\nImportantly, the intelligence of generative transformers grows with the scaling of the model and its pretraining data (13). There are tasks where their ability can be seen to increase gradually, and tasks where their ability emerges at a critical scale (14), typically because the latter involve brittle metrics. This happens, for example, in arithmetic tasks such as addition, which Turing used to illustrate that machines should be allowed to make mistakes, especially as they learn. Presenting the imitation game, he made the machine miscalculate the addition of 34,957 to 70,764: \u201c(Pause about 30 seconds and then give as answer) 105621\u201d (1, p. 434). Transformers can eventually learn to add numbers without being taught directly. However, because an addition is either right or wrong, the ability suddenly appears at a critical scale.\nThis is a phenomenon that Turing himself postulated for machine intelligence. In discussing how machine learning addresses \u201cLady Lovelace's objection,\u201d he wrote (1, p. 454):\nOne could say that a man can 'inject' an idea into the machine, and that it will respond to a certain extent and then drop into quiescence, like a piano string struck by a hammer . . . Another simile would be an atomic pile of less than critical size: an injected idea is to correspond to a neutron entering the pile from without Each such neutron will cause a certain disturbance which eventually dies away If, however, the size of the pile is sufficiently increased, the disturbance caused by such an incoming neutron will very likely go on and on increasing until the whole pile is destroyed . . . Is there a corresponding phenomenon for minds, and is there one for machines? There does seem to be one for the human mind.\nAlthough the intelligence of transformers increases as they gain experience and may eventually reach a critical point to acquire a skill, this does not mean that they are capable of conscious impersonation and deception. It just means that they prove the concept of machine intelligence in a way that is close enough to what Turing considered an \u201cadequate proof\u201d in connection with his test.\nIt is often said that transformers can only remember, and imitation does indeed require memorization. However, sustaining an extended conversation requires some intelligence beyond mere memorization. To see this, note that a purely memorizing machine would have to store answers to every possible scenario presented by an interlocutor, including their combinations, as in Turing's example of associating a sonnet with a literary character. This would require storage exponential in the length of the test. Following a similar observation, and considering the Planck constant, Stuart Shieber used an estimate of the information capacity of the entire known universe as an upper bound on memory. Based on memory alone, even such an extremely large machine could only sustain a conversation for less than a minute (15).\nIn any case, the learning efficiency of transformers is still very low, limited by a low-level architecture. On learning efficiency, Turing wrote (1, p. 457):\nThe use of punishments and rewards can at best be a part of the teaching process. Roughly speaking, if the teacher has no other means of communicating to the pupil, the amount of information which can reach him does not exceed the total number of rewards and punishments applied. By the time a child has learnt to repeat \u2018Casabianca' he would probably feel very sore indeed, if the text could only be discovered by a 'Twenty Questions' technique, every \u2018NO' taking the form of a blow. It is necessary therefore to have some other \u2018unemotional' channels of communication. If these are available it is possible to teach a machine by punishments and rewards to obey orders given in some language, e.g., a symbolic language.\nThat is, although Turing saw learning as the basis of all intelligent computing, he also considered the efficiency and control of the learning process. In analogy to the human child, machine learning should scale and reach higher levels of abstraction through the use of language.\nHere, sensorimotor technologies and multimodal approaches using multiple data modalities and embedding mechanisms open new possibilities, as Turing hoped, \u201cto provide the machine with the best sense organs that money can buy,\u201d and allow the learning process to \u201cfollow the normal teaching of a child ... Things would be pointed out and named, etc.\u201d (1, p. 460)."}, {"title": "Living in Turing Futures", "content": "For Turing, narrowing the gap \"between what machine and brain can do\u201d was indeed, as he wrote in a 1951 letter (3), \u201clargely a quantitative matter.\u201d However, hardware capacity alone, he noted, would not be enough: \u201cPerhaps we may have enough capacity, but just won't find an appropriate programme.\u201d Writing about computer architecture in 1946, Turing expressed suspicion of the \u201ctradition of solving one's difficulties by means of much equipment rather than by thought\u201d (16, p. 352). Several years earlier in 1925, he had written: \u201cI always seem to want to make things from the thing that is commonest in nature & with the least waste in energy\u201d (16, p. 19). Turing followed such naturalistic principles. He sought the natural and social development of the relatively low-energy cortex of a human child as a target model for learning machines (1, p. 457). In contrast, generative AI overexploits natural resources by consuming unsustainable amounts of computing power. As it becomes a general-purpose technology, AI must become sustainable by moving from power-hungry machine learning to Turing's nature-inspired science.\nFurther, when early digital computers were on the verge of replacing human computers, who were mostly women, the computer pioneers, with one exception, were oblivious to this near-term prospect. The exception was Alan Turing. After Douglas Hartree tried to reassure the public that computers would not automate \u201cthought,\u201d only \u201clabor,\u201d Turing responded that \"the masters,\u201d not just \u201cthe servants,\u201d were also \u201cliable to get replaced\" (10). He feared that those in positions of power would try to undermine intelligent machines in order to maintain their dominance. He suggested that automation should affect people equally in society, not continually displace the lower class of workers and benefit only a few owners of the means of production. Against the social division of labor, his position was not far from the idea that the wealth created by intelligent machines should be nationalized or socialized (17).\nTuring's suspicions about the uses of machines under the control of a few may have helped to motivate his conception of \u201cchild machines\u201d (1). They would be able to impersonate different profiles to deceive people when necessary, such as when they are supposed to prove their intelligence. His vision is in part a warning against our anthropocentrism and over-dominant place in nature. Pushed to its limits, it may find a representation in 2001's HAL.\nWhether to prevent dystopian futures or to steer towards utopian ones, the question of AI evaluation seems critical, and here again we can look to Turing for inspiration.\""}, {"title": "Turing-like AI Testing", "content": "As AI systems are increasingly deployed in high-stakes scenarios, we may need to move beyond aggregate metrics and static benchmarks of input-output pairs, such as the Beyond the Imitation Game Benchmark (BIG-bench) (14). We should be prepared to evaluate an Al's cognitive abilities in a way that resembles the realistic settings in which it will be used. This can be done with modern Turing-like tests (see Fig. 2).\nA first element to consider for this research direction is the introduction of adversarial testing, as in Turing's test, but without any human players involved (18). A second element is the design of statistical protocols, as first proposed in the 1990s for Turing-like tests based on interactive proofs (19). These are probabilistic proofs designed to impose an asymmetry that exploits the computational resources of the prover (in our case, player A) relative to the verifier (player C), thus preventing the latter from being gamed. Turing-like AI testing could be a robust approach to emerging problems such as data contamination (the machine playing C should be able to use data retrieval and augmentation to generate challenging new instances at test time) and poisoning (the machine playing C should statistically cover the data domain at scale and thus be able to detect training vulnerabilities in the AI under test).\nTuring hoped \"that machines [would] eventually compete with men in all purely intellectual fields\u201d (1, p. 460). However, he left us with his test, now stripped of its discursive and rhetorical elements and considered at the level of conceptual foundations, as in the early days of AI."}]}